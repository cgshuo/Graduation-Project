 Planning adequate audit strategies is a key success factor in a posterion X  fraud detection, e.g., in the fiscal and insurance domains, where audits are intended to detect tax evasion and fraudulent claims. A case study is presented in this paper, which illustrates how techniques based on classification can be used to support the task of planning audit strategies. The proposed approach is sensible to some conflicting issues of audit planning, e.g., the trade-off between maximizing audit benefits vs. minimizing audit costs. A methodological scenario, common to a whole class of similar applications, is then abstracted away from the case study. The limitations of available systems to support the identified overall KDD process, bring us to point out the key aspects of a logic-based database language, integrated with mining mechanisms, which is used to provide a uniform, highly expressive environment for the various steps in the construction of the considered case-study. Knowledge discovery in databases, data mining, classification, decision trees, fraud detection, logic-based database languages, integration of querying and mining. Fraud detection is becoming a central application area for knowledge discovery in databases, as it poses challenging technical and methodological problems, many of which are still open [6, 201. A major task in fraud detection is that of constructing models, or projiles, of fraudulent behavior, which may serve in decision support systems for: -preventing frauds (on-line fraud detection), or -planning audit strategies (a posteriori fraud detection.) The first case is typical of domains such as credit cards and mobile telephony [5, 181. The case of a posteriori fraud detection, applications, namely whenever we are faced with the problem of KDD-99 San Diego CA USA constructing models by analyzing historical audit data, to the in the fiscal and insurance domain, where an adequately targeted audit strategy is a key success factor for governments and insurance companies. In fact, huge amounts of frauded resources may be recovered in principle from well-targeted audits: for declarations in Italy is estimated between 3% and 10% of GNP [19]. This explains the increasing interest and investments of governments and insurance companies in intelligent systems for audit planning. 
From the point of view of KDD technology, we observe that, while data mining tools (classification, clustering, association rules,...) have reached a state of relative maturity, the overall 
KDD process, consisting of many crucial steps, is poorly supported by the available environments and systems. This is a severe limitation at two different levels: -at the design level, where a helpful methodological scenario -at the system level, where an integrated environment is 
These limitations are evident in fraud detection, where specific issues make the KDD process hard, including the complexity of data preparation, the need of combining diverse knowledge extraction paradigms, the need of sophisticated, domain-dependent model evaluation. 
In this paper, we present a case study on fiscal fraud detection, developed within a project aimed at investigating the adequacy and sustainability of KDD in the detection of tax evasion. The paper discusses: -the potential effectiveness of a KDD process oriented to -a methodology which supports the design of such KDD -an integrated environment for the development of such KDD constraints on the available resources (human and financial) to carry out the audits themselves. Therefore, planning has to face two conflicting issues: -maximizing audit benefits, i.e., define subjects to be selected -minimizing audit cosis, i.e., define subjects to be selected for 
Although these options are pertinent to the political level, the capability of designing systems for supporting this form of decisions are technically precise challenges: is there a KDD methodology for audit planning which may be tuned according to these options? What and how data mining tools may be adopted? 
How extracted knowledge may be combined with domain knowledge to obtain useful audit models? These are some of the questions addressed in Section 3 of the present paper: our experience in the case study indicates that classification-based 
KDD is an extremely promising direction for fiscal fraud detection, and calls for future larger-scale efforts. However, our approach is aimed at abstracting from the particular case study, and identifying the lines of a methodology for a whole class of applications -those centered on planning audit strategies. 
Besides the case study and the methodology, our third contribution is to show how a logic-based database language, integrated with mining mechanisms, can be conveniently used to formalize and develop the overall KDD process. The methodology adopted in the case study is formalized, in Section 4, using a query language that integrates the capabilities of deductive rules with the inductive capabilities of classification. concepts of KDD and data mining. Therefore, this section is limited at setting the general stage where the case study has been deployed, while finer grade details are clarified during the technical presentation. In 121, two different approaches in knowledge extraction are highlighted. -Hypothesis Verification: is a top-down approach, which -Knowledge Discovery: is a bottom-up approach, which starts When dealing with a concrete case of analysis, both approaches are interleaved, switching from one to the other in different phases of the same project. In the literature, knowledge discovery is further characterized as either directed or undirected. In directed knowledge discovery, the goal of the process is to explain the value of some particular attribute in terms of others. Usually a particular attribute is selected, which is called target, and the system is asked to estimate, classify, or predict the target attribute. In undirected knowledge discovery there is no target attribute, and the system is asked to identify patterns, or regularities, or relationships in the data that may be significant. A typical data mining tool for directed knowledge discovery is classification, whereas typical tools for undirected knowledge discovery are clustering and association rules. 
In this paper, we refer to a process of directed knowledge discovery, which consists of the following phases: a) Identify sources of available data b) Detect the model of analysis c) Prepare data for analysis d) Reduce data e) Build and train the model f) Evaluate the model 
These phases drive the presentation of the experiments conducted classifiers will learn from preclassified data where the target attribute is already known. In the phase of data preparation preclassified data will be partitioned into two subsets. The first subset is the training-set, and it will be used for building the initial classifier. The second subset is the test-set, which will be evaluate the classifiers on the basis of various quality indicators, both domain-independent and domain dependent. Decision tree is the classification technique used in this paper. 
Such a choice is a natural one when predictive models are to be developed. In particular, decision trees are perceived as a consolidated tool with high explanatory capability, as the obtained classifiers can be readily expressed using rules. Simple, explanatory models are needed when, as in this case study, it is crucial to convince the user of the usefulness of the KDD approach. On the other hand, a parallel approach using regression models [3] has been pursued with similar results. We are aware that other classification tools may be adopted, such as Bayes net model: however, the main focus of this research was the methodology needed to tailor the KDD process to the target class of applications. 
In our experiments, we used C5.0 [16], the most recent available version of the decision tree algorithm that J.Ross Quinlan has been evolving and refining for many years. C5.0 is an extension of the well-known C4.5 algorithm [15]. This classification tool offers several advanced mechanisms, which revealed useful in tuning the predictive model: -Pruning level, which allows tuning the severity of tree -Misclassification weights, which allows defining different -Adaptive boosting [73: is a technique that builds a sequence 
In this Section, a methodology for constructing profiles of fraudulent behavior is presented, aimed at supporting audit 176 planning. The reference paradigm is that of the KDD process, in the version of direct knowledge extraction [2]. The reference technique is that of classification, using decision trees [3, 151. 
Although we describe in detail the specific case study, emphasis is placed on the methodological issues of design and control of the 
KDD process, to the purpose of identifying the similarities of a whole class of applications. 
The dataset used in the case study consists of information from tax declarations, integrated with data from other sources, such as social benefits paid by taxpayers to employees, official budget documents, and electricity and telephone bills. Each tuple in the dataset corresponds to a (large or medium size) company that filed subject to refer to such companies. The initial dataset consists of 80643 tuples, with 175 numeric attributes (or features), where only a few of those are categorical. From this dataset, 4103 tuples correspond to audited subjects: the outcome of the audit is recorded in a separate dataset with 4103 tuples and 7 attributes, one of which represents the amount of evaded tax ascertained by the audit: such feature is named recovery. The recovery attribute takes value zero if no fraud is detected. 
It is important to notice that the subset of 4103 audited subjects was selected for an audit by the tax authority on the basis of a formalized procedure, based on expert X  X  knowledge. Therefore, the predictive models developed in the rest of this paper should be correctly understood as the final part of an overall, replicable process of audit selection and planning. Despite its relatively small size, the dataset of audited subjects has therefore a remarkable value. 
Together with domain experts, a cost model has been defined, to be included in the predictive model. In fact, audits are very expensive in both human and financial resources, and therefore it is important to focus audits on subjects that presumably return a high recovery. The challenging goal is therefore to build a classifier, which selects those interesting subjects. 
An intuitive example to explain the adopted cost model can be found in the marketing domain. Consider the problem of building the customer profile in a mail order company: a possibility is to assign either a cost or a profit to the customer reaction to a promotion of the kind  X  X ho replies gets a gift X : -if customer replies with an order, then there is a profit (= -if customer replies without an order, then there is a cost (= -if customer does not reply, then there is a cost (= mailing 
The cost model in our case study is developed, according to this approach, as follows. First, a new attribute audit-cost is defined, as a derived attribute, i.e., a function of other attributes. audit-cost represents an estimation, provided by the domain expert, of the cost of an audit in proportion to the size and the complexity of the subject to be audited. Next, we define another derived attribute actual-recovery, as the recovery of an audit after the audit costs. Therefore, for each tuple i, we define: The attribute actual-recovery is used to discriminate between subjects with a positive or negative value for such attribute. The key point is in using the cost model within the learning process itself, and not only in the evaluation phase. It is our conviction that this approach scales to a whole class of applications, namely whenever we are faced with the problem of detecting frauds  X  X  posteriori X , by analyzing historical audit data, to the purpose of planning effectively future audits. The target variable of our analysis is constructed from actual-recovery, by defining the class of actual recovery -cur in short -in such a way that, for each tuple i: The goal is a predictive model able to characterize the positive subjects, which are eligible to be audited. It should be noted that the proposed cost model does not consider such things as limitations on number of feasible audits or resources to perform audits. Indeed, the adaptation to limitations on resources and particular audit strategies is demanded to the phases of construction and tuning of the predictive model, described in the rest of the paper. Data trunsfotmation. Although out of the scope of the present paper, it is worth noting that this phase was extremely time consuming, due to the presence of legacy systems, huge operational databases (hierarchical and relational), inconsistent measure units and data scales. Data cleaning (row removal.) Noisy tuples, i.e., those with excessively deviating attribute values, have been removed, as well as those tuple with too many null attribute values. After data cleaning, the dataset consisted of total 3880 tuples: 3183 in negative car (82%), and 697 in positive car (18%). Attribute selection (column removal.) The selection of relevant attributes is a crucial step, which was taken together with domain experts. The available 175 attributes were reduced 20, by removing irrelevant, derived ones. set is an important parameter in a classification experiment. While model also increases, as the training error (i.e., the misclassification rate on training-set tuples) decreases. This does not imply that large training-sets are necessarily better: a complex model, with a low training error, may behave poorly on new tuples. This phenomenon is named overfitting: the classifier is excessively specialized on the training tuples, and has a high misclassification rate on new (test) tuples. The classical remedies to overfitting include downsizing the training-set, and increasing the pruning level. In our case study, we adopted an incremental samples approach classifiers over increasingly larger, randomly generated subsets of the dataset -lo%, 20%, 33%, 50%, 66%, 90% of the total dataset. We discovered that the resulting classifiers improve with increasing training-sets, independently from the pruning level. In complexity of the knowledge to be extracted. 
As a consequence, the 3880 tuples in the dataset were partitioned as follows: -training set: 35 14 tuples; -test set: 366 tuples. are trained to distinguish between positive cur (fruitful audits) and negative cur (unfruitful audits). Once the training phase is over, the actual-recovery (= ascertained evaded tax -audit cost) obtained from the audits of the subjects from the test-set which are classified as positive. This value can be matched against the real case, where all (366) tuples of the test-set are audited. This case, which we call Real, is characterized by the following (throughout the paper, #S denotes the cardinality of the set S): . audit#(Real) = #(test-set) = 366 . actual~recovety(Real) = . audit-costs(Rea1) = c. ,EleS, ser _ audit-costs(i) = 24.9 M Euro where recovery and costs are expressed in million euro X  X . As the whole dataset consists of audited subjects, by comparing the previous values of the Real case with those of the subjects classified as positive by the various classifiers, it is possible to evaluate the potential improvement of using data mining techniques to the purpose of planning the audits. Therefore, the classifiers resulting from our experiments are evaluated according to the following metrics, which represent domain-independent (1 and 2) and domain-dependent (3 through 6) indicators of the quality of a classifier X: where: 1. the confusion matrix, which summarizes the prediction of -TN = (i 1 predx(i) = cur(i) = negative) is the set of tuples -FP = (i 1 predx(i) = positive A cur(i) = negative) is the set -FN = (i 1 predx(i) = negative A car(i) = positive] is the set -TP = {i 1 predx(i) = car(i) = positive} is the set of tuples 2. the misclassification rate of X is the percentage of 3. the actual recovery of X is the total amount of actual 5. the profitability of X is the average actual recovery per audit, 6. finally, the relevance of X relates profitability (a domain-
We considered two distinct approaches to classifier construction, each driven by two different policies in audit planning: on one hand, we can aim at keeping FP as small as possible, in order to minimize wasteful costs. On the other hand, we can aim at keeping FN as small as possible, in order to maximize evasion recovery. Unless extremely precise (nearly ideal) classifiers X  are  X  An ideal classifier is one where FP = FN = 0. reached, which is rather unlikely in practice, the two policies are clearly conflicting: in order to keep FP small, we have to unbalance the tree construction procedure towards negatives, and therefore TP shrinks accordingly, while both FN and TN inevitably become larger. The situation is dual when we aim at keeping FN small. The first policy is preferable when resources for audits are limited, the second when resources are in principle infinite. In practice, it is needed to find an acceptable trade-off between the two conflicting policies, by balancing the level of actual recovery with the resources needed to achieve it. The classifier construction method is therefore presented highlighting the parameters that may be adequately tuned to reach the desired trade-off. The following is the list of main tuning methods used in our case study, which we perceive as most relevant for the whole class of applications. -The pruning level: the absence of overfitting enabled us to -The misclussifica~ion weights: these are constants that can be -The replication of minority class in the training-set: -The adaptive boosting: the idea is to build a sequence of We now present four classifiers, as an illustration to the above construction techniques, and assess their quality and adequacy to the objectives. The first two classifiers follow the  X  X inimize FP X  policy, whereas the other two classifiers follow the  X  X inimize FN X  policy. Experiment A simply uses the original training-set, and therefore we obtain a classifier construction biased towards on the majority class of training-set, i.e., the negative car. As a consequence, we enforce the  X  X inimize FP X  policy without using misclassification weights. To reduce errors, we employ lo-trees adaptive boosting; the number of voting trees was selected after trying different values. The confusion matrix of the obtained classifier is the following: 
Classifier A prescribes 59 audits (11 of which wasteful), and exhibits the following quality indicators: -misclassification-rate(A) = 22% (81 errors) -actual-recovery(A) = 141.7 M Euro -audit-costs(A) = 4 M Euro -profitability(A) = 2.401 -relevance(A) = 1.09 
Profitability of model A is remarkable: 141.7 M euro are recovered with only 59 audits, which implies an average of 2.401 
M euro per audit. If compared with the case Real, model A allow to recover 88% of the actual recovery of Real with 16% of audits. 
The following chart compares recovery the two cases. 
Experiment B brings to an extreme the  X  X inimize FP X  policy, by using the dataset with negative majority in conjunction with misclassification weights that make FP errors count twice as 
Adaptive boosting (3 trees) is also adopted. Again, the value of misclassification weights and the number of voting trees was selected after trying different values. The confusion matrix of the obtained model B is the following: 
Classifier B prescribes only 12 audits (2 of which wasteful), and exhibits the following quality indicators: -misclassification-rate(B) = 30% (110 errors) -actuuZ-recovery(B) = 15.5 M Euro -audit-costs(B) = 1.1 M Euro -profitability(B) = 1.291 -relevance(B) = 0.43 
The following chart compares recovery and number of audits in the Real case and model B. 
The small actual recovery of model B is due to the very small number of audits, and the accordingly large number of fruitful audits missed. This model, however, may be useful when resources for carrying out audits are very limited, or when combined with other models. 
Experiment C adopts the  X  X inimize FN X  policy, and tries to bias the classification towards the positive car. A training-set with replicated positive tuples is prepared, with a balanced proportion of the two classes, in conjunction with misclassification weights that make FN errors count thrice as much as FP errors (i.e., weight of FP = 1 and weight of FN = 3). Adaptive boosting (3-trees) is also adopted. The confusion matrix of the obtained model 
B is the following: 
Classifier C prescribes 188 audits (more than 50% of which wasteful), and exhibits the following quality indicators: -misclassification-rate(C) = 34% (126 errors) -actual-recovery(C) = 165.2 M Euro -audit-costs(C) = 12.9 M Euro -profitability(C) = 0.878 -relevance(C) = 0.25 
The following chart compares recovery and number of audits in the Real case and model C. 
Surprisingly enough, the actual recovery of model C is higher than that of the Real case, despite only 50% of audits are prescribed. This is due to the high number of subjects classified 
TN, which avoids many wasteful audits. Profitability is however sensibly lower than that of the models adhering to the  X  X inimize 
FP X  policy. 
Experiment D brings to an extreme the  X  X inimize FN X  policy, by using the replicated dataset with balanced classes in conjunction with misclassification weights that make FN errors count four times as much as FP errors (i.e., weight of FP = 1 and weight of 
FN = 4.) No boosting was adopted here. The confusion matrix of the obtained model D is the following: 
Classifier D prescribes 210 audits (50% of which wasteful), and exhibits the following quality indicators: -misclassification-rate(D) = 36,6% (134 errors) -actual-recovery(D) = 163.5 M Euro -audit-costs(D) = 14.4 M Euro -profitability(D) = 0.778 -relevance(D) = 0.21 
Compared with model C, FN has shrunk, so less fraudulent subjects escape an audit. Nevertheless, the actual recovery decreases, due to a larger FP. The following chart compares recovery and number of audits in the Real case and model D. 
More sophisticated models can be constructed by suitably combining diverse classifiers together. For instance, predictions of two classifiers can be put in conjunction, by considering fraudulent the subjects classified as positive by both classifiers. 
Conversely, predictions can be put in disjunction, by considering fraudulent the subjects classified as positive by either classifiers. 
The following are the indicators for the model A A C, which prescribes 58 audits: -actual-recovety(A A C) = 141.7 M Euro -audit-costs(A A C) = 3.9 M Euro -projitability(A A C) = 2.44 and the indicators for the model A V C, which prescribes 189 audits: -actual-recovety(A V C) = 16.5.1 M Euro -audit-costs(A V C) = 13.0 M Euro -projitability(A V C) = 0.87 Clearly, conjunction is another means to pursue the  X  X inimize 
FP X  policy, and conversely disjunction for the  X  X inimize FM policy. The first policy usually yields more profitable models, as the examples show. This form of combination may be iterated, e.g. combining the classifier A A C with another classifier E (a trade-off between B and D), obtaining a model A A C A E which prescribes 43 audits: -actual-recovety(A A C A E) = 56.0 M Euro -audit-costs(A A CA E) = 3.2 M Euro -profitability(A A C A E) = 1.3 and a model (A A C)V E which prescribes 80 audits: -actual-recovery((A A C) V E) = 144.0 M Euro -audit-costs((A A C) V E) = 5.2 M Euro -projitability((A A C) V E) = 1.8 
Classifiers can be combined also by voting. The following model is constructed by letting A, B, C and D cast a vote. Classifier E 
B, C and D. The resulting model E-arbitrates(A,B,C,D) prescribes 80 audits: -actual-recovery(E-arbitrates(A,B, C, D)) = 145.4 M Euro -audit-costs(E-arbitrates(A, B, C, D)) = 5.3 M Euro -profitability(E-arbitrates(A,B,C, D)) = 1.8 1 
In the final example, the combined classifier considers fraudulent set of 4. The resulting model at_least_3(A,B,C,D) prescribes 61 audits: -actualgecovety(atJeast_3(A,B,C,D)) = 143.8 M Euro -audit_costs(at_least_3(A,B, C, D)) = 4.2 M Euro -profitability(atJeast_3(A,B, C, D)) = 2.35 
The objective of this Section is to demonstrate how a logic-based database language, such as LDL++ [I, 13, 231, can support the various steps of the KDD process described in Section 3. We sketch how the methodology described in the previous Section can be systematically developed in a uniform, integrated environment using the deductive capabilities of LDL++. For lack of space, we shall omit a presentation of LDL++, and confine ourselves to mention that it is a rule-based language with a 
Prolog-like syntax, and a semantics that extends that of relational database query languages with recursion. Other advanced mechanisms, such as non-determinism [9], non-monotonicity [8, 221, and external calls [l], make LDL++ a highly expressive query language, and a viable system for knowledge-based applications. Two extensions of LDL++ with data mining capabilities have been proposed, and experimented in market basket analysis [IO, 11, 171. The original datasets are organized in two tables, or relations: the table general-subject containing the tuples with all their features, say Fl, . . , F20, and the table audited-subject containing the historical records of auditing outcomes. Subjects in the two tables are connected through the primary key ID. The LDL++ database schema is defined as follows: According to the methodology of Section 3.2, we need to join the two tables on the ID key, and to add the cost model as a new attribute. This can achieved through the following rule: The effect of this rule is to create a new relation (or view) ID key, extended with a derived attribute Actual-Recovery computed as specified in Section 3.2. Here, the audit-cost relation, whose rules are omitted, computes the cost of an audit as a function of the other attributes. Finally, the target feature CAR is added as another derived attribute, defined as the class (positive or negative) of actual recovery. The other phases of data preparation, such as data cleaning and attribute selection, are not described in detail here, although easily expressible in LDL++. For instance, the detection of noisy tuples (see Section 3.3) is readily expressed in LDL++ using recursion and counting over the list of attributes; a similar operation is rather awkward in SQL and other relational query languages. As an example, the following recursive rules the number of null features of the tuple ID. The construction of the training set can be automated by defining deterministically a sample S of the dataset, whose size is P% of total tuples. The definition of the relation, which uses the non-deterministic construct and the set mechanism, is omitted. It only the keys are recorded in it. 
The tuples that are not member of the training-set are automatically chosen for the test-set, as follows: 
Using LDL++, it is possible to define rules for automatic incremental tuning of parameters. For instance, we can specify a predicate that initially calls the induction of a tree using 10% of the tuples for training and evaluates the tree. The induction of a tuples is invoked. The process stops when tree quality stops improving. Such iterative tuning process can be used for all parameters (pruning factor, boosting, misclassification costs). 
The open architecture of LDL++ allows the use of external calls to other software packages, database systems, etc. We use this feature to interface to the external classification algorithm (C5.0), which is invoked by the tree-induction predicate, with the following parameters: pruning factor level (PF), misclassification costs (MC) and boosting (60). The following rule manages the process by first constructing a training-set of a specified size, and then calls the external induction program on the training-set. rules, in a standard format for the employed classification rules. Therefore the list of rules representing a tree has the following form: each feature and the final element for the predicted class. In general, Rulei = [Cl, Ca, , . , CZO, Predicted-CAR], where: 
For instance, the rule: is represented by the following list: 
The predicate prediction establishes the class predicted by a given tree (Tree-name) for the test tuple identified by ID, and the rule (PredictionRule) which is used to classify the tuple. precondition is satisfied by the feature values of a test-set tuple. 
Check of rule preconditions is performed through the relation satisfy, whose (simple) definition is omitted. To our purposes, it of the list: 
If the first rule does not succeeds in classifying the tuple, we recurse over the remaining rules: 
Finally, if no rule succeeds in classifying the tuple, the default class is assigned: 
The quality indicators of a classifier discussed in Section 3.4 may be conveniently defined by using the aggregates provided by 
LDL++. -# misclassification errors: count the tuples where predicted -# audits: count the tuples classified as positive -actual recovery: sum actual recovery of all tuples classified -relevance: -# Tp: count positive tuples classified as positive -# FP: count negative tuples classified as positive -# FN: count positive tuples classified as negative -# TN: count negative tuples classified as negative 
Model combination, as described in Section 3.55, is conveniently formalized in LDL++. For instance, the following rules specify tree conjunction: a tuple is classified as positive by Tl A T2 iff both Tl and T2 predict the tuple as positive. 
Beyond the various model combinations described in Section 3.5.5, we used LDL++ to perform model evaluation using cross validation, which confirmed the results discussed earlier. 
Beyond the techniques used in this paper, more sophisticated combinations of classifiers can be also accommodated, such as the meta-learning method [4]. In meta-learning, we consider a number of base classifiers and a test-set whose tuples are modified by incorporating the predictions of the base classifiers as new features. This modified test-set is then used the as a training-set for inducing a meta classifier. Two variants of meta-learning are usually considered: -class combiners: the tuples in the meta training-set have as -class-attribute combiner: the tuples in the meta training-set 
The rules for constructing the two variants of meta training-sets are the following. In this paper, a methodology for constructing profiles of fraudulent taxpayers is presented, aimed at supporting audit planning. The reference paradigm is that of the KDD process, in the version of direct knowledge extraction. The employed technique is that of classification, using decision trees. Although we described in detail the specific case study, emphasis was placed on the methodological issues of design and control of the KDD process, to the purpose of identifying the similarities of a whole class of applications. Briefly, the proposed methodology focuses on the following issues: The first consideration coming from the experience sketched in this paper is about the complexity of the KDD process. While the objectives of the various phases of the KDD process are clear, little support is provided to reach such objectives. Two main issues are, and will remain in the near future, the hot topics of KDD community research agenda: -the first point is methodology: it is crucial to devise methods -the second point is the need to identify the basic features of This paper tried to demonstrate how a suitable integration of deductive reasoning, such as that supported by logic database languages, and inductive reasoning, such as that supported by decision trees, provides a viable solution to many high-level problems in a particular class of applications. In particular, we showed how useful such integration is in the phase of model evaluation, where the uniform representation in the same logic formalism of data for the analysis and the results of the analysis itself allows a high degree of expressiveness. Put another way, we believe that the key to succeed in constructing effective decision support systems is the ability of integrating the results of knowledge extraction with expert rules -in our cases the cost model and the domain-dependent quality indicators. We then maintain that a logic database framework is the flexibility of a general-purpose programming language is combined with the simplicity of a rule-based language. the deductive capabilities of a logic-based database language, LDL++ [ 11, with the inductive capabilities of diverse data mining algorithms and tools, notably association rules, clustering and the Datasift system. One of the crucial choices is the form of coupling between the database and the data mining tools. So far, we have different coupling strategies for the various data mining tools, namely tight coupling for association rules and loose coupling for classification and clustering. Work in progress is trying to reach a better trade-off between a tighter coupling of querying and classification, and efficiency of execution. Another issue for future research is a thorough formalization of the methodology for the class of applications discussed in this paper, using the logic-based approach. However, our first necessary further step is to scale up both the environment and the methodology to larger datasets -in particular to larger sets of taxpayers. We are indebted with Maurizio Verginelli (SOGEI, Rome), the domain expert who made our study possible. Many thanks are owing to our colleagues Giuseppe Manco, Mirco Nanni and Domenico (Mimmo) Sac&amp;, who collaborated in the project, for many fruitful discussions. We are also grateful to Ross J. Quinlan for his comments on an earlier version of this paper, and to the referees for careful reviewing. This research has been carried out within a collaboration between the Italian Ministry of Finance and the PQE2000 program, and has been partially supported by SOGEI s.p.a. under grant n. A07C981137, Intelligent systems for the detection of tax evasion. [31 [41 [71 [91 [lo] Giannotti, F., Manco, M., Pedreschi, D., Turini, F. Using [I l] Giannotti, F., Manco, G., Nanni, M., Pedreschi, D., Turini, [12] Indurkhya, N., Weiss, S. M. Predictive data mining: a [13] LDL++ web site. htto://www.cs.ucla.edu/ldl/ [14] Quinlan, J. R. Simplifying decision trees. International [15] Quinlan, J. R. C4.5: Programs for Machine haming. [16] Rulequest Research web site. httn://www.ruleouest.com/ [17] Shen, W., Ong, K., Mitbander, B., Zaniolo, C. Metaqueries [18] Stolfo, S., Fan, D., Lee, W., Prodromidis, A., Chan, P.. [19] Tanzi, V., Shome, P. A Primer on Tax Evasion. In ZMF Staff [20] Uthurusamy, R. From Data Mining to Knowledge Discovery: [21] Winston P. H., Artificial Intelligence (3rd Edition). Addison [22] Zaniolo, C., Ami, N., Ong, K. Negation and Aggregates in [23] Zaniolo, C., Ceri, S., Faloutsos, C., Snodgrass, R.T., 
