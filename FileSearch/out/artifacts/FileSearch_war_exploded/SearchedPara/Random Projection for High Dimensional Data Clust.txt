 Xiaoli Zhang Fern xz@ecn.purdue.edu Carla E. Brodley brodley@ecn.purdue.edu High dimensionality poses two challenges for unsuper-vised learning algorithms. First the presence of irrele-vant and noisy features can mislead the clustering algo-rithm. Second, in high dimensions data may be sparse (the curse of dimensionality), making it dicult for an algorithm to nd any structure in the data. To ame-liorate these problems, two basic approaches to reduc-ing the dimensionality have been investigated: feature subset selection (Agrawal et al., 1998; Dy &amp; Brodley, 2000) and feature transformations, which project high dimensional data onto \interesting" subspaces (Fuku-naga, 1990; Chakrabarti et al., 2002). For example, principle component analysis (PCA), chooses the pro-jection that best preserves the variance of the data. In this paper we investigate how a relatively new trans-formation method, random projection (Papadimitriou et al., 1998; Kaski, 1998; Achlioptas, 2001; Bingham &amp; Mannila, 2001), can best be used to improve the clustering result for high dimensional data. Our mo-tivation for exploring random projection is twofold. First, it is a general data reduction technique. In contrast to other methods, such as PCA, it does not use any de ned \interestingness" criterion to \opti-mize" the projection. For a given data set, it may be hard to select the correct \interestingness" crite-rion. Second, random projection has been shown to have special promise for high dimensional data cluster-ing. In 1984, Diaconis and Freedman showed that vari-ous high-dimensional distributions look more Gaussian when randomly projected onto a low-dimensional sub-space. Recently, Dasgupta (2000) showed that random projection can change the shape of highly eccentric clusters to be more spherical. These results suggest that random projection combined with EM clustering (of Gaussian mixtures) may be well suited to nding structure in high dimensional data.
 However, the drawback of random projection is that it is highly unstable { di erent random projections may lead to radically di erent clustering results. This instability led us to investigate a novel instantiation of the cluster ensemble framework (Strehl &amp; Ghosh, 2002) based on random projections. In our frame-work, a single run of clustering consists of applying random projection to the high dimensional data and clustering the reduced data using EM. Multiple runs of clusterings are performed and the results are aggre-gated to form an n n similarity matrix, where n is the number of instances. An agglomerative clustering algorithm is then applied to the matrix to produce the nal clusters. Experimental results on three data sets are presented and they show that the proposed cluster ensemble approach achieves better clustering perfor-mance than not only individual runs of random pro-jection/clustering but also EM clustering with PCA data reduction. We also demonstrate that both the quality and the diversity of individual clustering so-lutions have strong impact on the resulting ensemble performance.
 In this section we rst illustrate that although a single run of random projection may lead to a less than sat-isfactory clustering result, it is possible to uncover the natural structure in the data by combining the results of multiple runs. We then present a new approach to clustering high dimensional data that rst combines the information of multiple clustering runs to form a \similarity" matrix and then applies an agglomerative clustering algorithm to produce a nal set of clusters. 2.1. A Single Random Projection A random projection from d dimensions to d 0 dimen-sions is a linear transformation represented by a d d 0 matrix R , which is generated by rst setting each en-try of the matrix to a value drawn from an i.i.d N(0,1) distribution and then normalizing the columns to unit length. Given a d -dimensional data set represented as an n d matrix X ,where n is the number of data points in X , the mapping X R results in a reduced-dimension data set X 0 .
 We applied random projection to a synthetic data set that consists of two thousand data points forming four Gaussian clusters in a fty-dimensional space. For this data, we chose a random projection that reduces the data to ve dimensions 1 producing data set X 0 .The EM algorithm was then applied to cluster X 0 .Al-though the real number of clusters is known, we used the Bayesian Information Criterion (BIC) (Fraley &amp; Raftery, 1998) to determine the number of clusters k because in most real applications k is unknown. In addition, the natural number of clusters may vary in di erent subspaces (Dy &amp; Brodley, 2000). Using the rst two principle components, Figure 1 plots the original clusters and two representative ex-amples of the clusters formed by random projection with EM clustering (RP+EM). The number of clus-ters chosen by RP+EM varied from run to run (in the gure both RP+EM results have two clusters) and oc-casionally RP+EM found all four clusters. The scat-ter plot of the RP+EM results suggest that random projection may distort the underlying structure of the data and result in unstable clustering performance. To some degree this contradicts Dasgupta's results (2000), which show that random projection preserves the sepa-ration among Gaussian clusters. However, Dasgupta's results were averaged across many runs and when the projected dimension was small, the results tended to have large variance. An alternative explanation is that for this data set, random projection needs more than ve dimensions to preserve the structure. There have been results (Achlioptas, 2001) about the required di-mensionality for a random projection to e ectively pre-serve distance . However, to our knowledge it is still an open question how to choose the dimensionality for a random projection in order to preserve separation among clusters in general clustering applications. On viewing these results, the rst conclusion one makes is that for this data set an individual run of RP+EM (with dimensionality of ve) produces subop-timal results. Further inspection reveals that di erent runs may uncover di erent parts of the structure in the data that complement one another . From Figure 1(b) and (c) we observe that each run uncovered some par-tial, but di erent, structure in the data. This suggests that the combination of multiple runs of RP+EM may lead to better results { we may be able to reveal the true structure of the data even without the knowledge of how to choose a proper dimensionality to preserve the original cluster structure. 2.2. Multiple Random Projections We combine the results of multiple runs of RP+EM with a two-step process. First, we aggregate the clus-tering results into a matrix that measures the \simi-larity" between each pair of data points. Then an ag-glomerative clustering algorithm is applied to produce the nal clusters. We discuss these steps below. Aggregating multiple clustering results: Fo r each run of random projection, EM generates a prob-abilistic model of a mixture of k Gaussians 2 in the projected d 0 -dimensional space. For data point i ,the soft clustering results P ( l j i; ) ;l =1 ;:::;k are given, representing the probability that the point belongs to each cluster under the model . We de ne P ij as the probability of data point i and j belonging to the same cluster under model and it can be calculated as: To aggregate multiple clustering results, the values of P ij are averaged across n runs to obtain P ij , an \esti-mate" of the probability that data point i and j belong to the same cluster. This forms a \similarity" matrix. We expect the P ij values to be large when data point i and j are from the same natural cluster and small oth-erwise. To test our conjecture, we performed thirty 3 runs of RP+EM on the synthetic data set and sepa-rated the aggregated P ij values into two groups based on if data point i and j are from the same cluster. Figure 2 shows the histograms of both groups of P ij values. It can be seen that the distributions of the two groups have di erent means and little overlap, which supports our conjecture. Note that if the two distri-butions are completely separated the true clustering structure can be easily recovered by thresholding P ij 's. Producing the nal clusters: To produce the nal clusters from the aggregated \similarity" matrix P , we apply an agglomerative clustering procedure whose basic steps are described in Table 1.
 In implementing the agglomerative algorithm, we need to de ne the similarity between two clusters and de-termine the proper number of clusters for a given data set. In our implementation, we de ne similarity as: This is equivalent to the complete-link distance-based agglomerative clustering algorithm (Duda &amp; Hart, 1973). We chose this de nition to ensure that when two points have very small \similarity" value (i.e., small possibility of belonging together according to P ) the algorithm will not group them together.
 In the experiments we observe that some data points are not similar to any other data points. Intuitively the decision of merging should not be based on these points because they can be the \outliers" of the data set. To avoid the impact of such points, we remove them during the merging process and assign them to the formed clusters afterward. Speci cally, we calcu-late the maximum similarity between data point i to the other data points as P max ( i )=max n j =1 P ij ,where j 6 = i . In the merging process, we discard 10% of the data points with the smallest P max values. After merg-ing we then assign these points to their most similar clusters, where the similarity between a data point i and a cluster c k is de ned 4 as : 1 k c To decide the cluster number k , we cannot apply commonly used techniques such as BIC because our method does not generate any description or model for the clusters. To solve this problem, we propose to continue merging until only a single cluster remains and at each step plot the similarity between the two clusters selected for merging. Figure 3 shows the plot of these similarity values for the synthetic data set de-scribed in Section 2.1. We observe a sudden drop of the similarity when the algorithm tries to merge two real clusters. In the experiments on other data sets, we also observed similar trends from the plots. This suggests that we can use the occurrence of a sudden similarity drop as a heuristic to determine k . Using the steps described above, we combined the re-sults of thirty runs of RP+EM and successfully recov-ered all four clusters from the synthetic data set with 100% accuracy.
 In summary, our method consists of three steps: 1) generate multiple clustering results using RP+EM, 2) aggregate the results to form a \similarity" matrix, and 3) produce nal clusters based on the matrix. Note that there are many di erent choices we can make for each of the three steps. Section 4 discusses the desirable properties for the rst step and a possible strategy for improving our approach. Our experiments are designed to demonstrate: 1) the performance gain of multiple random projections over a single random projection, and 2) that our proposed ensemble method outperforms PCA, a traditional ap-proach to dimensionality reduction for clustering. 3.1. Data Sets and Parameter Settings We chose two data sets with a relatively high dimen-sionality compared to the number of instances (HRCT and CHART) and one data set with a more traditional ratio (EOS). Table 2 summarizes the data set charac-teristics and our choice for the number of dimensions that we used for random projection and PCA. HRCT is a high resolution computed tomography lung image data set with eight classes (Dy et al., 1999). CHART is a data set of synthetically generated control chart time series with six di erent types of control charts (Het-tich &amp; Bay, 1999). EOS is an eight-class land cover classi cation data set. Although the class labels are available for all three data sets, they are discarded in the clustering process and only used during evaluation. We selected the dimensionality of PCA for each data set by requiring that 85% of the data variance be pre-served. For the CHART and EOS data sets, we set the dimensionality for RP to be the same as PCA in order to have a more direct comparison. However, for the HRCT data set, we chose a much smaller num-ber than that chosen for PCA for computation time reasons. 3.2. Evaluation Criteria Evaluating clustering results is a nontrivial task. Be-cause our method does not generate any model or de-scription for the nal clusters, internal criteria such as log-likelihood and scatter separability (Fukunaga, 1990) can not be applied. 5 Because our data sets are labeled, we can assess the cluster quality by using mea-sures such as conditional entropy and normalized mu-tual information (Strehl &amp; Ghosh, 2002). We chose to report results for both criteria because as we ex-plain below entropy is biased toward a large number of clusters and normalized mutual information under some conditions is biased toward solutions that have the same number of clusters as there are classes. Conditional Entropy (CE): Conditional entropy measures the uncertainty of the class labels given a clustering solution. Given m classes and k clusters, for a particular class i 2 [1 ::m ] and cluster j 2 [1 ::k ], we rst compute p ij , which is the probability that a mem-ber of cluster j belongs to class i . The entropy of the class labels conditioned on a particular cluster j is cal-culated as: E j =  X  entropy (CE) is then de ned as: CE = where n j is the size of cluster j and n is the total number of instances.
 We would like to minimize CE. Its value is 0 when each cluster found contains instances from only a single class. Note that we can also obtain a value of 0 if each cluster contains a single instance. Therefore it is clear that this criterion is biased toward larger values of k because the probability of each cluster containing instances from a single class increases as k increases. Because of this bias, we use CE only when comparing two clustering results with the same value of k . Normalized Mutual Information(NMI): Fo r a detailed description of NMI see (Strehl &amp; Ghosh, 2002). Let X be a r.v. representing the distribution of class labels [1 ::m ]and Y be a r.v. representing the dis-tribution of cluster labels [1 ::k ]. To calculate NMI be-tween r.v.'s X and Y , we rst compute the mutual in-formation between X and Y as MI = where p ij is de ned as above, p i is the probability of class i ,and p j is the probability of cluster j . Mutual information measures the shared information between X and Y . Note that its value is not bounded by the same constant for all data sets. NMI normalizes it onto the range [0,1] by: NMI = MI p and H ( Y ) denote the entropy of X and Y .
 NMI attains the optimal value of 1.0 when there is a one to one mapping between the clusters and the classes (i.e., each cluster contains one class and k = m ). Unlike CE and other criteria such as the RAND index (Rand, 1971), NMI is not biased by large k (Strehl &amp; Ghosh, 2002). However, note that if a class is multi-modal and our clustering solution correctly re-flects this, the value of NMI will not be 1.0. 3.3. Comparison with RP+EM Our method is built on the conjecture that combining multiple runs of RP+EM is better than a single run of RP+EM or PCA+EM. In this section we present the results of a comparison between our ensemble ap-proach and individual runs of RP+EM.
 We performed thirty runs of RP+EM to form a cluster ensemble for each data set. Note that we force each run of RP+EM to nd a xed number of clusters, which is set to be the number of classes. Although this is not necessary (or even desirable), it lets us remove the e ect of k in evaluating the cluster results. Table 3 reports the CE and NMI values of both the ensemble and individual runs of RP+EM. The values reported for RP+EM are averaged across the thirty runs. From the table we see that the ensemble im-proves the clustering results over its components for all three data sets (recall CE is to be minimized and NMI is to be maximized). The di erence in perfor-mance is the least for EOS, which we conjecture is due to less diversity in the individual clustering results for EOS. We explore this conjecture in Section 4. 3.4. Comparison with PCA+EM We compare our approach to PCA with EM clustering (of Gaussian Mixtures) in two scenarios: 1) where we force both methods to produce the same number of clusters, and 2) where we allow each approach to adapt the cluster number to its clustering solution. In our rst experiment we require both methods to form the same number of clusters. This allows us to re-move the influence of k on the evaluation criteria. We want to clarify that the individual runs of RP+EM still use BIC to search for k , because this will not bias the evaluation criteria and further because di erent runs naturally require di erent numbers of clusters (Dy &amp; Brodley, 2000). Note that the \natural" number of clusters in the data may or may not equal to the num-ber of classes in our labeled data sets, therefore we compare the two methods for ve di erent values of k . The cluster ensembles are formed by thirty runs of RP+EM and the reported results are averaged across ve runs of each method.
 Table 4 shows CE and NMI for ve di erent values of k . Recall that we want to maximize NMI and minimize CE and further that NMI ranges from 0 to 1 and CE ranges from 0 to log 2 ( m ), where m is the number of classes. For all three data sets, our method results in better performance as measured by both CE and NMI except for k = 14 for the CHART data set. We observe that the ensemble tends to have smaller variance than PCA+EM, particularly when k is large. From these results, we conclude that for these data sets 1) the ensemble method produces better clusters, and 2) it is more robust than PCA+EM.
 In our second comparison, we allow each method to determine its own cluster number, k . To decide k ,the cluster ensemble method uses the heuristic described in Section 2.2, and PCA+EM uses BIC. Table 5 re-ports the average NMI and k values for each method averaged over ve runs. Because CE is biased toward a larger number of clusters, we only use NMI in this com-parison. From Table 5 we see that the cluster ensemble method outperforms PCA+EM for all three data sets. In addition, NMI for the ensemble has lower variance than for PCA+EM. The di erence is more signi cant for HRCT and CHART than EOS, while HRCT and CHART had higher original dimensionality than EOS. From these limited experiments we conjecture that our method is most bene cial when the original dimension-ality is large. We want to mention that computation-ally our method is less ecient than PCA+EM but can be easily parallelized when time is a concern. For supervised ensemble approaches, diversity of the base-level classi ers has proven to be a key element in increasing classi cation performance (Dietterich, 2000). In the relatively new area of unsupervised en-sembles, the impact of diversity and quality of the in-dividual clustering solutions on the nal ensemble per-formance has not been fully understood.
 To perform this analysis we follow the approach taken by Dietterich (2000) and graph the diversity versus quality for each pair of clustering solutions in the en-semble. To measure diversity, we calculate the NMI between each pair of clustering solutions. To o bta i n a single quality measure for each pair, we average their NMI values as computed between each of the two so-lutions and the class labels from the labeled data set. Figure 4 shows the diversity-quality diagram for each of the three data sets. Note that when the NMI be-tween two solutions (shown on the y axis) is zero the diversity is maximized. In contrast, maximizing the average NMI of each pair (shown on the x axis), maxi-mizes their quality. Therefore we want our points to be close to the right-hand bottom corner of each graph. Thirty runs of RP+EM are used to form each ensem-ble, and ve ensembles are formed for each data set. In each graph we also show the average NMI for the ve nal ensemble solutions as reported in Table 5. Each of the three data sets shows somewhat di erent behavior. The left graph shows that the individual clustering solutions for the HRCT data set are highly diverse but have fairly low quality. For the CHART data set (middle), we see that RP+EM formed a set of clustering solutions with a wide range of quality and diversity. In contrast, EOS (right) has slightly higher quality than HRCT but much lower diversity.
 In comparing the diversity/quality results to the per-formance of the entire ensemble (indicated by the dot-ted line in each graph), we see evidence that for an ensemble of size thirty, high diversity leads to greater improvements in the ensemble quality. Speci cally, we see the least improvement of the ensemble over a sin-gle run of RP+EM for the EOS data set, which has signi cantly lower diversity than the other two. On the other hand, less improvement is obtained for the HRCT data set in comparison with the CHART data set, which suggests that the quality of individual clus-tering solutions also limits the performance of a xed-size ensemble. To gain further insight into these issues, we examined the impact of the ensemble size on per-formance.
 Figure 5 plots the NMI value of the ensemble's nal clustering solution for ensemble sizes ranging from ve to fty. The points in the graph are generated as fol-lows. To remove the e ect of k on NMI, for each en-semble size r we force our algorithm to produce a xed number, k , clusters for di erent values of k as shown in Table 4. The NMI values for di erent values of k are then averaged to obtain a single performance measure for each ensemble size. We repeat the above process ve times and average the results to obtain a stable estimate for the performance measure for each r . From Figure 5, we can see that increasing the ensem-ble size helps only for data sets with high diversity. For the CHART data set, We can see a clear and sta-ble trend of performance improvement as the ensemble size increases. For the HRCT data set, we observe a similar but less stable trend. For the EOS data set, the performance gain is negligible as ensemble size in-creases.
 These results suggest that the ensemble performance is strongly influenced by both the quality and the di-versity of the individual clustering solutions. If the in-dividual clustering solutions have little diversity, then not much leverage can be obtained by combining them. The quality of the individual solutions limits the per-formance of a xed-size ensemble and low quality solu-tions may cause the ensemble performance to oscillate as the ensemble size changes.
 As shown from the experimental results, random pro-jection successfully introduced high diversity into the clustering solutions for both HRCT and CHART data set. This suggests that random projection can produce diverse clustering solutions when the original dimen-sion is high and the features are not highly redundant. 6 An open question is how to improve the quality of the individual clustering solutions. Our future work will investigate a tentative solution { evaluate each clus-tering solution using criteria such as the log-likelihood of the Gaussian model and select only the \good" ones to form the ensemble. Techniques have been investigated to produce and combine multiple clusterings in order to achieve an im-proved nal clustering. Such methods are formally de-ned as cluster ensemble methods by Strehl and Ghosh (2002). Due to space limits, please refer to (Strehl &amp; Ghosh, 2002) for a comprehensive survey of the re-lated work. While our work can be considered as an extended instantiation of the general cluster ensem-ble framework, there are signi cant distinctions be-tween our work and previous studies in how we form the original clusters, how the clusters are combined and the core problem we are trying to solve { clus-tering high dimensional data. We conclude here with the major contributions of this work: 1) we examined random projection for high dimensional data cluster-ing and identi ed its instability problem, 2) we formed a novel cluster ensemble framework based on random projection and demonstrated its e ectiveness for high dimensional data clustering, and 3) we identi ed the importance of the quality and diversity of individual clustering solutions and illustrated their influence on the ensemble performance with empirical results. The authors were supported by NASA under Award number NCC2-1245.
 Achlioptas, D. (2001). Database-friendly random pro-jections. Proceedings of the Twentieth ACM Sympo-sium on Principles of Database Systems (pp. 274{ 281). ACM Press.
 Agrawal, R., Gehrke, J., Gunopulos, D., &amp; Raghavan,
P. (1998). Automatic subspace clustering of high dimensional data for data mining applications. Pro-ceedings of the 1998 ACM SIGMOD International Conference on Management of Data (pp. 94{105). ACM Press.
 Bingham, E., &amp; Mannila, H. (2001). Random pro-jection in dimensionality reduction: Applications to image and text data. Proceedings of the Sev-enth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (pp. 245{ 250). ACM Press.
 Chakrabarti, K., Keogh, E., Mehrotra, S., &amp; Pazzani,
M. (2002). Locally adaptive dimensionality reduc-tion for indexing large time series databases. ACM Transactions on Database Systems , 27 , 188{228. Dasgupta, S. (2000). Experiments with random pro-jection. Uncertainty in Arti cial Intelligence: Pro-ceedings of the Sixteenth Conference (UAI-2000) (pp. 143{151). Morgan Kaufmann.
 Dietterich, T. G. (2000). An experimental comparison of three methods for constructing ensembles of de-cision trees: Bagging, boosting and randomization. Machine learning , 2 , 139{157.
 Duda, R. O., &amp; Hart, P. E. (1973). Pattern classi ca-tion and scene analysis . John Wiley &amp; Sons. Dy, J. G., &amp; Brodley, C. E. (2000). Feature subset selection and order identi cation for unsupervised learning. Proceedings of the Seventeenth Interna-tional Conference on Machine Learning (pp. 247{ 254). Morgan Kaufmann.
 Dy, J. G., Brodley, C. E., Kak, A., Shyu, C., &amp; Broder-ick, L. S. (1999). The customized-queries approach to CBIR using EM. Proceedings of the IEEE Confer-ence on Computer Vision and Pattern Recognition (pp. 400{406). IEEE Computer Society Press.
 Fraley, C., &amp; Raftery, A. E. (1998). How many clusters? Which clustering method? Answers via model-based cluster analysis. The Computer Jour-nal , 41 , 578{588.
 Fukunaga, K. (1990). Statistical pattern recognition (second edition) . Academic Press.
 Hettich, S., &amp; Bay, S. D. (1999). The UCI KDD archive.
 Kaski, S. (1998). Dimensionality reduction by ran-dom mapping. Proceedings of the 1998 IEEE Inter-national Joint Conference on Neural Networks (pp. 413{418). IEEE Neural Networks Council.
 Papadimitriou, C. H., Raghavan, P., Tamaki, H., &amp; Vempala, S. (1998). Latent semantic index-ing: A probabilistic analysis. Proceedings of the Seventeenth ACM Symposium on the Principles of Database Systems (pp. 159{168). ACM press.
 Rand, W. M. (1971). Objective criteria for the evalua-tion of clustering methods. Journal of the American Statistical Association , 66 , 846{850.
 Strehl, A., &amp; Ghosh, J. (2002). Cluster ensembles -a knowledge reuse framework for combining multiple
