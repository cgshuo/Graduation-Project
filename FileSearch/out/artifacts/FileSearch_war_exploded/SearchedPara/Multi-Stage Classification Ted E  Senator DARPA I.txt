 evaluating and maximizing the accuracy of classifiers either individually or in ensembles, little effort has been devoted to analyzing how classifiers are typically deployed in practice. In many domains, classifiers are used as part of a multi-stage process that increases accuracy at the expense of more data collection and/or more processing resources as the likelihood of a positive class label increases. This paper systematically explores the tradeoffs inherent in constructing these multi-stage classifiers from a series of increasingly accurate and expensive individual classifiers, considering a variety of metrics such as accuracy, cost/benefit ratio, and lift. It suggests architectures appropriate for both independent instances and for highly linked data. subject to the well-known and commonly pointed out pitfall that even with a highly accurate classifier, almost all positive classifications will be false positives. (For example, a 99.9% accurate classifier applied to a population of 300,000,000 entities containing only 3000 true positives  X  or 0.001 %  X  would yield 299,997 false positives and only 2997 true positives, corresponding to over 100 times more false positives than true positives, while failing to detect 3 actual positives.) Many potential applications of data mining have been criticized as infeasible because of this fact. These criticisms have been made by knowledgeable people in respected publications, in both the popular press and in scientific journals. (See [14] and [17] for two particular examples.) However, these criticisms are based on the assumption that such an application would consist of a single classifier operating on a single database and result in an unacceptably severe action for the entities that are classified  X  X ositive X   X  an assumption that is not valid or even close to valid for realistic examples of useful and deployed detection systems and that would not be employed by any reasonable designer or accepted by any reasonable user of such an application. stages of classification to a carefully selected corresponding series of databases, with each stage providing both evidence and justification for additional data collection, access and/or analysis in the subsequent stage. At each stage in the process, only those instances that have a positive classification are considered for the next stage. This drastically reduces the overall false positive rate, while opening up the possibility of additional false negatives. mitigates this problem. Real domains of interest are characterized by strong linkages between entities. Following these linkages both enables the missed positive entities to be recovered and classified correctly through their connections to the correctly classified positive entities and also provides further reduction in false positives. (See [1], [7], and [12].) Finally, in the frequently occurring situations in which the phenomena of interest is characterized by combinations of entities, often with some amount of structure, these combined phenomena can be detected with adequate accuracy despite the rarity of the phenomena. have devoted much effort to techniques for creating better classifiers and, more recently, to techniques for combining individual classifiers to produce a more accurate combined classifier. (See [13] for reports on a series of workshops devoted to this subject and [11] for a recent book that is a comprehensive survey of the field.) However, this work has almost entirely focused at ensemble methods that combine classifiers in parallel, rather than at sequential combination of classifiers. (Minor exceptions are found in [20] and in the proposal for multilayered cascaded machines in [16]; however, these exceptions are still aimed at classifier construction rather than use, and do not consider use of additional data.) This existing research concentrates on the problem of efficiently constructing better classifiers that operate on a single shared data space rather than on the problem of developing effective applications given a set of classifiers and databases with specified performance characteristics and discriminative information content, respectively. classifiers that use relational data. (See, for example, [3], [4], [9].) These collective classification techniques typically outperform classifiers that use only propositional data. The algorithms typically are iterative in so far as they propagate evidence of class labels between connected entities; however, these iterations are a convergence process that seeks a self-consistent labeling of a single dataset rather than a multi-stage classification process of the type described and analyzed in this paper. Other recent work has addressed the problem of data mining of graphical data. (See [2] for an overview.) Active learning is a technique that uses data incrementally; however, it is also aimed at constructing a classifier rather than at using a classifier to detect instances, and it selects examples from a single data source. Its incremental acquisition of specific labeled examples is distinct from the multi-stage classification of all data instances discussed in this paper. detection in linked data is [8]; it considers the implications of relational versus propositional data, ranking versus classification, and multi-pass versus single-pass inference. However, the form of multi-pass inference it considers  X  as it clearly notes  X  is one in which the predictions of one pass are used to inform the results of the next, until convergence occurs and a joint consistent probability model is achieved. This form of multi-pass inference is distinct from the form discussed in this paper, in which a series of independent classifiers, typically operating on different data, are used to filter a set of entities. architectures for constructing multi-stage classifiers to detect rare phenomena. It is based on architectures used in real detection systems in several domains. These architectures consist of various design alternatives for combining classifiers in series and for selecting population subsets to which they should be applied. It models alternative methods of choosing the classifiers and combining these classifiers in series and it evaluates tradeoffs between alternative approaches. It shows that feasible design alternatives exist for the construction of useful and practical detection systems for real phenomena of interest. Based on the results of each stage, a decision is made regarding the further disposition of each entity under review. Two typical actions are 1) the acquisition and analysis of additional data about the entity to enable a reduction in its classification uncertainty and 2) the use of a more accurate and correspondingly more expensive classification test. While early stages may consist of entirely automated processes, later stages are typically characterized by an increased level of human involvement. The overall process consists not just of automated data analysis components, but also of human analyses, processes and procedures controlled by policies, and the overall controlling legal authorities. Human judgment  X  governed by management policies and legal authorities  X  is applied not only to the classification steps but to decisions about relative resource allocation between the steps and to issues such as thresholds and justifications for taking particular actions, such as acquiring additional information. These stages are highly interdependent  X  decisions made at one stage affect other stages. For example, the human resources available to conduct investigations limits the number of investigative leads desired to be produced by an automated classifier, thereby affecting the classification threshold that is employed and effectively choosing a particular setting on the classifier X  X  possible ROC curve. This generic characterization describes many domains, including for example, public health, fraud detection, intelligence and law enforcement, to name but a few. A general model of these multi-stage detection systems was introduced in [5] and [19] and is depicted in figure 1. the literature. The FinCEN Artificial Intelligence System (FAIS) [18] and NASD Regulation X  X  Advanced Detection System (ADS) [10] are two. The commonality of their design is discussed in [19]. Primary data, i.e., the data that are used for initial classification, are prepared and cleaned. Entity consolidation is performed and models of the entities X  activities are created though aggregation. Initial break detection is performed by various classification techniques. After initial classification, additional data about positively classified entities are obtained from secondary sources. Secondary sources can only be queried with specific entity identifiers. Once the secondary source data are available, link analysis techniques are used to perform the final classification. The systems can be viewed as a series of filters (or classifiers), each of which provides increased accuracy on a correspondingly smaller set of entities, at a constant classification cost per stage. (Classification can be very expensive when it consists of complex pattern matching on a massive data stream.) Other detection systems, both existing [6] and proposed [15] also exhibit this structure. system in a law enforcement domain. It was developed for FAIS and it illustrates the design principles of multi-stage classification. It begins with  X  X rimary X  data sources. These data sources are those that are considered to have a signal-to-noise ratio that can yield useful starting points for an investigation. Entities are inferred from the identification information provided with the transactions and from the linkages between transactions. Secondary data sources may be used for entity identification but not for classification at this point in the process. Two stages of automated classification are used. Stage 1 classifies some entities as  X  X ubjects. Based on this initial classification, additional information from the secondary sources may be queried for these specific entities and a stage 2 classification classifies some of the subjects as leads. These leads are reviewed by analysts; those that appear suspicious are regarded as targets and investigations are opened. It is at this point that the criterion of  X  X robable cause X  must be satisfied. Note that at all earlier stages other authorization criteria had to be met in order to  X  X romote X  an entity to the next level of suspicion. Some investigations lead to grand jury indictments, trials, and ultimately convictions. presented in section 2 but also a completely different domain that exhibits the same general characteristics: the overall system used in the US for HIV detection. * process is illustrative because it uses the same abstract strategy of multi-stage classification, even though the classification tests are based on biological samples rather than data analyses.
 screening is a recommended procedure for individuals in this population. Screening is a low-cost procedure. The most common screening test is the enzyme-linked immunosorbent assay (ELISA). Individuals who test positive are then given a second, confirmatory test, typically the Western Blot. The screening test has a high sensitivity (few false negatives), while the confirmatory test has a high specificity (few false positives) but less sensitivity. Testing positive on the confirmatory test means one is infected with HIV. Notification of positive results from the screening test is not recommended in many circumstances because of the high false-positive rate. Note that membership in the high-risk population is determined by behavioral factors, while both the screening and confirmatory tests are performed on biological data; i.e., the data used by the classifiers (i.e., the screening and confirmatory tests) is independent of the criteria used to select individuals who are part of the high-risk group. It is also noteworthy that testing can be anonymous or confidential up through the confirmatory test, maintaining individual privacy. it is confirmed, he is notified that he is infected. At this point, no more testing is needed to classify the individual as HIV-positive. However, the detection process does not end. Counselors encourage the HIV-positive individual to notify other individuals who he may have infected; this notification process leads to other infected individuals even if they are not included in the high-risk population or if they are one of the rare individuals who was falsely classified negative so they can be informed that they should be tested. It corresponds to the link analysis component of detection systems in other domains. architectures for a multi-stage detection system. The overall model structure is depicted in figure 3. Each specific instantiation is a different selection of components. We limit modeling and discussion to a high-risk population selection, two stages of classification, and one group detection calculation. Each classifier evaluates only the entities that have been classified as positive by the previous classifier. The errors from each stage are taken to be uncorrelated. straightforward. More important, two stages of classification is adequate in many single-source detection systems  X  once two classifications are performed, the entities that have been identified are few enough that the appropriate next sta ge is link analysis. Another view of this claim is that two stages of entity-based classification typically exhaust the information content of a single data source; to obtain more accurate classification then requires additional data sources rather than more sophisticated analyses. applied to an entire population. This baseline is the standard architecture on which individual classifiers are typically evaluated. Our first refinement of the baseline architecture is called the two-stage architecture. This two stage architecture employs two classifiers in series. If the errors in one of the classifiers arise from random events rather than from available data features, there might be reason to repeat an initial test, or, in our architecture, to employ the first stage classifier a second time on the same data, with no additional accuracy or cost. We call this the  X  X wo-first stage architecture. X  Likewise, given the availability of a more accurate second-stage classifier, perhaps there are advantages to using it twice. We call this the  X  X wo-second stage architecture. X  (Note that these two architectures make sense only if the classifiers operate on the same data.) We also consider using the more accurate classifier only once but on the entire population. This is referred to as the  X  X ll second-stage architecture. X  All of the above architectures can be preceded by the preprocessing step of selecting a high-risk population. problem of detecting phenomena that can occur or be recognized only when groups are acting together. We imagine a group of size N and consider the simplest situation  X  that a group will be detected and its plans thwarted if at least one of its members is detected. Table 1. The X X  X  in a box indicate that a model listed in the row includes the component named in the column. 3.2.1. Model Parameters. Model input parameters are: percentage is the percentage of the overall population in the high-risk group. High-risk lift is the additional likelihood that a member of the high-risk group is positive relative to the likelihood of an entity in the overall population. Care must be taken in selecting values for these parameters that they not be chosen so as to result in more positive entities in the high-risk group than in the whole population. The Stage 2 Test Improvement/Cost Ratio models the additional accuracy provided by the stage 2 test as well as the additional cost it is assumed to entail. To avoid introducing yet another parameter, we assume that the ratio of accuracy to cost is linear. The stage 1 cost is an arbitrary choice; other costs are expressed as a factor relative to it. 3.2.2. Computations. For each architecture included in table 1, the parameters described in section 3.2.1 are used to compute the expected values of the number of true positives and negatives in the population and, if it exists, in the high-risk group. Next, the expected number of true positives, true negatives, false positives, and false negatives is computed for the stage 1 classifier. Note that in the architectures that incorporate a high-risk group, only the high-risk group is subject to the stage 1 classifier. Any positive exemplars not contained in the high risk group are counted as false negatives. Using the number of true positives and false positive as input to the stage 2 classifier, the expected number of true positives, true negatives, false positives, and false negatives is computed. The negatives resulting from this second stage of classification are added to those resulting from the first stage and those not tested at all to yield the final true positive, false positive, true negative, and false negative numbers for the entire population. 3.2.3. Cost-Benefit Modeling. The overall evaluation of a set of classifiers in an operational setting is performed as a cost-benefit analysis. Costs include the cost of performing the classification itself combined with the cost of any incorrect classifications. Benefits include not only the benefit of correct classifications but potentially the benefits of deterrence; i.e., changes in the behavior of potential adversaries, but modeling this effect is beyond the scope of this paper. Costs and benefits depend on distribution in population as well as on classifier performance. 3.2.4. Assumptions and Limitations. The analyses of alternative architectures for multi-stage classification described in this paper are limited by the assumptions and limitations of the models, which include: classifier quality. While related, these metrics are not identical. Information retrieval is typically evaluated by precision and recall. Pattern recognition or target detection systems are measured by Receiver-Operating Characteristics (ROC curves). Public health professionals report positive predictive value and negative predictive value of a diagnostic test as a function of sensitivity and specificity and the characteristics of a population. The relationship between these measures is depicted in table 2 and by the following equations: tuning a classifier to reflect a tradeoff between False Positives and False Negatives, and because many point metrics depend not only on classifier characteristics but also on the distribution of classes in the population, which may not be known a priori , ROC curves and, in particular, the area under the (ROC) curve, are considered better methods of comparing classifiers. However, in practice, tuneable classifiers are rarely available to application developers, especially when each classifier is designed for different datasets. Rather, they embody a particular decision rule operating at a particular point on an ROC curve. For the purposes of this paper, we specify our classifiers X  performance in terms of Sensitivity and Specificity, metrics that are independent of the distribution of class labels. We compute PPV, NPV, P(detection) and P(false alarm) for each of the architectural alternatives. We also compute lift, defined as the improvement in signal/noise ratio obtained through classification, or more precisely as the ratio of true positives to total positives resulting from the classification process compared to the fraction of true positives in the overall population. Finally, we report the total error (overall % misclassified) and the computed benefit/cost ratio. For groups we compute the % of groups that would be detected under the assumption that detecting at least one group member results in the group X  X  being detected, the expected number of groups that are not detected (i.e., false negatives), and the associated benefit/cost ratio. combination of a scorer and a threshold, in a multi-stage architecture a binary decision must be made as to whether or not a particular entity is carried forward to a subsequent stage. Therefore, we consider only binary classifiers in this paper with no loss of generality. * various architectures described in section 3 were compared under a variety (over 25) of parameter settings. A population of 300,000,000 (ro ughly the population of the US, to one significant figure) was used for all the experiments. Somewhat realistic parameters chosen to characterize counter-terrorism detection (line T0) and HIV detection (line HIV0) are presented in table 3, with corresponding results in table 4. We next vary selected parameters from these base cases to examine the effect of the parameter choices. Because of the large number of degrees of freedom, even in this simplified model, it is not feasible to explore the entire state space at once. missed. Using the T0 parameters and varying group size, the number of undetected groups is shown in Table 6. Even small increases in group size almost guarantee that at least one member of every group will be detected. system would be used as the basis for further analysis in a sensitive domain such as counter-terrorism detection, in which the goal of these stages is to enrich the data to maximize productivity of the downstream processes. (This is similar to the law enforcement model presented in figure 2.) Figure 4 depicts results obtained from examining the effect of varying stage 1 specificity with sensitivity fixed at 75%; figure 5 depicts the result of varying sensitivity with specificity fixed at 95%. Figure 5 examines the effect of the Stage 2 Improvement/Cost Factor. All other parameters are as in T0. the full multi-stage classification architecture that takes advantage of the high-risk population and two classification stages is especially advantageous for extremely rare phenomena. Significant contributions to lift arise from the use of the more accurate classifier in the second stage. Most important, for groups whose actions depend on having all participants available, it is well within the realm of feasibility to disrupt them. However, detecting an entity in isolation would be difficult even with the proposed multi-stage classification techniques. Initial segmentation into a high-risk population risks missing some smaller groups but provides an even greater reduction in false positives. Maximum lift occurs over a range of sensitivity and specificity for the two-stage 2 and the high-risk 2 stage architectures.
 stage classification is a feasible design for the initial stages of detecting rare events, especially when there are strong and observable links between entities that can be used to compensate for false negatives. Multi-stage classification techniques can significantly reduce  X  to acceptable levels  X  the otherwise unacceptably large number of false positives that would result from even the most optimistically accurate single-stage classifier applied to very rare phenomena. It can eliminate most entities from suspicion early on, at a low cost in data collection and in testing, with acceptable impacts on overall detection effectiveness. For complex phenomena characterized by the necessity of all participants X  not being detected, multi-stage classification may by itself be sufficient to disrupt, and therefore prevent, the phenomena that is the subject of the detection process. And for phenomena for which links are not readily observable or that require a larger proportion of the participating entities to be detected, multi-stage classification can provide the initial leads for collective inference and link analysis techniques. Starting from known examples and following the links is a feasible approach to complex event detection, but so is multi-stage classification applied to individual entities. In combination it is likely that a wide range of practical and desirable applications can be designed and constructed. multi-stage classification for a phenomena present only 0.001% of the time, using two independent stages at 99% and 99.9% accuracy and with 5% of the population in a high-risk group that was 10 times more likely to be positive, would be expected to detect almost all the groups with minimal false positives. Perhaps the most interesting would be a comprehensive examination of the parameter space, providing an empirical sensitivity analysis. Related to this would be the use of Monte Carlo type simulations instead of Expected Value Models to explore the variance of the system characteristics in addition to the mean. Exploring alternative models of the cost of additional accuracy in the second stage classifier has been suggested. Allowing more than two classification stages  X  i.e., modeling the availability of multiple data sources that can be accessed only when previous analy ses suggest some likelihood of a positive classification  X  would be a natural extension. The conditional independence assumption between the classifiers could be weakened. More sophisticated group detection models would also be of interest, as would consideration of a distribution of group sizes and explicit modeling of link observability and link existence probabilities. Explicitly considering resource constraints at each stage of classification would model the reality of many organizations. Finally, combining the multi-stage classification architectures described in this paper with the relational classification models evaluated in [8] would be a major step towards a comprehensive understanding of a complete detection system for extremely rare events in complex relational domains. Ramakrishnan for their suggestions and ideas and for encouraging me to pursue this research and prepare this paper. I also thank my former colleagues at NASD and at FinCEN for helping to develop some of the ideas expressed in this paper over the course of many years of developing, deploying, and using break detection systems. 
