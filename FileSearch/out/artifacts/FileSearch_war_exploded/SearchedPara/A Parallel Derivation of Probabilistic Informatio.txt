 This paper investigates in a stringent mathematical formal-ism the parallel derivation of three grand probabilistic re-trieval models: binary independent retrieval (BIR), Poisson model (PM), and language modelling (LM).

The investigation has been motivated by a number of questions. Firstly, though sharing the same origin, namely the probability of relevance, the models differ with respect to event spaces. How can this be captured in a consistent notation, and can we relate the event spaces? Secondly, BIR and PM are closely related, but how does LM fit in? Thirdly, how are tf-idf and probabilistic models related?
The parallel investigation of the models leads to a number of formalised results: 1. BIR and PM assume the collection to be a set of non-relevant documents, whereas LM assumes the collection to be a set of terms from relevant documents. 2. PM can be viewed as a bridge connecting BIR and LM. 3. A BIR-LM equivalence explains BIR as a special LM case. 4. PM explains tf-idf, and both, BIR and LM probabilities express tf-idf in a dual way.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Theory Binary Retrieval Model, Poisson, Language Modelling
The search for the best retrieval model drives informa-tion retrieval (IR) research. [17] brought the BIR model, this forming a sound theoretical foundation for IR. Tuning of tf-idf led to probably the best tf-idf-based retrieval func-tion known as BM25 ([15] at TREC 1995). In late 90s, Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. language models emerged as an alternative retrieval model ([13]). Early 00s brought the divergence from randomness (DFR) branch of models ([3], [1]).

A physician might ask: What is the common ground of your models? For centuries, physicians have been investi-gating phenomena like gravity, light, and energy. For each phenomena, there is a model with significant maths around. Physicians look for the ground model that explains the other models: we hear, for example, of quarks and quantum theory . Are there IR quarks that explain the existing IR models?
This paper is not about IR quarks, nor does it propose a ground model for IR. This paper is best described as a parallel investigation of gravity, light, and energy, where the parallel investigation helps highlighting aspects and relation-ships of the single phenomena (models).

The contribution of this paper is the formal presentation of the parallel derivation of existing IR models. The deriva-tion leads to theorems and formulae that relate and explain existing IR models.

This paper looks at the three grand probabilistic retrieval models: binary independent retrieval (BIR), Poisson model (PM), and language modelling (LM). Results include, for example, the formalisation of event spaces. For BIR, judge-ments in documents form the event space, whereas for PM, frequencies of terms form the event space, and for LM, terms at locations form the event space (see section 3). Our re-search also addressed DFR, but this paper focusses on the three basic models only.

A paper on retrieval models is bound to have a low-recall reference list, and we selected relevant papers that deal with the relationships and explanations of models.
 Highly relevant is the introduction [11] in the book [6] on LM, where the authors make explicit that the probability of relevance is the origin of both, BIR and LM. [9] in the same book takes a detailed look at the relevance event, and [18] is a milestone in pointing at the different event spaces of probabilistic models, and raising attention for what happens when relating BIR and LM.

Further, the papers [8] and [16] are highly relevant. [8] provides a probabilistic explanation of tf-idf, the explana-tion being based on LM. The elegant formulation of the LM retrieval status value has been an important step for the efficient processing of LM; however, the tf-idf explanation mixes event spaces (usage of document frequencies for esti-mating the probability of a term in a collection, and usage of term frequencies for estimating the probability of a term in a document), and we pay in our paper particular attention to make the underlying event spaces explicit. [16] on theoretical arguments for idf draws on the rela-tionship between BIR and idf , pointing out how idf follows from BIR for missing relevance. [7] picked up the latter, and discusses a fully idf -based explanation of BIR.

The next class of relevant papers deals with the Poisson model: [14] divides the collection into two sets: elite and non-elite set of documents (elite set contains the documents that are  X  X bout X  a query concepts/terms, and in this set of documents the term usually occurs to a greater extend than in the rest of the documents). Dividing the collec-tion into two sets should show an improvement since the 2-Poisson model with two parameters better fits the divided collection, this also being reported in the Poisson investi-gation [12]. The results stress to consider two document sets, and this is exactly what we observe when relating BIR and LM probabilities, where the average document length of documents in the elite set of a term and the average docu-ment length in the collection form pillars of the bridge that connects BIR and LM probabilities. [5] investigates idf -based against ilf -based retrieval (the paper refers to token-based retrieval, but we apply the notion ilf ,inverselocation frequency). The work of Church/Gale has shown that the burstiness of terms (a bursty term is a term that, if it occurs, occurs several times) is a crucial feature of a good term.
The book [21] is probably to date the most adventurous approach for a ground model of IR, claiming that IR can be explained by linear algebra, vector (Hilbert) spaces and extensions. Different from work on ground models, our pa-per does not claim any unifying ground model. We derive the existing models in a unifying formalism and notation, without making any claim about a new model.
 The remainder of this paper is structured as follows:
Section 2: Brief review of the probability of relevance be-ing the origin of probabilistic models.
 Section 3: Parallel derivation of BIR, PM, and LM.
Section 4: Relevance assumption: BIR and PM assume the collection to be a set of non-relevant documents, whereas LM assumes the collection to be a set of terms from relevant documents.

Section 5: Poisson bridge: The Poisson parameter  X  can be viewed as a bridge connecting BIR and LM probabilities. Section 6: BIR-LM equivalence: Shows a case for which BIR and LM are equivalent.

Section 7: TF-IDF: PM explains tf-idf-like retrieval, and this can be represented using either BIR or LM probabilities. The probability of relevance can be viewed as the origin of BIR, PM, and LM. The conditional probability P ( r | d, q )of and the evidence probability P ( d, q ), where r is the relevance event, d is a document, and q is a query.
 A ranking based on P ( r | d, q ) is optimal, if the costs for reading a relevant document are assumed to be less than the costs for reading a non-relevant document (probabilistic ranking principle). Therefore, P ( r | d, q ) is the optimal basis for defining a retrieval status value ( RSV ).

The odds of an event (the fraction of the probability that the event is true and false, respectively) is a convenient rank-ing function: In the formulation with odds, P ( d, q )dropsout. d depend on q ,orlet q depend on d ,andweobtain: Equation 1 is the basis of binary independent retrieval (BIR) and the Poisson model (PM), and equation 2 is the basis of language modelling (LM) (see [11]).
 The events d and q are sequences (conjunctions) of events: For BIR, the events correspond to binary judgements on whetherornotatermoccursin d ;forPM,theeventscor-respond to the frequencies with which the terms occur in d ; for LM, the events correspond to terms .

We formalise and discuss the event spaces and other as-pects of the probabilistic IR models in the next section.
This section shows the parallel derivation of BIR, PM, and LM. For presenting such a complex mathematical topic in an understandable way, and for giving the reader an an-chor for looking up the issues and notations, we developed figures 1 and 2. They are relatively busy figures with lots of mathematical notation, and we will proceed step by step. The notation is partially based on [19].

Figure 1 points at the event spaces , background mod-els , frequencies , probabilities ,and term probability interpretations of each model. Figure 2 focuses on the document and query interpretations , retrieval sta-tus values (RSV X  X ) ,and parameter estimations .
BIR is based on a space of binary judgements over doc-uments, whereas PM is based on a space of frequencies of terms, and LM is based on a space of terms at locations. 1
The next section highlights that BIR involves 2  X  N T ( c ) judgement sequences (two sequences for each term, N T ( c ) is the number of terms) as background model, whereas PM involves two frequency sequences as background model, and LMinvolves one term sequence as background model.
A background model is based on the knowledge we have about the collection, and from this point of view, each of BIR, PM, and LM has a background model. Note that the notion  X  X ackground model X  is common for LM, and the par-allel derivation here applies this notion to all models.
Note in figure 1 the carefully chosen notation for referring to the elements of the event spaces, and for underlining the parallels of the models: j is a judgement ( { 1 , 0 } )fordocument d . f is a frequency ( { 0 , 1 , 2 ,... } )ofterm t . t is a term ( {  X  1 , X  2 ,... } )atlocation l .
BIR PM LM 3.1.1 Event spaces D :Setofdocuments T :Setofterms L : Set of locations
J : Set of judgements F : Set of frequencies T :Setofterms j d : judgement for document d 3.1.2 Background models 2  X  N T ( c ) sequences c t,x = j 1 ,j 2 ,... of judgements where j d is the judgement for document d ,and c t,x denotes the judgement sequence for the term t and the set x of documents.
 = f ,f 2 ,... of fre-is the location fre-denotes the lo-3.1.3 Frequencies n
D ( j, c t,x ): number of documents in judgement sequence c t,x for which judgement j occurs ) 3.1.4 Probabilities
The probability of judgement j ,given judgement sequence c t,x : : f ! 3.1.5 Term probability interpretations
In P BIR ( t | x ), interpret t as the event that j =1for t . n D ( t, x ):= n (1 ,c t,x ).
 f !
The background model could include global (collection-independent) knowledge such as individual user profiles or statistics over previous queries. For the scope of this paper, we consider the collection being the only evidence source. Still, the discussion here makes explicit how global knowl-edge is to be included: Depending on the model chosen, the global knowledge leads either to an adaptation (mixture) of judgements, frequencies, or terms. Next, consider the back-ground model for each of the probabilistic retrieval models.
BIR: The background model comprises several sequences of judgements. BIR is based on a set of relevant documents, and a set of non-relevant documents, hence, for each term t in the collection, there are two judgement sequences c t,r and c t,  X  r ,where c t,r is for the relevant and c t,  X  non-relevant documents, respectively. Thus, the BIR back-ground model consists of 2  X  N T ( c ) judgement sequences, where 2 is the number of document sets (relevant and non-relevant documents), and N T ( c )isthenumberofterms. PM: The background model consists of two sequences: The sequence c r represents the location frequencies in rele-vant documents, and the sequence c  X  r represents the location frequencies in non-relevant documents.

LM: The background model is simply the sequence of terms in the collection, i.e. the concatenation of all docu-ments.

From the background models X  simplicity (number of se-quences) point of view, we could argue that LM is the sim-plest model, followed by PM, followed by BIR.
BIR is based on document frequencies. N D ( x ) denotes the total number of documents in set x ( x := r for relevant documents, x :=  X  r for non-relevant), and n D ( j, x ) denotes the number of documents for which a judgement j holds. Since the number of relevant or non-relevant documents is the same for each term (  X  t : N D ( c t,x )= N D ( x )), we apply N D ( x ) as an abbreviation of N D ( c t,x ).

PM is based on the average (expected) within-document term frequency. Here, n L ( t, x ) is the number of locations in the set x of documents at which term t occurs. Then, in average, we expect n L ( t, x ) /N D ( x )locationswith t per document in the set x .

LM is based on location frequencies. N L ( y ) denotes the total number of locations in sequence y ,where y is a docu-ment or the collection, and n L ( t, y ) denotes the number of locations at which term t occurs in term sequence y .
BIR: The probability of a judgement is estimated as the fraction of the number of documents for which the judgement holds, divided by the total number of docu-ments. For example, let term t occur in two of five rele-vant documents, i.e. n D (1 ,c t,r )=2, N D ( r )=5. Then, that term t occurs in a relevant document.

PM: We need to chose a probability distribution for es-timating the probability of a frequency. The probabilistic X  X  first choice is Poisson (though terms are not randomly dis-tributed, and the set of relevant documents is usually rela-tively small, and therefore, pure Poisson is known to be not the end of the game, see [2], [4]). For the discussion in this paper, we stick to Poisson, and we will see in section 5 (Pois-son bridge), that the Poisson distribution parameter  X  ( t, c ), the average number of locations of term t per document, connects BIR and LM.

For reviewing how the Poisson distribution works, con-sider a term t that occurs in average  X  ( t, x ) times in docu-ment set x . For example, assume that we observe the fol-lowing location frequencies for six terms in the set x of doc-uments: 5 , 10 , 5 , 3 , 3 , 4( n L ( t 1 ,x )=5, n L ( t ample, for five documents ( N D ( x )=5),  X  ( t 1 ,x )=5 / 5=1,  X  ( t 2 ,x )=10 / 5=2.
 gle event probability that t occurs, and for large documents, the Poisson probability P ( F = f t | x )=  X  ( t,x ) f t f t proximates the probability that t occurs f t times in a doc-ument, assuming that t is randomly distributed in x .The characteristic feature of the Poisson probability is to peak Thus, P ( d ) is maximal for a document in which the term (lo-cation) frequencies reflect the frequencies in the set x from which the averages were computed.

LM: The probability of a term is estimated as the frac-tion of the number of locations at which the term occurs, divided by the total number of locations. For example, let term t occur at 100 of 10 6 locations in collection c ,then P ( T = t | c )=10  X  4 .
The parallel consideration of the probabilities and event spaces leads to a well-defined interpretation of the term tion and the theoretically sound estimation depends on the model, as the next paragraphs underline.

BIR: The notation P BIR ( t | r ) needs to be interpreted as an abbreviation. P BIR ( t | r ) is an abbreviation for P ( J = 1 | c t,r ), i.e. the random variable J takes the judgement, and c t,r is the judgement sequence for the term t and the set r of relevant documents.

PM: The notation P PM ( t | r ) is, similar to BIR, just an abbreviation. The notation P PM ( t | r ) is an abbreviation for P ( F = f t | c r ), i.e. the random variable F takes the fre-quency f t of term t ,and c r corresponds to the sequence of average frequencies derived from the set r of relevant docu-ments.

LM: The notation P LM ( t | c ) is consistent with the under-lying event space of terms.

Again, like for the background models, LM ranks top with respect to simplicity.
BIR interprets a document as a conjunction of indepen-dent binary judgements, whereas PM interprets a document as a conjunction of independent frequencies, and LM inter-prets a query as a conjunction of independent terms.
The definitions of the RSV  X  X  of BIR, PM, and LM are presented in a condensed way in figure 2. We are aware that this condensed derivation and RSV definitions are not easy to access, but the consistent and parallel framework in the figure provides the overall picture of the derivation.
BIR applies odds. The assumption that non-query terms do not affect the RSV reduces the multiplication over t  X  to t  X  d  X  q ,and t  X  d becomes t  X  q \ d . The multiplica-tion with a constant factor leads to RSV BIR . P ( t | x )isan abbreviation of P ( t | q,x ) (this corresponds to q  X  x = x ).
PM applies odds and assumptions similar to BIR. The multiplication with a constant factor leads to RSV PM .
LM applies a mixture of collection-based and document-based probabilities. The multiplication with a constant fac-tor leads to RSV LM . The LM mixture parameter is  X  ,since  X  , the letter used in many LM publications, denotes the pa-rameter of the Poisson distribution, and for a clear parallel derivation, we let  X  denote the LM mixture parameter.
BIR: Terms that occur more often in relevant than in non-relevant documents have a positive effect on the RSV , whereas terms that occur more often in non-relevant than in relevant documents have a negative effect on the RSV . Mathematically, we summarise this as follows: P BIR ( t | P
BIR ( t |  X  r ): good term, positive effect on RSV . P BIR P
BIR ( t |  X  r ): bad term, negative effect on RSV . P BIR P BIR ( t,  X  r ): neutral term, no effect on RSV .
PM: Terms for which the average frequency in relevant documents is greater than the average frequency in non-relevant documents, have a positive effect on the RSV , whereas terms for which the average occurrence in rele-vant documents is less than the average occurrence in non-relevant documents, have a negative effect on the RSV . Mathematically, we summarise this as follows:  X  ( t, r ) &gt; term, no effect on RSV . The occurrence count (frequency) n ( t, d ) increases the effect of a term.

LM: Large (small) P ( t | d ) implies strong (little) effect on the RSV . Small (large) P ( t | c ) implies strong (little) effect on the RSV . A document that misses a rare term ( P ( t | small or even zero, and P ( t | c )smallsince t is rare) misses a significant contribution to the RSV .
BIR applies statistics over judgements for documents, whereas PM applies statistics over frequencies of terms, and LM applies statistics over terms at locations.

For estimating the r -based parameters P BIR ( t | r )and  X  ( t, r ), BIR and PM use the relevance information avail-able, or, in case of missing relevance information, assume P
BIR PM LM 3.2.1 Document and query interpretations
Document is a conjunction of indepen-dent judgements: | q,x ) #  X  2 4
Y 3.2.2 Retrieval status values (RSV X  X ) Odds and assumption:  X 
Y Multiply O ( r | d, q )by Q  X  ( t, r )  X  ( t,  X  r ) 3.2.3 Ranking rationales (see text) 3.2.4 Parameter estimations ( r ) (  X  r )  X  n L ( t, c )
For estimating the  X  r -based parameters, BIR and PM ap-ply two approaches: Either consider a set of explicitly non-relevant documents (for example, the set of retrieved docu-ments minus the set of visited documents), or, assume  X  r i.e. assume that the majority of the documents in the col-lection are implicitly non-relevant. Whereas the parameter estimation reflects already the relevance assumption of BIR and PM, the relevance event (and thus the probability of relevance) is not yet related to LM, and we investigate this in the next section.
In section 2, we reviewed the probability P ( r | d, q )being the origin of BIR, PM, and LM. Now, we show formally that the parameter estimation (see figure 2) implies that BIR and PM assume the collection to be a set of non-relevant documents, whereas the interpretation of LM as an estimate of P ( r, d, q ) implies that LM assumes the collection to be a set of terms from relevant documents. We formalise this in the following theorem.
 Theorem 1 (Relevance Assumption). BIR and PM assume the collection to contain non-relevant docu-ments ,whereas LM assumes the col lection to be a set of terms from relevant documents.

Proof. The parameter estimation for BIR and PM (see figure 2) is as follows: For BIR and PM, the estimates show that the non-relevant set of documents is approximated by the statistics available for the whole collection.

For LM, the product of the term probabilities P LM ( t | d, c ) leads to the query probability P LM ( q | d, c ). With c = r , the LM query probability is an estimate of the probability P ( q | d, r ) in equation 2 (section 2). This makes explicit that LM assumes the collection c to be a sequence of terms from relevant documents.
This relevance assumption implies the interpretation of the document prior P ( d | r ) in equation 2 (remember that for BIR and PM, the query prior P ( q | r ) can be dropped since it is constant for all documents). The document prior collection c to represent the relevance event. As an example for the importance of P ( d | r ), consider [10], a web retrieval investigation on entry page search.
This section shows that the Poisson parameter  X  ( t, c )can be viewed as a bridge connecting BIR and LM. We cap-ture the mathematical relationship between document-based (BIR-based) and location-based (LM-based) probabilities as follows: For the verification, reconsider the definitions  X  ( t, c ):= n L ( t, c ) /N D ( c ), P BIR ( t | c ):= n estimations.

The two fractions in equation 3 have the following mean-ing: avgdl ( c ):= N L ( c ) /N D ( c ) is the average document term frequency of term t in t -documents (the documents in which term t occurs are the t -documents). Given these definitions, we obtain the following equation: P BIR ( t | c )  X  avgtf ( t, c )=  X  ( t, c )= avgdl ( c )  X  We refer to this equation as Poisson or BIR-LM bridge, since the left and right of the equation yield the Poisson parameter  X  ( t, c ).

For example, let  X  X ailing X  occur in 5 locations and 4 doc-collection have 100 locations and 10 documents ( N L ( c )= 100, N D ( c ) = 10). Then, the average within-document fre-quency of sailing is  X  ( sailing, c )=5 / 10.

In average, we expect avgtf ( sailing, c )=5 / 4loca-tions containing sailing per sailing-document, and we ex-pect avgdl ( c ) = 100 / 10 locations per document. For the probabilities, we obtain P BIR ( sailing | c )=4 / 10, and P this to be typical for real-world terms and collections, i.e. we expect for all terms avgtf ( t, c ) &lt; avgdl ( c )tohold.
Aterm t with high avgtf ( t, c ) is referred to as bursty term, and a term t with high P BIR ( t | c ) is referred to as frequent term. In the next section, we take a closer look at bursti-ness and frequency of terms, when discussing an interesting equivalence of BIR and LM.
Deriving the models in parallel poses the question for equivalences. We discuss in this section a condition which makes BIR and LM equivalent.

We define the equivalence of two models (rankings) as follows:
Definition 1 (Equivalence). Models A and B are equivalent iff The RSV  X  X  of equivalent models differ by a constant K (this being additive, since the RSV  X  X  are logarithmic). Equiva-lence is a stricter property than just ranking equivalence. We have results on ranking equivalence, too, however, due to the space restriction in this paper, we decided to present only a result on equivalence.

For which conditions are BIR and LM equivalent? Our ap-proach is to express RSV BIR such that we achieve an equiva-lence of BIR and LM. We formalise the BIR-LM equivalence in the following theorem.

Theorem 2 (BIR-LM Equivalence). If the RSV contribution coming from relevant documents is constant (same for all documents) ,i.e. and if the BIR term weight based on the collection (collec-tions is assumed to represent the statistics for non-relevant documents) is equal to the LM term weight multiplied by a constant  X  ,i.e.
 then BIR and LM are equivalent.
 Proof. Start with the definition of RSV BIR .
 Insert the condition for the term weights (equation 5).
What does this theorem bring? It explains BIR in the event space of LM, i.e. given P BIR ( t | c ), for BIR and LM to be equivalent, we obtain a function P LM ( t | d )forthewithin document term frequency which can be interpreted as the within-document term frequency assumed by BIR.
 Solve equation 5 for obtaining P LM ( t | d ). This leads to: Apply the Poisson bridge (equation 4), and we obtain: P
LM ( t | d )=(1  X  (1 +  X  )  X  P BIR ( t | c ))  X  Figure 3 shows P LM ( t | d ) as a function of P BIR ( t | P
LM ( t | d ) is plotted for various average term frequencies avgtf ( t, c ). The graphs keep avgdl ( c ) = 1000, LM mixture  X  =0 . 8, and  X  =1constant.

P LM ( t | d ) is maximal for P BIR ( t, c ) = 0, which is accord-ing to expectation, since rare terms have a positive effect for BIR, this being reflected in the maximal value for the within document frequency. The constant  X  in equation 5 is a scaling factor that allows to stretch the value interval of P
LM ( t | d ). For  X &lt; 1, we obtain accordingly greater values for P LM ( t | d ) than shown in figure 3.

Next, figure 4 illustrates the product P BIR ( t | c )  X  avgtf ( t, c )=  X  ( t, c ). Figure 4 divides the terms in rare and frequent based on P
BIR ( t | c ) on the horizontal axis, and in solitude and bursty based on avgtf ( t, c ) on the vertical axis. We will see in the next section on tf-idf, how the burstiness is reflected in tf-idf when tf-idf is derived from RSV PM . In this section, we explore the dual application of BIR and LM parameters for expressing tf-idf.
We show in this section that by starting from RSV PM ,we can derive a tf-idf retrieval function, and we show two equiv-alent formulations, one based on the probability P BIR ( t and one based on the probability P LM ( t | c ). These formu-lations stress the duality of BIR and LM: Depending on the event space, parameter estimation is different, however, P
BIR ( t | c )and P LM ( t | c ) can be alternatively used for ex-pressing RSV PM and tf-idf, respectively.
 The RSV PM is defined as follows (see figure 2): Apply the BIR side of the Poisson bridge (equation 4), and we obtain the BIR-based formulation of RSV PM : RSV PM ( d, q )= Apply the LM side of the Poisson bridge, and we obtain the LM-based formulation of RSV PM : RSV PM ( d, q )= The BIR-based and LM-based formulations of RSV PM stress that both, BIR and LM probabilities can be used in a dual way, and we link this in the next section to tf-idf.
We apply the definition idf ( t, x ):=  X  log P BIR ( t | x )and obtain the equation 9 from the BIR-based equation 7. RSV PM ( d, q )= (9)
Next, we define the inverse location frequency ( ilf )anal-theequation10fromtheLM-basedequation8:
The idf -based and ilf -based formulations of the PM differ from classical tf-idf in a factor that is for idf based on the average term frequencies of a term t in the documents in which t occurs (the t -documents), and for ilf ,thefactoris based on the average document lengths. Set avgtf ( t, r )= document normalisation.
 From the probability of relevance point of view, BIR and PM are based on P ( d | q,r ), whereas LM is based on P ( q This implies that there is no document normalisation in BIR and PM, whereas normalisation comes for free in LM.
To compensate for the lack of document normalisation in P ( d | q ) approaches, tf-idf, BM25, and pivoted document length ([20]) add normalisations. For example, traditional tf-idf replaces the total term frequency n L ( t, d )in RSV by the linear estimate tf ( t, d ):= P ( t | d )= n L ( t, d ) /N the normalisation being reflected by N L ( d ), the document length.

BM25 gives evidence that a tf -factor such as tf ( t, d ):= n ( t, d ) / ( n L ( t, d )+ K ) works better than the linear esti-mate. Growth of the BM25 tf is non-linear, and for K =1, we find the lower bound 0 . 5  X  tf ( t, d ), independent of docu-ment length. In BM25, the factor ( avgdl  X  dl ) / ( avgdl + dl ) awards short documents, and penalises long documents.
Pivoted document length helps to balance single term weights by replacing the non-pivoted normalisation factor by pivoted norm =(1  X  slope )  X  pivot + slope  X  old norm .For example, view N L ( d ) (the document length) as old norm , and the average document length could be the pivot. Then, we obtain a normalisation that assigns smaller term weights for small documents and larger term weights to large docu-ments, than the non-pivoted normalisation does. The slope parameter controls the impact of the pivot.
We derived  X  strictly parallel  X  the three grand prob-abilistic retrieval models: BIR, PM, and LM. The paral-lel derivation in a stringent mathematical formalism high-lighted the event spaces, background models, frequencies, probabilities, term probability interpretations (summarised in figure 1), and document/query interpretations, retrieval status values, and parameter estimations (summarised in figure 2).

There have been several motivations for this parallel derivation: First of all, the explanation of retrieval mod-els through the probability of relevance involves notations such as P ( t | c ) for the probability of a term, and this nota-tion is correct for LM but misleading for BIR and PM, since not terms but judgements and frequencies form the event spaces for BIR and PM, respectively. Therefore, this paper clarifies in a mathematical formalism the event spaces and notations of the models.

The second motivation has been the observation that there are ranking equivalences and even equivalences of the models; due to space restriction, we reported only one equivalence. Equivalences allow for an interesting insight of what the models assume. For example, looking at BIR through LM glasses shows that BIR assumes the probabili-uments, to be a function of the document occurrence in the erage term frequency in the elite set of the term, and the average document length.

The third motivation was born when looking for theo-retical arguments for tf-idf. Whereas for BIR and PM, the relationship is well researched, this paper highlights through the Poisson bridge the dual representation of tf-idf retrieval applying either BIR or LM parameters.

We carried out this theoretical investigation of the parallel derivation of the probabilistic models to clarify the origin and assumptions of the models, and to find relationships of the models. The result supports the well-defined refinements of the models, and it might support the search for the quarks of IR.

Acknowledgements: Hugo Zaragoza, Stephen Robert-son, Arjen de Vries, and Djoerd Hiemstra influenced signifi-cantly the content of this paper. Thanks to Hugo for enjoy-able and effective workshops, where Hugo questioned Pois-son, Stephen challenged event spaces, and Arjen stressed consistent notation. Thanks to Djoerd for the discussion in our IR seminar, and, thanks to the reviewer who went so deeply into the notation and formulae in this paper, and provided most valuable feedback.
