 Schema matching is the problem of finding correspondences (map-ping rules, e.g. logical formulae) between heterogeneous schemas. This paper presents a probabilistic framework, called sPLMap, for automatically learning schema mapping rules. Similar to LSD, dif-ferent techniques, mostly from the IR field, are combined. Our approach, however, is also able to give a probabilistic interpreta-tion of the prediction weights of the candidates, and to select the rule set with highest matching probability.
 Categories and Subject Descriptors: H.2.1 [Database Manage-ment]: Logical Design -Schema and subschema General Terms: Theory, Experimentation Keywords: sPLMap, schema matching, predicate logics, probability theory
Combining heterogeneous data from different sources, i.e. trans-forming data structured under one schema into data structured un-der a different schema, is an old but emerging problem in the  X  X n-formation age X  (e.g. for federations of existing digital libraries like the ACM DL or CiteSeer). This problem is currently under in-vestigation in the context of information integration [3] and data exchange [2]. When schemas change frequently over time, or a large amount of schemas is involved, automatic mapping genera-tion (called schema matching ) is mandatory.

This paper describes the sPLMap framework (probabilistic, logic-based mapping between schemas) [4, 5] for automatic schema matching, which is founded on information retrieval and machine learning techniques. From well-known approaches like LSD [1], it borrows the idea of combining several specialized components (called  X  X lassifiers X ) for finding the best mapping, e.g. based on at-tribute name comparison or comparing properties of the underlying 02-01, project  X  X epper X ) and in part by ISTI-CNR (project  X  X is-tributed Search in the Semantic Web X ).
 data instances. The major improvements over LSD are the support for data types (e.g. text, names, different date formats) in the match-ing process, and the usage of probability theory for estimating the degree of correctness of a learned rule. Thus, it provides a sound theoretical justification for its (optimum) selection, and a measure for the quality of a learned schema mapping.

In the following, a schema R =  X  R 1 ,..., R n  X  is a tuple of bi-nary relations R i containing document identifiers and attribute val-ues (belonging to a data type, e.g.  X  X ext X ,  X  X ame X ,  X  X ear X  or  X  X ateISO8601 X ). This structure corresponds to a linear list of multi-valued schema attributes . The instance of an attribute R denoted by R i , and, similarly, R is used for the schema and the corresponding schema instance .

A schema mapping rule has the form S i  X  T j . Formally, it speci-fies that pairs of document identifier and attribute values have to be copied from source instance S i into target instance T j . Let  X  set of rules with common rule head T j . Then, the result of applying all rules from  X  j onto the source instance S ; thus, denotes the instance derived by applying  X  onto S . Typically, the set  X  of mapping rules on which context and left out. Consider a target schema T =  X  T 1 ,..., T t  X  and a source schema S =  X  S 1 ,..., S s  X  together with two instances (which do not neces-sarily describe the same documents). The goal is to find the  X  X est X  set of mapping rules  X  which maximizes the probability Pr that the tuples in T and in the mapping result is, given a random tuple in T , estimate the probability that it is a tuple in
Each  X  can be partitioned into t sets  X  1 ,...,  X  t (where any  X  contains only rules with rule head T j ). As the rules in these sets operate independently onto different target attributes, we obtain: Now, Pr (  X  j , T , S ) is estimated as the probability that a tuple in is also in T j , and vice-versa: Unfortunately, there are exponentially many possible sets  X  computational simplification, we assume that S i joint for i 1 6 = i 2 . This leads us to the approximation:
Thus, in order to compute Pr (  X  j , T , S ) , the main task is to com-combine different classifiers CL 1 ,..., CL n (see table 1 for classi-fiers we use). Each classifier outputs a conditional probability Pr ( S i | T j , CL k ) as an approximation of Pr ( S i | T j ) bility Pr ( S i | T j ) is computed by combining the classifier predictions Pr ( S i | T j , CL k ) in a weighted sum:
In a first step, each classifier CL k computes an initial weight w (
S i , T j , CL k ) , which is then normalized (due to heterogeneous scales) into the probability Pr ( S i | T j , CL k )= f ( first normalization step, an arbitrary function can be used, e.g. lin-ear or logistic functions. A second normalization ensures that the final values for Pr ( S i | T j , CL k ) and for Pr ( T j |
Finally, the probability Pr ( CL k ) describes the probability that we rely on the judgment of classifier CL k . We simply set Pr (i.e., we average over all classifiers). Alternatively, one could aim at learning these probabilities for each classifiers individually.
This section briefly describes some experimental results; more results can be found in [4, 5]. We use a 2.382 document BibTeX collection (BIBDB) with a large overlap in attribute names, and an Open Archive collection of the Library of Congress (LOC) with 1.337 entries, available in MARC 21 and Dublin Core. Both collec-tions are split randomly into three sub-collections of approximately the same size (two for learning, one for evaluation). Classifiers from table 1 are used in isolation as well as in combination. As all LOC attributes use the same data type  X  X ext X , and do not share common names, only the content-oriented classifiers are used for this collection.

The results in table 2 use the typical IR measures precision, re-call and F-measure. For BIBDB, the combination of all classifiers yields 82% of precision and recall. According to the F-measure, the best schema classifier are CL N and CL S and the worst is CL the best content classifier is CL L and the worst is CL KL the combination of all classifiers yields 64% of precision and 22% of recall only. The best content classifier is CL kNN (which slightly outperforms the combination) and the worst is CL KL .

There is no general best classifier. Which classifier performs well always depends on the concrete schemas (and their instances). The experiments show for BIBDB (and other similar collections we cannot report here due to space limitations) satisfactory results for the combination of all classifiers, while the performance is worse for the difficult LOC collection (where the DC attributes often con-tain text which equals the concatenation of strings from multiple MARC 21 attributes). Typically, the name-based classifiers pro-vide good precision and rather low recall, while using data types perform worse. Content-oriented classifiers proved to be very use-ful for schema matching (w. r. t. both precision and recall), which is important in general settings where schema names alone are not sufficient to find matching attributes. Here, kNN and Naive Bayes provide high precision. CL KL yields low precision but can im-prove recall when combined with other classifiers. The experiments further showed that combining classifiers can clearly increase the matching quality.
We have presented sPLMap, a Probabilistic, Logic-based formal framework for the important schema matching problem. The pecu-liarity of our approach is that it combines neatly machine learning, information retrieval and heuristic techniques for learning a set of mapping rules. The framework has already been extended towards uncertain rules [4, 5]. In future, we plan to develop and evaluate ad-ditional classifiers CL K , and learn the probabilities Pr framework can easily be extended towards more complex rules, which e.g. combine the content of several attributes, and to new application areas like ontologies. [1] A. Doan, P. Domingos, and A. Y. Halevy. Reconciling [2] R. Fagin, P. G. Kolaitis, R. Miller, and L. Popa. Data [3] M. Lenzerini. Data integration: a theoretical perspective. In [4] H. Nottelmann and U. Straccia. A probabilistic approach to [5] H. Nottelmann and U. Straccia. sPLMap: A probabilistic
