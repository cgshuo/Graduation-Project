 They model dyadic data by a multiplicative interaction of two Gaussian process priors. U is given by a kernel (covariance) function  X  : U  X U  X  R . A Gaussian process (GP) defines a random function f : U  X  R , whose distribution is characterized by a mean function and the V the similarity between users and  X  measures the similarity between items. Model 1. The generative model of an SRM: 1. Draw kernel functions  X   X  X W  X  (  X ,  X   X  ) , and  X   X  X W  X  (  X ,  X   X  ) ; 2. For k = 1 ,...,d : draw random functions f k  X  X   X  (0 ,  X ) , and g k  X  X   X  (0 ,  X ) ; 3. For each pair ( u,v ) : draw y ( u,v )  X  p ( y ( u,v ) | z ( u,v ) , X  ) , where b ( u,v ) is the bias function over the U  X V . For simplicity, we assume b ( u,v ) = 0 . In the limit d  X   X  , the model converges to a special case where f k and g k can be analytically marginalized out and z becomes a Gaussian process z  X  N  X  (0 ,  X   X   X ) [15], with the covariance between pairs being a tensor kernel it is easy to see that the model reduces to probabilistic matrix factorization. dimensional inverted Wishart distribution [2] community. The advantage of this new notation is demonstrated by the following theorem [2]. Theorem 1. Let A  X  X W m (  X , K ) , A  X  R + , K  X  R + , and A and K be partitioned as where A 11 and K 11 are two n  X  n sub matrices, n &lt; m , then A 11  X  X W n (  X , K 11 ) . kernel functions, denoted by  X   X  IW  X  (  X ,  X   X  ) , such that any sub kernel matrix of size m  X  m of their sub kernel matrices is an identity matrix.
 Z U X V that contains I  X  O . Accordingly we let  X  be the covariance matrix of  X  on U covariance matrix of  X  on V 0 .
 Previously a variational Bayesian method was applied to SRMs [15], which computes the maximum Bayesian inference algorithm using Markov chain Monte Carlo sampling. By deriving equivalent than the previous work [15], and produce an excellent accuracy.
 Section 5 concludes. tation of Z O directly from Y I using Markov-chain Monte Carlo (MCMC) algorithm (specifically, make the MCMC inference more efficient for large scale data.
 distribution, respectively. 2.1 Models with Non-informative Priors Then the generative model is written as Model 2 and depicted in Figure 1. Model 2. The generative model of a matrix-variate SRM: 1. Draw  X   X  X W m (  X , X  I ( m ) ) and  X   X  X W n (  X , X  I ( n ) ) ; 2. Draw F |  X   X  X  m,d (0 ,  X   X  I ( d ) ) and G |  X   X  X  n,d (0 ,  X   X  I ( d ) ) ; 3. Draw s 2  X   X   X  2 (  X , X  2 ) ; 4. Draw Y | F , G ,s 2  X  X  m,n ( Z ,s 2 I ( m )  X  I ( n ) ) , where Z = FG &gt; . where N m,d is the matrix-variate normal distribution of size m  X  d ;  X  ,  X  ,  X  ,  X  and  X  2 are scalar parameters of the model. A slight difference between this finite model and Model 1 is that the coefficient 1 / this coefficient can be absorbed by  X  or  X  .
 Pr( s 2 | Y 2.2 Gibbs Sampling Method process using the property of Theorem 2 to take the advantage of d min( m,n ) . Theorem 2. If then, matrix variates, F and H , have the same distribution.
 Proof sketch. Matrix variate F follows a matrix variate t distribution, t (  X , 0 , X  I p ( H )  X  X  I Thus, matrix variates, F and H , have the same distribution. This theorem allows us to sample a smaller covariance matrix K of size d  X  d on the column side Figure 3). A similar idea was used in our previous work [16].
 Model 3. The alternative generative model of a matrix-variate SRM: 1. Draw K  X  X W d (  X , X  I ( d ) ) and R  X  X W d (  X , X  I ( d ) ) ; 2. Draw F | K  X  X  m,d (0 , I ( m )  X  K ) , and G | R  X  X  n,d (0 , I ( n )  X  R ) , 3. Draw s 2  X   X   X  2 (  X , X  2 ) ; 4. Draw Y | F , G ,s 2  X  X  m,n ( Z ,s 2 I ( m )  X  I ( n ) ) , where Z = FG &gt; . Let column vector f i be the i -th row of matrix F , and column vector g j be the j -th row of matrix G . In Model 3, { f i } are independent given K , (for Bayesian SRM).
 d ( m + n )) time complexity 2 , which is a dramatic reduction from the previous time complexity O ( m 3 + n 3 ) . 2.3 Models with Informative Priors an m  X  p matrix, and  X   X  = G  X  ( G  X  ) &gt; +  X  I p additional features of entities.
 Although such an informative  X   X  prevents us from directly sampling each row of F independently, illustrates this transformation.
 Theorem 3. Let  X  &gt; p ,  X   X  = F  X  ( F  X  ) &gt; +  X  I ( m ) , where F  X  is an m  X  p matrix. If where K 11  X  2 = K 11  X  K 12 K  X  1 22 K 21 , then F and H have the same distribution. Proof sketch. Consider the distribution which implies p ( F ) = p ( H 1 | H 2 = F  X  ) = p ( H ) . The following corollary allows us to compute the posterior distribution of K efficiently. Corollary 4. K | H  X  X W d + p (  X  + m, X  I ( d + p ) + ( H , F  X  ) &gt; ( H , F  X  )) . Proof sketch. Because normal distribution and inverse Wishart distribution are conjugate, we can derive the posterior distribution K from Eq. (1).
 (BSRM/F for BSRM with features) in Appendix. We note that when p = q = 0 , Algorithm 1 (BSRM/F) reduces to the exact algorithm for BSRM. Each iteration in this sampling algorithm can be computed in O ( d 2 r + d 3 ( m + n ) + dpm + dqn ) time complexity. 2.4 Unblocking for Sampling Implementation Blocking Gibbs sampling technique is commonly used to improve the sampling efficiency by re-as Step 4 and Step 9 of Algorithm 2, which is called BSRM/F with conditional Gibss sampling. We comparable to other low-rank matrix factorization approaches. Though such a conditional sampling cost of Algorithm 2 is usually less than that of Algorithm 1 when achieving the same accuracy. Algorithm 2. very popular in collaborative filtering applications, e.g., [12, 8, 13].
 offers a more general Bayesian framework that allows informative priors from entity features to a nonparametric prior for stochastic relational processes is described. Synthetic data: We compare BSRM under noninformative priors against two other algorithms: the fast max-margin matrix factorization (fMMMF) in [12] with a square loss, and SRM using varia-tional Bayesian approach (SRM-VB) in [15]. We generate a 30  X  20 random matrix (Figure 5(a)), in Figure 5(c)-5(e). BSRM outperforms the variational approach of SRMs and fMMMF. Note that because of the log-determinant penalty of the inverse Wishart prior, SRM-VB enforces the rank to be smaller, thus the result of SRM-VB looks smoother than that of BSRM. Table 1: RMSE (root mean squared error) and MAE (mean absolute error) of the experiments on EachMovie data. All standard errors are 0 . 001 or less.
 EachMovie data: We test the accuracy and the efficiency of our algorithms on EachMovie. The compare our approach against several competing methods: 1) User Mean, predicting ratings by the sample mean of the same user X  X  ratings; 2) Move Mean, predicting rating by the sample mean of ratings on the same movie; 3) fMMMF [12]; 4) VB introduced in [8], which is a probabilistic low-the SRM-VB of [15]. We test the algorithms BSRM and BSRM/F, both following Algorithm 2, Table 2: RMSE (root mean squared error) and MAE (mean absolute error) of experiments on Each-Movie data. All standard errors are 0 . 001 or less.
 and Algorithm 2, we run both algorithms and record the running time and accuracy in RMSE. The dimensionality d is set to be 100 . We compute the average Z O and evaluate it after a certain number of itera-tions. The evaluation results are shown in Figure 6. We run both algorithms for 100 iterations as the burn-in period, so that we can have an independent start sample. Af-ter the burn-in period, we restart to compute the averaged Z O and evaluate them, there-fore there are abrupt points at 100 iterations in both cases. The results show that the overall accuracy of Algorithm 2 is better at any given time. for all the participants.
 The features used in BSRM/F are generated from the PCA result of a binary matrix that indicates Table 3. The submitted result of BSRM/F(400) achieves RMSE 0 . 8881 on the test set. The running time is around 21 minutes per iteration for 400 latent dimensions on an Intel Xeon 2GHz PC. by transforming SRMs into equivalent models, which can be efficiently sampled. The experiments Acknowledgment: We thank the reviewers and Sarah Tyler for constructive comments. [4] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions . Chapman &amp; Hall/CRC, 2000. [7] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and N. Ueda. Learning systems of [9] J. S. Liu. Monte Carlo Strategies in Scientific Computing . Springer, 2001. [12] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative I entry of X . | X | 2 elements in Y with 0 for simplicity in notation Algorithm 1 BSRM/F: Gibbs sampling for SRM with features 5: Draw s 2  X   X   X  2 (  X  + r, X  2 + | Y  X  FG &gt; | 2 I ) .
 Algorithm 2 BSRM/F: Conditional Gibbs sampling for SRM with features 3: for each ( i,k )  X  X  0  X { 1 ,  X  X  X  ,d } do 6: end for 8: for each ( j,k )  X  X  0  X { 1 ,  X  X  X  ,d } do 11: end for 12: Draw s 2  X   X   X  2 (  X  + r, X  2 + |  X  | 2 I ) .
