 The collaborative filtering (CF) approach to recommender system has received much attention recently. However, pre-vious work mainly focuses on improving the formula of rat-ing prediction, e.g. by adding user and item biases, im-plicit feedback and time-aware factors, etc, to reach a bet-ter prediction by minimizing an objective function. How-ever, little effort has been made on improving CF by in-corporating additional regularization to the objective func-tion. Regularization can further bound the searching range of predicted ratings. In this paper, we improve the con-ventional rating-based objective function by using ranking constraints as the supplementary regularization to restrict the searching of predicted ratings in smaller and more likely ranges, and develop a novel method, called RankSVD++, based on the SVD++ model. Experimental results show that RankSVD++ achieves better performance than exist-ing main-streaming methods due to the addition of informa-tive ranking-based regularization. The idea proposed here can also be easily incorporated to the other CF models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms, Experimentation Collaborative Filtering, Matrix Factorization
A collaborative filtering (CF) [1] recommender system, primarily based on the neighborhood approach and the la-tent factor approach, produces personalized item recommen-dations to users by relying on past behaviors, e.g. ratings, transactions or even web click streams. A good CF system will not only enhance the satisfaction and loyalty of cus-tomers but also promote sales. Some e-commences and web service providers like Amazon and Netflix have adopted the CF-based recommendation to further their businesses.
Previous work on CF methods primarily focuses on im-proving the predicted function of rating. For instance, the factor models, e.g. SVD, SVD++ [3] and timeSVD++ [4], which are serial models originated from the basic Matrix Factorization (MF) model [1], are augmented by integrating user and item biases, implicit feedback of users, and tempo-ral effects respectively in calculating predicted ratings. The Netflix Prize announced the success of these improvements.
To enhance the effectiveness of CF recommender, we pro-pose to integrate the objective function with additional reg-ularization. In this paper, we address optimization of the classic rating-based metric in combination with a ranking-based regularization. The rating-based metric in [1] receives reward for a predicted rating  X  r close to the given rating value r and gets penalty for the predicted rating  X  r far away from the given one. The rank-based regularization receives re-ward for predicting the same pairwise ordering  X  r i &gt;  X  r the given ordering r i &gt;r j . The idea of this work is that a rating-based metric combined with extra ranking-based reg-ularization helps to restrict the searching of predicted rat-ings in smaller and more likely ranges. The regularization factor depends on the difference between the given rating and the predicted rating to measure pairwise ranking. This measurement not only can be well unified with the loss func-tion of rating but also make the new objective function a continuous function for convenient optimization. Figure 1: Given ratings r and predicted ratings  X  r 1 ,  X  r .The  X  r 1 is near-perfect for rating metric, but with bad ranking relation. The  X  r 2 is a near-perfect prediction of rating-based metric with an additional ranking-based regularization and is better than  X  r 1 .
This work addresses the limitation of the conventional rating-based metric of CF. Since the perfect prediction of rating metric provides a perfect ranking scheme, one might think that a good prediction of rating would also provide a good ranking schema. That is not true. Consider the distribution of given ratings r in Fig. 1, there are n  X  ratings with highest value r = 5 while just one with r =1. Judged according to the metric of rating, the prediction  X  r provides excellent rating performance, however, it expresses a very poor ranking relation. When we impose the ranking-based regularization into the ranking metric, the new model is able to generate a near-perfect prediction  X  r 2 which is bet-ter than  X  r 1 . In this work, we want to design such a model that exalts the performance of rating-based CF with added ranking-based regularization.
Let us assume a set of users U and a set of items I in a typical CF scenario. Each user u is associated with a set of items I u , which contains all the items that the user has rated. All known ( u, i ) pairs between U and I is denoted as aset R . d u and d i indicate the deviations of user u and item i from the average rating  X  of R . f -dimension factor vectors p u  X  R f and q i  X  R f describe the latent characters of the user u and item i . A second vector y i  X  R f of item i is used to characterize users based on the set of items they rated as a implicit feedback. In SVD++ model [3], the function of predicted rating is written as  X  r ui =  X  + d i + d u + q Now we investigate the scenario of combining the metric of SVD++ with the supplementary ranking-based regulariza-tion. A simple and successful pairwise ranking [2] is involved in this work. Similar to other norm terms, we also use the Frobenius norm to calculate the regularization of pairwise ranking, i.e. ( r i  X  r j )  X  ( X  r i  X   X  r j ) 2 . To combine with the metric of rating, we calculate the sum of associated ranking-based regularization of items in I u at each ( u, i )pairand normalize the sum with coefficient |I u |  X  0 . 5 . Meanwhile,  X  is used to adjust the weight of ranking regularization. The model with the new objective function is learnt by mini-mizing the regularized least squares function. The objective function can be expressed as where, the first two terms, i.e. the loss function of rating and regularized parameters of models in a ( u, i ) pair, compose the least square function of the SVD++ model [3]. The final term is the ranking-based regularization modeled by the loss between two given ratings and two predicted ratings. Finally, the objective function of our proposed RankSVD++ can be written as Since the criterion of RankSVD++ is composed of con-vex functions, it can be solved efficiently by the method of stochastic gradient descent [3]. We loop over all known ratings in R ,calculating: where, to simplify the expression, some abbreviates are used b ui = j  X  X  u e ui  X  e uj . Learning rate is denoted as  X  .Reg-ularization factors  X  1 ,  X  2 and  X  are used to avoid overfitting.
For our experiments, we use the Yahoo Music rating data [5]. About 6.4 million ratings of 16,883 users and 2,676 songs between years 2002 and 2006 are chosen, where each user gives at least 20 ratings. Three ratings of each user are randomly chosen as test data, the remaining ones are treated as training data. The experiments are repeated 10 times by sampling new training and test data.

To illustrate the effectiveness of our model, we compare it with a number of factor models and neighborhood meth-ods. The MF model is the basic SVD without the user and item biases. The SVD++ model improves SVD by inte-grating the implicit feedback of users. Since timeSVD++ needs time-aware factor as extra information, we did not implement it on this data. On the other hand, we imple-ment two neighborhood methods, i.e. user-based and item-based methods, using the Pearson correlation to measure the similarity. The learning rate and hyperparameters of factor models are searched on the first training data. The quality of predictions is measured by the Root Mean Squared Er-ror (RMSE)[1] (smaller is better). The average RMSEs of 10-time tests are listed in Fig. 2. Figure 2: Prediction accuracy of different methods measured by RMSE for varying dimensionality f .

The empirical results indicate the factor models outper-form two neighborhood models, and our RankSVD++ model performs the best among these factor models. All factor models benefit from the growing number of factor dimen-sions f , which can better express the latent characters of the users and items. The advantage delivered by RankSVD++ over SVD++ is consistently significant, due to the added constrains of ranking. Compared with the benefit brought by additional ranking regularization, the gain in accuracy by adding implicit feedback of users in SVD++ over SVD model is not as significant, and the gain decreases with the growing f . The gain in accuracy by integrating user and item biases in SVD over MF is smaller than the gain brought by ranking. Further evidence of the importance of combining ranking-based regularization is the fact that the RankSVD++ model at f = 10 is already more accurate than the other factor models at f = 200. The results show that RankSVD++ is a very effective single model.
