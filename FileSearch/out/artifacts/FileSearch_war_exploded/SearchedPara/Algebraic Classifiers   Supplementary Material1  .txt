 ran, as well as a brief description of the method of moments and how to give algebraic structure to an estimated distribution. For the experiments in Figure 3 and 7 of the main paper, we selected 30 different classification data sets from the UCI machine learning repository to perform our experiments. They were selected because they come from a number of different domains and have widely varying characteristics. They are: sulting means displayed in the plots. For the homomorphic learners, we could the non-homomorphic learners tested against. For consistency, we used this relatively low number for all trials.
 Bayesian classifier and HomStump, we used the normal distribution for continu-ous data and categorical distribution for discrete data. For AdaBoost, we set the maximum number of boosting rounds to 25. This is a small number of rounds to use for these simple classifiers, and it X  X  likely that using more rounds would have led to better performance. Ideally, we would have chosen the optimal number of rounds via cross-validation. The method of moments estimates a continuous probability distribution from a list of samples. It is a group learner and can be used as the base distribution for the Bayesian classifier. Given an n -element input vector ( x training, the formula for the j th moment is: The model for the distribution d estimated with k moments is the tuple: The binary operation d 0 = d 1 d 2 is defined as: where m j,i is the i th moment of distribution j . The empty element is: And the inverse is: Estimating a Normal distribution is a special case of this method because the sufficient statistics are the 0 th , 1 st , and 2 nd moments.
