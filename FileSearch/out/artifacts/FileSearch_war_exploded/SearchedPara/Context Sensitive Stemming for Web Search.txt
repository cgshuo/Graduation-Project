 Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar trans-formation to query terms. Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of addi-tional computation.

In this paper, we propose a context sensitive stemming method that addresses these two issues. Two unique proper-ties make our approach feasible for Web Search. First, based on statistical language modeling, we perform context sensi-tive analysis on the query side. We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine. This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and im-proves the precision at the same time. Second, our approach performs a context sensitive document matching for those expanded variants. This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision. Using word plural-ization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Query formulation Algorithms, Experimentation Stemming, language modeling, Web search Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Web search has now become a major tool in our daily lives for information seeking. One of the important issues in Web search is that user queries are often not best formulated to get optimal results. For example,  X  X unning shoe X  is a query that occurs frequently in query logs. However, the query  X  X unning shoes X  is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words  X  X unning shoes X .

Correctly formulating a query requires the user to accu-rately predict which word form is used in the documents that best satisfy his or her information needs. This is dif-ficult even for experienced users, and especially difficult for non-native speakers. One traditional solution is to use stem-ming [16, 18], the process of transforming inflected or de-rived words to their root form so that a search term will match and retrieve documents containing all forms of the term. Thus, the word  X  X un X  will match  X  X unning X ,  X  X an X ,  X  X uns X , and  X  X hoe X  will match  X  X hoes X  and  X  X hoeing X . Stem-ming can be done either on the terms in a document dur-ing indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing. Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows han-dling each query differently, and hence is preferred.
Although traditional stemming increases recall by match-ing word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched. When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal num-bers of queries are helped and hurt by the technique [6]. In addition, it reduces system performance because the search engine has to match all the word variants. As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa. Thus, one needs to be very cautious when using stemming in Web search engines.

One problem of traditional stemming is its blind trans-formation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word. For example, the word  X  X ook X  has four forms  X  X ook, books, booking, booked X , and  X  X tore X  has four forms  X  X tore, stores, storing, stored X . For the query  X  X ook store X , expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query. Transforming  X  X ook store X  to match  X  X ook stores X  is fine, but matching  X  X ook storing X  or  X  X ooking store X  is not. A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query. However uniform weighting is not go-ing to work and a query dependent weighting is still a chal-lenging unsolved problem [20].

A second problem of traditional stemming is its blind matching of all occurrences in documents. For the query  X  X ook store X , a transformation that allows the variant  X  X tores X  to be matched will cause every occurrence of  X  X tores X  in the document to be treated equivalent to the query term  X  X tore X . Thus, a document containing the fragment  X  X eading a book in coffee stores X  will be matched, causing many wrong doc-uments to be selected. Although we hope the ranking func-tion can correctly handle these, with many more candidates to rank, the risk of making mistakes increases.

To alleviate these two problems, we propose a context sensitive stemming approach for Web search. Our solution consists of two context sensitive analysis, one on the query side and the other on the document side. On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms. On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query. Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines.

We use pluralization handling as a running example for our stemming approach. The motivation for using plural-ization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance. As far as we know, no pre-vious research has systematically investigated the usage of pluralization in Web search. As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion. Experiments on general stem-ming yield additional significant improvements over plural-ization handling for long queries, although details will not be reported in this paper.

In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2. We describe the details of the context sensitive stemming approach in Section 3. We then perform extensive experi-ments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5. Finally, we conclude the paper in Section 6.
Stemming is a long studied technology. Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18]. The Porter stemmer is widely used due to its simplicity and effectiveness in many applications. However, the Porter stemming makes many mistakes be-cause its simple rules cannot fully describe English morphol-ogy. Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morpho-logically similar and occur in similar context as measured by expected mutual information [23]. We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context fea-tures which can be more than just adjacent words [15], and then only keep the morphologically similar words as candi-dates.

Using stemming in information retrieval is also a well known technique [8, 10]. However, the effectiveness of stem-ming for English query systems was previously reported to be rather limited. Lennon et al. [17] compared the Lovins and Porter algorithms and found little improvement in re-trieval performance. Later, Harman [9] compares three gen-eral stemming techniques in text retrieval experiments in-cluding pluralization handing (called S stemmer in the pa-per). They also proposed selective stemming based on query length and term importance, but no positive results were reported. On the other hand, Krovetz [14] performed com-parisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%). However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search. These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2]. We suspect the previous failures were mainly due to the two problems we mentioned in the intro-duction. Blind stemming, or a simple query length based selective stemming as used in [9] is not enough. Stemming has to be decided on case by case basis, not only at the query level but also at the document level. As we will show, if han-dled correctly, significant improvement can be achieved.
A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25]. To de-cide which expanded words to use, people often use pseudo-relevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract rel-evant words from these top documents as additional query words, and resubmit the expanded query again [21]. This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search. In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift ). Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query. Thus query expansion based on pseudo-relevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search. On the other hand, stemming is much more conservative since most of the time, stemming pre-serves the original search intent. While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision. The increase on recall is obvious. With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded.
On selective query expansion, Cronen-Townsend et al. [6] proposed a method for selective query expansion based on comparing the Kullback-Leibler divergence of the results from the unexpanded query and the results from the ex-panded query. This is similar to the relevance feedback in the sense that it requires multiple passes retrieval. If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful. It is expensive to deploy this in production Web search engines. Our method predicts the quality of expan-sion based on offline information without sending the query to a search engine.

In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search  X  stemming. Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision. It X  X  simple, yet very effi-cient and effective, making real time stemming feasible for Web search. Our results will affirm researchers that stem-ming is indeed very important to large scale information retrieval.
Our system has four components as illustrated in Fig-ure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and con-text sensitive document matching. Candidate generation (component 1) is performed offline and generated candidates are stored in a dictionary. For an input query, we first seg-ment query into concepts and detect the head word for each concept (component 2). We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4). Below we discuss each of the components in more detail.
One of the ways to generate candidates is using the Porter stemmer [18]. The Porter stemmer simply uses morphologi-cal rules to convert a word to its base form. It has no knowl-edge of the semantic meaning of the words and sometimes makes serious mistakes, such as  X  X xecutive X  to  X  X xecution X ,  X  X ews X  to  X  X ew X , and  X  X aste X  to  X  X ast X . A more conserva-tive way is based on using corpus analysis to improve the Porter stemmer results [26]. The corpus analysis we do is based on word distributional similarity [15]. The rationale of using distributional word similarity is that true variants tend to be used in similar contexts. In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word. We use the bigrams to the left and right of the word as its con-text features, by mining a huge Web corpus. The similarity between two words is the cosine similarity between the two corresponding feature vectors. The top 20 similar words to  X  X evelop X  is shown in the following table. 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word  X  X evelop X . Column score is the similarity score. To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list. After applying these rules, for the word  X  X evelop X , the stemming candidates are  X  X eveloping, developed, de-velops, development, developement, developer, developmen-tal X . For the pluralization handling purpose, only the can-didate  X  X evelops X  is retained.

One thing we note from observing the distributionally similar words is that they are closely related semantically. These words might serve as candidates for general query expansion, a topic we will investigate in the future.
For long queries, it is quite important to detect the con-cepts in the query and the most important words for those concepts. We first break a query into segments, each seg-ment representing a concept which normally is a noun phrase. For each of the noun phrases, we then detect the most im-portant word which we call the head word. Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity.

To break a query into segments, we have to define a cri-teria to measure the strength of the relation between words. One effective method is to use mutual information as an in-dicator on whether or not to split two words [19]. We use a log of 25M queries and collect the bigram and unigram frequencies from it. For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word. We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here. Table 2 shows some examples of query segmentation.
The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query. Query parsing is more difficult than sentence Table 2: Query segmentation: a segment is brack-eted. parsing since many queries are not grammatical and are very short. Applying a parser trained on sentences from docu-ments to queries will have poor performance. In our solu-tion, we just use simple heuristics rules, and it works very well in practice for English. For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like  X  X YZ of/in/at/from UVW X . In such cases, the head word is typically the last nonstop word of XYZ.
After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful.

Our statistics show that about half of the queries can be transformed by pluralization via naive stemming. Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse. Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost. In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful. Taking query  X  X otel price comparison X  as an ex-ample, we decide that hotel and price comparison are two concepts. Head words  X  X otel X  and  X  X omparison X  can be expanded to  X  X otels X  and  X  X omparisons X . Are both trans-formations useful?
To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web. The more likely a query to occur on the Web, the more relevant documents this query is able to return. Now the whole problem becomes how to calculate the probability of query to occur on the Web.

Calculating the probability of string occurring in a cor-pus is a well known language modeling problem. The goal of language modeling is to predict the probability of nat-urally occurring word sequences, s = w 1 w 2 ...w N ; or more simply, to put high probability on word sequences that ac-tually occur (and low probability on word sequences that never occur). The simplest and most successful approach to language modeling is still based on the n -gram model. By the chain rule of probability one can write the probability of any word sequence as An n -gram model approximates this probability by assum-ing that the only words relevant to predicting Pr( w i | w are the previous n  X  1 words; i.e.
 A straightforward maximum likelihood estimate of n -gram probabilities from a corpus is given by the observed fre-quency of each of the patterns where #(.) denotes the number of occurrences of a specified gram in the training corpus. Although one could attempt to use simple n -gram models to capture long range dependen-cies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of W n events, where W is the size of the word vocabulary. This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6). Also, because of the heavy tailed nature of language (i.e. Zipf X  X  law) one is likely to encounter novel n -grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n -grams is a cen-tral and unavoidable issue in statistical language modeling. One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with po-tentially missing n -grams) is to use some sort of back-off estimator.
 where is the discounted probability and  X  ( w i  X  n +1 ...w i  X  1 malization constant
The discounted probability (4) can be computed with dif-ferent smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing [5]. We used absolute smoothing in our experi-ments.

Since the likelihood of a string, Pr( w 1 w 2 ...w N ), is a very small number and hard to interpret, we use entropy as de-fined below to score the string.

Now getting back to the example of the query  X  X otel price comparisons X , there are four variants of this query, and the entropy of these four candidates are shown in Table 3. We can see that all alternatives are less likely than the input query. It is therefore not useful to make an expansion for this query. On the other hand, if the input query is  X  X otel price comparisons X  which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded. To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score. Table 3: Variations of query  X  X otel price compar-ison X  ranked by entropy score, with the original query in bold face.
Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants. For the query  X  X otel price com-parisons X , we decided that word  X  X omparisons X  is expanded to include  X  X omparison X . However, not every occurrence of  X  X omparison X  in the document is of interest. A page which is about comparing customer service can contain all of the words hotel price comparisons comparison . This page is not a good page for the query.

If we accept matches of every occurrence of  X  X omparison X , it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval. To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document. A variant match is considered valid only if the variant occurs in the same context as the original word does. The context is the left or the right non-stop segments 1 of the original word. Taking the same query as an example, the context of  X  X omparisons X  is  X  X rice X . The expanded word  X  X omparison X  is only valid if it is in the same context of  X  X omparisons X , which is after the word  X  X rice X . Thus, we should only match those occurrences of  X  X omparison X  in the document if they occur after the word  X  X rice X . Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size. If the expanded word  X  X omparison X  occurs within the context of  X  X rice X  within a window, it is considered valid. The smaller the window size is, the more restrictive the matching. We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases.
We will measure both relevance improvement and the stemming cost required to achieve the relevance. a context segment can not be a single stop word. We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11]. Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG ( K ) score for this query is calculated as follows: where g k is the weight for the document at rank k . Higher degree of relevance corresponds to a higher weight. A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights. We use dcg to rep-resent the average DCG (5) over a set of test queries.
Another metric is to measure the additional cost incurred by stemming. Given the same level of relevance improve-ment, we prefer a stemming method that has less additional cost. We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed.
We randomly sample 870 queries from a three month query log, with 290 from each month. Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming. We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words. In the end, we have 529 correctly spelled queries with at least 2 words.
Before explaining the experiments and results in detail, we X  X  like to describe the traditional way of using stemming for Web search, referred as the naive model . This is to treat every word variant equivalent for all possible words in the query. The query  X  X ook store X  will be transformed into  X  (book OR books)(store OR stores)  X  when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments.
The baseline model is the model without stemming. We first run the naive model to see how well it performs over the baseline. Then we improve the naive stemming model by document sensitive matching, referred as document sensi-tive matching model . This model makes the same stemming as the naive model on the query side, but performs conser-vative matching on the document side using the strategy described in section 3.5. The naive model and document sensitive matching model stem the most queries. Out of the 529 queries, there are 408 queries that they stem, corre-sponding to 46.7% query traffic (out of a total of 870). We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as se-lective stemming model . Based on language modeling pre-diction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model . We experiment with unigram language model and bigram lan-guage model. Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments.

To get a sense of how these models perform, we also have an oracle model that gives the upper-bound performance a stemmer can achieve on this data. The oracle model only expands a word if the stemming will give better results.
To analyze the pluralization handling influence on differ-ent query categories, we divide queries into short queries and long queries. Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words.
We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5. Each row in Table 4 is a stemming strategy de-scribed in section 4.4. The first column is the name of the strategy. The second column is the number of queries af-fected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg . The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy). The fourth column is the relative improvement over the baseline, and the last column is the p -value of Wilcoxon significance test.

There are several observations about the results. We can see the naively stemming only obtains a statistically insignif-icant improvement of 1.5% . Looking at Table 5, it gives an improvement of 2.7% on short queries. However, it also hurts long queries by -2.4% . Overall, the improvement is canceled out. The reason that it improves short queries is that most short queries only have one word that can be stemmed. Thus, blindly pluralizing short queries is rela-tively safe. However for long queries, most queries can have multiple words that can be pluralized. Expanding all of them without selection will significantly hurt precision.
Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from -2.4% to -1.6% for long queries, with an overall lift from 1.5% to 2.8% . The improvement comes from the conservative context sensitive document matching. An ex-panded word is valid only if it occurs within the context of original query in the document. This reduces many spurious matches. However, we still notice that for long queries, con-text sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem. While the chosen window size of 4 works the best amongst all the choices, it still al-lows spurious matches. It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases.
Selective word pluralization further helps resolving the problem faced by document context sensitive stemming. It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stem-ming in the first place. By predicting which word variants are going to be useful, we can dramatically reduce the num-ber of stemmed words, thus improving both the recall and the precision. With the unigram language model, we can re-duce the stemming cost by 26.7% (from 408/408 to 300/408) and lift the overall dcg improvement from 2.8% to 3.4% . In particular, it gives significant improvements on long queries. The dcg gain is turned from negative to positive, from  X  1 . 6% to 1 . 1%. This confirms our hypothesis that reducing unnec-essary word expansion leads to precision improvement. For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model.
The advantages of predictive word expansion with a lan-guage model is further boosted with a better bigram lan-guage model. The overall dcg gain is lifted from 3.4% to 3.9% , and stemming cost is dramatically reduced from 408/408 to 250/408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improve-ment overall all query traffic. For short queries, bigram lan-guage model improves the dcg gain from 4.4% to 4.7% , and reduces stemming cost from 272/272 to 150/272. For long queries, bigram language model improves dcg gain from 1.1% to 2.5% , and reduces stemming cost from 136/136 to 100/136. We observe that the bigram language model gives a larger lift for long queries. This is because the uncertainty in long queries is larger and a more powerful language model is needed. We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation.

Considering the tight upper-bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very sat-isfying. For short queries, the dcg gain upper-bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model. For long queries, the dcg gain upper-bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model. We may gain additional benefit with a more powerful language model for long queries. However, the difficulties of long queries come from many other aspects including the prox-imity and the segmentation problem. These problems have to be addressed separately. Looking at the the upper-bound of overhead reduction for oracle stemming, 75% (308/408) of the naive stemmings are wasteful. We currently capture about half of them. Further reduction of the overhead re-quires sacrificing the dcg gain.

Now we can compare the stemming strategies from a dif-ferent aspect. Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only. We can see that the number of affected queries decreases as the stem-ming strategy becomes more accurate ( dcg improvement). For the bigram language model, over the 250/408 stemmed queries, the dcg improvement is 6.1% . An interesting ob-servation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more dif-ficult queries (low dcg queries).
As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web. The lan-guage model should describe the occurrence of the string on the Web. However, the query log is also a good resource.
Note that this upperbound is for pluralization handling only, not for general stemming. General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics.
 Users reformulate a query using many different variants to get good results.

To test the hypothesis that we can learn reliable transfor-mation probabilities from the query log, we trained a lan-guage model from the same query top 25M queries as used to learn segmentation, and use that for prediction. We ob-served a slight performance decrease compared to the model trained on Web frequencies. In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries. Thus, the query log can serve as a good approximation of the Web frequencies.
Some linguistic knowledge is useful in stemming. For the pluralization handling case, pluralization and de-pluralization is not symmetric. A plural word used in a query indicates a special intent. For example, the query  X  X ew york hotels X  is looking for a list of hotels in new york, not the specific  X  X ew york hotel X  which might be a hotel located in Califor-nia. A simple equivalence of  X  X otel X  to  X  X otels X  might boost a particular page about  X  X ew york hotel X  to top rank. To capture this intent, we have to make sure the document is a general page about hotels in new york. We do this by requir-ing that the plural word  X  X otels X  appears in the document. On the other hand, converting a singular word to plural is safer since a general purpose page normally contains spe-cific information. We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property.
One type of mistakes we noticed, though rare but seri-ously hurting relevance, is the search intent change after stemming. Generally speaking, pluralization or depluraliza-tion keeps the original intent. However, the intent could change in a few cases. For one example of such a query,  X  X ob at apple X , we pluralize  X  X ob X  to  X  X obs X . This stem-ming makes the original query ambiguous. The query  X  X ob OR jobs at apple X  has two intents. One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co-founder of the company. Thus, the results after query stemming returns  X  X teve Jobs X  as one of the results in top 5. One solution is performing results set based analysis to check if the intent is changed. This is similar to relevance feedback and requires second phase ranking.

A second type of mistakes is the entity/concept recog-nition problem, These include two kinds. One is that the stemmed word variant now matches part of an entity or concept. For example, query  X  X ookies in san francisco X  is pluralized to  X  X ookies OR cookie in san francisco X . The results will match  X  X ookie jar in san francisco X . Although  X  X ookie X  still means the same thing as  X  X ookies X ,  X  X ookie jar X  is a different concept. Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words. For example,  X  X uote ICE X  is plural-ized to  X  X uote OR quotes ICE X . The original intent for this query is searching for stock quote for ticker ICE. However, we noticed that among the top results, one of the results is  X  X ood quotes: Ice cream X . This is matched because of affected queries before/after applying stemming the pluralized word  X  X uotes X . The unchanged word  X  X CE X  matches part of the noun phrase  X  X ce cream X  here. To solve this kind of problem, we have to analyze the documents and recognize  X  X ookie jar X  and  X  X ce cream X  as concepts instead of two independent words.

A third type of mistakes occurs in long queries. For the query  X  X ar code reader software X , two words are pluralized.  X  X ode X  to  X  X odes X  and  X  X eader X  to  X  X eaders X . In fact,  X  X ar code reader X  in the original query is a strong concept and the internal words should not be changed. This is the seg-mentation and entity and noun phrase detection problem in queries, which we actively are attacking. For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept.
We have presented a simple yet elegant way of stemming for Web search. It improves naive stemming in two aspects: selective word expansion on the query side and conserva-tive word occurrence matching on the document side. Using pluralization handling as an example, experiments on a ma-jor Web search engine data show it significantly improves the Web relevance and reduces the stemming cost. It also significantly improves Web click through rate (details not reported in the paper).

For the future work, we are investigating the problems we identified in the error analysis section. These include: entity and noun phrase matching mistakes, and improved segmentation. [1] E. Agichtein, E. Brill, and S. T. Dumais. Improving [2] E. Airio. Word Normalization and Decompounding in [3] P. Anick. Using Terminological Feedback for Web [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern [5] S. Chen and J. Goodman. An Empirical Study of [6] S. Cronen-Townsend, Y. Zhou, and B. Croft. A [7] H. Fang and C. Zhai. Semantic Term Matching in [8] W. B. Frakes. Term Conflation for Information [9] D. Harman. How Effective is Suffixing? JASIS , [10] D. Hull. Stemming Algorithms -A Case Study for [11] K. Jarvelin and J. Kekalainen. Cumulated Gain-Based [12] R. Jones, B. Rey, O. Madani, and W. Greiner. [13] W. Kraaij and R. Pohlmann. Viewing Stemming as [14] R. Krovetz. Viewing Morphology as an Inference [15] D. Lin. Automatic Retrieval and Clustering of Similar [16] J. B. Lovins. Development of a Stemming Algorithm. [17] M. Lennon and D. Peirce and B. Tarry and P. Willett. [18] M. Porter. An Algorithm for Suffix Stripping. [19] K. M. Risvik, T. Mikolajewski, and P. Boros. Query [20] S. E. Robertson. On Term Selection for Query [21] G. Salton and C. Buckley. Improving Retrieval [22] R. Sun, C.-H. Ong, and T.-S. Chua. Mining [23] C. Van Rijsbergen. Information Retrieval .
 [24] B. V  X elez, R. Weiss, M. A. Sheldon, and D. K. Gifford. [25] J. Xu and B. Croft. Query Expansion using Local and [26] J. Xu and B. Croft. Corpus-based Stemming using
