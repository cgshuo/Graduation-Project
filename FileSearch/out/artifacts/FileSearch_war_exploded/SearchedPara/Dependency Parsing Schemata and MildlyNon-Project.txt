 Universidade da Coru  X  na, Spain University of Sussex, UK University of Sussex, UK
We introduce dependency parsing schemata, a formal framework based on Sikkel X  X  parsing schemata for constituency parsers, which can be used to describe, analyze, and compare depen-dency parsing algorithms. We use this framework to describe several well-known projective and between them. We then use the framework to define new polynomial-time parsing algorithms their gap degree bounded by a constant k in time O(n 5 + 2k gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k )intime O(n 4 + 3k schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction
Dependency parsing involves finding the structure of a sentence as expressed by a set of directed links (called dependencies ) between individual words. Dependency for-malisms have attracted considerable interest in recent years, having been successfully and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head X  X odifier and head X  X omplement relationships, which form the basis of predicate X  X rgument structure, but are not rep-postulate the existence of non-lexical nodes; and some variants of dependency parsers are able to represent non-projective structures (McDonald et al. 2005), which is impor-tant when parsing free word order languages where discontinuous constituents are common.
 the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efficient implementations automatically (G  X  omez-Rodr  X   X guez, Vilares, and Alonso 2009). The formalism was origi-nally defined for context-free grammars (CFG) and since then has been applied to other constituency-based formalisms, such as tree-adjoining grammars (Alonso et al. 1999).
This article considers the application of parsing schemata to the task of dependency parsing. The contributions of this article are as follows.

Although some of these contributions have been published previously, this article presents them in a thorough and consistent way. The definition of dependency parsing schemata was first published by G  X  omez-Rodr  X   X guez, Carroll, and Weir (2008), along with some of the projective schemata presented here and their associated proofs. The results concerning mildly non-projective parsing in Section 7 were first published by G  X  omez-application of the formalism to Link Grammar, are entirely new contributions of this article.
 process which generates intermediate results called items . In particular, items in parsing trees that do not violate the constraints imposed by a grammar. A parsing schema can be used to obtain a working implementation of a parser by using deductive engines such as the ones described by Shieber et al. (1995) and G  X  omez-Rodr  X   X guez, Vilares, and
Alonso (2009), or the Dyna language (Eisner, Goldlust, and Smith 2005). 2. Dependency Parsing Schemata
Although parsing schemata were originally defined for CFG parsers, they have since been adapted to other constituency-based grammar formalisms. This involves finding 542 deduction steps that captures the formalism X  X  composition rules (Alonso et al. 1999).
Although it is less clear how to adapt parsing schemata to dependency parsing, a num-ber of dependency parsers have the key property of being constructive: They proceed by combining smaller structures to form larger ones, terminating when a complete parse for the input sentence is found. We show that this makes it possible to define a variant of the traditional parsing schemata framework, where the encodings of intermediate dependency structures are defined as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues.
 the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammar-driven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of depen-dency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilis-tic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form ( a , i ) position i as a dependent. Deduction steps in data-driven parsers can be associated with the D-rules corresponding to the links they create, so that parsing schemata for such parsers are defined using grammars of D-rules. In this way, we obtain a representation of some of the declarative aspects of these parsing strategies that is independent of the this representation is useful for designing control structures or probabilistic models for to make probabilistic decisions, as well as the information available at each of those choice points. Additionally, D-rules allow us to use an uniform description that is valid for both data-driven and grammar-driven parsers, because D-rules can function like grammatical rules.
 items for dependency parsers can be defined using partial dependency trees. How-those of Lombardo and Lesmo (1996) and Kahane, Nasr, and Rambow (1998). The formalism can be made general enough to include these parsers by using a novel way of representing intermediate states of dependency parsers based on a form of dependency trees that include nodes labelled with preterminals and terminals (G  X  omez-Rodr  X   X guez, only use this representation (called extended dependency trees ) in the grammar-based rithms with simple dependency trees. Some existing dependency parsing algorithms, for example, the algorithm of Eisner (1996), involve steps that connect spans which can represent disconnected dependency graphs. Such spans cannot be represented by a single dependency tree. Therefore, our formalism allows items to be sets of forests of partial dependency trees, rather than sets of trees.
 parsers.
 Definition 1
An interval (with endpoints i and j ) is a set of natural numbers of the form [ i .. j ] = i dependency graph for a string w = w 1 ... w n is a graph G = ( V , E ), where V and E  X  V  X  V .
 encoding the fact that the word w i is a syntactic dependent (or child )of w that w j is the parent , governor ,or head of w i . We write i a (possibly empty) path from i to j .The projection of a node i , denoted i ,isthesetof reflexive-transitive dependents of i ,thatis, i = { j  X  V refer to different dependency graphs, we use the notation i of a node i in the graph G .
 Definition 2
A dependency graph T for a string w 1 ... w n is called a dependency tree for that string if it contains no cycles and all of its nodes have exactly one parent, except for one node that has none and is called the root or head of the tree T , denoted head ( T ). The yield of a dependency tree T , denoted yield ( T ), is the ordered list of its nodes. We will use the term dependency forest to refer to a set of dependency trees for the same string, the generic term dependency structure to refer to a dependency tree or forest. Definition 3
We say that a dependency graph G = ( V , E )forastring w 1 interval for every i  X  V .
 Definition 4
Let  X  ( G ) be the set of dependency trees which are syntactically well-formed according to a given grammar G (which may be a grammar of D-rules or of CFG-like rules, as explained previously). We define an item set for dependency parsing as a set called an item , is a set of dependency forests for strings. For example, each member of the item [1, 5] in the item set of the parser by Yamada and Matsumoto (2003) that will be explained in Section 3.4 is a dependency forest with two projective trees, one with head 1 and the other with head 5, and such that the concatenation of their yields is 1 .. 5.
Figure 1 shows the three dependency forests that constitute the contents of this item under a specific grammar of D-rules.
 hand notation for such sets, as seen in the previous example. An alternative approach, 544 that denote sets of syntactic structures. Although the latter approach provides more flexibility, this makes defining the relationships between parsers less straightforward.
In any case, because tuple notation is used to write schemata under both approaches, the schemata we provide are compatible with both interpretations.
 analogous to those in Sikkel X  X  theory of constituency parsing (Sikkel 1997), and are not presented in full detail. A dependency parsing system is a deduction system ( where I is a dependency item set as defined here, H is a set containing initial items or hypotheses (not necessarily contained in I ), and D deduction steps defining an inference relation .
 item containing such a tree will be called a coherent final item for w for parsers that are constrained to a more restrictive class projective parsers, coherent final items will be those containing parse trees for w that are in T . For example, because we expect correct projective parsers to produce only projective structures, coherent final items for projective parsers will be those containing items , such that its intersection with final items produces the set of coherent final items.
The definition of coherent items depends on each particular proof. derivable final items it produces for any arbitrary string are coherent for that string. A parsingschemaissaidtobe complete if all coherent final items are derivable. A correct parsing schema is one which is both sound and complete.
 intermediate results that are obtained by the algorithm (items) and a set of operations that can be used to obtain new such results from existing ones (deduction steps); but it makes no claim about the order in which to execute the operations or the data structures to use for storing the results. 3. Projective Schemata
In this section, we show how dependency parsing schemata can be used to describe several existing projective dependency parsers. 3.1 Collins (1996)
One of the most straightforward projective dependency parsing strategies was intro-duced by Collins (1996), and is based on the CYK bottom X  X p parsing strategy (Kasami 1965; Younger 1967). Collins X  X  parser works with dependency trees which are linked to each other by creating links between their heads. The schema for this parser maps every set of D-rules G and input string w 1 ... w n to an instantiated dependency parsing system ( I
Item set: The item set is defined as I Col96 = { [ i , j , h ] is defined as the set of forests containing a single projective dependency tree T of  X  ( G ) on, we will implicitly assume that the dependency trees appearing in items of a parsing schema for a grammar G are taken from the set  X  ( G ) of syntactically well-formed trees according to G .
 dependency tree headed at h that spans the substring w i ... w Hypotheses: For an input string w 1 ... w n , the set of hypotheses is n + 1 } , where each item contains a forest with a single dependency tree having only one node i . This same set of hypotheses is used for all the parsers, so is not repeated for subsequent schemata. 4 Note that the nodes 0 and n + 1 used in the definition do not correspond to actual input words X  X hese are dummy nodes that we call beginning-of-sentence and end-of-sentence markers, respectively, and will be needed by several of the parsers described subsequently.

Final items: The set of final items is { [1, n , h ] | 1  X  parse trees for the input sentence whose head is some node h , expressing that the word w is the sentence X  X  syntactic head. 546 Deduction steps: The set of deduction steps, D Col 96 , is the union of the following: data-driven, D-rules are used as side conditions for the parser X  X  deduction steps. Side conditions restrict the inference relation by specifying which combinations of values are permissible for the variables appearing in the antecedents and consequent of deduction steps.
 only if the deduction system infers a coherent final item. When executing this schema with a deductive engine, the parse forest can be recovered by following back pointers, as in constituency parsers (Billot and Lang 1989).
 way linking decisions are taken. Statistical models can be used to determine whether a step linking words a and b in positions i and j  X  X .e., having ( a , i ) condition X  X s executed or not, and probabilities can be attached to items in order to as-sign different weights to different analyses of the sentence. The side conditions provide an explicit representation of the choice points where probabilistic decisions are made by the control mechanism that is executing the schema. The same principle applies to all
D-rule-based parsers described in this article. 3.2 Eisner (1996)
Based on the number of free variables used in deduction steps of Collins X  X  parser, it is apparent that its time complexity is O ( n 5 ): There are O ( n with which each of its L INK steps can be executed. 5 This complexity arises because a parentless word (head) may appear in any position in the items generated by the parser; the complexity can be reduced to O ( n 3 ) by ensuring that parentless words only appear Eisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005; Corston-Oliver et al. 2006). The parsing schema for this algorithm is defined as follows. Item set: The item set is where item [ i , j , True , False ] corresponds to [ i , j , j ] of the form { T 1 , T 2 } such that T 1 , T 2 are projective, head ( T is some k ( i  X  k &lt; j )suchthat yield ( T 1 ) = i .. k and yield ( T parent in the item. Items with one of the flags set to True represent dependency trees where the node i or j is the head, whereas items with both flags set to False represent pairs of trees headed at nodes i and j which jointly dominate the substring w Items of this kind correspond to disconnected dependency graphs.

Deduction steps: The set of deduction steps is as follows: where the R-L INK and L-L INK steps establish a dependency link between the heads of an item containing two trees (i.e., having both flags set to False ), producing a new item containing a single tree. The C OMBINE S PA N S step is used to join two items that overlap at a single word, which must have a parent in only one of the items, so that the result of joining trees coming from both items (without creating any dependency link) is a well-formed dependency tree.

Final items: The set of final items is { [0, n , False , True ] pendency trees rooted at the beginning-of-sentence marker 0, which acts as a  X  X ummy head X  for the sentence. In order for the algorithm to parse sentences correctly, we need to define D-rules to allow the real sentence head to be linked to the node 0. 3.3 Eisner and Satta (1999) which can be used for dependency parsing. This algorithm is conceptually simpler than
Eisner X  X  (1996) algorithm, because it only uses items representing single dependency trees, avoiding items of the form [ i , j , False , False ].

Item set: The item set is I ES99 = { [ i , j , i ] | 0  X  i items are defined as in Collins X  X  parsing schema.
 Deduction steps: The deduction steps for this parser are the following: 548 where L INK steps create a dependency link between two dependency trees spanning adjacent segments of the input, and C OMBINER steps join two overlapping trees by a graph union operation that does not create new links. C OMBINER mechanism as those in the algorithm of Eisner (1996), and L to those of Collins (1996), so this schema can be seen as being intermediate between those two algorithms. These relationships will be formally described in Section 4.
Final items: The set of final items is { [0, n ,0] } beginning-of-sentence marker 0 as their head, as in the previous algorithm. rithm appears to be more complex to understand and implement than the previous one, requiring four different kinds of items to keep track of the state of the automata used by the grammars. However, this abstract representation of its underlying seman-tics reveals that this parsing strategy is, in fact, conceptually simpler for dependency parsing. 3.4 Yamada and Matsumoto (2003)
Yamada and Matsumoto (2003) define a deterministic, shift-reduce dependency parser guided by support vector machines, which achieves over 90% dependency accuracy on
Section 23 of the Wall Street Journal Penn Treebank. Parsing schemata cannot specify which they are applied. However, deterministic parsers can be viewed as optimizations of underlying nondeterministic algorithms, and we can represent the actions of the underlying parser as deduction steps, abstracting away from the deterministic im-plementation details, obtaining a potentially interesting nondeterministic dependency parser.
 which act as heads of neighboring dependency trees. One of the actions creates a link located directly to the left of the target nodes becomes the new left target node. The other action is symmetric, performing the same operation with a right-to-left link. An O ( n 3 ) nondeterministic parser generalizing this behavior can be defined as follows.
Item set: The item set is I YM03 = { [ i , j ] | 0  X  i  X  j sponds to the item [ i , j , False , False ]in I Eis96 .
 Deduction steps: The deduction steps are as follows: overlapping at a head node, and creates a dependency link from their common head to one of the peripheral heads. Note that this is analogous to performing an Eisner L step immediately followed by an Eisner C OMBINE step, as will be further analyzed in Section 4.

Final items: The set of final items is { [0, n + 1] } grammar must not have D-rules of the form ( w i , i )  X  ( w allow the end-of-sentence marker to govern any words. If the grammar satisfies this a parse tree rooted at the beginning-of-sentence marker and with yield 0 .. n . other parsers described here. 3.5 Lombardo and Lesmo (1996) and Other Earley-Based Parsers
The algorithms presented so far are based on making individual decisions about de-pendency links, represented by D-rules. Other parsers, such as that of Lombardo and
Lesmo (1996), use grammars with CFG-like rules which encode the preferred order of dependents for each given governor. For example, a rule of the form N ( Det to allow N to have Det as left dependent and PP as right dependent. The algorithm by Lombardo and Lesmo is a version of Earley X  X  CFG parser (Earley 1970) that uses Gaifman X  X  dependency grammar (Gaifman 1965).
 contain extended dependency trees , trees that have two kinds of nodes: preterminal nodes and terminal nodes. Depending on whether all the preterminal nodes have been linked to terminals, extended dependency trees can either be grounded , in which case they are isomorphic to traditional dependency graphs (see Figure 3), or ungrounded ,as in Figure 4, in which case they capture parser states in which some structure has been predicted, but not yet found. Note that a dependency graph can always be extracted from such a tree, but in the ungrounded case different extended trees can be associated dependency trees in more detail.

Item set: The item set is I LomLes = { [ A (  X   X   X  ), i , j ]  X  and  X  are strings; P is a set of CFG-like rules; 7 and each item [ A (  X  the set of projective extended dependency trees rooted at A , where the direct children of
A are  X  X  , and the subtrees rooted at  X  have yield i .. j . Note that Lombardo and Lesmo X  X  550 extended dependency trees, as in Earley X  X  CFG parser but unlike items in D-rule-based parsers, which are finite sets.
 Deduction steps: The deduction steps for this parsing schema are as follows: Final items: The final item set is { [( S  X  ), 1, n ] } .
 parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the S CANNER always moves the dot over the head symbol over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997):
The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and parser. 3.6 Nivre (2003)
Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008).
 different kinds of transitions between configurations. The transition system defined by all the possible configurations and transitions is nondeterministic, and machine learning techniques are used to train a mechanism that produces a deterministic parser. between configurations (we use the symbol  X  for a stack and the notation  X  :: h for the stack resulting from pushing h into  X  ,and  X  i to represent a buffer of the form w works with are not items. Although the antecedents and consequents in this deduction system are parser configurations, they do not correspond to disjoint sets of dependency structures (several configurations may correspond to the same dependency structures), relations between parsers, because they rely on the properties of item sets. rules in the system that are implementing control structures, however, and expressing only declarative aspects of the parser X  X  tree building logic. To do this, we first obtain by storing an index f rather than the full buffer  X  grouping configurations that share common features, making them equivalent for the side conditions of the system: Instead of storing the full set of dependency links that the algorithm has constructed up to a given point (denoted by V ), we only keep track of whether elements in the stack have been assigned a head or not; and we represent this flag which is True if the corresponding node has been assigned a head or False if it has not: taining to the way in which the parser joins dependency structures and builds links between them. In particular, the Reduce step is just a mechanism to select which of a set configurations corresponding to the same dependency structure may have different involve the same dependency structures. To define an item set for this parser, we must establish which words could be on the stack at each configuration.
 node situated to its right, and is not covered by any dependency link ( j is covered by 552 linkable node and any node to the right of T without violating the projectivity property.
When the parser is reading a particular word at position f , the following properties hold for all nodes to the left of f (nodes 0 ... f  X  1):
A dependency parsing schema represents items with lists (instead of stacks) containing all the nodes found so far which are right-linkable, and a flag associated with each node indicating whether it has been assigned a head or not. Instead of using Reduce steps to decide which node to choose as a head of the one corresponding to the currently-read word, we allow any node in the list that does not have a headless node to its right to be the head; this is equivalent to performing several Reduce transitions followed by an L-link transition.
 Item set: The item set is I {
T 1 , ... , T w
Final items: The set of final items is { [ n + 1, (0, False ), ( v n } , the set of items containing a forest with a single projective dependency tree T headed at the dummy node 0, whose yield spans the whole input string, and which contains any set of right-linkable words.
 Deduction steps: The deduction steps are as follows: deductive engine would have exponential complexity. The linear complexity in Nivre X  X  algorithm is achieved by using a control strategy that deterministically selects a single transition at each state. 3.7 Covington X  X  (2001) Projective Parser
Covington (2001) defines a non-projective dependency parser, and a projective vari-ant called Algorithm LSUP (for List-based Search with Uniqueness and Projectivity). not parse all projective dependency structures, because when creating leftward links it assumes that the head of a node i must be a reflexive-transitive head of the node i which is not always the case. For instance, the structure shown in Figure 5 cannot be parsed because the constraints imposed by the algorithm prevent it from finding the head of 4.
 variant of Covington X  X  LSUP parser where these constraints have been relaxed. This implementation has the same tree building logic as the parser described by Nivre (2003), different realization of the schema shown in Section 3.6. 4. Relations Between Dependency Parsers
The parsing schemata framework can be exploited to establish how different algorithms are related, improving our understanding of the features of these parsers, and poten-tially exposing new algorithms that combine characteristics of existing parsers in novel ways. Sikkel (1994) defines various relations between schemata that fall into two cate-gories: generalization relations, which are used to obtain more fine-grained versions of parsers, and filtering relations, which can be seen as the converse of generalization and are used to reduce the number of items and/or steps needed for parsing. Informally, a parsing schema can be generalized from another via the following transformations: 554 A schema can be obtained from another by filtering in the following ways:
Many of the parsing schemata described in Section 3 can be related (see Figure 6), but for space reasons we sketch proofs for only the more interesting cases.
 Theorem 1 Yamada and Matsumoto (2003) sr  X   X  Eisner (1996).
 Proof 1 It is easy to see from the schema definitions that every deduction step in the Yamada and Matsumoto (2003) schema can be emulated by a sequence of inferences in the Eisner (1996) schema. For the I as the I NITTER s of both parsers are equivalent. Expressing the R-L and Matsumoto X  X  parser in the notation used for Eisner items gives: This can be emulated in Eisner X  X  parser by an R-L C lated by an L-L INK followed by a C OMBINE S PA N S in Eisner X  X .
 Theorem 2 Eisner and Satta (1999) sr  X   X  Eisner (1996).
 Proof 2 Writing R-L INK in Eisner and Satta X  X  parser in the notation used for Eisner items gives The proof corresponding to the L-L INK step is symmetric. As for the R-C
L-C OMBINER steps in Eisner and Satta X  X  parser, it is easy to see that they are particular cases of the C OMBINE S PA N S step in Eisner X  X , and therefore can be emulated by a single application of C OMBINE S PA N S .
 (1999) and Yamada and Matsumoto (2003) are more efficient, at the schema level, than that of Eisner (1996), in that they generate fewer items and need fewer steps to perform by Eisner X  X  parser. The optimization in Yamada and Matsumoto X  X  parser comes from followed by combining operations; whereas Eisner and Satta X  X  parser does the opposite, forcing combining operations to be followed by linking operations.
 each item can be in any position, we obtain an O ( n 5 ) parser which can be filtered into the parser of Collins (1996) by eliminating the C OMBINER we obtain an O ( n 5 ) head-corner parser based on CFG-like rules by an item refinement in which each Collins item [ i , j , h ] is split into a set of items [ A (  X  refinement relation between these parsers only holds if for every D-rule B a corresponding CFG-like rule A  X  ... B ... in the grammar used by the head-corner ing decisions makes the h indices redundant. This simplification is an item contraction which results in an O ( n 3 ) head-corner parser. From here, we can follow the procedure to other algorithms for CFGs. In this way, we can refine the head-corner parser to a variant of the algorithm by de Vreught and Honig (1989) (Sikkel 1997), and by successive filters we reach a left-corner parser which is equivalent to the one described by Barbero et al. (1998), and a step contraction of the Earley-based dependency parser by Lombardo 556 and Lesmo (1996). The proofs for these relations are the same as those given by Sikkel (1994), except that the dependency variants of each algorithm are simpler (due to the for schemata dVH1 , dVH2 , dVH3 ,and buLC shown in Figure 6 come from Sikkel (1994, 1997). These dependency parsing schemata are versions of the homonymous schemata whose complete description can be found in Sikkel (1997), adapted for dependency parsing. G  X  omez-Rodr  X   X guez (2009) gives a more thorough explanation of these relations and schemata. 5. Proving Correctness establish the correctness of one schema from that of related ones. In this section, we show that the schemata for Yamada and Matsumoto (2003) and Eisner and Satta (1999) are correct, and use this to prove the correctness of the schema for Eisner (1996). Theorem 3 The Eisner and Satta (1999) parsing schema is correct.
 Proof 3
To prove correctness, we must show both soundness and completeness. To verify soundness we need to check that every individual deduction step in the parser infers a coherent consequent item when applied to coherent antecedents (i.e., in this case, that steps always generate non-empty items that conform to the definition in Section 3.3).
This is shown by checking that, given two antecedents of a deduction step that contain a tree licensed by a set of D-rules G , the consequent of the step also contains such a tree.
The tree for the consequent is built from the trees corresponding to the antecedents: by a graph union operation, in the case of C OMBINER steps; or by linking the heads of both trees with a dependency relation licensed by G , in the case of L proving the stronger result that all coherent items are derivable. We show this by strong induction on the length of items, where the length of an item  X  = [ i , k , h ] is defined as length (  X  ) = k  X  i + 1. Coherent items of length 1 are the hypotheses of the schema (of the are derivable for all 1  X  m &lt; l , then items of length l are also derivable. struction, the root of T is labelled i .Let j be the rightmost daughter of i in T . Because T is projective, we know that the yield of j must be of the form l .. k , where i &lt; l k is the rightmost transitive dependent of j in T .

T , 8 T 2 be the tree obtained by removing all the nodes to the right of j from T be the tree obtained by removing all the nodes to the left of j from T
T derivable, as it can be obtained from these derivable items by the following inferences: under the induction hypothesis. The same can be shown for items of the form [ i , k , k ]by symmetric reasoning.
 Theorem 4 The Yamada and Matsumoto (2003) parsing schema is correct.
 Proof 4
Soundness is verified by building forests for the consequents of steps from those cor-responding to the antecedents. To prove completeness we use strong induction on the length of items, where the length of an item [ i , j ] is defined as j here because items of length 2 are generated by the Initter step) and showing that it can be inferred from derivable antecedents of length less than l , so it is derivable. If l &gt; 2, either i has at least one right dependent or k has at least one left dependent in the item. Suppose i has a right dependent; if T 1 and T 2 k in a forest in [ i , k ], we call j the rightmost daughter of i and consider the following trees:
The forest { U 1 , U 2 } belongs to the coherent item [ i , j ], and herent item [ j , k ]. From these two items, we can obtain [ i , k ]byusingtheL-L
Symmetric reasoning can be applied if i has no right dependents but k has at least one left dependent, analogously to the case of the previous parser.
 Theorem 5 The Eisner (1996) parsing schema is correct.
 Proof 5
By using the previous proofs and the relationships between schemata established ear-lier, we show that the parser of Eisner (1996) is correct: Soundness is straightforward, and completeness can be shown by using the properties of other algorithms. Because the set of final items in the Eisner (1996) and Eisner and Satta (1999) schemata are the same, and the former is a step refinement of the latter, the completeness of Eisner and
Satta X  X  parser directly implies the completeness of Eisner X  X  parser. Alternatively, we can use Yamada and Matsumoto X  X  parser to prove the correctness of Eisner X  X  parser if we 558 which are equally valid as final items since they always contain parse trees. This idea can be applied to transfer proofs of completeness across any refinement relation. 6. Non-Projective Schemata
The parsing schemata presented so far define parsers that are restricted to projective pendents of each node forms a contiguous substring of the input. We now show that the dependency parsing schema formalism can also describe various non-projective parsers. 6.1 Pseudo-Projectivity
Pseudo-projective parsers generate non-projective analyses in polynomial time by using a projective parsing strategy and postprocessing the results to establish non-projective links. This projective parsing strategy can be represented by dependency parsing schemata such as those seen in Section 3. For example, the algorithm of Kahane, Nasr, and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the I NITTER and P REDICTOR
The initialization step specified by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algo-rithm. The problem can be fixed either by using the step shown here instead (bottom X  X p
Earley strategy) or by adding an additional step turning it into a bottom X  X p left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and
Matsumoto (2003), adding additional shift and reduce actions to handle non-projective dependency structures. These extra actions allow the parser to link to nodes that are several positions deep in the stack, creating non-projective links. In particular, Attardi of actions that generalizes the previous ones to n positions deep for any n .Thus,the maximum depth in the stack to which links can be created can be configured according to the actions allowed. We use Att d for the variant of the algorithm that allows links only up to depth d ,and Att  X  for the original, unrestricted algorithm with unlimited depth actions. A nondeterministic version of the algorithm Att follows.

Item set: The item set is I Att = { [ h 1 , h 2 , ... , h h , ... , h m ] is the set of dependency forests of the form head ( T i ) = h i for each i  X  [1 .. m ]; and the projections of the nodes h wise disjoint, and their union is [ h 1 .. h m ]. Deduction steps: The set of deduction steps for Att d is the following:
Deduction steps for Att  X  are obtained by removing the constraint set (this restriction corresponds to the maximum stack depth to which dependency links can be created).

Yamada and Matsumoto X  X  parser, they differ in that an Attardi item of the form [0, n + 1] may contain forests with non-projective dependency trees.
 plementation of Att d has exponential complexity with respect to input length (though in the implementation of Attardi [2006], control structures determinize the algorithm).
Soundness of the algorithm Att  X  is shown as in the previous algorithms, and complete-ness can be shown by reasoning that every coherent final item [0, n + 1] can be obtained by first performing n + 1I NITTER stepstoobtainitems[ i , i + 1] for each 0 performing the L INK steps corresponding to the links in a tree contained in [0, n + 1] to obtain this final item. The algorithm Att d where d is finite is not correct with respect to the set of non-projective dependency structures, because it only parses a restricted subset of them (Attardi 2006). Note that the algorithm Att for every natural number d , since the set of deduction steps of Att 6.3 The MH k Parser
We now define a novel variant of Attardi X  X  parser with polynomial complexity by lim-iting the number of trees in each forest contained in an item (rather than limiting stack depth), producing a parsing schema MH k (standing for multi-headed with at most k heads per item). Its item set is I MH m  X  k } where [ h following:
As with the Att d parser, MH k parses a restricted subset of non-projective dependency structures, such that the set of structures parsed by MH k parsed by MH k + 1 .The MH  X  parser, obtained by assuming that the number of trees per forest is unbounded, is equivalent to Att  X  , and therefore correct with respect to 560 the set of non-projective dependency structures. For finite values of k , MH static filter of Att d , because its sets of items and deduction steps are subsets of those of Att d . Therefore, the set of structures parsed by MH d + 2 by Att d .
 of the parser by Yamada and Matsumoto (2003) that parses projective structures only, but by modifying the bound k we can define polynomial-time algorithms that parse larger sets of non-projective dependency structures. The MH of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005)
McDonald et al. (2005) describe a parser which finds a nonprojective analysis for a sentence in O ( n 2 ) time under a strong independence assumption called an edge-factored model : Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this max-imum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu pendencies between pairs of input words, and applies an MST algorithm to find a dependency tree covering all the words in the sentence and maximizing the sum of weights.
 into large structures until it finds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spanning tree, potentially creating cycles and forming an illegal dependency tree. A cycle elimination procedure is iteratively applied to this graph until a legal dependency tree is obtained. It is still possible to express declarative aspects of the parser with a parsing schema, although the importance of the control mechanism in eliminating cycles makes this schema schema for the MST parser. 6.5 Covington X  X  (1990, 2001) Non-Projective Parser
Covington X  X  non-projective parsing algorithm (Covington 1990, 2001) reads the input from left to right, establishing dependency links between the current word and previous words in the input. The parser maintains two lists: one with all the words encountered so far, and one with those that do not yet have a head assigned. A new word can be linked as a dependent of any of the words in the first list, and as a head of any of the words in the second list. The following parsing schema expresses this algorithm.
Item set: The item set is I CovNP = { [ i , h 1 , h 2 , ... , h an item [ i , h 1 , h 2 , ... , h k ] represents the set of forests of the form F = such that head ( T j ) = h j for every T j in F ; the projections of the nodes h pairwise disjoint, and their union is [1 .. i ]. Deduction steps: The set of deduction steps is as follows:
Final items: The set of final items is { [ n , h ] | 1  X  h forest with a single dependency tree T headed at an arbitrary position h of the string, and whose yield spans the whole input string. The time complexity of the algorithm is exponential in the input length n .
 Covington (2001, page 99), that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. parsing schema framework. For example, Kuhlmann (2010) presents a deduction sys-tem for a non-projective parser which uses a grammar formalism called regular de-pendency grammars . This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would first have to explain the formalism of regular dependency grammars. 7. Mildly Non-Projective Dependency Parsing
For reasons of computational efficiency, many practical implementations of dependency parsing are restricted to projective structures. However, some natural language sen-tences appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex:
Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). appearing in practice contain only small proportions of non-projective arcs. This has tures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates sev-eral such classes, based on well-nestedness and gap degree constraints (Bodirsky, malisms. Specifically, Kuhlmann shows that linear context-free rewriting systems 562 (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of well-nested dependency structures with gap degree at most k  X  1; and finally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1.
 rithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developing efficient data-driven manner rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses. In this section, we make four contributions to this enterprise.
 its correctness. The parser runs in time O ( n 7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta 2000), and can be optimized to O ( n lexicalized case. Secondly, we generalize our algorithm to any well-nested dependency structure with gap degree at most k , resulting in an algorithm with time complexity structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O ( n 4 + 3 k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP-complete (Satta 1992). Finally, we characterize the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks.
Nivre 2006). Let T be a dependency tree for the string w 1 Definition 5
The gap degree of a node k in T is the minimum g  X  ( N  X  X  tion of the node k ) can be written as the union of g + 1 intervals, that is, the number of discontinuities in k . The gap degree of the dependency tree T is the maximum of the gap degrees of its nodes.
 Note that T has gap degree 0 if and only if T is projective.
 Definition 6
The subtree induced by the node u in a dependency tree T is the tree T where E u = { i  X  j  X  E | j  X  u } . The subtrees induced by nodes p and q are interleaved if p  X  q =  X  and there are nodes i , j  X  p and k , l  X  dependency tree T is well-nested if it does not contain two interleaved subtrees, and a tree that is not well-nested is said to be ill-nested .

Projective trees are always well-nested, but well-nested trees are not always projective. 7.1 The WG 1 Parser
We now define WG 1 , a polynomial-time parser for well-nested dependency structures of gap degree at most 1. In this and subsequent schemata, each dependency forest in an item is a singleton set containing a dependency tree, so we will not make explicit mention of these forests, referring directly to their trees instead. Also note that in the parsers in this section we use D-rules to express parsing decisions, so dependency trees are assumed to be taken from the set of trees licensed by a given set of D-rules. The schema for the WG 1 parser is defined as follows: Item set: The item set is I WG1 = I 1  X  I 2 ,with trees with gap degree at most 1, rooted at h , and such that h = h ) have gap degree at most 1. We call items of this form gapped items , and the interval [ l .. r ]the gap of the item. Figure 7 shows two WG 1 items, one from
I , together with one of the trees contained in each of them. Note that the constraints h = j , h = i + 1, h = l  X  1, h = r are added to items to avoid redundancy in the item set.
Because the result of the expression { h } X  ([ i .. j ] \ different items representing the same dependency structures. Items  X  violating these constraints always have an alternative representation that does not violate them, which we can express with a normalizing function nm (  X  ) as follows: 564 Initial items: The set of initial items (hypotheses) is defined as the set where each item [ h , h , h , , ] represents the set containing the trivial dependency tree consisting of a single node h and no links. This is the same set of hypotheses used by [ h , h , h ] here for convenience when defining deduction steps. The same set of hypotheses subsequent schemata. Note that initial items are separate from the item set not subject to its constraints, so they do not require normalization.

Final items: The set of final items for strings of length n in WG which is the set of the items in I WG1 containing dependency trees for the complete input string (from position 1 to n ), with their head at any position h .
 Deduction steps: The deduction steps of the WG 1 parser are the following:
The WG 1 parser proceeds bottom X  X p, building dependency subtrees and combining them into larger subtrees, until a complete dependency tree for the input sentence is found. The parser logic specifies how items corresponding to the subtree induced by a particular node are inferred, given the items for the subtrees induced by the direct de-pendents of that node. Suppose that, in a complete dependency analysis for a sentence w ... w n , the node h has d 1 ... d p as direct dependents (i.e., we have dependency links d  X  h , ... , d from the ones corresponding to the subtrees induced by d 1 [ h , h , h , , ]. We infer p items representing the result of linking each of the dependent subtrees to the new head h . Second, apply the various C OMBINE obtained in the previous step into a single item. The C OMBINE operation between subtrees. Therefore, the result is a dependency tree containing all the dependent subtrees, and with all of them linked to h  X  X his is the subtree induced by h .
This process is applied repeatedly to build larger subtrees, until, if the parsing process is successful, a final item is found containing a dependency tree for the complete sentence. A graphical representation of this process is shown in Figure 8.
 566 structures in the class of well-nested structures of gap degree at most 1, for an input string under a set of D-rules. Concrete implementations of the schema may use proba-bilistic models or machine learning techniques to make the linking decisions associated with the D-rules, as explained in Section 3.1. The definition of such statistical models for guiding the execution of schemata falls outside the scope of this article. 7.2 Proof of Correctness for WG 1 this set satisfy the general definition of coherent final items; and then prove the stronger claims that all derivable items are coherent and all coherent items are derivable. The full correctness proof has previously been published (G  X  omez-Rodr  X   X guez, Weir, and Carroll 2008; G  X  omez-Rodr  X   X guez 2009), so for reasons of space we only sketch the proof here. these items must contain. Let T be a well-nested dependency tree headed at a node h , with all its edges licensed by our set of D-rules. We call such a tree a properly formed tree for the algorithm WG 1 if it satisfies the following conditions. 1. h is either of the form { h } X  [ i .. j ]or { h } X  ([ i .. j ] 2. All the nodes in T have gap degree at most 1 except for h , which can have one well-nested parse of gap degree  X  1 for that string. With these sets of coherent and coherent final items, we prove the soundness and completeness of WG Theorem 6 WG 1 is sound.
 Proof 6
Proving the soundness of the WG 1 parser involves showing that all derivable final items coherent. As in previous proofs, this is done by showing that each deduction step in the parser infers a coherent consequent item when applied to coherent antecedents. We proceed step by step, showing that if each of the antecedents of a given step contains at least one properly formed tree, we obtain a properly formed tree that is an element of the corresponding consequent. In the case of L INK steps, this properly formed consequent tree is obtained by creating a dependency link between the heads of the properly formed antecedent trees; for C OMBINE steps, it is obtained from the union of the antecedent trees. To prove that these consequent trees are properly formed, we show that they are well-nested, have a projection corresponding to the indices in the consequent item, and satisfy the gap degree constraint 2 required for the trees to be properly formed. Each trees. Theorem 7 WG 1 is complete.
 Proof 7
Proving completeness of the WG 1 parser involves proving that all coherent final items in WG 1 are derivable. We show this by proving the following, stronger claim. Lemma 1
If T is a dependency tree headed at a node h , which is a properly formed tree for WG then: 1. If h = { h } X  [ i .. j ], then the item [ i , j , h , , ] containing T is a derivable
This implies that all coherent final items are derivable, and therefore that WG complete. The lemma is proven by strong induction on the number of elements in h , which we denote #( h ).
 a tree contained in an initial item, which is derivable by definition. For the induction step, we take T to be a properly formed dependency tree rooted at a node h ,suchthat #( h ) = N for some N &gt; 1. Lemma 1 holds for T if it holds for every properly formed dependency tree T rooted at h such that #( h ) &lt; N . Let p be the number of direct children of h in the tree T . We have p  X  1, because by hypothesis #( h ) &gt; 1. With this, the induction step proof is divided into two cases, according to whether p = 1orp &gt; 1. single direct dependent of h is known to be derivable by the induction hypothesis. It can be shown case by case that the item corresponding to h by Lemma 1 can be inferred using L INK steps, thus completing the case for p = 1. For p &gt; 1, we use the concept of order annotations (Kuhlmann and M  X  ohl 2007; Kuhlmann 2010). Order annotations are strings that encode the precedence relation between the nodes of a dependency tree. The order annotation for a given node encodes the shape (with respect to this precedence relation) of the projection of each of the children of that node, that is, the number of intervals in each projection, the number of gaps, and the way in which intervals and gaps are interleaved. The concepts of projectivity, gap degree, and well-nestedness are associated with particular constraints on order annotations.
 annotation of the head h . The fact that the tree T is properly formed imposes constraints order annotations into a number of cases. Using the induction hypotheses and some relevant properties of order annotations we find that, for each of this cases, we can find a sequence of C OMBINE steps to infer the item corresponding to T from smaller coherent items. 7.3 Computational Complexity of WG 1 The time complexity of WG 1 is O ( n 7 ), as the step C OMBINE works with seven free string positions. This complexity with respect to the length of the 568 input is as expected for this set of structures, because Kuhlmann (2010) shows that they are equivalent to LTAG, and the best existing parsers for this formalism also perform in O ( n 7 ) (Eisner and Satta 2000). 9 Note that the C OMBINE only uses seven indices, not any additional entities such as D-rules. Hence, the O ( n complexity does not involve additional factors relating to grammar size. pairs of categories rather than pairs of words, a variant of this parser can be constructed lexicalized and unlexicalized items, except for the L INK a lexicalized item and an unlexicalized hypothesis to produce an unlexicalized item, and the C OMBINE S HRINKING G AP steps, which work only with unlexicalized items. binding the head to particular string positions. Finally, we need certain variants of the C
OMBINE S HRINKING G AP steps that take two unlexicalized antecedents and produce a lexicalized consequent; an example is the following:
Although this version of the algorithm reduces time complexity to O ( n a factor related to the number of categories, as well as constant factors due to having more kinds of items and steps than the original WG 1 algorithm. 7.4 The WG k Parser
The WG 1 parsing schema can be generalized to obtain a parser for all well-nested dependency structures with gap degree bounded by a constant k ( k the WG k parser. We extend the item set so that it contains items with up to k gaps, and modify the deduction steps to work with these multi-gapped items.
 Item set: The item set for the WG k parsing schema is where i , j , h  X  ( N  X  X  0 } ), 0  X  g  X  k ,1  X  h  X  n ,1  X  p  X  X  1, 2, ... , g } : l ( l such that h = { h } X  ([ i .. j ] \ g p = 1 [ l p .. r p ]), where each interval [ l normalization is defined as in WG 1 .
 Final items: The set of final items is defined as the set that parser. Deduction steps: The parser has the following deduction steps:
As expected, the WG 1 parser corresponds to WG k for k = 1. WG way as WG 1 , except that C OMBINE steps can create items with more than one gap. In all the parsers described in this section, C OMBINE steps may be applied in different orders to produce the same result, causing spurious ambiguity. In WG avoided when implementing the schemata by adding flags to items so as to impose a particular order on the execution of these steps. 7.5 Proof of Correctness for WG k
The proof of correctness for WG k is analogous to that of WG definition of properly formed trees to a higher gap degree. A properly formed tree in
WG k is a dependency tree T , headed at node h , such that the following hold. 1. h is of the form { h } X  ([ i .. j ] \ g p = 1 [ l p .. r 2. All the nodes in T have gap degree at most k except for h , which can have
With this, we define coherent items and coherent final items as for WG gap degree up to k and the head of a properly formed tree can have gap degree k + 1. 570
Completeness is shown by induction on #( h ). The base case is the same as for WG and for the induction step, we consider the direct children d p = 1 is proven by using L INK stepsjustasin WG 1 . In the case for p our proof on the order annotation for h , but we have to take into account that the set of possible annotations is larger when we allow the gap degree to be greater than 1, so we must consider more cases in this part of the proof. 7.6 Computational Complexity of WG k
The WG k parser runs in time O ( n 5 + 2 k ). As in the case of WG variables is C OMBINE S HRINKING G AP C ENTRE with 5 + 2 k free indices. Again, this constituency parsing: Kuhlmann (2010) shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled CFG in which the maximal rank of a nonterminal is k + 1. The constituency parser defined by Hotz and
Pitsch (1996) for these grammars also adds an n 2 factor for each unit increment of k .Note that a small value of k appears to be sufficient to account for the vast majority of the non-projective sentences found in natural language treebanks. For instance, the Prague
Dependency Treebank (Haji  X  c et al. 2006) contains no structures with gap degree greater than 4. Thus, a WG 4 parser would be able to analyze all the well-nested structures in this treebank, which represent 99 . 89% of the total ( WG
Increasing k beyond 4 would not produce further improvements in coverage. 7.7 Parsing Ill-Nested Structures: MG 1 and MG k
The WG k parser analyzes dependency structures with bounded gap degree as long as they are well-nested. Although this covers the vast majority of the structures that occur in natural language treebanks (Kuhlmann and Nivre 2006), a significant number of sen-tences contain ill-nested structures. Maier and Lichte (2011) provide examples of some linguistic phenomena that cause ill-nestedness. Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded.
This set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism is known to be NP-complete (Satta 1992). The reason for this complexity is the problem of unrestricted crossing configura-tions , appearing when dependency subtrees are allowed to interleave in every possi-ble way.
 degree and the number of dependents allowed per node: Kuhlmann (2010) presents regular dependency grammar . This parser is exponential in the gap degree, as well as in the maximum number of dependents allowed per node: Its complexity is O ( n where k is the maximum gap degree and m is the maximum number of dependents per node. In contrast, the parsers presented here are data-driven and thus do not need an explicit grammar. Furthermore, they are able to parse dependency structures with any number of dependents per node, and their computational complexity is independent of this parameter m .
 sense in which the structures appearing in treebanks are only  X  X lightly X  ill-nested. We generalize the algorithms WG 1 and WG k to parse a proper superset of the set of well-structures, which includes all the structures in several dependency treebanks. bottom X  X p process, where L INK steps are used to link completed subtrees to a head, and C OMBINE steps are used to join subtrees governed by a common head to obtain a larger structure. As WG k is a parser for well-nested structures of gap degree up to k , its C OMBINER steps correspond to all the ways in which we can join two sets of sibling subtrees meeting these constraints, and having a common head, into another. Therefore, this parser does not use C OMBINER steps that produce interleaved subtrees, because these would generate items corresponding to ill-nested structures.
 k , including some ill-nested ones, by having C OMBINER steps representing all ways in which two sets of sibling subtrees of gap degree at most k with a common head can be joined into another, including those producing interleaved subtrees. This does not mean that we build every possible ill-nested structure. Some structures with complex crossed configurations have gap degree k , but cannot be built by combining two structures of (well-nested or not) if there exists a binarization of that structure that has gap degree at most k . The parser works by implicitly finding such a binarization, because C steps are always applied to two items and no intermediate item generated by them can exceed gap degree k (not counting the position of the head in the projection). Definition 7 Let w 1 ... w n be a string, and T a dependency tree headed at a node h .A binarization of
T is a tree B in which each node has at most two children, such that: 1. Each node in B can be unlabelled, or labelled with a word position i . 2. A node labelled i is a descendant of j in B if and only if i
The projection of a node in a binarization is the set of reflexive-transitive children of that node. With this, condition (2) of Definition 7 can be rewritten i and the gap degree of a binarization can be defined as with a dependency structure, allowing us to define mildly ill-nested structures as follows.
 Definition 8
A dependency structure is mildly ill-nested for gap degree k if it has at least one binarization of gap degree  X  k . Otherwise, it is strongly ill-nested for gap degree k .
The set of mildly ill-nested structures for gap degree k includes all well-nested structures with gap degree up to k . We define MG 1 , a parser for mildly ill-nested structures for gap degree 1, as follows.

Item set and final item set: The item set and the final item set are the same as for WG except that items can contain any mildly ill-nested structures for gap degree 1, instead of being restricted to well-nested structures. 572 Deduction steps: Deduction steps include those in WG 1 , plus the following.
These extra C OMBINE steps allow the parser to combine interleaved subtrees with simple crossing configurations. The MG 1 parser still runs in O ( n ill-nested structures for gap degree k ,weaddaC OMBINE way of joining two structures of gap degree at most k into another. This is done in a and b to represent intervals of words in the projection of each of the structures, and will correspond to gaps in the joined structure. The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol. Given a string of this form, of length n ,with a  X  X  located at positions a 1 ... a p (1 b ... b q (1  X  b 1 &lt; ... &lt; b q  X  n ), and g  X  X  at positions g such that p + q + r = n , the corresponding C OMBINE step is as follows.
For example, the C OMBINE I NTERLEAVING G AP Cstepin MG 1 string abgab . Therefore, we define the parsing schema for MG nested structures for gap degree k , as the schema where the item set is the same as that of WG k , except that items now contain mildly ill-nested structures for gap degree k ;and the set of deduction steps consists of the L INK step in WG obtained as explained herein. 7.8 Computational Complexity of MG k
Because the string used to generate a C OMBINER step can have length at most 3 k + 2, and the resulting step contains an index for each symbol of the string plus two extra input. Note that this expression denotes the complexity with respect to n of the MG parser obtained for a given k : Taking k to be a variable would add an additional O (3 complexity factor, because the number of different C OMBINER to a given item grows exponentially with k . 7.9 Proof of Correctness for MG k
As for previous parsers, we only show here a sketch of the proof that MG
The detailed proof has been published previously (G  X  omez-Rodr  X   X guez, Weir, and Carroll 2008; G  X  omez-Rodr  X   X guez 2009).
 Theorem 8 MG k is correct.
 Proof 8
As with WG k , we define the sets of properly formed trees and coherent items for this algorithm. Let T be a dependency tree headed at a node h . We call such a tree a properly formed tree for the algorithm MG k if it satisfies the following. 1. h is of the form { h } X  ([ i .. j ] \ g p = 1 [ l p .. r 2. There is a binarization of T such that all the nodes in it have gap degree at
The sets of coherent and coherent final items are defined as in previous proofs. Sound-properly formed by building a binarization for them from the binarizations obtained from antecedent items. This part of the proof involves imposing additional constraints on binarizations, which are useful to provide a suitable way of combining binarizations obtained from antecedents of steps. Completeness is proven by showing the following, stronger claim.
 Proposition 1
Let T be a dependency tree headed at node h , and properly formed for MG h = { h } X  ([ i .. j ] \ g p = 1 [ l p .. r p ]), for g  X  T is derivable under this parser.

To prove this, we say that a binarization of a properly formed tree is a well-formed binarization for MG k if each of its nodes has gap degree which can have gap degree k + 1. We then reduce the proof to establishing the following lemma.
 Lemma 2
Let B be a well-formed binarization of a dependency tree T , headed at a node h and properly formed for MG k . If the projection of h in T is h p = 1 [ l p .. r p ]), for g under this parser.
 The lemma is shown by strong induction on the number of nodes of B (denoted # B ).
The base case where # B = 1 is trivial. For the induction step, we consider different cases depending on the number and type of children of the head node h of B . When h has a single child, we obtain the item corresponding to T from a smaller item, shown to be derivable by the induction hypothesis, by using a L INK step. Where h has two children in B , the relevant item is obtained by using a C OMBINER 574 7.10 Mildly Ill-Nested Dependency Structures
The MG k algorithm parses mildly ill-nested structures for gap degree k in polynomial time. The mildly ill-nested structures for gap degree k are those with a binarization of gap degree  X  k . Because a binarization of a dependency structure cannot have lower gap degree than the original structure, the mildly ill-nested structures for gap degree k all have gap degree  X  k . Given the relation between MG k and WG degree 1, but which is strongly ill-nested for gap degree 1. For all trees up to 10 nodes (excluding the dummy root node at position 0) all structures of gap degree k with
Even if T is strongly ill-nested for a gap degree, there is always an m
T is mildly ill-nested for m (every structure can be binarized, and binarizations have finite gap degree). For example, the structure in Figure 9 is mildly ill-nested for gap degree 2. Therefore, MG k parsers have the property of being able to parse any arbitrary dependency structure as long as we make k large enough. Structures like the one in
Figure 9 do not arise in dependency treebanks. None of the treebanks for nine different languages 10 contain structures that are strongly ill-nested for their gap degree (Table 1).
Therefore, in any of these treebanks, the MG k parser can parse every sentence with gap degree at most k in time O ( n 3 k + 4 ). 8. Link Grammar Schemata
Link Grammar (LG), introduced by Sleator and Temperley (1991, 1993), is a theory of syntax whose structural representation of sentences is closely related to projective dependency representations, but with some important differences.
Undirected links: Like dependency formalisms, LG represents the structure of sentences as a set of links between words. However, whereas dependency links are directed, the links used in LG are undirected: There is no distinction made between heads and dependents.
Cycles: The sets of links representing the structure of sentences in LG may contain cycles, in contrast to dependency structures.
 each of which is associated with a set of linking requirements. Given a link grammar G , a set of labelled links between the words of a sentence w for that sentence if it satisfies the following conditions: planarity (the links do not cross when drawn above the words), connectivity (the undirected graph defined by links is connected), and satisfaction (the links satisfy the linking requirements of all the words in the input). An input sentence is considered grammatical with respect to a link grammar G if it is possible to build a linkage for the sentence with the grammar G . labels of the links that can be established between that word and other words located to its left or to its right. Linking requirements can include constraints on the order of the links, for example, a requirement can specify that a word w can be linked to two words located to its left in such a way that the link to the farthest (leftmost) word has a particular label L 2 and the link to the closest word has a label L linking requirements: The requirements of words are expressed as a set of disjuncts .
Each disjunct corresponds to one way of satisfying the requirements of the word. We represent a disjunct for a word w as a pair of strings  X = ( R
L , L 2 , ... L p are the labels of the links that must connect w to words located to the left of w , which must be monotonically increasing in distance from w (e.g., L leftmost word that is directly linked to w ), and R 1 , R that must connect w to words to its right, also monotonically increasing in distance from w (e.g., R q links to the rightmost word that is directly connected to w ). 576 and dependency formalisms. Item sets for LG parsing schemata are defined as sets of partial syntactic structures, which in this case are partial linkages: Definition 9
Given a link grammar G and a string w 1 ... w n ,a partial linkage is any edge-labeled undirected graph H such that the following conditions hold.

Informally, a partial linkage is the result of choosing a particular disjunct from those words that are compatible with the requirements of the disjunct. Compatibility means that, for each word w i associated with a disjunct  X  i = ( R of labels of links connecting v i to words to its right, ordered from the leftmost to the rightmost such word, is of the form R i symmetrically, the list of labels of links connecting v i the rightmost to the leftmost, is of the form L j
Given such a linkage, the right linking requirements R i satisfied , and the same for the left linking requirements L requirements that are not satisfied (e.g., the requirement of a link R ciated with word w i ,with0 &lt; k  X  q ,suchthat k /  X  X  i tion 4), where items come from a partition of the set of partial linkages for a given link grammar G . With these item sets, LG parsing schemata are analogous to the depen-dency and constituency cases. As an example of an LG parsing schema, we describe the original LG parser by Sleator and Temperley (1991), and show how projective parsing schemata, such as those seen in Section 3, can be adapted to obtain new LG parsers. 8.1 Sleator and Temperley X  X  LG Parser
The LG parser of Sleator and Temperley (1991) is a dynamic programming algorithm that builds linkages top X  X own: A link between v i and v k dependency parsers seen in previous sections (Eisner 1996; Eisner and Satta 1999; Yamada and Matsumoto 2003), which build dependency graphs bottom X  X p.
 Item set: The item set for Sleator and Temperley X  X  parser is string w i ... w j of the input, w i is linked to words in that substring by links labelled  X  and has right linking requirements  X  unsatisfied, w j is linked to words in the substring by links labelled  X  and has left linking requirements  X  unsatisfied, B is True if and only in the span are transitively reflexively linked to one of the end words w all of their linking requirements satisfied.
 beginning of every input sentence (Sleator and Temperley 1991). Therefore, we assume sponds to a dummy word w n + 1 that must not be linkable to any other, and is used by the parser for convenience, as in the schema for Yamada and Matsumoto X  X  dependency parser (Section 3.4).
 is an item used to select a particular disjunct for a word w Deduction steps: The set of deduction steps is the following:
LG schemata indicates that the corresponding step adds a link labelled b between nodes 578 v item from each sequence of deduction steps that generates it. The S step chooses one of the available disjuncts for a given word w the top X  X own process by constructing a linkage that spans the whole string w but where no links have been constructed yet. Then, the P steps repeatedly divide the problem of finding a linkage for a substring w the smaller subproblems of finding linkages for w i ... w particular, the L EFT P REDICT step poses the subproblem of finding a linkage for w in which w i is not directly linked to w z ,andL EFT L INK while building a direct link from w i to w z .R IGHT proceed analogously for the substring w z ... w j . After these two smaller linkages have been found, they are combined by a C OMPLETER step into a larger linkage; the flags will contain a valid linkage satisfying the connectivity constraint. An example of this process, where a particular substring is parsed by using the L R
IGHT P REDICT steps to divide it into smaller substrings, is shown in Figure 10. The algorithm runs in time O ( n 3 ) with respect to the length of the input, because none of its deduction steps uses more than three independent string position indices.
Final items: The set of final items is { [0, n + 1,  X   X  ,  X 
True implies that their linkages for w 0 ... w n + 1 have at most two connected components, and we have assumed that the word w n + 1 cannot be linked to any other, so one of the components must link w 0 ... w n . 8.2 Adapting Projective Dependency Parsers to Link Grammar
We now exploit similarities between LG linkages and projective dependency structures to adapt projective dependency parsers to the LG formalism. As an example we present an LG version of the parser by Eisner (1996), but the same principles can be applied to other parsers: schemata for LG versions of the parsers by Eisner and Satta (1999) and Yamada and Matsumoto (2003) can be found in G  X  omez-Rodr  X   X guez (2009). forests contained in each dependency item. The corresponding LG items contain link-ages with the same structure as these forests. For example, because each forest in an item headed at the words w i and w j , the analogous item in the corresponding LG parsing schema will contain linkages with two connected components, one containing the word w and the other containing w j . The notion of a head is lost in the conversion because the undirected LG linkages do not make distinctions between heads and dependents.
This simplifies the notation used to denote items in some cases: For instance, we do not need to make a distinction between Eisner items of the form [ i , j , True , False ]andthose of the links. Therefore, items in the LG version of Eisner X  X  parser will use a single flag, indicating whether linkages contained in them have one or two connected components. to LG. If the original dependency steps always produce items containing projective dependency forests, the resulting LG steps produce items with planar linkages. When the original dependency steps have constraints related to the position of the head in word in a linkage to be its  X  X ead X  for the purpose of linking it to other linkages. or remove constraints to allow cycles, so that the parsers are able to link two words that are already in the same connected component of a linkage. In the schema obtained from
Eisner X  X  parser, this is done by allowing L INK steps to be applied to items representing fully connected linkages; in the schema corresponding to Eisner and Satta X  X  parser we allow C OMBINER steps to create a link in addition to joining two linkages; and in the schema for Yamada and Matsumoto X  X  parser we add a step that creates two links at the same time, combining the functionality of the L-L INK and R-L is constrained by disjuncts associated with words, we include disjunct information in items in order to ensure that only grammatical linkages are constructed. This is similar to the schema for Sleator and Temperley X  X  parser, but in this case items need to specify up parsers establish links from end words of an item to words outside the item X  X  span (which can be to the left or to the right of the span) rather than to words inside the span (which are always to the right of the left end word, and to the left of the right end word). Eisner (1996).
 Item set: The item set is where an item of the form [ i , j ,  X  1  X   X  1 ,  X  2 linkages over the substring w i ... w j of the input, satisfying the following conditions. 580 Deduction steps: The set of deduction steps for this parser is as follows:
These steps resemble those in the schema for Eisner X  X  dependency parser, with the exception that the L INK step is able to build links on items that contain fully connected parser). A version of the parser restricted to acyclic linkages can be obtained by adding the constraint that B must equal False in the L INK step.

Final items: The set of final items is { [0, n ,  X   X  ,  X   X  items containing fully connected linkages for the whole input string.
 and Matsumoto (2003) are not shown here for space reasons, but are presented by
G  X  omez-Rodr  X   X guez (2009). The relationships between these three LG parsing schemata are the same as the corresponding dependency parsing schemata, that is, the LG vari-ants of Eisner and Satta X  X  and Yamada and Matsumoto X  X  dependency parsers are step contractions of the LG variant of Eisner X  X  parser. As with the algorithm of Sleator and
Temperley, these bottom X  X p LG parsers run in cubic time with respect to input length. 9. Conclusions and Future Work
The parsing schemata formalism of Sikkel (1997) has previously been used to define, analyze, and compare algorithms for constituency-based parsing. We have shown how to extend the formalism to dependency parsers, as well as the related Link Grammar formalism.
 dency parsers: In Kuhlmann (2007, 2010) a grammatical deduction system was used to define a parser for regular dependency grammars. viewing them as transition systems. That model is based on parser configurations and transitions, and has no clear relationship to the approach described here. them to describe a wide range of existing projective and non-projective dependency nally formulated very differently X  X or example, establishing the relation between the dynamic programming algorithm of Eisner (1996) and the transition-based parser of
Yamada and Matsumoto (2003). We have also used the parsing schemata framework as a formal tool to verify the correctness of parsing algorithms.
 existing parsing algorithms, they can be used to define new parsers. We have presented an algorithm that can parse any well-nested dependency structure with gap degree bounded by a constant k with time complexity O ( n 2 k + 5 a wider set of structures that we call mildly ill-nested for a given gap degree k ,and presented an algorithm that can parse these in time O ( n treebanks, showing that all the sentences contained in them are mildly ill-nested for the problem of finding minimal fan-out binarizations of LCFRS to improve parsing efficiency (see G  X  omez-Rodr  X   X guez et al. 2009).
 mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our definition of mildly would be interesting to find a more grammar-oriented definition that would provide linguistic insight into this set of structures.
 called k-ill-nestedness , is more declarative than that of mildly ill-nestedness. However, it is based on properties that are not local to projections or subtrees, and there is no evidence that k-ill-nested structures are parsable in polynomial time.
 (Nivre 2003; McDonald et al. 2005) rely on statistically-driven control mechanisms that fall below the abstraction level of parsing schemata. Therefore, it would be useful to have an extension of parsing schemata allowing the description and comparison of these control structures in a general way.
 Acknowledgments 582 584
