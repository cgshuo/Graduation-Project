 liers and provides alternative estimators that achieve imp roved robustness [11, 13, 17, 23]. Outliers are understood to be observations that have been corrupted, incorrectly measured, mis-recorded, drawn under different conditions than those intended, or so atypical as to require separate model-to arbitrary changes made to a non-trivial fraction of the sample data X  X  go al that is equally rele-vant to machine learning research given that data sets are of ten collected with limited or no quality non-convex training criteria that have yet to yield efficien t global solution methods [13, 17, 23]. Although many robust regression methods have been proposed in the classical literature, M-estimators continue to be a dominant approach [13, 17]. These correspon d to the standard machine learning approach of minimizing a sum of prediction errors u nder a given loss function (assuming a fixed scaling). M-estimation is reasonably well understoo d, analytically tractable, and provides a simple framework for trading off between robustness again st outliers and data efficiency on in-convex loss, even a single data point can dominate the result . That is, any (non-constant) convex loss function exhibits necessarily unbounded sensitivity to even a single outlier [17,  X  5.4.1]. Al-though unbounded sensitivity can obviously be mitigated by imposing prior bounds on the domain achieves bounded outlier sensitivity by considering redescending loss functions (see [17,  X  2.2] for a definition), or more restrictively, bounded loss functions, both of which are inherently non-convex. Robust regression has also been extensively investigated i n computer vision [2, 26], where a similar conclusion has been reached that bounded loss functions are necessary to counteract the types of outliers created by edge discontinuities, multiple motion s, and specularities in image data. bounded loss functions [8, 15, 18, 19, 25] to replace the stan dard convex, unbounded losses deployed in support vector machines and boosting [9] respectively. I n fact, [16] has shown that minimizing any convex margin loss cannot achieve robustness to random misc lassification noise. The conclusion is necessary to ensure robustness against outliers X  X reating an apparent dilemma: one can achieve global training via convexity or outlier robustness via bou ndedness, but not both.
 In this paper we present a counterpoint to these pessimistic conclusions. In particular, we present a general model for bounding any convex loss function, via a pr ocess of  X  X oss clipping X , that ensures bounded sensitivity to outliers. Although the resulting op timization problem is not, by itself, con-vex, we demonstrate an efficient convex relaxation and round ing procedure that guarantees bounded response to data X  X  guarantee that cannot be established for a ny convex loss minimization on its own. The approach we propose is generic and can be applied to a ny standard loss function, be it for regression or classification. Our work is inspired by a nu mber of studies that have investigated robust estimators in computer vision and machine learning [ 2, 26, 27, 30]. However, these previ-ous attempts were either hampered by local optimization or r estricted to special cases; none had guarantees of global training and outlier insensitivity.
 Before proceeding it is important to realize that there are m any alternative conceptions of  X  X obust- X  X obust optimization X  [28, 29] considers minimizing the wo rst case loss achieved given prespecified bounds on the maximum data deviation that will be considered . Although interesting, these results do not directly bear on the question at hand since we explicit ly do not bound the magnitude of the of robustness is algorithmic stability under leave-one-ou t perturbation [3]. Although loosely related, algorithmic stability addresses the analysis of given lear ning procedures rather than describing how a stable algorithm might be generally achieved in the presen ce of arbitrary outliers. We also do not focus on asymptotic or infinitesimal notions from robust sta tistics, such as influence functions [11], nor impose boundedness assumptions on the domain and range o f the data or the predictor [5, 6]. We consider the standard supervised setting where one is giv en an input matrix X and output targets y , with the goal of learning a predictor h :  X  m  X  X  X  . Each row of X gives the feature representation for one training example, denoted X can be written as a generalized linear model; that is, the pre dictions are given by  X  y a fixed transfer function f (possibly identity) and a vector of parameters  X  . For training, we will consider the standard L where L denotes the loss function,  X  is the regularization constant, and n denotes the number of training examples. Normally the loss function L is chosen to be convex in  X  so that the minimization problem can be solved efficiently. Although convexity is imp ortant for computational tractability, it has the undesired side-effect of causing unbounded outlier sensitivity, as mentioned. Obviously, the severity of the problem will range from minimal to extreme de pending on the nature of the distribu-Standard Convex Loss Functions: Our general construction applies to arbitrary convex losse s, but we will demonstrate our methods on standard loss functio ns employed in regression and clas-order Taylor approximation, obtaining a loss L L
 X  ( X  y k y ) = ( X  y  X  y ) 2 / 2 A related construction is matching losses [14], which are determined by taking a strictly increasing a transfer f , a loss can be defined by L since F is differentiable and the assumptions on f imply that F is strongly convex. These two loss constructions are related by the equality L post-prediction KL-divergence y ln y L
F ( X  z k z ) = ln( e  X  z + 1)  X  ln( e z + 1)  X   X  ( z )( X  z  X  z ) losses are prevalent in regression and probabilistic class ification settings.
 transfer does not admit the matching loss construction, a su rrogate margin loss can be obtained by taking a nonincreasing function l such that lim Here y  X  y is known as the classification margin . Standard examples include misclassification loss, L deviance loss, L loss is furthermore chosen to be convex, efficient minimizat ion can be attained.
 To unify our presentation below we will simply denote all los s functions by  X  ( y, x  X   X  ) , with the understanding that  X  ( y, x  X   X  ) = L parameters  X  . Note that by their very convexity these losses cannot be rob ust: all admit unbounded sensitivity to a single outlier (the same is also true for L Bounded loss functions: As observed, non-convex loss functions are necessary to bou nd the ef-fects of outliers [17]. Black and Rangarajan [2] provide a us eful catalog of bounded and redescend-ing loss functions for robust regression, of which a represe ntative example is the Geman and Mc-makes plain, boundedness implies non-convexity (for any no n-constant function). It therefore ap-pears that bounded loss functions achieve robustness at the cost of losing global training guarantees. Our goal is to show that robustness and efficient global train ing are not mutually exclusive. Despite extensive research on regression and classification, almos t no work we are aware of (save perhaps [30] in a limited way) attempts to reconcile robustness to ou tliers with global training algorithms. Adapting the ideas of [2, 27, 30], given any convex loss  X  ( y, x  X   X  ) define the clipped loss as Figure 1 demonstrates loss clipping for some standard loss f unctions. Given a clipped loss, a robust form of training problem (1) can be written as Clearly such a training objective bounds the influence of any one training example on the final re-sult. Unfortunately, the formulation (3) is not computatio nally convenient because the optimization problem it poses is neither convex nor smooth. To make progre ss on the computational question we exploit a key observation: for any loss function, its corr esponding clipped loss can be indirectly expressed by an auxiliary optimization of a smooth objectiv e (if the original loss function itself was smooth). That is, given a loss  X  ( y, x  X   X  ) define the corresponding  X  -relaxed loss to be for 0  X   X   X  1 ; see Figure 1. This construction is an instance of an outlier process as described in [2] and is motivated by a special case hinge-loss construc tion originally proposed in [30]. The  X  -relaxation provides a convenient characterization of any clipped loss, since it can be shown in general that minimizing a corresponding  X  -relaxed loss is equivalent to minimizing the clipped loss. Proposition 1 For any loss function  X  ( y, x  X   X  ) , we have  X  (The proof is straightforward, but it is given in the supplem ent for completeness.) Proposition 1 now allows us to reformulate (3) as a smooth optimization usi ng the fact that the optimization is completely separable between the  X  Unfortunately, the resulting problem is not jointly convex in  X  and  X  even though it is convex in each given the other. Such marginal convexity might suggest that an alternating minimization strategy, however the proof of Proposition 1 shows that each minimizat ion over  X  will result in  X  losses greater than 1, or  X  the search to get trapped in local minima, requiring a more so phisticated approach to be considered. One contribution of this paper is to derive an exact reformul ation of (5) that admits a convex re-laxation and rounding scheme that retain bounded sensitivi ty to outliers. We first show how the relaxation can be efficiently solved by a scalable algorithm that eliminates any need for semidefinite programming, then provide a guarantee of bounded outlier se nsitivity in Section 5.
 Reformulation: To ease the notational burden, let us rewrite (5) in matrix-v ector form of individual training losses. Given that  X  ( , ) is convex in its second argument we will be able to exploit Fenchel duality to re-express the min-min form (6) i nto a min-max form that will serve as the basis for the subsequent relaxation. In particular, con sider the definition By construction,  X   X  ( y,  X  ) is guaranteed to be convex in  X  since it is a pointwise maximum over linear functions [4,  X  3.2].
 Lemma 1 For any convex differentiable loss function  X  ( y, x  X   X  ) such that the level sets of  X   X  x  X  (  X   X  v ) +  X  ( y, x  X  v ) are bounded, we have (This is a standard result, but a proof is given in the supplem ent for completeness.) For standard  X  ( y,  X  ) =  X  2 / 2 +  X y . Now let  X (  X  ) denote putting  X  in the main diagonal of a square matrix and main reformulation as follows.
 where  X  is an ( n + 1)  X  1 variable,  X  is an n  X  1 variable, and the matrix T (  X  ) is given by
T (  X  ) = The proof consists in first dualizing  X  in (6) via Lemma 1, which establishes the key relationship The remainder of the proof is merely algebra: given a solutio n  X  to (10), the corresponding solution  X  to (6) can be recovered via  X  = 1 Note that the formulation (10) given in Theorem 1 is exact. No approximation to the problem (6) amenable to an efficient algorithm: the objective is concave in  X  , conveniently, but it is not convex in  X  . The advantage attained by (10) however is that we can now der ive an effective relaxation. Relaxation: Let  X  ( M ) denote the main diagonal vector of the square matrix M and let tr ( M ) denote the trace. Consider the following relaxation where we used strong minimax duality to obtain (14) from (13) : since the constraint region on M is compact and the inner objective is concave and convex in  X  and M respectively, Sion X  X  mini-max theorem is applicable [22,  X  37]. Next enforce the constraint  X  ( M ) = 1 multiplier  X  : This relaxed formulation (16) is now amenable to efficient gl obal optimization: The outer problem is jointly concave in  X  and  X  , since it is a pointwise minimum of concave functions. The in ner optimization with respect to M can now be simplified by exploiting the well known result [21] : Therefore, given  X  and  X  , the inner problem is solved by the maximum eigenvector of T (  X  )+ X (  X  ) . Optimization Procedure: Given training data, an outer maximization can be executed j ointly over  X  and  X  to maximize (16). This outer problem is concave in  X  and  X  hence no local maxima exist. Although the outer problem is not smooth, many effective met hods exist for nonsmooth convex optimization [20, 31]. Each outer function evaluation (and subgradient calculation) requires the inner problem (17) to be solved. Fortunately, a simple power method [10] can be used to efficiently compute a maximum eigenvector solution to the inner problem by only performing matrix-vector multiplications on the individual factors of the two low ran k matrices making up T (  X  ) , meaning the inner problem can be solved without ever forming a large n  X  n matrix T (  X  ) . That is, if X is n  X  m each inner iteration requires at most O ( nm ) computation. Solution Recovery: At a solution, the values of (13) X (16) are equal, and all prov ide a lower bound on the original objective (6). Ideally, given a maximizer  X   X  for (14) one would recover a prediction model  X  via (12). However, (12) requires  X  to be acquired first, which could be obtained from a  X  that solves (10). Unfortunately, the relaxation step taken in (13) means that the solution to (14) (recovered from the  X  that solves (17)) does not necessarily solve (10): the inner solution  X  in (17) might not be unique. If it is unique, we immediately have the o ptimal solution to (10) hence an exact solution to the original problem (6). More typically, however, the maximum eigenvector is solution M  X  to (14) is not rank 1. In such cases we need to use a rounding pro cedure to recover an effective rank 1 approximation.
 set of maximum eigenvectors  X  V = {  X   X  much smaller than n +1 ). A solution can then be indirectly obtained by solving a sma ll semidefinite program to recover a k  X  k matrix C  X  that satisfies C  X  0 and  X  (  X  V C  X   X  V  X  ) = 1 C M can be recovered simply by computing  X   X   X  = P k explicitly minimize the  X   X  -relaxed loss (7) given  X   X  to recover  X   X  via  X   X  = arg min Although the rounding step has introduced an approximation , we establish that bounded outlier sen-experimentally that the gap from optimality is generally no t too large. Thus far we have proposed a robust training objective, provi ded an efficient convex relaxation that establishes a lower bound, and proposed a simple rounding me thod for recovering an approximate solution. The question remains as to whether the approximat e solution retains bounded sensitivity (  X   X  ,  X   X  ) denote the approximate solution obtained from the procedur e above.
 First, observe that an upper bound on the approximation erro r can be easily computed by subtract-ing the lower bound value obtained in (14) X (16) from R (  X   X  ,  X   X  ) . Our experiments below show that reasonable gaps are obtained in this way. Nevertheless one w ould still like to guarantee that the gap stays bounded in the presence of arbitrary outliers and leve rage points.
 K remains bounded, then there exists a c &lt;  X  such that R (  X   X  ,  X   X  )  X  c .
 That is, the  X  -relaxed loss obtained by the rounded solution stays bounde d in this case, even when accounting for the proposed relaxation and rounding proced ure and data perturbation. The complete Note that the proposed method cannot be characterized by min imizing a fixed convex loss. That is, the tightest convex upper bound for any convex loss function is simply given by the function itself, which corresponds to setting  X  method does not choose a constant  X   X  values, closer to 1 for inliers and closer to 0 for outliers. The resulting upper bound on the clipped loss (hence on the misclassification error in the margin loss case) is much tighter than that achieved by simply minimizing a convex loss. This outcome is demonstr ated clearly in our experiments. Table 1: Synthetic experiment with n = 200 , m = 5 , and t = 500 . Test error rates (RMSE) on clean In this section, we experimentally evaluate the preceding t echnical developments on synthetic and real data for both regression and classification.
 Regression: We first illustrate the behavior of the various regression te chniques by a simple demon-dispersed model. We compare the behaviours of standard regr ession losses: least-squares (L2), L (L1), the Huber minimax loss (HuberM) [13, 17], and the robus t Geman and McClure loss (GM) [2]. To these we compare the proposed relaxed method (ClipRelax) , along with an alternating minimizer Figure 2 demonstrates that the three convex losses, L2, L1 an d HuberM, are dominated by outliers. By contrast, ClipRelax successfully found the correct line ar model in each case. Note that the robust GM loss finds two different minima, corresponding to that of L 2 and ClipRelax respectively, hence it was not depicted in the plot. ClipAlt also gets trapped in l ocal minima as expected: it finds the correct model in Figure 2 (a) but incorrect models in Figure 2 (b) and (c).
 In our second synthetic regression experiment we consider l arger problems. Here a target weight vector  X  is drawn from N ( 0 , I ) , with inputs X outputs y by randomly re-sampling each y by an outlier probability p . Here 200 of the 700 examples are randomly chosen as the train ing set and the rest used for testing. We compare the same six methods : L2, L1, HuberM, GM, ClipAlt and ClipRelax. The regularization parameter  X  was set on a separate validation set. These experiments are repeated 20 times and average (Huber loss) test errors on clean data are reported (with standard this case the proposed relaxation performs comparably to th e the non-convex GM loss. Interestingly, method and the lower bound on the optimal  X  -robust loss (16) remains remarkably small, indicating our robust relaxation (almost) optimally minimizes the ori ginal non-convex clipped loss. Finally, we investigated the behavior of the regression met hods on a few real data sets. We chose three data sets: astronomy data containing outliers from [2 3], and two UCI data sets, seeding the the UCI data sets with outliers. Test results are reported on clean data to avoid skewing the reported results. For UCI data, outliers were added by resampling X outliers. The regularization parameter  X  was chosen through 10-fold cross validation on the training set. Note that in real regression problems one needs to obtai n an estimate for the the scale, given by the true standard deviation of the noise in the data. Here w e estimated the scale using the mean Table 2, one can see that on both data sets, ClipRelax clearly outperformed the other methods. L2 is clearly skewed by the outliers. Unsurprisingly, the classi cal robust loss functions, L1 and HuberM, perform better than L2 in the presence of outliers, but not as well as ClipRelax.
 Classification: We investigated the well known case study from [16] and compa red the proposed method to logistic regression (i.e. the logit, or binomial d eviance loss [12]) and the robust 1  X  tanh loss [19] in a classification context. Here 200 examples were drawn from the target distribution with label noise applied at various levels. The experiment was re peated 50 times to obtain average results and standard deviations. Table 3 shows the test error perfor mance in clean data of the different methods. From these results one can conclude that ClipRelax is more robust than standard logit the classification model produced by ClipRelax is closer to t he true model despite of the presence of outliers, demonstrating that the proposed method can be rob ust in a simple classification context. We have proposed a robust estimation method for regression a nd classification based on a notion of  X  X oss-clipping X . Although the method is not as fast as sta ndard convex training, it is scalable to problems of moderate size. The key benefit is competitive (or better) estimation quality than the state-of-the-art in robust estimation, while ensuring pro vable robustness to outliers and computable bounds on the optimality gap. To the best of our knowledge the se two properties have never been previously achieved simultaneously. It would be interesti ng to investigate whether the techniques ing GM, MM, L, R and S estimators [11, 13, 17, 23]. Connections with algorithmic stability [3] and influence function based analysis [5, 6, 11] merit further in vestigation. Obtaining tighter bounds on approximation quality that would enable a proof of consiste ncy also remains an important challenge.
