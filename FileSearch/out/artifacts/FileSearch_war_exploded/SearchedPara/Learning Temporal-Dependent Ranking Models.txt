 Web archives already hold together more than 534 billion files and this number continues to grow as new initiatives arise. Searching on all versions of these files acquired through-out time is challenging, since users expect as fast and precise answers from web archives as the ones provided by current web search engines. This work studies, for the first time, how to improve the search effectiveness of web archives, in-cluding the creation of novel temporal features that explore the correlation found between web document persistence and relevance. The persistence was analyzed over 14 years of web snapshots. Additionally, we propose a temporal-dependent ranking framework that exploits the variance of web char-acteristics over time influencing ranking models. Based on the assumption that closer periods are more likely to hold similar web characteristics, our framework learns multiple models simultaneously, each tuned for a specific period. Ex-perimental results show significant improvements over the search effectiveness of single-models that learn from all data independently of its time. Thus, our approach represents an important step forward on the state-of-the-art IR technology usually employed in web archives.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms; Performance; Experimentation web archives; temporal-dependent ranking
Web archive information retrieval (WAIR) addresses the retrieval of document versions from web archives, according to topical and temporal criteria of relevance. WAIR differs from typical IR and web IR, because a web archive corpus is distinctively composed by a stack of content collections har-vested from the web over time. Thus, each document may have several versions and the relevance of a version depends on the user X  X  period of interest. Another main difference of WAIR is that its multi-version web collections have differ-ent characteristics over time, which causes variations in the discriminative power of features used in ranking.

WAIR has been applied in at least 68 web archive initia-tives 1 undertaken by national libraries, national archives and consortia of organizations that are acquiring and preserving parts of the web. Web archives already hold more than 534 billion files (17 PB), of which some are historical records, such as opinions, decisions and photos of events. Pieced to-gether, these records form our collective memory and the possibility of looking into the past opens space for novel ap-plications. In the last years, applications based on data from web archives include tools for assessing the trustworthiness of statements [31], detecting web spam [5], improving web IR [10] or forecasting events [26]. However, despite web archive technology having achieved a good maturity level, the ef-fectiveness of the search services they provide still presents unsatisfactory results [8]. As result, information cannot be found and web archives are useless for their users.
In this work, we cope with the poor retrieval effective-ness of web archives by addressing three identified limita-tions. First, the ranking relevance of document versions is currently computed based only on the similarity of each con-tent with the query, ignoring many other features which have shown to improve web search engines. By employing state-of-the-art learning to rank (L2R) algorithms on such features we immediately obtained significant improvements, increas-ing more than three times the search effectiveness of state-of-the-art WAIR. Second, web archives preserve many years of collected web snapshots, but current WAIR approaches ig-nore the time dimension in such collections. We researched what relevant information to WAIR can be extracted from this time dimension, by exploring, for the first time, the long-term persistence of web documents. In our experi-ments, conducted over 14 years of web snapshots, we found that for navigational queries, relevant documents tend to have a longer lifespan and more versions. This result enabled us to obtain significant gains by modeling the persistence of web documents into novel ranking features. These features are especially important in web archives, because the query-independent features typically used to identify popular or important documents based on click-through data and the web-graph, are not available in this context. Web archives receive a much smaller volume of queries and clicks than web search engines, and the web-graphs are sparser since only a small part of the web is commonly collected and pre-served by each archive. Third, the characteristics of the web vary over time. For instance, the sites in the 90s did not have the richer layouts and more interactive interfaces of the early 00s with CSS and JavaScript. Other examples include the dynamics of the web link structure, which grows following a power law [19], and the dynamics of language in web contents, which have many terms appearing and dis-appearing every year [29]. We believe that a single general ranking model cannot predict the variance of web charac-teristics over such long periods of time. As a result, we present as the main contribution of this paper, an approach that learns and combines multiple ranking models specific for each period. Experimental results show that our ap-proach outperforms the search effectiveness of single-model approaches that fit all data independently of when it was created or updated. We refer to our approach as temporal-dependent ranking.
 The remainder of this paper is organized as follows. In Section 2, we cover the related work. Section 3 analyzes the long-term web document persistence, while a temporal-dependent ranking framework is proposed in Section 4. In Section 5, we present the experimental setup and report the results in Section 6. Section 7 finalizes with the conclusions and future work.
Much of the current effort on web archive development focuses on acquiring, storing, managing and preserving data [22]. However, the data must also be easily accessible to users who need to exploit and analyze them. Full-text search has become the dominant form of information access, espe-cially in web search systems, such as Google, which has a strong influence on the way users search in other settings. Surveys indicate that full-text search is also the preferred tool for accessing web archive data and the most used when supported [7]. Even with the high computational resources required for this purpose, 67% of world-wide web archives support full-text search for at least a part of their collec-tions [13]. However, the large majority of web archives that support full-text search are based on the Lucene search en-gine 2 or extensions of Lucene to handle web archives, such as NutchWAX 3 . The search services provided by these web archives are visibly poor and frequently deemed unsatisfac-tory [13]. An WAIR evaluation confirmed the low quality of search results retrieved with such technology [8]. This last work, like ours, focus in navigational queries, since this is the main information need of web archive users [6].
Some works leveraged temporal information to improve full-text search results of web search engines. One of the most common ideas is incorporating in language models the heuristic that the prior probability of a document being rel-evant is higher in the most recent documents [20]. Boosting the most recent documents is desirable for queries where a user intends to find recent events or breaking news. The distribution of the documents X  dates can also be explored, since it reveals time intervals that are likely to be of interest to the query [16]. For instance, when searching for tsunami , the peaks in the distribution may indicate when tsunamis oc-curred. Another idea is to favor more dynamic documents, since documents with higher relevance are more likely to change or change to a greater degree [10]. More popular and revisited documents are also more likely to change [1]. On the other hand, the most persistent terms are descrip-tive of the main topic and likely added early in the life of a document [1]. These persistent terms are especially use-ful for matching navigational queries, because the relevance of documents for these queries are expected to not change over time. To the best of our knowledge, we are the first studying the relation between long-term web document per-sistence and relevance for improving search effectiveness.
The L2R framework learns ranking models that fit all training data. However, a generic model is not always the best solution and may be overcome by a criteria-dependent model. For instance, Kang and Kim automatically classified queries and created a ranking model for each query type [17]. However, it is often hard to classify a given web search query due to its small number of terms, which makes this technique unfeasible in some cases or imprecise when the wrong model is chosen. To avoid the misclassification prob-lem, Geng et al. created a ranking model for each query q by using the k-nearest training queries of q measured by the similarity of their feature values [12]. The query fea-ture values were computed as the mean of the feature val-ues of the top search results ranked by a reference model (BM25). However, the training time required to create all these models is quite large and each model is learned with just a part of the training data. Bian et al. employed a clustering method to identify a set of query topics based on features extracted from the top search results [3]. Then, a ranking model was learned for each query topic. Each query contributed to learn each model according to the similarity between the query and the respective topic. Dai et al. fol-lowed this work, but integrated freshness with relevance to simultaneously optimize both [9]. In a different work, Salles et al. created a classification model per time interval and weighted them based on the temporal distance between the creation time of documents and the interval [28]. Our work is the first that learns ranking models taking into account the specificities of each time period.
Most ranking models have a static view of web documents and only consider their last version. We posit that web doc-ument persistence can be used to create discriminative fea-tures for improving the performance of ranking models. In this section, we analyze the correlation between the rele-vance of web documents and their long-term persistence.
We chose for this analysis a test collection built for WAIR research [8]. The general statistics are detailed in Table 1. This collection includes a corpus with 269 801 assessed web document versions using a three-level scale of relevance (not-Figure 1: Distribution of the lifespan of documents in years. relevant, relevant and very relevant). The assessed docu-ment versions were returned by different ranking models in response to 50 navigational queries randomly sampled from the logs of a public web archive. This selection strategy enables to get a high coverage of relevant documents, es-pecially because navigational queries tend to have only one (very) relevant document. The queries have 2.23 terms on average and 1/3 are restricted by date range.

The documents range over a period of 14 years, from 1996 to 2009. Such characteristics make this collection unique to study long-term persistence of web documents and their re-lation to relevance ranking. For instance, to study content change, Elsas and Dumais used a collection of 2 million doc-uments crawled for a period of 10 weeks [10], Adar et al. used 55 thousand documents crawled during 5 weeks [1], Fetterly et al. crawled 150 million documents over a period of 11 weeks [11] and Ntoulas et al. 150 web sites over the course of 1 year [24]. These are much shorter periods of analysis not so adequate to this study.
We analyzed the persistence of web documents measured by their lifespan (i.e. difference in days between the first and last versions) and their number of versions. For simpli-fication, we identified the versions of a URL by comparing their MD5 checksums.
 Figure 1 shows the lifespan distribution of web documents. Around 36% of documents have less than one year and hence, we assigned a lifespan of 0 years. This percentage is inferior to the 50% reported by Ntoulas et al. [24]. 14% have a lifespan between 1 and 2 years and near 8% have a lifespan longer than 10 years. Figure 2 shows the distribution of the number of versions of documents. Around 36% have just 1 version, 29% have between 2 and 10, and 35% have more than 10.

The lifespan and number of versions present different dis-tributions. While the number of versions fits a logarith-Figure 2: Distribution of the number of versions of documents over 14 years. mic distribution, the lifespan resembles a long tail distri-bution. When inspecting the documents, we saw that the document with most versions is the homepage of a news-paper ( http://www.correiomanha.pt/ ) with 1301 versions and a lifespan of 12 years and a half. The document with the longest lifespan contains a list of scientific books for the younger ( http://nautilus.fis.uc.pt/softc/Read_c/ l_infantis/infantis.html ) with a lifespan of 13 years and 2 months, but with just 8 versions. While all the documents with the highest number of versions have a long lifespan, the opposite is not true. In fact, the top ten documents with the longest lifespans have less than 15 versions. The Pearson correlation coefficient between the number of ver-sions and the lifespan of web documents is 0.52.
We found some interesting patterns when analyzing the relationship between the long-term persistence of web doc-uments and their relevance. Figure 3 shows the fraction of documents that have a lifespan longer than 1 year for each relevance level, i.e. the number of documents with a given relevance level and a lifespan longer than 1 year, divided by the total number of documents with that same relevance level. The figure shows that these documents are likely to have a higher relevance. The same correlation exists for documents between 1 and 5 years. The percentage of very relevant documents with more than 5 years is only 1% of the total documents for the 50 queries analyzed, which makes it difficult to identify any meaningful correlation. Neverthe-less, the sum of the relevant and very relevant fractions of documents is always superior to the not-relevant when con-sidering the documents with a lifespan longer than 1 year. This indicates that the relevant documents tend to have a longer lifespan.

Figure 4 shows the fraction of documents that have more than 10 versions for each relevance level. These documents tend to have a higher relevance, such as the documents be-tween 1 and 30 versions. The percentage of very relevant documents with more than 30 versions is only 1% of the total documents for the 50 queries analyzed. The 1% is the threshold where once again the correlation starts to be insignificant. However, the sum of the relevant and very relevant fractions of documents is always superior to the not-relevant when considering until 300 versions. After this number, the 4% of remaining documents present a differ-ent pattern. Even so, in general, these results indicate that relevant documents tend to have more versions. Figure 3: Fraction of documents with a lifespan longer than 1 year for each relevance level. Figure 4: Fraction of documents with more than 10 versions for each relevance level.
The lifespan and number of versions of documents are not correlated between them, but both are correlated with the relevance of documents. Hence, to leverage this correlation we modeled these measures of persistence with a logarithmic function that gives a higher score to: (1) documents with a longer lifespan; (2) documents with more versions. Both are defined by the same function: where, for the first case, x is the number of days between the first and last versions of document d , and for the second case, x is the number of versions of document d . The y is the maximum possible x for normalization. This is just an example of a function that can be used to create ranking features, such as these two features that we will use ahead in this study.
In this section, we present our temporal-dependent rank-ing framework for improving search effectiveness. First, we formalize the ranking problem. Second, we explain how to divide the training data by time, and third, how to use these data to create temporal-dependent models. Fourth, we de-scribe how to learn all models simultaneously and how to combine them to produce a final ranking score. Last, we present how to implement our framework.
The traditional ranking problem is to find a ranking model f with parameters  X  that receives X as input, where X is an m  X  d matrix of m query-document feature vectors of size d . This model f produces a vector  X  y of m ranking scores, one per query-document pair &lt; q,d &gt; , trying to predict the real relevance of document d for query q :
Manually finding and optimizing f is a laborious and prone to error work, especially when f combines multiple features. Hence, L2R algorithms automatically learn the best model  X  f , such that  X  f minimizes the given loss function L : where X i represents the i th query-document feature vector and y i the corresponding relevance label. As Eq. 3 shows, the typical L2R outcome is a single general model that ranks documents independently when they were created or up-dated.
Contrary to the traditional ranking problem, we learn multiple ranking models, each taking into account the spe-cific characteristics of a period. In order to achieve that, we first identify a set of temporal intervals T = { T 1 ,T 2 from which we then learn multiple ranking models M = { M 1 ,M 2 ,...,M n } . Each interval T k has associated a set of query-document feature vectors for training, where each fea-ture vector i belongs to T k if and only if the timestamp of the respective document version t i  X  T k .

There are several timestamps associated to a document version, such as the dates of creation, modification, crawl-ing or archiving. The creation and modification dates are good choices, since they refer to the time when a version was created. However, identifying them is not straightfor-ward. The metadata from the document X  X  HTTP header fields, such as Date, Last-Modified and Expires are not al-ways available, nor reliable. Studies estimate that from 35% to 64% of web documents have valid last-modified dates [14], but these percentages can be significantly improved by us-ing the dates of the web document X  X  neighbors, especially of web resources embedded in the selected document (e.g. im-ages, CSS, JavaScript) [25]. Nevertheless, for simplification, in this work we adopted the crawling date.
It is hard to establish clear temporal boundaries in web data, because the ranking features tend to change gradu-ally over time rather than abruptly. Thus, a model M k is learned using all training instances of all intervals T , but each training instance contributes with a different weight to the learning of M k . The instances of interval T k contribute with a maximum weight, while the instances of other in-tervals T j 6 = T k contribute with a weight defined by their temporal distance to T k . Consider Figures 5(a), 5(b) and 5(c) as illustrative examples. They depict the weights of a collection with web snapshots between time points t t . Let X  X  assume that we want to create 3 different mod-els, M = { M 1 ,M 2 ,M 3 } , taking into account the different characteristics of the web snapshots over time. For that, we divide the collection in 3 time intervals T = { T 1 ,T training instances of interval T 1 , such as v 1 , are used with Figure 5: Weights of training instances, such as v 1 , v and v 3 , when learning ranking models (a) M 1 , (b) M 2 and (c) M 3 . weight 1 when learning M 1 , while the other instances receive a weight that decreases as the timestamps of the instances move away from T 1 , such as v 2 and v 3 . Figures 5(b) and 5(c) show the values returned by temporal weight functions when learning M 2 and M 3 , respectively.

Contrary to typical learning to rank, our goal is to learn the best model  X  f for a temporal interval T k , such that minimizes the following loss function L : where  X  is the temporal weight function. We can adopt several  X  functions with the underlying idea that the weight decreases as the temporal distance increases, such as the following function: where distance ( X i ,T k ) is the absolute difference between the date of document version in X i and the closer date to interval T k , i.e. to the begin or end of T k . | T | denotes the total time covered by the collection. The  X  function may have a larger or a smaller slope  X  to learn ranking models with higher or lower contribution of the training instances. For instance, by having a  X  of 2 instead of 1, the ranking model will be learned with half the contribution of the train-ing instances and will ignore the instances in the half most distant intervals.
A temporal-dependent model has two advantages over a model that only learns from data of a segment of time. First, solutions where each model learns from a part of the training data tend to present bad performance results, because more data usually beats better machine learning algorithms [2]. Thus, each temporal-dependent model considers all training instances during learning, avoiding the problem of the lack of data. Second, a temporal-dependent model considers the dependency between datasets of different temporal intervals. A model will learn more from instances of closer intervals than from instances of intervals more far apart.

Another important aspect is that we want to minimize the overall prediction error of all temporal-dependent mod-els, since all will be employed to rank query results. Hence, we minimize a global relevance loss function, which evaluates the overall training error, instead of minimizing multiple in-dependent loss functions without considering the correlation and overlap between models, i.e. instead of minimizing Eq. 4 for each model, we minimize:  X  f 1 ,...,  X  f n = arg min where n is the number of temporal-dependent ranking mod-els. The minimization of this global loss function enables learning all models simultaneously to optimize a unified rele-vance target. Notice that each training instance X i is shared by each model f j and the closer the time interval T j to X the greater this sharing. Models based on data learned from time intervals far apart, will share little or no information of X . This is important for distant time intervals do not end up influencing negatively each other.

After learning all temporal-dependent models, we employ an unsupervised ensemble method to produce the final rank-ing score. We run each of the n ranking models f j against a testing instance X i multiplied by its temporal weight  X  to the corresponding interval T j . Then, we sum all scores produced by all ranking models:
This ensemble method follows the global loss function (Eq. 6) used in the learning phase, trying to minimize the overall prediction error and improve the final search effectiveness.
Our temporal-dependent ranking framework is quite flex-ible and can be implemented using different L2R algorithms as long as we adapt them to use the global loss function of Eq. 6. We followed the work of Bian et al. and adapted the RankSVM algorithm [3].

The goal of RankSVM is learning a linear model that min-imizes the number of pairs of documents ranked in the wrong relative order [15]. Formally, RankSVM minimizes the fol-lowing objective function: where X q i X q j implies that document i is ranked ahead of document j with respect to query q . C is a trade-off coeffi-cient between the model complexity ||  X  || 2 and the training
We modified the objective function of RankSVM following our global loss function, which takes into account the feature specificities of web snapshots over time. Each temporal-dependent ranking model M k is learned by minimizing the following objective function: s.t.
This section presents our experimental setup that enabled us to answer the following questions: 1. How much can we improve the search effectiveness of 2. Do temporal features intrinsic to web archives improve 3. Does our temporal-dependent ranking framework de-
Next, we give a brief description of the L2R dataset and the ranking features used in the experiments. Then, we present the compared ranking algorithms and models, and for last, the evaluation methodology and metrics.
The L2R dataset is composed by a set of &lt; query, docu-ment version, grade, features &gt; quadruples, where the grade indicates the relevance degree of the document version to the query. The features represent a vector of ranking fea-ture values, each describing an estimate of relevance for the &lt; query, document version &gt; pair.

From the 269 801 &lt; query, document version &gt; pairs as-sessed in the test collection described in Section 3.1, we ex-tracted 39 608 quadruples with 68 features. This is the size of the dataset, which has on average 843 versions per query. 3 queries were excluded from the 50, because their relevant and very relevant versions did not contain all features.
Table 2 shows the distribution of relevance judgments per relevance grade. As expected, the number of relevant and very relevant versions is much less than the not-relevant. No-tice that for each of these navigational queries there is usu-ally only one very relevant version and/or one relevant ver-sion. The dataset is publicly available for research at http: //code.google.com/p/pwa-technologies/wiki/L2R4WAIR .
The effectiveness of the ranking models greatly depends on the quality of the features they use. We give an overview of the classes of the 68 features released in the L2R dataset. Table 2: Distribution of relevance judgments in the L2R dataset per relevance grade.
 Each class explores a different type of data: term-weighting features estimate the similarity between the query and the different sections of a document version (anchor text of incoming links, text body, title and URL), such as Okapi BM25 [27]. term-distance features use the distance between terms in the different sections of a document version to quantify the relatedness between them, such as the Minimal Span Weighting function [23].
 URL features compute an importance measure based on the probability of URLs representing an entry page, using the number of slashes, their length, or if they refer to a domain, sub-domain or page [18]. web-graph features estimate the popularity or importance of a document version inferred from the graph of hyper-links between versions. These features include the number of inlinks to a version. temporal features consider the time dimension of the web.
They include the age of a document version and the two features described in Section 3.4 based on the long-term persistence of web documents.

Some of these features are typically used in web search engines and their results have been proven over time. The temporal features, however, were implemented specifically for this research. The complete list of features can be con-sulted online 4 . All feature values were scaled to a range between 0 and 1 using a min-max normalization.
The way L2R algorithms learn can be categorized into three approaches: pointwise, pairwise and listwise [21]. We employed three state-of-the-art L2R algorithms that cover the three approaches: pointwise: Random Forests consists of multiple regression trees, where each tree is built from a bootstrap sample of the training data and a random subset of features is selected to split each node of a tree [4]. The relevance score of each document is the average of the outputs of the individual regression trees. pairwise: RankSVM which is described in Section 4.5. listwise: AdaRank is a boosting algorithm that linearly com-bines  X  X eak learners X , which are iteratively selected as the feature that offers the best performance among all others [30]. Each new learner focus on the queries not ranked well on previous iteration, by giving more weight to them. RankSVM and AdaRank produce linear models, while Random Forests produce nonlinear models. In all experi-ments we used the RankSVM implementation available in the SVM rank software 5 and the implementation of the other two L2R algorithms in the RankLib software 6 .
To compare the search effectiveness of the proposed ap-proaches we evaluated the following ranking models: 1. Models with manually tuned features: these are baseline models. For comparison we included the results of three ranking models with manually tuned features, obtained from a related work [8]. The first model is the Okapi BM25 with parameters k1=2 and b=0.75 [27].
The second is Lucene X  X  term-weighting function 7 , which is computed over five fields (anchor text of incoming links, text body, title, URL and hostname of URL) with dif-ferent weights. The third is a small variation of Lucene used in NutchWAX, with a different normalization by field length. These last two models can be considered the state-of-the-art in WAIR, since the most advanced IR technol-ogy currently used in web archives is based on the Lucene and NutchWAX search engines [13]. 2. Models with regular features combined with L2R: these are another class of baseline models, but based on the technology usually employed in web search engines. These models contain all ranking features described in
Section 5.2, except the temporal features. The regular features were automatically combined using the L2R al-gorithms to create a single ranking model. These models are denoted as the single-model approach with regular features. 3. Models with all features combined with L2R: these are the same models as in the previous point, but with all ranking features, regular and temporal. All these features were automatically combined by L2R algorithms to create a single ranking model. We refer to these models as the single-model approach with all features. 4. Models with regular features combined with the temporal-dependent ranking framework: unlike the previous models created independently of the time of each document version, these ranking models were created us-ing the temporal-dependent ranking framework proposed in Section 4. The framework used equal intervals of time with an approximate number of training instances. The models only contain regular features. 5. Models with all features combined with the tempo-ral-dependent ranking framework: these are the same models as in the previous point, but with all ranking fea-tures, regular and temporal.
We chose a five-fold cross-validation to compare the aver-age performance of the different ranking models. The L2R dataset was divided in five folders, where each folder has three subsets: training, validation and testing. Each rank-ing model was created, for each folder, using the training data. The validation data was used to tune the parameters of the L2R algorithms and the test data was used only on the evaluation of the model to avoid overfitting. The final results are the averages of the five tests.

Each of the 50 evaluated navigational queries may have one very relevant version and several relevant versions. Con-sidering this fact, the ranking models were evaluated with two of the most used evaluation metrics: Precision at three cut-off values (P@1, P@5 and P@10) and the Normalized Discount Cumulative Gain at the same three cut-off values (NDCG@1, NDCG@5 and NDCG@10). P @ k measures the relevance of the top k document versions in a ranking list with respect to a query and is calculated as follows: where r(i) is the relevance of the document version ranked at position i . Precision works over binary judgments. Due to that, the very relevant and relevant judgments were both taken as relevant when using P@k.

NDCG@k handles multiple levels of relevance and gives a higher score to relevant documents in higher ranking posi-tions. It is calculated as follows: where Z k is a normalization constant for the perfect list to get a NDCG@k of 1.

The past experience in web archive assessment has shown that users do not want to see multiple versions of a URL on the search results, but rather only one URL with a link to a list of all the other versions of that URL [8]. This corresponds to the common behavior already implemented in the user interfaces of existing WAIR systems. Hence, we evaluate only the first document version shown in the search results and ignore all the other versions of the same URL, before applying P@k or NDCG@k.
In this section we report and discuss the results of the tested ranking models, summarized in Table 3.

The NutchWAX model performs better than the Lucene and BM25 models. However, its performance is significantly worse than the models produced by the L2R algorithms us-ing regular features. For instance, the model produced with the Random Forests algorithm, which presents the best re-sults of the three L2R algorithms, has a NDCG@10 of 0.650, while NutchWAX gets 0.174. This is more than a three times increase. All models derived from L2R algorithms achieved better results than NutchWAX in all metrics with a statisti-cal significance of p &lt; 0.01 using a two-tailed paired Student X  X  t-test. This strongly indicates, as expected, that the use of L2R with ranking features typically used in web search en-gines, improves the search effectiveness of web archives, but also that the commonly used WAIR engines have a quite poor performance.

All previous models are baselines. Hence, we compared only against the strongest baseline, i.e. the models with reg-ular features combined with L2R algorithms. We analyzed the discriminative power of the temporal ranking features by running the L2R algorithms with and without these fea-tures. We can see a clear pattern. The L2R algorithms almost always present statistically significant improvements for all metrics when using the temporal features. For in-stance, Random Forests has a NDCG@1 superior in 10% to the same algorithm learning without the temporal features and RankSVM increased 3 percentage points. Therefore, the temporal features intrinsic to web archives can be used to improve WAIR.
Finally, we analyzed the single-model approach versus the temporal-dependent ranking framework, with and without temporal features. Figures 6 and 7 show the NDCG@1, NDCG@5 and NDCG@10 values obtained with the temporal-dependent ranking framework, when using regular features or all features. We tested the framework with different time intervals (1, 2, 4, 7 and 14) and different slopes  X  in the temporal weight function (0.25, 0.5, 0.75, 1, 1.25 and 1.5). Notice that we used a test collection with 14 years of web snapshots. Thus, when we use 14 or 7 time intervals, it means that a model is created for each year or two years, respectively. The use of 1 time interval is similar to creating just one model, i.e. the single-model approach.

The results show that the proposed temporal-dependent ranking framework outperforms the single-model approach, with and without temporal features. We achieved improve-ments for all time intervals, but the highest improvements were obtained when we used 4 or 7 intervals. Results de-picted in Figure 6 without temporal features, show that the major increase for NDCG@1 was from 0.500 to 0.560 (+6%) when using 4 and 7 intervals, while for a NDCG@5 was from 0.485 to 0.551 (+6.6%) and for NDCG@10 was from 0.523 to 0.572 (+4.9%), both when using 4 intervals. Re-sults depicted in Figure 7 with temporal features, show that the major increase for NDCG@1 was from 0.530 to 0.590 (+6%) when using 7 intervals, while for a NDCG@5 was from 0.546 to 0.583 (+3.7%) and for NDCG@10 was from 0.571 to 0.604 (+3.3%), both when using 4 intervals. All these improvements, which present a statistical significance (p &lt; 0.05), indicate that the values of the ranking features change considerably over time in a way that can be learned by ranking models to better differentiate between relevant and not-relevant documents.

The slope  X  of the temporal weight function in Eq. 5 has an important impact in the final results. We obtained the worst results when  X  is larger than 1, i.e. when the con-tribution of the training instances is smaller. On the other hand, a small  X  , such as 0.25, caused a larger than desired contribution of the training instances. The best results were achieved with  X  between 0.5 and 1.

The temporal features and the temporal-dependent rank-ing framework, are independent approaches that demon-strate promising results. However, both approaches also work well together. In fact, the results displayed in Figure 7 show that we achieved the best results when we combined Length of the shortest text with all query terms in title Table 4: Top 6 most important ranking features for the temporal-dependent ranking framework. them. The NDCG@1, NDCG@5 and NDCG@10 are supe-rior in 9%, 10% and 8%, respectively, over the single-model approach using just regular features.
We analyzed why the temporal-dependent ranking frame-work produces better results than the typical single rank-ing model created by L2R algorithms. We sorted the rank-ing features by their importance, measured by the absolute weight assigned by RankSVM. We found that the top fea-tures are almost the same, whether using just one model or multiple temporal-dependent models. The difference be-tween ranking models created for different time intervals lies on small changes in the weights of features. This finding cor-roborates our observations that the characteristics of web documents evolve smoothly rather than abruptly and the temporal-dependent ranking models can adjust the feature weights to provide fine-grained ranking over time.
Table 4 shows the top 6 most important ranking features for the temporal-dependent ranking framework. From this table, we can see that BM25 and TF-IDF over all fields are the features with higher weight. The features based on long-term persistence of web documents, using the number of versions and the number of days between the first and last versions, are also at the top. RankSVM weighted some of these as the best features to identify relevant document versions for navigational queries.
The retrieval effectiveness of state-of-the-art WAIR sys-tems is poor, preventing users from unfolding the full po-tential of web archives. This work made a few contributions to face this problem. We studied, for the first time, the effects of long-term web document persistence in relevance ranking. In our experiments, conducted over 14 years of
NDCG@1
NDCG@5
NDCG@10 Figure 6: (a) NDCG@1, (b) NDCG@5 and (c) NDCG@10 results of the temporal-dependent rank-ing framework using different time intervals and  X  values of the temporal weight function. These mod-els contain regular features. web snapshots, we found that relevant documents tend to have a longer lifespan and more versions. Significant gains were achieved by modeling these persistence characteristics of web documents as novel ranking features. Additionally, since the characteristics of the web vary over time, both in structure and content, we proposed a temporal-dependent ranking framework that learns a different ranking model for each successive web period. Our experimental results show that the proposed multi-model framework outperforms a simpler approach based on a single ranking model, when both use the same L2R algorithms.

This work is focused in WAIR, but we believe that our approach could bring similar improvements to any digital libraries dealing with versioned content spanning long peri-ods. As future work, we intend to study the evolution of URLs over time and their impact on search, since we de-tected changes in many URLs X  top-level domains and sub-
NDCG@1
NDCG@5
NDCG@10 Figure 7: (a) NDCG@1, (b) NDCG@5 and (c) NDCG@10 results of the temporal-dependent rank-ing framework using different time intervals and  X  values of the temporal weight function. These mod-els contain regular and temporal features. domains. By tracking this evolution, we can better measure the long-term persistence of web documents. We also plan to investigate how to extend the temporal-dependent ranking framework to handle temporal diversity in search results. We thank FCT for the financial support of the Research Units of LaSIGE (PEst-OE/EEI/UI0408/2014) and INESC-ID (Pest-OE/EEI/LA0021/2013), and the DataStorm Re-search Line of Excellency (EXCL/EEI-ESS/0257/2012). [1] E. Adar, J. Teevan, S. Dumais, and J. Elsas. The web [2] M. Banko and E. Brill. Scaling to very very large [3] J. Bian, X. Li, F. Li, Z. Zheng, and H. Zha. Ranking [4] L. Breiman. Random forests. Machine learning , [5] Y. Chung, M. Toyoda, and M. Kitsuregawa. A study [6] M. Costa and M. J. Silva. Understanding the [7] M. Costa and M. J. Silva. Characterizing search [8] M. Costa and M. J. Silva. Evaluating web archive [9] N. Dai, M. Shokouhi, and B. Davison. Learning to [10] J. Elsas and S. Dumais. Leveraging temporal [11] D. Fetterly, M. Manasse, M. Najork, and J. L. Wiener. [12] X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and [13] D. Gomes, J. Miranda, and M. Costa. A survey on [14] D. Gomes and M. Silva. Modelling information [15] T. Joachims. Optimizing search engines using [16] R. Jones and F. Diaz. Temporal profiles of queries. [17] I. Kang and G. Kim. Query type classification for web [18] W. Kraaij, T. Westerveld, and D. Hiemstra. The [19] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph [20] X. Li and W. B. Croft. Time-based language models. [21] T. Liu. Learning to rank for information retrieval , [22] J. Masan`es. Web Archiving . Springer-Verlag New York [23] C. Monz. Minimal span weighting retrieval for [24] A. Ntoulas, J. Cho, and C. Olston. What X  X  new on the [25] S. Nunes, C. Ribeiro, and G. David. Using neighbors [26] K. Radinsky and E. Horvitz. Mining the web to [27] S. Robertson and H. Zaragoza. The probabilistic [28] T. Salles, L. Rocha, G. L. Pappa, F. Mour  X ao, [29] N. Tahmasebi, G. Gossen, and T. Risse. Which words [30] J. Xu and H. Li. Adarank: a boosting algorithm for [31] Y. Yamamoto, T. Tezuka, A. Jatowt, and K. Tanaka.
