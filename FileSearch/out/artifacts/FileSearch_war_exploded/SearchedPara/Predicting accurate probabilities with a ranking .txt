 Aditya Krishna Menon 1 AKMENON @ UCSD . EDU Charles Elkan 1 ELKAN @ UCSD . EDU Lucila Ohno-Machado 1 MACHADO @ UCSD . EDU University of Toronto, 160 College Street, Toronto, ON M5S 3E1, Canada Classification is the problem of learning a mapping from examples to labels, with the goal of categorizing future ex-amples into one of several classes. However, many real-world applications instead require that we estimate the probability of an example having a particular label. For ex-ample, when studying the click behaviour of ads in compu-tational advertising, it is essential to model the probability of an ad being clicked, rather than just predicting whether or not it will be clicked (Richardson et al., 2007). Accurate probabilities are also essential for medical screening tools to trigger early assessment and admission to an ICU (Subbe et al., 2001).
 In this paper, we propose a simple semi-parametric model for predicting accurate probabilities that uses isotonic re-gression in conjunction with scores derived from optimiz-ing a ranking loss . We analyze theoretically and empiri-cally where our approach can provide more reliable esti-mates than standard statistical workhorses for probability estimation, such as logistic regression. The model attempts to achieve good ranking (in an area under ROC sense) and regression (in a squared error sense) performance simul-taneously, which is important in many real-world appli-cations (Sculley, 2010). Further, our model is much less expensive to train than full-blown nonparametric methods, such as kernel logistic regression. It is thus an appeal-ing choice in situations where parameteric models are em-ployed for probability estimation, such as medical infor-matics and credit scoring.
 The paper is organized as follows. First, we provide mo-tivating examples for predicting probabilities, and define the fundamental concept of proper losses. We then review existing methods used to predict probabilities, and discuss their limitations. Next, we detail our method to estimate probabilities, based on optimizing a ranking loss and feed-ing the results into isotonic regression. Finally, we provide experimental results on real-world datasets to validate our analysis and to test the efficacy of our method.
 We first fix our notation. We focus on probability estima-tion for examples x  X  X with labels y  X  X  0 , 1 } . Each x has a conditional probability function  X  ( x ) : = Pr [ y = 1 | x ] . For our purposes, a model is some deterministic mapping  X  s :
X  X  R . A probabilistic model  X   X  is a model whose out-puts are in [ 0 , 1 ] , and may be derived by composing a model with a link function f : R  X  [ 0 , 1 ] . The scores of a model may be thresholded to give a classifier  X  y : X  X  X  0 , 1 } . We assume  X  s is learned from a training set { ( x i , y i ) } draws from X  X { 0 , 1 } . Classically, the supervised learning literature has focussed on the scenario where we want to minimize the number of misclassified examples on test data. However, practical applications of machine learning models often have more complex constraints and requirements, which demand that we output the probability of an example possessing a label. Examples of such applications include: Building meta-classifiers , where the output of a model is fed to a meta-classifier that uses additional domain knowl-edge to make a prediction. For example, doctors prefer to use a classifier X  X  prediction as evidence to aid their own decision-making process (Manickam &amp; Abidi, 1999). In such scenarios, it is essential that the classifier assess the confidence in its predictions being correct, which may be captured using probabilities; Using predictions to take actions , such as deciding whether or not to contact a person for a marketing cam-paign. Such actions have an associated utility that is to be maximized, and maximization of expected utility is most naturally handled by estimating probabilities rather than making hard decisions (Zadrozny &amp; Elkan, 2001); Non-standard learning tasks , where problem constraints demand estimating uncertainty. For example, in the task of learning from only positive and unlabelled examples, train-ing a probabilistic model that distinguishes labelled versus unlabelled examples is a provably (under some assump-tions) sufficient strategy (Elkan &amp; Noto, 2008). Intuitively, probability estimates  X   X  (  X  ) are accurate if, on av-erage, they are close to the true probability  X  (  X  ) . Quantify-ing  X  X lose to X  requires picking some sensible discrepancy measure, and this idea is formalized by the theory of proper loss functions, which we now discuss. A model for binary classification uses a loss function ` : { 0 , 1 } X  R  X  R + measure the discrepancy between a label y and the model X  X  prediction  X  s for some example x . If our model outputs probability estimates  X   X  by transforming scores with a link function f (  X  ) , we may equivalently think of there being a probabilistic loss ` P (  X  ,  X  ) such that ` ( y ,  X  s ) = ` empirical error of  X  s with respect to the loss ` is which is a surrogate for the generalization error The term L ` (  X  ,  X  s ) is a measure of discrepancy between an example X  X  probability of being positive and its pre-dicted score. Let s  X  (  X  ) = argmin s L ` (  X  , s ) . Then, we call a loss function ` Bayes consistent (Buja et al., 2005) if for every  X   X  [ 0 , 1 ] , s  X  (  X  )  X  (  X   X  1 2 )  X  0, meaning that we have the same sign as the optimal prediction under the 0-1 loss ` ( y ,  X  s ) = 1 [ y  X  s  X  0 ] . If s  X  then ( s  X  )  X  1 ( s  X  (  X  )) =  X  , so that the optimal scores are some transformation of  X  ( x ) . In such cases, we call the corresponding probabilistic loss ` P a proper (or Fisher-consistent ) loss (Buja et al., 2005), and say that ` corre-sponds to a proper loss.
 Many commonly used loss functions, such as square e a model with good regression performance according to squared error, say, can be thought to yield meaningful probability estimates. The hinge loss of SVMs, ` ( y ,  X  s ) = max ( 0 , 1  X  ( 2 y  X  1 )  X  s ) , is Bayes consistent but does not cor-respond to a proper loss function, which is why SVMs do not output meaningful probabilities (Platt, 1999). We now analyze two major paradigms for probability esti-mation, and study their possible failure modes. 3.1. Optimization of a proper loss A direct approach to predicting probabilities is to optimize a proper loss function on the training data using some hy-pothesis class, e.g. linear separators. Examples include lo-gistic regression and linear regression (after truncation to [ 0 , 1 ] ), which are instances of the generalized linear model framework, which assumes E [ y | x ] = f ( w T x ) for some link function f (  X  ) . The loss-dependent error measure, L ` is one metric by which we can choose amongst proper losses. For example, the discrepancy measures for square and logistic loss are (Zhang, 2004) where KL denotes the Kullback-Leibler divergence, and C , C 2 are independent of the prediction  X  s . Based on this, Zhang (2004) notes that logistic regression has difficulty when  X  ( x )( 1  X   X  ( x ))  X  0 for some x , by virtue of requir-ing |  X  s ( x ) | X   X  . This has been observed in practical uses of logistic regression with imbalanced classes (King &amp; Zeng, 2001; Foster &amp; Stine, 2004), with the latter proposing the use of linear regression as a more robust alternative. 3.2. Post-processing methods A distinct strategy is to train a model in some manner, and then extract probability estimates from it in a post-processing step. Three popular techniques of this type are Platt scaling (Platt, 1999), binning (Zadrozny &amp; Elkan, 2001), and isotonic regression (Zadrozny &amp; Elkan, 2002). We focus on the latter, as it is more flexible than the former two approaches by virtue of being nonparametric, and has been shown to work well empirically for a range of input models (Niculescu-Mizil &amp; Caruana, 2005).
 Isotonic regression is a nonparametric technique to find a monotone fit to a set of target values. In a learning context, the method was used in (Zadrozny &amp; Elkan, 2002) to learn meaningful probabilities from the scores of an input model. Mathematically, suppose we have predictions {  X  s i } n i = 1 some input model, with corresponding true labels { y i } n and WLOG suppose that  X  s 1  X   X  s 2  X  ...  X   X  s n . Then, isotonic regression learns scores {  X  s i } n i = 1 via the optimization This finds the best monotone fit to the training labels (as or-dered by the input model X  X  scores) in a squared loss sense. (In fact, the optimal solution will minimize any proper loss (Br  X  ummer &amp; Preez, 2007).) If the input scores {  X  s sorted, then there is an O ( n ) algorithm to solve this prob-lem, called pool adjacent violators (PAV) (Barlow et al., 1972).
 When y i  X  { 0 , 1 } , it is easy to verify that  X  s that the result is a probabilistic model. Indeed, isotonic re-gression can be thought of as nonparametrically learning a monotone link function f (  X  ) to create a probabilistic model f (  X  s (  X  )) . However, the resulting model is only defined on the training examples, and we need to define some inter-polation scheme to make predictions on future examples. One natural scheme is a linear interpolation between the training scores (Cosslett, 1983). Observe that isotonic re-gression preserves the ordering of the input model X  X  scores, although potentially introducing ties i.e. f (  X  s (  X  )) is not in-jective. To break ties on training examples, we may simply refer to the corresponding original model X  X  scores. Linear interpolation breaks most ties 1 on test examples. 3.3. Possible failure modes There are at least two main reasons why the above paradigms may not yield accurate probabilities: Misspecification . In practice, simple models based on parametric assumptions will often be misspecified: for ex-ample, logistic regression assumes the parametric form  X  ( x ) = 1 / ( 1 + e  X  w T x ) for some w , but this assumption may not always hold. While we cannot learn  X  ( x ) if we can-not represent it in our hypothesis class, Equation 1 says that our model X  X  predictions will in expectation be close to  X  ( x ) according to some discrepancy measure. It is possible for a model like logistic regression to be correctly specified up to the choice of link function, i.e.  X  ( x ) = f ( w T f (  X  ) is not the sigmoid function. The maximum likelihood estimates of a generalized linear model with a misspecified link function are known to be asymptotically biased (Czado &amp; Santner, 1992). Isotonic regression alleviates this partic-ular type of misspecification, but is still vulnerable if its input scores are misspecified.
 A natural defense against misspecification is using a nonparametric method such as kernel logistic regression (KLR). This model will be able to learn any measurable  X  ( x ) with a universal kernel (Zhang, 2004). In many prac-tical applications, such methods are seen as too expensive to both train (requiring O ( n 3 ) time (Zhu &amp; Hastie, 2005)) and test (requiring O ( n ) time to make a prediction, since the weights on training examples generally have full sup-port, unlike a kernel SVM).
 Finite-sample effects . When optimizing an unregularized proper loss on a finite training set of n examples, the prob-ability estimates may be biased. Indeed, the finite sample MLE for the parameters of a generalized linear model (such as logistic regression) have a bias of O ( 1 / n ) (Cordeiro &amp; McCullagh, 1991), and thus the probability estimates are also biased. King and Zeng (2001) show that the constant in the O (  X  ) depends on the imbalance in the classes, mean-ing that logistic regression can give biased probability esti-mates when attempting to model a rare event. It is possible to perform bias correction explicitly via a post-hoc modifi-cation of the learned parameters (King &amp; Zeng, 2001), or implicitly by choosing a Jeffrey X  X  prior regularizer (Firth, 1993).
 Similarly, isotonic regression may overfit even if the input scores give a good ranking on test data. This can happen when there are  X  X aps X  amongst the input scores. The sim-plest example is when the largest input score  X  s sociated with a positive label. Assuming there is only one example with this score, isotonic regression will predict the probability for any test example with score  X   X  s max to be 1, which is too optimistic and will likely be a poor model in this region of input space. The problem arises because we have insufficient representation of scores in [  X  s max , The semi-parametric route of isotonic regression is appeal-ing because it involves a simple post-processing step, while strictly enhancing the hypothesis class of the input model. For this reason, we focus on this semi-parametric paradigm in what follows. Our hope is to design a model that is at least as accurate, and not much more difficult to train than workhorses such as logistic regression.
 To use isotonic regression to get accurate estimates, we must specify what scores we will feed it as input. We may thus ask what characteristics such scores should possess so as to yield accurate probability estimates. We make the simple observation that isotonic regression interacts with the scores of the input model in only one way: it uses them to enforce the monotonicity constraint on the output. Thus, intuitively, isotonic regression will perform well when the (pairwise) ranking of the original scores is good, and so this should be our objective when training our input model. We now attempt to formalize this intuition, and present our proposed method. 4.1. Isotonic regression and ranking performance The real-valued score that a model assigns to each example may be used to rank examples according to confidence of having a positive label. The pairwise ranking performance of a model may be measured using the area under the ROC curve ( AUC ), being the probability that a randomly drawn positive example has a higher score than a randomly drawn negative example. It is formally defined below.
 Definition 1. (Cl  X  emenc  X on et al., 2006) The AUC A (  X  s (  X  )) of a model  X  s : X  X  R is We henceforth think of a model  X  s (  X  ) as equivalently repre-senting a ranker of examples. A natural quantity to study is the model  X  s (  X  ) that induces the Bayes-optimal ranker, optimal ranker to be  X  ( x ) , or some (strictly) monotone transform c (  X  ) thereof, and indeed this may be proven (Cl  X  emenc  X on et al., 2006). Therefore, finding accurate prob-abilities can conceptually be cast as finding accurate rank-ing, and then recovering the correct transformation c (  X  ) . We may now show that isotonic regression applied to a Bayes-optimal ranker (in the sense of AUC performance) will recover the true probabilities, by inferring the c (  X  ) dis-cussed above. This can be proven by observing that iso-tonic regression returns calibrated scores (see e.g. (Kalai &amp; Sastry, 2009) for a proof). Calibration of probability es-timates is defined as follows.
 Definition 2. (Schervish, 1989) We say that a model  X  s is calibrated if, for every  X   X   X  s [ X ] ,  X  = Pr [ y = 1 |  X  s = We now show that calibration and Bayes-optimal AUC per-formance implies accuracy of estimates.
 Proposition 1. Let the model  X  s be a Bayes-optimal ranker, is calibrated,  X  s ( x ) =  X  ( x ) for all x.
 Proof. Recall that for an optimal ranker,  X  s ( x ) = c ( brated, then by definition Any strictly monotone transformation c (  X  ) must have an in-verse c  X  1 (  X  ) . Thus the above may be rewritten as But we know that  X  ( x ) is a calibrated predictor: Therefore, c  X  1 ( s ) = s for all s , meaning c ( s ) = s , and thus,  X  s ( x ) =  X  ( x ) . 4.2. Our proposal: ranking loss + isotonic regression The above suggests a natural idea: directly optimize the AUC on the training set, and post-process its scores with isotonic regression. This can be viewed as learning a model that has good ranking performance (by virtue of first opti-mizing a ranking loss) as well as good probability estima-tion performance (by virtue of isotonic regression optimiz-ing every proper loss). With appropriate handling of ties, isotonic regression enforces strict monotonicity, and so its scores will have the same AUC as the original model. On a finite training set with n + positive and n  X  negative exam-ples, the empirical AUC A emp can be computed as which can be seen to measure the number of concordant pairs in the training set i.e. pairs of examples where the predicted scores respect the ordering according to the label. To maximize AUC, we may follow the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002), which uses a regularized convex approximation to the RHS of Equation 4: min where ` (  X  ,  X  ) is some convex loss function, and  X  (  X  ) is a reg-ularization function with strength  X  &gt; 0. We use a linear scoring function 2 i.e.  X  s ( x ; w ) = w T x , for which the regular-izer is generally taken to be the ` 2 norm 1 2 || w || 2 the above loss function nominally requires O ( n 2 ) time to compute the gradient, clever algorithms can speed this up (Joachims, 2006). Empirically, it has been observed that stochastic gradient descent on the objective converges in a fraction of an epoch (Sculley, 2009). The issue of how best to maximize AUC is not settled. For example, Kotlowski et al. (2011) show that the ranking er-ror (viz. 1  X  A ) of a model can be upper bounded by its balanced logistic loss (viz. the logistic loss balanced by the respective class priors), suggesting that in practice one may approximately maximize AUC using logistic regression. (We say  X  X pproximately X  because the result only provides a lower bound on the resulting AUC.) Consequently, post-processing the output of logistic regression with an isotonic regression fit is a worthwhile strategy to explore, and is in-deed something we look at in our experiments. (Results such as (King &amp; Zeng, 2001) suggest that logistic regres-sion is not appropriate for imbalanced data because its raw probabilities are biased, not its ranking of examples.) 4.3. Justification of model Our model operates by finding some  X  s ( x ) = w T x that op-timizes Equation 5, and then post-processing these scores with isotonic regression. To argue that this model learns something meaningful, we need to show two things: (a) the solution to the convex optimization problem of Equa-tion 5 will (asymptotically) yield a Bayes-optimal ranker, assuming the model is correctly specified, and (b) isotonic regression on top of a Bayes-optimal ranker will recover  X  ( x ) . Point (a) can be established if the underlying clas-sification model uses a universal kernel (Cl  X  emenc  X on et al., 2006). For a linear kernel, this means that we can learn the optimal ranking if the underlying probability is of the form c ( w T x ) for some monotone increasing c (  X  ) . Point (b) was established in Section 4.1, and it is further the case that the isotonic regression estimate on a finite training set is con-sistent, under mild regularity assumptions (Brunk, 1958). If our model is misspecified  X  that is,  X  ( x ) is not a mono-tone transformation of w T x  X  then the above analysis does not hold: the optimal ranker and the optimal regressor within our hypothesis class may be different. We can how-ever show the following weaker result about the empirical squared error resulting from our isotonic regression step. Proposition 2. Suppose a model  X  s : X  X  R has empirical AUC A emp on a training set with empirical base rate  X  Then, there is a model  X  s with the same empirical AUC, and empirical square loss at worst 1 2 p  X   X  ( 1  X   X   X  )( 1  X  Proof. We previously established that isotonic regression will maintain the empirical AUC, and so we focus on the resulting squared error. Recall that the empirical AUC pe-nalizes the number of discordant positive and negative ex-ample pairs. We may rewrite it as A emp = 1  X  k / n + n  X  that there are k discordant pairs. Suppose these pairs arise due to a positive and b negative examples, a  X  n + , b  X  n The worst placement of these pairs is if all the a positives have lower scores than the b negatives. In this case, we have k = ab , and it is easy to check that the resulting square loss is where it attains the value ka  X  by regression is Since the empirical AUC is concentrated around the true AUC (Agarwal et al., 2005), the above is easily extended to a bound in terms of the true AUC. However, this is still a bound on the training squared error, and so is not a true generalization bound. 4.4. Comparison to existing methods The first step of our method attempts to maximize the pair-wise ranking performance, and the isotonic regression step attempts to achieve low squared error. By construction, then, our method attempts to achieve both good ranking and regression (in a squared error sense) performance. Good performance in both metrics is important in many applica-tions, such as computational advertising (Richardson et al., 2007). The idea of learning models with good ranking and regression performance was proposed in the combined re-gression and ranking (CRR) framework of Sculley (2010). A similar model for logistic loss was proposed by Ertekin and Rudin (2011). The basic idea of such an approach is to simultaneously optimize the ranking and regression losses in a parametric manner, by minimizing a linear combina-tion of both losses. The hope is that this yields  X  X est of both worlds X  performance in these objectives. Empirically, Sculley (2010) observed that generally the AUC obtained from such an approach was no worse than that of optimiz-ing the ranking loss alone, while in some cases there was an improvement in the regression performance. By contrast, while we do make a parametric assumption for the ranking loss, our regression component is nonparametric and hence more powerful. Thus, in light of Sculley (2010) X  X  find-ing, we expect to achieve equitable ranking performance to methods like CRR, and better regression performance. As the previous section makes clear, the idea of post-processing scores with isotonic regression is not new. How-ever, to our knowledge, prior work has not studied the im-plications of applying this processing to a model that opti-mizes ranking performance; the idea is hinted at in (Scul-ley et al., 2011), but not discussed formally. Indeed, we argue that the scores from optimizing a ranking loss are the  X  X orrect X  ones to use as input to isotonic regression, in the sense of recovering the true probability when the ranker is correctly specified. (Previous work has looked at applying isotonic regression to a general ranker that assigns scores to pairs of examples (Flach &amp; Matsubara, 2007), but does not specifically consider finding the optimal pairwise ranker.) Our approach is related to the single-index model (Manski, 1975) class of probabilities, Pr [ y = 1 | x ] = f ( w T x ) , where f (  X  ) is an unknown link function, in contrast to a general-ized linear model which assumes a specific link function. The isotonic single-index model is where f (  X  ) is assumed to be monotone increasing. Many existing methods to learn single-index models rely on some form of iteration between optimizing for w and learning f (  X  ) . For example, the recent Isotron algorithm (Kalai &amp; Sastry, 2009) also uses isotonic regression to provably learn single index models, and re-lies on alternately updating w via a perceptron-like update, and running PAV to learn f (  X  ) . Our approach does not have similar generalization bounds, but is more direct and time-efficient, as it requires only a single call to the PAV algo-rithm. Our experiments aim to study the conditions under which our method may improve performance over linear or logis-tic regression, both on synthetic and real-world datasets. 5.1. Methods compared We denote our method by Rank + IR . For comparison, we used linear ( LinReg ) and logistic ( LogReg ) regression, as well as the results of post-processing these methods with isotonic regression. We also used the combined regression and ranking model ( CRR ) of Sculley (2010). We do not post-process CRR because that framework is explicitly de-signed with the aim of providing a good ranking as well as regression, which we would like to compare to our ap-proach; our hypothesis is that our method should provide the most accurate probabilities, while additionally provid-ing an equitable ranking to the CRR model.
 Following Sculley (2010), we use the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002) with lo-gistic loss to optimize for AUC directly, which lends it-self naturally to large-scale implementation using stochas-tic gradient descent. For this and the CRR model, we used the Sofia-ML package 3 . All models were regularized. To test the accuracy of probability estimates, where available, we use the domain-specific metric of interest e.g. overall utility, else we measure the mean squared error between test labels and model predictions. 5.2. Results on synthetic dataset We first study the performance of our proposed method on a synthetic dataset, to see the conditions under which we can expect it to improve performance over existing methods. In particular, we study the performance of various methods where the true probability model is where 0  X  a  X  1 2 controls the floor and ceiling of the prob-ability distribution. Such capped distributions arise in e.g. item response theory (Hambleton et al., 1991), where the probability of a student answering a question correctly is bounded from below by the success rate of random guess-ing. Logistic regression is misspecified for this link, al-though for a = 0 the sigmoid is a reasonable approxima-tion, while for a = 1 2 the probability is independent of x and thus can be modelled entirely by a bias term.
 We proceed as follows: we first pick some value for a , and drawn n samples in R 2 from N ( 0 , I ) . We then draw their corresponding labels, and train the various methods. We then create a separate test set through this same proce-dure, and evaluate the squared error of each model X  X  pre-dictions to the true probabilities of the data points (as op-posed to the labels for these points.) We repeat the process multiple times and find the average error. We do this for a  X  X  2  X  9 , 2  X  7 ,..., 2  X  1 } .
 Our results for n = 1000 samples are shown in Figure 1. As expected, at the endpoints of a  X  0 + and a = 1 2 , we see that there is not much to choose between the methods. However, for intermediate values of a , logistic regression X  X  performance severely deteriorates. Post-processing these scores with isotonic regression reliably estimates the floor and ceiling of the link function, and significantly improves performance. Using our method, where we post-process the scores obtained from a ranking loss, we get a small fur-ther boost in performance. 5.3. Results on real-world datasets We provide experimental results on datasets drawn from the three motivating problems described in Section 2. Hospital Discharge . The first dataset is from medical in-formatics (El-Kareh et al., 2010), where the goal is to pre-dict follow-up errors on microbiology cultures. Predict-ing the probability of an example having a follow-up er-ror helps an expert determine an appropriate action to take. There are 8668 examples with 10 features, and we create 20 random 80  X  20 train-test splits. Table 1 shows that our method does manage to achieve both good regression and ranking performance. Interestingly, isotonic regression slightly worsens the MSE for both linear and logistic re-gression, suggesting that the majority of the error arises from the basic parametric model for ranking examples it-self, rather than the choice of link function.
 KDDCup  X 98 . The second dataset is from the 1998 KDD Cup 4 . Here, the goal is to predict how much a individual will donate, so as to decide whether to contact them for a mail campaign (which costs money). The final utility mea-sure is the expected profit in dollars if one contacts all indi-viduals that the model predicts will donate (the profit takes into account the cost of contacting each individual). The data consists of 95 , 412 training examples and 96 , 367 test examples. We follow the strategy of (Zadrozny &amp; Elkan, 2001): we selected the 15 features it recommends, compute the probability an individual will respond to the campaign, and then compute the expected donation given a response. Table 2 summarizes the utility of the compared methods, as well as the AUC for the label of whether a person donates or not, on the provided test set. Our method gets an addi-tional profit of around $300 over logistic regression, along with a small improvement in AUC. Such additional rev-enue may be important in practice, especially with a larger pool of candidate donors. (Note that IR sometimes modi-fies AUC of the input model; this is because regularization strength is picked based on utility, rather than AUC.) GCAT . Lastly, we consider a classification scenario where the training set comprises only positive and unlabelled data. Based on (Elkan &amp; Noto, 2008), one way to solve this is to predict the probability of an example being labelled, call this Pr [ l = 1 | x ] , based on which we can estimate the proba-bility that it is positive by the identity Pr [ y = 1 | x ] = Pr [ l = 1 | x ] / c , where c = Pr [ l = 1 | y = 1 ] may be estimated by taking the average value of Pr [ l = 1 | x ] on the positive ex-amples. We simulate this scenario on the GCAT dataset 5 , comprising 23 , 149 examples and 47 , 236 features: we con-struct a training set by first picking 30% of the positives (which are assigned a positive label), and then 80% of the other examples (which are treated as unlabelled). We re-port the primary error measures in this problem, MSE and AUC in distinguishing positive versus negative examples. Table 3 summarizes the results from 20 random train-test splits. We see that post-processing logistic regression sig-nificantly improves the MSE performance over logistic re-gression and CRR, indicating the sigmoid link function is misspecified for this problem. Our method manages to fur-ther improve MSE, while achieving equitable ranking to other methods.
 Overall, on all three datasets, we see our method achieves both good ranking and regression performance, and on the KDDCup and GCAT datasets manages to improve overall regression performance. Note that logistic and linear re-gression are strong baselines, and that even small improve-ments in performance may be significant in practical appli-cations (Sculley, 2010). Many real-world applications of predictive models require predicting accurate probabilities of class membership. We studied the principles behind predicting accurate probabil-ities, and proposed a simple method to achieve it. Our method is based on post-processing the results of a model that optimizes a ranking loss with isotonic regression. The model is shown to have good empirical performance. In the future, it would be interesting to study the theoretical prop-erties of the model more closely, and evaluate the model in other scenarios requiring probability estimates.
 XJ and LOM were funded in part by the National Library of Medicine (R01LM009520) and NHLBI (U54 HL10846).
