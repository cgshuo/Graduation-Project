 Quoc Le qvl@google.com Tam  X as Sarl  X os stamas@google.com Alex Smola alex@smola.org Kernel methods are successful techniques for solving many problems in machine learning, ranging from clas-sification and regression to sequence annotation and feature extraction (Boser et al., 1992; Cortes &amp; Vap-nik, 1995; Vapnik et al., 1997; Taskar et al., 2004; Sch  X olkopf et al., 1998). At their heart lies the idea that inner products in high-dimensional feature spaces can be computed in an implicit form via a kernel function k : Here  X  : X  X  F maps elements of the observation space X into a high-dimensional feature space F . Key to kernel methods is that as long as kernel algorithms have access to k , we do not need to represent  X  ( x ) ex-plicitly. Most often, that means although  X  ( x ) can be high-dimensional or even infinite-dimensional, their in-ner products, can be evaluated in an inexpensive man-ner by k . This idea is known as the  X  X ernel trick. X  More concretely, to evaluate the decision function f ( x ) on an example x , one typically employs the kernel trick as follows f ( x ) =  X  w, X  ( x )  X  = This has been viewed as a strength of kernel methods, especially in the days that datasets consisted of ten thousands of examples. This is because the Represen-ter Theorem (Kimeldorf &amp; Wahba, 1970) states that such a function expansion in terms of finitely many coefficients must exist under fairly benign conditions even whenever the space is infinite dimensional. Hence we can effectively perform optimization in infinite di-mensional spaces.
 Unfortunately, on large amounts of data, this expan-sion turns into a significant limitation for computa-tional efficiency. For instance, (Steinwart &amp; Christ-mann, 2008) show that the number of nonzero  X  i (i.e., N , also known as the number of  X  X upport vectors X ) in many estimation problems can grow linearly in the size of the training set. As a consequence, as the dataset grows, the expense of evaluating f also grows. This property makes kernel methods expensive in many large scale problems.
 Random Kitchen Sinks (Rahimi &amp; Recht, 2007; 2008) 1 , the algorithm that our algorithm is based on, approximates the function f by means of multiplying the input with a Gaussian random matrix, followed by the application of a nonlinearity. If the expansion dimension is n and the input dimension is d (i.e., the Gaussian matrix is n  X  d ), it requires O ( nd ) time and memory to evaluate the decision function f . For large problems with sample size m n , this is typically much faster than the aforementioned  X  X ernel trick X  be-cause the computation is independent of the size of the training set. Experiments also show that this approxi-mation method achieves accuracy comparable to RBF kernels while offering significant speedup.
 Our proposed approach, Fastfood, accelerates Random Kitchen Sinks from O ( nd ) to O ( n log d ) time. The speedup is most significant when the input dimen-sion d is larger than 1000, which is typical in most applications. For instance, a tiny 32x32x3 image in the CIFAR-10 (Krizhevsky, 2009) already has 3072 di-mensions (and non-linear function classes have shown to work well for MNIST (Sch  X olkopf &amp; Smola, 2002) and CIFAR-10). Our approach relies on the fact that Hadamard matrices, when combined with Gaussian scaling matrices, behave very much like Gaussian ran-dom matrices. That means these two matrices can be used in place of Gaussian matrices in Random Kitchen Sinks and thereby speeding up the computation for a large range of kernel functions. The computational gain is achieved because unlike Gaussian random ma-trices, Hadamard matrices and scaling matrices are inexpensive to multiply and store.
 We prove that the Fastfood approximation is unbi-ased, has low variance, and concentrates almost at the same rate as Random Kitchen Sinks. Moreover, extensive experiments with a wide range of datasets show that Fastfood achieves similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster with 1000x less memory. These improvements, especially in terms of memory usage, make it possible to use kernel methods even for em-bedded applications. Our experiments also demon-strate that Fastfood, thanks to its speedup in train-ing, achieves state-of-the-art accuracy on the CIFAR-10 dataset (Krizhevsky, 2009) among permutation-invariant methods.
 Other related work Speeding up kernel methods has been a research focus for many years. Early work compresses function expansions after the problem was solved (Burges, 1996) by means of reduced-set expan-sions. Subsequent work aimed to reduce memory foot-print and complexity by finding subspaces to expand berg, 2001; Williams &amp; Seeger, 2001). They typi-cally require O ( n 3 + mnd ) steps to process m obser-vations and to expand d dimensional data into an n -dimensional function space. Moreover, they require O ( n 2 ) storage at least at preprocessing time to obtain suitable basis functions. Despite these efforts, these costs are still expensive for practical applications. Along the lines of Rahimi &amp; Recht (2007; 2008) X  X  work, fast multipole expansions (Lee &amp; Gray, 2009; Gray &amp; Moore, 2003) offer another interesting avenue for effi-cient function expansions. While this idea is attrac-tive when the dimensionality of the input dimension d is small, they become computationally intractable for large d  X  X  due to the curse of dimensionality in terms of partitioning. We start by reviewing some basic tools from kernel methods (Sch  X olkopf &amp; Smola, 2002) and then analyze key ideas behind Random Kitchen Sinks. 2.1. Mercer X  X  Theorem and Expansions At the heart of kernel methods is the theorem of (Mer-cer, 1909) which guarantees that kernels can be ex-pressed as an inner product in some Hilbert space. Theorem 1 (Mercer) Any kernel k : X  X X  X  R satisfying R k ( x,x 0 ) f ( x ) f ( x 0 ) dxdx 0  X  0 for all L measurable functions f can be expanded into Here  X  j &gt; 0 and the  X  j are orthonormal on L 2 ( X ) . The key idea of (Rahimi &amp; Recht, 2007; 2008) is to use sampling to approximate the sum in (2). In other words, they draw 2 Note that the basic connection between random basis functions was well established, e.g., by Neal (1994) in proving that the Gaussian Process is a limit of an in-finite number of basis functions. The expansion (3) is possible whenever the following conditions hold: 1. An inner product expansion of the form (2) is 2. The basis functions  X  j are sufficiently inexpensive 3. The sum P j  X  j &lt;  X  converges, i.e., k corre-Although condition 2 is typically difficult to achieve, there exist special classes of expansions that are com-putationally attractive. Specifically, whenever the ker-nels are invariant under an action of a symmetry group, we can use the eigenfunctions of its representa-tion to diagonalize the kernel.
 For instance, for the translation group the Fourier basis diagonalizes its action because translations can be represented by multiplications in Fourier space. Likewise, the rotation group SO( n ) leads to spheri-cal harmonics as the matching representation. For the symmetric group (i.e., permutations) we obtain corre-sponding invariants. In particular, a major focus of Random Kitchen Sinks is the class of translation in-variant kernels that can be written as a function of x  X  x 0 and have the properties Here the eigenfunctions are given by the Fourier basis k ( x,x 0 ) = Here  X  ( z )  X  0 is a kernel-specific weight that quantifies how much high frequency components are penalized. By construction the function  X  ( z ) is quite easily ob-tained by applying the Fourier transform to k ( x, 0)  X  in this case the above expansion is simply the inverse Fourier transform: This technique allows us to obtain explicit Fourier ex-pansions for a wide class of kernel functions (Gaus-sian RBF, Laplace, Matern, etc.). For instance, for Gaussian RBF kernel it is a Gaussian with the in-verse covariance structure. For the Laplace kernel it yields the damped harmonic oscillator spectrum and for the Matern kernel, i.e., Bessel functions, this yields the convolutions of the unit ball (Sch  X olkopf &amp; Smola, 2002). 2.2. Random Kitchen Sinks for Gaussians Rahimi &amp; Recht (2008) use this property of  X  ( z ) to generate approximations to the Gaussian RBF kernel, k ( x,x 0 ) = exp(  X  X  x  X  x 0 k 2 / (2  X  2 )), by drawing values z from a normal distribution: input Scale  X  2 , n , d
Sample entries in Z  X  R n  X  d i.i.d. from N (0 , X   X  2 ). for all x do end for As derived above, the associated feature map con-verges in expectation to the Gaussian RBF kernel. In fact, convergence occurs with high probability and at the rate of independent empirical averages (Rahimi &amp; Recht, 2007; 2008). This allows one to use primal space methods for training, and thus prevents the cost of computing decision function from growing as the dataset grows.
 This approach is still limited by the fact that we need to store Z and, more importantly, we need to compute Zx for each x . That is, each observation costs O ( nd ) operations and we need O ( nd ) storage. In the next section, we propose Fastfood that improves Random Kitchen Sinks further by approximating the random matrix using a set of simple transforms. Our main contribution is to show strategies for accel-erating Zx from O ( nd ) to O ( n log d ) time and how this can be used for constructing kernels using arbi-trary spectral distributions  X  ( z ) provided that they are spherically invariant, i.e., they must only depend on k z k 2 . In a nutshell, the approach relies on the fact that Hadamard matrices, when combined with Gaus-sian scaling matrices, behave very much like Gaus-sian random matrices. The adaptation to distributions other than Gaussians then occurs via rescaling by coef-ficients drawn from the equivalent radial distribution. 3.1. Gaussian RBF Kernels We begin with the Gaussian RBF case and extend it to more general spectral distributions subsequently. Without loss of generality assume that d = 2 l for some l  X  N . 3 For the moment assume that d = n . The ma-trices that we consider instead of Z are parameterized by a product of diagonal and simple matrices: Here  X   X  { 0 , 1 } d  X  d is a permutation matrix and H is the Walsh-Hadamard matrix. 4 S,G and B are all di-agonal random matrices. More specifically, B has ran-dom { X  1 } entries on its main diagonal, G has random Gaussian entries, and S is a random scaling matrix. V is then used to compute the feature map.
 The coefficients for S,G,B are computed once and stored. The Walsh-Hadamard matrix is given by The fast Hadamard transform, a variant of the FFT, allows us to compute H d x in O ( d log d ) time. When n &gt; d, we replicate (7) for n/d indepen-dent random matrices V i and stack them via V T = [ V 1 ,V 2 ,...V n/d ] T until we have enough dimensions. The feature map for Fastfood is then defined as In the next section, we will prove that this feature map approximates the RBF kernel. The rest of this section will focus on its attractiveness in terms of computa-tional efficiency.
 Lemma 2 (Computational Efficiency) The fea-tures of (8) can be computed at O ( n log d ) cost using O ( n ) permanent storage for n  X  d .
 Proof Storing the matrices S,G,B costs 3 n entries and 3 n operations for a multiplication. The permu-tation matrix  X  costs n entries and n operations. The Hadamard matrix itself requires no storage since it is only implicitly represented. Furthermore, the fast Hadamard transforms costs O ( n log d ) operations to carry out (we have O ( d log d ) per block and n/d blocks). Computing the Fourier basis for n numbers is an O ( n ) operation. Hence the total CPU budget is O ( n log d ) and the storage is O ( n ).
 Note that the construction of V is analogous to that of (Dasgupta et al., 2011). We will use these results in establishing a sufficiently high degree of decorrela-tion between rows of V . Also note that multiplying with a longer chain of Walsh-Hadamard matrices and permutations would yield a distribution closer to in-dependent Gaussians. However, as we shall see, two matrices provide a sufficient amount of decorrelation. 3.2. Basic Properties Now that we showed that the above operation is fast , let us give some initial indication why it is also useful and how the remaining matrices S,G,B,  X  are defined. Binary scaling matrix B : It is a diagonal matrix Permutation  X  : This ensures that the rows of the Gaussian scaling matrix G : This is a diagonal ma-Scaling matrix S : Note that the length of all rows We now analyze the distribution of entries in V . The rows of HG  X  HB have the same length.
 Any given row of HG  X  HB is iid Gaussian.
 The rows of SHG  X  HB are Gaussian. Rescaling Lemma 3 The expected feature map recovers the Gaussian RBF kernel, i.e., Moreover, the same holds for V 0 = 1 Proof We already saw above that any given row in V is a random Gaussian vector with distribution N (0 , X   X  2 I d ), hence we can directly appeal to the con-struction of (Rahimi &amp; Recht, 2008). This also holds for V 0 . The main difference being that the rows in V 0 are considerably more correlated. 3.3. Approximation Guarantees In this section we prove that the approximation that we incur relative to a Gaussian random matrix is mild. 3.3.1. Low Variance Theorem 4 shows that when approximating the RBF kernel with n features the variance of Fastfood (even without the scaling matrix S ) is at most the variance of straightforward Gaussian features, the first term in (12), plus O (1 /n ). In fact, we will see in experiments that our approximation works as well as an exact ker-nel expansion and Random Kitchen Sinks.
 Since the kernel values are real numbers, let us con-sider the real version of the complex feature map  X  for simplicity. Set y j = [ V ( x 0  X  x )] j and recall that  X  Thus we can replace  X  ( x )  X  C n with  X  0 ( x )  X  R 2 n n  X  1 / 2 sin([ V x ] j ), see (Rahimi &amp; Recht, 2007). Theorem 4 Let C (  X  ) = 6  X  4 h e  X   X  2 +  X  2 3 i and v = ( x  X  x 0 ) / X  . Then for the feature map  X  0 : R d  X  R 2 n obtained by stacking n/d i.i.d. copies of matrix V 0 =
Var  X  0 ( x ) &gt;  X  0 ( x 0 )  X  Moreover, the same holds for V = 1 estimates, each arising from 2 d features. Hence it X  X  sufficient to prove the claim for a single block, i.e. when n = d . We show the latter for V 0 in Theorem 5 and omit the near identical argument for V .
 Theorem 5 Let v = ( x  X  x 0 ) / X  and let  X  j ( v ) = cos( d  X  1 2 [ HG  X  HBv ] j ) denote the estimate of the ker-nel value that comes from the j th pair of random fea-tures for each j  X  X  1 ...d } . Then for each j we have where C (  X  ) = 6  X  4 h e  X   X  2 +  X  2 3 i .
 Proof Since Var( P X j ) = P j,t Cov( X j ,X t ) for any random variable X j , our goal is to compute Let w = 1  X   X  ( v ) = cos( z j ). Now condition on the value of u . Then it follows that Cov( z j ,z t | u ) =  X  jt ( u ) k v k  X  jt ( u )  X  [  X  1 , 1] is the correlation of z j and z t . To simplify the notation, in what follows we write  X  instead of  X  jt ( u ). Observe that the marginal distribu-tion of each z j is N (0 , k v k 2 ) as k u k = k v k and each element of H is  X  1. Thus the joint distribution of z j and z t is a Gaussian with mean 0 and covariance = E g [cos([ Lg ] 1 ) cos([ Lg ] 2 )]  X  E g [cos( z j )] E where g  X  R 2 is drawn from N (0 , 1 ). From the trigonometric identity it follows that we can rewrite E g [cos([ Lg ] 1 ) cos([ Lg ] 2 )] = That is, after applying the addition theorem we ex-plicitly computed the now one-dimensional Gaussian integrals.
 Likewise, since by construction z j and z j have zero mean and variance k v k 2 we have that
E g [cos( z j )] E g [cos( z t )] = E h [cos( k v k h )] 2 Combining both terms we obtain that the covariance can be written as
Cov[  X  j ( v ) , X  t ( v ) | u ] = e  X  X  v k 2 h cosh[ k v k To prove the first claim realize that here j = t and correspondingly  X  = 1. Plugging this into the above covariance expression and simplifying terms yields our first claim (13).
 To prove our second claim, observe that from the Tay-lor series of cosh with remainder in Lagrange form, it cosh( k v k 2  X  ) =1 + Note that we still conditioned on u . What remains is to bound E u [  X  2 ], which is small if E [ k u k small. The latter is ensured by HB , which acts as a randomized preconditioner. These calculations are fairly standard and can be found in Appendix A.1 of the supplementary material. 3.3.2. Concentration The following theorem shows that given error proba-bility  X  , the approximation error of a d  X  d block of Fastfood is at most O ( p log( d/ X  )) times larger than the error of Random Kitchen Sinks. We believe that this bound is not tight and could be further improved. We defer analyzing the concentration of n &gt; d stacked Fastfood features to future work.
 Theorem 6 For all x,x 0  X  R d let  X  k ( x,x 0 ) = P timate of the RBF kernel k ( x,x 0 ) that arises from a d  X  d block of Fastfood. Then we have that for all  X  &gt; 0 where  X  = Theorem 6 demonstrates almost sub-Gaussian conver-gence Fastfood kernel for a fixed pair of points x,x 0 . A standard -net argument then shows uniform con-vergence over any compact set of R d with bounded diameter (Rahimi &amp; Recht, 2007)[Claim 1]. Also, the small error of the approximate kernel does not signifi-cantly perturb the solution returned by wide range of learning algorithms (Rahimi &amp; Recht, 2007)[Appendix B] or affect their generalization error.
 We refer the interested reader to Appendix A.2. in the supplementary material for the proof of the theorem. Our key tool is concentration of Lipschitz continuous functions under the Gaussian measure (Ledoux, 1996). We ensure that Fastfood construct has a small Lips-chitz constant using Lemma 7.
 Lemma 7 (Ailon &amp; Chazelle, 2009) Let x  X  R d and t &gt; 0 . Let H  X  R d  X  d and B  X  R d  X  d denote the Hadamard and the binary random diagonal matrices in our construct. Then for any  X  &gt; 0 we have that 3.4. Changing the Spectrum Changing the kernel from a Gaussian RBF to any other radial basis function kernel is straightforward. After all, HG  X  HB provides almost spherically uniformly distributed random vectors with common length. Rescaling each direction of projection separately costs only O ( n ) space and computation. Consequently we are free to choose different coefficients S ii rather than (9). Instead, we may use Here c is a normalization constant and  X  ( r ) is the ra-dial part of the spectral density function of the regu-larization operator associated with the kernel. A key advantage over a conventional kernel approach is that we are not constrained by the requirement that the spectral distributions (5) be analytically com-putable. Even better, the spectra only need to be computable by some procedure (rather than have a closed-form representation).
 For concreteness consider the Matern kernel. Its spec-tral properties are discussed, e.g. in (Sch  X olkopf &amp; Smola, 2002). In a nutshell, given data in R d denote by  X  := d 2 a dimension calibration and let t  X  N be a fixed parameter, which is usually set experimentally. Moreover, denote by J  X  ( r ) the Bessel function of the first kind of order  X  . Then the kernel given by has as its associated Fourier transform Here  X  S ball in R d and N denotes convolution. In words, the Fourier transform of k is the t -fold convolution of  X  S As convolutions of distributions arise from adding in-dependent random variables this yields a simple algo-rithm for computing the Matern kernel: for each S ii do end for While this may appear costly, it only needs to be car-ried out once at initialization time. After that we can store the coefficients S ii . Also note that this addresses a rather surprising problem with the Gaussian RBF kernel  X  in high dimensional spaces draws from a Gaussian are strongly concentrated on the surface of a sphere. That is, we only probe the estimation problem with a fixed characteristic length. The Matern kernel, on the other hand, spreads its capacity over a much larger range of frequencies. In the following we assess the performance of Random Kitchen Sinks and Fastfood. The results show that Fastfood performs as well as Random Kitchen Sinks in terms of accuracy. Fastfood, however, is orders of mag-nitude faster and exhibits a significantly lower memory footprint. For simplicity, we focus on penalized least squares regression since in this case we are able to com-pute exact solutions and are independent of any other optimization algorithms. We also benchmark Fastfood on CIFAR-10 (Krizhevsky, 2009) and observe that it achieves state-of-the-art accuracy. This advocates for the use of non-linear expansions even when d is large. 4.1. Approximation quality We begin by investigating how well our features can approximate the exact kernel computation as n in-creases. For that purpose, we uniformly sample 4000 vectors from [0 , 1] 10 . We compare the exact kernel val-ues to Random Kitchen Sinks and Fastfood.
 The results are shown in Figure 1. We used the abso-lute difference between the exact kernel and the ap-proximation to quantify the error (the relative dif-ference also exhibits similar behavior and is thus not shown due to space constraints). The results are pre-sented as averages, averaging over 4000 samples. As can be seen, as n increases, both Random Kitchen Sinks and Fastfood converge quickly to the exact ker-nel values. Their performance is indistinguishable, as expected from the construction of the algorithm. Note, though, that fidelity in approximating k ( x,x does not imply generalization performance (unless the bounds are very tight). To assess this, we carried out experiments on all regression datasets from the UCI repository (Frank &amp; Asuncion, 2010) that are not too tiny, i.e., that contained at least 4 , 000 instances. We investigate estimation accuracy via Gaussian pro-cess regression using approximated kernel computation methods and we compare this to exact kernel computa-tion whenever the latter is feasible. For completeness, we compare the following methods: Exact RBF uses the exact RBF kernel. This is possi-Nystrom uses the Nystrom approximation of the ker-Random Kitchen Sinks uses the the Gaussian ran-Fastfood ( X  X adamard features X ) uses the random FFT Fastfood ( X  X ourier features X ) uses a variant of The results of the comparison are given in Table 2. As can be seen, there is virtually no difference be-tween the exact kernel, the Nystrom approximation, Random Kitchen Sinks and Fastfood. Somewhat sur-prisingly the Fourier features work very well. This indicates that the concentration of measure effects im-pacting Gaussian RBF kernels may actually be coun-terproductive at their extreme.
 In Figure 2, we show regression performances as a function of number of basis functions n on the CPU dataset and demonstrates that it is necessary to have a large n in order to learn highly nonlinear functions. Interestingly, although Fourier features do not seem to approximate the Gaussian RBF kernel, they per-form well compared to other variants and improve as n increases. This suggests that learning the kernel by direct spectral adjustment might be a useful applica-tion of our proposed method. 4.2. Speed of kernel computations In the previous experiments, we observe that Fastfood is on par with exact kernel computation, the Nystrom method, and Random Kitchen Sinks. The key point, however, is to establish whether the algorithm offers computational savings.
 For this purpose we compare Random Kitchen Sinks using Eigen 5 and our method using Spiral 6 . Both are highly optimized numerical linear algebra libraries in C++. We are interested in the time it takes to go from raw features of a vector with dimension d to the label prediction of that vector. On a small problem with d = 1 , 024 and n = 16 , 384, performing prediction with Random Kitchen Sinks takes 0.07 seconds. Our method is around 24x faster, taking only 0.003 seconds to compute the label for one input vector. The speed gain is even more significant for larger problems, as is evident in Table 1. This confirms experimentally the O ( n log d ) vs. O ( nd ) runtime and O ( n ) vs. O ( nd ) storage of Fastfood relative to Random Kitchen Sinks. 4.3. Random features for CIFAR-10 To understand the importance of nonlinear feature ex-pansions for a practical application, we benchmarked Fastfood, Random Kitchen Sinks on the CIFAR-10 dataset (Krizhevsky, 2009) which has 50,000 train-ing images and 10,000 test images. Each image has 32x32 pixels and 3 channels ( d = 3072). In our ex-periments, linear SVMs achieve 42.3% accuracy on the test set. Non-linear expansions improve the clas-sification accuracy significantly. In particular, Fast-food FFT ( X  X ourier features X ) achieve 63.1% while Fastfood ( X  X adamard features X ) and Random Kitchen Sinks achieve 62.4% with an expansion of n = 16 , 384. These are also best known classification accuracies using permutation-invariant representations on this dataset. In terms of speed, Random Kitchen Sinks is 5x slower (in total training time) and 20x slower (in predicting a label given an image) compared to Fastfood and Fastfood FFT. This demonstrates that non-linear expansions are needed even when the raw data is high-dimensional, and that Fastfood is more practical for such problems.
 In particular, in many cases, linear function classes are used because they provide fast training time, and especially test time, but not because they offer better accuracy. The results on CIFAR-10 demonstrate that Fastfood can overcome this obstacle.
 Summary We demonstrated that it is possible to compute n nonlinear basis functions in O ( n log d ) time, a significant speedup over the best competitive algo-rithms. This means that kernel methods become more practical for problems that have large datasets and/or require real-time prediction. In fact, Fastfood can be used to run on cellphones because not only it is fast, but it also requires only a small amount of storage. Acknowledgments We thank John Langford and Ravi Kumar for fruitful discussions.
 Ailon, N. and Chazelle, B. The fast Johnson X 
Lindenstrauss transform and approximate nearest neighbors. SICOMP , 2009.
 Aizerman, M. A., Braverman, A. M., and Rozono  X er,
L. I. Theoretical foundations of the potential func-tion method in pattern recognition learning. Autom. Remote Control , 25:821 X 837, 1964.
 Aronszajn, N. La th  X eorie g  X en  X erale des noyaux r  X eproduisants et ses applications. Proc. Cambridge Philos. Soc. , 39:133 X 153, 1944.
 Boser, B., Guyon, I., and Vapnik, V. A training algo-rithm for optimal margin classifiers. COLT 1992. Burges, C. J. C. Simplified support vector decision rules. ICML , 1996 Cortes, C. and Vapnik, V. Support vector networks. Machine Learning , 20(3):273 X 297, 1995.
 Dasgupta, A., Kumar, R., and Sarl  X os, T. Fast locality-sensitive hashing. SIGKDD , pp. 1073 X 1081, 2011. Fine, S. and Scheinberg, K. Efficient SVM training using low-rank kernel representations. JMLR , 2001. Frank, A. and Asuncion, A. UCI machine learning repository. http://archive.ics.uci.edu/ml .
 Girosi, F. An equivalence between sparse approxima-tion and support vector machines. Neural Compu-tation , 10(6):1455 X 1480, 1998.
 Girosi, F., Jones, M., and Poggio, T. Regularization theory and neural networks architectures. Neural Computation , 7(2):219 X 269, 1995.
 Gray, A. G. and Moore, A. W. Rapid evaluation of multiple density models. AISTATS , 2003.
 Jin, R., Yang, T., Mahdavi, M., Li, Y.F., and Zhou,
Z.H. Improved bound for the Nystrom X  X  method and its application to kernel classification, 2011. URL http://arxiv.org/abs/1111.2262 .
 Kimeldorf, G. S. and Wahba, G. A correspondence between Bayesian estimation on stochastic processes and smoothing by splines. Annals of Mathematical Statistics , 41:495 X 502, 1970.
 Kreyszig, E. Introductory Functional Analysis with Applications . Wiley, 1989.
 Krizhevsky, A. Learning multiple layers of features from tiny images. TR, U Toronto, 2009.
 Ledoux, M. Isoperimetry and Gaussian analysis. Lec-tures on probability theory and statistics, 1996. Lee, D. and Gray, A. G. Fast high-dimensional ker-nel summations using the Monte Carlo multipole method. NIPS , 2009.
 MacKay, D. J. C. Information Theory, Inference, and Learning Algorithms . Cambridge 2003.
 Mercer, J. Functions of positive and negative type and their connection with the theory of integral equa-tions. Royal Society London , A 209:415 X 446, 1909. Micchelli, C. A. Interpolation of scattered data: dis-tance matrices and conditionally positive definite functions. Constr. Approximation , 2:11 X 22, 1986. Neal, R. Priors for infinite networks. CRG-TR-94-1, U Toronto, 1994.
 Rahimi, A. and Recht, B. Random features for large-scale kernel machines. NIPS 20 , 2007.
 Rahimi, A. and Recht, B. Weighted sums of ran-dom kitchen sinks: Replacing minimization with randomization in learning. NIPS 21 , 2008.
 Sch  X olkopf, B., Smola, A. J., and M  X uller, K.-R. Nonlin-ear component analysis as a kernel eigenvalue prob-lem. Neural Comput. , 10:1299 X 1319, 1998.
 Sch  X olkopf, Bernhard and Smola, A. J. Learning with Kernels . MIT Press, Cambridge, MA, 2002.
 Smola, A. J. and Sch  X olkopf, B. Sparse greedy matrix approximation for machine learning. ICML , 2000. Smola, A. J., Sch  X olkopf, B., and M  X uller, K.-R. The Connection between Regularization Operators and Support Vector Kernels. Neur. Networks , 1998 Steinwart, Ingo and Christmann, Andreas. Support Vector Machines . Springer, 2008.
 Taskar, B., Guestrin, C., and Koller, D. Max-margin Markov networks. NIPS 16 , 2004.
 Tropp, J. A. Improved analysis of the subsampled randomized Hadamard transform Adv. Adapt. Data Anal., 2011.
 Vapnik, V., Golowich, S., and Smola, A. Support vec-tor method for function approximation, regression estimation, and signal processing. NIPS 9 , 1997. Wahba, G. Spline Models for Observational Data , CBMS-NSF , vol. 59, SIAM, Philadelphia, 1990. Williams, C. K. I. Prediction with Gaussian processes:
From linear regression to linear prediction and be-yond. In Jordan, M. I. (ed.), Learning and Inference in Graphical Models , Kluwer, 1998.
 Williams, C. K. I. and Seeger, M. Using the Nystrom method to speed up kernel machines. NIPS 13 , 2001. A.1. Low Variance Proof [Theorem 5 continued] Since G is diagonal and G ii  X  N (0 , 1) independently it holds that
Cov[ z,z ] = Cov[ HGu,HGu ] = H Cov[ Gu,Gu ] H &gt; Recall that H ij = H ji are elements of the Hadamard matrix. For ease of notation fix j 6 = t and let T = { i  X  [1 ..d ] : H ji = H ti } be the set of columns where the j th and the t th row of the Hadamard matrix agree. Then
Now recall that u =  X  w and that  X  is a random permutation matrix. Therefore u i = w  X  ( i ) for a ran-domly chosen permutation  X  and thus the distribution of  X  k v k 2 and 2 P i  X  R w 2 i  X  X  v k 2 where R is a randomly chosen subset of size d 2 in { 1 ...d } are the same. Let us fix (condition on) w . Since 2 E R P i  X  R w 2 i = k v k we have that Now let  X  i = 1 if i  X  R and 0 otherwise. Note that E (  X  i ) = 1 2 and if j 6 = k then E  X  (  X  i  X  k )  X  1 4 are (mildly) negatively correlated. From k w k = k v k it follows that Therefore from equations (17), and (18) it follows that Let b i be the independent  X  1 on the diagonal of B . Using the fact w i = 1  X  independent with similar calculations to the above it which shows that 1  X  densifies the input. Putting it all together we have
X Combining the latter with the already proven first claim establishes the second claim.
 A.2. Concentration Let us recall the following fundamental fact about the concentration of Gaussian measure.
 Definition 8 A function f : R d  X  R is Lipschitz con-tinuous with Lipschitz constant L if for all x,y  X  R d it holds that | f ( x )  X  f ( y ) | X  L k x  X  y k 2 . Theorem 9 (Inequality (2.9) in (Ledoux, 1996)) Let f : R d  X  R be Lipschitz continuous with constant L and g distributed according to N (0 ,I d  X  d ) . Then P [ | f ( g )  X  E g f ( g ) | X  t ]  X  2 e  X  t 2 / (2 L 2 ) Proof [Theorem 6] As both k and  X  k are shift invariant, set v =  X  ( x  X  x 0 ) and write k ( v ) = tion. Set u =  X  d  X  1 2 HBv , and z = HGu and define f ( G,  X  ,B ) = P d j =1 cos( z j ) /d . Observe that Lemma 3 ficient to prove that f ( G,  X  ,B ) concentrates around its mean. We X  X l accomplish this by showing that f is Lipschitz continuous as a function of G for most  X  and B . For a  X  R d let Using the fact that cosine is Lipschitz continuous with constant 1 we observe that for any pair of vectors a,b  X  R d it holds that For any vector g  X  R d let Diag( g )  X  R d  X  d denote the diagonal matrix whose diagonal is g . Observe that for any pair of vectors g,g 0  X  R d we have that k H Diag( g ) u  X  H Diag( g 0 ) u k 2  X k H k 2 k Diag( g  X  g Let G = Diag( g ) in the Fastfood construct and recall the definition of function h , (20). Combining inequali-ties (21) and (22) for any pair of vectors g,g 0  X  R d we have that | h ( H Diag( g ) u ))  X  h ( H Diag( g 0 ) u ) | X k u k  X  k g  X  g From u =  X  d  X  1 2 HBv and k  X  w k  X  = k w k  X  combined with Lemma 7 it follows that holds with probability at least 1  X   X  , where the proba-bility is over the choice of B . 7 Now condition on (24). From inequality (23) we have that the function g  X  h ( H Diag( g ) u ) = f (Diag( g ) ,  X  ,B ) is Lipschitz contin-uous with Lipschitz constant Therefore from Theorem 9 and from the independently chosen G jj  X  X  (0 , 1) it follows that P G h | f ( G,  X  ,B )  X  k ( v ) | X  p 2 log(2 / X  ) L i  X   X . (26) Combining inequalities (25) and (26) with the union bound concludes the proof.
 Quoc Le qvl@google.com Tam  X as Sarl  X os stamas@google.com Alex Smola alex@smola.org Kernel methods are successful techniques for solving many problems in machine learning, ranging from clas-sification and regression to sequence annotation and feature extraction (Boser et al., 1992; Cortes &amp; Vap-nik, 1995; Vapnik et al., 1997; Taskar et al., 2004; Sch  X olkopf et al., 1998). At their heart lies the idea that inner products in high-dimensional feature spaces can be computed in an implicit form via a kernel function k : Here  X  : X  X  F maps elements of the observation space X into a high-dimensional feature space F . Key to kernel methods is that as long as kernel algorithms have access to k , we do not need to represent  X  ( x ) ex-plicitly. Most often, that means although  X  ( x ) can be high-dimensional or even infinite-dimensional, their in-ner products, can be evaluated in an inexpensive man-ner by k . This idea is known as the  X  X ernel trick. X  More concretely, to evaluate the decision function f ( x ) on an example x , one typically employs the kernel trick as follows f ( x ) =  X  w, X  ( x )  X  = This has been viewed as a strength of kernel methods, especially in the days that datasets consisted of ten thousands of examples. This is because the Represen-ter Theorem (Kimeldorf &amp; Wahba, 1970) states that such a function expansion in terms of finitely many coefficients must exist under fairly benign conditions even whenever the space is infinite dimensional. Hence we can effectively perform optimization in infinite di-mensional spaces.
 Unfortunately, on large amounts of data, this expan-sion turns into a significant limitation for computa-tional efficiency. For instance, (Steinwart &amp; Christ-mann, 2008) show that the number of nonzero  X  i (i.e., N , also known as the number of  X  X upport vectors X ) in many estimation problems can grow linearly in the size of the training set. As a consequence, as the dataset grows, the expense of evaluating f also grows. This property makes kernel methods expensive in many large scale problems.
 Random Kitchen Sinks (Rahimi &amp; Recht, 2007; 2008) 1 , the algorithm that our algorithm is based on, approximates the function f by means of multiplying the input with a Gaussian random matrix, followed by the application of a nonlinearity. If the expansion dimension is n and the input dimension is d (i.e., the Gaussian matrix is n  X  d ), it requires O ( nd ) time and memory to evaluate the decision function f . For large problems with sample size m n , this is typically much faster than the aforementioned  X  X ernel trick X  be-cause the computation is independent of the size of the training set. Experiments also show that this approxi-mation method achieves accuracy comparable to RBF kernels while offering significant speedup.
 Our proposed approach, Fastfood, accelerates Random Kitchen Sinks from O ( nd ) to O ( n log d ) time. The speedup is most significant when the input dimen-sion d is larger than 1000, which is typical in most applications. For instance, a tiny 32x32x3 image in the CIFAR-10 (Krizhevsky, 2009) already has 3072 di-mensions (and non-linear function classes have shown to work well for MNIST (Sch  X olkopf &amp; Smola, 2002) and CIFAR-10). Our approach relies on the fact that Hadamard matrices, when combined with Gaussian scaling matrices, behave very much like Gaussian ran-dom matrices. That means these two matrices can be used in place of Gaussian matrices in Random Kitchen Sinks and thereby speeding up the computation for a large range of kernel functions. The computational gain is achieved because unlike Gaussian random ma-trices, Hadamard matrices and scaling matrices are inexpensive to multiply and store.
 We prove that the Fastfood approximation is unbi-ased, has low variance, and concentrates almost at the same rate as Random Kitchen Sinks. Moreover, extensive experiments with a wide range of datasets show that Fastfood achieves similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster with 1000x less memory. These improvements, especially in terms of memory usage, make it possible to use kernel methods even for em-bedded applications. Our experiments also demon-strate that Fastfood, thanks to its speedup in train-ing, achieves state-of-the-art accuracy on the CIFAR-10 dataset (Krizhevsky, 2009) among permutation-invariant methods.
 Other related work Speeding up kernel methods has been a research focus for many years. Early work compresses function expansions after the problem was solved (Burges, 1996) by means of reduced-set expan-sions. Subsequent work aimed to reduce memory foot-print and complexity by finding subspaces to expand berg, 2001; Williams &amp; Seeger, 2001). They typi-cally require O ( n 3 + mnd ) steps to process m obser-vations and to expand d dimensional data into an n -dimensional function space. Moreover, they require O ( n 2 ) storage at least at preprocessing time to obtain suitable basis functions. Despite these efforts, these costs are still expensive for practical applications. Along the lines of Rahimi &amp; Recht (2007; 2008) X  X  work, fast multipole expansions (Lee &amp; Gray, 2009; Gray &amp; Moore, 2003) offer another interesting avenue for effi-cient function expansions. While this idea is attrac-tive when the dimensionality of the input dimension d is small, they become computationally intractable for large d  X  X  due to the curse of dimensionality in terms of partitioning. We start by reviewing some basic tools from kernel methods (Sch  X olkopf &amp; Smola, 2002) and then analyze key ideas behind Random Kitchen Sinks. 2.1. Mercer X  X  Theorem and Expansions At the heart of kernel methods is the theorem of (Mer-cer, 1909) which guarantees that kernels can be ex-pressed as an inner product in some Hilbert space. Theorem 1 (Mercer) Any kernel k : X  X X  X  R satisfying R k ( x,x 0 ) f ( x ) f ( x 0 ) dxdx 0  X  0 for all L measurable functions f can be expanded into Here  X  j &gt; 0 and the  X  j are orthonormal on L 2 ( X ) . The key idea of (Rahimi &amp; Recht, 2007; 2008) is to use sampling to approximate the sum in (2). In other words, they draw 2 Note that the basic connection between random basis functions was well established, e.g., by Neal (1994) in proving that the Gaussian Process is a limit of an in-finite number of basis functions. The expansion (3) is possible whenever the following conditions hold: 1. An inner product expansion of the form (2) is 2. The basis functions  X  j are sufficiently inexpensive 3. The sum P j  X  j &lt;  X  converges, i.e., k corre-Although condition 2 is typically difficult to achieve, there exist special classes of expansions that are com-putationally attractive. Specifically, whenever the ker-nels are invariant under an action of a symmetry group, we can use the eigenfunctions of its representa-tion to diagonalize the kernel.
 For instance, for the translation group the Fourier basis diagonalizes its action because translations can be represented by multiplications in Fourier space. Likewise, the rotation group SO( n ) leads to spheri-cal harmonics as the matching representation. For the symmetric group (i.e., permutations) we obtain corre-sponding invariants. In particular, a major focus of Random Kitchen Sinks is the class of translation in-variant kernels that can be written as a function of x  X  x 0 and have the properties Here the eigenfunctions are given by the Fourier basis k ( x,x 0 ) = Here  X  ( z )  X  0 is a kernel-specific weight that quantifies how much high frequency components are penalized. By construction the function  X  ( z ) is quite easily ob-tained by applying the Fourier transform to k ( x, 0)  X  in this case the above expansion is simply the inverse Fourier transform: This technique allows us to obtain explicit Fourier ex-pansions for a wide class of kernel functions (Gaus-sian RBF, Laplace, Matern, etc.). For instance, for Gaussian RBF kernel it is a Gaussian with the in-verse covariance structure. For the Laplace kernel it yields the damped harmonic oscillator spectrum and for the Matern kernel, i.e., Bessel functions, this yields the convolutions of the unit ball (Sch  X olkopf &amp; Smola, 2002). 2.2. Random Kitchen Sinks for Gaussians Rahimi &amp; Recht (2008) use this property of  X  ( z ) to generate approximations to the Gaussian RBF kernel, k ( x,x 0 ) = exp(  X  X  x  X  x 0 k 2 / (2  X  2 )), by drawing values z from a normal distribution: input Scale  X  2 , n , d
Sample entries in Z  X  R n  X  d i.i.d. from N (0 , X   X  2 ). for all x do end for As derived above, the associated feature map con-verges in expectation to the Gaussian RBF kernel. In fact, convergence occurs with high probability and at the rate of independent empirical averages (Rahimi &amp; Recht, 2007; 2008). This allows one to use primal space methods for training, and thus prevents the cost of computing decision function from growing as the dataset grows.
 This approach is still limited by the fact that we need to store Z and, more importantly, we need to compute Zx for each x . That is, each observation costs O ( nd ) operations and we need O ( nd ) storage. In the next section, we propose Fastfood that improves Random Kitchen Sinks further by approximating the random matrix using a set of simple transforms. Our main contribution is to show strategies for accel-erating Zx from O ( nd ) to O ( n log d ) time and how this can be used for constructing kernels using arbi-trary spectral distributions  X  ( z ) provided that they are spherically invariant, i.e., they must only depend on k z k 2 . In a nutshell, the approach relies on the fact that Hadamard matrices, when combined with Gaus-sian scaling matrices, behave very much like Gaus-sian random matrices. The adaptation to distributions other than Gaussians then occurs via rescaling by coef-ficients drawn from the equivalent radial distribution. 3.1. Gaussian RBF Kernels We begin with the Gaussian RBF case and extend it to more general spectral distributions subsequently. Without loss of generality assume that d = 2 l for some l  X  N . 3 For the moment assume that d = n . The ma-trices that we consider instead of Z are parameterized by a product of diagonal and simple matrices: Here  X   X  { 0 , 1 } d  X  d is a permutation matrix and H is the Walsh-Hadamard matrix. 4 S,G and B are all di-agonal random matrices. More specifically, B has ran-dom { X  1 } entries on its main diagonal, G has random Gaussian entries, and S is a random scaling matrix. V is then used to compute the feature map.
 The coefficients for S,G,B are computed once and stored. The Walsh-Hadamard matrix is given by The fast Hadamard transform, a variant of the FFT, allows us to compute H d x in O ( d log d ) time. When n &gt; d, we replicate (7) for n/d indepen-dent random matrices V i and stack them via V T = [ V 1 ,V 2 ,...V n/d ] T until we have enough dimensions. The feature map for Fastfood is then defined as In the next section, we will prove that this feature map approximates the RBF kernel. The rest of this section will focus on its attractiveness in terms of computa-tional efficiency.
 Lemma 2 (Computational Efficiency) The fea-tures of (8) can be computed at O ( n log d ) cost using O ( n ) permanent storage for n  X  d .
 Proof Storing the matrices S,G,B costs 3 n entries and 3 n operations for a multiplication. The permu-tation matrix  X  costs n entries and n operations. The Hadamard matrix itself requires no storage since it is only implicitly represented. Furthermore, the fast Hadamard transforms costs O ( n log d ) operations to carry out (we have O ( d log d ) per block and n/d blocks). Computing the Fourier basis for n numbers is an O ( n ) operation. Hence the total CPU budget is O ( n log d ) and the storage is O ( n ).
 Note that the construction of V is analogous to that of (Dasgupta et al., 2011). We will use these results in establishing a sufficiently high degree of decorrela-tion between rows of V . Also note that multiplying with a longer chain of Walsh-Hadamard matrices and permutations would yield a distribution closer to in-dependent Gaussians. However, as we shall see, two matrices provide a sufficient amount of decorrelation. 3.2. Basic Properties Now that we showed that the above operation is fast , let us give some initial indication why it is also useful and how the remaining matrices S,G,B,  X  are defined. Binary scaling matrix B : It is a diagonal matrix Permutation  X  : This ensures that the rows of the Gaussian scaling matrix G : This is a diagonal ma-Scaling matrix S : Note that the length of all rows We now analyze the distribution of entries in V . The rows of HG  X  HB have the same length.
 Any given row of HG  X  HB is iid Gaussian.
 The rows of SHG  X  HB are Gaussian. Rescaling Lemma 3 The expected feature map recovers the Gaussian RBF kernel, i.e., Moreover, the same holds for V 0 = 1 Proof We already saw above that any given row in V is a random Gaussian vector with distribution N (0 , X   X  2 I d ), hence we can directly appeal to the con-struction of (Rahimi &amp; Recht, 2008). This also holds for V 0 . The main difference being that the rows in V 0 are considerably more correlated. 3.3. Approximation Guarantees In this section we prove that the approximation that we incur relative to a Gaussian random matrix is mild. 3.3.1. Low Variance Theorem 4 shows that when approximating the RBF kernel with n features the variance of Fastfood (even without the scaling matrix S ) is at most the variance of straightforward Gaussian features, the first term in (12), plus O (1 /n ). In fact, we will see in experiments that our approximation works as well as an exact ker-nel expansion and Random Kitchen Sinks.
 Since the kernel values are real numbers, let us con-sider the real version of the complex feature map  X  for simplicity. Set y j = [ V ( x 0  X  x )] j and recall that  X  Thus we can replace  X  ( x )  X  C n with  X  0 ( x )  X  R 2 n n  X  1 / 2 sin([ V x ] j ), see (Rahimi &amp; Recht, 2007). Theorem 4 Let C (  X  ) = 6  X  4 h e  X   X  2 +  X  2 3 i and v = ( x  X  x 0 ) / X  . Then for the feature map  X  0 : R d  X  R 2 n obtained by stacking n/d i.i.d. copies of matrix V 0 =
Var  X  0 ( x ) &gt;  X  0 ( x 0 )  X  Moreover, the same holds for V = 1 Proof  X  0 ( x ) &gt;  X  0 ( x 0 ) is the average of n/d independent estimates, each arising from 2 d features. Hence it X  X  sufficient to prove the claim for a single block, i.e. when n = d . We show the latter for V 0 in Theorem 5 and omit the near identical argument for V .
 Theorem 5 Let v = ( x  X  x 0 ) / X  and let  X  j ( v ) = cos( d  X  1 2 [ HG  X  HBv ] j ) denote the estimate of the ker-nel value that comes from the j th pair of random fea-tures for each j  X  X  1 ...d } . Then for each j we have where C (  X  ) = 6  X  4 h e  X   X  2 +  X  2 3 i .
 Proof Since Var( P X j ) = P j,t Cov( X j ,X t ) for any random variable X j , our goal is to compute Let w = 1  X   X  ( v ) = cos( z j ). Now condition on the value of u . Then it follows that Cov( z j ,z t | u ) =  X  jt ( u ) k v k  X  jt ( u )  X  [  X  1 , 1] is the correlation of z j and z t . To simplify the notation, in what follows we write  X  instead of  X  jt ( u ). Observe that the marginal distribu-tion of each z j is N (0 , k v k 2 ) as k u k = k v k and each element of H is  X  1. Thus the joint distribution of z j and z t is a Gaussian with mean 0 and covariance = E g [cos([ Lg ] 1 ) cos([ Lg ] 2 )]  X  E g [cos( z j )] E where g  X  R 2 is drawn from N (0 , 1 ). From the trigonometric identity it follows that we can rewrite E g [cos([ Lg ] 1 ) cos([ Lg ] 2 )] = h  X  N (0 , 1) and a 2  X  = L &gt; [1 ,  X  1] 2 = 2 k v k 2 (1  X   X  ). That is, after applying the addition theorem we ex-plicitly computed the now one-dimensional Gaussian integrals.
 Likewise, since by construction z j and z j have zero mean and variance k v k 2 we have that
E g [cos( z j )] E g [cos( z t )] = E h [cos( k v k h )] 2 Combining both terms we obtain that the covariance can be written as
Cov[  X  j ( v ) , X  t ( v ) | u ] = e  X  X  v k 2 h cosh[ k v k To prove the first claim realize that here j = t and correspondingly  X  = 1. Plugging this into the above covariance expression and simplifying terms yields our first claim (13).
 To prove our second claim, observe that from the Tay-lor series of cosh with remainder in Lagrange form, it cosh( k v k 2  X  ) =1 + Note that we still conditioned on u . What remains is to bound E u [  X  2 ], which is small if E [ k u k small. The latter is ensured by HB , which acts as a randomized preconditioner. These calculations are fairly standard and can be found in Appendix A.1 of the supplementary material. 3.3.2. Concentration The following theorem shows that given error proba-bility  X  , the approximation error of a d  X  d block of Fastfood is at most O ( p log( d/ X  )) times larger than the error of Random Kitchen Sinks. We believe that this bound is not tight and could be further improved. We defer analyzing the concentration of n &gt; d stacked Fastfood features to future work.
 Theorem 6 For all x,x 0  X  R d let  X  k ( x,x 0 ) = P timate of the RBF kernel k ( x,x 0 ) that arises from a d  X  d block of Fastfood. Then we have that for all  X  &gt; 0 where  X  = Theorem 6 demonstrates almost sub-Gaussian conver-gence Fastfood kernel for a fixed pair of points x,x 0 . A standard -net argument then shows uniform con-vergence over any compact set of R d with bounded diameter (Rahimi &amp; Recht, 2007)[Claim 1]. Also, the small error of the approximate kernel does not signifi-cantly perturb the solution returned by wide range of learning algorithms (Rahimi &amp; Recht, 2007)[Appendix B] or affect their generalization error.
 We refer the interested reader to Appendix A.2. in the supplementary material for the proof of the theorem. Our key tool is concentration of Lipschitz continuous functions under the Gaussian measure (Ledoux, 1996). We ensure that Fastfood construct has a small Lips-chitz constant using Lemma 7.
 Lemma 7 (Ailon &amp; Chazelle, 2009) Let x  X  R d and t &gt; 0 . Let H  X  R d  X  d and B  X  R d  X  d denote the Hadamard and the binary random diagonal matrices in our construct. Then for any  X  &gt; 0 we have that 3.4. Changing the Spectrum Changing the kernel from a Gaussian RBF to any other radial basis function kernel is straightforward. After all, HG  X  HB provides almost spherically uniformly distributed random vectors with common length. Rescaling each direction of projection separately costs only O ( n ) space and computation. Consequently we are free to choose different coefficients S ii rather than (9). Instead, we may use Here c is a normalization constant and  X  ( r ) is the ra-dial part of the spectral density function of the regu-larization operator associated with the kernel. A key advantage over a conventional kernel approach is that we are not constrained by the requirement that the spectral distributions (5) be analytically com-putable. Even better, the spectra only need to be computable by some procedure (rather than have a closed-form representation).
 For concreteness consider the Matern kernel. Its spec-tral properties are discussed, e.g. in (Sch  X olkopf &amp; Smola, 2002). In a nutshell, given data in R d denote by  X  := d 2 a dimension calibration and let t  X  N be a fixed parameter, which is usually set experimentally. Moreover, denote by J  X  ( r ) the Bessel function of the first kind of order  X  . Then the kernel given by has as its associated Fourier transform Here  X  S ball in R d and N denotes convolution. In words, the Fourier transform of k is the t -fold convolution of  X  S As convolutions of distributions arise from adding in-dependent random variables this yields a simple algo-rithm for computing the Matern kernel: for each S ii do end for While this may appear costly, it only needs to be car-ried out once at initialization time. After that we can store the coefficients S ii . Also note that this addresses a rather surprising problem with the Gaussian RBF kernel  X  in high dimensional spaces draws from a Gaussian are strongly concentrated on the surface of a sphere. That is, we only probe the estimation problem with a fixed characteristic length. The Matern kernel, on the other hand, spreads its capacity over a much larger range of frequencies. In the following we assess the performance of Random Kitchen Sinks and Fastfood. The results show that Fastfood performs as well as Random Kitchen Sinks in terms of accuracy. Fastfood, however, is orders of mag-nitude faster and exhibits a significantly lower memory footprint. For simplicity, we focus on penalized least squares regression since in this case we are able to com-pute exact solutions and are independent of any other optimization algorithms. We also benchmark Fastfood on CIFAR-10 (Krizhevsky, 2009) and observe that it achieves state-of-the-art accuracy. This advocates for the use of non-linear expansions even when d is large. 4.1. Approximation quality We begin by investigating how well our features can approximate the exact kernel computation as n in-creases. For that purpose, we uniformly sample 4000 vectors from [0 , 1] 10 . We compare the exact kernel val-ues to Random Kitchen Sinks and Fastfood.
 The results are shown in Figure 1. We used the abso-lute difference between the exact kernel and the ap-proximation to quantify the error (the relative dif-ference also exhibits similar behavior and is thus not shown due to space constraints). The results are pre-sented as averages, averaging over 4000 samples. As can be seen, as n increases, both Random Kitchen Sinks and Fastfood converge quickly to the exact ker-nel values. Their performance is indistinguishable, as expected from the construction of the algorithm. Note, though, that fidelity in approximating k ( x,x does not imply generalization performance (unless the bounds are very tight). To assess this, we carried out experiments on all regression datasets from the UCI repository (Frank &amp; Asuncion, 2010) that are not too tiny, i.e., that contained at least 4 , 000 instances. We investigate estimation accuracy via Gaussian pro-cess regression using approximated kernel computation methods and we compare this to exact kernel computa-tion whenever the latter is feasible. For completeness, we compare the following methods: Exact RBF uses the exact RBF kernel. This is possi-Nystrom uses the Nystrom approximation of the ker-Random Kitchen Sinks uses the the Gaussian ran-Fastfood ( X  X adamard features X ) uses the random FFT Fastfood ( X  X ourier features X ) uses a variant of The results of the comparison are given in Table 2. As can be seen, there is virtually no difference be-tween the exact kernel, the Nystrom approximation, Random Kitchen Sinks and Fastfood. Somewhat sur-prisingly the Fourier features work very well. This indicates that the concentration of measure effects im-pacting Gaussian RBF kernels may actually be coun-terproductive at their extreme.
 In Figure 2, we show regression performances as a function of number of basis functions n on the CPU dataset and demonstrates that it is necessary to have a large n in order to learn highly nonlinear functions. Interestingly, although Fourier features do not seem to approximate the Gaussian RBF kernel, they per-form well compared to other variants and improve as n increases. This suggests that learning the kernel by direct spectral adjustment might be a useful applica-tion of our proposed method. 4.2. Speed of kernel computations In the previous experiments, we observe that Fastfood is on par with exact kernel computation, the Nystrom method, and Random Kitchen Sinks. The key point, however, is to establish whether the algorithm offers computational savings.
 For this purpose we compare Random Kitchen Sinks using Eigen 5 and our method using Spiral 6 . Both are highly optimized numerical linear algebra libraries in C++. We are interested in the time it takes to go from raw features of a vector with dimension d to the label prediction of that vector. On a small problem with d = 1 , 024 and n = 16 , 384, performing prediction with Random Kitchen Sinks takes 0.07 seconds. Our method is around 24x faster, taking only 0.003 seconds to compute the label for one input vector. The speed gain is even more significant for larger problems, as is evident in Table 1. This confirms experimentally the O ( n log d ) vs. O ( nd ) runtime and O ( n ) vs. O ( nd ) storage of Fastfood relative to Random Kitchen Sinks. 4.3. Random features for CIFAR-10 To understand the importance of nonlinear feature ex-pansions for a practical application, we benchmarked Fastfood, Random Kitchen Sinks on the CIFAR-10 dataset (Krizhevsky, 2009) which has 50,000 train-ing images and 10,000 test images. Each image has 32x32 pixels and 3 channels ( d = 3072). In our ex-periments, linear SVMs achieve 42.3% accuracy on the test set. Non-linear expansions improve the clas-sification accuracy significantly. In particular, Fast-food FFT ( X  X ourier features X ) achieve 63.1% while Fastfood ( X  X adamard features X ) and Random Kitchen Sinks achieve 62.4% with an expansion of n = 16 , 384. These are also best known classification accuracies using permutation-invariant representations on this dataset. In terms of speed, Random Kitchen Sinks is 5x slower (in total training time) and 20x slower (in predicting a label given an image) compared to Fastfood and Fastfood FFT. This demonstrates that non-linear expansions are needed even when the raw data is high-dimensional, and that Fastfood is more practical for such problems.
 In particular, in many cases, linear function classes are used because they provide fast training time, and especially test time, but not because they offer better accuracy. The results on CIFAR-10 demonstrate that Fastfood can overcome this obstacle.
 Summary We demonstrated that it is possible to compute n nonlinear basis functions in O ( n log d ) time, a significant speedup over the best competitive algo-rithms. This means that kernel methods become more practical for problems that have large datasets and/or require real-time prediction. In fact, Fastfood can be used to run on cellphones because not only it is fast, but it also requires only a small amount of storage. Acknowledgments We thank John Langford and Ravi Kumar for fruitful discussions.
 Ailon, N. and Chazelle, B. The fast Johnson X 
Lindenstrauss transform and approximate nearest neighbors. SICOMP , 2009.
 Aizerman, M. A., Braverman, A. M., and Rozono  X er,
L. I. Theoretical foundations of the potential func-tion method in pattern recognition learning. Autom. Remote Control , 25:821 X 837, 1964.
 Aronszajn, N. La th  X eorie g  X en  X erale des noyaux r  X eproduisants et ses applications. Proc. Cambridge Philos. Soc. , 39:133 X 153, 1944.
 Boser, B., Guyon, I., and Vapnik, V. A training algo-rithm for optimal margin classifiers. COLT 1992. Burges, C. J. C. Simplified support vector decision rules. ICML , 1996 Cortes, C. and Vapnik, V. Support vector networks. Machine Learning , 20(3):273 X 297, 1995.
 Dasgupta, A., Kumar, R., and Sarl  X os, T. Fast locality-sensitive hashing. SIGKDD , pp. 1073 X 1081, 2011. Fine, S. and Scheinberg, K. Efficient SVM training using low-rank kernel representations. JMLR , 2001. Frank, A. and Asuncion, A. UCI machine learning repository. http://archive.ics.uci.edu/ml .
 Girosi, F. An equivalence between sparse approxima-tion and support vector machines. Neural Compu-tation , 10(6):1455 X 1480, 1998.
 Girosi, F., Jones, M., and Poggio, T. Regularization theory and neural networks architectures. Neural Computation , 7(2):219 X 269, 1995.
 Gray, A. G. and Moore, A. W. Rapid evaluation of multiple density models. AISTATS , 2003.
 Jin, R., Yang, T., Mahdavi, M., Li, Y.F., and Zhou,
Z.H. Improved bound for the Nystrom X  X  method and its application to kernel classification, 2011. URL http://arxiv.org/abs/1111.2262 .
 Kimeldorf, G. S. and Wahba, G. A correspondence between Bayesian estimation on stochastic processes and smoothing by splines. Annals of Mathematical Statistics , 41:495 X 502, 1970.
 Kreyszig, E. Introductory Functional Analysis with Applications . Wiley, 1989.
 Krizhevsky, A. Learning multiple layers of features from tiny images. TR, U Toronto, 2009.
 Ledoux, M. Isoperimetry and Gaussian analysis. Lec-tures on probability theory and statistics, 1996. Lee, D. and Gray, A. G. Fast high-dimensional ker-nel summations using the Monte Carlo multipole method. NIPS , 2009.
 MacKay, D. J. C. Information Theory, Inference, and Learning Algorithms . Cambridge 2003.
 Mercer, J. Functions of positive and negative type and their connection with the theory of integral equa-tions. Royal Society London , A 209:415 X 446, 1909. Micchelli, C. A. Interpolation of scattered data: dis-tance matrices and conditionally positive definite functions. Constr. Approximation , 2:11 X 22, 1986. Neal, R. Priors for infinite networks. CRG-TR-94-1, U Toronto, 1994.
 Rahimi, A. and Recht, B. Random features for large-scale kernel machines. NIPS 20 , 2007.
 Rahimi, A. and Recht, B. Weighted sums of ran-dom kitchen sinks: Replacing minimization with randomization in learning. NIPS 21 , 2008.
 Sch  X olkopf, B., Smola, A. J., and M  X uller, K.-R. Nonlin-ear component analysis as a kernel eigenvalue prob-lem. Neural Comput. , 10:1299 X 1319, 1998.
 Sch  X olkopf, Bernhard and Smola, A. J. Learning with Kernels . MIT Press, Cambridge, MA, 2002.
 Smola, A. J. and Sch  X olkopf, B. Sparse greedy matrix approximation for machine learning. ICML , 2000. Smola, A. J., Sch  X olkopf, B., and M  X uller, K.-R. The Connection between Regularization Operators and Support Vector Kernels. Neur. Networks , 1998 Steinwart, Ingo and Christmann, Andreas. Support Vector Machines . Springer, 2008.
 Taskar, B., Guestrin, C., and Koller, D. Max-margin Markov networks. NIPS 16 , 2004.
 Tropp, J. A. Improved analysis of the subsampled randomized Hadamard transform Adv. Adapt. Data Anal., 2011.
 Vapnik, V., Golowich, S., and Smola, A. Support vec-tor method for function approximation, regression estimation, and signal processing. NIPS 9 , 1997. Wahba, G. Spline Models for Observational Data , CBMS-NSF , vol. 59, SIAM, Philadelphia, 1990. Williams, C. K. I. Prediction with Gaussian processes:
From linear regression to linear prediction and be-yond. In Jordan, M. I. (ed.), Learning and Inference in Graphical Models , Kluwer, 1998.
 Williams, C. K. I. and Seeger, M. Using the Nystrom
