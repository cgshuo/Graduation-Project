 The acquisition of subsentential paraphrases has at-tracted a lot of attention recently (Madnani and Dorr, 2010). Techniques are usually developed for extract-ing paraphrase candidates from specific types of cor-pora, including monolingual parallel corpora (Barzi-lay and McKeown, 2001), monolingual comparable corpora (Del  X  eger and Zweigenbaum, 2009), bilin-gual parallel corpora (Bannard and Callison-Burch, 2005), and edit histories of multi-authored text (Max and Wisniewski, 2010). These approaches face two main issues, which correspond to the typical mea-sures of precision , or how appropriate the extracted paraphrases are, and of recall , or how many of the paraphrases present in a given corpus can be found effectively. To start with, both measures are often hard to compute in practice, as 1) the definition of what makes an acceptable paraphrase pair is still a research question, and 2) it is often impractical to extract a complete set of acceptable paraphrases from most resources. Second, as regards the pre-cision of paraphrase acquisition techniques in par-ticular, it is notable that most works on paraphrase acquisition are not based on direct observation of larger paraphrase pairs. Even monolingual corpora obtained by pairing very closely related texts such as news headlines on the same topic and from the same time frame (Dolan et al., 2004) often contain unre-lated segments that should not be aligned to form a subsentential paraphrase pair. Using bilingual cor-pora to acquire paraphrases indirectly by pivoting through other languages is faced, in particular, with the issue of phrase polysemy, both in the source and in the pivot languages.

It has previously been noted that highly parallel monolingual corpora, typically obtained via mul-tiple translation into the same language, consti-tute the most appropriate type of corpus for ex-tracting high quality paraphrases, in spite of their rareness (Barzilay and McKeown, 2001; Cohn et al., 2008; Bouamor et al., 2010). We build on this claim here to propose an original approach for the task of subsentential alignment based on the compu-tation of a minimum edit rate between two sentential paraphrases. More precisely, we concentrate on the alignment of atomic paraphrase pairs (Cohn et al., 2008), where the words from both paraphrases are aligned as a whole to the words of the other para-phrase, as opposed to composite paraphrase pairs obtained by joining together adjacent paraphrase pairs or possibly adding unaligned words. Figure 1 provides examples of atomic paraphrase pairs de-rived from a word alignment between two English sentential paraphrases.
The remainder of this paper is organized as fol-lows. We first briefly describe in section 2 how we apply edit rate computation to the task of atomic paraphrase alignment, and we explain in section 3 how we can inform such a technique with paraphrase candidates extracted by additional techniques. We present our experiments and discuss their results in section 4 and conclude in section 5. TER-P LUS (Translation Edit Rate Plus) (Snover et al., 2010) is a score designed for evaluation of Ma-chine Translation (MT) output. Its typical use takes a system hypothesis to compute an optimal set of word edits that can transform it into some existing reference translation. Edit types include exact word matching, word insertion and deletion, block move-ment of contiguous words (computed as an approx-imation), as well as variants substitution through stemming, synonym or paraphrase matching. Each edit type is parameterized by at least one weight which can be optimized using e.g. hill climbing. TER-P LUS is therefore a tunable metric. We will henceforth design as TER MT the TER metric (basi-cally, without variants matching) optimized for cor-relation with human judgment of accuracy in MT evaluation, which is to date one of the most used metrics for this task.

While this metric was not designed explicitely for the acquisition of word alignments, it produces as a by-product of its approximate search a list of align-ments involving either individual words or phrases, potentially fitting with the previous definition of atomic paraphrase pairs. When applying it on a MT system hypothesis and a reference translation, it computes how much effort would be needed to obtain the reference from the hypothesis, possibly independently of the appropriateness of the align-ments produced. However, if we consider instead a pair of sentential paraphrases, it can be used to reveal what subsentential units can be aligned. Of course, this relies on information that will often go beyond simple exact word matching. This is where the capability of exploiting paraphrase matching can come into play: TER-P LUS can exploit a table of paraphrase pairs, and defines the cost of a phrase substitution as  X  X  function of the probability of the paraphrase and the number of edits needed to align the two phrases without the use of phrase substitu-tions X . Intuitively, the more parallel two sentential paraphrases are, the more atomic paraphrase pairs will be reliably found, and the easier it will be for TER-P LUS to correctly identify the remaining pairs. But in the general case, and considering less appar-ently parallel sentence pairs, its work can be facil-itated by the incorporation of candidate paraphrase pairs in its paraphrase table. We consider this possi-ble type of hybridation in the next section. In this article, we use three baseline techniques for paraphrase pair acquisition, which we will only briefly introduce (see (Bouamor et al., 2010) for more details). As explained previously, we want to evaluate whether and how their candidate paraphrase pairs can be used to improve paraphrase acquisition on sentential paraphrases using TER-PLUS . We se-lected these three techniques for the complementar-ity of types of information that they use: statistical word alignment without a priori linguistic knowl-edge, symbolic expression of linguistic variation ex-ploiting a priori linguistic knowledge, and syntactic similarity. Statistical Word Alignment The GIZA++ tool (Och and Ney, 2004) computes statistical word alignment models of increasing complexity from parallel corpora. While originally developped in the bilingual context of Machine Translation, nothing prevents building such models on monolingual corpora. However, in order to build reliable models it is necessary to use enough training material including minimal redundancy of words. To this end, we will be using monolingual corpora made up of multiply-translated sentences, allowing us to provide GIZA++ with all possible sentence pairs to improve the quality of its word alignments (note that following common practice we used symetrized alignments from the alignments in both directions). This constitutes an advantage for this technique that the following techniques working on each sentence pair independently do not have.
 Symbolic expression of linguistic variation The F
ASTR tool (Jacquemin, 1999) was designed to spot term variants in large corpora. Variants are de-scribed through metarules expressing how the mor-phosyntactic structure of a term variant can be de-rived from a given term by means of regular ex-pressions on word categories. Paradigmatic varia-tion can also be expressed by defining constraints between words to force them to belong to the same morphological or semantic family, both constraints relying on preexisting repertoires available for En-glish and French. To compute candidate paraphrase pairs using F ASTR , we first consider all the phrases from the first sentence and search for variants in the other sentence, do the reverse process and take the intersection of the two sets.
 Syntactic similarity The algorithm introduced by Pang et al. (2003) takes two sentences as in-put and merges them by top-down syntactic fusion guided by compatible syntactic substructure. A lexical blocking mechanism prevents sentence con-stituents from fusionning when there is evidence of the presence of a word in another constituent of one of the sentence. We use the Berkeley Probabilistic parser (Petrov and Klein, 2007) to obtain syntac-tic trees for English and its Bonsai adaptation for French (Candito et al., 2010). Because this process is highly sensitive to syntactic parse errors, we use k -best parses (with k = 3 in our experiments) and retain the most compact fusion from any pair of can-didate parses. We used the methodology described by Cohn et al. (2008) for constructing evaluation corpora and as-sessing the performance of various techniques on the task of paraphrase acquisition. In a nutshell, pairs of sentential paraphrases are hand-aligned and define a set of reference atomic paraphrase pairs at the level of words or blocks or words, denoted as R atom , and also a set of reference composite paraphrase pairs obtained by joining adjacent atomic paraphrase pairs (up to a given length), denoted as R . Techniques output word alignments from which atomic candi-date paraphrase pairs, denoted as H atom , as well as composite paraphrase pairs, denoted as H , can be extracted. The usual measures of precision , recall and f-measure can then be defined in the following way: p =
To evaluate our individual techniques and their use by the tunable TER-PLUS technique (hence-forth TER P ), we measured results on two different corpora in French and English. In each case, a held-out development corpus of 150 paraphrase pairs was used for tuning the TER P hybrid systems towards precision (  X  p ), recall (  X  r ), or F-measure (  X  f ). 1 All techniques were evaluated on the same test set consisting of 375 paraphrase pairs. For English, we used the MTC corpus described in (Cohn et al., 2008), which consists of multiply-translated Chi-nese sentences into English, with an average lexical overlap 2 of 65 . 91% (all tokens) and 63 . 95% (content words only). We used as our reference set both the alignments marked as  X  X ure X  and  X  X ossible X . For French, we used the CESTA corpus of news articles 3 obtained by translating into French from various lan-guages with an average lexical overlap of 79 . 63% (all tokens) and 78 . 19% (content words only). These figures reveal that the French corpus tends to contain more literal translations, possibly due to the original languages of the sentences, which are closer to the target language than Chinese is to English. We used the Y AWAT (Germann, 2008) interactive alignment tool and measure inter-annotator agreement over a subset and found it to be similar to the value reported by Cohn et al. (2008) for English.

Results for all individual techniques in the two languages are given on Figure 2. We first note that all techniques fared better on the French corpus than on the English corpus. This can certainly be ex-plained by the fact that the former results from more literal translations, which are consequently easier to word-align.

TER MT (i.e. TER tuned for Machine Transla-tion evaluation) performs significantly worse on all metrics for both languages than our tuned TER P ex-periments, revealing that the two tasks have differ-ent objectives. The two linguistically-aware tech-niques, F ASTR and P ANG , have a very strong pre-cision on the more parallel French corpus, and also on the English corpus to a lesser extent, but fail to achieve a high recall (note, in particular, that they do not attempt to report preferentially atomic para-phrase pairs). G IZA ++ and TER P para perform in the same range, with acceptable precision and re-call, TER P para performing overall better, with e.g. a 1.14 advantage on f-measure on French and 3.27 on English. Recall that TER P works independently on each paraphrase pair, while G IZA ++ makes use of artificial repetitions of paraphrases of the same sen-tence.

Figure 3 gives an indication of how well each technique performs depending on the difficulty of the task, which we estimate here as the value (1  X  TER ( para 1 , para 2 )) , whose low values cor-respond to sentences which are costly to trans-form into the other using TER. Not surprisingly, TER P para and G IZA ++, and P ANG to a lesser ex-tent, perform better on  X  X ore parallel X  sentential paraphrase pairs. Conversely, F ASTR is not affected by the degree of parallelism between sentences, and manages to extract synonyms and more generally term variants, at any level of difficulty.

We have further tested 4 hybrid configurations by providing TER P para with the output of the other individual techniques and of their union, the latter simply obtained by taking paraphrase pairs output by at least one of these techniques. On French, where individual techniques achieve good perfor-mance, any hybridation improves the F-measure over both TER P para and the technique used, the best performance, using F ASTR , corresponding to an im-provement of respectively +2.35 and +24.28 over TER P para and F ASTR . Taking the union of all tech-niques does not yield additional gains: this might be explained by the fact that incorrect predictions are proportionnally more present and consequently have a greater impact when combining techniques without weighting them, possibly at the level of each prediction. 4 Successful hybridation on English seem harder to obtain, which may be partly attributed to the poor quality of the individual techniques relative to TER P para . We however note anew an improve-ment over TER P para of +1.81 when using F ASTR . This confirms that some types of linguistic equiva-lences cannot be captured using edit rate computa-tion alone, even on this type of corpus. In this article, we have described the use of edit rate computation for paraphrase alignment at the sub-sentential level from sentential paraphrases and the possibility of informing this search with paraphrase candidates coming from other techniques. Our ex-periments have shown that in some circumstances some techniques have a good complementarity and manage to improve results significantly. We are currently studying hard-to-align subsentential para-phrases from the type of corpora we used in order to get a better understanding of the types of knowledge required to improve automatic acquisition of these units.
Our future work also includes the acquisition of paraphrase patterns (e.g. (Zhao et al., 2008)) to gen-eralize the acquired equivalence units to more con-texts, which could be both used in applications and to attempt improving further paraphrase acquisition techniques. Integrating the use of patterns within an edit rate computation technique will however raise new difficulties.

We are finally also in the process of conducting a careful study of the characteristics of the para-phrase pairs that each technique can extract with high confidence, so that we can improve our hybri-dation experiments by considering confidence val-ues at the paraphrase level using Machine Learning. This way, we may be able to use an edit rate com-putation algorithm such as TER-PLUS as a more efficient system combiner for paraphrase extraction methods than what was proposed here. A poten-tial application of this would be an alternative pro-posal to the paraphrase evaluation metric P ARAMET -RIC (Callison-Burch et al., 2008), where individual techniques, outputing word alignments or not, could be evaluated from the ability of the informated edit rate technique to use correct equivalence units. This work was partly funded by a grant from LIMSI. The authors wish to thank the anonymous reviewers for their useful comments and suggestions.
