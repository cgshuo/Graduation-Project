 Hic hem Sah bi 1 sahbi@telecom-p aristech.fr Jean-Yv es Audib ert 2 ; 3 audiber t@cer tis.enpc.fr Willo w -ENS / INRIA, 45 rue d'Ulm, 75005 Paris, France. Ob ject recognition is one of the biggest challenges in vision and its interest is still gro wing (Ev eringham et al., 2007). Among existing metho ds, those based on mac hine learning (ML), sho w a particular interest as they are performan t and theoretically well grounded (Bishop, 2007). ML approac hes, suc h as the popular supp ort vector net works (Boser et al., 1992), basically require the design of similarit y measures, also referred to as kernels , whic h should pro vide high values when two objects share similar structures/app earances and should be invarian t, as much as possible, to the linear and non-linear transformations. Kernel-based object recognition metho ds were initially holistic , i.e., eac h object is mapp ed into one or multiple xed-length vectors and a similarit y, based on color, texture or shap e (Sw ain &amp; Ballard, 1991; Chap elle et al., 1999), is then de ned. Local kernels, i.e., those based on bags or local sets were introduced in order to represen t data whic h cannot be represen ted by ordered and xed-length feature vectors, suc h as graphs, trees, interest points, etc (Gartner, 2003). It is well kno wn that both holistic and local kernels should satisfy certain prop erties among them the positiv e de nite-ness, low complexit y for evaluation, exibilit y in order to handle variable-length data and also invariance. Holistic kernels have the adv antage of being simple to evaluate, discriminating but less exible than local kernels in order to handle invariance 1 . While the design of kernels gathering exibilit y, invariance and low complexit y is a challenging task; the pro of of their positiv e de niteness is sometimes harder (Cuturi, 2005). This prop erty also kno wn as the Mercer condition ensures, according to Vapnik's SVM theory (Vapnik, 1998), optimal generalization per-formance and also the uniqueness of the SVM solution. Consider a database of objects (images), eac h one seen as a constellation of local features, for instance interest points (Sc hmid &amp; Mohr, 1997; Lowe, 2004; Lazebnik et al., 2004), extracted using any suitable lter (Harris &amp; Stephens, 1988). Again, original holistic kernels explicitly (or implicitly) map objects into xed-length feature vectors and tak e the sim-ilarit y as a decreasing function of any well-de ned distance (Barla et al., 2002). In con trast to holistic kernels, local ones are designed in order to handle variable-length and unordered data. Tw o families of local kernels can be found in the literature; those based on statistical \length-insensitiv e" measures suc h as the Kullbac k Leibler divergence, and those whic h require a preliminary step of alignmen t. In the rst family , the authors in (Kondor &amp; Jebara, 2003; Moreno et al., 2003) estimate for eac h object (con-stellation of local features) a probabilit y distribution and compute the similarit y between two objects (two distributions) using the \Kullbac k Leibler divergence" in (Moreno et al., 2003) and the \Bhattac haryy a anit y" in (Kondor &amp; Jebara, 2003). Only the function in (Kondor &amp; Jebara, 2003) satis es the Mercer condition and both kernels were applied for image recognition tasks. In (W olf &amp; Shash ua, 2003), the authors discuss a new type of kernel referred to as \principal angles" whic h is positiv e de nite. Its de nition is based on the computation of the principal angles between two linear subspaces under an orthogonalit y constrain t. The authors demonstrate the validit y of their metho d on visual recognition tasks including classi cation of motion tra jectory and face recognition. An extension to subsets of varying cardinalit y is prop osed in (Shash ua &amp; Hazan, 2004). In this rst family of kernels, the main dra wbac k, in some metho ds, resides is the strong assumption about the used probabilistic mo dels in order to appro ximate the set of local features whic h may not hold true in practice.
 In the second family , the \max" kernel (W allra ven et al., 2003) considers the similarit y function, between two feature sets, as the sum of their matc hing scores and unlik e discussed in (W allra ven et al., 2003) this kernel is actually not Mercer (Bahlmann et al., 2002). In (Lyu, 2005), the authors introduced the \circular-shift" kernel de ned as a weigh ted com bination of Mer-cer kernels using an exp onen t. The latter is chosen in order to give more prominence to the largest terms so the resulting similarit y function appro ximates the \max" and also satis es the Mercer condition. The authors com bined local features and their relativ e an-gles in order to mak e their kernel rotation invarian t and they sho w its performance for the particular task of object recognition. In (Boughorb el, 2005), the au-thors introduced the \intermediate" matc hing kernel, for object recognition, whic h uses virtual local fea-tures in order to appro ximate the \max" while sat-isfying the Mercer condition. Recen tly, (Grauman &amp; Darrell, 2007) introduced the \pyramid-matc h" kernel, for object recognition and documen t analysis, whic h maps feature sets using a multi-resolution histogram represen tation and computes the similarit y using a weigh ted histogram intersection. The authors sho wed that their function is positiv e de nite and can be com-puted linearly with resp ect to the num ber of local fea-tures. Other matc hing kernels include the \dynamic programming" function whic h pro vides, in (Bahlmann et al., 2002), an e ectiv e matc hing strategy for hand-written character recognition, nev ertheless the Mercer condition is not guaran teed. 1.1. Motiv ation and Con tribution The success of the second family of local kernels strongly dep ends on the qualit y of alignmen ts whic h are dicult to obtain mainly when images con tain re-dundan t and rep eatable structures. Regardless the Mercer condition, a naiv e matc hing kernel (suc h as the \max"), whic h looks for all the possible alignmen ts and sums the best ones, will certainly fail and results into man y false matc hes (see Figure 1, left). The same argumen t is supp orted in (Sc hmid &amp; Mohr, 1997), for the general problem of visual features matc hing, about the strong spatial correlation between interest points and the corresp onding close local features in the image space. This limitation also app ears in closely related areas suc h as text analysis, and particularly string alignmen t. A simple example, of aligning two strings (\Sir" and \Hi Sir") using a simple similarit y mea-sho ws that without any extra information about the context (i.e., the sub-string) surrounding eac h char-acter in (\Sir" and \Hi Sir"), the alignmen t pro cess results into false matc hes (See Table 1). Henc e, it is necessary to consider the context as a part of the align-ment process when designing kernels.
 In this pap er, we introduce a new kernel, called \con text-dep enden t" (or \CDK") and de ned as the xed-p oint of an energy function whic h balances an \alignmen t qualit y" term and a \neigh borho od" crite-rion. The alignmen t qualit y is inversely prop ortional to the exp ectation of the Euclidean distance between the most likely aligned features (see Section 2) while the neigh borho od criterion measures the spatial coher-ence of the alignmen ts; given a pair of features ( f p ;f with a high alignmen t qualit y, the neigh borho od criterion is prop ortional to the alignmen t qualit y of all the pairs close to ( f p ;f q ). The gener al form of \CDK" captur es the similarity betwe en any two featur es by incorporating also their context, i.e., the similarity of the surr ounding featur es. Our prop osed kernel can be view ed as a varian t of \dynamic programming" kernel (Bahlmann et al., 2002) where instead of using the ordering assumption we consider a neigh borho od assumption whic h states that two points matc h if they have similar features and if they satis es a neigh borho od criterion i.e., their neigh bors matc h too. This also app ears in other well studied kernels suc h as Fisher (Jaakk ola et al., 1999), whic h implemen ts the conditional dep endency between data using the Mark ov assumption. \CDK" also implemen ts suc h dep endency with an extra adv antage of being the xed-p oint and the (sub)optimal solution of an energy function closely related to the goal of our application. This goal is to gather the prop erties of exibilit y, invariance and mainly discrimination by allo wing eac h local feature to consider its con text in the matc hing pro cess. Notice that the goal of this pap er is not to extend local features to be global and doing so (as in (Mortensen et al., 2005; Amores et al., 2005)) mak es local features less invarian t, but rather to design a similarit y kernel (\CDK") whic h captures the con text while being invarian t. Even though we investigate \CDK" in the particular task of object recognition, we can easily extend it to handle closely related areas in mac hine learning suc h as text alignmen t for documen t retriev al (Nie et al., 1999), mac hine translation (Sim et al., 2007) and bioinformatics (Sc holk opf et al., 2004).
 In the remainder of this pap er we consider the follo wing terminology and notation. A feature refers to a local interest point x p i = ( g ( x p i ) ; f ( x here i stands for the i th sample of the subset S p = f x whic h pro vides the class or the subset including x p i . ( x p i ) 2 R 2 stands for the 2 D coordinates of the interest-p oint x p i while f ( x p i ) 2 R s corresp onds to the descriptor of x p i (for instance the 128 coecien ts of the SIFT(Lo we, 2004)). We de ne X as the set of all possible features tak en from all the possible images in the world and X is a random variable standing for a sample in X . We also consider k t : X X ! R as a symmetric function whic h, given two samples ( x p i ;x q pro vides a similarit y measure. Other notations will be introduced as we go along through di eren t sections of this pap er whic h is organized as follo ws. We rst in-troduce in Section 2, our energy function whic h mak es it possible to design our con text-dep enden t kernel and we sho w that this kernel satis es the Mercer condition so we can use it for supp ort vector mac hine training and other kernel metho ds. In Section 3 we sho w the application of this kernel in object recognition. We discuss in Section 4 the adv antages and weaknesses of this kernel and the possible extensions in order to handle other tasks suc h as string matc hing and mac hine translation. We conclude in Section 5 and we pro vide some future researc h directions. De ne X = [ p 2 N + S p as the set of all possible interest points tak en from all the possible objects in the world. We assume that all the objects are sampled with a given cardinalit y i.e., jS p j = n , jS q j = m; 8 p;q 2 N + ( n and m migh t be di eren t). Our goal is to design a kernel K whic h pro vides the similarit y between any two objects (subsets) S p , S q in X .
 De nition 1 (Subset Kernels) let X be an input space, and consider S p ; S q X as two nite subsets of X . We de ne the similarity function or kernel K betwe en S p = f x p i g and S q = f x q j g as K ( S p ; S P here k is symmetric and con tinuous on X X , so K will also be con tinuous and symmetric. Since K is de ned as the cross-similarit y k between all the possible sample pairs tak en from S p S q , it is obvious that K has the big adv antage of not requiring any (hard) alignmen t between the samples of S p and S . Nev ertheless, for a given S p , S q , the value of K ( S p ; S q ) should be dominated by P i max j k x p i ;x so k should be appropriately designed (see Section 2.1). Let X be a random variable standing for samples tak en from S p and X 0 is de ned in a similar way for the subset S q . We design our kernel k ( x p i ;x q j ) = P ( X x ;X = x p i ) as the joint probability that x q j matc hes x . Again, it is clear enough (see Figure 1 and Table 1) that when this join t probabilit y is estimated using only the sample coordinates (without their con texts), this may result in man y false matc hes and wrong estimate of P ( X 0 = x q j ;X = x p i ) Before describing the whole design of k , we start with our de nition of con text-dep enden t kernels. De nition 2 (Con text-Dep enden t Kernels) we de ne a context-dep endent kernel k as any symmetric, continuous and recursive function k : X X ! R such that k ( x p i ;x q j ) is equal to c ( x p i ;x q j ) h here c is a positive (semi) de nite and context-fr ee (non-r ecursive) kernel, V ( x;x 0 ;y;y 0 ) is a monotonic decreasing function of any (pseudo) distanc e involving ( x;x 0 ;y;y 0 ) and h ( x ) is monotonic ally incr easing. 2.1. Approac h We consider the issue of designing k using a variational framew ork. Let I p = f 1 ;:::;n g , I q = f 1 ;:::;m g , = f k ( x p i ;x q j ) g , d ( x p i ;x q j ) = k f ( x p N p ( x ( p de nes a neigh borho od and N q is de ned in the same way for S q ). Consider ; 0, ( i;j ) 2I p I q , = f k ( x p i ;x q j ) g is found by solving min X s.t. k ( x p i ;x q j ) 2 [0 ; 1] ; X The rst term measures the qualit y of matc hing two descriptors f ( x p i ), f ( x q j ). In the case of SIFT, this is considered as the distance, d ( x p i ;x q j ), between the 128 SIFT coecien ts of x p i and x q j . A high value of d ( x p i ;x q j ) should result into a small value of k ( x and vice-v ersa.
 The second term is a regularization criterion whic h considers that without any a priori kno wledge about the aligned samples, the probabilit y distribution f k ( x p i ;x q j ) g should be at so the negativ e of the entrop y is minimized. This term also helps de ning a simple solution and solving the constrained minimiza-tion problem easily . The third term is a neigh borho od criterion whic h considers that a high value of k ( x p i should imply high kernel values in the neigh borho ods N p ( x to consider the con text (spatial con guration) of eac h sample in the matc hing pro cess.
 We form ulate the minimization problem by adding an equalit y constrain t and bounds whic h ensure that f k ( x p i ;x q j ) g is a probabilit y distribution. Prop osition 1 (1) admits a solution in the form of a context-dep endent kernel k t ( x p i ;x q j ) = v t ( x Z , with t 2 N + , Z t = P i;j v t ( x p i ;x q j ) and v t de ne d as which is also a Gibbs distribution.
 Pro of. the pro of, length y, is omitted and it is avail-able in a researc h rep ort (Sah bi et al., 2007). In (2), we set v 0 to any positiv e de nite kernel (see prop osition 3) and we de ne V ( x p i ;x r k ;x q j of any (pseudo) distance involving ( x p i ;x r k ), not neces-sarily symmetric . In practice, we consider g ( x p i ;x r 1 It is easy to see that k t is a P-k ernel on any S p S q (Haussler, 1999) (as the join t probabilit y over sample pairs tak en from any S p and S q sums to one), so the value of the subset kernel K ( S p ; S q ) de ned in (1) is constan t and useless . To mak e k t (up to a factor) a P-k ernel on X X (and not on S p S q ), we cancel the equalit y constrain t in (1) and we can pro ve in a similar way that k t ( x p i ;x q j ) is equal to v t ( x is still a con text-dep enden t kernel. 2.2. Mercer Condition Let X be an input space and let k t : X X ! R be symmetric and con tinuous. k t is Mercer, i.e., pos-itiv e (semi) de nite, if and only if any Gram (kernel scalar pro duct) matrix built by restricting k t to any nite subset of X is positiv e (semi) de nite. A Mer-cer kernel k t guaran tees the existence of a repro ducing kernel Hilb ert space H where k t can be written as a dot pro duct i.e., 9 t : X ! H suc h that 8 x;x 0 2 X , k ( x;x 0 ) = t ( x ) ; t ( x 0 ) .
 Prop osition 2 ex. (S-T aylor &amp; Cristianini, 2000) the sum and the product of any two Mer cer kernels is a Mer cer kernel. The exponential of any Mer cer kernel is also a Mer cer kernel.
 Pro of. see, for instance, (S-T aylor &amp; Cristianini, 2000).
 Now, let us state our result about the positiv e de nite-ness of the \CDK" kernel.
 Prop osition 3 consider g : X X ! R , let V ( x p i ;x p k ;x q j ;x q ` ) = g ( x p i ;x p k ) g ( x de nite. The kernel k t is then positive de nite. Pro of. Initially ( t = 0), k 0 is per de nition a positiv e de nite kernel. By induction, let us assume k t 1 a Mercer kernel i.e., 9 t 1 : k t 1 ( x;x 0 ) = t 1 ( x ) ; t 1 ( x 0 ) , 8 x;x 0 2 X . Now, the sucien t condition will be to sho w that
P y;y 0 V ( x;y;x 0 ;y 0 ) k t 1 ( y;y 0 ) is also a Mercer ker-nel. Then, by the closure of the exp onen tial and the pro duct (see prop osition 2), k t will then be Mercer. We need to sho w 8 x 1 ;:::;x d 2X ; 8 c 1 ;:::;c d 2 R ; ( ) = X We have ( ) = X Corollary 1 K de ne d in (1) is also a Mer cer kernel. Pro of. the pro of is straigh tforw ard for the partic-ular case n = m . As k t ( x p i ;x q j ) = h t ( x p i ) ; we can write K ( S p ; S q ) = P i;j h t ( x p i ) ; t ( x pro duct in some Hilb ert space. The pro of can be found in (S-T aylor &amp; Cristianini, 2000) for the general case of nite subsets of any length. 2.3. Algorithm and Setting The factor , in k t , acts as a scale parameter and it is selected using here E denotes the exp ectation and X r 1 (also X r 2 ) denotes a random variable standing for samples in S r . The coecien t con trols the tradeo between the alignmen t qualit y and the neigh bor-hood criteria. It is selected by cross-v alidation and it should guaran tee k t ( x p i ;x q j ) 2 [0 ; 1]. If be selected in [0 ; 2 A ].
 Consider P , Q as the intrinsic adjacency matrices of S p and S q resp ectiv ely de ned as P i;k = g ( x Q j;` = g ( x is iterativ ely found using Algorithm (\CDK") (see ta-ble 2) and con verges to a xed point (see. Section 2.4). Algorithm (CDK) 2.4. Con vergence Let us assume 0 g 1, and remind ( t ) 2 R n m be the vector of comp onen ts ( t ) i;j = k t ( x p i ;x q duce the mapping f : R n m ! R n m de ned by its comp onen t f i;j ( v ) as exp 1 By construction of the kernel k t , we have ( t ) = f ( t 1) . Let A and B satisfy Consider L = 2 B exp 2 A ; and let B = v 2 R n m : 8 1 i n; 1 j m; j v i;j j 1 be the kk 1 -ball of radius 1. Finally , let kk 1 denote Prop osition 4 If k (0) k 1 1 and 2 A , then we have f ( B ) B , and on B , f is L -Lipschitz for the norm kk 1 .
 In particular, if L &lt; 1 , then ther e exists a unique ~ v 2B such that f (~ v ) = ~ v , and the sequenc e ( ( t ) ) satis es Pro of. The rst assertion is pro ved by induction by chec king that for k v k 1 1, we have f i;j ( v ) exp 1 + 2 P k;` g ( x For the second assertion, note that for any v in B , exp 2 A , for any v;v 0 in B , we have k f ( v ) f ( v 0 ) k 1 = X ( ) X whic h pro ves the second assertion. The last assertion directly comes from the xed-p oint theorem. Exp erimen ts were conducted on the Swedish set (15 classes, 75 images per category) and a random sub-set of MNIST digit database (10 classes, 200 images per category). Eac h class in Swedish (resp. MNIST) is split into 50+25 (resp. 100 +100) con tours for training and testing. Interest points were sampled from eac h con tour in MNIST (resp. Swedish) and enco ded us-ing the 60 (resp. 16) coecien ts of the shap e-con text descriptor (Belongie et al., 2000). 3.1. Generalization and Comparison We evaluate k t ; t 2 N + using two initializations: (i) linear k 0 ( x;x 0 ) = k l ( x;x 0 ) = h x;x 0 i (ii) and goal is to sho w the impro vemen t brough t when using k ; t 2 N + , so we compared it against the standard con text-free kernels k l and k p (i.e., k t ; t = 0). For this purp ose, we trained a \one-v ersus-all" SVM classi er for eac h class in both MNIST and Swedish using the subset kernel K ( S p ; S q ) = P x 2S The performance are measured, on di eren t test sets, using n -fold cross-v alidation error ( n = 5). We remind that is set using (3) as the left-hand side of k t corresp onds to the Gaussian kernel with scale . In practice, = 0 : 1. The in uence (and the per-formance) of the righ t-hand side of k t increases as increases (see. Figure 2), nev ertheless the con vergence of k t to a xed point is guaran teed only if 2 [0 ; 2 A ]. Therefore, it becomes obvious that should be set practice, 0 g 1 and A = 1).
 Table 3 sho ws the 5-fold cross validation errors on MNIST and Swedish for di eren t iterations; we clearly see the out-p erformance and the impro vemen t of the \CDK" kernel ( k t , t 2 N + ) with resp ect to the con text-free kernels used for initialization ( k 0 = k l or k p .) The adjacency matrix P , in k t , pro vides the intrinsic prop erties and also characterizes the geometry of an object S p . Let us remind N p ( x translation and rotation invarian t and can also be made scale invarian t when p is adapted to the scale of ( x p i ). It follo ws that the righ t-hand side of our ker-nel is invarian t to any 2 D similarit y transformation. Notice, also, that the left-hand side of k t involves similarit y invarian t descriptors f ( x p i ), f ( x q j (and K ) is similarit y invarian t.
 One curren t limitation of our kernel k t resides in its evaluation complexit y. Assuming k t 1 kno wn, for a given pair x p i , x q j , this complexit y is O max ( N 2 ;s ) , where s is the dimension of f ( x p and N = max i;p # fN p ( x p i ) g . It is clear enough that when N &lt; is strictly equiv alen t to that of usual kernels suc h as the linear. Nev ertheless, the worst case ( N mak es our kernel evaluation prohibitiv e and this is mainly due to the righ t-hand side of k t ( x p i ;x q j ) whic h requires the evaluation of kernel sums in a hypercub e of dimension 4. A simple and straigh tforw ard gener-alization of the integral image (see for instance (Viola &amp; Jones, 2001)) will reduce this complexit y to O ( s ). Finally , the out-p erformance of our kernel comes essen tially from the inclusion of the con text. This strongly impro ves the precision and helps including the intrinsic prop erties (geometry) of objects. Even though tested only on visual object recognition, our kernel can be extended to man y other pattern analysis problems suc h as bioinformatics, speech and text. For instance, in text analysis and particularit y mac hine translation (Sim et al., 2007), the design of a similarit y kernel between words in two di eren t languages, can be achiev ed using any standard dictionary . Of course, the latter de nes similarit y between any two words ( w e ;w f ) indep enden tly from their bilingual training text (or bitext), i.e., the phrases where ( w e ;w f ) migh t app ear and this results into bad translation performances. A better estimate of similarit y between two words ( w e ;w f ), can be achiev ed using their con text i.e., the set of words whic h cooccur frequen tly with ( w e ;w f ) (Ko ehn et al., 2003). We introduced in this pap er a new type of kernels re-ferred to as con text-dep enden t. Its strength resides in the impro vemen t of the alignmen ts between interest points and this is considered as a preliminary step in order to increase the robustness and the precision of object recognition.
 We have also sho wn that our kernel is Mercer and ap-plicable to SVM learning. The latter, achiev ed for shap e recognition problems, has better performance than SVMs with con text-free kernels. Future work in-cludes the comparison of our kernel with other con text-free kernels and its application in scene and object un-derstanding using other standard databases.
 This work was supp orted in part by the Agence Na-tionale de la Rec herc he, pro ject \Mo deles Graphiques et Applications" and the pro ject \Infom@gic".
