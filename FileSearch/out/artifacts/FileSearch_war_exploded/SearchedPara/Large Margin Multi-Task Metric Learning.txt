 have started to build a theoretical foundation that underpins these initial empirical findings [1, 2, 3]. might not share the same classes (e.g. identifying multiple diseases from a particular patient data). treatment.
 rule that explicitly models the commonalities and specifics of different tasks. This section describes the large margin nearest neigh-bor algorithm as introduced in [20]. For now, we will focus on a single-task learning framework, with a train-ing set consisting of n examples of dimensionality d , { ( x i ,y i ) } n Here, c denotes the number of classes. The Maha-lanobis distance between two inputs x i and x j is de-fined as where M is a symmetric positive definite matrix (
M 0 ). The definition in eq. (1) reduces to the Eu-clidean metric if we set M to the identity matrix, i.e.
 M = I . The lmnn algorithm learns the matrix M for the Mahalanobis metric 1 in eq. (1) explicitly to en-hance k -nearest neighbor classification.
 Lmnn mimics the non-continuous and non-differentiable leave-one-out classification error of k NN with a convex loss function. The loss function encourages the local neighborhood around every input the squared distance d 2 M (  X  ,  X  ) : the impostors (marked as squares) to be further away than the target neighbors. P stated as the SDP shown in Table 1. This optimization problem has O ( kn 2 ) constraints of type (1) and (2) , along with the positive semidefinite constraint of a d  X  d matrix M . Hence, standard off-the shelf packages are not particularly suited to solve this SDP. For this paper we use the special purpose sub-gradient descent solver, developed in [20], which can handle data sets on the order of tens of thousands of samples. As the optimization problem is not sensitive to the exact choice of the tradeoff constant  X  [20], we set  X  = 1 throughout this paper. In this section, we briefly review the approach pre-throughout this section we will assume a binary classification scenario, in particular y i  X  X  +1 ,  X  1 } . optimization problem is to minimize the following cost: vector. previous section, we model the commonalities between various tasks through a shared Mahalanobis metric with M task t as M t for t &gt; 0 adds the task specific distance transformation. parameters M 1 ,..., M T . To ensure this balance, we use the regularization term stated below: M  X  0 = 0 and  X  t&gt; 0  X  X  X  , we want our formulation to reduce to T independent lmnn algorithms. 4.1 Theoretical Properties optimization is convex.
 pseudo-metrics for all 0  X  t  X  T .
 there exists some matrix L t such that L &gt; t L t = M 0 + M t . Hence we can rephrase eq.( 4) as and d (  X  ,  X  ) is a metric . that our new formulation preserves convexity.
 Theorem 2 The mt-lmnn optimization problem in Table 2 is convex. M t exclusively in terms of the squared distance d 2 (  X  ,  X  ) . This can be expressed as ( k convex [5], hence this concludes the proof. overview of the two datasets and then present results in various multi-task and domain adaptation settings. the dimensionality from 617 to 169.
 the same input data.
 over our method).
 use of mt-lmnn in the domain adaptation (or cold start) scenario. 5.1 Multi-task Learning following sub-sections.
 isolet1-5 are the 5 tasks and all of them share the same 26 labels. We compared the performance of our mt-lmnn algorithm with different baselines in table 3. The first 3 algorithms are k NN classifiers using different metrics.  X  X uc X  represents the Euclidean metric,  X  X -lmnn X  is the metric obtained from lmnn trained on the union of the training data of all tasks (essentially  X  X ooling X  all the data and ignoring the multi-task aspect),  X  X t-lmnn X  is single-task lmnn trained indepen-dent of other tasks. As additional compari-son we have also included results from lin-ear single-task and multi-task support vec-hidden layers) [6] denoted as  X  X t-net X  and  X  X t-net X  respectively. classification phase is shown in table 4.
 support vector machines.
 Label-Incompatible Multi-task Learning To demonstrate mt-lmnn X  X  ability to learn multiple tasks having dif-all of them share the same input.
 networks on these tasks. Mt-lmnn yields the lowest average performance across all tasks. very well in tasks with many classes (in particular 40-way classification of task 1). 5.2 Domain Adaptation plenty of data, that may not have the same sample distribution as that of the target . available.
 metrics are shown in Figure 3. In the absence of any training data from isolet5 (also referred to as the cold-start scenario ), we used the global metric M 0 learned by mt-lmnn on tasks isolet1-4. U-lmnn and mt-lmnn global metric perform much better than the Euclidean metric, with U-lmnn giving slightly better classification.
 With the availability of more data charac-teristic of the new task, the performance of mt-lmnn improves much faster than U-lmnn. Note that the Euclidean error actu-ally increases with more target data, pre-sumably because utterances from the same speaker might be close together in Eu-clidean space even if they are from different classes  X  leading to additional misclassifications. components.
 expressed as a convex optimization problem with the accompanying convergence guarantees. planes. mt-lmnn is applicable (and effective) on multiple multi-class tasks with different sets of classes. metrics that are shared only by a strict subset of the tasks.
 mt-lmnn provides a more integrative methodology for metric learning across multiple learning problems. Acknowledgments UCSD FWGrid Project, NSF Research Infrastructure Grant Number EIA-0303622. [5] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [6] R. Caruana. Multitask learning. Machine Learning , 28(1):41 X 75, 1997. [15] T. Evgeniou and M. Pontil. Regularized multi X  X ask learning. In KDD , pages 109 X 117, 2004. [18] I. T. Jolliffe. Principal Component Analysis . Springer-Verlag, New York, 1986.
