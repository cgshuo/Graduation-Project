 Learning temporal graph structures from time series data reveals important dependency relationships between current observations and histories. Most previous work focuses on learning and predicting with  X  X tatic X  temporal graphs only. However, in many applications such as mechanical systems and biology systems, the temporal dependencies might change over time. In this paper, we develop a dynamic temporal graphical models based on hidden Markov model regression and lasso-type algorithms. Our method is able to integrate two usually separate tasks, i.e. inferring underlying states and learning temporal graphs, in one unified model. The output temporal graphs provide better understanding about complex systems, i.e. how their dependency graphs evolve over time, and achieve more accurate predictions. We ex-amine our model on two synthetic datasets as well as a real application dataset for monitoring oil-production equipment to capture different stages of the system, and achieve promis-ing results.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications X  Data Mining General Terms: Algorithms, Performance, Design Keywords: Graphical models, time series data, structure learning
Learning the graph structures of graphical models from time-series data has wide applications, such as gene regula-tory network discovery [8], social network analysis [10], key performance indictor analysis in business applications [2], and so on. Previously, large amount of work has been de-voted to uncover the dependencies between variables from the same time-stamp [12]. For example, there have been extensive studies on learning static graph structure, i.e. a graph structure invariant over time, as well as learning dy-namic graph structures [6, 11].
 In contrast, there is very limited work on identifying  X  X em-poral graph structure X , i.e. the networks that reveal depen-dencies between current observations and histories. Many time-series algorithms such as vector autoregression (VAR) and autoregressive moving average (ARMA) have been de-veloped for forecasting with a  X  X tatic X  temporal graph [3]. In many applications, temporal graph structures might also change over time. For example, in biology systems, the tem-poral regulatory networks between genes vary over different stages of the life cycle of a cell; similarly, in financial ap-plications, the temporal dependencies between the stocks of different companies also change over time. Therefore it is es-sential to develop techniques to uncover the dynamic tempo-ral graphs so that better understanding about the evolving process of a complex system can be provided.

In order to learn the dynamic temporal graph structures, we need to infer both the underlying states and the tem-poral graphs associated with each state. Learning graph structures from data itself is an NP-hard problem [5], and many heuristic algorithms have been developed [12]. Most recently, L1-based learning algorithms establish themselves as one of the most promising techniques for structure learn-ing, especially for applications with inherent sparse graph structures [17, 13, 22]. The main idea of this type of al-gorithms is to impose an L-1 regularization penalty on the regression coefficients [21] so that they can effectively se-lect a set of sparse neighbors using lasso-type algorithms. It has been proved that this method can correctly recover the undirected network structure under certain assumptions about the data [17] and recent studies demonstrate its effec-tiveness with strong empirical evidence. Later the L1-based algorithms have been extended to learning static temporal graphs and achieve promising results [13, 22, 20].
In parallel, hidden Markov models (HMM) have been suc-cessful to uncover the underlying states for sequential data [19]. One naive approach to learn dynamic temporal graphs can be a two-step process, i.e. first train an HMM to in-fer the underlying states and then learn a state-dependent temporal graphs using only the observations associated with that state. One major disadvantage of this simple approach is that the underlying states and the temporal graph struc-tures are usually inter-dependent. Therefore, it would be better to integrate the two steps into one framework. In this paper, we develop a family of dynamic temporal graph learning techniques. It is based on one specific HMM for regression, called autoregressive HMM or in general hidden Markov model regression (HMMR), which is widely used in speech recognition and biology applications [19, 9, 4, 18]. We extend HMMR so that it can handle lasso-style regres-sion algorithms. In this way, we are able to combine previous work on L1-based structure learning algorithms with HMM for inferring underlying states in one unified model. As we show later, our model is easy to implement and computa-tionally efficient.

Most importantly, the dynamic temporal graphs output by our model have many applications. For example, in a system monitoring oil-production equipment, a time-series data from different sensors of the system can be observed. The dependency networks between sensor measures are not fully understood and they can change over time. The discov-ery of temporal graphs can provide additional insights about how the system functions over time. In addition, one of the biggest challenges in time-series analysis is to detect early signals of anomaly. The dynamic temporal graphs are able to capture the potentially different stages of a system and thus provide a more accurate baseline model for anomaly detection.

The rest of the paper is organized as follows: we first re-view the algorithms to learn static temporal graphs in Sec-tion 2; then we motivate why it is important to uncover dynamic temporal graphs and describe the details of our proposed algorithms in Section 3. We show experiment re-sults on two synthetic datasets in Section 4 and one applica-tion data for monitoring oil-production system in Section 5. Finally, we summarize the paper and conclude with future work.
Graphical models are a powerful tool to handle conditional probability distributions using graph theory [12]. It consists of a graph, in which nodes represent random variables and the absence of an edge represents independence, and a set of probability distributions defined over the graph. Simply put, we can think of the graphs as a convenient representa-tion of dependencies between variables. Learning the graph structures of dependency networks can provide significant insights about the data and has found applications in vari-ous domains [8, 10, 2].

For time series analysis, it is also important to reveal the dependencies between current observations and histories represented by temporal graphs . These graphs not only can provide insights about the concerned time series, but also are essential for parameter estimation, prediction, cross pre-diction, i.e. the use of one time series to help predict another time series, and so on. In this paper, we are interested in uncovering the feature graph, which characterizes the time-specific dependencies between features (variables), x 1 , ..., x The edges are labeled with a natural number called the lag of the edge.

First, we define the underlying graphical models for the feature graph. In the simple autoregressive (AR) model, an observation of time t is given by where  X  l are the parameters of the model, c is a constant and  X  t is the Gaussian noise. The stochastic model associ-ated with our feature graph is simply a vector autoregres-sive (VAR) model on the lagged temporal variables, given the assumption that each of the models is a linear Gaussian Figure 1: (A) Graphical model representation of VAR model (B) Temporal graph structure (C) Fea-ture graph view model. Figure 1 shows an example of VAR model and its corresponding feature graph.

Next, we introduce the feature graph: suppose we are given a time series data X =[ x 1 , x 2 ,..., x T ] T ,where x p -dimensional variables at time t , T is the length of the time series, and L is the time lag. A simple approach to learning feature graph is to use the regression model as VAR does. Letting x t denote the vector of all features at time t ,the linear regression model is defined as: where each of the  X  matrices are p  X  p coefficient matrices. Then we decide there is an edge from x i to x j in the feature graph if and only if the corresponding coefficients for x non-zero by significant test. Notice that in classical struc-ture learning scenarios (i.e. for i.i.d. observations instead of time-series data), the non-zero coefficients will only indicate an edge in the graph without directionality. In time series data, the observation that  X  X  cause appear before an effect X  can help us determine the directionality of an edge. Lasso-based Algorithm: In [2], an extension of the basic temporal graph learning algorithms is developed by impos-ing sparsity using lasso regression. Lasso is a linear regres-sion method that embodies variable selection using the L1-penalty term [21]. That is, the coefficients  X  , minimizes the sum of the average squared error of regressing for Y ,plusa constant times the L1-norm of the coefficients, namely, where  X  is a penalty parameter. Here the l 1 norm penalty introduces sparsity so that  X   X  j (  X  )=0formany j , and thus leads to improved interpretability. Previous work on struc-ture learning with lasso has proven its effectiveness both theoretically [17] and empirically [13, 22, 20].
 Extension to other regularizers: Recent advances in regularization theory have led to a series of extensions to the original lasso algorithm, such as elastic net [24] and group lasso [23]. These algorithms can also be adapted to struc-ture learning algorithms [16, 15]. For example, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. It imposes both L 1 and L 2 penalty on the coefficients, i.e. Real world data and a simulation study show that elastic net often outperforms lasso, while enjoying a similar sparse representation [24]. Furthermore, natural grouping infor-mation exist between variables in many applications, and we can make use of this type of information using group lasso, i.e. variables belonging to the same group should be either selected as a whole, or not.
In previous section, we review the algorithms for learn-ing a static temporal graph, i.e. the graph structure is invariant over time. This technique is very useful to pro-vide insights on the invariant dependencies between vari-ables. However, in many applications the temporal graphs will change over time. For example, in system biology, the gene regulatory networks will evolve over different stages of a cell-cycle; in many mechanical equipments, the system might undergo different states (e.g.  X  X ew X ,  X  X eteriorated X  and  X  X reak-down X ). The challenges involved with these ap-plications are the unavailability of stage (or state) informa-tion at each time stamp.

In this section, we describe a hidden Markov model ap-proach to automatically infer the hidden states and the tem-poral graphs associated with each state from time-series data. We extend hidden Markov model regression (HMMR) to handle lasso-type regression algorithms so that the ad-vantages of previous algorithms on learning static temporal graphs can be inherited.
Hidden Markov model has been successfully applied to se-quence labeling and segmenting task for time-series data. It is a generative model that assumes the data are generated from a finite set of  X  X idden X  states, each of which is asso-ciated with a probability distribution over the observations x . Hidden Markov model regression (HMMR) is a natural extension of the hidden Markov Model. It assumes the time-series data are generated from a doubly stochastic process, where the response variables is conditionally dependent on the values of the predictor variables as well as an underlying process of hidden states [19, 4, 18].
 Let O = { X , Y } be a multivariate time series, where X =[ x 1 , x 2 ,..., x l ] T is p-dimensional predictor variable and Y i =[ y 1 , y 2 ,..., y T ] is the response variable. Notice that y 1 can be either a single variable or vectors of variables. The objective of HMM regression is to learn a target func-tion that accurately predicts the values of y given the value of x at future time.

The data generation process in HMMR is as follows: first, we choose the value of hidden state s t given the state at pre-vious time stamp s t  X  1 determined by transition probability a (or prior probability  X  if it is the beginning of a time-series sequence), then generate y t from a multivariate normal dis-tribution whose mean is determined by  X  T s x t and variance  X  . Its graphical model representation is shown in Figure 2. The joint likelihood for the sequence of observations O and Figure 2: Graphical model representation of hidden Markov model regression hidden state sequence { s 1 ...s T } is given by where p y ( s t )= P ( y t | x t , X ,  X  ,s t ) is defined as p ( s t )= 1 The model parameters {  X , a , X ,  X  } can be estimated by max-imizing the likelihood function above using Baum-Welch al-gorithm [19]. Like HMM, we can also define the forward probability  X  and backward probability  X  as follows:  X  t ( s )= P ( y 1 ,...,y t ,s t = s | X )= b ( s )= P ( y t +1 ,...,y T | s t = s, X )= After some derivations, we have the estimator of the regres-sion parameters as follows:  X   X  s =( g s ( X ) T g s ( X ))  X  1 g s ( X ) g s ( Y )  X   X  s =( g s ( Y )  X   X   X  k g s ( X )) T ( g s ( Y )  X   X   X  s g where g s ( x t )=  X  t ( s ) b t ( s ) x t and g s ( y t )=  X  We can see that the solution can also be achieve by using a classical linear regression and reweighting the observed variables and response variables with weights  X  t ( s ) b
Next we examine how to extend HMMR to handle lasso-based regression algorithms. Following the idea in [14], we add a Laplacian prior for  X  as follows: Then the complete likelihood of HMMR with lasso is: The log likelihood of the observed data as
In this derivation, we use a frequentist approach for  X  with-out integrating it out as the full Bayesian approach does. To apply the EM algorithm, we define the following auxiliary function Q : By maximizing the auxiliary function Q , we are able to get the solution to the parameters. The terms relevant to  X  in Q is: Q =  X  =  X  =  X  From the forward-backward algorithm, we know P ( Y ,s t |  X  ( s ) b t ( s ). Therefore we have
Recent reexamination of gradient-based optimization al-gorithms, such as the coordinate descent, has shown that they are very effective to solve lasso-type regressions [7]. Following the idea, we compute the first derivative of Q  X  with respect to  X  : coordinate descent algorithms to get the solution to  X  .
For alternative, we can also examine the inverse of covari-ance matrix  X . It is proved that the inverse of covariance matrix is always symmetric posi tive-definite, therefore we can have the Cholesky decomposition of  X   X  1 as  X   X  1 = U Take it to eq (2), we have: Q If we further assume  X  is a diagonal matrix, the solution to  X  can be obtained using lasso by reweighting observed variables and response variables as
Other lasso-style regression algorithms, such as elastic net and group lasso, can also be extended similarly. We skip the full discussion. In this section, we describe how to use the extension of HMMR for lasso to learn dynamic temporal graphs. To apply HMMR for learning tem poral graphs, we can simply define the observed variables and response variables as fol-lows where L is the time lag we want to consider in the graph. Then there is an edge from x ( k ) to x ( j ) in the temporal graphs of state s when and only when the corresponding co-efficients in  X  s is non-zero. The description of our algorithm is shown below. We later refer this algorithm as HMMR lasso.
There are three parameters in our algorithms: the lasso penalty  X  , the number of hidden states S ,andthetimelag L . In lasso algorithms, the penalty  X  determines the number of non-zero coefficients, which can be decided using model se-lection methods, for example, AIC, BIC or cross-validation. In our experiment, we use cross-validation to choose  X  for lasso and elastic net. The number of hidden states S also determines the complexity of model, where model selection algorithms have to be applied. In our experiment, we first run HMM over the observations and then use BIC to de-termine the optimal number of hidden states S .Laterwe use S as the value for dynamic temporal graph learning al-gorithms. The time lag L determines how long histories the structure learning algorithm should examine. Theoreti-cally, we can set L as the length of observed sequences since lasso is able to select only relevant variables. However, the noise introduced by adding more irrelevant variables and the computational complexity involved prohibit us from doing so. For alternative, we can use feature selection algorithms, such as forward search and backward search [1].
 The computational complexity of our model is O ( NTS 2 + S  X  ), where  X  is the computational complexity of the regres-sion algorithm. We can see that our algorithm for learn-ing dynamic temporal graphs is computational efficient and therefore applicable to large scale applications.
The work is closely related to [6, 11], which learns dy-namic graph structure to reveal the dependencies between variables at the same time stamp. However, their algorithms impose similarity between graphs at adjacent time stamps, with the assumption that the graph structure should evolve gradually over time. In contrast, our algorithm assumes the graph structures remain relatively stable over time, but sig-nificant changes (i.e. those resulting in the change of hidden states) still can be captured. The advantage of our model is that we are able to use all the observations sharing the same state to construct the dependency graphs so that the learned graphs are more robust (since they are based on more data). In addition, our algorithms enjoys a much smaller computa-tional complexity.
We conduct our experiments on two synthetic datasets, both of which are generated from 2-state HMMR. The first one is Simulation Data I with univariate response observa-tions. Our goal is to verify whether HMMR extensions to lasso (and other lasso-type regression algorithms) can out-perform HMMR when a subset of features are assigned zero coefficients in the underlying model; The second dataset is Simulation Data II with multivariate response observa-tions. It is generated from state-dependent VAR model. Our goal is to examine whether our structure learning algo-rithms based on HMMR Lasso is able to uncover the true dynamic temporal graphs.

In Simulation data I, we simulated 2 datasets, each of which consists of 20 sequences of 30 predictor observations and 1 response observations. We use one data set as train-ing and the other as testing, and repeat the experiment 20 times. The response observations are generated from a 2-state HMMR model, where the regression coefficients for the two states are : The predictor variables are gene rated from multivariate nor-mal distribution with mean 0 and the correlation between x and x j is set to  X  | i  X  j | with  X  =0 . 5. We set the variance in the regression model as  X  = 3, which gives a signal to noise ratio of approximately 5.7. The transition probability is shown in Table 1 and the state prior  X  is set to 0.5 and 0.5. We run HMMR, HMMR lasso, and HMMR elastic net on this dataset. In general, the results of HMMR lasso and HMMR elastic net are similar, so we only show the results by HMMR lasso in most cases. Table 1 shows the learned transition probability, Figure 3 shows the mean and stand error of estimated coefficient values and Figure 4 shows the averaged likelihood of the test sequence. The results unani-mously demonstrate that HMMR lasso outperforms HMMR and its estimated parameters are very close to ground truth. Table 1: Learned Transition Matrix by HMMR and HMMR-lasso S1 0.4 0.6 0.3757 0.6243 0.3778 0.6222 S2 0.5 0.5 0.5950 0.4050 0.5450 0.4550 Figure 3: Learned coefficients by HMMR and HMMR lasso on simulation data I
In Simulation data II, we randomly generate two feature graphs with 10 features as shown in Figure 5, randomly se-lect the time lag as 2, randomly assign the edge in the graph with weights sampled from [  X  1 , +1] and set the standard de-viation as 1. These values determine the parameters of the corresponding linear gaussian model in VAR. We then apply a Markov model to generate the hidden state sequence with transition matrix as follows: a 11 =0 . 8, a 22 =0 . 7,  X  and  X  2 =0 . 5. Finally, we generate the sequence of observa-tions with one of the two VAR models chosen by its current state. The length of the time series sequence is 200. We want to stress that this is a very challenging task consider-ing the number of parameters to be estimated (20 * 10 * 2 +2*2+2= 406) and the limited number of observations in the dataset.

Figure 5 shows the learned temporal graphs using two types of approaches: (1) a two-step process by first running a HMM and then applying the static temporal graph learn-ing algorithm with VAR (HMM + VAR) or lasso (HMM + lasso), and (2) dynamic temporal graph learning algorithm with VAR (HMMR VAR), elastic net (HMMR elastic net) and lasso (HMMR lasso). We can see that the HMMR lasso algorithm outperforms all competing methods and yield a graph closest to the ground truth.
In this section, we examine the effectiveness of our algo-rithm for the application of monitoring oil-production equip-ment. The features in the collected dataset are measure-ments from sensors in a 3-stage separator, which is a cylin-drical or spherical vessel used to separate oil, gas and water from the total fluid stream produced by a well. There are 9 measurements in total, including 2 measured levels of the oil and water interface at each of the 3 stages, i.e. 1. 20LT 0017 mean (stage 1) 2. 20LT 0021 mean (stage 1) 3. 20LT 0081 mean (stage 2) Figure 4: Loglikelihood on 20 testing sets by HMMR and HMM lasso on simulation data I 4. 20LT 0085 mean (stage 2) 5. 20LT 0116 mean (stage 3) 6. 20LT 0120 mean (stage 3) as well as three overall measures, i.e. 7. daily-water-in-oil (DailyWIO) 8. daily-oil-in-water (DailyOIW), 9. Daily production.
 These measurements are recorded daily from 2006/1/1/ to 2008/6/29, resulting in 911 observations.

It is known that oil-production equipment is a rather com-plex system. Interestingly, like many other mechanical sys-tems, the dependency networks between measures have not been not fully understood and might change over time al-though the equipment is designed and built by human. For example, the dependency between measure  X 20LT 0021 mean X  at stage 2 and  X  X aily production X  varies depending on the oil-to-water mixing proportions. Therefore it would be inter-esting to examine whether the temporal graphs learned by our models can provide additional insights about the sys-tem. In addition, since the system will undergo different states during usage, such as  X  X ormal X ,  X  X artial break-down X ,  X  X eteriorated X , or  X  X ully breakdown X , it would very useful if we can detect anomaly by comparing current dependency networks with the one at normal state so that early signals can be discovered.

In this experiment, we examine three approaches, includ-ing: (1) learning a static temporal graph; (2) learning state-dependent temporal graphs by a two-step process, i.e. train-ing one HMM to uncover hidden states and then applying static temporal graph learning algorithm to observations of each state; (3) learning dynamic temporal graphs. For all the structure learning algorithms, we choose the penalty term  X  in lasso via cross-validation and the time lag is set to 2.

We start with approach (1) and show the learned static temporal graph in Figure 7(A) using lasso-based static learn-ing algorithm. The result is not desirable since the graph shows a rather complex dependency network and some of the dependencies, e.g.  X  X L20 0120 MEAN X  directly affects  X  X L20 0021 MEAN X , are believed false by field engineers. Since there are several possible system break-downs during the time when the data are collected (but unfortunately the complete log is not available), static graph learning algo-rithms might be confused by the unclean data involved (the observations during the system break-down period).
Next, we examine approach (2), i.e. learning state depen-dent temporal graphs by a two-step process. The first step istotrainanHMMmodelonthefullobservationdatain order to uncover the underlying generating model for the separator observations. One possible scenario is that the system has undergone  X  X idden X  states, e.g. healthy state, break-down state and deteriorated state. Using HMM, we are able to uncover the corresponding  X  X idden X  state for each time-step and therefore build the state-dependent temporal graphs.
 Determining the Number of Hidden States Domain knowledge usually plays a major role in determining the Figure 6: Loglikelihood of HMMs with varied num-ber of hidden states; x-axis: number of training it-erations; y-axis: loglikelihood of training data number of hidden states in HMMs. For instance, in biology applications, it is known that a cell will undergo 4 stages in one cell-cycle and therefore we can set the number of hidden states as 4. Without domain knowledge, we can decide the best number of hidden states using model selection criteria. The motivation for these criteria is to make a trade-off be-tween model fitness and model complexity. In HMM, the model fitness refers to the fit of HMM to the training data, i.e. the likelihood of the training data, and the model com-plexity is the number of parameters in the model, which is controlled by the number of hidden states S .Intheex-periment, we use the Bayesian information criterion (BIC), i.e.
 where N is the number of training examples, d = S ( S + p p + 1). Figure 6 shows the likelihood of the data with the varied number of states from 2 to 4. HMM with 3 hidden states achieves the lowest BIC score 2 and seems the most appropriate for our application.

Figure 6 shows the transition probability of 3-state HMM and the corresponding state sequence for our observations. From the results, we can see that (1) the system undergoes two major states, i.e. green state at the first half of the ob-servation time period and blue state at the second half. The transition from green state to blue state happens sometime around Oct 19, 2006, which agrees well with records in sys-tem logs; (2) the system enters the red states from time to time with short duration. The duration of the red states is longer in the second half of observation time period than in the first half. Based on the results and domain knowledge, we interpret the three states of HMM as follows: Green: nor-mal states; Blue: sub-normal states (deteriorated state that needs inspection and/or repair); Red: break-down states.
Figure 7(B-D) shows the learned static temporal graphs using the observations from the three states labeled by HMMs BIC score of 3-state HMM = 24307.44 + 834.58 = 25142.02 Figure 8: HMM results on oil-production equipment data. (Bottom) transition probabilities; (Top) pre-dicted state sequence (color representing states) respectively. We can see that some of the correlations are lost when the system changes from  X  X ormal X  state to  X  X ub-normal X  state. In  X  X reak-down X  state, DailyOIW is most affected by X20LT 0116 means. However, the graphs are too sparse to provide any useful insights, especially for the normal state which will be used for monitoring abnormal events.

Finally, we examine the results by approach (3) learning dynamic temporal graphs using HMMR lasso. In the exper-iment, we set the number of hidden states as 3 following our previous discussion. The learned graphs shown in Figure 9 seem very reasonable. Many of the identified dependen-cies are believed true by domain experts, for example, all the measures at the three stages are affected by the daily production; there is an interdependency between daily-OIW and daily production. By examining the transition matrix learned by lasso HMMR, we conjecture that the graphs in Figure 9 (A) (B) and (C) corresponds to the break-down, deteriorated and healthy states respectively. Further ex-amination of learned graphs can be conducted by domain experts.
In this paper, we develop a family of structure learning algorithms to uncover dynamic temporal graphs from time-series data. By extending the hidden Markov model regres-sion to handle lasso-type regression algorithms, our model integrates two usually separate tasks, i.e. inferring under-lying states and learning temporal graphs, in one unified model. In addition, our model naturally inherits all the ad-vantages of lasso-based structure learning algorithms. In the application of oil-production equipment monitoring system, we have demonstrated that our model is able to provide richer information and more powerful modeling capability than previous methods to learn static temporal graphs.
For future work, we are interested in a systematic eval-uation of our output temporal graphs for forecasting and anomaly detection. Another direction is to apply the model for other applications, such as system biology and climate modeling.
 We thank Alexandru Niculescu-mizil, Aurelie Lozano, Naoki Abe, Rick Lawrence and Yuan-chi Chang for discussing the ideas in the paper. We thank the anonymous reviewers for their valuable suggestions. (D) HMM state-Green ( X  X ormal X ) observations [1] D. W. Aha and R. L. Bankert. A comparative [2] A.Arnold,Y.Liu,andN.Abe.Temporalcausal [3] D. Brillinger. Remarks concerning graphical models [4] H. Cheng and P.-N. Tan. Semi-supervised learning [5] D. Chickering. Learning bayesian networks is [6] D. Eaton and K. Murphy. Bayesian structure learning [7] J. Friedman, T. Hastie, and R. Tibshirani.
 [8] N. Friedman. Inferring cellular networks using [9] K. Fujinaga, M. Nakai, H. Shimodaira, and [10] A. Goldenberg and A. Moore. Tractable learning of [11] F. Guo, S. Hanneke, W. Fu, and E. P. Xing.
 [12] M. I. Jordan. Learning in Graphical Models .TheMIT [13] S.-I. Lee, V. Ganapathi, and D. Koller. Efficient [14] S.-I. Lee, H. Lee, P. Abbeel, and A. Y. Ng. Efficient l1 [15] A.Lozano,N.Abe,Y.Liu,andS.Rosset.Grouped [16] A.Lozano,N.Abe,Y.Liu,andS.Rosset.Grouped [17] N. Meinshausen and P. Buhlmann. High dimensional [18] K. Noto and M. Craven. Learning hidden markov [19] L. Rabiner. A tutorial on hidden markov models and [20] P. Ravikumar. Approximate inference, structure [21] R. Tibshirani. Regression shrinkage and selection via [22] M. J. Wainwright, P. Ravikumar, and J. D. Lafferty. [23] M. Yuan and Y. Lin. Model selection and estimation [24] H. Zou and T. Hastie. Regularization and variable
