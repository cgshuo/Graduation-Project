 Information retrieval experimentation generally proceeds in a cycle of development, evaluation, and hypothesis testing. Ideally, the evaluation and testing phases should be short and easy, so as to maximize the amount of time spent in development. There has been recent work on reducing the amount of assessor effort needed to evaluate retrieval sys-tems, but it has not, for the most part, investigated the ef-fects of these methods on tests of significance. In this work, we explore in detail the effects of reduced sets of judgments on the sign test. We demonstrate both analytically and em-pirically the relationship between the power of the test, the number of topics evaluated, and the number of judgments available. Using these relationships, we can determine the number of topics and judgments needed for the least-cost but highest-confidence significance evaluation. Specifically, testing pairwise significance over 192 topics with fewer than 5 judgments for each is as good as testing significance over 25 topics with an average of 166 judgments for each X 85% less effort producing no additional errors.
 Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Perfor-mance Evaluation General Terms: Experimentation, Measurement Keywords: information retrieval, evaluation, hypothesis testing, test collections
Much work on retrieval systems is incremental: small changes to existing algorithms creating small gains in per-formance. Over time, small gains can build to substantial improvements. But small performance changes can happen for no reason but random chance, and whether they X  X e worth pursuing further cannot be evaluated by visually inspecting retrieval results. We need statistical hypothesis tests to in-struct us on whether a small change is worth following up on, or whether a line of research should be dropped. Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
In IR, hypothesis tests are performed over a set of queries, which are input to a system to produce a ranked list of doc-uments. Each ranked list is evaluated against a set of rel-evance judgments that indicate whether each document is relevant to the query. Unless a large set of relevance judg-ments is already available, they must be acquired by having human assessors read and judge documents. This is a very time-consuming process, and as a result, there has been a great deal of recent interest in small sets of judgments. But an evaluation over a small set of judgments will produce errorful measures of performance; it seems clear that they must affect the conclusions drawn from a hypothesis test.
We treat a hypothesis test as a binary decision-maker: the null hypothesis is either rejected or not rejected. For the decision to have any meaning, it must be tied to some implication about the reason for it. If the null hypothesis is rejected, we want it to be because we are unlikely to have observed a particular sample in a world in which that hy-pothesis is true. This is captured by the accuracy of the test: a test with high accuracy is not likely to falsely reject the null hypothesis.

On the other hand, if the null hypothesis is not rejected, we want it to be because our sample is unlikely to have been observed when the null hypothesis is not true. This is captured by the power of the test: a test with high power is likely to reject the null hypothesis when it is false. Power is important but subtle. If we decide to drop a line of research because it did not produce a significant result, we must be certain that the power of the test is high. If it isn X  X , then the failure to reject the null hypothesis is not meaningful.
Our goal in this work is to investigate how incomplete relevance judgments affect the conclusions we draw from hypothesis tests. Our focus is on power; as we will see, in-complete relevance judgments, when uncertainty is properly accounted for, do not affect the accuracy of the test.
We begin with a brief look at previous work on hypothe-sis testing in information retrieval. We then provide a tuto-rial on the sign test with special emphasis on the notion of power. This leads into our first major result: an expression to determine how many topics should be used to maintain power when there is uncertainty due to relevance judgments. After that, we describe how to estimate the uncertainty due to relevance judgments, leading to our next major result: a model for estimating the number of judgments needed to reach a given level of uncertainty with a given number of topics. We can then define a cost function for experimen-tation to find the optimal number of topics and judgments needed to run a significance test that has high power.
Investigations into the appropriate hypothesis tests to use in information retrieval experimentation go back at least as far as van Rijsbergen X  X  classic 1979 textbook [10]. Van Rijsbergen discusses the sign test, Wilcoxon sign rank test, and t-test, and concludes that since little is known about the distribution of evaluation measures, only weak tests like the sign test can be used.

Zobel [14] and Sanderson &amp; Zobel [8] undertook an empir-ical investigation of hypothesis test performance on retrieval systems that had been submitted to TRECs (Text REtrieval Conference) over the years. As these systems represent real retrieval systems over real topics that people might be in-terested in, they provide an opportunity to evaluate and compare tests on real data. Both works also investigate the effect of reducing assessor effort on evaluation by using other evaluation measures or reduced-depth pools of judgments.
Recently, there has been some interest in whether small test collections can generalize. There are two notions of  X  X eneralization X  in retrieval experimentation: generaliza-tion to a new set of systems that did not contribute any judgments to the set, and generalization to new topics that have not been seen before. The latter is the domain of hy-pothesis testing. Recent work on the TREC Web and Ter-abyte tracks has suggested that more topics and fewer rele-vance judgments provide evaluations as good as a few topics with a lot of relevance judgments.

Cormack &amp; Lyman investigated the effect of small test collections on the power of a test empirically, concluding that good evaluation can be provided by many topics with a small number of judgments for each [3].
 The work most closely related to this one is Jensen X  X  Ph.D. thesis [6]. He undertook a careful empirical inves-tigation into the power of hypothesis tests over large sets of topics evaluated on only the top retrieved results, addi-tionally investigating the effect that automatically-assigned relevance judgments have on power. His two findings are that large topic sets are necessary when evaluating over few retrieved results, and that automatic relevance assignments decrease power.

Our conclusions are the same as the previous two works: more topics with fewer judgments is at least as good as full sets of judgments. Above that, our contributions are an an-alytic investigation of the sign test leading to a cost function for determining the optimal number of topics and judgments needed, and an empirical evaluation of that cost function on real IR systems. We have elected to focus on the sign test due to its simplicity; we hope to perform a similar analysis for the t-test.
The sign test is appealing as it is one of the easiest to implement, the easiest to understand, and makes the fewest assumptions about the data. For simplicity, we will focus on the one-sided sign test; our results can be extended without much difficulty to the two-sided test.
 The two hypotheses in the one-sided sign test are: We assume we have n i.i.d. Bernoulli trials Y 1 ,Y 2 , ..., Y each of which having probability of success  X  , i.e. P ( Y 1) =  X  . The test statistic is S = n i =1 Y i ,thenumberof successes. S has a binomial distribution Binom ( n,  X  ). If the null hypothesis is true, the maximum expected number of successes is n X  0 , with a maximum variance of n X  0 (1 If S is much greater than that expectation, it is unlikely that the data is distributed according to the null hypothesis.  X  is generally chosen to reflect the hypothesis that each trial is equally likely to be a success or failure,  X  0 = 1 2 .If S is unlikely to have occurred when  X  0 = 1 2 ,thenwemay reasonably conclude that the observed values did not occur by chance.

For a given level of significance  X  , there is at least one  X  X ritical value X  c  X  such that P ( S  X  c  X  | n,  X  0 )= n i  X  ) n  X  i &lt; X  .Iftheobserved S is greater than the maximum c , we may reject the null hypothesis as being unlikely. In this formalism,  X  is the expected accuracy of the test. It defines the probability of making a Type I error, or false positive, of rejecting the null hypothesis when it is not true. Figure 1(a) shows the distribution of S under the null hy-pothesis for n = 50; the shaded region is the probability of rejecting the null hypothesis when  X  =0 . 05. If H 0 is true, the area of the shaded region corresponds to the probability of making a Type I error.

As n increases, the binomial distribution converges to a normal distribution: Therefore for large n , we can use a normal distribution func-tion to approximate the binomial, avoiding the computa-tional difficulty of calculating n i . The normal cumulative density function with zero mean and unit variance is gen-erally denoted by Greek letter  X ;  X ( x )= P ( X&lt;x )isthe probability that normalized random variable X takes on a value less than x .  X  is defined as the lower tail of the nor-mal distribution, but since our alternative hypothesis is that  X &gt; X  0 , we need the upper tail.
 We can estimate c  X  using the normal quantile function  X   X  The normal approximation is generally acceptable for n&gt; 25 [13].
The complement to accuracy is power . Power reflects the probability of making a false negative error, that is, failing to reject the null hypothesis when it is false. This is also known as Type II error and usually denoted  X  .Poweris1  X   X  ,the probability that the null hypothesis will be rejected when it is false.

Power is relevant when the null hypothesis is false; there-fore we need  X &gt; X  0 . It will also be useful to think in terms of a population  X  X ffect size X  h =  X   X   X  0  X  0 [2]. This is the per-cent increase in successes above what would be expected by the null hypothesis. If the null hypothesis is true, then effect size h = 0. For the purposes of analyzing the power of the one-sided sign test, we will define h&gt; 0. bounded by the critical value c  X  in both.

For a given significance level  X  and sample size n ,the power of the one-sided sign test is as follows. Let c  X  the maximum critical value at which we will reject the null hypothesis. For a given population success rate  X  ,poweris defined as: Figure 1(b) shows the region of the binomial distribution that would produce Type II errors when  X  =0 . 7.

From this equation we can see how each variable in the test ( n ,  X  , h ) affects the power. Increasing sample size n increases power. Increasing significance level  X  increases c and therefore decreases power. Increasing effect size h en-tails an increase in true success proportion  X  ,whichincreases power. Figures 2(a) and 2(b) show how power is affected as effect size, sample size, and significance level  X  change.
In reality, we can control n and  X  , but we have no control over h . In determining the number of trials, then, we must consider the minimum effect size we would like to be able to detect with high probability, while keeping the probability of making a false positive (Type I error) low.

We can also express power in terms of the normal approx-imation. When  X  0 = 1 2 , where  X  is the normal density function with zero mean and unit variance. (For details on the derivation of this expres-sion, see Cohen [2].)
We have introduced a lot of notation up to this point, and there is still more to be introduced. Table 1 provides an easy reference to the notation and its meaning.
Suppose we have two retrieval algorithms, A and B ,and asampleof n = 50 topics. Our null hypothesis is that algorithm A outperforms algorithm B half the time on the population of topics, i.e. whether one is better than the other is essentially random.
 For n =50and  X  =0 . 05, the critical value c  X  is 32: if A outperforms B on at least 32 of the 50 topics, we will reject the null hypothesis.

Suppose we know that h =0 . 4, i.e. A outperforms B on 70% of the topics in the population. 1 Using the normal approximation, the power of a test with 50 topics and  X  =
Obviously we have no practical way of knowing this, but the assumption will help demonstrate power. 0 . 05 is about 0 . 882, so the probability that we will draw a sample of 50 topics and fail to reject the null hypothesis on the basis of that sample is about 0 . 12.

Since 50 topics has become the standard for IR experimen-tation, it is interesting to calculate the power of the sign test to detect varying effect sizes with n = 50. If we want 80% power and 95% accuracy, the effect size must be at least 0 . 35, i.e. the better system needs to be better on at least 68% of the topics. For 60% power, the effect size must be at least 0 . 25, or 62 . 5% of the topics. For 95% power, the effect sizewouldhavetobe0 . 47; A would have to outperform B on 73 . 5% of topics.
Does the theory pan out? Are we able to detect significant differences between retrieval systems at the rate predicted by the analysis above?
In IR hypothesis testing studies, topics are generally taken to be i.i.d. samples from some population, and retrieval runs that were submitted to TREC tracks are used to test hy-potheses about hypothesis tests. We will follow this ap-proach, using the 249 topics from the 2004 TREC Robust track and the 110 submitted systems [12].

We will treat the 249 topics as a population from which we sample uniformly at random. We will take the population effect size h to be the effect size over the 249 topics in the  X  X opulation X .

We randomly selected samples of n topics from the set of 249. We performed sign tests on all 5995 pairs of systems. As stated above, with n = 50 we can detect an effect size of h =0 . 35 with power 0 . 8 at significance level 0 . 05 (using the normal approximation). If the analysis is correct, we should see that for all pairs with a population effect size of h  X  0 . 35, we correctly rejected the null hypothesis for 80% of them. We will refer to the percentage for which we do reject the null hypothesis as the  X  X bserved power X . This phrase is sometimes used to mean  X  X ost-hoc power X ; the concept of post-hoc power has been discredited by Hoenig &amp; Heisey [5]. By  X  X bserved power X  we simply mean the percentage of tests for which the null hypothesis was rejected, calculated over many rando mtrials.

Table 2 compares predicted and observed power for vari-ous sample sizes and population effect sizes. The predicted powers in this table are computed exactly, not using the normal approximation. The observed powers are calculated over multiple samples of n topics. Observed power is close to that predicted by the theory, though somewhat higher on average (this may be an artifact of the topic design process or of the particular systems submitted to the track). Note that the standard errors are rather high. The conclusions drawn from a test may vary a lot from sample to sample; a single hypothesis test is therefore not enough to draw strong conclusions.
Ties are trials for which Y i = 0, i.e. there is no measurable difference. Lehmann [7] describes two approaches to ties in the sign test. The usual practice is to discard all trials that resulted in Y i = 0 and reduce n accordingly. Figure 2(b) shows how power decreases as ties become more frequent. Table 2: Predicted and observed rates of detecting significance for varying sample size n and effect size h . Predicted power is denoted 1  X   X  . Observed power is 1  X   X   X  one standard error.

Another approach to ties is to randomly assign them to be successes or failures according to the null hypothesis  X  This also decreases the power of the test: the expected number of success after this procedure is  X  S = n  X  n 0 i i =1  X  0 ( n 0 the number of ties). If the null hypothesis is true, we are assigning ties to be successes at a rate lower than they would be if they were not ties, and therefore we have less ability to reject the null hypothesis; power decreases.
Of these two methods, the former reduces power less than the latter. In fact, the former method reduces power the least of all possible tie-handling methods [7].
By  X  X ncertainty X , we mean that there are trials for which we believe that Y i =1or Y i = 0, but there is a chance that our measurements are wrong. In IR, uncertainty can come from having incomplete or imperfect relevance judgments. We view uncertainty as being similar to a tie; it is a trial for which there is a measurable difference but the error of that measurement is high. We will denote an uncertain outcome as Y i .Ourcertaintyin Y i is the probability that Y i =1given Y = 1, i.e. the probability that we have correctly predicted the outcome of the trial. We will call this probability  X  . Informally, uncertainty = 1  X  certainty.
Failing to account for uncertainty entails using the esti-mates Y i and ignoring the uncertainty 1  X   X  .

Suppose the null hypothesis is true, i.e. h = 0. Given un-certainty 1  X   X  , what is the probability that we will reject the null hypothesis with significance  X  and sample size n ?The expectation is that there will be equal numbers of successes and failures. Each Y i we predict to be a success will actually be a failure with probability 1  X   X  ; it will actually be a success with probability  X  . Therefore E [ S ]= n ( 1 2  X  + 1 2 (1 which is exactly its expectation when there is no uncertainty at all. Therefore uncertainty does not affect the accuracy of the test.

Now suppose the null hypothesis is false, i.e. h&gt; 0and  X &gt; 1 2 .Ifcertainty  X &gt; 1 2 , the expected number of successes we will observe is n (  X  X  +(1  X   X  )(1  X   X  ))  X  n X  .Sincetheex-pectation is less than what it would be with no uncertainty, power will be reduced.
Above we discussed dealing with ties by randomly assign-ing them to be positive or negative. This can be generalized to the idea of uncertainty: we can incorporate uncertainty by treating instances that we are uncertain of the true out-come as ties, then assigning them to be success or failures randomly depending on how much uncertainty we have. For example, suppose the effect size is h =0 . 4, so  X  = P ( Y i =1)=0 . 7foreachtrial i . But suppose we have Y i an estimate of Y i in which we have certainty  X  = P ( Y i 1 |
Y i =1)=0 . 8. Then P ( Y i =1)= P ( Y i =1 | Y i =1) P ( Y 1)+ P ( Y i =1 | Y i =0) P ( Y i =0)=0 . 7  X  0 . 8+0 . 3  X  an effect size of only h =0 . 24. Our uncertainty has reduced the effect we can detect from 0 . 4to0 . 24, thus reducing the power of the test. To make up for the reduction in power, we would need 138 trials rather than 50.

We will define the  X  X djusted effect size X  h to be the re-duced effect size in the presence of uncertainty.
We can quantify the increase in trials necessary to make up for a loss in power due to uncertainty. Let h be the true effect size ( h&gt; 0) and h be the adjusted effect size, with h &lt;h due to certainty . 5  X   X &lt; 1(if  X &lt;. 5, we may simply flip our prediction and take  X  =1  X   X  ). Let n be the original sample size. The goal is to find n , the new sample size needed to be able to detect the adjusted effect size with the same power.

Since we want the power to be the same, we need to find the n that results in the difference in powers being zero: Since this represents the area under the normal curve from x 0 = Z  X   X  h when Z  X   X  h
Solving for n gives: This expression relies on knowing the true effect size, which of course we do not in practice. However, if  X  0 = 1 2 ,asit nearly always would, then and we have reduced it to an expression that relies only on the original sample size and the uncertainty  X  . Though we began with the assumption that we knew the popula-tion effect size, the final answer does not depend on that knowledge.

This is the first milestone in this work. As uncertainty increases, the number of trials needed to maintain a given level of power increases exponentially. Figure 3 shows how the number of topics must increase to maintain 80% power to detect an effect size of h =0 . 35 when n =50and  X  =0 . 05.
As we suggested above, a significant source of uncertainty may be incomplete or imperfect relevance judgments. This immediately implies that we can improve the power of the test in two ways: increasing the number of topics or increas-ing the number of relevance judgments for the extant topics.
In our IR example above, each topic is classified as a suc-cess or failure depending on the sign of the difference in some evaluation measure. In this section we follow [1] and show how to predict the sign of the difference based on incomplete relevance judgments, and how to estimate the probability that the predicted sign is correct.

We have elected to use the IR evaluation measure average precision (AP). Average precision is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) and its ability to re-trieve relevant documents (recall). It is typically written as the mean precision at the ranks of relevant documents: where R is the set of relevant documents, and r ( i )isthe rank of document i . We define  X  AP to be the difference in average precisions between two systems on the same topic. Given an incomplete set of judgments, we can predict  X  AP by assuming anything unjudged is nonrelevant. This is a standard assumption in IR evaluation. However, it gives us no way to assign a probability to our prediction.
Let X i be a random variable indicating the relevance of document i . If documents are ordered by rank, we can ex-press precision as prec @ i =1 /i i j =1 X j .
 Average precision becomes the quadratic equation where m is the collection size. For a closed form expression of  X  AP , we need to be able to calculate AP when documents are ordered arbitrarily, not necessarily by rank (since the two rankings will most likely be different). To do that, let a ij =1 / max { r ( i ) ,r ( j ) } .Then To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2. Average precision 1+ 2 3 because x A =0 ,x B =1 ,x C = 1. Though the ordering B, A, C is different from the labeling A, B, C ,it does not affect the computation.

Doing the same thing for the other list (using b ij rather than a ij ),wecanthenexpress X  AP as
We can now see the difference in average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents. This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.
 The expectation and variance of  X  AP are:
E [ X  AP ]  X  1 Var [ X  AP ]  X  1 where p i = p ( X i = 1), the probability that document i is relevant. For simplicity, we set p i = 1 2 for all unjudged documents.  X  AP asymptotically converges to a normal dis-tribution with expectation and variance as defined above. This means that we can use the normal cumulative density function to determine the probability that a difference in AP is less than 0.

Assuming topics are independent, we can easily extend this to mean average precision (MAP), the mean of average precisions calculated for a set of topics T .MAPisalso normally distributed with expectation and variance: And we define  X  MAP = MAP 1  X  MAP 2 analogously to  X  AP . X  MAP has an expectation and variance as well.
We will define confidence to be confidence = P ( X  MAP &lt; 0) =  X 
These are actually approximations to the true expectation and variance, but the error is a negligible O ( m 2  X  m ). Figure 4: Uncertainty in  X  MAP versus actual  X  MAP .
We defined certainty above as P ( Y i | Y i ). We would like to connect the idea of confidence to the idea of certainty. There is a rather natural connection: let Y i = sgn ( E [ X  AP i will define  X  = P ( Y i =1 | Y i =1)= P ( X  MAP &gt; 0). The reason for using  X  MAP to assign the probability rather than  X  AP is that topics are assumed to have been drawn i.i.d. from a population in which we have  X  certainty on every member. Certainties may vary from topic to topic, but the topic certainties are samples from a distribution with expectation equal to the population uncertainty.

In order to use confidence as certainty, we would like to see that if sgn ( X  AP )=1andcertaintyis  X  , then the pro-portion of pairs for which sgn ( E [ X  AP ]) = 1 is at least  X  . To test this, we used the Robust runs from Section 3.2.1.
For each pair of runs over the full set of 249 topics, we judge a  X  X ool X  of depth k (the top-ranked k documents by both runs for all topics), from k = 1 to 100. After each increment of k , we estimate the difference in average preci-sion Y i = sgn ( E [ X  AP ]) and the confidence P ( X  MAP &gt; 0) (with probability of relevance p i = 1 2 ).

The results are shown in Figure 4. The solid line is what we would see if confidence exactly predicted accuracy; since our points are uniformly above that line, it seems that confi-dence meets our requirements for a measure of uncertainty. Therefore we use  X  X onfidence X  and  X  X ertainty X  interchange-ably for the remainder of this work.
The evaluation in the previous section gives us data to es-timate the number of judgments it takes to reach increasing confidence levels with increasing numbers of topics.
Figure 5 shows the average number of judgments needed to achieve increasing confidence levels. Confidence levels may fluctuate, so that after achieving 70% confidence, a few more judgments cause confidence to drop below 70%. The judgments are the average minimum number required for confidence, i.e. the number of judgments made when con-fidence level  X  was first achieved. This models an assessor that stops judging the first time confidence reaches a given threshold. Figure 5: Number of judgments required to reach uncertainty levels for varying sample sizes. The fit lines are shown as well.

The figure shows an exponential relationship between judg-ments and confidence. It also shows a relationship between the number of topics and the number of judgments.
We will fit a curve to these plots to estimate the relation-ship. Define the estimated number of judgments needed to reach  X  confidence for n topics as We can fit this model using regression. Since the number of judgments is a count, we do not want to use an ordinary least squares estimation, which could lead to predictions that are less than one. Instead, we will fit a generalized linear model with a Poisson link function. This guarantees that all predic-tions will be at least 1. For more information on generalized linear models and Poisson regression, we refer the reader to Faraway [4] or Venables &amp; Ripley [11].

The result of fitting the Poisson regression to our data is is The R 2 for this model is 0 . 95, so it is a good fit.
The fact that it takes exponentially many judgments to in-crease confidence suggests that it may be more cost-effective to obtain a large number of topics with a few judgments for each, rather than judging a large number of judgments for a small number of topics. But recall from Figure 3 that the number of topics needed also increases exponentially as uncertainty increases. Therefore we need a cost-benefit anal-ysis to tell us what to do.
Given our equation for the new sample size needed to maintain power in the presence of uncertainty (Eq. 3) and our model for estimating the number of judgments (Eq. 5), we can figure out the most cost-beneficial confidence level to aim for.

We will define a cost function C associated with a level of confidence  X  and original sample size n .Thecostisthe Figure 6: Certainty/confidence versus estimated cost, and certainty versus adjusted sample size. total cost of developing n topics plus the total cost of ac-quiring the predicted number of relevance judgments needed to reach confidence  X  .Let C t be the cost of developing a topic. Let C j be the cost of judging one document. Suppose our sample size n has been selected in advance (or selected for us).

C (  X , n )= C t n + C j  X  j (  X , n )(7) which is obtained by substituting Eq. 3 for n and Eq. 5 for  X  j (  X , n ). Figure 5 shows how cost changes with target level of confidence  X  (wehaveset C j =1and C t =0forthis example).

We wish to minimize Eq. 7 with respect to  X  .Thereis no analytic minimum, but an approximate minimum can be found easily by binary search over discrete values of  X  .
Estimating the cost of developing a topic is difficult. The fact that topics can be reused more easily than relevance judgments (many TREC topics are  X  X ortable X  over collec-tions) should be considered, as should the fact that the same topics can be used to evaluate wildly diverse retrieval sys-tems that may retrieve complete different documents and therefore need completely different judgments. Sometimes topics are developed by a third party and given to us, or sampled from a query log at very little cost.
 When C t = 0, the cost function has an analytic minimum. We differentiate with respect to lambda. Equating this to zero and solving for  X  gives which means the level of confidence that minimizes cost is independent of the cost of making judgments, and indepen-dent of the original sample size. Figure 7: Effect size and power for 192 topics at 68% confidence. The solid lines are predicted by our analysis; the points are empirical performance. Instead of using n in our cost function, we could include Type I and Type II error rates and associated costs C  X  and C . We would then minimize along several dimensions (  X  ,  X  ,  X  ). Since estimating the cost of false positives and false negatives is tricky and to some degree personal, we do not explore this further.
Suppose we wish to be able to detect an effect size of 0 . 5 with 80% power. As Table 2 shows, about n =25topics is an appropriate sample size if there is no uncertainty due to relevance judgments, i.e. we have a full set of judgments and little to no assessor disagreement.

If we want no uncertainty, we must have  X  =1. Plug-ging into our cost function (7) (assuming topics are free) gives C (1 , 25)  X  1180 relevance judgments. Cost can be greatly reduced, though; if confidence is reduced to 80%, cost is reduced to C (0 . 8 , 25)  X  914. Plugging the coef-ficients from the model we trained above into Eq. 8, we find that the minimum cost is achieved when confidence is 68%: C (0 . 68 , 25)  X  620. The adjusted sample size needed to maintain the power of the test is n =25 1 2  X  . 68  X  1 so we estimate that judging 192 topics to 68% confidence costs nearly half as much as judging 25 topics to 100% con-fidence, without reducing the power of the test at all. Judg-ing a pool of depth 100 for 25 topics would require 4,161 judgments on average; our cost is 85% less than that.
To test whether our predictions matched reality, we picked 2,000 pairs of Robust systems at random. For each pair, we evaluated 192 topics to 68% confidence. We also evaluated 25 topics to 100% co nfidence and a pool of depth 100 for 25 topics.

The actual number of judgments needed to reach 68% confidence ranged from a minimum of 44 to a maximum of 10,164, with a mean of 794 but a median of only 357. About 70% of the trials required fewer than our prediction of 620 judgments. There is a great deal of variance in the num-ber of judgments needed, but indeed it required 1,000 fewer judgments on average for 192 topics than 25. It required 3,406 fewer judgments on average to judge 192 topics than to judge a pool of depth 100 for 25 topics.

What about the power of the adjusted sample size? We should be able to detect an effect size of 0 . 5 about 80% of the time. In fact, 83% of the pairs with a  X  X rue X  effect size of 0 . 5 were found to be significant with n = 192 topics.
We calculated  X  X bserved adjusted effect size X  by counting the number of topics for which Y i was positive and the num-ber for which Y i was nonzero. These observations are com-pared to the predicted adjusted effect size (using Eq. 2) in Figure 7(a). We generally underestimate the adjusted effect size; this is most likely because confidence underpredicts ac-curacy (Figure 4). We also calculated  X  X bserved power X  by counting the number of trials for which the null hypothesis is rejected at  X  =0 . 05. This is shown in Figure 7(b), along with the predicted power of 25 topics and 100% confidence in each. Empirically, using 192 topics with 68% confidence actually has more power than using 25 topics with 100% confidence; again, th is is most likely due to confidence un-derpredicting accuracy. Recall that since our  X  X opulation X  only consists of 249 topics, there is some effect between ev-erypair,sothenullhypothesisisalwaysfalseinthisdata; though the points at the left end of the plot are high, they are not Type I errors. In reality, there would be cases in which the null hypothesis is true.
In this section we show how to use uncertainty in exper-imental design. We assume that we have a new retrieval task. There are no existing topics and no existing relevance judgments. We would like to perform an experiment, but have limited resources. We need to find the minimum-cost experimental parameters that will still provide our desired level of power.

Since we do not have access to our previously-trained pre-diction model  X  j , so the first thing we need to do is train one. To do that, we first run a pilot study. Pilot studies are com-mon in disciplines in which the cost of running tests is high; Jensen [6] also proposed the use of pilot tests to determine the effect of uncertain judgments.

We  X  X evelop X  10 topics for the pilot study. The topics are submitted to the two retrieval systems under consideration, and all 10 topics are judged until there is 100% co nfidence in  X  MAP . This is the data we will use to train  X  j .
Training data for n  X  10 is obtained by simulating an evaluation over a subset of the topic using the judgments we have just made. For n&gt; 10, training data is obtained by sampling with replacement from our set of 10 topics and simulating an evaluation over this set. The results of the simulations are used to train  X  j . Then, using n and  X  j ,wemay find the minimum-cost number of topics and judgments.
The pilot study serves two purposes: one, to see if the experiment is worth continuing; if it is, then to estimate the amount of work necessary to carry it to completion. To experiment with this pilot study, we will again use Robust systems and topics. While this data set does not represent a new task or new topics, they provide a useful  X  X ruth X : if our pilot study and subsequent evaluation is able to correctly identify differences in Robust systems, it should work for other tasks and topics as well.

The ten topics randomly selected for the pilot study are 315, 350, 367, 393, 442, 602, 610, 670, 675, and 681. Pairs of systems are randomly selected and evaluated over these 10 topics. The model  X  j is trained as described above. The minimum-cost  X  and n are found using Eq. 7. We then randomly select n new topics (excluding the ones from the pilot study) and evaluate to confidence  X  on those.
We do two experiments. For the first, the first 249 top-ics are free ( C t = 0) but any more than that cost 20 C j this models receiving 249 topics from an official body (e.g. NIST) but no relevance judgments. For the second, all top-ics cost 20 times as much as relevance judgments. If the optimal number of topics is greater than 249, we create new topics by sampling with replacement from the existing top-ics. They will still be treated as different topics, so if we have duplicated topic 301, judgments for 301 will not count towards 301 . (As it turns out, we never had to do this.) As an example, consider systems polyudp5 and NLPR04SemLM . For the first 10 topics it requires 1,587 relevance judgments to reach 100% confidence in the difference between them. We simulate evaluating one topic, two topics, and so on; the Using this model, we predict the minimum cost to be achieved with 51 topics an d 85% confidenc e (1625 relevance judg-ments). It ends up taking 836 judgments to reach 85% con-fidence on 51 topics, so we overestimated the cost. We have money left over for a pizza party for our assessors!
For the first experiment, the extra cost of using more than 249 topics resulted in the model never selecting more than 216. Cost, therefore, is equivalent to the number of judg-ments. On average, over 2,000 trials, cost was predicted to be 1,492 judgments to reach 75% confidence over 157 topics. In actuality, it required 950 judgments on average to reach the target confidence. For comparison, the number of judg-ments needed to reach 100% co nfidence over 25 topics was 1,781; this is a 47% decrease in the number of judgments.
Since 542 fewer judgments were required than predicted, over many experiments our cost function will tend to per-form better than expected. However, more than half the trials required more judgments than predicted. The correla-tion between the predicted and actual number of judgments is 0.43, indicating that the model is doing a reasonable job but could be better. This is partially affected by the pilot sample. If the pilot sample is  X  X arder X  than the population, we may consistently overestimate the number of judgments required; if the pilot sample is  X  X asier X , we may consistently underestimate the number of judgments required. In this case it seems our pilot sample was a bit easier than the pop-ulation. Using a larger pilot sample could produce more accurate predictions, but of course would require a greater start-up cost in the pilot study.

The start-up cost is the number of judgments needed to reach 100% co nfidence on the pilot sample of 10 topics. Over 2,000 trials, the mean start-up cost was 871 judgments. No trial required more than 2,000 judgments, which again sug-gests our pilot sample is easier than the population. About 63% of trials required fewer than 1,000 judgments to start.
Figure 8 shows the observed power as well as the power for 25 topics at 100% confidence. Power is high. In fact, we have outperformed our predictions, making up for the trials in which we underpredicted the number of judgments.
For the second experiment, in which topics cost 20 times as much as relevance judgments, it turns out that it is often most cost-beneficial to judg e 25 topics to 100% co nfidence. In over half the trials, it was more cost-effective to judge 25 topics to 100% confidence. It was never cost-effective to judge more than 74 topics. Of course, these topics could then be reused, and would be free for the next experiment. Figure 8: Observed power over 2,000 trials train-ing a prediction model  X  j using data from a pilot study. The solid line is the power of using 25 topics at 100% confidence; the points are the empirical re-sult of using more topics with less confidence (fewer judgments).

Since fewer topics were used, the number of judgments was higher than the previous experiment in which topics were free. On average, 1438 judgments were made to reach 0 . 97 confidence over 29 . 2 topics. Including the cost of developing topics, total mean actual cost was 2022. This was greater than the predicted cost of 1858, though this time fewer than half of the trials required more judgments than predicted.
The observed power is actually a little worse than pre-dicted (not shown). This is because smaller sets of topics are more prone to errors due to sampling, even with 100% confidence. This is further reinforcement that more topics is superior.
We have proved that a large number of topics with a few relevance judgments for each is as good for evaluation pur-posesasasmallnumberoftopicswithalotofrelevance judgments for each. Furthermore, we have shown that the former is much less expensive than the latter: 50% less as-sessor effort compared to ha ving 100% co nfidence in each topic; 80% less assessor effort compared to judging a pool of depth 100 for each topic.

Our cost function depends on an accurate model of the number of judgments needed to reach a given confidence level, but there is such a huge amount of variance over sys-tems and topics that it is very difficult to predict with good accuracy. Accuracy can be improved: preliminary experi-ments suggest that measuring the similarity between ranked lists and including it as a feature in the model improves pre-dictions substantially. Additionally, rather than train to the average minimum number of judgments required to achieve a given confidence level, we could train a model using quan-tiles of judgments. Preliminary experiments with quantile regression models are encouraging.
 All of our experiments were done using pairs of systems. In reality, researchers would often have multiple systems to evaluate. Hypothesis testing in these situations becomes more difficult, as errors become more frequent simply by chance. This is known as the  X  X ultiple testing problem X . It requires ad hoc adjustments to Type I and Type II error rates, and is beyond the scope of this work.

Finally, an obvious direction for future work is to analyze the Wilcoxon sign rank test and the t-test in the same way. These tests are perhaps more widely used in IR experimen-tation than the sign test, and generally have more power to detect the types of differences we are interested in [9]. This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by the Defense Ad-vanced Research Projects Agency (DARPA) under contract number HR001-06-C-0023. Any opi nions, findings, and con-clusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. [1] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal [2] J. Cohen. Statistical Power Analysis for the Behavioral [3] G. V. Cormack and T. R. Lyman. Power and bias of [4] J. Faraway. Extending the Linear Model with R . [5] J. M. Hoenig and D. M. Heisey. The abuse of power: [6] E.C.Jensen. Repeatable Evaluation of Information [7] E.L.Lehmann. Testing Statistical Hypotheses . [8] M. Sanderson and J. Zobel. Information retrieval [9] M.D.Smucker,J.Allan,andB.Carterette.A [10] C. J. van Rijsbergen. Information Retrieval . [11] W. Venables and B. D. Ripley. Modern Applied [12] E. M. Voorhees. Overview of the 2004 trec robust [13] D. Wackerly, W. Mendenhall, and R. L. Sheaffer. [14] J. Zobel. How Reliable are the Results of Large-Scale
