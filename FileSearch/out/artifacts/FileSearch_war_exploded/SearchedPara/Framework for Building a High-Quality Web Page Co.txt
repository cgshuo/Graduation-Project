 The web is becoming more and more important as a potential information source to add value to high-quality scholarly information services. What is required is a web However, even with automation, this task requires a large amount of manual labor of the web data, and the sparseness of relevant pages. 
Many researchers have studied the topic of search and classification of web pages; however, most of these studies are of a best-effort type and pay no attention to quality assurance. Instead, we sought an efficient method to comprehensively build a homepage collection that would assure both high recall and high precision. We mainly focused on researchers X  homepages in the present study. 
Some studies show that it is generally effective to collect homepages by using various web-related features. Taking into account that a homepage is often represented by a logical page group, the information in the surrounding pages of a page group entry pages. Therefore, we propose a method to utilize features considering page group structures as a means of building a high-quality homepage collection with the support vector machine (SVM). 
Since there are a huge number of web pages, one foreseeable problem with our process into two steps: rough filtering for efficiently narrowing down the candidate pages with a very high recall, followed by accurate classification of the candidate target pages output from the rough filtering with both high recall and high precision. Both processes consider the page group structures. Our method achieves high requirements for the collection. The method proposed in this paper belongs to the web page classification domain, and closely related to the web page search and clustering domains. In these domains, what information sources to use is the most important consideration and how to use them is the second most important. 
The prior works tried to exploit, besides textual contents, various web-related logical page group, and while it is used to gather potential pages comprehensively, it tends to increase noise. Since comprehensiveness is a key factor for quality assurance of a web page collection, we mainly exploited the last source. 
Some studies on exploiting surrounding pages first classify a page based only on its content and then further use results derived from other information sources such as the link structure and directory structure [7]. However, such an approach does not work when an entry page contains no textual information except hyperlinks. Other studies first cluster web pages based on the local link structure, etc., and then merge effectiveness of this approach is limite d, probably because it also merges many irrelevant words from the surrounding pages. 
We also exploit the content of surround ing pages by considering the local link target homepages as possible, page group models are applied for considering the local link structure, so that the homepages presented on a single page or on a set of pages that constitutes a logical page group can be gathered. In the accurate classification, the features on surrounding pages belonging to different page groups are concatenated, so that the contexts corresponding to th e relative location are represented. 
Almost no prior study has tried to assure a quality level that would be high enough for practical applications. Our approach to assuring quality is by building a three-way classifier by combining a recall-assured classifier and a precision-assured classifier. The framework of our method has two steps (Fig. 1): rough filtering and accurate classification. with limited noise from the web. The input is all the web pages and the output is the candidate pages satisfying the required recall level. We set the rough filtering performance to, for example, at least 98% and desirably 99%. Precision does not matter so much, but fewer output pages is desirable given the condition on recall. 
The accurate classification is for accurately classifying the candidate pages output from the rough filtering into three classes: assured positive, assured negative, and uncertain. For example, the recall should be at least 95% and the precision should be at least 99%. Even with the state-of-the-art classification technology, it is impossible processing. Therefore, human involvement is indispensable for overcoming the gap between the requirement and the technology level. To assure high performance, a relatively high computer processing cost is allowed for the accurate classification compared with the rough filtering, whereas the number of pages that need manual assessment should be reduced as much as possible. 4.1 Overview of the Rough Filtering The rough filtering uses property-based keyword lists and several page group models. Fig. 2 illustrates the conceptual overview of the rough filtering. 
Each web page is first mapped to a document vector consisting of binary values, the keyword list are present in the web page. Next, for each of the page group models, the document vectors are merged by making a logical sum of each vector element. In this process, only the elements corresponding to the keyword lists of suitable types for each page group model are considered (in the figure, ignored elements are indicated with `x' at the output from PGM-I). They are further merged to the entry page's document vector to compose a final document vector. Here, a conceptual document represented by the final document vector is called a virtual entry page, and the process to merge the document vectors is called keyword propagation. Finally, scores of virtual entry pages are obtained by counting the number of 1's in the document vector, and those that scored more than or equal to a threshold score are output. The threshold requirement, e.g., 99%, and the output amount is reasonable. 4.2 Property-Based Keyword Lists In order to grasp the basic information elements common to homepages in the same category, we introduce property-based keyword lists, expecting that a certain number (not necessarily all) of them will be included in each target page or in its surrounding pages. Since no method is available for automatically extracting property-based keyword lists where each of them contains keywords grouped according to the same property, we used an ad hoc method to create keyword lists for the present work and containing 86 keywords for the researchers X  homepage category. Each of the keyword lists is then assigned a type, either organization-related or non-organization-related based on whether the keyword list corresponding to the properties common to the members in the same organization. Note that the actual keywords are in Japanese. 4.3 Page Group Models Taking into account logical page group structure in the same site, we propose four simple page group models (PGMs). Table 1 lists the definitions of surrounding pages and Table 2 those of PGMs. Single page model (SPM) and single site model (SSM) were used as two baselines. PGM-Od is intended to exploit all kinds of keywords in out-linked component pages in the lower levels of the directory subtree. PGM-Ou, PGM-I, and PGM-U are the directory path: PGM-Ou for out-linked pages, PGM-I for in-linked pages, and PGM-U for directory entry pages, respectively. Since simple PGMs usually introduce many irrelevant pages, we propose modified PGMs to limit such noise. PGM-Od@  X  propagates keywords only when the out-link page number is less than threshold  X  , based on the observations that some noise sources are large groups of pages mutually linked within a directory, and that an entry page having many out-links always contains sufficient keywords in itself. PGM-Ou#, PGM-I#, and PGM-U# propagate only organization-related keywords, based on the observation that non-organization-related keywords are not included in the upper information in component pages and collect insufficient information, we combine all of the modified PGMs to utilize as much as useful information. 4.4 Experiments For the experiments, we used a 100GB web corpus (NW100G-01, gathered from .jp domain and used for WEB Tasks of NTCIR Workshops). We randomly collected 11,338 pages containing some typical Japanese family names (Jname data) and manually assessed them, obtained 426 positive samples and 10,912 negative samples. For further evaluating, we used another corpus (NW1000G-04) addtionally. 
First, we experimented on individual simple PGMs. As what we expected, the superior to SSM since a lot of noise is introduced by the keyword propagation. 
Next, we experimented on modified PGMs and compared each of them with the corresponding simple PGM with typical parameters. We selected s=0 for PGM-Od since almost all non-organization-related keywords are collected from within the same directory. Around the 99% recall region, the page amount with simple PGM-Od increases by 80% over SPM, whereas modified PGM-Od can limit the increase to 50%. For PGM-Ou, PGM-I, and PGM-U around the 99% recall region, although the page amount for each simple PGM increases by 40% to 120% over that of SPM, the modified PGMs can limit the increase to almost the same level as SPM. 
Finally, we experimented on combinations of PGMs with several promising parameter sets. Fig. 3 shows the run results of the top three best-performing combinations of PGMs. We will refer to each of them hereinafter as follows: PGM-C1: PGM-Od@5(0,2),Ou#(1,3),I#(0,3),U#(0,3) PGM-C2: PGM-Od@10(0,2),Ou#(1,3),I#(0,3),U#(0,3) PGM-C3: PGM-Od@20(0,2),Ou#(1,3),I#(0,3),U#(0,3) Each of them uses all four modified PGMs with the same parameters except for  X  of PGM-Od. As s= 0 is used for PGM-Od, s= 1 is selected for PGM-Ou. The x -axis is the page amount n c ( i ) (1  X  i  X  12), namely, the number of pages in the corpus that scored at positive sample data, n p ( i ) is the number of positive sample data that scored at least i . every subsequent one corresponds to a threshold score incremented by 1. In general, a higher recall and a lower page amount indicate better performance; however, we put a priority on recall. The results show that even the best performing run PGM-C1 is inferior to SPM in all the recall ranges except for 100%. However, the proposed method reduced the page amount to a certain degree despite its use of PGMs. 4.5 Considerations 4.5.1 Ability to Find Potential Homepages We assessed the pages that were contained in Jname data and scored less than 3 with SPM, but scored at least 4 with PGM-C3 and found 13 new positive data. The results show that the proposed method can collect positive pages that cannot be gathered by SPM even if the threshold score is set to 3 to guarantee 99% recall for the manually assessed positive samples. Taking into account the new 13 positive data, the recalls of SPM at threshold scores of 2 and 3 should be corrected from 99.8% (425/426) to 97.0% (426/439) and from 99.1% (422/426) to 96.1% (422/439). By comparing these values with the recalls of the proposed method at threshold scores of 4 and 5 respectively, it is obvious that the proposed method outperforms SPM with 5% significance. 
Furthermore, four of the positive pages could not be gathered with SPM even when the threshold score was set to 1. This implies SPM can hardly achieve the recall goal for any feasible page amount. A failure analysis on all the three pages that scored only 3 with PGM-C1 through PGM-C3 revealed that they have similar patterns and scored only 2 with SPM. Although they have hyperlinks to the researchers X  personal homepages, our method cannot exploit them because they are on separate sites. 
To guarantee that the overall recall to be more than 98% regarding the confidence Consequently we selected PGM-C2 as the most appropriate one for the current goal. 4.5.2 Applicability to a Larger Data Set We applied the rough filtering to the larger data set (NW1000G-04) with a procedure similar to that for NW100G-01. The approximate computational complexities of the overall processing cost for the rough filtering is O( N log N ) where N is the number of web pages in the corpus. The same parameters of PGMs for NW100G-01 were applied to NW1000G-04. The threshold number  X  of out-link pages for PGM-Od was set to 20. The candidate pages were gathered with the threshold score 4. Table 3 compares the experimental results and it shows the rough filtering is not only applicable to but also more efficient for the larger data set. However, as we have not assessed the correctness of the output, the stability of the accuracy remains to be investigated. 5.1 Composition of the Accurate Classification Fig. 4 shows the composition of the accurate classification (95% recall and 99% precision are example quality requirements). We use two component classifiers to construct a three-way classifier. The recall-assured (precision-assured) classifier assures the target recall (precision) with the highest possible precision (recall). 
The pages output from the rough filtering are first input to the recall-assured are then input to the precision-assured classifier and its positive predictions are classified as  X  X ssured positive X . The remain ing pages are classified as  X  X ncertain X , which require manual assessment. 
The current work uses the SVM light package with a linear kernel, tuning with its options c and j . The experiment performance was evaluated by precision (#correct #positive samples) and F-measure (2*precision*recall / (precision+recall)). 5.2 Surrounding Page Group and Feature Set When a page (current page) is given, its surrounding pages are categorized into are shown in Table 4. In a logical page group, for example, G in,low consists of in-link pages in lower directories which might represent component pages having back links which might represent entry pages of the organization the researcher belongs to. denotes a surrounding page group, and w t  X  W t denotes a feature word. A feature set is concatenating feature subsets on pages in G cur and G *,upper , and the feature set  X  X -i-e-1 X  is composed by concatenating feature subsets on pages in G cur and G *,* . 5.3 Feature Word, Text Type, and Value Type English to tokenize the feature words from the corresponding texts. 
Plain-text-based features F plain,* (*) are extracted from textual content excluding tags, scripts, comments, etc. We use top 2,000 feature words W plain selected based on not more than 16 bytes long, omitting spaces in the Japanese case and not more than 4 words in the English case. The obtained words with less than 1% file frequency in Japanese and all the obtained words in English are used as feature words W tagged since all of them are considered to be property words. In the experiments, a feature set was composed by using feature words either plain alone or plain and tagged together. The latter case is indicated by the suffix  X  X tag X  of the run name. f tested to see if the feature word distribution within surrounding page groups is informative. The two value types are exclusively used for composing a feature set. Use of the real value type is indicated by the suffix  X  X real X  in the run name. 5.4 Experiments 5.4.1 Experiments Using WebKB Data Set We used the WebKB data set [9] (in English) for testing the effectiveness of the proposed features. All the 8,282 pages are classified into 7 categories, and 4 categories, student, faculty, course, and project, were used as positive samples respectively. As recommended by WebKB project, we used the leave-one-university-out cross-validation method and the pages of miscellaneous universities were always used as training data. A feature set composed by F plain,binary ( G cur ) only was used as the baseline. The experimental results of well performing feature sets compared with prior works are shown in Table 5. The overall experimental results show that tag-based features are consistently effective and the differences caused by the feature composition. The results show that our method out-performed almost all of the prior works. 5.4.2 Experiments Using NW100GB-01 Sample Data Set We manually prepared 906 positive samples and 20,366 negative samples from NW100G-01 (in Japanese). Five-fold cross validation was used. A feature set only performances of the two best performing classifiers. Their performances compared area, both o-i-e-1_tag_real and u-1_tag slightly outperform the baseline; while in the high recall region, o-i-e-1_tag_real evidently outperforms the baseline and u-1_tag also performs rather well. 5.5 Considerations 5.5.1 Effectiveness of the Proposed Features The experiment results on both data sets indicate that the proposed features are effective for general performance (F-measure) and that the proposed method is applicable to research-related homepa ge categories in English or Japanese. Furthermore, the proposed features improve the performances of the precision/recall-assured classifiers. Features on the surrounding page groups are generally effective, but their contributions vary. For precision-assured classifiers, the upper hierarchy such pages provide contextual informatio n, e.g., organization names and research fields, which are lacking from the current page itself but are very important for classifying the page with very high confidence. For recall-assured classifiers, all surrounding page groups (G *,* ) notably contribute to precision, despite their noisy nature. 
Other experiment results not presented here have shown the following. (1) Adding tagged-text-based features consistently gained performance. This result can be interpreted that noisy information from the surrounding pages is suppressed by the tagged-text-based features and consequently the useful information in the surrounding pages can be exploited. (2) The effect of value types varies depending on the feature sets, and the performance gain is marginal. 5.5.2 Reduction of Manual Assessment To know by how much the proposed method reduced the amount of pages requiring compositions of three-way classifiers. 
Table 7 shows estimated page numbers of classification output from NW100G-01 at three different quality requirements for two three-way classifiers, one using the both recall/precision-assured classifiers. 
Comparing the  X  X ncertain X  class sizes, o-i-e-1_tag_real significantly reduces the amount of pages requiring manual assessment, especially when the required quality is relaxed. We proposed a realistic framework with a two-step process of rough filtering and accurate introduced an idea of the local page group structure and demonstrated its uses for filtering and classifying web pages, where researchers' homepages were used as an example. Although we have just applied our method to research-related categories, we expect it will information service, we believe that high-quality collections built with our method would give a guarantee of high quality for the results of various domain-specific search engines. 
Even though our method fulfills the objectives of our research, for the accurate classification, we will further investigate a way to estimate the likelihood of the component pages and incorporate it in the current method. 
In conclusion, building a high-quality homepage collection by exploiting rich web-problem when we look at the diversity of web data. The method presented in this paper gives a general framework for solving such problems, and we hope that it will contribute to research on web information utilization. This study was partially supported by a Grant-in-Aid for Scientific Research B (No. 18300037) from the Japan Society for the Promotion of Science (JSPS). We used the NW100G-01 and NW1000G-04 document data sets with permission from the National Institute of Informatics (NII). We would like to thank Professors Akiko Aizawa and Atsuhiro Takasu of NII for their precious advice. 
