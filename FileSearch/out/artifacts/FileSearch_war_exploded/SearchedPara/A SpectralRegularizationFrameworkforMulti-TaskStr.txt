 learning), see [6] and references therein.
 the tasks, which can hence be learned independently with standard methods such as SVMs. When tral functions which relate to Schatten L these data sets. The y also highlight that our approach can be used for transfer learning. symmetric matrices, by S d set of d d orthogonal matrices. For every positi ve inte ger n , we dene IN let T be the number of tasks which we want to simultaneously learn. We assume for simplicity that each task t 2 IN where w but we have kept it constant for simplicity of notation.
 Our goal is to learn the vectors w minimizing the function P sharing some common structure and is given by where F : S d D 2 S d ++ we write D = U U &gt; , where U 2 O d , = Diag ( 1 ; : : : ; d ) , and dene the associated real function, as abo ve.
 of More generally , functions of the form f ( ) = ; 0 , allo w for combining shared features Thus, we propose to solv e the minimization problem after partial minimization over D admits a minimizer .
 to
D . That is, we can compute the inmum particular , in Section 4 we shall sho w that the rest are merely vector problems.
 setting [3]. in ( w; D ) , which in turn implies that (2.4) is a con vex optimization problem. We say that the real-v alued function g : (0 ; 1 ) ! IR is matrix concave of order d if where G is dened as in (2.3). The notation denotes the Loe wner partial order on S d : C D orders (and hence standard conca vity).
 Theor em 3.1. Let F : S d defined as ( w; D ) = w &gt; F ( D ) w is jointly con vex if and only if 1 Pr oof . By denition, is con vex if and only if, for any w (0 ; 1) , it holds that Let C := F ( D and z := w w &gt; Cw w &gt; B ( A + B ) 1 A ( A + B ) 1 B + I ( A + B ) 1 B &gt; B I ( A + B ) 1 B w ; 8 w 2 IR d , or to is identical to (see e.g. [10, Sec. 7.7]) or, using the initial notation, By denition, this inequality holds for any D conca ve of order d .
 is matrix conca ve of order d , function jointly con vex function [9, Sec. IV.2.4]. 4.1 Partial Minimization of the Penalty Term in (2.5) relates to the Schatten L B tak e the place of W W &gt; for bre vity .
 Lemma 4.1. Let F : S d ! S d be a spectr al function, B 2 S d and B . Then, inf f tr( F ( D ) B ) : D 2 S d ++ ; tr D 1 g = inf so that the corr esponding eig envalues are in the rever se order as the where result follo ws.
 power function.
 Pr oposition 4.2. Let B 2 S d Mor eover , if B 2 S d Pr oof . By Lemma 4.1, it suf ces to sho w the analogous statement for vectors, namely that where When inequality is sharp in all other cases, we replace ization with a Schatten L with p 2 (0 ; 2] , p = 2 s . The Schatten L note that generalization error bounds for Schatten L lines of [14]. 4.2 Lear ning Algorithm along with singular value decomposition of the matrix W . In particular , for the Schatten L penalty term (2.2) as where " &gt; 0 and let Reg (under the constraints in (2.5)), given by the formula Moreo ver, there exists a minimizer of problem (2.4), which is unique if p 2 (1 ; 2] . consists in solving the problem when D is x ed. Specically , introducing new variables for ( F ( D )) 1 other words, we simply learn the parameters w equation (4.2).
 by the algorithm con verges to the unique minimizer of Reg if of round-of f effects. kernel K S ++ ; tr D 1 g that the set K (see e.g. [11]), problem (2.4) is equi valent to minimizing the function over c in y 2 IR then the function E t : K ! [0 ; 1 ) defined for every K 2 K as is con vex.
 Pr oof . Without loss of generality , we can assume as in [15 ] that K For every a 2 IR m and K 2 K , we dene the function G which is jointly con vex by Theorem 3.1. Clearly , E con vexity of E The fact that the function E con vexity property stated in Theorem 3.1. dif ferent L experiments on transfer learning.
 error of the predicted from the actual ratings for the test data, averaged across people. The second data set is the school data set from the Inner London Education Authority (see accounting for the bias term.
 against the gradient descent algorithm, both implemented in Matlab, for the Schatten L needed in gradient descent, for computing the gradient of the Schatten L order of magnitude lar ger , leading to the lar ge dif ference in time cost. changes. Specically , we choose functions giving rise to Schatten L regularizer is simpler than HB and the data splits of [5] are not available). using the raw data ( D = I D variance of 13 : 9% with the raw representation. statistical properties of the tasks.
 This work was supported by EPSRC Grant EP/D052807/1, NSF Grant DMS 0712827 and by the IST Programme of the European Commission, PASCAL Netw ork of Excellence IST -2002-506778.
