 David F. Gleich dgleich @ purdue . edu Computer Science, Purdue University, West Lafayette, IN 47906 Michael W. Mahoney mmahoney @ icsi . berkeley . edu In an ideal setting, one begins with a well-defined objec-tive function and one develops (or calls as a subroutine) an algorithm to solve that problem  X  X xactly. X  In many appli-cations, however, the appropriate objective function might not be known in advance and / or one might have only an approximation algorithm available for that objective. In ad-dition, in practical settings, one might heuristically modify the algorithm X  e.g , stop at fewer than the theoretically-appropriate number of iterations, truncate very small entries of a large dense vector to zero, etc. X  X n order to achieve improved performance. In these cases, it can be di ffi cult to have a precise theoretical understanding of what these heuristic modifications are doing, even though anecdotal evidence suggests that these modifications can be the prime determinants of the practical performance of many machine learning algorithms on realistic data.
 To address these issues, we would like to introduce the term algorithmic anti-di ff erentiation for the following ac-tivity: analyzing a given algorithmic procedure (typically, an approximation algorithm or a heuristic) and reinterpret-ing it as a scheme that exactly solves a di ff erent (but, of course, related) optimization problem (implicitly, in the sense that the actual problem is never written down and may not even be known a priori ). We have chosen the term  X  X nti-di ff erentiation X  because many algorithmic pro-cedures arise as a means to solve exactly the optimality conditions of some optimization problem. If, instead, we begin with a practically-useful algorithmic procedure, the idea behind algorithmic anti-di ff erentiation is to find an op-timization problem for which that procedure exactly solves the the optimality conditions. Here, we are drawing an analogy between the optimality conditions as the derivative of the optimization problem; hence, reversing the proce-dure is  X  X nti-di ff erentiation. X  In a simple unconstrained optimization problem, this analogy is precise, as the opti-mality conditions are statements about the derivative of the objective function. 1 In this paper, we present a  X  X ase study X  of algorithmic anti-di ff erentiation, with a focus on two related problems having to do with finding locally-biased graph partitions. We start by showing (in Section 3.1) that the solution of a PageRank problem (Page et al., 1999) is a 2-norm variation of a 1-norm formulation of a min-cut linear program related to the so-called FlowImprove procedure (Andersen &amp; Lang, 2008). Although a similar fact was known for personalized PageRank and a locally-biased spectral partitioning prob-lem (Mahoney et al., 2012), our new result permits us to reverse the relationships and demonstrate (in Section 3.2) a min-cut / max-flow problem that relaxes to the standard PageRank problem with uniform teleportation.
 We then show (in Section 3.3) that a particularly e ffi cient pro-cedure for solving a personalized PageRank problem (An-dersen et al., 2006) implicitly corresponds to adding a 1-norm regularization term to the 2-norm PageRank objective. By examining the optimality conditions for this regularized problem, we will be able to understand why this procedure gives rise to both sparse solutions and a sparse truncated residual ; and our analysis also makes a hidden tolerance parameter in the algorithmic procedure an explicit feature of our approach.
 The utility of these results is that we can begin to under-stand more precisely the implicit side-e ff ects of heuristic design decisions that are often made in implementing al-gorithms. We illustrate this in Section 4. For example, we compare these procedures to illustrate subtle but impor-tant di ff erences between using max-flow / min-cut programs, their spectral relatives, and the  X  X runcated X  versions of their spectral relatives; and we point out as a remark that we can also use this same setup to understand other related di ff usion-based methods that have been popular recently in machine learning.
 Far from inventing the concept of algorithmic anti-di ff erentiation, our contribution is to make it precise and to provide a detailed case study of it for a class of algorithms that has received a great deal of interest recently in machine learning and data analysis. For instance, exploiting algo-rithmic anti-di ff erentiation ideas has been very fruitful for understanding the relationships between various iterative methods for solving linear systems of equations in linear algebra and scientific computing. As an example of this, Saunders (1995) (in his Results 8 and 9) demonstrates an equivalence relationship between the iterates of two widely-used algorithms. Although these problems are shown to be mathematically equivalent in exact arithmetic, the nu-merical properties X  X nd thus the performance in practical implementations X  X f these two methods di ff er substantially in the presence of  X  X oise X  introduced by roundo ff error; and thus one variant is much preferable in practice. (For this and related reasons alluded to below, we expect that precisely understanding algorithmic anti-di ff erentiation will help in the development of better variants of many popular machine learning algorithms in very large-scale settings.) We conclude this introduction with a brief survey of sev-eral prior examples of studying approximation algorithms to elicit surprising optimization properties. While not ex-haustive, these are the examples that most informed our approach; and they are most strongly related to instances of spectral methods, cuts, and flows. First, Dhillon et al. (2007) show that the kernel k -means algorithm is equivalent to a particular type of trace minimization, and they used this insight to produce a scalable graph clustering method based on the normalized cut objective. Subsequently, Kulis &amp; Jor-dan (2012) construct a Bayesian algorithmic anti-derivative, which gives rise to the Dirichlet process-means (DP-means) algorithm. Second, Koutra et al. (2011) show how an algo-rithmic approximation to a set of Belief Propagation equa-tions corresponds to solving a linear system that is closely related to a di ff usion process. Third, Chin et al. (2013) use the relationship between LASSO-style objectives and max-flow / min-cut problems (of the form of Prob. (2) be-low) and shortest paths problems in order to derive new runtime bounds for the LASSO problems that occur in ma-chine learning (Tibshirani, 1994). Finally, the work that is perhaps most-closely related to ours shows how algorithmic decisions such as early stopping (of an iterative algorithm) can be recast as linear operators that solve particular regu-larized SDPs (Orecchia &amp; Mahoney, 2011). This has the interpretation that approximate computation in and of itself can implicitly implement regularization ( e.g. , in the  X  X pace X  of approximation algorithms (Mahoney, 2012)), whereas our results below establish similar results for the solutions of those approximation algorithms. To draw precise relationships between various formulations of min-cut / max-flow problems on graphs, spectral variations of these problems, and related PageRank problems, we will require an unusually high degree of precision in our notation. Let G = ( V , E ,w ) be a connected, undirected graph with positive edge weights , and let n = | V | be the number of vertices. Fix an indexing of the vertices from 1 to n , and then the adjacency matrix is the n  X  n matrix A where The degrees of each node in G are given by the row-sums of the matrix A . Others have called these the weighted degrees, but since all of our graphs are weighted, we won X  X  make this distinction. The matrix D = diag( Ae ), i.e. , D is a n  X  n square diagonal matrix with the degree of each vertex on the diagonal; the vector d = De is the vector of degrees. The vector e will denote the all-ones vector; and e i will denote the vector with a one in the i th position and zeros everywhere else. In addition, for a set S , e S = P i  X  S are columns of the identity matrix for vertices in S in a fixed order; and vol( S ) is e T S d , the sum of degrees of nodes in S . Let m be the number of undirected edges in G ( i.e. , where each edge is only counted once), and fix an ordering of these m edges. The edge-node incidence matrix of G is an m  X  n matrix B where each row is of the form ( e i  X  e j ) for an edge ( i , j ). Note that the incidence matrix does not include any e ff ect of the edge weights and includes only the combinatorial structure of the graph. We let the diagonal matrix C hold the information on the edge weights, where the order of edges is the same. In particular, we use the notation C ( i , j ) to denote a diagonal of this matrix for the edge ( i , j ). In this case, the combinatorial Laplacian matrix is given by L = B T CB = D  X  A ; and the uniform random-walk transition matrix on G is given by P = D  X  1 A . As an aside, it is tempting to define the incidence matrix as  X  C  X  1 / 2 B  X  so as to preserve the relationship:  X  L This choice would end up causing confusion when we begin to discuss how PageRank and related di ff usion computations are related to the 1-norm min-cut problems. We have found it most convenient to treat the edge weights via a weighted norm, such that if x is a binary vector, then The spectral problem will then replace the 1-norm with a 2-norm, but it will preserve the weighting so that: The PageRank problem for an undirected graph can be de-fined as the solution of the linear equation (Arasu et al., 2002; Langville &amp; Meyer, 2006): where P is the random walk matrix for the graph (as defined above),  X  is the teleportation parameter which satisfies 0 &lt;  X  &lt; 1, and v is the non-negative teleportation distribution vector with v i  X  0 and P i v i = e T v = 1. This formulation is entirely equivalent to the standard definition as the stationary distribution of a random walk with restart or the PageRank Markov chain (Langville &amp; Meyer, 2006).
 Recall also that, for an undirected graph, the following formulations of PageRank are all equivalent: 1. ( I  X   X  AD  X  1 ) x = (1  X   X  ) v ; 2. ( I  X   X  A ) y = (1  X   X  ) D  X  1 / 2 v , 3. [  X  D + L ] z =  X  v where  X  = 1 / (1 +  X  ) and x = Dz . All of these relationships are straightforward to derive from the original PageRank equation. We will primarily use the first and third; but we note that the second arises in a setup of semi-supervised learning on a graph (Zhou et al., 2004), a topic upon which we will comment below. In this section, we present two algorithmic anti-derivatives. The first (in Section 3.1) relates the PageRank problem on an undirected graph to a particular minimum s , t -cut con-struction. Under certain conditions, this problem is a 2-norm minorant of a 1-norm formulation of the min-cut objective. We also (in Section 3.2) establish the reverse relationship that extracts a cut / flow problem from any PageRank prob-lem. The second (in Section 3.3) relates an approximation algorithm for the personalized PageRank problem to a 1-norm regularized version of the 2-norm objective. 3.1. Relating min-cut to PageRank Recall the 1-norm formulation of a linear program for the min-s , t -cut problem: When the weights are integers, these problems are usu-ally solved using an e ffi cient max-flow routine in order to compute a strictly-integral solution. Here, we consider a specific s , t -cut problem inspired by the FlowImprove pro-cedure (Andersen &amp; Lang, 2008), which was also used in recent work on a local version of this objective (Orecchia &amp; Zhu, 2014). In order to state this problem, we must fix a set of vertices S . These roughly correspond to the telepor-tation vector in the PageRank problem, and we make this analogy precise below. Once we have this set, we define a new weighted graph based on the original graph that we call the localized cut graph . This graph consists of the original graph, with two additional vertices, s and t , that connect to S and  X  S , respectively, with weights equal to  X  multiplied by the degree of each connected vertex. We illustrate a local-ized cut graph in Figure 1. We now state the formal version for completeness.
 Definition 1 Let G = ( V , E ) be a graph, let S be a set of vertices, possibly empty, let  X  S be the complement set, and let  X  be a non-negative constant. Then the localized cut graph is the weighted, undirected graph with adjacency matrix: where d S = De S is a degree vector localized on the set S , A is the adjacency matrix of the original graph G , and  X   X  0 is a non-negative weight. Note that the first vertex is s and the last vertex is t .
 In the remainder of the section, we X  X l use the  X  and S pa-rameter to denote the matrices for the localized cut graph. For example, B ( S ) is the incidence matrix of the localized cut graph, which depends on the set S : where, recall, the variable I S are the columns of the identity matrix corresponding to vertices in S . The edge-weights of the localized cut graph are given by the diagonal matrix C (  X  ), which depends on the value  X  . In this case, the min-imum weighted s , t cut in the flow graph solves the linear program: The following theorem is our first algorithmic anti-derivative. In it, we show that PageRank implicitly solves a 2-norm variation of the 1-norm formulation of the s , t -cut problem, as given in Prob. (3) . (Recall that the 2-norm is a minorant of the 1-norm.) Theorem 1 Let B ( S ) be the incidence matrix for the local-ized cut graph, and C (  X  ) be the edge-weight matrix. The PageRank vector z that solves with v = d S / vol ( S ) is a renormalized solution of the 2-norm cut computation: Specifically, if x (  X , S ) is the solution of Prob. (4) , then P roof This result is mainly algebraic. The key idea is that the 2-norm problem corresponds with a quadratic objective, which PageRank solves. The quadratic objective for the 2-norm approximate cut is: If we apply the constraints that x s = 1 and x t = 0 and let x G be the free set of variables, then we arrive at the unconstrained objective: Here, the solution x G solves the linear system The vector x G = vol ( S ) z , where z is the solution of the PageRank problem defined in the theorem, which concludes the proof. 3.2. Relating PageRank to min-cut The result of the previous subsection gives only one  X  X irec-tion X  of the relationship between the PageRank problem and the min-cut problem. That is, there is a cut / flow problem that gives rise to a PageRank problem. In this subsection, we establish the reverse relationship that extracts a cut problem from any PageRank problem. This result is of in-terest by itself, but it is also of interest as a precursor to our main result in the next subsection.
 Notice that the reason the proof of Theorem 1 works is that the edges we added had weights proportional to the degree of the node, and hence the increase to the degree of the nodes was proportional to their current degree. This property, in turn, causes the diagonal of the Laplacian matrix of the localized cut graph to become  X  D + D . This idea forms the basis of our subsequent analysis. For a general PageRank problem, however, we require a slightly more general definition of the localized cut graph, which we call a PageRank cut graph .
 Definition 2 Let G = ( V , E ) be a graph, and let s  X  0 be a vector such that d  X  s  X  0. Let s connect to each node in G with weights given by the vector  X  s , and let t connect to each node in G with weights given by  X  ( d  X  s ). Then the PageRank cut graph is the weighted, undirected graph with adjacency matrix: We use B ( s ) to refer to the incidence matrix of this PageRank cut graph; and note that when s = d S , then we return to the localized cut graph definition.
 With this, we state the following theorem, which is a sort of converse to Theorem 1, as well as a corollary of independent interest. Due to its similarity with the proof of Theorem 1, the proof of this theorem is omitted.
 Theorem 2 Consider any PageRank problem that fits the framework of Prob. (1) . The PageRank vector z that solves is a renormalized solution of the 2-norm cut computation: s = v . Specifically, if x (  X , S ) is the solution of the 2-norm cut, then Corollary 1 If s = e , then the solution of a 2-norm cut is a reweighted, renormalized solution of PageRank with v = e / n.
 That is, as a corollary of our framework, the standard Page-Rank problem with v = e / n gives rise to a cut problem where s connects to each node with weight  X  and t connects to each node v with weight  X  ( d v  X  1).
 Remark 1 With respect to solving an objective, e.g. , of the form of Prob. (3) , note that the weights C (  X  ) will not in general be integer-valued. In theory, any max-flow / min-cut solver will compute the correct solution for these cases; but we have observed poor behavior from most implementations of the e ffi cient push-relabel method (Goldberg &amp; Tarjan, 1988). The most reliable way to solve these problems for general non-integer weights is to use a commercial linear programming package. Another alternative is to scale and round the weights back to integers. This latter approach introduces an arbitrary small error and may demand many bits of precision in the integers.
 Remark 2 This machinery we have introduced will pro-duce other di ff usion equations as well, depending on the details of the setup, which may be of independent interest. For example, consider the equally natural setting where the edges from s to the graph and the edges from the graph to t are not degree-weighted, but we X  X e reweighted the graph instead: In this case, the 2-norm approximate cut solution on vertices of the graph solves: Remark 3 These ideas are likely applicable much more generally in di ff usion-based machine learning. Recall, e.g. , that the procedure of Zhou et al. (2004) for semi-supervised learning on graphs solves the following: This is exactly a PageRank equation for a degree-based scal-ing of the labels, and thus our construction from Theorem 2 is directly applicable. 3.3. Relating approximate PageRank and exact 1 -norm Next, we show that the Andersen, Chung, Lang (ACL) pro-cedure for approximating a personalized PageRank vec-tor (Andersen et al., 2006) (of the form considered in Sec-tion 3.1) exactly computes a hybrid 1-norm 2-norm variant of the min-cut problem. The balance between these two terms has the e ff ect of producing sparse PageRank solutions that also have sparse truncated residuals, and it also provides an interesting connection with ` 1 -regularized ` 2 -regression problems. We start by reviewing the ACL method.
 Consider the personalized PageRank problem ( I  X   X  AD  X  1 ) x = (1  X   X  ) v , where v = e i is localized onto a single node. If A is a connected, undirected graph, then x a strictly positive solution vector. ACL use an algorith-mic procedure to approximate this personalized PageRank vector with a bounded amount of work. In addition to the PageRank parameter  X  , the procedure has two parameters:  X  &gt; 0 is a accuracy parameter that determines when to stop, and 0 &lt;  X   X  1 is an additional approximation term that we introduce. As  X   X  0, the computed solution x goes to the personalized PageRank vector that is non-zero everywhere. The value of  X  has been 1 / 2 in most previ-ous implementations of the procedure. Here, we present a modified procedure that di ff ers slightly from their original algorithm that makes the e ff ect of  X  explicit. 2 1. x (1) = 0 , r (1) = (1  X   X  ) e i , k = 1 2. while any r j &gt;  X  d j d j is the degree of node j 5. k  X  k + 1 One of the important properties of this procedure is that the algorithm maintains the invariant r = (1  X   X  ) v  X  ( I  X   X  AD throughout. 3 This method is closely related to the Gauss-Seidel method, which is a coordinate descent method for solving the personalized PageRank linear system, except for a few key di ff erences. First, the algorithm only takes a partial step along the gradient in each coordinate. Second, the algorithm does not cycle through all the rows like the standard Gauss-Seidel procedure; instead, it just picks any row with a large residual (the original method used a queue of valid vertices).
 For any 0  X   X   X  1, this algorithm converges because the sum of entries in the residual always decreases monotonically. At the solution we will have which provides an  X  -norm style worst-case approximation guarantee to the exact PageRank solution.
 Our main result in this section establishes a precise algo-rithmic anti-derivative for the ACL procedure, in the case that  X  = 1. In the same way that Theorem 2 establishes that a PageRank vector can be interpreted as optimizing an ` 2 objective involving the edge-incidence matrix, the following theorem establishes that the ACL procedure to approximate this vector can be interpreted as solving an ` 1 -regularized ` objective.
 Theorem 3 Fix a subset of vertices S . Let x be the output from the Andersen, Chung, Lang procedure with  X  = 1 , 0 &lt;  X  &lt; 1 , v = d S / vol ( S ) , and  X  fixed. Set  X  =  X  vol ( S ) / X  , and let z of the sparsity-regularized cut problem: where z = 1 z G P roof If we expand the objective function and apply the constraint z s = 1 , z t = 0, Prob. (6) becomes: Consider the optimality conditions of this quadratic problem (where s are the Lagrange multipliers): These are both necessary and su ffi cient because (  X  D + positive definite. In addition, and for the same reason, the solution is unique.
 In the remainder of the proof, we demonstrate that vector x produced by the ACL method satisfies these conditions. To do so, we first translate the optimality conditions to the equivalent PageRank normalization: When the ACL procedure finishes with  X  ,  X  , and  X  as in the theorem, the vectors x and r satisfy: we satisfy the first condition with x = Dz G / vol ( S ). All of these transformations preserve x  X  0 and z G  X  0. Also, because  X  d  X  r , we also have s  X  0. What remains to be shown is z T G s = 0.
 Here, we show x T (  X  d  X  r ) = 0, which is equivalent to the condition z T G s = 0 because the non-zero structure of the vectors is identical. Orthogonal non-zero structure su ffi because z G s = 0 is equivalent to either x i = 0 or  X  d i (or both) for all i . If x i , 0, then at some point in the execution, the vertex i was chosen at the step r j &gt;  X  that iteration, we set r i =  X  d i . If any other step increments r , we must revisit this step and set r i =  X  d i again. Then at a solution, x i , 0 requires r i =  X  d i . For such a component, s = 0, using the definition above. For x s is irrelevant, and thus, we have x T (  X  d  X  r ) = 0. This proof makes the nature of  X  immediately clear. If  X  &lt; 1, then the output from ACL is not equivalent to the solution of Prob. (6) . That is, the renormalized solution will not satisfy z s = 0. Setting  X  &lt; 1, however, will compute a solution much more rapidly. This leaves open the question of the precise form of the anti-derivative when  X  &lt; 1. We believe that a precise characterization of these solutions exists, but have not been able to obtain a simple form.
 In this section, we present several empirical results that illustrate our theory from Section 3. We begin with a few pedagogical examples in order to state solutions of these problems that are correctly normalized. These precise values are sensitive to small di ff erences and imprecisions in solving the equations. We state them here so that others can verify the correctness of their future implementations X  X lthough the codes underlying our computations are also available for download. 4 We then illustrate how the 2-norm PageRank vector (Prob. (4) ) and 1-norm regularized vector (Prob. (6) ) approximate the 1-norm min-cut problem (Prob. (2) ) for a small but widely-studied social graph. 4.1. Pedagogical examples We start by solving numerically the various formulations and variants (min-cut, PageRank, and ACL) of our basic problem for the example graph illustrated in Figure 1. A summary of these results may be found in Table 1. To orient the reader about the normalizations (which can be tricky), we present the three PageRank vectors that satisfy the re-lationships x pr = Dz and vol ( S ) z = x (  X , S ), as predicted by the theory from Section 2 and Theorem 1. Note that z , x (  X , S ) and z G round to the discrete solution produced by the exact cut if we simply threshold at about half the largest value. Thus, and not surprisingly, all these formula-tions reveal X  X o a first approximation X  X imilar properties of the graph. Importantly, though, note also that the vector z
G has the same sparsity as the true cut-vector. This is a direct consequence of the implicit ` 1 regularization that we characterize in Theorem 3. Understanding these  X  X etails X  matters critically as we proceed to apply these methods to larger graphs where there may be many small entries in the PageRank vector z . 5 4.2. Newman X  X  netscience graph Consider, for instance, Newman X  X  netscience graph (New-man, 2006), which has 379 nodes and 914 undirected edges. We considered a set S of 16 vertices, illustrated in Figure 2. In this case, then the solution of the 1-norm min-cut problem (Prob. (2) ) has 15 non-zeros; the solution of the PageRank problem (which we have shown in Prob. (4) implicitly solves an ` 2 regression problem) has 284 non-zeros; 6 and the so-lution from the ACL procedure (which we have shown in Deg. x pr z x (  X , S ) x cut z G 2 0.0788 0.0394 0.8276 1 0.2758 4 0.1475 0.0369 0.7742 1 0.2437 7 0.2362 0.0337 0.7086 1 0.2138 4 0.1435 0.0359 0.7533 1 0.2325 4 0.1297 0.0324 0.6812 1 0.1977 7 0.1186 0.0169 0.3557 0 0 3 0.0385 0.0128 0.2693 0 0 2 0.0167 0.0083 0.1749 0 0 4 0.0487 0.0122 0.2554 0 0 3 0.0419 0.0140 0.2933 0 0 Prob. (6) solves an ` 1 -regularized ` 2 regression problem) has 24 non-zeros. The true  X  X in-cut X  set is large in both the 2-norm PageRank problem and the regularized problem. Thus, we identify the underlying graph feature correctly; but the implicitly regularized ACL procedure does so with many fewer non-zeros than the vanilla PageRank procedure. We have shown that the PageRank linear system corresponds to a 2-norm variation on a 1-norm formulation of a min-cut problem, and we have also shown that the ACL procedure for computing an approximate personalized PageRank vec-tor exactly solves a 1-norm regularized version of the Page-Rank 2-norm objective. Both of these results are examples of algorithmic anti-di ff erentiation, which involves extract-ing a precise optimality characterization for the output of an approximate or heuristic algorithmic procedure.
 While a great deal of work has focused on deriving new theoretical bounds on the objective function quality or run-time of an approximation algorithm, our focus is somewhat di ff erent: we have been interested in understanding what are the precise problems that approximation algorithms and heuristics solve exactly. Prior work has exploited these ideas less precisely in a much larger-scale application to draw very strong downstream conclusions (Leskovec et al., 2009), and our empirical results have illustrated this more precisely in much smaller-scale and more-controlled applications. Our hope is that one can use these insights in order to com-bine simple heuristic / algorithmic primitives in ways that will give rise to principled scalable machine learning algo-rithms more generally. We have early results along these lines that involve semi-supervised learning.

