 Shih-Wei Lin  X  Shih-Chieh Chen  X  Wen-Jie Wu  X  Chih-Hsien Chen Abstract The back-propagation network (BPN) is a popular tool with applications in a variety of fields. Nevertheless, different problems may require different parameter settings for a given network architecture. A dataset may contain many features, but not all features are beneficial for classification by the BPN. Therefore, a particle-swarm-optimization-based approach, denoted as PSOBPN, is proposed to obtain the suitable parameter settings for BPN and to select the beneficial subset of features which result in a better classification accuracy rate. A set of 23 problems with a range of examples and features drawn from the UCI (Univer-sity of California, Irvine) machine learning repository is adopted to test the performance of the proposed algorithm. The results are compared with several well-known published algo-rithms. The comparative study shows that the proposed approach improves the classification accuracy rate in most test problems. Furthermore, when the feature selection is taken into consideration, the classification accuracy rates of most datasets are increased. The proposed algorithm should thus be useful to both practitioners and researchers.
 Keywords Back-propagation network  X  Particle swarm optimization  X  Feature selection  X  Parameter determination 1 Introduction Neural network is a well-known machine learning algorithm among data mining techniques, and can be used in a variety of applications, such as modeling of electrostatic fluidized bed automotive [ 16 ], condition monitoring of a pneumatic process valve actuator [ 24 ], data-base integration [ 34 ], defective classification [ 39 ], consumer loan evaluation [ 43 ], location determination of mobile devices [ 61 ], and diagnosis of heart disease [ 67 ].

Back-propagation network (BPN) is one of the most popular neural network, which is a viable, reliable, and attractive approach for data processing because (1) BPNs are capable of modeling non-linear processes; (2) the data-driven features of BPNs make them powerful in parallel computing and capable of handling large amounts of data, and (3) BPNs have good fault tolerance and adaptability [ 4 ].

Before applying a BPN to problem solving, the parameter settings of the BPN, including thehiddenlayer,learningrate,momentumterm,numberofhiddenneurons,andlearningcycle must be determined. The parameter settings for network architectures must be determined carefully in order to avoid constructing a suboptimal network model that may significantly increase computational costs and produce inferior results.

In most pattern classification problems, given a large set of potential features, it is usually necessary to find a small subset with which to classify. Data lacking any feature selection may be redundant or noisy and may reduce the efficiency of classification. The main benefits of feature selection are as follows: (1) reducing computational cost and storage requirements, (2) dealing with the degradation of classification efficiency due to the finite size of training sample sets, (3) reducing training and prediction time, and (4) facilitating data understanding and visualization [ 1 ].

In feature selection, because each feature is necessary to determine whether it is useful or not, the task of finding the optimal subset of features is inherently combinatory. Therefore, feature selection is an optimization problem. An optimal approach is needed to evaluate all possiblesubsets.Thisresearchproposesaparticleswarmoptimization(PSO)basedapproach, denoted as PSOBPN, to obtain the appropriate parameter settings for a BPN, and to select the beneficial subset of features which result in better classification accuracy rate.
The remainder of this paper is organized as follows. Section 2 reviews the studies on the back-propagation network, feature selection, and particle swarm optimization. Section 3 elaborates on the proposed PSOBPN approach to determine the appropriate parameter set-tings for the BPN, and determines the beneficial subset of features. Section 4 describes the experiment results. Conclusions and future research are offered in the last section. 2 Literature review 2.1 BPN The BPN is a common neural network model whose architecture consists of multilayer perceptrons (MLP). The BPN uses the idea of  X  X he gradient steepest descent method X  to minimize the errors between actual and predictive output functions. An increased number of hidden layers and the transformation function of the smoothing differential can allow the network to apply the gradient steepest descent method to correct the network weights formula. Therefore, if there are enough hidden layers or hidden neurons, the linear threshold curve can approach any function [ 17 ]. The learning procedures of a BPN include initialization, forward, and reverse processes. The net input and output of each neuron in the hidden and output layers are computed. Ini-tially, the training data are fed to the input layer of the network. To compute the net input of the neuron, each input connected to the neuron is multiplied by its corresponding weight, and then summed. The error is propagated backwards by updating the weights and biases to reflect the error of the network X  X  prediction. The weights and biases are updated to reflect the propagated errors.

Since the parameter settings for a BPN are often designed quite differently due to the unique characteristics of the data, trial-and-error seems to be the most common way to iden-tify the optimum value of learning rate, momentum, hidden neurons, and learning cycle. The following are problems generally faced when using a BPN: (1) Learning rate and momentum term: Too high a learning rate will make it hard for it to (2) Number of hidden neurons: When there are too few hidden neurons, a larger error is (3) Learning cycle: In the study of Schittenkopf et al. [ 50 ] back-propagation on the illus-
Therefore, rule of thumb or  X  X rial and error X  methods are used to determine the parameter settings for network architectures. However, it is difficult to obtain the optimal parameter settings for network architectures. If parameter values are not set appropriately, that may lead to over-fitting or under-fitting problem [ 13 ].

Lin and Ting [ 35 ] and Wang et al. [ 65 ] have tested a few different network architectures to find the best network architecture. However, there is still a need for a systematic method of determining the appropriate network architecture for building the model. Common issues with the BPN-based modeling approach include the under-training problem, convergence problem, over-fitting problem, and topology optimization problem [ 9 ]. While the first two can be mitigated by careful selection of the stopping criteria, the latter two are of interest here through an optimization approach, which optimizes the network topology as well as reduces the risk of over-fitting.

Researchers have used three types of approaches to handle the problems discussed above. (1) Search for the appropriate parameter settings of the BPN: (2) Search for the optimal weights after training: (3) Neural network pruning:
The genetic algorithm has been extensively used in artificial neural network optimiza-tion and is known to achieve optimal solutions fair successfully. Li et al. [ 33 ] developed a GA-based combined BPN for estimation. The study shows that the BPN model combined with GA is more effective in finding the parameters of BPN than trial-and-error method. There are several genetic algorithm (GA) approaches used to enhance the BPN performance. They they only consider their problems; furthermore comparisons are hard to make. 2.2 Feature selection The BPN requires a dataset to construct a model. A dataset may consist of many features, but not all features are helpful for classification. If the dataset has tremendous noise and complex dimensionality, the BPN may face limitations in learning the classification patterns. Feature selection may be considered part of the data cleaning and/or pre-processing step where the actual extraction and learning of knowledge or patterns is done after a suitable set of features is extracted. It is a process that aims to refine the list of features used, thereby removing potential sources of noise and ambiguity.

Approaches for feature selection may be categorized into two models, filter models and wrapper models [ 40 ]. In a filter model, statistical approaches, such as factor analysis (FA), independent component analysis (ICA), principal component analysis (PCA), and discrimi-nant analysis (DA) have been devoted to the investigation of indirect performance measures, mostly based on distance and information measures, in feature selecting. Even though this model is faster, the resulting feature subset may not be optimal [ 40 ].

The wrapper model uses a variety of selection methods to choose feature subsets and then evaluates the result after the classification algorithm calculates the classification accuracy rate. If the relevant features can be selected or the noise removed, the classification accuracy rate classifier can be improved.

The wrapper model is widely used in BPN feature selection. Yang and Honavar [ 68 ] presented a DISTAL approach using feature selection to improve classification accuracy rate. Kim and Han [ 27 ] proposed a GA approach to perform feature selection in neural net-works for the prediction of a stock price index. Lezoray and Cardot [ 32 ] employed floating search methods [ 29 , 45 ] to do the feature selection finding that the classification accuracy rate apparently improves after the feature selection. Verikas and Bacauskiene [ 62 ]presenteda neural network-based feature selection technique. A network was trained with an augmented cross-entropy error function. Zhang et al. [ 72 ] applied a GA approach to feature selection in neural networks for fault defection in manufacturing industry. Sexton et al. [ 55 ]first calculated the network weight of architecture, then pruned the neural network architecture branches under a fixed number of hidden neurons, and finally used GA to conduct the feature selection. Sivagaminathan and Ramakrishnan [ 58 ] used ant colony optimization to optimize the feature subset that is suitable for feeding the neural network. Wang et al. [ 63 ] proposed a hybrid intelligent system called R-FC-DENN. Their experiments are carried out based on the UCI dataset. However, the above researches did not consider the parameter settings for network architectures of BPN at the same time.

Several researchers have proposed methods to obtain the optimal parameter settings for research did not consider the feature selection to find the optimal feature subset in order to increase the performance simultaneously. Lin et al. [ 36 ] developed a simulated-annealing-based back-propagation network (SABPN) to determine parameter settings and feature selec-tion simultaneously, but since the number of learning cycles is set to 500, the learning ability of the system may be limited. 2.3 Particle swarm optimization Particle swarm optimization [ 25 ] is an emerging population-based meta-heuristic that sim-ulates social behavior such as birds flocking to a promising position to achieve precise objectives in a multidimensional space. It has been applied successfully to a wide variety of highly complicated optimization problems [ 37 ] as well as various real-world problems [ 18 , 30 , 41 , 46 , 47 ]. Like evolutionary algorithms, PSO performs searches using a population (called a swarm) of individuals (called particles) that are updated from iteration to iteration. The size of population is denoted as p size . To discover the optimal solution, each particle changes its search direction according to two factors, its own best previous experience (pbest) and the best experience of all other members (gbest). Shi and Eberhart [ 56 ] termed pbest the cognitive part, and gbest the social part.

Each particle represents a candidate position (i.e., solution). A particle is considered as a point in a D -dimensional space, and its status is characterized according to its position and velocity. The D -dimensional position for the particle i at iteration t can be represented as x t iteration t ,whichisalsoa D -dimensionalvector,canbedescribedas v t In the simple version of PSO, there was no actual control over the previous velocity of the particles. In the later versions of PSO, this shortcoming was addressed by incorporat-ing a new parameter, called inertia weight introduced by Shi and Eberhart [ 57 ]. Let p t i = { p at iteration t . To search for the optimal solution, each particle changes its velocity based on the cognitive and social parts as using Eq. ( 1 ).
 weight ( w ) is used to slowly reduce the velocity of the particles to keep the swarm under control, and r 1 and r 2 are random numbers uniformly distributed in U (0,1). It is possible to clamp the velocity vectors by specifying upper and lower bounds on v max to avoid too rapid movement of particles in the search space. That is, the velocities of all the particles are limited within the range of [  X  v max ,v max ][ 51 ].
 Each particle then moves to a new potential solution based on the following equation: The basic process of the PSO algorithm is given as follows: Step 1: (Initialization) Randomly generate initial particles.
 Step 2: (Fitness) Measure the fitness of each particle in the population.
 Step 3: (Update) Compute the velocity of each particle with Eq. ( 1 ).
 Step 4: (Construction) For each particle, move to the next position according to Eq. ( 2 ). Step 5: (Termination) Stop the algorithm if termination criterion is satisfied; return to Step The process of PSO is finished if the termination condition is satisfied. 3 The proposed PSOBPN approach The BPN uses the idea of  X  X radient steepest decent method X  to minimize the errors between actual and predictive output functions. However, worse parameter values used may obtain the local optimal. Therefore, this study developed a particle swarm optimization (PSO) approach, termed PSOBPN, for parameter determination and feature selection in the BPN. The PSO approach is a multi-point search algorithm, which may be provide more probability to deter-mine appropriate parameter values to escape the local optimal. In some complex problems the BPN may need more learning cycles to learn the pattern. Therefore, compared with the SA approach proposed by Lin et al. [ 36 ], a variable is added for determining the number of learning cycles of BPN.

In order to find the best parameter settings for the BPN, the classification accuracy rate of testing data is adopted as the fitness value of PSO-based approach. Therefore, for the condition without feature selection, four decision variables, designated learning iteration, learning rate, momentum term, and number of hidden neurons are required. Therefore, for the feature selection, if n features are required to decide which features are chosen, and then 4 + n decision variables must be adopted. The value of these n variables ranges between 0 and 1. If the value of a variable is less than or equal to 0.5, then its corresponding feature is not chosen. Conversely, if the value of a variable is greater than 0.5, then its corresponding feature is chosen. For example, if the data set has six attributes and the BPN requires four parameters, there are ten variables used as shown in Fig. 1 . This solution can be decoded as follows. The learning cycle is 2,400, the learning rate is 0.375, the momentum is 0.579, the number of hidden neurons is 5, and the selected features are 1, 4, and 5. The parameter settings and selected features are then adopted to build a BPN classifier.

In order to discover the optimal solution of the PSOBPN, each particle represents a candi-date solution and changes its search direction according to two factors, its own best previous experience (pbest) and the best experience of all other members (gbest). Figure 2 shows the flowchart for the PSOBPN. First, the population of particles is initialized, each particle having a random position within the D -dimensional space and a random velocity for each dimension. Second, each particle X  X  fitness for the BPN is evaluated. Each particle X  X  fitness in this study is the classification accuracy rate. If the fitness is better than the particle X  X  best fitness, then the position vector is saved for the particle. If the particle X  X  fitness is better than the global best fitness, then the position vector is saved for the global best. Finally the particle X  X  velocity and position are updated until the termination condition is satisfied. The termination in this study is the pre-determined maximum number of solutions evaluated.
In this study, the classification accuracy rates for the datasets were measured by comparing the predict class and actual class. For example, in the classification problem with two-class positive and negative, a single prediction has the four different possible outcomes as shown in Table 1 .

The true positive (TP) and true negative (TN) are correct classifications. A false positive (FP) occurs when the outcome is incorrectly predicted as positive when it is actually negative. A false negative (FN) occurs when the outcome is incorrectly predicted as negative when it is actually positive. The overall classification accuracy rate is the number of correct classifi-cations divided by the total number of classifications, which is computed as TP+TN TP+TN+FP+FN .
In a multi-class prediction, the classification result is often displayed as a two-dimen-sional confusion matrix with a row and column for each class. Each matrix element shows the number of test cases for which the actual class is the row and the predicted class is the column.

To evaluate the classification accuracy rate, the k -fold approach [ 49 ] is used. This study set k as 10; that is, the data are divided into ten slices, and each slice of the data shares the same proportion of each class of data. Nine data slices were used as training data, while the tenth is used as the testing data. Since the number of data in each class was not a multiple of ten, the dataset could not be partitioned equitably. However, the ratio of the number of data in the training set to the number of data in the testing set was maintained as closely as possible to 9:1.The proposed PSOBPN approach was run tentimes to allow each slice of data to take a turn as the testing data. The classification accuracy rate in of this experiment was calculated by summing the individual accuracy rate for each run of testing, and then dividing the total by ten. 4 Experimental results The proposed approach was implemented using the C language and the Windows XP oper-ating system and run on a personal computer with Pentium IV-3.0GHz CPU and 512MB of RAM. In order to verify the proposed PSOBPN approach, 23 datasets in the UCI Machine Learning Repository [ 20 ] were used for evaluation. The number of features, instances, and classes for each UCI dataset used in this research are shown in Table 2 . Normalization is particularly useful for classification algorithms involving a neural network [ 17 ]. Thus, a fea-ture is normalized by scaling its value so that it falls within scaled to [  X  1, 1]. If instance has missing values in some of its features, the instance is then removed. The predicted data of the Boston housing dataset was transformed from continuous into a binary class [ 12 ].

Parameter selection may influence the quality of the computational results. To avoid too rapid movement of particles in the search space, the lower and upper bounds on v max is set to  X  1 and 1, respectively. In order to obtain better parameter values used in PSOBPN, the initial experiment is done as follows. Four datasets, Bupa, German, Pima, and Sonar, are used to test various combinations of parameters. At the beginning, the maximum number of solution evaluated is set to 50,000 (a large value), while w, c 1 , c 2 ,and p size issetto1.0,1.0, and 20, respectively. That is, the number of iterations is 2,500 (50,000/20 = 2,500). After several runs of execution, we found that the classification accuracy rate is stable when the maximum number of solution evaluated equal to 300 and 500 for the proposed approach without and with feature selection, respectively. After determining the maximum number of solution evaluated, the following combinations of parameters were tested:
Setting c 1 = 0 . 8 , c 2 = 1 . 5 ,w = 0 . 9, and p size = 10 seemed to give better results; there-fore they were used for further computational study. Because the proposed PSOBPN is a non-deterministic approach, the solution obtained may not be equal for the same data. Thus, the proposed PSOBPN approach is executed tentimes for each fold in the dataset. When not considering feature selection, due to the maximum number of solutions evaluated was 300 and the number of particles set to be 10, the number of iterations equals 30 (300/10). With feature selection, the number of features selected for use can be obtained by the PSOBPN approach. Since the PSOBPN approach has a larger solution space, in terms of number of features, the number of solutions evaluated is also larger. Because the maximum number of solutions evaluated was raised to 500, the number of iterations was 50 (500/10). The search ranges for parameter values of BPN are set as follows. The learning cycle ranged from 500 to 5000, the learning rate ranged from 0 to 0.45, while the momentum term ranged from 0.4 to 0.9. The BPN used one hidden layer and the sigmoid transfer function. The number of hidden neurons ranged from one to the number of features in two-class datasets, while the number of hidden neurons ranged from the number of classes to two multiplied by the sums of the number of input features and the number of output neurons.

The classification accuracy rates obtained by the proposed PSOBPN approach for each data set (both the training data and testing data) are shown in Table 3 . It is noted that because the difference in classification accuracy rates between the training data and testing data is not excessive, the proposed approach can avoid the over-fitting problem and achieve better classification accuracy rate.

In this study, results obtained by PSOBPN without feature selection are then compared with those of SABPN [ 36 ], SASVM [ 37 ], PSOSVM [ 38 ], MONNA [ 32 ], GA DISTAL [ 68 ], G-Prop [ 6 ], and NNPGA [ 19 ] as shown in Table 4 . Only the classification accuracy rates of Breast Cancer (old), (98.3% in PSOBPN and 99.0% in G-Prop), German (80.58% in PSOBPN and 80.92% in SABPN), Housing (99.71% in PSOBPN and 99.90% in SASVM), Iris (95.16% in PSOBPN and 97.33% in NNPGA), Segmentation (98.32% in PSOBPN and 99.57% in NNPGA) Sonar (89.93% in PSOBPN and 91.88% in SABPN), and Vowel (98.18% in PSOBPN and 99.90% in NNPGA), obtained by PSOBPN are inferior to the classification accuracy rates of other approaches. The remaining classification accuracy rates obtained by the proposed PSOBPN approach are higher than those of other approaches, and the compu-tation time is within an acceptable range.

Table 5 presents the results obtained by PSOBPN with feature selection and a number of published results using other machine learning algorithms. Comparing the classification accuracy rates of the proposed PSOBPN approach with those of SABPN [ 36 ], SASVM [ 37 ], PSOSVM [ 38 ], MONNA [ 32 ], GA+DISTAL [ 68 ], GAP (genetic algorithm and program-ming) [ 59 ], C4.5 (J48, the WEKA implementation of C4.5) [ 59 ], HIDER [ 10 ], XCS (Wilson X  X  XCS classifier) [ 10 ], OFA (ordered fuzzy ARTMAP) [ 2 ], LVSM (Lagrangian support vector machines) [ 44 ], and R-FC-DENN [ 63 ] only three two-class problems and five multi-class problems whose classification accuracy rates obtained by PSOBPN are lower than those of other approaches. Comparing the PSOBPN without feature selection with feature selection, better results are obtained by implementing the feature selection at the expense of some computation cost (Table 5 ). Although PSOBPN did not achieve the best classification results across these eight problems, the difference is not great. In general, the PSOBPN approach with feature selection performs well in both two-class and multi-class problems. Further, with feature selection, the proposed PSOBPN approach can also effectively delete certain moder-ating or non-affecting features while maintaining the same or better classification accuracy rate. The importance of the relationship of the remaining features for classification may be examined in the future.

Compared with the SABPN [ 36 ], one variable is added for determining the number of learning cycle of BPN in PSOBPN. Experimental results showed that the proposed PSOBPN can achieve better classification accuracy rate than those of SABPN in most of datasets. That is, the BPN may need more learning cycle to learn the pattern in some complex problems. The experimental results showed that the classification accuracy rates obtained by PSOBPN are better than those of SABPN in general. The result is accordance with the observation provided by Schittenkopf et al. [ 50 ].
 Finally, in order to verify whether a significant difference between the proposed PSO-BPN approach with feature selection and without feature selection exists, the results of the proposed PSOBPN approach with and without feature selection are compared, as shown in Table 6 . It can be noted in this Table, although the classification accuracy rates of three datasets (Australian, Housing, and Waveform with noise) are reduced and four dataset (Bal-ance scale, Car Evaluation, Wine, and Yeast) are same, those of the remaining datasets are increased. For the dataset whose classification accuracy rate increased, only the Breast (old) and Vehicle dataset does not have significant difference; all other datasets have p -value lower than 0.05, which means the significant difference exist. Therefore, the proposed PSOBPN approach with feature selection is better than the proposed PSOBPN approach without feature selection. 5 Conclusions and future research This study applied the particle swarm optimization-based approach to search for appropri-ate parameter values and beneficial features for BPN. The main contributions of this study include (1) The trial-and-error method traditionally used for BPN in determining the parameter (2) When the feature selection is taken into account it can improve the performance, reduce Compared with the previous studies, the classification accuracy rates of the proposed PSOBPN approach are better than those of other approaches. Furthermore, with the feature selection the proposed PSOPBN approach can also effectively delete some moderating or non-affecting features while maintaining the same or better classification accuracy rate. The results are by no means an exhaustive list of current machine learning algorithms, nor are they guaranteed to be the best performing algorithms available, but they give some results of the relative performance of our approach, which appears to be very good.

More studies can be done in the future. First, since the proposed approach is only applied and compared with others using classification problems, the efficiency of the proposed approach in the forecast of continuous values will be examined in the future. Second, the proposed PSO-based meta-heuristic is sensitive to parameter settings, and sometimes it may result in local optimal. Therefore, to perform a comprehensive study on alternative parame-ter tuning policies and to customize the algorithm by developing new parameter and new mechanism is room for further investigation. For example, other parameters, such as time interval, time-varying inertia weight, may be added to the PSO algorithm, while particle grouping may be used to split the population of particles into subgroups. These techniques may help PSO to avoid falling into local optimal and improve the performance. Third, since PSO is a meta-heuristics, it can be applied to other architectures of network, such as cascade neural network and RBF network (radial basis function network). Moreover, PSOBPN may be applied to a greater range of problems in the real world.
 References Author biographies
