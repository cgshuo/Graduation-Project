 Imagine measuring the distance of a rocket as it leaves Earth, and wanting to know how these mea-surements correlate with one another. How much does the value of the measurement at fifteen minutes depend on the measurement at five minutes? Once we X  X e learned this correlation structure, suppose we want to compare it to the dependence between measurements of the rocket X  X  velocity. To do this, it is convenient to separate dependence from the marginal distributions of our measure-ments. At any given time, a rocket X  X  distance from Earth could have a Gamma distribution, while its velocity could have a Gaussian distribution. And separating dependence from marginal distributions is precisely what a copula function does.
 While copulas have recently become popular, especially in financial applications [1, 2], as Nelsen processes is a subject still in its infancy. There are many open problems. . .  X  Typically only bivariate (and recently trivariate) copulas are being used and studied. In our introductory example, we are would therefore be useful to have a copula process , which can describe the dependencies between arbitrarily many random variables independently of their marginal distributions. We define such a process. And as an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV). In doing so, we provide a Bayesian framework for the learning the marginal distributions and dependency structure of what we call a Gaussian copula process .
 The volatility of a random variable is its standard deviation. Stochastic volatility models are used to predict the volatilities of a heteroscedastic sequence  X  a sequence of random variables with dif-ferent variances, like distance measurements of a rocket as it leaves the Earth. As the rocket gets further away, the variance on the measurements increases. Heteroscedasticity is especially impor-tant in econometrics; the returns on equity indices, like the S&amp;P 500, or on currency exchanges, are heteroscedastic. Indeed, in 2003, Robert Engle won the Nobel Prize in economics  X  X or methods of analyzing economic time series with time-varying volatility X . GARCH [4], a generalized version of Engle X  X  ARCH, is arguably unsurpassed for predicting the volatility of returns on equity indices and currency exchanges [5, 6, 7]. GCPV can outperform GARCH, and is competitive on financial data that especially suits GARCH [8, 9, 10]. Before discussing GCPV, we first introduce copulas and the copula process. For a review of Gaussian processes, see Rasmussen and Williams [11]. Copulas are important because they separate the dependency structure between random variables from their marginal distributions. Intuitively, we can describe the dependency structure of any multivariate joint distribution H ( x 1 ,...,x n ) = P ( X 1  X  x 1 ,...X n  X  x n ) through a two step process. First we take each univariate random variable X i and transform it through its cumulative distribution function (cdf) F i to get U i = F i ( X i ) , a uniform random variable. We then express the dependencies between these transformed variables through the n -copula C ( u 1 ,...,u n ) . Formally, an n -copula C : [0 , 1] n  X  [0 , 1] is a multivariate cdf with uniform univariate marginals: C ( u 1 ,u 2 ,...,u n ) = P ( U 1  X  u 1 ,U 2  X  u 2 ,...,U n  X  u n ) , where U 1 ,U 2 ,...,U n are standard uniform random variables. Sklar [12] precisely expressed our intuition in the theorem below. Theorem 1.1. Sklar X  X  theorem Let H be an n-dimensional distribution function with marginal distribution functions F 1 ,F 2 ,...,F n . Then there exists an n -copula C such that for all ( x 1 ,x 2 ,...,x n )  X  [  X  X  X  ,  X  ] n , If F 1 ,F 2 ,...,F n are all continuous then C is unique; otherwise C is uniquely determined on Range F 1  X  Range F 2  X  X  X  X  X  Range F n . Conversely, if C is an n -copula and F 1 ,F 2 ,...,F n are distribution functions, then the function H is an n-dimensional distribution function with marginal distribution functions F 1 ,F 2 ,...,F n .
 u ,u 2 ,...,u n  X  [0 , 1] n , In other words, (2) can be used to construct a copula. For example, the bivariate Gaussian copula is defined as where  X   X  is a bivariate Gaussian cdf with correlation coefficient  X  , and  X  is the standard univariate Gaussian cdf. Li [2] popularised the bivariate Gaussian copula, by showing how it could be used to study financial risk and default correlation, using credit derivatives as an example.
 By substituting F ( x ) for u and G ( y ) for v in equation (3), we have a bivariate distribution H ( x,y ) , with a Gaussian dependency structure, and marginals F and G . Regardless of F and G , the resulting H ( x,y ) can still be uniquely expressed as a Gaussian copula, so long as F and G are continuous. It is then a copula itself that captures the underlying dependencies between random variables, regardless of their marginal distributions. For this reason, copulas have been called dependence functions [13, 14]. Nelsen [3] contains an extensive discussion of copulas. Imagine choosing a covariance function, and then drawing a sample function at some finite number of points from a Gaussian process. The result is a sample from a collection of Gaussian random variables, with a dependency structure encoded by the specified covariance function. Now, suppose we transform each of these values through a univariate Gaussian cdf, such that we have a sample from a collection of uniform random variables. These uniform random variables also have this underlying Gaussian process dependency structure. One might call the resulting values a draw from a Gaussian-Uniform Process . We could subsequently put these values through an inverse beta cdf, to obtain a draw from what could be called a Gaussian-Beta Process : the values would be a sample from beta random variables, again with an underlying Gaussian process dependency structure. We could also transform the uniform values with different inverse cdfs, which would give a sample from different random variables, with dependencies encoded by the Gaussian process.
 The above procedure is a means to generate samples from arbitrarily many random variables, with arbitrary marginal distributions, and desired dependencies. It is an example of how to use what we call a copula process  X  in this case, a Gaussian copula process , since a Gaussian copula describes the dependency structure of a finite number of samples. We now formally define a copula process. Definition 2.1. Copula Process Let { W t } be a collection of random variables indexed by t  X  X  , with marginal distribution functions F , and let Q t = F t ( W t ) . Further, let  X  be a stochastic process measure with marginal distribution functions G t , and joint distribution function H . Then W t is copula process distributed with base measure  X  , or W t  X  X P (  X  ) , if and only if for all n  X  N , a i  X  R , Each Q t i  X  Uniform(0 , 1) , and G (  X  1) t Definition 2.2. Gaussian Copula Process W t is Gaussian copula process distributed if it is copula process distributed and the base measure  X  is a Gaussian process. If there is a mapping  X  such that  X ( W t )  X  GP ( m ( t ) ,k ( t,t 0 )) , then we write W t  X  X CP ( X  ,m ( t ) ,k ( t,t 0 )) .
 For example, if we have W t  X  X CP with m ( t ) = 0 and k ( t,t ) = 1 , then in the definition of a copula process, G t =  X  , the standard univariate Gaussian cdf, and H is the usual GP joint distribution function. Supposing this GCP is a Gaussian-Beta process , then  X  =  X   X  1  X  F B , where F B is a univariate Beta cdf. One could similarly define other copula processes.
 We described generally how a copula process can be used to generate samples of arbitrarily many random variables with desired marginals and dependencies. We now develop a specific and practical application of this framework. We introduce a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), as an example of how to learn the joint distribution of arbitrarily many random variables, the marginals of these random variables, and to make predictions. To do this, we fit a Gaussian copula process by using a type of Warped Gaussian Process [15]. However, our method-ology varies substantially from Snelson et al. [15], since we are doing inference on latent variables as opposed to observations, which is a much greater undertaking that involves approximations, and we are doing so in a different context. Assume we have a sequence of observations y = ( y 1 ,...,y n ) &gt; at times t = ( t 1 ,...,t n ) &gt; . The observations are random variables with different latent standard deviations. We therefore have n unobserved standard deviations,  X  1 ,..., X  n , and want to learn the correlation structure between these standard deviations, and also to predict the distribution of  X   X  at some unrealised time t  X  . We model the standard deviation function as a Gaussian copula process: Specifically, where g is a monotonic warping function, parametrized by  X  . For each of the observations y = ( y g ( f i ,  X  ) , using the shorthand f i to mean f ( t i ) .  X   X  X CP , because any finite sequence (  X  1 ,..., X  p ) is distributed as a Gaussian copula: where  X  is the standard univariate Gaussian cdf (supposing k ( t,t ) = 1 ),  X   X  is a multivariate Gaus-each  X  i . In (5), we have  X  = g  X  1 , because it is g  X  1 which maps  X  t to a GP. The specification in (5) is equivalently expressed by (6) and (7). With GCPV, the form of g is learned so that g  X  1 (  X  t ) is best modelled by a GP. By learning g , we learn the marginal of each  X  : F ( a ) =  X ( g  X  1 ( a )) for a  X  R . Recently, a different sort of  X  X ernel copula process X  has been used, where the marginals of the variables being modelled are not learned [16]. 1 Further, we also consider a more subtle and flexible form of our model, where the function g itself is indexed by time: g = g t ( f ( t ) ,  X  ) . We only assume that the marginal distributions of  X  t are stationary over  X  X mall X  time periods, and for each of these time periods (5)-(7) hold true. We return to this in the final discussion section. Here we have assumed that each observation, conditioned on knowing its variance, is normally distributed with zero mean. This is a common assumption in heteroscedastic models. The zero mean and normality assumptions can be relaxed and are not central to this paper. GP covariance function. To do this, we sample from delta, K ij = k ( t i ,t j ) , ( k  X  ) i = k ( t  X  ,t i ) , we have We also wish to learn z , which we can do by finding the  X  z that maximizes the marginal likelihood, Unfortunately, for many functions g , (10) and (13) are intractable. Our methods of dealing with this can be used in very general circumstances, where one has a Gaussian process prior, but an (optionally parametrized) non-Gaussian likelihood. We use the Laplace approximation to estimate which we sample from to make predictions of  X   X  . Using Laplace, we can also find an expression for an approximate marginal likelihood, which we maximize to determine z . Once we have found z with Laplace, we use Markov chain Monte Carlo to sample from p ( f  X  | y , z ) , and compare that to using Laplace to sample from p ( f  X  | y , z ) . In the supplement we relate this discussion to (9). 4.1 Laplace Approximation The goal is to approximate (11) with a Gaussian, so that we can evaluate (10) and (13) and make predictions. In doing so, we follow Rasmussen and Williams [11] in their treatment of Gaussian process classification, except we use a parametrized likelihood, and modify Newton X  X  method. First, consider as an objective function the logarithm of an unnormalized (11): The Laplace approximation uses a second order Taylor expansion about the  X  f which maximizes Newton X  X  method. The Newton update is f new = f  X  (  X  X  X  s ( f ))  X  1  X  s ( f ) . Differentiating (14), where W is the diagonal matrix  X  X  X  X  log p ( y | f ,  X  ) . If the likelihood function p ( y | f ,  X  ) is not log concave, then W may have negative entries. Vanhat-alo et al. [18] found this to be problematic when doing Gaussian process regression with a Student-t likelihood. They instead use an expectation-maximization (EM) algorithm for finding  X  f , and iterate ordered rank one Cholesky updates to evaluate the Laplace approximate marginal likelihood. But EM can converge slowly, especially near a local optimum, and each of the rank one updates is vul-nerable to numerical instability. With a small modification of Newton X  X  method, we often get close to quadratic convergence for finding  X  f , and can evaluate the Laplace approximate marginal likelihood in a numerically stable fashion, with no approximate Cholesky factors, and optimal computational requirements. Some comments are in the supplementary material but, in short, we use an approxi-mate negative Hessian,  X  X  X  X  s  X  M + K  X  1 , which is guaranteed to be positive definite, since M is formed on each iteration by zeroing the negative entries of W . For stability, we reformulate our optimization in terms of B = I + M 1 2 KM 1 2 , and let Q = M 1 2 B  X  1 M 1 2 , b = M f +  X  log p ( y | f ) , a = b  X  QK b . Since ( K  X  1 + M )  X  1 = K  X  KQK , the Newton update becomes f new = K a . With these updates we find  X  f and get an expression for  X  s which we use to approximate (13) and (11). The approximate marginal likelihood q ( y | z ) is given by R exp(  X  s ) d f . Taking its logarithm, where B  X  f is B evaluated at  X  f , and a  X  f is a numerically stable evaluation of K  X  1  X  f . To learn the parameters z , we use conjugate gradient descent to maximize (17) with respect to z . Since  X  f is a function of z , we initialize z , and update  X  f every time we vary z . Once we have found an optimum  X  z , we can make predictions. By exponentiating  X  s , we find a Gaussian approximation to the posterior (11), q ( f | y , z ) = N (  X  f ,K  X  KQK ). The product of this approximate posterior with p ( f  X  | f ) is Gaussian. Integrating this product, we approximate p ( f  X  | y , z ) as Given n training observations, the cost of each Newton iteration is dominated by computing the cholesky decomposition of B , which takes O ( n 3 ) operations. The objective function typically changes by less than 10  X  6 after 3 iterations. Once Newton X  X  method has converged, it takes only O (1) operations to draw from q ( f  X  | y , z ) and make predictions. 4.2 Markov chain Monte Carlo We use Markov chain Monte Carlo (MCMC) to sample from (11), so that we can later sample from p (  X   X  | y , z ) to make predictions. Sampling from (11) is difficult, because the variables f are strongly coupled by a Gaussian process prior. We use a new technique, Elliptical Slice Sampling [19], and find it extremely effective for this purpose. It was specifically designed to sample from posteriors with correlated Gaussian priors. It has no free parameters, and jointly updates every element of f . For our setting, it is over 100 times as fast as axis aligned slice sampling with univariate updates. To make predictions, we take J samples of p ( f | y , z ) , { f 1 ,..., f J } , and then approximate (10) as a mixture of J Gaussians: Each of the Gaussians in this mixture have equal weight. So for each sample of f  X  | y , we uniformly choose a random p ( f  X  | f i ,  X  ) and draw a sample. In the limit J  X   X  , we are sampling from the and one O ( J ) operation, a draw from (19) takes O (1) operations. 4.3 Warping Function The warping function, g , maps f i , a GP function value, to  X  i , a standard deviation. Since f i can take any value in R , and  X  i can take any non-negative real value, g : R  X  R + . For each f i to correspond to a unique deviation, g must also be one-to-one. We use This is monotonic, positive, infinitely differentiable, asymptotic towards zero as x  X   X  X  X  , and rare situations where the parameters  X  are trained to make g extremely small for certain inputs, at the expense of a good overall fit; this can happen when the parameters  X  are learned by optimizing observation.
 By inferring the parameters of the warping function, or distributions of these parameters, we are learning a transformation which will best model  X  t with a Gaussian process. The more flexible the warping function, the more potential there is to improve the GCPV fit  X  in other words, the better we can estimate the  X  X erfect X  transformation. To test the importance of this flexibility, we also try a simple unparametrized warping function, g ( x ) = e x . In related work, Goldberg et al. [20] place a GP prior on the log noise level in a standard GP regression model on observations, except for inference they use Gibbs sampling, and a high level of  X  X itter X  for conditioning.
 Once g is trained, we can infer the marginal distribution of each  X  : F ( a ) =  X ( g  X  1 ( a )) , for a  X  R . This suggests an alternate way to initialize g : we can initialize F as a mixture of Gaussians, and then map through  X   X  1 to find g  X  1 . Since mixtures of Gaussians are dense in the set of probability distributions, we could in principle find the  X  X erfect X  g using an infinite mixture of Gaussians [21]. In our experiments, we predict the latent standard deviations  X  of observations y at times t , and also  X   X  at unobserved times t  X  . To do this, we use two versions of GCPV. The first variant, which we simply refer to as GCPV, uses the warping function (20) with K = 1 , and squared exponential GP-EXP, uses the unparametrized warping function e x , and the same covariance function, except the amplitude A is a trained hyperparameter. The other hyperparameter l is called the lengthscale of the covariance function. The greater l , the greater the covariance between  X  t and  X  t + a for a  X  R . We train hyperparameters by maximizing the Laplace approximate log marginal likelihood (17). We then sample from p ( f  X  | y ) using the Laplace approximation (18). We also do this using MCMC (19) with J = 10000 , after discarding a previous 10000 samples of p ( f | y ) as burn-in. We pass mean and variance of  X   X  | y . We use the sample mean as a point predictor, and the sample variance for error bounds on these predictions, and we use 10000 samples to compute these quantities. For GCPV we use Laplace and MCMC for inference, but for GP-EXP we only use Laplace. We compare predictions to GARCH(1,1), which has been shown in extensive and recent reviews to be competitive with other GARCH variants, and more sophisticated models [5, 6, 7]. GARCH( p , q ) specifies y ( t )  X  P parameters a 0 ,a i and b j are estimated using a constrained maximum likelihood.
 We make forecasts of volatility, and we predict historical volatility. By  X  X istorical volatility X  we mean the volatility at observed time points, or between these points. Uncovering historical volatility is important. It could, for instance, be used to study what causes fluctuations in the stock market, or to understand physical systems.
 To evaluate our model, we use the Mean Squared Error (MSE) between the true variance, or proxy for the truth, and the predicted variance. Although likelihood has advantages, we are limited in space, and we wish to harmonize with the econometrics literature, and other assessments of volatility models, where MSE is the standard. In a similar assessment of volatility models, Brownlees et al. [7] found that MSE and quasi-likelihood rankings were comparable.
 When the true variance is unknown we follow Brownlees et al. [7] and use squared observations as a proxy for the truth, to compare our model to GARCH. 2 The more observations, the more reliable these performance estimates will be. However, not many observations (e.g. 100) are needed for a stable ranking of competing models; in Brownlees et al. [7], the rankings derived from high frequency squared observations are similar to those derived using daily squared observations. 5.1 Simulations To forecast, we use all observations up until the current time point, and make 1, 7, and 30 step ahead predictions. So, for example, in TRIG we start by observing t = 0 , and make forecasts at so on, until all data points have been observed. For historical volatility, we predict the latent  X  t at the observation times, which is safe since we are comparing to the true volatility, which is not used in training; the results are similar if we interpolate. Figure 1 panels a) and b) show the true volatil-ity for TRIG and JUMP respectively, alongside GCPV Laplace, GCPV MCMC, GP-EXP Laplace, and GARCH(1,1) predictions of historical volatility. Table 1 shows the results for forecasting and historical volatility.
 In panel a) we see that GCPV more accurately captures the dependencies between  X  at different times points than GARCH: if we manually decrease the lengthscale in the GCPV covariance func-tion, we can replicate the erratic GARCH behaviour, which inaccurately suggests that the covariance between  X  t and  X  t + a decreases quickly with increases in a . We also see that GCPV with an un-parametrized exponential warping function tends to overestimates peaks and underestimate troughs. In panel b), the volatility is extremely difficult to reconstruct or forecast  X  with no warning it will immediately and dramatically increase or decrease. This behaviour is not suited to a smooth squared exponential covariance function. Nevertheless, GCPV outperforms GARCH, especially in regions of low volatility. We also see this in panel a) for t  X  (1 . 5 , 2) . GARCH is known to respond slowly to large returns, and to overpredict volatility [22]. In JUMP , the greater the peaks, and the smaller the troughs, the more GARCH suffers, while GCPV is mostly robust to these changes. 5.2 Financial Data The returns on the daily exchange rate between the Deutschmark (DM) and the Great Britain Pound (GBP) from 1984 to 1992 have become a benchmark for assessing the performance of GARCH models [8, 9, 10]. This exchange data, which we refer to as DMGBP , can be obtained from www.datastream.com , and the returns are calculated as r t = log( P t +1 /P t ) , where P t is the number of DM to GBP on day t . The returns are assumed to have a zero mean function. We use a rolling window of the previous 120 days of returns to make 1, 7, and 30 day ahead volatility forecasts, starting at the beginning of January 1988, and ending at the beginning of January 1992 (659 trading days). Every 7 days, we retrain the parameters of GCPV and GARCH. Every time we retrain parameters, we predict historical volatility over the past 120 days. The average MSE for these historical predictions is given in Table 1, although they should be observed with caution; unlike with the simulations, the DMGBP historical predictions are trained using the same data they are assessed on. In Figure 1c), we see that the GARCH one day ahead forecasts are lifted above the GCPV forecasts, but unlike in the simulations, they are now operating on a similar lengthscale. This suggests that GARCH could still be overpredicting volatility, but that GCPV has adapted its estimation of how  X  t and  X  t + a correlate with one another. Since GARCH is suited to this financial data set, it is reassuring that GCPV predictions have a similar time varying structure. Overall, GCPV and GARCH are competitive with one another for forecasting currency exchange returns, as seen in Table 1. Moreover, a learned warping function g outperforms an unparametrized one, and a full Laplace solution is comparable to using MCMC for inference, in accuracy and speed. This is also true for the simulations. Therefore we recommend whichever is more convenient to implement. We defined a copula process, and as an example, developed a stochastic volatility model, GCPV, which can outperform GARCH. With GCPV, the volatility  X  t is distributed as a Gaussian Copula Process, which separates the modelling of the dependencies between volatilities at different times from their marginal distributions  X  arguably the most useful property of a copula. Further, GCPV fits the marginals in the Gaussian copula process by learning a warping function. If we had simply cho-sen an unparametrized exponential warping function, we would incorrectly be assuming that the log volatilities are marginally Gaussian distributed. Indeed, for the DMGBP data, we trained the warping function g over a 120 day period, and mapped its inverse through the univariate standard Gaussian cdf  X  , and differenced, to estimate the marginal probability density function (pdf) of  X  t over this period. The learned marginal pdf, shown in Figure 1d), is similar to a Gamma(4.15,0.00045) distri-bution. However, in using a rolling window to retrain the parameters of g , we do not assume that the marginals of  X  t are stationary; we have a time changing warping function.
 While GARCH is successful, and its simplicity is attractive, our model is also simple and has a number of advantages. We can effortlessly handle missing data, we can easily incorporate covariates other than time (like interest rates) in our covariance function, and we can choose from a rich class of covariance functions  X  squared exponential, Brownian motion, Mat  X  ern, periodic, etc. In fact, the volatility of high frequency intradaily returns on equity indices and currency exchanges is cyclical [23], and GCPV with a periodic covariance function is uniquely well suited to this data. And the parameters of GCPV, like the covariance function lengthscale, or the learned warping function, provide insight into the underlying source of volatility, unlike the parameters of GARCH. Finally, copulas are rapidly becoming popular in applications, but often only bivariate copulas are being used. With our copula process one can learn the dependencies between arbitrarily many ran-dom variables independently of their marginal distributions. We hope the Gaussian Copula Process Volatility model will encourage other applications of copula processes. More generally, we hope our work will help bring together the machine learning and econometrics communities.
 Acknowledgments : Thanks to Carl Edward Rasmussen and Ferenc Husz  X  ar for helpful conversa-tions. AGW is supported by an NSERC grant. References
