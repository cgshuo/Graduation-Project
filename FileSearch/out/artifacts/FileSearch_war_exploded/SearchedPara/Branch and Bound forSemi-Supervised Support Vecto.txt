 chapelle@tuebingen.mpg.de A major line of research on extending SVMs to handle partially labeled datasets is based on the following idea: solve the standard SVM problem while treating the unknown labels as additional optimization variables. By maximizing the margin in the presence of unlabeled data, one learns a decision boundary that traverses through low data-density regions while respecting labels in the input space. In other words, this approach implements the cluster assumption for semi-supervised learning  X  that points in a data cluster have similar labels. This idea was first introduced in [14] under the name Transductive SVM , but since it learns an inductive rule defined over the entire input space, we refer to this approach as Semi-supervised SVM (S 3 VM).
 Since its first implementation in [9], a wide spectrum of techniques have been applied to solve the non-convex optimization problem associated with S 3 VMs, e.g., local combinatorial search [9], gradient descent [6], continuation techniques [3], convex-concave procedures [7], and deterministic annealing [12]. While non-convexity is partly responsible for this diver-sity of methods, it is also a departure from one of the nicest features of SVMs. Several experimental studies have established that S 3 VM implementations show varying degrees of empirical success. This is conjectured to be closely tied to their susceptibility to local minima problems.
 The following questions motivate this paper: How well do current S 3 VM implementations approximate the exact, globally optimal solution of the non-convex problem associated with S
VMs ? Can one expect significant improvements in generalization performance by better approaching the global solution? We believe that these questions are of fundamental impor-tance for S 3 VM research and are largely unresolved. This is partly due to the lack of simple implementations that practitioners can use to benchmark new algorithms against the global solution, even on small-sized problems. Our contribution in this paper is to outline a class of Branch and Bound algorithms that are guaranteed to provide the globally optimal solution for S 3 VMs. Branch and bound techniques have previously been noted in the context of S 3 VM in [16], but no details were presented there. We implement and evaluate a branch and bound strategy that can serve as an upper baseline for S 3 VM algorithms. This strategy is not practical for typical semi-supervised settings where large amounts of unlabeled data is available. But we believe it opens up new avenues of research that can potentially lead to more efficient variants. Empirical results on some semi-supervised tasks presented in section 7 show that the exact solution found by branch and bound has excellent generalization performance, while other S
VM implementations perform poorly. These results also show that S 3 VM can compete and even outperform graph-based techniques (e.g.,[17, 13]) on problems where the latter class of methods have typically excelled. We consider the problem of binary classification. The training set consists of l labeled In the linear case, the following objective function is minimized on both the hyperplane parameters w and b , and on the label vector y u := [ y l +1 . . . y n ] &gt; , under constraints y i ( w  X  x i + b )  X  1  X   X  i , 1  X  i  X  n . Non linear decision boundaries can be constructed using the kernel trick [15]. While in general any convex loss function can be used, it is common to either penalize the training errors linearly ( p = 1) or quadratically ( p = 2). In the rest of the paper, we consider p = 2. The first two terms in (1) correspond to a standard SVM. The last one takes into account the unlabeled points and can be seen as an implementation of the cluster assumption [11] or low density separation assumption [6]; indeed, it drives the outputs of the unlabeled points away from 0 (see figure 1). Figure 1: With p = 2 in (1), the loss of a point with label y and signed output t is For simplicity, we take C  X  = C . But in practice, it is important to set these two values independently because C reflects our confidence in the labels of the training points, while C  X  corresponds to our belief in the low density separation assumption. In addition, we add the following balancing constraint to (1), This constraint is necessary to avoid unbalanced solutions and has also been used in the original implementation [9]. Ideally, the parameter r should be set to the ratio of positive points in the unlabeled set. Since it is unknown, r is usually estimated through the class ratio on the labeled set. In that case, one may wish to  X  X often X  this constraint, as in [6]. For the sake of simplicity, in the rest of the paper, we set r to the true ratio of positive points in the unlabeled set.
 Let us call I the objective function to be minimized: There are two main strategies to minimize I : a continuous optimization on w and b can be done [6]. But note that the constraint (2) is then not straightforward to enforce. (2) For a given y u , the optimization on w and b is a standard SVM training. Let X  X  define Now the goal is to minimize J over a set of binary variables (and each evaluation of J is a standard SVM training). This was the approach followed in [9] and the one that we take in this paper. The constraint (2) is implemented by setting J ( y u ) = +  X  for all vectors y u not satisfying it. 3.1 Branch and bound basics Suppose we want to minimize a function f over a space X , where X is usually discrete. A branch and bound algorithm has two main ingredients: Branching : the region X is recursively split into smaller subregions. This yields a tree Bounding : consider two (disjoint) subregions (i.e. nodes) A and B  X  X . Suppose that 3.2 Branch and bound for S 3 VM The aim is to minimize (3) over all 2 u possible choices for the vector y u , 1 which constitute the set X introduced above. The binary search tree has the following structure. Any node corresponds to a partial labeling of the data set and its two children correspond to the labeling of some unlabeled point. One can thus associate with any node a labeled set L containing both the original labeled examples and a subset S of unlabeled examples unlabeled set U = [ l + 1 . . . n ] \ S corresponding to the subset of unlabeled points which The root of the tree has only the original set of labeled examples associated with it, i.e S is empty. The leaves in the tree correspond to a complete labeling of the dataset, i.e. U is empty. All other nodes correspond to partial labelings.
 As for any branch and bound algorithm, we have to decide about the following choices, Branching: For a given node in the tree (i.e. a partial labeling of the unlabeled set), what Bounding: Which upper and lower bounds should be used? Exploration: In which order will the search tree be examined? In other words, which Concerning the upper bound, we decided to have the following simple strategy: for a leaf node, the upper bound is simply the value of the function; for a non leaf node, there is no upper bound. In other words, the upper bound is the best objective function found so far . Coming back to the notations of section 3.1, the set A is the leaf corresponding to the best solution found so far and the set B is the subtree that we are considering to explore. Because of this choice for the upper bound, a natural way to explore the tree is a depth first search . Indeed it is important to go to the leaves as often as possible in order to have a tight upper bound and thus perform aggressive pruning.
 The choice of the lower bound and the branching strategy are presented next. We consider a simple lower bound based on the following observation. The minimum of the objective function (1) is smaller when C  X  = 0 than when C  X  &gt; 0. But C  X  = 0 corresponds to a standard SVM, ignoring the unlabeled data. We can therefore compute a lower bound at a given node by optimizing a standard SVM on the labeled set associated with this node. We now present a more general framework for computing lower bounds. It is based on the dual objective function of SVMs. Let D (  X  , y U ) be the dual objective function, where y U corresponds to the labels of the unlabeled points which have not been assigned a label yet, The dual feasibility is Now suppose that we have a strategy that, given y U , finds a vector  X  ( y U ) satisfying (5). Since the dual is maximized, where J has been defined in (3).
 the minimum is taken over all y U satisfying the balancing constraint (2). Then lb is also a lower bound for the value of the objective function corresponding to that node. The goal is thus to find a choice for  X  ( y U ) such that a lower bound on Q can be computed efficiently. The choice corresponding to the lower bound presented above is the following. Train an SVM on the labeled points, obtain the vector  X  and complete it with zeros for the unlabeled points. Then Q ( y U ) is the same for all the possible labelings of the unlabeled points and the lower bound is the SVM objective function on the labeled points. the vector  X  by zeros, we complete it by a constant  X  which would typically be of the To lower bound Q , one can use results from the quadratic zero-one programming literature [10] or solve a constrained eigenvalue problem [8]. Finally, note that unless P U y i = 0, the constraint P  X  i y i = 0 will not be satisfied. One remedy is to train the supervised SVM with this amounts to penalizing the bias term b . At a given node, some unlabeled points have already been assigned a label. Which unlabeled point should be labeled next? Since our strategy is to reach a good solution as soon as possible (see last paragraph of section 3.2), it seems natural to assign the label that we are the most confident about. A simple possibility would be to branch on the unlabeled point which is the nearest from another labeled point using a reliable distance metric. But we now present a more principled approach based on the analysis of the objective value. We say that we are  X  X onfident X  about a particular label of an unlabeled point when assigning the opposite label results in a big increase of the objective value: this partial solution would then be unlikely to lead to the optimal one.
 Let us formalize this strategy. Remember from section 3.2 that a node is associated with a set L of currently labeled examples and a set U of unlabeled examples. Let s ( L ) be the SVM objective function trained on the labeled set, As discussed in the previous section, the lower bound is s ( L ). Now our branching strategy consists in selecting the following point in U , In other words, we want to find the unlabeled point x  X  and its label y  X  which would make the objective function increase as much as possible. Then we branch on x  X  , but start exploring propagation X  idea [17]: an unlabeled point which is near from a labeled point is likely to be of the same label; otherwise, the objective function would be large.
 A main disadvantage of this approach is that to solve (7), a lot of SVM trainings are necessary. It is however possible to approximately compute s ( L  X  { x , y } ). The idea is similar to the fast approximation of the leave-one-out solution [5]. Here the situation is  X  X dd-one-in X . If an SVM has been trained on the set L it is possible to efficiently compute the solution when one point is added in the training set. This is under the assumption that the set of support vectors does not change when adding this point. In practice, the set is likely to change and the solution will only be approximate.
 Proposition 1 Consider training an SVM on a labeled set L with quadratic penalization of the errors (cf (6) or (4) ). Let f be the learned function and sv be the set of support vectors. Then, if sv does not change while adding a point ( x , y ) in the training set, where S 2 x = K ( x , x )  X  v &gt; K  X  1 sv v , K and relies on the block matrix inverse formula. The algorithm is implemented recursively (see algorithm 1). At the beginning, the upper bound can either be set to +  X  or to a solution found by another algorithm.
 Note that the SVM trainings are incremental: whenever we go down the tree, one point is added in the labeled set. For this reason, the retraining can be done efficiently (also see [2]) since effectively, we just need to update the inverse of a matrix. We consider here two datasets where other S 3 VM implementations are unable to achieve satisfying test error rates. This naturally raises the following questions: Is this weak per-Algorithm 1 Branch and bound for S 3 VM(BB).
 Function: ( Y  X  , v )  X  S 3 VM( Y, ub ) % Recursive implementation Input: Y : a partly labeled vector (0 for unlabeled) Output: Y  X  : optimal fully labeled vector if P max(0 , Y i ) &gt; ur OR P max(0 ,  X  Y i ) &lt; n  X  ur then end if  X  X  X  Do not explore this subtree v  X  SVM( Y ) % Compute the SVM objective function on the labeled points. if v &gt; ub then end if  X  X  X  Do not explore this subtree if Y is fully labeled then end if Find index i and label y as in (7) % Find next unlabeled point to label
Y i  X  X  X  y % Start first by the most likely label ( Y  X  , v )  X  S 3 VM( Y, ub ) % Find (recursively) the best solution
Y i  X  X  X  Y i % Switch the label ( Y  X  2 , v 2 )  X  S 3 VM( Y, min( ub, v )) % Explore other branch with updated upper-bound if v 2 &lt; v then end if formance due to the unsuitability of the S 3 VM objective function for these problems or do these methods get stuck at highly sub-optimal local minima? 7.1 Two moons The  X  X wo moons X  dataset is now a standard benchmark for semi-supervised learning algo-rithms. Most graph-based methods such as [17] easily solve this problem , but so far, all S
VM algorithms find it difficult to construct the right boundary (an exception is [12] using an L 1 loss). We drew 100 random realizations of this dataset, fixed the bandwidth of an RBF kernel to  X  = 0 . 5 and set C = 10. Each moon contained 50 unlabeled points. We compared  X  S 3 VM[6], cS 3 VM[3], CCCP [7], SVM light [9] and DA [12]. For the first 3 methods, there is no direct way to enforce the constraint (2). However, these methods have a constraint that the mean output on the unlabeled point should be equal to some constant. This constant is normally fixed to the mean of the labels, but for the sake of consistency we did a dichotomy search on this constant in order to have (2) satisfied.
 Results are presented in table 1. Note that the test errors for other S 3 VM implementations are likely to be improved by hyperparameter tuning, but they will still stay very high. For comparison, we have also included the results of a state-of-the-art graph based method, LapSVM [13] whose hyperparameters were optimized for the test error and the threshold adjusted to satisfy the constraint (2).
 Matlab source code and a demo of our algorithm on the  X  X wo moons X  dataset is accessible as supplementary material with this paper. 7.2 COIL Extensive benchmark results reported in [4, benchmark chapter] show that on problems where classes are expected to reside on low-dimensional non-linear manifolds, e.g., hand-written digits, graph-based algorithms significantly outperform S 3 VM implementations. We consider here such a dataset by selecting three confusible classes from the COIL20 dataset [6] (see figure 2). There are 72 images per class, corresponding to rotations of 5 degrees (and thus yielding a one dimensional manifold). We randomly selected 2 images per class to be in the labeled set and the rest being unlabeled. Results are reported in table 2. The hyperparameters were chosen to be  X  = 3000 and C = 100.
 test errors; (2) other S 3 VM implementations fail completely in finding a good minimum of the objective function 2 and (3) the global S 3 VM solution can actually outperform graph-based alternatives even if other S 3 VM implementations are not found to be competitive. Concerning the running time, it is of the order of a minute for both datasets. We do not expect this algorithm to be able to handle datasets much larger than couple of hundred points. We implemented and evaluated one strategy amongst many in the class of branch and bound methods to find the globally optimal solution of S 3 VMs. The work of [1] is the most closely related to our methods. However that paper presents an algorithm for linear S 3 VMs and relies on generic mixed integer programming which does not make use of the problem structure as our methods can.
 This basic implementation can perhaps be made more efficient by choosing better bounding and branching schemes. Also, by fixing the upper bound as the currently best objective value, we restricted our implementation to follow depth-first search. It is conceivable that breadth-first search is equally or more effective in conjunction with alternative upper bound-ing schemes. Pruning can be done more aggressively to speed-up termination at the expense of obtaining a solution that is suboptimal within some tolerance (i.e prune B if a &lt; b  X  ). Finally, we note that a large family of well-tested branch and bound procedures from zero-one quadratic programming literature can be immediately applied to the S 3 VM problem for the special case of squared loss. An interesting open question is whether one can provide a guarantee for polynomial time convergence under some assumptions on the data and the kernel.
 Concerning the running time of our current implementation, we have observed that it is most efficient whenever the global minimum is significantly smaller than most local minima: in that case, the tree can be pruned efficiently. This happens when the clusters are well separated and C and  X  are not too small.
 For these reasons, we believe that this implementation does not scale to large datasets, but should instead be considered as a proof of concept: the S 3 VM objective function is very well suited for semi-supervised learning and more effort should be made on trying to efficiently find good local minima.

