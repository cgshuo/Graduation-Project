 Many national and international heritage institutes real-ize the importance of archiving the web for future culture heritage. Web archiving is currently performed either by harvesting a national domain, or by crawling a pre-defined list of websites selected by the archiving institution. In either method, crawling results in more information being harvested than just the websites intended for preservation; which could be used to reconstruct impressions of pages that existed on the live web of the crawl date, but would have been lost forever. We present a method to create representations of what we will refer to as a web collection X  X  aura : the web documents that were not included in the archived collection, but are known to have existed  X  due to their mentions on pages that were included in the archived web collection. To create representations of these unarchived pages, we exploit the information about the unarchived URLs that can be de-rived from the crawls by combining crawl date distribution, anchor text and link structure. We illustrate empirically that the size of the aura can be substantial: in 2012, the Dutch Web archive contained 12.3M unique pages, while we uncover references to 11.9M additional (unarchived) pages. Web Archives; Web Archiving; Web Crawlers; Anchor Text; Web Graph; Information Retrieval
Since 1996, web archiving has been performed by interna-tional and national heritage institutions such as the Internet Archive and national libraries, in order to preserve digital cultural heritage for future generations. While archiving the entire web remains an impossible task in terms of size and with regards to its ephemerality, parts of the web are being preserved by using crawlers that capture and archive web pages at the time of harvesting. Web archiving crawlers differ in their scope and settings; breadth-first crawls are designed to discover and capture as many pages as possible, while within domains, to infer characteristics of technology matu-ration over time . Other studies used archived web data to determine the rate of link persistence, or web change over time [8 , 9, 16 , 17 , 7]. The Memento project has used large samples of archived web data to characterize and measure the completeness of the archived web [1 , 2, 17 ]. Memento is an HTTP-based framework that facilitates locating past versions of a given resource, which, through an aggregator of resources from multiple web archives, indicates the location of archived versions of a given resource across multiple web archives [ 18]. Using the Memento aggregator, the coverage of web archives can be determined following the procedure of [2 ], albeit only at the level of domains.

In this study we propose an analysis at a page level; and do not stop at just uncovering the missing (unarchived) pages, but also propose to recover a representation of these by using anchor text representations.

Anchor text has previously been used to enrich the repre-sentations of web page content, primarily aiming to improve web retrieval. Craswell et al.[4 ] first experimented with site finding using anchor texts, considering anchor texts as pseudo documents. They considered the anchor text as surrogate documents and indexed these (for ranking by Okapi BM25)  X  instead of indexing the content of the target pages. They concluded that anchor texts can be more useful than content words for navigational queries. Kraft and Zien showed that anchor texts can produce higher quality query refinement suggestions than content text [ 11]. Fujii proposed a model for classifying queries into navigational and informational, and use different retrieval methods depending on the query type. The experimental results showed that content of web pages is useful for informational query types, whereas anchor text information and links are useful for navigational query types [6 ]. Koolen and Kamps concluded that anchor text has added value for ad hoc informational search, and can lead to significant improvements in retrieval effectiveness. They also evaluate some of the factors impacting the success of anchor text, including link density and collection size [ 10 ].
In the previous research discussed so far, the anchor text of a page has been considered as a resource that is comple-mentary to the page content, but treated as two independent representations. Dou et al. took the relationships between source and anchor texts into account, and distinguished be-tween links from the same website and links from related sites [5 ]. Similar in spirit, Meztler et al. set out to overcome the problem of anchor text sparsity by smoothing the influence of anchor text originating from with-in the same domain with what they termed the  X  X xternal X  anchor text: the aggregated anchor text from all pages that link to a page in the same domain as the page to be enriched [13 ].
The Dutch web archive, created and maintained by the KB, consists of 76,828 compressed ARC files, that were archived in the period from 2009 until 2012, accumulating to about 7 TB of raw data. Each ARC file contains multiple ARC records (the actually archived web content). In total, the collection consists of approximately 148M archived web documents. The analysis presented in this paper uses only the 2012 part of the archive, a collection consisting of approximately 39M archived web documents (see Table 1). The KB also
Before we discuss our analysis, we first detail the process to create the anchor text representations of the target pages thus obtained. We simply union all the anchor text representations corresponding to links that point to the target page, that existed in the year considered (i.e., 2012). For each target, we then determine the number of unique sources linking to the target page, the number of unique anchor texts used, the number of unique words these anchor texts consisted of, as well as the UNESCO codes for each incoming link.
This section provides an estimate of the volume and utility of the extracted representations of the unarchived pages, based on the representations X  amount of usable features, in-cluding the URL, anchor text, date and UNESCO code. Fig-ure 1 shows the distribution between pages in the archive and seedlist, pages that are archived and not in the seedlist, and pages which are neither in the seedlist nor the archive (the  X  X ura X ). The 2012 web archive collection contains 12,327,673 unique pages, of which 11,395,072 were included on the seedlist (and 932,601 were only included unintentionally). The  X  X ura X  that we uncovered contains an additional (unar-chived!) 11,897,662 page representations. In other words, the uncovered web that is only indirectly collected while crawl-ing consists of almost as many pages as the intentionally harvested collection! We now zoom in on the inter-domain links, aggregating link information for 3,205,354 unique pages that are not part of what is considered the web archive. The remainder of this section details a variety of features that can be used to represent the (missing) content of these unarchived pages in a variety of ways.
For all of the 3.2M impressions of unarchived pages men-tioned in our corpus, we have immediate access to a ba-sic representation consisting of their URL and estimated timestamp. From the URL, we can derive their domain-names, their level, and their top-level domain (or TLD). A simple slashcount (based on absolute URLs) indicates that 50,41% of these site representations are on the top or first level of their domain (e.g., http://www.example.com or http://www.example.com/forum). This finding is in line with previous research (e.g. [4 ], [ 19]) that demonstrated that entry pages of websites often have a higher number of inlinks than other pages of a site.

Table 2 shows the distribution of TLDs in the uncovered web. In earlier work, we generated TLD statistics from the open web crawl CommonCrawl (2012) 2 , that consists of over 1.2B webpages, where .com, .de, .org, .net., .uk and .nl https://github.com/norvigaward/naward15/wiki targets refer to a domain-name that has been hand-labelled by the KB. 4 The  X 31-History and biography X ,  X 06-Law and government administration X , and  X 08-Education X  are the most prominent categories among these classifications.
Another way of characterizing the pages that are outside of the archive, is by looking at their sources. The source X  X  URL structure (e.g., depth, TLD and words contained in the URL), timestamps and assigned UNESCO-code provide additional clues about the target, possibly enriching page representations. A first foray into this aspect of the repre-sentations provided us with insights about the categories of the sources. We checked which UNESCO codes are as-signed to the sources of the representations. For 78,34% of all target pages that we uncovered, we can derive at least one category coming from the sources of the links to that page. The top UNESCO categories for these sources are  X 06-Law and government administration X ,  X 23-Art and Architecture X , and  X 31-History and biography X . Additional analysis is how-ever necessary to determine whether these categories provide meaningful categorizations of the unarchived target sites.
We have presented a pilot study aimed to uncover the unar-chived Web. Our analysis of uncovered URLs (a set we refer to as a web archive X  X   X  X ura X ) extracted from the 2012 Dutch web archive indicates the wealth of data that could help to not just uncover, but also recover representations of the unar-chived web. The fact that the domain type distribution of uncovered URLs from the selection-based Dutch web archive resembles that of a broad domain web crawl reassures us that we may indeed make inferences about the unarchived web, by using its impressions in the archived web data. While none of the extracted features suffices to generate a rich representa-tion of the unarchived web, a combination of representations may contribute to enriched recovery. Combinations of repre-sentations may consist of anchor text, but also the derived structure of source and target sites, assigned categories, or other extractable features. In future research, we hope to determine thresholds on features like the number of unique words or the number of inlinks to a page in relation to the quality of the inferred page representation.
Some of these sites may have opted-out in spite of being originally selected for harvesting, while others have been referred to in the unintentionally crawled pages.
