 Google Inc.
 Uppsala University
There has been a rapid increase in the volume of research on data-driven dependency parsers in of languages X  X ue in large part to the CoNLL shared tasks X  X s well as the straightforward mechanisms by which dependency theories of syntax can encode complex phenomena in free word been made through an analysis of the two predominant paradigms for data-driven dependency parsing, which are often called graph-based and transition-based dependency parsing. Our type of parser makes and how they relate to theoretical expectations. Using these observations, we present an integrated system based on a stacking learning framework and show that such a system can learn to overcome the shortcomings of each non-integrated system. 1. Introduction
Syntactic dependency representations have a long history in descriptive and theoretical linguistics and many formal models have been advanced ,most notably Word Gram-mar (Hudson 1984) ,Meaning-Text Theory (Mel X   X  cuk 1988) ,Functional Generative De-scription (Sgall ,Haji  X  cov  X  a ,and Panevov  X  a 1986) ,and Constraint Dependency Grammar (Maruyama 1990). Common to all theories is the notion of directed syntactic depen-dencies between the words of a sentence ,an example of which is given in Figure 1 for the sentence A hearing is scheduled on the issue today ,which has been extracted from the
Penn Treebank (Marcus ,Santorini ,and Marcinkiewicz 1993). A dependency graph of a sentence represents each word and its syntactic modifiers through labeled directed arcs ,where each arc label comes from some finite set representing possible syntactic roles. Returning to our example in Figure 1 ,we can see multiple instances of labeled dependency relations such as the one from the finite verb is to hearing labeled SBJ indicating that hearing is the head of the syntactic subject of the finite verb. An artificial word has been inserted at the beginning of the sentence that will always serve as the single root of the graph and is primarily a means to simplify computation. tional linguistics community and have been successfully employed for many problems ranging from machine translation (Ding and Palmer 2004) to ontology construction (Snow ,Jurafsky ,and Ng 2005). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to long-distance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order. This is undoubt-edly one of the reasons for the emergence of dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Thus ,the example in Figure 1 contains an instance of a discontinuous construction through the subgraph rooted at the word hearing . Specifically ,the dependency arc from hearing to on spans the words is and scheduled ,which are not nodes in this subgraph. An arc of this kind is said to be non-projective .
 parsing ,which encompasses parsing systems that learn to produce dependency graphs for sentences from a corpus of sentences annotated with dependency graphs. The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist. Many data-driven parsing systems are grammar-less ,in that they do not assume the existence of a grammar that defines permissible sentences of the language. Instead ,the goal of most data-driven parsing systems is to discriminate good parses from bad for a given sentence ,regardless of its grammaticality.
Alternatively ,one can view such systems as parsers for a grammar that induces the language of all strings.
 availability of dependency annotated corpora for multiple languages X  X ost notably from the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi 2006; Nivre et al. 2007) X  X as led to a boom in research on data-driven dependency parsing. Making sense of this work is a challenging problem ,but an important one if the field is to continue to make advances. Of the many important questions to be asked ,three are perhaps most crucial at this stage in the development of parsers: 1. How can we formally categorize the different approaches to data-driven 2. Can we characterize the kinds of errors each category of parser makes 3. Can we benefit from such an error analysis and build improved parsers? The organizers of the CoNLL-X shared task on dependency parsing (Buchholz and
Marsi 2006) point out that there are currently two dominant approaches for data-driven 198 dependency parsing. The first category parameterizes models over dependency sub-graphs and learns these parameters to globally score correct graphs above incorrect ones. Inference is also global ,in that systems attempt to find the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reflect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996) ,as well as McDonald and colleagues (McDonald ,Crammer ,and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald ,Lerman ,and Pereira 2006) and others (Riedel ,C  X  ak X c X  ,and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007).
The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classification techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local ,in that systems scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reflect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng ,Asahara ,and Matsumoto 2006) ,Nivre (Nivre ,Hall ,and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006) ,and others (Attardi 2006; Attardi and Ciaramita 2007;
Johansson and Nugues 2007; Duan ,Zhao ,and Xu 2007; Titov and Henderson 2007a , 2007b).
 rameterization should require global learning and inference ,and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006) ,it is striking that recent work on data-driven dependency parsing has been dominated by global, exhaustive, graph-based models ,on the one hand ,and local, greedy, transition-based models ,on the other. Therefore ,a careful comparative analysis of these model types appears highly relevant ,and this is what we  X  X raph-based X  and  X  X ransition-based X  for these models ,although both graph-based and transition-based parameterizations can be (and have been) combined with different types of learning and inference. For example ,the system described by Zhang and Clark (2008) could be characterized as a transition-based model with global learning ,and the ensemble system of Zeman and  X  Zabokrtsk ` y (2005) as a graph-based model with greedy inference.
 transition-based models is that even though they appear to be quite different theoret-accuracies on a variety of languages. For example ,Table 1 shows the results of the two top performing systems in the CoNLL-X shared task ,those of McDonald ,Lerman ,and
Pereira (2006) (graph-based) and Nivre et al. (2006) (transition-based) ,which exhibit no statistically significant difference in accuracy when averaged across all languages.
This naturally leads us to our Question 2 ,that is ,can we empirically characterize the errors of these systems to understand whether ,in practice ,these errors are the same and expectations of these two parsing systems and Section 4 provides a fine-grained error analysis of each system on the CoNLL-X shared task data sets (Buchholz and
Marsi 2006). The result of this analysis strongly suggests that (1) the two systems do make different ,yet complementary ,errors ,which lends support to the categorization of parsers as graph-based and transition-based ,and (2) the errors made by each system are directly correlated with our expectations ,based on their theoretical underpinnings. achieve improved accuracies? In Section 5 we consider a simple way of integrating graph-based and transition-based models in order to exploit their complementary strengths and thereby improve parsing accuracy beyond what is possible by either model in isolation. The method integrates the two models by allowing the output of one model to define features for the other ,which is commonly called  X  X lassifier stacking. X 
This method is simple X  X equiring only the definition of new features X  X nd robust by allowing a model to learn relative to the predictions of the other. More importantly ,we rerun the error analysis and show that the integrated models do indeed take advantage of the complementary strengths of both the graph-based and transition-based parsing systems.
 systems ,is by no means new as there are a number of previous studies that have looked at combining phrase-structure parsers (Henderson and Brill 1999) ,dependency parsers (Zeman and  X  Zabokrtsk ` y 2005) ,or both (McDonald 2006). Of particular note is past work on combining graph-based and transition-based dependency parsers. Sagae and Lavie (2006) present a system that combines multiple transition-based parsers with a single graph-based parser by weighting each potential dependency relation by the number of parsers that predicted it. A final dependency graph is predicted by using spanning tree inference algorithms from the graph-based parsing literature (McDonald et al. 2005).
Sagae and Lavie report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing ,evaluated on data from the Penn Treebank. The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task.
 with beam search over a transition-based backbone incorporating both graph-based 200 and transition-based features ,that is ,features over both sub-graphs and transitions.
Huang and Sagae (2010) go even further and show how transition-based parsing can be tabularized to allow for dynamic programming ,which in turn permits an exponentially larger search space. Martins et al. (2008) present a method for integrating graph-based and transition-based parsers based on stacking ,which is similar to the approach taken in this work. Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly. For example ,Hall (2007) ,Riedel , C  X  ak X c X  ,and Meza-Ruiz (2006) ,Nakagawa (2007) ,Smith and Eisner (2008) ,and Martins ,
Smith ,and Xing (2009) attempt to overcome local restrictions in feature scope for graph-based parsers through both approximations and exact solutions with integer linear programming.
 types of errors these parsers make ,tie them to their theoretical expectations ,and show that integrating graph-based and transition-based parsers not only increases overall accuracy ,but does so directly exploiting the strengths of each system. Thus ,this is the first large-scale error analysis of modern data-driven dependency parsers. of the article is structured as follows: Section 2 describes canonical graph-based and transition-based parsing systems and discusses their theoretical benefits and limitations with respect to one another; Section 3 introduces the experimental setup based on the
CoNLL-X shared task data sets that incorporate dependency treebanks from 13 diverse languages; Section 4 gives a fine-grained error analysis for the two parsers in this setup; Section 5 describes a stacking-based dependency parser combination framework;
Section 6 evaluates the stacking-based parsers in comparison to the original systems with a detailed error analysis; we conclude in Section 7. 2. Two Models for Dependency Parsing
In this section we introduce central notation and define canonical graph-based and transition-based dependency parsing at an abstract level. We further compare and contrast their theoretical underpinnings with an eye to understanding the kinds of errors each system is likely to make in practice. 2.1 Preliminaries
Let L = { l 1 , ... , l | L | } be a set of permissible arc labels. Let x = w sentence where w 0 = ROOT . Formally ,a dependency graph for an input sentence x is a labeled directed graph G = ( V , A ) consisting of a set of nodes V and a set of labeled directed arcs A  X  V  X  V  X  L ;thatis,if( i , j , l )  X  A for i , j an arc from node i to node j with label l in the graph. In terms of standard linguistic dependency theory nomenclature ,we say that ( i , j , l ) head w i , dependent w j ,and syntactic role l .
 1. V = { 0 ,1 , ... , n } . 2. If ( i , j , l )  X  A ,then j = 0. 3. If ( i , j , l )  X  A ,then for all arcs ( i , j , l ) 4. For all j  X  V  X  X  0 } ,either (0 , j , l )forsome l  X  L or there is a non-empty
The first constraint states that the dependency graph spans the entire input. The sec-graph is connected through directed paths from the node 0 to every other node in the term dependency tree to refer to any valid dependency graph. The characterization of syntactic dependency graphs as trees is consistent with most formal theories (e.g.,
Sgall ,Haji  X  cov  X  a ,and Panevov  X  a 1986; Mel X   X  cuk 1988). Exceptions include Word Grammar (Hudson 1984) ,which allows a word to modify multiple other words in the sentence , which results in directed acyclic graphs with nodes possibly having multiple incoming arcs.
 one word occurring between w i and w j in the input sentence is not a descendant of w (where  X  X escendant X  is the transitive closure of the arc relation). Alternatively ,we can view non-projectivity in trees as breaking the nested property ,which can be seen through the arcs that cross in the example in Figure 1. Non-projective dependencies are typically difficult to represent or parse in phrase-based models of syntax. This can either be due to nested restrictions arising in context-free formalisms or computationally expensive operations in mildly context-sensitive formalisms (e.g. ,adjunction in TAG frameworks). 2.2 Global, Exhaustive, Graph-Based Parsing
For an input sentence, x = w 0 , w 1 , ... , w n consider the dense graph G fined as: 1. V x = { 0 ,1 , ... , n } . 2. A x = { ( i , j , l ) | i , j  X  V x and l  X  L } .

Let D ( G x ) represent the subgraphs of graph G x that are valid dependency graphs for the sentence x ,that is ,dependency trees. Because G x contains all possible labeled arcs ,the set D ( G x ) must necessarily contain all dependency trees for x .

Furthermore ,define the score of a graph as the sum of its arc scores ,
Thescoreofanarc, s ( i , j , l ) represents the likelihood of creating a dependency from head w i to modifier w j with the label l in a dependency tree. This score is commonly defined to be the product of a high dimensional feature representation of the arc and a 202 the parsing problem can be stated as
An example graph G x and the dependency tree maximizing the scoring function are given in Figure 2 for the sentence John saw Mary . We omit arcs into the root node for simplicity.
 scoring directed spanning tree for the graph G x originating out of the root node 0. It is not difficult to see this ,because both dependency trees and spanning trees must contain all nodes of the graph and must have a tree structure with root 0. The directed spanning tree problem (also known as the r-arborescence problem) can be solved for both the labeled and unlabeled case using the Chu-Liu-Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967) ,a variant of which can be shown to have an O ( n 1977). Non-projective arcs are produced naturally through the inference algorithm that searches over all possible directed trees ,whether projective or not.
 (McDonald ,Crammer ,and Pereira 2005; Koo et al. 2007; Smith and Smith 2007) ,which optimize the parameters of the model to maximize the difference in score/probability between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. Such a learning procedure is global because model parameters are set relative to the classification of the entire dependency graph ,and not just over single arc attachment decisions. Although a learning procedure that only optimizes the score of individual arcs is conceivable ,it would not be likely to produce competitive results.
 dependency graph. In the case of projective dependency trees ,polynomial time parsing algorithms were shown to exist ,but non-projective trees required approximate inference that used an exhaustive projective algorithm followed by transformations to the graph that incrementally introduce non-projectivity. In general ,inference and learning for graph-based dependency parsing is NP-hard when the score is factored over anything larger than arcs (McDonald and Satta 2007). Thus ,graph-based parsing systems cannot easily condition on any extended scope of the dependency graph beyond a single arc ,which is their primary shortcoming relative to transition-based systems. McDonald ,Crammer ,and Pereira (2005) show that a rich feature set over the input space ,including lexical and surface syntactic features of neighboring words , explore higher-order models for projective trees. Additionally ,work has been done on approximate non-factored parsing systems (McDonald and Pereira 2006; Hall 2007;
Nakagawa 2007; Smith and Eisner 2008) as well as exact solutions through integer linear programming (Riedel ,C  X  ak X c X  ,and Meza-Ruiz 2006; Martins ,Smith ,and Xing 2009).
McDonald ,Lerman ,and Pereira (2006) ,which uses pairwise arc scoring and approx-used to label each arc. This two-stage process was adopted primarily for computational reasons and often does not affect performance significantly (see McDonald [2006] for more). Throughout the rest of this study we will refer to this system as MSTParser (or
MST for short) ,which is also the name of the freely available implementation. 2.3 Local, Greedy, Transition-Based Parsing
A transition system for dependency parsing defines 1. a set C of parser configurations ,each of which defines a (partially built) 2. a set T of transitions ,each of which is a partial function t : C 3. for every sentence x = w 0 , w 1 , ... , w n ,
A transition sequence C 0, m = ( c 0 , c 1 , ... , c m ) for a sentence x = w quence of configurations such that c 0 = c x , c m  X  C x ,and ,for every c is a transition t  X  T such that c i = t ( c i  X  1 ). The dependency graph assigned to a sentence x = w 0 , w 1 , ... , w n by a sequence C 0, m = ( c 0 , c terminal configuration c m .
 of configuration c in a transition sequence leading to the optimal dependency graph for the given sentence. This score is usually defined by a classifier g taking as input a high dimensional feature representation of the configuration, s ( c , t ) = g ( f ( c ), t ). minal configuration c m  X  C x ,starting from the initial configuration c optimal transition t  X  out of every configuration c : 204
This can be seen as a greedy search for the optimal dependency graph ,based on a sequence of locally optimal decisions in terms of the transition system.
 of partially processed nodes ,a buffer  X  of remaining input nodes ,and a set A of labeled dependency arcs. The initial configuration for a sentence x = w c = ([0] ,[1 , ... , n ],  X  ) and the set of terminal configurations C rations of the form c = (  X  ,[], A ) (that is ,all configurations with an empty buffer and with arbitrary  X  and A ). The set T of transitions for this system is specified in Figure 3.
The transitions L EFT -A RC l and R IGHT -A RC l extend the arc set A with an arc (labeled l ) connecting the top node i on the stack and the first node j of the buffer. In the case of
L EFT -A RC l ,the node i becomes the dependent and is also popped from the stack; in the case of R IGHT -A RC l ,the node j becomes the dependent and is also pushed onto the stack. The R EDUCE transition pops the stack (and presupposes that the top node has transition extracts the first node of the buffer and pushes it onto the stack. is often referred to as arc-eager . When coupled with the greedy deterministic parsing strategy ,the system guarantees termination after at most 2 n transitions (for a sentence of length n ) ,which means that the time complexity is O ( n ) given that transitions can be performed in constant time. The dependency graph given at termination is guaranteed to be acyclic and projective and to satisfy dependency graph conditions 1 X 3 ,which means that it can always be turned into a well-formed dependency graph by adding arcs (0, i , l r ) for every node i = 0 that is a root in the output graph (where l cial label for root modifiers). Whereas the initial formulation in Nivre (2003) was lim-ited to unlabeled dependency graphs ,the system was extended to labeled graphs in
Nivre ,Hall ,and Nilsson (2004) ,and Nivre and Nilsson (2005) showed how the restric-tion to projective dependency graphs could be lifted by using graph transformation techniques to pre-process training data and post-process parser output ,a technique called pseudo-projective parsing . Transition systems that derive non-projective trees directly have been explored by Attardi (2006) and Nivre (2007 ,2009) ,among others. methods ,such as memory-based learning or support vector machines. The training data are obtained by constructing transition sequences corresponding to gold standard parses from a treebank. The typical learning procedure is local because only single transitions are scored X  X ot entire transition sequences X  X ut more global optimization methods have also been proposed. The primary advantage of these models is that the feature representation is not restricted to a limited number of graph arcs but can take into account the entire dependency graph built so far ,including previously assigned labels ,and still support efficient inference and learning. The main disadvantage is that the greedy parsing strategy may lead to error propagation as false early predictions can eliminate valid trees due to structural constraints and are also used to create features when making future predictions. Using beam search instead of strictly deterministic parsing can to some extent alleviate this problem but does not eliminate it. tion with pseudo-projective parsing ,which uses support vector machines to learn the scoring function for transitions and which uses greedy ,deterministic one-best search at parsing time. We will refer to this system as MaltParser (or Malt for short) ,which is also the name of the freely available implementation. 3 2.4 Comparison
In the previous two sections we have outlined the theoretical characteristics of canonical graph-based and transition-based dependency parsing systems. From now on ,our experiments will rely on two standard implementations: MSTParser ,a graph-based system ,and MaltParser ,a transition-based system. Here we contrast the two parsing systems with respect to how they are trained ,how they produce dependency trees for new sentences ,and what kinds of features they use.
 Training Algorithms. Both systems use large-margin learning for linear classifiers. MST-
Parser uses on-line algorithms (McDonald ,Crammer ,and Pereira 2005; Crammer et al. 2006) and MaltParser uses support vector machines (Cortes and Vapnik 1995). The primary difference is that MaltParser trains the model to make a single classification decision (create arc ,shift ,reduce ,etc.) ,whereas MSTParser trains the model to maxi-mize the global score of correct graphs relative to incorrect graphs. It has been argued and Pereira 2001). However ,it is expensive to train global models since the complexity of learning is typically proportional to inference. In addition ,MaltParser makes use of kernel functions ,which eliminates the need for explicit conjunctions of features.
Inference. MaltParser uses a transition-based inference algorithm that greedily chooses the best parsing decision based on a trained classifier and current parser history. MST-
Parser instead uses exhaustive search over a dense graphical representation of the 206 sentence to find the dependency graph that maximizes the score. On the one hand, the greedy algorithm is far quicker computationally ( O ( n )vs. O ( n
Edmonds algorithm and O ( n 3 ) for Eisner X  X  algorithm). On the other hand ,it may be prone to error propagation when early incorrect decisions negatively influence the parser at later stages. In particular ,MaltParser uses the projective arc-eager transition system first described in Nivre (2003) ,which has consequences for the form of error propagation we may expect to see because the system determines the order in which do not overlap ,then the leftmost arc has to be added first (because of arc-eagerness).
Therefore ,we can expect error propagation from shorter to longer overlapping arcs and from preceding to succeeding arcs.

Feature Representation. Due to the nature of their inference and training algorithms ,the feature representations of the two systems differ substantially. MaltParser can introduce a rich feature space based on the history of previous parser decisions. This is because use this structure to help improve future parsing decisions. By contrast ,MSTParser is forced to restrict the scope of features to a single or pair of nearby parsing decisions in order to make exhaustive inference tractable. As a result ,the feature representation available to the locally trained greedy models is much richer than the globally trained exhaustive models. Concisely ,we can characterize MSTParser as using global training and inference with local features and MaltParser as using local training and inference with global features. (For more information about the features used in the two systems, see Sections 3.2 and 3.3.)
These differences highlight an inherent trade-off between exhaustive inference algo-rithms plus global learning and expressiveness of feature representations. MSTParser favors the former at the expense of the latter and MaltParser the opposite. When analyzing ,and ultimately explaining ,the empirical difference between the systems , understanding this trade-off will be of central importance. 3. Experimental Setup
The experiments presented in this article are all based on data from the CoNLL-X shared task (Buchholz and Marsi 2006). In this section we first describe the task and the resources created there and then describe how MSTParser and MaltParser were trained for the task ,including feature representations and learning algorithms. 3.1 The CoNLL-X Shared Task
The CoNLL-X shared task (Buchholz and Marsi 2006) was a large-scale evaluation of data-driven dependency parsers ,with data from 13 different languages and 19 par-ticipating systems. The data sets were quite heterogeneous ,both with respect to size and with respect to linguistic annotation principles ,and the best reported parsing accuracy varied from 65.7% for Turkish to 91.7% for Japanese. The official evaluation metric was the labeled attachment score (LAS) ,defined as the percentage of tokens , excluding punctuation ,that are assigned both the correct head and the correct depen-dency label. 4 download and constitute a rich resource for comparative error analysis. In Section 4, we will use the outputs of MSTParser and MaltParser for all 13 languages ,together with the corresponding gold standard graphs used in the evaluation ,as the basis for an in-depth error analysis designed to answer Question 2 from Section 1. In Section 6 ,we will then evaluate our stacking-based parsers on the same data sets and repeat the error analysis. This will allow us to compare the error profiles of the new and old systems at a much finer level of detail than in standard evaluation campaigns.
 73,000 sentences for Czech to only 29,000 words and 1,500 sentences for Slovene. We also see that the average sentence length varies from close to 40 words for Arabic ,using slightly different principles for sentence segmentation than the other languages ,to less than 10 words for Japanese ,where the data consist of transcribed spoken dialogues.
Differences such as these can be expected to have a large impact on the parsing accuracy highest top score of all languages ,whereas Arabic has the second lowest. We also see that the amount of information available in the input ,in the form of lemmas (Lem) , coarse and fine part-of-speech tags (CPoS ,PoS) ,and morphosyntactic features (MSF) varies considerably ,as does the granularity of the dependency label sets (Dep). Fi-nally ,the proportion of non-projective structures ,whether measured on the token level (NPT) or on the sentence level (NPS) ,is another important source of variation. 208 words ,which makes it possible to evaluate the performance of a system over all lan-guages by simply concatenating the system X  X  output for all test sets and comparing this to the concatenation of the gold standard test sets. Thus ,most of the statistics used in the subsequent error analysis are based on the concatenation of all test sets. Because some of the phenomena under study are relatively rare ,this allows us to get more reliable esti-mates ,even though these estimates inevitably hide important inter-language variation.
Analyzing individual languages in more detail would be an interesting complementary study but is beyond the scope of this article. 3. 2Training MSTParser linear combination of a parameter vector, w ,and a corresponding feature vector for the arc, f ( i , j , l ). We use a two-stage approach to training. The first-stage learns a model to predict unlabeled dependency trees for a sentence. Thus ,arc scores do not condition on possible labels and are parameterized by features only over the head-modifier pair, potential unlabeled arc ( i , j ). These features represent both information about the head and modifier in the dependency relation as well as the context of the dependency via local part-of-speech information. We include context part-of-speech features for both the fine-grained and coarse-grained tags (when available).
 iments also contains features over adjacent arcs ,( i , j )and( i , k ) ,which we will denote compactly as ( i , j k ). Scores for adjacent arcs are also defined as a linear combination between weights and a feature vector s ( i , j k ) = w  X  f ( i , j k ) ,thus requiring us to de-fine the feature representation f ( i , j k ) ,which is outlined in Table 3(c). These features attempt to capture likely properties about adjacent arcs in the tree via their lexical and part-of-speech information. Finally ,all features in Table 3(a) X (c) contain two versions. conjoined with both the direction of dependency attachment (left or right) as well as the bucketed distance between the head and modifier in buckets of 0 (adjacent) ,1 ,2 ,3 ,4 , 5 X 9 ,and 10+.
 parameterized by a vector of weights and a corresponding feature vector s ( l w  X  f ( i , j , l ) ,where the score of a label l is now conditioned on a fixed dependency arc defined in Table 3(d). These features provide the lexical and part-of-speech context for determining whether a given arc label is suitable for a given head and modifier. Each feature in Table 3(d) again has two versions ,except this time the second version is only conjoined with attachment direction.
 dependency parser (McDonald ,Crammer ,and Pereira 2005; McDonald and Pereira 2006) and a log-linear arc-labeler (Berger ,Pietra ,and Pietra 1996) regularized with a zero mean Gaussian prior with the variance hyper-parameter set to 1.0. The unlabeled dependency parser was trained for 10 iterations and the log-linear arc-labeler was trained for 100 iterations. The feature sets and model hyper-parameters were fixed for all languages. The only exception is that features containing coarse part-of-speech or morphosyntactic information were ignored if this information was not available in a corresponding treebank. 3.3 Training MaltParser
Training MaltParser amounts to estimating a function for scoring configuration-transition pairs ( c , t ) ,represented by a feature vector f ( c , t ) in terms of arbitrary properties of the configuration c ,including the state of the stack  X  ,the input buffer  X  c ,and the partially built dependency graph G configuration by the arc set A c ). In particular ,many features involve properties of the two target tokens ,the token on top of the stack  X  c (denoted  X  the input buffer  X  c (denoted  X  0 c ) ,which are the two tokens that may become connected by a dependency arc through the transition out of c . The basic feature representation used for all languages in the CoNLL-X shared task included three groups of features:
Note in particular that features can be defined with respect to the partially built de-pendency graph G c . This is most obvious for the arc features ,which extract the labels of particular arcs in the graph ,but it is also true of the last lexical feature ,which picks out the word form of the syntactic head of the word on top of the stack. This is pre-cisely what gives transition-based parsers a richer feature space than their graph-based 210 counterparts ,even though graph-defined features are usually limited to a fairly small region of the graph around  X  0 c and  X  0 c ,such as their leftmost and rightmost dependents and their syntactic head (if available).
 for some languages depending on availability in the training data. This included the following:
Additional feature selection experiments were carried out for each language to the extent that time permitted. Complete information about feature representations can be found in Nivre et al. (2006) and on the companion web site. chines as implemented in the LIBSVM library (Chang and Lin 2001) ,with a quadratic class classification ,converting symbolic features to numerical ones using the standard technique of binarization. 7 One thing to note is that the quadratic kernel implicitly adds features corresponding to pairs of explicit features ,thus obviating the need for explicit feature conjunctions as seen in the feature representations of MSTParser. 4. Error Analysis
A primary goal of this study is to characterize the errors made by standard data-driven dependency parsing models. To that end ,this section presents a number of experiments that relate parsing errors to a set of linguistic and structural properties of the input and predicted/gold standard dependency trees. We argue throughout that the results of this analysis can be correlated to specific theoretical aspects of each model X  X n particular the trade-off previously highlighted in Section 2.4.
 sion ,or recall). Identical experiments using unlabeled parsing accuracies did not reveal any additional information. Statistical significance was measured X  X or each metric at each point along the operating curve X  X y employing randomized stratified shuffling at the instance level using 10,000 iterations. 8 Furthermore ,all experiments report aggre-gate statistics over the data from all 13 languages together ,as explained in Section 3. Finally ,in all figures and tables ,MSTParser and MaltParser are referred to as MST and
Malt ,respectively ,for short. 4.1 Length Factors It is well known that parsing systems tend to have lower accuracies for longer sentences.
This is primarily due to the increased presence of complex syntactic constructions involving prepositions ,conjunctions ,and multi-clause sentences. Figure 4 shows the accuracy of both parsing models relative to sentence length (in bins of size 10: 1 X 10, 11 X 20 ,etc.). System performance is almost indistinguishable ,but MaltParser tends to perform better on shorter sentences ,which require the greedy inference algorithm to make fewer parsing decisions. As a result ,the chance of error propagation is reduced significantly when parsing these sentences. However ,if this was the only difference between the two systems ,we would expect them to have equal accuracy for shorter sentences. The fact that MaltParser actually has higher accuracy when the likelihood of error propagation is reduced is probably due to its richer feature space relative to MSTParser.
 to sentence length. We define the length of a dependency from word w the main verb in a sentence. Shorter dependencies are often modifiers of nouns such as determiners or adjectives or pronouns modifying their direct neighbors. Figure 5 measures the precision and recall for each system relative to dependency lengths in the predicted and gold standard dependency graphs. Precision represents the percentage of predicted arcs of length d that were correct. Recall measures the percentage of gold standard arcs of length d that were predicted.
 precise for longer dependency arcs ,whereas MaltParser does better for shorter depen-dency arcs. This behavior can be explained using the same reasoning as above: Shorter dependency arcs are usually created first in the greedy parsing procedure of MaltParser and are less prone to error propagation. In contrast ,longer dependencies are typically constructed at the later stages of the parsing algorithm and are affected more by error propagation. Theoretically ,MSTParser should not perform better or worse for arcs of any length. However ,due to the fact that longer dependencies are typically harder to parse ,there is still a degradation in performance for MSTParser X  X p to 20% in the 212 extreme. However ,the precision curve for MSTParser is much flatter than MaltParser , which sees a drop of up to 40% in the extreme. Note that even though the area under the curve is much larger for MSTParser ,the number of dependency arcs with a length &gt; 10 is much smaller than the number with length &lt; 10 ,which is why the overall accuracy of the two systems is nearly identical. 4. 2Graph Factors
The structure of the predicted and gold standard dependency graphs can also provide insight into the differences between each model. For example ,measuring accuracy for levels of the dependency graph. For a given arc ,we define this distance as the number of arcs in the reverse path from the modifier of the arc to the root. For example ,the dependency arc from ROOT to is in Figure 1 would have a distance of 1 and the arc from hearing to A a distance of 3. Figure 6 plots the precision and recall of each system for arcs of varying distance to the root. Precision is equal to the percentage of dependency arcs in the predicted graph that are at a distance of d and are correct. Recall is the percentage of dependency arcs in the gold standard graph that are at a distance of d and were predicted.
 cise than MaltParser ,and vice versa for arcs further away from the root. This is probably MSTParser X  X  precision degrades as the distance to the root increases whereas Malt-near the middle. Dependency arcs further away from the root are usually constructed early in the parsing algorithm of MaltParser. Again a reduced likelihood of error propa-gation coupled with a rich feature representation benefits that parser substantially. Fur-thermore ,MaltParser tends to over-predict root modifiers ,which comes at the expense of its precision. This is because all words that the parser fails to attach as modifiers are automatically connected to the root ,as explained in Section 2.3. Hence ,low precision transition-based parser produces fragmented parses.
 its errors should be distributed evenly over the graph. For the most part this is true, modification (distance of 1) can be explained through the fact that this is typically a low-entropy decision X  X sually the parsing algorithm has to determine the main verb from a small set of possibilities. On the other end of the plot there is a slight downward trend for arcs of distance greater than 3 from the root. An examination of dependency length for predicted arcs shows that MSTParser predicts many more arcs of length 1 than MaltParser ,which naturally leads to over-predicting more arcs at larger distances from the root due to the presence of chains ,which in turn will lower precision for these arcs. In ambiguous situations ,it is not surprising that MSTParser predicts many length-1 dependencies ,as this is the most common dependency length across treebanks. Thus , whereas MaltParser pushes difficult parsing decisions higher in the graph ,MSTParser appears to push these decisions lower.
 sified as siblings if they represent syntactic modifications of the same word (i.e., i = i ).
In Figure 1 the arcs from the word is to the words hearing , scheduled ,and the period are all considered siblings under this definition. Figure 7 measures the precision and recall of each system relative to the number of predicted and gold standard siblings of each arc. There is not much to distinguish between the parsers on this metric. MSTParser is slightly more precise for arcs that are predicted with more siblings ,whereas MaltParser has slightly higher recall on arcs that have more siblings in the gold standard tree. Arcs closer to the root tend to have more siblings ,which ties this result to the previous ones. 4.3 Linguistic Factors as parts of speech and dependency types. However ,given the important typological differences that exist between languages ,as well as the diversity of annotation schemes used in different treebanks ,it is far from straightforward to compare these categories across languages. Nevertheless ,we have made an attempt to distinguish a few broad categories that are cross-linguistically identifiable ,based on the available documenta-tion of the treebanks used in the shared task. 214 iaries) ,nouns (including proper names) ,pronouns (sometimes also including deter-miners) ,adjectives ,adverbs ,adpositions (prepositions ,postpositions) ,and conjunctions (both coordinating and subordinating). For dependency types ,we have only managed to distinguish a general root category (for labels used on arcs from the artificial root, including either a generic label or the label assigned to predicates of main clauses ,which are normally verbs) ,a subject category ,and an object category (including both direct and indirect objects). Unfortunately ,we had to exclude many interesting types that could not be identified with high enough precision across languages ,such as adverbials , which cannot be clearly distinguished in annotation schemes that subsume them under a general modifier category ,and coordinate structures ,which are sometimes annotated with special dependency types ,sometimes with ordinary dependency types found also in non-coordinated structures.

This figure measures labeled dependency accuracy relative to the part of speech of the modifier word in a dependency relation. We see that MaltParser has slightly better accuracy for nouns and pronouns ,and MSTParser does better on all other categories ,in particular conjunctions. This pattern is consistent with previous results insofar as verbs and conjunctions are often involved in dependencies closer to the root that span longer distances ,whereas nouns and pronouns are typically attached to verbs and therefore occur lower in the graph and with shorter distances. Thus ,the average distance to the root is 3.1 for verbs and 3.8 for conjunctions ,but 4.7 for nouns and 4.9 for pronouns; and 1.6 for pronouns. Adverbs resemble verbs and conjunctions with respect to root distance (3.7) but group with nouns and pronouns for dependency length (2.3) ,so it appears that the former is more important here. Furthermore ,adverb modifiers have 2.4 siblings on average ,which is greater than the sibling average for conjunctions (2.1) , would be consistent with the graph in Figure 7.
 and 5.2 ,respectively ,a dependency length of 2.5/1.5 and a sibling average of 1.9/1.2 ,we would expect MaltParser to do better than MSTParser for these categories. Adpositions do tend to have a high number of siblings on average ,which could explain MSTParser X  X  performance on that category. However ,adjectives on average occur the furthest away from the root ,have the shortest dependency length ,and the fewest siblings. At present , we do not have an explanation for this behavior.
 node (mostly verbal predicates) ,and for subjects and objects. As already noted ,MST-
Parser has considerably better precision (and slightly better recall) for the root category, but MaltParser has an advantage for the nominal categories ,especially subjects. A pos-sible explanation for the latter result ,in addition to the length-based and graph-based factors invoked before ,is that MaltParser integrates labeling into the parsing process , which means that previously assigned dependency labels can be used as features.
This may sometimes be important to disambiguate subjects and objects ,especially in free-word order languages where a dependent X  X  position relative to the verb does not determine its syntactic role. 4.4 Discussion
The experiments in this section highlight the fundamental trade-off between global training and exhaustive inference on the one hand and expressive feature representa-tions on the other. Error propagation is an issue for MaltParser ,which typically performs worse on long sentences ,long dependency arcs ,and arcs higher in the graphs. But this is offset by the rich feature representation available to these models that result in better decisions for frequently occurring classes of arcs like short dependencies or subject is expected ,as the inference algorithm and feature representation should not prefer one type of arc over another.
 errors through the work of Sagae and Lavie (2006). However ,in that work an arc-based majority voting scheme was used that took only limited account of the properties of the words connected by a dependency arc (more precisely ,the overall accuracy of each parser for the part of speech of the dependent). The analysis in this work not only shows that the errors made by each system are different ,but that they are different in a way that can be predicted and quantified. This is an important step in parser development.
By understanding the strengths and weaknesses of each model we have gained insights towards new and better models for dependency parsing. 216 the strengths of each model ,we can perform two oracle experiments. Given the output of the two systems ,we can envision an oracle that can optimally choose which single parse or combination of sub-parses to predict as a final parse. For the first experiment the oracle is provided with the single best parse from each system ,say G = ( V , A )and
G = ( V , A ). The oracle chooses a parse that has the highest number of correctly pre-dicted labeled dependency attachments. In this situation ,the oracle labeled attachment score is 84 . 5%. In the second experiment the oracle chooses the tree that maximizes the number of correctly predicted dependency attachments ,subject to the restriction that the tree must only contain arcs from A  X  A . This can be computed by setting the weight of an arc to 1 if it is in the correct parse and in the set A set to negative infinity. One can then simply find the tree that has maximal sum of arc weights using directed spanning tree algorithms. This technique is similar to the parser voting methods used by Sagae and Lavie (2006). In this situation ,the oracle accuracy is 86 . 9%.
 for the individual systems. This indicates that there is still potential for improvement, just by combining the two existing models. More interestingly ,however ,we can use the analysis from this section to generate ideas for new models. Below we sketch some possible new directions: 1. Ensemble systems: The error analysis presented in this article could be used 2. Integrated/Hybrid systems: Rather than using an ensemble of several 3. Novel approaches: The theoretical analysis presented in this article reveals
In the next two sections we explore a model that falls into category 2. The system we propose uses a two-stage stacking framework ,where a second-stage parser conditions on the predictions of a first-stage parser during inference. The second-stage parser is also learned with access to the first-stage parser X  X  decisions and thus learns when to trust the first-stage parser X  X  predictions and when to trust its own. The method is not a traditional ensemble ,because the parsers are not learned independently of one another. 5. Integrated Models
As just discussed ,there are many conceivable ways of combining the two parsers , including more or less complex ensemble systems and voting schemes ,which only perform the integration at parsing time. However ,given that we are dealing with data-complementary models can learn from one another. In this article ,we propose to do this by letting one model generate features for the other in a stacked learning framework. dency parsing by McDonald (2006) ,who trained an instance of MSTParser using fea-tures generated by the parsers of Collins (1999) and Charniak (2000) ,which improved unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other NLP domains ,feature-based integration has been used by Taskar ,Lacoste-Julien ,and
Klein (2005) ,who trained a discriminative word alignment model using features de-rived from the IBM models ,by Florian et al. (2004) ,who trained classifiers on auxiliary data to guide named entity classifiers ,and by others.
 been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003) ,among method ,where the first-stage parser X  X  predictions replace ,rather than complement ,the gold standard annotation during training. Feature-based integration is also similar to parse reranking (Collins 2000) ,where one parser produces a set of candidate parses and a second-stage classifier chooses the most likely one. However ,feature-based in-tegration is not explicitly constrained to any parse decisions that the first-stage parser might make. Furthermore ,as only the single most likely parse is used from the first-stage model ,it is significantly more efficient than reranking ,which requires both com-putationally and conceptually more complex parsing algorithms (Huang and Chiang 2005). 5.1 Parser Stacking with Rich Features
As explained in Section 2 ,both models essentially learn a scoring function s : X where the domain X is different for the two models. For the graph-based model, X is possible configuration-transition pairs ( c , t ). But in both cases ,the input is represented by a k -dimensional feature vector f : X  X  R k . In a stacked parsing system we simply extend the feature vector for one model ,called the base model ,with a certain number of features generated by the other model ,which we call the guide model in this context.
The additional features will be referred to as guide features ,and the version of the base model trained with the extended feature vector will be called the guided model .The idea is that the guided model should be able to learn in which situations to trust the guide features ,in order to exploit the complementary strength of the guide model ,so that performance can be improved with respect to the base model.
 model can be described as follows. Assume as input a training set T = input sentences x t and corresponding gold standard dependency trees G train the guide model we use a cross-validation scheme and divide T into n different 218 and let M [ T ]( x ) be the result of parsing a new input sentence x with M [ T ]. Now ,consider a guide model C ,base model B ,and guided model B C . For each x in T ,define G x is the prediction of model C on training input x when C is trained on all the subsets of T ,except the one containing x . The reason for using this cross-validation scheme is that if C had been trained on all of T ,then G C x would not be representative of the types of errors that C might make when parsing sentence x . Using cross-validation in this way is similar to how it is used in parse reranking (Collins 2000). Now ,define a new training set of the form T = { ( x t , G C x input x is augmented with the cross-validation prediction of model C . Finally ,let
This means that ,for every sentence x  X  T , B C has access at training time to both the gold standard dependency graph G x and the graph G C x predicted by C .Thus, B to define guide features over G C x ,which can prove beneficial if features over G used to discern when parsing model C outperforms or underperforms parsing model
B . When parsing a new sentence x with B C , x is first parsed with model C [ T ] (this time trained on the entire training set) to derive an input x , G can be extracted also at parsing time. This input is then passed through model B 5. 2The Guided Graph-Based Model
The graph-based model ,MSTParser ,learns a scoring function s ( i , j , l ) dependencies. As described in Section 3.2 ,dependency arcs (or pairs of arcs) are repre-sented by a high dimensional feature vector f ( i , j , l ) feature vector over properties of the arc as well as the surrounding input (McDonald,
Crammer ,and Pereira 2005; McDonald ,Lerman ,and Pereira 2006). For the guided graph-based model ,which we call MST Malt ,this feature representation is modified to include an additional argument G Malt x ,which is the dependency graph predicted by
MaltParser on the input sentence x . Thus ,the new feature representation will map an arc and the entire predicted MaltParser graph to a high dimensional feature representation, the MaltParser output. The specific features used by MST  X  dependency to allow the guided model to learn weights relative to different surface syntactic environments. Features that include the arc label l are only included in the second-stage arc-labeler. Though MSTParser is capable of defining features over pairs of arcs ,we restrict the guide features to single arcs as this resulted in higher accuracies during preliminary experiments. 5.3 The Guided Transition-Based Model
The transition-based model ,MaltParser ,learns a scoring function s ( c , t ) figurations and transitions. The set of training instances for this learning problem is the set of pairs ( c , t )suchthat t is the correct transition out of c in the transition sequence that derives the correct dependency graph G x for some sentence x in the training set
T . As described in Section 3.3 ,each training instance ( c , t ) is represented by a feature configuration c .
 are extended to triples ( c , t , G MST x ) ,where G MST x the graph-based MSTParser for the sentence x to which the configuration c belongs. We d e fi n e m additional guide features ,based on properties of G feature vector accordingly to f ( c , t , G MST x )  X  R k + m are given in Table 5. Unlike MSTParser ,features are not explicitly defined to conjoin guide features with part-of-speech features. These features are implicitly added through the polynomial kernel used to train the SVM. 6. Integrated Parsing Experiments
In this section ,we present an experimental evaluation of the two guided models fol-lowed by a comparative error analysis including both the base models and the guided models. The data sets used in these experiments are identical to those used in Section 4.
The guided models were trained according to the scheme explained in Section 5 ,with two-fold cross-validation when parsing the training data with the guide parsers. Pre-liminary experiments suggested that cross-validation with more folds had a negligible impact on the results. Models are evaluated by their labeled attachment score on the test set using the evaluation software from the CoNLL-X shared task with default settings.
Statistical significance was assessed using Dan Bikel X  X  randomized parsing evaluation comparator with the default setting of 10,000 iterations. 6.1 Results
Table 6 shows the results ,for each language and on average ,for the two base models (MST ,Malt) and for the two guided models (MST Malt ,Malt combination scores based on both by taking the best graph or the best set of arcs relative to the gold standard ,as discussed in Section 4.4. First of all ,we see that both guided models show a consistent increase in accuracy compared to their base model, even though the extent of the improvement varies across languages from about half a percentage point (Malt MST on Chinese) up to almost four percentage points (Malt 220
Slovene). 11 It is thus quite clear that both models have the capacity to learn from features generated by the other model. However ,it is also clear that the graph-based MST model shows a somewhat larger improvement ,both on average and for all languages except
Czech ,German ,Portuguese ,and Slovene. Finally ,given that the two base models had the best performance for these data sets at the CoNLL-X shared task ,the guided models achieve a substantial improvement of the state of the art. tically significant difference between the two base models ,they are both outperformed by Malt MST (p &lt; 0.0001) ,which in turn has significantly lower accuracy than MST (p &lt; 0.0005).
 parsers in the spirit of pipeline iteration (Hollingshead and Roark 2007). For example, one could start with a Malt model ,use it to train a guided MST as the guide to train a Malt MST found that accuracy did not increase significantly and in some cases decreased slightly.
This was true regardless of which parser began the iterative process. In retrospect ,this result is not surprising. Because the initial integration effectively incorporates knowl-edge from both parsing systems ,there is little to be gained by adding additional parsers in the chain. 6. 2Error Analysis
The experimental results presented so far show that feature-based integration (stacking) is a viable approach for improving the accuracy of both graph-based and transition-based models for dependency parsing ,but they say very little about how the integration benefits the two models and what aspects of the parsing process are improved as a the error analysis presented in Section 4 ,but include both integrated models into the statistics for labeled attachment over all 13 languages together.
 intervals (1 X 10 ,11-20 ,etc.). As mentioned earlier ,Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation. The guided models ,Malt parser over the entire range of sentence lengths. However ,except for the two extreme data points (0 X 10 and 51 X 60) there is also a slight tendency for Malt for longer sentences (relative to its base model) and for MST short sentences (relative to its base model). Thus ,whereas most of the improvement for the guided parsers seems to come from a higher accuracy in predicting arcs in general, there is also some evidence that the feature-based integration allows one parser to exploit the strength of the other.
 lengths (predicted arcs for precision ,gold standard arcs for recall). With respect to recall , the guided models appear to have a slight advantage over the base models for short and medium distance arcs. With respect to precision ,however ,there are two clear patterns.
First ,the graph-based models have better precision than the transition-based models when predicting long arcs ,as discussed earlier. Secondly ,both the guided models have better precision than their base model and ,for the most part ,also their guide model.
In particular MST Malt outperforms MST for all dependency lengths and is comparable to Malt for short arcs. More interestingly ,Malt MST outperforms both Malt and MST for arcs up to length 9 ,which provides evidence that Malt MST 222 trust the guide features from MST for longer dependencies (those greater than length 4) and its own base features for shorter dependencies (those less than or equal to length 4). However ,for dependencies of length greater than 9 ,the performance of Malt to degrade. Because the absolute number of dependencies of length greater than 9 in the training sets is relatively small ,it might be difficult for Malt guide parser in these situations. Interestingly ,both models seem to improve most in the medium range (roughly 8 X 12 words) ,although this pattern is clearer for MSTParser than for MaltParser.

Again ,we find the clearest patterns in the graphs for precision ,where Malt has very low precision near the root but improves with increasing depth ,whereas MST shows the opposite trend ,as observed earlier. Considering the guided models ,it is clear that
Malt MST improves in the direction of its guide model ,with a five-point increase in precision for dependents of the root and smaller improvements for longer distances (where its base model is most accurate). Similarly ,MST largest in the range where its base model is inferior to Malt (roughly distances of 2 X  6) and is always superior to its base model. This again indicates that the guided models are learning from their guide models as they improve the most in situations where the base model has inferior accuracy.
 earlier ,we see that MST does better than Malt for all categories except nouns and pronouns. But we also see that the guided models in all cases improve over their base model and ,in most cases ,also over their guide model. The general trend is that MST improves more than Malt ,except for adjectives and conjunctions ,where Malt has a greater disadvantage from the start and therefore benefits more from the guide features.
The general trend is that the parser with worse performance for a particular part-of-speech tag improves the most in terms of absolute accuracy (5 out of 7 cases) ,again suggesting that the guided models are learning when to trust their guide features. The exception here is verbs and adverbs ,where MST has superior performance to Malt ,but MST Malt has a larger increase in accuracy than Malt MST .
 and root distance ,it is interesting to note that the guided models often improve even in situations where their base models are more accurate than their guide models. This suggests that the improvement is not a simple function of the raw accuracy of the guide model but depends on the fact that labeled dependency decisions interact in inference algorithms for both graph-based and transition-based parsing systems. Thus ,if a parser can improve its accuracy on one class of dependencies (for example ,longer ones) ,then we can expect to see improvements on all types of dependencies X  X s we do. 6.3 Discussion predicting arcs in general ,which results in better performance regardless of sentence length ,dependency length ,or dependency depth. However ,there is strong evidence that MST Malt improves in the direction of Malt ,with a slightly larger improvement compared to its base model for short sentences and short dependencies (but not for deep 224 dependencies). Conversely ,Malt MST improves in the direction of MST ,with a larger improvement for long sentences and for dependents of the root.
 integration. The likely explanation is the previously mentioned interaction between different dependency decisions at inference time. Because inference in MST is exact (or nearly exact) ,an improvement in one type of dependency has a good chance of influencing the accuracy of other dependencies ,whereas in the transition-based model , where inference is greedy ,some of these additional benefits will be lost because of error propagation. This is reflected in the error analysis in the following recurrent pattern: Where Malt does well ,Malt MST does only slightly better. But where MST is good,
MST Malt is often significantly better. Furthermore ,this observation easily explains the limited increases in accuracy of words with verb and adverb modifiers that is observed in Malt MST relative to MST Malt (Table 7) as these dependencies occur close to the root and have increased likelihood of being affected by error propagation.
 by the systems. Although both Malt and MST use discriminative algorithms ,Malt uses a batch learning algorithm (SVM) and MST uses an on-line learning algorithm (MIRA).
If the original rich feature representation of Malt is sufficient to separate the training data ,regularization may force the weights of the guided features to be small (as they are not needed at training time). On the other hand ,an on-line learning algorithm will recognize the guided features as strong indicators early in training and give them a high weight as a result. Frequent features with high weight early in training tend to have the most impact on the final classifier due to both weight regularization and averaging. This is in fact observed when inspecting the weights of MST Malt in Section 4.4 ,we see that there should be room for further improvement ,as the best guided parser (MST Malt ) does not quite reach the level of the graph selection oracle, let alone that of the arc selection oracle. Further exploration of the space of possible
As already noted ,there are several recent developments in data-driven dependency parsing ,which can be seen as targeting the specific weaknesses of traditional graph-based and transition-based models ,respectively. For graph-based parsers ,McDonald and Pereira (2006) ,Hall (2007) ,Nakagawa (2007) ,and Smith and Eisner (2008) attempt to overcome the limited feature scope of graph-based models by adding global features in conjunction with approximate inference. Additionally ,Riedel and Clarke (2006) and
Martins ,Smith ,and Xing (2009) integrate global features and maintain exact inference through integer linear programming solutions. For transition-based models ,the trend is to alleviate error propagation by abandoning greedy ,deterministic inference in fa-vor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson 2007a ,2007b) or conditional (Duan ,Zhao , and Xu 2007; Johansson and Nugues 2007). In addition ,Zhang and Clark (2008) has proposed a learning method for transition-based parsers based on global optimization similar to that traditionally used for graph-based parsers ,albeit only with approxi-mate inference through beam search ,and Huang and Sagae (2010) has shown how a subclass of transition-based parsers can be tabularized to permit the use of dynamic programming.
 served errors and algorithmic expectations ,is whether it is possible to characterize the errors of a new parsing system simply by analyzing its theoretical properties. This is a difficult question to answer. Consider a parsing system that uses greedy inference. One can speculate that it will result in error propagation and ,as a result ,a large number complex global decisions ,such a system might not suffer from error propagation. This is because the early local decisions are made correctly. Furthermore ,saying something difficulty when parsing certain phenomena across languages. Ultimately ,this is an empirical question. What we have shown here is that ,on a number of data sets ,our algorithmic expectations about two widely used dependency parsing paradigms are confirmed. 7. Conclusion
In this article ,we have shown that the two dominant approaches to data-driven depen-dency parsing X  X lobal ,exhaustive ,graph-based models and local ,greedy ,transition-based models X  X ave distinctive error distributions despite often having very similar parsing accuracy overall. We have demonstrated that these error distributions can be explained by theoretical properties of the two models ,in particular related to the funda-mental tradeoff between global learning and inference ,traditionally favored by graph-based parsers ,and a rich feature space ,typically found in transition-based parsers.
Based on this analysis ,we have proposed new directions of research on data-driven dependency parsing ,some of which are already beginning to be explored.
 integrated by letting one model learn from features generated by the other ,using the technique known as stacking in the machine learning community. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model ,which leads to a significant advancement of the state of the art in data-driven dependency parsing. Moreover ,a comparative error analysis reveals that the improvements are predictable from the same theoretical properties identified in the initial error analysis ,such as the tradeoff between global learning and inference ,on the one hand ,and rich feature representations ,on the other.
On a more general note ,we believe that this shows the importance of careful error analysis ,informed by theoretical predictions ,for the further advancement of data-driven methods in natural language processing.
 Acknowledgments References 226 228
