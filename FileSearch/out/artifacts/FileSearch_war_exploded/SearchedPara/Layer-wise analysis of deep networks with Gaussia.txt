 Local learning machines such as nearest neighbors classifie rs, radial basis function (RBF) kernel machines or linear classifiers predict the class of new data p oints from their neighbors in the input space. A limitation of local learning machines is that they c annot generalize beyond the notion has more variations (ups and downs) than the number of labele d samples available. This situation typically occurs on problems where an instance  X  let X  X  say, a handwritten digit  X  can take various forms due to irrelevant variation factors such as its positi on, its size, its thickness and more complex deformations. These multiple factors of variation can grea tly increase the complexity of the learning problem (Bengio, 2009).
 higher-level representation where regularities of higher order than simple continuity in the input space can be expressed. Engineered feature extractors, non local kernel machines (Zien et al., 2000) or deep networks (Rumelhart et al., 1986; LeCun et al., 1998; Hinton et al., 2006; Bengio et al., 2007) can implement these more complex regularities. Deep networ ks implement them by distorting the input space so that initially distant points in the input spa ce appear closer. Also, their multilayered nature acts as a regularizer, allowing them to share at a give n layer features computed at the previous layer (Bengio, 2009). Understanding how the representatio n is built in a deep network and how to train it efficiently received a lot of attention (Goodfellow et al., 2009; Larochelle et al., 2009; Erhan et al., 2010). However, it is still unclear how their nice rep resentation emerges from their complex structure, in particular, how the representation evolves f rom layer to layer.
 The main contribution of this paper is to introduce an analys is based on RBF kernels and on the kernel principal component analysis (kPCA, Sch  X  olkopf et al., 1998) that can capture and quantify the layer-wise evolution of the representation in a deep networ k. In practice, for each layer 1  X  l  X  L of the deep network, we take a small labeled dataset D , compute its image D ( l ) at the layer l of the deep network and measure what dimensionality the local mode l built on top of D ( l ) must have in order to solve the learning problem with a certain accuracy. Figure 1: As we move from the input to the output of the deep net work, better representations of the learning problem are built. We measure this improvement with the layer-wise RBF analysis presented in Section 2 and Section 3.2. This analysis relate s the prediction error e ( d ) to the di-mensionality d of a local model built at each layer of the deep network. As the data is propagated through the deep network, lower errors are obtained with low er-dimensional local models. The plots on the right illustrate this dynamic where the thick gray arr ows indicate the forward path of the deep network and where d We apply this novel analysis to a multilayer perceptron (MLP ), a pretrained multilayer perceptron (PMLP) and a convolutional neural network (CNN). We observe in each case that the error and the dimensionality of the local model decrease as we propagate t he dataset through the deep network. This reveals that the deep network improves the representat ion of the learning problem layer after layer. This progressive layer-wise simplification is illus trated in Figure 1. In addition, we observe that the CNN and the PMLP tend to postpone the discrimination to the last layers, leading to more transferable features and better-generalizing represent ations than for the simple MLP. This result suggests that the structure of a deep network, by enforcing a separation of concerns between low-level generic features and high-level task-specific featur es, has an important role to play in order to build good representations. We would like to quantify the complexity of a learning proble m p ( y | x ) where samples are drawn independently from a probability distribution p ( x, y ) . A simple way to do it is to measure how many degrees of freedom (or dimensionality d ) a local model must have in order to solve the learning problem with a certain error e . This analysis relates the dimensionality d of the local model to its prediction error e ( d ) .
 number of samples given to the learning machine, (2) the numb er of required hidden nodes of a neural network (Murata et al., 1994), (3) the number of suppo rt vectors of a SVM or (4) the number of leading kPCA components of the input distribution p ( x ) used in the model. The last option is chosen for the following two reasons: First, the kPCA components are added cumulatively to the pre diction model as the dimensionality of the model increases, thus offering stability, while in the c ase of support vector machines, previously chosen support vectors might be dropped in favor of other sup port vectors in higher-dimensional models.
 Second, the leading kPCA components obtained with a finite an d typically small number of samples n are similar to those that would be obtained in the asymptotic case where p ( x, y ) is fully observed ( n  X   X  ). This property is shown by Braun (2006) and Braun et al. (200 8) in the case of a single kernel, and by extension, in the case of a finite set of kernels .
 This last property is particularly useful since p ( x, y ) is unknown and only a finite number of observa-tions are available. The analysis presented here is strongl y inspired from the relevant dimensionality Figure 2: Illustration of the RBF analysis on a toy dataset of 12 samples. As we add more and more leading kPCA components, the model becomes more flexible, cr eating a better decision boundary. Note that with four leading kPCA components out of the 12 kPCA components, all the samples are already classified perfectly. dimensional toy example. In the next lines, we present the co mputation steps required to estimate the error as a function of the dimensionality.
 Let { ( x an indicator vector having value 1 at the index corresponding to the class of x dataset. We compute the kernel matrix K associated to the dataset: The kPCA components u eigenvectors u nitude: Let  X  U = ( u composition. We fit a linear model  X   X  that maps the projection on the d leading components of the training data to the log-likelihood of the classes where  X  is a matrix of same size as Y and where the exponential function is applied element-wise . The predicted class log-probability log( X  y ) of a test point ( x, y ) is computed as where k ( x, X ) is a matrix of size 1  X  n computing the similarities between the new point and each training point and where C is a normalization constant. The test error is defined as: The training and test error can be used as an approximation bo und for the asymptotic case n  X   X  where the data would be projected on the real eigenvectors of the input distribution. In the next sections, the training and test error are depicted respecti vely as dotted and solid lines in Figure 3 and as the bottom and the top of error bars in Figure 4. For each dim ension, the kernel scale parameter  X  that minimizes e ( d ) is retained, leading to a different kernel for each dimensio nality. The rationale for taking a different kernel for each model is that the optim al scale parameter typically shrinks as more leading components of the input distribution are obser ved. networks and the role of the structure for postponing discri mination), we consider three deep net-works of interest, namely a convolutional neural network (C NN), a multilayer perceptron (MLP) and a variant of the multilayer perceptron pretrained in an u nsupervised fashion with a deep belief network (PMLP). These three deep networks are chosen in orde r to evaluate how the two types of regularizers implemented respectively by the CNN and the PM LP impact on the evolution of the representation layer after layer. We describe how they are b uilt, how they are trained and how they are analyzed layer-wise with the RBF analysis described in S ection 2.
 The multilayer perceptron (MLP) is a deep network obtained by alternating linear trans formations and element-wise nonlinearities. Each layer maps an input v ector of size m into an output vector of size n and consists of (1) a linear transformation linear weight matrix of size n  X  m learned from the data and (2) a non-linearity applied elemen t-wise to the output of the linear transformation. Our implementat ion of the MLP maps two-dimensional images of 28  X  28 pixels into a vector of size 10 (the 10 possible digits) by applying successively the following functions: The pretrained multilayer perceptron (Hinton et al., 2006) that we abbreviate PMLP in this paper is a variant of the MLP where weights are initialized with a de ep belief network (DBN, Hinton et al., 2006) using an unsupervised greedy layer-wise pretr aining procedure. This particular weight problem than the simple MLP.
 The convolutional neural network (CNN, LeCun et al., 1998) is a deep network obtained by al-ternating convolution filters y = convolve a  X  b { x 1 , . . . , x m } the convolution filters w feature map by a factor two. Our implementation maps images o f 32  X  32 pixels into a vector of size 10 (the 10 possible digits) by applying successively the follo wing functions: The CNN is inspired by the structure of biological visual sys tems (Hubel and Wiesel, 1962). It combines three ideas into a single architecture: (1) only lo cal connections between neighboring pixels are allowed, (2) the convolution operator applies th e same filter over the whole feature map and (3) a pooling mechanism at the top of each convolution filt er adds robustness to input distortion. These mechanisms act as a regularizer on images and other typ es of sequential data, and learn well-generalizing models from few data points. 3.1 Training the deep networks Each deep network is trained on the MNIST handwriting digit r ecognition dataset (LeCun et al., 1998). The MNIST dataset consists of predicting the digit 0  X  9 from scanned handwritten digits of 28  X  28 pixels. We partition randomly the MNIST training set in thre e subsets of 45000, 5000 and 10000 samples that are respectively used for training the de ep network, selecting the parameters of the deep network and performing the RBF analysis.
 We consider three training procedures: These procedures are chosen in order to assess the forming of good representations in deep networks and to test the role of the structure of deep networks on diffe rent aspects of learning, such as the effectiveness of random projections, the transferability of features from one task to another and the generalization to new samples of the same distribution. 3.2 Applying the RBF analysis to deep networks In this section, we explain how the RBF analysis described in Section 2 is applied to analyze layer-wise the deep networks presented in Section 3.
 Let f = f the 10000 samples of the MNIST dataset on which the deep network hasn X  X  been trained. For each layer, we build a new dataset D ( l ) corresponding to the mapping of the original dataset D to the l first layers of the deep network. Note that by definition, the i ndex zero corresponds to the raw input data (mapped through zero layers): Then, for each dataset D (0) , . . . , D ( L ) we perform the RBF analysis described in Section 2. We use n = 2500 samples for computing the eigenvectors and the remaining 7500 samples to estimate the dimensionality of the model e ( d ) . A typical evolution of e ( d ) is depicted in Figure 1. The goal of this analysis is to observe the evolution of e ( d ) layer after layer for the deep networks and training procedures presented in Section 3 and to test th e two hypotheses formulated in Section 1 (the progressive emergence of good representations in deep networks and the role of the structure for postponing discrimination). The interest of using a loc al model to solve the learning problem is that the local models are blind with respect to possibly be tter representations that could be ob-tained in previous or subsequent layers. This local scoping property allows for fine isolation of the representations in the deep network. The need for local scop ing also arises when  X  X ebugging X  deep architectures. Sometimes, deep architectures perform rea sonably well even when the first layers do something wrong. This analysis is therefore able to detect t hese  X  X ugs X .
 The size n of the dataset is selected so that it is large enough to approx imate well the asymptotic case ( n  X   X  ) but also be small enough so that computing the eigendecompo sition of the kernel matrix of size n  X  n is fast. We choose a set of scale parameters for the RBF kernel corresponding between pairs of data points. Layer-wise evolution of the error e ( d ) is plotted in Figure 3 in the supervised training case. The layer-wise evolution of the error when d is fixed to 16 dimensions is plotted in Figure 4. Both figures capture the simultaneous reduction of error and dimensiona lity performed by the deep network when dimensions is sufficient to build a good model of the target ta sk. Figure 3: Layer-wise evolution of the error e ( d ) when the deep network has been trained on the target task. The solid line and the dotted line represent res pectively the test error and the training error. As the data distribution is mapped through more and mo re layers, more accurate and lower-dimensional models of the learning problem can be obtained.
 From these results, we first demonstrate some properties of d eep networks trained on an  X  X symp-totically X  large number of samples. Then, we demonstrate th e important role of structure in deep networks. 4.1 Asymptotic properties of deep networks When the deep network is trained on the target task with an  X  X sy mptotically X  large number of sam-ples ( 45000 samples) compared to the number of dimensions of the local mo del, the deep network builds representations layer after layer in which a low numb er of dimensions can create more accu-rate models of the learning problem.
 This asymptotic property of deep networks should not be thou ght of as a statistical superiority of deep networks over local models. Indeed, it is still possibl e that a higher-dimensional local model applied directly on the raw data performs as well as a local mo del applied at the output of the deep network. Instead, this asymptotic property has the followi ng consequence: Despite the internal complexity of deep networks a local int erpretation of the representation is pos-sible at each stage of the processing. This means that deep ne tworks do not explode the original data distribution into a statistically intractable distributi on before recombining everything at the output, but instead, apply controlled distortions and reductions o f the input space that preserve the statistical tractability of the data distribution at every layer. 4.2 Role of the structure of deep networks We can observe in Figure 4 (left) that even when the convoluti onal neural network (CNN) and the pretrained MLP (PMLP) have not received supervised trainin g, the first layers slightly improve the representation with respect to the target task. On the other hand, the representation built by a simple MLP with random weights degrades layer after layer. This obs ervation highlights the structural prior encoded by the CNN: by convolving the input with severa l random convolution filters and subsampling subsequent feature maps by a factor two, we obta in a random projection of the input data that outperforms the implicit projection performed by an RBF kernel in terms of task relevance. it is observed that training the deep network while keeping r andom weights in the first layers still allows for good predictions by the subsequent layers. In the case of the PMLP, the successive layers progressively disentangle the factors of variation (Hinto n and Salakhutdinov, 2006; Bengio, 2009) and simplify the learning problem.
 We can observe in Figure 4 (middle) that the phenomenon is eve n clearer when the CNN and the PMLP are trained on an alternate task: they are able to create generic features in the first layers that transfer well to the target task. This observation sugg ests that the structure embedded in the CNN and the PMLP enforces a separation of concerns between th e first layers that encode low-level features, for example, edge detectors, and the last la yers that encode high-level task-specific Figure 4: Evolution of the error e ( d dimensions. The top and the bottom of the error bars represen t respectively the test error and the training error of the local model.
 Figure 5: Leading components of the weights (receptive field s) obtained in the first layer of each learned by the MLP. The first component of the MLP trained on th e alternate task dominates all other components and prevents good transfer on the target ta sk. features. On the other hand, the standard MLP trained on the a lternate task leads to a degradation of representations. This degradation is even higher than in th e case of random weights, despite all the prior knowledge on pixel neighborhood contained implicitl y in the alternate task.
 Figure 5 shows that the MLP builds receptive fields that are sp atially informative but dissimilar between the two tasks. The fact that receptive fields are diff erent for each task indicates that the MLP tries to discriminate already in the first layers. The abs ence of a built-in separation of concerns between low-level and high-level feature extractors seems to be a reason for the inability to learn chines is in general not appropriate and supports the recent success of transfer learning on restricted portions of the deep network (Collobert and Weston, 2008; We ston et al., 2008) or on structured deep networks (Mobahi et al., 2009).
 When the deep networks are trained on the target task, the CNN a nd the PMLP solve the problem differently as the MLP. In Figure 4 (right), we can observe th at the CNN and the PMLP tend to postpone the discrimination to the last layers while the MLP starts to discriminate already in the first layers. This result suggests that again, the structure cont ained in the CNN and the PMLP enforces a separation of concerns between the first layers encoding lo w-level generic features and the last better generalization of the CNN and PMLP observed respecti vely in (LeCun et al., 1998; Hinton PMLP must be unsupervised and not supervised in order to buil d well-generalizing representations. We present a layer-wise analysis of deep networks based on RB F kernels. This analysis estimates for each layer of the deep network the number of dimensions th at is necessary in order to model well a learning problem based on the representation obtained at t he output of this layer. We observe that a properly trained deep network creates repr esentations layer after layer in which a more accurate and lower-dimensional local model of the lear ning problem can be built. We also observe that despite a steady improvement of represe ntations for each architecture of interest (the CNN, the MLP and the pretrained MLP), they do not solve th e problem in the same way: the CNN and the pretrained MLP seem to separate concerns by build ing low-level generic features in the first layers and high-level task-specific features in the last layers while the MLP does not enforce this separation. This observation emphasizes the limitati ons of black box transfer learning and, more generally, of black box training of deep architectures.
 Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greed y layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19 , pages 153 X 160. MIT Press, 2007. Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning , 2(1):1 X 127, 2009.
 Mikio L. Braun. Accurate bounds for the eigenvalues of the ke rnel matrix. Journal of Machine Learning Research , 7:2303 X 2328, Nov 2006.
 Mikio L. Braun, Joachim Buhmann, and Klaus-Robert M  X  uller. On relevant dimensions in kernel feature spaces. Journal of Machine Learning Research , 9:1875 X 1908, Aug 2008.
 R. Collobert and J. Weston. A unified architecture for natura l language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML , 2008.
 Dumitru Erhan, Yoshua Bengio, Aaron C. Courville, Pierre-A ntoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep le arning? Journal of Machine Learning Research , 11:625 X 660, 2010.
 Ian Goodfellow, Quoc Le, Andrew Saxe, and Andrew Y. Ng. Measu ring invariances in deep net-works. In Advances in Neural Information Processing Systems 22 , pages 646 X 654, 2009. G. E. Hinton and R. R. Salakhutdinov. Reducing the dimension ality of data with neural networks. Science , 313(5786):504 X 507, July 2006.
 Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast l earning algorithm for deep belief nets. Neural Comput. , 18(7):1527 X 1554, 2006.
 D. H. Hubel and T. N. Wiesel. Receptive fields, binocular inte raction and functional architecture in the cat X  X  visual cortex. The Journal of physiology , 160:106 X 154, January 1962.
 Kevin Jarrett, Koray Kavukcuoglu, Marc X  X urelio Ranzato, a nd Yann LeCun. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV X 09) . IEEE, 2009.
 Hugo Larochelle, Yoshua Bengio, J  X  er  X  ome Louradour, and Pascal Lamblin. Exploring strategies fo r training deep neural networks. J. Mach. Learn. Res. , 10:1 X 40, 2009. ISSN 1532-4435.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-bas ed learning applied to document recognition. Proceedings of the IEEE , 86(1):2278 X 2324, November 1998.
 Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep lea rning from temporal coherence in video. In L  X  eon Bottou and Michael Littman, editors, Proceedings of the 26th International Conference on Machine Learning , pages 737 X 744, Montreal, June 2009. Omnipress.
 Noboru Murata, Shuji Yoshizawa, and Shun ichi Amari. Networ k information criterion -determin-ing the number of hidden units for an artificial neural networ k model. IEEE Transactions on Neural Networks , 5:865 X 872, 1994.
 Genevieve B. Orr and Klaus-Robert M  X  uller, editors. Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop , volume 1524 of Lecture Notes in Computer Science , 1998. Springer.
 M. A. Ranzato, Fu J. Huang, Y. L. Boureau, and Y. LeCun. Unsupe rvised learning of invariant fea-ture hierarchies with applications to object recognition. In Computer Vision and Pattern Recog-nition, 2007. CVPR  X 07. IEEE Conference on , pages 1 X 8, 2007. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning re presentations by back-propagating errors. Nature , 323(6088):533 X 536, 1986.
 Bernhard Sch  X  olkopf, Alexander Smola, and Klaus-Robert M  X  uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Comput. , 10(5):1299 X 1319, 1998.
 Jason Weston, Fr  X  ed  X  eric Ratle, and Ronan Collobert. Deep learning via semi-sup ervised embedding.
In ICML  X 08: Proceedings of the 25th international conference on Machine learning , pages 1168 X  1175, 2008.
 Alexander Zien, Gunnar R  X  atsch, Sebastian Mika, Bernhard Sch  X  olkopf, Thomas Lengauer, and
Klaus-Robert M  X  uller. Engineering support vector machine kernels that rec ognize translation ini-tiation sites. Bioinformatics , 16(9):799 X 807, 2000.
