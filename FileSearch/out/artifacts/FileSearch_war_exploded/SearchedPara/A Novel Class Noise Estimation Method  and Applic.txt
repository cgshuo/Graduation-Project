 classification results no matter wh at machine learning method is classification in the presence of random noise on the class labels, which we call class noise. To model class noise, a class noise rate is normally defined as a small independent probability of the class labels being inverted on the whole set of training data. In this paper, we propose a method to es timate class noise rate at the level of individual samples in real data. Based on the estimation result, we propose two approaches to handle class noise. The first technique is based on modifying a given surrogate loss function. The second technique eliminates class noise by sampling. Furthermore, we prove that the optimal hypothesis on the noisy distribution can approximate the optimal hypothesis on the clean distribution using both approaches. Our methods achieve over 87% accuracy on a synthetic non-separable dataset even when 40% of the labels are inverted. Comparisons to other algorithms show that our methods outperform state-of-the-art approaches on several benchmark datasets in di fferent domains with different noise rates. I.5.1 [ PATTERN RECOGNITION ]: Models---Statistical; Algorithms, Design, Experimentation. Class Noise, Learning with Noise, Noise Elimination. A typical machine learning method uses a classifier learned from a labeled dataset (i.e., the training data) to predict the class labels of new samples (i.e., the testing data). In most of the classification applications, the labels of the training data are assumed correct. However, real-world datasets often contain noise which may occur either in the feature set of the data, referred to as the attribute noise , or in the labels of the data, referred to as the class noise . Many studies have focused on deal ing with attribute noise since it is quite common in data mining a pplications and m achine learning tasks. Researchers such as [1, 2] have indicated that class noise can potentially be more detrimental than attribute noise. The importance of addressing the class noise problem is not limited to its adverse impact on classifi cation performance [1]. We must point out that class noise is unavoidable in many real world applications including disease pred iction in medical applications [3], food labeling for the food industry [4], and manual data labeling in many natural language processing applications [5]. To handle class noise in learning algorithms, the first issue is how noise rate. The second issue is how to train a classifier knowing that the training data contains class noise. Generally speaking, there are two ty pes of strategies to deal with class noise. The first entails learning with noise data and the second is based on noise elimination. In the learning with noise method, each training sample is assigned a weight based on an The learning algorithm will consider the class noise probability while learning from the original noisy training data [6]. Unfortunately, this method requir es a priori knowledge of the class noise rate in the training data. In the noise elimination method , attempts are first made to detect and remove erroneous data from th e training set [7, 8, 9]. While this method can reduce the rate of class noise in the training data, it can also lead to the overall reduction of training samples. Because of the reduced training data, this method may lead to even worse results than when it is used for small training sets. The performance of both methods relies on an appropriate estimation of class noise rate. In this paper, we first propose a novel method to estimate the noise ra te of the training data at the level of individual samples and the estimation is done based on the sum of the Rademacher distribution [10] using k-Nearest Neighbors (k-NN) graphs. We then present two general methods to incorporate our proposed class noise estimation method into the two types of class noise handling strategies to reduce the impact introduce a sampling-based method to select high quality training data rather than to identify low quality data for elimination. The rest of the paper is organize d as follows. Section 2 provides a review of related works and basi c definitions. Section 3 presents the problem setup and some ba ckground information. Section 4 presents our proposed class noi se estimation method. Section 5 describes the methods to incorporate the proposed estimation into learning with noise strategy and the class noise elimination strategy. Section 6 presents performance evaluations based on both synthetic and real-world public datasets in different domains. Section 7 provides the conclusion and future work. Identifying class noise is an importa nt subject in machine learning. Previous work covers both the theo retical [11] and the application aspects [12, 13] of this topic. In this section, we briefly introduce these works from three perspectives: the source of class noise, the handling of class noise, and the application of class noise. Class noises exist for different reasons. When used for disease prediction in medical applications [3], training data contains a probability of false positive or negative because data comes from medical experiments. In other wo rds, class noises naturally exist and cannot be avoided. Food la beling for the food industry [4] also faces class noise problem. As shown by [4], because some food has high price than others, such as beef has a higher price than mutton, miss-labeling is an aforethought to achieve more benefit. The manual labeling of data in many natural language processing applications [5] natura lly contains cla ss noise because there is always a possibility of inter-annotator inconsistency. Given an x , being the set of features of a sample,  X  X  is the observed can be categorized into three different models based the dependency of noise to y and p [14]. The most common model used is the Noise Completely at Random Model [11, 15, 16]. In this model, class noise rate of a sample is completely random and independent of the labels and the feature set. Thus, an observed The other models assume dependency to either x or y [17-21]. The theoretical discussion about cl ass noise and learning was first proposed by Angluin and Laird in 1988 [11]. In their work, all instances of the labeled data for binary classification have a uniform probability )2/ X  X 0,1 X  of being inverted. This is referred to as the random classification noise. Class noises are typically assumed to be stochastic in algorithms that can handle class noises. The work by [6] assumes a learned noise rate from prior knowledge and every sample is given the same probability, a simple assump tion that may not be reasonable in all scenarios. The Cut Edge We ight statistic [8] also requires a prior assumed noise rate. This method uses the prior probability as a hypothesis to test if the training sample satisfies the null hypothesis. The method also requires the neighbors of a training sample to follow the central limit theorem, which is not reasonable because the set of nei ghbors are too small. Other works simply did not consider noi se rate [5, 7, 9, 22]. There are two basic categories of strategies to dealing with class noise in training data: learning with class noise [5, 6, 16] and class noise elimination [7, 8, 9]. The learning with noise strategy approximates a distribution of noiseless training data using the distribution of the original training data with class noise and a priori knowledg e about the class noise [6]. The problem, however, is that a priori knowledge of th e class noise is not generally available, limiting the applicability of this method. Li [16] use the Kernel Fisher method to estimate the class noise. Then the estimation is used to achieve a ro bust algorithm to tolerate noise. The class noise elimination strategy attempts to detect the samples with high noise probabilities and remove them from the training set. There are different methods to detect class noise that can be categorized as classification-based methods and graph-based methods. The classification-base d method was first proposed by Brodley in 1999 [7]. He used K-fold cross-validation to split the data to classify the remaining data. If the classification result for a considered class noise and is rem oved. Zhu et al. proposed a more efficient algorithm using the same ideal and made it suitable for large datasets [22]. Zhu also proposed a cost-sensitive approach based on K-fold cross-validation [1]. Sluba employed a 10-fold cross-validation to detect class noise [9] for their elimination. There are two major problems with the elimination approach. First, potentially inaccurate class noise identification. Second, the number of samples in the training data will be reduced, potentially leading to an adverse effect on the learning algorithm performance. In elimination-based methods, re liable noise estimation crucial and inaccurate estimations can potentially degrade performance compared to using the original noisy dataset. The graph-based method, known as the Cut Edge Weight statistic method, was proposed by Zighed [8]. The principal idea is based on the manifold assumption and a Bernoulli distribution assumption on the consistency of the labels. A similar approach was proposed by Jiang and Zhou, who used a k-NN graph to detect class noise without the need to consider noise rate as probabilities [23]. Problem Setup and Background Let D denotes the clean distribution with no class noise, and  X ( with true binary label  X   X   X (  X  ) X ,...,1,2=  X , X 1= . When there are class noises,  X   X  denotes the observed dist ribution which contains class noise, and the training sample from a noisy distribution  X  are  X (  X   X  X ,  X  ) ,  X (  X   X  X ,  X  ) ,...,  X (  X   X  X ,  X  ) , where the label  X  X  different from the true label  X   X  . In this paper, we want to estimate noise rate at the level of the individual samples rather than for the whole collection of training data. T hus, we give definition of class noise rate as follows: distribution, the class noise rate is the probability of the observed label different from the true label of  X   X  , denoted by  X   X ( X  According this definition, class no ise rate is defined on individual data samples. Thus, the first issue we need to address is how to estimate the class noise rate for each training sample. The main challenge, however, is that a lear ner can only see the noisy data  X (  X  X ,  X   X  X )  X  , and there is no a priori knowledge about the class noise rate and the clean distribution D . Assuming that we can find a re asonably good estimation method, the second issue we need to address is how to make use of the estimated result to train a classifier on the noisy distribution. Before we address the second i ssue, we need to know how to measure the performance of a classifier. Generally speaking, the given set of training data. Howeve r, in this paper, the observed function on noisy training data may not be the minimized loss function on the clean data. So our objective is to find the estimation of the class noise rate for each individual sample such that the loss function on the noisy distribution can be modified. Furthermore, the modified loss function can also minimize the loss on the clean distribution. In order to address the problem in a formal way, we give some definitions below. Definition.2. Let  X  X  X  X : X  be a real-value decision function, sample on clean distribution is 0-1 loss given by  X   X  the hat  X  X  denoted the modified loss function. We can then use  X   X  (  X  (  X  )  X  X , ) to denote the modified loss function of the noisy distribution because the loss function is defined under the noisy rate for each individual sample an d modify the loss function on the noisy distribution with an observed label. Three related risks are then defined as follows: Definition.3.1 : the Expectation of l -risk under the clean distribution D is defined as:  X   X , X  (  X  )  X = (  X , X  ) ~ X  distribution  X   X  is defined as :  X   X   X  , X   X  (  X  )  X = (  X , X   X  ) ~ X   X  Definition.3.3 : the Empirical  X   X  -risk is defined on the training data as:  X   X   X   X  (  X  ) =  X  noisy label will influence the marked object. The capped hat  X  X  means that the marked object is an estimated result. In this section, we first introduce our class noise model and the estimation method. For training data without any pr iori knowledge on the noise rate, previous works on class noise m odeling gives uniform noise rate on the entire training dataset. That is, the class noise rate is defined on the entire dataset as )2/ X  X 0,1 X  . In this work, we drop this assumption and propose noise estimation performed on individual data samples. According to Definition.1 , class noise rate requires the estimation which is generally impossible because the observed data is noisy. So, in this work, we model the class noise  X  (  X  X  the noisy data by a hypothesis testing method. For any sample  X (  X   X  X ,  X  ) from the training set with the true label  X  we first assume that the probability of  X ( X   X   X ,  X  continuous without any noisy label and all samples are independent of each other. Thus,  X  X &gt;0 ,  X  X &gt;0 , such that  X  X  X   X   X ,  X   X &lt; X  , and  X ( X  X   X   X ,  X   X ( X  X )  X   X ,  X  ) X &lt; X  and  X ( X  X   X ( X   X  ) X &lt; X  . Then, we can build a k-NN graph for noisy data  X ( enough. Thus, we consider them to share the same probability,  X ( X   X   X |  X   X ( X  X )  X   X |  X  ) on the clean distribution. The problem here is that we ne ither have clean data nor prior knowledge about the clean distri bution. This means that we cannot estimate  X ( X   X   X |  X  ) for each individual sample. On the other hand, it is reasonable to assume that the true label of an individual sample should be similar to its nearest sample. Let  X (  X  the candidate sample whose class noi se rate needs to be estimated and the k nearest samples as  X (  X   X  X ,  X  ) , here j = 1,2,.., k . We can then define a sign function on the candidate sample  X (  X  X ,  X  ) and its k nearest samples  X (  X   X  X ,  X  ) :  X  can indicate the difference between an individual sample and its nearest label. Even though,  X  X   X  can contain noise with some The idea is that if the label of a candidate sample is different to its nearest sample, it should have a higher probability to be a noisy label. Based on this, we introduce a similarity function here, normalized over [0,1], denoted by  X   X  X  X   X ( X  X  X =  X   X ,  X  ) . Then, for each candidate sample, we define a statistic factor,  X  referred to as the sum of noisy similarity, which can be used to measure the total noise level. Definition.4 : For any individual sample  X (  X   X  X ,  X   X  X )  X  normalized similarity  X   X  X  X  , the sum of noisy similarity is: The idea of similarity is only a hypothesis. Since the nearest samples may also contain noise, the sum of noisy similarity  X  not a constant. Rather, it should follow some distribution. Let us refer to this distribution as the Sum of Random Noise (SRN) . For a with any specific c is known and shown as the 6 distributions on background in Fig.1. Obviously, the larger c is, the higher SRN should be for an individual sample. From the definition of  X  know that the distribution of  X   X  is dependent on  X ( X   X  X  X  we cannot use the observed label to estimate  X ( X   X  X  X  assume that  X  X  X   X  X  X  =1 X = 1/2 here, which is based on the Principle of Entropy Maximum (PEM). That is, when there is no a priori knowledge, the best assumption for label difference is that they share the same probability. Under the principle of entropy maximum, the distribution of  X   X  is shown as the yellow shadow on the foreground in Fig.1, labeled as SRN . Now, we can define the class noise rate for  X ( probability of  X   X  being opposite from SRN . Because  X  indicates that the sample  X   X  has different label to its nearest neighbor and the similarity metric is between 0 to 1, the larger  X  we should consider only the upper quantile of SRN here. Now, we can give the class noise rate under Definition.5. Definition.5 : For any individual sample  X (  X   X  X ,  X  of noisy similarity  X   X  , the probability of SRN noted as  X   X 1 X   X  X  X  X   X  X  X (  X  ) . If the principle of entropy maximum reveals a best guessing result, the upper quantile of SRN is the probability of the candidate sample being  X  X orse X  than a gue ssing result. So Definition.5 can sample is defined as the probab ility of the observed label being worse than a guessed label . It should be noted that the 6 curves on the background in Fig.1 may easily be mistaken as a normal distribution. In fact, the curve will only approximate the normal distribution when the k in the k-However, a large k will lead to poor accuracy for kNN. [8, 12] actually used normal distribution in their work for noise estimation. Normal distribution is used as a compared estimation method in our experiment in Section 6. We think SRN is a more realistic estimation because in practice, k is much smaller than 25. rate for individual samp les under Definition.5. We first need to find the estimation of  X ( X   X   X | observed label contains noise, and there is no prior knowledge about the distribution of class noise, the best estimation of  X ( X   X |1  X  ) , denoted by  X   X   X (  X   X |1=  X  ) should minimize the risk of  X   X   X (  X   X |1=  X  ) : Since there is no prior distribution about  X ( X   X   X |1=  X  ) , we assume that  X ( X   X   X |1=  X  ) follows a uniform distribution. That is,  X ( X   X |1  X  ) is completely random. Therefore,  X   X  (  X   X  =1 |  X   X  ) =argmin It can easily calculated that  X   X  (  X   X  =1 |  X   X  ) =1/2 , which means the best assumption for  X ( X   X   X |1=  X  ) is  X  (  X   X  =1 |  X   X | X 1  X  ) when there is no prior knowledge about the clean distribution. This can also be explained by the Principle of Entropy Maximum . Under this assumption, the top k nearest samples of  X (  X   X ,  X  ) have the same probability  X ( X   X ( X   X   X |  X  ) , which implies that  X  X  X   X  X  X   X  X  X  =1 X =  X  X  X  Now, we can use this result to estimate the class noise rate under Definition.5. Theorem.1. ( main result ) The estimated class noise rate of  X (  X  X ,  X   X  X )  X  denoted by  X   X  (  X   X  ) , is: The proof of Theorem.1 is based on Lemma.1 to be introduced below. Lemma.1. Let  X   X  X  ,  X   X  X  ,...,  X   X  X  X  be independent Bernoulli random variables (  X  X  X   X  X  X   X  X  X  =1 X =  X  X  X  = X 1 X =1/2 .),  X   X  X  ,  X   X  X  be a sequence of real number such that  X   X   X (=  X  X  ,  X   X  X   X  and 0 X  . We can then conclude that where  X   X , X   X (  X  ) X , is defined as: Here,  X   X   X   X  and  X   X   X   X  are the  X   X  and  X   X  norms; and  X  X  This formula of  X   X , X  (  X   X   X , ) is well known as the K -method of real interpolation for Banach Space [24]. The proof of Lemma.1. was given by [10] in details whic h we will not repeat here. high cost. Here, we give Lemma.2 below which provides a simple sub-optimal solution of  X   X , X  (  X   X   X , ) . Lemma.2. Let  X   X  X  ,  X   X  X  ,...,  X   X  X  X  be independent Bernoulli random variables (  X  X  X   X  X  X   X  X  X  =1 X =  X  X  X  = X 1 X =1/2 .),  X   X  X  ,  X   X  X  be a sequence of real number such that  X   X   X (=  X  X  ,  X   X  X   X  and 0 X  , we have: where,  X   X   X   X  and  X   X   X   X  are the  X   X  and  X   X  norms. Proof : First, let  X  X   X   X  X =  X  and  X  X  X   X   X  X =  X  , where  X  and  X  are two real numbers. Then, there is a subspace of the interpolation in the Banach space and the optimal result on the subspace is greater than the norm. Thus: Assuming the Probability Density Function (PDF) of the SRN  X  (  X   X  ) is ) X ( X  . Such that based on Formula(4), So, This will lead to the result of Lemma 2 .
 According to Definition.4, the estimated class noise rate  X   X ( According to Lemma.3, the probability of  X (  X   X  X ,  X  Due to  X   X  X  X  is a Rademacher random vari able in our assumption, the probability of 0 X  and 0 X  are equal. Thus the estimated class noise rate is: For noise detection, the lower b oundary is more significant since it pertains to the minimum noise ra te of a labeled training sample. In practice, we can use as the estimation for each training sample. Here,  X  (  X   X  ) can either be incorporated in the learning algorithms to weigh the importance of a training sample or be used to identify samples for elimination. It should be noted that  X  (  X  symmetrical on  X   X  . It means that when  X   X  is less than 0, the upper quantile of SRN needs to be solved by the lower quantile of SRN which is introduced by the opposite sign function. In another word, when  X   X  is less than 0,  X   X  (  X   X  ) should be: This inference can be easily proven by the symmetry of  X  (  X  In this section, we introduce our l earning with noise strategy and class noise elimination method. Based on the class noise estimation given in Formula (5), we propose a method using the learning with noise strategy. The fundamental idea is to use the estim ated class noise rate to weigh a risk-of-loss function. The key is to ensure that the weighted loss function can adequately approximate the loss function for the clean training data. The loss function on the observed distribution is defined as original loss function with the obse rved labels weighted by their label correctness probabilities. The se cond is a penalty for the loss observed label is incorrect) weight ed by the class noise rate. The denominator is based on the average class noise rate to ensure that the noisy training data X  X  loss function approximates the clean training data X  X  loss function. This loss function is an updated version of the original loss function on noisy data. We are interested to know whether the minimum risk of the proposed loss function on the noisy training data can approximate the minimum risk of the loss function on the clean data. Let us assume that the clean distribution and noisy distribution are defined according to Definition 3.1 to Definition 3.3. As mentioned earlier in Section 3, we need to address two issues  X  expected risk  X   X   X  , X   X  (  X  ) converge to  X   X , X  distribution? If the answer to both questions is yes, the empirical risk then converges to the clean distribution. Thus, we can train a classifier for the cleaning distribu tion by the noisy data without any prior knowledge. Theorem.2. The empirical  X   X  -risk on the training data  X  converge to the risk  X   X   X  , X   X  (  X  ) of the loss function on the noise data with n independent and identically distributed training samples denote as  X (  X   X  X ,  X  ) ,  X (  X   X  X ,  X  ) ,...,  X (  X   X  X , ) X ,...,1,2 when n grows. Proof : Given n independent  X (  X   X  X ,  X  ) ,  X   X  (  X  noise rate estiminate by Fo rmula (5) with the expectation  X ( X   X  (  X   X  ) ) . According to the Chebyshev law of large numbers,  X  X &gt;0 ,  X  X &gt;0 , and 0 X  , such that when  X  X  , is true with a probability of at least  X 1 X  . For the same reason,  X  X &gt;0 ,  X  X &gt;0 , and 0 X  , when  X  X  , is true with a probability of at least  X 1 X  . satisfies With probability of at least  X 1 X  .  X  In other words, the empirical  X   X  -risk on the training data  X  converge to the risk  X   X   X  , X   X  (  X  ) of the loss function on the noise data when the size of training data is large enough. If we have a perfectly correct esti mation of the cla ss noise rate, it is true that Under this assumption,  X   X   X  , X   X  (  X  ) will converge to  X  clean distribution. In order to distinguish the perfect estimation result and the estimation by our method, let  X   X  estimation. Theorem.3. The minimum result of risk  X   X , X   X  X   X   X  will converge to the minimum risk  X   X , X  (  X  ) on the clean distribution when there is a perfect estimation of class noise rate. The proof of Theorem.3 is given in detail by [6], which will not repeated here. Based on the proof, the following inequality holds with a probability of at least  X 1 X  . Here,  X   X  is the minimizer of  X   X , X  (  X  ) on the clean distirbution,  X  the minimizer of  X   X , X   X  X   X   X  on the clean distribution and where  X   X  is an independent and identically distributed Rademacher random variable.  X  The right side of the inequality in Formula (7) is bound by the other words, the class noise estimation method indeed ensures the optimality of  X   X   X  with the class noise distribution approximating the risk of the clean distribution. Thus the risk of our estimation method  X   X , X   X  X   X   X  satisfy: Because the second item is bound, the risk of our estimate is determined by the first item which is related to the estimation result. This can be interpreted as that when the size of the training set grows, the performance of the classifier trained on the noisy data is determined by the estimation result given in Formula (5). Existing class noise elimination strategies attempt to detect and eliminate noisy samples. In this work, we propose a sampling based method which focuses on selecting only high quality samples to form the training dataset rather than focusing on noisy samples. The quality of a training sample is defined based on its class-noise rate: the lower the class-noise rate, the higher the quality. We refer to this as a sa mpling-based method. The idea is to use an ensemble learning algorithm as a group of aggregate classifiers, each working on a subset of the training data. Even if a sample is considered noise in one classifier, it may still be used as correct data in other sampling rounds. Generally speaking, an ensemble learning algorithm can use either bagging or boosting (e.g., Adaboost). In this work, we use only the bagging strategy for evaluation. Each training sample  X (  X   X  X ,  X  ) is assigned a weight  X   X   X (  X  ) . We use this weight to select samples in the training dataset. We deploy a bagging algorithm ba sed on the sampling results to obtain a stable classifier on the training data. Any basic classifier can be used in this framework. To ensure the effectiveness of this strategy, we need to confirm that the learning algorithm approximates the learner on a clean distribution. Let  X   X  denote the i -th sampling result with  X   X  optimal hypothesis on  X   X  . Assume an optimal hypothesis on the clean distribution  X  X  exists and is denoted by  X  to the Hoeffding boundary from Angluin and Laird [11], the difference between  X   X  and  X   X  is bound by: to the feature space and  X  is the class-noise rate of  X  class-noise rate is bound by the result of each sampling, the hypothesis on the clean distribution approaches zero as the size of  X  increases. The bagging strategy allows error correction in the sampling to achieve a superior and more stable result. From the introduction above, we can see that any learning algorithm should be composed by tw o parts. The first part is for noise estimation, the second part is for training. The training parts are dependent on the learning algorithms. Thus, the complexity discussed here is on the noise estimation part only. Since the estimation is based on a k-NN graph, usually the computational complexity of kernel based method such as kNN is dimension of the feature space. There are some methods to speed up kNN as done in [28, 29] by reducing d for sparse data. However, the computational complexity is still  X (O step is the estimation. According to Formula (5), the complexity is ) X  X (O . Thus, the computational complex of our estimation method is  X (O  X  ) X  X + X  . As k is fixed, and not related to data size, complexity is basically  X (O  X  ) X  . Our experiments evaluate the es timation performance and show its utility in improving learning performance for both online and offline learning algorithms. In principle, we can use any learning algorithm for this evaluation. For demonstration purpose, we use a simple linear algorithm using perceptron-based online learning to avoid complex parameter tuning. This is referred to as LiC (Linear Classification). For offline lear ning, we use bagged non-linear SVMs with polynomial kernels, referred to as SaC (Sampling Classification). The pseudo code for the LiC algorithm is given below Algorithm I: LiC Input: S: The training dataset with n samples;  X (  X  X ,  X  ): The training sample,  X (  X   X  X ,  X   X  X )  X  : Learning rate for perceptron learning  X  : Initialized weight of perceptron features Training: For each  X (  X   X  X ,  X  ) in S , build a k-NN graph and calculate  X  according to Formula (5) For i =1 to n : Compute the inner product:  X   X   X  X  X   X ,  X   X  Output:  X   X  For the SaC algorithm, the class noise rate estimation given in Formula (5) serves as the weight for each training sample during sampling. The pseudo code of the SaC algorithm is given below. Algorithm II: SaC Input: S: The training dataset with n samples;  X (  X   X  X ,  X  ): The training sample,  X (  X   X  X ,  X   X  X ) T : Number of iterations Training: For each  X (  X   X  X ,  X  ) in S , built a k-NN graph and calculate  X  according to formula (5) For j =1 to T : Sampling based on  X 1 X   X   X (  X  ) for each  X (  X   X  X ,  X  ) , the result is  X  Use the learner on  X   X  to train the classifier  X   X  Output: An ensemble classifier: = X   X   X   X  /T  X  Based on the two algorithms, we conduct two sets of experiments: public datasets with comparison s to other well-known methods. We prepared one linearly separable dataset and one non-linearly shown in Fig. 2. The correspondi ng testing datasets have similar distributions. The feature values are real numbers between 0 and 1 and the labels are binary (red ci rcles and blue crosses represent positive and negative labels, respectively). The ratio of positive and negative samples is 1:1. Stoc hastic class noise is introduced into the clean training data to invert some labels. Experiments on linearly separable data with 20% and 40% noise levels are shown in Fig.3(a) and Fig.3(b) respectively. Fig. 3(c) and 3(d) show the respective result of LiC on the testing data and Table 1 shows the accuracy of both LiC and the Perceptron algorithm without no ise estimation. Comparing Fig. 3(c) and 3(d), we can see that the decrease in accuracy is attributed to the shift of the linear separation boundary as the noise level increases (particularly evident at the bottom right corner). Table.1 indicates that noise estimation can make an obvious difference in performance. This is more apparent at higher noise level of 40%. LiC can achieve the improvement of 3.62% compared to the Perceptron algorithm. Fig. 4(a) and Fig. 4(b) show th e non-linearly separable data with 20% and 40% noise levels, respectively, and Fig. 4(c) and Fig. 4(d) show the result of SaC. Table 2 shows the accuracy of SaC compared to RBF kernel based SVMs. Note that the improvement of 2.54%, when the class noise rate is 20%, is larger than LiC because non-linearly separable problem is more sensitive to class noise. However, when the cl ass noise rate is 40%, the improvements of two algorithms are similar. In this set of experiments, we evaluate the performance of our proposed methods using seven public datasets for binary classification with different class-noise rates: (1) the LEU [23] set of cancer data; (2) the Splice dataset for DNA sequence splice-junction classification; (3) the UCI Adult dataset collection containing seven subsets (referred to as a1a to a7a according to original naming of the data) of independent training and testing data [26]; (4) the DBWorld e-mails DataSet in English (in short, DB), which consists of 64 e-mails manually collected from DBWorld mailing list as binary labels of either  X  X nnounces of conferences X  or  X  X verything else X ; (5) the Farm Ads DataSet in English (in short, FADS), which is collected from text ads found on twelve websites that deal with various farm animal related topics with binary labels based on whether the content owner approves of the ad or not; (6) the Twitter Dataset for Arabic Sentiment Analysis Dataset (in short, TDA) with the class labels being opinion polarity; (7) the Product Reviews from Amazon in three categories, Book, DVD and Music (in short, PRA). The The size, class ratio, types and dimensions of the datasets are listed in Table 3. 3 http://tcci. ccf.org.cn/conference /2013/dldoc/evdata03.zip To introduce the class noise into the training set, we stochastically invert the binary labels of trai ning samples with a probability of 10%, 20%, and 30%, respectively. For the datasets (1) to (3), we train the binary classification algorithm on the noisy data. The algorithm is tuned on a development set before being used on the experiment. We choose two kind of similarity measures, the cosine similarity for text data, and the Euclidean Distance based similarity for al l other data. The k in k-NN graph is 7 for LiC , SaC and CEWS. For the datasets (4) to (7), due to the lack of testing data, the result is based on 5-folds cross-validation. We compare the performance of LiC with two state-of-the-art learning algorithms for noisy data: (1) the widely used open source robust method NHERD [27] and (2) the class noise log loss method, l l og [6] which uses the same loss function as LiC . We also compare SaC with two state-of-the-art noise elimination systems: (3) Identifying Mislabeled Traini ng Data (IMTD) [7] and (4) Cut Edge Weight Statistics (CEWS) [8]. Both l l og and CEWS require prior knowledge of the class-noise fo r the training data. Thus, they are more appropriate for use as benchmarks rather than direct comparison to other algorithm s, and should be discussed separately. Table 4 shows the ac curacy of the six systems. Comparison to Other Methods Table 4 shows that our proposed algorithms LiC and SaC outperform the other the algorithms in most of the cases. In terms of both the macro average and the micro average which is weighted over the size of the diff erent class labels, our algorithms clearly outperform all other methods . Even though the class-noise outperform LiC and SaC for the 10% and 20% noise levels of two and three datasets, respectively. This is because l require that all samples have the same class-noise rate for the probability weighting. Fort the small training set, such as LEU, which has only 37 training samples, and DB, which has only 64 samples for training, the size effect is quite prominent. Intuitively, the noisy data elimination based method should not work well in these data. Results show that when the class noise rate increase to 20% and 30%, the high percentage of noise obviously have a big effect on the training data. Most methods have a big performance decrease for these two datasets, but LiC performs well and shows a significant advantage over other methods. This is because in the small training set, the size of training data is more important than class noise estimation, LiC can also show a higher performance than l l og even though l l og also has the same size of training data. When the training set grows, such as Splice which has 1,000 samples or TDA which has 2,000 samples, the quality of training samples becomes much more impor tant. For these datasets, both LiC and SaC show better performance than the other methods. When the training set size becomes larger, the advantages of our methods become even more obviously for UCI Adult with 16,100 samples and the review text from Amazon with 4,000 samples. When the noise level is at the 30% level, LiC and SaC can achieve a 5% higher micro average accuracy than other methods. Generally speaking, both LiC and SaC show a more stable and more accurate performance in most cases as shown in Fig.5. For the cases which LiC and SaC cannot achieve the best performance, they are at least as competitiv e as the other methods. Thus, our proposed method is the overall best performer. Comparison between LiC and SaC We now compare the performance of the two proposed algorithms LiC and SaC . LiC is a learning with noise algorithm. Therefore, it should be more sensitive to the percentage of class noise in the elimination based method. It should be more sensitive to the size of training data. Take the sma ll training dataset LEU as an example, it has only 37 sample s. Thus any elimination on the training data will lead to degradation of performance. In this dataset , LiC is definitely a better choice than SaC . In general, we are more interested in the suitabili ty of the two proposed strategies with respect to training data size. To eliminate the extremely small training datasets, we use  X  X mall X  to means the size of training data at around 1,000 or so. For training set size at around 100 or so, we called them as  X  X iny training set X . To examine the relationship between training set size and algorithm performance, we further analyze performance in more details for the UCI Adult dataset because the subsets are ordered by increasing size and all are from the same distribution and feature space. Fig. 6 shows the comparison of LiC ( Red ) to SaC ( Yellow ) for different dataset sizes and noise rates using color spectrum. The different shades indicate the relative suitability of the ordinate is the class noise rate. We observe the following facts: 1. LiC is in efficient for small training sets(Indicated by pure 2. Irrespective of the training size, LiC outperforms SaC for low 3. When the training size is relatively large, both LiC and SaC For text classification, because n-gram models are used to extract features, the feature space can be quite sparse and the elimination of noisy data may reduce the perfor mance of classifier sharply. So, for tiny training sets such as DB, the size effect is very prominent. Note that LiC achieves an acceptable performance for DB even when the class-noise rate is 30%, whereas, all noise elimination based algorithms including SaC show very poor performance at higher noise levels. It is similar to the comparison on LEU dataset. Fig. 7 shows the performance for text classification which tend to have sparseness problem in the feature space. Here, the abscissa is the training set size and the ordinate is the class-accuracy. We observe the following from Fig. 7: 1. For the tiny training set, SaC is not acceptable and LiC 2. For the small training set, SaC outperforms LiC when the 3. Irrespective of the training size, SaC outperforms LiC for low 4. When the training size is relatively large, LiC and SaC show The performance of SaC is better than LiC when the class noise is low. The reason behind is that SVMs shows a higher performance than perceptron based algorithm when the feature space is large and sparse. So SaC is more suitable algorithm text classification . robust on class noise rate. When the training set is relatively large, both algorithms perform simila rly well and converge to a consistent result. Th is is proven through both the theoretical boundary analysis and the experimental results. We apply our method to both the l earning with noise strategy and the class noise elimination strategy with calculated theoretic bounds for proof of accuracy. Bo th algorithms are competitive with state-of-the-art techniques and show superior performance on real datasets. We analyze the algorithm performance on different training dataset sizes and class-noi se rates. Results conform to the learning theorem provided in Equation (7). In future work, we will investigate noise handling in semi -supervised tasks such as semi-supervised classification, transductive transfer learning, and also look into the domain adaptation problem. We will also consider different noise rate for different classes since label noise rates are often class-conditional in practice. Acknowledgements [1] Zhu, X., and Wu, X "Class noise vs. attribute noise: A [2] S X ez, J. A., Galar, M, Luengo, J, and Herrera, F. "Analyzing [3] Joseph, L., Gyorkos, T. W., and Coupal, L.. "Bayesian [4] Cawthorn, D. M., Steinman, H. A., and Hoffman, L. C.. "A [5] Beigman, E. and Klebanov, B. B..  X  X earning with [6] Natarajan, N., Dhillon, I. S. , and Ravikumar, P..  X  X earning [7] Brodley, C. E., and Friedl, M. A.. "Identifying mislabeled [8] Zighed, D.A., Lallich, S., Muhlenbach, F..  X  X  Statistical [9] Sluban, B., Gamberger, D., a nd Lavrac, N.. "Advances in [10] Montgomery-Smith, S. J. "The distribution of Rademacher [11] Angluin, D., and D.Laird, P. "Learning from Noisy [12] Zhang, M. L., and Zhou, Z. H.. "CoTrade: Confident Co-[13] Gui, L., Xu, R. F., Lu, Q., et al. "Cross-lingual Opinion [14] Fr X nay, B., and Verleysen, M..  X  X lassification in the [15] Heskes, T.  X  X he use of being Stubborn and Introspective, X  In [16] Li, Y., Wessels, L. F. A., Ridder, D., and Reinders, M. J. T.. [17] S cott, C., Blanchard. G., and Handy, G..  X  X lassification with [18] Lawrence, N. D., and Sch X lkopf, B..  X  X stimating a Kernel [19] Perez, C. J., Giron, F. J., Martin, J., Ruiz, M., and Rojano, C.. [20] Klebanov, B. B., and Beig man, E..  X  X rom Annotator [21] Kolcz, A., and Cormack, G. V..  X  X enre-based [22] Zhu, X., Wu, X., and Chen, Q. J.. "Eliminating Class Noise [23] Jiang, Y., and Zhou, Z. H.. "E diting Training Data for k-NN [24] Bennett, C., and Sharpley, M..  X  X nterpolation of Operators X . [25] Golub, T. R., Donna K. S., Pablo Tamayo, C. H., Michelle [26] Platt, J. C.  X  X ast Training of Support Vector Machines using [27] Crammer, K., and Lee, D.. "Lea rning via Gaussian Herding." [28] Cui, B., Ooi, B. C., Su, J., and Tan, K. L.  X  X ontorting high [29] Hui, J., Ooi, B.C., Shen, H., Yu, C., Zhou, A.: An adaptive 
