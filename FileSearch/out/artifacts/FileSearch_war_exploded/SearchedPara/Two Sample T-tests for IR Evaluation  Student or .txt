 There are two well-known versions of the t -test for comparing means from unpaired data: Student X  X  t -test and Welch X  X  t -test. While Welch X  X  t -test does not assume homoscedasticity (i.e., equal vari-ances), it involves approximations. A classical textbook recom-mendation would be to use Student X  X  t -test if either the two sample sizes are similar or the two sample variances are similar, and to use Welch X  X  t -test only when both of the above conditions are vio-lated. However, a more recent recommendation seems to be to use Welch X  X  t -test unconditionally. Using past data from both TREC and NTCIR, the present study demonstrates that the latter advice should not be followed blindly in the context of IR system eval-uation. More specifically, our results suggest that if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch X  X  t -test may not be reliable.
 statistical significance; test collections; topics; variances
The present study concerns IR evaluation where two means from different samples need to be compared. The classical approach for this would be to employ a two-sample (i.e., unpaired) t -test to discuss whether, given the observed sample means, the popula-tion means may be different. There are two well-known versions of the two-sample t -test: Student X  X  t -test and Welch X  X  t -test. While Welch X  X  t -test does not assume homoscedasticity (i.e., equal vari-ances), it involves approximations. A classical textbook recom-mendation would be to use Student X  X  t -test if either the two sam-similar, and to use Welch X  X  t -test only when both of the above con-ditions are violated [3]. However, a more recent recommendation seems to be to use Welch X  X  t -test unconditionally. For example, Daniel Laken X  X  blog posted on January 26, 2015 recommends re-searchers to  X  X lways use Welch X  X  t-test instead of Student X  X  t-test, X  while demonstrating the superiority of Welch X  X  t -test using simu-lated data 1 .The t.test function provided in the stats library of the R language uses Welch X  X  t -test by default 2 ; hence researchers who use this function as a black box may well be using Welch X  X  t -test all the time. In fact, in as far back as 1981, Gans [1] recom-mended  X  X he automatic use of the Welch test alone X  as an option, based on simulations. The present study seeks to obtain the right recommendations for the purpose of IR system evaluation using real data from TREC and NTCIR.
In the IR evaluation literature, researchers have focussed mainly on the paired data setting, because the most basic method for com-paring two IR systems is to use the same test collection with a sin-gle topic set to compare two systems. For example, Smucker et al. [7] compared the sign test, the Wilcoxon signed rank test, the paired t -test, the bootstrap test and the randomisation test from the viewpoint of how similar the p -values of different test types are to one another, and concluded that the use of the two nonparametric tests (sign and Wilcoxon) should be discontinued. Urbano et al. [8] conducted a follow-up study on the same set of paired significance tests, using repeated topic set splitting experiments in a way simi-lar to earlier studies (e.g. [10]) to quantify the discrepancies across the pairs of topic sets for comparing two systems. Contrary to the recommendation by Smucker et al. ,Urbano et al. report that  X  X he permutation test is not optimal under any criterion. X 
In contrast to the aforementioned studies, the present study con-cerns two-sample t -tests for comparing two means, with sample clude comparing sets of clicks from two different search engines, between-subject design user experiments, and comparing the  X  X ard-ness X  of two test collections using the same IR system. Sakai [4] used the two-sample bootstrap test (in addition to the paired boot-strap test) for the purpose of comparing different evaluation mea-sures in terms of  X  X iscriminative power. X 
The common assumptions are a s follows. We have scores x that each obey N (  X  2 , X  2 2 ) . The population means and variances purposes. http://daniellakens.blogspot.nl/2015/01/ always-use-welchs-t-test-instead-of.html http://127.0.0.1:27533/library /stats/html/t. test.html
Student X  X  two-sample t -test further assumes homoscedasticity:  X  1 =  X  2 2 . While this is a strong assumption, it is also known that this test is quite robust to assumption violations. For this test, we first define a pooled variance V =( S 1 + S 2 ) / X  where  X  is the degree of freedom for V given by  X  =  X  1 +  X  2 , X  1 = n 1 n  X  1 . The test statistic is: which is compared against t (  X  ;  X  ) , the critical t value for  X  n + n 2  X  2 degrees of freedom with the significance level of  X  .
The good news about Welch X  X  t -test is that it does not assume ho-moscedasticity; the bad news is that it involves approximations [3], test, the test statistic is which is compared against t (  X   X  ;  X  ) ,where
Welch X  X  t -test approximates the distribution of the following statis-tic by a  X  2 distribution with  X  0 degrees of freedom: Furthermore, it estimates  X  0 as  X   X  0 =  X   X  using Eq. 3. Do these approximations have any consequences for IR evaluation?
Nagata [3] clarifies the relationship between the above two t -tests analytically. Let a = n 2 /n 1 ,b = V 2 /V 1 . Then it is easy to Note that g (1 ,b )= g ( a, 1) = 1 . Hence, if either n 1 V sample is no larger than 1.5 times the other, or if the larger variance 20% or so [3]. As for the degrees of freedom, it can be shown that:  X   X  Since h (1 , 1) = 1 holds, having n 1 = n 2 and V 1 = V 2 is a sufficient condition for obtaining  X   X  =  X  . Also, it can be verified that h ( a, b )=  X   X  / X  is much smaller than one if b = V close to one and a = n 2 /n 1 is far from one. That is, Welch X  X  has relatively low statistical power (due to its degree of freedom  X   X  being much smaller than Student X  X   X  ) when the variances are roughly equal but the sample sizes are quite different.
In order to compare Student X  X  and Welch X  X  t -tests in terms of re-liability, we adopt a method similar to that of Webber et al. [11]: given a topic set of size n , with m runs that processed these topics, randomly partition the topics into two sets of size n 1 and n 2 ,re-spectively. For each of the m runs and a given evaluation measure, conduct a two-sided, two-sample test to see if the two means for the same run are statistically significant. The ground truth is that they are not , since the scores actually come from the same system. The random partitioning is done B = 1000 times, so a test col-lection with m runs will yield Bm = 1000 mp -values for each significance test type with a given evaluation measure. Table 1 shows a summary of the two data sets used in this study. We chose them because (a) we wanted about n = 100 topics (or more if available) with graded relevance assessments; (b) we wanted data (with many runs) from different evaluation confer-ences to ensure generalisability.  X  X REC99 X  comprises 110 runs from the TREC 2004 robust track [9], with 99 robust track top-ics. While this data set has many runs, the 99 topics come from two robust track rounds: 50 from TREC 2003 and 49 from 2004. In contrast,  X  X TCIR97, X  which comprises 40 Simplified Chinese runs from the NTCIR-7 ACLIA IR4QA task [6], uses 97 topics that originate from a single round of the task.

For each data set with a given evaluation measure, we experi-mented with four different sample size ratios a = n 2 /n 1 as shown in Table 2 to obtain a total of 4000 mp -values. Recall that, accord-ing to Nagata [3], Student X  X  and Welch X  X  t statistics should not be vastly different for the a =1 ( n 1 : n 2 =50:50 )and a =1 ( n 1 : n 2 =40:60 ) settings, regardless of how the two sample variances differ. We experimented with four evaluation measures: (binary) Average Precision (AP) , Q-measure (Q), normalised Dis-counted Cumulative Gain (nDCG) and normalised Expected Re-ciprocal Rank (nERR). Unlike the other three measures, nERR is known to be suitable for navigational information needs due to its diminishing return property; for this very reason, it is known to be statistically less stable than the other measures [5].
Figure 1 plots Welch X  X  p -values against Student X  X  p -values for the 110,000 two-sample t -tests conducted with the TREC99 data, for nDCG ((a)-(d)) and nERR ((e)-(h)) with different target sample size ratios. The graphs for TREC99 with AP and Q, as well all all graphs for NTCIR97, are omitted due to lack of space. How-ever, the overall picture is the same for all evaluation measures and across the two data sets, and we believe that our findings are general. In each graph, the blue rectangle represents the situation where Student X  X  t -test obtains a p -value smaller than  X  a false positive) while Welch X  X  t -test does not; the red rectangle rep-resents the opposite situation; the bottom left square represents the situation where both tests obtain a false alarm at  X  =0 . can be observed that when the target sample size ratio is a ( n 1 : n 2 =50:50 ), the two tests are indeed virtually identical and false alarms are very rare; as we gradually increase the sample size ratio until it reaches a =9 . 0 ( n 1 : n 2 =10:90 ), we obtain more and more false alarms on both sides of the diagonal.
Tables 3 and 4 show the false positive rates for all of our TREC99 and NTCIR97 experiments, respectively. Based on the discussion provided in Section 3.3, we categorised the observations into three classes: the first is for those where the variance ratio satisfies b  X  3 / 2 : recall that Student X  X  and Welch X  X  t statistics are expected to be similar to each other in this situation. The other two classes ( b&lt; 2 / 3 and b&gt; 3 / 2 ) represent the situations where the sample variances are very different. For example, Table 3 Section (III) Column (d) provides the following information about our TREC99 experiments for nDCG with the sample size ratio a =9 . 0 (with the actual sample sizes n 1 =10 ,n 2 =89 as shown in Table 1):
More generally, we can observe the following consistent trends from Tables 3 and 4:
The above results, based on real IR system runs from both TREC and NTCIR, suggest that the advice  X  X lways use Welch X  X  t -test in-stead of Student X  X  t -test X  should not be followed blindly in IR sys-tem evaluation.
For the purpose of reliable IR system evaluation, we compared two versions of two-sample t -tests: Student X  X  t -test (which assumes homoscedasticity) and Welch X  X  t -test (which relies on approximated distributions). Our topic set splitting experiments with runs from and b&gt; 3 / 2 are equivalent: it is just a matter of swapping the two samples, since the two tests are symmetric with respect to the two samples. So we should expect similar results in Column (a) for these two ranges of b . The slight caveat is that we actually have n Table 3: TREC99 false positives at  X  =0 . 05 : Student/Welch. The higher false positive rate in each condition is shown in bold. The total number of observations for each variance ra-tio is shown in parentheses. both TREC and NTCIR are consistent across different evaluation measures and across these two different IR venues. While neither our equal variance settings nor our equal sample size settings do not demonstrate any advantages of Student X  X  t -test over Welch X  X  t -test, our results do suggest that if the sample sizes differ substantially and if the larger sample has a substantially larger variance, Welch X  X  t -test may be less reliable. Hence we argue that the advice  X  X l-ways use Welch X  X  t -test instead of Student X  X  t -test X  should not be followed blindly in IR system evaluation.

In practice, we recommend IR researchers to examine the sample conscious decision, rather than (say) relying on a default setting in the t.test function provided in R. We also recommend IR researchers to report explicitly which version of the two-sample t -test was used in their experiments, even if we may be able to spot a Welch X  X  t -test when the degree of freedom reported is not an integer (Eq. 3).
 I would like to thank Professor Yasushi Nagata for his helpful com-mentsonmyresults. Table 4: NTCIR97 false positives at  X  =0 . 05 : Student/Welch. The higher false positive rate in each condition is shown in bold. The total number of observations for each variance ra-tio is shown in parentheses.
