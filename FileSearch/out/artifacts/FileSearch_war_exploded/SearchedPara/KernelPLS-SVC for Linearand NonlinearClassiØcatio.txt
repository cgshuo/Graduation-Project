 Roman Rosipal rrosipal@mail.ar c.nasa.go v and Leonard J Trejo ltrejo@mail.ar c.nasa.go v Bryan Matthews bmatthews@mail.ar c.nasa.go v The partial least squares (PLS) metho d (Wold, 1975; Wold et al., 1984) has been a popular modeling, regres-sion, discrimination and classi X cation technique in its domain of origin|c hemometrics. In its general form, PLS creates score vectors (comp onen ts, laten t vectors) by using the existing correlations between di X eren t sets of variables (blocks of data) while also keeping most of the variance of both sets. PLS has proven to be useful in situations where the number of observ ed variables is much greater than the number of observ ations and high multicollinearit y among the variables exists. This situation is also quite common in the case of kernel-based learning where the original data are mapp ed to a high-dimensional feature space corresp onding to a re-producing kernel Hilbert space (RKHS). Motiv ated by the recen t results in kernel-based learning and supp ort vector machines (Vapnik, 1998; Sch X olkopf &amp; Smola, 2002) a new metho d for classi X cation is prop osed. This is based on the kernel orthonormalized PLS metho d for dimensionalit y reduction combined with a supp ort vector machine for classi X cation (SVC) (Vapnik, 1998; Sch X olkopf &amp; Smola, 2002).
 Consider the ordinary least squares regression with outputs Y to be an indicator vector coding two classes with two di X eren t labels represen ting class mem ber-ship. The regression coe X cien t vector from the least squares solution is then prop ortional to the linear discriminan t analysis (LDA) direction (Hastie et al., 2001). This close connection between LDA and least square regression partially justi X ed the use of PLS for discrimination. However, showing the close connection between Fisher's LDA, canonical correlation analysis (CCA) and orthonormalized PLS metho ds, Bark er and Rayens (2003) more rigorously justi X ed the use of PLS for discrimination. This connection also shows the preference of using orthonormalized PLS or its non-linear kernel varian t for dimensionalit y reduction in comparison to linear or nonlinear kernel-based princi-pal comp onen ts analysis (PCA) for discrimination. In comparison to PLS regression on the dumm y ma-trix Y , the use of SVC on selected PLS score vectors is motiv ated by the possibilit y of constructing an optimal separating hyperplane , a better control for overlap be-tween classes when the data are not separable, using a theoretically more principled \hinge" loss function instead of a squared-error loss function and  X nally to avoid the problem of masking of the classes in multi-class classi X cation (Vapnik, 1998; Hastie et al., 2001). Alternativ ely, other metho ds for classi X cation (for ex-ample, LDA, logistic regression) applied on extracted PLS score vectors can be considered. A RKHS is uniquely de X ned by a positiv e de X nite ker-nel function K ( x ; y ); that is, a symmetric function of two variables satisfying the Mercer theorem conditions (Sch X olkopf &amp; Smola, 2002). Consider K ( :; : ) to be de- X ned on a compact domain X  X X ; X  X  R N . The fact that for any such positiv e de X nite kernel there ex-ists a unique RKHS is well established by the Moore-Aronszjan theorem . The form K ( x ; y ) has the follow-ing reproducing property where h :; : i H is the scalar product in H . The function K is called a reproducing kernel for H .
 It follows from Mercer's theorem that each positiv e de X nite kernel K ( x ; y ) can be written in the form where f  X  i ( : ) g S i =1 are the eigenfunctions of the integral operator  X  K : L 2 ( X ) ! L 2 ( X ) and f  X  i &gt; 0 g S i =1 are the corresp onding positiv e eigen-values. Rewriting (1) in the form K ( x ; y ) = it becomes clear that any kernel K ( x ; y ) also corre-sponds to a canonical (Euclidean) dot product in a possibly high-dimensional space F where the input data are mapp ed by The space F is usually denoted as a feature space and ff p number of basis functions  X  i ( : ) also de X nes the dimen-sionalit y of F . Because the PLS technique is not widely known,  X rst a description of linear PLS is provided which will sim-plify the description of its nonlinear kernel-based vari-ant (Rosipal &amp; Trejo, 2001). 3.1. Linear Partial Least Squares Consider a general setting of the linear PLS algorithm to model the relation between two data sets (blocks of observ ed variables). Denote by x 2 X  X  R N an N -dimensional vector of variables in the  X rst block of data and similarly y 2 Y  X  R M denotes a vector of variables from the second set. PLS models the re-lations between these two blocks by means of laten t variables. Observing n data samples from each block of variables, PLS decomp oses the ( n  X  N ) matrix of zero mean variables X and the ( n  X  M ) matrix of zero mean variables Y into the form where the T , U are ( n  X  p ) matrices of the extracted p score vectors (comp onen ts, laten t vectors), the ( N  X  p ) matrix P and the ( M  X  p ) matrix Q represen t matri-ces of loadings and the ( n  X  N ) matrix F and the ( n  X  M ) matrix G are the matrices of residuals. The PLS metho d, which in its classical form is based on the nonlinear iterativ e partial least squares (NIP ALS) algorithm (Wold, 1975),  X nds weight vectors w ; c such that = [ cov ( t ; u )] 2 where cov ( t ; u ) = t T u =n denotes the sample covari-ance between the score vectors t and u . The NIPALS algorithm starts with random initialization of the Y-score vector u and repeats a sequence of the following steps until convergence: After the convergence, by regressing X on t and Y on u , the loading vectors p = ( t T t )  X  1 X T t and q = ( u T u )  X  1 Y T u can be computed.
 However, it can be shown that the weight vector w also corresp onds to the  X rst eigen vector of the following eigen value problem (H X oskuldsson, 1988) The X-scores t are then given as Similarly , eigen value problems for the extraction of t , u and c estimates can be deriv ed (H X oskuldsson, 1988). The nonlinear kernel PLS metho d is based on mapping the original input data into a high-dimensional feature space F . In this case the vectors w and c cannot be usually computed. Thus, the NIPALS algorithm needs to be reform ulated into its kernel varian t (Lewi, 1995; Rosipal &amp; Trejo, 2001). Alternativ ely, the score vector t can be directly estimated as the  X rst eigen vector of the following eigen value problem (H X oskuldsson, 1988) (this can be easily shown by multiplying both sides of (4) by X matrix and using (5)) The Y-scores u are then estimated as 3.2. Nonlinear Kernel Partial Least Squares Now, consider a nonlinear transformation of x into a feature space F . Using the straigh tforw ard connection between a RKHS and F , Rosipal and Trejo (2001) have extended the linear PLS model into its nonlinear kernel form. E X ectiv ely this extension represen ts the construction of a linear PLS model in F . Denote  X  as the ( n  X  S ) matrix of mapp ed X -space data  X ( x ) into an S -dimensional feature space F . Instead of an explicit mapping of the data, prop erty (2) can be used resulting in where K represen ts the ( n  X  n ) kernel Gram matrix of the cross dot products between all input data points selected kernel function. Similarly , consider a mapping of the second set of variables y into a feature space F 1 and denote by  X  the ( n  X  S 1 ) matrix of mapp ed Y -space data  X ( y ) into an S 1 -dimensional feature space F . Analogous to K de X ne the ( n  X  n ) kernel Gram matrix K 1 given by the kernel function K 1 ( :; : ). Using this nota-tion the estimates of t (6) and u (7) can be reform u-lated into its nonlinear kernel varian t Similar to linear PLS, a zero mean nonlinear kernel PLS model is assumed. To centralize the mapp ed data in a feature space F the following procedure must be applied (Sch X olkopf et al., 1998; Rosipal &amp; Trejo, 2001) where I n is an n -dimensional identity matrix and 1 n represen ts a ( n  X  1) vector with elemen ts equal to one. The same is true for K 1 .
 After the extraction of new score vectors t ; u the ma-trices K and K 1 are de X  X ted by subtracting their rank-one appro ximations based on t and u . The di X er-ent forms of de X  X tion corresp ond to di X eren t forms of PLS (see Wegelin (2000) for a review). Because (4) corresp onds to the singular value decomp osition of the transp osed cross-pro duct matrix X T Y , compu-tation of all eigen vectors from (4) at once involves a sequence of implicit rank-one de X  X tions of the over-all cross-pro duct matrix. Although the weight vectors f w i g p i =1 will be mutually orthogonal the corresp ond-ing score vectors f t i g p i =1 , in general, will not be mu-tually orthogonal. The same is true for the weight vectors f c i g p i =1 and the score vectors f u i g p i =1 form of PLS was used by Sampson et al. (1989) and in accordance with Wegelin (2000) it is denoted as PLS-SB. The kernel analog of PLS-SB results from the computation of all eigen vectors of (8) at once. PLS1 (one of the blocks has single variable) and PLS2 (both blocks are multidimensional) generally used as regres-sion metho ds use a di X eren t form of de X  X tion. The de X  X tion in the case of PLS1 and PLS2 is based on rank-one reduction of the  X  and  X  matrices using a new extracted score vector t at each step. It can be written in the kernel form for K matrix as follows (Rosipal &amp; Trejo, 2001) and in the same way for K 1 . This de X  X tion is based on the fact that the  X  matrix is de X  X ted as  X   X   X   X  tp T =  X   X  tt T  X  , where p is the vector of load-ings corresp onding to the extracted unit norm score vector t . Similarly for the  X  matrix the de X  X tion has the form  X   X   X   X  tc T =  X   X  tt T  X  . In the case of PLS1 and PLS2 score vectors f t i g p i =1 are mutually orthogonal. In general, this is not true for f u i g p i =1 (H X oskuldsson, 1988). Consider a set of N -dimensional samples f x i 2 X  X  R Now de X ne the ( n  X  g  X  1) class mem bership matrix Y to be where f n i g g i =1 denotes the number of samples in each class, P g i =1 n i = n and 0 n R g  X  1 space covariance matrices  X  x and  X  y , respec-tively, and the cross-pro duct covariance matrix  X  xy . Again, the matrices X and Y are considered to be zero mean. Furthermore, let H = P g i =1 n i (  X  x i  X   X  x )(  X  x the among-classes and within-classes sums-of-squares, where  X  x i = 1 n represen ts a N -dimensional vector for the j th sample in the i th class.
 CCA is a metho d which  X nds a pair of linear transfor-mations of each block of data with maximal correla-tion coe X cien t. This can be formally describ ed as the maximization problem where similar to our previous notation the symbols cor r and var denote the sample correlation and vari-ance, respectiv ely. An estimate of the weight vector a is given as the solution of the following eigen value problem (Mardia et al., 1997) where the eigen values  X  corresp onds to the squared canonical correlation coe X cien t.
 Without the assumption of Gaussian distribution of individual classes, Fisher developed a discrimination metho d based on a linear projection of the input data such that among-classes variance is maximized rela-tive to the within-classes variance. The directions onto which the input data are projected are given by the eigen vectors a of the eigen value problem In the case of two-class discrimination with multi-normal distributions with the same covariance matri-ces, Fisher's LDA  X nds the same discrimination direc-tion as LDA using Bayes theorem to estimate posterior class probabilities|the metho d providing the discrim-ination rule with minimal expected misclassi X cation error (Mardia et al., 1997; Hastie et al., 2001). The connection between Fisher's LDA directions and the directions given by CCA using a dumm y matrix Y for group mem bership was  X rst recognized by Bartlett. This connection expressed using the previously de X ned notation was form ulated by Bark er and Rayens (2003) (see also 11.5.4, Mardia et al., 1997) in the following two theorems: Theorem 1 Theorem 2 The proof of the  X rst theorem can by found in Bark er and Rayens (2003). Using the prop erty of the general-ized eigen value problem, Theorem 1 and the fact that ( n  X  1) S x = E + H , the second theorem can be proved. A very close connection between Fisher's LDA, CCA and PLS metho ds for multi-class discrimination has been shown in Bark er and Rayens (2003). This con-nection is based on the fact that PLS can be seen as a form of penalized CCA with penalties given by PCA in X -and Y -spaces. Bark er and Rayens (2003) suggested to remo ve the not meaningful Y -space penalt y var ( Yc ) in the PLS discrimination scenario. This modi X cation in fact rep-resen ts a special case of the previously prop osed or-thonormalized PLS metho d (Worsley , 1997) using the indicator matrix Y . In this case (4) is transformed into the eigen value problem Using Theorem 1 and the fact that S xy = ( n  X  1) X T Y and S y = ( n  X  1) Y T Y the eigen vectors of (10) are equiv alent to the eigensolutions of Thus, this modi X ed PLS metho d is based on eigenso-lutions of the among-classes sum-of-squares matrix H which connects this approac h to CCA or equiv alently to Fisher's LDA. The connection between CCA, Fisher's LDA and PLS motiv ates the use of the orthonormalized PLS metho d for discrimination. The kernel varian t 1 of this ap-proac h will transform (8) into the following equations where ~ Y = Y ( Y T Y )  X  1 = 2 represen ts a matrix of un-correlated and normalized original output variables. Interestingly , in the case of two-class discrimination the direction of the  X rst kernel orthonormalized PLS score vector t is identical with the  X rst score vector found by either the kernel PLS1 or the kernel PLS-SB metho d. This immediately follows from the fact that Y
T Y is a number in this case. In this two-class sce-nario KYY T is of a rank one matrix and kernel PLS-SB extracts only one score vector t . In contrast, ker-nel orthonormalized PLS (or equiv alently kernel PLS1) can extract additional score vectors, up to the rank of K , each being similar to directions computed with CCA and Fisher's LDA on de X  X ted feature space ma-trices. This provides more principled dimensionalit y reduction in comparison to PCA based on the crite-rion of maxim um data variation in the F -space alone. In the case of multi-class discrimination the rank of the Y matrix is equal to g  X  1 which determines the maxi-mum number of score vectors that may be extracted by the kernel orthonormalized PLS-SB metho d. 2 Again, similar to the one-dimensional output scenario the de- X  X tion of the Y matrix at each step can be done us-ing the score vectors t . For simplicit y consider this de X  X tion scheme in the original input space: X 1 = ( I n  X  tt T ) X = P d X , ~ Y 1 = P d ~ Y , where P d = P d T represen ts a projection matrix. Using these de X  X ted matrices X 1 and ~ Y 1 the eigenproblem (10) can be written in the form X T 1 ~ Y ~ Y T X 1 w =  X  w : Thus, similar to the previous two-class discrimination the solution of this eigenproblem can be interpreted as the solution of (11) using the among-classes sum-of-squares matrix now computed on de X  X ted input space matrix X 1 . Kernel varian ts of CCA and Fisher's LDA have been prop osed (Lai &amp; Fyfe, 2000; Mika et al., 1999). Al-though the same relations among CCA, Fisher's LDA and PLS in a feature space F can be considered, ker-nel CCA and kernel Fisher DA su X er from a singularit y problem in the case of higher dimensionalit y S &gt; n . Both algorithms at some point need to invert singular matrices which is avoided by using the regularization concept of adding a small ridge (jitter) parameter on the diagonal of those matrices. The connection be-tween a regularized form of CCA, PLS and orthonor-malized PLS was developed in the context of canonical ridge analysis by Vino d (1976).
 On several classi X cation problems the use of kernel PCA for dimensionalit y reduction or de-noising fol-lowed by linear SVC computed on the reduced F -space data represen tation has shown good results in compar-ison to nonlinear SVC using the original data repre-sentation (Sch X olkopf &amp; Smola, 2002; Sch X olkopf et al., 1998). However, the theory of the previous section suggest to replace the kernel PCA data prepro cess-ing step with a more principled kernel orthonormal-ized PLS approac h. In comparison to kernel Fisher DA this may become more suitable in the situation of non-Gaussian class distribution in a feature space F where more than g  X  1 discrimination directions may better de X ne an overall discrimination rule. The ad-vantage of using linear SVC as the follow up step is motiv ated by the construction of an optimal separating hyperplane in the sense of maximizing of the distance to the closest point from either class (Vapnik, 1998; Sch X olkopf &amp; Smola, 2002). Moreo ver, when the data are not separable the SVC approac h provides a way to control the exten t of this overlap. Thus, the kernel or-thonormalized PLS is combined with the  X  -SVC or the C-SV C (Sch X olkopf &amp; Smola, 2002) classi X er and this metho dology is denoted as kernel PLS-SV C. A pseudo code of the metho d is provided in the Appendix. The usefulness of the kernel PLS-SV C metho d was tested on several benchmark data sets of two-class clas-si X cation and on a real world problem of discriminating and classifying  X nger movemen ts from periods of non-movemen t based on electro encephalograms (EEG). 6.1. Benc hmark Data Sets The data sets used in R X atsch et al. (2001) and Mika et al. (1999) were chosen. The data sets are freely available and can be downloaded from http://www.first.gmd.de/~raetsch . The data sets consist of 100 di X eren t training and testing partitions (except Splice and Image, consisting of 20 partitions). In all cases the Gaussian kernel was used. The un-known parameters (width of the Gaussian kernel, num-ber of PLS score vectors,  X  and C parameters for  X  -SVC and C-SV C, respectiv ely) were selected based on the minim um classi X cation error using  X ve-fold cross validation (CV) on the  X rst  X ve training sets. The results are summarized in Table 1. Very good behavior of kernel PLS-SV C metho d can be observ ed. The null hypothesis about equal means using the C-SVC and kernel PLS-SV C metho ds was tested using a paired t -test (the individual test set classi X cation er-rors for kernel Fisher DA are not available). The non-parametric sign and Wilco xon matc hed-pairs signed-ranks tests were also used to test null hypotheses about the direction and size of the di X erences within pairs. The signi X cance level for all tests was set to  X  = 0 : 05. On  X ve data sets (Banana, Diab etes, Ring-norm, Twonorm, Waveform) the null hypothesis about equal means was rejected. The one-sided alternativ e of both nonparametric tests indicated lower classi X cation errors of the kernel PLS-SV C approac h in all  X ve cases and also on B. Cancer. The paired t -test did not reject the null hypothesis about equal means on the Heart data set, but the one-sided alternativ e of the nonpara-metric tests indicate lower classi X cation errors using C-SV C. The number of selected kernel PLS comp o-nents determined by the CV approac h was lower than 10 except for the Image data set where 27 score vectors were used. A relativ ely large impro vemen t in terms of averaged classi X cation error over kernel Fisher DA can be seen in this case. Interestingly , this superiorit y of kernel PLS-SV C over kernel Fisher DA was also ob-served in the case when only one PLS score vector was used (German, Ringnorm, Twonorm) but not on the Heart data set.
 6.2. Finger Movemen t Detection In an experimen t designed to detect  X nger movemen ts using EEG subjects performed a self-paced single  X n-ger tap about every  X ve seconds (Trejo et al., 2003). In four runs the subject was instructed to alternate between the pinkie and index  X ngers on a single hand. Half of those runs were left and half were right hand only. In two runs the subject was then instructed to alternate between both hands keeping the same time separation between taps. Each run contained appro xi-mately 50 single taps. 62-channel EEG and 2-channel electro oculogram were recorded using a Neuroscan 64 channel EEG cap with two 32 channel syn-amps sampled at 1000 Hz. The electrom yogram was also recorded using two electro des placed on each wrist. The raw EEG was cut into one-second intervals with 300 ms before the motion and 700 ms after the be-ginning of the motion. The intervals were down sam-pled from 1000 to 128 data points using the Matlab routine resample . Both right and left hand intervals were labeled as motion and classi X ed against periods of non-motion of equal length using the kernel PLS-SV C classi X er and  X  -SVC classi X er alone. The experimen t with the same subject was repeated two times with an interval of 56 days between the sessions. These days are denoted Day 1 and Day 2, respectiv ely. Due to impedance problems with one of the electro des (O 1 ) during the second day session only 61 channels of EEG were used. A total of 225 periods of movemen t and 579 periods of non-mo vemen t was extracted for Day 1 and 288 movemen t periods versus 657 non-mo vemen t peri-ods for Day 2. The dimensionalit y of each period was 7808 (61 electro des times 128 time points). The accuracy to classify both  X nger movemen t and non-mo vemen t periods on data measured during Day 2 was based on the linear kernel PLS-SV C model. The model was trained on Day 1 data. The same was done using Day 2 data to predict Day 1. The parameters for the kernel PLS-SV C models were estimated using 10-fold CV on each day's data separately . First, the number of PLS score vectors was  X xed and the  X  pa-rameter was estimated. In Fig. 1 the dependence of the correct classi X cation rate on the number of selected PLS score vectors is depicted. The asterisks indicate the correct classi X cation rate when the  X nal number of the PLS score vectors was determined using the CV approac h. The graphs indicate that a classi X cation ac-curacy of over 90% can be achieved. Using a range of  X  values for  X  -SVC a maxim um correct classi X cation rate for the Day 1 to Day 2 scenario was 93.0% and 90.7% for the Day 2 to Day 1 scenario. The results with the nonlinear kernel PLS-SV C model using Gaussian ker-nel do not indicate impro vemen t in comparison to its linear varian t.
 In the prop osed discrimination scenario the individ-ual PLS score vectors can be considered as di X eren t spatio-temp oral processes with respect to di X eren tia-tion between movemen t and non-mo vemen t periods. In the case of linear kernel PLS the corresp onding weight vectors w (eq. (4)) can be computed and their values re X  X ct importan t time points and spatial loca-tions with respect to discrimination. These weight vectors were plotted as scalp topographical maps at di X eren t times relativ e to  X nger movemen t (for exam-ple, Fig. 2). Based on the close visual inspection of these maps 16 EEG channels were selected out of all 61 channels. The selected electro des were located pre-dominan tly over the right-hand side sensori-motor area (white areas in Fig. 2). Four electro des from the oc-cipital area (two on each side) were also selected (pos-terior black areas in Fig. 2). The results using this reduced number of electro des are plotted in Fig. 1. In both cases the plots indicate comparable results to those achieved with the full EEG montage. However, in the second case when the Day 2 to Day 1 prediction scenario was used a reduced setting of the electro des has a tendency of over X tting for a higher number of used comp onen ts. To further justify this electro de re-duction 100 di X eren t training and testing partitions with the ratio of splits 40:60% were created. This was done for each day indep enden tly. Using 10-fold CV on both the full and reduced montage training parti-tions, linear kernel PLS-SV C models were compared in terms of correct classi X cation rates achieved on 100 test partitions. For Day 1 the averaged correct classi- X cation for reduced set of electro des was 89.6%  X  1 : 5 in comparison to 89.3%  X  1 : 4 using the full EEG mon-tage. For Day 2 the results were 91.9%  X  1 : 3 and 92.2%  X  1 : 1, respectiv ely. In both cases, using the paired t -test, the null hypothesis about equal means was not rejected ( p -values &gt; 0.05). A new kernel PLS-SV C classi X cation technique was prop osed. Results achieved on 13 benchmark data sets demonstrate usefulness of the prop osed metho d and its comp etitiv eness with other state-of-the-art classi X -cation metho ds. On six benchmark data sets a statis-tically signi X can t superiorit y of kernel PLS-SV C over C-SV C was observ ed. In contrast, this tendency was observ ed only in one case for C-SV C. In terms of aver-aged classi X cation error the superiorit y of kernel PLS-SVC over kernel Fisher DA was observ ed in 10 out of 13 benchmark data sets. In seven cases this was achieved using more than one score vector, which sug-gests, that a single direction extracted by kernel Fisher DA on these data sets is not adequate to discriminate two di X eren t classes.
 In the case of  X nger movemen t detection from EEG, a linear kernel PLS approac h provided a way to|in practice desirable|reduce the number of used elec-trodes without the degradation of the classi X cation accuracy . It is the topic of a curren t more detailed study to analyze the individual spatio-temp oral pro-cesses as de X ned by the extracted PLS score vectors. This would provide a more principled way for the se-lection of importan t spatial and temp oral changes dur-ing the  X nger motion. The topographical maps con-structed using the weight vectors of the constructed  X  -SVC models (one weight vector for each model) have shown similarit y between the plots using the weight vectors corresp onding to the  X rst PLS score vectors. However, these weight vectors represen t a \global" dis-crimination of spatio-temp oral processes. Moreo ver, they are computed using the supp ort vectors only. A theoretical connection between Fisher's LDA, CCA and PLS was describ ed. This connection indicates that in the case of dimensionalit y reduction with respect to discrimination in F the kernel orthonormalized PLS metho d should be preferred over kernel PCA. Bark er, M., &amp; Rayens, W. S. (2003). Partial least squares for discrimination. Journal of Chemomet-rics , 17 , 166{173. de Jong, S., Wise, B. M., &amp; Ricker, N. L. (2001).
Canonical partial least squares and continuum power regression. Journal of Chemometrics , 15 , 85{ 100.
 Hastie, T., Tibshirani, R., &amp; Friedman, J. (2001). The Elements of Statistic al Learning . Springer. H X oskuldsson, A. (1988). PLS Regression Metho ds. Journal of Chemometrics , 2 , 211{228.
 Lai, P. L., &amp; Fyfe, C. (2000). Kernel and Nonlinear
Canonical Correlation Analysis. International Jour-nal of Neur al Systems , 10 , 365.
 Lewi, P. J. (1995). Pattern recognition, re X  X ction from a chemometric point of view. Chemometrics and Intelligent Laboratory Systems , 28 , 23{33. Mardia, K. V., Kent, J. T., &amp; Bibb y, J. M. (1997). Multivariate Analysis . Academic Press.
 Mika, S., R X atsch, G., Weston, J., Sch X olkopf, B., &amp;
M X  uller, K. R. (1999). Fisher discriminan t analysis with kernels. Neur al Networks for Signal Processing IX (pp. 41{48).
 R X atsch, G., Onoda, T., &amp; M X  uller, K. R. (2001). Soft margins for AdaBo ost. Machine Learning , 42 , 287{ 320.
 Rosipal, R., &amp; Trejo, L. J. (2001). Kernel Partial Least Squares Regression in Repro ducing Kernel Hilbert
Space. Journal of Machine Learning Research , 2 , 97{123.
 Sampson, P. D., Streissguth, A. P., Barr, H. M., &amp;
Bookstein, F. L. (1989). Neurob ehavioral e X ects of prenatal alcohol: Part II. Partial Least Squares analysis. Neur otoxic ology and tetralogy , 11 , 477{491. Sch X olkopf, B., &amp; Smola, A. J. (2002). Learning with Kernels { Supp ort Vector Machines, Regularization, Optimization and Beyond . The MIT Press.
 Sch X olkopf, B., Smola, A. J., &amp; M X  uller, K. R. (1998).
Nonlinear Comp onen t Analysis as a Kernel Eigen-value Problem. Neur al Computation , 10 , 1299{1319. Trejo, L. J., Wheeler, K., Jorgensen, C., Rosipal, R.,
Clan ton, S., Matthews, B., Hibbs, A., Matthews, R., &amp; Krupk a, M. (in press, 2003). Multimo dal Neuro-electric Interface Developmen t. IEEE Transactions on Neur al Systems and Rehabilitation Engine ering . Vapnik, V. N. (1998). Statistic al Learning Theory . New York: Wiley .
 Vino d, H. D. (1976). Canonical ridge and economet-rics of joint production. Journal of Econometrics , 4 , 147{166.
 Wegelin, J. A. (2000). A survey of Partial Least
Squar es (PLS) metho ds, with emphasis on the two-block case (Technical Report). Departmen t of Statis-tics, Univ ersity of Washington, Seattle.
 Wold, H. (1975). Soft Modeling by Laten t Variables; the Nonlinear Iterativ e Partial Least Squares Ap-proac h. In J. Gani (Ed.), Persp ectives in Probability and Statistics, Papers in Honour of M.S. Bartlett , 520{540. Academic Press, London.
 Wold, S., Ruhe, H., Wold, H., &amp; Dunn III, W. J. (1984). The collinearit y problem in linear regres-sion. The partial least squares (PLS) approac h to generalized inverse. SIAM Journal of Scienti X c and Statistic al Computations , 5 , 735{743.
 Worsley , K. J. (1997). An overview and some new developmen ts in the statistical analysis of PET and fMRI data. Human Brain Mapping , 5 , 254{258. In the case of one-dimensional output SIMPLS algo-rithm provides the same solution than PLS1. Thus, for two-class classi X cation a computationally more ef- X cien t SIMPLS algorithm can be used (de Jong et al., 2001). This is based on the fact that in this case t / KY . The kernel PLS-SV C algorithm can be then de X ned in three major steps: 1) kernel PLS comp onen ts extraction 2) projection of test samples (Rosipal &amp; Trejo, 2001) 3)  X  -SVC or C-SV C build on score vectors T , T t In the case of multi-class classi X cation ( g &gt; 2) a kernel varian t of the NIPALS algorithm (Rosipal &amp; Trejo, 2001) with uncorrelated outputs ~ Y or eigenproblem (12) has to be solved to extract f t i g p i =1 and f u i
