 Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and ac-count for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a novel hierarchical recurrent encoder-decoder ar-chitecture that makes possible to account for sequences of previous queries of arbitrary lengths. As a result, our sug-gestions are sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced sug-gestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered fea-ture sets. Results show that our model outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our architecture is general enough to be used in a variety of other applications. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Query formulation Keywords: Recurrent Neural Networks; Query Suggestion.
Modern search engines heavily rely on query suggestions to support users during their search task. Query suggestions can be in the form of auto-completions or query reformula-tions. Auto-completion suggestions help users to complete their queries while they are typing in the search box. In this paper, we focus on query reformulation suggestions, that are produced after one or more queries have already been submitted to the search engine.
 c  X 
Search query logs are an important resource to mine user reformulation behaviour. The query log is partitioned into query sessions, i.e. sequences of queries issued by a unique user and submitted within a short time interval. A query session contains the sequence of query reformulations issued by the user while attempting to complete the search mis-sion. Therefore, query co-occurrence in the same session is a strong signal of query relatedness and can be straightfor-wardly used to produce suggestions.

Methods solely relying on query co-occurrence are prone to data sparsity and lack coverage for rare and long-tail queries, i.e. unseen in the training data. A suggestion sys-tem should be able to translate infrequent queries to more common and effective formulations based on similar queries that have been seen in the training data. Amongst the inter-esting models that have been proposed, some capture higher order collocations [7], consider additional resources [20, 41], move towards a word-level representation [8, 9] or describe queries using a rich feature space and apply learning to rank techniques to select meaningful candidates [28, 34].
An additional desirable property of a suggestion system is context-awareness . Pairwise suggestion systems operate by considering only the most recent query. However, previous submitted queries provide useful context to narrow down ambiguity in the current query and to produce more focused suggestions [22]. Equally important is the order in which past queries are submitted, as it denotes generalization or specification reformulation patterns [18]. A major hurdle for current context-aware models is dealing with the dramatic growth of diverse contexts, since it induces sparsity, and classical count-based models become unreliable [10, 15].
Finally, relatively unexplored for suggestion systems is the ability to produce synthetic suggestions. Typically, we as-sume that useful suggestions are already present in the train-ing data. The assumption weakens for rare queries or com-plex information needs, for which it is possible that the best suggestion has not been previously seen [20, 40]. In these cases, synthetic suggestions can be leveraged to increase cov-erage and can be used as candidates in complex learning to rank models [28].

We present a generative probabilistic model capable of producing synthetic, context-aware suggestions not only for popular queries, but also for long tail queries. Given a se-quence of queries as prefix, it predicts the most likely se-quence of words that follow the prefix. Variable context lengths can be accounted for without strict built-in limits. Topically similar terms and queries are close in the embedding space. Query suggestions can be mined by sampling likely contin-uations given one or more queries as context. Prediction is efficient and can be performed using standard natural lan-guage processing word-level decoding techniques [23]. The model is robust to long-tail effects as the prefix is considered as a sequence of words that share statistical weight and not as a sequence of atomic queries.

As an example, given a user query session composed of two queries cleveland gallery  X  lake erie art issued sequentially, our model predicts sequentially the words cleveland , indian , art and  X  , where  X  is a special end-of-query symbol that we artificially add to our vocabulary. As the end-of-query token has been reached, the suggestion given by our model is cleveland indian art . The suggestion is contextual as the concept of cleveland is justified by the first query thus the model does not merely rely on the most recent query only. Additionally, the produced suggestion is synthetic as it does not need to exist in the training set.

To endow our model with such capabilities, we rely on recent advances in generative natural language applications with neural networks [3, 11, 27]. We contribute with a new hierarchical neural network architecture, called hierarchical recurrent encoder-decoder (HRED), that allows to embed a complex distribution over sequences of sentences within a compact parameter space. Differently from count-based models, we avoid data sparsity by assigning single words, queries and sequences of queries to embeddings, i.e. dense vectors bearing syntactic and semantic characteristics (Fig-ure 1) [4]. Our model is compact in memory and can be trained end-to-end on query sessions. We envision future ap-plications to various tasks, such as query auto-completion, query next-word prediction and general language modeling.
Suggestion models need to capture the underlying sim-ilarities between queries. Vector representations of words and phrases, also known as embeddings , have been success-fully used to encode syntactic or semantic characteristics thereof [3, 4, 25, 35]. We focus on how to capture query similarity and query term similarity by means of such em-beddings. In Figure 1 (a) and (b), we plot a two-dimensional projection of the word and query embeddings learnt by our model. The vectors of topically similar terms or queries are close to each other in the vector space.

Vector representations for phrases can be obtained by av-eraging word vectors [25]. However, the order of terms in queries is usually important [38]. To obtain an order-sensitive representation of a query, we use a particular neu-ral network architecture called Recurrent Neural Network (RNN) [3, 26]. For each word in the query, the RNN takes as input its embedding and updates an internal vector, called recurrent state, that can be viewed as an order-sensitive summary of all the information seen up to that word. The first recurrent state is usually set to the zero vector. After the last word has been processed, the recurrent state can be considered as a compact order-sensitive encoding of the query (Figure 2 (a)).

A RNN can also be trained to decode a sentence out of a given query encoding. Precisely, it parameterizes a con-ditional probability distribution on the space of possible queries given the input encoding. The process is illustrated in Figure 2 (b). The input encoding may be used as initial-ization of the recurrence. Then, each of the recurrent states is used to estimate the probability of the next word in the sequence. When a word is sampled, the recurrent state is updated to take into account the generated word. The pro-cess continues until the end-of-query symbol  X  is produced.
The previous two use cases of RNNs can be pipelined into a single recurrent encoder-decoder, as proposed in [11, 39] for Machine Translation purposes. The architecture can be used to parameterize a mapping between sequences of words. This idea can be promptly casted in our framework by pre-dicting the next query in a session given the previous one. With respect to our example, the query encoding estimated by the RNN in Figure 2 (a) can be used as input to the RNN in Figure 2 (b): the model learns a mapping between the consecutive queries cleveland gallery and lake erie art . At test time, the user query is encoded and then decoded into likely continuations that may be used as suggestions.
Although powerful, such mapping is pairwise, and as a result, most of the query context is lost. To condition the prediction of the next query on the previous queries in the session, we deploy an additional, session-level RNN on top of the query-level RNN encoder, thus forming a hierarchy of RNNs (Figure 3). The query-level RNN is responsible to cleveland gallery Figure 2: (a) An encoder RNN processing the query cleveland gallery followed by a special end-of-query symbol  X  . Each solid arrow represents a non-linear transformation. (b) A decoder RNN generating the next query in the session, lake erie art , from a query encoding as input. encode a query. The session-level RNN takes as input the query encoding and updates its own recurrent state. At a given position in the session, the session-level recurrent state is a learnt summary of the past queries, keeping the informa-tion that is relevant to predict the next one. At this point, the decoder RNN takes as input the session-level recurrent state, thus making the next query prediction contextual.
The contribution of our hierarchical recurrent encoder-decoder is two-fold. The query-level encoder RNN maps similar queries to vectors close in the embedding space (Fig-ure 1 (b)). The mapping generalizes to queries that have not been seen during training, as long as their words appear in the model vocabulary. This allows the model to map rare queries to more useful and general formulations, well beyond past co-occurred queries. The session-level RNN models the sequence of the previous queries, contextualizing the predic-tion of the next query. Similar contexts are mapped close to each other in the vector space. This property allows to avoid sparsity, and differently from count-based models [10, 15], to account for contexts of arbitrary length.
We start by presenting the technical details of the RNN architecture, which our model extends. We consider a query session as a sequence of M queries S = { Q 1 ,...,Q M } sub-mitted by a user in chronological order, i.e. Q m &lt; t Q where &lt; t is the total order generated by the submission time, and within a time frame, usually 30 minutes. A query Q m a sequence of words Q m = { w m, 1 ,... ,w m,N m } , where N is the length of query m . V is the size of the vocabulary.
For each query word w n , a RNN computes a dense vec-tor called the recurrent state, denoted h n , that combines w n with the information that has already been processed, i.e. the recurrent state h n  X  1 . Formally: where h n  X  R d h , d h is the number of dimensions of the recur-rent state, f is non-linear transformation and the recurrence is seeded with the 0 vector. The recurrent state h n acts as a compact summary of the words seen up to position n .
Usually, f consists of a non-linear function, i.e. the lo-gistic sigmoid or hyperbolic tangent, applied element-wise to a time-independent affine transformation [26]. The com-plexity of the function f has an impact on how accurately the RNN can represent sentence information for the task at hand. To reduce the fundamental difficulty in learning long-term dependencies [5], i.e. to store information for longer se-quences, more complex functions have been proposed such as the Long Short-Term Memory (LSTM) [17] and the Gated Recurrent Unit (GRU) [11].

Once Eq. 1 has been run through the entire query, the recurrent states h 1 ,...,h N can be used in various ways. In an encoder RNN, the last state h N may be viewed as an order-sensitive compact summary of the input query. In a decoder RNN, the recurrent states are used to predict the next word in a sequence [11, 26]. Specifically, the word at position n is predicted using h n  X  1 . The probability of seeing word v at position n is: sociated to word i , i.e. a word embedding, and the denomi-nator is a normalization factor. A representation of the em-beddings learnt by our model is given in Figure 1 (a). The semantics of Eq. 2 dictates that the probability of seeing word v at position n increases if its corresponding embed-ding vector o v is  X  X ear X  the context encoded in the vector h n  X  1 . The parameters of the RNN are learned by maximiz-ing the likelihood of the sequence, computed using Eq. 2.
We choose to use the Gated Recurrent Unit (GRU) as our non-linear transformation f . GRUs have been demon-strated to achieve better performance than simpler param-eterizations at an affordable computational cost [11]. This function reduces the difficulties in learning our model by eas-ing the propagation of the gradients. We let w n denote the one-hot representation of w n = v , i.e. a vector of the size of the vocabulary with a 1 corresponding to the index of the query word v . The specific parameterization of f is given by: r n =  X  ( I r w n + H r h n  X  1 ) , (reset gate) u n =  X  ( I u w n + H u h n  X  1 ) , (update gate)  X  h n = tanh( Iw n + H ( r n  X  h n  X  1 )) , (candidate update) h n = (1  X  u n )  X  h n  X  1 + u n  X   X  h n , (final update) where  X  is the logistic sigmoid,  X  ( x )  X  [0 , 1],  X  represents the element-wise scalar product between vectors, I,I u ,I R the word w n while the H matrices specialize in retaining or forgetting the information in h n  X  1 . In the following, this function will be noted GRU ( h n  X  1 ,w n ).

The gates r n and u n are computed in parallel. If, given the current word, it is preferable to forget information about the past, i.e. to reset parts of h n , the elements of r n pushed towards 0. The update gate u n plays the opposite role, i.e. it judges whether the current word contains rele-vant information that should be stored in h n . In the final update, if the elements of u n are close to 0, the network discards the update  X  h n and keeps the last recurrent state h n  X  1 . The gating behaviour provides robustness to noise in the input sequence: this is particularly important for IR as it allows, for example, to exclude from the summary non-discriminative terms appearing in the query.
Our hierarchical recurrent encoder-decoder (HRED) is pic-tured in Figure 3. Given a query in the session, the model encodes the information seen up to that position and tries to predict the following query. The process is iterated through-out all the queries in the session. In the forward pass, the model computes the query-level encodings, the session-level recurrent states and the log-likelihood of each query in the session given the previous ones. In the backward pass, the gradients are computed and the parameters are updated.
For each query Q m = { w m, 1 ,...,w m,N m } in the training session S , the query-level RNN reads the words of the query sequentially and updates its hidden state according to: where GRU enc is the query-level encoder GRU function in Eq. 3, h m,n  X  R d h and h m, 0 = 0, the null vector. The recur-rent state h m,N m is a vector storing order-sensitive informa-tion about all the words in the query. To keep the notation uncluttered, we denote by q m  X  h m,N m the vector for query m . In summary, the query-level RNN encoder maps a query to a fixed-length vector. Its parameters are shared across the queries. Therefore, the obtained query representation q m is a general, acontextual representation of query m . The computation of the q 1 ,...,q M can be performed in paral-lel, thus lowering computational costs. A projection of the generated query vectors is provided in Figure 1 (b).
The session-level RNN takes as input the sequence of query representations q 1 ,...,q M and computes the sequence of session-level recurrent states. For the session-level RNN, we also use the GRU function: dimensionality and s 0 = 0. The number of session-level re-current states s m is M , the number of queries in the session.
The session-level recurrent state s m summarizes the queries that have been processed up to position m . Each s m bears a particularly powerful characteristic: it is sensitive to the order of previous queries and, as such, it can potentially en-code order-dependent reformulation patterns such as gener-alization or specification of the previous queries [18]. Addi-tionally, it inherits from the query vectors q m the sensitivity to the order of words in the queries. The RNN decoder is responsible to predict the next query Q m given the previous queries Q 1: m  X  1 , i.e. to estimate the probability: The desired conditioning on previous queries is obtained by initializing the recurrence of the RNN decoder with a non-linear transformation of s m  X  1 : mary into the decoder space and b 0  X  R d h . This way, the information about previous queries is transferred to the de-coder RNN. The recurrence takes the usual form: where GRU dec is the decoder GRU, d m,n  X  R d h [11]. In a RNN decoder, each recurrent state d m,n  X  1 is used to com-pute the probability of the next word w m,n . The probability of word w m,n given previous words and queries is: where o v  X  R d e is the output embedding of word v and  X  is a function of both the recurrent state at position n and the last input word: the first word of Q m , we set w m, 0 = 0, the 0 vector. Instead of using the recurrent state directly as in Eq. 2, we add another layer of linear transformation  X  . The E o parameter accentuates the responsibility of the previous word to predict the next one. This formulation has shown to be beneficial for language modelling tasks [29, 11, 26]. If o v is  X  X ear X  the vector  X  ( d m,n  X  1 ,w m,n  X  1 ) the word v has high probability under the model.
The model parameters comprise the parameters of the three GRU functions, GRU enc , GRU dec , GRU ses , the output parameters H o ,E o ,b o and the V output vectors o i . These are learned by maximizing the log-likelihood of a session S , defined by the probabilities estimated with Eq. 6 and Eq 9: The gradients of the objective function are computed using the back-propagation through time (BPTT) algorithm [32]. corresponds to an inference problem. A user submits the sequence of queries S = { Q 1 ,...,Q M } . A query suggestion is a query Q  X  such that: where Q is the space of possible queries, i.e. the space of sentences ending by the end-of-query symbol. The solution to the problem can be approximated using standard word-level decoding techniques such as beam-search [11, 23]. We iteratively consider a set of k best prefixes up to length n as candidates and we extend each of them by sampling the most probable k words given the distribution in Eq. 9. We obtain k 2 queries of length n + 1 and keep only the k best of them. The process ends when we obtain k well-formed queries containing the special end-of-query token  X  . Example. Consider a user who submits the queries cleve-land gallery  X  lake erie artist . The suggestion system pro-ceeds as follows. We apply Eq. 4 to each query obtaining the pute the session-level recurrent states by applying Eq. 5 to the query vectors. At this point, we obtain two session-level erate context-aware suggestions, we start by mapping the
Table 1: HRED suggestions given the context. last session-level recurrent state, s lake erie art , into the initial decoder input d 0 using Eq. 7. We are ready to start the sam-pling of the suggestion. Let us assume that the beam-search size is 1. The probability of the first word w 1 in the sugges-tion is computed using Eq. 9 by using d 0 and w 0 = 0, the null vector. The word with the highest probability, i.e. cleveland , is added to the beam. The next decoder recurrent state d is computed by means of Eq. 8 using d 0 and w 1 = cleveland . Using d 1 , we are able to pick w 2 = indian as the second most likely word. The process repeats and the model selects art and  X  . As soon as the end-of-query symbol is sampled, the context-aware suggestion cleveland indian art is presented to the user. In Table 1 we give an idea of the generated suggestions for 2 contexts in our test set.
 suggestion conditioned on the history of previous queries through Eq. 6. This makes our model integrable into more complex suggestion systems. In the next section, we choose to evaluate our model by adding the likelihood scores of candidate suggestions as additional features into a learning-to-rank system.
We test how well our query suggestion model can pre-dict the next query in the session given the history of previ-ous queries. This evaluation scenario aims at measuring the ability of a model to propose the target next query, which is assumed to be one desired by the user. We evaluate this with a learning-to-rank approach (explained in Section 4.3), similar to the one used in [27, 36] for query auto-completion and in [28, 34] for query suggestion. We first generate candi-dates using a co-occurrence based suggestion model. Then, we train a baseline ranker comprising a set of contextual features depending on the history of previous queries as well as pairwise features which depend only on the most recent query. The likelihood scores given by our model are used as additional features in the supervised ranker. At the end, we have three systems: (1) the original co-occurrence based ranking, denoted ADJ; (2) the supervised context-aware ranker, which we refer to as Baseline Ranker; and (3) a supervised ranker with our HRED feature. We evaluate the performance of the model and the baselines using mean reciprocal rank (MRR). This is common for tasks whose ground truth is only one instance [22, 27].
We conduct our experiments on the well-known search log from AOL, which is the only available search log that is large enough to train our model and the baselines. The queries in this dataset were sampled between 1 March, 2006 Table 2: Full statistics about training time, memory impact and decoding time with a beam size of 50. and 31 May, 2006. In total there are 16,946,938 queries submitted by 657,426 unique users. We remove all non-alphanumeric characters from the queries, apply a spelling corrector and lowercasing. After filtering, we sort the queries by timestamp and we use those submitted before 1 May, 2006 as our background data to estimate the proposed model and the baselines. The next two weeks of data are used as a training set for tuning the ranking models. The remaining two weeks are split into the validation and the test set. We define the end of a session by a 30 minute window of idle time [21]. After filtering, there are 1,708,224 sessions in the background set, 435,705 in the training set, 166,836 in the validation set and 230,359 in the test set.
The most frequent 90 K words in the background set form our vocabulary V . This is a common setting for RNN ap-plied to language and allows to speed-up the repeated sum-mations over V in Eq. 9 [11, 39]. Parameter optimization is done using mini-batch RMSPROP [13]. We normalize the gradients if their norm exceeds a threshold c = 1 [29]. The training stops if the likelihood of the validation set does not improve for 5 consecutive iterations. We train our model 1 using the Theano library [2, 6]. The dimensionality of the query-level RNN is set to d h = 1000. To memo-rize complex information about previous queries, we ensure a high-capacity session-level RNN by setting d s = 1500. The output word embeddings o i are 300 dimensional vec-tors, i.e. d e = 300. Our model is compact and can easily fit in memory (Table 2). The complexity of the decoding is largely dominated by the computation of the output prob-abilities, giving O ( nkV d e ), where n is the generated query length and k the beam size. In the future, the computational cost may be greatly reduced by employing locality sensitive hashing (LSH) based techniques [37].
Given a session S = { Q 1 ,...,Q M } , we aim to predict the target query Q M given the context Q 1 ,...,Q M  X  1 . Q M  X  1 called the anchor query and will play a crucial role in the se-lection of the candidates to rerank. To probe different capa-bilities of our model, we predict the next query in three sce-narios: (a) when the anchor query exists in the background data (Section 4.4); (b) when the context is perturbed with overly common queries (Section 4.5); (c) when the anchor is not present in the background data (Section 4.6).

For each session, we select a list of 20 possible candidates to rerank. The exact method used to produce the candidates will be discussed in the next sections. Once the candidates are extracted, we label the true target as relevant and all the others as non-relevant. We choose to use one of the state-of-the-art ranking algorithms LambdaMART as our supervised ranker, which is the winner in the Yahoo! Learning to Rank
An implementation of the model is available at https://github.com/sordonia/hred-qs . Figure 4: Proportion (%) of short (2 queries), medium (3 or 4 queries) and long (at least 5 queries) sessions in our test scenarios.
 Challenge in 2010 [43]. We tune the LambdaMART model with 500 trees and the parameters are learnt using standard separate training and validation set.

We describe the set of pairwise and contextual features (17 in total) used to train a supervised baseline prediction model, denoted Baseline Ranker. The Baseline Ranker is a competitive system comprising features that are comparable with the ones described in the literature for query auto-completion [22, 27] and next-query prediction [15]. suggestion, we count how many times it follows the anchor query in the background data and add this count as a fea-ture. Additionally, we use the frequency of the anchor query in the background data. Following [22, 28] we also add the Levenshtein distance between the anchor and the suggestion. Suggestion features include: the suggestion length (charac-ters and words) and its frequency in the background set. tures corresponding to the character n -gram similarity be-tween the suggestion and the 10 most recent queries in the context. We add the average Levenshtein distance between the suggestion and each query in the context [22]. We use the scores estimated using the context-aware Query Vari-able Markov Model (QVMM) [15] as an additional feature. QVMM models the context with a variable memory Markov model able to automatically back-off shorter query n -grams if the exact context is not found in the background data. decoder contributes one additional feature corresponding to the log-likelihood of the suggestion given the context, as de-tailed in Section 3.4.
For each session in the training, validation and test set, we extract 20 queries that most likely follow the anchor query in the background data, i.e. with the highest ADJ score. The session is included if and only if at least 20 queries have been extracted and the target query appears in the candidate list. In that case, the target query is the positive candidate and the 19 other candidates are the negative examples. Note that a similar setting has been used in [22, 27] for query auto-completion. We have 18,882 sessions in the training set, 6,988 sessions in the validation set and 9,348 sessions in the Method MRR  X % ADJ 0.5334 -
Baseline Ranker 0 . 5563 +4.3% + HRED 0.5749 +7.8%/+3.3% Table 3: Next-query prediction results. All im-provements are significant by the t-test ( p &lt; 0.01). test set. The distribution of the session length is reported in Figure 4. The scores obtained by the ADJ counts are used as an additional non-supervised baseline.
 model and the baselines. Baseline Ranker achieves a relative improvement of 4 . 3% with respect to the ADJ model. We find that the HRED feature brings additional gains achiev-ing 7 . 8% relative improvement over ADJ. The differences in performance with respect to ADJ and the Baseline Ranker are significant using a t -test with p &lt; 0 . 01. In this general next-query prediction setting, HRED boosts the rank of the first relevant result.
 have an impact on the performance of context-aware models. In Figure 5, we report separate results for short (2 queries), medium (3 or 4 queries) and long sessions (at least 5 queries). HRED brings statistically significant improvements across all the session lengths. For short sessions, the improvement is marginal but consistent even though only a short context is available in this case. The semantic mapping learnt by the model appears to be useful, even in the pairwise case. ADJ is affected by the lack of context-awareness and suffers a dra-matic loss of performance with increasing session length. In the medium range, context-aware models account for previ-ous queries and achieve the highest performance. The trend is not maintained for long sessions, seemingly the hardest for the Baseline Ranker. Long sessions can be the result of complex search tasks involving a topically broad informa-tion need or changes of search topics. Beyond the intrinsic difficulty in predicting the target query in these cases, exact context matches may be too coarse to infer the user need. Count-based methods such as QVMM meet their limitations due to data sparsity. In this difficult range, HRED achieves its highest relative improvement with respect to both ADJ (+15%) and the Baseline Ranker (+7%), thus showing ro-bustness across different session lengths.
 mance obtained by HRED on long sessions can be obtained using a shorter context. For each long session in our test set, we artificially truncate the context to make the predic-tion depend on the anchor query, Q M  X  1 , only (1 query), on Q M  X  2 and Q M  X  1 (2 queries), on 3 queries and on the entire context. When one query is considered, our model behaves similarly to a pairwise recurrent encoder-decoder model trained on consecutive queries. Figure 6 shows that when only one query is considered, the performance of HRED is similar to the Baseline Ranker (0.529) which uses the whole context. HRED appears to perform best when the whole context is considered. Additional gains can be ob-tained by considering more than 3 queries, which highlights the ability of our model to consider long contexts. Figure 5: Next-query performance in short, medium and long sessions. All differences in MRR are sta-tistically significant by the t-test ( p &lt; 0.01). Figure 6: Variation of HRED performance with re-spect to the number of previous queries considered. The evolution is computed on long sessions.
Query sessions contain a lot of common and navigational queries such as google or facebook which do not correspond to a specific search topic. A context-aware suggestion sys-tem should be robust to noisy queries and learn to discard them from the relevant history that should be retained. We propose to probe this capability by formulating an original robust prediction task as follows. We label the 100 most frequent queries in the background set as noisy 2 . For each entry in the training, validation and test set of the previous next-query prediction task, we corrupt its context by insert-ing a noisy query at a random position. The candidates and the target rest unchanged. The probability of sampling a noisy query is proportional to its frequency in the back-ground set. For example, given the context airlines  X  united airlines and the true target delta airlines , the noisy sample google is inserted at a random position, forcing the models to predict the target given the corrupted context airlines  X  united airlines  X  google .
 affects the performance of ADJ. Cases in which the corrup-tion occurred at the position of the anchor query severely harm pairwise models. The Baseline Ranker achieves sig-nificant gains over ADJ by leveraging context matches. Its
A similar categorization has been proposed in [31]. Method MRR  X % ADJ 0.4507 -
Baseline Ranker 0 . 4831 +7,2% + HRED 0.5309 +17,8%/+9.9% Table 4: Robust prediction results. The improve-ments are significant by the t-test ( p &lt; 0.01). Figure 7: Magnitude of the elements in the session-level update gates. The darker the image, the more the model discards the current query. The vector corresponding to google , u g , is darker, i.e. the net-work mainly keeps its previous recurrent state. performance is inferior to the baseline ADJ performance in the next-query setting reported in Table 3 (0.5334). HRED appears to be particularly effective in this difficult setting achieving a relative improvement of 17 . 8% over ADJ and 9 . 9% over the Baseline Ranker, both statistically significant. Comparative to the next-query task, the improvements over ADJ and the Baseline Ranker are 2.5 and 3 times higher respectively. Our model appears to be more robust than the baselines in these extreme cases and can better reduce the impact of the noisy query.
 bring little information to predict future queries in the ses-sion, HRED may automatically learn to be robust to the noise at training time. The hierarchical structure allows to decide, for each query, if it is profitable to account for its contribution to predict future queries. This capability is sustained by the session-level GRU, which can ignore the noisy queries by  X  X urning-off X  the update gate u n when they appear (see Section 3.1.1). Given the corrupted context airlines  X  united airlines  X  google , the session-level GRU computes three update gate vectors: u a , u ua , u g , each cor-responding to a position in the context. In Figure 7, we plot the magnitude of the elements in these vectors. As the model needs to memorize the initial information, u a shows a significant number of non-zero (bright) entries. At this point, general topical information has already been stored in the first recurrent state. Hence, u ua shows a larger num-ber of zero (dark) entries. When google is processed, the network tends to keep past information in memory by fur-ther zeroing entries in the update gate. Interestingly, this mechanism may be used to address other search log related tasks such as session-boundary detection.
To analyze the performance of the models in the long-tail, we build our training, validation and test set by retaining the sessions for which the anchor query has not been seen in the background set, i.e. it is a long-tail query. In this case, we cannot leverage the ADJ score to select candidates Method MRR  X % ADJ 0.3830 -
Baseline Ranker 0 . 6788 +77.2% + HRED 0.7112 +85.3% / +5.6% Table 5: Long-tail prediction results. The improve-ments are significant by the t-test ( p &lt; 0.01). to rerank. For each session, we iteratively shorten the an-chor query by dropping terms until we have a query that appears in the background data. If a match is found, we proceed as described in the next-query prediction setting, that is, we guarantee that the target appears in the top 20 queries that have the highest ADJ scores given the anchor prefix. The statistics of the obtained dataset are reported in Figure 4. As expected, the distribution of lengths changes substantially with respect to the previous settings. Long-tail queries are likely to appear in medium and long sessions, in which the user strives to find an adequate textual query. fix matching, ADJ suffer a significant loss of performance. The performances of the models generally confirm our pre-vious findings. HRED improves significantly by 5.6% over the Baseline Ranker and proves to be useful even for long-tail queries. Supervised models appear to achieve higher absolute scores in the long-tail setting than in the general next-query setting reported in Table 3. After analysis of the long-tail testing set, we found that only 8% of the ses-sion contexts contain at least one noisy query. In the gen-eral next-query prediction case, this number grows to 37%. Noisy queries generally harm performance of the models by increasing the ambiguity in the next query prediction task. This fact may explain why the Baseline Ranker and HRED perform better on long-tail queries than in the general case. It is interesting to see how the improvement of HRED with respect to the Baseline Ranker is larger for long-tail queries than in the general setup (5.6% to 3.3%). Although not ex-plicitly reported, we analyzed the performance with respect to the session length in the long-tail setting. Similarly to the general next-query prediction setting, we found that the Baseline Ranker suffers significant losses for long sessions while our model appears robust to different session lengths.
The previous re-ranking setting does not allow to test the generative capabilities of our suggestion system. We perform an additional user study and ask human evaluators to assess the quality of synthetic suggestions. To avoid sampling bias towards overly common queries, we choose to generate sug-gestions for the 50 topics of the TREC Web Track 2011 [12]. The assessment was conducted by a group of 5 assessors. To palliate the lack of context information for TREC queries, we proceed as follows: for each TREC topic Q M , we extract from the test set the sessions ending exactly with Q M and we take their context Q 1 ,...,Q M  X  1 . After contextualization, 19 TREC queries have one or more queries as context and the remaining are singletons. For HRED, we build synthetic queries following the generative procedure described in Sec-tion 3.4. In addition to QVMM and ADJ, we compare our model with two other baselines: CACB [10], which is similar to QVMM but builds clusters of queries to avoid sparsity, Figure 8: User study results, which compare the effectiveness of HRED with the baseline techniques. and SS (Search Shortcuts) [9], which builds an index of the query sessions and extracts the last query of the most similar sessions to the source context. Note that we do not compare the output of the previous supervised rankers as this would not test the generative capability of our model. Each asses-sor was provided with a random query from the test bed, its context, if any, and a list of recommended queries (the top-5 for each of the methods) selected by the different methods. Recommendations were randomly shuffled, so that the as-sessors could not distinguish which method produced them. Each assessor was asked to judge each recommended query using the following scale: useful, somewhat useful, and not useful. The user study finished when each assessor had as-sessed all recommendations for all 50 queries in the test bed. Figure 8 reports the results of the user study averaged over all raters. Overall, for HRED, 64% of the recommendations were judged useful or somewhat useful. The quality of the queries recommended by HRED is higher than our baselines both in the somewhat and in the useful category. proposed by He et al. [15]. The authors use a Variable Mem-ory Markov model (QVMM) and build a suffix tree to model the user query sequence. We used this model as a context-aware baseline feature in our supervised ranker. The method by Cao et al. [10] is similar but they build a suffix tree on clusters of queries and model the transitions between clus-ters. We didn X  X  notice any improvements by adding this model as a feature in our case. For both models, the number of parameters increases with the depth of the tree inducing sparsity. Instead, our model can consider arbitrary length contexts with a fixed number of parameters. Jiang et al. [22] and Shokouhi et al. [36] propose context-aware approaches for query auto-completion. We adopted a similar framework for query suggestion and use our model as a feature to rank the next-query. Santos et al. [34] and Ozertem et al. [28] also use learning to rank approach for query suggestion. In those cases, the rankers are trained using pairwise features and do not consider previous queries. Interestingly, the au-thors model explicitly the usefulness of a suggestion by using click data and the result list.

Query suggestion algorithms use clustering methods to find similar queries so that they can be used as sugges-tions for one another [1, 42]. We demonstrated that our model exhibits similar clustering properties due to the em-beddings learnt by the neural network. Other works build a Query Flow Graph (QFG) to capture high-order query co-occurrence [7, 33]. Operating at the query-level, these meth-ods suffer from the long-tail problem. Bonchi et al. [8] pro-pose a solution to these problems by introducing the Term-QFG (TQG), where single query terms are also included into the graph. However, suggestion requires repeated complex random walks with restart. Similarly, our model can han-dle rare queries as long as their words appear in the model vocabulary. Vahabi et al. [41] find suggestions to long-tail queries by comparing their search results. Although effec-tive, the approach requires to have 100 results per query. A related approach is the Search Shortcut [9] which avoids the long-tail problem by means of a retrieval algorithm.
Few synthetic suggestion models have been proposed in the literature. Szpektor et al. [40] use a template generation method by leveraging WordNet. Jain et al. [20] combine different resources and use a machine learning approach to prune redundant suggestions. These methods achieve au-tomatic addition, removal and substitution of related terms into the queries. By maximizing the likelihood of the session data, our model learns to perform similar modifications. several applications in a variety of tasks, ranging from Infor-mation Retrieval (IR) [19, 35], Language Modeling (LM) [26, 30] and Machine Translation (MT) [11, 39]. Cho et al. [11] and Sutskever et al. [39] use a Recurrent Neural Network (RNN) for end-to-end MT. Our model bears similarities to these approaches but we contribute with the hierarchical structure. The idea of encoding hierarchical multi-scale rep-resentations is also explored in [16]. In IR, neural networks embeddings were used by Li et al [24]. The authors used deep feed-forward neural networks to use previous queries by the same user to boost document ranking. In [19, 35], the authors propose to use clickthrough data to learn a rank-ing model for ad-hoc IR. Recently, Grbovic et al. [14] used query embeddings to include session-based information for sponsor search. Our model shares similarities with the in-teresting recent work by Mitra [27]. The authors use the pairwise neural model described in [35] to measure similarity between queries. Context-awareness is achieved at ranking time, by measuring the similarity between the candidates and each query in the context. Our work has several key differences. First, we deploy a novel RNN architecture. Sec-ond, our model is generative. Third, we model the session context at training time. To our knowledge, this is the first work applying RNNs to an IR task.
In this paper, we formulated a novel hierarchical neural network architecture and used it to produce query sugges-tions. Our model is context-aware and it can handle rare queries. It can be trained end-to-end on query sessions by simple optimization procedures. Our experiments show that the scores provided by our model help improving MRR for next-query ranking. Additionally, it is generative by def-inition. We showed with a user study that the synthetic generated queries are better than the compared methods.
In future works, we aim to extend our model to explic-itly capture the usefulness of a suggestion by exploiting user clicks [28]. Then, we plan to further study the synthetic generation by means of a large-scale automatic evaluation. Currently, the synthetic suggestions tend to be horizontal , i.e. the model prefers to add or remove terms from the con-text queries and rarely proposes orthogonal but related re-formulations [20, 41]. Future efforts may be dedicated to di-versify the generated suggestions to account for this effect. Finally, the interactions of the user with previous sugges-tions can also be leveraged to better capture the behaviour of the user and to make better suggestions accordingly. We are the most excited about possible future applications be-yond query suggestion: auto-completion, next-word predic-tion and other NLP tasks such as Language Modelling may be fit as possible candidates.
 We would like to thank Jianfeng Gao,  X Ca  X glar G  X  ul  X cehre and Bhaskar Mitra for their precious advice, enlightening discus-sions and invaluable moral support. We gratefully acknowl-edge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.
