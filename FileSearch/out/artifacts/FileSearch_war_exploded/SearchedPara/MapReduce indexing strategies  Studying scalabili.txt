 1. Introduction
The Web is the largest known document repository, and poses a major challenge for Information Retrieval (IR) systems, such as those used by Web search engines or Web IR researchers. Indeed, while the index sizes of major Web search engines are a closely guarded secret, these are commonly accepted to be in the range of billions of documents. However, Dean and Ghemawat (2010) have admitted to holding more than  X  X  X  dozen distinct datasets at Google [of] more than 1PB in size X  X . For researchers, the terabyte-scale TREC ClueWeb09 corpus 1 of 1.2 billion Web documents poses both indexing and retrieval chal-an important problem. Moreover, at such scale, highly distributed architectures are needed to achieve sufficient indexing throughput.

In this work, we investigate distributed indexing with regard to the MapReduce programming paradigm, that has been gaining popularity in commercial settings, with implementations by Google ( Dean &amp; Ghemawat, 2004 ) and Yahoo! ( Schon-feld, 2008 ). Microsoft has also developed a framework which can be configured to provide similar functionality ( Isard, Budiu, machines. It applies the intuition that many common large-scale tasks can be expressed as map and reduce operations ( Dean &amp; Ghemawat, 2004 ), thereby providing an easily accessible framework for parallelism over multiple machines.
However, while MapReduce has been widely adopted within Google, and is reportedly used for their main indexing pro-cess, the original MapReduce framework implementation and other programs using it remain (understandably) internal only. Moreover, there have been few systematic studies undertaken into the scalability of MapReduce beyond that contained  X  within the original MapReduce paper ( Dean &amp; Ghemawat, 2004 ), which in particular demonstrates the scalability for only the simple operations grep and sort. Indeed, with ever-increasing Web corpora needing to be indexed by information retrie-val systems, and the associated uptake of distributed processing frameworks like MapReduce, the case for a detailed study into its viability for such a critical IR task is clear.

In particular, we describe four different methods of performing document indexing in MapReduce, from initial suggestions by Dean and Ghemawat, to more advanced strategies employed by the Nutch and Terrier IR systems. We implement these MapReduce indexing strategies within the Terrier IR platform ( Ounis et al., 2006 ), using the freely available Hadoop ( Apache
Software Foundation, 2010 ) MapReduce implementation. Moreover, we perform experiments over multiple standard TREC test corpora to determine the most efficient indexing strategy examined. We then leverage the most efficient of these to evaluate indexing scalability in MapReduce, both in terms of corpus size and horizontal hardware scaling.
 tion 3 introduces the MapReduce paradigm; Section 4 describes our four strategies for document indexing in MapReduce;
Section 5 describes our experimental setup, research questions, experiments, and analysis of results; Concluding remarks are provided in Section 6 . 2. Indexing single-pass indexing strategy is deployed in the open source Terrier IR platform ( Ounis et al., 2006 ) on which this work is based (Section 2.2 ). We then provide details of how an indexing process can be distributed to make use of multiple machines (Section 2.3 ). 2.1. Index structures index. Usually, a corpus covers many documents, and hence the index will be held on a large storage device  X  commonly one the inverted index contains a posting list , which lists the documents  X  represented as integer document-IDs (doc-IDs)  X  con-taining the term. Each posting in the posting list also stores sufficient statistical information to score each document, such as the frequency of the term occurrences and, possibly, positional information (the position of the term within each document, which facilitates phrase or proximity search) ( Witten et al., 1999 ) or field information (the occurrence of the term in various semi-structured areas of the document, such as title, enabling these to be higher-weighted during retrieved). The inverted index does not store the textual terms themselves, but instead uses an additional structure known as a lexicon to store these along with pointers to the corresponding posting lists within the inverted index. A document or meta index may also be cre-ated to store information about the indexed documents, such as an external name for the document (e.g. URL), or the length of the document ( Ounis et al., 2006 ), so that they can be effectively ranked and presented to the user. The process of gen-erating all of these data structures is known as indexing , and typically happens offline, before being used to determine search results. 2.2. Single-pass indexing kens may then be removed (e.g. low information bearing stop-words), or transformed in such a way that retrieval is bene-fited, (e.g. stemming) before being collated into the final index structures ( Witten et al., 1999 ). Current state-of-the-art indexing uses a single-pass indexing method ( Heinz &amp; Zobel, 2003 ), where the (compressed) posting lists for each term are built in memory as the corpus is scanned. However, it is unlikely that the posting lists for very many documents would fit wholly in the memory of a single machine. Instead, when memory is exhausted, the partial indices are  X  X lushed X  to disk. Once all documents have been scanned, the final index is built by merging the flushed partial indices.
 tional information such as positions or fields can also be held within each posting. As per modern compression schemes, only using some form of compression, e.g. Elias-Gamma compression ( Elias, 1975 ). During a flush, these compressed posting lists are written to disk verbatim (bit-for-bit correct), while during merging, the flush-local doc-IDs are aligned such that these are correct across the entire index. 2.3. Distributing indexing able memory. However, should we want to take advantage of multiple machines, this can be achieved in an intuitive manner by deploying an instance of this indexing strategy on each machine ( Tomasic &amp; Garcia-Molina, 1993 ). For machines with more than one processor, one instance per processing core is possible, assuming that local disk and memory are not satu-rated. As described by Ribeiro-Neto and Barbosa ( Ribeiro-Neto, de Moura, Neubert, &amp; Ziviani, 1999 ), each instance would index a subset of the input corpus to create an index for only those documents. It should be noted that if the documents to be indexed are local to the machines doing the work (which we refer to as a shared-nothing approach), such as when each machine has crawled the documents it is indexing, then this strategy will always be optimal (will scale linearly with process-ing power). However, in practical terms, fully machine-local data is difficult to achieve when a large number of machines is involved. This stems from the need to split and distribute the corpus without overloading the network or risking un-recov-erable data loss from a single point of failure.

Distributed indexing has seen some coverage in the literature. Ribeiro-Neto and Barbosa ( Ribeiro-Neto et al., 1999 ) compared three distributed indexing algorithms for indexing 18 million documents. Efficiency was measured with re-spect to local throughput of each processor, not in terms of overall indexing time. Unfortunately, they do not state the underlying hardware that they employ, and as such their results are difficult to compare to. Melnik, Raghavan, Yang, and Garcia-Molina (2001) described a distributed indexing regime designed for the Web, with considerations for updat-able indices. However, their experiments did not consider scalability of the indexing approach as the number of nodes is increased.

In ( Dean &amp; Ghemawat, 2004 ), Dean and Ghemawat proposed the MapReduce paradigm for distributing data-intensive processing across multiple machines. Section 3 gives an overview of MapReduce. Section 4 reviews prior work on MapRe-duce indexing, namely that of Dean and Ghemawat, who suggest how document indexing can be implemented in MapRe-duce. Furthermore, we also describe the more complex MapReduce document indexing strategies implemented by the Nutch and Terrier IR systems. 3. MapReduce
MapReduce is a programming paradigm for the processing of large amounts of data by distributing work tasks over multiple processing machines ( Dean &amp; Ghemawat, 2004 ). It was designed at Google as a way to distribute computational tasks which are run over large datasets. The core idea is that many types of tasks that are computationally intensive in-volve doing a map operation with a simple function over each  X  X ecord X  in a large dataset, emitting key/value pairs to com-prise the results. The map operation itself can be easily distributed by running it on different machines, each processing their own subset of the input data. The output from each of these is then collected and merged into the desired results by reduce operations.

By using the MapReduce abstraction, the complex details of parallel processing, such as fault tolerance and node avail-ability are hidden in a conceptually simple framework ( Manning, Raghavan, &amp; Sch X tze, 2008 ), allowing highly distributed tools to be easily built on top of MapReduce. Indeed, various companies have developed tools to perform operations on large-scale datasets on top of MapReduce implementations. Google X  X  Sawzall ( Pike, Dorward, Griesemer, &amp; Quinlan, 2005 ) and Yahoo X  X  Pig ( Olston, Reed, Srivastava, Kumar, &amp; Tomkins, 2008 ) are examples of data flow languages which are built upon MapReduce. Microsoft uses a distributed framework called Dryad which can provide similar functionality ( Yu et al., 2008 ). Indeed, the Nebula scripting language uses the Dryad framework to provide data mining capabilities ( Isard et al., 2007 ). However, it is of note that MapReduce trades the ability to perform code optimisation (by abstracting from the internal workings) for easy implementation through its framework, meaning that an implementation in MapReduce is unlikely the optimal solution, but will be cheaper to produce and maintain ( Johnson, 1997 ).

MapReduce is designed from a functional programming perspective, where functions provide definitions of operations over input data. A single MapReduce job is defined by the user as two functions. The map function takes in a key/value pair tion are then automatically sorted by their key (key2). The reduce function collects the map output and then merges the val-ues with the same key to form a smaller final result.

A single MapReduce job has many map tasks that each operate on a subset of the input data, using the map function de-fined for that job. The MapReduce framework queues tasks such that each processor receives only a single task at any one time. When all map tasks have finished, a smaller number of reduce tasks then operate on the merged output of the map tasks. Map or reduce tasks may run on different machines, allowing parallelism to be achieved. In common with functional programming design, each task is independent of other tasks of the same type, and there is no global state, or communication between maps or between reduces. Fig. 1 shows the main stages of a MapReduce job. In particular, in the figure, the output of three map tasks is sorted, then processed by two reduce tasks.

A partitioner decides which reduce task processes each &lt;key2, value2&gt; pair. Usually, the partitioner splits map output be-of the two reduce tasks should process each output pair from a map task. Moreover, if the ordering of values in each reduce task is important, then the output can be partitioned using both key2 and value2. Notably, the map output collected by each reduce task is subjected to a secondary sort as it is being merged together, hence allowing both map tasks and intermediate sorting to be performed in parallel.
 ( Dean &amp; Ghemawat, 2004 ). For this, the map function takes the document file-name (key1) and the contents of the document (value1) as input, then for each term in the document emits the term (key2) and the integer value  X 1 X  (value2). The reduce func-pseudo-code for the word counting map and reduce functions. Note that this is a very simple example for illustrative purposes.
This has a central advantage that the map functions can operate on data that may be  X  X ack-local X  or  X  X achine-local X   X  i.e. does not have to transit intra-and inter-data centre backbone links, and hence does not overload a central file storage service.
Hence, good throughput can be achieved because data is always as local as possible to the processors doing the work. Inter-mediate results of map tasks are stored on the processing machines themselves. To reduce the size of this output (and there-fore the network traffic), it may be merged using a combiner , which acts as a reduce task local to each machine. A central master machine provides job and task scheduling, which attempts to perform tasks as local as possible to the input data. various machine learning algorithms could be parallelised using the MapReduce paradigm. However, experiments were only carried out on single systems, rather than a cluster of machines. In such a situation, MapReduce provides an easy framework to distribute non-cooperating tasks of work, but misses the central data locality advantage facilitated by a MapReduce with small experimental datasets of only 88 MB and 770 MB, it would again fail to see benefit in the data-local scheduling of tasks. More recently, Pavlo et al. (2009) performed an analysis of MapReduce in comparison to parallel databases, which indicated that MapReduce was inefficient in comparison for data analysis. However, Dean and Ghemawat later responded ( Dean &amp; Ghemawat, 2010 ), highlighting faulty assumptions about MapReduce and poor implementations as the reason for the lacklustre performance. Importantly, they point out that the input-output data format needs to be as efficient as pos-sible ( Google Inc., 2010 ), to avoid needless data transfer between map and reduce tasks, and that often the final reduce task outputs may not need to be merged upon job completion, as further jobs can be modified to access sharded data structures.
In an IR system, indexing is a disk-intensive operation, where large amounts of raw data have to be read and transformed into suitable index structures. In this work, we show how indexing can be implemented in a MapReduce framework. How-ever, the MapReduce implementation described in ( Dean &amp; Ghemawat, 2004 ) is not available outside of Google. Instead, we use the Hadoop ( Apache Software Foundation, 2010 ) framework, which is an open-source Java implementation of MapRe-duce from the Apache Software Foundation, with developers contributed by Yahoo! and Facebook, among others. In the next section, we describe several indexing strategies in MapReduce, starting from that proposed in the original MapReduce paper ( Dean &amp; Ghemawat, 2004 ). 4. Indexing in MapReduce
In this section, we describe multiple existing approaches to indexing under MapReduce. Firstly, we describe two possible interpretations of indexing as envisaged by Dean and Ghemawat in their original seminal MapReduce paper ( Dean &amp; Ghe-(Section 4.1.2 ). Then, we describe two alternative MapReduce indexing strategies used by the Nutch (Section 4.2 ) and Terrier IR platforms (Section 4.3 ), respectively. 4.1. MapReduce indexing as envisaged by Dean and Ghemawat The original MapReduce paper by Dean and Ghemawat (2004) presents a short description for performing indexing in MapReduce, which is directly quoted below: forms a simple inverted index. It is easy to augment this computation to keep track of word positions. X  X 
The implicit claim being made in the original paper ( Dean &amp; Ghemawat, 2004 ) is that efficient indexing could be trivially implemented in MapReduce. However, we argue that this claim over-simplifies the details, and provides room for a useful study to allow document indexing in MapReduce to be better understood. For example, for an inverted index to be useful, the term frequencies within each document need to be stored. Though this is not accounted for in Dean and Ghemawat X  X  paper, there are two possible interpretations on how this could be achieved within the bounds laid out in the quotation above. We detail these interpretations below in Sections 4.1.1 and 4.1.2 , respectively. 4.1.1. Per-token indexing
The literal interpretation of the description above would be for the map function to output a set of &lt;term, doc-ID&gt; pairs for each token in a document. Then, the reduce function aggregates for each unique term the doc-IDs in which that term occurs to obtain the term frequencies, before writing the completed posting list for that term to disk. Fig. 3 provides a pseu-do-code implementation of the map and reduce functions for this strategy.
 tf times. This strategy has the advantage of a very simple map phase, as it emits on a per-token basis. Hence, we refer to this transferred to the appropriate reduce task. Hence, with this indexing interpretation, the intermediate map data would be extremely large  X  indeed, similar to the size of the corpus, as each token in the corpus is emitted along with a doc-ID. Having large amounts of intermediate map data will increase map-to-reduce network traffic, as well as lengthening the sort phase.
These are likely to have an effect on the job X  X  overall execution time. 4.1.2. Per-term indexing (Ivory) sive, and will cause too much disk input/output (IO) when writing intermediate map output to disk, and again when moving frequency of the term in the document. In this way, the number of emit operations which have to be done is significantly reduced, as we now only emit once per unique term per-document. The reduce task for this interpretation is also much sim-pler than the earlier interpretation, as it only has to sort instances by document to obtain the final posting list sorted by ascending doc-ID. Indeed, as this strategy emits once per-term, we refer to this as the per-term indexing strategy. code implementation of the map and reduce functions for this strategy are shown in Fig. 4 . It should also be noted that the per-token strategy can be adapted to generate tf s instead, through the use of a combiner function (see Section 3 ), which per-forms a localised merge on each map task X  X  output.
 an implementation in this manner is still inefficient, because a large amount of disk IO is required to store, move and sort the temporary map output data. 4.2. Per-document indexing (Nutch) indexing strategy, using the Hadoop MapReduce implementation. By inspection of the source of Nutch v0.9, we have deter-mined that the indexing strategy differs from the general outline described in Section 4.1 above. Instead of emitting terms, this strategy only tokenises the document during the map phase, hence emitting &lt;doc-ID, Document&gt; tuples from the map function. Each document contains the textual forms of each term and their corresponding frequencies. The reduce phase is then responsible for writing all index structures. Pseudo-code implementations of map and reduce functions for this strategy are shown in Fig. 5 . As one emit is made per-document, we refer to this strategy as per-document indexing.
 substantially more data (i.e. the textual form and frequency of each unique term in the document). We believe that this is a step-forward towards reducing intermediate map output, since the larger document-level emits can achieve higher lev-els of compression than single terms. A central advantage of the Nutch approach is that documents are sorted by document name (e.g. URL), allowing anchor text from other documents to be indexed on the same reduce task. However, a study of anchor text indexing techniques is outwith the scope of this study. Hence, we examine the scalability of this technique for indexing document content only. 4.3. Per-posting list indexing (Terrier)
Lastly, we examine the MapReduce indexing strategy we previously proposed in ( McCreadie, Macdonald, &amp; Ounis, 2009, 2009 ) and that is currently employed by the Terrier IR platform. This approach is based upon the single-pass indexing strat-egy described earlier in Section 2.2 . The indexing process is split into multiple map tasks. Each map task operates on its own subset of the data, and is similar to the single-pass indexing corpus scanning phase. In particular, as documents are pro-cessed by each map task, compressed posting lists are built in memory for each term. However, when memory runs low or all documents for that map have been processed, the partial index is flushed from the map task, by emitting a set of &lt;term, posting list&gt; pairs for all terms currently in memory. These flushed partial indices are then sorted by term, map and flush numbers before being passed to a reduce task. As the flushes are collected at an appropriate reduce task, the posting lists for each term are merged by map number and flush number, to ensure that the posting lists for each term are in a globally correct ordering. The reduce function takes each term in turn and merges the posting lists for that term into the full posting list, as a standard index. Elias-Gamma compression is used as in non-distributed indexing to store only the distance between doc-IDs. Fig. 6 provides a pseudo-code implementation of map and reduce functions for this indexing strategy.
The fundamental difference between this strategy and those described earlier, is what the map tasks emit. In contrast to the per-token, per-term and per-document indexing strategies, this strategy builds up a posting list for each term in mem-ory. Over many documents, memory will eventually be exhausted, at which time all currently stored compressed posting the positive effect of minimising both the size of the map task output, as well as the frequency and number of emits. More-over, as the posting lists themselves are compressed using Elias-Gamma compression, the size of the emits will be very small. 3 Compared to the per-token and per-term indexing strategies, the per-posting list strategy emits far less, but the size of each emit will be much larger. Compared to the per-document indexing strategy, there will likely be less emits (dependent on memory available), and as the reduce task is operating on term-sorted data, it does not require a further sort and invert operation to generate an inverted index. Moreover, the emit values will only contain doc-IDs instead of textual terms, mak-ing them considerably smaller. 4.4. Discussion
A summary of the map output types of the various MapReduce indexing algorithms is provided in Table 1 . We have implemented all of these algorithms within the auspices of the Terrier IR system. In the following, we discuss some notable implementation details, which remain consistent across our implementations of these algorithms. Firstly, it should be re-iterated that in MapReduce, each map task is not aware of its context in the overall job. For indexing, this means that the doc-IDs emitted from the map phases cannot be globally correct. Instead, these doc-IDs start from 0 in each map. To allow the reduce tasks to calculate the correct doc-IDs, each map task produces a  X  X  X ide-effect X  X  file, detailing the number of doc-uments emitted per map.
 reduce tasks should be used to ensure high parallelism, and hence high efficiency ( White, 2009 ). Intuitively, there are two partitioning schemes that could be applied for indexing, depending upon the desired output index format. In particular, by-reduce task will create an index for a subset of the document in the corpus, in a similar fashion to by-document partitioning reduce tasks, with the final inverted index being split across the files created by each reduce task. Table 1 also shows the support of the various MapReduce indexing strategies for the by-term and by-map partitioning schemes. It is of note that the per-document strategy does not support by-term partitioning, as each map output is the aggregation of multiple terms, while, in contrast, the other strategies can support both by-term and by-document partitioning.
 for an English corpus, one reduce task could handle terms for one or more of the 26 characters. An alternative is to randomly partition terms amongst the reduce tasks. 4 However, while this ensures a good balance of reduce task workload, the posting partitioning in this paper.
 fault tolerant. If any map or reduce task fails then an identical task will be added to the queue of tasks for that job. Multiple failures by any single machine cause that machine to be black-listed and its work to be transferred elsewhere. Note that if a task should repeatedly fail then the job will be terminated, as a complete index is unable to be built. 5. Experiments and results
In the following experiments, we aim to determine the efficiency of multiple indexing implementations in MapReduce. In particular, we compare each of the four indexing approaches described in Section 4 in terms of processing time consumed at each stage of the indexing process. Furthermore, we also investigate how MapReduce indexing scales as we increase corpus size and horizontally scale hardware. 5.1. Research questions
To measure the efficiency of our implementations of the MapReduce indexing strategies and therefore the suitability (or otherwise) of MapReduce for indexing, we investigate four important research questions, which we address by experimen-tation in the remainder of this section: 1. Which of the four indexing strategies described earlier in Section 4 is the most efficient? (Section 5.4 ) 2. How does MapReduce indexing scale with corpus size? (Section 5.5 ) 3. Does MapReduce scale close to linearly as hardware is scaled horizontally? (Section 5.6 ) 4. What effect does the number of reduce tasks have on indexing performance? (Section 5.7 ) 5.2. Evaluation metrics
In this paper, we employ multiple metrics for measuring indexing performance. To compare how the four different index-ing strategies described earlier (see Section 4 ) perform in detail, we measure the average time taken over all map and reduce tasks. In this way, we examine any tradeoff between map and reduce phase durations for each strategy, as well as their over-all performance. However, to give a general overall measure of indexing performance, we measure the throughput of the system, in terms of MB/s. (megabytes per second). We calculate throughput as collectionsize / timetaken where collection size is the compressed size on disk for a single copy of the collection in MB (megabytes). The time taken is the full time taken by the job, measured in seconds.

Research questions 2, 3 and 4 address the suitability for indexing at a large-scale. We measure horizontal scalability (i.e. as more resources hardware is added) in terms of speed-up ( S m ). Intuitively, speed-up ensures that not only should speed improve as more resources are added, but that such a speed increase should reflect the quantity of those resources. In par-ticular, speed-up S m  X  T 1 T T is the execution time in parallel, using m machines ( Hill, 1990 ). For instance, if we increase the available resources by a S = m ) is the ideal scenario for parallel processing. However, it is hard to achieve in a parallel environment, because of the growing influence of small sequential sections of code as the number of processors increases (known as Amdahl X  X  law ( Am-dahl, 1967 )), or due to overheads in job scheduling, monitoring and control. 5.3. Experimental setup
Following ( Zobel, Moffat, &amp; Ramamohanarao, 1996 ), which prescribes guidelines for evaluating indexing techniques, we now give details of our experimental cluster setup, consisting of 30 identical machines. Each machine contains two quad-core AMD Opteron processors, 16 GB of RAM, and contains one 250 GB SATA hard disk (manufacturer: Seagate, model num-ber: ST32502N), of which 95 GB is available for MapReduce files. All machines run the Linux Centos 5.3 operating system and are connected together by a gigabit Ethernet switch on a single rack. The Hadoop (version 0.18.2) distributed file system into 64 MB blocks, which are each replicated to two machines. 5 The machines are part of a dedicated Hadoop cluster which can process multiple jobs simultaneously (assuming there are enough processing cores to service them all). However, to avoid any possibility of interference between jobs, for example from network overloading, we run each job sequentially. Ma-chines not allocated to a job are left idle, however these machines are still part of the distributed file system and as such can accept requests for data from each job. No other processes were running on the cluster during our experiments. gigabyte-scale TREC corpora, i.e. WT2G, .GOV and .GOV2, as well as one terabyte-scale corpus ClueWeb09_English_1. The statistics of each corpora are presented in Table 2 , including number of documents, number of tokens, number of unique terms, and the number of pointers (posting list entries). Note that ClueWeb09_English_1 is a subset of 50 million English documents from the ClueWeb09 corpus. 6 This subset was used for multiple tracks in TREC 2009 ( Clarke, Craswell, &amp; Soboroff, 2009 ), and was known as ClueWeb09  X  X  X  set.
 egy which does not support this partitioning scheme (see Section 4.4 ). Furthermore, in the following experiments (unless otherwise stated), each MapReduce job was allocated all 30 machines, 240 map tasks and 26 reduce tasks. Notably, the num-ber of map tasks chosen was based upon the maximum number of tasks that could be run in parallel, i.e. 240 tasks since each of our 30 machines contributed 8 processors. The small number of reduce tasks was mandated by our by-term partitioning strategy, i.e we have one reduce task per initial character in the English alphabet (see Section 4.4 ). Lastly, we note that the output of each map task is automatically and transparently compressed by Hadoop using GZip, to reduce disk IO and net-work traffic between map and reduce tasks. 5.4. Algorithmic indexing efficiency (see Section 5.1 ). To evaluate this, for each indexing strategy, we record the average time to complete each of the MapReduce map and reduce phases. In particular, in each map task, documents are being read from disk, tokenised, and possibly aggre-gated. Reduce tasks are responsible for writing the final index structures. Furthermore, we also measure the volume of map output data generated, as well as the overall time taken by each indexing job.
 data when indexing the TREC WT2G and .GOV corpora with 30 machines, 240 map tasks and 26 reduce tasks. Note that for this test we report performance only on the smaller gigabyte-scale corpora, since evaluation of the per-token strategy proved to be impractical for indexing at a larger scale, i.e. the volume of local storage required exceeded that available to us. which has to take place during the map phase. This also negatively impacts the reduce phase, as large amounts of data need to be transferred to the reducer, before being merged together into the index structures.
 152 s for WT2G). This is due to the reduced map output size, which is half that of the per-token strategy. However, per-term indexing is still slower than the per-document and per-posting list strategies, which exhibit smaller map output sizes and smaller overall indexing times.
 duces shorter map phases, as less processing takes place in each map task. In contrast, the map phase of per-posting list indexing is slower because posting lists are built by each mapper. However, by making this extra processing at the map phase, the size of the map output is reduced (e.g. from 459 MB to 449 MB for WT2G and 2008 MB to 1619 MB for .GOV) and moreover, the reduce tasks are markedly faster, resulting in an overall shorter indexing job. In particular, in our imple-mentation of the per-document indexing strategy, each reduce task corresponds to running a single-pass indexer on already tokenised documents. In contrast, the reduce tasks for the per-posting list indexing strategy work with much less data, and are merely correcting the doc-IDs before writing to the final inverted index files (see Section 4.3 ). As such, although the dif-ference in processing time reported for these two strategies is not large, we would expect the disparity in performance to continue to escalate as the corpus size grows. This is because under the per-posting list strategy, the savings that result from posting-list compression will become more pronounced as posting lists become larger, whilst per-document compression will remain roughly constant.

In summary, the simple per-token and per-term indexing strategies as interpreted from the original MapReduce paper ( Dean &amp; Ghemawat, 2004 ) (of which the latter is employed by the Ivory IR system ( Lin, 2009 )) are markedly less efficient than more recent per-document and per-posting list indexing approaches deployed by the Nutch and Terrier IR platforms. Of these two, the per-posting list implementation is overall more efficient. As such, in the remainder of these experiments we deploy the per-posting list MapReduce indexing implementation only. 5.5. Scaling to larger corpora
In Section 5.4 , we determined that the per-posting list MapReduce indexing strategy was the most efficient of those examined. In this section, we investigate our second research question (see Section 5.1 ), to determine how this strategy scales as corpora size increases. In particular, we compare indexing performance over three gigabyte-scale TREC test corpora: WT2G, .GOV and .GOV2, as well as one terabyte-scale TREC corpus, i.e. ClueWeb09_English_1. For MapReduce to be viable as a platform for distributed indexing, overall indexing throughput should be maintained as corpus size increases.
Table 4 shows the indexing throughput for each of the four TREC test corpora based upon its compressed index size. From the results in the table, we observe that throughput shows an unexpectedly large variance over the test corpora. To examine this in more detail, we provide an analysis of each indexing job. In particular, Fig. 7 a X  X  show the time taken by each indi-vidual map when indexing the four investigated corpora.

Taking each corpus in turn, Fig. 7 a illustrates the reason for the low throughput for WT2G. In particular, we can see that many of the map tasks do not start until the job is well under way. This is due to the manner in which Hadoop assigns map tasks to available machines. In particular, Hadoop only assigns two map tasks to a given machine in a 5 s iteration. For small jobs, such as indexing WT2G where each map task only lasts 10 s, this is a significant overhead to the startup of the indexing job. This issue was also identified by O X  X alley and Murthy (2009) when deploying MapReduce for sorting large datasets  X  it is of note that future versions of Hadoop have optimisations to decrease this latency.

From Table 4 , we see that the indexing of the .GOV corpus had 4.5 times the throughput of WT2G. However, on exami-nation of Fig. 7 b, we see that it is similarly affected by latency in task startup. In contrast, for the .GOV2 and Clue-Web09_English_1 corpora ( Fig. 7 c and d), map tasks took significantly longer, and hence, startup latency overheads were negligible, with overall indexing times and throughputs being significantly higher. Indeed, for the two largest corpora, index-ing throughput was approx. 65 X 75 MB/s.

For .GOV and .GOV2, there is a noticeable downwards trend in map duration for the later map tasks. This is explained by the file size distributions in these corpora, which decreases for later bundles of documents. In our implementation, corpora files are split evenly across map tasks  X  a better splitting would spread files evenly across the map tasks based on their size. In contrast, for ClueWeb09_English_1, the distribution of work over the map tasks is fairly even, with the exception of the last few map tasks, which are smaller files containing Wikipedia articles.

To summarise, our indexing implementation in MapReduce shows promising throughput, which is increased as collection size increases, due to the reduced overheads in starting short-lived map tasks. Indeed, this supports the notion that MapRe-duce indexing is scalable, as the rate at which a corpus is indexed does not degrade for larger corpora. 5.6. Horizontal hardware scaling
We now investigate how well indexing under MapReduce performs as we scale hardware horizontally with processing power, i.e add more machines for work (third research question, see Section 5.1 ). As mentioned in Section 2.3 , when distrib-uted indexing uses machine-local data, optimal linear scaling can be achieved. The alternative to shared-nothing is to use a  X  X  X hared-everything X  X  setting, where central fileserver(s) serve the corpora to be processed by multiple indexing machines. However, such a centralised approach can suffer from a single point of failure. Moreover, centralised fileservers are often a bottleneck  X  indeed, we have previously shown that, for indexing, a single RAID-based fileserver scaled to only 4 machines (3 processors per machine) ( McCreadie et al., 2009 ).

As noted in Section 3 , a core advantage of MapReduce is the ability to apply the distributed file system (DFS) to avoid centralised storage of data and, moreover, to take advantage of data locality to avoid excess network IO. Using MapReduce for indexing cannot be as fast as a shared-nothing indexing setting, due to overheads in job setup, monitoring and control (c.f. Amdahl X  X  law). Hence, we evaluate MapReduce indexing in terms of the extent to which throughput increases in a linear fashion with the machines allocated for work.
 ticular, we measure the speed-up gained as more machines are allocated for map and reduce indexing tasks. Fig. 8 reports the speed-up gained upon indexing the largest corpus investigated, i.e. the ClueWeb09_English_1 corpus, using {1,5,10,15,20,25,30} machines, when allocating 240 map tasks and 26 reduce tasks, both for the indexing process as a whole and the individual map and reduce phases.

From Fig. 8 , we observe that, as expected, MapReduce indexing as a whole scales sub-linearly. However, if we deconstruct the indexing process into map and reduce tasks, we observe that the map tasks scale better with processing power than the job as a whole. Indeed this is close enough to perfect linear scaling that we would deem it satisfactory for large-scale index-ing. However, as more machines are allocated, the reduce task does not scale with processing power in this setting, thereby decreasing overall scalability.
 In explanation, recall that the total number of reduce tasks allocated is 26  X  one for terms starting with each letter of the English alphabet. Hence, when five or more machines are allocated, at least 40 processors will be available for reducing  X  more than the number of reduce tasks. In these cases, once the reduce phase is underway, many processing cores will be left idle. However, this is not an insurmountable issue, as by splitting the map output by multiple characters instead of just the first, the number of final output indices, and therefore reduce tasks required increases, hence supporting a higher degree of parallelism. 7 In particular, under such a  X  X refix-tree X  approach, the partitioner would allocate terms to reducers based on the expected size of the posting lists for terms with similar prefixes. The expected distribution would need to be obtained from a source outwith the corpus being indexed, as the partitioner of each map task does not have access to the global term prefix distribution.

From Fig. 8 , we also note that speed-up does not increase smoothly as the number of machines is increased. Indeed, be-tween 15 and 25 machines, speed-up is less than at other points in the figure. To explain this issue, we refer to Fig. 9 , which shows the time taken by each of the 240 map tasks when indexing the ClueWeb09_English_1 corpus with both 15 and 25 machines. From this, we note that for both indexing jobs, not all map tasks can be processed at once, as the number of map tasks exceeds the number of processing cores available. Instead, in both cases, as many map tasks as possible are run, and then the remaining are held until a processor becomes free. Unfortunately, this results in a similar processing time for both jobs, even though the latter task has 40% more processors available. This is a direct result from the majority of cores are left idle during the second  X  X atch X  of jobs for the 25 machine task. From this, we can see that it is important to set the number of map tasks with reference to the number of processing cores available. This contrasts to normal Hadoop configuration, where the number of map tasks is a constant, or defined according to the number of splits of the input data that can be obtained.
Overall, we conclude that the map task within the indexing process scales in a fashion that is suitable for large-scale indexing. Indeed, this is an important result, as typically the map phase dominates the indexing job in terms of time taken. Hence this is an considerable step toward showing that MapReduce indexing is scalable. Moreover, we have shown that, ide-ally, the number of map tasks should be approximately equal to the number of processors available for work (for fault tol-erance, leaving a few processors free allows single task failures to be quickly re-run ( White, 2009 )). However, with only 26 reduce tasks, scaling beyond 26 processors did not decrease reduce phase time. In the next section, we examine how varying the number of reduce tasks affects indexing time. 5.7. Reducer scaling
In this section, we examine how well the reduce phase of a MapReduce indexing job scales with regard to the number of processors. In particular, we evaluate the total speed-up observed for the reduce phase as we increase the number of reduce tasks. Ideally, speed-up should be close to linear with the number of reduce tasks allocated, assuming that map phase output is evenly split across reduce tasks.
 {1,4,8,12,16,10,24,26} reduce tasks using the per-posting list indexing strategy. In particular, we can see that from 1 to 26 reduce tasks, close-to linear speed-up is achieved.
 tions. Fig. 11 shows the distribution of reduce task durations for indexing the ClueWeb09_English_1 corpus. From the figure, we observe that not all reduce tasks take a uniform duration. This is expected, as the initial character of indexed terms will infrequent nature.
 large-scale indexing. Hence, we conclude that MapReduce in general is suitable for indexing terabyte and larger scale collections. 6. Conclusion
In this paper, we investigated the common IR process of indexing within the context of the distributed processing para-digm MapReduce. In particular, we detailed four different strategies for applying document indexing within the MapReduce paradigm.

Through experimentation on four standard TREC test corpora of varying size and using the Java Hadoop implementation of MapReduce, we experimented upon those four indexing strategies. We firstly showed that of the four, the two interpreted from Dean and Ghemawat X  X  original MapReduce paper ( Dean &amp; Ghemawat, 2004 ), i.e. per-token and per-term indexing, gen-erate too much intermediate map data, causing an overall slowness of indexing. Indeed, we conclude that these strategies are impractical for indexing at a large-scale.

Moreover, while the per-document indexing strategy employed by the Nutch IR platform proved to be more efficient, it is compromised by lengthy reduce phases resulting from their increased responsibilities. In contrast, the per-posting list index-ing strategy that we previously proposed, and that is employed by the Terrier IR platform, proved to be the most efficient indexing strategy. This is due to its use of local machine memory and posting list compression techniques to minimise map-to-reduce traffic.
 Furthermore, we examined how per-posting list indexing scaled with both corpus size and horizontal hardware scaling. In particular, we showed that while our specific implementation of the indexing strategy was suboptimal, the algorithm overall scales in a close to linear fashion as we scale hardware horizontally and that variance observed in indexing through-put could be attributed to time lost as Hadoop assigned tasks to machines, which is negligible on terabyte-scale corpora.
Overall, we conclude that the per-posting list MapReduce indexing strategy should be suitable for efficiently indexing lar-ger corpora, including the full billion document TREC ClueWeb09 collection. Indeed, we have shown indexing performance on a 50 million document subset of that corpus. Moreover, as a framework for distributed indexing, MapReduce conveniently provides both data locality and resilience. Finally, it is of note that an implementation of the per-posting list MapReduce indexing strategy described in this paper is now freely available for use by the community as part of the Terrier IR Platform. 8 Acknowledgements
We are thankful to Mark Meenan and Derek Higgins of the University of Glasgow Computing Service for timely and exclu-sive access to their computer cluster.
 References
