 In large and complex graphs of social, chemical/biological, or other relations, frequent substructures are commonly shared by different graphs or by graphs evolving through different time periods. Tensors are natural representations of these complex time-evolving graph data. A factorization of a ten-sor provides a high-quality low-rank compact basis for each dimension of the tensor, which facilitates the interpretation of frequent substructures of the original graphs. However, the high computational cost of tensor factorization makes it infeasible for conventional tensor factorization methods to handle large graphs that evolve frequently with time.
To address this problem, in this paper we propose a novel iterative tensor factorization ( ITF ) method whose time com-plexity is linear in the cardinalities of all dimensions of a ten-sor. This low time complexity means that when using ten-sors to represent dynamic graphs, the computational cost of ITF is linear in the size (number of edges/vertices) of graphs and is also linear in the number of time periods over which the graph evolves. More importantly, an error estimation of ITF suggests that its factorization correctness is comparable to that of the standard factorization method. We empirically evaluate our method on publication networks and chemical compound graphs, and demonstrate that ITF is an order of magnitude faster than the conventional method and at the same time preserves factorization quality. To the best of our knowledge, this research is the first work that uses impor-tant frequent substructures to speed up tensor factorizations for mining dynamic graphs.
 H.2.8 [ Database Applications ]: Data mining Algorithms Tensor factorization; dynamic graphs; scalability.
Widely used statistical data mining methods that detect dynamic patterns are usually based on matrix operations, such as SVD [7] and PCA [14], which can only deal with two  X  X imensions X  of the original data (by the  X  X ow dimen-sion X  and the  X  X olumn dimension X  of matrices). When data sets are multi-dimensional such as dynamic time-evolving networks, tensor-based methods can more effectively and naturally represent all dimensions of the data. However, similar to performing SVD on matrices, the use of tensors raises the problem of tensor factorizations, from which it is possible to identify the principle low-rank factors of a ten-sor (i.e., the linear combinations of original dimensions that contribute the most to the overall variance of the tensor). These low-rank factors are important since the new feature space formed by these factors preserves the main variance of the original tensor.

To successfully perform tensor factorization on dynamic graphs, a major challenge that needs to be addressed is how to handle scalability. Scalability is an issue not only because tensors are composed of multiple dimensions, but also because the cardinality of each dimension in itself can be very large. For example, if we use a 3-mode tensor, authors  X  journalNames  X  publicationT ime ,torepresenta dynamic publication network, each of the three dimensions can have thousands or more unique values. For another example, when one uses a tensor to represent a collection of chemical compounds (e.g., atoms  X  bonds  X  compounds ) where each compound is a graph of atoms [11], the cardi-nality of the third dimension of this tensor is usually in the tens of thousands. Moreover, with new bioassays constantly being tested over time, the number of compounds that need to be incorporated into the tensor will tend continue to grow larger. This phenomenon makes the computational cost of the standard tensor factorization extremely expensive and infeasible for analyzing time-varying graphs/networks 1 .
Given the scalability challenge, a good tensor factoriza-tion method should be able to (1) factorize large tensors ef-ficiently using much less time than standard methods, while at the same time (2) obtain low-rank factors that preserve the main variance of the tensors. To achieve this goal, in this
Due to the generic nature of the iterative tensor factoriza-tion technique we propose, we use the two terms  X  X etwork X  and  X  X raph X  interchangeably. paper we propose to reduce the time complexity of tensor factorization by updating the factor matrices in an iterative manner. The rationale behind this design is based on the observation that, in many practical domains, the most im-portant substructures of graphs are commonly shared by all graphs, or by graphs evolving in different time periods. In other words, it is usually not the important substructures, but the actual combinations of these substructures, which are changing with time or changing across different graphs. If one has discovered the most important substructures of a set of graphs at an earlier time, then in many domains the graphs at a later time can generally be well explained by those earlier substructures using an efficient updating procedure. From the perspective of tensor factorization on graphs, if we have found low-rank factors of a subset of a tensor, then after we iteratively update these factors at a very low cost, the rest of the tensor data can also be well represented by the updated factors. The notion of an im-portant substructure is essentially a column vector from a low-rank factor matrix, which comprises a linear combina-tion of the original features (e.g., edges). Note that we do not assume the structural changes of the graphs are smooth  X  even if the graphs change frequently, the later graphs can still be explained by the earlier factor matrices as long as the later graphs are comprised of important substructures (i.e., rather than being comprised of noisy or random sub-structures).

In the preceding chemical compound example, although there are a huge number of compounds that can be encoded into a tensor, most of these compounds share many common substructures, such as aromatic rings, hydroxyls and amines. The major difference among chemical compounds, in terms of preserving the main variance of the data, is in what for-mulations of those substructures comprise each compound. There might be some special substructures that only exist in a few specific compounds, but we note that such substruc-tures would contribute very little in representing the main variance of a data set, and thus would have little influence in the decomposed factor matrices. Similarly for publication networks, the publication patterns of an author at different times would vary in the concrete combinations of journals or conferences in which the author has published. For example, we expect authors in the number theory domain to continue to publish in number theory journals, rather than change their focus to marketing or finance journals. To this end, the combination of number theory journals are the common important factors that can be discovered early on from a subset of a tensor.

Based on these observations, in this paper we propose a novel factorization strategy that first factorizes a subset of a tensor, and then iteratively refines its factor matrices by efficient and effective updating operations on the remaining data in the tensor. Moreover, we generalize the iterative fac-torization process from one mode to all modes of a tensor, which further reduces its overall time complexity. In sum-mary, the major contributions of this paper are as follows: 1. We propose an iterative tensor factorization method 2. We derive an analytical error estimation of the iter-3. We apply the general ITF algorithm to practical prob-The rest of the paper is structured as follows. We review related literature in Section 2. Section 3 defines the ten-sor factorization problem, and introduces our ITF algorithm with an error estimation. We explain how tensor decompo-sition can be applied to dynamic graph mining in Section 4. Details of the dataset we use and the empirical evaluations are reported in Section 5. We conclude in Section 6 with directions for future work.
In this section, we review existing methods that are closely related to this research. A detailed survey on tensor factor-ization methods can be found in [9].

Many factorization methods have been proposed for ten-sor analysis based broadly on two approaches: Tucker de-composition [19] and canonical polyadic decomposition (aka PARAFAC/CANDECOMP decomposition) [3], both of which can be considered as higher-order generalizations of matrix SVD and PCA. Kroonenberg et al. [12] proposed to use the method of alternating least squares (ALS) in solving Tucker decomposition for three-way arrays, which was then extended and popularized by Kapteyn et al. [8] for n-way ar-rays. Sun et al. [16,17] have proposed methods for incremen-tal tensor analysis, which are aimed at solving the decom-position problem when there is a stream of many tensors: in [16] they used sliding windows to track the changes of covariance matrices, based on which their algorithms made decisions on whether to update the projection matrices; and in [17] they introduced the notion of a  X  X orgetting factor X  which is added onto older covariance matrices. This forget-ting factor controlled the amount of information that could be considered in future updates. However, these methods did not avoid the extremely expensive computational costs of diagonalization/eigen-decomposition on any mode of a tensor, which severely limits the efficiency of their methods. Moreover, they did not provide any analysis of the error estimation of their methods. Kolda et al. [10] designed al-gorithms to improve memory efficiency for sparse tensors, but when tensors are not highly sparse their method re-quires much more CPU time than normal decompositions. Recently, Schifanella et al. [15] proposed to use metadata, which is a priori background knowledge in addition to the original data, to design a domain hierarchy by which they reduce the time of tensor decomposition. However, this method cannot address more general scenarios where such metadata is not available for the original data.

The major difference between our proposed iterative method and the above existing methods is that the method proposed in this paper avoids the full decomposition / diagonalization on the covariance matrix in all modes of a tensor, which is the main cause of the high computational complexities of tensor factorization. Furthermore, we provide an error es-timation of our method, which illustrates its suitability for
Notation Definition and Description a lowercase normal font represents scalar values a boldface lowercase represents vectors U boldface uppercase represents matrices U i,j the scalar at the { i, j } position of U X , Y , S calligraphic font represents tensors X i,j,k,... the scalar at the { i, j, k, ... } position of
U i the projection factor matrix on the i th dimen-
U the set of projection matrices U i on all dimen-n i the cardinality of the i th dimension of the orig-r i the desired rank of the i th dimension of the  X  d the tensor mode product of a tensor and a ma-mining graph data. A detailed analytical presentation of the proposed method is provided in the next section.
The dimensionality of a tensor is also called the tensor X  X  modes . For example, matrices are tensors of 2 modes. Ta-ble1liststhenotationweuseintherestofthepaper.We denote scalars by letters in lowercase normal font, e.g., a; vectors (tensors of 1 mode) by boldface lowercase letters, e.g., a ; matrices (tensors of 2 modes) by boldface uppercase letters, e.g., U ; and tensors with 3 or more modes by letters of calligraphic font, e.g., X .Atensorcanberepresented in general by a multi-dimensional array in R n 1  X  n 2  X  ... where n i (1  X  i  X  M ) is the cardinality of the i th mode, and M is the total number of modes. Now we give the fol-lowing essential definitions to introduce the overall tensor factorization problem.

Definition 1. ( Tensor mode product ): The product trix U  X  R n d  X  r , denoted by X X  d U ,isanewtensorofsize
Definition 2. ( Tensor mode unfolding ): The unfold-ing (i.e., matricization) on the d th mode of a tensor X X  X the other indices of X .
 The overall problem we aim to solve in this paper is to ef-ficiently factorize a tensor in the manner of Tucker factor-ization [19], which approximates a large tensor by using a small core tensor by changes of basis:
Definition 3. ( Tucker tensor factorization ): Given Algorithm 1 HOSVD ( X ,r 1 ,r 2 , ..., r M ): the standard method for higher-order singular value decomposition.
 Input: the original tensor X , and the desired ranks on each mode r 1 , r 2 , ... , r M .
 Output: acoretensor Y , and projection factor matrices U d (1  X  d  X  M ).  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  1: for d = 1,2,...,M do 2: X d  X  unfold( X , d ) by Definition 2; 3: Compute the covariance matrix of the d th mode un-4: U d  X  the first r d eigenvectors of C d ; 5: end for 6: Y X  X  X  1 ( U 1 ) T  X  2 ( U 2 ) T ...  X  M ( U M ) T ; 7: return Y and U d (1  X  d  X  M ).
 Algorithm 2 Tucker ( , X ,r 1 ,r 2 , ..., r M ): the standard al-ternating least square method for Tucker tensor decomposi-tion.
 Input: convergence threshold ,originaltensor X ,andthe desired ranks r 1 ,r 2 , ..., r M .
 Output: core tensor Y , and the projection matrices U d (1 d  X  M ).  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  1: Initialize U d (1  X  d  X  M )by HOSVD ( X ,r 1 ,r 2 , ..., r 2: repeat 3: for d = 1, 2, ..., M do 5: X d  X  unfold( X , d ); 6: Covariance matrix of current mode C d  X  X d X T d ; 7: U d  X  the first r d eigenvectors of C d ; 8: end for 10: until the increase of ||Y|| is less than . 11: return Y and U d (1  X  d  X  M ). on each mode r 1 ,r 2 , ..., r M ,factorize X into a core ten-d  X  r d (1  X  d  X  M ), such that the factorization error X  X  Y X  1 U 1  X  2 U 2 ....  X  M U M is minimized.
 In other words, the Tucker tensor factorization is equivalent to solving the following minimization problem: where 1  X  d  X  M . The core tensor Y is an approxima-tion of the original tensor X by changing its basis through projecting all original data into U d on each dimension. The function f ( Y , U d ) can be minimized to zero if r d = n modes d . How to solve Equation 1 scalably and effectively is the main challenge we address in this paper. We start from introducing the standard method of solving Equation 1 in the next subsection.
Higher-order singular value decomposition (HOSVD) [4] forms the basis of many tensor decomposition methods de-veloped recently [5,18]. Details of the HOSVD method are shown in Algorithm 1.

In line 2 of Algorithm 1, the matrix X d obtained from unfolding on the d th mode of X is of size n d  X  i = d n i hence the formulation of the covariance matrices has com-is designed to deal with partitions of any number of dimensions. plexity O ( n 2 d i = d n i ). The covariance matrix C obtained in line 3 is of size n d  X  n d , which means the time complexity for computing the eigenvectors of C in line 4 is O ( n 3 d Therefore the overall time complexity for Algorithm 1 is each mode.

In the standard Tucker tensor factorization, it is com-mon to use alternating least squares (ALS) (shown in Al-gorithm 2) to repeat the singular vector projection pro-cess until the variance of the core tensor Y stops increas-ing. Because the size of the projected matrix X d formed in line 5 of Algorithm 2 is of size n d  X  i = d r i ,thecom-plexity of the covariance matrix formulation in line 6 is O ( n 2 d i = d r i ). The body of the for loop of the Tucker method is very similar to HOSVD, which makes it easy to show that the overall time complexity of the Tucker method for the eigen-decomposition and the second is for computing the covariance matrices (the factor C is the number of itera-tions needed for convergence, which can be non-trivial). The complexity of the standard Tucker method is also in cubic time to each mode of a tensor, which is very expensive.
In this subsection, we explain how we solve the factoriza-tion problem iteratively in one mode. The solution to the problem in Equation 1 has the following property: For brevity, we use U to denote the product of projection factor matrices U d | M d =1 , and use Y U to denote the product of thecoretensor Y with all projection matrices. Then Equa-tion 2 can be rewritten as X X  X  U . From the orthogonality of
U we know that Y = Y UU T  X  X  U T ,whichmeans Y is X projected on the subspace formed by U . Similarly we have
Now we derive our one-mode ITF method which refines the core tensor and projection matrices iteratively in one mode. As illustrated in Figure 1, we first factorize a small sub-tensor S from the original tensor X , and then update the obtained factorization by gradually taking into account the remaining data of X . Counting from the zero index on the d th mode of X , we extract a sub-tensor whose d th mode is the same size as the desired core tensor and whose other modes are the same sizes as those of X ,sowehavea Without loss of generality, we assume it is the 1st mode of X where a sub-tensor S is extracted, so S X  R r 1  X  n 2 ...  X  We denote the formulation of S by S X X 1: r 1 where we use the subscripts to represent the size of S on the 1st mode and omit the fixed sizes n d (2  X  d  X  M )onothermodes. By partitioning the first mode of X and extracting the sub-tensor, the original tensor can be represented as: where the size of X r 1 + i (1  X  i  X  n 1  X  r 1 )is1  X  n 2 n M ,whichwecalla slice of X .

Suppose we have obtained the factorization of the sub-tensor: S X X 1: r 1  X   X  Y X  1  X  U 1  X  2  X  U 2 ...  X  M  X  is of the same size as the desired core tensor,  X  U 1 is of size r  X  r 1 ,and  X  U i | M know X 1: r 1  X  X  1: r 1  X  U T  X  U ,where  X  U denotes  X  next slice after X 1: r 1 (e.g., X r 1 +1 ) was not included in the decomposition of S , it is possible that this next slice cannot be recovered from the projection in the subspace formed by  X  U . So we introduce the notion of  X  departure  X , denoted by  X , with which the projection recovery in Equation 3 can hold
We now explain the rationale of using the departure  X  from a graph mining perspective. By using a tensor to represent dynamic graphs, the columns of  X  U (i.e., the sin-gular vectors) are necessarily the major substructures (lin-ear combinations of original features) of previous graphs. The information contained in  X  denotes new formulations of substructures from the current graph that cannot be rep-resented by previously discovered substructures in  X  U .In-tuitively, when important representative substructures are shared among current and previous graphs, the value of  X  would always be small.

After factorizing the sub-tensor, we form a new tensor {  X  Y , X r 1 +1  X  U T + X   X  U T } , where the second term of this new tensor comprises the component of X r 1 +1 that can be repre-sented in the subspace formed by  X  U ,andalsothe X  departure  X  of
X r 1 +1 (i.e.,  X ) that cannot be recovered from the subspace projection. We factorize this new tensor into where U 1 is of size ( r 1 +1)  X  r 1 and U i | M i =2 are all of size r i  X  r i . Then the new updated projection matrices for {X 1: r 1 , X r 1 +1 } are U  X  U ,where U denotes the set of projec-tion matrices U i | M i =1 . Algorithm 3 ITF_OneMode ( X ,r 1 ,r 2 , ..., r M , l ): iteratively decomposing one mode of a tensor by using the 1st mode as an illustration.
 Input: original tensor X , the desired ranks r 1 , r 2 , ... , r and a mode index l on which the tensor is to iteratively decomposed (we use l =1 as an example to explain this al-gorithm in the paper X  X  texts).
 Output: core tensor Y , and the set of all projection matri-ces U .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  1: Form a sub-tensor X 1: r l from the first r 1 slices of 2: {  X  Y ,  X  U } X  HOSVD ( X 1: r l ,r 1 ,r 2 , ..., r M 3: Y old  X   X  Y ; 4: U old  X   X  U ; 5: for i=1, 2, ..., n 1  X  r 1 do 6: Obtain the  X  X eparture X   X   X  X  r 1 + i U T U  X  X  r 1 + i 8: {Y new , U new } X  HOSVD ( X new ,r 1 ,r 2 , ..., r M ); 10: end for 11: Y X  X  U T ; 12: return Y and U .

We then take {X 1: r 1 , X r 1 +1 } as a new sub-tensor that re-places S ,andtreat X r 1 +2 as the new next slice from the original X .Thisiterativeprocessisrepeateduntilwereach the final slice of X (i.e., X n 1 ). We call such an iterative updating procedure the one-mode ITF method.
 The complete procedure of ITF_OneMode is presented in Algorithm 3, where lines 2 and 8 reuse HOSVD as a subrou-tine. Line 1 has the time complexity of O ( r 3 1 + M d =2 r so the overall time complexity of ITF_OneMode is r where among the five terms the first four terms are for de-composing the first sub-tensor, and the last term is for up-dating all successive n 1  X  r 1 slices of the original tensor. Note that n 1 only appears as a linear coefficient in the complexity expression, and that is why it is possible to design a graph mining technique based on ITF that is linear in the size of the graphs. Moreover, as we explain in the next subsection, this computational complexity can be further significantly reduced by applying ITF onto all modes of a tensor.
Compared to the complexity of standard decomposition methods, the reduced complexity of ITF_OneMode arises from computations on the magnitude of r 1 rather than n 1 in both the covariance matrices formulation and their eigen-decompositions. We note that the standard Tucker method becomes a special case of ITF_OneMode when r 1 = n 1 .
By applying the above iterative updating process to all modes of a tensor, we obtain the final ITF method shown in Algorithm 4. Starting from the first mode (line 2 to 4), this algorithm constructs and factorizes a sub-tensor extracted from all modes of the original tensor (line 21) before pro-ceeding to the iterative refinement of factor matrices. We use a recursion to formulate sub-tensors, so that after one mode completes its updating procedure, the algorithm re-cursively returns the decomposition of that mode to other modes, until it reaches the last mode. This makes Algorithm for iteratively decomposing all modes of a tensor. Input: original tensor X , the desired ranks r 1 , r 2 , ... , r and a mode index l (for recursion purposes).
 Output: core tensor Y , and the set of all projection matri-ces U .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  1: // l is uninitialized in the beginning of the algorithm. 2: if mode index l is uninitialized then 3: Initialize l  X  1; 4: end if 5: if l has not reached the last mode of X then 6: // A recursion for applying ITF to all modes. 7: l  X  l +1; 8: {  X  Y ,  X  U } X  ITF ( X ,r 1 ,r 2 , ..., r M , l ); 9: Y old  X   X  Y ; 10: U old  X   X  U ; 11: // Iteratively update factor matrices on the l th mode 12: for i=1, 2, ..., n l  X  r l do 13: Obtain the  X  X eparture X   X   X  X  r l + i  X  X  r l + i U T U ; 15: {Y new , U new } X  HOSVD ( X new ,r 1 ,r 2 , ..., r M ); 17: end for 18: Y X  X  U T ; 19: else 20: // Factorize a sub-tensor extracted from all modes 22: Y old  X   X  Y ; 23: U old  X   X  U ; 24: // Iteratively update factor matrices on the last mode 25: for i=1, 2, ..., n l  X  r l do 26: Obtain the  X  X eparture X   X   X  X  r l + i  X  X  r l + i U T U ; 28: {Y new , U new } X  HOSVD ( X new ,r 1 ,r 2 , ..., r M ); 30: end for 31: Y X  X  U T ; 32: end if 33: return Y and U . 4 different to Algorithm 3 in that any tensor being factorized in Algorithm 4 is of size r 1  X  r 2  X  r 3  X  ...  X  r M , in contrast to that of r 1  X  n 2  X  n 3  X  ...  X  n M in Algorithm 3.
For Algorithm 4, the complexity of line 21 (for decompos-ing the first sub-tensor) is O ( M d =1 r 3 d + M d =1 r 2 and those of line 15 and line 28 on mode i (for decom-posing a new sub-tensor containing a successive slice) are On each mode i ,thereexist n i  X  r i slices to be updated, so the overall complexity of Algorithm 4 is O ( M d =1 r 3 1) explains that using one slice to update the decomposition is cheaper than using multiple slices at a time. It also shows that the complexity of ITF is linear in all modes of a tensor, with all n i (1  X  d  X  M ) being linear coefficients only. We can observe that the reduction in time complexity of ITF compared to that of the standard Tucker method results from the avoidance of computations on the magnitude of n i but on r i , for all modes.

Similar to the standard method, we apply the ALS method to ITF by replacing line 1 and line 7 of Algorithm 2 with our iterative decompositions. We omit the details of ALS for ITF due to page limits.
Our ITF method uses the notion of departure ( X  X  X ) in the tensor approximations. By deriving the following theorem, we show that in ITF theupdateofslice X r 1 +1 on the factor-ization of sub-tensor X 1: r 1 is an effective approximation of a standard direct decomposition on {X 1: r 1 , X r 1 +1 }
Theorem 1. Given the factorization of all previously ob-served slices X 1: r 1 +1 =  X  Y  X  U and a new data slice note the information loss of projecting X r 1 +1 onto  X  U =
X r 1 +1  X  X  r 1 +1  X  U T  X  U , then the approximation error of us-ing ITF compared to using the standard method is no higher norm.

Proof. From Equation 5 we have: By multiplying both sides of the above equation by  X  U ,we obtain: ( r 1 +1)  X  n 2  X  n 3  X  ...  X  n M and the slice {X r 1 +1  X   X + X  is in n 2  X  n 3  X  ...  X  n M .

By contrast, a direct decomposition on the new tensor {X Then by incorporating the definition of the Frobenius norm the approximation error of using ITF compared to using the standard method is =  X  = ||  X   X   X   X  U T  X  U || F where  X  and  X  denote the singular values of tensor {X 1: r 1 X respectively. Given a tensor A and its smallest cardinality min A ,the Frobenius norm can be derived as ||A|| F = tr ( AA  X  )= singular values, and A  X  is the conjugate transpose of A .
This theorem demonstrates that, compared to the stan-dard method, the approximation error of ITF in the worst case is upper bounded by a linear expression of the departure  X  and the projection matrices  X  U , where the columns of indicate previously discovered major frequent substructures. This result can be used to estimate the effects of the iter-ative updating operations on graph mining. As discussed in Section 3.2, when important frequent substructures are commonly shared across different graphs, the value of the departure  X  would typically be small, which suggests low factorization errors of ITF . This expectation is further con-firmed by the empirical results in Section 5.
In the previous section we in troduced different ways of decomposing tensors. Now we explain how we use these tensor decomposition methods to (1) weight emerging edges (word pairs) in publication networks, and (2) classify graphs.
By using the example of publication networks, we define an edge in a graph as a pair of key words extracted from paper titles. We treat the mode of word pairs in a tensor as a feature of the network, and use tensor factorization to per-form feature selection (i.e., dimensionality reduction) on this mode. Given the original data tensor X , both the standard Tucker factorization (Algorithm 2) and our ITF method (Al-gorithm 4) produce two outputs, the core tensor Y ,andthe projection matrices U d (1  X  d  X  M ). Moreover, the matri-ces in all dimensions of Y are arranged in decreasing order of the magnitudes of their variances (i.e., the ordered singu-lar values for the case of matrix decomposition), which is an indication of the significance of the corresponding column vector in U d . In this regard, to analyze the original tensor X , we only need to keep a small number of column vectors in each projection matrix U d , the variances of whose cor-responding matrices in Y can preserve the major variances of
X . Thus, suppose word pairs are in the first mode of the tensors, in order to reduce the number of features in the word pair mode (assume it is the first mode of X ), we only have to look at the r 1 columns contained in U 1 .The r 1 columns of U 1 are linear combinations of the original fea-tures, so if we take the Frobenius norm of each of the n 1 rows of U 1 , these norms can be used as indications of the significance (weights) of the original n 1 features. To this end, all original n 1 word pairs have the obtained weights as-sociated with them. These weights suggest the significance of word pairs in each time period, which we use to weight emerging research topics.
We apply ITF to classification problems by utilizing the new data points that lie in the new low-dimension feature space formed by factor matrices. Given a set of graphs, each of which is associated with a class, the task is to predict the class of a new graph. For many type of graphs, one can gen-eralize their adjacency matrices and extend them to a tensor of 5 modes: vertices  X  vertices  X  verticesLabels  X  edgeLa-bels(or weights)  X  observations(graph instances) .Onecan think of the 5-mode tensors as standard features  X  obser-vations data sheets where the features are represented by 4-mode tensors. Suppose it is the first vertex dimension on which we want to obtain low-dimension new features, then the new feature spaces are defined by the columns of fac-tor matrix U 1 . Given the original data points D original obtain new data points D new which lie in the new spaces by projecting the original data onto the first factor matrix: D fold D new (c.f. Definition 2) on its instance dimension so that it can be learned by standard classifiers. These new data points D new produced by ITF arewhatweuseinthe classification process in comparison with data points pro-duced by the standard Tucker method.
Theobjectivesofourexperimentsareto (1) evaluate the computation time costs and preservation of variances of the ITF algorithms for different settings of the desired ranks, and to (2) test the practical usefulness of the variance preserved by ITF in terms of weighting emerging edges and classifying graphs. We implement 3 ITF by using the Matlab Tensor Toolbox [1]. We use the DBLP bibliography [13] to obtain the publication networks, and use chemical compound data to perform graph classification. The convergence threshold of ALS (c.f. Algorithm 2) is set to 10  X  4 for both the standard Tucker and the ITF method. Experiments are conducted on a PC with a 3.0GHz CPU and 4GB memory.
From the DBLP repository, we select journals and confer-ences whose full book/proceeding titles contain the phrases  X  X ata mining X  or  X  X nowledge discovery X  over a 20 year pe-riod from 1991 to 2010. Four properties (i.e., dimensions) are extracted: (i) pairs of words that appeared in paper titles; (ii) (co-)authors; (iii) booktitles (names of journals and conferences/workshops proceedings); (iv) time of pub-lication. To avoid using trivial features, we only include word pairs that appeared at least 5 times and authors who have at least 5 publication records in DBLP through the 20 years. After removing stop words, we obtain 11917 word pairs (with 1897 unique words), 6399 authors and 171 book-titles, which form 20 tensors (one for each year) in the space of
R 11917  X  6399  X  171 . For tracking the changes of patterns over time, we calculate the difference of all consecutive two years of tensors and use the resulting  X  X ifference tensors X  as the input of the decomposition algorithms.
 In the settings of the following experiments, we evaluate ITF on the first dimension (i.e., word pairs) of the tensors, and set the desired number of singular vectors of other di-mensions ( r 2 and r 3 ) consistently to 50. We note that the focus of this paper is not to determine the best number of singular vectors in approximating the original tensors, but to investigate the improvements of ITF over the standard Tucker method on a consistent number of singular vectors. Hence we do not evaluate the effects of different r 2 and r values, but study the performance of different factorization methods using constant values of r 2 and r 3 .
Comparisons between the Tucker method and ITF method in factorizing the 20 years of tensors are shown in Figure 2. Figure 2(a) demonstrates that compared to the standard Tucker method, ITF uses significantly less time to factor-
We provide our implementation of ITF at http://people. eng.unimelb.edu.au/liuw/ITF.html . Figure 2: Comparisons between Tucker and ITF on compu-tation time and preserved variance when factorizing tensors of DBLP data. We can observe that ITF is able to pre-serve almost the same amount of variance compared to the standard method while using significantly less computation the sub-figures X  legends represent the standard method and the ITF method for r 1 = { 10 , 50 , 100 } . Note that the time complexity of standard method is a function of n 1 (besides other dimensions) and is independent of r 1 . ize the same tensor on the same number of desired singular vectors. This advantage of ITF is more apparent when r 1 is small  X  for example when r 1 = 10, ITF is able to finish the decomposition process in less than 50 minutes for all the 20 years. By contrast, the standard method whose time com-plexity is independent of the value of r 1 takes a much longer time to finish (e.g., about 10 times longer than ITF ). More-Figure 3: Computation time of the standard Tucker method and the ITF method on a controlled number of graph edges when r 1 is fixed at 100. over, the variances preserved by ITF are highly comparable to the standard decomposition method. The values shown in Figures 2(b), 2(c) and 2(d) illustrate that the core tensors obtained from ITF can represent almost as much variance as those obtained from the standard method.

We are also interested in the computation time of the standard and ITF methods when the size of the tensor grows. This scenario is closely related to the dynamic graph mining problem in an online setting, since the number of edges (e.g., word pairs) in practical networks and graphs are usually very likely to increase with time. We control the number of word pairs in the original graphs (which is also the cardinality of the first dimension of the original tensor), and vary it from 500 to more than 5000. The computation time needed in the decompositions by the standard Tucker method and the ITF method is shown in Figure 3. The trends in the figure demonstrate that the computational cost of ITF is not only linear to the value of n 1 but also much lower than that of the standard decomposition method. Moreover, it also can be observed that the larger the size of the graph (i.e., the value of n 1 ), the larger the difference between the computation time needed for ITF and for the standard Tucker method.
In Section 4 we explained how the outputs of tensor factor-ization can be applied in weighting the original word pairs, and in this experiment we use that theory to evaluate the correctness of ITF against the Tucker method. After we decomposed twenty years of tensors, from each year we ob-tained a list of weights for the original word pairs. To dis-card trivial word pairs, we only consider those whose weights are higher than the threshold 0.0001 (note that the column vectors in the projection matrices are unit vectors ,which means the weight of a word pair is typically between 0 and 1). After deleting those word pairs that do not pass this threshold, we obtain subgraphs of the original graphs. To find how popular a research topic is, we weight paper titles by using the edges of these subgraphs. We calculate the sum of the weights of the word pairs that a paper title contains as the weight of that paper title. Thus, paper titles that have higher weights are the ones whose words occupy more variance in the original tensor.

To test whether the weights of paper titles assigned by ITF are similar to those from the standard Tucker method, in each year we select 50 papers that are the highest weighted by ITF , and compare them with the weights produced by the Tucker method. Since the weights suggest the variance in their corresponding tensors, we conduct these compar-isons to check whether the weights found by ITF agree with those found by the Tucker method on the same set of word pairs. We apply paired t -tests on the two groups of weights, one from ITF and the other from the Tucker method, un-der the null hypothesis that the weights are not significantly different between the two groups. The last column 4 of Ta-ble 2 shows the p -values returned from the t -tests, which are mostly very large. This does not reject the null hypoth-esis, suggesting that ITF and Tucker assign insignificantly different weights to the same set of word pairs. This result validates the correctness of the factorization produced by ITF , in that the main variance of key word pairs preserved by ITF is as good as that preserved by the computationally expensive Tucker method.
In this experiment, we compare the IFT and the Tucker method on graph classification problems, where we clas-sify the points that are projected into the row-rank feature spaces defined by the factor matrices. We use chemical com-pound data sets from anti-cancer bioassays 5 ,whereeach compound is treated as a graph of atoms. The task is to predict whether a compound is positive or negative in anti-cancer activities. Although there are limited types of atoms and bonds in the data, the total number of compounds is very large (Table 3).
 Table 3: Statistics of chemical compound data sets.  X  X os% X  represents the proportion of compounds that are positive.
Similar to the graph formulations discussed in Section 4.2, for each chemical compound data set we construct a tensor of 5 modes, where the types of atoms in a compound are converted to labels of vertices, and the lengths of bonds be-tween atoms are treated as weights of edges. Figure 4 shows comparisons of the ITF and the Tucker method in their fac-torization time and preserved variance of each data set. It is easy to observe that ITF factorizes each data set by using much less time than the Tucker method, while preserving
Due to space limit, in this table we only present compar-isons in nine years time from 1996 to 2005. http://pubchem.ncbi.nlm.nih.gov Figure 4: Comparisons between the Tucker and the ITF methods on factoring tensors of chemical compound data sets. similar amounts of variance. Such results indicate that ITF is able to capture the most important substructures that are shared across different compounds.
In addition to the evaluation of preserved variances, we test the correctness of the factor matrices produced by ITF by examining the new data points that lie in the new spaces defined by the factor matrices. A linear logistic regression is used to learn and classify these data points by 5-fold cross validation with 10 repeated runs. The results of classifica-tion on different factorization methods is reported in Table 4. Note that due to the class imbalance, we use the area under the ROC curve (AUC) as the evaluation metric, which is more appropriate than using overall accuracy as the metric.
While there are various ways to compare classifiers across multiple data sets, we adopt the strategy proposed in [6] which evaluates classifiers by performing Friedman tests. We compare the ITF and the Tucker methods on the same ranks ( r 1 =50and r 1 = 100), where p  X  X alues that are lower than 0.05 reject the hypothesis with 95% confidence that the classifiers in the comparison are not statistically differ-ent. As we can see from the high p -values in the bottom of table Table 4, the data points projected onto subspaces formed by ITF are insignificantly different to those by the Table 4: Comparisons of ITF andTuckeronclassifyingthe data points located in their corresponding subspaces, using linear logistic regression.  X  F. test  X  represents the Friedman significance test.
 Breast .583  X  .09 .593  X  .08 .740  X  .05 .722  X  .05 Leukemia .602  X  .04 .607  X  .06 .699  X  .07 .683  X  .04 Melanoma .687  X  .06 .703  X  .05 .693  X  .06 .710  X  .04 Ovarian .699  X  .04 .711  X  .04 .740  X  .04 .731  X  .02 Prostate .641  X  .04 .635  X  .03 .686  X  .04 .689  X  .05 F. test 0.2573 0.5236 Tucker method from the perspective of distinguishing class labels. This suggests that the factor matrices generated by ITF capture as much intrinsic variance as those by the Tucker method in both class labels, and that the new features (e.g., columns of a factor matrix) from ITF are competent repre-sentatives of the row-rank basis of the compound data sets. Moreover, the setting of cross validation also indicates that ITF is insensitive to the orderings of graphs.
The main focus of this paper is on scalably factorizing tensors that represent dynamic graphs. A good tensor fac-torization method should be able to decompose tensors in a computationally cheap manner and also be able to preserve themainvarianceofthedata. Wehaveshownthatthe proposed ITF method is able to significantly reduce compu-tation time compared to the standard factorization method, while the variance preserved by ITF is comparably effective. The superiority of ITF arises from the fact that important substructures are usually shared among different graphs, or graphs across different time periods. We have applied ITF to the weighting of emerging research topics in dynamic publi-cation networks/graphs, and have shown that the time com-plexity of ITF is linear in the size of the networks or graphs, which makes ITF feasible to be applied in an online setting. We have also applied ITF to graph classification problems using chemical compound data sets, showing that ITF is not only a significant speed up of the Tucker method but also captures the intrinsic variance of the original data with respect to different class labels. Moreover, the results from cross validations on graph classification experiments demon-strate that the iterative algorithm ITF is insensitive to the orderings of graphs.

The advantage of ITF is not limited to Tucker decomposi-tion. Other tensor factorization methods such as canonical polyadic decomposition and non-negative tensor factoriza-tion can also benefit from the iterative procedure of ITF . These are the future research directions of ITF we are plan-ning to investigate.
 This research was supported under Australian Research Coun-cil X  X  Discovery Project s funding sche me (DP110102621). [1] B. Bader and T. Kolda. Efficient Matlab computations [2] J. Bunch and J. Hopcroft. Triangular factorization [3] J. Carroll and J. Chang. Analysis of individual [4] L.DeLathauwer,B.DeMoor,andJ.Vandewalle.A [5] L. De Lathauwer and D. Nion. Decompositions of a [6] J. Dem X  sar. Statistical comparisons of classifiers over [7] G. Golub and C. Van Loan. Matrix computations , [8] A.Kapteyn,H.Neudecker,andT.Wansbeek.An [9] T. Kolda and B. Bader. Tensor decompositions and [10] T. Kolda and J. Sun. Scalable tensor decompositions [11] X. Kong and P. Yu. Semi-supervised feature selection [12] P. Kroonenberg and J. De Leeuw. Principal [13] M. Ley. The DBLP computer science [14] K. Ravi Kanth, D. Agrawal, and A. Singh.
 [15] C. Schifanella, K. Candan, and M. Sapino. Fast [16] J. Sun, S. Papadimitriou, and P. Yu. Window-based [17] J. Sun, D. Tao, and C. Faloutsos. Beyond streams and [18] J. Sun, D. Tao, S. Papadimitriou, P. Yu, and [19] L. Tucker. Some mathematical notes on three-mode
