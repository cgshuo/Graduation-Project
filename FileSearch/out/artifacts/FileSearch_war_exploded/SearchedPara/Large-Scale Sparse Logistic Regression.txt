 Logistic Regression is a well-known classification method that has been used widely in many applications of data mining, machine learning, computer vision, and bioinformatics. Sparse logistic re-gression embeds feature selection in the classification framework using the 1 -norm regularization, and is attractive in many appli-Specifically, we formulate the problem as the 1 -ball constrained smooth convex optimization, and propose to solve the problem us-ing the Nesterov X  X  method, an optimal first-order black-box method for smooth convex optimization. One of the critical issues in the use of the Nesterov X  X  method is the estimation of the step size at each of the optimization iterations. Previous approaches either ap-plies the constant step size which assumes that the Lipschitz gra-step size which leads to slow convergence in practice. In this pa-per, we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate. Empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large-scale problems.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithms Logistic regression, sparse learning, 1 -ball constraint, Nesterov X  X  method, adaptive line search method that has been used widely in many applications includ-ing document classification [4, 5], computer vision [9], natural lan-guage processing [14, 22], and bioinformatics [2, 20, 29, 34, 35]. For applications with many features but limited training samples, LR is prone to overfitting. Regularization is commonly applied to reduce overfitting and obtain a robust classifier. The 2 ularization has been used extensively in the LR model, leading to a smooth (differentiable) unconstrained convex optimization prob-lem. Standard optimization algorithms such as Newton method and conjugate gradient method [3, 23] can be applied for solving such a formulation. Recently, there is a growing interest in applying the -norm regularization in the LR model. The use of the 1 -norm regularization has long been recognized as a practical strategy to obtain a sparse model. The 1 -norm regularized sparse LR model is attractive in many applications involving high-dimensional data ously [6, 10, 11, 16, 18, 26, 28, 30, 31].

Solving the 1 regularized logistic regression is, however, more challenging than solving the 2 regularized counterpart, since the regularization term is non-differentiable. Many algorithms have regression algorithm (IRLS-LARS) used a quadratic approxima-tion for the average logistic loss function, which was subsequently solved by the LARS method [7, 18]. The Bayesian logistic regres-sion (BBR) algorithm used a cyclic coordinate descent method for the Bayesian logistic regression [8]. Glmpath is a general solver for the 1 regularized generalized linear models using path following fiers [17]; and it can solve the sparse logistic regression. In [16], an interior-point method was proposed for solving the 1 regular-ized logistic regression. Recently, an algorithm based on the fixed point continuation algorithm [13] was proposed to solve the ularized logistic regression [32]. An extensive comparison among twelve sparse logistic regression algorithms was given in [30].
In this paper, we propose the Lassplore algorithm for solving the problem as the 1 -ball constrained logistical regression formu-tiable, and the problem domain set is closed and convex. We further propose to solve this problem using the Nesterov X  X  method, which One of the critical issues in the use of the Nesterov X  X  method is the estimation of an appropriate step size in each of the optimization iterations. One simple approach is to apply a constant step sizes, which assumes that the Lipschitz gradient is given in advance. An-other approach is to estimate the appropriate step size via the in-exact line search scheme; however, such a scheme generates a se-quence of decreasing step size and may result in slow convergence in practice. We propose to make use of the Nesterov X  X  estimate se-quence for deriving a specialized adaptive line search scheme, in which the appropriate step size is tuned adaptively for each of the optimization iteration. The proposed line search scheme can lead to improved efficiency in practical implementations while preserv-ing the optimal convergence rate. We have performed experimental studies using a collection of large-scale data sets. Empirical com-efficiency of the proposed Lassplore algorithm for the large-scale sparse logistic regression problems.
 Organization : We introduce sparse logistic regression in Section 2, review the Nesterov X  X  method in Section 3, derive an adaptive line search scheme in Section 4, present the Lassplore algorithm in Sec-tion 5, report empirical studies in Section 6, and conclude this paper in Section 7.
Let a  X  R n denote a sample, and b  X  X  X  1 , +1 } be the associ-ated (binary) class label. Logistic regression model is given by: the sample a , w  X  R n is the weight vector, and c  X  R intercept. w T a + c =0 defines a hyperplane in the feature space, on which Prob ( b | a )=0 . 5 . The conditional probability Prob ( b 0 . 5 otherwise.

Suppose that we are given a set of m training data { a i ,b where a i  X  R n denotes the i -th sample and b i  X  X  X  1 , +1 notes the corresponding class label. The likelihood function asso-ciated with these m samples is defined as negative of the log-likelihood function is called the (empirical) lo-gistic loss, and the average logistic loss is defined as: which is a smooth and convex function. We can determine w by minimizing the average logistic loss: leading to a smooth convex optimization problem.

When m , the number of training samples is smaller than n ,the dimension of the samples, directly solving the logistic regression formulation in (3) is ill-posed and may lead to overfitting. A stan-dard technique to avoid overfitting is regularization.
When adding an 1 -norm regularization to f ( w ,c ) , we obtain the 1 -norm regularized logistic regression problem: where &gt; 0 is a regularization parameter. The solution to the norm regularized logistic regression can be interpreted in a Bayesian framework as the maximum a posteriori probability estimate of and c ,when w has a Laplacian prior distribution on R n and covari-ance I ,and c has the uniform prior on R .

Since the 1 regularization term is nonsmooth (non-differentiable), solving the 1 -norm regularized logistic regression is much more challenging than solving the 2 -norm case. Despite of its compu-tational challenge, the 1 -norm regularized logistic regression has sulting model and its empirical success [11, 16, 18, 28, 30, 31]. to treat (4) as a nonsmooth optimization problem, and then solve grafting [28], shooting [11], BBR [8]. The second strategy is to ap-ply some smooth approximation to the 1 -norm, so that (4) can be approximated by a smooth function, which can then be solved by smooth optimization methods, e.g., the smoothl1 [30]. The third strategy is to introduce additional v ariables to reformulate (4) as a smooth optimization problem with smooth constraint functions, tion problem with a smooth objective function, e.g. the iteratively reweighted least squares (IRLS-LARS) [18].

The Lassplore algorithm proposed in this paper belongs to the equivalent counterpart, the 1 -ball constrained logistic regression: for some value of z  X  0 , the radius of the 1 -ball. It is known that there is a one-to-one correspondence between (4) and (5).
We can also employ two types ( 1 and 2 )ofregularizationby solving the following optimization problem The problem (6) reduces to: (i) the logistic regression (3), when setting  X  =0 and z a sufficiently large value; (ii) the regularized logistic regression, when setting  X &gt; 0 and z asuffi-sion, when setting  X  =0 . In the following discussion, we focus on solving the more general formulation (6).
Our proposed Lassplore algorithm is a first-order black-box or-acle method that evaluates at each iteration the function value and gradient. In (6), the value and gradient of  X  2 w 2 are easy to com-pute, so we focus on f ( w ,c ) in the following discussion.
Let b =[ b 1 ,b 2 ,...,b m ] T  X  R m , 1  X  R n be the vector of all ones, A =[ b 1 a 1 ,b 2 a 2 ,...,b m a m ] T  X  R m  X  n ,and m -dimensional vector with all entries being c . Denote f ( [  X  w f ( w ,c ) T ,  X  c f ( w ,c )] T .Wehave where p i = Prob ( b i | a i ) is the conditional probability of b a , denotes the componentwi se multiplication, and ./ denotes componentwise division. We can compute f ( w ,c ) as From (7)-(10), we see that only the matrix-vector multiplications involving A and A T are required for computing the objective and the gradient. Thus, when A is sparse, we can efficiently deal with sparse logistic regression problems with large m and n .
Since our proposed Lassplore algorithm is built on the Nesterov X  X  method with an adaptive line search scheme, we first review the Nesterov X  X  method in Section 3, and derive an adaptive line search Nesterov X  X  method can deal with constrained optimizations by the usage of gradient mapping [25]. For convenience of illustration, we first focus on the unconstrained smooth convex optimization in Sections 3 &amp; 4, and then discuss the extension to the constrained optimization in Section 5. The discussions in Sections 3 &amp; 4 are applicable to the general smooth convex optimization.
Let us consider the following unconstrained smooth convex min-imization problem: where the function g ( x ) belongs to the class of convex and differ-ential family S 1 , 1  X ,L ( R n ) , with L and  X  satisfying L defined in (12) is called the Lipschitz gradient of the function g ( monotone when g ( x ) is convex. Moreover, when  X &gt; 0 , g ( is called a strongly convex function. When the function g ( twice differentiable, we have where I n is an n  X  n identity matrix, and A B indicates that the matrix B  X  A is positive semidefinite. The inequality (14) implies that, L is the upper-bound of the largest eigenvalue of g ( and similarly  X  is the lower-bound of the smallest eigenvalue of g ( x ) ,  X  x . It is clear that the relationship  X   X  L always holds. In the following discussion, we denote x  X  and g  X  as an optimal solution and the optimal objective function value, respectively.
The Nesterov X  X  method utilizes two sequences: { x k } and where { x k } is the sequence of approximate solutions, and is the sequence of searching points. The searching point affine combination of x k  X  1 and x k as where  X  k is a tuning parameter. The approximate solution can be computed as a gradient step of s k as where 1 /L k is the step size. Fig. 1 illustrates how the Nesterov X  X  method works. Starting from an initial point x 0 , we compute and x k +1 recursively according to (15) and (16), and arrive at the optimal solution x  X  .
 In the Nesterov X  X  method,  X  k and L k are two key parameters. When they are set properly, the sequence { x k } can converge to the optimal x  X  at a certain convergence rate. The Nesterov X  X  constant scheme [25] and the Nemirovski X  X  line search scheme [24] are two well-known ones for setting  X  k and L k . Figure 1: Illustration of the Nesterov X  X  method. We set x , and thus s 1 = x 1 . The search point s k is the affine combi-nation of x k  X  1 and x k (the dashed lines), and the next approx-imate solution is obtained by a gradient step of s k (solid lines). We assume that x 6 = x 7 = x  X  ,and x  X  is an optimal solution.
The Nesterov X  X  constant scheme assumes that both L and  X  are known in advance, and it sets L k = L and  X  k according to L and  X  . It has been shown that the resulting scheme can achieve the lower complexity bounds (in the same order) for the class S 1 , first-order black-box methods, and thus the Nesterov X  X  method is an optimal first-order black-box method.

Although the Nesterov X  X  constant scheme can achieve the lower complexity bounds, one major limitation is that both L and  X  need to be known in advance, which is however not the case in many act computation of L and  X  might be much more challenging than solving the problem itself. To address the problem that L is usually unknown in advance, Nemirovski [24] proposed a line search scheme for determining L (see Algorithm 1). In this scheme, we first initialize L k (see Step 2), and then apply a line search process (Steps 4-11) for the sequence L k is non-decreasing with increasing k . Moreover, L k is upper-bounded by 2 L since once L k  X  L , the condition in Step 6 always holds [24, Chapter 10.2, page 163]. For  X  computed based on the sequence { t k } . As the sequence independent of L ,  X  and the function g ( x ) , the sequence identical for all the (smooth convex) optimization problems. Algorithm 1 The Nemirovski X  X  Line Search Scheme Input: L 0 &gt; 0 , x 1 = x 0 , N , t  X  1 =0 , t 0 =1 Output: x N 1: for k =1 to N do 3: Compute s k = x k +  X  k ( x k  X  x k  X  1 ) 4: for j =1 to ... do 5: Compute x k +1 = s k  X  1 L k g ( s k ) 6: if g ( x k +1 )  X  g ( s k )  X  1 2 L k g ( s k ) 2 then 7: goto Step 12 8: else 9: L k =2 L k 10: end if 11: end for 12: Set t k =(1+ 13: end for
T HEOREM 1. [24, Chapter 10.2, pages 163-165] Let L g the optimal solution set. Then for Algorithm 1, we have
Theorem 1 shows that the scheme presented in Algorithm 1 is (i) it cannot achieve the Q-linear rate [25] for the class even when  X &gt; 0 is known; and (ii) the step size 1 /L k allowed to be monotonically decreasing (note, in proving the con-vergence rate in Theorem 1, the relationship L k  X  L k +1 itly enforced [24, Chapter 10.2, page 165]). In the next section, we shall propose an adaptive line search scheme that can avoid these limitations. In this section, we propose an adaptive line search scheme for the Nesterov X  X  method. Our line search scheme is built upon the esti-mate sequence [25, Chapter 2.2], which will be reviewed in Sec-tion 4.1. In our line search scheme, we do not assume that L and  X  are known in advance, but we assume that,  X   X  , the lower-bound of  X  is known in advance. This assumption is reasonable, since 0 is always a lower-bounded of  X  as shown in (13). Moreover, for the sparse logistic regression formulation in (6) of Section 2.1, we have  X   X   X  . The proofs follow similar arguments in [25], and are given in the Appendix.
Definition 1. [25, Chapter 2.2] A pair of sequences {  X  k {  X  k  X  0 } is called an estimate sequence of the function g ( following two conditions hold:
The following theorem provides a systematic way for construct-ing the estimate sequence:
T HEOREM 2. [25, Chapter 2.2] 1 Let us assume that: 1. g ( x ) is smooth and convex, with Lipschitz gradient L and 2.  X  0 ( x ) is an arbitrary function on R n . 3. { s k } is an arbitrary searching sequence on R n . 4. {  X  k } satisfies:  X  k  X  (0 , 1) and 5.  X  0 =1 .
 Then {  X  k ( x ) , X  k } defined by the recursive rules: is an estimate sequence.

If we choose a simple quadratic function for  X  0 ( x ) as  X   X  0 +  X  0 2 x  X  v 0 2 , then we can specify the estimation sequence defined in Theorem 2 as [25]:
Compared to the theorem proposed in [25], we employ  X   X  in (21) rather than  X  ,where  X   X  is a known lower-bound of  X  .Wemakesuch a substitution for cases where  X  is unknown. where the sequences {  X  k } , { v k } and {  X   X  k } satisfy: v  X  +1 =(1  X   X  k )  X  k +  X  k  X   X , (24)  X 
The estimate sequence defined in Definition 1 has the following important property:
T HEOREM 3. [25, Chapter 2.2] Let {  X  k ( x ) } and {  X  k an estimate sequence. For any sequence { x k } ,if we have
Our proposed line search scheme is based on Theorem 3. Specif-{ x k } (generated with the adaptive step size condition in (26), so that the convergence rate of the solution se-quence can be analyzed with the sequence {  X  k } , according to The-orem 3.
 We first show how to satisfy the condition (26) in the following Lemma:
L EMMA 1. Let v 0 = x 0 and  X   X  0 = g ( x 0 ) . If we compute the approximate solution x k +1 and searching point s k by where v k is updated according to (23), and  X  k , X  k x +1 satisfy then, the condition in (26) holds.

Next, we show in the following lemma that s k in (29) can be simplified as the combination of x k  X  1 and x k :
L EMMA 2. The search point given in (29) can be computed by where
Based on the results in Lemmas 1 &amp; 2, we propose our adaptive line search scheme shown in Algorithm 2. The while loop from Step 2 to Step 11 determines whether L k should be increased, so mirovski X  X  line search scheme, L k is upper-bounded by 2 L ,since (31) always holds when L k  X  L . In Step 12, we initialize L L +1 = L k  X  h (  X  ) . Here,  X  =2 L k g condition in Step 6. Intuitively, when  X  is large, the step size used in computing x k +1 as the gradient step of s k is small. In this paper, we employ the following simple piecewise linear function: Thus, L k +1 is reduced to 0 . 8 L k when  X  is large. Our experiments show that this particular choice of h ( . ) works well. We set  X  cording to (33). It is clear that  X  k is dependent on L k Algorithm 2 An Adaptive Line Search Scheme Input:  X   X  ,  X   X  1 =0 . 5 , x  X  1 = x 0 , L 0 = L  X  1 ,  X  Output: x N 1: for k =0 to N do 2: while 1 do 3: Compute  X  k  X  (0 , 1) as the root of L k  X  2 k =(1  X   X  4: Compute s k = x k +  X  k ( x k  X  x k  X  1 ) 6: if g ( x k +1 )  X  g ( s k )  X  1 2 L k g ( s k ) 2 then 7: goto Step 12 8: else 9: L k =2 L k 10: end if 11: end while 13: Set  X  k +1 =(1  X   X  k )  X  k 14: end for
Recall that the step size 1 L k is only allowed to be monotonically decreasing, i.e., L k  X  L k +1 , in the Nemirovski X  X  scheme. We show that although the step size is allowed to decrease, the pro-posed line search scheme preserves the convergence property, as summarized in the following theorem:
T HEOREM 4. For Algorithm 2, we have  X  N  X  min and
From (35), we can observe that, the smaller L k is, the smaller  X 
N is. Therefore, by decreasing the value of L k , we can accelerate the convergence.

Next, we compare our proposed line search scheme with exist-ing ones. The proposed scheme is clearly different from Nesterov X  X  constant scheme, as we assume that the Lipschitz gradient is not known in advance. It differs from the Nemirovski X  X  scheme in the following aspects. First, L k is allowed to decrease in our scheme, our scheme,  X  k is dependent on L k , while  X  k in the Nemirovski X  X  scheme is independent on L k . Third, the Nemirovski X  X  scheme can achieve the Q-linear convergence rate [25] in the strongly convex case, if  X   X  (&gt;0), the lower-bound of  X  is known. We are ready to present the Lassplore algorithm for solving (6). strained optimization in Section 5.1, and then present the Lassplore algorithm in Section 5.2.
The constrained optimization problem in (6) is a special case of the following constrained smooth convex optimization: where G is a convex set. In (6), G is the 1 -ball.

To deal with the constrained optimization problem (37), we con-struct the gradient mapping, which acts a similar role as the gradi-ent in unconstrained optimization. Let L x &gt; 0 .Wedefine distance between y and x . Minimizing g L x , x ( y ) in the domain G is the problem of Euclidean projections onto G :  X  G ( x  X  1 We call the  X  X radient mapping" of g ( . ) on G . From (39), we have which shows that,  X  G ( x  X  1 L x f ( x )) can be viewed as the result of the  X  X radient" step in the anti-direction of the gradient mapping p ( L x , x ) with stepsize 1 L x .

With the gradient mapping, the discussions in Sections 3 &amp; 4 can Moreover, the constrained problem has the same convergence rate as the unconstrained one [25, Chapter 2.2.3].
In this subsection, we present the proposed Lassplore algorithm for solving (6). For convenience of illustration, we denote the ap-proximate solution x k =[ w T k ,c k ] T , and searching point [( k ) T ,s c k ] T . We also note that g (
Algorithm 3 is an application of Algorithm 2 to solve the sparse logistic regression problem. Next, we point out the main differ-ences. First, due to the 1 -ball constraint, in Step 5, w puted by the Euclidean projection  X  G ( . ) . In our problem, G = { x  X  R n | x 1  X  z } is the 1 -ball, and thus  X  G ( . ) is the Eu-clidean projection onto the 1 -ball. We make use of method pro-posed in [21] for computing the pr ojection. Sec ond, the c ondition in Step 6 is replaced with g ( x k +1 )  X  g ( s k )+ g ( s s L k is  X  X ppropriate" for s k [24, Chapter 11]. Due to such a change, we also revise the computation of  X  in Step 12.

By the similar analysis, we can extend Algorithm 1 to solve the sparse logistic regression problem. Algorithm 3 Lassplore: Large-Scale Sparse Logistic Regression Input:  X   X  =  X  , z&gt; 0 ,  X   X  1 =0 . 5 , L 0 ,  X  0  X   X   X  , Output: x 1: for k =0 to ... do 2: while 1 do 3: Compute  X  k  X  (0 , 1) as the root of L k  X  2 k =(1  X   X  4: Compute s k = x k +  X  k ( x k  X  x k  X  1 ) 6: if g ( x k +1 )  X  g ( s k )+ g ( s k ) , x k +1  X  s k + 7: goto Step 12 8: else 9: L k =2 L k 10: end if 11: end while 13: if convergence criterion is satisfied then 14: x = x k +1 and terminate the algorithm 15: end if 16: end for Table 1: Statistics of the test data sets. m denotes the sample size, and n denotes the data dimensionality.

We have performed experimental studies to evaluate the scala-bility of the pr oposed algorithm using the f ollowing six data sets: colon-cancer (colon) [1], leukemia (leu) [12], duke breast-cancer of the test data sets are given in Table 1 (for rcv1, real-sim, and news20, we use a total of 2,000 samples in the following experi-ments). All experiments were carried out on an Intel (R) (T2250) 1.73GHZ processor. The source codes are available online 2
In this experiment, we examine the convergence property of the proposed Lassplore algorithm. We conduct experiments on the three large data sets including real-sim, rcv1, and news20. We set the 1 -ball radius z = m ,the 2 regularization parameter  X  =0 and  X   X  =  X  . We run the algorithms for a total of 1,000 iterations, and report both L k ( 1 /L k is the step size) and the objective function value in (6). The results are shown in Fig. 2, where the red curve corresponds to the proposed adaptive line search scheme, denoted as  X  X dap", and the blue curve corresponds to the Nemirovski X  X  line search scheme, denoted as  X  X emi".

We can observe from Fig. 2 that (i) in the Nemirovski X  X  scheme, the value of L k is nondecreasing, and L k becomes quite large af-ter a few iterations; (ii) in the proposed scheme, the value of L Figure 2: Comparison of the proposed adaptive line search scheme (Adap) and Nemirovski X  X  line search scheme (Nemi) in terms of the value of L k (left column) and the objective func-tion value (right column). For all the plots, the y -axisisplotted in a logarithmic scale. varies during the iterations; (iii) in most cases, the value of L the proposed scheme is about 1 / 10 of the one in the Nemirovski X  X  scheme. As a result, the proposed scheme converges much faster, which is clear from the plots in the right column; and (iv) the pro-posed Lassplore algorithm converges rapidly in the first few itera-tions (note that the y-axis is plotted in a logarithmic scale), which is consistent with the result in Theorem 4.
In this experiment, we evaluate the pathwise solutions. It is of-ten the case in practical applications that the optimal 1 parameter z is unknown. One common approach for solving this problem is to compute the solutions corresponding to a sequence of values of the parameter, e.g., z 1 &lt;z 2 &lt;...&lt;z the optimal one is chosen by evaluating certain criteria. This can be done by simply applying the Lassplore algorithm to solving the s independent problems (called  X  X old-start"). However, a more ef-ficient approach is through the so-called  X  X arm-start", which uses the solution of the previous problem as the warm-start of the lat-ter. Indeed, the proposed Lassplore algorithm can benefit from the warm-start technique, as the solution corresponding to z i corresponding to z i +1 .

We conduct the experiments using the colon data set. We choose logarithmic scale. The results are presented in Fig. 3 (left plot). We can observe from the figure that the warm-start approach requires a fewer number of iterations (and thus less computation time) than the cold-start approach. We show the number of nonzeros of the solution w under different values of z in the right plot of Fig. 3. We can observe that the number of nonzeros usually increases when the radius z becomes larger. Figure 3: Comparison of cold-start and warm-start for com-puting the pathwise solutions using the colon data set in terms of the number of iterations required (left plot). The right plot shows the number of nonzeros of the solution w . z is uniformly distributed over [0 . 005 m, 0 . 5 m ] on a logarithmic scale.
In this experiment, we compare the proposed Lassplore algo-rithm with two recent solvers: Proj ectionL1 [30] and l1-logreg [16] in terms of the computational time for solving the sparse logistic regression. ProjectionL1 has been shown to be one of the fastest methods among the twelve methods studied in [30], and l1-logreg is quite efficient for solving large-scale sparse logistic regression. Both ProjectionL1 and the proposed algorithms are implemented in Matlab; while l1-logreg is implemented in C, with various ex-ternal supports such as BLAS, LAPACK and Intel MKL libraries. The results reported below should be interpreted with caution:  X  X t is very difficult, if not possible, to carry out a fair comparison of solutions methods, due to the issue of implementation (which can have a great influence on the algorithm performance), the choice of algorithm parameters, and the different stopping criterion" [16].
Both ProjectionL1 and l1-logreg solve the 1 -norm regularized logistic regression, while our proposed algorithms solve the constrained logistic regression. To make a fair comparison, we first run the competing algorithm to obtain the solution corresponding to a given 1 -norm regularization parameter , from which we com-pute the corresponding radius z of the 1 -ball, and finally run our proposed algorithms with the computed z . It is known that there exists a max [16], at which the solution to the problem (4) is zero, and thus is usually set as a fractional ratio of max .
 Comparison with ProjectionL1 We use the colon data set in this experiment. We terminate the proposed algorithm, once it achieves increasing dimensionality (under a fixed sample size), we conduct an experiment by sampling the first 100, 200, 300, 400, and 500 dimensions. We try four settings for : 10  X  1 max , 10 10 can observe from the figure that (i) the computational time of Pro-jectionL1 grows much faster than the proposed algorithms when the data dimensionality increases ; and (ii) the proposed algorithm based on the adaptive line search scheme consumes much less time than the one based on the Nemirovski X  X  scheme, which is consis-tent with our previous study. We observe a similar trend on other two small data sets including leukemia and duke breast-cancer. We have not performed the comparison on the three large data sets, as ProjectionL1 does not scale to large data sets.
 Comparison with l1-logreg We use all the six data sets in this ex-periment. l1-logreg employs the duality gap as the stopping crite-rion, and we try the following three settings: 10  X  3 , 10 for exploring the time efficiency under different precisions. Mean-while, we try the following five values for the 1 Figure 4: Comparison of the proposed algorithms and Projec-tionL1 in terms of the computational time (in seconds) using the colon data set. The x -axis denotes the data dimensionality and the y -axis denotes the computational time. The four plots (from left to right and from top to bottom) correspond to the 1 regularization parameter 10  X  1 max , 10  X  2 max , 10  X  3 10 and 10  X  5 max .Foragiven and duality gap, we first run l1-logreg to compute the solution ( w 1 ,c 1 ) ; we then compute the objective value f ( w 1 ,c 1 ) ; and finally we run the proposed algorithm until the obtained objective function value is within the corresponding duality gap of f ( w 1 ,c 1 ) .

The results are shown in Table 2. We can observe from the table tic regression problems of high dimensionality (see Table 1); (ii) the proposed algorithm based on the adaptive line search scheme outperforms the one based on the Nemirovski X  X  scheme by a large margin. In most cases, Adap is over three times faster than Nemi; and (iii) Nemi is generally slower than l1-logreg, while Adap is very competitive with l1-logreg in most cases.
In this paper, we propose the Lassplore algorithm for solving smooth optimization problem, and propose to solve the problem by the Nesterov X  X  method, an optimal first-order black-box method for the smooth convex optimization. One of the critical issues in the use of the Nesterov X  X  method is the estimation of the step size scheme and the Nemirovski X  X  line search scheme are two well-known approaches for setting the step size. The former scheme assumes that the Lipschitz gradient of the given function (to be op-timized) is known in advance, which may not be the case in prac-tice; the latter scheme requires a decreasing sequence of the step sizes which leads to a slow convergence. In this paper, we propose an adaptive line search scheme which allows to adaptively tune the step size and meanwhile guarantees an optimal convergence rate. We have conducted an extensive empirical study by comparing the proposed algorithm with several state-of-the-art algorithms. Our empirical results demonstrate the scalability of the Lassplore algo-rithm for solving large-scale problems.

The efficiency of the proposed algorithm depends on the choice of the function h (  X  ) in (34). We plan to explore other choices of  X  1 , 10  X  2 , ... , 10  X  5 ) correspond to different ratios of over h (  X  ) to further improve the algorithm. Many real-world classifica-tion problems involve data from multiple classes. We plan to extend the Lassplore algorithm for solving large-scale sparse multinomial logistic regression. This work was supported by NSF IIS-0612069, IIS-0812551, CCF-0811790, NIH R01-HG002516, and NGA HM1582-08-1-0016. Proof of Theorem 2: Prove by induction. Considering  X  0 =1 , we have  X  0 ( x )  X  (1  X   X  0 ) g ( x )+  X  0  X  0 ( x )  X   X  belongs to the family class S 1 , 1  X ,L ( R n ) ,wehave[25]: Let (19) holds for some k  X  0 . Then from (21), we have where the first inequality follows from (40) and  X   X   X   X  ,andthelast equality utilizes (20). Therefore, (19) holds for k +1 . Moreover, the condition 4 ensures that  X  k  X  0 .
 Proof of Lemma 1: Prove by induction. It is easy to verify that g ( x 0 )=  X   X  0 holds. Let g ( x k )  X   X   X  we have where the first inequality follows from  X   X  k  X  g ( x k ) and v g ( from (28-31).
 Proof of Lemma 2: From (23), (28), (29) and (30), we can write v +1 as the combination of x k and x k +1 as: where the first equality follows from (29), the second equality fol-lows from (30), and the last equality follows from (28). Hence, from (29), we can write s k +1 as: where the first equality follows from (30), the second equality uti-lizes (43), and the last equality follows from the definition of  X  in (33).
 Proof of Theorem 4: Prove by induction. As required by the input of Algorithm 2,  X  0  X   X   X  .If  X  k  X   X   X  ,wehave Therefore, we conclude that  X  k  X   X   X  always holds. From (30), we have  X  k +1 = L k  X  2 k  X   X   X  ,sothat  X  k  X   X  =1 (1  X   X  k ) , we can get
We have  X  0  X   X  0  X  0 ,since  X  0 =1 .If  X  k  X   X  0  X  k ,wehave  X  +1 =(1  X   X  k )  X  k +  X  k  X   X   X  (1  X   X  k )  X  k  X  (1  X   X  k lows from  X  k +1 =(1  X   X  k )  X  k . Therefore, L k  X  2 k  X   X  k +1 always holds.

Since  X  k  X  (0 , 1) and  X  k +1 =(1  X   X  k )  X  k , it is clear that  X  strictly decreasing. Denote a k = 1  X  We have a 0 = 1  X   X  0 =1 ,since  X  0 =1 . From (47), we have which leads to Incorporating (46) and (49), we obtain (35).
 In Algorithm 2, Steps 3-6 ensure that conditions (28-31) hold. According to Lemma 1, the condition (26) holds. By using Theo-rem 3, we obtain (36).
