 Relational datasets are being generated at an alarmingly rapid rate across organizations and industries. Compressing these datasets could significantly reduce storage and archival costs. Traditional compression algorithms, e.g., gzip, are suboptimal for compressing relational datasets since they ignore the table structure and relation-ships between attributes.

We study compression algorithms that leverage the relational structure to compress datasets to a much greater extent. We develop S
QUISH , a system that uses a combination of Bayesian Networks and Arithmetic Coding to capture multiple kinds of dependencies among attributes and achieve near-entropy compression rate. S also supports user-defined attributes: users can instantiate new data types by simply implementing five functions for a new class inter-face. We prove the asymptotic optimality of our compression al-gorithm and conduct experiments to show the effectiveness of our system: S QUISH achieves a reduction of over 50% in storage size relative to systems developed in prior work on a variety of real datasets.
From social media interactions, commercial transactions, to sci-entific observations and the internet-of-things, relational datasets are being generated at an alarming rate. With these datasets, that are either costly or impossible to regenerate, there is a need for periodic archival, for a variety of purposes, including long-term analysis or machine learning, historical or legal factors, or public or private access over a network. Thus, despite the declining costs of storage, compression of relational datasets is still important, and will stay important in the coming future.
 One may wonder if compression of datasets is a solved problem. Indeed, there has been a variety of robust algorithms like Lempel-Ziv [21], WAH [20], and CTW [17] developed and used widely for data compression. However, these algorithms do not exploit the re-lational structure of the datasets: attributes are often correlated or dependent on each other, and identifying and exploiting such cor-relations can lead to significant reductions in storage. In fact, there are many types of dependencies between attributes in a relational dataset. For example, attributes could be functionally dependent on other attributes [3], or the dataset could consist of several clusters of tuples, such that all the tuples within each cluster are similar to each other [8]. The skewness of numerical attributes is another im-portant source of redundancy that is overlooked by algorithms like Lempel-Ziv [21]: by designing encoding schemes based on the dis-tribution of attributes, we can achieve much better compression rate than storing the attributes using binary/float number format.
There has been some limited work on compression of relational datasets, all in the recent past [3, 6, 8, 12]. In contrast with this line of prior work, S QUISH uses a combination of Bayesian Net-works coupled with Arithmetic Coding [19]. Arithmetic Coding is a coding scheme designed for sequence of characters. It requires an order among characters and probability distributions of charac-ters conditioned on all preceding ones. Incidentally, Bayesian Net-works fulfill both requirements: the acyclic property of Bayesian Network provides us an order (i.e., the topological order), and the conditional probability distributions are also specified in the model. Therefore, Bayesian Networks and Arithmetic Coding are a perfect fit for relational dataset compression.

However, there are several challenges in using Bayesian Net-works and Arithmetic Coding for compression. First, we need to identify a new objective function for learning a Bayesian Net-work, since conventional objectives like Bayesian Information Cri-terion [15] are not designed to minimize the size of the compressed dataset. Another challenge is to design a mechanism to support at-tributes with an infinite range (e.g., numerical and string attributes), since Arithmetic Coding assumes a finite alphabet for symbols, and therefore cannot be applied to those attributes. To be applicable to the wide variety of real-world datasets, it is essential to be able to handle numbers and strings.

We deal with these challenges in developing S QUISH . As we show in this paper, the compression rate of S QUISH is near-optimal for all datasets that can be efficiently described using a Bayesian Network. This theoretical optimality reflects in our experiments as well: S QUISH achieves a reduction in storage on real datasets of over 50% compared to the nearest competitor. The reason be-hind this significant improvement is that most prior papers use sub-optimal techniques for compression.

In addition to being more effective at compression of relational datasets than prior work, S QUISH is also more powerful . To demon-strate that, we identify the following desiderata for a relational dataset compression system:  X  Attribute Correlations (AC). A relational dataset compression  X  Lossless and Lossy Compression (LC). A relational dataset com- X  Numerical Attributes (NA). In addition to string and categorical  X  User-defined Attributes (UDA). A relational dataset compres-In contrast to prior work [3, 6, 8, 12] X  X ee Table 1 X  X ur system, S QUISH , can capture all of these desiderata. To support UDA (User Defined Attributes), S QUISH surfaces a new class interface, called the S QU ID (short for S QUISH Interface for Data types): users can instantiate new data types by simply implementing the required five functions. This interface is remarkably powerful, especially for datasets in specialized domains. For example, a new data type corresponding to genome sequence data can be implemented by a user using a few hundred lines of code. By encoding domain knowledge into the data type definition, we can achieve a signif-icant higher compression rate than using  X  X niversal X  compression algorithms like Lempel-Ziv [21].
 Table 1: Features of Our System contrasted with Prior Work
The rest of this paper is organized as follows. In Section 2, we formally define the problem of relational dataset compression, and briefly explain the concepts of Arithmetic Coding. In Section 3 we discuss Bayesian Network learning and related issues, while details about Arithmetic Coding are discussed in Section 4. In Section 5, we prove the asymptotic optimality of the compression algorithm. In Section 6, we conduct experiments to compare S prior systems and evaluate its running time and parameter sensi-tivity. We describe related work in Section 7. All our proofs can be found in our technical report [7], along with a brief review of Bayesian Networks and illustrative examples of our compression algorithm.

The source code of S QUISH is available on GitHub: https://gith ub.com/Preparation-Publication-BD2K/db_compress In this section, we define our problem more formally, and provide some background on Arithmetic Coding.
We follow the same problem definition proposed by Babu et al. [3]. Suppose our dataset consists of a single relational table T , with n rows and m columns (our techniques extend to multi-relational case as well). Each row of the table is referred to as a tuple and each column of the table is referred to as an attribute. We assume that each attribute has an associated domain that is known to us. For instance, this information could be described when the table schema is specified.

The goal is to design a compression algorithm A and a decom-pression algorithm B , such that A takes T as input and outputs a compressed file C ( T ) , and B takes the compressed file C ( T ) as input and outputs T 0 as the approximate reconstruction of T . The goal is to minimize the file size of C ( T ) while ensuring that the recovered T 0 is close enough to T .

The closeness constraint of T 0 to T is defined as follows: For each numerical attribute i , for each tuple t and the recovered tuple t t and t 0 respectively, and i is error threshold parameter provided by the user. For non-numerical attributes, the recovered attribute value must be exactly the same as the original one: t i = t that this definition subsumes lossless compression as a special case with i = 0 .
Arithmetic coding [19, 11] is a state-of-the-art adaptive compres-sion algorithm for a sequence of dependent characters. Arithmetic coding assumes as a given a conditional probability distribution model for any character, conditioned on all preceding characters. If the sequence of characters are indeed generated from the prob-abilistic model, then arithmetic coding can achieve a near-entropy compression rate [11].

Formally, arithmetic coding is defined by a finite ordered alpha-bet A , and a probabilistic model for a sequence of characters that specifies the probability distribution of each character X tioned on all precedent characters X 1 ,...,X k  X  1 . Let { a string of length n . To compute the encoded string for { a first compute a probability interval for each character a
We define the product of two probability interval as:
The probability interval for string { a n } is the product of proba-bility intervals of all the characters in the string:
Let k be the smallest integer such that there exists a non-negative integer 0  X  M &lt; 2 k satisfying: Then the k -bit binary representation of M is the encoded bit string of { a n } .

An example to illustrate how arithmetic coding works can be found in Figure 1. The three tables at the right hand side specify the probability distribution of the string a 1 a 2 a 3 . The blocks at the left hand show the associated probability intervals for the strings: for example,  X  X ba X  corresponds to [0 . 12 , 0 . 204] = [0 , 0 . 4]  X  [0 . 3 , 1]  X  map each possible string { a n } to disjoint probability intervals. By using these probability intervals to construct encoded strings, we can make sure that no code word is a prefix of another code word.
Notice that the length of the product of probability intervals is exactly the product of their lengths. Therefore, the length of each probability interval is exactly the same as the probability of the corresponding string Pr ( { a n } ) . Using this result, the length of encoded string can be bounded as follows: The overall workflow of S QUISH is illustrated in Figure 2. S uses a combination of Bayesian networks and arithmetic coding for Figure 2: Workflow of the Compression and Decompression Algorithm compression. The workflow of the compression algorithm is the following: 1. Learn a Bayesian network structure from the dataset, which 2. Apply arithmetic coding to compress the dataset, using the 3. Concatenate the model description file (describing the Bayesian In this section, we focus on the first step of this workflow. We focus on the remaining steps (along with decompression) in Section 4.
Although the problem of Bayesian network learning has been extensively studied in literature [9], conventional objectives like Bayesian Information Criterion (BIC) [15] are suboptimal for the purpose of compressing datasets. In Section 3.1, we derive the cor-rect objective function for learning a Bayesian network that mini-mizes the size of the compressed dataset and explain how to modify existing Bayesian network learning algorithms to optimize this ob-jective function.
 The general idea about how to apply arithmetic coding on a Bayesian network is straightforward: since the graph encoding the structure of a Bayesian Network is acyclic, we can use any topolog-ical order of attributes and treat the attribute values as the sequence of symbols in arithmetic coding. However, arithmetic coding does not naturally apply to non-categorical attributes. In Section 3.2, we introduce S QU ID, the mechanism for supporting non-categorical and arbitrary user-defined attribute types in S QUISH . S interface for every attribute type in S QUISH , and example S for categorical, numerical and string attributes are demonstrated in Section 3.3 to illustrate the wide applicability of this interface. We describe the S QU ID API in Section 3.4.
Many Bayesian network learning algorithms search for the opti-mal Bayesian network by minimizing some objective function (e.g., negative log-likelihood, BIC [15]). These algorithms usually have two separate components [9]:  X  A combinatorial optimization component that searches for a  X  A score evaluation component that evaluates the objective func-The two components above are independent in many algorithms. In that case, we can modify an existing Bayesian network learning algorithm by changing the score evaluation component, while still using the same combinatorial optimization component. In other words, for any objective function, as long as we can efficiently evaluate it based on a fixed graph structure, we can modify existing Bayesian network learning algorithms to optimize it.
 In this section, we derive a new objective function for learning a Bayesian network that minimizes the size of compressed dataset. We show that the new objective function can be evaluated effi-ciently given the structure graph. Therefore existing Bayesian Net-work learning algorithms can be used to optimize it.

Suppose our dataset D consists of n tuples, and each tuple t contains m attributes a i 1 ,a i 2 ,...,a im . Let B be a Bayesian net-work that describes a joint probability distribution over the attributes. Clearly, B contains m nodes, each corresponding to an attribute. The total description length of D using B is S ( D |B ) = S ( B ) + S ( Tuples |B ) , where S ( B ) is the size of description file of B , and S ( Tuples |B ) is the total length of encoded binary strings of tuples using arithmetic coding. For the model description length S ( B ) , we have S ( B ) = P m i =1 S ( M i ) , where m is the number of attributes in our dataset, and M 1 ,..., M m are the models for each attribute in B . The expression S ( Tuples |B ) is just the sum of the S ( t (the lengthes of the encoded binary string for each t i ). We have the following decomposition of S ( t i |B ) : where parent ( a ij ) is the set of parent attributes of a is the indicator function of whether a is a numerical attribute or not, and j is the maximum tolerable error for attribute a will justify this decomposition in Section 3.3, after we introduce the encoding scheme for numerical attributes.

Therefore, the total description length S ( D |B ) can be decom-posed as follows:
S ( D |B )  X  Note that the term in the second line does not depend on either B or M i . Therefore we only need to optimize the first summation. We denote each term in the first summation on the right hand side as obj j :
For each obj j , if the network structure (i.e., parent ( a then obj j only depends on M j . In that case, optimizing S ( D |B ) is equivalent to optimizing each obj j individually. In other words, if we fix the Bayesian network structure in advance, then the param-eters of each model can be learned separately.

Optimizing obj j on M j is exactly the same as maximizing like-lihood. For many models, a closed-form solution for identifying maximum likelihood parameters exists. In such cases, the optimal M j can be quickly determined and the objective function S ( D |B ) can be computed efficiently.
 Structure Learning In general, searching for the optimal Bayesian network structure is NP-hard [9]. In S QUISH , we implemented a simple greedy algo-rithm for this task. The algorithm starts with an empty seed set, and repeatedly finds new attributes with the lowest obj j these new attributes to the seed set. The pseudo-code can be found in our technical report [7].

The greedy algorithm has a worst case time complexity of O ( m where m is the number of columns and n is the number of tuples in the dataset. For large datasets, even this simple greedy algorithm is not fast enough. However, note that the objective values obj are only used to compare different models. So we do not require exact values for them, and some rough estimation would be suffi-cient. Therefore, we can use only a subset of data for the structure learning to improve efficiency. Encoding and Decoding Complex Attributes Before applying arithmetic coding on a Bayesian network to com-press the dataset as we stated earlier, there are two issues that we need to address first: To address these difficulties, we introduce the concept of S short for S QUISH Interface for Data types. A S QU ID is a (possibly infinite) decision tree [18] with non-negative probabilities associ-ated with edges in the tree, such that for every node v , the proba-bilities of all the edges connecting v and v  X  X  children sum to one.
Figure 3 shows an example infinite S QU ID for a positive nu-merical attribute X . As we can see, each edge is associated with a decision rule and a non-negative probability. For each non-leaf node v 2 k  X  1 , the decision rules on the edges between v children v 2 k and v 2 k +1 are x  X  k and x &gt; k respectively. Note that these two decision rules do not overlap with each other and covers all the possibilities. The probabilities associated with these two rules sum to 1 , which is required in S QU ID. This S scribes the following probability distribution over X :
In Section 4, we will show that we can encode or decode an attribute using Arithmetic Coding if the probability distribution of this attribute can be represented by a S QU ID.

As shown in Figure 3, a S QU ID naturally controls the maximum tolerable error in a lossy compression setting. Each leaf node v corresponds to a subset A v of attribute values such that for every a  X  A v , if we start from the root and traverse down according to the decision rules, we will eventually reach v . As an example, in Figure 3, for each leaf node v 2 k we have A v 2 k = ( k  X  1 ,k ] . Let a v be the representative attribute value of a leaf node v , then the maximum possible recovery error for v is:
Let T i be the S QU ID corresponding to the i th attribute. As long as for every v  X  T i , v is less than or equal to the maximum tol-erable error i , we can satisfy the closeness constraint (defined in Section 2.1).
 Using User-defined Attributes as Predictors
To allow user-defined attributes to be used as predictors for other attributes, we introduce the concept of attribute interpreters, which translate attributes into either categorical or numerical values. In this way, these attributes can be used as predictors for other at-tributes.

The attribute interpreters can also be used to capture the essen-tial features of an attribute. For example, a numerical attribute could have a categorical interpreter that better captures the inter-nal meaning of the attribute. This process is similar to the feature extraction procedure in many data mining applications, and may improve compression rate.
In S QUISH , we have implemented models for three primitive data types. We intended these models to both illustrate how S can be used to define probability distributions, and also to cover most of the basic data types, so the system can be used directly by casual users without writing any code.

We implemented models for the following types of attributes:  X  Categorical attributes with finite range.  X  Numerical attributes, either integer or float number.  X  String attributes Categorical Attributes
The distribution over a categorical attributes can be represented using a trivial one-depth S QU ID.
 Numerical Attributes
For a numerical attribute, we construct the S QU ID using the idea of bisection . Each node v is marked with an upper bound v lower bound v l , so that every attribute value in range ( v pass by v on its path from the root to the corresponding leaf node. Each node has two children and a bisecting point v m , such that the two children have ranges ( v l ,v m ] and ( v m ,v r ] respectively. The branching process stops when the range of the interval is less than 2 , where is the maximum tolerable error. Figure 4 shows an example S QU ID for numerical attributes.

Since each node represents a continuous interval, we can com-pute its probability using the cumulative distribution function. The branching probability of each node is:
Clearly, the average number of bits that is needed to encode a numerical attribute depends on both the probability distribution of the attribute and the maximum tolerable error . The following theorem gives us a lower bound on the average number of bits that is necessary for encoding a numerical attribute (the proof can be found in our technical report [7]):
T HEOREM 1. Let X  X  X  X  R be a numerical random vari-able with continuous support X and probability density function f ( X ) . Let g : X  X  { 0 , 1 }  X  be any uniquely decodable encoding function, and h : { 0 , 1 }  X   X  X be any decoding function. If there exists a function  X  : X  X  R + such that: and g,h satisfies the -closeness constraint: Then Furthermore, if g is the bisecting code described above, then where l = min v ( v r  X  v l ) is the minimum length of probability intervals in the tree.

Equation (1) is a mild assumption that holds for many common probability distributions, including uniform distribution, Gaussian distribution, and Laplace distribution [2].

To understand the intuition behind the results in Theorem 1. Let us consider a Gaussian distribution as an example: In this case, Substituting into the first expression, we have: Note that when is small compared to  X  (e.g., &lt; 1 10  X  ), the last term is approximately zero. Therefore, the number of bits needed to compress X is approximately log 2  X  .
 Now consider the second result, Let l = 2 , and when &lt; 1 10  X  , the last term is approximately zero. Comparing the two results, we can see that the bisecting scheme achieves near optimal compression rate.

Theorem 1 can be used to justify the decomposition in Sec-tion 3.1. Recall that we used the following expression as an ap-proximation of len ( g ( X )) : Compared to either the upper bound or lower bound in Theorem 1, the only term we omitted is the term related to  X  ( X ) . As we have seen in the Gaussian case, this term is approximately zero when is smaller than  X  10 where  X  is the standard deviation parameter. The same conclusion is also true for the Laplace distribution [2] and the uniform distribution.
 String Attributes
The S QU ID for string attributes can be viewed as having two steps: 1. determine the length of the string 2. determine the characters of the string The length of a string can be viewed as an integer, so we can use exactly the same bisecting rules as for numerical attributes. After that, we use n more steps to determine each character of the string, where n is the string X  X  length. The probability distribution of each character can be specified by conventional probabilistic models like the k -gram model.
In S QUISH , S QU ID is defined as an abstract class [1]. There are five functions that are required to be implemented in order to define a new data type using S QU ID. These five functions allow the system to interactively explore the S QU ID class: initially, the current node pointer is set to the root of S QU ID; each function will either acquire information about the current node, or move the current node pointer to one of its children. Table 2 lists the five functions together with their high level description.

We also develop another abstract class called S QU IDM ODEL S
QU IDM ODEL first reads in all the tuples in the dataset, then gen-erates a S QU ID instance and an estimation of the objective value obj j derived in Section 3.1:
There are two reasons behind this design:
S QU IDM ODEL requires six functions to be implemented. These functions allow the S QU IDM ODEL to iterate over the dataset and generate S QU ID instances. The specification of these functions and the psuedo-code of their interactions with S QU ID can be found in our technical report [7].

A S QU IDM ODEL instance is initialized with the target attribute and the set of predictor attributes. After that, the model instance will read over all tuples in the dataset and need to decide the optimal choice of parameters. The model also needs to return an estimate of the objective value obj j , which will be used in the Bayesian network structure learning algorithm to compare models. Finally, S
QU IDM ODEL should be able to generate S QU ID instances based on parent attribute values.
In this section, we discuss how we can use arithmetic coding cor-rectly for compression and decompression given a Bayesian Net-work. In Section 4.1, we discuss implementation details that en-sure the correctness of arithmetic coding using finite precision float numbers. In Section 4.2 we describe the decompression algorithm.
We use the same notation as in Section 3.1: a tuple t contains m attributes, and without loss of generality we assume that they follow the topological order of the Bayesian network: We first compute a probability interval for each branch in a S For each S QU ID T , we define PI T as a mapping from branches of T to probability intervals. The definition is similar to the one in Section 2.2: let v be any non-leaf node in T , suppose v has k chil-dren u 1 ,...,u k , and the edge between v and u i is associated with probability p i , then PI T ( v  X  u i ) is defined as:
Now we can compute the probability interval of t . Let T j S
QU ID for a j conditioned on its parent attributes. Denote the leaf node in T j that a j corresponds to as v j . Suppose the path from the root of T j to v j is u j 1  X  u j 2  X  ...  X  u jk j  X  v j . Then, the probability interval of tuple t is: where  X  is the probability interval multiplication operator defined in Section 2.2. The code string of tuple t corresponds to the largest subinterval of [ L,R ] of the form [2  X  k M, 2  X  k ( M +1)] as described in Section 2.2.

In practice, we cannot directly compute the final probability in-terval of a tuple: there could be hundreds of probability intervals in the product, so the result can easily exceed the precision limit of a floating-point number.

Algorithm 1 shows the pseudo-code of the precision-aware com-pression algorithm. We leverage two tricks to deal with the finite precision problem: the classic early bits emission trick [10] is de-scribed in Section 4.1.1; the new deterministic approximation trick is described in Section 4.1.2.
 Algorithm 1 Encoding Algorithm
Without loss of generality, suppose Define [ L i ,R i ] as the product of first i probability intervals: If there exist positive integer k i and non-negative integer M that Then the first k i bits of the code string of t must be the binary representation of M i . Define Then it can be verified that Therefore, we can immediately output the first k i bits of the code string. After that, we compute the product: We can recursively use the same early bit emitting scheme for this product. In this way, we can greatly reduce the likelihood of preci-sion overflow.
For probability intervals containing 0 . 5 , we cannot emit any bits early. In rare cases, such a probability interval would exceed the precision limit, and the correctness of our algorithm would be com-promised.

To address this problem, we introduce the deterministic approxi-mation trick. Recall that the correctness of arithmetic coding relies on the non-overlapping property of the probability intervals. There-fore, we do not need to compute probability intervals with perfect accuracy: the correctness is guaranteed as long as we ensure these probability intervals do not overlap with each other.

Formally, let t 1 ,t 2 be two different tuples, and suppose their probability intervals are:
The deterministic approximation trick is to replace  X  operator with a deterministic operator that approximates  X  and has the following properties:
In other words, the product computed by operator is always a subset of the product computed by  X  operator, and operator al-ways ensures that the product probability interval has length greater than or equal to after emitting bits. The first property guarantees the non-overlapping property still holds, and the second property prevents potential precision overflow. As we will see in Section 4.2, these two properties are sufficient to guarantee the correctness of arithmetic coding.
When decompressing, S QUISH first reads in the dataset schema and all of the model information, and stores them in the main mem-ory. After that, it scans over the compressed dataset, extracts and decodes the binary code strings to recover the original tuples. Algorithm 2 Decoding Algorithm Algorithm 2 describes the procedure to decide the next branch. The decoder maintains two probability intervals I b and I probability interval corresponding to all the bits that the algorithm has read in so far. I t is the probability interval corresponding to all decoded attributes. At each step, the algorithm computes the product of I t and the probability interval for every possible attribute value, and then checks whether I b is contained by one of those probability intervals. If so, we can decide the next branch, and update I t accordingly. If not, we continue reading in the next bit and update I b .

By calling Algorithm 2 repeatedly, we can gradually decode the whole tuple. The full decoding procedure with an illustrative ex-ample can be found in our technical report [7].

Notice that Algorithm 2 mirrors Algorithm 1 in the way it com-putes probability interval products. This design is to ensure that the encoding and decoding algorithm always apply the same de-terministic approximation that we described in Section 4.1.2. The following theorem states the correctness of the algorithm (the proof can be found in our technical report [7]):
T HEOREM 2. Let [ l 1 ,r 1 ] ,..., [ l n ,r n ] be probability intervals with r i  X  l i  X  where is the small constant defined in Sec-tion 4.1.2. Let s be the output of Algorithm 1 on these probabil-ity intervals. Then Algorithm 2 can always determine the correct branch from alternatives using s as input:
We can prove that S QUISH achieves asymptotic near-optimal compression rate for lossless compression if the dataset only con-tains categorical attributes and can be described efficiently using a Bayesian network (the proof can be found in [7]):
T HEOREM 3. Let a 1 ,a 2 ,...,a m be categorical attributes with joint probability distribution P ( a 1 ,...,a m ) that decomposes as such that
Suppose the dataset D contains n tuples that are i.i.d. samples from P . Let M = max i card ( a i ) be the maximum cardinality of attribute range. Then S QUISH can compress D using less than H ( D ) + 4 n + 32 mM c +1 bits on average, where H ( D ) is the entropy [5] of the dataset D .

Thus, when n is large, the difference between the size of the compressed dataset using our system and the entropy 1 of D is at most 5 n , that is only 5 bits per tuple. This indicates that S asymptotically near-optimal for this setting.
 When the dataset D contains numerical attributes, the entropy H ( D ) is not defined, and the techniques we used to prove Theo-rem 3 no longer apply. However, in light of Theorem 1, it is likely that S QUISH still achieves asymptotic near-optimal compression.
In this section, we evaluate the performance of S QUISH against the state of the art semantic compression algorithms SPARTAN [3] and ItCompress [8] (see Table 1). For reference we also include the performance of gzip [21], a well-known syntactic compression algorithm.

We use the following four publicly available datasets:
By Shannon X  X  source coding theorem [5], there is no algorithm that can achieve compression rate higher than entropy.
The first three datasets have been used in previous papers [3, 8], and the compression ratio achieved by SPARTAN, ItCompress and gzip on these datasets have been reported in Jagadish et al. X  X  work [8]. We did not reproduce these numbers and only used their reported performance numbers for comparison. For the Census dataset, the previous papers only used a subset of the attributes in the experiments ( 7 categorical attributes and 7 numerical at-tributes). Since we are unaware of the selection criteria of the at-tributes, we are unable to compare with their algorithms, and we will only report the comparison with gzip.

For the Corel and Forest-Cover datasets, we set the error toler-ance as a percentage ( 1% by default) of the width of the range for numerical attributes as in previous work. For the Census dataset, we set all error tolerance to 0 (i.e. the compression is lossless). For the Genomes dataset, we set the error tolerance for integer attributes to 0 and float number attributes to 10  X  8 .

In all experiments, we only used the first 2000 tuples in the struc-ture learning algorithm to improve efficiency. All available tuples are used in other parts of the algorithm.
Figure 5 shows the comparison of compression rate on the Corel and Forest-Cover datasets. In these figures, X axis is the error tol-erance for numerical attributes (% of the width of range), and Y axis is the compression ratio, defined as follows:
As we can see from the figures, S QUISH significantly outper-forms the other algorithms. When the error tolerance threshold is small (0.5%), S QUISH achieves about 50% reduction in com-pression ratio on the Forest Cover dataset and 75% reduction on the Corel dataset, compared to the nearest competitor ItCompress (gzip) , which applies gzip algorithm on top of the result of ItCom-press. The benefit of not using gzip as a post-processing step is that we can still permit tuple-level access without decompressing a larger unit.

The remarkable superiority of our system in the Corel dataset re-flects the advantage of S QUISH in compressing numerical attributes. Numerical attribute compression is known to be a hard problem [14] and none of the previous systems have effectively addressed it. In contrast, our encoding scheme can leverage the skewness of the distribution and achieve near-optimal performance.

Figure 6 shows the comparison of compression rate on the Cen-sus and Genomes datasets. Note that in these two datasets, we set the error tolerance threshold to be extremely small, so that the com-pression is essentially lossless. As we can see, even in the lossless compression scenario, our algorithm still outperforms gzip signifi-cantly. Compared to gzip, S QUISH achieves 48% reduction in com-pression ratio in Census dataset and 56% reduction in Genomes dataset.
In this dataset, many attributes are optional and these numbers indicate the average number of attributes that appear in each tuple.
As we have seen in the last section, S QUISH achieved superior compression ratio in all four datasets. In this section, we use de-tailed case studies to illustrate the reason behind the significant im-provement over previous papers. In this section, we study the source of the compression in S categorical attributes. We will use three different treatments for the categorical attributes and see how much compression is achieved for each of these treatments:
We will use the Genomes and Census dataset here since they consist of mostly categorical attributes. We keep the compression algorithm for numerical attributes unchanged in all treatments. Fig-ure 7 shows the compression ratio of the three treatments:
As we can see, the compression ratio of the basic domain cod-ing scheme can be improved up to 70 % if we take into account the skewness of the distribution in attribute values. Furthermore, the correlation between attributes is another opportunity for com-pression, which improved the compression ratio by 50% in both datasets.

An interesting observation is that the Column treatment achieves comparable compression ratio as gzip in both datasets, which sug-gests that gzip is in general capable of capturing the skewness of distribution for categorical attributes, but unable to capture the cor-relation between attributes.
We now study the source of the compression in S QUISH for numerical attributes. We use the following five treatments for the numerical attributes:
We use the Corel dataset here since it contains only numerical attributes. The error tolerance in all treatments except the last are set to be 10  X  7 to make sure the comparison is fair (IEEE single for-mat has precision about 10  X  7 ). All the numerical attributes in this dataset are in range [0 , 1] , with a distribution peaked at 0 . Figure 8 shows the compression ratio of the five treatments.

As we can see, storing numerical attributes as float numbers in-stead of strings gives us about 55% compression. However, the compression rate can be improved by another 50% if we recognize distributional properties (i.e., range and skewness). Utilizing the correlation between attributes in the Corel dataset only slightly im-proved the compression ratio by 3% . Finally, we see that the benefit of lossy compression is significant: even though we only reduced the precision from 10  X  7 to 10  X  4 , the compression ratio has already been improved by 50% . Table 3 lists the running time of the five components in S All experiments are performed on a computer with eight 3 3.4GHz Intel Xeon processors. For the Genomes dataset, which contains 2500 attributes X  X n extremely large number X  X e constructed the Bayesian Network manually. Note that none of the previous papers have been applied on a dataset with the magnitude of the Genomes dataset (both in number of tuples and number of attributes).
As we can see from Table 3, our compression algorithm scales reasonably: even with the largest dataset Genomes , the compres-sion can still be finished within hours. Recall that since our algo-rithm is designed for archival not online query processing, and our goal is therefore to minimize storage as much as possible , a few hours for large datasets is adequate.

The running time of the parameter tuning component can be greatly reduced if we use only a subset of tuples (as we did for structure learning). The only potential bottleneck is structure learn-ing, which scales badly with respect to the number of attributes ( O ( m 4 ) ). To handle datasets of this scale, another approach is to partition the dataset column-wise, and apply the compression algo-rithm on each partition separately. We plan to investigate this in future work.

We remark that, unlike gzip [21], S QUISH allows random access of tuples without decompressing the whole dataset. Therefore, if users only need to access a few tuples in the dataset, then they will only need to decode those tuples, which would require far less time than decoding the whole dataset.
We now investigate the sensitivity of the performance of our al-gorithm with respect to the Bayesian network learning. We use the Census dataset here since the correlation between attributes in this dataset is stronger than other datasets, so the quality of the Bayesian network can be directly reflected in the compression ratio.
Since our structure learning algorithm only uses a subset of the training data, one might question whether the selection of tuples in the structure learning component would affect the compression ra-tio. To test this, we run the algorithm for five times, and randomly choose the tuples participating in the structure learning. Table 4 shows the compression ratio of the five runs. As we can see, the variation between runs are insignificant, suggesting that our com-pression algorithm is robust.
 No. of Exp. 1 2 3 4 5 Comp. Ratio 0.0460 0.0472 0.0471 0.0468 0.0476
We also study the sensitivity of our algorithm with respect to the number of tuples used for structure learning. Table 5 shows the compression ratio when we use 1000 , 2000 and 5000 tuples in the structure learning algorithm respectively. As we can see, the
The implementation is single-threaded, so only one processor is used. compression ratio improves gradually as we use more tuples for structure learning.

Although compression of datasets is a classical research topic in the database research community [14], the idea of exploiting at-tribute correlations (a.k.a. semantic compression) is relatively new. Babu et al. [3] used functional dependencies among attributes to avoid storing them explicitly. Jagadish et al. [8] used a cluster-ing algorithm for tuples. Their compression scheme stores, for each cluster of tuples, a representative tuple and the differences be-tween the representative tuple and other tuples in the cluster. These two types of dependencies are special cases of the more general Bayesian network style dependencies used in this paper.

The idea of applying arithmetic coding on Bayesian networks was first proposed by Davies and Moore [6]. However, their work only supports categorical attributes (a simple case). Further, the authors did not justify their approach by either theoretically or ex-perimentally comparing their algorithm with other semantic com-pression algorithms. Lastly, they used conventional BIC [15] score for learning a Bayesian Network, which is suboptimal, and their technique does not apply to the lossy setting.

The compression algorithm developed by Raman and Swart [12] used Huffman Coding to compress attributes. Therefore, their work can only be applied to categorical attributes and can not fully utilize attribute correlation (the authors only mentioned that they can ex-ploit attribute correlations by encoding multiple attributes at once). The major contribution of Raman X  X  work [12] is that they formal-ized the old idea of compressing ordered sequences by storing the difference between adjacent elements, which has been used in search engines to compress inverted indexes [4] and also in column-oriented database systems [16].

Bayesian networks are well-known general purpose probabilis-tic models to characterize dependencies between random variables. For reference, the textbook written by Koller and Friedman [9] cov-ers many recent developments. Arithmetic coding was first intro-duced by Rissanen [13] and further developed by Witten et al. [19]. An introductory paper written by Langdon Jr. [10] covers most of the basic concepts of Arithmetic Coding, including the early bit emission trick. The deterministic approximation trick is original. Compared to the overflow prevention mechanism in Witten et al. X  X  work [19], the deterministic approximation trick is simpler and eas-ier to implement.
In this paper, we propose S QUISH , an extensible system for com-pressing relational datasets. S QUISH exploits both correlations be-tween attributes and skewness of numerical attributes, and thereby achieves better compression rates than prior work. We also de-velop S QU ID, an interface for supporting user-defined attributes in S
QUISH . Users can use S QU ID to define new data types by sim-ply implementing a handful of functions. We develop new encod-ing schemes for numerical attributes using S QU ID, and prove its optimality. We also discuss mechanisms for ensuring the correct-ness of Arithmetic Coding in finite precision systems. We prove the asymptotic optimality of S QUISH on any dataset that can be ef-ficiently described using a Bayesian Network. Experiment results on two real datasets indicate that S QUISH significantly outperforms prior work, achieving more than 50% reduction in storage. We thank the anonymous reviewers for their valuable feedback. We acknowledge support from grant IIS-1513407 awarded by the National Science Foundation, grant 1U54GM114838 awarded by NIGMS and 3U54EB020406-02S1 awarded by NIBIB through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initia-tive (www.bd2k.nih.gov), the Siebel Energy Institute, and the Fac-ulty Research Award provided by Google. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies and organizations.
