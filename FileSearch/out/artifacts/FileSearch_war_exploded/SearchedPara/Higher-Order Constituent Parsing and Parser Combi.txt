 Factorization is crucial to discriminative parsing. Previous discriminative parsing models usually fac-tor a parse tree into a set of parts. Each part is scored separately to ensure tractability. In dependency parsing (DP), the number of dependencies in a part is called the order of a DP model (Koo and Collins, 2010). Accordingly, existing graph-based DP mod-els can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al., 2005a; McDonald et al., 2005b), second-order (McDonald and Pereira, 2006; Carreras, 2007) and third-order (Koo and Collins, 2010) models.

Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. Then, the previous discriminative con-stituent parsing models (Johnson, 2001; Henderson, 2004; Taskar et al., 2004; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Finkel et al., 2008) are the first-order ones, because there is only one grammar rule in a part. The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent pars-ing, using some parts containing more than one grammar rule as non-local features.

In this paper, we present a higher-order con-stituent parsing model 1 based on these previous works. It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. Evaluated on the PTB WSJ and Chinese Treebank, it achieves its best F1 scores of 91.86% and 85.58%, respectively. Com-bined with other high-performance parsers under the framework of constituent recombination (Sagae and Lavie, 2006; Fossum and Knight, 2009), this model further enhances the F1 scores to 92.80% and 85.60%, the highest ones achieved so far on these two data sets. Discriminative parsing is aimed to learn a function f : S  X  X  from a set of sentences S to a set of valid parses T according to a given CFG, which maps an input sentence s  X  S to a set of candidate parses T ( s ) . The function takes the following discrimina-tive form: where g ( t,s ) is a scoring function to evaluate the event that t is the parse of s . Following Collins (2002), this scoring function is formulated in the lin-ear form where  X ( t,s ) is a vector of features and  X  the vector of their associated weights. To ensure tractability, this model is factorized as where g ( Q ( r ) ,s ) scores Q ( r ) , a part centered at grammar rule instance r in t , and  X ( Q ( r ) ,s ) is the vector of features for Q ( r ) . Each Q ( r ) makes its own contribution to g ( t,s ) . A part in a parse tree is illustrated in Figure 1. It consists of the center grammar rule instance NP  X  NP VP and a set of im-mediate neighbors, i.e., its parent PP  X  IN NP , its children NP  X  DT QP and VP  X  VBN PP , and its sibling IN  X  of . This set of neighboring rule in-stances forms a local structural context to provide useful information to determine the plausibility of the center rule instance. 2.1 Feature The feature vector  X ( Q ( r ) ,s ) consists of a series of features {  X  i ( Q ( r ) ,s )) | i  X  0 } . The first feature  X  ( Q ( r ) ,s ) is calculated with a PCFG-based gen-erative parsing model (Petrov and Klein, 2007), as defined in (4) below, where r is the grammar rule in-stance A  X  B C that covers the span from the b -th to the e -th word, splitting at the m -th word, x,y and z are latent variables in the PCFG-based model, and I (  X  ) and O (  X  ) are the inside and outside probabili-ties, respectively.

All other features  X  i ( Q ( r ) ,s ) are binary func-tions that indicate whether a configuration exists in Q ( r ) and s . These features are by their own na-ture in two categories, namely, lexical and structural. All features extracted from the part in Figure 1 are demonstrated in Table 1. Some back-off structural features are used for smoothing, which cannot be presented due to limited space. With only lexical features in a part, this parsing model backs off to a first-order one similar to those in the previous works. Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. 2.2 Decoding The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Follow-ing Huang (2008), this algorithm traverses a parse forest in a bottom-up manner. However, it deter-mines and keeps the best derivation for every gram-mar rule instance instead of for each node. Be-cause all structures above the current rule instance is not determined yet, the computation of its non-local structural features, e.g., parent and sibling fea-tures, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP  X  NP VP in Figure 1, the algorithm will extract child features from its children NP  X  DT QP and VP  X  VBN PP . The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i.e., PP  X  IN NP .

This algorithm is more complex than the approx-imate decoding algorithm of Huang (2008). How-ever, its efficiency heavily depends on the size of the parse forest it has to handle. Forest pruning (Char-niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for ef-ficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can re-fer to Chen (2012) for more technical details of this algorithm. Following Fossum and Knight (2009), our con-stituent weighting scheme for parser combination uses multiple outputs of independent parsers. Sup-pose each parser generates a k-best parse list for an input sentence, the weight of a candidate constituent c is defined as where i is the index of an individual parser,  X  i the weight indicating the confidence of a parser,  X  ( c,t i,k ) a binary function indicating whether c is contained in t i,k , the k -th parse output from the i -th parser, and f ( t i,k ) the score of the k -th parse as-signed by the i -th parser, as defined in Fossum and Knight (2009).

The weight of a recombined parse is defined as the sum of weights of all constituents in the parse. How-ever, this definition has a systematic bias towards se-lecting a parse with as many constituents as possible for the highest weight. A pruning threshold  X  , simi-lar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a re-combined parse. The parameters  X  i and  X  are tuned by the Powell X  X  method (Powell, 1964) on a develop-ment set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Tree-bank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Sec-tion 22 of WSJ treebank as development set.

In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-experts (Hinton, 2002). A factor  X  is introduced to balance the two models. It is tuned on a development set using the gold sec-tion search algorithm (Kiefer, 1953). The parame-ters  X  of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008).

The performance of our first-and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where  X  indicates a tuned balance factor. This parser is also combined with the parser of Charniak and Johnson (2005) 2 and the Stanford. parser 3 The best combination results in Table 3 are achieved with k =70 for English and k =100 for Chinese for selecting the k-best parses. Our results are compared with the best previous ones on the same test sets in Tables 4 and 5. All scores listed in these tables are calculated with evalb , 4 and EX is the complete match rate . This paper has presented a higher-order model for constituent parsing that factorizes a parse tree into larger parts than before, in hopes of increasing its power of discriminating the true parse from the oth-ers without losing tractability. A performance gain of 0 . 3% -0 . 4% demonstrates its advantage over its first-order version. Including a PCFG-based model as its basic feature, this model achieves a better performance than previous single and re-scoring parsers, and its combination with other parsers per-forms even better (by about 1% ). More importantly, it extends the existing works into a more general framework of constituent parsing to utilize more lexical and structural context and incorporate more strength of various parsing techniques. However, higher-order constituent parsing inevitably leads to a high computational complexity. We intend to deal with the efficiency problem of our model with some advanced parallel computing technologies in our fu-ture works.
