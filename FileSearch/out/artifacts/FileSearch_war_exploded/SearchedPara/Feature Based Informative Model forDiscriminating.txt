 The explosive growth of available choices from content providers has given great prominence to reco mmendation systems. In the past years, recommendation sys-tems have shown great potential to help users find interesting items from large item space[1,2]. Due to its great benefits to both users and content providers, rec-ommendation systems have been actively re searched since it was introduced[3,4].
For this year X  X  KDD Cup Challenge, Yahoo! Labs released a large music rating dataset. The contest consists of two tracks. The first track is a rating prediction problem that aims at minimizing RMSE (Root Mean Square Error). It is similar to the famous Netflix Prize Challenge 2 . The task of the second one is to discrim-inate the 3 songs rated highly by the user from the 3 ones never rated by her. In this task  X  X ate highly X  means a rating greater than or equal to 80. We tackle this problem as a top-n recommendation problem. That is, the three songs with higher prediction scores are regarded as the user X  X  favorite songs, while the other 3 songs are considered to be unrated.
In this paper, we use ranking oriented SVD to solve this problem. A nega-tive sampling technique is utilized to furt her improve prediction accuracy. Most importantly, we propose to use a feature based informative model to incorpo-rate different kinds of information into a single model. The ensemble of many algorithms is a useful approach to improve the overall performance. This has been proved by the winners of the Netflix P rize[5]. Different a lgorithms capture different information of the dataset, so they blend well. All the publicized results on KDD Cup Track2 also adopt ensemble techniques to boost their final pre-dictions. However, ensemble usually n eeds extra computatio ncost.Thereisno doubt that a single model with comparable performance to ensemble models is more acceptable. Here we propose such a model. Different kinds of information, such as taxonomy of items (more on this in Section 2.2), item neighborhoods, user specific features and implicit feedback, are integrated into a single model. With this model, we achieve an error rate of 3.10% on the test set. This is the best result of single predictors among all publicized results on this task, even better than the performance of many ensemble models.

The reminder of this paper is organi zed as follows. In Section 2 we present the preliminaries, stating our task and giving some key properties of the dataset. Symbols that will be used later are also defined in this section. Ranking oriented SVD and the negative sampling strategies are detailed in Section 3. Section 4 focuses on the feature based informative model. In Section 5 we give our experimental results. Related works are summarized in Section 6. Finally in Section 7 we conclude our work. 2.1 Problem Statement For each user in the test set, six songs are given. Three of them are selected randomly from the songs rated highly by the user, and the other three are selected with probability proportional to the number of high ratings the song receives. The motivation for this setti ng is, for popular songs, user may have already heard about them. If he/she still doesn X  X  rate these songs, it is likely that he/she doesn X  X  like them. Participants need to separate the three positive examples from the three negative ones. The evaluation metric is error rate. That is, the ratio of wrongly classified test c ases with respect to the total number of test cases. 2.2 Key Properties of the Yahoo! Music Dataset For the task of binary user preference pred iction, Yahoo! Labs released a dataset consisting of 67 million ratings from 24 thousand users on 30 thousand items. An important property of this dataset is the taxonomy information. That is, items have four categories: artist, album, song and genre. As we will show in Section 5, the taxonomy information can decrease error rate significantly. 2.3 Notation Definition We consider the whole rating dataset R =[ r u,i ] to be a sparse matrix. The letter u and i are used to denote user and item respectively. r u,i is user u s rating on item i , 0  X  r u,i  X  100 . Bold letters are used for matrices and vectors, non bold U is used to represent the whole user set and the letter I for the whole item is neighborhoods of item i computed by Pearson Co rrelation Coefficient. In this section we will elaborate the ranking oriented SVD model with negative sampling for the binary user reference prediction problem. In Section 3.1 we briefly present classical SVD models that try to minimize RMSE. Section 3.2 focuses on the ranking oriented SVD models and the strategies to pair rating records. Finally in Section 3.3 we introduce the negative sampling approach used in this paper. 3.1 Classical SVD Models Classical SVD [6] models mainly focus on approximating the rating of user u on item i by Here  X  is the global average of the rating dataset R , b u and b i are user and item bias respectively. The two vectors p u , q i  X  R f are f dimensional vectors used to capture the latent attributes of users and items. The parameters are learnt by minimizing RMSE.
 We denote SVD models that try to minimize loss function (2) by ReSVD (Re-gression SVD) .These models gained great success in the Netflix Prize. However, as shown by [7], they usually don X  X  work as well in the choice prediction problem. 3.2 Ranking Oriented SVD A natural solution to the choice prediction problem is learning to rank. Eigen-Rank [8] and pLPA (probabilistic Latent Preference Analysis) [9] are two such models. Ranking oriented SVD models are first proposed by Nathan N. Liu et al.[10]. The key technique in these models is to turn either implicit or explicit feedback into user dependent pairwise preference regarding items. The pairwise preference is denoted by the variable  X  uij , which takes value of +1 if user u prefers item i to item j ,and  X  1 if the opposite holds. For our task of binary user preference prediction, we d erive the pairwise prefer ence by two ways. The first is based on the gap between two ratings, and the second uses two boundaries. These two ways correspond to the definition of  X  uij in Equation (3) and (4). We denote them as GAP _ PAIR and BOUND _ PAIR respectively. Here t, t ub and t lb are pre-defined thresholds. In Section 5 we give our experi-input for our ranking oriented SVD models.

We follow the work of [10] to use Bradley-Terry model [11] to design the loss function for the preference prediction problem. Under this model, each user u is user. The higher  X  u,i is compared to  X  u,j , the more likely that  X  uij =+1 .This relation can be described using a sigmoid function: To adopt the this model to SVD, we take  X  u,i = X  r u,i . This parametrization leads to the following maximum likelihood training procedure: We denote SVD models that optimize the loss function defined in (6) as RaSVD (Ranking oriented SVD). 3.3 Negative Sampling for SVD Models Negative sampling for CF is proposed by Rong Pan et al. [12] to solve the OCCF problem. In OCCF, only positive examples are available. Such situations include users X  visiting history to news page recommendation, users X  clicking and trading history on online shopping site.

The negative sampling strategy we employ is user-oriented pairwise sampling which proves to work well in [12]. For every ( u, i ) pair in R witharatinghigher than a pre-define threshold  X  , we select k items not rated by u as the negative examples for this user. The negative exa mples are selected using the same ap-proach as the negative examples in the test data. That is, the probability of an item being selected as negative example is proportional to the number of high ratings it receives in the rating set R . The score of negative example is 0 . Such positive/negative item pairs are used as the training data for our ranking ori-ented SVD models. We will show the impact of k and  X  on prediction accuracy in Section 5. In this section we elaborate the informative model we use in this paper. Sec-tion 4.1 presents a feature based collaborative filtering framework (abbreviate to FCFF )[13], which serves as the basis of our informative model. Following sections will focus on how to incorporate different kinds of information into the framework. 4.1 Feature Based Collaborative Filtering Framework Our informative model is based on the feature based collaborative filtering frame-work proposed by Chen at al. [13]. The input format of FCFF is similar to LibSVM[14], users just need to define features to implement new model. The model equation of FCFF is: y (  X  ,  X  ,  X  )=  X  + We can see from Equation (7) that in FCFF , the features can be divided into three groups:  X  ,  X  ,  X  . They represent global, user and item dependent features need to be learnt by minimizing the loss function (2) or (6). Another important property of FCFF is that global features are not involved in factorization. They are linear features. Incorporating the se features into SVD models can help us capture information from different sources. We will show this in the following sections.

For the basic SVD model defined in Equation (1), the features for RaSVD are defined in Equation (8): Here index is a function used to map feature to a unique ID. In Equation (8) posId is the ID of the positive example and negId is the ID of the negative example. For ReSVD , features can be defined in a similar way. We omit the details due to space limitation. 4.2 Taxonomy-Aware SVD Models As we mentioned in Section 2.2, an important property of the Yahoo! Music dataset is the taxonomy information. In particular, each song belongs to one album and one artist. Each album belongs to one artist. Moreover, every song or album has zero or multi genres. Intuitively, items that are closely correlated in the taxonomy may receive si milar ratings from the sam e user. For example, given that a user rates highly on an artist(album), we can conclude that it X  X  likely that the user will also rate highly on the songs belonging to the artist(album). In this section we integrate this information into FCFF .

To capture the taxonomy relationship, for each item i in I ,weintroducea q i are used to predict the score of the item itself, while the new bias term b i and latent vector q i are used to help the prediction of the children of item i .If i is artist, its children include the albums and songs of artist i .If i is album, its children are all the songs belonging to album i . This leads to the following formulation of the estimation of r u,i : Here Al ( i ) is the album of item i ,and Ar ( i ) is the artist of item i .Forrank models, the features of this taxonomy-aware SVD can be defined as follows:  X  is defined the same way as Equation (8). 4.3 Integrating Implicit Feedback As pointed out by early works [15,3], implicit feedback has great potential to improve the performance of recommendation systems. Compared to explicit feed-back where explicit ratings are required, implicit feedback emphasizes more on which items the user rates. After integrating implicit feedback, prediction score for unknown ( u, i ) pair is: In this model, each item i is associated with a new latent vector q i ,whichis used to uncover information embedded in users X  implicit feedback. 4.4 Integrating Neighborhood Information In this section we will show how to integrate neighborhood information into the feature based informative model. Combining neighborhood and SVD model into a single predictor is first proposed by Y. Koren et al. [3] with the following model:  X  r u,i =  X  + b i + b u + &lt; p u , q i &gt; + 1  X  Here  X  b u,j is a baseline estimator composed only by user bias and item bias. w N ( u, i ; k )= I ( u ) N ( i ; k ) .

For our binary user preference prediction problem, we find that implicit feed-back is much more useful than explicit feedback. Integrating implicit neighbor-hood information into Equation (11) we get: To implement the model described in (13) under FCFF , we just need to add new global features. ( posId, g ) / ( negId, g ) . 4.5 Integrating Taxonomy Based Classifier As we mentioned in Section 4.2, users X  preferences on artist and album have great impact on their attitudes towards the corresponding songs. In this section, we integrate a taxonomy based classifier into our informative model. The classifier uses four user dependent features:  X  User X  X  rating on the artist of the song. If not rated, this value is 0 ;  X  Whether the rating on the artist of the song is higher than 80;  X  User X  X  rating on the album of the song. If not rated, this value is 0 ;  X  Whether the rating on the album of the song is higher than 80; If we denote these four features by b u,j (0  X  j  X  3) , the new model is: To incorporate the taxonomy based classifier into FCFF , we just need to define new global features for b u,j in a similar way as Equation (14). In this section we give our experimental results on the test set provided by Yahoo! Labs. In the test set, there are 101172 users. For each user, 6 songs are given. The total number of test cases is 607032. For each test case, we need to label the song as either positive or negative example for the user. The evaluation metric is the ratio of wrongly labeled test cases with respect to the total number of test cases.

To encourage further research on the pr oblem, we open source all the code and implementations of our experiments. The implementation of FCFF is available at our laboratory page 3 . The code used to generate features for all the models in Section 4 is also released 4 . 5.1 Performance of Informative Model on Error Rate In this section we show the effectiveness o f informative model. Before giving the experimental results, we first introduce some abbreviations that will be used in later sections in Table 1.

The  X + X  symbol in the table means  X  X ecursive combining X . For example, the  X + X  in the last line of Table 1(b) means combining taxonomy-aware SVD, im-plicit feedback, item neighborhood and taxonomy based classifier into a single model. This is also our best predict ion model. We denote this model as In-foSVD (Informative SVD).

Table 2 shows the error rate of differen t models with triple negative sam-pling. The results show that incorporating extra information into FCFF ,such as taxonomy of items, item neighborhoods, user specific features and implicit feedback, can always help lower error rate. Taking RaSVD+BOUND_PAIR as an example, incorporating item taxonomy into BSV D decreases error rate by 10.8%. After integrating implicit feedback, a decrease of 7.5% in error rate is achieved. Item neighborhood and taxonom y based classifier bring in even larger improvement. By incorporating these two sources of information, error rate de-creases by 25.1% and 20.3% respectively. In Table 3 we give the performance of the same models as Table 2 with no negative sampling. The results confirm our analysis above. To sum up, compared to BSVD , InfoSVD can at least decrease error rate by 42.3%. 5.2 Best Models In this section we give the results of the best models for both regression SVD and ranking oriented SVD. The parameter settings are also presented. After this we give a comparison of our results and all the publicized results on this task.
For both regression SVD and ranking oriented SVD, the best prediction mod-els are InfoSVD defined in Equation 15. This again confirms the effectiveness of FBCF in our user preference prediction problem. By integrating different sources of information into a single model, we can indeed lower error rate significantly.
Table 4 shows the best results of ReSVD and RaSVD .  X  is sampling threshold and k is the number of negative examples generated for each triple ( u, i, r u,i ) with r u,i  X   X  . An error rate of 3.10% is achieved by InfoSVD with ranking oriented loss function defined in Equation 6.

Table 5 gives a comparison of the results of InfoSVD and the top 10 teams on the leaderboard. We can see from the table that InfoSVD achieves the lowest error rate among all the single predictors, outperforming the second one of 3.49% by 11.2%. Additionally, the performance of InfoSVD is even better than some ensemble models. 5.3 Impact of Negative Sampling and Ranking Oriented SVD Comparing Table 2 and Table 3 we find that negative sampling can improve recommendation accuracy significantly. With triple negative sampling, the error rate of the best model decreases from 9 .63% to 3.33%. Another interesting ob-servation from Table 2 and Table 3 is the performance comparison of ReSVD and RaSVD . As we can see from these two tables, without negative sampling, ReSVD performs better in most cases. However, after bringing in extra negative examples, the performance of RaSVD grows faster and outperforms ReSVD .
Table 6 shows the effect of parameter k in negative sampling. We can see from Table 6 that increasing k can always bring in improvement in prediction accuracy, but the gain becomes less as k increases. Since the impact of  X  on error rate is not so obvious when  X  varies from 20 to 80, we omit the experiment results on different values of  X  . CF algorithms fall into two categories: neighborhood model [16,3] and matrix fac-torization model [3,17]. For choice prediction problems, learning to rank [8,9,10] and negative sampling [12] have been proved to work well. EigenRank[8] and pLPA[9] are two typical learning to rank models. These models address the ranking problem directly without a rating prediction step. Other learning to rank models[10,18,19] assign scores to items as traditional approaches, but the scores are just used to rank items, instead of approximating the real ratings. Rendel et al.[18] propose to use Bayesian probabilistic ranking model for top-n recommendation. Shi et al.[19] propose a list-wise learning to rank model with matrix factorization.

As we can see from experiments, our pro posed feature-based informative model lowers error rate significantly compared to basic SVD. LibFM[20] also uses idea of feature-base d factorization model. Compared to their model, our model distinguishes features types, which allows us to incorporate useful infor-mation such as neighborhood and taxonomy more naturally. In this paper, we mainly study the feature based CF framework for discriminat-ing uses X  favorite songs from ones unrated by her. Under this framework, new models can be implemented easily by defining features in the input data. With this framework, we achieve an error rate of 3.10% with a single predictor, which is the best performance of all single predi ctors on this task. The effectiveness of ranking oriented models and negative sampling are also presented.

For future work, we plan to investigate the performance of our informative model in a more general and practical scenario: top-n recommendation. In this case, metrics like recall and precision in information retrieval can be used.
