 g Assessing seman tic similarit y between text documen ts is a crucial asp ect in Information Retriev al systems. In this work, we prop ose to use hyperlink information to deriv e a similarit y measure that can then be applied to compare any text documen ts, with or without hyperlinks. As link ed documen ts are generally seman tically closer than unlink ed documen ts, we use a training corpus with hyperlinks to in-fer a function a; b ! sim ( a; b ) that assigns a higher value to link ed documen ts than to unlink ed ones. Tw o sets of exp erimen ts on di eren t corp ora sho w that this function compares favorably with OKAPI matc hing on documen t re-triev al tasks.
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retriev al]: Miscellaneous I.2.6 [Arti cial Intelligence]: Learning General Terms: Algorithms, Exp erimen tation Keyw ords: hyperlinks, similarit y measure, matc hing mea-sure, term weigh ting, gradien t descen t, neural net works
Automatic techniques to access and organize documen t collections are essen tial to fully bene t from large text cor-pora. Sev eral of these metho ds require a measure to quan tify seman tic similarities between text items: e.g. clustering re-lies on documen t comparisons, while Information Retriev al (IR) dep ends on documen t/query similarities.

In this work, our goal is to infer a measure of similarit y relying on the seman tic relationships con tained in a hyper-link ed corpus. In suc h a corpus, links can be considered as indicators of topic relatedness, i.e. link ed documen ts tend to be seman tically closer than unlink ed documen ts [1]. Therefore, we prop ose to iden tify a measure of similarit y a; b ! sim ( a; b ) suc h that, for any documen t d , the doc-umen ts whic h are link ed to it are considered more similar than those whic h are not: where L ( d ) is the set of documen ts link ed with d . For that purp ose, a gradien t descen t strategy [2] is adopted: we rst introduce a parameterized measure of similarit y a; b ! sim ( a; b ) and a cost C whic h indicates how far sim is from the condition (1), then gradien t descen t optimization is used to select the parameters whic h minimize C for a given training corpus D train .

The inferred measure sim can then be applied to any pair of text documen ts, with or without hyperlinks, in any con text where a text similarit y measure is needed. In order to evaluate this approac h, we compared the inferred measure with the state-of-the-art OKAPI matc hing measure [3] over two retriev al tasks (see Section 3). In this con text, our mo del LinkL earn is sho wn to impro ve both precision at top 10 and average precision with resp ect to OKAPI .

In the remainder of this pap er, Section 2 describ es the prop osed metho d, Section 3 presen ts the exp erimen ts and results, and Section 4 dra ws some conclusions. This section describ es the two main parts of the Link-Learn Mo del: the parameterized measure of similarit y sim is rst de ned and the cost C related to condition (1) is then introduced.
Our mo del relies on the Vector Space Mo del (VSM): eac h documen t d is represen ted with a vector ( d 1 ; : : : ; d ing the vocabulary size, and the documen ts are then com-pared according to the inner pro duct of their vectors, The weigh t d i of a term i in a documen t d is a function of tf d;i (the num ber of occurrences of i in d ), idf i (the inverse documen t frequency of i ) and ndl d (the length of documen t d divided by the average documen t length): This choice is motiv ated by the fact that functions of those three variables have led to the best performances in TREC IR benc hmarks 1 , OKAPI BM25 being the most used of those functions: where K and B are hyperparameters. In our case, the func-tion g is chosen to be the pro duct of three Multi-La yer Per-ceptron (MLP) functions: 1 NIST Text Retriev al Conference, trec.nist.go v This parameterization mak es the simplifying assumption that tf d;i , idf i and ndl d variables are indep enden t. Suc h a hypothesis impro ves greatly the mo del eciency (see [4] for further explanations) while still allo wing for good per-formance (see Section 3).
As men tioned above, it is desirable that, for any documen t d , the documen ts whic h are link ed to it (i.e. the documen ts of L ( d )) are considered more similar to d than any other documen ts (1). A simple cost would hence be the prop ortion of documen t triplet d 2 D train , l + 2 L ( d ), l = 2 L ( d ) for whic h the above prop erty is not satis ed: where I fg is the indicator function, i.e. I f c g = 1 if c is true and zero otherwise and L ( d ) is the set of documen ts link ed with d (i.e. the documen ts referring to d and the documen ts referred to by d ).

Similarly to the 0 = 1 loss (i.e. error rate) in the case of classi cation, C 0 = 1 cannot be directly minimized through gradien t descen t [2]. Hence, we prop ose to minimize an up-per bound of this quan tity: where and x ! k x k + is 0 for x &lt; 0 and x otherwise. C is actually an upp er bound of C 0 = 1 since 8 x 2 R ; I f x &lt; 0 g k 1 x k This function C is deriv able almost everywhere and we can hence select the parameters of the MLPs (2) whic h minimize C through gradien t descen t [2] (see [4] for further details).
The follo wing section describ es the two sets of retriev al ex-perimen ts performed in order to assess the prop osed metho d. In both cases, LinkL earn is compared with OKAPI .
The exp erimen ts presen ted in the follo wing are performed over the Wikip edia corpus [5]. This dataset consists of 450 ; 000 encyclop edia articles, eac h article referring to other related articles using hyperlinks.

The corpus has been randomly split into 3 subsets of 150 ; 625 documen ts: train , valid and test . The train set is used for gradien t descen t (i.e. C is minimized over this set) and valid is used to select the hyperparameters for both LinkL earn (the num ber of hidden units in the MLPs, the num ber of training iterations and the learning rate of the gradien t descen t) and OKAPI ( K and B ).

The test set is used only for evaluation, in whic h we per-form a related documen t searc h: eac h documen t is consid-ered to be a query whose relev ant documen ts are the doc-umen ts link ed with d . Table 1 sho ws that, according to all performance measures, LinkL earn outp erforms OKAPI . To have a more complete evaluation, we also compared LinkL earn and OKAPI matc hing measure on TREC queries for the TDT-2 corpus [6]. Without re-training or adapta-tion, the measure inferred from the hyperlink ed Wikip edia data has been applied as a query/do cumen t matc hing mea-sure to the non-h yperlink ed TDT-2 corpus.

The results obtained over TDT-2 con rm those obtained over Wikip edia (see Table 2): the use of LinkL earn leads to an impro vemen t with resp ect to OKAPI matc hing according to the di eren t performance measures used.
In this pap er, we introduced LinkL earn , a gradien t descen t approac h to deriv e a documen t similarit y measure from a hyperlink ed training corpus: the measure is selected suc h that, in most cases, a documen t is considered more simi-lar to the documen ts with whic h it is link ed than to the other documen ts. This approac h has sho wn to be e ectiv e in an IR con text: the use of the similarit y measure inferred by LinkL earn has led to higher retriev al performances when compared to the state-of-the-art OKAPI matc hing measure. Acknowledgmen ts: This work has been performed with the supp ort of the Swiss NSF through the NCCR{IM2 pro ject. It was also supp orted by the PASCAL Europ eean Net work of Excellence, funded by the Swiss OFES. [1] B. D. Davison, \Topical localit y in the web," in ACM [2] C. Bishop, Neur al Networks for Pattern Recognition , [3] S. E. Rob ertson et al, \Ok api at TREC-3," in NIST [4] David Grangier et al, \Inferring documen t similarit y [5] \Wikip edia, the free encyclop edia," www.wikip edia.org. [6] C. Cieri et al, \The TDT-2 text and speech corpus," in
