 In this paper we address the issue of learning to rank for doc-ument retrieval using Thurstonian models based on sparse Gaussian processes. Thurstonian models represent each doc-ument for a given query as a probability distribution in a score space; these distributions over scores naturally give rise to distributions over document rankings. However, in general we do not have observed rankings with which to train the model; instead, each document in the training set is judged to have a particular relevance level: for example  X  X ad X ,  X  X air X ,  X  X ood X , or  X  X xcellent X . The performance of the model is then evaluated using information retrieval (IR) metrics such as Normalised Discounted Cumulative Gain (NDCG). Recently Taylor et al. presented a method called SoftRank which allows the direct gradient optimisation of a smoothed version of NDCG using a Thurstonian model. In this approach, document scores are represented by the outputs of a neural network, and score distributions are cre-ated artificially by adding random noise to the scores. The SoftRank mechanism is a general one; it can be applied to different IR metrics, and make use of different underlying models. In this paper we extend the SoftRank framework to make use of the score uncertainties which are naturally pro-vided by a Gaussian process (GP), which is a probabilistic non-linear regression model. We further develop the model by using sparse Gaussian process techniques, which give im-proved performance and efficiency, and show competitive re-sults against baseline methods when tested on the publicly available LETOR OHSUMED data set. We also explore how the available uncertainty information can be used in prediction and how it affects model performance.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models, search process Algorithms, Experimentation Information retrieval, ranking, Gaussian process, learning
Our task in information retrieval is to choose and present, possibly in an ordered list, a set of documents relevant to the query entered by a user. In order to design and improve retrieval systems we need to evaluate the quality of the re-trieved results. In practice this is usually done using a set of judged documents for multiple queries. The documents are judged for relevance to the query either using binary labels or a graded scale. An IR metric or utility is a function of the judged labels for a returned set of documents. Many such metrics have been developed (see e.g. [14]) to try to capture various aspects of user preference for the retrieved set. For example, in a ranked list a user pays most attention to the head of the list, and we therefore want to make sure we get the very relevant documents right at the top.
Normalized Discounted Cumulative Gain (NDCG) [6] is an IR metric that is a function of graded relevance labels, for example 0 ( X  X ad X ) to 3 ( X  X xcellent X ), which focuses on the top of a ranking using a discount function. It is defined, assuming a top rank of 0, as: The gain g ( r ) of the document at rank r may for example be linear in the rank g ( r ) = l ( r ), or an exponential func-the document at rank r . R is the truncation rank. Where no subscript is defined, it should be assumed that R = N , the number of documents associated with the query under consideration. A popular choice for the rank discount is D ( r ) = 1 / log(2 + r ) and G R, max is the maximum value of P r =0 g ( r ) D ( r ) obtained when the documents are optimally ordered by decreasing label value. The intuition behind the NDCG metric is as follows. The discount function ensures we focus on getting documents in the right order at the top of the ranking. The gain function ensures we place more emphasis on the highly relevant documents. The normaliza-tion allows us to meaningfully average over many different queries, for which there may be some queries with many more relevant documents than others.
To produce a ranked list of documents for a given query, many methods use a score function to map from document (and query) features x to a real-valued score s . For a docu-ment indexed by j : where w is a vector of parameters defining the score func-tion. To produce a ranking the documents are ordered ac-cording to score. An IR metric such as NDCG is then used to evaluate the ranked list.

The learning task is then to find suitable parameters w of the score function, from a training set of judged query-document pairs. One way of doing this is to ignore the IR metric completely for the learning, and to concentrate on the labels themselves. For example, we can perform regression directly on the (graded) labels, or perhaps more properly, ordinal regression [4]. An alternative way of training is to first turn the labels into pairwise preferences [5, 7, 2]. How-ever, given we are to be evaluated on a particular IR metric, it may be advantageous to use the same metric to guide the training of the parameters of the ranking function. The suc-cessful LambdaRank is an example of a system which implic-itly does this [1]. The most direct way to achieve this is to optimize the IR metric on the training data with respect to the parameters of the ranking function. Efficient optimiza-tion in a high dimensional parameter space can only really be achieved using gradient-based methods. Here we hit a problem: typical IR metrics such as NDCG are non-smooth functions of the document scores s j (and hence parameters w ) because they only depend on the ordering of the scores rather than the score values themselves [15]. Such metrics are therefore not amenable to gradient based optimization.
Recently Taylor et al. [17] have developed a general method called SoftRank to smooth IR metrics, giving differentiable objective functions suitable for gradient optimization. The high level idea is to replace the deterministic score of (2) with an uncertain or probabilistic one: where N denotes a Gaussian distribution and  X  2 s is a shared smoothing variance. The softened objective function is then created by computing the expectation of the original IR met-ric with respect to these score distributions. An exact an-alytical form for this expectation is intractable in general, but a close approximation can be made by first converting from score to rank distributions. We give here a high level exposition of this procedure; for more details refer to [17].
A set of N score distributions (3) defines a distribution on the possible N ! orderings of the documents. For example, to sample an ordering from the distribution we would first sample N scores from the score distributions, and then sort the scores to obtain the ordering. This is an example of a Thurstonian model for orderings [18]. The first point to note is that any procedure we have for computing expectations involving such a sort will not be differentiable, so we have to avoid this. The second point is that in many cases (e.g. with NDCG) we do not actually need the full ordering distribu-tion to compute the expectation, and instead we can deal with individual document rank distributions p j ( r ). This is the probability that document j will be at rank r under the N score distributions, and is a marginal of the full distribu-tion on orderings. For illustration refer to figures 1 and 2 of [17], which show example Gaussian score distributions and their corresponding soft rank distributions.

For the purposes of this paper it is sufficient to know that we can compute close approximations to the rank distribu-tions p j ( r ) using a recursive procedure that does not involve an explicit sort, and hence we can compute derivatives with respect to parameters. The computation involves as an in-termediate step the calculation of the pairwise probabilities  X  ij that document i ranks higher than document j : As an example of smoothing an IR metric we focus on NDCG. We can rewrite the formula for NDCG (1) as a sum over document indices rather than document ranks: Now the SoftNDCG is defined as the expected NDCG with respect to the rank distributions: The additive separation of NDCG over documents in equa-tion (5) makes it clear why we actually only need the marginal rank distributions p j ( r ) rather than the full distribution over all rankings.

Finally, to optimize G soft we need to compute derivatives with respect to the parameters w of the ranking function:  X  w . This is essentially just application of the chain rule, noting that G soft is a function of the rank distributions p which are in turn functions of the pairwise probabilities  X  which are in turn functions of the score means  X  s j , which are finally functions of the ranking parameters w :  X  s f ( x j , w ). Taylor et al. [17] use a 2-layer neural net for the ranking function f . Taylor et al. [17] demonstrated good results using Soft-Rank, showing that optimizing SoftNDCG on the training set yields test set results comparable to the state-of-the-art LambdaRank [1], but having several additional benefits, in-cluding the ability to explicitly train any metric which can be built from score probabilities. However, conceptually at least, some problems still remain. What is the meaning of the smoothing variance parameter  X  2 s , and how should it be set? In [17] it is fixed during training, although an extra degree of freedom (the scale of f ) means that it is effectively being optimized. From a Bayesian perspective though it makes sense to regard the variance  X  2 s as the uncertainty associated to the function due to uncertainty in the model parameters w . In this sense there should be an individual posterior uncertainty for each document  X  2 j , reflecting the fact that we will be more confident about some documents than others. In practice it is hard to achieve this tractably using a neural net. 1 Gaussian process models, however, are
One would need to pass parameter distributions through non-linear models which do give analytically tractable un-certainty estimates. We turn to this class of models in the next section.
We briefly summarize GPs for regression, but see e.g. [13] for more detail. A Gaussian process defines a prior distribu-tion over functions f ( x ), such that any finite subset of func-tion values f = { f n } N n =1 is multivariate Gaussian distributed given the corresponding feature vectors X = { x n } N n =1 The covariance matrix K ( X , X ) is constructed by evaluating a covariance or kernel function between all pairs of feature vectors: K ( X , X ) ij = K ( x i , x j ).

The covariance function K ( x , x 0 ) expresses some general properties of the functions f ( x ) such as their smoothness, scale etc. It is usually a function of a number of hyper-parameters  X  which control aspects of these properties. A standard choice is the ARD + Linear kernel 2 : K ( x i , x j ) = c exp  X  for smoothly varying functions with linear trends. There is an individual lengthscale hyperparameter  X  d for each input dimension, allowing each feature to have a differing effect on the regression.

In standard GP regression the actual observations y = { y n } N n =1 are assumed to lie with Gaussian noise around the underlying function: p ( y n | f n ) = N ( y n | f n ,  X  2 out the latent function values we obtain the marginal likeli-hood: which is typically used to train the GP by finding a (local) maximum with respect to the hyperparameters  X  and noise variance  X  2 .

Prediction is made by considering a new input point x and conditioning on the observed data and hyperparameters. The distribution of the output value at the new point is then: where K ( x , X ) is the kernel evaluated between the new in-put point and the N training inputs. The GP is a non-parametric model, because the training data are explicitly required at test time in order to construct the predictive distribution. the non-linearities of the neural net, and also define a proper likelihood, for a full Bayesian treatment.  X  X utomatic relevance determination X  (ARD) [10]
Figure 1a shows the GP predictive mean and two standard deviation lines (10) plotted for a simple 1D noisy sine wave data set. The hyperparameters of the kernel have been set by optimizing the marginal likelihood as described above. The important point to notice is that the predictive vari-ance captures the inherent uncertainty in the function, with tight error bars in regions of observed data, and with grow-ing error bars away from observed data. It is this intuitive property we hope to introduce to the SoftRank model using a GP.

One limitation of GPs is that they are prohibitive for large data sets because training requires O ( N 3 ) time due to the in-version of the covariance matrix. Once the inversion is done, prediction is O ( N D ) for the predictive mean and O ( N the predictive variance per new test case. We return to this problem in the next section.
There are several ways in which Gaussian processes could be applied to ranking. The simplest is to ignore the NDCG objective for training, and to simply use a GP regression model on the relevance labels themselves, treating them as real values. Indeed, we tried this first, but found it did not work especially well. The GP seemed to model the numer-ous  X  X ad X  labels well, at the expense of the  X  X ood X  labels. The direct optimization of NDCG in training with SoftRank was designed specially to get around this type of problem  X  the NDCG objective forces the model to concentrate on get-ting the relevant documents to the top of the ranking. We therefore wanted to combine a GP model with the SoftRank training scheme.
Notice that the GP predictive mean and variance func-tions (10) are of exactly the right form to be used as the score means and uncertainties in SoftRank (3): p ( s j ) = hood (subject of future work), this combined GP/SoftRank model cannot be treated in a fully Bayesian way. Rather we regard the GP mean and variance functions (10) as param-eterized functions to be optimized in the same way as the neural net in SoftRank. This treatment of the GP is similar to using a radial basis function (RBF) network in place of the neural net, except that the variance function exhibits the characteristic GP property of growing error bars away from observations.

Looking at equation (10) we see that the regression out-puts y can be made into virtual or prototype observations  X  they are free parameters to be optimized. This is because all training information enters through the NDCG objective, rather than directly as regression labels. In fact we can go one step further and realise that the corresponding set of input vectors X on which the GP predictions are based do not have to correspond to the actual training inputs, but can be a much smaller set of free inputs. To summarize, the
Different weighting of data might yield better results; an-other approach would be to use a GP with an ordinal re-gression likelihood for the labels [3]. (b) GP based on 10 prototype points (marked as crosses). Prototypes and hyperparameters trained using regression objective.
 mean and variance for the score of document j are given by:  X  2 ( x j ) =  X  2 + K ( x j , x j ) where ( X u , y u ) is a small set of M prototype feature-vector / score pairs. These prototype points are free parameters that are optimized along with the hyperparameters  X  using the SoftNDCG gradient training. The advantage of using a small set of M prototypes is that we now have a sparse model, and the previously problematic O ( N 3 ) training time reduces to O ( N M 2 + N M D ). We hope that if these proto-types are positioned well then they can mimic the effect of using all the training data.

To give an illustration of how this works we can actually plug in equation (11) into a point-wise regression objective rather than SoftNDCG, and optimize prototype points and hyperparameters for the sine wave data of Figure 4. The result is shown in Figure 1b, where the prototype points are plotted as crosses, and again the predictive mean and two standard deviation functions are shown. We see that a qual-itatively similar effect is achieved, using only 10 prototypes, when compared to using a full GP trained using marginal likelihood. The main difference seems to be that a greater linear trend is picked up using this type of training, but the desired growing error bars away from observed data remain.
To implement the SoftNDCG optimization we just need to remember that the  X  ij (4) are now a function of both the score means and variances:  X  ij  X  Pr( S i  X  S j &gt; 0) = We also need derivatives of  X  s j and  X  2 j in (11) with respect to prototypes ( X u , y u ) and hyperparameters. For this we first need to compute the derivatives with respect to the kernel matrices of (11), and then we need the derivatives of the kernel function (8) with respect to the prototypes and
Pointwise regression objective: P j log  X  2 j + ( y j  X   X  s hyperparameters. Splitting up the calculation in this way allows different kernels to be explored with relative ease. Details are shown in appendix A.
The technique of optimizing a small set of prototype points that we showed in section 5.1 is in the same spirit as previ-ously developed more sophisticated sparse GP methods such as the SPGP/FITC model [16, 12]. We wanted to see if there is something to be gained from using the FITC sparse GP model in conjunction with SoftRank. There is not space here to discuss the details of the FITC approximation, but the important point is that it provides a different parameterized form for the mean and variance functions  X  a more complex version of equation (11). The detailed equations are given in the appendix, (29) and (30). The FITC approximation simi-larly depends on a small set of M prototype inputs X u (also known as inducing or pseudo inputs), but whereas in (11) only M outputs y u are used, in FITC all training outputs are taken into account. In standard SPGP/FITC regression training [16] the true training labels are used as the out-puts, and the marginal likelihood is used to optimize the prototype inputs. However when combined with SoftRank to optimize the NDCG metric, we again have the option to make the outputs y virtual observations and optimize them too. The true labels are then just used in computing the NDCG objective and its derivatives. The training cost is of the same order as GP-Rank, O ( N M 2 + N M D ), but with the better FITC approximation we might improve performance and need fewer prototypes.

Training the FITC-Rank model proceeds exactly as de-scribed in section 5.1, except that the derivatives are more complicated. Again details are given in appendix B.
The description of the training of the models in the pre-vious sections, and the complexities of the derivatives, may give the impression that these models are very expensive and hence practically unsuitable. However, whilst training may be a little slow, it can be done off-line. The important con-sideration is query-time prediction. This involves computing the score mean  X  s ( x ) for a new document with features x . In both GP-Rank and FITC-Rank, this is simply a weighted sum of a small number ( M , the number of prototypes) of kernel evaluations: where the weights  X  u can be computed off-line after train-ing. It is easy to see where this form arises by looking for example at equation (11). Therefore computing the score of a document at query-time is only O ( M D ) where M is the number of prototypes and D is the number of input features.
One of the potential advantages of using a Gaussian pro-cess model for the scores is that the individual document uncertainties might give us further information to use to de-cide a ranking at test time. Clearly the baseline procedure is to simply order according to the predicted score means  X  s . This also corresponds to the most probable ordering un-der the model described in section 3.1. However, we may decide that we want to be conservative in our predictions, and prefer documents that we are more certain about even if their mean scores are slightly less. Alternatively we may want to be risky and promote documents we are less certain about with the hope that some will turn out to be very good documents. Having uncertainty estimates for the document scores allows us this extra choice. A useful heuristic is to rank according to  X  s +  X  X  , where  X  is negative for a conser-vative ranking and positive for a risky ranking. 5 Of course, in the spirit of this paper, we actually want to give a ranking that maximizes test NDCG. Since we do not have a proper generative distribution on labels, we cannot compute the op-timal ranking. However, loosely speaking, the NDCG met-ric as used here actually encourages a risky ranking. This is because of the exponential gain function, which rewards top rated documents very highly. All other things being equal, if we are more uncertain about a document there is a bigger chance it might be a top rated document (or a bottom rated document), and top rated documents pick up a large reward from the exponential gain. We explore some of these ideas in the experiments section.
We carried out experiments using the benchmark Learning to Rank (LETOR) OHSUMED data set [9]. The advantages of this data set are that it is publicly available, comes with a number of precalculated IR content features, and provides reference baseline evaluation results and an evaluation tool.
The OHSUMED data set is a collection of medical pub-lication abstracts. It contains 106 queries, and a total of 16,140 judged query-document pairs, classified as definitely , possibly , or not relevant . 25 standard IR features are ex-tracted from the records. The data comes ready-split into 5 folds each consisting of a 60/20/20 partition into training, validation, and test sets. Models are built on each of the five folds using the training set; the validation set can be used to select a model from a set of training runs and/or from a set of models produced within a training run; and the test set is used to evaluate the chosen model.
Computing the variance  X  2 at prediction time (see (11)) is slightly more expensive than computing the means alone  X  O ( M D + M 2 ). However, an efficient alternative is to first order the documents based on mean score, and then just reorder the top ten using  X  s +  X  X  .
There are several design choices which are common to both the GP-Rank and FITC-Rank model, including the number of prototypes per label value, the type of kernel func-tion, how prototypes are initialised, and how kernel hyper-parameters are initialised. Also, as discussed in [17], there is a design choice for the SoftNDCG objective where an ar-gument is made for training with a discount function which is less severe than the discount function used in final eval-uation; experiments show that this provides better general-isation to test sets. In these experiments a linear discount function was used for the SoftNDCG objective. An LBFGS algorithm [11] was used to perform the optimisation.
After some initial experimentation, 4 prototypes per label value (i.e. 12 in total) were chosen for the GP-Rank model, and 2 prototypes per label value (i.e. 6 in total) were chosen for the FITC-Rank model; these prototypes were randomly initialised from the training data within each label value. In general, GP-Rank requires more prototypes. Choosing the number of prototypes is an important issue; if we want to make reliable use of the uncertainty in these models, then we need to ensure that we avoid overfitting for both the vari-ances and means. In the results section we comment further on this. The small total number of prototypes needed (6 X  12) make query-time performance analogous to a single layer neural net with a similar number of hidden nodes. The virtual output values for the prototypes (in the GP-Rank case) and the training data (in the FITC-Rank case) were initialised to their observed label values, offset to be zero-mean. An ARD + Linear kernel (8) was used. The length parameters of the ARD sub-kernel were initialised to  X  The ARD scale parameter was initialised to the standard deviation of the sample labels. Each linear kernel param-eter was initialised to be the inverse of the square of the corresponding ARD length parameter.

One other difference from the reported baseline experi-ments is that we have chosen to use a corpus-level rather than query-level feature scaling. There were two goals for the experiments. (a) to compare GP-Rank and FITC-Rank against themselves, and against the baseline models, and (b) to see if uncertainty could be explicitly used in a simple way to improve test performance. To those ends each experiment was of the following form. Models were evaluated using  X  s +  X  X  to determine rankings (as described in section 5.3) with  X  taking on 11 different values ranging from  X  0 . 5 to 0 . 5 in increments of 0 . 1. For a given experiment each fold was processed in turn. The best validation model for each value of  X  was maintained across a set of 10 trials.  X  X est X  was taken to be the NDCG@5 evaluation on the validation set; the rationale for this was to pick a balance between a highly noisy criterion such as NDCG@1 and a lower truncation value where improvements based on reorderings might not show up. The output of a given experiment, then, is a set of 55 models  X  11 for each fold. These models were applied to the relevant test fold, using the corresponding value of  X  and averaged over the fold, thus producing 11 overall test evaluations for the ex-periment to go along with 11 overall training and validation evaluations. The  X  X est X  validation result was then used to choose the value of  X   X  more on this in the results section. Figure 2: NDCG performance vs. ranking riskiness. The documents are sorted according to  X  s +  X  X  , where  X  is varied from -0.5 to 0.5.
Figure 2a shows the validation, test and training evalua-tions at NDCG@5 for FITC-Rank averaged over several ex-periments. 1- X  error bars are shown for validation and test. This is quite an instructive graph for several reasons. First, it shows fairly pronounced concave curves for each data set. Secondly these all peak at or close to an  X  of 0, indicat-ing that, for LETOR OHSUMED at least, we don X  X  get any benefit from explicit use of the model X  X  uncertainty informa-tion. The similar shapes for training and validation suggest that we are not overfitting. Other experiments that we have done show that if the number of prototypes is increased, then there is much more disparity between the training and validation curves, with the training curve peaking for pos-itive values of  X  . This disparity, or lack thereof, as well as the validation evaluations, can help decide what is a suitable number of prototypes for our model.

We see that the evaluation NDCG@5 has almost identical value for an  X  of 0, and an  X  of  X  0 . 1. However, the variance across experiments is noticeably less for  X  = 0 which makes it the better choice.

Figure 2b shows the validation, test and training evalu-ations at NDCG@5 for GP-Rank averaged over several ex-periments. Here the situation is very different than for the FITC-Rank case in that there is no obvious peak in the curves, though there is some similarity in shape  X  especially between the training and test curves. This then leaves no clear choice for  X  , though perhaps a  X  = 0 is indicated due to the lower variance and the training set peak.

With these choices, we can now do some model compar-isons. Figure 3a shows the performance of GP-Rank against FITC-Rank, plotting the average test evaluations at each NDCG truncation level. In addition 1  X  error bars are shown for FITC-Rank  X  these indicate that FITC-Rank has a consistently higher NDCG at each truncation level. We therefore conclude that the extra sophistication of the FITC sparse GP approximation makes a significant gain in perfor-mance over the simpler GP approach, also with a reduced query-time cost due to fewer prototypes.

Figure 3b compares FITC-Rank with the LETOR base-lines [9]. In order to avoid clutter, only a representative sub-set of baselines is shown, including the best and the worst of them. It can be seen that FITC-Rank performs consistently well against these baselines. FRank, the best of the base-lines, beats FITC-Rank at the top three truncation levels, but FITC-Rank performs best for all the other truncation levels. In this paper we have described two ways of combining Gaussian process models with the SoftRank method for di-rectly optimizing NDCG in training  X  we refer to these as GP-Rank and FITC-Rank to reflect the choice of underly-ing Gaussian Process model. The Gaussian process allows the smoothing uncertainties in SoftRank to reflect modelling uncertainty in the learnt score function.

We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting, and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model. We have shown very competitive results rel-ative to the LETOR-provided baseline models. The ability of our models to take into account different levels of uncer-tainty for different data produces effective models, and, for FITC-Rank in particular, performance is consistent across experiments and gives significant improvements against the baseline models. However, we have found little evidence, at least for the LETOR OHSUMED data set, that explicit use of the uncertainty information can improve model perfor-mance in terms of NDCG . Explicit uncertainty information can, of course, be used for other purposes  X  for example, to provide for the user a confidence level for each document in the ranking.
 An illustration of this is provided in the table below for the OHSUMED query  X  X nfections in renal transplant patients X , where the top 5 documents according to FITC-Rank score mean have been listed. Also shown are the score uncer-tainties, including measurement noise. These partly show just how noisy the measurements are, but also give us use-ful information. Conservatively, we may wish to promote or highlight documents 2 and 4 because we are more confident about their rankings, and we may wish to demote document 5 further due to high uncertainty.  X  s  X  Document title
Although this paper has focused on NDCG, the method can be used in conjunction with any metric that can be described in terms of a score for a ranking based on rele-vance labels. Other discontinuous metrics can be naturally smoothed in this way, including standard IR metrics such as Average Precision. [1] C. Burges, R. Ragno, and Q. V. L. Le. Learning to [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [3] W. Chu and Z. Ghahramani. Gaussian processes for [4] K. Crammer and Y. Singer. Pranking with ranking. In [5] R. Herbrich, T. Graepel, and K. Obermayer. Large [6] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [7] T. Joachims. Optimizing search engines using [8] N. D. Lawrence. Learning for larger datasets with the [9] T.-Y. Liu. LETOR: Benchmark datasets for learning [10] R. M. Neal. Bayesian learning for neural networks. In [11] J. Nocedal and S. Wright. Numerical Optimization, [12] J. Qui  X nonero Candela and C. E. Rasmussen. A [13] C. E. Rasmussen and C. K. I. Williams. Gaussian [14] S. Robertson and H. Zaragoza. On rank-based [15] S. Robertson, H. Zaragoza, and M. Taylor. A simple [16] E. Snelson and Z. Ghahramani. Sparse Gaussian [17] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [18] L. L. Thurstone. A law of comparative judgement.
Here we derive the derivatives of  X  s j and  X  2 j in (11) with respect to prototypes ( X u , y u ) and hyperparameters. The first step is to compute the derivatives with respect to the kernel matrices of (11). We follow a similar treatment to that given in [8].

We restate (11) in slightly more compact notation: where both K jj and K uu include the noise variance  X  2 , and j refers to a single prediction location.

First differentiating the mean (14): and then the variance (15):
Suppose, now, that we have an objective J = J (  X  s , v s which is a function of the parameters of the predictive distri-butions for a set of points { x s j } S j =1 , and suppose that we have the corresponding vectors of derivatives  X  X   X  X  this paper, SoftRank provides the objective and these vec-tors of derivatives. We can efficiently calculate the deriva-tives of J with respect to each kernel matrix in batch form using matrix calculations. Due to space limitations, we only show one such calculation:
Derivatives with respect to kernel hyperparameters and prototype inputs can now be calculated directly from the objective derivatives in the previous section, using the chain rule. Such calculations are dependent on the particular ker-nel and are straightforward (e.g. by differentiating equa-tion (8)).

A subscript f corresponds to the training data function values, a subscript u corresponds to the inducing (prototype) variables, and a subscript j corresponds to a single new point for which we want the predictive distribution. The following definitions help simplify the notation: K ff includes the  X  2 noise term, and the mask function uses the second matrix argument to mask out the first matrix argument. In the case of FITC, M is the Identity matrix and so the mask function will restrict K ff to its diagonal values, with zeroes elsewhere.
 By standard matrix lemmas:
The mean and variance of the FITC predictive distribu-tion are given by
Let  X  = f ( K uu , K uf , K ff , A, D ) be any function with the specified matrix parameters as formal arguments (for exam-ple, the log evidence, or the mean of the predictive distri-bution evaluated at a test point). Then, extrapolating from [8], the matrix derivatives can be calculated as where
To apply the general formulae to the predictive mean given in (29) we first need to calculate the formal deriva-tives which make up the right hand sides. Those that are non-zero are given by: where These can then be plugged into equations (31), (32), (33), and (34) to obtain  X  X  j  X  X 
In addition,  X  j is a function of K uj and y . These deriva-tives can be written down directly as:
Similarly for the predictive variance of (30), those that are non-zero are given by: These can then be plugged into equations (31), (32), (33), and (34) to obtain  X  X  j  X  X 
In addition, v j is a function of K uj and K jj . These deriva-tives can be written down directly as:
Finally objective and parameter derivatives follow simi-larly to A.2 and A.3, by use of the chain rule and batching calculations.
