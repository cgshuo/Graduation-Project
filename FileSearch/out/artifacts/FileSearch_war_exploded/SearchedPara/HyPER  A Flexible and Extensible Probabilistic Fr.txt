 As the amount of recorded digital information increases, there is a growing need for flexible recommender systems which can incorporate richly structured data sources to im-prove recommendations. In this paper, we show how a re-cently introduced statistical relational learning framework can be used to develop a generic and extensible hybrid rec-ommender system. Our hybrid approach, HyPER (HY-brid Probabilistic Extensible Recommender), incorporates and reasons over a wide range of information sources. Such sources include multiple user-user and item-item similarity measures, content, and social information. HyPER auto-matically learns to balance these different information sig-nals when making predictions. We build our system using a powerful and intuitive probabilistic programming language called probabilistic soft logic [ 1], which enables efficient and accurate prediction by formulating our custom recommender systems with a scalable class of graphical models known as hinge-loss Markov random fields. We experimentally evalu-ate our approach on two popular recommendation datasets, showing that HyPER can effectively combine multiple in-formation types for improved performance, and can signifi-cantly outperform existing state-of-the-art approaches. Hybrid recommender systems, graphical models, probabilis-tic programming, probabilistic soft logic.
Recent work on hybrid recommender systems has shown that recommendation accuracy can be improved by combin-ing multiple data modalities and modeling techniques within a single model [ 2, 3, 4, 5, 6]. Existing hybrid recommender systems are typically designed for a specific problem do-main, such as movie recommendations, and are limited in their ability to generalize to other settings or make use of any further information. As our daily lives become increas-ingly digitally connected, the list of data sources available for recommendations continues to grow. There is a need for general-purpose, extensible frameworks that can make use of arbitrary data modalities to improve recommendation.
The challenge of custom model-building has been exten-sively studied in the fields of probabilistic programming [7 ] and statistical relational learning (SRL) [8 ], which provide programming language interfaces for encoding knowledge and specifying models. Probabilistic programs can be used to encode graphical models for reasoning with graph-struct-ured probabilistic dependencies. Graphical models are a natural approach to recommendations given that the user-item rating matrix can be interpreted as a graph, with weight-ed edges between users and items corresponding to the re-spective ratings [4 ].

In modern recommendation contexts, a bipartite user-item graph is insufficient to represent all available informa-tion, such as user-user and item-item similarity, content, so-cial information, and metadata. For example, neighborhood-based collaborative filtering techniques can be interpreted as predicting ratings based on an extension of the user-item graph with additional edges between pairs of similar users or similar items (Figure 1). We need a more general repre-sentation to reason over this richly structured information.
In this paper, we propose a general hybrid recommender framework, called HyPER (HYbrid Probabilistic Extensible Recommender), which leverages the flexibility of probabilis-tic programming in order to build adaptable and extensi-ble hybrid recommender systems which reason over complex data. In particular, we use a modeling language called prob-abilistic soft logic (PSL) [ 9]. PSL is especially well-suited to collaborative-filtering based recommendation graphs as it is able to fuse information from multiple sources and it was originally designed as a flexible framework for reason-ing and combining similarities [ 10]. It provides a general declarative framework for combining entity similarities, at-tribute similarities, and information from additional sources including the predictions of other algorithms. The models defined by PSL programs, called hinge-loss Markov random fields (HL-MRFs), are amenable to efficient and scalable in-ference, which is crucial in a recommendation context.
Our contributions include (1) a general and extensible hy-brid recommender system with a probabilistic programming interface, (2) a method for learning how to balance the dif-ferent input signals in the hybrid system, and (3) exten-sive experimental studies using several information sources which validate the performance of the proposed framework and highlight contribution of each source to the final pre-c  X  diction. To the best of our knowledge, our proposed Hy-PER framework is the first which provides a mechanism to extend the system by incorporating and reasoning over cur-rently unspecified additional information types and similar-ity measures. We evaluate our system on two rich datasets from the local business and music recommendation domains (Yelp and Last.fm) comparing our model to state-of-the-art recommendation approaches. Our results show that HyPER is able to effectively combine multiple information sources to improve recommendations, resulting in significantly im-proved performance over the competing methods in both datasets.
Recommender systems play a significant role in many ev-eryday decision-making processes which affect the quality of our lives, from the restaurant we have lunch at, to the hotel for our vacation, to the music we listen to. Tradi-tional recommender systems primarily leverage underlying similarities between users and items in order to make pre-dictions based on observed ratings. Content-based filtering (CB) approaches compute these similarities by using fea-tures extracted from content to build user profiles, which are compared with content features of items. While content-based approaches can recommend newly added items, they are limited by a lack of serendipity. The recommendations are limited to the user X  X  known likes and do not generally include items out of the user X  X  (recorded) comfort zone [11 ].
Collaborative filtering (CF) techniques address this by identifying similar users or items based on their rating pat-terns instead of content, using methods such as neighborhood-based approaches and matrix factorization models. How-ever, collaborative filtering methods typically do not per-form well in  X  X old-start X  settings, where there are few rat-ings for a user or an item [12 ]. Moreover, pure rating-based collaborative filtering approaches cannot take advantage of data which may be available in addition to ratings.
To address these shortcomings, hybrid recommender sys-tems (HRSs) were introduced, combining content-based and collaborative-filtering techniques (e.g. [2 , 3, 4]). HRS tech-niques can improve performance over content-based and col-laborative filtering methods alone, especially in the case where the ratings matrix is sparse [ 2]. However, existing HRSs have their own limitations. First, they are problem-and data-specific. Each HRS is typically motivated by a specific problem domain (e.g. movie recommendations) and the solution is fine-tuned to solve a specific problem with datasets of specific characteristics. Hence, HRSs typically cannot be generalized to different problem domains or input data, or be easily expanded to incorporate knowledge from richer datasets.

As the web has evolved into a participatory, user-driven platform, additional information is increasingly becoming available. Users form social networks, give verbal feedback on items via reviews, endorse or down-vote items or other users, form trust relationships,  X  X heck-in X  at venues, and per-form many other social actions that may potentially be lever-aged to better understand users in order to improve recom-mendations. A flexible and extensible hybrid recommender system which can make use of this wealth of information is increasingly important.
 The remainder of the paper is structured as follows. In Section 3 we introduce HyPER, a general hybrid recommen-dation framework which is extensible and customizable using a probabilistic programming interface. We systematically evaluate our framework in Section 4, and place our system in the context of related work in Section 5. Finally, we con-clude with a discussion in Section 6.
We propose HyPER, a general hybrid framework that combines multiple different sources of information and mod-eling techniques into a single unified model. HyPER offers the capability to extend the model by incorporating addi-tional sources of information as they become available. Our approach begins by viewing the recommendation task as a bipartite graph, where users U and items I are the vertices, and ratings are edges between users and items [4 ]. Using PSL [1 ], a flexible statistical relational learning system with a probabilistic programming interface, this graph is then augmented to construct a probabilistic graphical model with additional edges to encode similarity information, predicted ratings, content and social information, and metadata. We then train the graphical model to learn the relative impor-tance of the different information sources in the hybrid sys-tem, and make predictions for target ratings, using graphical model learning and collective inference techniques.
Figure 1 shows an overview of our modeling approach. In the figure, items and users are nodes, and green edges repre-sent the ratings that users gave to items, with edge weights corresponding to the rating values. The goal is to predict the edge weights for unobserved edges, denoted as dashed lines. Neighborhood-based approaches find the k most simi-lar users or similar items, and use their ratings to make these predictions. In our graph-based representation, we interpret these k -nearest neighbor relationships as k edges which are added to the graph. In Figure 1, blue edges encode user similarities and red edges correspond to item similarities.
We can further encode additional sources of information and outputs of other recommendation algorithms within this graph-based representation in a similar way, i.e. in the form of additional links or nodes. For instance, latent factor methods identify latent representations which can be used to augment the graph with weighted edges encoding predic-tions of user-item ratings based on the latent space. The latent representations can also be used to construct addi-tional user-user and item-item edges by identifying similar users and similar items in the latent space. Content infor-mation and metadata, such as demographics and time in-formation, can be incorporated in the graph representation by identifying further similarity links, or by adding nodes with attribute values, and edges to associate these values with users and items. Furthermore, social information from digital social media is inherently relational, and can readily be incorporated into a graph-based representation.

Having encoded all available information in the graph, the next step is to reason over this graph to predict unobserved user-item rating edges. We view the prediction task as infer-ence in a graphical model, the structure of which is defined by our graph representation. As scalability is important for recommendation tasks in practice, we use a hinge-loss Markov random field (HL-MRF) formulation [ 9]. HL-MRFs are highly scalable as they admit exact inference by way of efficient parallel algorithms. In the next section we briefly re-view HL-MRFs. We then describe our unified recommender system modeling framework in detail in Section 3.2 , and we show how to learn the relative importance of the information sources for our hybrid model in Section 3.3 .
Hinge-loss Markov random fields (HL-MRFs) [ 9] are a gen-eral class of conditional probabilistic models over continuous random variables which admit tractable and efficient infer-ence. The key to the tractability of these models is the use of hinge-loss feature functions. More formally, a hinge-loss Markov random field defines a conditional probability den-sity function over random variables Y conditioned on X , where  X  j is a hinge-loss potential function, of the form Here, ` is linear function of X and Y , and p j  X  { 1 , 2 } op-tionally squares the potential. The variables in X and Y are in the unit interval [0,1]. Each  X  j is associated with a weight w j which determines its importance in the model. We learn the weights from the data, as discussed later in Section 3.3 . Note that Equation 1 is log-concave in Y , so maximum a posteriori (MAP) inference to find the optimal Y in HL-MRFs can be solved exactly via convex optimiza-tion. We use the alternating direction method of multipliers (ADMM) approach of Bach et al. [9 ] to perform this opti-mization efficiently and in parallel.

HL-MRFs can be specified using a probabilistic program-ming language called Probabilistic Soft Logic (PSL) [1 ], and this is the language we use to specify our unified recommen-dation framework. PSL is a declarative first-order logical language where logical rules are composed of continuous re-laxations of Boolean logical operators. These rules define templates for hinge-loss potential functions, which are in-stantiated to construct an HL-MRF model. For example, a  X  b corresponds to the hinge function max( a  X  b, 0), and a  X  b corresponds to max( a + b  X  1 , 0). We refer the reader to [1 ] for a detailed description of PSL operators.
To illustrate PSL in a movie recommendation context, the following rule encodes that users tend to rate movies of their preferred genres highly:
LikesGenre ( U,G )  X  IsGenre ( M,G )  X  Rating ( U,M ) , where LikesGenre ( U,G ) is a binary observed predicate, IsGenre ( M,G ) is a continuous observed predicate in the interval [0 , 1] capturing the affinity of the movie to the genre, and Rating ( U,M ) is a continuous variable to be inferred, which encodes the star rating as a number between 0 and 1, with higher values corresponding to higher star ratings. For example, we could instantiate U = Jim , G = classics and M = Casablanca . This instantiation results in a hinge-loss potential function in the HL-MRF,
The strengths of the HyPER framework include the ability to extensibly incorporate multiple sources of information in a unified hybrid recommendation model, as well as learning how to balance these signals from training data. HyPER models are specified using a collection of PSL rules which encode graph-structured dependency relationships between users, items, ratings, content and social information. Ad-ditionally, the model provides the flexibility to incorporate prior predictions, such as mean-centering predictors and the results of other recommendation algorithms. In what fol-lows, we present the rules that define the HL-MRF model for the core of the HyPER framework. We emphasize that while this set of rules covers a breadth of input sources, the model can be readily extended to incorporate other sources of information such as time, implicit feedback, and social interactions, with the introduction of new PSL rules. More-over, additional similarity measures and recommendation al-gorithms can straightforwardly be included with analogous rules. HyPER builds upon the ideas of Fakhraei et al. [13 ], which is an important precursor to this work.
Motivated by the basic principles of the neighborhood-based approach, we can define PSL rules of this form:
This rule captures the intuition that similar users give sim-ilar ratings to the same items. The predicate Rating ( u,i ) takes a value in the interval [0 , 1] and represents the nor-malized value of the rating that a user u gave to an item i , while SimilarUsers sim ( u 1 ,u 2 ) is binary, with value 1 iff u 1 is one of the k -nearest neighbors of u 2 . The similarities can be calculated with any similarity measure sim , as we will describe in Section 3.2.3 . The above rule represents a template for hinge functions which reduce the probability of predicted ratings as the difference between Rating ( u 2 ,i ) and Rating ( u 1 ,i ) increases, for users that are neighbors.
Similarly, we can define PSL rules to capture the intu-ition of item-based collaborative filtering methods, namely that similar items should have similar ratings from the same users: The predicate SimilarItems sim ( i 1 ,i 2 ) is binary, with value 1 iff i 1 is one of the k -nearest neighbors of i 2 (using similarity measure sim ), while Rating ( u,i ) represents the normalized value of the rating of user u to item i , as discussed above.
By including both types of rules described in Sections 3.2.1 and 3.2.2 we can define an HL-MRF model that com-bines user-based and item-based techniques to predict rat-ings. There exist many measures available to compute simi-larities between entities for user-based and item-based meth-ods, and these different measures capture different notions of similarity. For instance, in neighborhood-based approaches, vector-based similarity measures are broadly used, whereas in latent factor approaches other similarities, applicable to the low dimensional space, are preferred. While most ex-isting recommender systems are designed to use a single similarity measure, HyPER allows for the simultaneous in-corporation of multiple similarity measures, and can auto-matically adjust the importance of each based on training data.

In this instantiation of our HyPER framework we use the most popular similarity measures in the neighborhood-based recommendations literature [4 ]. More specifically, we apply Pearson X  X  correlation and cosine similarity measures to cal-culate similarities between users and items; for the items we additionally apply the adjusted cosine similarity mea-sure. To incorporate matrix-factorization collaborative fil-tering, and inspired by Hoff et al. [14 ], we compute similar users and items in the low-dimensional latent space using two popular distance measures in that space, namely, cosine and Euclidean. The user similarities are identified using the following rules:
Analogous rules are introduced to identify similar items, but are omitted due to space limitations. As noted earlier, this initial set of similarity measures can be readily expanded by adding the corresponding rules, in the same form as above.
Each individual user considered in a recommender system has her own biases in rating items (e.g. some users tend to be stricter than others). Moreover, each item X  X  rating is in-fluenced by its overall quality and popularity (e.g. a popular blockbuster may get higher ratings on average than a low-budget movie). To address such biases, a recommender sys-tem needs to incorporate a normalization mechanism, both per user, and per item. Using mean-centering normalization for neighborhood-based approaches, or including intercept terms in probabilistic latent factor models, addresses this issue and generally improves performance [ 4]. In our Hy-PER framework we encode this intuition with rules that encourage the ratings to be close to the average, per-user and per-item: The predicate AverageUserRating ( u ) represents the average of the ratings over the set of items that user u provided in the training set. Similarly, AverageUserRating ( i ) represents the average of the user ratings an item i has received. The pair of PSL rules per-user and per-item corresponds to a  X  X -shaped X  function centered at the average rating, which penalizes the predicted rating for being different in either direction from this average.

In order to capture cases where we have no information about a user or an item, we use a general prior rating cen-tered at the average value of all of the ratings in the system (i.e. the average over all items rated by all users). We en-code this prior with the following rules: The real-valued predicate PriorRating represents the aver-age of all of the ratings.
Incorporating other sources of information pertaining to the items, the users, and the respective ratings to our frame-work is straightforward. In the present instantiation of our framework, we use the content of the items to find similar items: In this rule, the predicate SimilarItems Content ( i 1 resents items that have similar content-based features (e.g. in the movie recommendation domain such features are the genre, actor, director, etc.), instead of similar ratings.
The HyPER framework can also incorporate social infor-mation, when this is available. For instance, in the present instantiation of the system, we leverage social network friend-ship links as follows:
Note that our framework is flexible and can incorporate many other sources of information that are available. For in-stance, we can leverage demographic information by comput-ing similarity neighborhood relationships in demographic feature space and employing the rule:
Every recommendation algorithm has strengths and weak-nesses which may depend on data-specific factors such as the degree of sparsity or the shape of the data matrix. This im-poses a big limitation in the recommendation process, as choosing one algorithm as the core of a recommender sys-tem limits its strength to a particular set of domains. In this work, our motivation is to provide a flexible framework that can be used as-is to generate accurate recommendations for any domain and data regime. Therefore, instead of selecting a single recommendation algorithm, we propose to incorpo-rate the predictions from different methods into our unified model. These predictions are further augmented with any other available information, using the rules discussed above. For example, the predictions from matrix factorization (op-timizing regularized squared error via stochastic gradient descent) (MF), Bayesian probabilistic matrix factorization (BPMF) [ 15], and item-based collaborative filtering can be incorporated in the model via the following rules:
Additional algorithms can be easily incorporated in a sim-ilar manner.
An important task of any hybrid recommender system is to trade off and balance the different information sources ac-cording to their informativeness for predicting ratings. Each of the first-order rules introduced above corresponds to a dif-ferent information source in our hybrid model, and is asso-ciated with a non-negative weight w j in Equation 1. These weights determine the relative importance of the informa-tion sources, corresponding to the extent to which the corre-sponding hinge function  X  j alters the probability of the data under Equation 1, with higher weight w j corresponding to a greater importance of information source j . For each rule we learn a weight using Bach et al. [9 ] X  X  approximate maxi-mum likelihood weight learning algorithm for templated HL-MRFs. The algorithm approximates a gradient step in the conditional likelihood, by replacing the intractable expectation with the MAP solu-tion based on w , which can be rapidly solved using ADMM.
Due to the hinge-loss formulation, inference and learning are relatively scalable via ADMM, which can be performed in parallel. The UMD/UCSC implementation of PSL uses a single-machine multi-threaded ADMM algorithm, and we use this implementation for our experiments on datasets with around 100,000 ratings. For deployment in industrial applications with millions of users, items, and ratings, the main bottleneck for scalability is memory to store the ground model. This can be addressed simply in the current imple-mentation of PSL by dividing the graph into densely con-nected subgraphs, e.g. a subgraph per city, that are inferred independently and in parallel on different machines. Al-ternatively, a fully distributed implementation of ADMM would straightforwardly facilitate the scaling of HyPER to the big data setting via model parallelism.
In this section we evaluate our HyPER framework with comparison to state-of-the-art recommender algorithms. We report experimental results on two popular datasets for both the complete hybrid model and for each individual compo-nent of our hybrid models. 1 Code is available at https://github.com/pkouki/recsys2015 Dataset Yelp Last.fm No. of users 34,454 1,892 No. of items 3,605 17,632 No. of ratings 99,049 92,834
Content 514 business 9,719 artist tags
Social 81,512 12,717
Sparsity 99.92% 99.72%
For our experimental evaluation we used the Yelp aca-demic dataset and the Last.fm dataset. 23 Our goal with Yelp is to recommend local businesses to users by predicting the missing ratings of businesses based on previous ratings. For our experiments, we used all businesses, users, and rat-ings from Scottsdale, Arizona, one of the largest cities in the dataset. Since we employ user and item similarities as well as social information, it makes sense to focus those relation-ships within the subgroup of the businesses of one physical location. Additionally, we used the categories of each busi-ness as content information and the explicit user friendships provided as social information. Yelp users give ratings from 1 to 5 stars, which we linearly scaled into the [0,1] range that PSL operates over for the purposes of our model.
For the Last.fm dataset our goal is to recommend artists to users. As Last.fm does not provide explicit user-artist rat-ings we leverage the number of times a user has listened to an artist to construct implicit ratings. We use a simple model-based approach, where the repeated-listen counts for each user across artists are modeled with a negative-binomial dis-tribution. We used this distribution as it is appropriate for count data where the sample variance is greater than the sample mean, which is typically the case for Last.fm. For each user, we fit a negative binomial to their counts via maximum likelihood estimation, and we calculate the user X  X  implicit rating for an artist as the cumulative distribution function (CDF) of the distribution, evaluated at the artist X  X  count. This corresponds to the proportion of hypothetical artists that a user would listen to less than the given artist, under the model. The Last.fm dataset also includes tags on artists that we use for content-based information, as well as user friendship data that we use for social recommendation.
We deliberately selected two datasets with a similar total number of ratings but a different ratio of users to items. Different recommendation methods may perform better with more users than items or vice versa, and hybrid systems must account for this. We provide the summary statistics of the two datasets in Table 1.

To learn the appropriate balance between information sources for HyPER, i.e. to learn the weights of each rule in the model, we train using the approximate maximum like-lihood method described in Section 3.3 , with 20% of the training folds treated as the prediction target variables Y . During testing, we performed MAP inference to make pre-dictions using ADMM. We report the root mean squared error (RMSE) and the mean absolute error (MAE). We com-pute these metrics by performing 5-fold cross-validation and reporting the average cross-validated error. https://www.yelp.com/academic dataset http://grouplens.org/datasets/hetrec-2011/
We report overall results with comparison to a selection of competing algorithms in Table 2, and show more detailed results for the individual components of our hybrid models in Table 3. The following sections discuss these results.
We study the performance of HyPER in comparison to several state-of-the-art models. We considered the following baselines:
The performance of our model is statistically significantly better than the baselines at  X  = 0 . 05 for both datasets and evaluation metrics when using paired t-test. We denote with bold the numbers that are statistically significantly better. These results confirm our initial intuition that by incorpo-rating a wide variety of information sources and balancing them appropriately, the HyPER framework manages to per-form very well with rich and diverse datasets. We explore HyPER components in more detail in the following section.
For each type of information, we further evaluated our approach by building simple HyPER models with each rule individually, and comparing these to combined hybrid sub-models comprising all of the corresponding rules of that type. Each sub-model also included the corresponding mean-centering rules (e.g. the user-average rating rule for the user-based models). To balance the effect of each rule, we performed weight learning within each training fold to learn rule weights. We report the results in Table 3. The re-sults show that for each information type, the HyPER model http://www.dato.com which combines all of the corresponding components per-forms significantly better than each component, considered in isolation. We denote with bold the cases where the per-formance of each HyPER model is statistically significantly better than all the individual models in the same category at  X  = 0 . 05 using paired t-test. The final HyPER model shown in the last line, which combines all of the available informa-tion into a single hybrid model, also performs statistically significantly better than all sub-models and baselines.
Mean-Centering Priors: We created simple HyPER models using only the average rating of each user, or the average rating of each item, or the average overall rating, as well as a combined model. In the case of Yelp, the item-average model had a lower error compared to the user average rule, while the opposite was true for Last.fm (Ta-ble 3(a)). This may be because the ratio of users to items is different in the two datasets. The combined model per-formed better than the individual models in both datasets.
Neighborhood-Based Collaborative Filtering: We constructed individual models based on the similarities de-scribed in Section 3.2.1 . The number of neighbors is typ-ically set to between 20 and 50 in the literature [ 4], and so we used 50 neighbors for users/item in all experiments. We also employed a mean-centered approach by providing each of these models with the corresponding average-rating mean-centering rules (e.g. the average user-rating rule for user-based collaborative filtering). As in the previous exper-iment, user-based techniques perform poorly on Yelp, but have better performance on Last.fm (Tables 3(b) and 3(c)). The opposite is true for the item-based techniques, which perform poorly on Last.fm, but better on Yelp. The perfor-mance varied between the different similarity measures, with distances computed in the latent space usually performing the best individually. Again, the HyPER combination of all similarity measures improves performance.

Additional Sources of Information: We constructed individual and hybrid models using friendship information, as well as content similarity between items based on the busi-ness category and the tags of artists for Yelp and Last.fm re-spectively. We used Jaccard similarity for both datasets. In each sub-model we also provided additional rules for mean centering using both average user and item ratings. Con-tent and friendship information help performance in both datasets, and the model that combines both content and so-cial information matched and often improved on the best individual models X  performance (Table 3(d)).

Leveraging Existing Algorithms: As discussed in sec-tion 3.2.6 , our framework is able to combine predictions from a number of different models. In Table 3(e) we show the per-formance of three baseline recommenders, and in the fourth line we present the results of a HyPER ensemble which com-bines the results of those recommenders, without any addi-tional rules. The combined model performed better than the individual baselines.

Relative Importance of Information Sources: When performing weight learning (Section 3.3 ), the learned weights of the rules are indicative of the relative importance of the signals. For Last.fm, average user ratings had a high rule weight while average item ratings did not, while the re-verse was true for Yelp, suggesting a difference in the im-portance of user judiciousness versus item popularity be-tween the data sets. Item similarities had a high weight for Last.fm, while MF predictions had a high weight for Yelp. Negated rules, which decrease predicted ratings, were typi-cally weighted lower than their non-negated counterparts. In general, the rules for BPMF predictions had high weights.
There is a large body of work on recommender systems; see Ricci et al. [16 ] for an overview. We focus our related work discussion on hybrid recommender systems, and partic-ularly systems that can incorporate multi-relational and het-erogeneous data as well as graphical modeling approaches. In Burke [ 17] X  X  taxonomy of hybrid recommender systems our work falls into the  X  X eature augmentation X  category.
Hybrid systems typically combine two or more approaches in order to provide better recommendations, usually content-based and collaborative filtering approaches [ 18 , 19 ] or vari-ations of collaborative filtering approaches [ 20 ]. Gunawar-dana and Meek [18 ] present a domain-agnostic hybrid ap-proach for using content to improve item-item modeling. After the Netflix Prize competition, ensemble methods [ 21] have gained popularity. Factorization Machines [22 ] are a general matrix factorization method that can be applied to design hybrid factorization models. Recently, as user-generated content has become available, researchers have studied how to leverage information such as social relation-to improve recommendations. Incorporating additional in-formation for users and/or items is especially beneficial in cold-start settings [27 ]. Dooms [28 ] argues that a flexible recommendation system that automatically generates good hybrid models would be very valuable as information sources increase. Our model provides such flexibility, allowing for the combination of as many information sources as are avail-able. Fakhraei et al. [ 13] use PSL to reason over multiple similarity measures for predicting drug-target interactions. Our approach extends this model in several important ways for the recommender systems domain.

Chen at al. [ 29 ] learn the strength of ties between users based on multi-relational network information. The learned network is combined with item-based collaborative filtering to improve recommendation results. Burke et al. [30 , 31 ] integrate different dimensions of data about users in a het-erogenous network by using metapaths to create multiple two-dimensional projections representing relationships be-tween entities (e.g. users-tags) and then linearly combining these projections. Also using metapaths, Yu et al. [ 32 ] pro-pose a global and a personalized recommendation model. In their approach, implicit feedback is incorporated into meta-paths and latent features for users and items are generated using matrix factorization.

De Campos et al. [ 3] propose a probabilistic graphical modeling recommendation approach using Bayesian networks. Their approach combines individual predictions from content-based and user-based collaborative filtering components. Hoxha and Rettinger [33 ] also discuss a probabilistic graph-ical modeling representation, using Markov Logic Networks (MLNs) [34 ] to combine content with collaborative filtering. Both MLNs and HL-MRFs operate on undirected graphical models using a first-order logic as their template language, while Bayesian networks are directed. We chose HL-MRFs because they can represent ordered data such as ratings, and due to their scalability with parallel convex optimization for inference. Speed and scalability is of paramount importance in recommender systems and in particular when we run the prediction task collectively over multiple types of input data with a variety of similarity measures.
In this paper we presented HyPER, a new hybrid rec-ommender system which is flexible, problem-agnostic, and is easily extensible via a probabilistic programming inter-face. HyPER uses a hinge-loss MRF formulation, allowing scalable and accurate inference. Our comprehensive experi-ments demonstrate that HyPER can learn to appropriately balance many information sources, resulting in improved performance over previous state-of-the-art approaches on two benchmark datasets.

