 University of Sussex University of Sussex University of Sussex University of Sussex
We present a new framework for compositional distributional semantics in which the distribu-both mutual disambiguation and generalization. 1. Introduction model semantic composition. Although there has recently been considerable interest in this problem, it remains unclear what distributional composition actually means. Our view is that distributional composition is a matter of contextualizing the lexemes being composed. This goes well beyond traditional word sense disambiguation, where each lexeme is assigned one of a fixed number of senses. Our proposal is that composition involves deriving a fine-grained characterization of the distributional meaning of each lexeme in the phrase, where the meaning that is associated with each lexeme is bespoke to that particular context.
 of the lexemes in the phrase. To achieve this we need a structure within which all of the lexemes X  semantics can be overlaid. Once this is done, the lexemes can collectively agree on the semantics of the phrase, and in so doing, determine the semantics that they have in the context of that phrase. Our process of composition thus creates a single structure that encodes contextualized representations of every lexeme in the phrase. by aggregating distributional features across all uses of the lexeme found within the corpus, where distributional features arise from co-occurrences found in the corpus. The distributional features of a lexeme are associated with weights that encode the strength features that don X  X  fit the context are reduced, while the weight of those features that are compatible with the context can be boosted.
 word wooden in the context of the phrase wooden floor . The uncontextualized represen-tation of wooden presumably includes distributional features associated with different although we may have observed in a corpus that it is plausible for the adjective wooden we need to find a way to down-weight the distributional features of being something that can modify actor and voice , while up-weighting the distributional features of being something that can modify table and toy .
 inferences can also be made with respect to distributional features that involve higher-order grammatical dependencies. 1 For example, suppose that we have observed that a director fired the wooden actor . We want this distributional feature of wooden to be down-weighted in the distributional representation of wooden in the context of wooden table , since things made of wood do not typically lose their job.
 down-weighting distributional features arising in contexts such as Prices fell through the floor , while up-weighting distributional features arising in contexts such as I polished the concrete floor .
 to do with the noun that this sense of wooden could modify X  X re internal to the phrase wooden floor in the sense that they are alternatives to one of the words in the phrase.
Although it is specifically a floor that is wooden , our proposal is that the contextualized representation of wooden should recognize that it is plausible that nouns such as chair remaining distributional features are external to the phrase. For example, the verb mop could be an external feature, because things that can be modified by wooden can be the direct object of mop . The external features of wooden and floor with respect to the phrase wooden floor provide something akin to the traditional interpretation of the distributional semantics of the phrase, namely, a representation of those (external) contexts in which this phrase can occur.
 of the phrase, they provide a way to embellish the characterization of the distributional 728 meaning of the lexemes in the phrase. Recall that our goal is to infer a rich and fine-grained representation of the contextualized distributional meaning of each of the lexemes in the phrase.
 as a matter of contextualization, the question arises as to how to realize this conception.
Because each lexeme in the phrase needs to be able to contribute to the contextualization of the other lexemes in the phrase, we need to be able to align what we know about each of the lexeme X  X  distributional features so that this can be achieved. The problem is that the uncontextualized distributional knowledge associated with the different lexemes in the phrase take a different perspective on the feature space. To overcome this we need to: (a) provide a way of structuring the distributional feature space, which we do by typing modify the perspective that each lexeme has on this structured feature space in such a way that they are all aligned with one another.
 clude higher-order dependency relations in this space. However, in contrast to previ-ous proposals, the higher-order dependency relations provides structure to the space code a lexeme X  X  distributional knowledge with a hierarchical structure that we call an
Anchored Packed Dependency Tree ( A PT ). As we show, this data structure provides a way for us to align the distributional knowledge of the lexemes that are being com-implemented. 2. The Distributional Lexicon
In this section, we begin the formalization of our proposal by describing the distribu-lexemes. Table 1 provides a summary of the notation that we are using.
 and let T V , R be the set of dependency trees where every node is labeled with a member of V , and every directed edge is labeled with an element of R . Figure 1 shows eight examples of dependency trees. 2.1 Typed Co-occurrences
When two lexemes w and w 0 co-occur in a dependency tree 3 type of this co-occurrence, capturing the syntactic relationship that holds between these  X  a co-occurrence type (path)  X   X  (  X  ) the co-occurrence type produced by reducing  X  k w k (  X  ) the node (weighted lexeme multiset) in k w k for co-occurrence type  X   X   X  , w  X  a distributional feature in vector space  X  X  X  X  occurrences of the two lexemes. In particular,  X  encodes the sequence of dependencies that lie along the path in t between the occurrences of w and w from w to w 0 in t initially travels up towards the root of t (against the directionality of the dependency edges) until an ancestor of w 0 is reached. It then travels down the tree to w 0 (following the directionality of the dependencies). The string  X  must, therefore, not only encode the sequence of dependency relations appearing along the path, but also whether each edge is traversed in a forward or backward direction. In particular, given the path  X  v 0 , ... , v k  X  in t , where k &gt; 0, w labels v encodes the co-occurrence type associated with this path as follows: 730 Hence, co-occurrence types are strings in R  X  R  X  , where R = { r | r  X  R } . refers to the length of the dependency path. It is also convenient to be able to refer to the inverse of a co-occurrence type. This can be thought of as the same path, but traversed in the reverse direction. To be precise, given the co-occurrence type  X  = x each x i  X  R  X  R for 1  X  i  X  n , the inverse of  X  , denoted  X  where r  X  1 = r and r  X  1 = r for r  X  R . For example, the inverse of in Figure 1(a). uniformity to our typing system that simplifies the formulation of distributional com-position in Section 4, and leads to the need for a refinement to our co-occurrence type en-codings. Because we permit paths that traverse both forwards and backwards along the same dependency X  X or example, in the co-occurrence  X  white / it is logical to consider  X  white / JJ , AMOD  X  DOBJ  X  DOBJ However, in line with our decision to include  X  white / dependency cancellation process in which adjacent, complementary dependencies are r  X  R are replaced with , and this process is repeated until no further reductions are possible.
 follows:
For the remainder of this article, we only consider reduced co-occurrence types when associating a type with a co-occurrence.
 number of times that the co-occurrence  X  w ,  X  , w 0  X  occurs in t is denoted #(  X  w ,  X  , w and, given some corpus C of dependency trees, the sum of all #(  X  w ,  X  , w on the corpus C is not expressed in our notation.
 of each distributional feature. A variety of alternatives are considered during the ex-perimental work presented in Section 5. Among the options we have considered are probabilities and various versions of positive pointwise mutual information. Although, in practice, the precise method for weighting features is of practical importance, it is not an intrinsic part of the theory that this article is introducing. In the following exposition we denote the weight of the distributional feature  X   X  , w expression W ( w ,  X   X  , w 0  X  ). 2.2 Anchored Packed Trees
Given a dependency tree corpus C  X  T V , R and a lexeme w  X  V , we are interested in cap-turing the aggregation of all distributional contexts of w in C within a single structure. We achieve this with what we call an Anchored Packed Dependency Tree (A 732 aggregate of all distributional features of a lexeme over a corpus of dependency trees, but they can also be used to express the distributional features of a lexeme that has been contextualized within some dependency tree (see Section 4).
 first instance we define it as a mapping from pairs (  X  , w such that k w k (  X  , w 0 ) gives the weight of the typed co-occurrence  X  w ,  X  , w weights of distributional features of w . In other words, for each  X   X  R
The restriction of k w k to co-occurrence types that are at most order k is referred to as a k -th order A PT . The distributional lexicon derived from a corpus C is a collection of lexical entries where the entry for the lexeme w is the elementary A ever, because an A PT encodes co-occurrences that are aggregated over a set of depen-dency trees, they can also be interpreted as having a tree structure. In our tree-based particular, k w k (  X  ) is thought of as a node that is associated with the weighted lexeme multiset in which the weight of w 0 in the multiset is k w k (  X  , w k w k ( ) as the anchor of the A PT k w k .
 in Figure 1. On the far left we give the letter corresponding to the sentence in Figure 1 that generated the typed co-occurrences. Each column corresponds to one node in the A non-empty nodes are displayed.

Figure 2, which is the elementaryA PT for dry / JJ . The weighted multiset at the anchor node is denoted k w k ( ). The lexeme dry / JJ occurs three times, and the weight in trees in Figure 1:  X  dry / JJ , AMOD  X  AMOD , fizzy /  X  dry / JJ , AMOD  X  AMOD , clean / JJ  X  , all of which involve the co-occurrence type These lexemes appear in the multiset k w k ( ) because  X  ( 3. A PT Similarity
One of the most fundamental aspects of any treatment of distributional semantics is that it supports a way of measuring distributional similarity. In this section, we describe a straightforward way in which the similarity of two A PT s can be measured through a mapping from A PT s to vectors.

The vector space that we use to encode A PT s includes one dimension for each element of
FEATS , and we use the pair  X   X  , w  X  to refer to its corresponding dimension. (a) we bought (b) (c) he folded (c) (d) (e) the man hung up (f) a boy bought (c) he folded (g) she folded up (h) he folded value that the vector  X   X  , w 0  X   X  FEATS : 734 where  X  (  X  , w ) is a path-weighting function that is intended to reflect the fact that not all of the distributional features are equally important in determining the distributional similarity of two A PT s. Generally speaking, syntactically distant co-occurrences pro-vide a weaker characterization of the semantics of a lexeme than co-occurrences that are syntactically closer. By multiplying each W ( w ,  X   X  , w capture this, given a suitable instantiation of  X  (  X  , w ).
 selecting one of the co-occurrences  X  w ,  X  0 , w 0  X  , where w co-occurrence type  X  . We can estimate these path probabilities from the co-occurrence counts in C as follows: where p (  X  | w ) typically falls off rapidly as a function of the length of  X  as desired. measured in terms of the similarity of vectors be measured in a variety of ways (Lin 1998; Lee 1999; Weeds and Weir 2005; Curran 2004). One popular option involves the use of the cosine measure:
It is common to apply cosine to vectors containing positive pointwise mutual informa-tion (PPMI) values. If the weights used in the A PT s are counts or probabilities, then they can be transformed into PPMI values at this point.
 occurrences associated with a lexeme are being differentiated, vectorized A much sparser than traditional vector representations used to model distributional semantics. This can be mitigated in various ways, including: 4. Distributional Composition
In this section we turn to the central topic of the article, namely, distributional composi-tion. We begin with an informal explanation of our approach, and then present a more precise formalization. 4.1 Discussion of Approach elementary A PT s in the distributional lexicon can be placed in the same vector space (see Section 3), there is an important sense in which A PT are not comparable. For example, many of the dimensions that make sense for verbs, such as those involving a co-occurrence type that begins with sense for a noun. However, as we now explain, the co-occurrence type structure present in an A PT allows us to address this, making way for our definition of distributional composition.
 of this A PT is the node at which the lexeme dry / JJ appears. We can, however, take a different perspective on this A PT  X  X or example, one in which the anchor is the node at which the lexemes bought / VBD and folded / VBD appear. This A
Figure 3. Adjusting the position of the anchor is significant because the starting point of the paths given by the co-occurrence types change. For example, when the A shown at the top of Figure 3 is applied to the co-occurrence type node at which the lexemes we / PRP and he / PRP appear. Thus, this A modifies can take as their direct object. In fact, it looks rather like the elementary A for some verb. The lower tree in Figure 3 shows the elementary A (the center A PT shown in Figure 2), where the anchor has been moved to the node at which the lexemes folded / VBD , hung / VBD , and bought / faded text. These are nodes and edges that are removed from the A change in anchor placement. The elementary tree for dry / the fact that at least some of the nouns that dry / JJ modifies can be the direct object of a verb, or the subject of a verb. When we move the anchor, as shown at the top of Figure 3, we resolve this ambiguity to the case where the noun being modified is a direct object.
The incompatible parts of the A PT are removed. This corresponds to restricting the co-occurrence types of composed A PT s to those that belong to the set R the case for elementary A PT s. For example, note that in the upper A neither the path DOBJ  X  NSUBJ from the node labeled with bought / the node labeled caused / VBD , nor the path DOBJ  X  SUBJ  X  bought / VBD and folded / VBD to the node labeled laughter / that dry / JJ can plausibly modify as direct objects have elementary A sense  X  X ompatible X  with the A PT produced by shifting the anchor node as illustrated at the top of Figure 3. An example is the A PT for folded /
Figure 2. Loosely speaking, this means that, when applied to the same co-occurrence type, the A PT in Figure 3 and the A PT at the bottom of Figure 2 are generally expected to give sets of lexemes with related elements.
 have, in effect, aligned all of the nodes of the A PT s for dry / 736 (a) we bought (b) (c) he folded (c) (d) (e) the man hung up (f) a boy bought nodes they correspond to in the A PT for folded / VBD . Not only does this make it possible, in principle at least, to establish whether or not the composition of dry / and folded / VBD is plausible, but it provides the basis for the contextualization of A we now explain.
 characterization of the distributional semantics of the lexeme in context. There are two distinct aspects to the contextualization of A PT s, both of which can be captured through A
PT composition: co-occurrence filtering  X  X he down-weighting of co-occurrences that are not compatible with the way the lexeme is being used in its current context; and co-occurrence embellishment  X  X he up-weighting of compatible co-occurrences that appear in the A PT s for the lexemes with which it is being composed.
 through A PT composition. The process of composing the elementary A lexemes that appear in a phrase involves two distinct steps. First, the elementary A for each of the lexemes being composed are aligned in a way that is determined by the dependency tree for the phrase. The result of this alignment of the elementary A that each node in one of the A PT s is matched up with (at most) one of the nodes in each of the other A PT s. The second step of this process involves merging nodes that have been matched up with one another in order to produce the resulting composed A
PT that represents the distributional semantics of the dependency tree. It is during this second step that we are in a position to determine those co-occurrences that are compatible across the nodes that have been matched up.
 shown in the upper center of the figure. In the lower right, the figure shows the full A
PT that results from merging the six aligned A PT s, one for each of the lexemes and around the dependency tree we show the elementary A PT dependency tree. Note that the tree shown in gray within the A dependency tree are labeled with single lexemes, whereas each node of the A labeled by a weighted lexeme multiset. The lexeme labeling a node in the dependency tree is one of the lexemes found in the weighted lexeme multiset associated with the corresponding node within the A PT . We refer to the nodes in the composed A come from nodes in the dependency tree (the gray nodes) as the internal context , and the remaining nodes as the external context .
 of the anchor. The specific adjustments to the anchor locations are determined by the dependency tree for the phrase. For example, Figure 5 shows a dependency analysis of 738 folded / VBD dry / JJ clothes / NNS the phrase folded dry clothes . To align the elementary A we do the following: folded / VPD , which have been aligned as determined by the dependency tree shown in
Figure 5. Each column of lexemes appears at nodes that have been aligned with one three nodes have been aligned: (i) the node in the elementary A bought / VBD and folded / VBD appear; (ii) the node in the elementary A the elementary A PT for folded / VBD , that is, the node at which folded / the second phase of composition, these three nodes are merged together to produce a single node in the composed A PT .
 notion of A PT alignment. We do this by first defining so-called offset A formalizes the idea of adjusting the location of an anchor. We then define how to align all of the A PT s for the lexemes in a phrase based on a dependency tree. 4.2 Offset A PT s
Given some offset,  X  , a string in R  X  R  X  , the A PT A when offset by  X  is denoted A setting an A PT by  X  involves moving the anchor to the position reached by following the path  X  from the original anchor position. In order to define A
A representation, we need to specify the  X  0 such that A  X  (  X  ) and A (  X  (weighted lexeme multiset). 740 occurrence type reduction operator that was introduced in Section 2.2. Given a string  X  in R  X  R  X  and an A PT A , the offset A PT A  X  is defined as follows. For each  X   X  R and w  X  V : or equivalently, for each  X   X  R  X  R  X  :
As required, Equation (7) defines A  X  by specifying the weighted lexeme multiset we obtain when A  X  is applied to co-occurrence type  X  as being the lexeme multiset that A produces when applied to the co-occurrence type  X  (  X  X  ).
 call this A PT A . Note that A is anchored at the node where the lexeme dry / Consider the A PT produced when we apply the offset at the top of Figure 3. Let us refer to this A at which the lexemes bought / VDB and folded / VBD appear. Now we show how the two
Equation (8). In both cases the offset  X  = AMOD  X  DOBJ . path offset,  X   X  1 , to all of the co-occurrence types in A and then repeatedly applying addresses a node in A , then  X  0 addresses a node in A  X  iff  X  4.3 Syntax-Driven A PT Alignment We now make use of offset A PT s, as defined in Equation (7), as a way to align all of the A
PT s associated with a dependency tree. Consider the following scenario: resulting A PT is formed by merging n multisets, one from each of the elements of tion 4.4.
 whole, the same A PT , when associated with different anchors (i.e., when offset in some appropriate way) provides a representation of each of the contextualized lexemes that appear in the tree.
 dependency tree t , denoted k w i ; t k , is the A PT that satisfies the equality:
Alternatively, this can also be expressed with the equality: distributional semantics of a dependency tree to be the A of that tree that has been contextualized by the other lexemes appearing below it in the tree.
 are composed at once to produce the resulting (composed) A alternative strategies that could be formulated. One possibility is fully incremental left-elementary A PT s for the first two lexemes are composed, with the resulting A being composed with the elementary A PT for the third lexeme, and so on. It is always 742 possible to compose A PT s in this fully incremental way, whatever the structure in the dependency tree. The tree structure, however, is critical in determining how the adjacent A
PT s need to be aligned. 4.4 Merging Aligned A PT s We now turn to the question of how to implement the function F that appears in
Equation (9). F takes a set of n aligned A PT s, { A 1 , ... A dependency tree t . It merges the A PT s together node by node to produce a single A
F { A 1 , ... A n } , that represents the semantics of the dependency tree. Our discussion, therefore, addresses the question of how to merge the multisets that appear at nodes that are aligned with each other and form the nodes of the A tributionally compatible with the lexeme given the corpus. When lexemes in some phrase are composed, our objective is to capture the extent to which the co-occurrences arising in the elementary A PT s are mutually compatible with the phrase as a whole. position to determine the extent to which co-occurrences are mutually compatible: Co-occurrences that need to be compatible with one another are brought together through the alignment. We consider two alternative ways in which this can be achieved. ibility of co-occurrences. In particular, a co-occurrence is only deemed to be compatible with the composed lexemes to the extent that is distributionally compatible with the intersection. In particular, for all  X   X  R  X  R  X  and w 0  X  V : it would particularly benefit from distributional smoothing (Dagan, Pereira, and Lee 1994), which can be used to improve plausible co-occurrence coverage by inferring co-occurrences in the A PT for a lexeme w based on the co-occurrences in the A distributionally similar lexemes.
 occurrence for each of the lexemes being composed. In particular, for all  X   X  ( R  X  R ) and w 0  X  V :
Although this clearly achieves co-occurrence embellishment, whether co-occurrence filtering is achieved depends on the weighting scheme being used. For example, if negative weights are allowed, then co-occurrence filtering can be achieved. of our proposal, and therefore worth dwelling on. In Section 4.1, when discussing
Figure 4, we made reference to the notions of internal and external context. The internal context of a composed A PT is that part of the A the dependency tree that generated the composed A PT . One might have expected that the only lexeme appearing at an internal node is the lexeme that appears at the corre-sponding node in the dependency tree. However, this is absolutely not the objective:
At each node in the internal context, we expect to find a set of alternative lexemes that are, to varying degrees, distributionally compatible with that position in the A expect that a lexeme that is distributionally compatible with a substantial number of the lexemes being composed will result in a distributional feature with non-zero weight in the vectorized A PT . There is, therefore, no distinction being made between internal and external nodes. This enriches the distributional representation of the contextualized lexemes, and overcomes the potential problem arising from the fact that as larger and larger units are composed, there is less and less external context around to characterize distributional meaning. 5. Experiments In this section we consider some empirical evidence in support of A consider some of the different ways in which A PT present a number of case studies showing the disambiguating effect of A in adjective X  X oun composition. Finally, we evaluate the model using the phrase-based compositionality benchmarks of Mitchell and Lapata (2008, 2010). 5.1 Instantiating A PT s
We have constructed A PT lexicons from three different corpora. during composition. First there is the composition operation itself. We have explored variants that take a union of the features such as add and max and variants that take an intersection of the features such as mult , min , and intersective add , where intersective add ( a , b ) = a + b iff a &gt; 0 and b &gt; 0; 0 otherwise. being composed. The weights in the elementary A PT s can be counts, probabilities, or some variant of PPMI or other association function. Although it is generally accepted determination of lexical similarity, there is a choice over whether these weights should 744 In the instantiation that we refer to as as compose first , A
These are composed and transformed to PPMI scores before computing cosine similar-ities. In the instantiation that we refer to as compose second , A scores.
 lation. First, it is common (Levy, Goldberg, and Dagan 2015) to delete rare words when calculations because they co-occur with very few of the targets. Their inclusion will tend the other hand, improves efficiency. In other experiments, we have found that a feature frequency threshold of 1,000 works well. On a corpus the size of Wikipedia (1 . 5 billion (when including only first-order paths) and approximately 230,000 dimensions (when including paths up to order 2).
 word embeddings on word similarity tasks.
 is analogous to the use of negative sampling in word embeddings, can be advanta-geous. When shifting PMI, all values are shifted down by log k before the threshold is applied.
 weighting, and inverse path length or harmonic function (which is equivalent to the dynamic context window used in many neural implementations such as GloVe [Pennington, Socher, and Manning 2014]). 5.2 Disambiguation Here we consider the differences between using aligned and unaligned A resentations as well as the differences between using F ing out adjective X  X oun (AN) composition. From the clean wiki corpus described in
Section 5.1, a small number of high-frequency nouns were chosen that are ambigu-ous or broad in meaning together with potentially disambiguating adjectives. We use the compose first option where composition is carried out on A probabilities.
 composition with the disambiguating adjective are then examined. In order to calcu-late similarities, contexts are weighted using the variant of PPMI advocated by Levy,
Goldberg, and Dagan (2015), where cds is applied with  X  = 0 . 75. However, no shift is applied to the PPMI values because we have found shifting to have little or negative effect when working with relatively small corpora. Similarity is then computed using the standard cosine measure. For illustrative purposes the top ten neighbors of each word or phrase are shown, concentrating on ranks rather than absolute similarity scores.
 A PT representations when the noun shoot is placed in the contexts of green and six-week .
Boldface is used in the entries of compounds where a neighbor appears to be highly suggestive of the intended sense and where it has a rank higher or equal to its rank in the entry for the uncontextualized noun. In this example, it is clear that merging the unaligned A PT representations provides very little disambiguation of the target noun.
This is because typed co-occurrences for an adjective mostly belong in a different space to typed co-occurrences for a noun. Addition of these spaces leads to significantly lower absolute similarity scores, but little change in the ranking of neighbors. Although we only show one example here, this observation appears to hold true whenever words with different part of speech tags are composed. Intersection of these spaces via F as illustrated by Table 3.
 disambiguating effect is even more apparent when F INT representations (see Table 3).
 ing aligned A PT representations. Again, boldface is used in the entries of compounds where a neighbor appears to be highly suggestive of the intended sense and where it has a rank higher or equal to its rank in the entry for the uncontextualized noun. In these examples, we can see that both F UNI and F INT appear to be effective in carrying out some disambiguation. Looking at the example of musical group , both F the relative similarity of band and music to group when it is contextualized by musical . However, F INT also leads to a number of other words being selected as neighbors that 746 not the case when F UNI is used X  X he other neighbors still appear related to the general meaning of group . This trend is also seen in some of the other examples such as ethnic group , human body , and magnetic field . Further, even when F neighbors selected appear to be higher frequency, more general words than when F is used.
 has on the number of non-zero dimensions in the composed representations. Ignoring the relatively small effect the feature association function may have on this, it is obvious that F UNI should increase the number of non-zero dimensions, whereas F decrease the number of non-zero dimensions. In general, the number of non-zero di-mensions is highly correlated with frequency, which makes composed representations based on F UNI behave like high-frequency words and composed representations based on F INT behave like low-frequency words. Further, when using similarity measures neighbors of high-frequency entities (with a large number of non-zero dimensions) are other high-frequency entities (also with a large number of non-zero dimensions). Nor is it unusual to find that the neighbors of low-frequency entities (with a small number of non-zero dimensions) are other low-frequency entities (with a small number of non-zero dimensions). Weeds, Weir, and McCarthy (2004) showed that frequency is also a surprisingly good indicator of the generality of the word. Hence F general neighbors and F INT leads to more specific neighbors.
 amples where only two words are composed, using F INT position of an entire sentence would tend to lead to very sparse representations. The such as F INT must necessarily only include the lexemes actually used in the sentence. F
UNI , on the other hand, will have added to these internal representations, suggesting 748 that those internal (and external) contexts that are not supported by a majority of the lexemes in the sentence will tend to be considered insignificant and therefore will be ignored in similarity calculations. By using shifted PPMI, it should be possible to further reduce the number of non-zero dimensions in a representation constructed using F observed. 5.3 Phrase-Based Composition Tasks Here we look at the performance of one instantiation of the A benchmark tasks for phrase-based composition. data set, introduced by Mitchell and Lapata (2010), which contains human similarity judgments for adjective X  X oun (AN), noun X  X oun (NN), and verb X  X bject (VO) combina-tions on a seven-point rating scale. It contains 108 combinations in each category such
This data set has been used in a number of evaluations of compositional meth-ods including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann and Blunsom (2013), and Kiela and Clark (2014). For example, Blacoe and
Lapata (2012) show that multiplication in a simple distributional space (referred to here as an untyped VSM ) outperforms the distributional memory (DM) method of Baroni and
Lenci (2010) and the neural language model (NLM) method of Collobert and Weston (2008).
 appear to be the calculation of Spearman X  X  rank correlation coefficient  X  between model phrase pairs being judged by 6 humans, this would lead to a data set containing 648 data points. The procedure is discussed at length in Turney (2012), who argues that this method tends to underestimate model performance. Accordingly, Turney explicitly uses a different procedure where a separate Spearman X  X   X  is calculated between the model scores and the scores of each participant. These coefficients are then averaged to give the performance indicator for each model. Here, we report results using the original
M&amp;L method (see Table 5). We found that using the Turney method, scores were typically higher by 0 . 01 to 0 . 04. If model scores are evaluated against aggregated human scores, the values reported here.

This is the same corpus used by Mitchell and Lapata (2010) and for the best performing algorithms in Blacoe and Lapata (2012). We note that the larger concat corpus was used by Blacoe and Lapata (2012) in the evaluation of the DM algorithm (Baroni and Lenci 2010). We use the compose second option, where the elementary A PPMI. With regard to the different parameter settings in the PPMI calculation (Levy, Goldberg, and Dagan 2015), we tuned on a number of popular word similarity tasks:
MEN (Bruni, Tran, and Baroni 2014); WordSim-353 (Finkelstein et al. 2001); and SimLex-999 (Hill, Reichart, and Korhonen 2015). In these tuning experiments, we found that context distribution smoothing gave mixed results. However, shifting PPMI ( k = 10) gave optimal results across all of the word similarity tasks. Therefore we report results we report results for both F UNI and F INT . Results are shown in Table 5. F F F F
PPMI by log 10 consistently improves results for F UNI , but has a large negative effect corpus. Shifting PPMI reduces the number of non-zero dimensions in each vector, which increases the likelihood of a zero intersection. In the case of AN composition, all of the intersections were zero for this setting, making it impossible to compute a correlation. outperforms DM and NLM as tested by Blacoe and Lapata (2012). This method of com-position also achieves close to the best results in Mitchell and Lapata (2010) and Blacoe and Lapata (2012). It is interesting to note that our model does substantially better than noun composition. Exploring why this is so is a matter for future research. We have undertaken experiments with a larger corpus and a larger range of hyper-parameter settings, which indicate that the performance of the A PT icantly. However, these results are not presented here, because an equitable comparison with existing models would require a similar exploration of the hyper-parameter space across all models being compared. 5.3.2 Experiment 2: The M&amp;L2008 Data Set. The second experiment uses the M&amp;L2008 data set, introduced by Mitchell and Lapata (2008), which contains pairs of intransitive sensitives together with human judgments of similarity. The data set contains 120 unique subject, verb, landmark triples with a varying number of human judgments similarity of the verb and the landmark given the potentially disambiguating context of the subject. For example, in the context of the subject fire one might expect glowed to be close to burned but not close to beamed . Conversely, in the context of the subject face one might expect glowed to be close to beamed and not close to burned . 750 and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental separate points are created for each human annotator, as discussed in Section 5.3.1. on this data set. In the evaluation of Dinu, Pham, and Baroni (2013), the lexical function algorithm, which learns a matrix representation for each functor and defines composi-tion as matrix-vector multiplication, was the best-performing compositional algorithm at this task. With optimal parameter settings, it achieved around  X  = 0 . 26. In this eval-uation, the full additive model of Guevara (2010) achieved  X  &lt; 0 . 05. we used the same corpus to construct our A PT lexicons, namely, the concat corpus described in Section 5.1. Otherwise, the A PT lexicon was constructed as described in PPMI. Results are shown in Table 6.
 that was the best performing model in the evaluation of Dinu, Pham, and Baroni (2013). In that evaluation, the lexical function model achieved between 0.23 and 0.26, depending on the parameters used in dimensionality reduction. Using vanilla PPMI, without any context distribution smoothing or shifting, F to be considered as features. This makes sense when using an additive model such as F achieves  X  = 0 . 23, which equals the performance of the multi-step regression algorithm performance. This is largely because of the intersective nature of the composition F F F F operation X  X f shifting PPMI removes a feature from one of the unigram representations, it cannot be recovered during composition. 6. Related Work
Our work brings together two strands usually treated as separate though related prob-lems: representing phrasal meaning by creating distributional representations through composition; and representing word meaning in context by modifying the distributional representation of a word. In common with some other work on lexical distributional similarity, we use a typed co-occurrence space. However, we propose the use of higher-order grammatical dependency relations to enable the representation of phrasal mean-ing and the representation of word meaning in context. 6.1 Representing Phrasal Meaning
The problem of representing phrasal meaning has traditionally been tackled by tak-ing vector representations for words (Turney and Pantel 2010) and combining them tence. Mitchell and Lapata (2008, 2010) found that simple additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgments of simple paired phrases.
 the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Although using a neural network architecture, the intuitions behind such distributed representations
Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics. For example, the CBOW architecture pre-dicts the current word based on context (which is viewed as a bag-of-words) and the skip-gram architecture predicts surrounding words given the current word. Mikolov dimensional representations for words that appear to capture both syntactic and seman-tic regularities. Mikolov et al. (2013b) also demonstrated the possibility of composing skip-gram representations using addition. For example, they found that adding the vectors for Russian and river results in a very similar vector to the result of adding the vectors for Volga and river . This is similar to the multiplicative model of Mitchell and
Lapata (2008) since the sum of two skip-gram word vectors is related to the product of two word context distributions.
 operation, the underlying framework is very different. Specifically, the actual vectors added depend not just on the form of the words but also their grammatical relationship within the phrase or sentence. This means that the representation for, say, glass window is not equal to the representation of window glass . The direction of the between the words leads to a different alignment of the A different representation for the phrases.
 and machine learning, use syntactic information, and specialize the data structures to the task in hand. For adjective X  X oun phrase composition, Baroni and Zamparelli (2010) 752 and Guevara (2010) borrowed from formal semantics the notion that an adjective acts as a modifying function on the noun. They represented a noun as a vector, an adjective as a matrix, which could be induced from pairs of nouns and adjective noun phrases, and composed the two using matrix-by-vector multiplication to produce a vector for the noun phrase. Separately, Coecke, Sadrzadeh, and Clark (2011) proposed a broader compositional framework that incorporated from formal semantics the notion of func-tion application derived from syntactic structure (Montague 1970; Lambek 1999). These two approaches were subsequently combined and extended to incorporate simple tran-sitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al. 2013).
 vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and however, was made much more flexible by requiring and using task-specific labeled training data to create task-specific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al. 2012, 2013).
 large amounts of training data. For example, running regression models to accurately number of exemplar compositions containing that adjective or verb. Socher X  X  MV-RNN model further requires task-specific labeled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus.
 theoretic semantic framework, incorporating a generative model that assigned proba-bilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distribu-tional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and
Mooney (2011) combined word-level distributional vector representations with logic-based representation using a probabilistic reasoning framework. Lewis and Steedman lexicon for Combinatory Categorial Grammar (CCG; Steedman 2000), which first maps natural language to a deterministic logical form and then performs a distributional clus-tering over logical predicates based on arguments. The CCG formalism was also used by Hermann and Blunsom (2013) as a means for incorporating syntax-sensitivity into vector space representations of sentential semantics based on recursive auto-encoders (Socher et al. 2011a, 2011b). They achieved this by representing each combinatory step in a CCG parse tree with an auto-encoder function, where it is possible to parameterize both the weight matrix and bias on the combinatory rule and the CCG category. relations in order to determine phrasal-level similarity. This work uses two different captures instances where the components of a composed noun phrase bore similarity to another word through a mix of those similarity types. Crucially, it views similarity of phrases as a function of the similarities of the components and does not attempt to derive modified vectors for phrases or words in context. Dinu and Thater (2012) also compared computing sentence similarity via additive compositional models with an alignment-based approach, where sentence similarity is a function of the similarities of component words, and simple word overlap. Their results showed that a model based on a mixture of these approaches outperformed all of the individual approaches on a number of textual entailment data sets. 6.2 Typed Co-occurrence Models
In untyped co-occurrence models, such as those considered by Mitchell and Lapata (2008, 2010), co-occurrences are simple, untyped pairs of words that co-occur together (usually within some window of proximity but possibly within some grammatical relation). The lack of typing makes it possible to compose vectors through addition and multiplication. However, in the computation of lexical distributional similarity using grammatical dependency relations, it has been typical (Lin 1998; Lee 1999; Weeds and
Weir 2005) to consider the type of a co-occurrence (for example, does dog occur with eat as its direct object or its subject?) as part of the feature space. The distinction between vector spaces based on untyped and typed co-occurrences was formalized by Pad  X  o and Lapata (2007) and Baroni and Lenci (2010). In particular, Baroni and Lenci showed that typed co-occurrences based on grammatical relations were better than untyped co-occurrences for distinguishing certain semantic relations. However, as shown by Weeds,
Weir, and Reffin (2014), it does not make sense to compose typed features based on first-order dependency relations through multiplication and addition, because the vector spaces for different parts of speech are largely non-overlapping.
 node words. This allowed words that are only indirectly related within a sentence to be considered as co-occurring. For example, in a lorry carries apples , there is a path of length 2 between the nouns lorry and apples via the node carry . However, they also used a word-based basis mapping, which essentially reduces all of the salient grammatical for lorry , these would be mapped to the basis elements carry and apples , respectively. 6.3 Representing Word Meaning in Context
A long-standing topic in distributional semantics has been the modification of a canon-
Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess 1996; Erk and and Pinkal 2010, 2011; Van de Cruys, Poibeau, and Korhonen 2011; Erk 2012). been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad  X  o 2008; Thater, Dinu, and Pinkal 2009; Thater, F  X  urstenau, and Pinkal 2010, 2011).
 that computes the meaning of a word in the context of another word via selectional preferences. This approach was shown to work well at ranking paraphrases taken from the SemEval-2007 lexical substitution task (McCarthy and Navigli 2007). In the Erk and
Pad  X  o approach, the meaning of ball in the context of the phrase catch ball is computed by 754 that can be caught ). Although this approach is based on very similar intuitions to ours, it is in fact quite different. The lexical vector that is modified is not the co-occurrence vector, as in our model, but a vector of neighbors computed from co-occurrences. For example, the lexical vector for catch in the Erk and Pad  X  o approach might contain throw , catch , and organize . These neighbors of catch are then combined with verbs that have been seen with ball in the direct object relation using vector addition or component-wise multiplication. Thus, it is possible to carry out this approach with reference only to observed first-order grammatical dependency relationship. In their experiments, they used the  X  X ependency-based X  vector space of Pad  X  o and Lapata (2007) where target and context words are linked by a valid dependency path (i.e., not necessarily a single first-order grammatical relation). However, higher-order dependency paths were purely used to provide extra contexts for target words, than would be seen in a traditional first-order dependency model, during the computation of neighbor sets. Further, the Erk and
Pad  X  o approach does not construct a representation of the phrase because this model is focused on lexical disambiguation rather than composition and it is not obvious how one would carry out further disambiguations within the context of a whole sentence. considered a broader range of operations for combining two vectors where individual vector components are reweighted. Specifically, they found that reweighting vector components based on the distributional similarity score between words defining vector components and the observed context words led to improved performance at ranking paraphrases.
 words typically have different syntactic environments, making it difficult to combine information in the respective vectors. They build on Thater, Dinu, and Pinkal (2009), where the meaning of argument nouns was modeled in terms of the predicates they co-occur with (referred to as a first-order vector) and the meaning of predicates in terms of second-order co-occurrence frequencies with other predicates. These predicate vectors can be obtained by adding argument vectors. For example, the verb catch will contain counts on the dimension for kick introduced by the direct-object ball and counts on verbs, thus making this notion of second-order dependency compatible with that used in work on word sense discrimination (Sch  X  utze 1998) rather than referring to second-order (or higher-order) grammatical dependencies as in this work. Contextualization can then be achieved by multiplication of a second-order predicate vector with a first-order argument vector because this selects the dimensions that are common to both.
Thater, F  X  urstenau, and Pinkal (2010) presented a more general model where every word is modeled in terms of first-order and second-order co-occurrences and demonstrate high performance at ranking paraphrases. 7. Directions for Future Work 7.1 Representations
There are a number of apparent limitations of our approach that are simply a reflection of our decision to adopt dependency-based syntactic analysis. formations, compound sentence structures) will disrupt sentence-level comparisons using a simple A PT structure based on surface dependency relations, but this can be addressed, for example, by syntax-based pre-processing. The A in this regard.
 modifiers. Hence the phrases happiest blonde person and blonde happiest person receive the same dependency representation and therefore also the same semantic representation.
However, we believe that our approach is flexible enough to be able to accommodate a more sensitive grammar formalism that does allow for distinctions in modifier scope grammar formalisms, including CCG (Steedman 2000).
 trend of working with prediction-based word embeddings. Although there has been initial evidence (Baroni, Dinu, and Kruszewski 2014) that prediction-based methods are superior to count-based methods at the lexeme level (e.g., for synonym detection and concept categorization), it has also been shown (Levy and Goldberg 2014) that the skip-gram model with negative sampling as introduced in Mikolov et al. (2013a) is equivalent to implicit factorization of the PPMI matrix. Levy, Goldberg, and Dagan (2015) also demonstrated how traditional count-based methods could be improved by transferring hyperparameters used by the prediction-based methods (such as context distribution smoothing and negative sampling). This led to the count-based methods outperforming the prediction-based methods on a number of word similarity tasks. A find a way to produce lower dimensionality A PT representations without destroying the necessary structure that drives composition. The advantages of this from a com-putational point of view are obvious. It remains to be seen what effect the improved generalization also promised by dimensionality reduction will have on composition via A union and intersection can lead to nearest neighbors that are clearly disambiguating. On benchmark phrase-based composition tasks, the performance of union in A believe that the performance of intersection in A PT composition is currently limited by
Even given a very large corpus, there are always many plausible co-occurrences that have not been observed. One possible solution, which we explore elsewhere, is to smooth the word representations using their distributional neighbors before applying an intersective composition operation. 7.2 Applications In Section 5.2, we demonstrated the potential for using A disambiguation / induction. Uncontextualized, elementary A corpus-determined mixture of co-occurrences referencing different usages. The A generated by a dependency tree, however, provides contextualized lexeme representa-tions where the weights have been adjusted by the influence of the contextual lexemes weighted, and the co-occurrences found in other circumstances down-weighted. In 756 other words, A PT structures automatically perform word sense induction on lexeme-level representations and this is demonstrable through the lexeme similarity measure.
For example, we observed that the contextualized lexeme representation of body in the A PT constructed by embedding it in the phrase human body had a relatively high similarity to the uncontextualized representation of brain and a relatively low similarity to council , whereas the equivalent lexeme representation for body embedded in the A PT constructed for the phrase legislative body showed the reverse pattern. semantic relations into a single notion of similarity. For example, when comparing representations based on grammatical dependency relations, the most similar word to an adjective such as hot will usually be found to be its antonym cold . This is because hot and cold are both used to modify many of the same nouns. However, if, as in the A framework, the representation of cold includes not only the direct dependents of cold , but more differences between its representation and that of hot might be found. One would imagine that the things that are done to hot things are more different to the things that are done to cold things than they are to the things that are done to very warm things . Further, the examples in Section 5.2 raise the possibility that different composition operations might be used to distinguish different semantic relations including hypernyms, hy-ponyms, and co-hyponyms. For example, F UNI tends to lead to more general neighbors (e.g., hypernyms) and F INT tends to lead to more specific neighbors (e.g., hyponyms). ous measure of the appropriateness / plausibility of a complete phrase or sentence, based on a combination of semantic and syntactic dependency relations. A measure the plausibility of a lexeme when embedded in a dependency tree, suggest-as the Microsoft Research Sentence Completion Challenge (Zweig and Burges 2012).
Here, the objective is to identify the word that will fill out a partially completed sentence in the best possible way. For example, is flurried or profitable the best completion of the sentence below:
We can compose the A PT s for the partially completed sentence. Comparing the result measurement of which candidate is more plausible. An improved language model has implications for parsing, speech recognition, and machine translation.
 that represents an entire phrase or sentence. The composed A provides such a structure, but leaves open the question as to how this structure might be exploited for phrase-level or sentence-level semantic comparison.
 tation of the whole dependency tree but also contextualized (vector) representations for the lexemes in the dependency tree. This makes available to us any analytical technique that requires separate analysis of lexical components of the phrase or sentence. How-ever, this leads to the problem of how to read the structure at the global phrase/sentence-level.
 from the A PT anchored at the head of the phrase or sentence being considered. Thus, the phrasal vector for a red rose would be created taking the node containing rose as the anchor. In other words, the vector representation of the phrase a red rose will be the same as the contextualized representation of rose . Similarly, the vector representation for the sentence he took the dog for a walk will be the same as the contextualized representation of the verb took .
 phrase-or sentence-level vectors produced in this manner will provide some coher-ent numerical measure of distributional similarity. This approach should be useful for paraphrase recognition tasks. For example, in order to identify good candidate paraphrases for questions in a question-answering task, Berant and Liang (2014) use a paraphrase model based on adding word embeddings constructed using the CBOW model of Mikolov et al. (2013). Although the authors achieve state of the art using a mixture of methods, a paraphrase model based on the addition of vectors of untyped co-occurrences alone cannot distinguish meanings where syntax is important. For exam-ple, the sentences Oswald shot Kennedy and Kennedy shot Oswald would have the same representations. On the other hand, A PT composition is syntax-driven and will provide a representation of each sentence that is sensitive to lexical meaning and syntax. other syntax-driven proposals, is that the same structure is used to represent words, words and phrases of different lengths can easily be compared within our model. An adjective X  X oun compound such as male sibling is directly comparable with the single noun brother . Further, there is no need for there to be high similarity between aligned components of phrases or sentences. For example, the phrase female scholar can be of their external contexts. 8. Conclusions
This article presents a new theory of compositional distributional semantics. It uses a single structure, the A PT , which can represent the distributional semantics of lexemes, phrases, and even sentences. By retaining higher-order grammatical structure in the representations of lexemes, composition captures mutual disambiguation and mutual generalization of constituents. A PT s allow lexemes and phrases to be compared in benchmark phrase-based composition tasks.
 word sense induction, word sense disambiguation, parse reranking, dependency pars-ing and language modeling more generally, and also paraphrase recognition. Further suited to each of these applications.
 Acknowledgments 758 References 760
