 Outlier mining in d -dimensional point sets is a fundamental and well studied data mining task due to its variety of ap-plications. Most such applications arise in high-dimensional domains. A bottleneck of existing approaches is that implicit or explicit assessments on concepts of distance or nearest neighbor are deteriorated in high-dimensional data. Follow-ing up on the work of Kriegel et al. (KDD  X 08), we inves-tigate the use of angle-based outlier factor in mining high-dimensional outliers. While their algorithm runs in cubic time (with a quadratic time heuristic), we propose a novel random projection-based technique that is able to estimate the angle-based outlier factor for all data points in time near-linear in the size of the data. Also, our approach is suitable to be performed in parallel environment to achieve a parallel speedup. We introduce a theoretical analysis of the quality of approximation to guarantee the reliability of our estima-tion algorithm. The empirical experiments on synthetic and real world data sets demonstrate that our approach is effi-cient and scalable to very large high-dimensional data sets. H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms outlier detection, high-dimensional, angle-based, random pro-jection, AMS Sketch
Outlier mining is a fundamental and well studied data mining task due to the variety of domain applications, such as fraud detection for credit cards, intrusion detection in network traffic, and anomaly motion detection in surveil-lance video, etc. Detecting outliers is to identify the objects that considerably deviate from the general distribution of the data. Such the objects may be seen as suspicious objects due to the different mechanism of generation. For example, consider the problem of fraud detection for credit cards and the data set containing the card owners X  transactions. The transaction records consist of usage profiles of each customer corresponding the purchasing behavior. The purchasing be-havior of customer usually changes when the credit card is stolen. The abnormal purchasing patterns may be reflected in transaction records that contain high payments, high rate of purchase or the orders comprising large numbers of du-plicate items, etc.

Most such applications arise in very high-dimensional do-mains. For instance, the credit card data set contains trans-action records described by over 100 attributes [21]. To de-tect anomalous motion trajectories in surveillance videos, we have to deal with very high representational dimensionality of pixel features of sequential video frames [16]. Because of the notorious  X  X urse of dimensionality X , most proposed ap-proaches so far which are explicitly or implicitly based on the assessment of differences in Euclidean distance metric between objects in full-dimensional space do not work effi-ciently. Traditional algorithms to detect distance-based out-liers [13, 19] or density-based outliers [6, 18] suffer from the high computational complexity for high-dimensional near-est neighbor search. In addition, the higher the dimension-ality is, the poorer the discrimination between the nearest and the farthest neighbor becomes [1]. That leads to a sit-uation where most of the objects in the data set appear likely to be outliers based on the evaluation on their neigh-borhood using concepts like distance or nearest neighbor in high-dimensional space.

In KDD 2008, Kriegel et al. [14] proposed a novel out-lier ranking approach based on the variance of the angles between an object and all other pairs of objects. This ap-proach, named Angle-based Outlier Detection (ABOD), eval-uates the degree of outlierness of each object on the assess-ment of the broadness of its angle spectrum. The smaller the angle spectrum of a object to other pairs of objects is, the more likely it is an outlier. Because  X  angles are more stable than distances in high-dimensional space  X  [15], this approach does not substantially deteriorate in high-dimensional data. In spite of many advantages of alleviating the effects of the  X  X urse of dimensionality X  and being a parameter-free mea-sure, the time complexity taken to compute ABOD is signifi-cant with O ( dn 3 ) for a data set of n objects in d -dimensional space. To avoid the cubic time complexity, the authors also proposed heuristic approximation variants of ABOD for ef-ficient computations. These approximations, however, still rely on nearest neighbors and require high computational complexity with O ( dn 2 ) used in sequential search for neigh-bors. Moreover, there is no analysis to guarantee the accu-racy of these approximations.

In this paper, we develop a near-linear time algorithm to approximate the variance of angles for each data object. Our proposed approach works in O ( n log n ( d + log n )) time for a data set of size n in d -dimensional space, and outputs an un-biased estimator of variance of angles for each object. The main technical insight is the combination between random hyperplane projections [11, 7] and AMS Sketch on product domains [12, 5], which enables us to reduce the computa-tional complexity from cubic time complexity in the na  X   X ve approach to near-linear time complexity in the approxima-tion solution. Another advantage of our algorithm is the suitability for parallel processing. In fact, we can achieve a nearly linear (in the number of processors used) parallel speedup of running time. We give a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm. The empirical experiments on real world and synthetic data sets demonstrate that our approach is efficient and scalable to very large high-dimensional data.
The organization of the paper is as follows. In Section 2, we briefly review related work. The algorithm descrip-tion including preliminaries and the proposed approach is presented in Section 3. Section 4 introduces the analysis of the accuracy of our approach. In Section 5, we show experi-mental evaluations of our proposed approach with synthetic and real world data sets. Finally, we make some conclusions about our work in Section 6.
A good outlier measure is the key aspect for achieving effectiveness and efficiency when managing the outlier min-ing tasks. A great number of outlier measures have been proposed, including global and local outlier models. Global outlier models typically take the complete database into ac-count while local outlier models only consider a restricted surrounding neighborhood of each data object.

Knorr and Ng [13] proposed a simple and intuitive distance-based definition of outlier as an earliest global outlier model in the context of databases. The outliers with respect to parameter k and  X  are the objects that have less than k neighbors within distance  X  . A variant of the distance-based notion is proposed in [19]. This approach takes the distance of a object to its k th nearest neighbor as its outlier score and retrieve the top m objects having the highest outlier scores as the top m outliers. The distance-based approaches are based on the assumption, that the lower density region that the data object is in, the more likely it is an outlier. The basic algorithm to detect such distance-based outliers is the nested loop algorithm [19] that simply computes the dis-tance between each object and its k th nearest neighbor and retrieve top m objects with the maximum k th nearest neigh-bor distances. To avoid the quadratic worst case complex-ity of nested loop algorithm, several key optimizations are proposed in the literature. Such optimizations can be clas-sified based on the different pruning strategies, such as the approximate nearest neighbor search [19], data partitioning strategies [19] and data ranking strategies [4, 10, 20]. Al-though these optimizations may improve performance, they scale poorly and are therefore inefficient as the dimension-ality or the data size increases, and objects become increas-ingly sparse [2].

While global models take the complete database into ac-count and detect outliers based on the distances to their neighbors, local density-based models evaluate the degree of outlierness of each object based on the local density of its neighborhood. In many applications, local outlier mod-els give many advantages such as the ability to detect both global and local outliers with different densities and pro-viding the boundary between normal and abnormal behav-iors [6]. The approaches in this category assign to each ob-ject a local outlier factor as the outlierness degree based on the local density of its k -nearest neighbors [6] or the multi-granularity deviation of its -neighborhood [18]. In fact, these approaches implicitly rely on finding nearest neighbors for every object and typically use indexing data structures to improve the performance. Therefore, they are unsuitable for the requirements in mining high-dimensional outliers.
Due to the fact that the measures like distance or near-est neighbor may not be qualitatively meaningful in high-dimensional space, recent approaches focus on subspace pro-jections for outlier ranking [2, 17]. In other words, these ap-proaches take a subset of attributes of objects as subspaces into account. However, these approaches suffer from the difficulty of choosing meaningful subspaces [2] or the expo-nential time complexity in the data dimensionality [17]. As mentioned above, Kriegel at al. [14] proposed a robust angle-based measure to detect high-dimensional outliers. This ap-proach evaluates the degree of outlierness of each data object on the assessment of the variance of angles between itself and other pairs of objects. The smaller the variance of angles be-tween a object to the residual objects is, the more likely it is outlier. Because the angle spectrum between objects is more stable than distances as the dimensionality increases [15], this measure does not substantially deteriorate in high-dimensional data. However, the na  X   X ve and approximation approaches suffer from the high computational complexity with cubic time and quadratic time, respectively.
As elaborated above, using concepts like distance or near-est neighbor for mining outlier patterns in high-dimensional data is unsuitable. A novel approach based on the variance of angles between pairs of data points is proposed to alleviate the effects of  X  X urse of dimensionality X  [14]. Figure 1 shows the variance of angles for the three kinds of points. Notice that the border and inner points of the cluster have very large variance of angles whereas this value is much smaller for the outliers. In other words, the smaller the angle vari-ance of a point to the residual points is, the more likely it is an outlier. This is because the points inside the cluster are surrounded by other points in all possible directions while the points outside the cluster are positioned in particular di-rections. Therefore, we use the variance of angles (VOA) as the outlier factor to evaluate the degree of outlierness of each point of the data set. The proposed approaches in [14] do not deal directly with the variance of angles but variance of cosine of angles weighted by the corresponding distances of the points instead. We argue that the weighting factors are less and less meaningful in high-dimensional data due to the  X  X urse of dimensionality X . We expect the outlier rankings based on the variance of cosine spectrum with or without weighting factors and the variance of angle spectrum are likely similar in high-dimensional data. We therefore for-mulate the angle-based outlier factor using the variance of angles as follows:
Definition 1. Given a point set S  X  R d , | S | = n and a point p  X  S . For a random pair of different points a,b  X  S \{ p } , let  X  apb denote the angle between the difference vec-tors a  X  p and b  X  p . The angle-based outlier factor VOA( p ) is the variance of  X  apb : where MOA 2 and MOA 1 are defined as follows: MOA 2 ( p ) = It is obvious that the VOA measure is entirely free of pa-rameters and therefore is suitable for unsupervised outlier detection methods. The na  X   X ve ABOD algorithm computes the VOA for each point of the data set and return the top m points having the smallest VOA as outliers. However, the time complexity of the na  X   X ve algorithm is in O ( dn 3 cubic computational complexity means that it will be very difficult to mine outliers in very large data sets.
The general idea of our approach is to efficiently compute an unbiased estimator of the variance of the angles for each point of the data set. In other words, the expected value of our estimate is equal to the variance of angles and we show that it is concentrated around its expected value. These estimated values are then used to rank the points. The top m points having the smallest variances of angles are retrieved as top m outliers of the data set.

In order to estimate the variance of angles between a point and all other pairs of points, we first project the data set on the hyperplanes orthogonal to random vectors whose coor-dinates are independently chosen from the standard normal distribution N (0 , 1). Based on the partitions of the data set after projection, we are able to estimate the unbiased mean of angles for each point. We then approximate the second moment and derive its variance by using the AMS Sketches to summarize the frequency moments of the points projected on the random hyperplanes. The combination between ran-dom hyperplane projections and AMS Sketches on product domains enables us to reduce the computational complex-ity to O ( n log n ( d + log n )) time. In the following we start with some basic notions of random hyperplane projection and AMS Sketch, then propose our approach to estimate the variance of angles for each point of the data set.
Following Charikar [7], we take random vectors r 1 ,..., r  X  R d such that each coordinate is chosen independently from the standard normal distribution N (0 , 1).

For i = 1 ,...,t and points a,b,p  X  S consider the inde-pendent random variables For a vector r i we see that X ( i ) apb = 1 only if the vectors a  X  p and b  X  p are on different sides of the hyperplane orthogonal happens is proportional to  X  apb , as exploited in the seminal papers of Goemans and Williamson [11] and Charikar [7]. More precisely we have: Lemma 2. For all a,b,p,i , Pr[ X ( i ) apb = 1] =  X  apb / (2  X  ) . Note that we also have Pr[ X ( i ) bpa = 1] =  X  apb symmetry [11]. By using t random vectors r i , we are able to boost the accuracy of the estimator of  X  apb . In particular, random projections will be presented in the Section 4.
Alon at al. [3] described and analyzed a sketching ap-proach, called AMS Sketch, to estimate the second frequency moment of a high-dimensional vector.

Lemma 3. Given a high-dimensional vector w 1 ,  X  X  X  ,w q take a 4-wise independent vector s  X  { X  1 } q . The AMS Sketch is the value Z = P q i =1 s i w i . Define Y = Z E [ Y ] = P q i =1 w 2 i and Var [ Y ]  X  2 ( E [ Y ]) 2 .
Recently, Indyk and McGregor [12], and Braverman et al. [5] have considered AMS Sketches with two different 4-wise independent vectors for outer product. In this case, we view the matrix as vector of matrix elements.
Lemma 4. Given two different 4-wise independent vec-tors s 1 ,s 2  X  { X  1 } q . The AMS Sketch of an outer product ( uv ) , where by definition ( uv ) ij = u i v j , is: Define Y = Z 2 then E [ Y ] = P ij ( u i v j ) 2 or squared Frobe-nius norm of the outer product ( uv ) and Var [ Y ]  X  8 ( E [ Y ]) That is, the AMS sketch of the outer product is simply the product of the AMS sketches of the two vectors (using dif-ferent 4-wise independent random vectors).
To avoid the cubic time complexity, we propose a near-linear time algorithm to estimate the variance of angles for each data point based on random hyperplane projections. Given a random vector r i and a point p  X  S , we estimate MOA 1 ( p ) using Lemma 2 as follows: where the sets L ( i ) p = { x  X  S \{ p } | x  X  r i &lt; p  X  r R p = { x  X  S \{ p } | x  X  r i &gt; p  X  r i } consist of the points on each side of p under the random projection.

Note that this value is an unbiased estimator of mean of angles between the point p and the other pairs of points. We boost the accuracy of the estimation by using t random projections. We therefore have the more accurate unbiased estimator of MOA 1 ( p ):
Since estimation of the second moment is more compli-cated, we first present the general idea by considering a less efficient approach and then propose an efficient algorithm to compute the unbiased second moment estimator. Fo-cus on a single point p , suppose that we fix an arbitrary jection using the random vector r i , we take the two vectors u ,v i  X  X  0 , 1 } n  X  1 such that their k th coordinate corresponds to the k th point of the set S \{ p } . The k th coordinate of u (or v i ) is 1 if the k th point of the set locates on the left (or right) partition, and 0, otherwise. We consider the matrix P = P t i =1 ( u i v i ) where ( u i v i ) is the outer product of u v . Note that all diagonal elements of P are 0. Consider any pair of points a,b  X  S \{ p } where a = x i and b = x j , it is clear that P ij is the number of times that a locates on the left side and b locates on the right side after t projections. We can therefore estimate  X  2 apb , the squared angle between p and a,b based on the element P ij of the matrix P .
E [ P 2 ij ] = So we have the unbiased estimator: Therefore, we can compute MOA 2 ( p ) based on all elements of P as follows: From the equation above, we can estimate MOA 2 ( p ): However, the squared Frobenius norm k P k 2 F will not be com-puted exactly, since we do not know how to achieve this in less than quadratic time. Instead, it will be estimated us-ing AMS Sketches on product domains. Let AMS ( L ( i ) p AMS ( R ( i ) p ) be the AMS Sketches of the vectors u i and v (using different 4-wise independent random vectors), respec-tively. Due to linearity the sketch of sum of distributions is equal to the sum of sketches of the distributions, so: We therefore derive the second moment estimator F 2 ( p ):
Based on the estimators of MOA 1 ( p ), and MOA 2 ( p ) for any point p described above, we introduce FastVOA, a near-linear time algorithm to estimate the variance of angles for all points of the data set. The pseudo-code in Algorithm 1 shows how FastVOA works.

At first, we project the data set S on the hyperplanes or-thogonal to random projection vectors (Algorithm 2). Ran-domProjection() returns a data structure L containing the Algorithm 1 FastVOA( S,t,s 1 ,s 2 ) Ensure: Return the variance estimator for all points 1: L X  RandomProjection ( S,t ) 2: F 1  X  FirstMomentEstimator ( L ,t,n ) 3: for i = 1  X  s 2 do 4: Y i  X  P s 1 j =1 ( FrobeniusNorm ( L ,t,n )) 2 /s 1 5: end for 6: F 2  X  median { Y 1 ,  X  X  X  , Y s 2 } 7: Var  X  [0] n 8: for j = 1  X  n do 10: Var [ j ] = F 2[ j ]  X  ( F 1[ j ]) 2 11: end for 12: return Var Algorithm 2 RandomProjection( S,t ) Ensure: Return a list L = L 1 L 2  X  X  X  L t where L i is a list of 1: L X  X  X  2: for i = 1  X  t do 3: Generate a random vector r i whose coordinates are 4: Let L i be an empty list containing pairs of point ID 5: for j = 1  X  n do 6: Insert ( x j ,x j  X  r i ) into the list L i 7: end for 8: Sort L i based on the dot product order 9: Insert L i into L 10: end for 11: return L information of the partitions of S under t random projec-tions. Using L , we are able to efficiently identify the values | L p | and | R ( i ) p | corresponding to each point p and r pseudo-code in Algorithm 3 computes the first moment es-timator for each point. Similarly, we also make use of L to compute the Frobenius norm k P k F for each point p in Algorithm 4. To boost the accuracy of the AMS Sketches, we have to repeat the computation of FrobeniusNorm() s 1 s times, and output F 2 as the median of s 2 random variables Y 1 ,  X  X  X  , Y s 2 , each being the average of s 1 values (lines 3 -6). After that, the second moment estimator and variance value for each point are computed in lines 9 -10.
It is clear that the computational complexity of FastVOA depends on Algorithms 2 -4. We note that Algorithm 2 takes O ( tn ( d + log n )) time in computing dot products and sorting for all points while both Algorithm 3 and 4 run in O ( tn ) time. Since we have to repeat the Algorithm 4 in s s 2 times, the total running time is O ( tn ( d + log n + s To guarantee the accuracy of FastVOA, we have to choose t = O (log n ) and s 1 s 2 sufficiently large to boost the accu-racy of estimation as analyzed later in Section 4. Therefore, the running time is dominated by the AMS Sketch compu-tational time. That means FastVOA runs in O ( s 1 s 2 n log n ) time.

It is worth noting that Algorithms 2 -4 use the for loop with t random vectors that performs the same independent operations for each random vector. Therefore, we can simply Algorithm 3 FirstMomentEstimator( L ,t,n ) Ensure: Return the first moment estimator for all points 1: F 1  X  [0] n 2: for i = 1  X  t do 3: C l  X  [0] n ,C r  X  [0] n 4: L i  X  X  [ i ] 5: for j = 1  X  n do 6: idx = L i [ j ] .pointID 7: C l [ idx ] = j  X  1 8: C r [ idx ] = n  X  1  X  C l [ idx ] 9: end for 10: for j = 1  X  n do 11: F 1[ j ] = F 1[ j ] + C l [ j ] C r [ j ] 12: end for 13: end for Algorithm 4 FrobeniusNorm( L ,t,n ) Ensure: Return k P k F for each point p 1: F 2  X  [0] n 2: Initialize 4-wise independent vectors S l [ n ], S r [ n ] whose 3: for i = 1  X  t do 4: AMS l  X  [0] n ,AMS r  X  [0] n 5: L i  X  X  [ i ] 6: for j = 2  X  n do 7: idx 1 = L i [ j ] .pointID 8: idx 2 = L i [ j  X  1] .pointID 9: AMS l [ idx 1] = AMS l [ idx 2] + S l [ idx 2] 10: end for 11: for j = n  X  1  X  1 do 12: idx 1 = L i [ j ] .pointID 13: idx 2 = L i [ j + 1] .pointID 14: AMS r [ idx 1] = AMS r [ idx 2] + S r [ idx 2] 15: end for 16: for j = 1  X  n do 17: F 2[ j ] = F 2[ j ] + AMS l [ j ] AMS r [ j ] 18: end for 19: end for 20: return F 2 parallelize this loop in these three algorithms to achieve a nearly linear (in the number of processors used) speedup.
It has already been argued that our estimators are unbi-ased, i.e., produce the right first and second moments in ex-pectation: E [ F 1 ( p )] = MOA 1 ( p ) and E [ F 2 ( p )] = MOA
In this section we analyze the precision, showing bounds on the number of random projections and AMS sketches needed to achieve a given precision  X  . This will imply that the variance is estimated within an additive error of O (  X  ). For MOA 1 ( p ) we get this directly with high probability, whereas for MOA 2 ( p ) the basic success probability of the estimator F 2 ( p ) is only 3 / 4. However, by repeating the sec-ond moment estimation procedure s 2 = O (log(1 / X  )) times and taking the median outlier score for each point, the suc-cess probability can be magnified to 1  X   X  , for any  X  &gt; 0 as argued in [3].
We will use the following version of the Chernoff bound from [8, Theorem 1.1]:
Lemma 5. Let Y = P t i =1 Y ( i ) be a sum of independent random variables with values in [0; 1] . For any  X  &gt; 0 ,
Consider the probability (over choice of vectors r 1 ,..., r that F 1 ( p ) deviates from MOA 1 ( p ) by more than  X  . Split-ting the sum F 1 ( p ) t/ X  into t terms we can apply Lemma 5. Then we get that its deviation from the mean exceeds  X t/ X   X   X  2  X  2 ln( n ) this probability is at most 2 /n 2 . So the prob-ability that all of n first moment estimators have error at most  X  is 1  X  O (1 /n ).

Next, consider the probability (again over choice of vectors r ,...,r t ) that the first version of the second moment esti-mator, F 0 2 ( p ), deviates from its expectation by more than  X  , given that F 1 ( p ) deviates by at most  X  from its expectation. Looking at (2) we see that this happens when || P || 2 F devi-ates from its expectation by at least n  X  1 2 t 2  X / X  2 it suffices to show that each squared entry P 2 ij deviates by at most 1 4  X t 2 / X  2 from its expectation with high probability. cator random variables, which means that Lemma 5 applies. For t &gt; 16  X   X  2  X  4 ln( n ) we get that P ij deviates from its ex-pectation by at most 1 4  X t/ X  2 with probability 1  X  O (1 /n Since P ij  X  t this implies that P 2 ij deviates by at most  X t 2 / X  2 , as desired. The total error for F 0 2 ( p ), accounting for all 2 n  X  1 2 entries of P , is therefore bounded by  X  with probability 1  X  O (1 /n 2 ).

Finally, we should account for the error caused by the use of AMS sketches in the final estimator F 2 ( p ), equa-tion (3). By Lemma 4 the variance of the estimator is bounded by 8 MOA 2 ( p ) 2 . Taking the average of s 1 sketches the variance is reduced to at most 8 MOA 2 ( p ) 2 s Chebychev X  X  inequality, the probability that F 2 ( p ) deviates by  X   X  2 MOA 2 ( p ) from its expectation of MOA 2 ( p ) is at most: For s 1 &gt; 32  X  4 / X  2 this is less than 1/4. It is simple to ver-ify that this deviation corresponds to a deviation for F 2 of 2  X  . As stated above, the failure probability is reduced exponentially by repeating the estimation s 2 times.
We implemented all algorithms in C++ and conducted experiments in a 2.67 GHz core i7 Windows platform with 3GB of RAM on both synthetic and real world data sets.
For the sake of fair comparison, we made use of the same synthetic data generation process as ABOD approaches [14]. We generated a Gaussian mixture including 5 equally weighted clusters having random means and variances as normal points and employed a uniform distribution in full-dimensional space as the outliers. For each synthetic data set, we generated 10 outliers which are independent on the Gaussian mixture. We evaluated the performance of all algorithms on synthetic data sets with varying sizes and dimensions.

For the real world high-dimensional data sets, we picked three data sets (Isolet, Multiple Features and Optical Dig-its) designed for classification and machine learning tasks from UCI machine learning repository [9]. Isolet contains the pronunciation data of 26 letters of the alphabet while Multiple Features and Optical Digits consist of the data of handwritten numerals ( X 0 X  - X 9 X ). For each data set, we picked all data points from some classes having common behaviors as normal points and 10 data points from another class as outliers. For instance, we picked points of classes C, D, and E of Isolet that share the  X  X  X  sound as normal points and 10 points from class Y as outliers. Similarly, we picked points of classes 6 and 9 of Multiple Features, classes 3 and 9 of Op-tical Digits as normal points because of the similar shapes and 10 points of class 0 as outliers. It is worth noting that there are some outliers that probably locate on the region covered by inliers. Therefore, we are not able to isolate ex-actly all outliers. Instead, we expect our algorithms to rank all outliers into sufficiently high positions.
This subsection presents the accuracy experiments to eval-uate the reliability of our estimation algorithm. As analysis in the Section 4, the estimators F 1 ( p ), F 0 2 ( p ), and F any point p of the data set can deviate from their expecta-tions by more than with probability at most  X  by using a sufficiently large number of random projections t = O (log n ) and AMS Sketches s 1 s 2 . Note that F 2 ( p ) is the second mo-ment estimator using AMS Sketch while F 0 2 ( p ) is based on only random projections. At first, we carried out experi-ments to measure the accuracy of estimators based on only random projections. We measured the deviation error of F ( p ) and F 0 2 ( p ) from their expectations with error probabil-ity  X  = 0 . 1. We took t in ranges [100, 1,000] and conducted experiments on 2 synthetic data sets having 1,000 points on 50 and 100 dimensions, namely Syn50 and Syn100, as well as the three real world data sets, namely Isolet, Mfeat and Digit. Figures 2.a and 2.b display the deviation error ( ) from expectation of the estimators F 1 ( p ) and F 0 2 error probability  X  = 0 . 1. Using these two estimators, we derived the variance estimator and measured its deviation from expectation with  X  = 0 . 1, as shown in Figure 2.c. Al-though the theoretical analysis requires a sufficiently large number of random projections t to achieve the small , the results on 5 data sets surprisingly show that with a rather small t , we are able to estimate exactly the variance of an-gles for all points. With t = 600, 90% number of points of 5 data sets have the first moment, the second moment and the derived variance estimators deviating from their expec-tations at most 0.035, 0.08 and 0.015 respectively. When t increases to 1,000, 90% of points of 5 data sets have the variance estimator deviate from its expectation by at most 0.01. Therefore, for such data sets having large difference between VOA of outliers and VOA of border points, the use of random projections to estimate VOA can achieve good performance on detecting outliers.

To quantify the error due to the AMS Sketches, we mea-sured the error probability  X  of the variance estimator us-ing AMS Sketches and fixing parameters t = 1 , 000 ,s 7 , 200 ,s 2 = 50 , = 0 . 1 on all data sets. Concretely, we computed the number of points p of the data set such that its variance estimator by using AMS Sketch deviates by more than V OA ( p ) from its expectation V OA ( p ). Table 1 presents the error probability of variance estimators on 5 data sets.
 Table 1: Error probability of variance estimator us-ing AMS Sketch on 5 data sets
It is clear that the two synthetic data sets obtain very small errors while the real world data sets take rather large errors, especially on Isolet. This is because the variance es-timator of all points of the data set may be underestimated or overestimated by using AMS Sketch. To guarantee the capability of our approximation approach on detecting out-liers, we analyzed the accuracy of outlier ranking between the brute force algorithm called SimpleVOA and the approx-imate algorithm FastVOA. The accuracy of outlier ranking is defined as | A  X  B | m where A and B are the top m positions retrieved by SimpleVOA and FastVOA algorithms, respec-tively. Figure 3 shows the accuracy of outlier ranking be-tween SimpleVOA and FastVOA where m is in ranges 10 -100.
 Figure 3: The accuracy of outlier ranking between SimpleVOA and FastVOA on 5 data sets.

The results of outlier ranking indicate that FastVOA pro-vided a rather high accurate ranking on all data sets. While 2 synthetic data sets and Multiple Feature show a highly accurate ranking for all ranges of top positions, the other data sets offered a medium accurate ranking when m &lt; 30 but more accurate when m &gt; 40. Although the use of AMS Sketch may lead to underestimate or overestimate of the variance estimator, FastVOA still introduces good perfor-mance on ranking data points based on VOA.
It is obvious that our approaches are dealing directly with the variance of angles (VOA) while the approaches in [14] compute the variance of cosine of angles weighted by dis-tances (ABOF). This subsection demonstrates experiments to measure the effectiveness of both measures on detect-ing outliers. For each measure, we compared the quality of outlier ranking provided by brute force (SimpleVOA and ABOD) and approximation algorithms (FastVOA and FastA-BOD). For the sake of fair comparison, we used the precision-recall graph to evaluate the capability of each algorithm to retrieve the most likely outliers. The precision is the num-ber of retrieved points that are indeed outliers. For each precision level, we measured the recall as the percentage of the number of outliers in the retrieved set.
 Figure 4: Precision-Recall Graph for 4 synthetic data sets. Each graph describes the behavior on 1,000 and 5,000 points.

For synthetic data sets, we generated 4 data sets with varying sizes of 1,000 and 5,000 points and dimensions of 50 and 100. We observed that the differences of VOA between outliers and border points on synthetic data sets become large when the size increases. Therefore, we adjusted the parameter settings for FastVOA on synthetic data sets of size 5,000 points to reduce the time complexity. In par-ticular, we determined t = 100, s 1 = 1600, s 2 = 10. We kept the same parameter setting as Section 5.2 for the other data sets. The sample size of FastABOD is chosen as 0 . 1 n as [14]. Let us note that both ABOD and FastABOD of-fered perfect results on 4 synthetic data sets. That means all 10 outliers were ranked into the top 10 positions. There-fore, we did not show the results of ABOD and FastABOD on synthetic data sets. Figure 4 depicts the precision-recall graph for synthetic data sets. Figure 4.a shows the results of brute force (SimpleVOA 1 and SimpleVOA 2 ) and approxi-mation algorithms (FastVOA 1 and FastVOA 2 ) on the 2 data sets of 50 dimensions and varying sizes of 1,000 and 5,000 points. In the medium dimensionality of 50, VOA did not work well in the small data set size but achieved almost perfect performance in the large data set by ranking all 10 outliers between top 11 retrieved points. It is clear that the better performance of SimpleVOA leads to the better per-formance of FastVOA. Results of 2 synthetic data sets with 100 dimensions are displayed in Figure 4.b. Since the effect of weighting factors in ABOF is not meaningful in high-dimensional data, SimpleVOA and FastVOA show competi-tive results with ABOD and FastABOD with almost perfect performance.

Figure 5 shows the observed precision-recall graphs for 3 real world data sets. On Isolet, SimpleVOA and ABOD of-fered almost perfect performance by ranking 10 outliers in top 10 and top 16 positions, respectively. FastABOD pro-vided better outlier ranking than FastVOA on detecting 7 outliers in top 10 positions. However, on ranking all 10 out-liers, both of them did not work well for large recall levels. Both SimpleVOA and FastVOA performed rather well on Multiple Features by ranking all outliers on the top 16 po-sitions while both ABOD and FastABOD performed very badly. All approaches had difficulties to detect outliers on Optical Digits. However, the VOA based approaches clearly offered better results than the ABOF based ones.
This section compares the running time of 3 algorithms, namely FastVOA, LB ABOD and FastABOD on large high-dimensional data sets. In fact, there are very few large real world data sets where the outliers are identified exactly in advance. Therefore, we decided to evaluate the efficiency of these 3 approaches on synthetic data sets. We carried out experiments measuring the CPU time of each approach on data sets with varying both size and dimensions in ranges 10,000 -100,000 points and 100 -1,000 respectively. It is clear that both LB ABOD and FastABOD run in O ( dn 2 ) time while the running time of FastVOA depends on the parameters t,s 1 ,s 2 . As mentioned in Section 5.3, we can use rather small parameter settings for FastVOA in very large high-dimensional synthetic data sets without reducing the accuracy. Therefore, we set t = 100, s 1 = 1600, s 2 for FastVOA and the sample size of FastABOF chosen as 0 . 1 n . Let us note that the value 0 . 1 n becomes rather large when the data set size increases. In contrast, FastVOA only needs rather small number of random projections and the AMS Sketch sizes. As analysis in the Section 3.4.4, the total running time of FastVOA is O ( tn ( d + log n + s 1 With the choice of parameters above, the total running time of FastVOA is still dominated by the computation time of AMS Sketches with O ( ts 1 s 2 n ) time.
 Figure 6.a shows the CPU time in (ms) of FastVOA, LB ABOD and FastABOD for data sets having 100 dimen-sions and sizes of 10,000 -100,000 points while Figure 6.b displays the CPU time in (ms) for data sets having size of 20,000 points and dimensions in 100 -1,000. It is clear that the running time of FastVOA is linear time in the size of data set and independent on number of dimensions. In contrast, both LB ABOD and FastABOD run in quadratic time in the size of data set and linear time in number of dimensions.
 Figure 6: Comparison of CPU time of FastVOA, FastABOD and LB ABOD.

We conclude the efficiency evaluation of FastVOA by illus-trating its suitability for parallel processing. We made use of Open Multi-Processing API (OpenMP) supporting multi-platform shared memory multiprocessing programming in C++ to parallelize the for loop of random projection vec-tors in Algorithms 2 -4 of Section 3.4.3. We measured the parallel speedup when running on 4 processors of Core i7 machine. Table 2 illustrates a nearly linear parallel speedup of FastVOA (in the number of processors used) on synthetic data sets with size of 10,000 points on 100 dimensions.
In this paper, we introduced a random projection-based algorithm to approximate the variance of angles between pairs of points of the data set, a robust outlier score to de-tect high-dimensional outlier patterns. By combining ran-dom projections and AMS Sketches on product domains, our approximation algorithm runs in near-linear time in the size of data set and is suited for parallel processing. We presented a theoretical analysis of the quality of approxima-tion to guarantee the reliability of our estimation algorithm. The empirical experiments on synthetic and real world data sets demonstrate the scalability, effectiveness and efficiency of our approach on detecting outliers in very large high-dimensional data sets.
We deeply thank Dr. M. Schubert for his released syn-thetic data generator and useful comments about ABOD. We also thank T. L. Hoang for useful discussion in the early stage of this work. [1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On [2] C. C. Aggarwal and P. S. Yu. Outlier detection for [3] N. Alon, Y. Matias, and M. Szegedy. The space [4] S. D. Bay and M. Schwabacher. Mining distance-based [5] V. Braverman, K.-M. Chung, Z. Liu, [6] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and [7] M. Charikar. Similarity estimation techniques from [8] D. P. Dubhashi and A. Panconesi. Concentration of [9] A. Frank and A. Asuncion. UCI machine learning [10] A. Ghoting, S. Parthasarathy, and M. E. Otey. Fast [11] M. X. Goemans and D. P. Williamson. Improved [12] P. Indyk and A. McGregor. Declaring independence [13] E. M. Knorr and R. T. Ng. Algorithms for mining [14] H.-P. Kriegel, M. Schubert, and A. Zimek.
 [15] H.-P. Kriegel, M. Schubert, and A. Zimek. Outlier [16] V. Mahadevan, W. Li, V. Bhalodia, and [17] E. M  X  uller, M. Schiffer, and T. Seidl. Statistical [18] S. Papadimitriou, H. Kitagawa, P. B. Gibbons, and [19] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient [20] Y. Wang, S. Parthasarathy, and S. Tatikonda. Locality [21] R. Wheeler and J. S. Aitken. Multiple algorithms for
