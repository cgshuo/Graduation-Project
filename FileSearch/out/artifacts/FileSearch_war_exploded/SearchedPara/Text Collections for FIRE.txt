 The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create a Cranfield-like evaluation framework in the spirit of TREC, CLEF and NTCIR, for Indian Lan-guage Information Retrieval. For the first year, six Indian languages have been selected: Bengali, Hindi, Marathi, Pun-jabi, Tamil, and Telugu. This poster describes the tasks as well as the document and topic collections that are to be used at the FIRE workshop.
 H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Experimentation, Languages, Measurement, Performance
The success of TREC, CLEF, and NTCIR has established the importance of building reusable, large-scale standard test collections in information access research. The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create a similar platform for Indian Language Informa-tion Retrieval (ILIR) in order to encourage research in ILIR by providing the data and a common forum for comparing models and techniques. This effort is a part of a nation-wide project (called the Cross-Lingual Information Access (CLIA) project) that is funded by the Indian government, and is being carried out by a consortium of ten academic and industrial institutions. The broad goal of this project is to develop resources for ILIR, along with a complete, cross-lingual information access system for English and six other Indian languages, viz. Bengali, Hindi, Marathi, Punjabi, Tamil, and Telugu. Of these languages, Hindi and Bengali rank among the top ten most-spoken languages of the world.
FIRE addresses the evaluation-related issues pertaining to ILIR. Its aim is to build test collections in the six chosen Indian languages. These are the first test collections to be constructed for IR experiments in these languages (except Hindi, which was addressed in the TIDES surprise language exercise [1]). As usual, participants will run their systems on the test collections. Results of the system evaluations will be The corpus consists predominantly of news articles pub-lished in the six languages during the period from September 2004 to September 2007 in various on-line news sources. Be-sides news articles, the corpus also includes some document s from the health and tourism domains.

We are currently in the process of obtaining permission from the respective publishing houses to distribute the cor -pus to interested groups for research use.
The original articles are often written using non-standard , font-based encoding schemes. All such documents have been transcoded to use the UTF-8 encoding scheme, so that the corpus is uniformly in Unicode. In most cases, each docu-ment contains a single news article, and consists of a title, the author X  X  / correspondent X  X  name and the body of the article. Some statistics about the corpus in its present for m is given in Table 2.

The Bengali corpus consists of 555,124 unique words. Af-ter light stemming using a statistical stemmer, the dictio-nary size reduces to 312,411, while after aggressive stemmi ng using the same stemmer, the lexicon size reduces to 168,437. The maximum, minimum and mean document size in this corpus are 61891, 136, and 6278.40 bytes respectively. The corpus is also classified (based on the original categorizat ion of the news articles) into major categories like Business, Ra-jya (state news), Travel, Editorials, Bidesh (international news), Desh (national news), Sports, Health, etc. Similar details for the other languages will be available soon.
A set of 95 topics has been created based on manual in-spection of the news published in the six languages during the period September 2004 to September 2007. This news can be divided into three categories, viz. international, n a-tional, and regional. While there is a large overlap across languages in terms of international and national news con-tent, regional news is often specific to each language. The topics were created keeping this in mind. The topics have been translated to all the six languages and English. A typ-ical topic has a title, a description, and a narrative sectio n. Some of these topics are expected to be discarded as either too easy or too difficult on the basis of preliminary experi-ments. Of the remainder, 50 topics will be used for the final evaluation. A training set of about 30 topics will also be distributed in early June, 2008.
Since the number of participants in the final evaluation may not be as large as at the other major evaluation fora, some preliminary pooling will be done for each query. Three automatic retrieval methods based on (i) two variants of
