
Organizing textual documents into a hierarchical taxon-omy is a common practice in knowledge management. Be-side textual features, the hierarchical structure of directo-ries reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this infor-mation into text mining processes. In this paper, we pro-pose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the orig-inal taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class prox-imity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCV1) benchmark and the Swissprot protein data-base.
A characteristic of textual documents is the high dimen-sionality (typically tens of thousands). Thus, dimensional-ity reduction plays an important role in reducing computa-tional costs and in improving the performance of text min-ing algorithms. Another typical characteristic of text ap-plications is that often a document is allowed to belong to more than one class, i.e. the documents are multi-labeled. For example, a news article about a football team could be-long to both categories  X  X ports X  and  X  X usiness X . Large text databases usually contain large amounts of classes. To al-low easy navigation and express the inheritance relation-ships between these classes, the classes are often organized in a class hierarchy or taxonomy.

The taxonomy is an intrinsic structure of categories and documents. Each node in a class hierarchy represents a subclass of the father node. The leaf nodes describe ba-sic classes that are not distinguishable any further, while the root corresponds to the most general class, comprising all documents. An example is the large topic hierarchy of a web directory service like Yahoo! that allows us to navi-gate to any category among several thousands topics by just a few clicks. Additional examples for large topic trees are the library of congress catalogue or biological class systems like Gene Ontology [5].

In this paper, we describe a novel approach to exploit a given class hierarchy for text indexing. The idea is to di-rectly integrate the information that is contained in the class hierarchy, into a new highly descriptive feature space. We interpret the classes in the hierarchy as  X  X ridges X  connect-ing the documents and introduce a hierarchy-regularized latent semantic indexing. Our method naturally incorpo-rates the similarity between the classes into feature trans-formation. Thus, large distances between the documents belonging to very similar classes are penalized, while large distances between the documents belonging to dissimilar classes are encouraged. Furthermore, the generated output space considers the specificity of classes, i.e. very general classes are considered to be less informative than very spe-cific classes. Thus, classes that are close to the leaf nodes and classes being characterized by a small number of doc-uments play a more important role. Our method employs both the similarity and the specificity of the classes, an as-pect that has not been sufficiently addressed by previous methods. Finally, our method handles multi-labeled doc-uments in a natural way, while other approaches (e.g. see [15]) often need to involve constraints having combinatoric complexity.

Our proposed hierarchy-regularized approach is used to develop a novel textual feature reduction technique, called hierarchy-regularized latent semantic indexing (HLSI). The resulting feature space offers the possibility to integrate a class hierarchy into a variety of text mining and retrieval tasks. Furthermore, it i ncreases the efficiency of these techniques due to the smaller dimensionality of the out-put space. Our experimental evaluation demonstrates that the proposed approach is capable to derive low dimensional and highly descriptive feature spaces that allow fast and ac-curate classification on two real-world data sets. The first is the Reuters Corpus Volume 1 benchmark (RCV1) and the second is the Swissprot [2] protein database.

The rest of the paper is organized as follows: Section 2 describes our hierarchy-regularized approach. In section 3, we briefly survey related work in the area of feature re-duction and hierarchical classification of multi-labeled doc-uments. Section 4 presents our experimental results on two real-world data sets and section 5 conclude the paper.
Given a set of predefined classes C and a set of labeled documents { ( x i ,y i ) } ,where x i is an n -dimensional feature vector and y i  X  X  the class label, we want to learn a func-tion that can predict the labels for new test documents. In hierarchical classification, the training data does not sim-ply consist of  X  X eature-label X  pairs, but also a hierarchical structure of classes, which offers some additional informa-tion about the characteristics of data. Formally, we define the training data in the following way.
 Definition 2.1. A hierarchy structured training set T ( X , C ,g,s ) consists of (1) a set of N labeled documents X =[ x 1 ,..., x N ] ,where x i  X  R n ; (2) a set of classes C = { c 1 ,...,c l } ; (3) a function g : X X C X  X  X  1 , 1 } with g ( x i ,c k )=1 if x i belongs to c k and -1 otherwise; (4) a function s : C\{ c r } X  X  , such that s ( c k ) gives the direct father class of c k ,where c r is the root class.
Because the classes are organized in a tree structure, the following condition should be fulfilled: c k = s ( c k ) ,and g ( x i ,c k )=1  X  g ( x i ,s ( c k )) = 1 . Note that the defini-tion allows the multi-label case where one document can be assigned to multiple leaf classes.

In addition, we define some operators on the tree: (1) h ( x i ) returns the corresponding classes as well as all of their ancestors containing x i ;(2) H ( x i , x j )= h ( x h ( x j ) returns the common classes of x i and x j ;(3) | c k is the number of documents in class c k .
A class hierarchy is not just a notion of class proximity, but also a way to describe the similarity between the doc-uments. A class is like a  X  X ridge X  connecting all the docu-ments within this class. Therefore, we define a hierarchy-induced similarity graph as follows: C4 C5 A A B B
Figure 1. A hierarchy structured training set and the corresponding hierarchy-induced similarity graph.
 Definition 2.2. A hierarchy-induced similarity graph G|
T ( V, E ) consists of (1) a set V of vertices with a bijective function to X ;(2)aset E  X  V  X C X  V of edges ,where [ i, k, j ]=[ j, k, i ]  X  E is the edge between documents and x j via class c k  X  H ( x i , x j ) , i = j .
In this graph, each vertex corresponds to an document x . For each class c k  X  H ( x i , x j ) two documents x i , have in common, the graph induces an edge [ i, k, j ] .Since every document is part of the root class, there is at least one edge connecting any pair of documents and thus the graph is fully connected (cf. figure 1).

The hierarchy can be used to derive implications about the connections between the documents. The similarity of two documents naturally depends on the number of edges connecting them. If two documents share a common leaf class, the number of edges tends to be rather big because each of the predecessor classes provides an additional edge as well. Thus, documents sharing specific classes are con-nected by more edges than documents that only share very general common classes. However, the specificity of a class is not exclusively dependent on its level in the hierarchy, but also on the number of documents belonging to the class. For example, if two documents are the only documents belong-ing to a particular class, then the class is very specific and the connection between the both documents is very strong.
In order to express the strength of these connections, we define the edge weight w : E  X  R + as follows: where the weights of edges from x i via c k to x j evenly di-vided by the size of c k . This is consistent with the intuition that more popular classes are less informative for indicat-ing documents X  similarities. Accordingly, by summing over all the shared classes, the induced connection strengths be-tween two documents is computed as Then the vertex degree d : V  X  R + is defined as: which is the total strength of all of edges connected with x . Our definition of edge weights and vertex degrees can also be justified from a random walk point of view. Suppose a reader is browsing documents in a hierarchical directory. The transition probability from document x i to document x j via category c k should be Then the expected transition probability from x i to x j is Eq. (5) indicates that the transition probability depends not only on the number of classes shared by x i and x j ,but it is also dependent on the size of these classes. There-fore, transitions across high-level branches are considered to be rather unlikely while transitions within deep or small branches occur with a rather high probability.

In our approach, multi-lab eled documents are naturally handled. More importantly, these multi-labeled documents are connected to the documents from different branches and somehow inform a closeness of these branches. Thus the similarity of documents and the similarity of classes are fur-ther informing to each other, which is similar to the hub-authority idea in web search [12].
We seek for a mapping function  X : X X  R m ,m n , that maps feature vector x into a new m -dimensional space. It is desired to ensure the mapping functions  X ( x ) to be consistent with respect to the structure of G| T ( V, E ) .Let the mapping function  X  contain m elementary functions, and each of them  X  : X X  R map documents into a one-dimensional space. Intuitively, a tight connection between two documents should induce similar outputs in the new space. Similar to the idea of spectral clustering [17], the cost induced by an one-dimensional mapping function is defined as: The cost function emphasizes the variations of  X  ( x ) be-tween tightly connected documents. In the following, we call  X (  X  ) the smoothness functional , since it measures the non-smoothness of  X  with respect to the hierarchy structure. Furthermore, Eq. (6) can now be rewritten into the follow-ing form: where  X  =[  X  ( x 1 ) ,..., X  ( x N )] and is an N  X  N matrix: In this paper, we mainly consider linear functions  X  ( x )= w x . Then we replace  X (  X  ) by  X ( w ) and write the smooth-ness functional as: The cost can be easily plugged into a formalism of latent semantic indexing (LSI), to ensure the derived features con-sistent with the structure of hierarchies (see Sec. 2.4).
The high dimensionality (typically tens of thousands) of text data always hampers the generalization of learn-ing machines and seriously increases the computational costs. However, in general, the effective subspace responsi-ble for the document labels has often a lower dimensional-ity. Latent semantic indexing (LSI)[6] is a popular feature-reduction technique for text data that identifies such a sub-space. The method is however unsupervised and cannot in-corporates additional information.

In this section, we employ t he hierarchical structure to identify the effective subspace of text data. Various algo-rithms (e.g. clustering, classification and retrieval) can then be efficiently and effectively based on the new low dimen-sional feature space.

First, we derive a formalism of LSI such that the hierarchy-induced cost Eq. (7) can be easily plugged in. Let  X : X X  R m be the feature mapping consisting of m linear functions  X  j ( x )= w j x , j =1 ,...,m . LSI finds the pro-jections of data X =[ x 1 ,..., x N ] by applying singular value decomposition (SVD): where U =[ u 1 ,..., u N ] is an N  X  N matrix, D is an N  X  n diagonal matrix with diagonal entries sorted non-increasingly, and V =[ v 1 ,..., v n ] an n  X  n matrix. Then the results of mapping  X  on X are given by the first m columns of U .

In the following theorem, we interpret SVD from a dif-ferent point of view.
 Theorem 2.3. Let X = UDV be the singular value de-composition of X .Then u j = Xw j where w j are the solu-tions to Proof. we give the sketch. U = XVD  X  1 apparently sug-gests u j = Xw j .Let K = XX , then it is known that u j are the directions maximizing u Ku with constraints u u =1 and u  X  u 1 ,..., u j  X  1 . The objective can be replace by min u u K  X  1 u . Inserting u j = Xw j to the op-timization problem completes the proof.

Theorem 2.3 gives the formalism of LSI that enables us to easily perform hierarchy-regularized LSI (HLSI). Since we wish to have the mapping functions to be consistent with the class hierarchy, the optimization problem for HLSI is denoted as follows: where  X   X  R + , w X Xw is the cost induced by the hier-archy structure, and  X  determines how much the projections should tend to follow the structure of input features. From the regularization point of view,  X  prevents the mappings from being over-fitted by the hierarchy structure. When  X   X  X  X  , HLSI becomes identical to LSI.

By setting the derivatives of its Lagrange formalism to be zero, it turns out that the linear weights are the solutions to a generalized eigenvalue problem: The m generalized eigenvectors with the smallest eigenval-ues are the linear weights w j , j =1 ,...,m , of the feature mapping functions.

Since text data is usually very high-dimensional, it is very expensive to solve the large scale generalized eigen-value problem. The following theorem enables the algo-rithm to work in the dual space where the dimensionality depends on the number of documents.
 Theorem 2.4. The solutions w j , j =1 ,...,m , to the HLSI problem have the form Proof. Let S be the space span { x 1 ,..., x N } and P the projection onto it. Then w = P w +( I  X  P ) w = w + w  X  . Since w  X  does not affect  X ( w ) but only increases w 2 , w  X  must be zero at the optimum. Therefore w  X  S which completes the proof.

Then, the HLSI problem has the dual form,  X  j ( x )= where K = XX . The problem is also equivalent to a generalized eigenvalue problem: Finally, the learned mapping functions transform a high-dimensional feature vector x to a m -dimensional space. In the new space, data mining and retrieval tasks can be effi-ciently done.
HLSI also suggests a direct optimization approach to handle the multi-label hierarchical categorization problem. Clearly, the simplest solution is to train binary classifiers for each leaf class c k . Given the training documents X = [ x i ,..., x N ] with labels ( y k ) i  X  X  +1 ,  X  1 } for class c linear classifier 1  X  k ( x )= w k x can be learned by where (  X  ,  X  ) is the loss function,  X   X  R + ,and w 2 is the regularizer preventing overfitting. The optimization in Eq. (11) treats all negative documents identically. There is no bigger penalty if a document is miss-classified into a leaf class which is faraway in the tree from the correct class. Therefore, we insert  X ( w )= w X Xw into the opti-mization problem Eq. (11) and get the following objective function
J k ( w )= where  X ,  X   X  R + and I  X  R N  X  N is an identity matrix. In Eq. (12) there are two parts of loss based on empirical data: one is the conventional classification loss ( w x i , ( y the other is the h ierarchy-induced loss  X ( w ) .
The square error loss ( w x i , ( y k ) i )=( w x i  X  y k was often reported to achieve superior performance in text categorization [22, 21]. For this case, the estimate of w has closed form: which is derived by setting  X  X  k ( w )  X  w =0 .

Let us note that the methods suggested in this section have a close connection to HLSI as suggested in Sec. 2.4. Here, the feature projection is implicitly done via the reg-ularization. Despite its equivalence, explicit feature map-pings enable learning methods to work on a low dimen-sional feature space and greatly improve the efficiency. This advantage is very important for real-world applications.
Dimensionality reduction is a well established approach in data mining and information retrieval. One sort of the most well-known techniques is feature selection, like mu-tual information, in formation gain and  X  2 statistic [20]. In general, established feature selection methods ignore the dependency between features, which exists obviously in textual data sets. Furthermore it is difficult for feature se-lection to deal with multi-label problems. Another sort of dimensionality reduction techniques is feature transforma-tion/mapping. A representative approach is latent seman-tic indexing [6], which uses singular value decomposition (SVD) to find the principal components of term-document matrices. However, this method is unsupervised and thus the found dimensions are not necessarily relevant with class labels.

Our approach is a supervised feature mapping method. It considers the co-occurrence between features and handles multi-labeled problems in a natural way. Similar methods are the canonical correlation analysis (CCA) [10], partial least square (PLS) and linear discriminant analysis (LDA) [18]. LDA aims to find transformation directions that max-imize distances between class means and minimize vari-ances within classes. However, LDA can only handle the single-label problems. PLS and par ticularly CCA are clas-sical statistical methods and measure the linear correlation between two multidimensional data sets (e.g. inputs and outputs). The difference of both methods is that in CCA the correlation is normalized by variances within two data sets. Compared to CCA, our approach considers addition-ally hierarchical structure of output space, when optimizing directions with respect to normalized correlations.
The resulting feature representations with our approach are usable for any problem settings in data mining and in-formation retrieval. In particular for hierarchical classifi-cation one can add the information of the class taxonomy to the loss function as suggested in Sec. 2.5. However, none of the former hierarchical classification approaches [13, 14, 8, 1, 15, 4, 7, 3] does directly influence the em-ployed feature space using a class hierarchy. Another con-ceptual difference is that we use the information about the specificity of the classes as well as the information about the similarity between classes.
In order to demonstrate the advantages of the introduced approach, we evaluated our methods on two real-world data sets. The first is the Reuters Corpus Volume 1 (RCV1) which consists of 806,791 English news stories. We ran-domly chose 10,000 documents from this data set having 31,613 class labels. These class labels refer to a class hier-archy of 81 classes, 64 leaf classes and 17 inner classes. The depth of the tree is 4 and each topic is represented by at least 20 documents. In all 10,000 documents occur 9,705 differ-ent words. The second data set is derived from the Swiss-Prot[2] protein database that contains textual annotations of proteins. The entries in Swissprot provide links to the class system of Gene Ontology (GO) [5] which is used as a class hierarchy. We selected the subtree  X  X xido-reductase X  from GO, which contains 125 categories. The corresponding en-tries in Swissprot comprise 8,335 proteins having 18,955 labels. The class hierarchy has also a maximum depth of 4 and provides 94 leaf classes and 31 inner classes. For each category, there are at least 10 entries available. This document collection contains 10,404 different words. For both data sets, we derived an original feature space of word vectors by dropping the words being contained in less than 5 documents and afterwards applied TFIDF. The output space of labels is a subset of { X  1 , 1 } N  X |C| (see Def. 2.1 (3)).
One important application of feature reduction is to visu-alize data patterns in a 2 or 3-dimensional space, providing an impression about the quality of the underlying feature space. Therefore, we used LSI, CCA and HLSI to project the documents into a two-dimensional space and visualized them. The results for RCV1 data are displayed in Fig. 2. Let us note that we observed a similar visualization for the Swissprot data, but we had to omit it due to the space limi-tation. The mapping functions are computed based on 2000 documents and were afterwards applied to project another set 1000 documents 2 . We visualized 3 top-level classes and one second-level class C15 which is a subclass of the class CCAT. In Fig. 2 different colors and marks are used to distinguish the classes. The results of LSI and CCA do not present a very meaningful data distribution because the classes are not separated very well. On the other hand, us-ing HLSI provided a visualization which is quite relevant to the class memberships, in the sense that documents from the same class often display a close distance. This result indicates that the method emp loys the multi-label informa-tion to represent the class similarities. Since class C15 is a subclass of CCAT, HLSI mapped the members of C15 into a concentrated subregion of area it mapped the members of CCAT. Thus, the projection preserved the inheritance rela-tionship within the data. The good performance of HLSI on visualization demonstrates that the proposed algorithm effectively detects the meaningful subspaces within hierar-chical data. Thus, the resulting feature spaces should allow fast and accurate solutions for various learning and retrieval tasks. Our second set of experiments studied the quality of HLSI in terms of dimensionality reduction for text clas-sifications. The experimental results are evaluated using macro-averaged F 1 and micro-averaged F 1 , which are suit-able to measure the classification accuracy when the classes are very unbalanced. In particular, micro-averaged F 1 re-flects the quality on the classes with a large number of pos-itive documents, while macro-averaged F 1 emphasizes on the minor classes, which correspond to the leaf classes in the hierarchical case. As a comparison, we investigate the quality of 4 different feature spaces: The first was generated by LSI, the second by HLSI, the third by CCA and the last was the original feature space. For classification, we used a linear support vector machines (SVMs) that was imple-mented in the SV M light package [11]. For each run, we randomly selected 2000 documents with the constraint that each leaf class had at least 5 positive documents. Treating the selected data as training set, we trained classifiers for all inner classes and leaf classes. Then, the trained models were used to predict the class of the remaining 8000 docu-ments. For LSI, CCA and HLSI, the same set of 2000 doc-uments were employed for learning the feature mapping. Because of large number of training documents were used kernelized CCA [9] and kernelized PCA [16]. We changed the dimensionality of projections and compared change of performance. The experiment was randomized for 10 times and the mean and error bar of the results were computed.
Finally the results are shown in Fig. 3. We can see that, the full-feature case is always working very well. HLSI gives the performance significantly better than LSI. In the case of micro-averaged F 1 for RCV1 data, the performance of 50-dimensional HLSI features is almost as good as full features, while the cases of more than 80-dimensional HLSI features are even better than full features. In the meantime LSI needs 200 dimensions to reach almost the same per-formance. Similar observations can be made in the other 3 subplots. In general, 50-dimensional HLSI features for RCV1 data and 80-dimensional features for Swissprot data are sufficient to give comparable accuracy as the full-feature case, however, the calculatio n for training the mapping only needs to be done once, summing up over the training of all classifiers, the total cost is much smaller than SVMs using full features. In our experiments, we observed on RCV1 data a 12 times improvement of efficiency with 50-dimensional HLSI, and on Swissprot database a 9 times im-provement for 80-dimensional HLSI.

We also performed top-down classification on the class hierarchy, as the method was mentioned in many papers (e.g. [13]). However, we did not observe any big differ-ences from the setting we just described. All the methods including HLSI showed the same behaviors. Moreover, we test the performance of the proposed hierarchy-regularized classifier, however the method shows close predictive accu-racy as SVMs using the full feature. The reason seems to be that with 2000 training documents, the classifiers converges to almost the same hyperplane anyway. Compared to CCA, HLSI gives significant better visualization performance and quite better classification performance on RCV1 with 20-50 dimensions. It shows exactly that HLSI improves quality of found directions by using hierarchy. 4.4. Sensibility of  X 
There is a parameter  X  in HLSI algorithm. Here we ex-amine how sensitive the performance of HLSI on the set-ting of  X  . Here we run the same setting of experiments de-scribed in the last section, with only  X  changed. All the evaluations are averaged from 10 random repeats. We only report the mean in Fig. 4, while omit the error bar since some curves stay very close, like the cases of  X  =0 . 01 or 0 . 001 in subplots (c) and (d), which are almost com-pletely overlapped. From the figures, we can see for the RCV1 data, the optimal choice of  X  is 0.01 or 0.1, while for the Swissprot data, the optimal setting is 0.001 or 0.01. In general, the methods using the hierarchy information is always better than LSI without this information. However, the setting for RCV1 data seems to be a little bit sensitive X  large values (e.g.  X   X  X  X  to become LSI) or small values (e.g.  X  =0 . 001 ) both degrade the performance. In general, the setting of parameter  X  depends on the nature of data. In practice, we need a valid set to guide the selection. class hierarchies in text mining. Our method is based on a hierarchy-regularized approach that incorporates the prox-imity of classes within the hierarchy which implies a con-nection between the documents belonging the same class. Additionally, the approach uses the specificity of classes which can be measured by the number of documents be-longing to each class. We use our approach to derive a new hierarchy-regularized method for feature transforma-tion called HLSI. HLSI enables us to integrate the informa-tion within a class hierarchy into a variety of learning and retrieval tasks. Additionally, our experiments on two real-world text data sets demonstrate that the proposed meth-ods are capable to derive low dimensional and highly de-scriptive feature spaces that mirror the structure of the un-derlying class hierarchy. Thus, they are well-suited for a variety of learning tasks employing hierarchical class sys-tems. For future work, an interesting direction is to develop a global method for feature selection based on hierarchy in-duced graphs. This is especially interesting for applications demanding human understandable class models.
 features micro-averaged F 1 (a,c) and macro-averaged F 1 (b,d).
