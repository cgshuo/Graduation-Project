 This research is investigating a new interaction paradigm for In-teractive Information Retrieval (IIR), where all input and output is mediated via speech. While such information systems have been important for the visually impaired for many years, a renewed fo-cus on speech is driven by the growing sales of internet enabled mobile devices. Presenting search results over a speech-only com-munication channel involves a number of challenges for users due to cognitive limitations and the serial nature of the audio chan-nel [2]. Other research has shown that one cannot just  X  X olt on X  speech recognizers and screen readers to an existing system [5]. Therefore the aim of this research is to develop a new framework for effective and efficient IIR over a speech-only channel: a Spoken Conversational Search System (SCSS) which provides a conversa-tional approach to determining user information needs, presenting results and enabling search reformulations. This research will go beyond current Voice Search approaches by aiming for a greater integration between document search and conversational dialogue processes in order to provide a more efficient and effective search experience when using a SCSS. We will also investigate an infor-mation seeking model for audio and language models.

Presenting a Search Engine Result Page (SERP) over a speech-only communication channel presents a number of challenges, e.g., the textual component of a standard search results list has been shown to be ineffectual [4]. The transient nature of speech poses problems due to memory constraints, and makes the possibility of  X  X kimming X  back and forth over a list of results (a standard pro-cess in browsing a visual list) difficult. These issues are greatly exacerbated when the result being sought is further down the list.
This research will advance the knowledge base by: Thus this research will transform search over a speech-only com-munication channel by using an inherently interactive and conver-sational experience.

Developing a suitable SCSS requires an iterative user-centered approach, allowing us to design with the user in mind while match-ing the user X  X  mental model [3]. Within a mixed-methods method-ology several techniques will be used to form design decisions, i.e., role plays, Wizard of Oz methodologies and crowdsourcing. These techniques will allow us to gather data about user interaction pat-ters and understand their linguistic behaviour [1, 2], and form an information seeking model for audio.

Our preliminary study measured the impact of the length of web search summaries in audio communication channels. The analysis showed that users preferred shortened summaries for queries with a clear query intent [6]. These findings emphasized the importance of developing techniques that can both predict when a query needs to be refined and provide suggestions for refinement to a conversa-tional interface.

Thus the overall findings will advance the development of spo-ken search user interfaces since we address established challenges of Voice Search, but also seek to integrate these challenges within a framework for a SCSS.
 H.5.1 [ Multimedia Information Systems ]; H.3.3 [ Information Search and Retrieval ]; H.5.2 [ User Interfaces ] Conversational Search; Interactive Information Retrieval; Search Result Summarisation; Spoken Retrieval
