 Curren t searc h engines do not, in general, perform well with longer, more verb ose queries. One of the main issues in pro-cessing these queries is iden tifying the key concepts that will have the most impact on e ectiv eness. In this pap er, we de-velop and evaluate a technique that uses query-dep enden t, corpus-dep enden t, and corpus-indep enden t features for au-tomatic extraction of key concepts from verb ose queries. We sho w that our metho d achiev es higher accuracy in the iden-ti cation of key concepts than standard weigh ting metho ds suc h as inverse documen t frequency . Finally , we prop ose a probabilistic mo del for integrating the weigh ted key concepts iden ti ed by our metho d into a query , and demonstrate that this integration signi can tly impro ves retriev al e ectiv eness for a large set of natural language description queries deriv ed from TREC topics on sev eral newswire and web collections. H.3.3 [ Information Searc h and Retriev al ]: Query For-mulation Algorithms, Exp erimen tation, Theory Information retriev al, verb ose queries, key concepts extrac-tion
Automatic extraction of concepts of interest from a larger body of text have pro ved to be useful for summarization [16], keyw ord extraction [15], con ten t-targeted adv ertising [33], named entity recognition [4] and documen t clustering [11]. In this pap er, we describ e an extension of automatic concept extraction metho ds for the task of extracting key concepts from verb ose natural language queries.

Information retriev al researc h is generally more focused on keywor d queries : terse queries that con tain only a small selection of key words from a more verb ose description of the actual information need underlying the query . TREC topics illustrate the di erence between a keyw ord query and a description query . A TREC topic consists of sev eral parts, eac h of whic h corresp onds to a certain asp ect of the topic. In the example at Figure 1, we consider the title (denoted &lt; title &gt; ) as a keyw ord query on the topic, and the descrip-tion of the topic (denoted &lt; desc &gt; ) as a natural language description of the information request. &lt;num&gt; Number 829 &lt;title&gt; Spanish Civil War support &lt;desc&gt; Provide information on all kinds of material international support provided to either side in the Spanish Civil War.
 Figure 1: An example of &lt; title &gt; and &lt; desc &gt; of a TREC topic.

It migh t app ear obvious to the reader that the key con-cept of the topic in Figure 1 is Spanish Civil War , rather than, say, material international supp ort , whic h only serv es to complemen t the key concept. However, there is no ex-plicit information in the description itself to indicate whic h of these concepts is more imp ortan t.

A simple exp erimen t illustrates this point. When running the &lt; desc &gt; query from Figure 1 on three commercial web searc h engines, the rst page of the results (top ten retriev ed documen ts) for eac h of the searc h engines con tains six, four and zero documen ts related to the Spanish Civil War, re-spectiv ely. Only one of the searc h engines returns docu-men ts men tioning international supp ort during the war. In con trast, running the &lt; title &gt; query from Figure 1 results, for all three searc h engines, in all the documen ts returned on the rst page referring to some asp ect of Spanish Civil War, including international supp ort during the war.
A verb ose query could also poten tially con tain two or more equally essen tial key concepts. For example, consider a query What did Steve Jobs say about the iPo d? 1 , whic h con tains two key concepts, Steve Jobs and iPo d , that must
This example originally app eared on the Powerset blog: http://blog.p owerset.com/ Table 1: Retriev al e ectiv eness comparison (mean average precision) for &lt; title &gt; and &lt; desc &gt; on sev eral TREC collections. be someho w related in a retriev ed documen t in order for it to be relev ant. When examining the top ten documen ts re-triev ed by three commercial web searc h engines in resp onse to this query , we note that some of them con tain only infor-mation about a single key concept (e.g., a magazine article \What Did the Professor Say? Chec k Your iPod"), while others con tain both concepts, but with no explicit relation between them (e.g., a blog entry \iP od Giv eaway: Design a Stev e Jobs Mo vie Poster" ). In con trast, when examining the top ten documen ts retriev ed in resp onse to a keyw ord query \steve jobs" +iPo d , we note that most of them discuss Stev e Jobs in some relation to the iPod (e.g., a link to a video documen ting an iPod introduction by Stev e Jobs, whic h did not app ear on the rst page of results for the more verb ose query).

Our goal in this pap er is to overcome the dicult y of key concepts detection in verb ose natural language queries. We hypothesize that the iden ti cation of the key query con-cepts will have a signi can t positiv e impact on the retriev al performance for verb ose queries (suc h as &lt; desc &gt; queries), whic h often mix sev eral key and complemen tary concepts, as discussed in the above examples. Treating all query con-cepts equally causes loss of focus on the main topics of the query in the retriev al results, e.g., returning documen ts that discuss material international supp ort , but not the Spanish Civil War .

This loss of focus causes a parado xical situation in that a keyw ord query that pro vides less information about the topic than its more verb ose natural language coun terpart attains better retriev al e ectiv eness. Indeed, when compar-ing the e ectiv eness of the retriev al using either &lt; title &gt; or &lt; desc &gt; query types, we note that &lt; title &gt; queries consis-ten tly perform better on a variet y of TREC collections (see Table 1).

In this pap er we: (i) presen t a general probabilistic mo del for incorp orating information about key concepts into the base query , (ii) dev elop a sup ervised mac hine learning tech-nique for key concept iden ti cation and weigh ting, and (iii) empirically demonstrate that our technique can signi can tly impro ve retriev al e ectiv eness for verb ose queries.
In this section we presen t our mo del of key concept selec-tion for verb ose queries. We start by dev eloping a formal probabilistic mo del for the utilization of key concepts for information retriev al. We then pro ceed to detail the su-pervised mac hine learning technique used for key concept iden ti cation and weigh ting.
We start by ranking a documen t d in resp onse to query q by estimating the probabilit y p ( q j d ), whic h is a standard approac h in the language modeling retriev al mo del [25, 10]. Next, we consider all possible implicit concepts c i (we defer the treatmen t of concept iden ti cation to Section 2.2) that could poten tially generate the actual query q , and get that
A common way to estimate a join t conditional probabilit y is using a linear interp olation of the individual conditional probabilities [20, 18, 30]. Accordingly , we use a linear inter-polation of p ( q j d ) and p ( q j c i ) to estimate p ( q j d; c ing some probabilit y algebra, we can rank a documen t d in resp onse to a query q using an estimate where 0 is a free parameter in [0 ; 1]. Assuming a uniform distribution for both p ( q ) and p ( c i ) (whic h is reasonable, given no prior kno wledge), the above is rank-equiv alen t to where is normalized suc h that = 0
We note that Equation 2 tak es a general form, where eac h documen t is rank ed according to the com bination of its prob-abilit y of generating the query itself, and a weigh ted sum of its probabilities of generating eac h implicit concept c i concept weigh ts are determined by how well they \represen t" the query q . As using all possible implicit concepts for rank-ing a documen t is infeasible, and moreo ver the probabilit y p ( c i j q ) will be close to zero for all but very few query-related concepts, one may appro ximate the ranking above by using only a xed num ber of concepts with the highest weigh ts. Thus, Equation 2 may be interpreted as a query expansion technique suc h as local and global documen t analysis [32], laten t concept expansion [23] or query expansion using ran-dom walk mo dels [9] among others.

Alternativ ely, we may only consider the explicit concepts, i.e., the concepts that app ear in the actual query q . This is the approac h we tak e in this pap er, as we are interested in disco vering key concepts in verb ose natural language queries. Thus, Equation 2 at above reduces to
We use Equation 3 for examining the bene ts for retriev al performance attained by our technique for concept iden ti -cation and weigh ting discussed in the follo wing sections.
We note that the ranking principle presen ted in Section 2.1 is not committed to any particular de nition of a con-cept. A concept can be a single word, an idiom, a restricted collo cation or a free com bination of words [3].With this in mind, we use noun phrases extracted from the queries as concepts.

Noun phrases have pro ven to be reliable for key concept disco very in some past work on information retriev al [32, 7, 2] and natural language pro cessing [15], and are exible enough to naturally distinguish between words, collo cations, entities and personal names among others. For instance, the description query presen ted in Figure 1 can be split into the follo wing noun phrases: [information, kinds, material inter-national supp ort, side, Spanish Civil War] . These phrases represen t the di eren t asp ects presen t in the actual query .
This section focuses on concept weigh ting. Giv en Equa-tion 3, we treat the probabilit y p ( c i j q ) as a weigh t assigned to eac h query concept that re ects how well concept c i rep-resen ts the query at hand.

Instead of estimating p ( c i j q ) directly , we tak e a di eren t approac h, whic h allo ws us to leverage some non-query spe-ci c information towards key concept detection. We mak e the follo wing assumptions:
Assumption A. Eac h concept c i can be assigned to one
Assumption B. A global function h k ( c i ) exists, suc h that Follo wing the assumptions at above, we use a normalized varian t of h k ( c i ), to deriv e an estimate In other words, concepts for whic h we have the highest con-dence in mem bership in class KC are regarded as the best \represen tations" for the query among other query concepts.
There are sev eral well-kno wn word weigh ting techniques we could use for deducing h k ( c i ). However, instead of giving a preference to any single technique, we choose a sup ervised mac hine learning approac h, and use di eren t weigh tings as input features for the weigh t assignmen t algorithm. Similar approac hes were sho wn to work well in some previous work on keyphrase detection [12, 29, 15].

Formally , we consider a training set of lab eled instances where x i is a feature vector represen ting the concept c l is a binary lab el, indicating whether c i 2 KC . Giv en the training set, we seek to learn a ranking function of the form h : X ! R , suc h that h k ( x i ) &gt; h k ( x j ) entails that con-cept c i has a higher con dence in mem bership in class KC than concept c j . Once the learning pro cess completes, we can pro ceed to directly use h k ( x i ) to calculate an estimate ^ p ( c i j q ), as sho wn in Equation 4.
A possible interpretation for h k ( c i ) is a conditional proba-bilit y p ( KC j c i ), but we do not require h k ( c i ) to be a prop er probabilit y function.

What is left in order to complete the deriv ation of the mac hine learning metho d is the feature generation pro cess for eac h instance x i . We use a mix of both novel weigh ting features and weigh ting features used in previous work on keyphrase detection [12, 29, 15] to represen t eac h concept. Table 2 presen ts the summary of the features used for con-cept weigh ting. More detailed explanation of eac h feature is given below.
 is cap ( c i ) This feature is a Boolean indicator that is set tf ( c i ) Concept term frequency in the corpus. We assume idf ( c i ) Concept inverse document frequency in the corpus. ridf ( c i ) Residual IDF is the di erence between the ob-wig ( c i ) Weighte d Information Gain (WIG) [34] measures g tf ( c i ) We use the Google n-gr ams [5] (English word n-qp ( c i ), qe ( c i ) We use a large query log consisting of ap-
Using sup ervised mac hine learning techniques for an au-tomatic extraction of key concepts from documen ts was rst prop osed by Turney [29], and later explored by sev eral other researc hers [12, 15]. Similar mac hine learning techniques have also pro ved bene cial for other tasks suc h as named entity recognition [4], con ten t-targeted adv ertising [33] and summarization [16].

Key concept detection in verb ose queries has been a sub-ject of some previous work in information retriev al. Allan et al. [2] use a set of linguistic and statistical metho ds and a pro ximit y operator to disco ver core terms in &lt; desc &gt; queries. According to Allan et. al [2], a core term is a term that must be presen t in a documen t for the documen t to be rel-evant. Callan et al. [7] use noun phrases, named-en tities recognition, exclusionary constrain ts and pro ximit y opera-tors to con vert &lt; desc &gt; queries into structured INQUER Y queries. Although similar to our work, the key concepts extraction and weigh ting techniques discussed in these pa-pers are focused on queries deriv ed from TREC topics (e.g., remo ving \stop-phrases" common in TREC queries, or as-signing higher weigh ts to concepts that app ear in the title of the topic), while our key concept extraction metho d is more general and is not biased towards any speci c query type.

Some previous work on query expansion [32, 6, 9] uses expansion term groupings to balance the various asp ects of the original query in the expanded query . Single terms [6], noun phrases [32] or n-grams [9] are used to determine query asp ects. These metho ds, however, do not assign explicit weigh ts to asp ects.

Recen t work by Kumaran and Allan [17] addresses the is-sue of extracting the optimal (in terms of retriev al e ectiv e-
In our exp erimen ts M = 50.
Liv e Searc h 2006 searc h query log excerpt. ness) sub-query from the original long query . Their approac h involves extracting a short list of candidate sub-queries us-ing the mutual information measure and presen ting this list to the user, allo wing her to replace the original query by one of the candidates from the list. This approac h resulted in signi can t impro vemen ts over retriev al with the original &lt; desc &gt; queries.

In most of the previous work on retriev al the di eren tia-tion between key and non-k ey concepts is done via statisti-cal metho ds suc h as term and documen t frequency weigh t-ing [27] or term-sp eci c smo othing [14, 21]. However, these metho ds are usually based on single word collection statis-tics, and do not alw ays capture the key conc epts , rather than words. In con trast to previous work, we do not con-strain ourselv es to a speci c weigh ting scheme to detect the key query concepts. Instead, we adopt the sup ervised ma-chine learning approac h [29, 12, 15] for key query concept extraction, and use a div erse mix of features as inputs for the sup ervised mac hine learning algorithm.
In this section, we describ e the exp erimen tal results of our work. Section 4.1 focuses on the concept classi cation and weigh ting exp erimen ts, Section 4.2 is dedicated to the anal-ysis of the features used in the mac hine learning algorithm, and Section 4.3 details the retriev al exp erimen ts based on the weigh ted concepts.

We pro vide a summary of the corp ora used for our exp er-imen ts in Table 3. We note that collections vary both by type (ROBUST04 is a newswire collection, while W10g and GO V2 are web collections), num ber of documen ts and num-ber of available topics, thus pro viding a div erse exp erimen-tal setup for assessing the robustness of our classi cation, weigh ting and retriev al metho ds.
 Table 3: Summary of TREC collections and topics used in Section 4
Noun phrases were extracted using Mon tyLingua natural language pro cessing tool [19]. All concept classi cation and weigh ting exp erimen ts were performed using the algorithms implemen ted in Weka [31], a collection of mac hine learning algorithms for data mining tasks. Indexing and retriev al was done using Indri [28], whic h is a part of the Lem ur Toolkit [24]. All indexes and topics were stopp ed using a standard INQUER Y stop words list [1] and stemmed using a Porter stemmer [26]. Either &lt; desc &gt; or &lt; title &gt; portions of the TREC topics were used to construct the queries. In all retriev al exp erimen ts, Diric hlet smo othing with = 1500 is used.
In order to assess the e ectiv eness of the sup ervised ma-chine learning approac h outlined in Section 2.3, we emplo y the AdaBo ost.M1 meta-classi er with C4.5 decision trees as base learners [13]. AdaBo ost.M1 \boosts" the rep eated runs 1 ; : : : ; T of the base learners on various distributions over the training data into a single comp osite learner, whic h is often more e ectiv e than using any of the individual base learners. The AdaBo ost.M1 metho d was selected for sev eral reasons. First, it consisten tly outp erformed other classi -cation metho ds suc h as C4.5 decision tree or Naiv e Bayes classi er in the preliminary exp erimen ts we have conducted. Second, its output for a single input instance x i can be inter-preted not only as a binary classi cation decision ( c i 2 KC or c i 2 NK C ), but also as a weigh ted com bination of base hypotheses P j =1 ;:::;T w j h j ( x i ) [13]. This com bination nat-urally translates into a con dence function h k ( c i ), presen ted in Section 2.3.

In the rst suite of exp erimen ts, we examine how well our prop osed approac h separates the key and the non-k ey concepts. To this end, all the noun-phrases are extracted from the &lt; desc &gt; queries, and a single noun phrase, whic h is deemed to be the most suitable candidate, is selected as a key concept for eac h query (e.g., for the example at Figure 1, Spanish Civil War is selected as a key concept).
The AdaBo ost.M1 classi er is trained using a set of la-beled instances. At the test phase, for eac h tested query , the extracted noun phrases are rank ed according to their con dence level of belonging to class KC | h k ( c i ). The highest rank ed noun phrase is then selected as a key con-cept for the query .

We note that although some queries migh t con tain more than one key concept, selecting a single key concept per query has two imp ortan t adv antages. First, it poten tially simpli es the man ual key concept selection task, as the as-sessor is not required to determine the optimal num ber of key concepts for eac h query . Second, it establishes a lower bound on the accuracy of the key concept selection pro cess de ned above, as it pro vides a minimal amoun t of training data.

We use a cross-v alidation approac h; for eac h of the col-lections, queries are divided into subsets of 50 queries eac h. Eac h subset, in turn, is used as a test set, while the rest of the queries serv e as a lab eled training set. Exp erimen ts are run separately for eac h collection, and average results over all test sets are rep orted.

Our second exp erimen t suite is designed to test whether our key concept classi cation approac h indeed outp erforms a simple non-sup ervised weigh ting approac h where idf ( c weigh ts are directly used to rank the extracted noun phrases, and, as before, the highest rank ed noun phrase is selected as a key concept for the query .

Table 4 rep orts the accuracy and the mean recipr ocal rank (MRR) results when either AdaBo ost.M1 or idf ( c i ) ranking are used for key concept classi cation. Accuracy is sim-ply the percen tage of the correctly iden ti ed key concepts. MRR is the mean of the recipro cal ranks at whic h the key concepts were returned. A higher MRR score indicates a higher con dence in mem bership in KC class for key con-cepts, compared to other concepts in the query .

Table 4 sho ws that for all the tested collections AdaBo ost.M1 outp erforms idf ( c i ) ranking. We note that the di erence in the performance is inversely prop ortional to the collection size, whic h con rms our hypothesis (see Section 2.3) that for smaller corp ora, features other than collection term and documen t coun ts should be used in order to prev ent concept frequency underestimation.
 In a posterior analysis, we disco vered that most of the Table 4: Comparison of accuracy and MRR results for ROBUST04, W10g and GO V2 collections, when using either the AdaBo ost.M1 algorithm with the features detailed at Table 2, or a single idf ( c i ) feature for concept classi cation. classi cation errors were a result of an ambiguit y in a key concept selection pro cess. E.g., for the description query How are pets or animals used in ther apy for humans and what are the bene ts? (topic 794), the ther apy concept was mark ed as a key concept, while the classi cation algorithm assigned the highest rank to pets or animals concept, and the ther apy concept was rank ed second. Clearly , neither of the concepts describ es the query in its entiret y, and a key concept is better expressed by a com bination of the two (whic h is re ected by the title of the topic pet ther apy ). Multiple key concepts were more common in the RO-BUST04 collection than in the other two collections, as its topics tend to con tain more verb ose and grammatically com-plex description queries (e.g., consider the description query A relevant document would discuss how e e ctive government orders to better scrutinize passengers and luggage on inter-national ights and to step up screening of all carry-on bag-gage has been. (topic 341), for whic h the concept inter-national ights was mark ed as a key concept for a lack of a better choice). This migh t o er an explanation to the fact that the classi cation accuracy is lower for ROBUST04, compared to the GO V2 and W10g collections.

Another common issue is the case when a true key concept is \mask ed" by a non-k ey concept exhibiting the traits of a key concept. For example, in the description query What violent activities have Kur ds, or memb ers of the Workers Party of Kur distan (PKK), carrie d out in Germany? (topic 611), a key concept Kur distan PKK is mask ed by a strong collo cation violent activities .

In Section 4.3 we use the fact that although we do not at-tain a perfect accuracy in our classi cation exp erimen ts, the con dence levels assigned to eac h concept are generally re-liable, and integrate the weigh ted concepts into the original &lt; desc &gt; query to impro ve the retriev al performance.
In this section, we analyze the utilit y of the various fea-tures used in the classi cation task describ ed at above (refer bac k to Table 2 for the summary of the features).

We rep eat the key concept classi cation exp erimen ts us-ing AdaBo ost.M1, as describ ed in the previous section, while varying the features. In eac h iteration, we remo ve a single feature from the full feature set. Assuming indep endence between the di eren t features, a decrease in accuracy indi-cates how much does the remo ved feature con tribute to the overall accuracy rep orted in Table 4. Table 5 rep orts the feature analysis exp erimen ts results.

We note that feature con tribution to the overall accuracy varies in di eren t collections. For GO V2, a large web col-lection, external features suc h as g tf ( c i ), qp ( c i i ) wig ( c i ) g tf ( c i ) qp ( c i ) qe ( c i ) the most to classi cation performance are mark ed in bold. Table 6: Accuracy and MRR results for RO-BUST04, W10g and GO V2 collections, when using the AdaBo ost.M1 algorithm with the subset of fea-tures that attains the best classi cation accuracy ac-cording to Table 5. Feature ridf ( c i ) is not used for classi cation on collections ROBUST04 and GO V2. have little or no impact on the overall accuracy , while ac-curacy on ROBUST04 and W10g, whic h are smaller, de-grades with the remo val of these features. On the other hand, collection-dep enden t features suc h as tf ( c i ), idf ( c and ridf ( c i ) have little positiv e (or negativ e, as in the case of ridf ( c i )) impact on the overall accuracy for the ROBUST04, the smallest collection among the three. Surprisingly , both is cap ( c i ) and ridf ( c i ) have a negativ e, alb eit small, impact on the overall accuracy for a GO V2 collection.

Follo wing the analysis at Table 5 we remo ve the feature ridf ( c i ) from the feature sets for collections ROBUST04 and GO V2, and rep ort the classi cation accuracy and MRR at-tained by using an optimal com bination of features for all the collections in Table 6.
In this section, we explore the retriev al bene ts of using the concept classi cation and weigh ting technique discussed in the previous sections. We treat the normalized concept con dence function h k ( c i ) obtained by the concept ranking by AdaBo ost.M1 presen ted in Section 4.1 as an estimate for the probabilit y ^ p ( c i j q ), and rank documen ts according to Equation 3. (In this, and all the subsequen t exp erimen ts, the optimal feature com bination as detailed in Table 6 is used for concept classi cation and ranking.)
First, we calibrate the num ber of weigh ted concepts added to the base &lt; desc &gt; query . The results are presen ted in Fig-ure 2. We note that an addition of the single highest rank ed concept to the original query , i.e., the integration of a con-cept iden ti ed as a key concept in Section 4.1, pro vides a substan tial performance boost (in terms of mean average precision) on all the tested collections. In two out of three collections, performance is further increased when a second-highest rank ed concept is also integrated into the base query . We note that adding more than two concepts attains no fur-ther retriev al performance gain. This can be interpreted as the fact that, on average, the description queries used for our exp erimen ts may be expressed by a com bination of two key concepts. This is eviden t from an example at Figure 1: Figure 2: Changes in MAP , when varying the num-ber of concepts added to the original &lt; desc &gt; query . description query can be rewritten as a com bination of con-cepts \Sp anish Civil War" +\material international supp ort" without noticeable loss in query expressiv eness. We note, however, that although this observ ation holds for a large portion of tested queries, there are cases in whic h relying entirely on the extracted concepts and discarding the origi-nal query causes a degradation in query expressiv eness. (For instance, consider the description for Topic 714: What re-strictions are plac ed on older persons renewing their drivers' licenses in the U.S.? for whic h the two highest ranking con-cepts are older persons and drivers license . Clearly , these two concepts do not fully express the entire query .)
Due to the above observ ations, we use the com bination of the base &lt; desc &gt; query and the two highest-rank ed terms weigh ted by ^ p ( c i j q ), whic h results in optimal retriev al perfor-mance for most collections (see Figure 2), for our next suite of retriev al exp erimen ts. (W e have also conducted exp eri-men ts with the unweigh ted varian t of this com bination, but it yielded sligh tly inferior performance to that of the com bi-nation of the weigh ted concepts, and thus is not considered in the remainder of this section.)
We compare the retriev al results (in terms of precision at 5 and mean aver age precision ) obtained by this setting, de-noted KeyConc ept[2] &lt; desc &gt; , to the results obtained using either title or description query alone (denoted &lt; title &gt; and &lt; desc &gt; , resp ectiv ely). In addition, we compare the e ec-tiveness of this metho d to retriev al using sequential depen-dency mo del [22], denoted SeqDep &lt; desc &gt; , whic h integrates the base &lt; desc &gt; query with sequen tial bi-grams deriv ed from query words, and uses ordered and un-ordered pro x-imit y operators. We presen t examples of Indri structured queries [28] represen ting eac h of these retriev al techniques &lt; title &gt; #combine( Spanish Civil War support )
SeqDep &lt; desc &gt;
KeyConc ept[2] &lt; desc &gt; according to the AdaBo ost.M1 algorithm output.
 39 : 80 t 19 : 28 56 : 88 d 27 : 53 t d 40 : 40 t 20 : 46 t d 56 : 77 d 27 : 27 t d ing retriev al metho d per collection. Signi can t di erences with in Table 7. Comparison of the retriev al results obtained by the di eren t techniques is presen ted in Table 8.

We note that for all collections KeyConc ept[2] &lt; desc &gt; out-performs &lt; desc &gt; retriev al both in terms of MAP and prec@5, often to a statistically signi can t degree. For ROBUST04 and W10g collections KeyConc ept[2] &lt; desc &gt; also outp er-forms the &lt; title &gt; retriev al. We note, however, that for these collections &lt; title &gt; queries are more verb ose than the &lt; ti-tle &gt; queries for the GO V2 collection.

Table 8 demonstrates that the retriev al performance (in terms of MAP) of KeyConc ept[2] &lt; desc &gt; metho d is sligh tly inferior to the performance of SeqDep &lt; desc &gt; on GO V2 col-lection, and is better than the performance of SeqDep &lt; desc &gt; on ROBUST04 and W10g collections. No statistically sig-ni can t di erences between the retriev al e ectiv eness of the two metho ds were found.

In terms of retriev al eciency , the queries pro duced by our metho d are more succinct (see example at Table 7), es-pecially in the case of the verb ose &lt; desc &gt; queries discussed here, than those pro duced by SeqDep &lt; desc &gt; . This indi-cates that most of the retriev al performance gain can be attributed to only a few highest rank ed concepts. We also note that, compared to the sequen tial dep endency mo del, we do not incorp orate any pro ximit y information in our queries, and treat our concepts as \bag of words" queries. We leave the exploration of the bene ts of application of pro ximit y operators to our retriev al metho ds for future work.
In this pap er we address the issue of retriev al using ver-bose queries. We use sev eral standard TREC collections and corresp onding topics to demonstrate that the curren t retriev al metho ds perform better, on average, with keyw ord title queries than with their longer description coun terparts.
One of the main issues in pro cessing verb ose queries is the dicult y of iden tifying the key concepts. This dicult y can poten tially lead to a lack of focus on the main topics of the query in the retriev ed documen t set. To this end, we prop ose a sup ervised mac hine learning technique for disco vering key concepts in verb ose queries. We detail the query-dep enden t, corpus-dep enden t and corpus-indep enden t features used as inputs for our mac hine learning algorithm. We use our tech-nique to iden tify and assign weigh ts to key concepts in natu-ral language description queries deriv ed from TREC topics, and sho w that using our iden ti cation metho d substan tially impro ves the average accuracy in key concept iden ti cation over the standard inverse documen t frequency measure, even if the size of the available training set is relativ ely small.
Next, we use the highest-rank ed concepts for eac h query to impro ve the retriev al e ectiv eness of the verb ose queries on sev eral standard TREC newswire and web collections. We prop ose a formal probabilistic mo del for incorp orating query and key concepts information into a single structured query , and sho w that using these structured queries results in a statistically signi can t impro vemen t in retriev al per-formance over using the original description queries on all tested corp ora. In some cases, our structured queries even attain a better retriev al performance than the title queries on the same topic.

The empirical results of this pap er are encouraging, as us-ing a simple query-concept com bination mo del outp erforms the original query to a statistically signi can t degree, and attains a comparable retriev al performance to that of using a more elab orate (and computationally more demanding) sequen tial dep endency mo del. We believ e that further gains in retriev al performance migh t be attained by emplo ying ad-ditional syn tactic and seman tic features of natural language verb ose queries and by exploring the utilization of di eren t pro ximit y operators for query-concept com bination.
This work was supp orted in part by the Cen ter for Intelli-gen t Information Retriev al, in part by Microsoft Liv e Labs, and in part by NSF gran t #IIS-0534383. Any opinions, nd-ings and conclusions or recommendations expressed in this material are the authors' and do not necessarily re ect those of the sponsor.
