 Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DB-pedia (Lehmann et al., 2014) have become ex-tremely useful resources for many NLP-related ap-plications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form ( h,r,t ) , indicating that entities h and t are connected by relation r . Al-though powerful in representing complex data, the symbolic nature makes KGs hard to manipulate.
Recently, knowledge graph embedding has at-tracted much attention (Bordes et al., 2011; Bor-des et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the in-herent structure of the original graph.

Most of the existing KG embedding methods model triples individually, ignoring the fact that entities connected to a same node are usually im-plicitly related to each other, even if they are not directly connected. Figure 1 gives two examples. Shaquille O Neal and NBA in the former ex-ample and Nevada and Utah in the latter exam-ple are implicitly related to each other, through the intermediate nodes Phoenix Suns and USA re-spectively. We refer to such implicit relationships as contextual connectivity patterns (CCPs). Re-lationships explicitly represented in triples are re-ferred to as local connectivity patterns (LCPs). In most of the existing methods, only LCPs are ex-plicitly modeled.

This paper proposes a two-stage embedding scheme that explicitly takes into account both C-CPs and LCPs, called context-dependent KG em-bedding . In the first stage, each CCP is formalized as a knowledge path , i.e., a sequence of entities and relations occurring in the pattern. A word em-bedding model is adopted to learn embeddings of entities and relations, by taking them as pseudo-words. The embeddings are enforced compatible within each knowledge path, and hence can cap-ture CCPs. In the second stage, the learned em-beddings are fine-tuned by an existing KG embed-ding technique. Since such a technique requires the embeddings to be compatible on each individ-ual triple, LCPs are also encoded.

The advantages of our approach are three-fold. 1) It fully exploits both CCPs and LCPs, and can obtain more accurate embeddings. 2) It is a gen-eral scheme, applicable to a wide variety of word embedding models in the first stage and KG em-bedding models in the second. 3) No auxiliary data is further required in the two-stage process, except for the original graph.

We evaluate our approach on two publicly avail-able data sets, and achieve significant and consis-tent improvements over state-of-the-art methods in the link prediction and triple classification tasks. The learned embeddings are not only more accu-rate but also more stable. We are given a KG with nodes corresponding to entities and edges to relations. Each edge is denot-ed by a triple ( h,r,t ) , where h is the head entity, t the tail entity, and r the relation between them. Entities and relations are represented as vectors, matrices, or tensors in a continuous vector space. Context-dependent KG embedding aims to auto-matically learn entity and relation embeddings, by using observed triples O in a two-stage process. 2.1 Modeling CCPs The first stage models CCPs conveyed in the KG. Each CCP is formalized as a knowledge path, i.e., a sequence of entities and relations occurring in the pattern. For the CCPs in Figure 1, the associ-ated knowledge paths are: We fix the length of knowledge paths to 5. Dur-ing path extraction, we ignore the directionality of
Given the extracted knowledge paths, we em-ploy word embedding models to pre-train the em-beddings of entities and relations, by taking them as pseudo-words. We use two word embedding models: CBOW and Skip-gram (Mikolov et al., 2013a; Mikolov et al., 2013b). In CBOW, words in the context are projected to their embeddings and then summed. Based on the summed embedding, log-linear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classi-fiers are further adopted to predict its context. We restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and re-lation embeddings pre-trained in this way are re-quired to be compatible within each knowledge path, and thus can encode CCPs.

Perozzi et al. (2014) and Goikoetxea et al. (2015) have proposed similar ideas, i.e., to gener-ate random walks from online social networks or from the WordNet knowledge base, and then em-ploy word embedding techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with differen-t types of edges. Both nodes (entities) and edges (relations) are included during knowledge path ex-traction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embeddings as final output. 2.2 Modeling LCPs The second stage models LCPs conveyed in the KG. We employ three state-of-the-art KG embed-ding models, namely SME (Bordes et al., 2014), TransE (Bordes et al., 2013), and SE (Bordes et al., 2011) to fine-tune the pre-trained embeddings. These three models work in the following way. First, entities are represented as vectors, and re-lations as operators in an embedding space, char-acterized by vectors (SME and TransE) or matri-ces (SE). Then, for each triple ( h,r,t ) , an energy function f r ( h,t ) is defined to measure its plausi-bility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation em-beddings, a margin-based ranking loss, i.e., L = is minimized. Here, t + = ( h,r,t )  X  O is an ob-served (positive) triple; N t + is the set of negative triples constructed by replacing entities in t + , and t  X  = ( h 0 ,r,t 0 )  X  N t + ;  X  is a margin separating positive and negative triples; [ x ] Table 1 summarizes the entity/relation embed-dings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014).
We adopt stochastic gradient descent to solve the minimization problem, by taking entity and re-lation embeddings pre-trained in the first stage as fine-tuned in this way are required to be compati-ble within each triple, and thus can encode LCPs.
Socher et al. (2013) have proposed a similar idea, i.e., to use embeddings learned from an aux-iliary corpus as initial values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and nat-urally avoids the entity linking task. We test our approach on the tasks of link predic-tion and triple classification. Two publicly avail-able data sets are used. The first is WN18 released Net, consisting of 18 relations and the entities con-nected by them. The second is NELL186 released quent 186 relations in NELL (Carlson et al., 2010) and the associated entities. Triples are split into training/validation/test sets, used for model train-ing, parameter tuning, and evaluation respectively. Knowledge paths are extracted from training sets. Table 2 gives some statistics of the data sets.
To perform context-dependent KG embedding, we use CBOW and Skip-gram in the pre-training stage, and SME, TransE, and SE in the fine-tuning stage. We take randomly initialized SME, TransE, and SE as baselines, denoted as *-Random. We do not compare to the setting that employs only CBOW or Skip-gram, since it does not provide an energy function to calculate triple plausibility, which hinders the evaluation of both tasks. 3.1 Link Prediction Link prediction is to predict whether there is a spe-cific relation between two entities.
 Evaluation Protocol. For each test triple, the head is replaced by every entity in the KG, and the energy is calculated for each corrupted triple. Ranking the energies in ascending order, we get the rank of the correct answer. We can get another rank by corrupting the tail. We report two metrics on the test sets: Mean (averaged rank) and Hit-s@10 (proportion of ranks no larger than 10). Implementation Details. To train CBOW and Skip-gram, we use the word2vec implementation-itive one. The context size is fixed to 5. To train SME, TransE, and SE, we use the implementation-We vary the learning rate in { 0 . 01 , 0 . 1 , 1 , 10 } , the dimension k in { 20 , 50 } , and the margin  X  in { 1 , 2 , 4 } . The best model is selected by monitor-ing Hits@10 on the validation sets, with a total of at most 1000 iterations over the training sets. Results. Table 3 reports the results on the test sets of WN18 and NELL186. The improvements of CBOW/Skip-gram over Random are also given. Statistically significant improvements are marked by  X  (sign test, significance level 0.05). The result-s show that a pre-training stage consistently im-proves over the baselines for all the methods on both data sets. Almost all of the improvements are statistically significant. 3.2 Triple Classification Triple classification aims to verify whether an un-seen triple is correct or not.
 Evaluation Protocol. Triples in the validation and test sets are labeled as positive instances. For each positive instance, we construct a negative in-stance by randomly corrupting the entities. During classification, a triple is predicted to be positive if the energy is below a relation-specific thresh-old  X  r ; otherwise negative. We report two metric-s on the test sets: micro-averaged accuracy (per-instance average) and macro-averaged accuracy (per-relation average).
 Implementation Details. We use the same pa-rameter settings as in the link prediction task. The relation-specific threshold  X  r is determined by maximizing Micro-ACC on the validation sets. Results. Table 4 reports the results on the test sets of WN18 and NELL186. The results again demonstrate both the superiority and the generali-ty of our approach. 3.3 Discussions This section is to explore why pre-training helps in KG embedding, specifically in link prediction.
We first test different random initializations in traditional KG embedding models. We run SME (linear) twice on WN18, with two different initial-ization settings. Both are randomly sampled from the same uniform distribution, but with differen-t seeds, referred to as Random-I and Random-II. Each setting finally gets 10,000 ranks on the tween the two settings, we analyze the ranks indi-vidually, rather than reporting aggregated metrics (Mean and Hits@10). Specifically, we distribute the 10,000 instances into different bins according to the ranks given by one setting (e.g. Random-I). Instances assigned to the i -th bin have the same rank of i , that means, they are all ranked in the i -th position by this setting. Then, within each bin, we calculate the average rank of the instances given by the other setting (e.g. Random-II). If the av-erage rank differs drastically from the bin ID, the instances in this bin are ranked significantly dif-ferently by the two settings. Figures 2(a) and 2(b) show the results, with the instances distributed ac-cording to Random-I and Random-II respectively. In both cases, we retain the bins with ID no larger than 50, covering about 85% of the instances. In most of the bins, the average rank (red bars in the figures) differs drastically from the bin ID (black bars in the figures), indicating that the ranks giv-en by Random-I and Random-II are significantly different at the instance level. The results demon-strate the non-convexity of SME (linear): different initial values lead to different local minimum.
We further compare the settings of initial val-ues 1) randomly sampled from a uniform distri-bution (Random) and 2) pre-trained by Skip-gram (Skip-gram). The results are given in Figures 2(c) and 2(d). In most of the bins Skip-gram has an average rank lower than the bin ID (Figure 2(c)), while Random has an average rank much higher than the bin ID (Figure 2(d)), implying that Skip-gram performs better than Random-I at the in-stance level. The results indicate that pre-training might help in finding better initial values which lead to better local minimum.

Finally we test our two-stage KG embedding scheme where the skip-gram model itself is giv-en two different initialization settings, say Skip-gram-I and Skip-gram-II. The results are given in Figures 2(e) and 2(f). In each of the first 20 bins, Skip-gram-I and Skip-gram-II get an average rank almost the same with the bin ID, implying that the two settings perform quite similarly, particularly at the highest ranking levels. The results indicate that a pre-training stage might help in obtaining more stable embeddings. We have proposed a novel two-stage scheme for KG embedding, called context-dependent KG em-bedding. In the pre-training stage CCPs are encod-ed by a word embedding model, and in the fine-tuning stage LCPs are encoded by a traditional KG embedding model. Since both types of connectiv-ity patterns are explicitly taken into account, our approach can obtain more accurate embeddings. Moreover, our approach is quite general, applica-ble to various word embedding and KG embed-ding models. Experimental results on link predic-tion and triple classification demonstrate the supe-riority, generality, and stability of our approach.
As future work, we plan to 1) Investigate the ef-ficacy of longer CCPs (i.e. knowledge paths with lengths longer than 5). 2) Design a joint model that encodes LCPs and CCPs simultaneously. More-over, our approach actually reveals the possibili-ty of a broad idea, i.e., initializing an embedding model by another embedding model. We would also like to test the feasibility of other such strate-gies, e.g., initializing SME by TransE, so as to combine the benefits of both models.
 We would like to thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by the National Natural Science Foundation of China (grant No. 61402465), the S-trategic Priority Research Program of the Chinese Academy of Sciences (grant No. XDA06030200), and the National Key Technology R&amp;D Program (grant No. 2012BAH46B03).
