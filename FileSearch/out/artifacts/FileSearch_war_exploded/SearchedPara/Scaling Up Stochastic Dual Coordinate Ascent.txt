 Stochastic Dual Coordinate Ascent (SDCA) has recently emerged as a state-of-the-art method for solving large-scale supervised learning problems formulated as minimization of convex loss functions. It performs iterative, random-coordinate updates to maximize the dual objective. Due to the sequential nature of the iterations, it is typically implemented as a single-threaded algorithm limited to in-memory datasets. In this paper, we introduce an asyn-chronous parallel version of the algorithm, analyze its con-vergence properties, and propose a solution for primal-dual synchronization required to achieve convergence in practice. In addition, we describe a method for scaling the algorithm to out-of-memory datasets via multi-threaded deserializa-tion of block-compressed data. This approach yields suffi-cient pseudo-randomness to provide the same convergence rate as random-order in-memory access. Empirical evalu-ation demonstrates the efficiency of the proposed methods and their ability to fully utilize computational resources and scale to out-of-memory datasets.
Efficient linear learning techniques are essential for train-ing accurate prediction models in big-data business-critical applications. Examples of such applications include text classification, click probability estimation in online advertis-ing, and malware detection. In these domains, dimension-ality of representation induced by key predictive features is very high: for word n-grams, user IPs, advertisement IDs, and file signatures, many millions and billions of possible values exist.

Despite high overall dimensionality, examples in such do-mains are typically very sparse with few non-zero features encoded directly or via feature hashing. This results in com-putationally cheap prediction, making linear models a pop-ular choice for high-throughput applications. For additional accuracy gains, linear models can be extended via polyno-mial expansions explicitly or implicitly [2].
 c  X 
While training linear models has been a well-studied area of machine learning and optimization for decades, in recent years, a number of advances in stochastic gradient methods have considerably advanced the state-of-the-art. In particu-lar, Stochastic Dual Coordinate Ascent (SDCA) has emerged as a highly competitive algorithm due to its combination of excellent performance on benchmarks, lack of learning rate to tune, and strong convergence guarantees [19, 9, 22].
SDCA is a primal-dual optimization algorithm that re-quires sequential random-order access to training examples. The per-example iterative nature of the coordinate updates effectively results in SDCA being a single-threaded in-memory algorithm, which limits its scalability to very large train-ing sets, and underutilizes modern hardware with commonly available multiple CPU cores.

This paper addresses the problem of scaling SDCA to modern multi-core hardware and large out-of-memory datasets. First, we introduce a basic asynchronous parallelization scheme for SDCA, A-SDCA, and prove that it retains the fast lin-ear convergence guarantees of single-threaded SDCA to a certain suboptimality level. We observe that a naive imple-mentation of the algorithm routinely fails to achieve asymp-totic convergence to optimal values due to lack of synchro-nization between primal and dual updates, and propose a modified version of the algorithm, Semi-asynchronous SDCA (SA-SDCA), which periodically enforces primal-dual syn-chronization in a separate thread, which empirically results in convergence.

Our second contribution is a method for scaling SDCA to out-of-memory datasets. Our approach departs from pre-vious algorithms for out-of-memory learning that rely ei-ther on repeated same-order streaming through examples, or repeated iterations through individual blocks, neither of which is suitable for SDCA. Instead, we propose a block-compressed binary deserialization scheme that includes in-dexing for random block access, while supporting random-order within-block iteration. By offloading decompression and disk I/O to separate threads, the proposed method pro-vides efficient access to data in pseudo-random order, which empirically is shown to provide similar convergence behavior as truly random-order access.

We empirically demonstrate that proposed techniques re-sult in significant speedups and full hardware utilization, as well as the ability to train on out-of-memory datasets ef-fectively. Extensive comparisons on multi-gigabyte datasets demonstrate strong gains over existing state-of-the-art im-plementations, setting a new high bar for large-scale super-vised learning.
We consider regularized empirical loss minimization of lin-ear predictors. Let x 1 ,...,x n  X  R d be the feature vectors of n training examples, and w  X  R d be a weight vector which generates linear predictions w T x i for i = 1 ,...,n . In addi-tion, let  X  i : R  X  R be a convex loss function associated with linear prediction w T x i . Our goal is to minimize the following regularized empirical loss where  X  &gt; 0 is a regularization parameter. This formula-tion is used in many well-known classification and regres-sion problems. For binary classification, each feature vec-tor x i is associated with a label y i  X  { X  1 } . We obtain linear SVM (without the bias term) by using the hinge loss  X  ( a ) = max { 0 , 1  X  y i a } , and regularized logistic regression is obtained by setting  X  i ( a ) = log(1 + exp(  X  y i a )). For linear regression problems, each x i is associated with a response y  X  R , and we use  X  i ( a ) = ( a  X  y i ) 2 .

Our methods described in this paper can be readily ex-tended to work with a more general formulation, that is, we can replace (  X / 2) k w k 2 2 with a general convex regularizer g ( w ). For example, g ( w ) =  X  k w k 1 or g ( w ) =  X  (  X  2 / 2) k w k 2 2 . Details for handling such more general regu-larizations can be found in [17]. Here we focus on the ` regularization for clarity and simplicity.

The Stochastic Dual Coordinate Ascent (SDCA) solves a dual problem of (1). More specifically, let  X   X  i : R  X  the convex conjugate of  X  i , i.e.,  X   X  i ( u ) = max a ( u  X  a  X   X  The dual problem is to maximize the dual objective Notice that  X   X  R n and each dual variable  X  i is associated with a different example in the training set. At each iteration of SDCA, a dual coordinate is chosen uniformly at random and D (  X  ) is maximized with respect to that coordinate while the rest of the dual variables are not modified.

Let w ? = arg min P ( w ) and  X  ? = arg min D (  X  ) be the primal and dual optimal solutions respectively. If we define then w ? = w (  X  ? ) and P ( w ? ) = D (  X  ? ). We say the solu-tion w  X  R d is  X  P -sub-optimal if the primal sub-optimality P ( w )  X  P ( w  X  ) is less than  X  P .
In this section, we introduce some typical smoothness as-sumptions on the convex losses  X  i , and explain its implica-tions for the dual function defined in (2). Throughout this paper, we use k X k to denote the Euclidean norm k X k 2 . Definition 1. A function  X  i is called L -Lipschitz continuous if there exists a positive constant L such that for all a,b  X  Definition 2. A function  X  i is (1 / X  )-smooth if it is differ-entiable and its derivative is (1 / X  )-Lipschitz continuous, i.e., for all a,b  X  R , we have For convex functions, this is equivalent to
If  X  i is (1 / X  )-smooth, then its convex conjugate  X   X  i strongly convex (see, e.g., [8]). That is, for all u,v  X  s  X  [0 , 1], we have
If  X  i is (1 / X  )-smooth, then we define the condition number which is a key quantity in characterizing the complexity of optimization algorithms. For example, it is shown in [19] that the number of iterations of the SDCA algorithm to find a w  X  R d such that P ( w )  X  P ( w ? )  X  (with high probability) is O (( n +  X  ) log(1 / )).
Many popular machine learning problems are represented by very sparse datasets. Specifically, in datasets where the number of examples n and dimensionality d in problem (1) can be very large, the number of non-zero elements in the feature vectors x i  X  R d can be relatively small. Here we give a formal characterization of sparse datasets, following the model used in Hogwild! [13].

We can construct a hypergraph G = ( V,E ) representing the sparsity patterns in the dataset. The hypergraph X  X  ver-tex set is V = { 1 ,...,d } , with each vertex v  X  V denot-ing an individual coordinate in the space of feature vectors (
R d ). Each hyper edge e  X  E represents a training example, which covers a subset of vertices indicating its nonzero co-ordinates.Since | E | = n , we can also label the hyper edges by i = 1 ,...,n .
 Several statistics can be defined for the hyper graphs G . The first one characterizes the maximum size of the hyper edges, or number of non-zero elements in an example: The next one bounds the maximum fraction of edges that covers any given vertex: which translate into the maximum frequency of appearance of any feature in the training examples. We also define which is the maximum fraction of edges that intersect any given edge, and can serve as a measure of sparsity of the hypergraph.
Algorithm 1: A-SDCA (on each processor) repeat 2 sample i  X  X  1 ,...,n } uniformly at random and let e 3 read current state w and  X  i 6 for v  X  e do 8 end until stop
In this section, we first describe an asynchronous paral-lel SDCA algorithm (A-SDCA) based on a shared mem-ory model with multiple processors (threads). Convergence analysis of A-SDCA shows that it exhibits fast linear con-vergence before reaching a dataset-dependent suboptimality level, beyond which it may not converge asymptotically to the optimal parameter values. A study of the algorithm X  X  empirical performance revealed that asynchronous updates of primal and dual are problematic, leading us to derive a semi-asynchronous SDCA technique (SA-SDCA), in which periodic synchronization of the primal and dual variables allow satisfying equation (3), and empirically result in con-vergence to optimal values demonstrated in Section 5.
Suppose we have a shared-memory computer with m pro-cessors (threads). Each processor has read and write access to a state vector w  X  R d stored in shared memory. In the A-SDCA algorithm, each processor follows the procedure shown in Algorithm 1. In line 7, w v and x i,v denote the components of weight vector w and training example x i , re-spectively, indexed by v  X  X  1 ,...,d } .

When there is only one processor (thread), Algorithm 1 is equivalent to the sequential SDCA algorithm in [19]. For multiple processors, each operation in Algorithm 1 can be considered a local event that occurs asynchronously across processors. In particular, the dual update computation in line 4 of Algorithm 1 takes the bulk of computation time during each loop execution, and may take each processor a different amount of time to complete. As a result, when a particular processor updates components of w in the shared memory (lines 6-8), the component w v on the right-hand side of line 7 may be different from the one read in line 3 (which was used to compute the update  X   X  i ). Despite this asynchronicity, we assume the component-wise addition in line 7 is atomic .

In order to analyze the performance of Algorithm 1, we define a global iteration counter t = 0 , 1 , 2 ,... . We increase t by 1 whenever some component of w in the shared memory is updated by a processor. Thus, line 7 of Algorithm 1 can be written as: where k ( t ) denotes the time at which line 3 of Algorithm 1 was executed (with k ( t )  X  t ). The formula (12) assumes the operations in lines 6-8 of Algorithm 1 are indivisible (or si-multaneous), when the global event counter t is incremented. If this cannot be guaranteed in the implementation, we can still analyze a modified version where the for loop in lines 6-8 is replaced by updating a single v  X  e , picked randomly from the set e (of nonzero feature coordinates).

In terms of the global counter t , computation of dual up-date  X   X  k ( t ) i in line 4 can be written as: We assume that dual variables remain fixed: allowing the dual update in line 5 to be consistent: This requires that no more than one processor can work on the same example i . It can be easily guaranteed by parti-tioning the datasets into m subsets S 1 ,...,S m  X  X  1 ,...,n } , and each processor p only works on random samples from the local subset S p , for p = 1 ,...,m .

We assume that the lag between the read and write oper-ations at each processor is bounded, i.e., there is a constant  X  such that Another assumption we make is that the updates  X   X  i are always bounded, i.e., there is a constant M &gt; 0 such that |  X   X  ( t ) i | X  M, for all i  X  X  1 ,...,n } and all t  X  0 . (14) Based on the above assumptions, the following theorem de-scribes the behavior of the A-SDCA algorithm (see proof in Appendix A): Theorem 3. Suppose each loss function  X  i is convex and (1 / X  ) -smooth, and we initialize the shared state by  X  0 and w (0) = w (  X  (0) ) . Let the sequence of w ( t ) be generated by Algorithm 1, indexed by the global iteration counter t . If where  X  = 1 / (  X  X  ) and then we have E [ P ( w ( T ) )  X  D (  X  ( T ) )]  X   X  P whenever Here  X  and M are the constants in equations (13) and (14) respectively, and  X  and  X  are the statistics of hypergraph as defined in (9) and (11) respectively.

The theorem proves that the A-SDCA algorithm enjoys a fast linear convergence up to suboptimality level K ( n +  X  )(2 + n +  X  ). This suboptimality level depends on the spar-sity parameters  X  and  X  of the dataset, as well as the lag  X  , which usually grows with the number of processors m .
With a single processor, when Algorithm 1 reduces to the sequential SDCA method, there is no the lag between the iteration counter:  X  = 0. Consequently, K = 0 and theorem yields the rate previously proven for sequential SDCA in [19].
In the multiple processor case, consider the typical setting with  X   X  1 / of sequential SDCA. regarded as a constant, we have  X  = 1 / (  X  X  )  X  case, the suboptimality level scales as To make the result in Theorem 3 meaningful, we need the suboptimality level be a small constant, which requires This condition can be satisfied by many sparse datasets. When this condition is not satisfied, A-SDCA algorithm may fail to converge to desired suboptimality gap, degrading gen-eralization accuracy.

This is illustrated in Figure 1 that shows the performance of A-SDCA on two large datasets. For the KDD 2010 dataset, convergence suboptimality gap degrades gracefully when the number of threads (hence the lag  X  ) increases. For the KDD 2012 dataset, increasing the number of threads beyond one leads to convergence at a high suboptimal gap, failing to improve after the first epoch.

Our analysis of empirical results revealed that the primary reason that A-SDCA does not converge asymptotically to the optimal solution is that, due to the asynchronous up-dates, the following primal-dual relation does not hold in general: which, by contrast, always holds in the sequential (1 thread) case. As a result, we observe the update w ( t )  X  1  X n P not converging in the asynchronous case.

The above observation motivates us to propose a semi-asynchronous SDCA method, SA-SDCA, described in Algo-rithm 2, which is the primary contribution of this paper. We solve the above problem by periodically forcing the synchro-nization of the primal and dual variables to enforce their correspondence in Eq. (15).

Note that in Algorithm 2, the synchronization thread that computes w sync does not block the SDCA threads. Instead, it consumes a dynamically-updated most-recent version of
Algorithm 2: Semi-asynchronous SDCA (SA-SDCA)  X  during the computation of w sync , allowing full utilization of CPU at all times. In experiments described in Section 5, we observe nearly linear speed-ups and convergence in both suboptimality gap and holdout-set error accuracy, empiri-cally demonstrating the effectiveness of SA-SDCA.
While the previous section has proposed an attractive asynchronous parallelization of SDCA with strong theoret-ical guarantees, the assumption of random access to exam-ples implies that the dataset is sufficiently small to reside in memory. The growth of modern industrial datasets to tens of gigabytes and higher, however, invites a technique for ef-ficiently providing high-speed random-order access to disk-based datasets. This section introduces such a technique based on decoupling the data input interfaces, and imple-menting them for disk-based data with block-wise compres-sion and indexing on top of multi-threaded, buffered I/O.
The basis for the proposed method is a block-compressed binary format with indexing that provides random-order ac-cess to blocks. Examples in the dataset are partitioned into equal-sized blocks. Random-order block access is provided by an offset table, with within-block random access provided by upfront decompression of the block upon access.

The algorithm consumes data via an abstraction of an it-erator over shuffled examples. This shuffling is not truly uniform, as it involves two dependent levels on randomness: blocks are read from disk in uniformly random order, fol-lowed by random-order iteration over examples in the block. Threading is orchestrated to coordinate reading compressed blocks from disk with simultaneous decompression and with consumption of examples by the learning algorithm.

Without compression this process is heavily I/O bound, hence compression provides better balancing of available CPU cores and disk throughput. Because I/O and decom-pression threads do not perform floating-point computations, on modern hyper-threaded hardware these threads do not interfere with the training threads described in the previous section.

Zlib compression [6] works well as it minimizes CPU costs while achieving high compression rates for typical datasets. Furthermore, we note that reduction in data size with com-pression may result in file size that effectively leads to in-memory reading due to disk caching.

The user chooses the count of examples per block when writing the file. For performant shuffling, this choice should ideally balance some practical considerations: blocks should be large enough that seeks do not dominate I/O and com-pute time, but small enough that decompressed blocks fit within L3 cache, so that each access of an example in a block is a cache hit.

We note that this approach of a block-partitioned dataset shares motivation with earlier work on out-of-memory SVM training [24]. Despite some similarities, there are two key distinctions between approaches. First, the approach above performs complete streaming pass over the data, whereas [24] loads makes multiple passes over each block loaded into memory. The second key difference that we are plugging our shuffling example iterator into an existing SDCA learner with a general data access interface, not proposing a new learner coupled to a particular storage format. In contrast, earlier work was centered around devising a novel block min-imization framework that could perform SVM training when only a subset of the dataset was available in memory at any given time.
We present an experimental study of the proposed meth-ods covering three areas:
All experimental results were obtained by optimizing the logistic loss. For convergence analysis experiments, we used L 2 parameters (  X  ) that give best generalization accuracy as measured on held-out test sets. Optimum loss values were obtained by running single-threaded SDCA sufficiently long for between-iterations improvement to be within single floating point precision. For each setting, experiments were repeated 5 times using different random seeds.
 Datasets used in this section are summarized in Table 1. We note that all datasets are multi-gigabyte in size and very sparse. For KDD 2010 [21], we used the featurized version on LibSVM website. For KDD 2012 [14] and Criteo [5], we performed a random 90/10 train-test split on the pub-licly available train sets (hosted by Kaggle), preprocessing categorical features by hashing using 25 hash bits. Table 1: Datasets summary. The #Features column denotes the total number of non-zero features.
In this section, we analyze empirical convergence and scal-ing properties, as well as accuracy, of SA-SDCA. Experi-ments were performed on a hyper-threaded machine with 8 physical cores, hence we investigated parallelization up to 7 threads, reserving one thread for loss computation and periodic primal-dual syncing.

Figure 2 demonstrates that on sparse datasets, SA-SDCA algorithm converges as quickly as the baseline sequential SDCA for a given effective number of passes over data. AUC curves on bottom-most sub-figures show that results with re-spect to hold-out error mirror those for suboptimality, with near-linear scaling for both with respect to the number of threads.
In next set of experiments, we investigate the effectiveness of the technique proposed in Section 4 for out-of-memory training. Figure 3 contains results for no shuffling (other than once before training), uniform-random, and pseudo-random shuffling, yielding several interesting observations. First, we note that while pseudo-random shuffling X  X  con-vergence rate lags that of true random shuffling, it signif-icantly outperforms not shuffling while still allowing disk-based training.

More importantly, per iteration, out-of-memory training is actually faster computationally than standard in-memory training. This is due to two reasons: first, for some datasets, the block-compressed data reduced physical dataset foot-print enough to be at least partially cached in memory by the operating system, which reduces disk-access penalties after the initial iteration. Second, block-based shuffling strategy has better higher-level cache efficiency than the uniformly random shuffling scheme, resulting in faster wall-clock per-formance.
In this subsection, we compare SA-SDCA with leading lin-ear learning implementations detailed below. It is important to emphasize that comparing different software implemen-tations of learning algorithms is inherently difficult, and we tried our best to ensure fairness. To this end, we ran a ran-dom hyper-parameter search [3] for all competing algorithm over 50 trials on a homogeneous cluster of nodes with 6-core 2.5GHz CPUs and 48GB of RAM (except for LibLinear as noted below). The following learners were compared: super-polynomial convergence rate of the SA-SDCA
For all methods, the same loss function (log-loss) and L 2 regularization parameters were used. LBFGS and LIBLIN required loading datasets into memory, while the rest were streaming, with SGD1, SGD5, SDCA1, and SDCA5 using the out-of-memory pseudo-shuffling described in section 4. For each learner, we select top 20 AUC results, shown in Fig-ure 4, with the right panel zooming into the top-performance quadrant indicated by dotted lines in the left panel. These results demonstrate that our baseline sequential SDCA implementation is competitive with LibLinear, while both outperform VW and L-BFGS. The difference confirms that SDCA demonstrates faster convergence than primal methods, and that dataset reshuffling between iterations is essential for learning optimal parameters. The results also show that SA-SDCA effectively speeds up sequential SDCA by fully utilizing computing resources of modern multi-core processors.
Recent attention to dual coordinate descent methods was brought by [9], who have shown that they allow achieving linear convergence rates for large-scale linear SVM prob-lems. More generally, [19] proposed and analyzed SDCA method for regularized risk minimization in which a sig-nificantly better convergence rate than the commonly used Stochastic Gradient Descent (SGD) methods was proven. In related work, [16] proposed Stochastic Average Gradient (SAG) method for smoothed convex loss functions, which also achieves linear convergence while preserving the itera-tion complexity of SGD methods, but also requires careful selection of learning step size. In a more general setting, an accelerated variant of SDCA was proposed in [20],with supe-rior performance achieved for a sufficient condition number.
In order to addresses the problem of scaling these meth-ods to modern multi-core hardware systems, a number of synchronous parallel algorithms were introduced in recent years, which assume distributed computation across multi-ple nodes [15, 26, 26, 23]. In [7, 13, 12], the sparsity has been utilized to develop asynchronous parallel coordinate descent and stochastic gradient type algorithms. In particular, the Hogwild! framework of [13] provided inspiration for the A-SDCA algorithm in Section 3, from which our SA-SDCA method is derived.

In both [13] and [12], it is assumed that there is a bound on the lag between when a processor reads w and the time when this processor makes its update to a single element of w . Moreover, it was shown that a near-linear speedup on a multi-core system can be achieved if the number of proces-sors is O ( n 1 / 4 ). Despite this attention, very little work exists on scaling up dual coordinate ascent. [18] have considered the mini-batch approach, where updates are computed on example subsets and aggregated collectively. Experimental evaluation has shown that mini-batches slow down conver-gence, inviting the use of either Nesterov acceleration or approximate Newton step.

In more recent work, [10] have considered a data-distributed variant of SDCA, named CoCoA, where a master node ag-gregates updates computed by multiple worker machines on local examples. Results reported in [10] on relatively small datasets do not appear competitive, however. For RCV1, a common text classification benchmark, CoCoA is reported to take 300 seconds on an 8-node cluster to reach the pri-results on KDD 2012 data. Bottom: results on Criteo data. mal sub-optimality of 10  X  4 . In contrast, it takes the single-threaded SDCA implementation that is our baseline approx-imately 5 seconds to reach the same suboptimality level.
We have described, analyzed and evaluated two techniques for scaling up Stochastic Dual Coordinate Ascent (SDCA) to large datasets: asynchronous updates with primal-dual synchronization, and pseudo-random iteration via indexed, block-compressed serialization. Empirical results demon-strate strong performance in comparison to existing state-of-the-art software for linear learning. This work yields a new state-of-the-art baseline for single-node linear learning, and invites an investigation of combining the method with distributed learning approaches. Further investigation into pseudo-random access is another interesting direction for further research, calling for theoretical analysis of conver-gence implications of imperfect randomness, and investigat-ing alternative designs, such as the use of quasi-random (low-discrepancy) sequences, that could yield random-quality con-vergence with even higher throughput.
Authors wish to thank Wei-Sheng Chin for assistance with computing LibLinear baseline results, and John Langford and Paul Mineiro for Vowpal Wabbit hyper-parameter sug-gestions. [1] A. Agarwal, A. Beygelzimer, D. J. Hsu, J. Langford, [2] A. Agarwal, O. Chapelle, M. Dud  X  X k, and J. Langford. [3] J. Bergstra and Y. Bengio. Random search for [4] L. Bottou. Stochastic gradient descent tricks. In [5] O. Chapelle et al. [6] P. Deutsch and J.-L. Gailly. Zlib compressed data [7] J. Duchi, M. Jordan, and B. McMahan. Estimation, [8] J.-B. Hiriart-Urruty and C. Lemar  X echal. Fundamentals [9] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, [10] M. Jaggi, V. Smith, M. Tak  X ac, J. Terhorst, [11] D. C. Liu and J. Nocedal. On the limited memory bfgs [12] J. Liu, S. J. Wright, C. R  X e, V. Bittorf, and S. Sridhar. [13] F. Niu, B. Recht, C. R  X e, and S. J. Wright. Hogwild: A [14] Y. Niu, Y. Wang, G. Sun, A. Yue, B. Dalessandro, [15] P. Richt  X arik and M. Tak  X a X c. Distributed coordinate [16] N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic [17] S. Shalev-Shwartz and T. Zhang. Proximal stochatic [18] S. Shalev-Shwartz and T. Zhang. Accelerated [19] S. Shalev-Shwartz and T. Zhang. Stochastic dual [20] S. Shalev-Shwartz and T. Zhang. Accelerated [21] Stamper, Niculescu-Mizil, Ritter, Gordon, and [22] T. Suzuki. Stochastic dual coordinate ascent with [23] T. Yang. Trading computation for communication: [24] H.-F. Yu, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. [25] H.-F. Yu, F.-L. Huang, and C.-J. Lin. Dual coordinate [26] Y. Zhang, M. J. Wainwright, and J. C. Duchi.

In this appendix, we sketch the proof of Theorem 3. The proof mainly follows the framework in [19, 17], combined with additional techniques for handling asynchronicity in [13]. First, we need the following key lemmas, which we prove in a longer report.
 Lemma 4. Assume that each  X   X  i is  X  -strongly convex and s  X  [0 , 1] . Then the sequence of w ( t ) and  X  ( t ) Algorithm 1 with  X  (0) = 0 satisfy where
H
Moreover, the following observation is presented in [19] and thus, it is presented here without proof.
 Lemma 5. For all  X  , D (  X  )  X  P ( w  X  )  X  P (0)  X  1 and D (0)  X  0 .

The proof of our basic results stated in Theorem 3 relies on the boundedness of the expected increase in dual objec-tive from below by the duality gap. Lemma 4 implies that the duality gap can be further lower bounded using dual suboptimality and solved to obtain the convergence of dual objective based on recursion. Note that since,  X  i is (1 / X  ) -smooth, we can assume that there exist M  X  R such that  X  i is locally M 2 -Lipshitz continuous and subsequently we have ||  X   X  || 2  X  M . Moreover,  X   X  i is  X  -strongly convex and from Lemma 4 we have By choosing s =  X n X  1+  X n X   X  [0 , 1], we have H t +1  X  0 and subsequently
Let  X  t D : = D (  X   X  )  X  D (  X  t )  X  P ( w t )  X  D (  X  t recursion on (16), we obtain and (1  X  s n ) t  X  e  X  s n t implies In addition, the last term o the right hand side of (17) can be bounded using integral test as which implies
E [  X  t D ]  X   X  M If it is desired to have E [  X  t D ]  X  D then we need e or equivalently
K 1 + 1 +  X  X n Therefore, the dual problem sub-optimality is bounded by  X  Moreover, the duality gap can be presented as
E P ( w t )  X  D (  X  t )  X  n Based on (18) and (19) we have E P ( w t )  X  D (  X  t )  X  ( n +  X  )( e  X  t  X  + n (1  X  K ( n +  X  )) + where  X  = 1 / (  X  X  ). Moreover, based on 1  X  K ( n +  X  ) &lt; 0, if then we have which implies we obtain a duality gap of at most  X  P when-ever
