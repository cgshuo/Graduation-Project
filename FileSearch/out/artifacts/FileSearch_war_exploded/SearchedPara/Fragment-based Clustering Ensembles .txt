 Clustering ensembles combine differe nt clustering solutions into a single robust and stable one. Most of existing methods become highly time-consuming when the data size turns to large. In this paper, we study the properties of the defined  X  X lustering fragment X  and put forward a useful proposition. Solid proofs are presented with two widely used goodness meas ures for clustering ensembles. Finally, a new ensemble framewo rk termed as fragment-based clustering ensembles is proposed. Theoretically, most of existing methods can be improved by adopting this framework. To evaluate the proposed framework, three new methods are introduced by bring three popular clustering ensemble methods into our framework. The experime ntal results on several public data sets show that the thr ee introduced methods are greatly improved in computational complexity and also achieved better or similar accurate results than the original methods. H.2.8 [ Database Management ]: Database Applications  X  Data mining ; 1.5.3 [ Pattern Recognition ]: Clustering  X  Algorithms Algorithms, Experimentation, Theory Clustering Ensembles, Frag ment, Mutual Information Clustering ensembles, also know n as consensus clustering or clustering aggregation, have em erged as a powerful method to combine multiple inconsistent clustering solutions. Many applications arisen in vari ous settings and from different disciplines can be transferred into the problem of clustering ensembles. Some typical applications are: categorical data clustering, heterogeneous data clustering, outlier detection, distributed clustering, knowle dge reuse and aggregating clusterings of different methods [1]. There have been a great number of clustering aggregation methods in the literature, which can be roughly classified into: voting based methods [2], graph-based methods [3], mixture model based methods [4] and searching based methods [1]. We note that the computational complexity of clustering ensembles usually depends on th e data size. Many existing clustering ensemble methods are qua dratic of the data size. With the data size increasing, thes e methods become highly time consuming and are unable to be app lied in real applications. This reminds us that if the data size is decreased before employing the ensemble algorithm, the time c onsumption can be reduced. Each clustering can be viewed as a partition of the original data. We find that the whole data set cons ists of data subsets (named as  X  X lustering fragment X ) in which data points keep together in all of the input clustering. We call such subsets as  X  X lustering fragments (or fragments for simplification) X . In many clustering ensemble problems, the number of produced fragments is far smaller that the original data size. Naturally, an interesting question is arising: can we solve the clustering ense mble problem merely based on the fragments? If the answer is  X  X ES X , the computational complexity of aggregation on fragments should be much lower than that of existing methods. We propose a useful proposition to answer the question. The propositi on is proved within two widely used goodness measures. Then a fragment-based ensemble framework is introduced. Experime ntal results demonstrate that the new methods which are based on the proposed framework outperform the original methods significantly in time complexity and achieve better (or similar) results than the original ones. The remainder of this paper is organized as follows. Section 2 states the clustering fragment extraction algorithm. Section 3 provides our main theory and corresponding proof. Section 4 introduces the fragment based clus tering ensemble framework and proposes three new methods by m odifying existing ones. Section 5 gives our experimental evaluati ons on several public data sets and some discussions. Conclusi ons are given in Section 6. This section introduces the  X  X  lustering fragments X  extraction algorithm. Some symbols used in the paper are defined as follows. Let X = { x 1 , x 2 , ... , x n } denote a set of data points.  X  = {  X   X  } be a set of H input partitions of X . Each partition indicates a clustering and  X  i ( x j ) denotes the label assigned to x partition. Let F represent the set of fragments. For each data point, its labels given by the partitions  X  form a label sequence, for example,  X   X  1 ( x j )  X  2 ( x j to get each point X  X  label sequence by accessing each partition once. If suitable data structure is used, the time complexity is O ( n * H ), where N represents the number of produced fragments. Table 1 shows the pseudo code of the extraction algorithm. In Fig. 1, there are approximately 1000 data points. Three different partitions achieved by di fferent clustering algorithms are given in Fig.1 (a), (b) and (c ). Fig.1 (d) shows the produced fragments which are enclosed by blue lines and borders. The number of fragments is 11 which are far less than the data size 1000. It can be easily observed that the points in the same fragment keep together in each partition in Fig.1 (a), (b) and (c). The previous section has intr oduced the clustering fragments X  extraction algorithm. This secti on presents our theory about whether the clustering aggregation can be achieved directly on fragments. We give proof for our theory under two widely used goodness measures in clusteri ng ensembles. The first is the distance criterion proposed by Ginois [1]. The distance is used to measure the disagreement between two clustering. With this criterion, clustering ensemble b ecomes an optimization problem to find a new partition with the minimized total distance. Studies in [5, 7] are based on the distan ce criterion. The second measure Input: X = { x 1 , x 2 , ... , x n },  X  = {  X  1 ,  X  Output: Fragments ( F ) Steps: 1. Initialization: string Ls ( j ) =  X  , i = 1 : n ; 2. map&lt;string, list&lt;integer&gt;&gt; F ; 3. for each partition  X  i do 4. for each data point x j do 5. Ls ( j ) = Ls ( j )  X   X  i ( x j ) 6. end for 7. end for 8. for j =1 : n 9. if ( l = F .find ( Ls ( j ))) == null 10 generate a new integral list and insert j into the new list; 11. else 12. insert j into list l . 13. end for 14. return F . is the mutual information criterion. This criterion applies mutual information to measure the agreement between two clustering. Studies in [2, 6] apply this criterion. Our theory can be summarized as the following proposition. aggregation. All the data points lo cated in the same clustering fragment definitely share the same label in  X  * , or saying they are definitely in the same cluster of  X  * .
 Intuitively, the above proposition is reasonably true. However, without a universal goodness measures for any candidate partition, it is difficult to give a direct proof. We note that most existing methods aim to optimize the distance criterion or mutual information criterion. If the proposition is proved to be true under these two measures, our new framework is adaptable to most of existing methods. We then introduce two theorems. Theorem 1 . Let  X  * be the optimal partition under the distance criterion, the proposition is true. Theorem 2 . Let  X  * be the optimal partition under the mutual information criterion, the proposition is true. The following subsection will give the proof. Due to lack of space, we only give the proof sketch (Detailed steps are given in the full version of this paper). Assuming that, on the contrary, we get a candidate partition  X  and that there exists at least one fragment whose points scatter in different clusters of  X  a . We denote one of the scattered fragments as F s and the point subset of F s in the m -th cluster of  X  Now we focus on two arbitrary clusters m 1 and m 2 given by  X  We introduce the following Lemma using the above assumptions. Lemma 3. The goodness of  X  a can be increased through one of Proof . Please refer to the full version of this paper. Lemma 4 . When using distance criterion to measure the goodness of a candidate partition, there never exists an optimal partition in which points from the same fragment are scattered in different clusters .
 Proof . Assuming there is an optimal partition  X  * points of a fragment into different clusters. According to Lemma 3, the goodness of  X  * can be increased, which indicates that  X  not an optimal partition.  X  As a result, Theorem 1 is true according to Lemma 4. We still assume that there is a candidate partition (  X  a as the one in previous subsec tion. It has two variations:  X  X  and  X  X  X  . One (  X  X  ) is produced by transferring data in (1) m other (  X  X  X  ) is produced by transferring data in (2) m Lemma 5 . Without loss of generality, we assume  X  (  X  X  X  )  X   X  (  X  X  ). In this case,  X  (  X  X  X  ) &gt;  X  (  X  a ).
 Proof . Please refer to the full version of this paper. Lemma 6. When using mutual information criterion, there never exists an optimal partition in which points from the same fragment are scattered in different clusters . Proof . Assuming there is an optimal partition  X  * points of a fragment into different clusters. According to Lemma 5, the goodness of  X  * can be increased, which indicates that  X  not an optimal partition.  X  As a result, Theorem 2 is true according to Lemma 6. Intuitively, Proposition 1 will hold under any goodness measures. However, Theorem 1 and Theorem 2 are still very useful due to that most current methods are ba sed on these two measures. Since the optimal partition is a combination of the fragments, the optimal partition can be deduced directly by managing points of a fragment as a whole. This approach can be summarized as the fragment-based clustering ensemble framework shown in Figure 2. We present three new methods by modifying three existing typical ones to evaluate the proposed framework in the paper: Agglomerative, Furthest and Local Search (Their details can refer to [1]). To differ from the original methods, the terms of the new ones are added by the prefix  X  X - X . We only take F-Agglomerative as an example to illustrate the implementation details shown in Table 2 due to limited space. The other two methods (F-Furthest and F-Local Search) have the similar procedures and can be found in the full version of the paper. The complexity of Agglomerative is O ( nH ) for fragment extraction and O ( N 2 H ) + O ( N running the aggregation method. It is obvious that the complexity can be greatly reduced if N &lt;&lt; n . This conclusion is still available for F-Furthest and F-Local Search. This section reports the results of the proposed fragment-based ensemble methods as well as the original ones on six public data sets from UCI Repository [8]: Hayes-Roth (#1), Glass (#2), Breast (#3), Yeast (#4), Wave (#5) and Magic (#6). We employ running time as well as both the classification error E and disagreement error E d defined in [1] to evaluate results of each method. E c is defined as: where m i denotes the size of the majority class in cluster C Because Hayes-Roth is a category data set, the input partitions are generated according to each of its attribute. For other sets, the input partitions are obtained by using K-means algorithm with different initialized number of centers. Figure 3-5 show the running time of the three pairs of algorithms over the six data sets. It can be observed when the size is 160, the running time of original methods nearly equals to that of fragment-based ones. The main reason is that fragment-based methods need to extract fragmen ts and the time-consumption of fragment extraction can not be ignored compared with the following fragment ensemble when the size is 160. However, with the data size increasing; the time-consumption of fragment extraction occupies little proportion. As a consequence, the running time of original methods increase sharply while that of the fragment-based methods maintains very small. Figure 6-8 show the performance comparison between the three pairs of algorithms respectively in terms of E c . It can be observed that the fragment-based methods achieve comparable or better results than the original ones through the six data sets except the Wave set (#5). For the Wave se t, both F-Agglomerative and F-Furthest are inferior to their or iginal methods. The discussion part will give an analysis. Table 3 shows the performance comparing between the three pairs of algorithms respectively in terms of E d . We can observe that in most cases, the fragment-based met hods yields lower error. Three exceptional cases are bold and italic-represented. The experimental results over the six data sets show the satisfactory performance of the fra gment-based ensemble methods, which is consist with our theoreti cal analysis in time complexity of fragment-based framework. We observe that the running time of fragment based methods main ly depends on the number of fragments and appear to be insensitive to the size of point set. Input : X = { x 1 , x 2 , ... , x n },  X  = {  X  1 ,  X  Output: clusters Steps: (1) Extract fragments using the algorithm in Table 1; (2) Place each fragment in a single cluster; (3) Calculate the average distance between each pair of clusters (4) Merge the corresponding clusters a nd go to (3) if the smallest (5) Output the current obtained clusters. ) #4 ( X 10 5 ) #5 ( X 10 6 ) #6 ( X 10 This paper defines the  X  X lustering fragment X  and studies its useful properties. Based on the cluste ring fragments, we propose a new clustering ensembles framework : fragment-based clustering ensembles. This framework is based on the proposition that an optimal partition should ensure the data points of a fragment locate in the same cluster. We have proved this proposition under two widely used goodness measures : distance measure and mutual information measure. Because the size of fragment set is usually far smaller than the data size, existing methods can be improved with respect to the time complexity. Theoretically, most of existing methods can be improved by bring into this framework. To utilize the efficiency of the proposed framework, three new ensemble methods are presented, i. e. F-Agglomerative, F-Furthest and F-LocalSearch. We conducted experiments on six public data sets. The results show that th e three new methods significantly outperform their original methods in terms of running time, which demonstrates the efficiency of our framework. We are very appreciative to Dr. Hanzi Wang for his revision in writing. We would like to thank the anonymous reviewers for their very useful comments and s uggestions. This work is partly supported y NSFC (Grant No.60672040 and 60672040). [1]Gionis, A., Mannila, H. and Tsaparas, P. 2007. Clustering [2]Ayad, H.G. and Kamel, M.S. 2008. Cumulative Voting [3]Fern, X.Z. and Brodley, C.E ., 2004. Solving cluster ensemble [4]Lange, T. and Buhmann, J., 2005. Combining partitions by [5]Li, T. and Ding, C., 2008. Wei ghted consensus clustering. In [6] Zhou, Z.H. and W. Tang ., 2006. Clusterer ensemble.
 [7]Bahloul, S.N., Rouba, B. and Amghar, Y. 2008. Minimization [8] http://archive.ics.uci.edu. Ec(%) Time(s)
