 Chengqi Zhang 1,2 , Xiaofeng Zhu 3 , Jilian Zhang 3 , Yongsong Qin 3 , and Shichao Zhang 3 where each instance consists of a feature vector (conditional attributes) and an output value (class label). In real world applications, however, the training set often contains missing values that can generate bias that affects the quality of the supervised learning process or the performance of classification. However, most learning (or mining) algorithms are based on the assumption without missing values. 
This paper proposes a new imputation algorithm to handle missing attributes based imputation method and be referred to a non-parametric method in statistics. It is also efficient for dealing with categorical a ttributes. Specifically, this approach uses a grey relational grade (denoted as GRG ) to substitute for Minkowski distance or other alternative similarity measures during the process of searching for the nearest neighbor under the assumption which requires the instance i to have a same class label as the instance j when calculating GRG ( i, j ). This can efficiently reduce the time complexity. In addition, this approach can get over the slow convergence rate of EM algorithm through an EM-like iteration imputation method and makes an optimal use of all observed values including those instances with missing values. We evaluate the performance of our method using several UCI datasets. The experimental results show that our approach is superior to k-NN and mean substitution methods. 
The rest of this paper is organized as follows. Section 2 briefly recalls related work. In Section 3, we design our GBKII algorithm. Section 4 describes our experiments on UCI dataset [1]. We conclude this paper in Section 5. Currently, there are two mainstream directions for dealing with the missing values. One of these is based on machine learning. However, the methods based on machine learning perhaps destroy the original distribution of dataset during the process of imputing. Moreover, some methods (such as C4.5) usually only handled the discrete missing values occurring in categorical attributes and continuous ones. Multiple Imputation fills in missing values with repeating independently M times [3]. EM-like iteration imputation method. However, GBKII is different from the MI and EM algorithms. In the first iteration imputation, we use the mean (or mode) values of all the observed attribute values to fill in the missing values in order to make the best use of the all information. From the second imputation process, iteration imputation is based on the unknown a priori. In fact, we have usually not any priori knowledge about the data. Our GBKII algorithm is a non-parametric method that is different from EM algorithm in which both the E and M steps depend on parametric models. 3.1 The Nearest Neighbor Imputation Method Usually, calculating the nearest neighbor instance is based on the Minkowski distance or devise a distance metric that combines distances measured between symbolic and for some application domains, such as domains with numeric attributes. Caruana demonstrated grey relational analysis, which is more appropriate to determine the  X  X earness X  (or relationship) between two instances than Minkowski distances or others do GRG instead of Minkowski distances (or other distance metrics) during the process of searching for the similarities between instances. 3.2 The Grey Relational Analysis Grey Relational Analysis ( GRA ), which is founded upon measuring the similarity of emerging trends among instances, is a method of GST . Consider a set of observations xxx x K , where = KK and a class label D i . The grey relational coefficient (GRC) is defined as: where [0,1]  X   X  (  X  is a distinguishing coefficient, normally, let  X  =0.5), as follow: smaller than that between x 0 and x 2 ; otherwise the former is larger than the latter. 3.3 GBKII Algorithm Previous work, such as kernel method, imputes the missing values utilizing the instances without missing value as the reference instances. It may possibly ignore two databases have a more serious problem about values missing especially in industrial database, such as in [7], of the 4383 records in this database, none of the records were The GBKII algorithm is presented as follows: 
The First Iteration 1.0 // t-th iteration 
Repeat 2.0.1 Compute GRG(i,j) base on Equation (2) 2.0.2 Get k Nearest Instances 2.0.3 Imputation Ming Values 2.0.4 t --; // t is the iteration time Until (convergence or t&lt;=0) complete and only 33 variables out of 82 have more than 50% of the records complete. (2) An incomplete instance ma y already contain enough information for model construction, even though it still contains missing values. In Table 1, the value compute GRG ( a, i ) ( i is the instance without missing values, such as instance d and e ) because the class label is different from the missing instance. So it is reasonable for us to impute missing values with all observed values including those instances that contain some missing values based on the above analysis. 
However, we cannot impute missing values with all the information of the observed values before the missing values have not yet been patched up. For example, we cannot compute the GRG ( a,b ) as the 2nd attribute in instance b(denoted as observed in Table 1. In GBKII , we apply the first imputation strategy to make the best imputation is based on the imputation results of the t-th imputation until convergence or satisfying the demand of users. We compute the mean (or the mode if the attribute is categorical) for each continuous-attributed observed values whose class labels are the same, i.e., we use the mean (or mode) as the initial imputed value of the missing value. Imputation with mean (or mode) is a popular and reasonable imputation method in machine learning and statistics. However, [8] thought to impute with the mean (or mode) is valid if and only if the dataset is chosen from a population with a normal distribution. However, in real world application, we cannot know really the is reasonable based on the first imputation for dealing with the missing values. 
In the second iteration imputation, for example in Table 1, we assume all the values are all observed (the missing values have got in the first iteration imputation by the mean or the mode) except the value of MV(a,1) if we want to impute the missing among these instances whose class label are  X  1  X  X s same as the missing instance a , then we impute MV(a,1) based on method in Figure 1 through the step 2.0. We regard the attribute C 3 in instance a as missing when we have imputed the value in attribute C 1 in instance a and want to impute MV(a,3) in second iteration, in this case we regard the in instance a by turn utilizing the same method after imputing C 3 in second iteration. And so forth, we can impute all missing values in the dataset. During the third until the imputation results satisfy the demand of users or the algorithm reaches convergence. 
At last, we present in detail how the proposed approach is extended to deal with x is minimal). Thus, the proposed approach can be applied to numeric and categorical attributes with missing values. 3.4 Convergence of the Imputed Values In our algorithm, the first iteration, which uses the mean (or mode) as the initial filled-in value of the missing values under the assumption of same class label, is obviously convergence in statistics, but in the process of the other iterations, we are not able to make similar proof for the non-parametric method. The reason is that there are few theoretical results regarding the validity of k-NN in the literature due to the difficulty convergence of the GBKII method on UCI datasets. 
Note that in this paper, the algorithm of iteration imputation with grey based k-NN method is denoted as  X  X oclassified X ; the algorithm of iteration imputation based Euclidean distance k-NN method under the same class label between the missing missing attribute values based mean or mode under the same class label is denoted as  X  X eanMode X . 
The imputation method converges when the  X  X ean change in filled-in values X  reaches to zero. The meaning of  X  X ean change in filled-in values X  is the distance between the mean of all imputed in last iteration imputation and the mean of all imputed in the current. Caruana [8] thinks that the  X  X ean change in filled-in values X  close as possible to zero in the non-parametric model (such as k-NN , kernel method) diagnosis data set and water-treatment domain data set respectively. The results show fastest with respect to convergence among these three algorithms for the hepatitis dataset and water-treatment dataset. The  X  X ean change in imputed values X  of our GBKII is the smallest among the three algorithms when running the two UCI datasets. 4.1 Experimental Evaluation on Prediction Accuracy First, the GBKII approach is evaluated on Iris dataset and the Pima dataset in order to demonstrate the approach X  X  effectiveness. There are no missing values in the datasets and the attribute data are missing at random and the missing rate are fixed to 5%. As we had no prior information about the optimal k for a specified application, the varied from 1 to 30. We iterate imputation 20 times based on the analysis of Figure 2. The accuracy of prediction was measured using the Root Mean Square Error (RMSE) as follows: total number of missing values. The larger the RMSE is, the worse the prediction accuracy is. 
From Figure 3, we can see that the RMSE for grey base k-NN iteration imputation k-NN iteration imputation algorithm and mean algorithm regardless of the varied k (the number of the nearest neighbors), the performance of RMSE for GBKII is better than the  X  X oclassified X  between the grey based methods. 4.2 Experimental Evaluation on Classification Error Rate Two UCI datasets (i.e., Hepatitis Diagnosis Problem dataset and Water-Treatment Domain dataset) are applied to compare the performances of the five algorithms. At Table 2) which is an incomplete dataset and hasn X  X  filled up with any imputation methods by C5.0 (available at www.rulequest.com), then we get four completed datasets by four imputation methods, for example  X  X oclassified X , GBKII , k-NN, and  X  X eanMode X  X . We present the results of classification Error Rate of these four algorithms. Rate of incomplete dataset for Hepatitis and Water-Treatment dataset 
From Table 2, the results of the four imputation methods are significantly well than the method no imputing, this show we maybe impute the missing values rather than algorithm (including  X  X oclassified X  and GBKII ) obviously outperforms the Euclidean based k-NN iteration imputation algorithm and mean algorithm in classification error  X  X oclassified X  method. 4.3 Experimental Evaluation on Single Imputation and Iteration Imputation In this subsection, we compare the performances of our algorithm with the single imputation method (the single imputation method imputes missing values by using the adopt the iteration imputation method to deal with missing attributes. As we have seen, GBKII is an instance-based imputation method and a nonparametric method in statistics. Different from existing imputation methods, GBKII is able to deal with categorical attributes with missing values. In this approach, the grey relationship) between two instances than Minkowski distance does, has been used to rate of iteration imputation. On the other hand, GBKII searches for the nearest neighbor instance with the same class label between the instance and the missing instance, which can reduce the time complexity, and improves the prediction errors. In particular, this EM-like iteration imputation method can get over the problem of the slow convergence rate of EM algorithm and make the best use of all the information of observed values including the values with missing values. Experimental results of four UCI datasets have showed that our method is superior to k-NN and mean (or mode) substitution in convergence rate, RMSE for prediction accuracy and classification error rate. 
