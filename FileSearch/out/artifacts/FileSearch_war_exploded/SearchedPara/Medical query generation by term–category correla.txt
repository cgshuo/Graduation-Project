 1. Introduction
When searching for medical information, users often have specific information needs ( Zeng et al., 2004 ). However, the queries generated by the users are often too short and general to express the information needs ( Eysenbach &amp; K X hler, 2002; Zeng et al., 2004 ), mainly due to the difficulty in constructing proper keyword-based queries ( Ivanitskaya, O X  X oyle, &amp; Casey, 2006 ). To tackle the imprecise keyword-based queries, natural language descriptions offer a natural way for the users to express their information needs more easily and precisely ( Bader &amp; Theofanos, 2003; Luo, Tang, Yang, &amp; Wei, 2008; Spink et al., 2004 ). Many techniques were thus developed to measure the relevance of medical texts with respect
Raj, Kumar, &amp; Mittal, 2007 ), and several benchmark biomedical databases were constructed to facilitate the development of such techniques (e.g., the OHSUMED database, Hersh, 1994 ; the databases in the TREC genomics track
However, search engines and text databases often operate on keyword-based queries, incurring the necessity of automat-ically generating keyword-based queries from the natural language information need descriptions. The challenge of the query generation lies in selecting terms from the descriptions, which are often unstructured and brief (e.g., one to two sentences). 1.1. Problem definition
In this paper, we develop and evaluate a technique MQG (Medical Query Generator) that generates keyword-based que-ries from natural language descriptions of medical information needs. MQG aims at serving as a front-end component of a measurement and ranking). Technically speaking, given an information need, MQG generates a query by selecting the terms that have stronger term X  X ategory correlation to medical categories. These terms are capable of indicating the main topic of the information need, and hence are helpful in retrieving relevant texts for the information need. 1.2. Related work
Many previous studies developed techniques to generate queries as well. However they often focused on expanding que-ries by adding terms, rather than selecting query terms from information need descriptions. More specifically, queries were
Chu, 2005 ). We will show that, by properly selecting query terms from information need descriptions, MQG may achieve better performance than those techniques that expand queries with a complete set of related terms, even though MQG does not expand queries with the related terms.

On the other hand, terms that are related to query contexts were extracted from the history of query and click through nearby terms of query terms ( Finkelstein et al., 2001 ). Terms that are related to user preferences were extracted from rele-vance feedback (documents designated by the user, Somlo, 2003 ), sample documents ( Balog, Weerkamp, &amp; de Rijke, 2008 ), and pseudo-relevance feedback (documents retrieved and top-ranked, Tao &amp; Zhai, 2006 , and documents retrieved and clas-and we expect that MQG may complement the techniques by focusing more on the selection of query terms from informa-tion need descriptions.

In selecting terms, previous studies developed several techniques, although many of them selected terms from other sources such as the documents retrieved ( Ghani et al., 2001 ) and sample or relevant documents designated by the users ( Balog et al., 2008; Somlo &amp; Howe, 2003 ). Among the techniques, odds-ratio ( Ghani et al., 2001 ) and TFIDF (term fre-employed and shown to be the best ones, indicating that those special terms that appear frequently in a document tend to be the ones that should be selected. Odds-ratio is not applicable to the problem we study in the paper, since only information need descriptions are given, without relevant documents designated. TFIDF is applicable to the problem; however it does not con-sider main topics of the information need descriptions. Although detection of the main topics may be modeled as a text clas-
We will show that, by mining for term X  X ategory correlation to detect main topics of the information need descriptions, MQG may select more proper terms to retrieve relevant texts. When compared with several state-of-the-art techniques, MQG retrieves fewer references for ranking, and retrieves more relevant references that may be ranked high for users to access. 2. Methods
The research method of the paper consists of two phases: the development phase and the evaluation phase in which MQG is developed and empirically evaluated respectively. 2.1. Development of MQG
Development of MQG is based on a hypothesis: in the description of an information need, those terms that are strongly correlated to medical categories should be good in indicating the main topics of the information need, and hence they are helpful in retrieving relevant texts for the information need. The hypothesis is based on the observation that medical cate-gories may serve as basic semantic units for indexing medical information. several terms that are strongly correlated to a category, the information need may be about the category, and hence these terms should be selected to retrieve texts for the information need.

In response to the hypothesis, MQG consists of two modules: an offline module and an online module. To develop the offline module, there are three tasks: identification of terms, measurement of the correlation between each term and medical category, and removal of those terms that are not good in query generation. To develop the online module, there are two tasks: selection of terms from an input information need, and generation of a query (based on the selected terms) for the information need. The resulting design for the two modules is presented in Section 3 . 2.2. Evaluation of MQG
An experiment is designed to empirically justify the contributions of MQG. Table 1 outlines the main settings for the experiment, which are described in the following subsections. 2.2.1. Experimental data
Experimental data is from OHSUMED ( Hersh, 1994 ), which is a popular and publicly accessible database of abstracts of medical references. There are 348,566 references in OHSUMED, with each reference indexed by MeSH (Medical Subject
Headings) terms that indicate its main topic. 4 An indexing term corresponds to a category label. Since both MQG and the base-lines need training data to conduct training, we conduct 4-fold cross validation by evenly splitting the references into four parts. 5 Experiments are conducted four times so that each part is used for testing only once and the other three parts are for training. Average performance in the four experiments is reported.

OHSUMED provides 106 information needs (information requests) issued by healthcare professionals. It also provides mappings between each information need and its definitively relevant references. The mappings serve as the basis to eval-uate query generation techniques  X  when references are retrieved for an information need, the relevance mappings are con-sulted to determine which references are relevant to the information need.

A few steps are conducted to preprocess the references and the information needs, including transforming all characters into lower case, replacing non-alphanumeric characters and  X  X  X  X  with a space character, and removing stop words. 2.2.2. Baseline query generation techniques
Previous systems employed OHSUMED to evaluate text classification (e.g., Lan, Tan, Su, &amp; Low, 2007; Yang &amp; Pedersen, 1997 ), text filtering (e.g., those that joined the filtering track of TREC, Robertson, 2000 ), text ranking (e.g., Cao et al., generating keyword-based queries to retrieve candidate references for further processing. The text classification and filtering systems did not generate queries to retrieve references, while the text ranking and concept-based retrieval systems assumed that a certain set of candidate references were retrieved and preprocessed (e.g., indexed) for ranking, and hence focused on how the candidate references were ranked, rather than how the candidate references were retrieved. In practice, MQG may be a front-end processor for them to retrieve candidate references to process.

Therefore, to measure contributions of MQG, we implement several baselines that employ state-of-the-art techniques in selecting query terms. As noted in Section 1 , previous related studies noted that TFIDF was one of the best techniques in selecting query terms ( Balog et al., 2008; Somlo &amp; Howe, 2003 ). Therefore, we set up several baselines that employ TFIDF weighting to select query terms from information need descriptions: when two or more terms may be selected, the baselines assign higher priorities to those having higher TFIDF values. to a text ranker, which often employs TF and IDF (or their variants) of terms as key factors for ranking, uments that contain the terms with higher priorities assigned by the baselines tend to be ranked high by the ranker as well.
As defined in Table 1 , to conduct complete performance comparison, we implement nine baseline query generation tech-niques, based on four design options: (1) whether all single words (1-grams) in the information need descriptions are treated as candidate terms, Ref. baselines (2) whether a medical dictionary is employed so that only those terms that are in the information need descriptions and in (3) whether retrieval equivalence terms are used to expand the queries, where retrieval equivalence terms of a term t are (4) whether term matching is relaxed to conduct  X  X  X ag-of-word X  matching where a multi-word query term t is said to
With the four design options and the TFIDF term weighting technique noted above, the baselines may comprehensively represent previous popular query generation methodologies.

More specifically, the medical dictionary is from a popular thesaurus MeSH, which contains 24,767 main headings (med-of indexing and retrieval, although they are not always strictly synonymous with the main headings. and their retrieval equivalence terms. Since the list of retrieval equivalence terms are complete and manually-encoded, the baselines may represent several state-of-the-art query expansion techniques. As in many previous approaches, when consecu-tive words in a query match multiple MeSH terms, the baselines selects the longest one as the matched term.
Note that the baselines might generate null queries. For example, when all terms in an information need description are not in MeSH, the baseline that employs MeSH to select terms may generate a null query. Since null queries could not be acceptable in practice, once a baseline generates a null query for an information need, it selects a single word (from the description) with the largest IDF value to form the query. 2.2.3. Evaluation criteria
In practice, references retrieved by a query are often ranked by a text ranker so that a certain number of references may be selected for further processing. A query generator should thus aims at two goals: (1) retrieving more relevant references that may be ranked high by the ranker, and (2) reducing the load incurred to the ranker so that the ranker may serve more queries in the same period of time.

For the first goal (retrieving more relevant references that may be ranked high), we employ P @ X as the evaluation crite-P @ X is measured by Eq (1) .

Therefore, P @ X actually measures the precision rate when X references are output by the system. We report average P @ X performance on the 106 information needs in the 4-fold cross validation. To conduct complete evaluation, we test several settings for X in the range of 3 X 400.

On the other hand, to evaluate how the query generators reduce the load incurred to the ranker, we employ the number of references retrieved. The number reflects the load incurred to the ranker: the more references retrieved, the heavier load incurred to the ranker, reducing the number of queries that may be served by the ranker in the same period of time. We report the average number of references retrieved for the 106 information needs in the 4-fold cross validation. A better query generator should achieve higher P @ X , and at the same time, retrieve fewer references for ranking. 2.2.4. The underlying text ranker
Since the numerator of P @ X is the number of relevant references that are retrieved and ranked top X by a text ranker, we 2006; Liu et al., 2007 ). As many other text rankers, BM25 ranks a reference d higher if d shares more highly-weighted terms with the information need description q , although it has an additional consideration on the mean reference length. Those references that are ranked high by BM25 are thus expected to be ranked high by other state-of-the-art rankers as well. More specifically, the BM25 score of a reference d with respect to an information need q is measured by Eq. (2) .
In Eq. (2) , the summation is conducted over each word t i in training references, and TF is term frequency. 3. Results
MQG is developed and empirically evaluated. When compared with the baselines, MQG retrieves more relevant refer-ences that are ranked high for users to access, and reduces the load incurred to the text ranker by retrieving fewer documents for ranking. 3.1. Development of MQG
As outlined in Fig. 1 , MQG consists of an offline feature mining module and an online query generation module. The for-mer identifies features by mining a given set of medical texts that are labeled with categories, and based on the features, the latter generates a query for each infirmation need description. 3.1.1. Offline feature mining
When mining for features, MQG only employs categorized medical texts as the training data  X  no domain-specific the-sauri are employed. The basic idea is that, medical categories are basic semantic units for medical texts, and hence mining for those features that are correlated to the categories may help to identify semantic units of medical information needs, and accordingly generate proper queries for the information needs.

More specifically, the feature mining algorithm is defined in Table 2 . MQG identifies three kinds of features: word X  X at-extracts word pairs and single words from each training text (Ref. Step 1).

MQG estimates correlation strengths of the single words and word pairs to individual categories. It employs the v strength of a single word or a word pair t to a category c is v
In Eq. (3) , N is the total number of training texts, A is the number of training texts that are in c and contain t , B is the number of training texts that are not in c but contain t , C is the number of training texts that are in c but do not contain c ,if A D &gt; B C .
 (Ref. Step 3.1.1). In that case, the maximum correlation strength ( v (Ref. Steps 3.1.1.1 X 3.1.1.3). Similarly, for each single word w and category c , MQG records whether w is positively correlated to c (Ref. Steps 5.1.1.1 and 5.1.2), and computes the maximum correlation strength ( v egories (Ref. Steps 5.1.1.2 X 5.1.1.4). Finally, to remove those features whose correlation strengths are not high enough, MQG word pairs and single words whose maximum correlation strengths are below the threshold are not selected as features (Ref.
Steps 7 and 8). 3.1.2. Online query generation
Based on the features mined, MQG generates a query for each input information need description. The basic idea is that, the query should be composed of those terms that may (1) describe main topics of the information need, and (2) have stron-ger correlation strengths to indicate semantic units of the main topics.

More specifically, Table 3 presents the algorithm of the online query generation module. The generated query is expressed with multiple words in each term integrated in conjunction form (Ref. return in Table 3 ). For example, for the information are selected, the query is (( pericardial AND effusion )OR( radiation )). Disjunction of multiple terms may make the query rep-resentative to cover all the terms, while conjunction of the words in each term may make the query precise to express each term. 12
Therefore, the main challenge of the query generation lies in the selection of the terms. MQG first identifies the main top-the 6th word are positively correlated to category c , 13 semantics of a word often heavily depends on its neighbors.

Based on the classification, the strings corresponding to the main categories (main topics) of the information need may be the categories with the largest score. Consider the above example about the 65th information need. If category c gets the highest score, the corresponding four words ( chronic inflammatory demyelinating polyneuropathy ) will form the string corre-sponding to the main topic of the information need. Those strings identified in this way tend to convey the main topics of the information need, and hence should be the main strings from which query terms are extracted (Ref. Step 5 in Table 3 and G3 in Fig. 1 ).

Table 4 presents the algorithm for extracting query terms from the main strings. MQG first extracts from the strings all single words and word pairs that are selected in mining (Ref. Steps 3.1 and 3.2 in Table 4 ), with those words not seen in mining considered as well (assigned with a strength higher than xThreshold defined in Table 2 , Ref. Step 3.2.2 in Table 4 ).
The single words and word pairs are then sorted in decreasing order of their maximum correlation strengths (Ref. Step 5 in Table 4 ). Consider the above example about the 65th information need. If the main string is  X  X  chronic inflammatory demy-the semantic units of the strings.

It should be noted that, there may be no terms identified, due to two reasons: (1) no main topics may be identified (i.e., F is 0 for each category c , after the execution of Step 2 in Table 3 ), or (2) no terms may be extracted from the corresponding duced in offline feature mining presented in Section 3.1.1 ). In that case, MQG directly identifies the terms from the whole information need description, without relying on information need classification (Ref. Step 6.1 in Table 3 ). If such an ap-proach cannot identify terms either, MQG selects the word t with the highest IDF, Ref. Step 6.2 in Table 3 .
Also note that, since the output query is a disjunction of terms, retrieval by one term may cover retrieval by another term, than  X  bc  X , and hence  X  bc  X  becomes redundant as well. 15 compact and precise (Ref. Step 7 in Table 3 and G4 in Fig. 1 ). 3.2. Evaluation of MQG
The experiment outlined in Section 2.2 is conducted to evaluate MQG. For objective performance comparison, the base-lines also integrate the selected terms in DNF as well (as MQG does), which is good to express the information needs. over, the baselines and MQG require a parameter that governs the maximum number of terms that may be extracted from an information need description. When the parameter is larger, the generated query will be more  X  X  X exically X  similar to the infor-mation need but not necessarily more proper in retrieving those relevant references that may be ranked high. compare the best performance of MQG and the baselines, we empirically find the best parameter settings for them. For each report the best P @ X and the number of references retrieved when the best P @ X is achieved.

Fig. 2 illustrates the P @ X performance of each query generator, and Table 6 shows the improvements provided by MQG over each baseline, with statistically significant improvements identified (two-tailed and paired t -test with 95% confidence level). The results show that (1) MQG achieves higher P @ X than all the baselines under all the different settings for X , indicating that it may retrieve (2) Relaxing the matching of terms tends to improve the baselines (Ref. Dict vs. Dict_R; Dict + E vs. Dict + E_R; Dict + 1G (3) Expanding queries with retrieval equivalence terms is helpful for the baselines as well (Ref. Dict vs. Dict + E; Dict_R vs. (4) When fewer top-ranked references are considered (i.e., X
Moreover, Fig. 3 shows the number of references retrieved by each query generator. MQG retrieves fewer references than all the baselines under all the different settings for X , largely reducing the load incurred to the text ranker.
Therefore, without requiring any thesauri of medical terms and their retrieval equivalence terms, MQG successfully re-trieves more relevant references that may be ranked high, with fewer references retrieved for ranking. Table 5 raises three examples to illustrate the contributions. It lists performance of MQG, 1G, and Dict. For the 34th information need, 1G and  X  adult  X  is the term that is included in MeSH and gets the highest IDF in the information need. Unfortunately, since the two terms are not topic-indicative, many documents are retrieved without achieving better P @50. By mining for category term in MeSH.

On the other hand, for the 65th information need in Table 5 , X  polyneuropathy  X  gets a higher priority of being selected in both 1G and Dict, while MQG only selects  X  demyelinating  X  since candidate terms ordered by their correlation strengths are better P @50 than 1G and Dict as well, since among the top-50 references retrieved by MQG, there is a relevant reference not in the top-50 references retrieved by 1G and Dict. 18 The relevant reference does not mention  X  polyneuropathy  X , which is se-as a query term, and hence its inclusion to the query misleads the search engine, which retrieves more non-relevant references that are ranked high.

However, for the 30th information need in Table 5 , MQG does not contribute improvement. The baseline 1G retrieves the fewest references (284), although among them no relevant references may be ranked top 50. MQG and Dict retrieve similar numbers of references (1001 vs. 1018), and achieve the same but quite low P @50 (0.001). An analysis shows that the infor-mation need has three relevant references but only one of them is retrieved by MQG and Dict and ranked top 50. The other two relevant references talk about  X  X  X enal insufficiency X  but do not mention  X  X  X ubular necrosis, X  which is a target concept of the information need, making them unable to be retrieved and ranked high. To retrieve them, the query should contain  X  X  X e-nal insufficiency, X  which is more general than the target  X  X  X ubular necrosis. X  However, it is challenging to set up proper gen-erality to select general terms for a term, and the query may be too general to precisely express the information need and hence retrieves many irrelevant references. Moreover, to rank the two relevant references high, the ranker should be revised to consider the general terms, which do not appear in the information need description. The effective and efficient way to consider the general terms is thus a challenge for the development of the ranker as well.

It is also interesting to note that, MQG not only reduces the load incurred to the text ranker, but also reduces the load incurred to the search engines since it employs fewer terms in the queries. The reduction may be more obvious when com-pared with those baselines that expand queries with retrieval equivalence terms, since on average a MeSH term has 21 re-trieval equivalence terms. 4. Conclusion
Since natural language descriptions are helpful for users to precisely describe their medical information needs but search engines often operate on keyword-based queries, there is a necessity of a front-end processor that generates keyword-based queries from the descriptions of medical information needs. In response to the necessity, we consider a hypothesis: in the description of an information need, those terms that are strongly correlated to medical categories should be good in indicat-ing the main topics of the information need, and hence they should be selected into the query so that relevant texts may be retrieved for the information need.

A technique MQG is thus developed based on the hypothesis, and the validity of the hypothesis is justified by the eval-uation of MQG. By mining for category-correlated terms, MQG may properly select terms from the descriptions of medcial information needs. When compared with several state-of-the-art query generatoion techniques, MQG retrieves more rele-vant information that may be ranked high, without relying on any thesauri of medical terms and their retrieval equivalence terms. Moreover, MQG reduces the load incurred to both the text ranker (by retrieving fewer documents for ranking) and the search engines (by using fewer terms in the queries generated).
 Acknowledgements
This research was supported by the National Science Council of the Republic of China under the Grant NSC 96-2221-E-320-001-MY3. The authors are grateful for the valuable comments from the anonymous reviewers of the paper. References
