
School of Control and Computer, North China Electric Power University, Beijing, China
Institute of Automation, Chinese Academy of Sciences, Beijing, China School of Automation, Southeast University, Nanjing, Jiangsu, China 1. Introduction
Clustering [5,26] refers to the fundamental problem of unsupervised classification in pattern recog-nition, aiming to group similar objects into their respective categories. As one of the most widely used techniques for exploratory data analysis, it attracts much attention and various clustering methods have been presented [6,16,18], most of which either deal with original data, e.g., K -means [6,23], their lin-ear transformation, e.g., spectral clustering [18], or its simple non-linear transformation, e.g., kernel K -means [6]. However, if original data is not well distributed due to large intra-variance as shown in formance because they cannot provide a highly non-linear transformation from the original space to the feature space as shown in the right part of Fig. 1.

In order to solve the above problem, we attempt to map original data space to a new feature space which is more suitable for clustering. The auto-encoder network [1,9] is one kind of deep neural net-work (DNN) [8,9] that can effectively handle this problem. It provides a non-linear mapping function by iteratively learning the encoder and the decoder. The encoder is actually a non-linear mapping func-tion, and the decoder demands accurate data reconstruction from the representation generated by the encoder. This process is iterative, which guarantees that the mapping function is stable and effective to represent the original data. Different from kernel K -means [6], which also introduces non-linear transformations with fixed kernel functions, the transformation function in deep auto-encoder is highly non-linear, learned by optimizing an objective function.
However, the basic auto-encoder network is originally designed for data representation, aiming to minimize the reconstruction error. In order to make it suitable for clustering, we propose a new objective function embedded into the deep auto-encoder network model. It contains two parts corresponding to reconstruction and regularization. The second part mainly refers to the distance between data and their corresponding cluster centers in the transformed feature space. During optimization, data representation and clustering centers are updated iteratively, from which a stable performance of clustering is achieved and the new representation is more compact with respect to the cluster centers. The right part of Fig. 2 illustrates such an example. To evaluate the effectiveness of this model, we conduct a series of exper-iments in four widely used databases for clustering. The experimental results show that our method performs much better than traditional clustering algorithms measured by three widely used evaluation criterions.

The rest of the paper is organized as follows. Firstly we discuss related work and the background of our proposed method in Section 2, and then explain the details of our method in Section 3. Experimental settings and results are provided in Section 4, and finally, Section 5 concludes the paper and discusses future work. 2. Related work and background Popular clustering models. A large number of clustering algorithms [26] for dealing with various data have been proposed [2,5,6,11,29], among which K -means [6,23] is an representative and classic clus-tering algorithm aiming to processing data in the original data space, and kernel K -means [6] is newly developed in order to apply clustering in feature space through a simply non-linear transformation. However, it is difficult for these two algorithms to achieve satisfying performance in dealing with large datasets with many categories. Spectral clustering [29] is also a popular clustering method, but its main disadvantage is its expensive computation, especially for large data sets since it relies on computing eigenvector of a matrix. With the same computational problem, normalized cuts clustering ( N -cuts) [30] is also a widely used clustering method. In addition of traditional clustering algorithms, neural network based clustering models [7] are also been proposed, such as artificial neural networks (ANNs) [12], which has been used extensively over the past three decades for both classification and clustering. Other neural networks based clustering models include Kohonen X  X  self-organizing map (SOM) [13,19] and ARTs [3]. Beyond these classic methods, some researchers proposed new techniques for clustering such as maximum margin clustering (MMC) [25,30] and nonnegative matrix factorization (NMF) [27,28]. Deep neural networks. With the development of deep learning, th e deep neural networks (DNN) attracts more and more attentions, and many surprisingly good performance have been reported, e.g., Alex et al. won the ILSVRC2012 competition by using deep CNNs [14] with great advantage over the traditional bag-of-features model. As DNN ha s a strong ability of non-linear ma pping, unsupervised learning like clustering based on DNN performs well in recent researches [1,10]. A large number of methods based on DNN are proposed, e.g., the authors of [4] proposed a clustering framework that uses K Restricted Boltzmann Machines ( K -RBMS) to learn multiple non-linear subspaces in the original image space.
This paper is an extension of our original conference paper, which has been presented in [22]. In the conference paper, we proposed an effective clustering algorithm based on auto-encoders, which is one kind of deep neural networks (DNNs). In this paper, we re-organize the representation of the proposed clustering framework of [22], and show the influence of using different lambda. We also complement a widely used criterion, i.e., purity (short for PUR), which is useful for comparisons by others who use PUR in evaluation, and an experiment on the PIE face datasets (one of the most challenging image and test more complicated structure. Additionally, some new visualization results, related work and future work are also included. In this paper, we try to propose a clustering model based on deep auto-encoder network, which can deal with difficult clustering problems. Details will be explained in the following sections.
 3. Proposed model
As shown in Fig. 2, the data layer (e.g., the pixel representation of an image) is firstly mapped to the feature layer, which is then used to reconstruct the data layer. The objective is minimizing the re-construction error as well as the distance between data points and corresponding clusters in the feature layer. This process is implemented via a four-layer deep auto-encoder network, in which a non-linear mapping is resolved to enhance data representation in the data layer. Without loss of generality, in the next subsections, we firstly introduce the basic auto-encoder network, and then explain how to apply it in clustering. 3.1. Basic auto-encoders networks
For clarity, we take an one-layer auto-encoder network as an example which consists of an encoder and a decoder. The encoder maps an input x i to its hidden representation h i . The mapping function is usually non-linear and the following is a common form: aims to learn a useful hidden representation by minimizing the reconstruction error. Thus, given N training samples, the parameters W 1 , W 2 , b 1 and b 2 can be resolved by the following optimization problem:
In our proposed model, the deep auto-encoder network is adopted. Generally, it is constructed by stacking multiple one-layer auto-encoders. That is, the hidden representation of the previous one-layer auto-encoder is fed as the input of the next one. For more details of the auto-encoder network and its optimization, readers are referred to [1,9]. 3.2. Clustering based on deep auto-encoder
According to the above analysis, deep auto-encoder is a powerful model to train a mapping function, which ensures the minimum reconstruction error from the code layer to the data layer. Usually, the feature layer has less dimensionality than the data layer. Therefore, deep auto-encoder can learn an effective representation in a low dimensional space, and it can be considered as a non-linear mapping model, performing much better than PCA [9]. However, auto-encoder contributes little to clustering because it does not pursue that similar input data obtain the same representations in the feature layer, which is the nature of clustering. To solve this problem, we propose a new objective function and embed it into the auto-encoder model: where N is the number of samples in the dataset; f t (  X  ) is the non-linear mapping function at the t th centerofthe i th sample in the feature layer. This objective ensures that the data representations in the feature layer are close to their corresponding cluster centers, and meanwhile the reconstruction error is still under control, which is importa nt to obtain stable non-linear mapping.

Two components need to be optimized: the mapping function f (  X  ) and the cluster centers c .Tosolve this problem, an alternate optimization method is proposed, which firstly optimizes f (  X  ) while keeps c fixed, and then updates the cluster center: the number of samples in this cluster. The sample a ssignment computed in the last iteration is used to our method in Algorithm 1.
 Algorithm 1 Deep auto-encoder based clustering algorithm 1: Input: Dataset X , the number of clusters K , hyper-parameter  X  , 2: Initialize sample assignment C 0 randomly. 3: Set t to 1 . 4: repeat 5: Update the mapping network by minimizing Eq. (4) with sto-6: Update cluster center c t via Eq. (6). 7: Partition X into K clusters and update the sample assignment 8: t = t +1 . 9: until t&gt;T 10: Output: Final sample assignment C . 4. Experiments 4.1. Experimental setups Database . All algorithms are tested on 4 databases: MNIST, 2 USPS, 3 YaleB 4 and PIE. 5 They are all well-known and widely used databases for evaluating clustering algorithms. 1. MNIST contains 70,000 handwritten digits images (0  X  9), including 60,000 training examples and 2. USPS consists of 4,649 handwritten digits images (0  X  9) with the resolution of 16  X  16 pixels. 3. YaleB is composed of 5,850 faces image over ten categories, and each image with the size of 30  X  4. PIE is subset [2] of CMU faces datasets [21]. It is composed of 11,554 images over 68 categories, Parameters . For MNIST, USPS and YaleB, we chose the clustering model based on a four-layer deep auto-encoder network with the structure of 1000-250-50-10. As for PIE, we adopt a larger network with the structure of 1000-2000-1000-680 which is more suitable in case of data with more categories whereas less samples in each category, e.g., PIE datasets has 68 classes, each with approximately 170 samples. space (or the feature layer of auto-encoder). The parameter  X  in Eq. (4) is set by cross validation. That is 0.1 on MNIST and PIE, 0.6 on USPS and YaleB. To classify the influence of lambda, as shown in Fig. 3, among a series of lambda, the best one for YaleB is around 0.5 X 0.7. The weights W in the auto-encoder network are initialized via a standard restricted Boltzmann machine (RBM) pre-training [9]. The influence of the iteration number will be evaluated in the following subsection. Baseline Algorithms . In order to demonstrate the effectiveness of our method, we compare our method with three classic and widely used clustering algorithms: K -means [6,23], spectral clustering [18] and N -cut [20]. For detailed analysis of these algorithms, please see Section 2.
 Evaluation Criterion. Three metrics are used to evaluate experimental results explained as follows. 1. Normalized mutual information (NMI) [16]. Let R denote the label obtained from the ground truth 3. Accuracy (ACC) [27]. Given an image x i ,let c i be the resolved cluster label and r i be the ground 4.2. Quantitative results
In this subsection, we evaluate the influence of the iteration number in our algorithm. Figure 4 shows the change of NMI, PUR and ACC as the iteration number increases on four databases. In order to show the speed of proposed clustering algorithm, we also provide a visualization of updating cluster centers (on MNIST digits) as shown in Fig. 5.

It can be obviously found that the performance is enhanced fast in the first ten iterations, which demon-strates that our proposed method is effective and efficient. After dozens of iteration, e.g., 40 X 60, both NMI, PUR and ACC become very stable. Thus, in the rest of experiments, we report the results after 50 iterations. The performances of the different methods on four datasets are shown in Table 1. Apparently that our method is better than or at least comparable to their best cases. 4.3. Visualization
The visualized results on MNIST are shown to provide an in-depth analysis in this subsection. We draw in Fig. 6 the distribution of ten categories of digits obtained by K -means, spectral clustering and proposed method. For proposed method, most of histograms in the right of Fig. 6 are single-peak distri-butions, demonstrating the compactness of data representation. Admittedly, the cases of digits 4 and 9 are not so good. Some images in these two classes are even difficult to classify by human. The digital images in the below of Fig. 7 are the reconstructed results of cluster centers in the feature layer. It is proposed clustering method.

For comparison, we also show the average data representations over all clusters by K -means and spectral clustering in the left and middle of Fig. 6, respectively. The results are much worse, and can be easily understood with the motivation of our method. Generally, K -means uses a similar iteration procedure as ours in Algorithm 1 except that it is performed in the original pixel space. That is, the iteration of K -means is performed in the data layer, whereas ours in the feature layer, which is mapped from the data layer with a highly non-linear function by exploiting the hidden structure of data with the deep auto-encoder network. It also performs better than spectral clustering using linear mapping. 4.4. Difference of spaces
In this subsection, we analyze the difference of three spaces, i.e., the original data space, the space learned via non-linear mapping with original auto-encoder, and the one learned by our method. Corre-spondingly, we apply K -means clustering in these spaces. Their clustering results are shown in Table 2. Obviously, the clustering performance in the space of auto-encoder is much better than the one in the original space, and much worse than the one proposed by us. This result justifies two viewpoints: 1) Non-linear mapping by auto-encoder can greatly improve the representation of data for clustering; 2) Our proposed objective function, defined in Eqs (4) X (6), is effective to further enhance clustering due to the design of increasing data compactness as analyzed in Section 3.2. The results are tested on the MNIST datasets. 5. Discussion In this paper, we have proposed a new clustering method based on the deep auto-encoder network. By well designing the constraint of the distance between data and cluster centers, we obtain a stable and compact representation, which is more suitable for clustering. As this deep architecture can learn a powerful non-linear mapping, the data can be effectively partitioned in the transformed feature space. The experimental results have demonstrated the advantage of the proposed model compared with classic and widely used clustering algorithms.

In future, we will try to apply a series of experiments to explore how the network structure influences the performance of clustering. It may help us to reach a better and thorough understanding of the deep neural network (DNN) and clustering via non-linear transformations. We also plan to add drop-out [24] to prevent the over-fitting problem in training networks according to [ 10]. Besides, a pr obability-based model in assigning data to their corresponding cluster centers is potential in future work, which can decrease the possibility of local optimal solution.
 Acknowledgements
This work was jointly supported by National Natural Science Foundation of China (61135002, 61175003, 61203252), and the Fundamental Research Funds for the Central Universities Grant. References
