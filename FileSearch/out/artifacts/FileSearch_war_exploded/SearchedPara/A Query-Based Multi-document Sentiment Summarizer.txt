 Review websites, such as Epinions.com, which offer users a plat-form to share their opinions on diverse products and services, provide a valuable source of opinion-rich information. Browsing through archived reviews to locate different opinions on a product or service, however, is a time-consuming and tedious task, and in most cases, the large amount of available information is difficult for users to absorb. To facilitate the process of synthesizing opin-ions expressed in reviews on a product or service P specified in a user query/question Q , we introduce QMSS , a query-based multi-document sentiment summarizer. QMSS creates a summary for Q , which either reflects the general opinions on P or is tailored to specific facets (i.e., features) and/or sentiment of P as specified in Q . QMSS (i) identifies the facets addressed in reviews retrieved for Q , (ii) employs a sentence-based, sentiment classifier to deter-mine the polarity of each sentence in each review, and (iii) clusters sentences in reviews according to the facets captured in the sen-tences, which are identified using a keyword-label extraction algo-rithm. This process dictates which sentences in the reviews should be included in the summary for Q . Empirical studies have veri-fied that QMSS is highly effective in generating summaries that satisfy users X  information needs and ranks on top among the state-of-the-art query-based multi-document sentiment summarizers. H.3.5 [ Information Storage and Retrieval ]: Online Information Services X  Web-based Services Design, Experimentation, Algorithms, Performance
Websites established exclusively to archive reviews, such as Con-sumerreports.org and Epinions.com, provide users a forum to ex-
The full version of this paper can be downloaded from http:// faculty.cs.byu.edu/  X  dennis/papers.html press their opinions on various products, i.e., anything that is vis-ible, living, or nonliving, such as a person, a movie, or a book These websites archive abundant opinions that are valuable for web users who seek feedback on a product. Searching and browsing through large collections of reviews with different opinions, how-ever, is undesirable, since it is a time-consuming and tedious task.
To facilitate the task of synthesizing opinions expressed in var-ious reviews on a particular product P specified in a user query/ question Q , we introduce QMSS , a query-based, multi-document sentiment summarizer. Given Q and a set of reviews R on P ex-tracted in response to Q from well-known review websites, QMSS creates an extractive summary for Q , which is tailored towards the information needs expressed in Q . A summary for Q is (i) general if Q inquires common feedback on P , (ii) sentiment-specific if Q asks for positive (negative, respectively) information about P , (iii) facet-specific if Q queries specific facets of P (such as the battery life of a camera), and (iv) facet-sentiment-specific if Q looks for positive (negative, respectively) information on specific facets of P .Since QMSS considers the sentiment expressed in Q to create its summary, it is different from other query-based multi-document summarizers that do not capture sentiment in reviews.

QMSS generates summaries that are (i) non-redundant , (ii) co-hesive , and (iii) diverse (in terms of the variety of facets inquired). In addition, QMSS summaries achieve high coverage (in terms of information covered in the summaries) and include meaning-ful and relevant sentences. It (i) does not depend on pre-defined knowledge, (ii) is highly accurate in capturing useful information presented in a set of reviews, and (iii) summarizes multiple reviews in response to general ,aswellas specific , users X  information needs. QMSS is domain-independent and thus can generate summaries of reviews with different contents and structures, such as blog posts and movie reviews. By developing QMSS , we minimize the time and efforts imposed on web users to browse through reviews re-trieved by search engines. QMSS advances the technologies that support the development of systems which facilitate information access on reviews and their representation on the web.
In this section, we detail the design of QMSS in creating a sum-mary of multiple reviews retrieved in response to a query. Each component of QMSS addresses a particular research problem on its own: (i) identifying products, facets, and sentiment keywords in a user X  X  question using a multi-class SVM based on a number of novel features, (ii) finding opinions on various facets of P us-ing a novel sentence clustering algorithm based on word correla-tion factors, (iii) condensing each review by excluding sentences
To simplify the discussion, from now on whenever we refer to the term  X  X roduct", we mean  X  X roduct" or  X  X ervice". in the review that are redundant, relatively uninformative, or lack of opinions, and (iv) ensuring that each QMSS -generated multi-document summary is coherent and concise by employing a simple, yet highly effective, sentence selection algorithm. QMSS adopts a one-against-all impl ementation of a multi-class SVM to identify information needs expressed in a query. To train a multi-class SVM, each training in stance is an input vector which captures the SVM-features of a non-numerical keyword K in a training query Q as a succession of  X 1 X  ( X 0 X , respectively). Each binary value in a vector represents the presence (absence, respec-tively) of an SVM-feature F (defined by us below) if F applies (does not apply, respectively) to K .
To verify each SVM-feature accurately identifying its corre-sponding keywords (in queries) that are either Products , Facets , Sentiment Keywords ,or Non-Essential Terms , we conducted an em-pirical study using a dataset, denoted Property-DS , which does not overlap with the dataset introduced in Section 3.3 for analyzing the performance of QMSS  X  X  multi-class SVM. Property-DS consists
Stopwords are commonly-occurred words, such as articles, prepo-sitions, and conjunctions, which carry little meaning. of 3,000 opinion questions randomly extracted from Yahoo! An-swers (answers.yahoo.com) and WikiAnswers (wiki.answers.com). Keywords in each of the questions in Property-DS were identi-fied as either products, facets, sentiment keywords, or non-essential terms by independent assessors prior to conducting the evaluation. The study shows that on the average 91% of keywords in Property-DS questions of different types are correctly identified by the SVM-features, with the lowest in the 80% and highest 100%.

In identifying the user X  X  information needs in Q , QMSS repre-sents each keyword K in Q using the SVM-features and relies on the trained SVM to determine whether K is a product, facet, sen-timent keyword, or non-essential term. QMSS also detects nega-tion terms (or their stemmed versions) in Q , such as  X  X ot",  X  X o",  X  X ithout",  X  X xcept",  X  X xcluding",  X  X emove",  X  X othing", and  X  X eave out", so that the polarity of a sentiment keyword in Q is reversed if it is preceded by a negation term.
Having identified the product P specifiedinaquery Q , QMSS queries Epinions.com, Consumersearch.com, and Consumerre-ports.org, three well-known websites dedicated exclusively for archiving reviews, to retrieve reviews on P for creating the sum-mary for Q . Since search engines at these websites simply process exact keyword searches without accurately relating the sentiment applied to P in Q , QMSS extracts reviews from these websites using a simplified query Q which includes solely P specified in Q . QMSS gathers the top-33 reviews extracted from each of the three review repositories 3 in response to Q , which yields the set of reviews R for creating a summary (of R ) in response to Q .
Since each review W in R may contain sentences that do not reflect an opinion on P , it is imperative to eliminate non-sentiment information presented in W while preserve the opinions expressed in W . (Eliminating non-sentiment information can significantly shorten the processing time of creating a summary of R as a side effect.) To accomplish this task, QMSS retains sentences in W with the highest combined sentence scores , which are rich in opin-ions, to be included in the condensed version of W . The scores include (i) sentence similarity , which determines the similarity be-tween a sentence S and other sentences in W , (ii) significance fac-tor , which indicates the significance of each word in S in capturing the content of W , and (iii) sentiment score of S , which is computed by using the linguistic type (such as adjective, adverb, or noun) and the SentiWordNet (sentiwordnet.is ti.cnr.it/search.php) score, a nu-merical value describing the neutral, positive, or negative orienta-tion, of each word in S (see Sections 2.4.4 and 2.5.4 for details). In addition, QMSS pre-processes W using anaphora resolution , which identifies successive references of the same discourse entity in W . We denote the set of condensed reviews Cond-Reviews .
Sinceareviewin Cond-Reviews may address various facets of the same product specified in Q , prior to creating a summary for Q using Cond-Reviews , QMSS clusters sentences in Cond-Reviews according to the facets they address, if there are any. In doing so, QMSS (i) generates a set of cluster labels , which are facets chosen from Cond-Reviews , (ii) ranks the generated cluster labels, and (iii) assigns each sentence S in Cond-Reviews to a cluster C based on the similarity between the label of C and S . QMSS
QMSS retrieves the first 33 reviews from each repository, if they exist, since a collection of a hundred documents was found to be an ideal set for generating good, informative summaries [1]. clusters sentences to facilitate the process of creating summaries for queries that are General or Facet-Specific ( Sentiment-Specific or Facet-Sentiment-Specific , respectively), avoid redundancy ,and achieve high coverage (by including significant information in re-views) on the product being processed.
QMSS creates concise and accurate (keyword) labels that re-flect the facets addressed in Cond-Reviews usingasuffixarrayal-gorithm, which has been proved to be efficient and effective in dis-covering key phrases in large text collections. The suffix array al-gorithm generates a list of candidate cluster labels which are the suffixes in Cond-Reviews sorted alphabetically. Since not all the candidate cluster labels represent facets describing the product P in Cond-Reviews , QMSS removes labels that (i) are numerical key-words, (ii) cross sentence boundaries , since sentence markers indi-cate a topical shift, (iii) are incomplete , i.e., included as substrings in other labels, (iv) end in the Saxon genitive form ,or(v)are senti-ment keywords, i.e., terms that express a positive/negative polarity (that are considered only in sentiment summarization). Moreover, candidate cluster labels that reference web addresses are discarded and stopwords at the beginning or end of a label are removed, since stripping heading/trailing stopwords increases readability.
The selected cluster labels identify the facets to facilitate the task of grouping sentences in the condensed reviews according to the facets they describe. To capture the content-significance of the selected cluster labels, QMSS proceeds to rank the labels using various widely-adopted measures, which have been proved to be effective in identifying representative cluster labels and are defined as follows:
QMSS computes a ranking score for each cluster label L ,de-noted LRank ( L ), which reflects the significance of L in capturing the content of Cond-Reviews , by combining Freq , Stability ,and Sig _ L 5 of L using the Stanford Certainty Factor .

LRank ( L )= Freq
Dependence identifies labels that characterize the contents of sen-tences in one cluster in contrast to others. The higher the mutual information of L is, the more dependent L is as a cluster label. Since Freq , Stability ,and Sig _ L are in different numerical scales, QMSS normalizes the values using a logarithmic equation so that they are in the same range.
To cluster sentences that address (similar) facets of a product P in Cond-Reviews , we develop a simple, effective clustering method.
Given the set of selected cluster labels CL (created in Sec-tion 2.4.1), QMSS computes the degree of similarity between each sentence S in Cond-Reviews and each label L in CL , denoted LS _ Sim ( L, S )= tively) is the number of non-stopwords in S ( L , respectively), w ( w j , respectively) is a non-stopword in S ( L , respectively), and wcf ( w i , w j )isthe word-correlation factor [2] of w i word-correlation factors were generated using a set of approxi-mately 880,000 Wikipedia documents, and the correlation factor of any two non-stopwords is computed using their (i) frequency of co-occurrence and (ii) relative distances in each Wikipedia docu-ment. As the length of S can potentially affect LS _ Sim ( L , S ), since the longer S is, the higher LS _ Sim ( L , S )is, QMSS nor-malizes LS _ Sim ( L , S ) by dividing the correlation factors of non-stopwords in S and L by the number of non-stopwords in S . Having computed LS _ Sim ( L , S ) for each sentence S in Cond-Reviews with respect to each label L in CL , S is assigned to the cluster C with label L C such that the LS _ Sim ( L C ,S ) higher than all the LS _ Sim scores of S and other labels.
After generating clusters of sentences based on the facets of a product P , QMSS assigns a ranking score to each sentence S in each cluster C which indicates the relative degree of significance of S in capturing the content of C using the following measures:
QMSS computes a ranking score for S , denoted CRank ( S ), which reflects the relative degree of significance of S in C in cap-turing the content of C and is defined using the Stanford Certainty Fact or of various measures applied to S .The ranking score is for-mally defined as CRank ( S )=( SSF ( S )+ Sent _ Si m ( S LS _ Si m ( L C ,S )+ NE ( S )+ SL ( S )) / (1  X  Min { SSF Sent _ Sim ( S ) ,LS _ Sim ( L C ,S ) ,NE ( S ) ,SL ( S ) } ) 6
Sentences in each cluster with the highest CRank scores are po-tential sentences to be included in the summary of Cond-Reviews , the set of condensed reviews, in response to Q .
QMSS creates a single summary in response to the informa-tion need specified in a user query Q . A summary is (i) General , if Q inquires on common feedback of a particular product P , (ii) Sentiment -Specific ,if Q asks for positive or negative information about P , (iii) Facet -Specific ,if Q queries on specific facets of P , or (iv) Facet -Sentiment -Specific ,if Q looks for sentiment informa-tion on specific facets of P .
We adopt the length of (approximately) 250 words as the size of each QMSS -generated summary according to the guidelines defined for (some of) the datasets archived by the Text Analy-sis Conference (TAC) (nist.gov/tac/data/forms/index.html), which provides benchmark datasets for assessing the performance of a query-based multi-document summarizer. Since QMSS is a sentence-based, extractive summarizer, QMSS includes in a sum-mary as many sentences as possible up till the size limit so that the total number of words in the summary being constructed after including the second to the last chosen sentence is less than 250.
In creating a summary, QMSS includes some sentences in clus-ters created in Section 2.4.3. To determine which sentences are to be extracted from which cluster and included in the summary,
Since SSF, Sent _ Sim, LS _ Sim, NE ,and SL are in different numerical scales, QMSS normalizes the scores using a logarith-micscalesothattheyareinthesamerange.
 QMSS relies on the ranked cluster labels, as introduced in Sec-tion 2.4.2. Using the ranked labels, QMSS includes in the sum-mary of Q one sentence from a cluster at a time, starting from the cluster with the highest-ranked label (based on its LRank score defined in Equation 1), up till the limit of the summary size is reached. If the number of sentences that should be included in a summary exceeds the number of generated clusters after selecting the highest-ranked sentence in each cluster, then in the subsequent iterations QMSS selects the next highest-ranked sentence S in cluster C with the lowest similarity score , denoted LSS , with re-spect to the sentence(s) S in C that has (have) already been in-cluded in the summary for Q .The LSS score of S is computed as the sum of the word-correlation factors [2] between each non-stop, stemmed word in S and S . By considering the LSS score of a sentence S in a cluster with respect to S in the summary being constructed for Q , QMSS ensures that S and S are dis-tinct in contents, which avoids redundancy and maximizes cover-age in terms of information included in the summary, a novelty of QMSS . Furthermore, if a facet F is preceded by a negation term in Q , any cluster label that includes F is excluded from the sen-tence selection process.
The General summary for Q addresses different facets and sen-timents of a product being reviewed. It consists of the highest-ranked sentences (regardless of their polarity), along with probably the ones with the lowest LSS scores, in (highly ranked) clusters created in Section 2.4.3, which are chosen by following the proce-dure established in Section 2.5.2 to be included in the summary.
The Sentiment -Specific summary for Q is created in the same manner as a General summary, except that only the highly-ranked sentences (along with probably the ones with the lowest LSS score) in clusters which satisfy the sentiment (i.e., positive or negative) specified in Q (identified using the keyword tagger introduced in Section 2.1) are included in the summary for Q . To determine the positive or negative polarity of a sentence S in a cluster, QMSS calculates the overall sentiment score of S by subtracting the sum of its negative word SentiWordNet scores from the sum of its pos-itive word SentiWordNet scores that reflects the (degree of) sen-timent of S such that if its sentiment score is positive (negative, respectively), then S is labeled as positive (negative, respectively). Employing this sentence-based, sentiment approach, QMSS in-cludes in its Sentiment -Specific ( Facet -Sentiment -Specific , respec-tively as introduced in Section 2.5.6) summaries sentences reflect-ing the sentiment specified in the corresponding query.
To create the Facet -Specific summary for Q , QMSS first iden-tifies the labels (from the set of labels created in Section 2.4.1) that are highly similar to each of the facets F (determined using the keyword tagger in Section 2.1) specified in Q . To identify cluster labels highly similar to F , QMSS employs a reduced ver-sion of the word-similarity matrix which contains 13% of the most frequently-occurring words (based on their frequencies of occur-rence in the Wikipedia documents), and for the remaining 87% of the less-frequently-occurring words, only exact-matched correla-tion factors, i.e., correlation factors of values 1.0, are used. A la-bel L (and its cluster) is considered highly similar to F only if the word-correlation factor of ( w 1 , w 2 ) between the non-stop, stemmed word w 1 in L and w 2 in F exists in the reduced matrix, which significantly minimizes the processing time to identify the desired labels without affecting the quality of the created summaries.
In creating the Facet -Specific summary, QMSS follows the same procedure as detailed in Section 2.5.2 for selecting sentences, regardless of their polarity, to be included in the summary. In-stead of considering ranked labels, QMSS relies on the ranking of the highly similar labels with respect to F computed using word-correlation factors. Moreover, the content of the Facet-Specific summary of Q is uniformly divided among each of the facets spec-ified in Q , i.e., the number of sentences to be included in the sum-mary is uniformly extracted from the clusters with labels highly-similar to or the same as each facet specified in Q . The Facet -Sentiment -Specific summary is created in response to Q by including solely the sentences in clusters which (i) reflect the polarity specified in Q (as detailed in Section 2.5.4) and (ii) belong to the clusters with labels highly similar to or the same as a facet in Q (as in a Facet -Specific summary presented in Section 2.5.5).
In this section, we introduce the dataset and metrics used for as-sessing the performance of QMSS and detail the evaluation of the effectiveness of QMSS in generating sentiment summaries that satisfy the information needs specified in users X  queries.
We rely on the benchmark dataset set up for the Opinion Sum-marization Pilot task of the 2008 Text Analysis Conference, de-noted TAC-08, which includes a set of 87 (squishy-list) questions, to assess the effectiveness of QMSS . The assessment includes (i) identifying the information need specified in a user X  X  query Q , (ii) creating summaries that satisfy the information inquired in Q ,and (iii) generating a high-quality summary of multiple documents on a topic. For each question Q , which is treated as a query, provided by TAC-08, there is (i) a set of documents D (extracted from the TREC Blog06 collection 7 ) that serves as the source for creating a summary (of D ) and (ii) a list of expert-created sentences/phrases that are expected to be included in a summary (of D ) with respect to Q . Since the goal of the Opinion Summarization task is to eval-uate multi-document sentiment su mmaries created in response to a query, TAC-08 is ideal for evaluating the effectiveness of QMSS . To assess the performance of the multi-class SVM adopted by QMSS for identifying the information needs expressed in users X  queries, we compute the accuracy ratio, which is defined as the proportion of the number of keywords correctly identified by the multi-class SVM as products, facets, sentiment keywords, or non-essential terms over the total number of keywords in the 87 queries.
To verify whether QMSS -generated summaries satisfy the in-formation needs specified in users X  queries ,werelyonthe Nugget-Pyramid score [3]. Consider a test query Q in TAC-08,  X  X hat is positive about Carmax?". A human assessor creates a list of rel-evant  X  nuggets ," which are expert-created phrases/sentences, e.g.,  X  X armax prices are firm," that address different aspects of Q .It
TREC Blog06 is a collection of blog posts downloaded from the web between December 2005 and February 2006. is expected that a  X  X ood" summary includes (the majority of the) nuggets in the corresponding list of relevant nuggets .Weevalu-ate a QMSS -generated summary S created in response to Q by verifying the (non-)existence of a conceptual match between each provided nugget and S , which is a match independent of the dis-tinct wording used in S and the nugget.
To evaluate the quality of the multi-document summaries created by QMSS , we apply the quality measures defined by TAC-08 that assess the performance of the summarizers participated at the 2008 Text Analysis Conference (nist.gov/tac/data/past-blog06/2008/Op SummQA08.html#OpSumm). Quality measures, which are in the range of 1 to 10 (with 1 being the worse and 10 the best), reflect the overall quality of a generated summary in terms of its grammatical-ity , non-redundancy , structure and coherency , responsiveness ,and readability . Each measure is computed for each QMSS -created multi-document summary to be evaluated.
To train the multi-class SVM (i ntroduced in Section 2.1) for identifying products, facets, sentiment keywords, and non-essential terms in users X  queries, we constructed a dataset, denoted Train -SV M , which consists of approximately 32,000 keywords ex-tracted from 5,000 (opinion) queries retrieved from WikiAnswers and Yahoo! Answers. To validate the effectiveness of the multi-class SVM on Train -SV M 8 , we adopted the ten-fold cross-validation approach so that in each of the 10 repetitions, 90% of the instances in Train-SVM were used for training and the remain-ing 10% for validation purpose.

The accuracy of QMSS in identifying the information needs specified in users X  queries was computed using the trained multi-class SVM on each keyword in each of the 87 queries in TAC-08. The overall accuracy of the trained multi-class SVM on identifying the 452 keywords in TAC-08 queries is 96% as shown in Figure 1, along with the accuracy ratios achieved by QMSS on detecting products, facets, sentiment keywords, and non-essential terms.
Note that only 27 of the 452 keywords in TAC-08 questions were facets , out of which 4 were misclassified as products . The low num-ber of facets in TAC-08, when compared with products, sentiment keywords, and non-essential terms, causes the accuracy ratio for identifying facets to be the lowest (in the mid-80%).
We have validated QMSS in creating summaries that satisfy the information needs expressed in users X  queries using the Nugget-Pyramid score. The minimum , maximum , median ,and average Nugget-Pyramid scores of QMSS based on TAC-08 are 0, 0.31, 0.2, and 0.183, respectively. In comparing the (average) Nugget-Pyramid scores achieved by nineteen 9 query-based multi-document summarizers using TAC-08, QMSS ranks third . Note that thirty-six multi-document summarizers o riginally took part at the 2008 TAC. However, seventeen of the summarizers relied on snippets , which are answers to queries in TAC-08 provided by TAC, in cre-ating the summaries. To perform unbiased comparisons, we only considered the nineteen summarizers that operate in a manner sim-ilar to QMSS , i.e., they do not rely on external information, such
Keywords in Train-SVM and TAC-08 questions were previously labeled by independent assessors as Products , Facets , Sentiment Keywords ,or Non-Essential Terms .
The Nugget-Pyramid and quality scores introduced in Sec-tion 3.2 for the nineteen summarizers are available at nist.gov/tac/ protected/past-blog06/2008/QA2008_runs.tar.gz. Figure 1: Accuracy of the multi-class SVM of QMSS in iden-tifying users X  information needs specified in TAC-08 queries as snippets, in creating a summary. In addition, we have evaluated 22 summaries, as opposed to the 87 summaries created using the questions and documents in TAC-08. The choice follows the eval-uation premises defined by TAC, which provides assessment, us-ing the computed Nugget-Pyramid scores, for only 22 summaries generated by each of the 19 automatic multi-document summariz-ers. The two summarizers that are ranked higher than QMSS have achieved a Nugget-Pyramid score of 0.251 and 0.223, respectively, which are close to the average score, 0.183, of QMSS and verify the effectiveness of QMSS in generating summaries that satisfy users X  information needs.
We assess the quality of QMSS -generated summaries using each of the five quality measures introdu ced in Section 3.2.3. Follow-ing the evaluation methodology established by TAC, we relied on independent appraisers, who manually evaluated the grammatical-ity , non-redundancy , structure and coherency , responsiveness ,and readability of the summaries created by QMSS using TAC-08 (as introduced in Section 3.1). The quality scores of QMSS -generated summaries are shown in Table 1.
 To establish a baseline measure on the quality scores achieved by QMSS -created summaries, we compare the performance, based on quality scores, of the nineteen automatic multi-document sum-marizers (mentioned in Section 3.4) with QMSS basedon22sum-maries created using TAC-08 (as discussed in Section 3.4). As shown in Table 1, QMSS is significantly outperformed (with 95% confidence) by at most six (out of nineteen) summarizers in terms of creating summaries that are e ither higher in grammaticality, non-redundancy, structure and coherence, responsiveness, or readabil-ity. In fact, there is not a single system examined that achieves higher scores than QMSS in all of the quality measures. QMSS achieves the highest responsiveness score among all the nineteen summarizers and is only outperformed by one of the nineteen sum-marizers in creating summaries that are non-redundant .
The summarization systems that outperform QMSS on either the grammar , readability ,or structure and coherence quality mea-sures employ natural language processing techniques to touch up the summaries shown to users as the final products. The summa-rization approach of QMSS , on the other hand, is purely extrac-tive, i.e., it solely extracts sentences in the original document(s) without refinement to create a summary. The summarizer which achieves a better non-redundancy quality measure than QMSS uses textual graphs to model opinions in a review and relies on word QMSS Qual. Significantly Out-Sign. Out-Grammaticality 5.73 4 (16, 21, 31, 36) 14 Non-redundancy 7.12 1 (36) 17 Str. &amp; Coherence 2.82 6 (17, 18, 30, 31, 34, 36) 8 Responsiveness 3.59 0 18 Readability 4.12 3 (14, 25, 35) 8 Table 1: Comparisons between QMSS and 19 summarizers participated at TAC-08 where numbers in parentheses are IDs order to determine redundant sentences, which is not employed by QMSS for simplicity.

Among the 19 summarizers QMSS compared with, CLASSY and PolyU (with System ID 36 and 31, respectively) stand out, since they outperform QMSS on two or more measures. CLASSY generates a summary for a query by (i) splitting and trimming sen-tences in a set of documents using pre-defined rules, (ii) extract-ing query terms from a previously constructed list of (un)important keywords, (iii) scoring and selecting sentences using the condi-tional probability, and (iv) removing redundant sentences with less than 8 distinct terms. CLASSY X  X  algorithm, keyword lists, and sentence trimming are based on observations on the dataset pro-vided by TAC, whereas QMSS requires no accommodations on any dataset. PolyU, on the other hand, measures the informative richness of a sentence using a set of facets and the opinions ex-pressed in sentences, and removes paragraphs in a document with-out informative sentences to increase the grammar and structure and coherence scores, but decreases the nugget pyramid score.
We have introduced QMSS , a query-based, extractive, multi-document sentiment summarizer, which summarizes reviews re-trieved from opinion-rich repositories in response to a query Q posted by a web user who is interested in feedback compiled by other users on a product P . QMSS creates a (i) general sum-mary for Q that addresses the various facets (and sentiments) of P specified in Q , or (ii) specific summary that includes particular facets and/or sentiment of P . QMSS uses a new label extraction approach to identify facets of P and a novel clustering algorithm based on word correlation factors to group sentences in reviews based on the facets of P addressed in the sentences. We have as-sessed QMSS using TAC-08, a well-known benchmark dataset, that was created for evaluating query-based multi-document senti-ment summarizers. Empirical studies show that QMSS ranks at the top among the state-of-the-art approaches in generating query-based, high-quality summaries. [1] D. Dunlavy, D. O X  X eary, J. Conroy, and J. Schlesinger. QCS: [2] J. Koberstein and Y.-K. Ng. Using word clusters to detect [3] J. Lin and D. Demner-Fushman. Will pyramids built of [4] B. Schiffman, A. Nenkova, and K. McKeown. Experiments in [5] L. Zhang, Y. Pan, and T. Zhang. Focused named entity
