 Stroke is the third leading cause of death and the principal cause of serious long-term disability in the United States. Accurate prediction of stroke is highly valuable for early in-tervention and treatment. In this study, we compare the Cox proportional hazards model with a machine learning approach for stroke prediction on the Cardiovascular Health Study (CHS) dataset. Specifically, we consider the common problems of data imputation, feature selection, and predic-tion in medical datasets. We propose a novel automatic fea-ture selection algorithm that selects robust features based on our proposed heuristic: conservative mean . Combined with Support Vector Machines (SVMs), our proposed fea-ture selection algorithm achieves a greater area under the ROC curve (AUC) as compared to the Cox proportional haz-ards model and L 1 regularized Cox model. Furthermore, we present a margin-based censored regression algorithm that combines the concept of margin-based classifiers with cen-sored regression to achieve a better concordance index than the Cox model. Overall, our approach outperforms the cur-rent state-of-the-art in both metrics of AUC and concor-dance index. In addition, our work has also identified poten-tial risk factors that have not been discovered by traditional approaches. Our method can be applied to clinical predic-tion of other diseases, where missing data are common and risk factors are not well understood.
 J.3 [ Computer Application ]: Life and medical sciences; I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern recognition ]: Design methodology Experimentation, Algorithms, Performance This work was done while the author was at Robert Bosch LLC, Research and Technology Center Corresponding author.

Stroke is the third leading cause of death and the principal cause of serious long-term disability in the United States [2]. Stroke risk prediction can contribute significantly to its pre-vention and early treatment. Numerous medical studies and data analyses have been conducted to identify effective pre-dictors of stroke. The Framingham Study [6, 34] reported a list of stroke risk factors including age, systolic blood pres-sure, the use of anti-hypertensive therapy, diabetes mellitus, cigarette smoking, prior cardiovascular disease, atrial fib-rillation, and left ventricular hypertrophy by electrocardio-gram. Furthermore, in the past decade, a number of other studies [25, 23, 24, 26] have led to the discovery of more risk factors such as creatinine level, time to walk 15 feet, and others.

Most previous prediction models have adopted features (risk factors) that are verified by clinical trials or selected manually by medical experts. For example, Lumley et al. [24] built a 5-year stroke prediction model based on the Cardio-vascular Health Study [8] dataset using a set of 16 manually selected features (given in [25]) from a total of roughly one thousand features. With a large number of features in cur-rent medical datasets, it is a cumbersome task to identify and verify each risk factor manually. On the other hand, machine learning algorithms are capable of identifying fea-tures highly related to stroke occurrence efficiently from the huge set of features; therefore, we believe machine learning can be used to: (i) improve the prediction accuracy of stroke risk and (ii) discover new risk factors.

Lumley et al. X  X  [24] 5-year stroke prediction model adopted the Cox proportional hazards model, one of the most com-monly used statistical methods in medical research [3]. It has been extensively studied [1, 3] and applied to the predic-tion of various diseases including stroke [16, 24, 21]. How-ever, the performance of the original Cox model depends heavily on the quality of the pre-selected features. To ad-dress this problem, several approaches have been proposed recently [9, 28].

Thus far, there have been very few studies on compar-ing the Cox regression with machine learning methods in making predictions on censored data. Kattan [18] compared Cox proportional hazards regression with several machine learning methods (neural networks and tree-based meth-ods) based on three urological datasets. However, Kattan X  X  study focused on datasets with only five features, while ma-chine learning algorithms are expected to effectively han-dle many more features. In addition, the paper considered only some relatively simple machine learning algorithms and high-performance machine learning algorithms such as SVM and logistic regression were not explored.

This paper presents an integrated machine learning ap-proach for stroke risk prediction. We investigated machine learning algorithms to improve the prediction accuracy and conducted extensive comparisons between our results and those with the Cox proportional hazards model. Using the CHS dataset as a benchmark, we first duplicated the re-sults of Lumley et al. [24] as a baseline. We then compared our machine learning approach with the baseline results and an extended version of the Cox model with feature selec-tion. According to our experiments, our approach consis-tently outperformed the Cox model.

Our approach considers the problems of data imputation, feature selection, and prediction in medical datasets. We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic: con-servative mean . We combine this feature selection algorithm with the popular SVM algorithm.

Furthermore, we present a margin-based censored regres-sion algorithm that combines the concept of margin-based classifiers with censored regression to achieve a better con-cordance index than the Cox model. In addition, our work has also identified potential risk factors that have not been discovered by traditional approaches. Last, we note that this method can be applied to clinical prediction of other diseases, where missing data are common and risk factors are not well understood.

In summary, our main contributions are: 1. An extensive evaluation of the problems of data im-2. A novel feature selection algorithm, Conservative Mean 3. A novel risk prediction algorithm, Margin-based Cen-4. Discovery of new (previously unknown) potential risk 5. An integrated machine learning approach that signif-
This paper is organized as follows. Section 2 describes Cox proportional hazards regression and the L 1 regularized Cox models that we use as the baselines in this study. Section 3 describes the various machine learning-based methods we compare against the Cox models, and Section 4 provides the experimental results of our approach. Finally, Section 5 presents our conclusions.
Cox proportional hazards model is widely adopted in clin-ical studies and used heavily in stroke prediction. We briefly compare the Cox model to some of our other approaches.
The Cox proportional hazards model is given by where h ( t | x ) is the hazard value at time t given the feature vector x  X  R d for an individual, h 0 ( t ) is an arbitrary baseline hazard function,  X   X  R d are the parameters that we are trying to estimate for the model, and d is the number of features for each individual.

This model is known as a semi-parametric model because the baseline hazard function is treated non-parametrically. Thus, we can see that the parameters have a multiplicative effect on the hazard value which makes it different from the linear regression models [20].

The Cox model is part of the Generalized Linear Model (GLM) family. Another member of this family is the logistic regression model, where the output takes the following form:
In this study, we investigated both the Cox model and logistic regression model for stroke prediction. In addition, we broadened our approach to other non-regression models such as SVM, taking an agnostic view on what the best model is for stroke prediction. We found that while the Cox model performs reasonably well for stroke prediction, it is inferior to more general machine learning models, such as SVM or margin-based censored regression (proposed in this paper).
Finding the best estimate for  X  in equation (1) and (2) is typically computationally difficult, particularly given a large number of features. By introducing a complexity-based penalty term, we can identify irrelevant features and remove them from our model. The L 1 regularized sparse learning problem has the following general form: where g (  X  ) is a convex function,  X  is a vector of length d , and  X  &gt; 0 is a regularization parameter.

In this study, we evaluated both L 1 regularized Cox model and L 1 regularized logistic regression. We found that L regularized feature selection gives better performance over the baselines (i.e., selecting features manually) by reducing the feature set to the most relevant ones.
We present an integrated machine learning approach to stroke prediction. Our approach takes the following steps: 1. Apply a systematic method for imputing the missing 2. Select the relevant feature subset based on an auto-3. Apply learning algorithms to evaluate the prediction
For evaluating the performance of our methods, we used the following metrics: area under the ROC curve and con-cordance index. To define these precisely, we first outline the notation.
Consider a dataset { ( x (1) ,y (1) ,t (1) ) ,..., ( x ( m ) where x ( i )  X  R d is the feature vector 1 for individual i (i.e., d is the number of features), m is the number of individuals in the dataset, y ( i ) is the occurrence of stroke within a pre-defined time frame ( y = 1 if stroke occurs and 0 otherwise), and t ( i ) is the time of stroke. If the stroke does not occur within the pre-defined time frame for individual i , we set t ( i ) = t max , where t max is the duration of the time frame (e.g., 5 years). Now, we define the set of indexes of all positive and negative examples as M p = { i | y ( i ) = 1 } and M n = { i | y ( i ) = 0 } respectively. Given a prediction function f : R d  X  R , we can compute the prediction estimate for individual i as f ( x ( i ) ). 2
The area under the ROC curve (or AUC) is one of the most important metrics for evaluating the performance of classifiers in the medical diagnosis domain (where positive samples are usually small in number) as it considers both sensitivity and specificity, providing a balanced measure for classifier performance. Specifically, the AUC (associated with the function f ) is defined as follows [5, 14]: where 1 (  X  ) is an indicator function. The AUC is used to eval-uate the performance of the binary stroke classification task. Essentially, this metric gives an estimate of how accurately the model can answer the question,  X  X s individual A likely to have a stroke within the next 5 years? X .
We would also like to measure how accurately the predic-tions reflect relative risk of stroke of two randomly selected individuals. A commonly used metric in survival models for this evaluation is the concordance index [15, 29]. The con-cordance index is a generalization of the concept of AUC designed to handle (i) continuous values for prediction and (ii) censored data. Similar to the AUC, it takes values from 0.5 (completely random) to 1.0 (perfect prediction). The concordance index gives an estimate of how well the output of a prediction model matches the relative time of the event for all pairs of individuals that can be ordered. In essence, it allows us to measure the ability of the model to answer the question,  X  X s individual A or individual B more likely to have a stroke? X  Formally, the concordance index is defined as:
Concordance Index = 1 |  X  | X
We extend the feature vector with a constant 1 as the in-tercept term.
Throughout this paper, we interpret the binary classifica-tion output  X  y  X  X  0 , 1 } of the prediction function value f ( x ) as follows:  X  y = 1  X  X  X  f ( x ) &gt; 0. where 1 ( . ) is the indicator function as before, and |  X  | denotes the number of edges in the ordered graph of t . 3 We assume that a larger value of f corresponds to a higher risk of stroke.
In the following sections, we describe the details of data imputation, feature selection, and prediction models.
Clinical data often has significant omissions due to indi-viduals dropping out of the survey, errors in data collection and so on. Missing data often leads to an inaccurate predic-tive model. Data imputation can be used to remedy missing data. We filled-in missing entries using the following meth-ods:
As a post-processing step to impute discrete-valued fea-tures, we rounded the imputed values to the nearest discrete value. The imputation algorithms were evaluated using the following metrics: 1. Imputation accuracy (adopted from [7]): 2. Overall stroke prediction performance (measured by
Selecting relevant features [13] is crucial for building an ac-curate model of clinical data. For example, the CHS dataset has a large number of attributes, ranging from demographic information and clinical history to biomedical and physical measurements. However, only a small subset of attributes is highly relevant to stroke prediction. The traditional ap-proach to stroke prediction has been to use manually se-lected features based on risk factors analyzed by medical and clinical studies [4, 24, 33, 36]. Instead of manually se-lecting features, we evaluate three machine learning-based algorithms for selecting features automatically: forward fea-ture selection, L 1 regularized logistic regression, and  X  X on-servative mean X  feature selection.
Forward feature selection [12] greedily adds one feature at a time. The best subset of features was selected based on cross-validation. Note that adding more features does not necessarily improve the test performance since overfitting may occur.
An ordered graph G ( N,E ) of t = { t (1) ,...,t ( m ) } is defined on a set of m nodes, N = { n 1 ,...,n m } , and a set of edges, E , where ( n i ,n j )  X  E  X  X  X  t ( i ) &lt; t ( j ) . Figure 1: An illustration of Algorithm 2 for K = 3 . We use the sets T 1 , T 2 and T 3 shown in the shaded boxes as validation sets, and the corresponding sets T  X  1 , T  X  2 and T  X  3 for training to find cross-validation estimates to optimize the value of the threshold, t . CM ( T ,K ) refers to the ConservativeMean ( D ,K ) func-tion defined in Algorithm 1.
L 1 regularized logistic regression [30] is a popular algo-rithm for feature selection. L 1 regularization has the bene-ficial effect of regularizing model coefficients (as in L ularization), but yields sparse models that are more eas-ily interpretable [27, 32, 35]. This model has a regulariza-tion parameter that controls the  X  X parseness X  of the weights. Consequently, the features with nonzero weights are selected for prediction.
Here we present a novel and efficient feature selection algo-rithm, Conservative Mean (CM) feature selection. Consider a setting where positive examples are small in number and non-homogeneous. Then, the prediction performance may vary significantly depending on how the training and test-ing examples are sampled. We want to select features that are relevant, yet robust to variations due to sampling over a small number of non-homogeneous examples.

In order to incorporate the above intuition, we introduce the heuristic of conservative mean (  X   X   X  ), where  X  and  X  refer to the mean and standard deviation of the AUC of a feature 4 respectively. The setting is similar to K -fold cross-validation, but we also want to consider the variance across different folds along with the average of the prediction per-formance. In addition, we want to evaluate the performance of each feature individually. Therefore, subtracting the stan-dard deviation from the mean provides a more  X  X onservative X  estimate of the performance of each feature as compared to using the mean alone, which is the typical approach. 5  X  (or  X  ) refers to mean (or standard deviation) of the AUC of a given feature over K -folds of the dataset. See Algo-rithm 1 for precise definition.
The same heuristic can be used to optimize any other met-ric such as classification accuracy. Furthermore, this heuris-Table 1: Notation for a dataset D with d features and m examples, i.e., D = { ( x (1) ,y (1) ) ,..., ( x ( m ) (x Symbol Description D . x ( i ) , x ( i ) i -th example in the dataset D .y ( i ) , y ( i ) i -th label in the dataset D . x j , ( x (1) j ,...,x ( m ) j ) j -th features in the dataset D . y , ( y (1) ,...,y ( m ) ) set of all labels in the dataset
To describe the conservative mean heuristic formally, we first introduce the notation in Table 1. The key observation here is that when we consider monotonic prediction func-tions over a single feature, then we only need to compute the AUC over the feature values and the labels (without considering the prediction functions). This is because AUC is invariant under mapping from monotonic functions. For example, this eliminates the need to compute the weight vec-tor or intercept term for a linear SVM when using a single feature as input. The AUC is only affected by the sign of the weight vector which can be easily determined to be the one that ensures the AUC is greater than or equal to 0.5. Specifically, the following two Lemmas provide the formal basis for the efficient computation.

Lemma 1. Given any monotonically increasing function f : R  X  R and a dataset D = { ( x (1) ,y (1) ) ,..., ( x ( m ) is equal to AUC ( D . x j , D . y ) .

Proof. For notational convenience, we define i.e., function values for j -th features. Then, the following equalities hold (for all j  X  X ): The second step holds because a monotonically increasing function does not affect the relative ordering of D. x j , caus-ing the AUC to remain unchanged.

Lemma 2. Given a hypothesis space H of monotonic (either strictly increasing or strictly decreasing) prediction functions f : R  X  R and a training set, D tr , and a validation set, D val , we define f  X  as follows: Then, the following holds: AUC ( f  X  ( D val . x j ) , D val . y ) (11) = tic could be applied to other feature selection algorithms that use cross-validation for selecting features (e.g., forward feature selection).
 Algorithm 1 Computing the conservative mean vector function ConservativeMean( D ,K ): Input: D : dataset with d features K : number of folds
Output: v : Conservative mean vector (of length d ) begin Divide D evenly into K disjoint sets D 1 ,..., D K such that D = D 1  X  ...  X  X  K and D k  X  X  l =  X  ,  X  k,l .

Set D  X  k , D X  X  k ,  X  k . for j := 1 to d do end for where
AUC(predictions, labels) returns the area under the ROC curve given the predictions and labels.  X  ( s ) , q 1 K P K k =1 ( s k  X   X  ( s )) 2 end
Proof. By definition of f  X  , AUC ( f  X  ( D tr . x j ) , D 0 . 5. Since we consider only monotonic functions, f  X  is either monotonically increasing or monotonically decreasing. If f is monotonically increasing, we have (from the Lemma 1):
AUC ( f  X  ( D. x j ) , D . y ) = AUC ( D . x j , D . y )) ,  X D (12)  X  AUC ( f  X  ( D val . x j ) , D val . y ) = AUC ( D val . x Similarly, if f  X  is monotonically decreasing, we have AUC ( f  X  ( D. x j ) , D . y ) = AUC (  X  X  . x j , D . y )) ,  X D (13)  X  0 . 5  X  AUC ( f  X  ( D tr . x j ) , D tr . y ) = AUC (  X  X   X  AUC ( D tr . x j , D tr . y ))  X  0 . 5 ,and
AUC ( f  X  ( D val . x j ) , D val . y ) = AUC (  X  X  val . x
Therefore, we can efficiently compute a robust estimate of prediction performance for each feature (summarized as conservative mean vector v ) as described in Algorithm 1.
Based on the conservative mean heuristic for ranking the features, we can now describe an algorithm to select the appropriate number of features. The overall procedure is described in Algorithm 2 (see Figure 1 for illustration). The training data is initially split into K folds, and we com-pute the conservative mean vector by holding out the k -th fold each time, resulting in a total of K vectors. We then compute the vector  X  v by taking the average of the conser-vative mean vectors to select a robust set of features that generalize well across all folds. Finally, given a threshold value t  X  [0 , 1], we select all features I , { j |  X  v j
Note that selecting a threshold is equivalent to selecting a number of features ranked according to their value of the  X  v vector.
 Algorithm 2 Conservative mean feature selection Input: T : dataset with d features
K : number of folds t : threshold  X  [0 , 1] Output:
I : Set of selected feature indexes  X  X  1 ,...,d } . begin Divide T evenly into K disjoint sets T 1 ,..., T K such that T = T 1  X  ...  X  X  K and T k  X  X  l =  X  ,  X  k,l .

Set T  X  k , T  X  X  k ,  X  k .  X  v := ~ 0  X  R d for k := 1 to K do end for
I := { j |  X  v j  X  t } end threshold value can be determined using cross-validation, as described in the caption of Figure 1.
In this section, we describe the learning algorithms that we used for stroke prediction: Support Vector Machines and Margin-based Censored Regression (MCR).
SVM is a popular machine learning algorithm that is widely used for classification. Conceptually, SVM optimizes the  X  X argin X  between positive and negative examples. We can formulate the stroke prediction problem as predicting the occurrence of stroke over a pre-defined time frame, which makes it a binary classification problem that fits into the framework of SVM. Furthermore, SVM solvers can be used to optimize the area under the ROC curve directly, so they are well suited for the task of stroke prediction. We used lin-ear SVMs implemented using SVM-perf [17] in this study.
Since the SVM is in principle developed for classification, we use it to predict whether or not a stroke would occur within a given time frame while ignoring the information about when the stroke occurred. However, the time of stroke is indicative of the relative risk level of an individual. Incor-porating this information would enable us to answer ques-tions such as  X  X s individual A or individual B more likely to have a stroke? X  and  X  X hen is a stroke likely to occur?, X  whereas SVM predictions are generally unable to address these concerns.
 To address these concerns, we propose the Margin-based Censored Regression algorithm that unifies linear regres-sion with an SVM-like classifier on censored data. More specifically, we propose a convex optimization problem as described below.

Consider a dataset { ( x (1) ,y (1) ,t (1) ) ,..., ( x ( m ) in Section 3.1. The time of stroke is then normalized as  X  t ( i ) = t ( i ) /t max . We then use a monotonically decreasing function to transform the normalized time of stroke to a  X  X azard value, X  as in the Cox model. We use the function z (  X  t formation, we have z (  X  t ( i ) ) = 0 for i  X  { i | y z (  X  t ( i ) ) &gt; 0 for i  X  X  i | y ( i ) = 1 } . 7
Given the above transformation, our goal is to find a weight vector w such that w T x ( i ) is  X  X lose to X  z ( addition, we want to be able to distinguish between positive and negative examples; in other words, we want to find w such that the individuals who experienced a stroke are well separated from the individuals who did not. This is achieved by imposing w T x ( i )  X   X  for the individuals who did not have a stroke, where is the desired margin between positive and negative examples (set to 1 in our experiments). Finally, similar to SVM, we introduce a penalty term P i  X  ( i ) to al-low for non-separable datasets, and to reduce the sensitivity to outliers.

To sum up, we formulate the problem as minimize subj. to w T x ( i )  X  X  X  +  X  ( i ) ,  X  i  X  X  i | y ( i ) = 0 } , (14) where C and  X  are the hyperparameters for the misclassifi-cation loss penalty and for regularization respectively, and  X  : R  X  R is the regression loss penalty, which we fixed as the Huber function [11] 8 in our study. To solve prob-lem (14), we used CVX , a package for specifying and solving convex programs [11, 10].

Note that we can easily apply the kernel trick to this model, as in SVM. Furthermore, the objective function can be modified to optimize the AUC directly in the same way as SVM-perf [17].
The Cardiovascular Heart Study [8] is a study of risk fac-tors for cardiovascular diseases in people over the age of 65. According to the Centers for Disease Control and Preven-tion, nearly three-quarters of all strokes occur in people over the age of 65. 9 This makes CHS an invaluable resource for the investigation of risk factors and the prediction of stroke. In the original cohort recruited in the first phase of the CHS, 5,201 individuals were examined yearly from 1989 to 1999, with a total of about one thousand attributes collected an-nually through medical examinations, questionnaires, and phone contacts. Events such as stroke and hospitalization were verified by specialists and recorded for each individual.
The comprehensiveness of the CHS dataset makes it one of the most widely used benchmarks for studying risk fac-tors for cardiovascular diseases, including stroke. However, it is also very challenging to use the CHS dataset effectively due to a significant fraction of missing values and a large number of features in the dataset. For example, about 25% of the baseline measurements in the CHS dataset are miss-ing, and some entries are recorded as  X  X nknown X  or  X  X efuse to answer. X 
For the CHS dataset, z (  X  t ( i ) ) roughly ranges from 0 to 5.
The Huber function  X  ( x ) is defined as 2 | x | X  1 for | x | X  1, and | x | 2 for | x | X  1. http://www.cdc.gov/Stroke/facts.htm
In our experiments, we considered the 5-year stroke pre-diction 10 problem on the original cohort. To begin with, we removed the individuals with pre-baseline stroke (as done in [24]) and the features with more than 60% missing en-tries. 11 This criterion was chosen because features with too many missing entries often turn out to be irrelevant. Af-ter preprocessing, the final dataset consisted of 796 features and 4 , 988 examples with 299 occurrences of stroke. Then the data was divided randomly with a ratio of 9 : 1 for training and testing respectively, while keeping the ratio of the positive and negative examples constant. This process was repeated to obtain a fixed set of 5 randomly sampled train and test sets. 12 In the remaining sections,  X  X verage test AUC X  refers to the average of AUC obtained by eval-uating the prediction algorithm on the test set over these 5 random trials. We also define  X  X verage test concordance index X  in a similar way.
The data imputation quality was evaluated using 10-fold cross-validation. For each feature j , we first removed all the examples that contained missing values for the particu-lar feature. Then, we divided the examples in the resulting data into training and validation sets with the ratio of 9:1 respectively. Treating feature j of the validation set as being unobserved, we used the training data to impute the values of the particular feature of the validation data. 13 The im-puted values were then compared with the actual values in the validation dataset to obtain the performance metrics de-scribed in Section 3.2. This process was repeated for every feature and the results were averaged. The summary results are shown in Table 2. 14 For the computation of the area under the ROC curve, we used conservative mean feature selection and SVM for stroke prediction. 15
Among the imputation methods, linear regression gave the smallest RMSD and MAD values, which suggested that it achieved the highest imputation accuracy. However, the overall stroke prediction quality with column median was the best with an area of 0.774 under the ROC curve. In the following sections, we report the results using column median as the default imputation method.
As this method is computationally very expensive, the number of features was initially reduced from 796 to about 200 using L 1 regularized logistic regression. Then we ran for-
Only the cases of stroke that occurred within 5 years after the baseline measurements were considered positive exam-ples.
Some features represented  X  X efuse to answer X  values as 9 or 99. We replaced these entries with  X  X issing X  before data imputation.
These random trials were used for all the remaining exper-iments to ensure all the results are directly comparable.
For the linear regression imputation, any missing values in the other features were filled in using the column mean.
Imputation methods without rounding have been left out as the results were very similar.
Imputation with regularized EM was computationally ex-pensive, so we used L 1 logistic regression on data imputed with column median to reduce the number of features to about 200 before applying EM.
 ward feature selection on this reduced set to obtain the final set of features. Using SVM for prediction, we selected the optimal number of features through 10-fold cross-validation. The final prediction performance was an average test AUC of 0.751, which is slightly worse than that of using SVM and the 16 features used in [24]. In our experiments, this method selected a much larger number of features than other feature selection algorithms, which indicates that it may be susceptible to overfitting.
The L 1 regularized logistic regression (L1LR) was used for feature selection, followed by SVM for prediction. The implementation of L1LR was done using the SLEP pack-age [22]. The optimal regularization parameter  X   X  was as-signed to be the value that maximized the area under the ROC curve for 10-fold cross-validation. The value of  X   X  then used to run L1LR on the entire training set to select the final set of features for testing. The average test AUC was 0.764, which is better than that of the L 1 regularized Cox feature selection algorithm.
Conservative mean selection was run using 10-fold cross-validation for both the generation of the conservative mean vectors as described in Algorithm 1, and to obtain the subset of optimal features as described in Algorithm 2. As observed for the forward search feature selection, using the maximum cross-validation AUC may result in overfitting, and thus we propose a simple method to reduce this effect.

Throughout our experiments, we observed that overfit-ting may occur when the performance on the training set increases with increasing number of features while the cross-validation performance decreases or remains fairly constant. An example of this can be seen in Figure 2. Hence, we can say that the  X  X xtent of overfitting X  increases as the gap be-tween the cross-validation AUC (CV AUC) and train AUC increases. Therefore, we estimated this extent of overfitting by subtracting the cross-validation AUC from the training AUC. Then, we computed a more conservative estimate of the CV AUC as  X  X V AUC X   X  ( X  X rain AUC X   X   X  X V AUC X ). 16 In Figure 2, we can see that the CV AUC remains fairly con-stant after about 30 features. If the CV AUC was maximized without accounting for the overfitting, we would select 120 features instead of 30.

Furthermore, we compare the effect of using conservative mean ( X  v j =  X  ( s )  X   X  ( s ) X ) against using mean ( X  v in Algorithm 1 while keeping the remaining algorithms un-changed. The average test AUC decreases from 0.774 (con-servative mean) to 0.759 (mean) when using SVM for pre-diction. The difference in performance clearly shows that conservative mean selects more robust features as compared
This method improved average test AUC by about 0.5% for all the algorithms. Figure 2: Plot showing the cross-validation AUC and train AUC as features are added. We used CM feature selection with SVM for prediction on a ran-dom trial.
 Table 3: Average test AUC combining various fea-ture selection algorithms with our prediction algo-rithms to mean alone. Furthermore, we note that the best per-formance for both SVM and MCR are obtained using the conservative mean feature selection algorithm.

Also note that the conservative mean selection algorithm is significantly more computationally efficient than the for-ward feature selection algorithm. On the same machine, forward feature selection took about 60 hours to select fea-tures from a set of about 200 features, while conservative mean took less than 10 minutes to select from a set of 796 features.
First, we evaluated the performance of our prediction al-gorithms based on the area under the ROC curve. The prediction performance when using the feature selection al-gorithms (described in the previous sections) is compared against the set of 16 manually selected features used by Lum-ley et al., as shown in Table 3. When using SVM or MCR 17 for prediction, we found that all the feature selection al-gorithms except forward feature selection performed better than using manually selected features. Overall, CM feature selection performed the best for both prediction methods. We also found that MCR performed better than SVM for all the feature selection methods.

In Table 4, we compare the performance of our algorithms against the current state-of-the-art Cox proportional haz-
In our experiments, we added a small L 1 regularization penalty to the MCR objective function.
 Table 4: Average test AUC using different algo-rithms with comparison to the Cox models Algorithm Avg. Test AUC MCR + CM feature selection 0.777 SVM + CM feature selection 0.774 SVM + 16 features (used in [24]) 0.753 SVM + forward selection 0.751 Cox + L 1 feature selection 0.747 Cox + 16 features (used in [24]) 0.734 Table 5: Average test concordance index of Cox model and MCR using different sets of features Method C oncordance Index MCR + CM feature selection 0.770 MCR + 16 features (used in [24]) 0.757 SVM + CM feature selection 0.760 SVM + 16 features (used in [24]) 0.747 Cox + CM feature selection (for MCR) 0.737
Cox + 16 features (used in [24]) 0.730 ards model and L 1 regularized Cox model. All our methods outperformed these baseline models. The best method was combining CM feature selection with MCR for prediction, which achieved a 16% error reduction in the average test AUC as compared to the Cox model (as used in [24]).
Second, we evaluated the concordance index to compare the ability of MCR, SVM and Cox model to predict the relative risk of stroke. From Table 5, we observe that the MCR algorithm outperforms the other models when using the same set of features. Also, the features selected using CM significantly increased the concordance index for all the models. It is interesting to note that the SVM performs bet-ter than the Cox model even though it does not use infor-mation about the relative risk of stroke. The combination of CM feature selection and MCR for prediction gave the best performance with a concordance index of 0.770.
In addition to achieving better results, our method can automatically identify potential risk factors without carry-ing out extensive medical studies to understand each one in detail. This would allow for a quick method of characteriz-ing a new disease and identifying its predictors before other studies confirm them. Furthermore, this procedure could also be used to suggest risk factors that might have been previously unexplored.

In our experiments, we found the top features by ranking the average of the conservative mean vectors over multiple random trials in descending order. Table 6 shows a repre-sentative set of features found among the top 40 features. Note that there is a large overlap between the top features selected by our feature selection algorithm and those iden-tified by medical studies. This verifies that our algorithm is reliable in identifying risk factors. Thus, the features that are highly ranked but have not been clinically tested might be probable risk factors.

For example,  X  X ny ECG abnormality X  is a highly ranked factor, whereas  X  X trial fibrilliation by ECG X  is a commonly accepted risk factor of stroke. It may be possible that all ECG abnormalities are more indicative of stroke than just atrial fibrilliation. Also,  X  X inimental score X  could be an im-portant risk factor because it gives an indication of the cere-Table 6: A representative set of features obtained from the top 40 features selected by CM feature selection Feature description Average (  X   X   X  ) Age  X  0.6064 Number of symbols correctly coded* 0.5828 Maximal inflation level* 0.5820 Systolic blood pressure  X  0.5738 Calculated 100 point score* 0.5681 Total medications* 0.5634 Isolated systolic hypertension  X  0.5588 General health* 0.5519 Calculated hypertension status  X  0.5500 Time (in sec) to walk 15 feet  X  0.5485 Any ECG abnormality* 0.5461 Right/left % Stenosis  X  0.5444 Cardiac Injury Score  X  0.5426 Min. ankle arm ratio* 0.5374 Diabetic status defined by ADA  X  0.5342 Minimental score 35 point* 0.5337 Left ventricular mass  X  0.5337 Creatinine  X  0.5273
FVC percent predicted* 0.5239 * Potential risk factors that are not found in previous work (to the best of our knowledge)  X  Clinically established risk factors of stroke brovascular activity of an individual, which could be corre-lated to stroke risk. In addition, our results suggest a few other potential risk factors for stroke, such as total num-ber of medications and FVC percent predicted. Further in-vestigation of these features could lead to improved stroke prediction.
As we have shown in this study, the conservative mean fea-ture selection performs very well for the CHS dataset. How-ever, we realize that this feature selection algorithm may not work well in other datasets with highly correlated features as it evaluates the performance of each feature individually. To address this problem, we could use an L 1 regularized feature selection algorithm (e.g., L 1 regularized logistic re-gression) to prune the features before applying conservative mean feature selection for fine-tuning.

In this paper, we have presented an integrated machine learning approach combining the elements of data imputa-tion, feature selection and prediction. We provide an ex-tensive comparison of machine learning methods with the Cox proportional hazards model and show that the machine learning methods significantly outperform the Cox model in terms of both binary stroke prediction and stroke risk es-timation. Specifically, we propose the conservative mean heuristic for feature selection, which gives us the best per-formance as compared to other methods. In addition, we present a novel prediction algorithm, Margin-based Cen-sored Regression, that achieves a better concordance index than the Cox model. Further, our method can be used for identifying potential risk factors for diseases without per-forming clinical trials. We hope that this paper will motivate the application of machine learning methods in healthcare data analysis.
 We thank Andrew Ng and Chaitanya Rastogi for helpful advice and the National Heart, Lung, and Blood Institute (NHLBI) for providing the CHS dataset. Support for this work from the Research and Technology Center of Robert Bosch LLC is gratefully acknowledged.
