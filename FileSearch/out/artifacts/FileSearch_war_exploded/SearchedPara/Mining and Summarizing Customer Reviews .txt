 Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summa rization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selec ting a subset or rewrite some of the original sentences from th e reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using review s of a number of products sold online demonstrate the effectiveness of the techniques. H.2.8 [ Database Management ]: Database Applications  X  data mining . I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  text analysis . Algorithms, Experimenta tion, Human Factors. Text mining, sentiment classifi cation, summarization, reviews. With the rapid expansion of e-co mmerce, more and more products are sold on the Web, and more and more people are also buying products online. In order to enhance customer satisfaction and shopping experience, it has become a common practice for online merchants to enable their customers to review or to express opinions on the products that they have purchased. With more and more common users becoming comfortable with the Web, an increasing number of people are wr iting reviews. As a result, the number of reviews that a product receives grows rapidly. Some popular products can get hundreds of reviews at some large merchant sites. Furthermore, many reviews are long and have only a few sentences containing opinions on the product. This makes it hard for a potential customer to read them to make an informed decision on whether to purchase the product. If he/she only reads a few reviews, he/she may get a biased view. The large number of reviews also makes it hard for product manufacturers to keep track of customer opinions of their products. For a product manufacturer, there are additional difficulties because many merchant sites may sell its pr oducts, and the manufacturer may (almost always) produce many kinds of products.
 In this research, we study the problem of generating feature-based summaries of customer reviews of products sold online. Here, features broadly mean product features (or attributes) and functions. Given a set of customer reviews of a particular product, the task involves three subtasks: (1) identifying features of the product that customers have expr essed their opinions on (called product features ); (2) for each feature, identifying review sentences that give positive or negative opinions; and (3) producing a summary using th e discovered information. Let us use an example to illustrate a feature-based summary. Assume that we summarize the reviews of a particular digital camera, digital_camera_ 1. The summary looks like the following: Digital_camera_ 1: Feature: picture quality Positive: 253 Negative: 6 Feature: size Negative: 10 ... In Figure 1, picture quality and (camera) size are the product features. There are 253 customer reviews that express positive opinions about the picture quality, and only 6 that express negative opinions. The &lt;individual review sentences&gt; link points to the specific sentences and/or the whole reviews that give positive or negative comments about the feature. With such a feature-based summary, a potential customer can easily see how the existing cust omers feel about the digital camera. If he/she is very interested in a particular feature, he/she can drill down by following the &lt;individual review sentences&gt; link to see why existing customers like it and/or what they complain about. For a manufacturer, it is possible to combine summaries from multiple merchant sites to produce a single report for each of its products. Our task is different from traditi onal text summarization [15, 39, 36] in a number of ways. First of all, a summary in our case is structured rather than another (but shorter) free text document as produced by most text summariza tion systems. Second, we are only interested in features of the product that customers have opinions on and also whether the opinions are positive or negative. We do not summarize th e reviews by selecting or rewriting a subset of the original sentences from the reviews to capture their main points as in traditional text summarization. As indicated above, our task is pe rformed in three main steps: (1) Mining product features that have been commented on by (2) Identifying opinion sentences in each review and deciding (3) Summarizing the results. This step aggregates the results of Section 3 presents the detailed techniques for performing these tasks. A system, called FBS ( F eature -B ased S ummarization), has also been implemented. Our experimental results with a large number of customer reviews of 5 products sold online show that FBS and its techniques are highly effectiveness. Our work is closely related to Dave, Lawrence and Pennock X  X  work in [9] on semantic classifica tion of reviews. Using available training corpus from some Web sites, where each review already has a class (e.g., thumbs-up and thumbs-downs, or some other quantitative or binary ratings), th ey designed and experimented a number of methods for building sen timent classifiers. They show that such classifiers perform quite well with test reviews. They also used their classifiers to classify sentences obtained from Web search results, which are obtaine d by a search engine using a product name as the search query. However, the performance was limited because a sentence contains much less information than a review. Our work differs from theirs in three main aspects: (1) Our focus is not on classifying each review as a whole but on classifying each sentence in a review. Within a review some sentences may express positive opinions about certain product features while some other se ntences may express negative opinions about some other product features. (2) The work in [9] does not mine product features from reviews on which the reviewers have expressed their opinions. (3) Our method does not need a corpus to perform the task. In [30], Morinaga et al. compare reviews of different products in one category to find the reputation of the target product. However, it does not summarize re views, and it does not mine product features on which the re viewers have expressed their opinions. Although they do find some frequent phrases indicating reputations, these phrases may not be product features (e.g.,  X  X oesn X  X  work X ,  X  X enchmark result X  and  X  X o problem(s) X ). In [5], Cardie et al discuss opinion-oriented information extraction. They aim to create summary representations of opinions to perform question answering. They propose to use opinion-oriented  X  X cenario templates X  to act as summary representations of the opinions expressed in a document, or a set of documents. Our task is different. We aim to identify product features and user opinions on these features to automatically produce a summary. Also, no template is used in our summary generation. Our work is also related to but different from subjective genre classification, sentiment classification, text summarization and terminology finding. We discuss each of them below. Genre classification classifies texts into different styles, e.g.,  X  X ditorial X ,  X  X ovel X ,  X  X ews X  ,  X  X oem X  etc. Although some techniques for genre classificati on can recognize documents that express opinions [23, 24, 14], they do not tell whether the opinions are positive or negative. In our work, we need to determine whether an opinion is positive or negative and to perform opinion classification at the sentence level rather than at the document level. A more closely related work is [17], in which the authors investigate sentence subjectivity cl assification and concludes that the presence and type of adjectives in a sentence is indicative of whether the sentence is subjective or objective. However, their work does not address our speci fic task of determining the semantic orientations of thos e subjective sentences. Neither do they find features on which opinions have been expressed. Works of Hearst [18] and Sack [35] on sentiment-based classification of entire documen ts use models inspired by cognitive linguistics. Das and Chen [8] use a manually crafted lexicon in conjunction with severa l scoring methods to classify stock postings on an investor bulle tin. Huettner and Subasic [20] also manually construct a disc riminant-word lexicon and use fuzzy logic to classify sentimen ts. Tong [41] generates sentiment timelines. It tracks online discussi ons about movies and displays a plot of the number of positive and negative sentiment messages over time. Messages are classified by looking for specific phrases that indicate the author X  X  sentiment towards the movie (e.g.,  X  X reat acting X ,  X  X onderful visual s X ,  X  X neven editing X ). Each phrase must be manually added to a special lexicon and manually tagged as indicating positive or negative sentiment. The lexicon is domain dependent (e.g., movies) and must be rebuilt for each new domain. In contrast, in our work, we only manually create a small list of seed adjectives tagged w ith positive or negative labels. Our seed adjective list is also domain independent. An effective technique is proposed to grow this list using WordNet. Turney X  X  work in [42] applies a specific unsupervised learning technique based on the mutual information between document phrases and the words  X  X xcellent X  and  X  X oor X , where the mutual information is computed using statistics gathered by a search engine. Pang et al. [33] examine several supervised machine learning methods for sentiment cl assification of movie reviews and conclude that machine l earning techniques outperform the method that is based on human-t agged features although none of existing methods could handle the sentiment classification with a reasonable accuracy. Our work differs from these works on sentiment classification in that we perform classification at the sentence level while they determine the sentiment of each document. They also do not find features on which opinions have been expressed, which is very important in practice. Existing text summarization techniques mainly fall in one of the two categories: template instan tiation and passage extraction. Work in the former framework includes [10, 39]. They emphasize on identification and extraction of certain core entities and facts in a document, which are packaged in a template. This framework requires background knowledge in orde r to instantiate a template to a suitable level of detail. Therefore, it is not domain or genre independent [37, 38]. This is different from our work as our techniques do not fill any template and are domain independent. The passage extraction framework [e.g., 32, 25, 36] identifies certain segments of the text (typically sentences) that are the most representative of the document X  X  cont ent. Our work is different in that we do not extract representative sentences, but identify and extract those specific product features and the opinions related to them. Boguraev and Kennedy [2] propose to find a few very prominent expressions, objects or events in a document and use them to help summarize the document. Our work is again different as we find all product features in a set of customer reviews regardless whether they are prominent or not. Thus, our summary is not a traditional text summary. Most existing works on text summarization focus on a single document. Some researchers also studied summarization of multiple documents covering similar information. Their main purpose is to summarize the similarities and differences in the information content among these documents [27]. Our work is related but quite different because we aim to find the key features that are talked about in multiple reviews. We do not summarize similarities and differences of reviews. In terminology finding, there are basically two techniques for discovering terms in corpora: symbolic approaches that rely on syntactic description of te rms, namely noun phrases, and statistical approaches that exploit the fact that the words composing a term tend to be found close to each other and reoccurring [21, 22, 7, 6]. Howe ver, using noun phrases tends to produce too many non-terms (low precision), while using reoccurring phrases misses many lo w frequency terms, terms with variations, and terms with only one word. Our association mining based technique does not have th ese problems, and we can also find infrequent features by exploiting the fact that we are only interested in features that the users have expressed opinions on. Figure 2 gives the architectural overview of our opinion summarization system. The inputs to the system are a product name and an entry Web page for all the reviews of the product. The output is the summary of the reviews as the one show n in the introduction section. The system performs the summarization in three main steps (as discussed before): (1) mining pr oduct features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. These steps are performed in multiple sub-steps. Given the inputs, the system firs t downloads (or crawls) all the reviews, and put them in the revi ew database. It then finds those  X  X ot X  (or frequent) features that many people have expressed their opinions on. After that, the opinion words are extracted using the resulting frequent features, and se mantic orientations of the opinion words are identified with the help of WordNet. Using the extracted opinion words, the system then finds those infrequent features. In the last two steps, the orientation of each opinion sentence is identified and a final summary is produced. Note that POS tagging is the part-of-speech tagging [28] from natural language processing, which help s us to find opinion features. Below, we discuss each of the sub-steps in turn. Product features are usually nouns or noun phrases in review sentences. Thus the part-of-speech tagging is crucial. We used the NLProcessor linguistic parser [31] to parse each review to split text into sentences and to produce the part-of-speech tag for each word (whether the word is a noun, verb, adjective, etc). The process also identifies simple noun and verb groups (syntactic chunking). The following shows a sentence with POS tags. &lt;VG&gt; &lt;W C='VBP'&gt; am &lt;/W&gt;&lt;W C='RB'&gt; absolutely &lt;/W&gt; &lt;W C='NN'&gt; camera &lt;/W&gt;&lt;/NG&gt;&lt;W C='.'&gt; . &lt;/W&gt;&lt;/S&gt; NLProcessor generates XML output . For instance, &lt;W C= X  X N X &gt; indicates a noun and &lt;NG&gt; indicates a noun group/noun phrase. Each sentence is saved in the re view database along with the POS tag information of each word in the sentence. A transaction file is then created for the generation of frequent features in the next step. In this file, each line contains words from one sentence, which includes only the identified nouns and noun phrases of the sentence. Other components of the sentence are unlikely to be product features. Some pre-processing of words is also performed, which includes removal of st opwords, stemming and fuzzy matching. Fuzzy matching is used to deal with word variants and misspellings [19]. This sub-step identifies product features on which many people have expressed their opinions. Be fore discussing frequent feature identification, we first give some example sentences from some reviews to describe what kinds of opinions that we will be handling. Since our system aims to find what people like and dislike about a given product, how to find the product features that people talk about is the crucial step. However, due to the difficulty of natural language understanding, some types of sentences are hard to deal with. Let us see an easy and a hard sentence from the reviews of a digital camera:  X  X he pictures are very clear. X  In this sentence, the user is satis fied with the picture quality of the camera, picture is the feature that the user talks about. While the feature of this sentence is exp licitly mentioned in the sentence, some features are implicit and hard to find. For example,  X  X hile light, it will not easily fit in pockets. X  This customer is talking about the size of the camera, but the word size does not appear in the sentence. In this work, we focus on finding features that appear exp licitly as nouns or noun phrases in the reviews. We leave finding implic it features to our future work. Here, we focus on finding frequent features, i.e., those features that are talked about by many customers (finding infrequent features will be discussed late r). For this purpose, we use association mining [1] to find all fre quent itemsets. In our context, an itemset is simply a set of words or a phrase that occurs together in some sentences. The main reason for using association mining is because of the following observation. It is co mmon that a customer review contains many things that are not directly related to product features. Different customers us ually have different stories. However, when they comment on product features, the words that they use converge. Thus using a ssociation mining to find frequent itemsets is appropriate because those frequent itemsets are likely to be product features. Those noun/noun phrases that are infrequent are likely to be non-product features. We run the association miner CBA [26], which is based on the Apriori algorithm in [1] on the transaction set of noun/noun phrases produced in the previous step. Each resulting frequent itemset is a possible feature. In our work, we define an itemset as frequent if it appears in more th an 1% (minimum support) of the review sentences. The generated frequent itemsets are also called candidate frequent features in this paper. However, not all candidate fre quent features generated by association mining are genuine features. Two types of pruning are used to remove thos e unlikely features. Compactness pruning : This method checks features that contain at least two words, which we call feature phrases , and remove those that are likely to be meaningless. The association mining algorithm does not consider the position of an item (or word) in a sentence. However, in a sentence, words that appear together in a specific order are more likely to be meaningful phrases. Therefore, some of the frequent feature phrases generated by associa tion mining may not be genuine features. Compactness pruning aims to prune those candidate features whose words do not appear together in a specific order. See [19] for the detailed definition of compactness and also the pruning procedure. Redundancy pruning: In this step, we focus on removing redundant features that contain single words. To describe the meaning of redundant features , we use the concept of p-support ( pure support ). p-support of feature ftr is the number of sentences that ftr appears in as a noun or noun phrase, and these sentences must contain no feature phrase that is a superset of ftr . We use a minimum p-support value to prune those redundant features. If a feature has a p-support lower than the minimum p-support (in our system, we set it to 3) and the feature is a subset of another feature phrase (which suggests that the feature alone may not be interesting), it is pruned. For instance, life by itself is not a useful feature while battery life is a meaningful feature phrase. See [19] for more explanations. We now identify opinion words. These are words that are primarily used to express subjec tive opinions. Clearly, this is related to existing work on distinguishing sentences used to express subjective opinions from sentences used to objectively describe some factual information [43]. Previous work on subjectivity [44, 4] has esta blished a positive statistically significant correlation with the presence of adjectives. Thus the presence of adjectives is useful for predicting whether a sentence is subjective, i.e., expressing an opinion. This paper uses adjectives as opinion words. We also limit the opinion words extraction to those sentences that contain one or more product features, as we are only interested in customers X  opinions on these product features. Let us first define an opinion sentence. Definition: opinion sentence
If a sentence contains one or more product features and one or more opinion words, then the sentence is called an opinion sentence.
 We extract opinion words in the following manner (Figure 3): For example, horrible is the effective opinion of strap in  X  The strap is horrible and gets in the way of parts of the camera you need access to . X  Effective opinions will be useful when we predict the orientation of opinion sentences. For each opinion word, we need to identify its semantic orientation, which will be used to predict the semantic orientation of each opinion sentence. The semantic orientation of a word indicates the direction that the word deviates from the norm for its semantic group. Words that en code a desirable state (e.g., beautiful, awesome) have a positive orientation, while words that represent undesirable states have a negative orientation (e.g., disappointing). While orientations apply to many adjectives, there are also those adjectives that have no orientation (e.g., external, digital) [17]. In this work, we are interested in only positive and negative orientations. Unfortunately, dictionaries and similar sources, i.e., WordNet [29] do not include semantic orientation information for each word. Hatzivassiloglou and McKeown [16] use a supervised learning algorithm to infer the semantic orientation of adjectives from constraints on conjunctions . Although their method achieves high precision, it relies on a large corpus, and needs a large amount of manually tagged training data. In Turney X  X  work [42], the semantic orientation of a phrase is calculated as the mutual information between the given phr ase and the word  X  X xcellent X  minus the mutual information be tween the given phrase and the word  X  X oor X . The mutual inform ation is estimated by issuing queries to a search engine a nd noting the number of hits. The paper [42], however, does not report the results of semantic orientations of individual words/phrases. Instead it only gives the classification results of reviews. We do not use these techniques in this paper as both works rely on statistical information from a rather big corpus. Their methods ar e also inefficient. For example, in [42], for each word or phrase, a Web search and a substantial processing of the returned results are needed. In this research, we propose a simple and yet effective method by utilizing the adjective synonym set and antonym set in WordNet [29] to predict the semantic orientations of adjectives. In WordNet, adjectives are organized into bipolar clusters, as illustrated in Figure 4. The cluster for fast/slow , consists of two Each half cluster is headed by a head synset , in this case fast and its antonym slow . Following the head synset is the satellite synsets , which represent senses that are similar to the sense of the head adjective. The other half cluster is headed by the reverse antonymous pair slow/fast , followed by satellite synsets for senses of slow [12].
 In general, adjectives share the same orientation as their synonyms and opposite orientations as their antonyms. We use this idea to predict the orientation of an adjective. To do this, the synset of the given adjective and the antonym set are searched. If a synonym/antonym has known orient ation, then the orientation of the given adjective could be set correspondingly. As the synset of an adjective always contains a sense that links to head synset, the search range is rather larg e. Given enough seed adjectives with known orientations, we can almo st predict the orientations of all the adjective words in the review collection. Thus, our strategy is to use a se t of seed adjectives, which we know their orientations and then gr ow this set by searching in the WordNet. To have a reasonably broad range of adjectives, we first manually come up a set of very common adjectives (in our experiment, we used 30) as the s eed list, e.g. positive adjectives: great , fantastic , nice , cool and negative adjectives: bad , dull . Then we resort to WordNet to predict the orientations of all the adjectives in the opinion word list. Once an adjective X  X  orientation in the process. The complete procedure for predicting semantic orientations for all the adjectives in the opini on list is shown in Figure 5. Procedure OrientationPrediction takes the adjective seed list and a set of opinion words whose orient ations need to be determined. It calls procedure OrientationSearch iteratively until no new opinion word is added to the seed list. Every time an adjective with its orientation is added to the seed list, th e seed list is updated; therefore calling OrientationSearch repeatedly is necessary in order to exploit the newly added information. 1. Procedure OrientationPrediction ( adjective_list , seed_list ) 2. begin 3. do { 4. size 1 = # of words in seed_list ; 5. OrientationSearch ( adjective_list, seed_list ); 6. size 2 = # of words in seed_list ; 7. } while ( size 1  X  size 2 ); 8. end 1. Procedure OrientationSearch ( adjective_list , seed_list ) 2. begin 3. for each adjective w i in adjective_list 4. begin 5. if ( w i has synonym s in seed_list ) 6. { w i  X  X  orientation= s  X  X  orientation; 7. add w i with orientation to seed_list ; } 8. else if ( w i has antonym a in seed_list ) 9. { w i  X  X  orientation = opposite orientation of a  X  X  10. add w i with orientation to seed_list ; } 11. endfor; 12. end 
Figure 5: Predicting the semant ic orientations of opinion Procedure OrientationSearch searches WordNet and the seed list for each target adjective word to predict its orientation (line 3 to line 11). In line 5, it searches synset of the target adjective in WordNet and checks if any synonym has known orientation. If so, the target orientation is set to the same orientation as the synonym (line 6) and the target adjectiv e along with the orientation is continues to search antonym set of the target word in WordNet and checks if any antonym has known orientation (line 8). If so, the target orientation is set to the opposite of the antonym (line 9) and the target adjective with its orientation is inserted into the seed list (line 10). If neither synonyms nor antonyms of the target word have known orientation, the function just continues the same process for the next adjective since the word X  X  orientation may be found in a later call of the proce dure with an updated seed list. For those adjectives that WordNet cannot recognize, they are discarded as they may not be valid words. For those that we cannot find orientations, they will also be removed from the opinion words list and the user will be notified for attention. If the user feels that the word is an opinion word and knows its sentiment, he/she can update the seed list. In our experiments, there is no user involvement (those removed opinion words are dropped). For the case that the synonyms/antonyms of an adjective have different known sema ntic orientations, we use the first found orientation as the orientation for the given adjective. Frequent features are the  X  X ot X  features that people comment most about the given product. However, there are some features that only a small number of people talk ed about. These features can also be interesting to some potential customers and the manufacturer of the product. The que stion is how to extract these infrequent features (association mining is unable to identify such features)? Considering the following sentences: Sentences 1 and 2 share the same opinion word amazing yet describing different features: sentence 1 is about the pictures , and sentence 2 is about the software . Since one adjective word can be used to describe different object s, we could use the opinion words to look for features that cannot be found in the frequent feature generation step using association mining. We extract infrequent features using the procedure in Figure 6: We use the nearest noun/noun phrase as the noun/noun phrase that the opinion word modifies because that is what happens most of the time. This simple heuristic seems to work well in practice. A problem with the infrequent feature identification using opinion words is that it could also find nouns/noun phrases that are irrelevant to the given product. The reason for this is that people can use common adjectives to describe a lot of objects, including both interesting features that we want and irrelevant ones. This, however, is not a serious problem because the number of infrequent features, compared with the number of frequent features, is small. They account for around 15-20% of the total number of features as obtained in our experimental results. Infrequent features are generate d for completeness. Moreover, frequent features are more importa nt than infrequent ones. Since we rank features according to their p-supports, those wrong infrequent features will be ranked very low and thus will not affect most of the users. We now reach the step of predicting the orientation of an opinion sentence, i.e., positive or negative. In general, we use the dominant orientation of the opinion words in the sentence to positive/negative opinion prevails, the opinion sentence is regarded as a positive/negative one. In the case where there is the same number of positive and negative opinion words in the sentence, we predict the orientation using the average orientation of effective opinions or the orientation of the previous opinion sentence (recall that effective opinion is the closest opinion word for a feature in an opinion sentence). This is an effective method as our experimental results s how. The detailed procedure is described in Figure 7. Procedure SentenceOrietation deals with three situations in predicting the semantic orientation of an opinion sentence: 1. The user likes or dislikes most or all the features in one 2. The user likes or dislikes most of the features in one sentence, 3. All the other cases. For case 1, the dominant orientati on can be easily identified (line 5 to 10 in the first procedure, SentenceOrietation ). This is the most common case when people express their opinions. For case 2, we use the average orientation of effective opinions of features instead (line 12 to 18). Effective opinion is assumed to be the orientation of the opinion sentence to be the same as the orientation of previous opinion sentence (line 19). We use the context information to predict the sentence orientation because in most cases, people express their positive/negative opinions together in one text segment, i.e., a few consecutive sentences. For a sentence that contains a but clause (sub-sentence that starts with but , however , etc.), which indicates sentimental change for the features in the clause, we first use the effective opinion in the clause to decide the orientation of the features. If no opinion appears in the clause, the opposite orientation of the sentence will be used. Note that in the procedure wordOrientation , we do not simply take the semantic orientation of the opinion word from the set of opinion words as its orientation in the specific sentence. We also consider whether there is a nega tion word such as  X  X o X ,  X  X ot X ,  X  X et X , appearing closely around the opinion word. If so, the opinion orientation of the sentence is the opposite of its original orientation (lines 4 and 5). By closely we mean that the word distance between a negation word and the opinion word should not exceed a threshold (in our experiment, we set it to 5). This simple method deals with the sentences like  X  the camera is not easy to use  X , and  X  it would be nicer not to see little zoom sign on the side  X . This method is quite e ffective in most cases. After all the previous steps, we are ready to generate the final feature-based review summary, which is straightforward and consists of the following steps:  X  For each discovered feature, related opinion sentences are put  X  All features are ranked according to the frequency of their The following shows an example summary for the feature  X  picture  X  of a digital camera. Note that the individual opinion sentences (and their corresponding reviews, which are not shown here) can be hidden using a hyperlink in order to enable the user to see a global view of the summary easily. 1. Procedure SentenceOrietation () 2. begin 3. for each opinion sentence s i 4. begin 5. orientation = 0; 6. for each opinion word op in s i 7. orientation += wordOrientation ( op, s i ); 8. /*Positive = 1, Negative = -1, Neutral = 0*/ 9. if ( orientation &gt; 0) s 10. else if ( orientation &lt; 0) s i  X  X  orientation = Negative; 12. for each feature f in s i 13. orientation += 14. wordOrientation ( f X  X  effective opinion, s i ); 16. s i  X  X  orientation = Positive; 17. else if ( orientation &lt; 0) 18. s i  X  X  orientation = Negative; 19. else s i  X  X  orientation = s i-1  X  X  orientation; 20. } 21. endfor; 22. end 1. Procedure wordOrientation ( word , sentence ) 2. begin 3. orientation = orientation of word in seed_list ; 4. If (there is NEGATION_WORD appears closely 5. orientation = Opposite( orientation ); 6. end 
Figure 7: Predicting the orientations of opinion sentences A system, called FBS ( F eature -B ased S ummarization), based on the proposed techniques has been implemented in C++. We now evaluate FBS from three perspectives: 1. The effectiveness of feature extraction. 2. The effectiveness of opinion sentence extraction. 3. The accuracy of orientation prediction of opinion sentences. We conducted our experi ments using the customer reviews of five electronics products: 2 digita l cameras, 1 DVD player, 1 mp3 player, and 1 cellular phone. The reviews were collected from Amazon.com and C|net.com. Products in these sites have a large number of reviews. Each of the reviews includes a text review and a title. Additional information av ailable but not used in this project includes date, time, aut hor name and location (for Amazon reviews), and ratings. For each product, we first crawled and downloaded the first 100 reviews. These review documents were then cleaned to remove HTML tags. After that, NLProcessor [31] is used to generate part-of-speech tags. Our system is then applied to perform summarization. For evaluation, we manually read all the reviews. For each sentence in a review, if it shows user X  X  opinions, all the features on which the reviewer has expre ssed his/her opinion are tagged. Whether the opinion is positive or negative (i.e., the orientation) is also identified. If the user gives no opinion in a sentence, the sentence is not tagged as we are only interested in sentences with opinions in this work. For each product, we produced a manual feature list. Column  X  X o. of ma nual features X  in Table 1 shows the number of manual features for each product. All the results generated by our system are compared with the manually tagged results. Tagging is fairly straightforward for both product features and opinions. A minor complicati on regarding feature tagging is that features can be explicit or implicit in a sentence. Most features appear explicitly in opinion sentences, e.g., pictures in  X  X he pictures are absolutely amazing X . Some features may not appear in sentences. We call such features implicit features, e.g., size in  X  it fits in a pocket nicely  X . Both explicit and implicit features are easy to identify by the human tagger. Another issue is that judging opinions in reviews can be somewhat subjective. It is usually easy to judge whether an opinion is positive or negative if a sentence clearly expresses an opinion. However, deciding whethe r a sentence offers an opinion or not can be debatable. For t hose difficult cases, a consensus was reached between the primary human tagger (the first author of the paper) and the secondary tagger (the second author of the paper). Table 1 gives the precision and recall results of the feature generation function of FBS. We evaluated the results at each step of our algorithm. In the table, column 1 lists each product. Columns 3 and 4 give the recall and precision of frequent feature generation for each product, which uses association mining. The results indicate that the frequent features contain a lot of errors. Using this step alone gives poor results, i.e., low precision. Columns 5 and 6 show the corresponding results after compactness pruning is performed. We can see that the precision is improved significantly by this pruning. The recall stays steady. Columns 7 and 8 give the results after pruning using p-support. There is another dramatic impr ovement in the precision. The recall level almost does not cha nge. The results from Columns 4-8 clearly demonstrate the effectiveness of these two pruning techniques. Columns 9 and 10 give the results after infrequent feature identification is done. The recall is improved dramatically. The precision drops a few percents on average. However, this is not a major problem because the infrequent features are ranked rather low, and thus will not affect most users. To further illustrate the effectiveness of our feature extraction pruning step, we compared the features generated using our method with terms found by the well known and publicly available term extraction and indexing system, FASTR [11] of Christian Jacquemin. Table 2 shows the r ecall and precision of FASTR. We observe that both the average recall and precision of FASTR are significantly lower than those of our method. After a close inspection of the terms generated by FASTR, we see that there are two major reasons that lead to its poor results. First of all, FASTR generates a large number of terms, as shown in the fourth column  X  X o. terms X  of Table 2. The average number of terms found by FASTR is 377. Most of these term s are not product features at all (although many of them may be noun phrases). Secondly, FASTR does not find one-word terms, but only term phrases that consist of two or more words. Our feature extraction method finds both one-word terms and term phrases. Comparing the results in Table 1 and Table 2, we can clearly see that the proposed method is much more effective for our task. Table 3 shows the evaluation results of the other two procedures: opinion sentence extraction and sentence orientation prediction. The average recall of opinion sentence extraction is nearly 70%. The average precision of opinion sentence extraction is 64%. Note that as indicated earlier determining whether a sentence expresses an opinion is subjective. Our result analysis indicates that people like to describe their  X  X tories X  with the product lively: they often mention the situation that they used the product, the detailed product features used, and also the results they got. While human taggers do not regard thes e sentences as opinion sentences as there is no indication of whether the user likes the features or not, our system labels these sentences as opinion sentences because they contain both product features and some opinion adjectives. This d ecreases precision. Although these sentences may not show strong user opinions towards the product features, they may still be beneficial and useful. Our system has a good accuracy in predicting sentence orientations: the average accuracy for the five products is 84%. This shows that our method of using WordNet to predict adjective semantic orientations and orient ations of opinion sentences are highly effective. 
Table 3: Results of opinion sentence extraction and sentence Product name Digital camera1 0.719 0.643 0.927 Digital camera2 0.634 0.554 0.946 Cellular phone 0.675 0.815 0.764 Mp3 player 0.784 0.589 0.842 DVD player 0.653 0.607 0.730 In summary, we can see that our techniques are very promising, especially for sentence orientation prediction. We believe that they may be used in practical settings. We also note three main limitations of our system: (1) We have not dealt with opinion sentences that need pronoun resolution [40]. For instance,  X  it is quiet but powerful  X . To understand what it represents, pronoun resolution needs to be performed. Pronoun resolution is a complex and computational expensive problem in natural language processing (NLP). We plan to adapt some existing techniques from NLP to suit our needs. (2) We only used adjectives as indicators of opinion orientations of sentences. However, verbs and nouns can also be used for the purpose, e.g.,  X  I like the feeling of the camera  X ,  X  I highly recommend the camera  X . We plan to address this issue in the future. (3) It is also important to study the strength of opinions. Some opinions are very strong and some are quite mild. Highlighting strong opi nions (strongly like or dislike) can be very useful for both individual shoppers and product manufacturers. In this paper, we proposed a set of techniques for mining and summarizing product reviews base d on data mining and natural language processing methods. The objective is to provide a feature-based summary of a large number of customer reviews of a product sold online. Our experimental results indicate that the proposed techniques are very promis ing in performing their tasks. We believe that this problem w ill become increasingly important as more people are buying and expressing their opinions on the Web. Summarizing the reviews is not only useful to common shoppers, but also crucial to product manufacturers. In our future work, we plan to further improve and refine our techniques, and to deal with the outstanding problems identified above, i.e., pronoun resolution, determining the strength of opinions, and investigating opini ons expressed with adverbs, verbs and nouns. Finally, we will also look into monitoring of customer reviews. We believe that monitoring will be particularly useful to product manufacturers because they want to know any new positive or negative comments on their products whenever they are available. The keyword here is new . Although a new review may be added, it may not contain any new information. The research in this paper was supported by the National Science Foundation under the grant NSF IIS-0307239. [1]. Agrawal, R. &amp; Srikant, R. 1994. Fast algorithm for mining [2]. Boguraev, B., and Kennedy, C. 1997. Salience-Based [3]. Bourigault, D. 1995. Lexter: A terminology extraction [4]. Bruce, R., and Wiebe, J. 2000. Recognizing Subjectivity: A [5]. Cardie, C., Wiebe, J., W ilson, T. and Litman, D. 2003. [6]. Church, K.W. and Hanks , P. 1990. Word Association [7]. Daille, B. 1996. Study and Implementation of Combined [8]. Das, S. and Chen, M., 2001. Yahoo! for Amazon: [9]. Dave, K., Lawrence, S., and Pennock, D., 2003. Mining the [10]. DeJong, G. 1982. An Overview of the FRUMP System . [11]. FASTR. http://www.limsi. fr/Individu/jacquemi/FASTR/ [12]. Fellbaum, C. 1998. WordNet: an Electronic Lexical [13]. Finn, A. and Kushmerick, N. 2003. Learning to Classify [14]. Finn, A., Kushmerick, N., and Smyth, B. 2002. Genre [15]. Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell, J. [16]. Hatzivassiloglou, V. a nd Mckeown, K., 1997. Predicting [17]. Hatzivassiloglou, V. a nd Wiebe, 2000. J. Effects of [18]. Hearst, M, 1992. Direction-ba sed Text Interpretation as an [19]. Hu, M., and Liu, B. 2004. Mining Opinion Features in [20]. Huettner, A. and Subasic, P., 2000. Fuzzy Typing for [21]. Jacquemin, C., and Bourig ault, D. 2001. Term extraction [23]. Karlgren, J. and Cutting, D. 1994. Recognizing Text Genres [24]. Kessler, B., Nunberg, G., and Schutze, H. 1997. Automatic [25]. Kupiec, J., Pedersen, J., and Chen, F. 1995. A Trainable [26]. Liu, B., Hsu, W., Ma, Y. 1998. Integrating Classification [27]. Mani, I., and Bloedorn, E., 1997. Multi-document [28]. Manning, C. and Sch X tze, H. 1999. Foundations of [29]. Miller, G., Beckwith, R, Fellbaum, C., Gross, D., and [30]. Morinaga, S., Ya Yamanishi, K., Tateishi, K, and [31]. NLProcessor  X  Text Analysis Toolkit . 2000. [32]. Paice, C. D. 1990. Constr ucting Literature Abstracts by [33]. Pang, B., Lee, L., and Vaithyanathan, S., 2002. Thumbs [34]. Reimer, U. and Hahn, U. 1997. A Formal Model of Text [35]. Sack, W., 1994. On the Co mputation of Point of View. [36]. Salton, G. Singhal, A. Bu ckley, C. and Mitra, M. 1996. [37]. Sparck J. 1993a. Discourse Modeling for Automatic Text [38]. Sparck J. 1993b. What might be in a summary? Information [39]. Tait, J. 1983. Automatic Summarizing of English Texts. [40]. Tetreault, J. 1999. Analysis of Syntax-Based Pronoun [41]. Tong, R., 2001. An Operati onal System for Detecting and [42]. Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic [43]. Wiebe, J. 2000. Learning Subjective Adjectives from [44]. Wiebe, J., Bruce, R., and O X  X ara, T. 1999. Development 
