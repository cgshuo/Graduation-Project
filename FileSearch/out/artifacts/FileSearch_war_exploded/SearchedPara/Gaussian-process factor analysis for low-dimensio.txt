 Neural responses are typically studied by averaging noisy spiking activity across multiple experi-mental trials to obtain fi ring rates that vary smoothly over time. However, particularly in cognitive tasks (such as motor planning or decision making) where the neural responses are more a re fl ection of internal processing rather than external stimulus drive, the timecourse of the neural responses may differ on nominally identical trials. In such settings, it is critical that the neural data not be averaged across trials, but instead be analyzed on a trial-by-trial basis [1, 2, 3, 4].
 Single-trial analyses can leverage the simultaneous monitoring of large populations of neurons in vivo , currently ranging from tens to hundreds in awake, behaving animals. The approach adopted by recent studies is to consider each neuron being recorded as a noisy sensor re fl ecting the time-evolution of an underlying neural process [3, 5, 6, 7, 8, 9, 10]. The goal is to uncover this neural process by extracting a smooth, low-dimensional neural trajectory from the noisy, high-dimensional recorded activity on a single-trial basis. The neural trajectory provides a compact representation of the high-dimensional recorded activity as it evolves over time, thereby facilitating data visualization and studies of neural dynamics under different experimental conditions.
 A common method to extract neural trajectories is to fi rst estimate a smooth fi ring rate pro fi le for each neuron on a single trial (e.g., by convolving each spike train with a Gaussian kernel), then apply a static dimensionality reduction technique (e.g., principal components analysis, PCA) [8, 11]. Smooth fi ring rate pro fi les may also be obtained by averaging across a small number of trials (if the neural timecourses are believed to be similar on different trials) [6, 7, 9, 10], or by applying more advanced statistical methods for estimating fi ring rate pro fi les from single spike trains [12, 13]. Numerous linear and non-linear dimensionality reduction techniques exist, but to our knowledge only PCA [8, 9, 11] and locally linear embedding (LLE) [6, 7, 10, 14] have been applied in this context to neural data.
 While this two-stage method of performing smoothing then dimensionality reduction has provided informative low-dimensional views of neural population activity, there are several aspects that can be improved. (i) For kernel smoothing, the degree of smoothness is often chosen in an ad hoc way. We would instead like to learn the appropriate degree of smoothness from the data. Because the opera-tions of kernel smoothing, PCA, and LLE are all non-probabilistic, standard likelihood techniques for model selection are not applicable. Even if a probabilistic dimensionality reduction algorithm is used, the likelihoods would not be comparable because different smoothing kernels yield different smoothed data. (ii) The same kernel width is typically used for all spike trains, which implicitly assumes that the neural population activity evolves with a single timescale. We would instead like to allow for the possibility that the system operates under multiple timescales. (iii) PCA and LLE have no explicit noise model and, therefore, have dif fi culty distinguishing between spiking noise (whose variance may vary both across neurons and across time) and changes in the underlying low-dimensional neural state. (iv) Because the smoothing and dimensionality reduction are performed sequentially, there is no way for the dimensionality reduction algorithm to in fl uence the degree or form of smoothing used. This is relevant both to the identi fi cation of the low-dimensional space, as well as to the extraction of single-trial neural trajectories.
 We fi rst brie fl y describe relatively straightforward extensions of the two-stage methods that can help to address issues (i) and (iii) above. For (i), we adopt a goodness-of-fi t metric that measures how well the activity of each neuron can be predicted by the activity of all other recorded neurons, based on data not used for model fi tting. This metric can be used to compare different smoothing kernels and allows for the degree of smoothness to be chosen in a principled way. In Section 6, we will use this as a common metric by which different methods for extracting neural trajectories are compared. For (iii), we can apply the square-root transform to stabilize the spiking noise variance and factor analysis (FA) [15] to explicitly model possibly different independent noise variances for different neurons. These extensions are detailed in Sections 2 and 3.
 Next, we introduce Gaussian-process factor analysis (GPFA), which uni fi es the smoothing and di-mensionality reduction operations in a common probabilistic framework. GPFA takes steps toward addressing all of the issues (i) X (iv) described above, and is shown in Section 6 to provide a bet-ter characterization of the recorded population activity than the two-stage methods. Because GPFA performs the smoothing and dimensionality reduction operations simultaneously rather than sequen-tially, the degree of smoothness and the relationship between the low-dimensional neural trajectory and the high-dimensional recorded activity can be jointly optimized. Different dimensions in the low-dimensional space (within which the neural state evolves) can have different timescales, whose optimal values can be found automatically by fi tting the GPFA model to the recorded activity. As in FA, GPFA speci fi es an explicit noise model that allows different neurons to have different indepen-dent noise variances. The time series model involves Gaussian processes (GP), which only require the speci fi cation of the correlation structure of the neural state over time.
 A critical assumption when attempting to extract a low-dimensional neural trajectory is that the recorded activity evolves within a low-dimensional manifold. Previous studies have typically as-sumed that the neural trajectories lie in a three-dimensional space for ease of visualization. In this work, we will investigate whether this low-dimensional assumption is justi fi ed in the context of motor preparation and execution and, if so, attempt to identify the appropriate dimensionality. Sec-tions 2 and 3 detail GPFA and the goodness-of-fi t metric, respectively. Section 4 relates GPFA to dynamical systems approaches. After describing the experimental setup in Section 5, we apply the developed methods to neural activity recorded in premotor and motor cortices during reach planning and execution in Section 6. The motivation for GPFA can be traced back to the use of PCA for extracting informative low-dimensional views of high-dimensional neural data. Consider spike counts taken in non-overlapping time bins. PCA (or its probabilistic form, PPCA [15]) attempts to fi nd the directions in the high-dimensional data with greatest variance. This is problematic for neural data for two reasons. First, because neurons with higher mean counts are known to exhibit higher count variances, the directions found by PCA tend to be dominated by the most active neurons. Second, PCA assumes that the spiking noise variance is time independent; however, neurons are known to change their fi ring rates, and therefore noise variances, over time. A possible solution is to replace the Gaussian likelihood model of PPCA with a point-process [5] or Poisson [3] likelihood model. Here, we consider a simpler approach that preserves computational tractability. The square-root transform is known to both stabilize the variance of Poisson counts and allow Poisson counts to be more closely modeled by a Gaussian distribution, especially at low Poisson means [16]. Thus, the two issues above can be largely resolved by applying PCA/PPCA to square-rooted spike counts, rather than raw spike counts. However, the spiking noise can deviate from a Poisson distribution [17], in which case the noise variance is not entirely stabilized. As will be shown in Section 6, the square-rooted counts can be better characterized by further replacing PCA/PPCA with FA [15], which allows different neurons to have different independent noise variances.
 In this work, we extend FA for use with time series data. PCA, PPCA, and FA are all static dimen-sionality reduction techniques. In other words, none of them take into account time labels when applied to time series data; the time series data are simply treated as a collection of data points. GPFA is an extension of FA that can leverage the time label information to provide more powerful dimensionality reduction. The GPFA model is simply a set of factor analyzers (one per timepoint, each with identical parameters) that are linked together in the low-dimensional state space by a Gaussian process (GP) [18] prior. Introducing the GP allows for the speci fi cation of a correlation structure across the low-dimensional states at different timepoints. For example, if the system under-lying the time series data is believed to evolve smoothly over time, we can specify that the system X  X  state should be more similar between nearby timepoints than between faraway timepoints. Extract-ing a smooth, low-dimensional neural trajectory can therefore be viewed as a compromise between the low-dimensional projection of each data point found by FA and the desire to string them together using a smooth function over time. The GPFA model can also be obtained by letting time indices play the role of inputs in the semiparametric latent factor model [19].
 The following is a mathematical description of GPFA. Let y : ,t  X  R q  X  1 be the high-dimensional vector of square-rooted spike counts recorded at timepoint t  X  X  1 ,...,T } , where q is the number of neurons being recorded simultaneously. We seek to extract a corresponding low-dimensional latent neural state x : ,t  X  R p  X  1 at each timepoint, where p is the dimensionality of the state space ( p&lt;q ). For notational convenience, we group the neural states from all timepoints into a neural trajectory observations y : ,t and neural states x : ,t constrain the covariance matrix R to be diagonal, where the diagonal elements are the independent noise variances of each neuron. In general, different neurons can have different independent noise variances. Although a Gaussian is not strictly a distribution on square-rooted counts, its use in (1) preserves computational tractability.
 The neural states x : ,t at different timepoints are related through Gaussian processes, which embody the notion that the neural trajectories should be smooth. We de fi ne a separate GP for each dimension of the state space indexed by i  X  X  1 ,...,p } [20]. The form of the GP covariance can be chosen to provide different smoothing properties on the neural trajectories. In this work, we chose the commonly-used squared exponential (SE) covariance function where K i ( t 1 ,t 2 ) denotes the ( t 1 ,t 2 ) th entry of K i and t 1 ,t 2  X  X  1 ,...,T } . The SE covariance is de fi ned by its signal variance  X  2 f,i  X  R + , characteristic timescale  X  i  X  R + , and noise variance n,i  X  R + . Due to redundancy in the scale of X and C ,we fi x the scale of X and allow C to be learned unconstrained, without loss of generality. By direct analogy to FA, we de fi ned the prior distribution of the neural state x : ,t at each timepoint t to be N ( 0 ,I ) by setting  X  2 f,i =1  X   X  2 n,i , where 0 &lt; X  2 n,i  X  1 . Furthermore, because we seek to extract smooth neural trajectories, we set  X  2 n,i to a small value ( 10  X  3 ). Thus, the timescale  X  i is the only (hyper)parameter of the SE covariance that is learned. The SE is an example of a stationary covariance; other stationary and non-stationary GP covariances [18] can be applied in a seamless way.
 The parameters of the GPFA model can be learned in a straightforward way using the expectation-maximization (EM) algorithm. In the E-step, the Gaussian posterior distribution P ( X | Y ) can be computed exactly because the x : ,t and y : ,t across all timepoints are jointly Gaussian, by de fi ni-tion. In the M-step, the parameters updates for C , d ,and R can be expressed in closed form. The characteristic timescales  X  i can be updated using any gradient optimization technique. Note that the degree of smoothness (de fi ned by the timescales) and the relationship between the low-dimensional neural trajectory and the high-dimensional recorded activity (de fi ned by C ) are jointly optimized. Furthermore, a different timescale is learned for each state dimension indexed by i . For the results shown in Section 6, the parameters C , d ,and R were initialized using FA, and the  X  i were initialized to 100 ms. Although the learned timescales were initialization-dependent, their distributions were similar for different initializations. In particular, most learned timescales were less than 150 ms, but there were usually one or two larger timescales around 300 and 500 ms.
 Once the GPFA model is learned, we can apply a post-processing step to orthonormalize the columns of C . Applying the singular value decomposition, C x : ,t can be rewritten as U C ( D C V C x : ,t ) , where orthonormalized neural state at timepoint t . While each dimension of x : ,t possesses a single char-acteristic timescale, each dimension of  X  x : ,t represents a mixture of timescales de fi ned by the columns sponding columns of U C ) are ordered by the amount of data covariance explained. In contrast, the elements of x : ,t (and the corresponding columns of C ) have no particular order. Especially when the number of state dimensions p is large, the ordering facilitates the identi fi cation and visualization of the dimensions of the orthonormalized neural trajectory that are most important for explaining the recorded activity. Because the columns of U C are orthonormal, one can readily picture how the low-dimensional trajectory relates to the high-dimensional space of recorded activity, in much the same spirit as for PCA. This orthonormalization procedure is also applicable to PPCA and FA. In fact, it is through this orthonormalization procedure that the principal directions found by PPCA are equated to those found by PCA. We would like to directly compare GPFA to the two-stage methods described in Section 1. Neither the classic approach of comparing cross-validated likelihoods nor the Bayesian approach of com-paring marginal likelihoods is applicable here, for the same reason that they cannot be used to select the appropriate degree of smoothness in the two-stage methods. Namely, when the data are altered by different pre-smoothing operations (or the lack thereof in the case of GPFA), the likelihoods are no longer comparable. Instead, we adopted the goodness-of-fi t metric mentioned in Section 1, whereby a prediction error is computed based on trials not used for model fi tting. The idea is to leave out one neuron at a time and ask how well each method is able to predict the activity of that neuron, given the activity of all other recorded neurons. For GPFA, the model prediction for neuron the j th row of Y . The model prediction can be computed analytically because all variables in Y are jointly Gaussian, by de fi nition. Model predictions using PPCA and FA are analogous, but each timepoint is considered individually. The prediction error is de fi ned as the sum-of-squared errors between the model prediction and the observed square-rooted spike count across all neurons and timepoints.
 One way to compute the GPFA model prediction is via the low-dimensional state space. One can fi rst estimate the neural trajectory using all but the j th neuron P ( X | Y  X  j, : ) , then map this estimate back out into the space of recorded activity for the j th neuron using (1) to obtain  X  y j, : . Equivalently, one can convert P ( X | Y  X  j, : ) into its orthonormalized form before mapping it out into the space of recorded activity using the j th row of U C . Because the orthonormalized dimensions are ordered, we can evaluate the prediction error using only the top  X  p orthonormalized dimensions of  X  x : ,t , where  X  p  X  X  1 ,...,p } . This reduced GPFA model can make use of a larger number p of timescales than its effective dimensionality  X  p . Another way to extract neural trajectories is by de fi ning a parametric dynamical model that describes how the low-dimensional neural state evolves over time. A fi rst-order linear auto-regressive (AR) model [5] captures linear Markovian dynamics. Such a model can be expressed as a Gaussian process, since the state variables are jointly Gaussian. This can be shown by de fi ning a separate fi rst-order AR model for each state dimension indexed by i  X  X  1 ,...,p } Given enough time ( t  X  X  X  )and | a i | &lt; 1 , the model will settle into a stationary state that is equivalent to (2) with as in [21]. Different covariance structures K i can be obtained by going from a fi rst-order to an n th-order AR model. One drawback of this approach is that it is usually not easy to construct an n th-order AR model with a speci fi ed covariance structure. In contrast, the GP approach described in Section 2 requires only the speci fi cation of the covariance structure, thus allowing different smooth-ing properties to be applied in a seamless way. AR models are generally less computationally de-manding than those based on GP, but this advantage shrinks as the order of the AR model grows. Another difference is that (5) does not contain an independent noise term  X  2 n,i  X   X  t innovations noise  X  2 i in (4) is involved in setting the smoothness of the time series, as shown in (5). Thus, (4) would need to be augmented to explicitly capture departures from the AR model. One may also consider de fi ning a non-linear dynamical model [3], which typically has a richer set of dynamical behaviors than linear models. The identi fi cation of the model parameters provides insight into the dynamical rules governing the time-evolution of the system under study. However, espe-cially in exploratory data analyses, it may be unclear what form this model should take. Even if an appropriate non-linear model can be identi fi ed, learning such a model can be unstable and slow due to approximations required [3]. In contrast, learning the GPFA model is stable and approximation-free, as described in Section 2. The use of GPFA can be viewed as a practical way of going beyond a fi rst-order linear AR model without having to commit to a particular non-linear system, while retaining computational tractability. The details of the neural recordings and behavioral task can be found elsewhere [22]. Brie fl y, a rhesus macaque performed delayed center-out reaches to visual targets presented on a fronto-parallel screen. On a given trial, the peripheral reach target was presented at one of 14 possible locations  X  two distances (60 and 100 mm) and seven directions (0, 45, 90, 135, 180, 225, 315 X ). Delay periods were randomly chosen between 200 and 700 ms. Neural activity was recorded using a 96-electrode array (Cyberkinetics, Foxborough, MA) in dorsal premotor and motor cortices. Only those units (61 single and multi-units, experiment G20040123 ) with robust delay period activity were included in our analyses. We considered neural data for one reach target at a time, ranging from 200 ms before reach target onset to movement end. This period comprised the 200 ms pre-target time, the randomly chosen delay period (200 X 700 ms), the monkey X  X  reaction time (mean  X  s.d.: 293  X  48 ms), and the duration of the monkey X  X  reach (269  X  40 ms). Spike counts were taken in non-overlapping 20 ms bins, then square-rooted. For the two-stage methods, these square-rooted counts were smoothed over time using a Gaussian kernel. We also considered smoothing spike trains directly, which yielded qualitatively similar results for the two-stage methods.
 Using the goodness-of-fi t metric described in Section 3, we can fi nd the appropriate degree of smoothness for the two-stage methods. Fig. 1 shows the prediction error for PPCA (red) and FA (green) for different kernel widths and state dimensionalities. There are two primary fi ndings. First, FA yielded lower prediction error than PPCA across a range of kernel widths and state dimension-alities. The reason is that FA allows different neurons to have different independent noise variances. Second, for these data, the optimal smoothing kernel width (s.d. of Gaussian kernel) is approxi-mately 40 ms for both FA and PPCA. This was found using a denser sweep of the kernel width than shown in Fig. 1.
 It is tempting to try to relate this optimal smoothing kernel width (40 ms) to the timescales  X  i learned by GPFA, since the SE covariance has the same shape as the Gaussian smoothing kernel. However, nearly all of the timescales learned by GPFA are greater than 40 ms. This apparent mismatch can be understood by considering the equivalent kernel of the SE covariance [23], which takes on a sinc-like shape whose main lobe is generally far narrower than a Gaussian kernel with the same width parameter. It is therefore reasonable that the timescales learned by GPFA are larger than the optimal smoothing kernel width.
 The same goodness-of-fi t metric can be used to compare the two-stage methods, parametric dynam-ical models, and GPFA. The parametric dynamical model considered in this work is a fi rst-order AR model described by (2) and (5), coupled with the linear-Gaussian observation model (1). Note that a separate stationary, one-dimensional fi rst-order AR model is de fi ned for each of the p latent dimen-sions. As shown in Fig. 1, the fi rst-order AR model (blue) yielded lower prediction error than the two-stage methods (PPCA: red, FA: green). Furthermore, GPFA (dashed black) performed as well or better than the two-stage methods and the fi rst-order AR model, regardless of the state dimen-sionality or kernel width used. As described in Section 3, the prediction error can also be computed for a reduced GPFA model (solid black) using only the top  X  p orthonormalized dimensions, in this case based on a GPFA model fi t with p =15 state dimensions. By de fi nition, the dashed and solid black lines coincide at  X  p =15 . The solid black curve reaches its minimum at  X  p =10 (referred to as p  X  ). Thus, removing the lowest fi ve orthonormalized dimensions decreased the GPFA prediction error. Furthermore, this prediction error was lower than when fi tting the GPFA model directly with p =10 (dashed black).
 These latter fi ndings can be understood by examining the orthonormalized neural trajectories ex-tracted by GPFA shown in Fig. 2. The traces plotted are the orthonormalized form of E [ X | Y ] . The panels are arranged in decreasing order of data covariance explained. The top orthonormalized dimensions indicate fl uctuations in the recorded population activity shortly after target onset (red Figure 2: Orthonormalized neural trajectories for GPFA with p =15 . Each panel corresponds to one of the 15 dimensions of the orthonormalized neural state, which is plotted versus time. The orthonormalized neural trajectory for one trial comprises one black trace from each panel. Dots indicate time of reach target onset (red), go cue (green), and movement onset (blue). Due to differing trial lengths, the traces on the left/right half of each panel are aligned on target/movement onset for clarity. However, the GPFA model was fi t using entire trials with no gaps. Note that the polarity of these traces is arbitrary, as long as it is consistent with the polarity of U C . Each trajectory corresponds to planning and executing a reach to the target at distance 60 mm and direction 135 X . For clarity, only 10 trials with delay periods longer than 400 ms are plotted. dots) and again after the go cue (green dots). Furthermore, the neural trajectories around the time of the arm movement are well-aligned on movement onset. These observations are consistent with previous analyses of the same dataset [22], as well as other studies of neural activity collected during similar tasks in the same cortical areas. Whereas the top 10 orthonormalized dimensions (upper and middle rows) show repeatable temporal structure across trials, the bottom fi ve dimensions (lower row) appear to be largely capturing noise. These  X  X oise dimensions X  could be limiting GPFA X  X  pre-dictive power. This is con fi rmed by Fig. 1: when the bottom fi ve orthonormalized dimensions were removed, the GPFA prediction error decreased.
 It still remains to be explained why the GPFA prediction error using only the top 10 orthonormalized dimensions is lower than that obtained by directly fi tting a GPFA model with p =10 . Each panel in Fig. 2 represents a mixture of 15 characteristic timescales. Thus, the top 10 orthonormalized dimensions can make use of up to 15 timescales. However, a GPFA model fi t with p =10 can have at most 10 timescales. By fi tting a GPFA model with a large number of state dimensions p (each with its own timescale) and taking only the top  X  p = p  X  orthonormalized dimensions, we can obtain neural trajectories whose effective dimensionality is smaller than the number of timescales at play. Based on the solid black line in Fig. 1 and Fig. 2, we consider the effective dimensionality of the recorded population activity to be p  X  =10 . In other words, the linear subspace within which the recorded activity evolved during reach planning and execution for this particular target was 10-dimensional. Across the 14 reach targets, the effective dimensionality ranged from 8 to 12. All major trends seen in Fig. 1 were preserved across all reach targets. GPFA offers a fl exible and intuitive framework for extracting neural trajectories, whose learning algorithm is stable, approximation-free, and simple to implement. Because only the GP covariance structure needs to be speci fi ed, GPFA is particularly attractive for exploratory data analyses, where the rules governing the dynamics of the system under study are unknown. Based on the trajectories obtained by GPFA, one can then attempt to de fi ne an appropriate dynamical model that describes how the neural state evolves over time. Compared with two-stage methods, the choice of GP covariance allows for more explicit speci-fi cation of the smoothing properties of the low-dimensional trajectories. This is important when investigating (possibly subtle) properties of the system dynamics. For example, one may wish to ask whether the system exhibits second-order dynamics by examining the extracted trajectories. In this case, it is critical that second-order effects not be built-in by the smoothness assumptions used to extract the trajectories. With GPFA, it is possible to select a triangular GP covariance that assumes smoothness in position, but not in velocity. In contrast, it is unclear how to choose the shape of the smoothing kernel to achieve this in the two-stage methods.
 In future work, we would like to couple the covariance structure of the one-dimensional GPs, which would allow for a richer description of the multi-dimensional neural state x : ,t evolving over time. We also plan to apply non-stationary GP kernels, since the neural data collected during a behavioral task are usually non-stationary. In addition, we would like to extend GPFA by allowing for the discovery of non-linear manifolds and applying point-process likelihood models.
 This work was supported by NIH-NINDS-CRCNS 5-R01-NS054283-03, NSF, NDSEGF, Gatsby, SGF, CDRF, BWF, ONR, Sloan, and Whitaker. We would like to thank Dr. Mark Churchland, Melissa Howard, Sandra Eisensee, and Drew Haven.

