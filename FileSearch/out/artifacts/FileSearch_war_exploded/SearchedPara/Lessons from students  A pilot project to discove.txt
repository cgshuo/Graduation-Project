
Although perspectives and terminology used may differ, the importance of terminological r e-lation s in terminology research and management is appreciated by many scholars. Attention has focused at various points on the classification and description of relations that are relevant for terminology work , on methods for extracting these from texts, and on the relevance of and ap-proaches to integrating this information into te r-minology resources . Among the first proposals for relation -enriched terminology resources was 
Meyer et al. X  X  (1992) terminological knowledge base (TKB), a terminology resource that d e-scribes not only a range of concepts but also a variety of relationships that hold between them. even (semi -)automatically from texts (cf.
L X  X omme and Marshman 2006) in the form of knowledge -rich contexts (KRCs ) (Meyer 2001). 
These excerpts of texts often contain knowledge patterns  X  X .e., combinations of terms or other linguistic units that express concepts, linked by lexical markers of the relations between them  X  and can both provide information to assist in un-derstanding the terms and concepts and illustrate the linguistic items in use . As excerpts of  X  X uthentic X  texts, KRCs can also illustrate vari a-tion in concepts  X  expression and the lexical mark ers used in various communicative situ a-tions (e.g. Condamines 2002, 2008; Marshman and L X  X omme 2008; Marshman et al. 2009).
 ers who have investigated various strategies for developing and populating TKBs (e.g. 
Condamines and Rebeyrolle 2000, 2001; Faber et al. 2011; Faber and San Mart X n 2011; Le X n et al. 2011, 2013) in a selection of domains. Some projects have addressed the use of terminological relationships in the form of ontologies (e.g. Cabr X  et al. 2004; Gillam et al. 2005; Maroto and 
Alcina 2009), as part of an incr easing movement towards the integration of terminology and on-tology (e.g. Temmerman and Kerremans 2003; cf. also Roche et al. 2011). Still others (e.g. 
L X  X omme 2013, 2013a) have described lexical relationships between terms. Most of these r e-sources have be en in electronic form, although some specialized print dictionaries (e.g. Dancette and R X thor X  2000) have include d such info r-mation. tized in resources, there is still no standard model for relation choice and representation . This may be true in part because the wide v ariety of users of terminology resources and purposes for their use (e.g. Sager 1990) entails diverse needs in this area. Faber and San Mart X n ( 2011: 48 ) express the need for  X  X ustomized X  design of terminology resources: observation can be questioned : translators (who are seen as the primary users of terminology da-tabases in contexts such as Canada  X  X ) may not be as interested in domain knowledge per se as in terms, equivalents, synonyms and their use (in-cluding the contexts in which they occur) . These and similar observations have led s ome to con-clude that conventional terminology resources such as the large t erm banks, including terminologique (GDT) 2 , are not adequate for translators X  needs. The same can be said for other resources: ontology-based resources may also not be easy to understand and use for non -subject -field specialists such as translators and terminologists (Cabr  X  et al. 2004: 87). some of the resources that are available to speci f-ic groups and how (and how w ell) they meet the needs of these group s. In this paper, we will f o-cus on the needs of trainee translators: individu-als who are likely in need of both subject -field and linguistic knowledge to carry out a transl a-tion task, but may attribute different levels of importance to each kind of knowledge, and may evaluate the resources that supply this knowledge different ly from other groups and from one another . We will gather information about trainee translators X  reactions to resources from a questionnaire completed by users of three terminology resources, and try to extrapolate some guidelines for the creation of effective re-sources based on this feedback.
 of the currently available terminology resources (section 2). We will outline the methodology used to gather information for this pilot study (section 3), and then will present and discuss some findings (section 4), before wrapping up with some brief remarks, suggested guidelines derived from the observations, and ideas for f u-ture work (section 5).
In this section, we will provide a brief overview of the conventional term banks used in the pr o-ject (2.1), as well as a few examples of relation -enriched resources and how they have compl e-mented this basic model with terminol ogical r e-lations (2.2), and then describe the 
CREATerminal prototype used in this study (2.3). 2.1 Conventional term banks
The largest and most widely used term banks today are mainly constructed on traditional mo d-els such as those described by Pavel and Nolet (2001) and Dubuc (2002) , and provide a range of information to translators, students, writers and other users.

Plus term bank (Government of Canada 2013; see also Pavel and Nolet 2001) has very broad cove rage, including over four million terms (most in English and French, but with a growing component of Spanish and Portuguese) from a wide range of domains . In addition to admini s-trative information including dates of modific a-tion and record authors, i ts ter m records contain largely standard term record fields of domain and sub-domain, terms, equivalents, sources, part -of-speech labels, usage labels, definitions, contexts, observations and in some cases phraseologisms (although not all of these fields may appear on each record) . years, the GDT now presents terms (mainly 
French and English, with a small complement of other languages) from a wide range of domains in a term record format that calls particular atten-tion to French terms and to the associated usage information (particularly appropriateness for use in Quebec ). In addition to (mostly French) def i-nitions, some records include illustrations and notes to clarify meaning (including distinctions between relate d terms and concepts) and usage, as well as administrative fields . resources is uneven, with any such information generally found in definitions , contexts or obse r-vations/ notes. 2.2 Enriching term inology resources with 
In filling the gaps in this traditional term record model and developing the idea of TKBs or on-tologies, a number of projects have addressed the needs of users for additional relation info r-mation. Meyer et al. X  X  (1992) COGNITERM project was followed by other projects including 
GenomaKB 3 (Cabr  X  et al. 2004; Feliu et al. 2004) that integrated corpora and bibliographical i n-formation with a terminological database and an ontology to provide an i ntegrated , multilingual resource that would meet the needs of non -sub ject -field specialists in the field of the g e-nome. This type of integration reflects some of the observations of Bowker (2011), which hig h-lighted the usefulness of access to corpus data for translators researching terms. portance of context of use and its potential for disambiguati on in the description of terms and project ( Faber et al. 2011; Le X n et al. 2011, 2013) . This multilingual resource in the field of environmental science provides access not only to definitions of concepts , but also to visual i n-formation in the form of both illustrations and dynamic relation maps that illustrate connections bet ween terms and other elements (including generic-specific, part -whole and various non-hierarchical relations) based on an approach in-spired by Fillmore X  X  Frame Semantics (Faber et al. 2011; Faber and San Mart X n 2011) . The d y-namic visualization options allow the user to view a wide range of connections and to navigate by follow ing links between concepts, in order to better understand the ir complex interconnections. by a team headed by Marie -Claude L X  X omme at the Universit X  de Montr X al X  X  Observatoire de linguistique Sens -Texte. Developed based on corpus data from the perspective of lexico-semantic terminology , and calling upon princ i-ples of Explanatory and Combina torial Lexico g-raphy (Mel X uk et al. 1995; L X  X omme 2012) and later on Frame Semantics, these resources pr o-vide extensive descriptions of links between terms (including nouns, verbs, adjectives and phrases) in the fields of computing and the Inte r-net and of the environment, respectively. In addi-tion to part -of-speech labels, equivalents, contexts and definitions, term s are accompanied by an analysis of their actantial structures and typical actants, as well as a list of terminological relationships that may include synonyms, ant o-nyms, hyponyms, hypernyms, meronyms, and holonyms, as well as a number of  X  custom  X  rel a-tions observed in the corpora. A visual interface , the DiCoInfo v isuel (Robichaud 2012) allows users to view connections between the terms d e-scribed in the DiCoInfo . potential for explicitly describing a wide variety of relationships relevant in terminology, as well as a range of o ptions for making this information easily accessible to users, including increased access to a variety of contexts and options for various approaches to navigation within the r e-source, including a visual interface. 2.3 The CREATerminal prototype
Another in the list of relation -rich resources, but far less developed than those described above, is the CREATerminal prototype. In development since 2007, it aims to provide a useful resource for translators , built based on the content of pop-ularized, bilingual (Engli sh-French) documents in the field of breast cancer (e.g. Marshman and Van Bolderen 2009; Marshman, Gari  X py and Harms 2012) . T he information contained in the 
CREATerminal prototype was extracted from bilingual Canadian web sites ( e.g. Health Can a-da, the Canadian Cancer Society, and the Cana-dian Breast Cancer Foundation) . da tabase with three main tables: one has an entry for each of the approximately 85 concepts cov-ered in the resource, and links the terms ident i-fied fo r each concept with their equivalents in the other language; one includes approximately 250 bilingual contexts showing the terms and their equivalents in use, and the third presents a total of approximately 800 bilingual KRCs that illustrate terminological relations (generic-specific, part -whole, cause -effect and entity -function) that involve the concepts and include lexical relation markers. These KRCs are ann o-tated to identify the relationship present, the rel a-tion marker, the related items , and their sources. record form  X  X hich shows terms and equiv a-lents, and offers buttons to display exam ples and 
KRCs illustrating different relations  X  X r view complete list s of KRCs for each relation type or lexical re lation markers for the relations . neric queries that allow the user to search for specific character strings in term records, exa m-ples and KRCs . 
This pilot project focuses on the comparison of 
GDT by a sample of students in translation pr o-grams (B.A. and graduate programs) at the Un i-versity of Ottawa. These students were predominantly Anglophone and registered in courses that included a component of terminol o-gy and/or terminography. The students were first introduced to the concept and relevance of rel a-tions in the field of terminology in their courses and with an introductory in -class exercise, and to the CREATerminal model and how to consult and search it. (A ll had previously used both 
TERMIUM  X  Plus and the GDT i n their cours e-work and were assumed to be comfortable with their use.) They were then asked to carry out a translation task and invited to complete a n op-tional, anonymous online questionnaire summ a-rizing their experiences after class.
 short (1-3 sentence) excerpts of popularized texts on breast cancer. A mix of English to French and 
French to English translation was offered, and students were asked to try both (so that they would be translating both into and out of their 
L2 ). Students were asked to pay particular atten-tion to highlighted terms in the excerpts and to look them up in the three terminological r e-sources. All concepts corresponding to the hig h-lighted terms were described in at least two of the terminology resources used in the compar i-son, although occasionally term forms or terms themselves varied.
 cerpts as possible in a thirty -minute period. They then were invited to complete the questionnaire, delivered via the Survey Monkey interface. The first section of the questionnaire gathered general information on the respondents X  perceptions of several subject s: the resource s X  usefulness for understanding concepts in the excerpts and for writ ing about them; what the respondents found most and least useful about each resource; and which resources they would use again for a sim i-lar task. The second section (on a new page) ad-dressed terminological relations speci fically , and asked about students X  perceptions of how well terminological relations were described in each resource, as well as how useful the information about terminological relations in general was for understanding concepts and for writing about them . Respondents were also asked to evaluate the usefulness of individual record fields con-tai ning this relation -related information . Finally, the third section asked students to identify which fields they would consider useful in their own translation -oriented term records (i.e. whether they currently included them, planned to include them, would consider including them, or did not and would not include them). rating scale from 1 to 4, with 1 representing a negative evaluation (e.g.  X  X ot at all useful X  for questions about usefulness, and  X  X o not and will not include X  for questions about term record field s) and 4 representing a positive evaluation (e.g.  X  X ery useful X ,  X  X urrently include X ). Average scores were computed automatically by Survey Monkey based on these scales.
 consult /use  X  option was provided. Participants were also offered the option to list and evaluate additional resources they consulted.
 pate in the survey. A very high dropout rate of almost 50% after the first question suggests that many may have first accessed the questionnaire to familiarize themselves with its contents (as t he main questions could only be accessed after co n-senting to participate) , and either return ed later to com plete it or were dissuaded by the nature or length of the questionnaire. Of the 13 respond-ents who continued to the second question, 7 continued to t he final question. 3.1 Some l imitations of the methodology
An important limitation of this study is the small sample size and the high dropout rate. Important ethical considerations involved in the collection of data from students required great care to avoid coercion and ensure anonymity, which unfort u-nately limited opportunities to encourage partic i-pation and follow up with potential respondents (in addition to imposing significant restrictions on the general methodology) . Moreover, the na-ture of the sample itself should be taken into a c-count, as it consists of students from a single academic setting, and those most likely to partic-ipate were doubtless those who had a particular interest in terminology in general and termin o-logical relations in particular. necessarily restricted by time limitations and coverage limitations for the three banks, and the approach used to introduce variety by giving a choice of excerpts to translate (coupled with the survey -based metho dology) also made it impo s-sible to verify exactly which term records in each resource were consulted by each individual . survey -based methodology for data collection are also significant in themselves. We accessed only respondents X  perceptions of their experience, and thus were not able to objectively measure aspects of this experience, or to provide a fine -grained portrait of how the various resources were act u-ally used.
 en as purely indicative clues to help in identif y-ing key concerns in creating student -friendly, relation -rich terminology resources (and certai n-ly not as evaluations of the quality of any speci f-ic resource) . Given the limitations of the sample , no statistical evaluation of the data will be ca r-ried out beyond the comparison of average scores from multiple-choice questions and pe r-centages of respondents within the group.
In the first section of the questionnaire, respon d-ents were asked to evaluate and compare the use-fulness of the three resources for two main tasks: understanding concepts (i.e. decoding the source text) and writing about concepts (i.e. encoding the target text).
 ful ness of the three resources, the 13 respondents found all of the resources to be between  X  X airly useful X  and  X  X ery useful X  for understanding con-cepts: TERMIUM  X  Plus had the highest average score of 3.46 out of 4, followed by the GDT at 3.17 and finally the CREATerminal at 3.00. For writing about concepts, the scores showed a wi d-er rage and fell just slightly below  X  X airly useful X  into the range of  X  X omewhat useful X . In contrast to the previous ranking, the CREATerminal scored highest, with an average score of 3.36, score of 2.92 and finally the GDT at 2.73.
 for the two purposes most likely reflects the strengths of different types of data . Among the chief complaints were some gaps in information (e.g. of definitions, contexts and cooccurrents in with searching and display in all three resources (e.g. having to scroll down or through various 
Plus and the GDT, or having to work with one query at a time and to close tabs between searc h-es in the CREATerminal).
 each resource the coverage and variety of equiv alents included were valued. Among the stre ngths of TERMIUM  X  Plus , respondents cited broad cov erage of terms and concepts and incl u-sion of bilingual information X  X oth likely to a s-sist with understanding  X  X s well as ease of use and precise searching. The GDT X  X  strengths, as identified by the students, included the notes provided about usage, origin, etc. These might fulfill a decoding or an encoding function. Fina l-ly, the numerous, bilingual KRCs in the CR E-
ATerminal seemed most helpful for writing about concepts.
 the defining and the illustrating functions of te r-minology resources . This may represent an ex-ception to the general observation that translators tend to be most concerned with equivalents and less with definitions, perhaps because these are students working in a largely unfamiliar field  X  or because they were asked specifically abou t the understanding of concepts . questionnaire, 4 of the 7 respondents reported currently storing definitions on their term re c-ords , and 2 of the others reported planning to include them (an overall score of 3.43) . In co n-trast, none of the students reported currently stor-ing relation -related fields, although between 3 and 5 of the respondents (depending on the field) indicated that they would consider including them. The students seemed more likely to co n-sider including conventional term record fields (ranging from a score of 2.5 for phraseologisms to 3.29 for contexts and 3.43 for definitions ) than relation -related fields ( ranging from 1.5 for sources of terminological relations to 2.33 for a context illustrating the relationships).
 when asked about the usefulness of the different types of relations described in the CREATerm i-nal , the 9 respondents indicated that they were useful to varying degrees , with the highest ave r-age score (3.5 out of 4) for the generic -specific relation, followed by part -whole (3.2), entity -function (3.0) and finally cause -effect (2.8). 
When asked abou t specific elements of the ann o-tated KRCs that were helpful for understanding the concepts (excluding the terms themselves) , the highest -ranked fields were the example source ( with an average of 3.2) and the French example (3.17). T he other fields , except for the 
French relation marker (2.67) , scored 3.0, indi-cating that these elements were considered fairly useful. For writing about concepts, the English context explaining the relation was on average ranked most useful (3.75), followed by the Eng-lish lexic al relation marker (3.5) , the French co n-text (3.2) and the English related term/item (3.0) . 
All other field s scored below 3.0. The average score from 10 responses to the final question from the section indicated that the CREATerm i-nal provided the most useful information about terminological relations (with a score of 3.56 out of 4), followed by the GDT at 2.88 and finally observations above, in that information about terminological relations appears to be useful, but not very likely to be stored by students in their own records (perhaps because of the complexity and labour -intensiveness of the task ) and also unlikely to be thoroughly covered in conventio n-al termino logy resources . We thus see the need for  X  X hird -party X  terminology resources that do integrate this information to fill the gap for trai n-ee translators (and those with similar needs) . tions of users when asked whi ch resources they would use for a similar task again. Of the 12 r e-spondents to this question, 83% indicated that use the CREATerminal, and 50% would use the 
GDT. (It should nevertheless be noted that the respondents were mostly Anglophone and that the data suggest that they were paying particular attention to information for encoding in English, which is not the primary purpose of the GDT.)
This study has elicited some encouraging reac-tions from students, indicating that relation -enriched resources can meet some perceived needs in carrying out a translation task. From the literature and findings described above, we can observe a high priority accorded to equivalents (wh ich is not surpri sing) and to the understan d-ing of concepts (e.g. via definitions and KRCs) . 
This may well reflect the nature of the students X  experience, in their need to interpret concepts that are almost inevi tably unfamiliar (and cha l-lenging given the fact that the tra nslations were of excerpts and not whole texts, which would provide more information to help with interpret a-tion). There is also a positive evaluation of the usefulness of relation -related informati on X  particularly for writing  X  X s evidenced by the evaluation of the CREATerminal resource and the willingness to use it again.
 we can derive some preliminary, suggested guidelines for the creation of a relation -rich te r-minology resource that would meet the expect a-tions of the students:
Students X  reactions suggest that for this user group, the user -friendliness of resources is fu n-damental. Regardless of resources X  content (and coverage was highly valued by the respondents) , it seems that easy access to this information may be equally important . The inclusion of visual interfaces such as in the EcoLexicon and the D i-
CoInfo visu el  X  X articularly if these are smoothly integrated into an interface that also allows for easy consultation of textual material X  X re pro m-ising avenues for future development.
 students did find the relation information they consulted helpful, and seemed to be particularly drawn to it in the form of KRCs. This may be due to a focus on information that can be useful for writing about concepts as well as understan d-ing them . In any case, to satisfy the needs of this user group, it seems beneficial to include as wide a range of KRCs as possible (or practical) to take advantage of the dual function of these items (while nevertheless maintaining efficient integr a-tion and organization of the material to ensure easy navigation). Despite the time investment, advantages to including selected KRCs in a r e-source rather than offering (only) direct access to corpus data may include both speed and ease of access to information  X  X articularly for users who are new to the subject field in question and may need assistance for the first stages of r e-search  X  X s well as the ability to exploit the data they contain, e.g. for visual representation of re l-evant relationships . mation where available. A bilingual format is common to TERMIUM (which often includes definitions and contexts in both languages, usua l-ly from comparable resources) and the CR E-
ATerminal (which includes parallel bilingual contexts). As noted by Bowker (2011: 221 ), in spite of traditional terminology guidelines, tran s-lators increasingly tend to value translated sources (parallel corpora, translation memories ) and the rapidity and ease of use these info r-mation sources offer . Although the benefits of comparable corpora in terminology work are well established, it seems that the inclusion of complementary translated information can be an asse t in the eyes of the trainee translators. use of these resources in more detail and to better understand to what extent these preliminary guidelines are relevant , and why . It will be e s-sential to gather more da ta from a wider variety of users in order to identify more generalizable trends in requirements and preferences. A more in-depth study of the use of the resources by pa r-ticipants (e.g. using screen recording and inte r-views , or  X  X esources permitting  X  X sing ey e-tracking and keystroke logging tools to monitor users X  activity ) could allow us to obtain a more accurate and detailed picture of how students use such resources and their contents , and what fa c-tors they take into account in evaluations.
 sign, use and usefulness of student -friendly, rel a-tion -rich resources, we will be better able not only to produce richer and more useful tools but also to better train students to use and even cr e-ate them in the workplace . 
The author wishes to thank the organizations who granted permission to use their texts in the 
CREATerminal project, as well as the numerous research assistants at uOttawa who have contri b-uted to the project. Thanks are also extended to the Social Science and Humanities Research 
Council of Canada and the University of Ottawa and uOttawa Faculty of Arts for funding various stages of the project. 
Term extraction plays an important role in a wide range of applications including information retrieval (Yang et al., 2005), keyphrase extrac-tion (Lopez and Romary, 2010), information ex-traction (Yangarber et al., 2000), domain ontol-ogy construction (Kietz et al., 2000), text classi-fication (Basili et al., 2002), and knowledge min-ing (Mima et al., 2006). In many of these ap-plications the specificity level of a term is a rel-evant characteristic, but despite the large body of work in term extraction there are few methods that are able to identify general terms or intermediate level terms. Take for example the following struc-ture from the AGROVOC vocabulary 1 : resources  X  natural resources  X  mineral resources  X  lig-nite , where resources is an upper level term, natu-ral resources and mineral resources are intermedi-ate level terms, and lignite is a leaf. Intermediate level terms are specific to a domain but are broad enough to be usable for summarisation and clas-sification. Methods that make use of contrastive corpora to select domain specific terms favour the leaves of the hierarchy, and are less sensitive to generic terms that can be used in other domains.
Instead, we construct a domain model by iden-tifying upper level terms from a domain corpus.
This domain model is further used to measure the coherence of a candidate term within a domain.
The underlying assumption is that top level terms (e.g., resource ) can be used to extract intermedi-ate level terms, in our example natural resources and mineral resources . Our method for construct-ing a domain model is evaluated directly through an expert survey as well as indirectly based on its contribution to intermediate level term extraction.
While domain modelling is tested and exemplified with English, the ideas presented here are not lan-guage dependent and can be applied to other lan-guages, but this is outside the scope of this work.
We start by giving an overview of related work in term extraction in Section 1. Then, an approach to construct a domain model based on domain co-herence is proposed in Section 2, followed by a method to apply domain models for term extrac-tion. The experimental part of the paper starts with a direct evaluation of a domain model through a user survey (Section 3). A first set of experiments is carried in a standard setting for term evaluation, while the second set of experiments is application-driven, using corpora annotated for keyphrase ex-traction, information extraction, and information retrieval. We conclude this paper in Section 4, giv-ing a few directions for future work.
 1 Related work Methods for term extraction that use corpus statis-tics alone are faced with the challenge of distin-guishing general language expressions (e.g., last week ) from terminological expressions. A solu-tion to this problem is to use contrastive corpora (Huizhong, 1986). Several contrastive measures are proposed including domain relevance (Park et al., 2002), domain consensus (Velardi et al., 2001), and word impurity (Liu et al., 2005). In this work we propose an approach to compute do-main specificity based on a domain model, that is less sensitive to leaf terms and is better suited for intermediate level terms.

The domain model proposed in this work is de-rived from the corpus itself, without the need for external corpora. An automatic method for iden-tifying the upper level terms of a domain has ap-plications beyond the task of term extraction. Al-though not named as such, upper level terms were previously used for text summarisation (Teufel and Moens, 2002). The authors manually identi-fied a set of 37 nouns including theory , method , prototype and algorithm , without considering a principled approach to extract them. The work presented here is similar to (Barri ` ere, 2007), but instead of re-ranking terms based on their similar-ity to each other we make use of domain model terms, reducing data sparsity issues.

In our experiments we employ two state of the art methods for term extraction, the NC-value ap-proach (Frantzi et al., 2000) and TermExtractor 2 (Velardi et al., 2001). The former is a hybrid method that ranks terms using only corpus statis-tics, while the latter exploits contrastive corpora. NC-value is based on raw frequency counts and considers nested multi-word terms by penalising frequency counts of shorter embedded terms. Ad-ditionally, it incorporates context information in a re-ranking step using top ranked terms. Con-text words (nouns, verbs and adjectives) are identi-fied based on their occurrence with top candidates. Our method is an extension of this approach that uses domain models instead of selecting context words based on frequency alone.

TermExtractor is a popular approach that com-bines different term extraction techniques includ-ing domain relevance, domain consensus and lex-ical cohesion. Domain Relevance ( DR ) compares the probability of a term t in a given domain D i with the maximum probability of the term in other domains used for contrast D j and is measured as:
Domain Consensus ( DC ) identifies terms that have an even probability distribution across the corpus that represents a domain of interest, and is estimated through entropy as follows: where d is a document in the domain D i . Fi-nally, the degree of cohesion among the words w j that compose the term t is computed through a measure called Lexical Cohesion ( LC ). Let | t | be the length of t in number of words, and f ( t,D i ) the frequency of t in the domain D i , then Lexical Cohesion is defined as:
The weight TE used for ranking terms by Ter-mExtractor is a linear combination of the three methods described above: TE ( t,D i ) =  X   X  DR +  X   X  DC +  X   X  LC (4)
While general terms typically have a high do-main consensus, the domain relevance measure boosts narrow terms that have limited usage out-side of the domain. For example the term system is not identified as relevant for Computer Science be-cause it is frequently used in general language and in other specific domains as biology. In this work we take a different approach to compute domain specificity that can be applied for general terms by using a domain coherence measure that does not use external corpora. Two general purpose cor-pora, the Open American National Corpus 3 and a corpus of books from Project Gutenberg 4 , are used as contrastive corpora for our implementa-tion of TermExtractor. The books selected from Project Gutenberg include the bible, the complete works of William Shakespeare, James Joyce X  X  Ulysses and Tolstoy X  X  War and Peace . We con-sider only the default setting of TermExtractor as-signing equal weights to each measure in Equation 4. 2 Constructing a domain model based on We begin this section by describing an approach for domain modelling based on domain coherence in Section 2.1. Then, we discuss a modification of the NC-value approach which makes it better suited for intermediate level terms (Section 2.2). We conclude this section by describing a novel method for term extraction using a domain model in Section 2.3. 2.1 Domain modelling A domain model is represented as a vector of words which contribute to determine the domain of the whole corpus. Let  X  be the domain model, and w 1 to w n a set of generic words, specific to the domain, then:
The number of words n can be empirically set according to a cutoff associated weight. Previous work on using domain information for word sense disambiguation (Magnini et al., 2002) has shown that only about 21% of the words in a text actu-ally carry information about the prevalent domain of the whole text, and that nouns have the most significant contribution (79.4%). Several assump-tions are made to identify words that are used to construct a domain model from a domain corpus: 1. Distribution: Generic words should appear 2. Length: Only single-word candidates are 3. Content: Only content-bearing words are of 4. Semantic Relatedness: A term is more gen-The distribution assumption implies that rare terms are more specific, similar with the frequency-based measure previously used for measuring tag generality (Benz et al., 2011). This might not always be the case, for example a sim-ple search with a search engine shows that arte-fact or silverware are more rarely used than the term spoon , although the first two concepts are more generic. However, in this work we are in-terested in extracting basic-level categories as the-orised in psychology (Hajibayova, 2013). A basic-level category is the preferred level of naming, that is the taxonomical level at which categories are most cognitively efficient. A counter example can be found for the length assumption as well, as the longer term inorganic matter is more general than the single word knife , but in this case we would simply consider as a candidate the single word matter which is more generic than the compound term. Both length and frequency of occurrence are proposed as general criteria for identifying basic-level categories (Green, 2005).

The first three assumptions are used for can-didate selection, while the fourth assumption is used to filter the candidates. A possible solution for building a domain model is to use a standard termhood measure for single-word terms. Most approaches for extracting single-word terms make use of contrastive corpora, ranking higher specific words that are rarely used outside of the domain.
But our domain model is further used for term extraction, therefore it is important that we use generic words to insure a high recall.

We interpret coherence as semantic relatedness to quantify the coherence of a term in a do-main. The measure used for semantic relatedness is Pointwise Mutual Information (PMI). First, we extract multi-word terms using a standard term extraction technique, then we use the top ranked terms to filter candidate words using the following scoring function for domain coherence: where  X  is the domain model candidate,  X  is top ranked multi-word term,  X  is the set of top ranked multi-word terms and P (  X , X  ) is the probability that the word  X  appears in the context of the term  X  . In our implementation, the set  X  contains the best terms extracted by our baseline term extrac-tion method described in Section 2.2, but any other term extraction method can be applied in this step. A small sample from domain models extracted us-63 ing our domain coherence method for Computer Science, Food and Agriculture, and the Biomedi-cal Domain, is shown in Table 1. 2.2 Baseline term extraction method Our baseline approach for intermediate level term extraction is frequency-based, similar to the C-value method (Ananiadou, 1994), but we mod-ify its ranking function. The main difference is the way we take into consideration embedded terms. In previous work, this information is used to decrease frequency counts, as shorter terms are counted both when they appear by themselves and when they are embedded in a longer term. We ar-gue that the number of longer terms that embed a term can be used as a termhood measure. In our experiments, this measure only works for embed-ded multi-word terms, as single-word terms are too ambiguous. The baseline scoring method b is defined as: where  X  is the candidate string, |  X  | is the length of  X  , f is its frequency in the corpus, and e  X  is the number of terms that embed the candidate string  X  . The parameter  X  is used to linearly combine the embeddedness weight and is empirically set to 3.5 in our experiments. 2.3 Using domain coherence for term Although we proposed a method to build a do-main model in Section 2.1, the question of how to use this domain model in a termhood measure remains unanswered. Again, the solution is to rely on the notion of domain coherence, which is defined in this case as the semantic relatedness between a candidate term and the domain model described above. The assumption is that a cor-rect term should have a high semantic relatedness with representative words from the domain. This method favours more generic candidates than con-trastive corpora approaches, therefore it is better suited for extracting intermediate level terms.
The same measure of semantic relatedness is used as for the domain model, the PMI measure.
The domain coherence DC of a candidate string  X  is defined as follows: where  X  is a word from the domain model, and  X  is the domain model constructed using Equation 6. Using generic terms to build the domain model is crucial for ensuring a high recall as these words are more frequently used across the corpus. In our implementation context is defined as a window of 5 words. 3 Experiments and Results
Evaluating term extraction results across domains is a challenge, because finding domain experts is difficult for more than one domain. An al-ternative is to reuse datasets annotated for appli-cations where term extraction plays an important role, for example, keyphrase extraction or index term assignment. Three technical domain cor-pora are used in our experiments: Krapivin , a cor-pus of scientific publications in Computer Science (Krapivin et al., 2009); GENIA , a corpus of ab-stracts from the biomedical domain (Ohta et al., 2001); and FAO , a corpus of reports about Food and Agriculture (Medelyan and Witten, 2008) col-lected from the website of the Food and Agricul-ture Organization of the United Nations 5 . The
Krapivin corpus provides author and reviewer as-signed keyphrases for each publication. The GE-
NIA corpus is exhaustively annotated with biomed terms, with about 35% of all noun phrases anno-tated as biomed terms. The FAO dataset provides index terms assigned to each document by profes-sional indexers. It is not only the document size that varies considerably across these three cor-pora, but also the number of annotations assigned to each document as can be seen in Table 2.

We evaluate our measure for building a domain model in Computer Science, by identifying a list of general words with the help of a domain ex-pert in Section 3.1. We envision two sets of ex-periments: a standard term extraction evaluation where the top ranked terms are evaluated against the list of unique annotations provided in the eval-uation datasets (Section 3.2.1), and a second set of experiments where each term extraction approach is used to assign candidates to documents in com-bination with a document relevance measure in Section 3.2.2. 3.1 Intrinsic evaluation of a domain model A domain expert was asked to investigate nouns used in the ACM Computing Classification Sys-tem 6 . The expert was provided with the list of nouns and their frequency in the taxonomy and was required to identify nouns that refer to generic concepts. A set of 80 nouns were selected in this manner including system , information , and soft-ware . Only one annotator was involved because of the complexity of the task, that implies the analy-sis and filtering of several hundred words. We esti-mate the inter-annotator agreement by analysing a subset of the selected words through a survey with 27 participants. A quarter of the selected words are combined with the same number of randomly selected rejected words and the resulting list is sorted alphabetically. The Fleiss kappa statistic for interrater agreement is 0.34, lying in the fair agreement range. 80% of the words from our gold standard domain model were selected by at least half of the participants.

We compare our method ( DC ) with two other benchmarks, the contrastive termhood measure used in TermExtractor, and the frequency-based method used by NC-value to select context words (
NCV weight ). Again, context is defined as a window of 5 words. A domain model has many similarities with probabilistic topic modelling, al-though it provides less structure. We compare our approach with a popular approach to topic mod-elling, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We experimented with different num-bers of topics but we report only the best results achieved for 75 topics ( LDA 75 ).

The results of this experiment are shown in Fig-ure 1, in terms of F-score. Several conclusions can be drawn from this experiment. First, the methods that analyse the context of top ranked terms (i.e., our domain coherence measure, DC , and the weight used for context words in the NC-value, w NCV ) perform better than the contrastive measure used in TermExtractor, with statistically significant gains. Also, our domain coherence method outperforms the more simple frequency-based weight used in NC-value, although this re-sult is not statistically significant. As expected, the words ranked high by TermExtractor are too specific for a generic domain model. The topic modelling approach identifies several words from the gold standard but much less than our approach and these are evenly distributed across latent top-ics. These conclusions will be further investigated across two other domains, using gold standard terms annotated for three different applications in
Section 3. 3.2 Term extraction evaluation results
We implement and compare the baseline method presented in Section 2.2 and the method based on domain coherence described in Section 2.3, against the NC-value and TermExtractor methods, which are used as benchmarks. The same candi-date selection method is used for all the evaluated approaches. Candidate terms are selected through syntactic analysis by defining a syntactic pattern for noun phrases. To assure the results are compa-rable, the same number of context words is used 65 in our implementation of the NC-value approach as the size of the domain model. Two general pur-pose corpora, the Open American National Cor-pus 7 and a corpus of books from Project Guten-berg 8 , are used as contrastive corpora for our im-plementation of TermExtractor. We considered only the default setting for TermExtractor, assign-ing equal weights to each measure. 3.2.1 Standard term extraction evaluation
While keyphrases and index terms suit well our purposes, as they are terms of an intermediate level of specificity, meant to summarise or clas-sify documents, many of the terms annotated in GENIA are too specific. We discard the annotated terms that are mentioned in less than 1% of the documents from corpus, based on our distribution assumption. For each of the three datasets, the top ten thousand ranked terms were evaluated. We in-crementally analysed portions of the ranked lists computed using the baseline approach ( Baseline ), the baseline approach linearly combined with the domain coherence measure ( Baseline+DC ), and the two benchmarks, NC-value and TermExtrac-tor . The precision value for a portion of the list is scaled against the overall number of candidates considered. First, we observe that all methods perform better on the GENIA (Figure 4) and the Krapivin corpus (Figure 2), with the best methods achieving a maximum precision close to 60% at the top of the ranked list.

The Food and Agriculture use case is more chal-lenging, as the best method achieves a precision of less than 20%, as can be seen in Figure 3. Also, the contrastive corpora measure employed in
TermExtractor yields considerably worse results on all three domains, because the extracted terms are too specific. The baseline method, that re-wards embedded terms, outperforms the NC-value method on the Computer Science domain, and in the biomedical domain, but it performs slightly worse on the Agriculture domain. The combina-tion of our baseline method with the domain co-herence measure (referred to as Baseline + DC in the legend) yields the most stable behaviour, out-performing all other measures across the three do-mains, considerably so in the biomedical domain (Figure 4) and at the top of the ranked list in Com-puter Science (Figure 2). In Computer Science, domain coherence significantly outperforms the best performing state-of-the-art method, NC-value (Figure 2). In Biomedicine, the improvement is statistically significant, with a gain of 106% at top 20% of the list (Figure 4). 3.2.2 Application-based evaluation
An important reason for developing termhood measures is that they are needed in specific ap-plications, for example keyphrase extraction and index term extraction. Typically, a termhood mea-sure is combined with different measures of docu-ment relevance in such applications, as the candi-dates are assigned at the document level. We make use of the standard information retrieval measure
TF-IDF in combination with the considered term extraction scoring functions to assign terms to documents. The best results are obtained by us-ing domain coherence as a post-processing step.
In this experiment, the PostRankDC approach was computed by re-ranking the top 30 candidates se-lected using our baseline approach described in 66 Equation 7, based on their domain coherence.

The application-based evaluation proposed in this work allows us to evaluate both precision and recall, and consequently F-score can be used as an evaluation metric. The results for keyphrase extraction in Computer Science are presented in Table 3, while the results for index term extrac-tion in the Agriculture domain are shown in Ta-ble 4. The results for document level term extrac-tion from the Biomed corpus appear in Table 5. All three methods yield a higher performance on the GENIA corpus. The results on the Agricul-ture corpus are again the lowest, because a larger number of candidates has to be analysed.

Our Baseline method outperforms the NC-value approach on the Krapivin corpus and on the GENIA corpus, but not on the FAO corpus. We can observe that the domain coherence approach (
PostRankDC ) improves over our baseline ap-proach ( Baseline ) on all three domains. The im-provement is statistically significant compared to the best state-of-the-art method in Computer Sci-ence, NC-value. NC-value outperforms TermEx-tractor in Computer Science and Agriculture, but TermExtractor performs better in Biomedicine. Although both NC-value and TermExtractor make use of domain-independent features for ranking, their performance varies across domains and ap-plications. At the same time, combining our domain coherence approach ( PostRankDC ) with our baseline method in a post-ranking step dis-plays a more stable behaviour, achieving the best performance on the Computer Science domain (Krapivin) and similar results with the results of the best method in Biomedicine (GENIA) and Agriculture (FAO). 4 Conclusions In this study, we proposed an approach to iden-tify intermediate level terms through domain mod-elling and a novel domain coherence measure, ar-guing that approaches that make use of contrastive corpora are only suitable for updating existing ter-minology resources with more specific terms and not for summarisation or classification tasks. The contributions described in this work are three-fold:
Experiments discussed in this paper show that term extraction performance depends on the do-main, although systems make use of domain-independent features. Our domain coherence ap-proach based on a domain model performs well across domains, while the performance of the
NC-value and TermExtractor benchmarks is more domain-dependent. The results lead to the conclu-sion that using a domain model is more appropri-ate than using statistical approaches based on con-trastive corpora, for extracting intermediate level terms. Future work will include an unsupervised learning-to-rank approach for term extraction, that will allow a more principled integration of domain coherence measures with standard term extraction features. The method proposed here can be used as a specificity measure, and we currently investi-gate this in the context of constructing generalisa-tion hierarchies of concepts.
 Acknowledgments This work has been funded in part by the European
Union under Grant No. 258191 for the PROMISE 67 project, as well as by a research grant from Sci-ence Foundation Ireland (SFI) under Grant Num-ber SFI/12/RC/2289.
 References
