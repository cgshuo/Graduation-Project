
It has long been understood that even when confronted with a ten-gigabyte file containing data to be statisti-cally analyzed, the actual information-theoretic amount haps merely a few hundred megabytes. This insight is currently most commonly used by data analysts to take high-dimensional real-valued datasets and reduce their dimensionality using principal components analy-sis, with little loss of meaningful information. This can turn an apparently intractably large data mining prob-lem into an easy problem. PCA is applicable to real-valued data, and is usually a lossy form of compression: some information is lost in the transformation. categorical (i.e. symbolic) values, and about using 
Bayesian network learning to discover interrelationships that allow very aggressive compression of the data. Pur-thermore, this compression can be completely lossless. number between 0 and 1. The arithmetic encoder begins with a range R = [0, 1). As each symbol in the source sequence is encoded, the current range R is subdivided into a partitions, where a is the number of possible values the symbol could have taken on; the size of each of these partitions is proportional to the probability of symbol taking on the corresponding value. The current range is then restricted to the partition corresponding to the source symbol being encoded. Once all symbols have been processed in this manner, the encoder outputs the binary representation of a number within the final subrange, to a sufficient precision to disambiguate the number from all numbers outside of this subrange. 
Arithmetic coding achieves asymptotically optimal compression performance as the number of symbols in the encoded block tends to infinity, assuming that the model probability distributions used for encoding per-fectly reflect the probability distributions inherent in the data. As described above, arithmetic coding ap pears to require the use of arbitrary-precision arithmetic operations in order to manipulate the current range R; However, it is possible to use limited-precision integer arithmetic to approximate  X  X erfect X  arithmetic coding with only a small loss in compression performance. 
Both Huffman encoding and arithmetic coding re-quire probabilistic models of the data they encode. We now discuss a class of probabilistic models particularly well-suited for modelling probability distributions over categorical datasets. Suppose we have a dataset D in which each record variables X1, . . . , X,. Now suppose we wish to model D with a probability distribution P(Xi, X2,. . . , Xn) that we may use to calculate the probability that a randomly selected record I from D will have any specific value ~1 assigned to X1, any specific value 212 assigned to X2, and so forth. Naturally, the most accurate  X  X odel X  of D would be an enormous lookup table specifying how many records in D have any given set of assignments of values to X1 through XN; however, such a model would typically be useless for compression, since it would usually require as much space as D itself. What kinds of probabilistic models might be useful for compression? 
Bayesian networks [ 121, also commonly known as belief networks, are a popular class of probabilistic models that work well in conjunction with compression, although they are primarily used for data analysis and decision-making. A Bayesian network consists of a directed acyclic graph (or  X  X AG X ) in which each vertex corresponds to a variable, plus a probability distribution P(Xi IIIx, ) for each variable Xi, where IIxi is the set of X:s parents in the DAG. Given such 
The second algorithm takes two sweeps through the dataset. In the first sweep, the mutual information between all pairs of variables is computed. Using this information, as well as BIC-like penalty terms for the number of parameters required, the algorithm uses a greedy heuristic algorithm to add arcs to the empty Bayesian network in order to arrive at a network in which each node has at most c parents. A second sweep is then made over the dataset to fill in the probability tables of the resulting network. See [4] for further details. This algorithm is somewhat similar to an algorithm previously used by Sahami for classification [14]. In the special case where c is 1, this algorithm reduces to a penalized version of Chow and Liu X  X  dependency-tree algorithm [3], and is provably optimal. While the network chosen by this greedy algorithm won X  X  generally be as accurate as one found via a more thorough search, this algorithm has the advantage of being more computationally feasible on datasets with many records or attributes. In applications in which we will only wish to scan through the dataset sequentially, we can take advantage of potential correlations between the ith record and i + lth record to obtain further compression. We use Dynamic Bayesian Networks [5] to seek out and learn such correlations and exploit them in a tighter encoding. These networks are learned with a greedy algorithm similar to the greedy Bayesian network-learning algorithm described in the previous section. Even more aggressive compression can be obtained when we are performing a data mining task that is indifferent to the order in which records are presented. In that case we can deliberately pre-sort the records to induce inter-record correlations where none existed originally and save even more space. Again, see [4] for further details. In this section, we examine the effectiveness of learning Bayesian networks in order to perform compression with arithmetic coding on real datasets. In conjunction with the Bayesian network learning algorithms discussed above, we used a limited-precision arithmetic coding library written by Carpinelli et al. [I] based on a paper by Moffat et al. [9]. network -referred to hereafter as a Huffman network for convenience -in which each node actually models a group of variables in the dataset rather than an individual variable. Each group of variables is Huffman coded as a single unit. For example, if a group contains three binary variables, then that group is Huffman coded as if it were a single variable taking on eight possible values; each of these eight values is assigned a probability equal to the joint probability of the corresponding combination of values for the original three binary variables. between variables residing in different groups, we allow the probability distribution over the possible values for each group to be conditioned on the values of other variables. For example, in Figure lA, six variables have been placed into three groups. The joint probability distribution of all the variables in Group 3 (namely, variables 22 and xs) is conditioned on the values of variables x3, x4, and 2s. This conditioning is represented in the graph by arcs from x3, x4, and x5 to Group 3. Assuming all the variables are binary, this means that Group 3 requires eight Huffman trees -one for each possible combination of values to x3, x4 and x5. 
Each of these trees then has four leaves -one for each possible combination of 22 and 2s. Note, however, that 
Group 3 is not conditioned on the value of x1, despite the fact that xi is in the same group as x4 and 25. This added flexibility can help in certain situtations -for example, if 12 and 2s are independent of x1 given x4 and x5, then conditioning Group 3 on the value of xi would double the number of Huffmann trees required by Group 3 without increasing Group 3 X  X  coding efficiency. 
Bayesian network over the original variables in which all variables in the same group are completely connected (e.g., Figure 1B). This representation tells us what dependencies between variables are being modelled by the coding scheme associated with the Huffman network. At the same time, the Huffman network can be thought of as a Bayesian network over the groups themselves (e.g., Figure lC), where an arc exists from group G to group G X  if and only if an arc exists from at least one variable in G to the group G X  in the Huffman network. This view summarizes how the coding groups in the Huffman network are connected, thus telling us which groups of variables need to be decoded before other groups can be decoded. pression as follows. First, we take one pass through the dataset to compute contingency tables for each of the groups in the network. The contingency ta-ble for a given group with a set of variables V and set of conditioning variables P counts how many times Bayesian networks 
As one might expect, the H&amp;man-based coding performed slightly (1 to 8%) worse than arithmetic coding in terms of file size, since arithmetic coding does not have to output integral numbers of bits for individual variables or groups of variables as the Huffman-based coding does. However, the Huffman-based decoding was two to four times faster than arithmetic decoding, and still achieved significantly smaller files than those achieved by the dictionary-based approaches (see section 4). While this is still much slower than the decoding speed of gzip and bzip2, there are further optimizations to be performed that will be examined in [4]. Automatically-learned Bayesian networks have been used previously in conjunction with arithmetic encoding in recent research by Brendan Frey [6]. Rather than learning the structure of the Bayesian network, Frey uses a fixed network structure in which each node has many parents; the probability of each node given its parents is paramaterized using logistic regression. This approach has disadvantages compared to the approaches we describe here, although it has some advantages as well. In particular, Frey X  X  approach may be more difficult to apply to nonbinary datasets. Additionally, Frey X  X  use of hidden variables necessitates the use of a complex  X  X its-back X  coding scheme in order to achieve decent compression rates, and this scheme may prevent fine-grained random access to the data. Comparisons with Frey X  X  approach will be performed in future research. This paper has proposed that Bayesian networks, in addition to their other virtues, may be an important tool for dataset compression. Experimental results show that excellent compression ratios are obtainable with arithmetic coding in conjunction with Bayesian networks. The Huffman networks introduced here provide nearly the same compression ratios, but with significantly less computational time. 
