 The minimum description length (MDL) principle recommends to use, among competing models, more regularity has been detected, hence the better will predictions be. The MDL principle can be regarded as a formalization of Ockham X  X  razor, which says to select the simplest model consistent with the data.
 Multistep lookahead sequential prediction. We consider sequential prediction problems, i.e. hav-x ` +1  X  X  for `  X  ` ( x ) = 0 , 1 , 2 ,... . Classical prediction is concerned with h = 1 , multi-step looka-head with 1 &lt;h&lt;  X  , and total prediction with h =  X  . In this paper we consider the last, hardest case. An infamous problem in this category is the Black raven paradox [Mah04, Hut07]: Having observed ` black ravens, what is the likelihood that all ravens are black. A more computer science problem is (infinite horizon) reinforcement learning, where predicting the infinite future is necessary for evaluating a policy. See Section 6 for these and other applications.
 Discrete MDL. Let M = { Q 1 ,Q 2 ,... } be a countable class of models =theories=hypotheses= probabilities over sequences X  X  , sorted w.r.t. to their complexity=codelength K ( Q i ) = 2log 2 i (say), containing the unknown true sampling distribution P . Our main result will be for arbitrary measur-able spaces X , but to keep things simple in the introduction, let us illustrate MDL for finite X . In this case, we define Q i ( x ) as the Q i -probability of data sequence x  X  X  ` . It is possible to code x in log P ( x )  X  1 bits, e.g. by using Huffman coding. Since x is sampled from P , this code is optimal (shortest among all prefix codes). Since we do not know P , we could select the Q  X  X  that leads to the shortest code on the observed data x . In order to be able to reconstruct x from the code we need to know which Q has been chosen, so we also need to code Q , which takes K ( Q ) bits. Hence x can be coded in min Q  X  X  { X  log Q ( x )+ K ( Q ) } bits. MDL selects as model the minimizer Main result. Given x , the true predictive probability of some  X  X uture X  event A is P [ A | x ] , e.g. A could be x ` +1: ` + h or any other measurable set of sequences (see Section 3 for proper definitions). We consider the sequence of predictive measures MDL x [  X | x ] for ` = 0 , 1 , 2 ,... selected by MDL. Our main result is that (see Theorem 1). The analogous result for Bayesian prediction is well-known, and an immediate corollary of Blackwell&amp;Dubin X  X  celebrated merging-of-opinions theorem [BD62]. Our primary con-tribution is to prove the analogous result for MDL. A priori it is not obvious that it holds at all, and indeed the proof turns out to be much more complex.
 Motivation. The results above hold for completely arbitrary countable model classes M . No inde-pendence, ergodicity, stationarity, identifiability, or other assumption need to be made. The bulk of previous results for MDL are for continuous model classes [Gr  X  u07]. Much has been shown for classes of independent identically distributed (i.i.d.) random variables [BC91, Gr  X  u07]. Many results naturally generalize to stationary-ergodic sequences like ( k th-order) Markov. For in-stance, asymptotic consistency has been shown in [Bar85]. There are many applications violating these assumptions, some of them are presented below and in Section 6. For MDL to work, P needs to be in M or at least close to some Q  X  X  , and there are interesting environments that are not even close to being stationary-ergodic or i.i.d.
 Non-i.i.d. data is pervasive [AHRU09]; it includes all time-series prediction problems like weather forecasting and stock market prediction [CBL06]. Indeed, these are also perfect examples of non-ergodic processes. Too much green house gases, a massive volcanic eruption, an asteroid impact, or another world war could change the climate/economy irreversibly. Life is also not ergodic; one inattentive second in a car can have irreversible consequences. Also stationarity is easily violated in multi-agent scenarios: An environment which itself contains a learning agent is non-stationary (during the relevant learning phase). Extensive games and multi-agent reinforcement learning are classical examples [WR04].
 Often it is assumed that the true distribution can be uniquely identified asymptotically. For non-ergodic environments, asymptotic distinguishability can depend on the realized observations, which prevent a prior reduction or partitioning of M . Even if principally possible, it can be practically burdensome to do so, e.g. in the presence of approximate symmetries. Indeed this problem is the primary reason for considering predictive MDL. MDL might never identify the true distribution, but our main result shows that the sequentially selected models become predictively indistinguishable. For arbitrary countable model classes, the following results are known: The MDL one-step lookahead predictor (i.e. h = 1 ) of three variants of MDL converges to the true predictive distribution. The proof technique used in [PH05] is inherently limited to finite h . Another general consistency result reference to [BC91] contains only results for i.i.d. sequences which do not generalize to arbitrary merging-of-opinions result.
 The countability of M is the severest restriction of our result. Nevertheless the countable case is useful. A semi-parametric problem class S  X  d =1 M d with M d = { Q  X ,d :  X   X  IR d } (say) can be reduced to a countable class M = { P d } for which our result holds, where P d is a Bayes or NML or other estimate of M d [Gr  X  u07]. Alternatively, S d M d could be reduced to a countable class by considering only computable parameters  X  . Essentially all interesting model classes contain such a countable topologically dense subset. Under certain circumstances MDL still works for the non-computable parameters [Gr  X  u07]. Alternatively one may simply reject non-computable parameters on philosophical grounds [Hut05]. Finally, the techniques for the countable case might aid proving general results for continuous M , possibly along the lines of [Rya09].
 Contents. The paper is organized as follows: In Section 2 we provide some insights how MDL works in restricted settings, what breaks down for general countable M , and how to circumvent the problems. The formal development starts with Section 3, which introduces notation and our main result. The proof for finite M is presented in Section 4 and for denumerable M in Section 5. In Section 6 we show how the result can be applied to sequence prediction, classification and regression, discriminative learning, and reinforcement learning. Section 7 discusses some MDL variations. Before starting with the formal development, we describe how MDL works in some restricted set-tings, what breaks down for general countable M , and how to circumvent the problems. For deter-ministic environments, MDL reduces to learning by elimination, and results can easily be understood. Consistency of MDL for i.i.d. (and stationary-ergodic) sources is also intelligible. For general M , MDL may no longer converge to the true model. We have to give up the idea of model identification, and concentrate on predictive performance.
 Deterministic MDL = elimination learning. For a countable class M = { Q 1 ,Q 2 ,... } of de-terministic theories=models=hypotheses=sequences, sorted w.r.t. to their complexity=codelength K ( Q i ) = 2log 2 i (say) it is easy to see why MDL works: Each Q is a model for one infinite se-Q consistent with x P 1: ` and for h = 1 predicts x Q ` +1 . This (and potentially other) Q becomes (forever) inconsistent if and only if the prediction was wrong. Assume the true model is P = Q m . Since elimination occurs in order of increasing index i , and Q m never makes any error, MDL makes at most m  X  1 prediction errors. Indeed, what we have described is just classical Gold style learning h wrong predictions before the error is revealed. (Note that at time ` only x P ` is revealed.) Hence the total number of errors is bounded by h  X  ( m  X  1) . The bound is for instance attained on the class consisting of Q i = 1 ih 0  X  , and the true sequence switches from 1 to 0 after having observed m  X  h ones. For h =  X  , a wrong prediction gets eventually revealed. Hence each wrong Q i ( i&lt;m ) gets eventually eliminated, i.e. P gets eventually selected. So for h =  X  we can (still/only) show that the number of errors is finite. No bound on the number of errors in terms of m only is possible. For instance, for M = { Q 1 = 1  X  ,Q 2 = P = 1 n 0  X  } , it takes n time steps to reveal that prediction 1  X  is wrong, and n can be chosen arbitrarily large.
 Comparison of deterministic  X  probabilistic and MDL  X  Bayes. The flavor of results carries over to some extent to the probabilistic case. On a very abstract level even the line of reasoning carries over, although this is deeply buried in the sophisticated mathematical analysis of the latter. So the special deterministic case illustrates the more complex probabilistic case. The differences are as follows: In the probabilistic case, the true P can in general not be identified anymore. Further, while the Bayesian bound trivially follows from the 1/2-century old classical merging of opinions result [BD62], the corresponding MDL bound we prove in this paper is more difficult to obtain. Consistency of MDL for stationary-ergodic sources. For an i.i.d. class M , the law of large num-P x 1 P ( x 1 )log[ P ( x 1 ) /Q ( x 1 )] with P -probability 1. Either the Kullback-Leibler (KL) divergence is  X  , i.e. asymptotically MDL does not select Q . For countable M , a refinement of this argument shows that MDL eventually selects P [BC91]. This reasoning can be extended to stationary-ergodic M , but essentially not beyond. To see where the limitation comes from, we present some troubling examples.
 Trouble makers. For instance, let P be a Bernoulli (  X  0 ) process, but let the Q -probability that x = 1 be  X  t , i.e. time-dependent (still assuming independence). For a suitably converging but  X  X s-cillating X  (i.e. infinitely often larger and smaller than its limit) sequence  X  t  X   X  0 one can show that stationary distributions for which MDL does not converge (not even to a wrong distribution). One idea to solve this problem is to partition M , where two distributions are in the same partition if and only if they are asymptotically indistinguishable (like P and Q above), and then ask MDL to only identify a partition. This approach cannot succeed generally, whatever particular criterion is tinguishable, e.g. P = Q on the remainder of the sequence. For x 1 = 0 , let P and Q be asymptotically distinguishable distributions, e.g. different Bernoullis. This shows that for non-ergodic sources like this one, asymptotic distinguishability depends on the drawn sequence. The first observation can lead to totally different futures.
 Predictive MDL avoids trouble. The Bayesian posterior does not need to converge to a single (true or other) distribution, in order for prediction to work. We can do something similar for MDL. At each time we still select a single distribution, but give up the idea of identifying a single distribu-tion asymptotically. We just measure predictive success, and accept infinite oscillations. That X  X  the approach taken in this paper. The formal development starts with this section. We need probability measures and filters for infinite sequences, conditional probabilities and densities, the total variation distance, and the concept of merging (of opinions), in order to formally state our main result.
 Measures on sequences. Let ( X  := X  X  , F ,P ) be the space of infinite sequences with natural fil-tration and product  X  -field F and probability measure P . Let  X   X   X  be an infinite sequence sam-pled from the true measure P . Except when mentioned otherwise, all probability statements and expectations refer to P , e.g. almost surely (a.s.) and with probability 1 (w.p.1) are short for with P -probability 1 (w. P .p.1). Let x = x 1: ` =  X  1: ` be the first ` symbols of  X  .
 For countable X , the probability that an infinite sequence starts with x is P ( x ) := P [ { x } X X  X  ] . The w.p.1. For other probability measures Q on  X  , we define Q ( x ) and Q [ A | x ] analogously. General X are considered at the end of this section.
 Convergence in total variation. P is said to be absolutely continuous relative to Q , written P and Q are said to be mutually singular , written P  X  Q , iff there exists an A  X  X  for which P [ A ] = 1 and Q [ A ] = 0 . The total variation distance (tvd) between Q and P given x is defined as Q is said to predict P in tvd (or merge with P ) if d ( P,Q | x )  X  0 for ` ( x )  X  X  X  with P -probability 1. Note that this in particular implies, but is stronger than one-step predictive on-and off-sequence [KL94]. The famous Blackwell and Dubins convergence result [BD62] states that if P is absolutely continuous relative to Q , then (and only then [KL94]) Q merges with P : Bayesian prediction. This result can immediately be utilized for Bayesian prediction. Let M := { Q 1 ,Q 2 ,Q 3 ,... } be a countable (finite or infinite) class of probability measures, and Bayes [ A ] := P
Q  X  X  Q [ A ] w Q with w Q &gt; 0  X  Q and P Q  X  X  w Q = 1 . If the model assumption P  X  X  holds, then obviously P Bayes, hence Bayes merges with P , i.e. d ( P, Bayes | x )  X  0 w.p.1 for all P  X  X  . Unlike many other Bayesian convergence and consistency theorems, no (independence, ergodicity, stationarity, identifiability, or other) assumption on the model class M need to be made. The analo-gous result for MDL is as follows: Theorem 1 (MDL predictions) Let M be a countable class of probability measures on X  X  con-taining the unknown true sampling distribution P . No (independence, ergodicity, stationarity, iden-tifiability, or other) assumptions need to be made on M . Let be the measure selected by MDL at time ` given x  X  X  ` . Then the predictive distributions MDL x [  X | x ] converge to P [  X | x ] in the sense that K ( Q ) is usually interpreted and defined as the length of some prefix code for Q , in which Q ( x ) w Q / Bayes ( x ) , the maximum a posteriori estimate MAP x := argmax Q  X  X  { Pr( Q | x ) } X  MDL x . Hence the theorem also applies to MAP. The proof of the theorem is surprisingly subtle and complex compared to the analogous Bayesian case. One reason is that MDL x ( x ) is not a measure on X  X  . Arbitrary X . For arbitrary measurable spaces X , definitions are more subtle, essentially because point probabilities Q ( x ) have to be replaced by probability densities relative to some base measure M , usually Lebesgue for X = IR d , counting measure for countable X , and e.g. M [  X  ] = Bayes [  X  ] for general X . We have taken care of that all results and proofs are valid unchanged for general X , with Q (  X  ) defined as a version of the Radon-Nikodym derivative relative to M . We spare the reader the details, since they are completely standard and do not add any value to this paper, and space is limited. The formal definitions of Q ( x ) and Q [ A | x ] can be found e.g. in [Doo53, BD62]. Note that MDL x is independent of the particular choice of M . We first prove Theorem 1 for finite model classes M . For this we need the following Definition and Lemma: Definition 2 (Relations between Q and P ) For any probability measures Q and P , let It is well-known that the Lebesgue decomposition exists and is unique. The representation of the Radon-Nikodym derivative as a limit of local densities can e.g. be found in [Doo53, VII  X  8]: Z ` (  X  ) := Q verge w.p.1. Q r P implies that the limit Z r  X  is the Radon-Nikodym derivative dQ r /dP . (Indeed, Doob X  X  martingale convergence theorem can be used to prove the Radon-Nikodym theorem.) Q s  X  P implies Z r  X  = 0 w.p.1. So g is uniquely defined and finite w.p.1.
 Lemma 3 (Generalized merging of opinions) For any Q and P , the following holds: that Q merges with P . ( iii ) says that even if P 6 Q , we still have d ( P,Q | x )  X  0 on almost every sequence that has a positive limit of Q ( x ) /P ( x ) .
 Proof. Recall Definition 2. P [ X   X  ] = 0 . Therefore P Q . ( i  X  ) Assume P Q : Choose a B for which P [ B ] = 1 and Q s [ B ] = 0 . Now Q r [ X   X  ] = R  X   X  g dP = 0 implies 0  X  Q [ B  X   X   X  ]  X  Q s [ B ] + Q r [ X   X  ] = 0 + 0 . By P Q this implies P [ B  X   X   X  ] = 0 , hence P [ X   X  ] = 0 . ( ii ) That P Q implies P [ ~  X ] = 1 is Blackwell-Dubins X  celebrated result. The result now follows from (i). we can assume 0 &lt;P [ X   X  ] &lt; 1 . Consider measure P 0 [ A ] := P [ A | B ] conditioned on B :=  X  \  X   X  .  X  Now (ii) implies d ( P 0 ,Q | x )  X  0 with P 0 probability 1. Since P 0 P we also get d ( P 0 ,P | x )  X  0 w. P 0 .p.1.
 now follows from The intuition behind the proof of Theorem 1 is as follows. MDL will asymptotically not select Q for which Q ( x ) /P ( x )  X  0 . Hence for those Q potentially selected by MDL, we have  X  6 X   X   X  , hence  X   X  ~  X  , for which d ( P,Q | x )  X  0 (a.s.). The technical difficulties are for finite M that the eligible Q depend on the sequence  X  , and for infinite M to deal with non-uniformly converging d , i.e. to infer d ( P, MDL x | x )  X  0 .
 Proof of Theorem 1 for finite M . Recall Definition 2, and let g Q ,  X   X  Q , ~  X  Q refer to some Q  X  X  X  { Q 1 ,...,Q m } . The set of sequences  X  for which some g Q for some Q  X  X  is undefined has P -measure zero, and hence can be ignored. Fix some sequence  X   X   X  for which g Q (  X  ) is defined for all Q  X  X  , and let M  X  := { Q  X  X  : g Q (  X  ) = 0 } .
 Consider the difference For Q  X  X   X  , the r.h.s. is +  X  , hence Since M is finite, this implies Therefore, since P  X  X  , we have MDL x 6 X  X   X   X  `&gt;` 0 , so we can safely ignore all Q  X  X   X  and focus on Q  X  M  X  := M\M  X  . Let  X  1 := T Q  X  M can also assume  X   X   X  1 .
 This implies d ( P, MDL x | x )  X  sup excluded in our considerations has measure zero, d ( P, MDL x | x )  X  0 w.p.1, which proves the theorem for finite M . The proof in the previous Section crucially exploited finiteness of M . We want to prove that the probability that MDL asymptotically selects  X  X omplex X  Q is small. The following Lemma estab-lishes that the probability that MDL selects a specific complex Q infinitely often is small. Lemma 4 (MDL avoids complex probability measures Q ) For any Q and P we have P [ Q ( x ) /P ( x )  X  c infinitly often ]  X  1 /c .
 Proof. P [  X  ` 0  X  `&gt;` 0 : Q ( x ) (a) is true by definition of the limit superior lim , (b) is Markov X  X  inequality, (c) exploits the fact that the limit of Q ( x ) /P ( x ) exists w.p.1, (d) uses Fatou X  X  lemma, and (e) is obvious. For sufficiently complex Q , Lemma 4 implies that L Q ( x ) &gt;L P ( x ) for most x . Since convergence is non-uniform in Q , we cannot apply the Lemma to all (infinitely many) complex Q directly, but need to lump them into one  X  Q . Proof of Theorem 1 for countable M . Let the Q  X  X  = { Q 1 ,Q 2 ,... } be ordered somehow, e.g. in increasing order of complexity K ( Q ) , and P = Q n . Choose some (large) m  X  n and let f M := { Q m +1 ,Q m +2 ,... } be the set of  X  X omplex X  Q . We show that the probability that MDL selects infinitely often complex Q is small:
P [ MDL x  X  f M infinitely often ]  X  P [  X  ` 0  X  `&gt;` 0 : MDL x  X  f M ]  X  P [  X  ` 0  X  `&gt;` 0  X  Q  X  f M : L Q ( x )  X  L P ( x )] = P [  X  ` 0  X  `&gt;` 0 : sup The first three relations follow immediately from the definition of the various quantities. Bound (a) is the crucial  X  X umping X  step. First we bound While MDL  X  [  X  ] is not a (single) measure on  X  and hence difficult to deal with,  X  Q is a proper prob-ability measure on  X  . In a sense, this step reduces MDL to Bayes. Now we apply Lemma 4 in (b) to the (single) measure  X  Q . The bound (c) holds for sufficiently large m = m  X  ( P ) , since  X   X  0 for m  X  X  X  . This shows that for the sequence of MDL estimates Hence the already proven Theorem 1 for finite M implies that d ( P, MDL x | x )  X  0 with probability at least 1  X   X  . Since convergence holds for every  X &gt; 0 , it holds w.p.1. Due to its generality, Theorem 1 can be applied to many problem classes. We illustrate some imme-diate implications of Theorem 1 for time-series forecasting, classification, regression, discriminative learning, and reinforcement learning.
 Time-series forecasting. Classical online sequence prediction is concerned with predicting x ` +1 predicting x ` +1: ` + h for some h&gt; 0 . Hence Theorem 1 implies good asymptotic (multi-step) predic-tions. Offline learning is concerned with training a predictor on x 1: ` for fixed ` in-house, and then selling and using the predictor on x ` +1:  X  without further learning. Theorem 1 shows that for enough training data, predictions  X  X ost-learning X  will be good.
 Classification and Regression. In classification (discrete X ) and regression (continuous X ), a sam-conditional probability P (  X  x |  X  y ) shall be learned. For reasons apparent below, we have swapped the If we assume that also  X  y follows some distribution, and start with a countable model class M of i.i.d., we don X  X  need to invoke our general result.
 Discriminative learning. Instead of learning a generative [Jeb03] joint distribution P (  X  x,  X  y ) , which requires model assumptions on the input  X  y , we can discriminatively [LSS07] learn P (  X |  X  y ) directly without any assumption on y (not even i.i.d). We can simply treat y 1:  X  as an oracle to all Q , define M P more generally for any causal process, we have Q ( x | y ) = Q ( x | y 1:  X  ) . Since the x given y are not identically distributed, classical MDL consistency results for i.i.d. or stationary-ergodic sources do not apply. The following corollary formalizes our findings: Corollary 5 (Discriminative MDL) Let M3 P be a class of discriminative causal distributions typical examples. Further assume M is countable. Let MDL x | y := argmin Q  X  X  { X  log Q ( x | y ) + K ( Q ) } be the discriminative MDL measure (at time ` given x,y ). Then sup A MDL x | y [ A | x,y ]  X  P [ A | x,y ]  X  0 for ` ( x )  X  X  X  , P [  X | y 1:  X  ] almost surely, for every sequence y 1:  X  . For finite Y and conditionally independent x , the intuitive reason how this can work is as follows: then P (  X |  X  y ) can be learned. For infinite Y and deterministic M , the result is also intelligible: Every function.
 Reinforcement learning (RL). In the agent framework [RN03], an agent interacts with an envi-ronment in cycles. At time t , an agent chooses an action y t based on past experience x &lt;t  X  ( x perception x t with probability  X  ( x t | x &lt;t y 1: t ) (say). Then cycle t + 1 starts. Let P ( xy ) = Q tionarity, ergodicity) assumption on  X  and  X  . They may be POMDPs or beyond.
 Corollary 6 (Single-agent MDL) For a fixed policy=agent  X  , and a class of environments {  X  1 , X  2 ,... }3  X  , let M = { Q i } with Q i ( x | y ) = Q with joint P -probability 1.
 The corollary follows immediately from the previous corollary and the facts that the Q i are causal and that with P [  X | y 1:  X  ] -probability 1  X  y 1:  X  implies w. P .p.1 jointly in x and y . In reinforcement learning [SB98], the perception x t := ( o t ,r t ) consists of some regular observation o and a reward r t  X  [0 , 1] . Goal is to find a policy which maximizes accrued reward in the long run. The previous corollary implies Corollary 7 (Fixed-policy MDL value function convergence) Let V P [ xy ] := E P [  X | xy ] [ r ` +1 +  X r ` +2 +  X  2 r ` +3 + ... ] be the future  X  -discounted P -expected reward sum (true value of  X  ), and simi-w. P .p.1. for any policy  X  .
 using 0  X  f  X  1 / (1  X   X  ) and Corollary 6.
 Since the value function probes the infinite future, we really made use of our convergence result in total variation. Corollary 7 shows that MDL approximates the true value asymptotically arbitrarily well. The result is weaker than it may appear. Following the policy that maximizes the estimated (MDL) value is often not a good idea, since the policy does not explore properly [Hut05]. Neverthe-less, it is a reassuring non-trivial result. MDL is more a general principle for model selection than a uniquely defined procedure. For instance, there are crude and refined MDL [Gr  X  u07], the related MML principle [Wal05], a static , a dynamic , and a hybrid way of using MDL for prediction [PH05], and other variations. For our setup, we could have defined multi-step lookahead prediction as a product of single-step predictions: MDLI ( x 1: ` ) := Q version. Both, MDL x and MDLI are  X  X tatic X  in the sense of [PH05], and each allows for a dynamic and a hybrid version. Due to its incremental nature, MDLI likely has better predictive properties than MDL x , and conveniently defines a single measure over X  X  , but inconveniently is 6 X  X  . One reason for using MDL is that it can be computationally simpler than Bayes. E.g. if M is a class of MDPs, then MDL x is still an MDP and hence tractable, but MDLI like Bayes are a nightmare to deal with. Acknowledgements. My thanks go to Peter Sunehag for useful discussions.
