 We present an evolutionary clustering method which can be applied to multi-relational knowledge bases storing resource annotations expressed in the standard languages for the Se-mantic Web. The method exploits an effective and language-independent semi-distance measure defined for the space of individual resources, that is based on a finite number of dimensions corresponding to a committee of discriminating features (represented by concept descriptions). A maximally discriminating group of features can be obtained with the randomized optimization methods described in the paper. The clustering algorithm represents the possible clusterings as strings of central elements (medoids, w.r.t. the given met-ric) of variable length. Hence, the number of clusters is not required as a parameter since the method is able to find an optimal choice by means of the evolutionary operators and of a proper fitness function. We also show how to assign each cluster with a newly constructed intensional definition in the employed concept language. An experimentation with some ontologies proves the feasibility of our method and its effectiveness in terms of clustering validity indices. H.3.3 [ Information Storage and Retrieval ]: Clustering; I.5.4 [ Pattern Recognition ]: Clustering Algorithms, Measurement Conceptual clustering, unsupervised learning, metric learn-ing, genetic programming, evolutionary algorithms, descrip-tion logics, randomized optimization In the inherently distributed applications related to the Semantic Web (henceforth SW) there is an extreme need of Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. automatizing those activities which are more burdensome for the knowledge engineer, such as ontology construction, matching and evolution. These phases can be assisted by specific supervised [5, 18, 13] or unsupervised learning meth-ods [16, 8] crafted for knowledge bases expressed in the stan-dard representations of the field and complying with their semantics.

In this work, we investigate on unsupervised learning for knowledge bases expressed in such standard languages. In particular, we focus on the problem of conceptual clustering of semantically annotated resources. The benefits of con-ceptual clustering [24] in the context of semantically anno-tated knowledge bases are manifold. Clustering annotated resources enables the definition of new emerging concepts ( concept formation ) on the grounds of the concepts defined in a knowledge base; supervised methods can exploit these clusters to induce new concept definitions or to refining ex-isting ones ( ontology evolution ); intensionally defined group-ings may speed-up the task of search and discovery (see [5] for an application of dissimilarity measures to retrieval); a clustering may also suggest criteria for ranking the retrieved resources based on the distance from the centers.

Essentially, most of the clustering methods are based on the application of similarity (or density) measures defined over a fixed set of attributes of the domain objects. Classes of objects are taken as collections that exhibit low interclass similarity (density) and high intraclass similarity (density). These methods are rarely able to take into account some form of background knowledge that could characterize object configurations by means of global concepts and semantic re-lationships. This hinders the interpretation of the outcomes of these methods which is crucial in the SW perspective which enforces sharing and reusing the produced knowledge in order to enable forms of semantic interoperability across different knowledge bases and applications.

Conceptual clustering methods can answer these require-ments since they have been specifically crafted for defining groups of objects through (simple) descriptions based on selected attributes [24]. In the perspective, the expressive-ness of the language adopted for describing objects and clus-ters (concepts) is extremely important. Related approaches, specifically designed for terminological representations ( De-scription Logics [1], henceforth DLs), have recently been introduced [16, 8]. They pursue logic-based methods for at-tacking the problem of clustering w.r.t. some specific DL languages. The main drawback of these methods is that they are language-dependent, which prevents them to scale to the standard SW representations that are mapped on complex DLs. Moreover, purely logic methods can hardly handle noisy data.

These problems motivate the investigation on similarity-based clustering methods which can be more noise-tolerant and language-independent. Specifically, the extension of distance-based techniques is proposed, which can cope with the standard SW representations and profit by the bene-fits of a randomized search for optimal clusterings. Indeed, the method is intended for grouping similar resources w.r.t. a notion of similarity, coded in a distance measure, which fully complies with the semantics knowledge bases expressed in DLs. The individuals are gathered around cluster centers according to their distance. The choice of the best centers (and their number) is performed through an evolutionary approach [9, 17].

From a technical viewpoint, upgrading existing distance-based algorithms to work on multi-relational representa-tions, like the concept languages used in the SW (RDF through OWL), requires similarity measures that are suit-able for such representations and their semantics. A the-oretical problem is posed by the Open World Assumption (OWA) that is generally made on the language semantics, differently from the Closed World Assumption (CWA) which is standard in other contexts. Moreover, as pointed out in a seminal paper on similarity measures for DLs [3], most of the existing measures focus on the similarity of atomic concepts within hierarchies or simple ontologies.

Recently, dissimilarity measures have been proposed for some specific DLs [5]. Although they turned out to be quite effective for specific inductive tasks, they were still partly based on structural criteria which makes them fail to fully grasp the underlying semantics and hardly scale to more complex ontology languages. Moreover, they have been con-ceived for assessing concept similarity, whereas, for other tasks, a notion of similarity between individuals is required.
Therefore, we have devised a family of dissimilarity mea-sures for semantically annotated resources, which can over-come the aforementioned limitations [7]. Following the cri-terion of semantic discernibility of individuals, a family of measures is derived that is suitable for a wide range of lan-guages since it is merely based on the discernibility of the input individuals with respect to a fixed committee of fea-tures represented by a set of concept definitions. Hence, the new measures are not absolute, they rather depend on the knowledge base they are applied to. Thus, also the choice of good feature sets deserves a preliminary optimization phase, which can be performed by means of a randomized search procedures such as simulated annealing or genetic program-ming , defining mutation and crossover steps through the re-finement operators recently proposed in the context of on-tology evolution [18, 13].

In this setting, instead of the notion of centroid that char-acterizes the distance-based algorithms descending from k-means [14], originally developed for numeric or ordinal fea-tures, we recur to the notion of medoids [15] as central in-dividuals in a cluster. The proposed clustering algorithm employs genetic programming as a search schema. The evo-lutionary problem is modeled by considering populations made up of strings of medoids with different lengths. The medoids are computed according to the semantic measure induced with the methodology mentioned above. On each generation, the strings in the current population are evolved by mutation and cross-over operators, which are also able to change the number of medoids. Thus, this algorithm is also able to suggest autonomously a promising number of clusters. Accordingly, the fitness function is based both on the optimization of a cluster cohesion index and on the pe-nalization of lengthy medoid strings.

The remainder of the paper is organized as follows. Sect. 2 presents the basics of the target representation and the se-mantic similarity measure adopted with the clustering algo-rithm. This algorithm is presented and discussed in Sect. 3. After Sect. 4 surveying the related work, we report in Sect. 5 an experiment aimed at assessing the validity of the method on some ontologies available in the Web. Conclusions and extensions are finally examined in Sect. 6.
In the following, we assume that resources, concepts and their relationship may be defined in terms of a generic on-tology language that may be mapped to some DL language with the standard model-theoretic semantics (see the DLs handbook [1] for a thorough reference). As mentioned in the previous section, one of the advantages of our method is that it does not depend on a specific language for semantic annotations.

In the intended framework setting, a knowledge base K =  X  X  , A X  contains a TBox T and an ABox A . T is a set of concept definitions. The complexity of such definitions de-pends on the specific DL language constructors. A contains assertions (ground facts) on individuals (domain objects) concerning the current world state, namely: class-membership C ( a ), a is an instance of concept C relations R ( a,b ), a is R -related to b The set of the individuals referenced in the assertions ABox A will be denoted with Ind ( A ). The unique names assump-tion can be made on the ABox individuals 1 therein.
As regards the required inference services, like all other instance-based methods, the measure proposed in this sec-tion requires performing instance -checking , which amounts to determining whether an individual, say a , belongs to a concept extension, i.e. whether C ( a ) holds for a certain con-cept C . In the simplest cases (primitive concepts) this re-quires simple lookups, yet for defined concepts the reasoner may need to perform a number of inferences. Besides, dif-ferently from the standard DB settings, due to the OWA, the reasoner might be unable to provide a definite answer. Hence one has to cope with this form of uncertainty.
For our purposes, we need a function for measuring the similarity of individuals. It can be observed that individ-uals do not have a syntactic structure that can be com-pared. This has led to lifting them to the concept descrip-tion level before comparing them (recurring to the approx-imation of the most specific concept of an individual w.r.t. the ABox) [5].
 For clustering procedures, such as the one specified in Sect. 3, we have developed a new measure with a definition Each individual can be assumed to be identified by its own URI, however this is not bound to be a one-to-one mapping. that totally depends on semantic aspects of the individuals in the knowledge base. On a semantic level, similar indi-viduals should behave similarly with respect to the same concepts. We have introduced a novel measure for assess-ing the similarity of individuals in a knowledge base, which is based on the idea of comparing their semantics along a number of dimensions represented by a committee of con-cept descriptions.

Following some techniques for distance induction borrowed from ILP [23], we propose the definition of totally semantic distance measures for individuals in the context of a knowl-edge base which is also able to cope with the OWA.

The rationale of the new measure is to compare individu-als on the grounds of their behavior w.r.t. a given set of features, that is a collection of concept descriptions, say F = { F 1 ,F 2 ,...,F m } , which stands as a group of discrimi-nating features expressed in the considered DL language.
A family of dissimilarity measures for individuals inspired to the Minkowski X  X  distances ( L p ) can be defined as fol-lows [7]: Definition 2.1 (family of dissimilarity measures). Let K =  X  X  , A X  be a knowledge base. Given set of con-cept descriptions F = { F 1 ,F 2 ,...,F m } , a family of functions { d is defined as follows:  X  a,b  X  Ind ( A ) d p ( a,b ) := where p &gt; 0 and the i -th projection function  X  i of vector  X  , i  X  X  1 ,...,m } , is defined by:  X  a  X  Ind ( A ) The superscript F will be omitted when the set of features is fixed.

The case of  X  i ( a ) = 1 / 2 corresponds to the case when a reasoner cannot give the truth value for a certain member-ship query. This is due to the OWA normally made in this context.
Compared to other proposed distance (or dissimilarity) measures for individuals [5], the presented function does not depend on the constructors of a specific language, rather it requires only the instance-checking service used for deciding whether an individual is asserted in the knowledge base to belong to a concept extension or, alternatively, if this could be derived as a logical consequence.

It is easy to see that the functions { d F p } p  X  N are dissimi-larity measures. Even more so, the standard properties for semi-distances can be proven [7]:
Proposition 2.1 (semi-distance). For a fixed feature set and p &gt; 0 , given any three instances a,b,c  X  Ind ( A ) . it holds that: 1. d F p ( a,b )  X  0 2. d F p ( a,b ) = d F p ( b,a ) 3. d F p ( a,c )  X  d F p ( a,b ) + d F p ( b,c )
The functions are not metrics because it cannot be proved that if d F p ( a,b ) = 0 then a = b (whereas the opposite impli-cation easily holds). This is the case of indiscernible indi-viduals with respect to the given set of features F . If the property were strictly required, a distance could be derived either by considering equivalence classes [25] or, if the unique names assumption were made, by introducing equality as a new meta-feature  X  0 .

Note that the projection functions for the individuals in the knowledge base can be computed in advance thus de-termining a speed-up in the actual computation of the mea-sure. This is very important for the measure integration in algorithms which massively use this distance, such as all instance-based methods.
The underlying idea in the measure definition is that sim-ilar individuals should exhibit the same behavior w.r.t. the concepts in F . Here, we make the assumption that the feature-set F represents a sufficient number of (possibly redundant) features that are able to discriminate really different indi-viduals.

Preliminary experiments, where the measure has been ex-ploited for instance-based classification ( Nearest Neighbor algorithm) and similarity search [25], demonstrated the ef-fectiveness of the measure using the very set of both primi-tive and defined concepts found in the knowledge bases.
However, the choice of the concepts to be included in the committee F is crucial and may be the object of a prelim-inary learning problem to be solved ( feature selection for metric learning ).

Various optimizations of the feature set can be foreseen as concerns its definition. Among the possible sets of fea-tures we will prefer those that are able to discriminate the individuals in the ABox:
Definition 2.2 (good feature set). Let F be a set of concept descriptions F = { F 1 ,F 2 ,...,F m } . F is a good feature set for the knowledge base K =  X  X  , A X  iff  X  a,b  X  Ind ( A ) ,a 6 = b,  X  i  X  X  1 ,...,m } :  X  i ( a ) 6 =  X  i Then, when the previously defined function is parameter-ized on a good feature set, it has the properties of a metric function.

Namely, since the function is strictly dependent on the committee of features F , two immediate heuristics arise: Finding optimal sets of discriminating features, should profit also by their composition employing the specific constructors made available by the representation language of choice.
These objectives can be accomplished by means of ran-domized optimization techniques, especially when knowl-edge bases with large sets of individuals are available. Namely, part of the entire data can be drawn in order to learn opti-mal F sets, in advance with respect to the successive usage for all other purposes. Figure 1: Feature set optimization algorithm based on genetic programming.
A specific optimization algorithm founded in genetic pro-gramming has been devised to find optimal choices of dis-criminating concept committees. The resulting algorithm is depicted in Fig. 1. Essentially it searches the space of all possible feature committees starting from an initial guess (determined by the call to the makeInitialFS () procedure) based on the concepts (both primitive and defined) currently referenced in the knowledge base K , starting with a commit-tee of a given cardinality ( INIT CARD ). This initial cardi-nality may be determined as a function of d log 3 ( N ) e , where N = | Ind ( A ) | , as each feature projection can categorize the individuals in three sets.

The outer loop gradually augments the cardinality of the candidate committees. It is repeated until the threshold fitness is reached or the algorithm detects some fixpoint: employing larger feature committees would not yield a bet-ter feature set with respect to the best fitness recorded in the previous iteration (with fewer features). Otherwise, the extendFS () procedure extends the current for the next gen-erations by including a newly generated random concept.
The inner while-loop is repeated for a number of gen-erations until a stop criterion is met, based on the maxi-mal number of generations maxGenerations or, alternatively, when a minimal fitness threshold fitnessThr is crossed by some feature set in the population, which can be returned.
As regards the bestFitness () routine, it computes the best fitness of the feature sets in the input vector. Fitness can be determined as the discernibility factor yielded by the feature set, as computed on the whole set of individuals or on a smaller sample. For instance, given the fixed set of individuals IS  X  Ind ( A ) the fitness function may be: where  X  is a normalizing factor that can depend on the over-all number of couples involved.

As concerns finding candidate sets of concepts to replace the current committee (the generateOffsprings () rou-tine), the function was implemented by recurring to some transformations of the current best feature sets: The possible refinements of concept description are language-specific. E.g. for the case of ALC logic, refinement operators have been proposed in [18, 13].

This is iterated till a suitable number of offsprings is gen-erated. Then these offspring feature sets are evaluated and the best ones are included in the new version of the cur-rentFSs array; the best fitness value for these feature sets is also computed.

When the while-loop is over the current best fitness is compared with the best one computed for the former feature set length; if an improvement is detected then the outer repeat-loop is continued, otherwise (one of) the former best feature set(s) is selected and returned as the result of the algorithm.
The randomized optimization algorithm based on genetic programming just described may suffer from being possi-bly caught in plateaux or local minima if a limited number of generations are explored before checking for an improve-ment. This is likely due to the extent of the search space, which, in turn, depends on the language of choice. More-over. maintaining a single best genome for the next genera-tion may slow down the search process.

To prevent such cases, different randomized search proce-dures which aim at global optimization should be adopted. Hence, a method based on simulated annealing [4] has also been proposed [7], whose algorithm is reported in Fig. 2.
The algorithm searches the space of feature sets starting from an initial guess (determined by makeInitialFS ( K )) based on the concepts (both primitive and defined) currently referenced in the knowledge base, which can be freely com-bined to form new descriptions. Figure 2: Feature Set optimization procedure based on simulated annealing.

The loop controlling the search is repeated for a number of times that depends on the temperature temp controlled by the cooing function  X  T () which gradually decays to 0, when the current feature committee can be returned. In this cycle, the current feature set is iteratively refined call-ing procedure randomSuccessor () which makes a step in the space by refining the current set. Then the fitness of the new feature set is compared to that of the current one determining the increment of energy  X  E . If this is posi-tive then the candidate committee replaces the current one. Otherwise it will (less likely) be replaced with a probability that depends on  X  E and on the current temperature.
The energy increase  X  E is determined by the fitness () function applied to the new and current feature sets, which can be computed as the average discernibility factor, defined as above.

As concerns finding candidates to replace the current com-mittee, randomSuccessor () can be implemented by recur-ring to simple transformations of the feature set: Note that these transformation may change the cardinality of the current feature set. As mentioned before, refining concept descriptions is language-dependent. Complete op-erators are to be preferred to ensure exploring the whole search space.

Given a suitable cooling schedule, the algorithm is known to find an optimal solution. To control the complexity of the process alternate schedules may be preferred that guaran-tee the construction of suboptimal solutions in polynomial time [4].
Many similarity-based clustering algorithms (see [14]) can be applied to semantically annotated resources stored in a knowledge base, exploiting the measures discussed in the previous section.

We focussed on the techniques based on evolutionary meth-ods which are able to determine also an optimal number of clusters, instead of requiring it as a parameter (although the algorithm can be easily be modified to exploit this informa-tion that greatly reduces the search-space).

Conceptual clustering requires also to provide a defini-tion for the detected groups, which may be the basis for the formation of new concepts inductively elicited from he knowledge base. Hence, the conceptual clustering procedure consists of two phases: one that detects the clusters in the data and the other that finds an intensional definition for the groups of individuals detected in the former phase.
The first clustering phase implements a genetic program-ming learning scheme, where the designed representation for the competing genomes is made up of strings (lists) of in-dividuals of different lengths, with each gene standing as prototypical for a cluster.

Specifically, each cluster will be represented by its proto-type recurring to the notion of medoid [15, 14] on a cate-gorical feature-space w.r.t. the distance measure previously defined. Namely, the medoid of a group of individuals is the individual that has the minimal distance w.r.t. the others. Formally. in this setting:
Definition 3.1 (medoid). Given a cluster of individu-als C = { a 1 ,a 2 ,...,a n } X  Ind ( A ) , the medoid of the cluster is defined:
In the proposed evolutionary algorithm, the population will be made up of genomes represented by a list of medoids G = { m 1 ,...,m k } of variable lengths. The algorithm per-forms a search in the space of possible clusterings of the indi-viduals, optimizing a fitness measure that maximizes the dis-cernibility of the individuals of the different clusters (inter-cluster separation) and the intra-cluster similarity measured in terms of the d F p pseudo-metric.

On each generation those strings that are considered as best w.r.t. a fitness function are selected for passing to the next generation. Note that the algorithm does not prescribe a fixed length of the genomes (as, for instance in k-means and its extensions [14]), hence it searches a larger space aim-ing at determining an optimal number of clusters for the data at hand.

Fig. 3 reports a sketch of the algorithm, named ECM Evolutionary Clustering around Medoids . After the call to the initialize () function returning (to currentPopulation ) a randomly generated initial population of popLength medoid strings, it essentially consists of the typical generation loop Figure 3: ECM : the Evolutionary Clustering around Medoids algorithm. of genetic programming where a new population is computed and then evaluated for deciding on the best genomes to be selected for survival to the next generation.

On each iteration new offsprings of current best cluster-ings in currentPopulation are computed. This is performed by suitable genetic operators explained in the following. The fitnessVector recording the quality of the various offsprings (i.e. clusterings) is then updated, and then the best off-springs are selected for the next generation.

The fitness of a single genome G = { m 1 ,...,m k } is com-puted by distributing all individuals among the clusters ide-ally formed around the medoids in that genome. For each medoid m i ( i = 1 ,...,k ), let C i be such a cluster. Then, the fitness is computed by the function: The factor  X  ( k ) is introduced in order to penalize those clus-terings made up of too many clusters that could enforce the minimization in this way (e.g. by proliferating singletons). A suggested value may be  X  ( k ) = the experiments (see Sect. 5).

The loop condition is controlled by the maximal number of generation (the maxGenerations parameter) ensuring that eventually it may end even with a suboptimal solution to the problem. Besides other parameters can be introduced for controlling the loop based on the best fitness value obtained so far or on the gap between the fitness of best and of the worst selected genomes in currentPopulation . Eventually, the best genome of the vector (supposed to be sorted by fitness in descending order) is returned.

It remains to specify the nature of the generateOff-springs procedure and the number of such offsprings, which may as well be another parameter of the ECM algorithm. Three mutation and one crossover operators are implemented: deletion ( G ) drop a randomly selected medoid: insertion ( G ) select m  X  Ind ( A ) \ G that is added to G : replacementWithNeighbor ( G ) randomly select m  X  G crossover ( G A , G B ) select subsets S A  X  G A and S B  X  G
A (10+60) selection strategy has been implemented, with the numbers indicating, resp., the number of parents se-lected for survival and the number of their offsprings gener-ated employing the mutation operators presented above.
The representation of centers by means of medoids has two advantages. First, it presents no limitations on attributes types, and, second, the choice of medoids is dictated by the location of a predominant fraction of points inside a cluster and, therefore, it is less sensitive to the presence of outliers. In k-means case a cluster is represented by its centroid, which is a mean (usually weighted average) of points within a cluster. This works conveniently only with numerical attributes and can be negatively affected even by a single outlier.

An algorithm based on medoids has several favorable prop-erties. Since it performs clustering with respect to any speci-fied metric, it allows a flexible definition of similarity. Many clustering algorithms do not allow for a flexible definition of similarity, but allow only Euclidean distance in current implementations. In addition, medoids are robust represen-tations of the cluster centers that are less sensitive to out-liers than other cluster profiles, such as the cluster means of k-means . This robustness is particularly important in the common context that many elements do not belong exactly to any cluster, which may be the case of the membership in DL knowledge bases, which may be not ascertained given the OWA.
The second phase is more language dependent. The var-ious cluster can be considered as training examples for a supervised algorithm aimed at finding an intensional DL definition for one cluster against the counterexamples, rep-resented by individuals in different clusters [16, 8].
Each cluster may be labeled with an intensional concept definition which characterizes the individuals in the given cluster while discriminating those in other clusters [16, 8]. Labeling clusters with concepts can be regarded as a num-ber of supervised learning problems in the specific multi-relational representation targeted in our setting [13]. As such it deserves specific solutions that are suitable for the DL languages employed.

A straightforward solution may be found, for DLs that al-low for the computation of (an approximation of) the most specific concept ( msc ) and least common subsumer ( lcs ) [1] (such as ALC ). The first operator, given the current knowl-edge base and an individual, provides (an approximation of) the most specific concept that has the individual as one of its instances. This would allow for lifting individuals to the concept level. The second operator computes minimal generalizations of the input concept descriptions.

Given these premises, the learning process can be de-scribed through the following steps: let C j be a cluster of individuals
As an alternative, other algorithms for learning concept descriptions expressed in DLs may be employed [18, 13]. In-deed, concept formation can be cast as a supervised learning problem: once the two clusters at a certain level have been found, where the members of a cluster are considered as positive examples and the members of the dual cluster as negative ones. Then any concept learning method which can deal with this representation (and semantics) may be utilized for this new task.
The unsupervised learning procedure presented in this pa-per is mainly based on two factors: the semantic dissimilar-ity measure and the clustering method. To the best of our knowledge in the literature there are very few examples of similar clustering algorithms working on complex represen-tations that are suitable for knowledge bases of semantically annotated resources. Thus, in this section, we briefly discuss sources of inspiration for our procedure and some related ap-proaches.
As previously mentioned, various attempts to define se-mantic similarity (or dissimilarity) measures for concept lan-guages have been made, yet they have still a limited appli-cability to simple languages [3] or they are not completely semantic depending also on the structure of the descrip-tions [5]. Very few works deal with the comparison of indi-viduals rather than concepts.

In the context of clausal logics, a metric was defined [21] for the Herbrand interpretations of logic clauses as induced from a distance defined on the space of ground atoms. This kind of measures may be employed to assess similarity in deductive databases . Although it represents a form of fully semantic measure, different assumptions are made with re-spect to those which are standard for knowledgeable bases in the SW perspective. Therefore the transposition to the context of interest is not straightforward.

Our measure is mainly based on Minkowski X  X  measures [25] and on a method for distance induction developed by Sebag [23] in the context of machine learning , where metric learning is developing as an important subfield. In this work it is shown that the induced measure could be accurate when employed for classification tasks even though set of features to be used were not the optimal ones (or they were redun-dant). Indeed, differently from our unsupervised learning approach, the original method learns different versions of the same target concept, which are then employed in a vot-ing procedure similar to the Nearest Neighbor approach for determining the classification of instances.

A source of inspiration was also rough sets theory [22] which aims at the formal definition of vague sets by means of their approximations determined by an indiscernibility relationship. Hopefully, these methods developed in this context will help solving the open points of our framework (see Sect. 6) and suggest new ways to treat uncertainty.
Our algorithm adapts to the specific representations de-vised for the SW context a combination of evolutionary clus-tering and the distance-based approaches (see [14]). Specifi-cally, in the methods derived from k-means and k-medoids each cluster is represented by one of its points.

Early versions of this approach are represented by the al-gorithms PAM , CLARA [15], and CLARANS [20]. They implement iterative optimization methods that essentially cyclically relocate points between perspective clusters and recompute potential medoids. The leading principle for the process is the effect on an objective function. The whole dataset is assigned to resulting medoids, the objective func-tion is computed, and the best system of medoids is retained. In CLARANS a graph is considered whose nodes are sets of k medoids and an edge connects two nodes if they differ by one medoid. While CLARA compares very few neigh-bors (a fixed small sample), CLARANS uses random search to generate neighbors by starting with an arbitrary node and randomly checking maxneighbor neighbors. If a neigh-bor represents a better partition, the process continues with this new node. Otherwise a local minimum is found, and the algorithm restarts until a certain number of local minima is found. The best node (i.e. a set of medoids) is returned for the formation of a resulting partition. Ester et al. [6] ex-tended CLARANS to deal with very large spatial databases.
Our algorithm may be considered an extension of evo-lutionary clustering methods [11] which are also capable to determine a good estimate of the number of clusters [9]. Besides, we adopted the idea of representing cluster-ings (genomes) as strings of cluster centers [17] transposed to the case of medoids for the categorical search spaces of interest.
 Other related recent approaches are represented by the UNC algorithm and its extension to the hierarchical clus-tering case H-UNC [19]. Essentially, UNC solves a multi-modal function optimization problem seeking dense areas in the feature space. It is also able to determine their num-ber. The algorithm is also demonstrated to be noise-tolerant and robust w.r.t. the presence of outliers. However, the ap-plicability is limited to simpler representations w.r.t. those considered in this paper.

Further comparable clustering methods are those based on an indiscernibility relationship [12]. While in our method this idea is embedded in the semi-distance measure (and the choice of the committee of concepts), these algorithms are based on an iterative refinement of an equivalence relation-ship which eventually induces clusters as equivalence classes.
As mentioned in the introduction, the classic approaches to conceptual clustering [24] in complex (multi-relational) spaces are based on structure and logics. Kietz &amp; Morik pro-posed a method for efficient construction of knowledge bases for the BACK representation language [16]. This method exploits the assertions concerning the roles available in the knowledge base, in order to assess, in the corresponding re-lationship, those subgroups of the domain and ranges which may be inductively deemed as disjoint. In the successive phase, supervised learning methods are used on the discov-ered disjoint subgroups to construct new concepts that ac-count for them. A similar approach is followed in [8], where the supervised phase is performed as an iterative refinement step, exploiting suitable refinement operators for a different DL, namely ALC .
The feasibility of the clustering algorithm has been eval-uated with an experimentation on knowledge bases selected from standard repositories. Note that for testing our algo-rithm we preferred using populated ontologies (which may be more difficult to find) rather than randomly generating assertions for artificial individuals, which might have biased the procedure.
A number of different populated knowledge bases repre-sented in OWL were selected from various sources 2 , namely: FSM , SurfaceWaterModel , Transportation , NewTes-tamentNames , and Financial . Table 1 summarizes im-portant details concerning the ontologies employed in the experimentation. Of course, the number of individuals gives only a partial indication of the number of assertions (RDF triples) concerning them which affects both the complexity of reasoning and distance assessment.

For each populated knowledge base, the experiments have been repeated for ten times. In the computation of the dis-tances between individuals (the most time-consuming oper-ation) all concepts in the knowledge base have been used for the committee of features, thus guaranteeing meaning-ful measures with high redundancy. The Pellet reasoner 3 (ver. 1.4) was employed to perform the inferences (instance-checking) that were necessary to compute the projections.
The experimentation consisted of 10 runs of the algorithm per knowledge base. The indexes which were chosen for the experimental evaluation were the following: the generalized R-Squared ( modRS ), the generalized Dunn  X  X  index, the aver-age Silhouette index, and the number of clusters obtained. In the following explanation of these quality measures, we will consider a generic partition P = { C 1 ,...,C k } of n in-dividuals in k clusters.

The R-Squared index [10] is a measure of cluster separa-tion, ranging in [0,1]. Instead of the cluster means, we gen-eralize the measure by computing it w.r.t. their medoids, namely: where SS b is the between clusters Sum of Squares defined as follows: where m is the medoid of the whole dataset and SS t is the within cluster Sum of Squares that is defined:
The generalized Dunn X  X  index is a measure of both com-pactness (within clusters) and separation (between clusters). The original measure is defined for numerical feature vectors in terms of centroids and it is known to suffer from the pres-ence of outliers. To overcome these limitations, we adopt a
See the Prot  X eg  X e library: http://protege.stanford.edu/ plugins/owl/owl-library and the website: http://www. cs.put.poznan.pl/alawrynowicz/financial.owl http://pellet.owldl.com generalization of Dunn X  X  index [2] that is modified to deal with medoids. The new index can be defined: where  X  p is the Hausdorff distance for clusters derived 4 d , while the cluster diameter measure  X  p is defined: which is more noise-tolerant w.r.t. the original measure. Of course, this measure, ranging in [0 , +  X  [, has to be maxi-mized.

Conversely, the average Silhouette index [15] is a measure ranging in the interval [-1,1], thus suggesting an absolute best value for the validity of a clustering. For each individual x , i  X  X  1 ,...,n } , the average distance to other individuals within the same cluster C j , j  X  X  1 ,...,k } , is computed: Then the average distance to the individuals in other clusters is also computed: Hence, the Silhouette value for the considered individual is obtained as follows: Finally, the average Silhouette value s for the whole cluster-ing is computed:
We also considered the average number of clusters result-ing from the repetitions of the experiments on each knowl-edge base. A stable algorithm should return almost the same number of clusters on each repetition. It is also interesting to compare this number to the one of the primitive and de-fined concepts in each ontology (see Tab. 1), although this is not a hierarchical clustering method.
As mentioned, the experiment consisted in 10 runs of the evolutionary clustering procedure with an optimized feature set (computed in advance). each run took from a few min-utes to 41 mins on a 2.5GhZ (512Mb RAM) Linux Machine. Note that these timings include the pre-processing phase, that was needed to compute the distance values between all couples of individuals. Indeed, the elapsed time for the core clustering algorithm is actually very short (max 3 minutes). The outcomes of the experiments are reported in Tab. 2. For each for each knowledge base and index, the average values observed along the various repetitions is considered.  X  p is defined  X  p ( C i ,C j ) := max { d p ( C i ,C j ) ,d interval of values are reported.
 Moreover, the standard deviation and the range of minimum and maximum values are also reported.

The R-Squared index values denotes an acceptable degree of separation between the various clusters. We may inter-pret the outcomes observing that clusters present a higher degree of compactness (measured by the SS w component). It should also pointed out that flat clustering penalizes sep-aration as the concepts in the knowledge base are not nec-essarily disjoint. Rather, they naturally tend to form sub-sumption hierarchies. Observe also that the variation among the various runs is very limited.

Dunn X  X  index measures both compactness and separation; the rule in this case is the larger the better . Results are good for the various bases. These outcomes may serve for further comparisons to the performance of other clustering algorithms. Again, note that the variation among the vari-ous runs is very limited, so the algorithm was quite stable, despite its inherent randomized nature.

It can be observed that for the average Silhouette measure, that has a precise range of values, the performance of our algorithm is generally very good with a degradation with the increase of individuals taken into account. Besides, note that the largest knowledge base (in terms of its population) is also the one with the maximal number of concepts which provided the features for the metric. Thus in the resulting search space there is more freedom in the choice of the ways to make one individual discernible from the others. Surpris-ingly, the number of clusters is limited w.r.t. the number of concepts in the KB, suggesting that many individuals gather around a restricted subset of the concepts, while the others are only complementary (they can be used to discern the various individuals). Such subgroups may be detected extending our method to perform hierarchical clustering.
As regards the overall stability of the clustering proce-dure, we may observe that the main indices (and the number of clusters) show very little variations along the repetitions (see the standard deviation values), which suggests that the algorithm tends to converge towards clusterings of compa-rable quality with generally the same number of clusters. As such, the optimization procedure does not seem to suffer from being caught in local minima. However, the case needs a further investigation. Indeed, the optimization performed by the clustering procedure is two-fold: it does not optimize the choice of the clustering medoids but also their number, which is normally considered as a fixed parameter for other algorithms (see Sect. 4).

Other experiments (whose outcomes are not reported here) showed that sometimes the initial genome length may have an impact to the resulting clustering, thus suggesting the employment of different randomized search procedures (e.g. again simulated annealing or tabu search) which may guar-antee a better exploration of the search space.
This work has presented a framework for evolutionary conceptual clustering that can be applied to standard re-lational representations for knowledge bases in the SW con-text. Its intended usage is for discovering interesting group-ings of semantically annotated resources and can be applied to a wide range of concept languages. Besides, the induction of new concepts may follow from such clusters, which allows for accounting for them from an intensional viewpoint.
The method exploits a dissimilarity measure, that is based on the undelying resource semantics w.r.t. a number of di-mensions corresponding to a committee of features repre-sented by a group of concept descriptions in the language of choice. A preliminary learning phase, based on randomized search, can be exploited to optimize the choice of the most discriminating features.

The evolutionary clustering algorithm is an extension of distance-based clustering procedures employing medoids as cluster prototypes so to deal with complex representations of the target context. Variable-length strings of medoids yielding different partitions are searched guided by a fitness function based on cluster separation. As such the algorithm can also determine the length of the list, i.e. an optimal number of clusters.

As for the metric induction part, a promising research line, for extensions to matchmaking, retrieval and classifi-cation, is retrieval by analogy [5]: a search query may be issued by means of prototypical resources; answers may be retrieved based on local models (intensional concept descrip-tions) for the prototype constructed (on the fly) based on the most similar resources (w.r.t. some similarity measure). The presented algorithm may be the basis for the model construction activity. The distance measure may also serve as a ranking criterion.

The natural extensions of the clustering algorithm that may be foreseen are towards incrementality and hierarchical clustering. The former may be easily achieved by assigning new resources to their most similar clusters, and restarting the whole algorithm when some validity measure crosses a given threshold. The latter may be performed by wrapping the algorithm within a level-wise procedure starting with the whole dataset and recursively applying the partitive method until a criterion based on quality indices determines the stop. This may produce more meaningful concepts during the next supervised phase.

Better fitness functions may be also investigated for both distance optimization and clustering. For instance, some clustering validity indices can be exploited in the algorithm as measures of compactness and separation.
The authors would like to thank the anonymous reviewers for making useful points for the improvement of the paper and for providing suggestions for further investigations.
This work was partially supported by the regional interest projects DIPIS ( Distributed Production as Innovative Sys-tem ) and DDTA ( Distretto Digitale Tessile Abbigliamento ) in the context of the semantic web service discovery.
