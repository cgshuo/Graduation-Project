 The increasing amount of content available via digital tele-vision has made TV program recommenders valuable tools. In order to provide personalized recommendations, recom-mender systems need to collect information about user pref-erences. Since users are reluctant to invest much time in explicitly expressing their interests, preferences often need to be implicitly inferred through data gathered by monitor-ing user behavior. Which is, alas, less reliable.
This article addresses the problem of learning TV prefer-ences based on tracking the programs users have watched, whilst dealing with the varying degrees of reliability in such information. Three approaches to the problem are discussed: use all information equally; weight information by its relia-bility or simply discard the most unreliable information.
Experimental results for these three approaches are pre-sented and compared using a content-based filtering recom-mender built on a Na  X   X ve Bayes classifier.
 H.4 [ Information Systems Applications ]: Miscellaneous; I.2 [ Artificial Intelligence ]: Learning Algorithms Recommender, Reliability, Implicit Preferences
Digital television offers viewers with more content than one can reasonably consume. Profusion of content however is not at all a guarantee of user satisfaction if finding which content to watch becomes a tedious task and damages the user experience. Systems which deliver users with person-alized recommendations are therefore seen as valuable tools to link users with content of potential interest to them [11]. The efficiency of recommender systems, however, greatly de-pends not only on good prediction algorithms but also on the amount and the accuracy of the user data available to train these algorithms. There exist basically two main user profiling strategies: explicit and implicit.

Explicit user modeling requires users to manually provide information about their tastes. This may take various forms, from bootstrap questionnaires to  X  X n-the-fly X  X eedback about content via a rating scale. Though seen as very accurate, the main problem with explicit preferences, in addition to vary-ing interpretation of the same rating scale, is the extra bur-den they place on users who generally consider television to be a passive activity. As such, explicit profiles are often too scarce to be really useful because users for instance do not rate enough items. Additionally, such profiles do not conve-niently evolve with time. The claimed accuracy of explicit preferences is also questionable as users have a psychological tendency to describe themselves more as what they would like to be than what they actually are.

Implicit user modeling , on the other hand, relies on data mining techniques to automatically extract user interests and preferences from their actions. Such a mechanism there-fore does not impose any extra effort on users. However, due to their lack of user control, implicit preferences are often perceived as more intrusive and less accurate than explicit ones [8]. Indeed, it is often difficult to accurately map user actions into indications of preferences. However, thanks to greater amounts of data it allows to be collected which sig-nificantly increases the size of the training sets used by the prediction algorithms, recommendations based on implicit preferences can be at least as accurate as those relying on explicit user preferences [10].

In the end, the best system is likely to be a hybrid which uses both modeling strategies.

Implicit preferences, which are based on assumptions, in-herently suffer from uncertainty. As quoted from Kelly and Teevan [6]:  X  X ore tools that allow for the accurate and re-liable collection of data [...] need to be developed X  . Indeed, the objective of this paper is to investigate the best practices to address such uncertainty. Three different approaches to deal with unreliable implicit preference inputs are therefore proposed and compared: (1) the first one does not perform any particular treatment to the data and will serve as ref-erence; (2) the second approach weights preference inputs by their perceived confidence and (3) the last approach au-thoritatively discards preference inputs which do not ensure enough reliability.

The next section presents a literature review of implicit preferences used in TV recommender systems. Section 3 dis-cusses the issues linked to uncertainties in implicit preference indicators and discusses the proposed strategies to address them. Section 4 compares experimental results for the pro-posed approaches to deal with uncertainty. The final section discusses these results and opens future work directions.
Implicit preferences have initially been applied to brows-ing the Internet [4]. Various forms of implicit modeling hav e been developed. Oard and Kim first [9], Kelly and Teevan later [6], proposed a classification of implicit feedback te ch-niques along two axes: the type of user action ( e.g. , exam-ine, retain, annotate) and the scope of the action ( i.e. , only a portion of the content, the entire object or even its class) .
Implicit preferences have also already been used in the television domain with some success [1, 10, 2]. The com-mercial and popular TiVo system uses an explicit 7-point rating scale (ranging from 3 thumbs-down to 3 thumbs-up) to collect explicit user preferences, but it also considers any user recording of a program as a one thumb-up implicit rat-ing [1]. TiVo researchers further point out that actions on erences as well. Kim et al. [7] developed a slightly more complex approach which combines statistics about watched programs with sequences of user actions, such as:  X  X lay  X  Fast Backward  X  Play X  to create their model of user pref-erences.

Baudisch and Brueckner [2] took an interesting and origi-nal approach with TV Scout, a web-based recommendation system providing personalized TV schedules. This system continuously and unobtrusively gathers information about user interests from implicit feedback. However, in order to lower the entry barrier for new users ( X  X old-start X  issue), the system initially presents itself as a retrieval system, so that all user effort leads to an immediate result. Eventually , when enough user preferences have been collected, it grad-ually evolves into a filtering system. Their live experiment s strongly supported this phased approach.

Zimmerman et al. [12] evaluated an implicit recommender system with two different implicit methods: Bayesian statis -tics and Decision Trees. Their results showed that different methods perform better for different users or even for differ-ent types of shows, but fusing the results of the two methods improved the robustness of the system.

Regarding uncertainty in implicit preferences, Claypool et al. [4], developed the  X  X urious Browser X , which has been designed to capture both explicit feedback and implicit pre f-erence indicators in order to evaluate which implicit indic a-tors are correlated with the stated user interest. Their ex-periments showed that time spent on a web page is a reliable sign of user X  X  interest. This was an interesting study in the domain of web browsing that would be worth replicating in the domain of TV recommendation.
The implicit preference indicator we used in our TV rec-ommender system is based on the amount of a given pro-gram the user has watched. Such information has been sug-gested as one of the best possible indicators for TV rec-ommenders [1] and seems to be an appropriate alternative for the successful web-page reading time indicator. This is somehow different from previous work where, probably due to a lack of enough detailed information, only the fact that a program has been watched or not-watched is taken into Number of sessions Figure 1: Distribution of the viewing sessions ac-cording to percentage of the program watched. This is based on six-months of viewing information for 5000 representative viewers. account ( e.g. , [12]). Bearing in mind that users do not have an exhaustive knowledge of the TV schedule, we think that the fact one user did not watch a particular program does not necessary indicate that this user does not like it. The user may simply not have been aware of the program. On the contrary, we believe that the fact a user watched a pro-gram only briefly is a strong indication of dislike, especial ly, if the user zapped to another channel.

The information of how much of a program a user has watched is often difficult to capture. In a normal set-top-box scenario, the basic information that is known is the chan-nel that is on. From the EPG, the corresponding program can be identified with some accuracy (note that there is of-ten some uncertainty associated with advertisement breaks ). More difficult still is identifying which members of the house -hold are actually viewing the program or even whether some-one is actually watching the television or the television ha s experiments already identifies the viewers and advertiseme nt breaks and is therefore free from such issues. However, in real systems, these issues may also need to be addressed.
Despite the fact that user and program are correctly iden-tified, uncertainty issues still remain. The fact that one of the users watched a program does not guarantee this user liked it. Did you never regret having watched a bad pro-gram? The basic assumption behind our implicit preference mechanism hence is that the amount of a program the user has watched depends on user X  X  appreciation of the program. This postulation may sensibly and statistically prove vali d for programs either almost entirely or hardly watched by users. Intermediate situations, however, are not so straig ht-forward to correctly categorize. The distribution of the percentage of program viewed in our viewing data, demon-strates that the viewing patterns of users actually mostly f all within the most extreme and therefore more reliable cases (see Figure 1). This again suggests that this measure is po-tentially a very useful preference indicator. However, the re is still about half of the viewing sessions that fall within t he more uncertain categories. To deal with this uncertainty, we suggest three strategies: Neutral  X  One straightforward approach to the problem Weighted  X  A slightly more expensive approach is to Selective  X  Since there is usually quite a lot of implicit in-
The following section compares these different approaches via an empirical evaluation of the predictions and recom-mendations that can be obtained with each one of them.
This section presents the different experiments made to test the approaches proposed to deal with uncertainty in implicit preferences. This section first describes the data set and recommender algorithm used. Next it describes in de-tail how the different approaches to uncertainty were imple-mented. This is followed by the description of the evaluatio n method and finally the results obtained in the experiments.
Considering the difficulty in collecting user data for test dataset. BARB collects TV viewing data from a represen-tative set of UK households in order to provide minute-by-minute estimates of the number of people watching the dif-ferent terrestrial, cable and satellite broadcasting chan nels at any one time. Its main difference compared to standard viewing data collected on a set-top-box, is that viewing ses -sions identify the users that were actually watching TV.
Our BARB dataset comprises 6 months of viewing infor-mation, from January to June 2005, for approximately 3000 households (5000 individual viewers). This viewing infor-mation was combined with program schedule information for the same period to produce viewing sessions which in-dicate how much of a given program a given individual has watched. For performance reasons, experiments were con-ducted with a subset of 30  X  X ypical X  users ( i.e. with no obvi-ous abnormal viewing patterns like unusual amount of time spent watching TV or switching channels) selected for their tendency to watch TV alone (to limit the influence of others in their program choices). The idea is that these users can potentially provide more accurate implicit preferences. E ach viewing session in the dataset describes the person, the gen re of the program watched and the percentage of this program that has been watched.

Evaluating a recommender with such a dataset would mean that only programs watched by the user (however briefly) could be considered in the evaluation process. However, in a live system, the recommender has to select programs from the whole schedule, so it is important to make sure that programs that are not usually watched at all by the user are also handled correctly. For this reason, programs from the schedule which had not been viewed by the user were added to the dataset with no viewing information. Since the whole schedule is quite large, it was decided to only use a sample of the schedule consisting of 20 random programs per day. This results in special examples with no viewing information which are ignored during training and handled specially while testing.
The recommender algorithm selected for the experiments follows a machine learning approach. A Na  X   X ve Bayes (NB) classifier is trained to learn the classification of the exam-ples using a set of training examples taken from the dataset. Each example is derived from a TV viewing session and it is characterized by a genre and a class (matching/non-matching) attribute. Genre is a nominal attribute from BARB specification that takes 1 out of 18 possible cate-gories and is used as the description of the program. The class attribute determines whether the item is of interest t o the user or not. In other words, the NB classifier learns the probability that a program matches user X  X  interest given it s Genre.

For the evaluation a subset of testing examples (which are distinct from the training ones) is taken from the dataset. The recommender algorithm sorts these test items according the NB classifier X  X  estimated probability of them matching user preferences. The top 100 items are selected for the rec-ommendation shortlist. The reason for this limit is that we believe it is not sensible to present a larger recommendatio n list to the user. Furthermore and in order to ensure high precision in recommendations, elements with less than 80% of matching probability are discarded from the recommenda-tion shortlist. This threshold is referred to as the minimum confidence of the recommender.
The interpretation of user behaviors provides implicit in-formation about their preferences. Different assumptions can be taken to determine whether the user liked or disliked a program. These determine the examples X  classification. As mentioned previously, the assumption behind our exper-iments is that users watch much of the programs they like and quickly skip those that they do not. However, this leaves the question of how much of the program constitutes an in-Weight dication of preference or non-preference. Furthermore, di f-ferent examples have different associated uncertainty. Whi le a program watched in totality is likely a good indication of preference, a program only half-watched can be considered as ambiguous. The different approaches proposed take dif-ferent strategies to handle this underlying uncertainty. T hey are described in detail next.

Regardless of the strategy chosen, examples with no view-ing information, which have been randomly picked from the schedule are never classified.
The Neutral strategy simply treat all viewing examples equally. It classifies an example as matching user preferenc es if the user watched the program for over 50% of its duration and it classifies it as non-matching otherwise.
The Weighted strategy uses the same classification strat-egy as the Neutral strategy but it weights examples accord-ing to their uncertainty. Examples with high level of un-certainty are given a smaller weight so that they have less impact on the recommender X  X  learning. For example, it takes two learning examples of 0.5 weight to influence the classi-fier X  X  preference model as much as a single training example of weight 1. The training examples are weighted by a func-tion ( f ) of the percentage watched ( w ):
This function is illustrated in Figure 2.
The selective strategy simply discards examples that are too ambiguous. With this strategy, examples are classified such that:
Similarly to the Neutral strategy, all training examples have the same learning impact ( i.e. , same weight).
The best way to evaluate these different approaches would be to have the feedback from users themselves. Unfortu-nately, this information was not available to us, as it is fre -quently the case.

Following a typical machine learning evaluation, the same  X  X lassification scheme X  would be used for both the training and the testing data. This would not be a concern since usually what is under evaluation is the classifier and the classification is a given. Here we are actually looking into different  X  X lassification schemes X  so, in a certain way, we ar e training the classifier for different problems. Using a typic al approach would result in evaluating which one is the easi-est problem to learn. Our preliminary experiments showed that the Selective classification is easier to learn than the Neutral classification. This indicates that extreme viewin g behavior is easier to learn but does not say much about the recommendation quality of the system from a user satisfac-tion point of view.

Thus, our three approaches are first all evaluated using the Neutral Classification and then using the Selective Clas-sification . As such, these two evaluations were respectively named Neutral and Selective. This allows comparing the ap-proaches in equal grounds and avoiding a biased result that would be obtained by preferring one classification over the other.
 For all experiments, 4 evaluation metrics were taken: Accuracy  X  Measures the proportion of test examples cor-Breese Score  X  Measures the sum of the probabilities of Precision  X  Considering all the matching or non-matching Recall  X  Measures the proportion of item examples that
In the evaluation process, the dataset is divided into 26 week periods. The recommender iteratively and cumula-tively learns one week of data and is then tested against data of the following week. The first evaluation is performed without the recommender being trained and the last evalu-ation is tested against the last week which is never used for training.

All evaluation metrics are calculated individually for eac h user and each week. Individual user results are averaged to obtain final results. Graphs and tables also show the 95% confidence interval (assuming a Normal distribution) for each measure. item ten in the list. table shows the mean of the last 20 weekly evaluations.
Further to the results presented for each implicit prefer-ence strategy, results are also presented for a random rec-ommender. The random recommender randomly selects one hundred items for recommendation and randomly classifies items as either matching or not matching user preferences. These latter results are used as baseline for our implicit learning mechanisms.
The first set of experiments made uses the Neutral evalu-ation. It compares the performance of different approaches when the test examples are classified with the Neutral clas-sification. This means the Neutral strategy should have a slight advantage since this was the problem it was trained for. Results for the three approaches are shown in Figure 3 and summarized in Table 1. Baseline results for a non-learning recommender system are also shown.

The second set of experiment uses the Selective evalua-tion to compare performances of the three approaches. In this case, the Selective approach is expected to benefit from a slight advantage. Table 1 also shows this second set of results.

Results obtained with the two different evaluation ap-proaches are consistent. All the implicit preferences stra te-gies are viable in that they all provide a significant im-provement towards the baseline recommender (random rec-ommender). Note that the random recommender has a very good recall evaluation compared to the trained recom-menders. This is explained by the fact that the random rec-ommender always suggests one hundred recommendations while the trained recommenders restrict their shortlists t o items with a high confidence value, which often results in shorter lists. Everything else being equal shorter lists wi ll have a smaller proportion of total number of matching items. However by using shorter lists, the learning recommenders can ensure much better precision which is usually much more important than recall.
 All three strategies are very similar in terms of accuracy, Breese score and precision. Their evaluation only differs significantly in terms of recall. The Selective approach rec -ommends a higher proportion of matching items whilst the Neutral Evaluation recommends a smaller proportion.
This suggests that the Selective approach is more use-ful in that it can generate more recommendations with the same level of precision. Note that the discriminative level of a recommender can be adjusted by the choice of minimum confidence value required for short-listing an item for reco m-mendation. So varying this value can potentially change the differences in performance between the different approaches . For understanding the influence of the minimum confidence values, the graphs in Figures 4, 5 and 6 show an analysis of the use of different minimum confidence values. This anal-ysis is based on the Selective evaluation, but similar resul ts are obtained using the Neutral evaluation.

The first graph shows recall and precision results for dif-ferent values of the minimum confidence. Results are shown both for the Neutral and Selective approaches. This anal-ysis is consistent with the results presented previously: i t shows that recall for the Selective approach is always bet-ter or identical compared to the Neutral approach and that
The last couple of graphs show the distribution of items according to their estimated confidence value when learning with the Neutral strategy (see Figure 5) and with the Selec-tive strategy (see Figure 6). Both show the distribution of items matching and not matching user X  X  preferences accord-ing to the classifier X  X  confidence in recommending the item ( i.e. , the probability of this item matching user X  X  preferences according to the classifier). The distribution of unclassifi ed items is also shown. These are the items that either do not have any viewing information or which were left unclassified by the Selective classification. Graphs show the percentage of programs in each classification category that fall within the different confidences ranges. A mean is taken of the val-ues calculated for the recommendation outputs of all users during the latest 4 weeks.

An ideal recommender should have all matching items with high confidence values (matching bars towards the right in the histogram) and all non-matching items with low con-fidence values (non-matching bars towards the left in the histogram). The true value of unclassified items being un-known, a uniform distribution (or a concentration around middle confidence values) is expected. Arguably, these item s should neither be simply discarded nor preferred over all other items. This would mean that the recommender (1) was not giving preference to items the user has shown clear disinterest over items that the user has not seen at all and (2) was not preferring items not seen by the user over items that the user has shown interest in.

The recommenders shown in the graph have a reasonable distribution of the items according to confidence. Matching items have a tendency to have higher confidence values and fected by the minimum confidence value. item with no limit on list size. % of total programs within the week Figure 5: The distribution of sessions according to the Neutral classifier X  X  confidence in selecting them for recommendation. non-matching items have a tendency to have smaller confi-dence values. Non-classified items are neither preferred no r unfavoured. The graphs suggest though that the Selective approach is better at attributing higher confidence values t o matching items, thus confirming this approach as advanta-geous.

As a final point, note that if the recommenders had been evaluated under their own classification scheme, then the Se -lective approach would have a clear advantage towards the Neutral approach in term of accuracy (73.9 vs. 67.9), preci-sion (0.85 vs. 0.76) and recall (0.39 vs. 0.12). This compar-ison is much more beneficial to the Selective approach than the ones we use confirming our preliminary assertion that this comparison is not really fair.
The results of our implicit preference experiments are first a supplementary proof, if ever required, of the validity of implicit preferences for TV program recommendations. The precision of 88% for the Weighted strategy for instance in-dicates that theoretically, on average, only 1 out of 10 rec-ommendations would not meet user X  X  preferences, which is a very good score. Taking into account that the prediction algorithm was a simple Na  X   X ve Bayes using only the Genre metadata as parameter, we believe there is additional space for improvement by using information such as time or broad-cast channel in addition.
 Further to the point, results suggest that either the Weighted or Selective approaches are better than the Neu-tral approach. With respect to which one of the Weighted or the Selective approaches works best to address uncertainty , we feel that our experimental results are not entirely clear . This decision also needs to be motivated by the use-case. On the one hand, results suggest that the Weighted approach may give a slightly better precision score which is very im-portant as the cost of making a wrong recommendation is % of total programs within the week Figure 6: The distribution of sessions according to the Selective classifier X  X  confidence in selecting them for recommendation. often high. On the other hand, the Selective approach misses less recommendation opportunities ( i.e. , has a significantly better recall) and is the most resource-effective (it relies on less learning examples to learn and does not need the ex-tra weighting of the examples). From a strict observation of our results and taking into account that precision values in our experiments are statistically identical, the Select ive approach is better. However, we think there may exist sce-narios for which the Weighted approach will be preferable.
The main difficulty we found with our experiments relied in the evaluation methodology. All our metrics remain theo-retical since the assumptions used to test recommendations are the same as the ones used to infer the user preferences. Future experiments shall be designed so that users are di-rectly able to evaluate the recommendation lists or even the ir learnt preferences. This should also allow comparing impli cit recommendations with others built on explicit user prefer-ences. In the meanwhile, we proposed some solutions for experiments with implicit preferences. These may be useful when explicit preferences are not available as it is often th e case. These included, using different implicit approaches i n the testing phase, adding in examples not covered in the implicit preference examples ( i.e. , in our case using exam-ples from the full schedule and not being restricted to those programs actually watched by the users) and different visu-alization techniques.

Bearing in mind that our BARB dataset is not strictly identical to data that can be logged on a commercial set-top-box, future work will also aim at defining filtering strategie s to remove noise from collected data and evaluate the impact of having multiple users using the same implicit recommen-dation system. [1] K. Ali and W. van Stam. TiVo: Making show [2] P. Baudisch and L. Brueckner. TV scout: Lowering [3] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [4] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit [5] S. Gadanho. TV recommendations based on implicit [6] D. Kelly and J. Teevan. Implicit feedback for inferring [7] M. Kim, J. Lim, K. Kang, and J. Kim. Agent-based [8] D. M. Nichols. Implicit rating and filtering. In [9] D. W. Oard and J. Kim. Modeling information [10] D. O X  X ullivan, B. Smyth, D. Wilson, K. M. Donald, [11] B. Smyth and P. Cotter. Personalized electronic [12] J. Zimmermana, K. Kurapati, A. L. Buczak,
