 Language model (LM) adaptation is crucial to automatic spee ch recognition (ASR) as it enables higher-level contextual information to be effectively inc orporated into a background LM improving recognition performance. Exploiting topical context for L M adaptation has shown to be effective for ASR using latent semantic analysis (LSA) such as LSA usin g singular value decomposition [1], Latent Dirichlet Allocation (LDA) [2, 3, 4] and HMM-LDA [5, 6 ]. One issue in LSA is the bag-of-word assumption which ignores word ordering. For docume nt classification, word ordering may not be important. But in the LM perspective, word ordering is crucial since a trigram LM normally performs significantly better than a unigram LM for word pred iction. In this paper, we investigate whether relaxing the bag-of-word assumption in LSA helps im proving the ASR performance via LM adaptation.
 We employ bigram LSA [7] which is a natural extension of LDA to relax the bag-of-word assump-tion by connecting the adjacent words in a document together to form a Markov chain. There are two main challenges in bigram LSA which are not addressed pro perly in [7] especially for large-where V and K denote the vocabulary size and the number of topics. Therefo re, model smoothing becomes critical. Secondly, model initialization is impor tant for EM training, especially for bigram LSA due to the model sparsity. To tackle the first challenge, w e represent bigram LSA as a set of
K topic-dependent backoff LM. We propose fractional Kneser-Ney smoothing 1 which supports Figure 1: Graphical representation of bigram LSA. Adjacent words in a document are linked to-gether to form a Markov chain from left to right. fractional counts to smooth each backoff LM. We show that our formulation recovers the original Kneser-Ney smoothing [9] which supports only integral coun ts. To address the second challenge, we propose a bootstrapping approach for bigram LSA training using a well-trained unigram LSA as an initial model.
 During unsupervised LM adaptation, word hypotheses from th e first-pass decoding are used to es-timate the topic mixture weight of each test audio to adapt bo th unigram and bigram LSA. The adapted unigram and bigram LSA are combined with the backgro und LM in two stages. Firstly, marginal adaptation [10] is applied to integrate unigram LS A into the background LM. Then the in-termediately adapted LM from the first stage is combined with bigram LSA via linear interpolation with the interpolation weights estimated by minimizing the word perplexity on the word hypotheses. The final adapted LM is employed for re-decoding.
 Related work includes topic mixtures [11] which perform doc ument clustering and train a trigram LM for each document cluster as an initial model. Sentence-l evel topic mixtures are modeled so that the topic label is fixed within a sentence. Topical N-gram mod el [12] focuses on phrase discovery and information retrieval. We do not apply this model becaus e the phrase-based LM seems not outperform the word-based LM.
 fractional Kneser-Ney smoothing algorithm. In Section 3, w e present the LM adaptation approach based on marginal adaptation and linear interpolation. In S ection 4, we report LM adaptation results on Mandarin and Arabic ASR, followed by conclusions and futu re work in Section 5. Latent semantic analysis such as LDA makes a bag-of-word ass umption that each word in a docu-ment is generated irrespective of its position in a document . To relax this assumption, bigram LSA has been proposed [7] to modify the graphical structure of LD A by connecting adjacent words in a document together to form a Markov chain. Figure 1 shows the g raphical representation of bigram LSA where the top node represents the prior distribution ove r the topic mixture weights and the middle layer represents the latent topic label associated t o each observed word at the bottom layer. The document generation procedure of bigram LSA is similar t o LDA except that the previous word is taken into consideration for generating the current word : efficient algorithm for bigram LSA training via variational Bayes approach and model bootstrapping which are scalable to large settings in Section 2.2. Thirdly , we formulate the fractional Kneser-Ney smoothing to generalize the original Kneser-Ney smoothing which supports only integral counts in Section 2.3. Figure 2: Left: Dirichlet-Tree prior of depth two. Right: Va riational E-step as bottom-up propaga-tion and summation of fractional topic counts. 2.1 Topic correlation Modeling topic correlations is motivated by an observation that documents such as newspaper arti-cles are usually organized into main-topic and sub-topic hi erarchy for document browsing. From this perspective, a Dirichlet prior is not appropriate since it a ssumes topic independence. A Dirichlet-Dirichlet-Tree. A depth-one Dirichlet-tree is equivalent to a Dirichlet prior in LDA. The sampling procedure for the topic mixture weight  X   X  p (  X  ) can be described as follows: The structure and the number of outgoing branches of each Dir ichlet node can be arbitrary. In this paper, we employ a balanced binary Dirichlet-tree. 2.2 Model training Gibbs sampling was employed for bigram LSA training [7]. Des pite the simplicity, it can be slow and inefficient since it usually requires many sampling iter ations for convergence. We present a variational Bayes approach for model training. The joint li kelihood of a document w N topic sequence z N By introducing a factorizable variational posterior distr ibution q ( z N over the latent variables and applying the Jensen X  X  inequal ity, the lower bound of the marginalized document likelihood can be derived as follows: where the expectation is taken using the variational poster ior q ( z N the partial derivative of the auxiliary function Q (  X  ) with respect to q ( z Dirichlet-Tree posterior q (  X  ) . Setting the derivatives to zero yields: E-Steps: where Eqn 7 is motivated from the conjugate property that the Dirichlet-Tree posterior given the topic sequence z N Figure 2 (Right) illustrates that Eqn 7 can be implemented as propagation of fractional topic counts in a bottom-up fashion with each branch as an accumulator for  X  and set it to zero: M-Step (unsmoothed): where N to test if the i -th word in document d is vocabulary v . C bigram ( u, v ) belonging to topic k in document d . Intuitively, Eqn 12 simply computes the relative frequency of the bigram ( u, v ) . However, this solution is not practical since bigram LSA as signs zero probability to unseen bigrams. Therefore, bigram LSA s hould be smoothed properly. One simple approach is to use Laplace-smoothing by adding a smal l count  X  to all bigrams. However, this approach can lead to worse performance since it will bia s the bigram probability towards a as a standard backoff LM smoothed by fractional Kneser-Ney s moothing as described in Section 2.3. Model initialization is crucial for variational EM trainin g. We employ a bootstrapping approach using a well-trained unigram LSA as an initial model for bigr am LSA so that p ( w approximated by p ( w LSA in memory during the EM training. To make the training pro cedure more practical, we apply bigram pruning during statistics accumulation in the M-ste p when the bigram count in a document a bigram. With the sparsity, there is no need to store K copies of accumulators for each bigram and thus reducing the memory requirement significantly. The pruned bigram counts are re-assigned to the most likely topic of the current document so that the co unts are conserved. For practical each topic-dependent LM is smoothed individually using the merged count file. 2.3 Fractional Kneser-Ney smoothing Standard backoff N-gram LM is widely used in the ASR communit y. The state-of-the-art smoothing for the backoff LM is based on Kneser-Ney smoothing [9]. The b elief of its success is due to the preservation of marginal distributions. However, the orig inal formulation only works for integral counts which is not suitable for bigram LSA using fractional counts. Therefore, we propose the fractional Kneser-Ney smoothing as a generalization of the original formulation. The interpolated form using absolute discounting can be expressed as follows : where D is a discounting factor. In the original formulation, D lies between 0 and 1 . But in our formulation, D can be any positive number. Intuitively, D controls the degree of smoothing. If D is set to zero, the model is unsmoothed; If D is too big, bigrams with counts smaller than D are pruned from the LM.  X  ( u ) ensures the bigram probability sums to unity. After summing over all possible v on both sides of Eqn 13 and re-arranging terms,  X  ( u ) becomes: where C denotes the number of word types following u with the bigram counts bigger than D . In Kneser-Ney smoothing, the lower-order distribution p which can be estimated using the preservation of marginal di stributions: Eqn 25 generalizes Kneser-Ney smoothing to integral and fra ctional counts. For the original formu-lation, C with D less than one. As a result, the D term cancels out yielding the original formulation which counts the number of words preceding v and thus recovering the original formulation. Intuitively , the numerator in Eqn 25 measures the total discounts of obser ved bigrams ending at v . In other words, fractional Kneser-Ney smoothing estimates the lowe r-order probability distribution using the relative frequency over discounts instead of word counts. With this approach, each topic-depe ndent LM in bigram LSA can be smoothed using our formulation. Unsupervised LM adaptation is performed by first inferring t he topic distribution of each test audio frequency over the branch posterior counts  X  mixture weight  X   X  and the adapted unigram and bigram LSA are computed as follow s: The unigram LSA marginals are integrated into the backgroun d N-gram LM p adaptation [10] as follows: Marginal adaptation has a close connection to maximum entro py modeling since the marginal con-straints can be encoded as unigram features. Intuitively, b igram LSA would be integrated in the same fashion by introducing bigram marginal constraints. Howev er, we found that integrating bigram features via marginal adaptation did not offer further impr ovement compared to only integrating un-igram features. Since marginal adaptation integrates a uni gram feature as a likelihood ratio between the adapted marginal p explanation is that marginal adaptation corresponds to onl y one iteration of generalized iterative scaling (GIS). Due to the large number of bigram features in t erms of millions, one GIS iteration may not be sufficient for convergence. On the other hand, simp le linear LM interpolation is found to be effective in our experiment. The final LM adaptation for mula is provided using results from Eqn 27 and Eqn 28 as a two-stage process: where  X  is tuned to optimize perplexity on word hypotheses from the fi rst-pass decoding on a per-audio basis. Our LM adaptation approach was evaluated using the RT04 Mand arin Broadcast News evaluation system. The system employed context-dependent Initial-Fi nal acoustic models trained using 100-hour broadcast news audio from the Mandarin HUB4 1997 traini ng set and a subset of TDT4. 42-dimension features were extracted after linear discrimina nt analysis projected from a window of independent and speaker-adaptive acoustic models. For the second-pass decoding, we applied stan-dard acoustic model adaptation such as vocal tract length no rmalization and maximum likelihood linear regression on the feature and model spaces. The train ing corpora include Xinhua News 2002 (January X  X eptember) containing 13M words and 64k document s. A background 4-gram LM was trained using modified Kneser-Ney smoothing using the SRILM toolkit [15]. The same training corpora were used for unigram and bigram LSA training with 200 topics. The vocabulary size is 108k words. Discounting factor D for fractional Kneser-Ney smoothing was set to 0 . 4 . First-pass decoding was first performed to obtain an automat ic transcript for each audio show. Then unsupervised LM adaptation was applied using the automatic transcript to obtain an adapted LM for second-pass decoding using the approach described in Se ction 3. Word perplexity and character error rates (CER) were measured on the Mandarin RT04 test set . Matched pairs sentence-segment word error test was performed for significance test using the NIST scoring tool.

Topic index Top bigrams sorted by p ( u, v | k )  X  X opic-61 X  { +  X  ( X  X  student), { + s  X  ( X  X  education), s  X  + { (education  X  X )  X  X opic-62 X  | b + w  X  (expert cultivation), L  X  + D  X  (university chancellor)  X  X opic-63 X  Z +  X   X   X  F (and social security), { +  X  ( X  X  employment),  X  X opic-64 X  { +  X   X  ( X  X  research),  X  +  X  V (expert people),  X  +  X  (etc area)  X  X opic-65 X  |  X  +  X  O (Human DNA sequence), { +  X  O ( X  X  DNA) Table 2: Character Error Rates (Word perplexity) on the RT04 test set. Bigram LSA was applied in addition to unigram LSA.
 +bigram LSA (Kneser-Ney, 30 topics) 14.5 (604) 20.7 (1502) 39.0 (2736) 24.1 4.1 LM adaptation results Most of the top bigrams appear either as phrases or words atta ched with a stopword such as { ( X  X  in English). Table 2 shows the LM adaptation results in CER and p erplexity. Applying both unigram and bigram LSA yields consistent improvement over unigram L SA in the range of 6.4% X 8.5% statistically significant at 0.1% significance level. We com pared our proposed fractional Kneser-Ney smoothing with Witten-Bell smoothing which also supports f ractional counts. The results showed number of topics in bigram LSA helps despite model sparsity. We applied extra EM iterations on top of the bootstrapped bigram LSA but no further performanc e improvement was observed. 4.2 Large-scale evaluation We evaluated our approach using the CMU-InterACT vowelized Arabic transcription system dis-criminatively trained on 1500 -hour transcribed audio using MMIE for the GALE Phase-3 eval uation. A large background 4-gram LM was trained using 962M-word tex t corpora with 737k vocabulary. Unigram and bigram LSA were trained on the same corpora and we re applied to lattice rescoring on Dev07 and unseen Dev08 test sets with 2.6-hour and 3-hour aud io shows containing broadcast news (BN) and broadcast conversation (BC) genre. Table 3 shows th at bigram LSA rescoring reduces the overall word error rate by more than 3.0% relative compared t o the unadapted baseline on both sets trigram LSA compared to bigram LSA which may be due to data spa rseness.
 Table 3: Lattice rescoring results in word error rate on Dev0 7 (unseen Dev08) using the CMU-InterACT Arabic transcription system for the GALE Phase-3 e valuation.
 We present a correlated bigram LSA approach for unsupervise d LM adaptation for ASR. Our con-tributions include efficient variational EM for model train ing and fractional Kneser-Ney approach for LM smoothing with fractional counts. Bigram LSA yields a dditional improvement in both per-plexity and recognition performance in addition to unigram LSA. Increasing the number of topics for bigram LSA helps despite the model sparsity. Bootstrapp ing bigram LSA from unigram LSA saves computation and memory requirement during EM trainin g. Our approach is scalable to large training corpora and works well on different languages. The improvement from bigram LSA is statistically significant compared to the unadapted baseli ne. Future work includes applying the pro-posed approach for statistical machine translation.
 We would like to thank Mark Fuhs for help parallelizing the bi gram LSA training via condor. [1] J. R. Bellegarda,  X  X arge Vocabulary Speech Recognition with Multispan Statistical Language [2] D. Blei, A. Ng, and M. Jordan,  X  X atent Dirichlet Allocati on, X  in Journal of Machine Learning [3] Y. C. Tam and T. Schultz,  X  X anguage model adaptation usin g variational Bayes inference, X  in [4] D. Mrva and P. C. Woodland,  X  X nsupervised language model adaptation for mandarin broad-[6] B. J. Hsu and J. Glass,  X  X tyle and topic language model ada ptation using HMM-LDA, X  in [7] Hanna M. Wallach,  X  X opic Modeling: Beyond Bag-of-Words,  X  in International Conference [8] P. Xu, A. Emami, and F. Jelinek,  X  X raining connectionist models for the structured language [9] R. Kneser and H. Ney,  X  X mproved backing-off for M-gram la nguage modeling, X  in Proceedings [10] R. Kneser, J. Peters, and D. Klakow,  X  X anguage model ada ptation using dynamic marginals, X  [11] R. Iyer and M. Ostendorf,  X  X odeling long distance depen dence in language: Topic mixtures [12] X. Wang, A. McCallum, and X. Wei,  X  X opical N-grams: Phra se and topic discovery, with an [13] T. Minka,  X  X he dirichlet-tree distribution, X  1999. [14] Y. C. Tam and T. Schultz,  X  X orrelated latent semantic mo del for unsupervised language model [15] A. Stolcke,  X  X RILM -an extensible language modeling to olkit, X  in Proceedings of Interna-
