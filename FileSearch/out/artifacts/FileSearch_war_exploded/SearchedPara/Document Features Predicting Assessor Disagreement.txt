 The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict as-sessor disagreement and rank documents by decreasing probabil-ity of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that ag-gregates a document X  X  ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calcu-late. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Measurement, performance, experimentation Retrieval experiment, evaluation
Human assessors are used in information retrieval evaluation to judge the relevance of a document for a given topic. Assessors frequently disagree on the relevance of a document to a topic, how-ever. A study by [7] found that the probability that a second asses-sor would agree with a first assessor X  X  judgment that a document was relevant was only two in three. A survey of such studies done by [2] found similar results as well. While [7] found that assessor disagreement had limited effect on the comparative evaluation of systems, it does have a major impact upon the evaluation of their absolute effectiveness. Moreover, a simulation study by [4] sug-gests that the effect on comparative evaluation depends upon the nature of disagreement, and that an overly liberal (or careless) as-sessor introduces considerable noise even to the comparison of re-trieval systems.

While assessor disagreement has been frequently observed, and its effect on retrieval evaluation somewhat studied, less work has been done on the factors that lead to assessor disagreement. [9] ob-serves that there is great variability in disagreement between differ-ent assessor pairs and on different topics. Regarding assessor-level effects, [8] find that assessor training has little effect on reliability (legally trained assessors no more than untrained assessors on e-discovery tasks). Regarding topic-level effects, [11] find that more detailed assessor instructions do not seem to increase disagreement.
In addition to assessor-level and topic-level effects on assessor disagreement, there may be document-level effects: some docu-ments may be more likely to provoke assessor disagreement than others. [10] have begun work in this direction, using metarank in-formation across multiple runs to predict disagreement. If one as-sessor finds a document relevant, but it is generally lowly ranked by retrieval systems, then a second assessor is likely to disagree with the original assessor, and conversely with originally-irrelevant but highly-ranked documents.

In the current paper, we investigate the relation between asses-sor disagreement and various topic-independent document features. One set of such features are various metrics of the reading level or reading difficulty of a document. Our hypothesis is that documents that are more difficult to read will provoke higher levels of asses-sor disagreement. We also consider document length (hypothesiz-ing that longer documents will provoke more disagreement) and document coherence (hypothesizing that less coherent documents will provoke more disagreement). Finally, we extend the metarank method of [10] by considering not only average rank across differ-ent retrieval systems, but also the variability in the ranking X  X sing disagreement between retrieval systems as a predictor of disagree-ment between human assessors.

If reliable document-level predictors of assessor disagreement can be found, then they can be used to efficiently direct multiple assessments towards those documents most likely to provoke as-sessor disagreement. We consider this as a ranking problem, in which documents must be ranked by decreasing probability of as-sessor disagreement, examining the case in which this ranking must be made without any initial relevance assessment having been per-formed. Our experimental results indicate that document-level fea-tures give a significant improvement over random choice in pre-dicting assessor disagreement. Moreover, where initial relevance assessments are not available, document-level features pre dict as-sessor disagreement as strongly as meta-rank features, without re-quiring a large and diverse set of retrieval runs to calculate.
One surprise of the study is that while reading level features are predictive of assessor disagreement, the correlation is the opposite of that posited in our hypothesis: documents scored as easier to read are more, not less, likely to provoke assessor disagreement than those scored as difficult to read. This suggests that reading level features are themselves correlated with some other aspect of document construction or content, which if more directly identified could lead to even stronger predictors of assessor disagreement; a question which is left to future work.

The remainder of the paper is structured as follows. A descrip-tion of our logistic regression model along with all the document-level features is given in Section 2. Section 3 describes our ex-periments along with the dataset used in this work, and a detailed analysis of our results is given in Section 4. Section 5 summarizes our findings and sketches future work.
Our approach to the problem of predicting assessor disagreement consists of two main components: identifying features, and devel-oping a modeling technique.
We predict the probability that a document will attract diver-gent binary relevance assessments from two or more assessors ( D ), based upon various document level features s = h s i i , as p ( D = 1 | s ) . As we are predicting a probability, it is natural to apply a logistic regression to this problem: where s i is the score for feature i , and the probability p is the pre-dicted value. The fitted value  X  0 in Equation 1 is the intercept, which gives the log-odds of disagreement when the score is 0 , while  X  is the score coefficient for feature i , which gives the change or  X  X lope X  in log odds of disagreement for every one point increase in the given feature scores. The slope gives the strength of relation-ship between feature scores and probability of disagreement, while intercept the shifts the regression curve up or down the score axis.
A model can be built for each topic individually, or a univer-sal model can be built using all queries in our dataset. The degree to which a universal model is a good approximation for per-topic models depends upon the strength of per-topic factors in influenc-ing disagreement. The closer the universal model is to the per-topic models, the more likely it is that a generalized model can be built, that is able to predict assessor disagreement on new collec-tions based only the feature scores.
In this section, we discuss in detail the various predictors that we use in Equation 1 to estimate assessor disagreement. The lo-gistic model described in Section 2.1 relies heavily on the feature scores and identifying good predictors of disagreement is critical. We use a combination of simple document characteristic features and reading level features to estimate disagreement.
The simple document quality features are described below:
We employ a number of standard metrics of reading level, based upon simple textual statistics. More complicated statistical and lan-guage model approaches are left for future work [5]. [10] propose using the metarank of a document across multiple retrieval runs as a predictor that a second assessor would disagree with an original assessor, given the original assessor X  X  judgment. The metarank method used was the meta-AP score of [1], which is a document X  X  implicit average precision (AP) weight in a rank-ing. [10] used average meta-AP score as their predictor. We add to this, maximum meta-AP score and standard deviation of meta-AP scores, the last of which is a measure of the disagreement be-tween retrieval systems over what rank a document should be re-turned at. Note also that [10] assume that the assessment of the original assessor was available, and build separate models for the originally-relevant and originally-irrelevant conditions; in this pa-per, however, we assume no assessments have been made, and build a single model to predict assessor disagreement.
We use the multiply-assessed TREC 4 AdHoc dataset described by [7]. The dataset consists of 48 topics, with up to 200 relevant and 200 irrelevant pooled documents selected for multiple assess-ment by two alternative assessors, additional to the assessment of the topic author (who we refer to as the original assessor). We re-strict ourselves only to documents from the Associated Press sub-collection to avoid biases introduced by the non-random method of selecting documents for multiple assessment, and follow [7] in dropping Topics 201 and 214, as the original assessor found no documents relevant for the former, and the first alternative assessor found none relevant for the latter. We regard the assessment of a document as  X  X isagreed X  if the three relevance assessors do not all give the same assessment; this is the condition that our model will attempt to predict.
We build per-topic models (Section 2.1) for performing feature analysis (Section 4.1), but a universal model for ranking by pre-dicted disagreement (Section 4.2), since we assume that it is re-dundant to perform multiple assessments just to train up per-topic models in practice; learning-to-rank methods that adapt models for topics is left to future work. The model construction and evaluation method used in the disagreement ranking stage is described below.
We evaluate the quality of the rankings of documents by proba-bility of disagreement using 11 point precision X  X ecall curves, mean average precision, and precision at various cutoffs, with the ground truth being documents that the three assessors disagree upon the relevance of.
We first analyze the relationship between individual features and assessor disagreement by performing per-topic regressions (Sec-tion 4.1), then investigate the usefulness of these features as pre-dictors of disagreement by building and testing universal (cross-validated) models (Section 4.2).
We test our hypotheses that: (1) documents with higher compre-hension difficulty, (2) longer documents, and (3) documents that are less focused on a topic (less cohesive), are more likely to be disagreed upon. For each feature, we build a logistic regression model on each topic with that feature as the single predictor, and observe the coefficients that the feature achieves across the 48 top-ics (the  X  values in Equation 1). We calculate the average coeffi-cient, and perform a two-sided, one sample t-test to test whether this coefficient differs significantly from zero across the 48 topics.
Table 4.1 reports our results. The metarank features are all highly significant. Entropy is also a significant positive predictor. In so far as entropy measures topic diffuseness, this confirms our hypothe-sis that more diffuse documents provoke higher levels of disagree-ment. Many of the reading level predictors also prove significantly correlated with disagreement. Surprisingly, however, the correla-tion is in the opposite direction from the hypothesis. Documents that get lower reading level scores, and therefore are marked as being easier to reading, in fact provoke higher levels of assessor disagreement. (Recall that FleschIndex is the only reading level feature where higher scores mean easier comprehension.)
Next, we investigate how useful our method is at predicting as-sessor disagreement, using a universal (cross-validated) model to rank the documents of each topic by decreasing probability of as-sessor disagreement. Table 4.2 summarizes performances for aver-age precision and precision at various cutoffs. We add as a baseline the expected precision achieved by a random sorting of the doc-uments, which is just the macroaveraged proportion of disagreed documents per topic. A universal model that combines all our Table 1: Results of significance test using two-sided one samp le t-test with p-values and mean co-efficient scores across all 48 topics.
 Table 2: Performance Comparison at various ranks with signif -icant improvement over expected random scores indicated by * (paired t-test). The results are based on 5-fold cross validation across 48 topics. features (denoted by  X  X ll Combined X ) and a model that uses the metarank features significantly improves over random ordering un-der all measures. All the other features give a significant improve-ment over random order for MAP only, suggesting that top-of-ranking performance is mediocre. Entropy does best, as in Ta-ble 4.1, whereas the combined reading levels, despite being signif-icant correlated with disagreement give very little benefit in terms of predicting disagreement under a universal model.
We started this paper with three hypotheses, namely that the doc-uments that assessors are more likely to disagree on are: (1) docu-ments with higher comprehension difficulty; (2) longer documents; and (3) documents that are less cohesive. At least in so far as these three conditions are captured by the measures we have used, our results have been mixed. The correlation between entropy and dis-agreement confirms the third hypothesis, and provides a weakly useful practical predictor of disagreement. The relationship be-tween document length and disagreement (our second hypothesis), if it exists, is too weak for our experiments to detect as significant. Most surprisingly of all, our first hypothesis, that difficult docu-ments would provoke more disagreement, has not only failed to be confirmed, but in fact the reverse has been observed: it is easier documents that provoke the most disagreement.

As it seems intuitively hard to believe that it is in fact easily-comprehended documents that assessors disagree the most about, a more likely interpretation of our results is that the reading level measures are picking up some other aspect of document content, syntax, or representation that tends to provoke disagreement in as-sessors. An informal examination of disagreed-upon documents that attracted easy reading level scores, for instance, suggests that a disproportionate number of them are transcripts of spoken text X  presidential debates, speeches, interviews, and the like. These tend to have short sentences, but diffuse topics, and may be difficult to read quickly. Further work is to determine whether there are other text metrics that can more directly and accurately target the aspects of a document that predict assessor disagreement.

