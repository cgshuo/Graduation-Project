 1. Introduction
The oxygen supply into broths constitutes one of the decisive factors of cultivated microorganism growth and can play an important role in the scale-up and economy of aerobic biosynth-esis systems. The aeration efficiency depends on oxygen solubi-lization and diffusion rate into the broths on the bioreactor capacity to satisfy the oxygen demand of the microbial population ( Galaction et al., 2004 ). From the experiments carried out on biosynthesis systems of single-cell proteins using various water insoluble hydrocarbon substrates it was observed that the addi-tion of a non-aqueous organic phase may induce significant increase in oxygen transfer rate from air to microorganisms, without requiring a supplementary intensification of mixing ( Cascaval et al., 2006 ; Amaral et al., 2008 ). These compounds can enhance the oxygen transfer rate through microorganisms when added to growth media, due to their higher oxygen solubilization capacity compared with water. Thus, they were defined as oxygen-vectors.

The main oxygen-vectors used in biotechnology are hydrocar-bons and perfluorocarbons, as well as oil added as antifoam agent ( Amaral et al., 2008 ; Clarke and Correia, 2008 ; Correia and Clarke, 2009 ; Correia et al., 2010 ). Oxygen-vectors have no toxicity against the cultivated microorganisms and, in some cases, they could be used as supplementary sources of carbon and energy.

However, although the oxygen transfer has been previously modeled for different aerobic fermentation conditions, there is no information concerning the applications of the artificial neural networks (ANN) for the systems containing oxygen-vectors.
Compared to the classical modeling methods, the ANN approach does not require knowledge of the mathematical form between main variables and it can be applied when a description of the phenomena is not available due to complexity or nonlinear interdependencies between variables ( Aschi et al., 2007 ). Often, neural models provide better results than phenomenological ones, if a sufficient and representative set of data is available.
Because of the flexible capacity to learn from input data and the capability to grasp certain patterns that tend to be overlooked by common statistical methods ( Meng and Lin, 2008 ), as well as due to the potential good results, the multilayer perceptron neural networks are chosen for the current study.

The first step in using an artificial neural network for a specific problem is finding the best network topology, an important factor that determines the outcome of the problem being solved because it influences both functionality and performance ( Curteanu, 2004 ). The methods used for determining the best topology can be classified into the following types: (1) trial and error methods, based on successive trials ( Piuleac et al., 2010 ); (2) empirical or statistical methods ( Lisboa et al., 1998 ; Balestrassia et al., 2009 ;
Ghate and Dudul, 2010 ; Furtuna et al., 2010 ), which study the influence of different internal parameters and choose their optimal values depending on the network performance; (3) hybrid methods such as fuzzy inference ( Rutkowska and
Starczewski, 2000 ; Wu et al., 2007 ; Kasabov, 1996 ); (4) construc-tive (complexification)/destructive (pruning) methods, which develop the topology incrementally (adding or pruning neurons or connections) ( Ragg et al., 2007 ; Garcia-Pedrajas and Ortiz-
Boyer, 2007 ) ; (5) evolutionary computation techniques (ECTs) such as genetic algorithms (GA), differential evolution (DE) or cooperative co-evolution (CC). ECTs are methods based on popu-lation optimization techniques that use a simplified model of natural genetic evolution for performing searches for the solution ( Bedri Ozer, 2010 ; Curteanu and Leon, 2007 ). Various methods for developing neural network topology, with their advantages and disadvantages, along with some applications in chemistry, are presented in the Curteanu and Cartwright (in press) review. topology determination include the balance between exploitation and exploration (each search handles this dilemma using a stochastic selection of the parents of each generation), evaluation (user defined parameters control the search), parallelization (because the individuals can be trained independently, the search can be easily parallelized), scalability (the algorithms are scalable regarding computing time and performance), avoiding the local minima (using complexification/decomplexification methods, the local minima is avoided) ( Ragg et al., 2007 ).
 posed by Storn and Price (1995) . The guiding principle is the
Darwinian one, whereby new solutions of a problem are intro-duced into a pool of potential solutions by mathematically manipulating the existing ones. The algorithm has a simple and compact structure that utilizes common concepts of ECT in a stochastic direct search approach ( Subudhi and Jena, 2008 ).
Because it is a search method over continuous spaces like those describing an ANN structure, and because it is a robust algorithm with fast convergence, the DE algorithm is chosen to determine the topology and internal parameters of the ANNs used in this work. To obtain efficient models, the optimal (or near optimal) topology of ANN is an important requirement.
 applications: training (optimizing) ANNs and designing the topol-ogy of a network. Among the first attempts to optimize a neural network with the DE algorithm is that of Fischer et al. (1999) , the optimization being performed on a fixed architecture. The goal of the Fischer et al. X  X  work was to find a new training method, and the obtained results were encouraging. After that, new variations on the same idea appeared. Plagianakos et al. (2001) used the DE algorithm to train neural networks with discrete activation functions. Subudhi and Jena (2008 , 2009) combined DE and
Levenberg Marquardt algorithms to create a new hybrid method to train a neural network for nonlinear system identification. An attempt to determine a partial architecture of an ANN using
DE was made by Lahiri and Khalfe (2010) , the network being constructed by determining the best number of neurons in the hidden layer, the weights, and the activation functions. The topology was constrained to have only one hidden layer, but there can be good architectures with two or even three hidden layers. Bhuiyan (2009) proposed an algorithm based on DE to determine the topology and back propagation algorithm and to partially train the network during the evolution process.
As it can be observed, extensive work was done to train the neural network using DE, but the area of finding the best topology of the network with the DE algorithm is mainly unexplored. In this paper we propose a method for using DE to determine both the topology and internal parameters of two neural networks used for predicting and classifying the behavior of the simulated fermentation systems in the presence of n-dodecane as oxygen-vector, from the viewpoint of the oxygen transfer rate or oxygen mass transfer coefficient.

Because it is a global search algorithm, DE can search on vast spaces as those describing the topology and parameters of neural networks. The novelty of this approach consists in using the DE algorithm for performing the topology search and, at the same time, accomplishing the training phase for a series of neural networks used for solving specific behavior modeling. Two variants of the DE algorithm were used: the classical variant proposed by Storn and Price (1995) and the self-adaptive metho-dology proposed by Brest et al. (2006) . This approach was chosen for two main reasons: first, it has a simple self-adaptive mechan-ism (the equations describing the self-adaption are simpler com-pared to other proposed methodologies), and second, it performs well on a series of benchmarks ( Brest et al., 2006 ). Although the benchmark tests are good for testing the performance of algo-rithms, the real world problems are usually more complex and have dependencies that cannot be determined using specific tools.
Consequently, these variants of the DE algorithm were compared using different error m easurements in two case studies: the prediction and classificat ion of the oxygen mass transfer coefficient in stirred bioreactors. The prediction problem represents the determination of the results for sets of data whose outcome is not known, while classification represents the identification of the type in which an instance from the data group can be included.
RRMSE relative root mean squared error n number of neurons from all hidden layers
N number of training data np number of individuals in the DE population
P a power consumption for mixing of aerated broths p m probability of a component to be selected from the
S number of networks in the stack u i i th individual from the trial population obtained after
X list of vectors representing the current population of x i i th potential solution from the current populations of
The prediction of the oxygen mass transfer as a function of viscosity, air superficial velocity, specific power input and oxygen-vector volumetric fraction, as well as the classification of the oxygen mass transfer on three distinct classes determined by applying value ranges to the followed output were realized.
The advantages of employing neural network strategies for prediction and classification consist in reducing the time neces-sary to prepare and carry extensive experiments and saving money for expensive materials and equipment required in experi-ments. The expertise needed for the end user in setting and running the neural network application represents the biggest disadvantage of this approach.

In order to determine the best n eural network used to solve the prediction problem, simulations were made using two strategies involving individual neural networks and stacked neural networks. For the classification case, only the individual strategy was used because the results obtained were in an acceptable range of errors.
The paper is organized as follows. Section 2 presents the method used for data collection and the analysis performed in order to create the database necessary for modeling techniques. In Section 3, theoretical aspects of the DE algorithm are described. The results obtained out of the simulations are presented in Section 4. The last section concludes the paper. 2. Data collection and analysis
The experiments were carried out in a laboratory bioreactor (Biostat A, B. Braun Biotech International) of 5 l (4 l working volume, ellipsoidal bottom), with computer controlled and recorded parameters. The experimental equipment and the oper-ating parameters have been described in the previous papers ( Galaction et al., 2004 ; Cascaval et al., 2006 ).

In the experiments, the simulated broths (carboxymethylcellu-lose sodium salt solutions) with apparent viscosity in the domain of 10 X 330.5 cP have been used. Because of the difficulty implied by the in-situ measurement of viscosity during the experiments, the viscosity was measured before and after each experiment using a viscometer of Ostwald type. Both the experiments and viscosity measurements were carried out at a temperature of 25 1 C. n-Dodecane (SIGMA Chemie GmbH) was used as oxygen-vector (density 750 g l 1 at 20 1 C, oxygen solubility 54.9 10 3 gl 1 at 35 1 C and atmospheric air pressure ( Cascaval et al., 2006 )). Its volumetric fraction into the broth varied between 0.05 and 0.20.
 For determining the mass transfer coefficient of oxygen ( k the static method has been used ( Galaction et al., 2004 ). This method consists of the measurement of the rate of increase in dissolved oxygen concentration in broth after it was lowered by passing nitrogen gas through the system for about 20 min. The nitrogen gas flow was stopped when the oxygen concentra-tion was nearly 0 and it was followed by aeration at certain operating conditions (rotation speed, power input, aeration rate, etc.). The solved oxygen concentrations in broth were measured using an oxygen electrode of InPro 6000 Series type (Mettler Toledo). As it was underlined in the literature, because the k values were in all cases less than 0.4 s 1 , it was assumed that the response of the oxygen electrode to the change in the oxygen concentration is sufficiently fast and does not affect the accuracy of determination ( Galaction et al., 2004 ; Cascaval et al., 2006 ).
From a mathematical point of view, the system can be described by a nonlinear function f ( in 1 , in 2 , in 3 , in in 1 represents the apparent viscosity and its values are in the interval [10.4 10 3 , 330.5 10 3 ](Pa s), in 2 is the air superficial velocity and it can have values in the interval [8.36 10 4 5.02 10 3 ](m s 1 ), in 3 is the specific power input, which can take values in the domain [91.95, 963.3](W m 3 ) and in 4 oxygen-vector volumetric fraction with values in the interval [0.05, 0.2]. The output out represents the mass transfer coefficient of oxygen, varying from 0.14 to 10.8 ( k L a (s 1 )).
As it can be observed, the input data varies substantially, and, consequently, a normalization or standardization procedure is applied using Eq. (1). Normalization is a linear scale conversion that assigns the same absolute value to the corresponding relative variation ( Leeghim et al., 2008 ): x where x represents the current value, Min () is the minimum of the raw values and Max () is the maximum of the raw values.
This procedure is performed once at the beginning of the proposed methodology and can be considered as the first step in determining neural networks for process modeling using DE. The advantages of normalization consist in the reduction of the estimation error and the processing time required by the neural network training process ( Leeghim et al., 2008 ). These elements, among others, indicate overall efficiency. 3. Differential evolution for determining the best parameters of a neural network 3.1. DE algorithm
DE is a heuristic applied to various types of practical optimiza-tion problems. These kinds of problems are usually non-differ-entiable, noisy and simulation-based. These characteristics make them very difficult or even impossible to solve using gradient-based methods ( Kaelo and Ali, 2007 ). The DE algorithm has the following steps: 1. Initialization . Like all population-based algorithms, DE starts with a population of vectors representing the potential solu-tion: X  X f x 1 , x 2 , ... , x n g X  2  X 
The initial population is generally initialized by randomly generating a set of vector solutions, but there is also the alternative of introducing the solution vector from another source. The initialization plays an important role in overall algorithm performance because the initial population must be distributed throughout the entire problem space, a probability distribution function (PDF) being used to resolve this aspect.
To initialize the population, a uniform or non-uniform dis-tribution can be used. In this paper a set of three initialization methods are tested: Random (Uniform distribution), Gauss distribution (Normal distribution), and Halton points. Halton points are an example of quasi-random number sequence and are often used in the field of numerical integration ( Halton and
Weller, 1964 ). 2. Mutation . After initialization, a set of intermediary solutions called mutants are created by adding a randomly sampled vector with a scaled difference between other two vectors ( Price et al., 2005 ). In Eq. (3), the most simple mutation strategy is exemplified. It corresponds to the variant in which only a differential term is used: m where m i represents the mutant vector, x 0 is the base vector, x 1 and x 2 are the difference vectors and F is the scale factor. The base, the target and the difference vectors are different 3. Crossover . After the creation of mutant vectors, the current than the former. Initially, a start index (SI) is randomly generated, and the SI element is copied from the mutant to the trial vector.
Random values between 0 and 1 are generated and the elements from the mutant are copied to the trial one until they are less than
Cr . After that, all remaining elements are copied from the target vector. An example of exponent ial crossover is illustrated in Fig. 1 .
The Cr parameter, with values in the interval (0, 1], provides extra diversity to the trial vector pool, especially near 1 ( Price et al., 2005 ). Because the determination of the best possible value for this parameter is largely problem dependent, there is no known method generally applied that p rovides good results. However, thereareafewguidelinesgivendown.

The optimal values of Cr for exponential and binomial crossover are not necessarily the same. Zaharie (2007) observed that exponential crossover is more sensitive for values (0.9, 1] than for (0, 0.9]. In the case of the binomial crossover, Davendra and
Onwubolu (2009) consider that intermediate values produce good results, the general accepted values being in the interval [0.8 X 1.0].
The crossover method influences Cr and F parameters, exponen-tial crossover requesting an F value greater than binomial cross-over ( Zaharie, 2007 ). Because the crossover is not so important ( Qing, 2009 ), only the binomial version was tested in this work.
In literature, three methods fo r determining the best control parameters are known: deterministic (parameters are deter-mined using a deterministic law), adaptive (feedback informa-tion from the search process is used to determine the change ( Brestetal.,2007 )) and self-adaptive (the parameters are encoded directly into the algorithm ( Feoktistov, 2006 )).
The self-adaptive vari ants tend to behave better than the classical ones,asvariousstudiesreportencouragingresults( Brestetal., 2006 ; Das and Suganthan, 2011 ; Zhang and Sanderson, 2009 ; Coelho and Mariani, 2008 ; Thangaraj et al., 2009 ).

One of the first study on self-adaptation was performed by Brest et al. (2006) . Each individual received two characteristics related to the control parameters F and Cr . The idea is that these encoded control parameters determine better individuals, which have a higher probability of survival. F and Cr are calculated as: where rand i , i  X  1.4 are random generated numbers between 0and1; t 1 and t 2 are the probabilities to adjust the factors F and
Cr . In this work, we are referring to this methodology as Brest X  X  self-adaptive mechanism or simpler as jDE. 4. Selection . In this step, the next generation is created. If the trial vector u i has a better fitness function than the target vector, it will replace the target vector in the next generation pool of solutions. Using this principle, only the best solutions are chosen.

In the current application, the fitness function is in inverse ratio with the mean squared error in the training phase ( MSE training The neural network topology and internal parameters were optimized so that the maximization of the fitness functions is obtained.

The current generation is then replaced with the next genera-tion and steps 2 X 4 are repeated until a stop criterion is reached. In our application, the stop criteria are the mean square error reaching a very low level, or the number of epochs reaching a preset value (1000 in this case), whatever comes first. The other parameter that was kept fixed in the current work was np (population number) with a value of 400. Consequently, for each generation, 400 fitness evaluations, one for each generated trial individual, were performed. 3.2. DE and ANN
In this work, the DE algorithm was applied for simultaneous determination of neural network topology and internal para-meters. This algorithm was chosen based on the following main reasons: increased performance and suitability compared with other algorithms on various benchmarks and real world problems, simplicity and reduced number of control parameters. There are various methods that can be applied to the problem at hand (structural and parametric optimization of MLP neural networks used for modeling the oxygen mass transfer in the presence of oxygen-vectors) among which several examples can be enumer-ated: genetic algorithms, evolutionary strategies, empirical rules combined with training methodologies. Among all these meth-odologies, DE is distinguished as a simple and powerful algorithm that has been seldom used for simultaneous optimization of topology and internal parameters of neural networks.

Determining a good topology, along with the best internal parameters, supposes finding the suitable values for which the results returned by the network are in a global minimum of the problem being solved. The ANNs used in this work are the multilayer perceptrons (MLPs). A MLP is a feed-forward artificial neural network with an architecture organized in layers. Each layer is fully connected to the next, with no loops in their internal structure. MLP is the best known and the most used type of neural network. In the current work, this type of ANN was chosen because it has a simple structure and is a universal approximator ( Hornik et al., 1989 ).

The objective function of the optimization problem (fitness function of DE) is calculated according to Fitness  X  1 MSE where low is a parameter introduced to eliminate the case in which the denominator is equal to 0. The value of the low parameter was a very low value represented by exp ( 10).
The number of neurons in the hidden layers is the most common parameter chosen by all researchers approaching this problem, the other parameters varying from one researcher to another. The parameters considered in this work are the number of hidden layers, the number of neurons in each hidden layer, the activation functions for each neuron, the biases for each neuron and the input weights associated with each neuron. Consequently, five decision variables are considered for the optimization, among which the number of hidden layers and the number of neurons in each hidden layer represent structural variables and the other three, the activation functions for each neuron, the biases for each neuron and the weights associated with each neuron are the variables for parametric optimization.

An important aspect in modeling the architecture and training of a neural network using DE is coding the neural network in the right manner. If the coding is done such that the differential evolution algorithm does not understand it or cannot work wit h it, the results will not be the expected ones. To solve this problem, a floating-point vector was created that contains all the information for the neural network. The structure of the vector is shown in Fig. 2 . L represents the number of hidden layers, H 1 is the number of neurons in the first hidden layer and H 2 is the number of neurons in the second hidden layer. For each neuron, k , the weight is indicated by w k b and the activation function by a k . The activation functions used are step, linear and bipolar sigmoid functions, each being coded using integer numbers. The number of inputs, outputs, hidden layers and neurons in each hidden layer are also integers, the other parameters being represented using real values.

The number of total values that need to be optimized is the same as the length of the encoded vector that depends on the topological characteristics of the network. For example, for a network with one hidden layer with 10 neurons, the length is determined by 1 [characteristic indicating the number of hidden layers]  X  2[two characteristics reserved for the number of neurons in the first and second layers]  X  (inputs 10  X  10 outputs) [characteristics contain-ing all the weights]  X  (10  X  outputs) [biases]  X  (10  X  outputs) [activa-tion functions]  X  23  X  10 inputs  X  12 outputs.

Most of the real world problems can be modeled with great accuracy by networks with two hidden layers, and, for this reason, the neural network is limited to two hidden layers in the proposed application. Because the search space for optimizing the neural network is large, even when a maximum number of two hidden layers is used, limiting the maximum number of the neurons allowed in the hidden layers of the networks represents one of the methods that can be applied to reduce the number of characteristics that need to be optimized.

The optimal number of hidden neurons depends on a series of factors such as the number of inputs and outputs, the number and accuracy of the training data, the complexity between the input X  output mapping, the architecture, the type of hidden unit activa-tion function, the training algorithm, etc. ( Xu and Chen, 2008 ).
A simple rule indicates that n  X  N / d ( Xu and Chen, 2008 ), where n is the number of hidden neurons, d is the input dimension of the target function and N is the number of training data. In our work, 75% of the overall data (229) was used for training. Consequently n  X  171/4  X  43. In Xu and Chen (2008) it is specified that if this formula is used, a number greater than 30 neurons is determined, then the optimal n is close to ( N /( d log N )) 1/2 . With the last formula n  X  6. Because 43 is quite high and 6 quite low, and because these values were determined using the case when 30 neurons are obtained with a simple equation, 30 is considered to be an acceptable number of neurons (as a maximum limit) for the problem at hand.

Fernandes and Lona (2005) show that when the number of inputs is larger than the number of outputs (as in our case), the number of neurons in the first hidden layer is between 10 and 20 and in the second hidden layer between 15 and 25. Taking into consideration references Xu and Chen (2008) and Fernandes and Lona (2005) , and the recommendation that the number of neurons in the first hidden layer must be larger than in the second hidden layer, a maximum of 20 neurons in the first hidden layer and 10 in the second layer were chosen for the current application. network which models the current problem is 353 and it has four inputs and one output. Also, this value represents the dimension-ality ( D ) of the search space for DE algorithm. Based on this value, the np parameter of DE algorithm has to be set. Various research-ers considered that np should be high (3 D  X 10 D ) in order to have a fast convergence rate ( Feoktistov, 2006 ), but, at the same time, it must be as small as possible to avoid long computational time.
Taking into consideration that the dimensionality of the space is already high, a much larger np than D was not desirable, and, as a result, the value of np was set to 400. Because the number of generations was set to 1000, it results that a total number of 400,000 for function evaluation is performed. This is an accep-table value, considering the number of parameters that need to be optimized.
 coded vector. Because DE is constructed based on the idea of using real values, in the case of integer values, the normal behavior is changed in order to obtain proper results. At the last step of each iteration (before computing the fitness data, and, implicitly, before transforming the DE individuals into neural networks), the integer parameters change their values by round-ing to the closest integer. The formula applied in this case is ch  X  where ch indicates the value of a characteristic of the individual. 4. Results and discussion presence of n-dodecane as oxygen-vector, a database with 229 data was used, represented by the oxygen mass transfer coeffi-cient in the simulated broth.
 testing data (25%). The inputs and outputs of the neural network were fixed, their number being determined by the database structure in use. The four inputs were apparent viscosity (Pa s), air superficial velocity (m s 1 ), specific power input (W m oxygen-vector volumetric fraction, and the output was the mass transfer coefficient of oxygen ( k L a (s 1 )). The goal of the neural modeling is to evaluate the effectiveness of the mass transfer of oxygen depending on working conditions in bioreactors in two distinct situations: prediction and classification.
 between the inputs and the outputs in order to evaluate the outputs for sets of unknown data with high accuracy. In fact, the mass transfer coefficient of oxygen is evaluated as function of working conditions. The classification problem consists in placing the indivi-dual items into groups or classes based on quantitative information on one or more characteristics. In the current work, the classes are createdbyintroducingvaluerangesforthemasstransfercoefficient.
For both of these two problems, neural networks determined in various runs of the DE algorithm found near-optimal solutions. 4.1. Prediction involved in growth of the cultivated microorganisms, the accurate prediction of this parameter based on modeling methods having a decisive influence in optimization and scaling-up of the fermen-tation process.

The most important aspect in determining the near-optimal neural network architecture with DE was finding the best para-meters of the algorithm. Because various papers suggested that the optimal values for Cr parameter are close to 1, the interval used in this paper for testing it was [0.9, 1). The minimum values for F , determined with Eq. (2), in conditions in which np was fixed to 400, were 0.037 for Cr  X  0.9 and 0.035 for Cr  X  1. Consequently, the interval used in simulations for the F parameter was [0.1, 1.4].
Table 1 contains the fitness indices and the network topologies obtained through DE simulations for neural networks with max-imum 20 neurons in the first hidden layer and maximum 10 neurons in the second hidden layer. The first column and row indicate the variations in the F and Cr parameters, respectively. Because the fitness function (Eq. (9)) was in inverse ratio with
MSE training , and because the smaller values for this parameter indicated the better neural networks, the optimal values of F and Cr were chosen by the greatest fitness value, highlighted in Table 1 by bold style.

The topology is represented in the round brackets, the coding corresponding to inputs:neurons_first_hidden_layer:neurons_se-cond_hidden_layer:ouputs for a neural network with two hidden layers, and inputs:neurons_hidden_layer:outputs for a network with one single hidden layer. For instance, (4:7:7:1) represents a neural network with 4 neurons in the input layer, 2 hidden layers with 7 and 7 neurons and 1 output layer with 1 neuron.
This is a feed-forward neural network type; in this work, the DE algorithm is designed to develop only multilayer perceptrons.
The best results were obtained when Cr  X  0.98 and F  X  0.4, for a network with 1 hidden layer and 6 neurons in the hidden layer.
Further tests with classical DE approach used these values of F and Cr as default settings for the DE parameters.

In order to obtain a better neural network and to determine the influence of the initialization method on the results, a series of simulations using random initialization, Halton points and
Cauchy distribution were made. Due to the random factor of the initialization, mutation and crossover steps, for each simulation performed, different networks with specific topology and para-meters were obtained. The DE algorithm has a random factor that influences the results and, therefore, multiple runs were made in order to determine the best solution. Ten simulations are listed in
Table 2 . The results obtained by the current DE algorithm cannot be mixed to create a medium value per variant; therefore, the data was sorted in an ascending order, the best fitness values being positioned at the 10th index. As shown in Tables 1 and 2 , sets of pairs are formed by the fitness value and the correspond-ing network topology.

The best results were obtained using the Gaussian initializa-tion type but, for the current problem, it can be observed that there are no major differences between the three initialization versions. The mean and the variance values for the Gaussian initialization were 0.5 and 0.25, respectively. As a result, the best architecture chosen was the one characterized by the fitness value equal to 45.889.

In order to appreciate the results of the obtained neural network, four types of errors are used: mean square error (MSE), root mean square error (RMSE), relative root mean squared error (RRMSE) and average percent relative errors. These measures of precision were computed, both in training and testing phases. The average percent relative error was determined using the following formula: Er  X  1 N where exp and net indices denote experimental and neural network values for the mass transfer coefficient of oxygen ( k and N represents the total number of data.
 prediction of the oxygen mass transfer coefficient has perfor-mance indices relatively good ( MSE training  X  0.0217, MSE 0.0218, Fitness  X  45.889, RMSE testing  X  0.875 and RRMSE testing 0.860), minimum MSE, RMSE, RRMSE and maximum fitness functions being required for acceptable results.
 mechanism is better than the classical approach, a series of tests using the jDE approach (a self-adaptive variant proposed by Brest et al. (2006) ) was also performed to determine which method is better for our case study. The results of ten simulations realized with the Gauss initialization and jDE are listed in a sorted ascending order in Table 3 .
 dology has a 4:6:1 topology with MSE training  X  0.0110, MSE 0.0127, Fitness  X  90.260, RMSE testing  X  0.667 and RRMSE testing 0.669. Comparisons between the predictions of the neural net-work (4:2:1) determined with classical DE approach, as well as predictions of the (4:6:1) network determined with jDE, and experimental data are given in Figs. 3 and 4 for the training set and the testing set, respectively.
 with the two DE approaches, excepting the weights, are listed.
Each neuron has an activation function and an external parameter called bias, which controls the threshold level. In the current work, three types of activation functions were used: step, linear and bipolar sigmoid functions, which are referred to as Step ,
Linear , and Sigmoid , respectively ( Table 4 ). In order to indicate each neuron, notations (Hp.q) and (O.k) were used to represent the q th neuron from the p th hidden layer and the k th neuron from the output layer, respectively. The values for biases are computed within the DE algorithm.

The best network determined with the self-adaptive metho-dology has significant improved performance compared with the best network determined with the classical DE approach. These results tend to confirm the superiority of the self-adaptive mechanism observed in other studies. But, although the perfor-mance indexes were better for the (4:6:1) network, average percent relative errors more than 20 % were registered in training and testing stages, which means that the neural network did not simulate the data to a satisfactory degree of performance.
These results forced the introduction of a new approach using multiple neural networks combined in a structure called stack or aggregated neural network. The neural network stacks were known to perform a better generalization task than a single neural network.
 Creating stacked neural networks implies two major steps. The first step consists in generatin g the ensemble which represents a list of neural networks and, the s econd, efficiently combining the previously created networks ( Torres-Sospedra et al., 2008 ).
Multiple alternatives can be used for combining the networks, among which Torres-Sospedra et al. (2006) enumerate Output Average, Majority Vote, Winner Takes all, Borda count, Bayesian Combination, Dynamically Averaged Networks, Stacked General-ization and Stacked Generalization Plus. The choice of using one of the above methods is determined by the nature of application the stacked network is created for.

In this paper, two methods were used to create the stacked neural network: Output Average and Stacked Generalization. The Output Average is a method in which the result is determined by averaging the outputs of the individual neural networks. In the case of Stacked Generalization, instead of choosing static weights, a new neural network adjusts the weights ( Yang et al., 2002 ). The Output Average is the most used method encountered in literature but, being a linear method, it cannot account for possible nonlinear correlations between ensemble members. Due to the ability of modeling the nonlinear correlations, Stacked Generalizations was used as the first method of creating stacked neural networks.

The results obtained with the two DE approaches for a series of 10 simulations and an ensemble of 3 networks, individually determined at each simulation and described as 1st ANN, 2nd ANN and 3rd ANN, are listed in a sorted order in Table 5 . For the Output Average method, General results indicate the fitness function calculated for the entire structure of stacked neural networks. For the Stacked Generalization method, General results given by the 4th ANN indicate the fitness function and the topology of the last neural network used for weight determination. At the same time, the row General results given by the 4th ANN reflects the overall fitness of the stacked neural network structure because the 4th network also calculates the final output and the performance indices.

As it can be observed from Table 5 , the results obtained with the Stacked Generalization method are better than the ones determined with the Output Average method, in both versions of the DE algorithm. As expected, the self-adaptive variant jDE outperforms the classical approach, the best neural network having (4:8:1) topology with MSE training  X  0.002, MSE testing
RMSE testing  X  0.511, RRMSE testing  X  0.477 and Fitness  X  401.491. In the testing stage, average percent relative error computed with Eq. (9) was about 3.5% and the correlation had the value of 0.982.
The Friedman multivariate adaptive regression spline method ( Friedman, 1991 ) was also applied to the same training and testing data in order to determine whether the results obtained with the best stacked neural networks found by the DE algorithm are better and the methodology of using evolutionary algorithms to structural and parametric optimization of neural networks is appropriate in the case of modeling the oxygen mass transfer coefficient. For the testing data, the errors for the multivariate adaptive regression spline method were MSE  X  1.242, RMSE  X  1.1145,
RRMSE  X  0.4279, the percent relative error  X  81.554% and the correlation  X  0.916.

The differences between the desired results, the obtained results using neural networks determined with jDE approach and the ones determined with the Friedman multivariate adap-tive regression spline methods were compared in Fig. 5 for the training data and in Fig. 6 for the testing data.

As it can be observed by comparing Fig. 3 with Fig. 5 and Fig. 4 with Fig. 6 , in the case of stacked neural networks, the differences between simulation and experiment were lower than those provided by the individual neural networks. In addition, for the stacked neural networks determined with the jDE algorithm, predictions are closer to the experimental data than the ones determined with the Friedman multivariate adaptive regression spline method. This indicates that the methodology using stacked networks is more suitable to this case study than the Friedman approach. Although good results were obtained with stacked neural networks, when time is an important factor, the method is not recommended because the time span required for the determination of a stacked neural network is greater than the one needed for a single neural network, with a factor greater than or equal to the number of networks in the stacked version. 4.2. Classification
The goal of this problem is to individually place items into groups or classes based on the mass transfer coefficient and to efficiently determine the corresponding classes for unknown data.
The same input data was used as in the prediction problem (viscosity, aeration rate, specific power consumption and oxygen-vector volumetric fraction), but the output indicates a series of classes corresponding to the following mass transfer coefficient ranges: 0 X 5 10 2 , 5 X 8 10 2 s 1 and the last one includes mass transfer coefficients greater than 8 10 2 s 1 . The consid-ered ranges of mass transfer coefficient are relevant for selecting the operational or constructive characteristics of the bioreactor or sparger X  X mpeller system. In direct relation with the value of oxygen mass transfer rate (implicitly, of aeration process perfor-mance), depending on the broths viscosity of the biomass con-centration and morphology, certain parameters for operating the bioreactor have to be considered: less or more intense mixing, less or more intense aeration, re-designing the impeller type, baffles and sparger. The limits of the mentioned domains of the mass transfer coefficient correspond to the most encountered situation in real aerobic fermentation systems.
 The class coding and the correspondent values are listed in
Table 6 . For example, if a value is in the 0 X 5 10 2 s 1 range, the class associated with it is coded as 001.

The number of outputs corresponding to the neural network used to solve this problem is equal to the number of classes created in order to classify the data.

Several tests were carried out using the classical DE approach and the jDE self-adaptive variant. For the classical DE, the control parameters were fixed to Cr  X  0.98, F  X  0.4 and, for both strategies, a Gaussian approach was used as initialization method. Surpris-ingly, the best network was determined by the simple DE version and not by jDE, as it can be observed in Table 7 , where a list of ten ordered fitness and topology results for each strategy is shown. These results indicate that, when the control parameters are well chosen, and depending on the approached problem, the classical DE version can outperform the jDE self-adaptive variant. The parameters of the best network obtained are listed in Table 8 . The same notations as those in Table 4 are used. The error coefficients resulted using the best neural network are shown in Table 9 .

Good results were obtained for all three classes (percentages of correct answers more than 90 % for each class, in both training and testing phases). In the training phase, the percentage of the correct classes predicted by the best network was 95.32% and in the testing phase it was 96.49%. This indicates that the results obtained with the best neural network determined were accurate, which means that the network modeled the classification pro-blem very well. In addition, due to the high rate of success, the stacked neural networks methodology was not tested in this case.
For the oxygen mass transfer coefficient, in the presence of n-dodecane, the DE algorithm found better networks when classification, rather than prediction, was pursued. 5. Conclusions
Differential Evolution (DE) is an optimization method based on principles of evolutionary algorithms. The main idea of this technique is to provide new items to a pool of potential solutions by mathematically manipulating the existing ones.

In this paper, DE was applied to develop the best neural network, which means finding the suitable parameters of the network for which the results are in global minimum of the problem being solved. The parameters considered in this work for a feed-forward configuration were the number of hidden layers, the number of neurons in the hidden layers, the activation functions for each neuron, the biases for each neuron and the weights associated with each neuron.

The oxygen mass transfer is the most important parameter involved in the growth of the cultivated microorganisms, the accurate prediction of this parameter based on modeling methods having a decisive influence on optimization and scaling-up of the fermentation process. The efficiency of the oxygen transfer into the fermentation broths could be enhanced by adding oxygen-vectors, such as hydrocarbons or fluorocarbons, without increas-ing the energy consumption for mixing or aeration.
 tion and classification. For the prediction problem, two approaches, i.e. with simple and stacked neural networks, were considered, while for the classification problem only the simple ANN approach was tested.
 motivated by the fact that the simple ANN had a low performance and various studies indicate that stacked neural networks per-formed better than the simple ones. As expected, better results were obtained using the second strategy, but the approach had a major drawback: a long computational time determined by the optimization of S  X  1 neural networks, where S represents the number of networks in the stack. Consequently, it was considered that the stacked neural approach must be used only when the computational time is not an issue.
 applying a discretization method. Value ranges for the mass transfer coefficient were established and each instance was associated with a specific class based on the corresponding interval. A simple feed-forward neural network with two hidden layers, designed through a procedure using DE algorithm, pro-vided accurate results.

Comparing the models obtained with the classical DE approach and with jDE mechanism, it can be observed that jDE is better in providing good results in the prediction case, when using both single and stacked neural networks, while in the classification case it fails in determining the best network.
Consequently, it can be concluded that the self-adaptive mechan-ism not always outperforms the classical DE, depending of the type of problem (and process) to be solved.

Based on the low errors obtained in both the cases (prediction and classification), the DE algorithm proved to be an adequate solution for obtaining the best architecture of a neural network.
Due to its flexible capacity, the modeling methodology devel-oped in this paper can be easily extended for other experimental data and applications.
 Acknowledgments This work was done by financial support provided through Project PC 71-006/2007, in the frame of the National Program for Research,
Development, and Innovation II and by EURODOC  X  X  X octoral Scholar-ships for research performance at European level X  X  project, financed by the European Social Found and Romanian Government.
 References
