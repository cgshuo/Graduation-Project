 One of the biggest challenges of commercial search engines is how to handle tail queries, or queries that occur very infrequently. Fre-quent queries, also known as head queries, are easier to handle largely because their intents are evidenced by abundant click-through data (query logs). Tail queries have little historical data to rely on, which makes them di ffi cult to be learned by ranking algorithms. In this paper, we leverage knowledge from two resources to fill the gap. The first is a general knowledgebase containing di t granularities of concepts automatically harnessed from the Web. The second is the click-through data for head queries. From the click-through data, we obtain an understanding of queries that trig-ger clicks. Then, we show that by extracting single or multi-word expressions from both head and tail queries and mapping them to a common concept space defined by the knowledgebase, we are able to transfer the click information of the head queries to the tail queries. To validate our approach, we conduct large scale exper-iments on two real data sets. One is a mixture of head and tail queries, and the other contains pure tail queries. We show that our approach e ff ectively improves tail query search relevance. Categories and Subject Descriptors I.2.0 [ Artificial Intelligence ]: General; H.2.8 [ Database Manage-ment ]: Database applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval. General Terms Algorithms; Experimentation Keywords Knowledgebase; Short Text Conceptualization; Query and URL Understanding; Search Relevance. Web search queries follow a heavy tailed Zipf distribution [31]. We classify queries into head queries and tail queries [8]. Head queries, also known as frequent queries, are associated with rich historical click information. This enables search engines to utilize statistical models to predict the URL X  X  click-through rate as search relevance [19, 28]. Tail queries, also known as rare queries, do not Figur e 1: An example of tail query search solution. q is a new coming tail query that has not seen before. q 1 ;  X  X  X  ; q historical queries triggered the clicks of URLs u 1 ;:::; have much user click information. Unfortunately, tail queries con-stitute the majority of unique queries submitted by users in their day-to-day use of search engines. It is very valuable but a big challenge for search engines to improve search relevance for tail queries.

For example, we can evaluate query and document similarity in the TFIDF (term frequency, inverse document frequency) vector s-pace as a relevance measure. This leads to a list of candidate URL-s. As shown in Fig. 1, for a new tail query q , we obtain candidate URLs u 1 ;:::; u J by query document similarity. However, many of them may have low relevance even though they have high similari-ty, because simple content similarities using TFIDF may be di ent from the users X  intents as clicks can reveal. For example, when a user searches a very rare car insurance, e.g.,  X  X eely king kong car insurance, X  he really means the insurance of the particular car rather than the car itself or even the character King Kong who dam-aged cars in a movie. Thus, the challenge is how to better estimate the intents of tail queries?
Several approaches have tried to use search logs to improve search relevance for tail queries [12, 13, 16, 29, 34]. For example, we can use the set of queries that historically triggered clicks of a URL as the description of the URL, since if a query triggers the click of a URL, we regard that the URL satisfies the intent of the user. An example of this approach is shown in Fig. 1. Given the goal is to rank URLs u 1 ;  X  X  X  ; u J by their relevance to q , the relevance can be converted to a similarity measurement between q and q 1 ;  X  X  X  ; this model. Since the link between q i and u j can be weighted by the number of clicks of u j triggered by q i , the user intent of tail query q can be estimated by the weighted average of the historical queries X  intents.

The above approach of converting the relevance problem into measuring click-weighted query similarities could be useful in prac-tice. Nonetheless, there is still some drawback, since a query is just a piece of short text, and there is not su ffi cient contextual infor-mation to compare the semantics of two short texts. For exam-ple,  X  X eattle hotel jobs X  and  X  X eattle hotel X  are two queries, and they have relatively smaller edit distance than the distance between queries  X  X ars planet X  and  X  X enus. X  However, the latter two queries are more semantically similar and the former two represent dif-ferent meanings. Therefore, the string similarity or bag-of-words based similarity for queries are not enough to evaluate the semantic similarity. Several approaches tried to use latent topic models [13] or deep learning model [17] to extract the latent or hierarchical se-mantics of queries. In practice, they are slow to train and test. Thus, they are only applied to query-title similarities but not historical queries [17, 13]. However, we will show that, considering both the semantics about historical queries and the user clicks can be much more useful.

In this paper, instead of evaluating query similarity base on their surface forms or latent topics, we use explicit semantic representa-tion of queries. We then evaluate the query similarity in the seman-tic space. For example, we know  X  X eattle hotel jobs X  is di from  X  X eattle hotel, X  since  X  X obs X  is mapped to concepts di from the concepts  X  X eattle X  and  X  X otel X  are mapped to. Moreover, both  X  X ars planet X  and  X  X enus X  can be mapped to the concept plan-et or planet from the Sun . In this case, the similarity between these two queries is much larger than the similarity evaluated based on the strings. Thus, if we can correctly parse queries into sub-phrases and identify their concepts, we can relate queries which are di ent on their surfaces. Then the challenges are how to parse queries into single or multi-word expressions automatically, how to create the concept space, and how to map queries to the concept space. The concept space should be large enough to have broad coverage of queries and should be with di ff erent granularities of semantics to describe all kinds of queries.
 To obtain the concepts, we use a probabilistic knowledgebase, Probase [36]. Probase contains several millions of concepts and related instances and attributes names, which are extracted from a text corpus of 1.68 billion Web pages [36]. It covers more than 80% of Web queries evaluated on 50-million randomly selected queries from a real-world search engine [36]. Since the concepts, instances and attributes are single or multi-word expressions, they enable us to detect terms in queries which are more semantically meaning-ful. Then we extend the technique called  X  X hort text conceptual-ization X  [32] to capture multiple topics inside a query using simple but e ff ective clustering, and consequently provide a general way to map both queries and URLs to the concept space.

Our contributions are the following two key components that to-gether enable us to better characterize tail queries with semantics for search relevance.
In this section, we briefly review the methods of query expansion and URL representation for improving search relevance.
Query expansion approaches mainly fall into two categories: corpus-based and general-purpose knowledge-based. The corpus-based approaches use snippets [37], anchor texts [21] or search log ses-sion data [30] to cluster historical queries. The general-purpose knowledge-based approaches involve di ff erent knoweldgebases, in-cluding WordNet [25], Wikipedia [24], ConceptNet [14, 20] or the combination of multiple knowledgebases [6, 15, 26].

However, corpus-based and general-purpose knowledge-based methods are insu ffi cient for the tail query problem. Given a tail query, even if we can expand it correctly, the expanded queries might still be tail queries. In addition, one should be very care-ful using the expanded queries, because search results are sensitive to terms. Therefore, in practice, query expansion is often used as a preprocessing technique to find synonyms, morphological forms of words or to perform spell checking. Moreover, query expansion can be used as a tool to do query suggestion after the user submits a query.
Since URLs cannot be directly compared with queries, they are usually represented by related texts. Besides the document content of a URL, real-world search engines also seek other stronger sig-nals to help improve the similarity measure between queries and URLs to improve relevance. There are two main types of fields used to construct the representation of URLs. The source gener-ated by the author of a Web page is called the content field, while the source generated by the other authors is called the popularity field [29, 34]. Content fields include page body, title, and the URL text. Unlike content fields, popularity fields can be the anchor texts, and the queries that trigger the clicks, which can better capture us-er X  X  intent information [29, 34].

Inspired by the initial work proposed in [29, 34] to use populari-ty fields to help search, there have been several advanced statistical methods developed [12, 13, 16]. Huang et al. [16] demonstrated that phrases (or multi-word expressions) can be more useful than words for modeling the similarity between queries and URL rep-resentations, using an n-gram language model. Gao et al. also extended the word-level translation model [2] to the phrase-level translation [12], and showed that phrases with unrestricted lengths can help improve search relevance. Language models and trans-lation models are very powerful tools, however, they  X  X o not map di ff erent terms into the semantic clusters but only learn the mapping relationships directly between a term in a document and a term in a query X  [13]. Therefore, sometimes the semantically related queries are not identified using such models. Thus, Gao et al. [13] intro-duced a latent topic model to improve the bag-words-model. They claims that latent topic models [3] can e ff ectively capture the se-mantic meaning of queries and documents. More recently, Huang et al. improved the latent model by learning a deep structure to incorporate hierarchical semantic information [17]. However, they are still word based instead of phrase based. Moreover, training such latent models is time consuming, and thus these latent models are only applied to the title field. In addition, latent models are not explicitly understandable for human.

Our work also addresses the problem with single or multi-word expressions, but we instead use a Web-scale knowledgebase. Our knowledgebase contains concepts, attributes, instances, and their relationships with good precision and coverage of Web queries [36]. Thus, the multi-word expressions extracted using the knowledge-base are more meaningful. Moreover, the knowledgebase contains a set of taxonomic concepts with di ff erent granularities, which re-flect di ff erent levels of semantics. Thus, mapping the terms (ex-tracted single or multi-word expressions) to the explicit concept space can naturally incorporate the hierarchical semantic structure as deep learning model [17]. Egozi et al. [9] have demonstrated that mapping queries to the explicit concept space can significantly improve the search relevance results, however, they haven X  X  incor-porated the historical click information or other popularity fields. In this work, we combine the benefits of explicit semantic analysis which can be understood by human for debugging, and the pop-ularity fields that reflect users X  intents, and develop a new model. Our model can transfer the understanding of queries from head to tail by mapping queries to the same concept space, and leveraging the click information.
In this section, we briefly introduce how we acquire relevan-t knowledge, and the basic idea of conceptualization. There are many knowledgebases including manually built ones such as Word-Net [11], Open Directory Project (ODP) 1 , Wikipedia 2 , Cyc [22], and Freebase [4], and automatically constructed ones such as Know-ItAll [10], TextRunner [1], WikiTaxonomy [27], YAGO [33], NEL-L [5], and Probase [36] 3 .

In this paper, we use Probase as the knowledge source that fill-s the gap between sparse input and understanding. Probase con-tains more than two million concepts learned iteratively from 1.68 billion web pages. The taxonomy is probabilistic, which means every claim in Probase is associated with some probabilities that model typicality, correctness, ambiguity, and other characteristic-s. The probabilities are derived from evidences found in Web da-ta, search log data, and other available data. The knowledge we acquire centers around two relationships that are associated with concepts, namely, the isA relationship, e.g., China isA emerging market , and the isAttributeOf relationship, e.g., population isAt-tributeOf country . We acquire such knowledge through iterative information extraction from web data. Specifically, to obtain isA relationships, we use Hearst patterns and other syntactic patterns. For instance, from a sentence  X ... European artists such as Pablo Pi-casso ..., X  we obtain Pablo Picasso isA European artist . To obtain isAttributeOf relationships, we consider sentences such as  X  What is the population of China? X , from which we obtain population isAt-tributeOf China . We perform sophisticated inference to clean and enrich the knowledge acquired from the syntactic patterns [36].
In order for the knowledge to be useful for conceptualization or understanding, it needs to satisfy two requirements. First, the knowledge should have a broad coverage. People use millions of concepts (e.g., emerging market , European artist , etc.) in their communication and a query can contain any of those concepts. Thus, we need a general knowledgebase that can cover as many concepts as possible. Existing knowledgebases [1, 4, 10, 11, 22, 27, 33] contain a limited number of concepts, which limits their power in interpreting the natural language. Probase covers more than 80% of Web queries evaluated on 50 millions randomly se-lected queries from a real-world search engine [36], which means it can cover almost all the head queries and most of the tail queries. This lays the foundation for understanding the human generated queries which express subtle meanings and intents.
 Second, in order to support inference, the acquired knowledge in Probase provides probabilistic information. For example, the prob-http: // www .dmoz.org / http: // www.wikipedia.org / http: // research.microsoft.com / probase / ability P ( instance | concept ) tells us how typical the instance the given concept . Intuitively, knowing that both  X  X obin X  and  X  X en-guin X  are birds is not enough. We need to know robin is a much more typical bird than penguin, that is, when people talk about birds, it is more likely that they are thinking about robins than pen-guins. Such information is essential for understanding the intent behind a short text. Besides P ( instance | concept ), we also obtain P ( concept | instance ), P ( concept | attribute ), and P ( attribute These values are obtained during the information extraction pro-cess, for example: where n ( instance ; concept ) denotes the number of times instance and concept appear in the same sentence in a corpus, and n ( concept ) is the frequency of the concept . We use external knowledgebases for short text understanding. We first map queries (head queries and tail queries) to the concept space (Section 4.1) and then we expand URLs by adding textual context so that we can map URLs to the concept space as well (Section 4.2).
Given a query, which consists of a set of terms (single or multi-word expressions), we want to infer the concepts from the query. We call the process conceptualization. Fig. 2 shows two examples of conceptualization. We can see that from terms  X  X icrosoft X  and  X  X pple, X  we get company related concepts, while for terms  X  X ear X  and  X  X pple, X  we get fruit related concepts. However, queries are complicated. For example, they often contain multiple topics (e.g., a query such as  X  X bama X  X  real-estate policy X ). In this section, we describe the techniques of conceptualizing any short text. Before mapping a query to concepts we want to parse it mean-ingfully into a set of terms that exist in our knowledgebase. For example, for query  X  X isposable chopstick manufacturer, X  we ob-tain { X  X isposable chopstick, X   X  X anufactures X  X , where both are the longest terms in the knowledgebase that appear in the query (the knowledgebase also contains  X  X hopstick X  as a term, but we assume it does not contain  X  X isposable chopstick manufacturer X  as a sin-gle term). Similarly, there are many cases where we should make decisions to find the most meaningful terms in the queries.
In general, suppose our knowledgebase contains a set of terms { t ;  X  X  X  ; t concept c . A piece of short text consists of a sequence of words. A term occurs as a contiguous sequence of words in a short text. We use l ( t ) to denote t  X  X  length. For example, l ( X  X isposable chopstick X ) = 2. We use n ( t ) to denote the number of concepts t corresponds with. For example, if t =  X  X pple X  corresponds with the concept-s of fruit and company only, then n ( t ) = 2. A term may corre-spond to a large number of concepts. As an example, we have n ( X  X riving school X ) = 18 and n ( X  X an Diego X ) = 327.

Our parsing method tries to find the Longest Covering Term (LCT) of short text (see Algorithm 1). We say a term t covers a word w if both t and w appear in the short text and t contains w . For example,  X  X isposable chopstick X  covers  X  X hopstick. X  The algorithm ensures that every word in the short text is covered by a term (if possible). We denote t i  X  t k if l ( t i ) &gt; l ( t k ), or if n ( t
Here, we use instance to denote a sub-concept or an entity. more important the concept is to represent the text.
 Algorithm 1 The Longest Covering Term Algorithm 1: Input: a short text; 2: Best covering term set T  X  X  X  ; 3: Break the text into a sequence of words ( w 1 ;  X  X  X  ; w 5: for i = 1 to N do 9: T  X  X  10: end for 11: Output: best covering terms T .
 Figur e 3: The first line  X  X ruck driving school in San Diego X  is a given short text, which contains 6 words; these 6 words make up 7 terms. Each word is labeled with its position in the tex-t, and each term is labeled with its range. Each word chooses the longest covering term, as denoted by an arrow in the fig-ure. Note that the word  X  X riving X  has two covering terms of the same length,  X  X riving school X  is chosen since it corresponds to more concepts than  X  X ruck driving. X  When multiple terms are detected, we choose the best term (with respect to  X   X   X ) to cover each word. An example is shown in Fig. 3. For now, let us assume all terms in a query reflect the same top-ic (e.g.,  X  X icrosoft X  and  X  X pple X  are companies , while  X  X icrosoft windows, X  and  X  X indows 8 X  refer to operating systems ), and that our goal is to derive that topic. To do this, we identify the top K candidate concepts C = { c k } ranked by their likelihood from a set of terms of unknown types T = { t 1 ;:::; t L } . Here, the type of a term can be either an instance or an attribute. Note that the same term can serve as an instance as well as an attribute. For example,  X  X opulation X  can be an attribute of country , but it can also be an in-stance of geographical data . However, it is rare that a term is both an instance and an attribute of the same concept.

We introduce an auxiliary variable z l to indicate the type of term t . Specifically, z l = 1 if t l is an instance, and z l = 0 if t attribute. Given that the knowledgebase contains noise, we han-dle the logic in a discriminative manner. We introduce a noisy-or model to first infer the probability P ( c k | t l ): where term t l invokes concept c k if it is an instance of c attribute of c k . Here, we have where term t l is regarded as an instance e l , and where term t l is regarded as an attribute a l , and E is the set of in-stances that are related to attribute a l and concept c k the naive Bayes rule, we derive the concept posterior given a set of terms by: where P ( c k | t l ) is given by Eq. (2) [32].
 The parsing results of many queries consist of terms that reflect more than one topic. For example, for query  X  X isposable chop-stick manufacturer, X  we get { X  X isposable chopstick, X   X  X anufactur-er X  X , and for query  X  X labama home insurance, X  we get { X  X labama, X   X  X ome insurance X  X . The previous conceptualization method per-forms a joint inference on the terms. The Bayesian rule in Eq. (5) tends to emphasize the common concepts belonging to the terms. Even after using smoothing techniques in naive Bayes [32], it may still lead to general, vague, and even meaningless concepts such as object , item , thing , etc.
 We capture multiple topics in a query through term clustering. We first represent each term t i as a concept vector c = ( c R M where M is the total number of concepts in the knowledgebase. In practice, we only retrieve the top K concepts for each term and thus c i is a sparse vector. Then we formulate the term relationship as a connected weighted graph G = {V ; E} , where V is the vertex set and E is the edge set. We define the terms detected in a query as vertices in the graph, and define weights on the edges between them important the concept is to represent the text. using cosine similarities between concept vectors. More specifical-ly, we define the weight s ij between t i and t j as: Then, we devise a clustering algorithm to find connected sub-graphs after filtering out edges whose weights are below a threshold. We use a pre-defined threshold to filter out non-significant edges. Thus, terms that are not semantically related will not be clustered togeth-er. Then we discover connected sub-graphs using a simple graph traverse algorithm, and all the terms that are connected to each oth-er will be grouped together. The time complexity is in quadratic order of the number of terms detected in a query O ( L 2 ), because we compute pairwise similarity in the beginning of the algorithm. In general, a query has very small number of terms. Therefore, the computational cost for clustering is not significant.

Given the term clusters, we then use Eq. (5) to conceptualize each cluster of terms. In this way, the common concepts of a set of related terms will be ranked higher than the individual ones. For example, as [32] pointed, given  X  X hina X  and  X  X apan, X  the top con-cepts will contain Asian country and eastern country . However, given  X  X hina X  and  X  X ussia, X  the top concepts will contain emerging market and developing country . Then for each cluster r discovered by simple graph cut, we represent it as a concept vector c is the concept vector of r th cluster of query q , using Eq. (5). Then the conceptualization of a query is given by: where L is the number of terms detected in a query, and l number of terms grouped into cluster r . Now the conceptualization result of a query is a weighted mixture of multiple topics, where each topic is described by a set of related concepts. When we compare the semantic relationship between this query and the other queries, di ff erent topics will contribute di ff erently.
Fig. 4 shows two examples of conceptualization results. For the query  X  X icrosoft window 8, X  our algorithm automatically deter-mines that there is only one topic about operating systems in that query. For the query  X  X labama home insurance, X  our algorithm i-dentifies that there are two topics, and balance these two topics to get better concept distribution.
In web search, given a query q , we want to evaluate its relevance to a set of URLs u 1 ; u 2 ;  X  X  X  . Figure 5 illustrates the setting of the problem. Using the techniques described in Section 4.1, we map q to a set of concepts c 1 ; c 2 ;  X  X  X  . The same query q is also mapped to a set of URLs u 1 ; u 2 ;  X  X  X  . Our goal is thus to establish the relationship between the concepts and the URLs, so that knowledge from the URLs can be channeled by the concepts to the tail query. In this section, we show how we can introduce another layer in the figure (shown as the top layer in the figure), through which the knowledge in the concepts can be used to filter and rerank the URLs. Figur e 5: Knowledge from concepts goes through an additional layer to act upon a set of URLs and evaluate their relevance. st here means short text, which can be clicked query, anchor text, or title of URL.
 URLs by themselves do not contain much information. They are not natural language texts and the conceptualization mechanisms we developed cannot be applied to URLs directly. In our approach, we introduce an additional layer as shown in Fig. 5: the textual rep-resentation of the URLs. Our goal is to convert URLs to a textual representation, and then map the textual representation to the con-cept space. The question is then, what are the appropriate textual representations of URLs. There are a few options. In this paper, we discuss three of them: clicked queries, anchor texts, and URL titles. jobs (2332); hotjobs (1271); ...
Stores (46); ... dmv written test questions and answers (45); ... about The Bridge (1) (1); what type of reaction is Zn + HCl (1); ... northern trust economic research (32); northern trust economics (2); northern trust paul kasriel (1) enable resumable timeout sql (1) We add textual representation for URLs as an additional layer in Fig. 5. Whether the layer consists of a set of clicked queries, or a set of anchor texts, or a title, we can conceptualize them as short texts. This enables us to map the URLs to concepts indirectly through the added layer.

Let u denote a URL, and let { st 1 ;:::; st N } denote its textual rep-resentation, where st i is short text (a clicked query, an anchor text, or a title). We map u to a concept vector c u by Here, w st i is the score of short text st i , and it is defined as w log[count( st i ) + 1] where count( st i ) is the frequency of short text st co-occurring with URL u (that is, how many times a unique anchor text is used when referencing u in a hypertext, or how many times u is clicked in response to a query). The intuition is to weight the con-cepts with the count. For example, if a query triggered more clicks of that URL, the concepts describing that query will be weighted more importantly when computing the concepts of the URL. Now, we have represented each URL as a vector of concepts. Then the queries and the URLs are comparable in the same space. For historical queries, we compute the query-URL pairwise simi-larities and input them to a learning to rank algorithm associated with the other features extracted from the pairs to learn the param-eters. For the online search problem, we should compute the con-ceptualization of URLs o ffl ine. This can be handled using an en-terprise level MapReduce-type system in days. Given a new query, which may be a tail query we have never seen before, we compute the concepts of that query and compute the similarities between the query and the candidate URLs. Then the similarities are input as features for the learning to rank system for further judgement of relevance.
In this section, we present the experimental results conducted on two real data sets, and show the performance of how conceptu-alization can improve search relevance. The results for relevance are evaluated using the NDCG (normalized discounted cumulative  X  15.98 19.84  X  15.75  X  2.61 3.23  X  2.59  X  309.37 68.00  X  1,460.93  X  1.68 1.48  X  1.01  X  182,084.84 8,777.85  X  950,575.36  X  380.51 229.24  X  1,506.12 g ain) [18] score: NDCG@ K = 100 Z relevance label for the document ranked at position r ; K is the level that NDCG is computed; and Z is a normalization constant which makes NDCG@ K = 100 for the best ranking list. Generally, we compute the average NDCG score over all queries. Obviously, ND-CG can be used to evaluate multilevel ranking values in terms of positions for search relevance.

To train a competitive ranking function, we use a gradient boost-ing tree algorithm [35] which is accepted as a state-of-the-art  X  X earn-ing to rank X  algorithm. Additionally, our gradient boosting tree can directly optimize NDCG.

To derive a consistent and trustful experimental report, we con-duct our experiment with the two-fold cross validation approach. That means the data are split into training and test sets. Training data are used for learning the ranking function, and test data are used to calculate NDCG results. All the experiments are based on six random splitting trials to compute the average NDCG scores. In this section, we briefly introduce the two data sets we used. The first data is a mixture of head and tail queries uniformly sam-pled from search logs. The second data set is elaborately selected which contains only tail queries that appeared only once in one day. A comparison of statistics of two data sets is shown in Ta-ble 3. We see that, the average query length of the mixture data is less than the one of tail data. Averagely speaking, tail queries tend to be individual and longer than head queries. The data sets are quite large, because we need to compute all related historical queries and anchor texts associated to the URLs. In practice, we compute the scores using a MapReduce-type mechanism under a distributed computing environment. Our approach can be easily s-caled up because we deal with each short text individually using the same knowledgebase. For each pair of query and URL, on average it needs tens of milliseconds on a 2.8GHz PC machine.
We collect the anchor text data from one day X  X  snapshot of Web pages from a commercial search engine. The data contains billions of URLs and anchor texts with counts, and it takes 286G on disk to store the raw data. We filter out the anchor text data with the URLs contained in the two benchmark data sets. The statistics of the data is described in Table 4.

We see the average word count in anchor text is 3.36 and 3.23 for mixture and tail data sets, namely there are on average about three words for each anchor text. Moreover, the average number of unique anchor text for each URL is 14.47 and 68.00 for mixture and tail data set. It seems that the mixture data set contains both head and tail URLs, however, in the tail data set, there are more head URLs. The variances of numbers of unique anchor text for both data sets are large. This means the distribution of the number has very long tail.

We collect the clicked query data with two data sets to demon-strate the use of historical click information. The first click data contains five months search log, and the second click data contains ten months search log. We name these two sets Click 5M and Click 10M . They cost 178G and 323G on disk respectively. We also join these two data sets with the two benchmark data sets. The statistical numbers are shown in Tables 5 and 6.

We can see that there are more valid URLs in the 10 months X  data than in the 5 months X  data. The average query word number is about 2 for mixture benchmark and 1.5 for tail benchmark. This shows that, there are more head queries in the historically clicked query data sets, and the clicked queries joined with the tail bench-mark data set tend to be head queries. This is consistent with the conclusion that in the tail data set, there are more head URLs. The average number of query clicks for each URL in tail data is also larger than the one in the mixture data. Moreover, the number of query clicks of each URL in 10 months X  data is about two times the number of the 5 months X  data. However, the unique number of queries for each URL in the 10 months X  data is only one and a half times the number of 5 months X  data. This means when we collect the data over a longer time period, the coverage does not grow as fast as the number of clicks grows. This means, head queries and URLs receive more clicks.
We compare our methods with di ff erent sources with two rank-ing baselines. We have four data sources of both mixture and tail queries, i.e., title, anchor, click 5M and click 10M. The title source contains the landing page titles of URLs. The anchor and click data are extracted as the above introduced. The four data sources will have di ff erent ranking NDCG results because they do not have the same URL set. From Tables 4, 5 and 6 we see that there are dif-ferent valid URL numbers for di ff erent source and data. Therefore, in the following experiments, we only show how significant con- X  9.37 10.41  X  6.28  X  1.66 1.49  X  1.10  X  351,854.21 15,898.87  X  1,839,522.70  X  610.28 362.84  X  2,448.04 T able 7: Comparison of NDCG scores with di ff erent single sim-ilarity measure for mixture data.
 JSScore + Edit 40.29 42.21 42.25 41.98
Jaccard + Edit 39.79 41.47 41.04 40.54 ceptualization based similarities can improve the search relevance results respectively.
 Given a pair of URL u and query q , we measure their similarity based on the concept vectors c u and c q . We adopt three similarity scores used in this experiment: (1) cosine similarity, (2) Jaccard similarity, and (3) Jenson-Shannon (JS) divergence. JS divergence is a popular method to measure the similarity between two prob-ability distributions. Although we are not strictly using a proba-bilistic approach, we use normalized values in the concept vector to approximate a distribution to compute the JS score.

Besides the three similarity scores for concepts, we also test ed-it distance based similarity, which is used to compare our seman-tic approach with the string similarity based approach. The edit distance-based similarity is computed based on where w st i is defined as the same value as in Eq. (8), EditDist( q is the edit distance between two strings of queries q and st short text related to URL), and Len( q ) is the length of the string of q .

The results are shown in Tables 7 and 8. Among cosine, Jaccard and JSScore, cosine is the best. Jaccard similarity loses the infor-mation of concept weight. Moreover, our approach is not strictly a probabilistic approach. Therefore the JSScore may not be able to best evaluate the similarity between two sets of concepts. Fur-thermore, we can see that  X  X osine + edit X  gives the best results. We do not argue that concept similarity can solely beat edit distance, since semantically related queries are only a portion of the relat-ed queries. There are also a lot of query refinements with similar surface. Therefore, we argue that our signal is a very good comple-mentary signal to simple surface based similarities.
 To show how our method can improve the state-of-the-art methods, we first show how the concept information can be used to improve the content-based ranking scores. We inject our concept based sim-ilarity scores to real ranking system, in which the content features include about 1,000 dimensions. Particularly, those features in-clude BM25 and BM25F [34], lexical features, statistical counting Table 8: Comparison of NDCG scores with di ff erent single sim-ilarity measure for tail data.
 JSScore + Edit 29.07 28.01 30.57 31.38
Jaccard + Edit 29.21 30.38 30.80 28.85 features, etc. [7, 23, 35]. The learning to rank machine automati-cally combines di ff erent features to fit the training data [35]. The comparison results are shown in Fig. 6 for mixture data and Fig. 7 for tail data. We can see that the improvement achieved by our approach is larger on tail data than on mixture data. This may be because the popularity fields of mixture data contain more infor-mation for each URL. Thus, the additional information provided by Probase is not as significant as it is for the tail data. Moreover, we see that click 10M data and click 5M data achieve more im-provements than anchor texts. The historical queries contain more information on both semantics and users X  intents.
 To check whether the improvement is only achieved through the click information, we also compare with a full ranker. This ranker makes use of both content and click information, including the fea-tures generated by click model [38] and translation model [12]. The comparison results are shown in Figs. 8 and 9. We can see that, for the tail data, concept information can also show improvement. However, for the mixture data, only click 10M data shows small improvement. This is because the click information for the mixture data is already good, and the concept does not present enough addi-tional information. The click 10M data shows small improvements while click 5M does not, since click 10M X  X  data contains richer his-torical queries that can be used to model the intent. Nonetheless, for tail queries, click information is still not enough.
The above experiments show that our method is especially use-ful when exact click information is not enough to evaluate the rele-vance score.
In web search, one of the most challenging problems is how to handle tail queries. In this paper, we introduce a novel approach to transfer our understanding of head queries to tail queries. We develop a core technique called conceptualization that maps a short text to concepts defined in a general probabilistic knowledgebase. This allows us to represent queries by concepts with weights. Fur-thermore, we expand URLs with textual context (by leveraging the click-through graph, anchor texts, web page titles, etc.) so that we can map URLs to concepts as well. Once both queries (head and tail queries alike) and URLs are in the same concept space, we can compute their similarity as a relevance measure. We perform exper-similarities computed based on Probase. computed based on Probase. computed based on Probase. based on Probase. iments on two real world data sets to show the e ff ectiveness of this approach in improving search relevance, especially for tail queries. We thank Mei Li for her kind help on the evaluation of ranking system. We would like to thank Jun Xu for his helpful discussion. We also thank the reviewers for their valuable comments to improve this paper. [1] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and [2] A. Berger and J. La ff erty. Information retrieval as statistical [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [4] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. [5] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr., [6] K. Collins-Thompson and J. Callan. Query expansion using [7] P. Donmez, K. M. Svore, and C. J. C. Burges. On the local [8] D. Downey, S. Dumais, and E. Horvitz. Heads and tails: [9] O. Egozi, S. Markovitch, and E. Gabrilovich. Concept-based [10] O. Etzioni, M. J. Cafarella, D. Downey, S. Kok, A.-M. [11] C. Fellbaum, editor. WordNet: an electronic lexical database . [12] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based translation [13] J. Gao, K. Toutanova, and W. tau Yih. Clickthrough-based [14] M.-H. Hsu and H.-H. Chen. Information retrieval with [15] M.-H. Hsu, M.-F. Tsai, and H.-H. Chen. Combining wordnet [16] J. Huang, J. Gao, J. Miao, X. Li, K. Wang, F. Behr, and C. L. [17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. [18] K. J X rvelin and J. Kek X l X inen. Ir evaluation methods for [19] T. Joachims. Optimizing search engines using clickthrough [20] A. Kotov and C. Zhai. Tapping into knowledge base for [21] R. Kraft and J. Zien. Mining anchor text for query [22] D. B. Lenat and R. V. Guha. Building Large [23] P. Li, C. J. C. Burges, and Q. Wu. Mcrank: Learning to rank [24] Y. Li, W. P. R. Luk, K. S. E. Ho, and F. L. K. Chung. [25] S. Liu, F. Liu, C. Yu, and W. Meng. An e ff ective approach to [26] R. Mandala, T. Tokunaga, and H. Tanaka. Combining [27] S. P. Ponzetto and M. Strube. Deriving a large-scale [28] M. Richardson, E. Dominowska, and R. Ragno. Predicting [29] S. Robertson, H. Zaragoza, and M. Taylor. Simple bm25 [30] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy.
 [31] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. [32] Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. Short text [33] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core [34] K. M. Svore and C. J. C. Burges. A machine learning [35] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Adapting [36] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: A [37] Z. Yin, M. Shokouhi, and N. Craswell. Query expansion [38] Y. Zhang, W. Chen, D. Wang, and Q. Yang. User-click
