 ORIGINAL PAPER Anna Tonazzini  X  Emanuele Salerno  X  Luigi Bedini Abstract Ancient documents are usually degraded by the presence of strong background artifacts. These are often caused by the so-called bleed-through effect, a pattern that interferes with the main text due to seeping of ink from the reverse side. A similar effect, called show-through and due to the nonperfect opacity of the paper, may appear in scans of even modern, well-preserved documents. These degrada-tions must be removed to improve human or automatic read-ability. For this purpose, when a color scan of the document is available, we have shown that a simplified linear pattern overlapping model allows us to use very fast blind source separation techniques. This approach, however, cannot be applied to grayscale scans. This is a serious limitation, since many collections in our libraries and archives are now only available as grayscale scans or microfilms. We propose here a new model for bleed-through in grayscale document im-ages, based on the availability of the recto and verso pages, and show that blind source separation can be successfully applied in this case too. Some experiments with real-ancient documents are presented and described.
 Keywords Grayscale document restoration  X  Bleed-through cancellation  X  Blind source separation  X  Independent component analysis 1 Introduction Improving document readability is a common need in libraries and archives. Since the original documents should not be altered physically, readability problems are often treated by applying image-processing techniques on digital document scans. Another need whose importance is rapidly increasing is to include digital documents into searchable databases. For this task, machine-readable versions of the original texts are required. When dealing with very large collections, speed is an issue, and the machine-readable documents should be generated by a minimum of human intervention. This is normally done by optical character recognition systems (OCR), whose performance, however, depends on the quality of the data. Since ancient documents are often severely degraded, image-processing techniques can also be helpful as a preprocessing step before applying OCR. When degraded originals are to be managed, different classes of digital image restoration algorithms can thus become essential to both improve human readability and obtain acceptable OCR performances.
 degradations. Bleed-through occurs when seeping of ink from the back page (normally denoted as verso ) produces a pattern that interferes with the main text in the front page (normally denoted as recto ). The show-through distortion consists of a similar interfering pattern due to transparency of paper, and may also appear in modern, well-preserved documents. Removing the bleed-through or show-through patterns from a digital document scan is not trivial, especially from ancient originals, where interferences of this kind are usually very strong. Indeed, dealing with strong bleed-through is practically impossible by any simple thresholding technique, since the intensities of the unwanted background can be very close to the ones of the main text. Thus, adaptive and/or structural approaches have to be adopted. For the general problem of textured background removal, the authors in [ 1 ] suggest the investigation of multistage thresholding techniques. In [ 2 ], segmentation and grouping techniques, based on the Gestalt cognitive rules, are used to eliminate interfering strokes from skeletonized versions of handwritten documents. In [ 3 ], local adaptive filters are applied for homogeneous and textured background removal from handwritten grayscale documents. Other work done on the specific problem of bleed-through/show-through removal has mainly exploited information from both the recto and verso pages [4 X 6]. Besides requiring a preliminary registration of the two sides, these techniques are usually expensive, as they are based on steps of segmentation, to identify the bleed-through areas, followed by inpainting of estimated pure background areas [ 7 ]. In [ 8 , 9 ], a color scan from a single side is required, but a thresholding technique can only be used in the framework of multiresolution analysis and adaptive binarization. image as the linear combination of the interfering texts, and to separate them by processing multiple  X  X iews X  of the mixed object. When a color scan of the document is avail-able, three different views can be obtained from the red, green, and blue image channels. Even more views can be available in the cases where the document scans are obtained from narrowband hyperspectral sensors. Since the mixture coefficients are generally not known, the separation of the different patterns can be classified as a blind source sep-aration (BSS) problem. We attempted to solve this prob-lem through independent component analysis (ICA) [ 11 ]or other statistical techniques. In practice, this can be viewed as an adaptive representation of the document in a new color space, where the transformed color maps are mutually inde-pendent or, at least, uncorrelated. Although our image model is simplified [ 4 ], it has already proved to give interesting results in removing bleed-through [ 12 ], and extracting par-tially hidden features, such as paper watermarks [ 13 ]and underwritten texts in palimpsests [ 14 ].
 ways available. Large repositories of digitized documents or microfilms already exist in many archives and libraries, where the images have been captured in grayscale only. For these images, a BSS strategy such as the one described ear-lier cannot be applied. However, we will show here that the grayscale recto and verso sides of a document affected by bleed-through can still be modeled as a linear superposition of the main texts in the two pages, so that BSS can still be at-tempted. Furthermore, although the mixing coefficients are unknown, they can reasonably be supposed to give rise to a symmetric mixing matrix, so that the ICA approach is equiv-alent to a simpler and faster decorrelation of the observed data [ 15 ].
 introduce our linear data model. In Sect. 3 , we recall the properties of different decorrelation matrices, and, in Sect. 4 , we present some experimental results with real printed or manuscript documents. Some final remarks are giveninSect.5. 2 Formulation of the problem Let r ( t ) and v( t ) , t = 1 , 2 ,..., T , be the grayscale images obtained by scanning the recto and verso pages of a docu-ment, respectively, where t is a pixel index. Also, suppose that recto and verso have been spatially registered, after a horizontal flip of, say, the verso. We consider r ( t ) and as a linear combination of the two images s 1 ( t ) and s t = 1 , 2 ,..., T , representing the clean main texts in the recto and the verso, respectively. We can write: r ( t ) = A 11 s 1 ( t ) + A 12 s 2 ( t ) v( t ) = A 21 s 1 ( t ) + A 22 s 2 ( t ) (1) where A 12 / A 11 and A 21 / A 22 represent the intensity atten-uations of the ink seeping from the verso to the recto and, respectively, from the recto to the verso. Such attenuations depend on the features of the transmission medium (paper, parchment, etc.) and on other factors, such as ink fading. In general, coefficients A ij are not known. Functions r and are the data, while s 1 and s 2 are the source patterns to be sep-arated, i.e. estimated from the data with no knowledge of the mixing coefficients. This is normally called a blind source separation (BSS) problem. It is known that one condition under which this type of problems can be solved is that the source functions are mutually independent. In this hypothe-sis, and if some additional assumptions are verified, both the sources and the mixing coefficients can be estimated from the data alone. In our case, however, some specific physical constraints allow us to relax the strict independence require-ment, so that separation can be achieved by simply orthogo-nalizing the data images. We assume that the intensity con-tributions of the main texts are the same in the two pages, that is, A 11 = A 22 . We also assume that the attenuation of the bleed-through pattern in the two pages is the same, that is, A 12 = A 21 . Moreover, we expect that the contribution of the main text in each page is stronger than the contribu-tion of the bleed-through, i.e. A 12 &lt; A 11 and A 21 &lt; In summary, we assume a symmetric and diagonal-dominant mixing matrix. This is reasonable when dealing with printed pages or, in the case of manuscripts, when the two pages have been written at two close moments, with the same ink, by the same writer, and with the same pressure on the paper. Rather than taking this information into account explicitly, we will exploit it to choose the most convenient among var-ious separation techniques available.
 general model we proposed in [ 10 ] for the multispectral scans of a single document page. In that case, vectors x ( were assumed to have N components, and vectors s ( t ) were assumed to have M components. Since we considered docu-ment images containing homogeneous texts or drawings, we also assumed that the color of each source is almost uniform. In that case, A ij represented the mean emissivity of the i th source at the j th wavelength channel. The data are thus a collection of T samples from a random N -vector x ,whichis generated by linearly and instantaneously mixing the com-ponents of a random M -vector s through an N  X  M mixing matrix A x ( t ) = A s ( t ) t = 1 , 2 ,..., T (2) In the present case, our model of Eq. ( 1 ) is exactly the same of Eq. ( 2 ), restricted to the case M = N = 2, and where the mixing matrix coefficients are related to ink attenuation indices rather than emissivities, and x = ( r ,v) . count for the phenomenon of interfering texts in documents, which derives from complicated processes of ink diffusion and paper absorption. Just to mention one aspect, in the pix-els where two texts are superimposed to each other, the re-sulting intensity is not the sum of the intensities of the two components, but it is likely to be some nonlinear combina-tion of them. For example, for the phenomenon of show-through, a nonlinear model is derived in [ 4 ], but this must be linearized to have a tractable problem. Another simplifi-cation we adopt is to neglect both noise and blur. As already said, although the linear model is just a rough approximation of the reality, it has demonstrated to be useful in different ap-plications. 3 The proposed solutions: ICA, PCA and whitening When no additional assumption is made, problem of Eqs. ( 1 ) or ( 2 ) is clearly underdetermined, since any nonsingular choice for A can give an estimate of s ( t ) that accounts for the evidence x ( t ) . Even if no specific information is available, statistical assumptions can often be made on the sources. In particular, it can be assumed that the sources are mutually independent. If this assumption is justified, both A and s can be estimated from x . As mentioned in the section  X  X ntroduc-tion, X  this is the ICA approach [ 11 ]. Mutual source indepen-dence can be enforced by assuming a factorized form for the joint prior density of s P ( s ( t )) = The separation problem can be formulated as the max-imization of the density in Eq. ( 3 ), subject to the con-straint x = A s . This is equivalent to search for a matrix W = ( w 1 , w 2 ,..., w N ) such that, when applied to the data x = ( x 1 , x 2 ,..., x N ) , produces the set of vectors w that are maximally independent, and whose distributions are givenbythe P i . By taking the logarithm of Eq. ( 3 ), the prob-lem solved by ICA algorithms is then  X  W = arg max Matrix  X  W is an estimate of A  X  1 , up to arbitrary scale factors and permutations on the columns. Hence, each vector  X  s i  X  w x is one of the original source vectors up to a scale factor. sides independence is that all the sources, but at most one, are nongaussian. To enforce nongaussianity, generic super-gaussian or subgaussian distributions can be used as source priors. These have proved to give very good estimates for the mixing matrix and for the sources as well, no matter of the true source distributions, which, on the other hand, are usually unknown [ 16 ].
 no apparent physical reason why our sources should be independent, no ICA-based algorithm is assured to achieve separation. However, it is intuitively clear that one can try to maximize the information content in each component of the data vector by decorrelating the observed image channels. In fact, it is directly observable that, while the recto and verso pages are usually highly correlated in presence of bleed-through, the individual main text patterns are, at least, less correlated. Decorrelating the two views thus gives them a new representation, which could make them coincide with the individual source patterns.
 erality, let us assume to have zero-mean data vectors. We should find a linear transformation y ( t ) = W x ( t ) such that y M  X  N matrix and the notation  X  means expectation. In other words, the components of the transformed data vec-tor y should be orthogonal. It is clear that this operation is not unique, since, given an orthonormal basis of a subspace, any rigid rotation of it still yields an orthonormal basis of the same subspace. It is well known that linear data process-ing can help to restore color text images. In [ 17 ], the authors compare the effect of many fixed linear color transforma-tions on the performance of a recursive segmentation algo-rithm. They argue that the linear transformation that obtains maximum-variance components is the most effective. They thus derive a fixed transformation that, for a large class of images, approximates the Karhunen X  X oeve transformation, which is known to produce maximum-variance orthogonal vectors. This approach is also called principal component analysis (PCA), and one of its purposes is to find the most useful among a number of variables [ 15 ]. In our case, we can estimate the 2  X  2 data covariance matrix as R Since the data are normally correlated, matrix R xx will be nondiagonal. The covariance matrix of vector y is R To obtain a vector y whose components are mutually orthog-onal, R yy should be diagonal. Let us perform the eigenvalue decomposition of matrix R xx , and call V x the matrix of the eigenvectors of R xx ,and x the diagonal matrix of its eigen-values, in increasing order. Now, it is easy to verify that all of the following choices for W yield a diagonal R yy :
W W
W Matrix W o produces a set of vectors y i ( t ) that are orthogo-nal to each other and whose Euclidean norms are equal to the eigenvalues of the data covariance matrix. This is what PCA does [ 15 ]. By using matrix W w , we obtain a set of orthogo-nal vectors of unit norms, i.e. orthogonal vectors located on a spherical surface ( whitening ,or Mahalanobis transform ). This property still holds true if any whitening matrix is mul-tiplied from the left by an orthogonal matrix. In particular, if we use matrix W s defined in Eq. ( 9 ), we have a whiten-ing matrix with the further property of being symmetric. In [ 15 ], it is observed that application of matrix W s is equiva-lent to ICA when matrix A is symmetric. This is the prop-erty that we intend to exploit here, due to the mentioned fea-tures of the mixing matrix in the model of Eq. ( 1 ) assumed for our application. In this way, we can derive a separation technique that, while performing similarly to ICA, is much faster and simpler. In more general situations, ICA applies a further rotation to the output vectors, based on higher-order statistics.
 dominant matrix, with A 11 = A 22 , has not been explic-itly introduced in the solutions, and can be used to check the results a posteriori . In other words, if our assumptions are reasonable, our estimate of the mixing matrix should approximately be symmetric, diagonal dominant, and with equal diagonal elements. In the 2  X  2 case considered here, for example, besides W s , W o is also symmetric. Thus, we can at first consider them as our candidate demixing matri-ces, since they verify at least one of our assumptions. Nev-ertheless, by using all the other assumptions to check the consistence of the solutions, we will see that only W able to produce satisfactory separations, in accordance with the previously mentioned theoretical result that symmetric whitening is equivalent to ICA for symmetric mixing matri-ces. 4 Experimental results Our experimental work has consisted in applying matrices W o and W s to typical images of documents degraded by bleed-through or show-through distortions, and in compar-ing the results with the ones produced by ICA. Our aim was to obtain clean texts in the whitened vectors. Of course, the results are different for different whitening matrices. We show here some examples from our experimentation.
 recto and verso pages of a real X  X ake manuscript affected by a strong bleed-through. This is a double-sided real manuscript, manually built so to obtain a natural bleed-through. We compared the results of the FastICA algorithm [ 18 ], the PCA, and the symmetric whitening, all applied to the two views, and found that practically the same result is obtained by ICA and symmetric orthogonalization (SO). The esti-mated mixing matrices are Note that both matrices have been rescaled so as to have their first entries equal to 1, and both are diagonal dominant with diagonal elements nearly equal.
 PCA, which gives the following estimated mixing matrix: Note that this matrix is not diagonal dominant, and that the separation result is unsatisfactory (Fig. 2 ). For example, the second output, i.e. the principal component corresponding to the highest eigenvalue, is given by a combination of the two input mixtures with coefficients of the same sign.
 the recto and verso scans of a real document. Again, FastICA and symmetric orthogonalization perform similarly, giving a pretty good separation of the two main texts.
 qualitatively. Indeed, standard quantitative measures would require ground truth data to be available, and this is not fea-sible in actual practice, since ground truth can only be given in synthetic experiments. However, there is another strategy to evaluate, sometimes quantitatively, the performance of an algorithm. This is based on the improvements that can be achieved by coupling the algorithm under investigation to some subsequent task. Of course, this would be a measure of the performance of the latter task when the quality of the input data is modified, but this would also be a specific mea-sure of usefulness for the algorithm under investigation. In our case, we assessed the usefulness of our method on im-age analysis procedures such as OCR. Since we only have a commercial OCR software available, we carried out our evaluation on printed texts. We considered recto and verso pairs captured from a printed book and affected by a strong show-through. We then evaluated the OCR performance be-fore and after the application of our restoration method. Af-ter analyzing several pages, we found it difficult to quantify the improvement in recognition rate permitted by our pro-cedure, since the strong interference prevented any charac-ter recognition almost everywhere when no restoration was applied. In most cases, the automatic OCR was not able to distinguish between text strings and grayscale images and, sometimes, show-through strokes were interpreted as punc-tuation marks or other symbols. Thus, we were only able to compare the OCR performance in the areas where a sig-nificant part of the text was correctly recognized even with no restoration. The results of one of the experiments per-formed are shown in Figs. 4 and 5 . In Fig. 4 ,weshowthe original and processed versions of a recto X  X erso pair. The show-through has been removed quite effectively, but sig-nificant residuals can be observed in Fig. 4 c and d. This de-pends, again, on the simplified model we use. In this case, it is the assumption of space invariance of the mixing matrix that is to be questioned, since the transparency of the pa-per is not actually uniform. Another critical issue is the nec-essary registration between recto and flipped verso pages. We noted that the quality of the results strongly relies on a very accurate registration. To be able to compare the results of OCR, we selected a part of the text where most charac-ters were recognized even without restoration. In Fig. 5 ,we show the OCR outputs from this area, taken from about the middle of the recto page. The erroneous or spurious charac-ters are highlighted by bounding boxes, while the wrongly merged or split characters are underlined. The advantage of applying our strategy is apparent, since OCR on the pro-cessed image only produced five errors, while OCR on the original scan produced dozens of misinterpetations and also much spurious text due to the presence of the interfering material. 5 Conclusions We have derived an approximated but physically sound lin-ear model for describing the overlapping of texts in the grayscale recto and verso scans of documents affected by bleed-through or show-through distortion. In particular, this model satisfies some physical intuitions, such as the possible symmetry and the diagonal dominance of the mixing matrix that describes the relative ink attenuation in the two images. Based on the blind source separation theory, we developed a simple and fast technique which is able to separate the two mixed texts, thus producing clean versions of the recto and verso pages. This technique has its theoretical justifi-cation in the independent component analysis approach to BSS, and in the fact that symmetric data sphericization al-lows the sources to be separated when the mixing matrix is symmetric.
 quires a single, very fast, processing step, with no need for segmentation or inpainting. Its usefulness is mainly related to the possibility of quickly processing large databases of grayscale microfilms or digital scans, already available in many libraries and archives. Indeed, while new color or mul-tispectral acquisitions could not be permitted, owing to the poor conservation status of the originals, grayscale recto and verso scans are often either available or readily obtained from microfilm archives.
 of paramount importance for improving both human and au-tomatic readability. In this respect, we showed some suc-cessful examples from an experimentation with real-ancient documents.
 oversimplified model assumed. In particular, linearity, in-stantaneousness, space invariance, lack of flexibility with re-spect to nonperfect registration often constitute serious lim-itations on the applicability of our approach.
 development of more accurate numerical models for the phenomenon of pattern overlapping in documents, and the derivation of new BSS algorithms specifically designed for such models.
 References
