 Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for clas-sifying large sparse data including those from CTR predic-tion. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and com-pare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.
 Machine learning; Click-through rate prediction; Computa-tional advertising; Factorization machines
Click-through rate (CTR) prediction plays an important role in advertising industry [1, 2, 3]. Logistic regression is probably the most widely used model for this task [3]. Given a data set with m instances ( y i , x i ) ,i = 1 ,...,m , where y is the label and x i is an n -dimensional feature vector, the model w is obtained by solving the following optimization problem. Part of the work were done when these authors were in National Taiwan University.
 Table 1: An artificial CTR data set, where + (  X  ) represents the number of clicked (unclicked) impressions.
 In problem (1),  X  is the regularization parameter, and in the loss function we consider the linear model:
Learning the effect of feature conjunctions seems to be crucial for CTR prediction; see, for example, [1]. Here, we understanding of feature conjunctions. An ad from Gucci has a particularly high CTR on Vogue. This information is however difficult for linear models to learn because they learn the two weights Gucci and Vogue separately. To ad-dress this problem, two models have been used to learn the effect of feature conjunction. The first model, degree-2 poly-nomial mappings (Poly2) [4, 5], learns a dedicate weight for each feature conjunction. The second model, factorization machines (FMs) [6], learns the effect of feature conjunction by factorizing it into a product of two latent vectors. We will discuss details about Poly2 and FMs in Section 2.
A variant of FM called pairwise interaction tensor factor-ization (PITF) [7] was proposed for personalized tag recom-mendation. In KDD Cup 2012, a generalization of PITF called  X  X actor model X  was proposed by  X  X eam Opera Solu-tions X  [8]. Because this term is too general and may easily be confused with factorization machines, we refer to it as  X  X ield-aware factorization machines X  (FFMs) in this paper. The difference between PITF and FFM is that PITF con-siders three special fields including  X  X ser, X  X  X tem, X  and  X  X ag, X  while FFM is more general. Because [8] is about the over-all solution for the competition, its discussion of FFM is limited. We can conclude the following results in [8]: 1. They use stochastic gradient method (SG) to solve the optimization problem. To avoid over-fitting, they only train with one epoch. 2. FFM performs the best among six models they tried. In this paper, we aim to concretely establish FFM as an effective approach for CTR prediction. Our major results are as follows.  X  Though FFM is shown to be effective in [8], this work may be the only published study of applying FFMs on
CTR prediction problems. To further demonstrate the effectiveness of FFMs on CTR prediction, we present the use of FFM as our major model to win two world-wide
CTR competitions hosted by Criteo and Avazu.  X  We compare FFMs with two related models, Poly2 and
FMs. We first discuss conceptually why FFMs might be better than them, and conduct experiments to see the difference in terms of accuracy and training time.  X  We present techniques for training FFMs. They include an effective parallel optimization algorithm for FFMs and the use of early-stopping to avoid over-fitting.  X  To make FFMs available for public use, we release an open source software.
 This paper is organized as follows. Before we present FFMs and its implementation in Section 3, we discuss the two existing models Poly2 and FMs in Section 2. Experi-ments comparing FFMs with other models are in Section 4. Finally, conclusions and future directions are in Section 5. Code used for experiments in this paper and the package LIBFFM are respectively available at:
Chang et. al [4] have shown that a degree-2 polynomial mapping can often effectively capture the information of fea-ture conjunctions. Further, they show that by applying a linear model on the explicit form of degree-2 mappings, the training and test time can be much faster than using ker-nel methods. This approach, referred to as Poly2, learns a weight for each feature pair: number. The complexity of computing (2) is O (  X  n 2 ), where  X  n is the average number of non-zero elements per instance.
FMs proposed in [6] implicitly learn a latent vector for each feature. Each latent vector contains k latent factors, where k is a user-specified parameter. Then, the effect of feature conjunction is modelled by the inner product of two latent vectors: The number of variables is n  X  k , so directly computing (3) costs O (  X  n 2 k ) time. Following [6], by re-writing (3) to where the complexity is reduced to O (  X  nk ).

Rendle [6] explains why FMs can be better than Poly2 tration using the data set in Table 1. For example, there is only one negative training data for the pair (ESPN, Adi-das). For Poly2, a very negative weight w ESPN , Adidas might be learned for this pair. For FMs, because the prediction of (ESPN, Adidas) is determined by w ESPN  X  w Adidas , and be-cause w ESPN and w Adidas are also learned from other pairs (e.g., (ESPN, Nike), (NBC, Adidas)), the prediction may be more accurate. Another example is that there is no training data for the pair (NBC, Gucci). For Poly2, the prediction on this pair is trivial, but for FMs, because w NBC and w Gucci meaningful prediction.

Note that in Poly2, the naive way to implement h ( j 1 ,j This approach requires the model as large as O ( n 2 ), which is usually impractical for CTR prediction because of very large n . Vowpal Wabbit (VW) [9], a widely used machine learning package, solves this problem by hashing j 1 and j 2 . implementation is similar to VW X  X  approach. Specifically, where the model size B is a user-specified parameter.
In this paper, for the simplicity of formulations, we do not include linear terms and bias term. However, in Section 4, we include them for some experiments.
The idea of FFM originates from PITF [7] proposed for recommender systems with personalized tags. In PITF, they assume three available fields including User, Item, and Tag, and factorize (User, Item), (User, Tag), and (Item, Tag) in separate latent spaces. In [8], they generalize PITF for more fields (e.g., AdID, AdvertiserID, UserID, QueryID) and ef-fectively apply it on CTR prediction. Because [7] aims at recommender systems and is limited to three specific fields (User, Item, and Tag), and [8] lacks detailed discussion on FFM, in this section we provide a more comprehensive study of FFMs on CTR prediction. For most CTR data sets like that in Table 1,  X  X eatures X  can be grouped into  X  X ields. X  In our example, three features ESPN, Vogue, and NBC, belong to the field Publisher, and the other three features Nike, Gucci, and Adidas, belong to the field Advertiser. FFM is a variant of FM that utilizes this information. To explain how FFM works, we consider the following new example: Clicked Publisher (P) Advertiser (A) Gender (G)
Recall that for FMs,  X  FM ( w , x ) is
More precisely, [4] includes the original features as well, though we do not consider such a setting until the experi-ments.
See http://github.com/JohnLangford/vowpal wabbit/ wiki/Feature-interactions for details. Table 2: Comparison of the number of variables and the complexity for prediction among LM, Poly2, FM, and FFM. In FMs, every feature has only one latent vector to learn the latent effect with any other features. Take ESPN as an example, w ESPN is used to learn the latent effect with Nike ( w ESPN  X  w Nike ) and Male ( w ESPN  X  w Male ). However, because Nike and Male belong to different fields, the latent effects of (EPSN, Nike) and (EPSN, Male) may be different.
In FFMs, each feature has several latent vectors. Depend-ing on the field of other features, one of them is used to do the inner product. In our example,  X  FFM ( w , x ) is w We see that to learn the latent effect of (ESPN, NIKE), w
ESPN , A is used because Nike belongs to the field Adver-tiser, and w Nike , P is used because ESPN belongs to the field Publisher. Again, to learn the latent effect of (EPSN, Male), w
ESPN , G is used because Male belongs to the field Gender, and w Male , P is used because ESPN belongs to the field Pub-lisher. Mathematically, where f 1 and f 2 are respectively the fields of j 1 and j f is the number of fields, then the number of variables of FFMs is nfk , and the complexity to compute (4) is O (  X  n It is worth noting that in FFMs because each latent vector only needs to learn the effect with a specific field, usually
Table 2 compares the number of variables and the com-putational complexity of different models.
The optimization problem is the same as (1) except that  X 
LM ( w , x ) is replaced by  X  FFM ( w , x ). Following [7, 8], we use stochastic gradient methods (SG). Recently, some adap-tive learning-rate schedules such as [10, 11] have been pro-posed to boost the training process of SG. We use AdaGrad [10] because [12] has shown its effectiveness on matrix fac-torization, which is a special case of FFMs.

At each step of SG a data point ( y, x ) is sampled for updating w j 1 ,f 2 and w j 2 ,f 1 in (4). Note that because x is highly sparse in our application, we only update dimensions with non-zero values. First, the sub-gradients are where  X  =  X  log(1 + exp(  X  y X  FFM ( w , x ))) Algorithm 1 Training FFM using SG 1: Let G  X  R n  X  f  X  k be a tensor of all ones 2: Run the following loop for t epochs 3: for i  X  X  1 ,  X  X  X  ,m } do 4: Sample a data point ( y, x ) 5: caclulate  X  6: for j 1  X  non-zero terms in { 1 ,  X  X  X  ,n } do 7: for j 2  X  non-zero terms in { j 1 + 1 ,  X  X  X  ,n } do 8: calculate sub-gradient by (5) and (6) 9: for d  X  X  1 ,  X  X  X  ,k } do 10: Update the gradient sum by (7) and (8) 11: Update model by (9) and (10) Second, for each coordinate d = 1 ,...,k , the sum of squared gradient is accumulated: Finally, ( w j 1 ,f 2 ) d and ( w j 2 ,f 1 ) d are updated by: where  X  is a user-specified learning rate. The initial values of w are randomly sampled from a uniform distribution be-tween [0 , 1 / order to prevent a large value of ( G j 1 ,f 2 )  X  procedure is presented in Algorithm 1.

Empirically, we find that normalizing each instance to have the unit length makes the test accuracy slightly better and insensitive to parameters. Modern computers are widely equipped with multi-core CPUs. If these cores are fully utilized, the training time can be significantly reduced. Many parallelization approaches for SG have been proposed. In this paper, we apply Hog-wild! [13], which allows each thread to run independently without any locking. Specifically, the for loop at line 3 of Algorithm 1 is parallelized.

In Section 4.4 we run extensive experiments to investigate the effectiveness of parallelization.
Consider the widely used LIBSVM data format: where each (feat, val) pair indicates feature index and value. For FFMs, we extend the above format to That is, we must assign the corresponding field to each fea-ture. The assignment is easy on some kinds of features, but may not be possible for some others. We discuss this issue on three typical classes of features.
 For linear models, a categorical feature is commonly trans-formed to several binary features. For a data instance we generate the following LIBSVM format.
 Note that according to the number of possible values in a categorical feature, the same number of binary features are generated and every time only one of them has the value 1. In the LIBSVM format, features with zero values are not stored. We apply the same setting to all models, so in this paper, every categorical feature is transformed to several binary ones. To add the field information, we can consider each category as a field. Then the above instance becomes Consider the following example to predict if a paper will be accepted by a conference. We use three numerical features  X  X ccept rate of the conference (AR), X  X  X -index of the author (Hidx), X  and  X  X umber of citations of the author (Cite): X  There are two possible ways to assign fields. A naive way is to treat each feature as a dummy field, so the generated data is: However, the dummy fields may not be informative because they are merely duplicates of features.

Another possible way is to discretize each numerical fea-ture to a categorical one. Then, we can use the same setting for categorical features to add field information. The gener-ated data looks like: where the AR feature is rounded to an integer. The main drawback is that usually it is not easy to determine the best discretization setting. For example, we may transform 45.73 to  X 45.7, X   X 45, X   X 40, X  or even  X  X nt(log(45.73)). X  In addition, we may lose some information after discretization. On some data sets, all features belong to a single field and hence it is meaningless to assign fields to features. Typically this situation happens on NLP data sets. Consider the fol-lowing example of predicting if a sentence expresses a good mood or not: In this example the only field is  X  X entence. X  If we assign this field to all words, then FFMs is reduced to FMs. Readers may ask about assigning dummy fields as we do for numeri-cal features. Recall that the model size of FFMs is O ( nfk ). The use of dummy fields is impractical because f = n and n is often huge.
In this section, we first provide the details about the ex-perimental setting in Section 4.1. Then, we investigate the impact of parameters. We find that unlike LM or Poly2, FFM is sensitive to the number of epochs. Therefore, in Section 4.3, we discuss this issue in detail before proposing an early stopping trick. The speedup of parallelization is studied in Section 4.4.

After checking various properties of FFMs, in Sections 4.5-4.6, we compare FFMs with other models including Poly2 and FMs. They are all implemented by the same SG method, so besides accuracy we can fairly compare their training time. Further in the comparison we include state-of-the-art packages LIBLINEAR [14] and LIBFM [15] for training LM/Poly2 and FMs, respectively. Data Sets We mainly consider two CTR sets Criteo and Avazu from Kaggle competitions, 3 though in Section 4.6 more sets are considered. For feature engineering, we mainly apply our winning solution but remove complicated components. 4 For example, our winning solution for Avazu includes the ensem-ble of 20 models, but here we only use the simplest one. For other details please check our experimental code. A hashing the two data sets are:
For both data sets, the labels in the test sets are not pub-licly available, so we split the available data to two sets for training and validation. The data split follows from how test sets are obtained: For Criteo , the last 6,040,618 lines are used as the validation set; for Avazu , we select the last 4,218,939 lines. We use the following terms to represent different sets of a problem.  X 
Va : The validation set mentioned above.  X 
Tr : The new training set after excluding the validation set from the original training data.  X 
TrVa : The original training set.  X 
Te : The original test set. The labels are not released, so we must submit our prediction to the original evaluation systems to get the score. To avoid over-fitting the test set, the competition organizers divide this data set to two subsets  X  X ublic set X  on which the score is visible during the competition and  X  X rivate set X  on which the score is available after the end of competition. The final rank is determined by the private set.
 For example, CriteoVa means the validation set from Criteo .
Criteo Display Advertising Challenge: http://www. kaggle.com/c/criteo-display-ad-challenge. Avazu Click-Through Rate Prediction: http://www.kaggle.com/c/ avazu-ctr-prediction.
The code and documents of our winning solution to the two competitions can be found in http://github. com/guestwalk/kaggle-2014-criteo and http://github.com/ guestwalk/kaggle-avazu Platform All experiments are conducted on a Linux workstation with 12 physical cores on two Intel Xeon E5-2620 2.0GHz proces-sors and 128 GB memory.
 Evaluation Depending on the model, we change  X  ( w , x ) in (1) to  X   X  tions 1-3. For the evaluation criterion, we consider the lo-gistic loss defined as where m is the number of test instances.
 Implementation We implement LMs, Poly2, FMs, and FFMs all in C++. For FMs and FFMs, we use SSE instructions to boost the efficiency of inner products. The parallelization discussed in Section 3.2 is implemented by OpenMP [16]. Our implemen-tations include linear terms and bias term as they improve performance in some data sets. These terms should be used in general as we seldom see them to be harmful.

Note that for code extensibility the field information is stored regardless of the model used. For non-FFM models, the implementation may become slightly faster by a simpler data structure without field information, but our conclusions from experiments should remain the same.
We conduct experiments to investigate the impact of k ,  X  , and  X  . The results can be found in Figure 1. Regarding the parameter k , results in Figure 1a show that it does not affect the logloss much. In Figure 1b, we present the relationship between  X  and logloss. If  X  is too large, the model is not able to achieve a good performance. On the contrary, with a small  X  , the model gets better results, but it easily over-fits the data. We observe that the training logloss keeps decreasing. For the parameter  X  , Figure 1c shows that if we apply a small  X  , FFMs will obtain its best performance slowly. However, with a large  X  , FFMs are able to quickly reduce the logloss, but then over-fitting occurs. From the results in Figures 1b and 1c, there is a need of early-stopping that will be discussed in Section 4.3.
Early stopping, which terminates the training process be-fore reaching the best result on training data, can be used to avoid over-fitting for many machine learning problems [17, 18, 19]. For FFM, the strategy we use is: 1. Split the data set into a training set and a validation set. 2. At the end of each epoch, use the validation set to calcu-late the loss. 3. If the loss goes up, record the number of epochs. Stop or go to step 4. 4. If needed, use the full data set to re-train a model with the number of epochs obtained in step 3.
 A difficulty in applying early stopping is that the logloss is sensitive to the number of epochs. Then the best epoch on the validation set may not be the best one on the test set. We have tried other approaches to avoid the overfitting Figure 1: The impact of  X  ,  X  , and k on FFMs. To make experiments faster, we randomly select 10% instances from CriteoTr and CriteoVa as the training and the test sets, respectively. such as lazy update 5 and ALS-based optimization methods. However, results are not as successful as that by early stop-ping of using a validation set.
Because the parallelization of SG may cause a different convergence behavior, we experiment with different numbers of threads in Figure 2. Results show that our parallelization still leads to similar convergence behavior. With this prop-erty we can define the speedup as: The result in Figure 3 shows a good speedup when the num-ber of threads is small. However, if many threads are used, the speedup does not improve much. An explanation is that if two or more threads attempt to access the same memory address, one must wait for its term. This kind of conflicts can happen more often when more threads are used. http://blog.smola.org/post/943941371/ lazy-updates-for-generic-regularization-in-sgd Figure 2: The convergence of using different number of threads. Figure 3: The speedup of using multi-threading. We respec-tively use CriteoTr and CriteoVa as the training and the test sets.
To have a fair comparison, we implement the same SG method for LMs, Poly2, FMs, and FFMs. Further, we com-pare with two state-of-the-art packages:  X  LIBLINEAR: a widely used package for linear models. For
L2-regularized logistic regression, it implements two opti-mization methods: Newton method to solve the primal problem, and coordinate descent (CD) method to solve the dual problem. We used both for checking how opti-mization methods affect the performance; see the discus-sion in the end of this sub-section. Further, the exist-ing Poly2 extension of LIBLINEAR does not support the hashing trick, 6 so we conduct suitable modifications and denote it as LIBLINEAR-Hash in this paper.  X  LIBFM: As a widely used library for factorization ma-chines, it supports three optimization approaches includ-ing stochastic gradient method (SG), alternating least squares (ALS), and Markov Chain Monte Carlo (MCMC). We tried all of them and found that ALS is significantly bet-ter than the other two in terms of logloss. Therefore, we consider ALS in our experiments.

For the parameters in all models, from a grid of points we select those that lead to the best performance on the vali-dation sets. Results on Criteo and Avazu with the list of https://www.csie.ntu.edu.tw/  X cjlin/libsvmtools/#fast training testing for polynomial mappings of data parameters used can be found in Table 3. Clearly, FFMs outperform other models in terms of logloss, but it also re-quires longer training time than LMs and FMs. On the other end, though the logloss of LMs is worse than other models, it is significantly faster. These results show a clear trade-off between logloss and speed. Poly2 is the slowest among all models. The reason might be the expensive computation of (2). FM is a good balance between logloss and speed. For LIBFM, it performs closely to our implementation of FMs in terms of logloss on Criteo . 7 However, we see that our implementation is significantly faster. We provide three possible reasons:  X  The ALS algorithm used by LIBFM is more complicated than the SG algorithm we use.  X  We use an adaptive learning rate strategy in SG.  X  We use SSE instructions to boost inner product opera-tions.

Because logistic regression is a convex problem, ideally, for either LM or Poly2, the three optimization methods (SG, Newton, and CD) should generate exactly the same model if they converge to the global optimum. However, practically results are slightly different. In particular, LM by SG is better than the two LIBLINEAR-based models on Avazu. In our implementation, LM via SG only loosely solves the optimization problem. Our experiments therefore indicate that the stopping condition of optimization methods can affect the performance of the resulting model even if the problem is convex.
In the previous section we focus on two competition data sets, but it is important to see how FFMs perform on other data sets. To answer this question, we consider more data sets for the comparison, where most of them are not CTR data. Note that following the discussion in Section 3.3, we do not consider data sets with single-field features. The reason is that depending on how we assign fields, FFMs either become equivalent to FMs, or generate a huge model.
Here we briefly introduce the data sets used.  X 
KDD2010-bridge : 8 This data set includes both numerical and categorical features.  X 
KDD2012 : 9 This set contains both numerical and categor-ical features. Because our evaluation is logloss, we trans-form the original target value  X  X umber of clicks X  to a bi-nary value  X  X licked or not. X  Further, we sub-sample 10% of negative instances because the data set is huge and con-tains much more negative instances than positive ones.  X  cod-rna : 10 This set contains only numerical features.  X  ijcnn : 10 This set contains only numerical features.  X  phishing : 11 This set contains only categorical features. The performance of LIBFM on AvazuVa is as good as the FM we have implemented, but the performance on AvazuTe is poor. It is not entirely clear what happened, so further investigation is needed. http://pslcdatashop.web.cmu.edu/KDDCup/downloads. jsp http://www.kddcup2012.org/c/kddcup2012-track2/data http://www.csie.ntu.edu.tw/  X cjlin/libsvmtools/datasets/ binary.html http://archive.ics.uci.edu/ml/datasets/Phishing+ Websites LM-SG  X  = 0 . 2 , X  = 0 ,t = 13 688 0.46262 93 0.46224 91 LM-LIBLINEAR-CD s = 7 ,c = 2 2,280 0.46239 91 0.46201 89 LM-LIBLINEAR-Newton s = 0 ,c = 2 10,380 0.46320 105 0.46263 96 Poly2-LIBLINEAR-Hash-CD s = 7 ,c = 2 62,144 0.44917 14 0.44897 14 LIBFM  X  = 40 ,k = 40 ,t = 20 23,700 0.45012 14 0.45000 15 LIBFM  X  = 40 ,k = 40 ,t = 50 131,000 0.44904 14 0.44887 14 LIBFM  X  = 40 ,k = 100 ,t = 20 54,320 0.44853 11 0.44834 11 LIBFM  X  = 40 ,k = 100 ,t = 50 398,800 0.44794 9 0.44778 8 LM-SG  X  = 0 . 2 , X  = 0 ,t = 10 1360 0.39023 61 0.38833 64 LM-LIBLINEAR-CD s = 7 ,c = 1 3045 0.39131 115 0.38943 119 LM-LIBLINEAR-Newton s = 0 ,c = 1 4090 0.39269 182 0.39079 183 Poly2-LIBLINEAR-Hash-CD s = 7 ,c = 1 7,288 0.38510 10 0.38298 9 LIBFM  X  = 40 ,k = 40 ,t = 20 18,712 0.39137 122 0.38963 127 LIBFM  X  = 40 ,k = 40 ,t = 50 41,720 0.39786 935 0.39635 943 LIBFM  X  = 40 ,k = 100 ,t = 20 39,719 0.39644 747 0.39470 755 LIBFM  X  = 40 ,k = 100 ,t = 50 91,210 0.40740 1,129 0.40585 1,126  X  adult : 12 This data set includes both numerical and cate-gorical features.
 For KDD2010-bridge , KDD2012 , and adult , we simply dis-spectively. For cod-rna and ijcnn , where features are all numerical, we try both approaches mentioned in Section 3.3 to obtain field information: applying dummy fields and dis-cretization.

For the parameter selection, we follow the same procedure in Section 4.5. We split each set into training, validation, model trained with parameters that achieve the best logloss on the validation set.

The statistics and experimental results of each data set are described in Table 4. FFMs significantly outperform other models on KDD2010-bridge and KDD2012 . The com-mon characteristic among these data sets are:  X  Most features are categorical.  X  The resulting set is highly sparse after transforming cate-gorical features into many binary features. http://archive.ics.uci.edu/ml/datasets/Adult However, on phishing and adult , FFM is not significantly better. For phishing , the reason might be that the data is not sparse so FFM, FM, and Poly2 have close performance; for adult , it seems feature conjunction is not useful because all models perform similarly to the linear model.

When a data set contains only numerical features, FFMs may not have an obvious advantage. If we use dummy fields, then FFMs do not out-perform FMs, a result indicating that the field information is not helpful. On the other hand, if we discretize numerical features, though FFMs is the best among all models, the performance is much worse than that of using dummy fields. We summarize a guideline of apply-ing FFMs on different kinds of data sets:  X  FFMs should be effective for data sets that contain cate-gorical features and are transformed to binary features.  X  If the transformed set is not sparse enough, FFMs seem to bring less benefit.  X  It is more difficult to apply FFMs on numerical data sets. In this paper we discuss efficient implementations of FFMs. We demonstrate that for certain kinds of data sets, FFMs outperform three well-known models, LM, Poly2, and FM, in terms of logloss, with a cost of longer training time. For the future work, the over-fitting problem discussed in Section 4.3 is an issue that we plan to investigate. Besides, for the ease of implementation we use SG as the optimiza-tion method. It is interesting to see how other optimization methods (e.g., Newton method, coordinate descent) work on FFMs.
 This work was supported in part by MOST via grants 104-2221-E-002-047-MY3 and 104-2622-E-002-012-CC2 and MOE of Taiwan via grant 105R7872. [1] O. Chapelle, E. Manavoglu, and R. Rosales,  X  X imple [2] H. B. McMahan, G. Holt, D. Sculley, M. Young, [3] M. Richardson, E. Dominowska, and R. Ragno, [4] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, [5] T. Kudo and Y. Matsumoto,  X  X ast methods for [6] S. Rendle,  X  X actorization machines, X  in Proceedings of [7] S. Rendle and L. Schmidt-Thieme,  X  X airwise [8] M. Jahrer, A. T  X  oscher, J.-Y. Lee, J. Deng, H. Zhang, [9] J. Langford, L. Li, and A. Strehl,  X  X owpal Wabbit, X  [10] J. Duchi, E. Hazan, and Y. Singer,  X  X daptive [11] H. B. McMahan,  X  X ollow-the-regularized-leader and [12] W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin,  X  X  [13] F. Niu, B. Recht, C. R  X e, and S. J. Wright, [14] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [15] S. Rendle,  X  X actorization machines with libFM, X  ACM [16] L. Dagum and R. Menon,  X  X penMP: an industry [17] C. M. Bishop, Pattern Recognition and Machine [18] G. Raskutti, M. J. Wainwright, and B. Yu,  X  X arly [19] T. Zhang and B. Yu,  X  X oosting with early stopping:
