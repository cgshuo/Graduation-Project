 REGULAR PAPER Moonjung Cho  X  Jian Pei  X  Ke Wang Abstract In some business applications such as trading management in finan-cial institutions, it is required to accurately answer ad hoc aggregate queries over data streams. Materializing and incrementally maintaining a full data cube or even its compression or approximation over a data stream is often computationally prohibitive. On the other hand, although previous studies proposed approximate methods for continuous aggregate queries, they cannot provide accurate answers. In this paper, we develop a novel prefix aggregate tree (PAT) structure for online warehousing data streams and answering ad hoc aggregate queries. Often, a data stream can be partitioned into the historical segment , which is stored in a tradi-tional data warehouse, and the transient segment , which can be stored in a PAT to answer ad hoc aggregate queries. The size of a PAT is linear in the size of the transient segment, and only one scan of the data stream is needed to create and in-crementally maintain a PAT. Although the query answering using PAT costs more than the case of a fully materialized data cube, the query answering time is still kept linear in the size of the transient segment. Our extensive experimental results on both synthetic and real data sets illustrate the efficiency and the scalability of our design.
 Keywords Data warehousing  X  Data cube  X  Data stream  X  Online analytic processing (OLAP)  X  Aggregate query 1 Introduction Data warehousing and online analytic processing (OLAP) are essential facilities for many data analysis tasks and applications. Given a multidimensional base ta-ble, a data warehouse materializes a large set of aggregates from the table. By proper indexes in a data warehouse, various aggregate queries (OLAP queries) can be answered online.
 can be huge. For example, if a base table has 20 dimensions and the cardinality of each dimension is 10, then the total number of aggregate cells is 11 20  X  6 . 7  X  10 20 . Even if only on average one out of 10 10 aggregate cells is non-empty (i.e., covering some tuple(s) in the base table), the total number of non-empty aggregate cells still can be up to 6 . 7  X  10 10 ! Thus, computing and/or materializing a complete data cube is often expensive in both time and space, and hard to be online. swering ad hoc aggregate queries over fast data streams. In this paper, we are particularly interested in the applications where the accurate instead of approxi-mate answers to the queries are mandatory. For example, trading in futures market is often a high-risk and high-return business in many financial institutions. Trans-actional data and market data are collected in a timely fashion. Dealers often raise various ad hoc aggregate queries about the data in recent periods, such as  X  list the total transaction amounts and positions in the last 4 hours, by financial products, counter parties, time-stamp (rounded to hour), mature date and their combina-tions.  X  In those applications, it is required to maintain the recent data in a sliding window ,and provide accurate and online answers to ad hoc aggregate queries over the current sliding window .
 streams where the speed of a data stream can be of gigabytes per second and the cardinality of the IP addresses is 2 32 . In such situations, it is impossible to obtain accurate answers. Approximate answers usually provide sufficiently good insights. However, the target applications investigated in this paper, such as the transactional data streams in business, are substantially different. First, accurate answers are mandatory in many business applications. This is particularly impor-tant for some business applications such as those in the financial industry. Second, the data streams studied here often are not extremely fast, and the cardinality of the data is not very huge. Instead, they have a manageable speed. For example, since the modern computers easily have gigabytes of main memory and typically the transactions in those applications will be in the scale of millions per day, it is reasonable to assume that the current sliding window of transactions can be held into main memory. Thus, it is possible to obtain accurate answers to ad hoc aggregate queries, though the task is still challenging.
 tional data warehouse often updates in batch periodically, such as daily mainte-nance at nights or weekly maintenance during weekends. Such updates are often conducted offline. Online aggregate queries about the most recent data cannot be answered by the traditional data warehouses due to the delay of the incremental updates.
 fortunately, the size of a data cube is likely exponential to the dimensionality and much larger than the sliding window. Moreover, the cubing runtime is also expo-nential to the dimensionality and often requires multiple scans of the tuples in the sliding window or the intermediate results. However, in a typical data stream, each tuple can be seen only once, and the call-back operations can be very expensive. Thus, a data cube resulted from a reasonably large sliding window is usually too large in space and too costly in time to be materialized and incrementally main-tained online.
 gregates online by scanning the data stream only once, and still retain the high performance of online answering ad hoc aggregate queries? In other words, can we get a good tradeoff between the query answering efficiency and the efficiency of indexing and maintenance (i.e., the size of index and the time of building and maintaining the index)? To answer aggregate queries online, there are three fac-tors needed to be considered, namely the space to store the aggregates, the time to create and maintain the aggregates, and the query answering time. While the existing static data cubing methods focus on reducing the last of them, the goal of this paper is to trade off the query answering time a little bit against the space and time of incremental maintenance .
 Challenge 1 Can we avoid computing the complete cube but still retain the capability of answering various aggregate queries efficiently? Our contribution We propose a solution that the transient segment (i.e., a sliding window) of a data stream is maintained in an online data warehouse, which is enabled by the idea of materializing only the prefix aggregate cells and the infix aggregate cells . We show that they form just a small subset of the complete data cube, and the total number of prefix aggregate cells is linear to the number of tuples in the sliding window and the dimensionality. With such a small subset of aggregates cells, many aggregate queries, including both point queries and range queries, still can be answered efficiently.
 Challenge 2 How can we compute, maintain and index the selected aggregates from a data stream? Our contribution We devise a novel data structure, prefix aggregate tree (PAT), size of PAT is bounded. Algorithms are developed to construct and incrementally maintain PAT. Our experimental results indicate that PAT is efficient and scalable for fast and large data streams.
 Challenge 3 How can we answer various aggregate queries efficiently? Our contribution We develop efficient algorithms to answer essential aggre-cality property of side-links of PAT enable various aggregate queries be answered efficiently. An extensive performance study shows that the query answering is ef-ficacious over large and fast data streams.
 framework and review related work. The prefix aggregate tree structure as well as its construction and incremental maintenance are presented in Sect. 3 . The query answering algorithms are developed in Sect. 4 . The extensive experimental results are reported in Sect. 5 . Section 6 concludes the paper. 2 Framework and related work 2.1 The framework In this study, we model a data stream as an (endless) base table S (
T , D 1 ,..., D n , M ) ,where T is an attribute of time-stamps , D 1 ,..., D n are n dimensions in discrete domains, and M is the measure . For the sake of simplicity, we use positive integers starting from 1 as time-stamps.
 it is reasonable to assume that the tuples having time-stamp  X  arrive before the ones having time-stamp ( X  + 1 ) . Tuples having the same time-stamp may be in arbitrary order.
 However, those data warehouses have to be incrementally maintained periodically and the maintenance is often offline. That is, it is difficult to answer aggregate queries on the recent data that has not been loaded into the data warehouse in the last update.
 ceptually, the historical segment is the data arrived before the last update of the central data warehouse and thus has been archived. The transient segment, in turn, is the data that has not been archived in the central data warehouse, and should be updated and maintained online in an online data warehouse.
 applications and some prototype implementations of commercial databases. How-ever, to the best of our knowledge, there exists no previous study on how to con-struct and maintain an online data warehouse for the transient segment. transient segment? Consider the scenario that the central data warehouse is just updated. Then, the online data warehouse contains very little data and many ag-gregate queries about the recent data cannot be answered using the online data warehouse. To avoid this problem, the online data warehouse should maintain the tuples whose time-stamps are in a sliding window of size  X  ,where  X  is the length of the periodicity that the central data warehouse conducts a regular update. online data warehouse are about multi-dimensional aggregates of tuples falling in the sliding window of [ t  X   X  + 1 , t ] .
 COUNT and AVG in SQL. We consider the following two kinds of queries in this paper.
 ( X , d consider a data stream of transaction records in an endless table transaction (Time-stamp, Branch-id, Prod-id, Counter-party-id, Amount) where Branch-id , Prod-id and Counter-party-id are the dimensions, and Amount is the measure. Suppose the sliding window is of size 24 hours. A point query may ask for  X  the total amount of  X  X old X  at 10 am  X , where the query cell is ( 10am, *, gold, * ). Here, symbols  X   X   X  X n dimensions Branch-id and Counter-party-id mean every transaction in any branch and with any counter-party counts.
 returned. As another example, query cell ( *, Paris, *, * ) stands for the total trad-ing amount in Paris in the current sliding window, including all products and all customers.
 some dimensions. Thus, a range query may cover multiple query cells. For exam-ple, a range query may ask for  X  the total amount of  X  X old X  and  X  X il X  in Paris and range. For example, the above range query is answered by one total amount that covers all the transactions in the two cities and about the two products, in the last 2 hours. 1 If this is not the case, we can easily divide the range into two sub-ranges, one in the historical data, and the other in the sliding window. The query can be answered accordingly.
 queries can be raised. Many complex OLAP queries can be decomposed into a set of queries in the above two categories.
 online data warehouse and answer ad hoc aggregate queries.
 Problem statement. Given a data stream S and a size of sliding window  X  .We want to construct and maintain an online data structure W ( t ) so that, at any instant t , any point queries and range queries about the data in time [ t  X   X  + 1 , t ] can be answered precisely and efficiently from W ( t ) .
 two conditions. 1. The size of W ( t ) is linear in the number of tuples in the current sliding window 2. W ( t ) can be constructed and maintained by scanning the tuples in the stream W is called an online data warehouse of stream S . 2.2 Related work Chaudhuri and Dayal [ 8 ] and Widom [ 43 ] present excellent overviews of the major technical progresses and research problems in data warehousing and OLAP. It has been well recognized that OLAP is more efficient if a data warehouse is used. Many approaches have been proposed to compute data cubes efficiently from scratch [ 6 , 33 , 34 , 45 ]. In general, they speed up the cube computation by sharing partitions, sorts, or partial sorts for group-bys with common dimensions. often huge. Some studies investigate partial materialization of data cubes [ 6 , 20 , 22 ]. Methods to compress data cubes are studied in [ 25 , 26 , 37 , 38 , 42 ]. Moreover [ 4 , 5 , 41 ] investigate various approximation methods for data cubes. In [ 35 , 38 ], Cubetree and Dwarf are proposed by exploring the prefix and suf-fix sharing among dimension values of aggregate cells. Quotient cube [ 25 ]isa non-redundant compression of data cube by exploring the semantics of aggregate cells, and QC-tree [ 26 ] is an effective data structure to store and index quotient cube. As compressions of a data cube, they can be used to answer queries directly, and quotient cube can further support some advanced semantic OLAP operations, such as roll-up/drill-down browsing.
 proposed. In [ 36 ], Sarawagi provides an excellent survey on related methods. A data warehouse may need to be updated in a timely fashion to reflect the changes of data. Refs. [18, 30 X 32] study the maintenance of views in data warehouses. cessing, such as monitoring statistics over streams and query answering (e.g., [3, 13 X 15]) and multi-dimensional analysis (e.g., [ 9 ]). Please see Babcock et al. [ 2 ] for a comprehensive overview. While many of them focus on answering contin-uous queries , few of them consider answering ad hoc queries by warehousing data streams. Probably, the work most related to this paper is [ 9 ], where a linear-regression based approach is proposed to accumulate the multi-dimensional ag-gregates from a data stream, and a variation of the H-tree structure [ 20 ]isused to materialize some selected roll-up/drill-down paths for OLAP. However, their method assumes that the streaming data can be summarized effectively by lin-ear regression and can only provide approximate answers to (preset) aggregate queries, and no efficient method is presented to answer various ad hoc aggregate queries in general. Moreover, the selected roll-up/drill-down paths are hard to de-termine. It is unclear how the H-tree structure can be stored and incrementally maintained effectively for data streams.
 of transactions, where a transaction is a set of items, the frequent itemset mining problem for data streams is to maintain the set of itemsets that appear at least  X  n times in the transactions seen so far, where is a minimum support threshold, and n is the number of transactions seen so far. Some methods put weights to transactions, and the more recent transactions have heavier weights. Frequency can be viewed as a type of aggregates. However, all of the previous methods are approximate approaches. They cannot provide the exact answers, though some methods can provide different types of quality guarantees.
 dexing data in a sliding window over data stream and answering ad hoc aggregate queries accurately.
 in data mining and data warehousing indices, including cube forest [ 23 ], FP-tree data structure. We will further compare our approaches to those important existing methods in Sect. 3.2 , after the major technical ideas of PAT are brought up. 3 Prefix aggregate tree (PAT) In this section, we devise the prefix aggregate tree (PAT) data structure. We also develop algorithms to construct and incrementally maintain prefix aggregate tree. Hereafter, all aggregate queries are ad hoc ones. 3.1 Data structure Consider a data stream S ( T , D 1 ,..., D n , M ) . Let the size of the sliding window be  X  . In order to answer any aggregate query about the data in the sliding window, we have to store the tuples in the sliding window. An intuitive way to store the tuples compactly is to use a prefix tree, as shown in the following example. Example 3.1 Let the data stream as our running example be S ( T , A , B , C , D , M ) , where T and M are the attributes of time-stamps and the measure, respectively. The tuples at instants 1 and 2 are shown in Table 1 . Suppose aggregate function SUM is used, and the size of the sliding window  X  = 2.
 can be organized into a prefix tree, as shown in Fig. 2 a.
 register the information at the tree nodes, as shown in Fig. 2 b. Each tree node has an aggregate table , such that the time-stamps and the aggregates by instants are registered.
 window. Each internal node in the tree registers the aggregate of the tuples whose aggregates of tuples having a 1 and stores them by instants.
 tree to the node. Hereafter, we will write a node as a string of dimension values, such as a 1 , a 1 b 1 and a 1 b 1 c 2 .
 of tuples sharing the  X  X refix X  from the root to the node. They are called prefix aggregate cells.
 Definition 3.1 (Prefix aggregate cell) Consider a data stream S ( T , D 1 , ..., D For any tuple, we always list the dimension values in the order of D 1 , ..., D n . Let S t be the set of tuples in the current sliding window [ ( t  X   X  + 1 ), t ] of S .An aggregate cell c = ( X , d 1 ,..., d n ) is a prefix aggregate cell if (1) there exists a k Theorem 1 By storing only the prefix aggregate cells, any ad hoc aggregate queries about the current sliding window can be answered.
 Proof Clearly, the answer to any ad hoc aggregate query about the current sliding window can be derived from the complete set of tuples in the window. Let us consider tuples in the current window. If a tuple t is unique in the current sliding window, t is (trivially) a prefix aggregate cell. If t is not unique, then there exists a prefix aggregate cell which has the same value as t on every dimension. In other window.
 Theorem 2 (Numbers of aggregate cells/prefix aggregate cells) Given a base table of n dimensions and k tuples, let n aggr and n p be the number of aggregate 1 ) + 1 ) and ( n + 1 )  X  n Proof When tuples share some values in some dimensions, they share the corre-sponding aggregate cells. When all tuples in the base table have the same value on every dimension, n aggr is minimized. When the k tuples do not share any common value in any dimension, each tuple leads to 2 n  X  1 unique aggregate cells, and all gate cells. When all tuples in the base table have the same value on every dimen-sion, n p is minimized. When the k tuples do not share any prefix, each tuple leads to n prefix aggregate cells, and all tuples share aggregate cell (  X  ,...,  X  ) . Thus, n aggr is maximized to case, linear in the number of tuples in the sliding window and the dimensionality, and thus is substantially smaller than that of all aggregate cells. It suggests that the set of prefix aggregate cells is a promising candidate of an online data warehouse for a data stream.
 swered by browsing the tree and extracting the related tuples in the current sliding window. However, if the current sliding window is large and thus the prefix tree is also large, browsing a large tree may not be efficient. We should build some light-weight index in the tree to facilitate the search.
 tree in Fig. 2 b. To answer this query, we need to access all the tuples having value b on dimension B . To facilitate the search, it is natural to introduce the side links that link all nodes carrying the same label together.
 ( a nodes carrying label c 1 in the subtree rooted at node a 1 . That is, we want a local linked list of nodes having label c 1 in the subtree rooted at node a 1 . carrying the same label should be linked consecutively.
 subtree, the query can be answered even faster. This information can be registered as the head of the sub-linked list of c 1 in the a 1 -subtree.
 having c 1 in the a 1 -subtree, and set up a pointer to the head of the sub-linked list associated with the head of the linked list. prefix tree in Fig. 2 b, resulting in a prefix aggregate tree structure, as shown in Fig. 3 .
 Definition 3.2 (Prefix aggregate tree) In general, given the current sliding win-of the prefix aggregate cells in the window with the following two kinds of links.  X  Side links All nodes having the same dimension value as the label are linked  X  Infix links At node v = d 1  X  X  X  d k ( 1  X  k  X  ( n  X  2 )) , for every dimension Theorem 3 Consider a base table of n dimensions and the cardinality of each dimension is l. In the worst case, the number of infix aggregate cells (and thus the infix links in a PAT) is Proof From the definition of PAT, the number of infix aggregate cells and that of infix links are identical. The worst case happens when every possible combination of dimension values appears in the base table, where the base table has l n unique tuples. Each internal node in the PAT has l children. Thus we have the upper bound. In such a situation, there are ( l + 1 ) n aggregate cells.
 and infix aggregate cells is still a small subset of all the aggregate cells. Practical data is usually skewed. As verified by our experimental results, the PAT is much smaller than the size of all aggregate cells.
 ( d root node is at level 0), and stores the following information: (1) The aggregate table, which has 2 columns, the time-stamp and the aggregate, and at most  X  records; (2) Pointers to up to l k + 1 children, where l k + 1 is the cardinality of di-mension D k + 1 ; (3) At most n i = k + 2 l i infix aggregate links and the corresponding aggregate tables, where l i is the cardinality of dimension D i ; and (4) A side link to the next node at the same level carrying the same label.
 O ( X   X  n easily in millions, the number of dimensions and the cardinality in each dimension are often pretty small. All nodes at the same level of the tree have the same size. A PAT can be easily stored and managed in main memory or on disk.
 the order of dimensions affects the size of the resulting PAT. Heuristically, if we order the dimensions in cardinality ascending order, then the tuples may have good chances to share prefixes and thus the resulting PAT may have a small number of nodes. The tradeoff is that the tree nodes may be large due to the large number of infix links. On the other hand, if we sort dimensions in the cardinality descending order, then the number of nodes may be large but the nodes themselves may be small. Theoretically, finding the best order to achieve a minimal PAT is an NP-hard problem. This problem is similar to the problem of computing a minimal FP-tree by ordering the frequent items properly. In Sect. 5 , we will study the effect of ordering on the size of PAT by experiments. 3.2 Comparison: PAT versus previous methods Prefix tree (trie) structures have been extensively used in the previous studies on data mining and data warehousing. PAT is another prefix tree structure. At the first glance, PAT may look similar to some of the previous structures, including However, there are some essential differences.
 actions in a prefix tree structure such that transactions sharing common prefixes also collapse to the same prefix in the tree. There are three critical differences between an FP-tree and a PAT. First, FP-tree is for transaction data and PAT is for relation data. While transactions may have different lengths, all tuples stored in a PAT have the same length. Infix links do not appear in an FP-tree. Second, FP-tree does not bear the locality property. Instead, the side links in an FP-tree are built as transactions arrive. As shown later, the locality property in a PAT facili-tates the aggregate query answering substantially. Last, an FP-tree is for frequent itemset mining. During the mining, an FP-tree is scanned multiple times, and the mining results are output. A PAT is for aggregate query answering. It is built and maintained by one scan of the data stream. A query answering algorithm searches the PAT to answer aggregate queries.
 data warehousing. PAT distinguishes itself from those designs in the following two aspects.
 structures, except for H-tree , potentially store all aggregates, though compression is explored . As a result, the number of nodes in PAT is linear in the number of tuples in the sliding window and the dimensionality, while those structures are ex-ponential to the dimensionality. Moreover, the size of tree nodes in PAT is regular, as analyzed before. The advantages on size and regularity of tree nodes make PAT feasible for streaming data.
 the dimensionality. A PAT can be regarded as an extension of an H-tree to support aggregate queries for data streams. The critical difference is that an H-tree does not have infix links and thus the query answering on an H-tree directly can be very costly.
 the locality property. As will be shown soon, the locality property and the infix links facilitate query answering substantially. In most of the previous structures, the search is based on values and is conducted dimension by dimension. quantified in Theorem 3 . The infix links have to meet the locality requirement. Theorem 4 will discuss the procedure. As will be shown, it takes the extra cost in time linear in the dimensionality to maintain the locality. 3.3 PAT construction We consider constructing a PAT by reading tuples into main memory one by one (or batch by batch), and each tuple can be read into main memory only once the algorithm is presented in Fig. 4 and elaborated in this subsection.
 Example 3.2 Let us construct a PAT by reading the tuples in Table 1 one by one. path, a row ( 1 , 6 ) is registered in the aggregate table. The infix links from root ( a not need the tuple any more.
 The aggregate values at nodes root and a 1 should be both updated to ( 1 , 8 ) ,since they are on the path of the inserted tuple. The infix links from root to a 1 b 1 and from a 1 b 1 to a 1 b 1 c 1 d 1 are created and associated with the infix aggregate cells (  X  , b sulting PAT is exactly the one shown in Fig. 3 .
 jor components: building the prefix tree, which is straightforward, and creat-ing/maintaining the correct infix links and side-links, which should follow the procedure justified in the following theorem so that the locality property is re-spected.
 Theorem 4 Let T be a PAT that satisfies the locality property. When a new node v of label d i is created, the following procedure adjusts the side-links and infix-links so that the resulting PAT preserves the locality property: If v is a child of the root node, no infix link and side-link are needed; else, the side-links and infix links with respect to v should be adjusted as follows: 1. The closest ancestor v of v should be allocated such that v has an infix link 2. If v does not exist, then an infix link should be built from every ancestor of v 3. Otherwise, let u be the node pointed by the d i -infix link of v , and V be the Proof The correctness of Step 2 is clear since in such a case, v is the first node carrying label d i . The corresponding infix links should be created.
 locality property, v should be inserted to the head of the non-empty consecutive sublist of its closest ancestor. Once the locality property holds for the smallest subtree containing the new node, the locality property holds for any larger subtrees containing the smallest subtree, since the PAT before the insertion has the locality property.
 mensionality. At each node, a table of aggregates is maintained. Each table has two columns: time-stamp and aggregate, and at most  X  rows. The aggregate at in-stant t should be stored at the ( t mod  X ) -th row. Therefore, the cost of maintaining and searching the table is constant. 3.4 Incremental maintenance Suppose we have a PAT at instant t . At instant ( t + 1 ) , the new data tuples should be read in and inserted into the tree, and the data at instant ( t  X   X  + 1 ) should be removed, so that the sliding window is moved forward to [ t  X   X  + 2 , t + 1 ] .The algorithm is shown in Fig. 5 . We explain the critical details as follows. node v whose aggregate table contains only an aggregate at instant ( t  X   X  + 1 ) . Suppose that some tuples from the stream at instant ( t + 1 ) will contribute a new row in v  X  X  aggregate table. If deletions go first, the node would be removed since its aggregate table is empty after the deletion. Then, the insertion of the tuples at instant ( t + 1 ) will have to recreate the node. To avoid such an unnecessary deletion-and-re-creation, we should let the insertions of tuples at instant ( t + 1 ) go first before the deletions of tuples at instant ( t  X   X  + 1 ) .
 into the tree.
 previous instants. The only tuples scanned are those at instant ( t + 1 ) . Ana  X   X ve method is as follows. We search the PAT. For each node that contains an aggregate at instant ( t  X   X  + 1 ) in its aggregate table, the corresponding row in the aggregate table should be removed. If the aggregate table becomes empty, then the node should be deleted.
 taining aggregates at instant ( t  X   X  + 1 ) . Aggressively updating a large number of nodes may degrade the online performance. Moreover, how to locate the nodes containing aggregates at instant ( t  X   X  + 1 ) is another problem. Browsing the whole tree can be very expensive.
 rows of instants ( t  X   X  + 1 ) or earlier have to be removed at instant ( t + 1 ) ,in order to release the space. Other than that, the deletions of the old aggregates of instant ( t  X   X  + 1 ) from the nodes are deferred and conducted as a byproduct of future insertions. The idea is elaborated in the following example.
 Example 3.3 Suppose the tuples at instant 3 are as shown in Table 2 . Since the size of the sliding window  X  = 2, the tuples at instant 3 should be inserted and the tuples of instant 1 should be removed. Let us consider how the PAT in Fig. 3 should be incrementally maintained.
 at the first row of the aggregate table, since 3 mod 2 = 1. It overwrites record ( 1 , 8 ) automatically. Similarly, we update the aggregate tables at nodes a a b 2 , and the aggregate tables for the related infix links, respectively. Please note that the removal of data at instant 1 from these nodes are conducted as a byproduct of the insertions, i.e., we do not actively search for the nodes whose aggregate tables having rows of instant 1. To complete the insertion, two new nodes, a 1 b 2 c 2 nodes as well as the appropriate side-links and infix links are adjusted. The second tuple, ( 3 , a 2 , b 2 , c 1 , d 2 , 1 ) can be inserted similarly.
 rows of instant 1. To find such nodes, we maintain an integer for each leaf node, called the last update time-stamp (LUT), which is the latest time-stamp that the node is updated. All leaf nodes having the same LUT are linked together as a linked list. By browsing the linked list for LUT = 1, we remove the leaf nodes and their ancestors that have only aggregates at instant 1. The upward search stops when the first ancestor having aggregates at instant other than 1 is encountered. resulting tree is shown in Fig. 6 . at instant 1. However, this information does not affect our query answering. This row will be removed in the future. For example, at instant 4, if there is a new tuple having a 1 b 1 as a prefix, the row will be overwritten and the node will be updated. Otherwise, the node will be removed when we clean up nodes containing only aggregates at instant 2 or earlier.
 scans the tuples having time-stamp ( t + 1 ) once, and inserts them into the existing PAT. By following the LUT list of ( t  X   X  + 1 ) , the maintenance algorithm removes those tree nodes and the corresponding infix links whose aggregate tables have only rows of instant ( t  X   X  + 1 ) or earlier. It never browses the complete PAT during the incremental maintenance.
 Theorem 5 The time complexity of constructing and incrementally maintaining a PAT i s O ( nl ) , where n is the number of tuples needed to inserted into the PAT, and l is the cardinality.
 Proof The complexity follows the algorithms in Figs. 4 and 5 . For each tuple, the insertion time and the time to maintain the locality of the infix links are linear in the dimensionality l . 4 Aggregate query answering We consider how to answer aggregate queries of two categories: point queries and range queries. 4.1 Answering point queries Point queries can be answered efficiently using a PAT. The algorithm is shown in Fig. 7 . We illustrate the major idea in the following example.
 Example 4.1 (Point query answering) Let us use the PAT shown in Fig. 3 to an-swer some illustrative point queries about the sliding window [ 1 , 2 ] in the data stream of our running example.
 gregate cell. Its aggregate, 2, is registered in the aggregate table of node a 1 b 1 . Following the path from the root to the node, we retrieve the answer immediately. cell. Instead, it is an infix aggregate cell. Thus, by the aggregate table associated with the infix link of label b 1 at node root , we can retrieve the aggregate. Please note that the aggregates are stored by instants in the aggregate table, i.e., two rows ( 1 , 2 ) and ( 2 , 7 ) are in the aggregate table of the infix link. We need to get the sum of them since  X  = X  in this query cell.
 by one node in the tree. Instead, following the infix link of label b 1 at node root , we can reach the linked list of all nodes having b 1 . Following the side-links, we can access all the nodes of b 1 in the tree.
 the infix link of label d 1 at the node. For example, at node a 1 b 1 , by the infix link visiting any node of d 1 . Similarly, at node a 2 b 1 ,wecanretrievetheaggregateof (  X  , a aggregate cells, 6, answers the query.
 ing the tree edge root -a 2 andinfixlinkof c 2 at node a 2 , we reach the local linked list of c 2 in the subtree of a 2 . The locality property of the side-links and the infix link avoids the search of the complete linked list of c 2 .Since a 2 b 1 c 2 has only one tuple matching the aggregate cell.
 ever, the aggregate table at node a 1 b 2 does not contain any row of instant 2. Thus, we can return null immediately without searching the subtree any more. guarantees that we do not need to search any extra nodes or branches. 4.2 Answering range queries Algorithm 4 can be called repeatedly to answer the queries. However, calling Algorithm 4 and thus searching the PAT repeatedly may not be efficient. lowing example.
 Example 4.2 (Range query answering) Let us consider how to answer range query ( 2 ,  X  , { b rithm 4 can be called four times to answer the point queries, respectively, and the sum, 7, should be returned.
 We start from the root node, since the first non- X  dimension value in the query cell should be either b 0 or b 1 on dimension B , we search the infix links of b 0 and b 1 from the root node. Since there exists no infix link of b 0 , we prune the range query rooted at the nodes having label b 1 , which are linked by the side-links. That is, a part of the search space is also pruned.
 them one by one. For node a 1 b 1 , since the next non- X  dimension value in the query d are searched, and only the infix link of d 2 hasarowoftime-stamp2.Thus,the aggregate 3 is extracted. Similarly, aggregate value 4 is extracted from the subtree of a 2 b 1 . Thus, the sum 7 is returned.
 answering range queries is that, instead of searching a PAT many times, we con-duct the search using the range query from the root of a PAT. At each node under the search, the query range is narrowed using the information of the available in-fix links and the corresponding aggregate tables, and the unnecessary nodes are pruned from the search space using the range specification in the query. By pro-gressive pruning, we search the PAT only once for any range query. 5 Experimental results In this section, we report the experimental results from a systematic performance study. All the algorithms are implemented in C++ on a laptop PC with a 2.8 GHz Pentium 4 processor, a 60 GB hard drive, and 512 MB main memory, running Microsoft Windows XP operating system. In all of our experiments, the PATs reside in main memory.
 erate the data sets, our data generator takes the parameters of the Zipf factor, the dimensionality, the number of tuples, and the cardinality in each dimension. To generate a tuple, we generate the data for each dimension independently following the Zipf distribution. The dimensions in the synthetic data sets are independent, and there is no correlation among any dimensions.
 data cube and data warehouse computation, including [ 6 , 33 , 38 , 42 ]. To some extent, it is a benchmark approach to generating synthetic data sets for data cube computation.
 weather data set is well accepted as a benchmark data set for data cube computa-tion [ 6 , 33 , 38 , 42 ].
 method as described in [ 6 ] and a baseline method. The baseline method just simply stores and sorts all tuples in the current sliding window. As expected, the baseline method uses the least main memory to store the data and costs the least runtime to maintain the current sliding window. The tradeoff is the slowest query answering performance. To answer any query, the baseline method has to scan the tuples in the current sliding window. A binary search can be used to locate the tuples match-ing the time interval of the query. On the other hand, the BUC method computes the whole data cube. It costs the most in computing the whole cube and storing the aggregates. We measure both the main memory cost of BUC and the size of the data cube computed by BUC, which is stored on disk. To answer a query, BUC only needs to conduct a binary search to allocate the corresponding aggregate tu-ple. Thus, the query answering time is fast. PAT is in between: to compute and store the aggregates for the current sliding window, it costs more space and run-time than the baseline method but less than the BUC method; on the other hand, in query answering, it searches less than the baseline method accordingly. 5.1 Query answering We first report the performance of answering point queries. The results on range queries are similar. In each test, we randomly select 1000 aggregate cells from the data cube and use them as the point queries. In other words, all queries are on non-empty aggregates.
 BUC. In this performance study, we store the whole data cube in main memory as a table, and all tuples are sorted according to the dictionary order. Therefore, answering any point query using the whole cube can be done by a binary search. This is probably the best case of query answering using a data cube. In real ap-plications, usually a whole data cube cannot be held into main memory. To this extent, our experiments favor the query answering using the whole cube. In Fig. 8 , we measure the query answering time taken by 1000 queries. All curves are plot-ted in logarithmic scale.
 query answering performance than the baseline method. The baseline method is two orders of magnitudes slower. That simply indicates that, in order to answer aggregate queries online, materializing some aggregate cells is very effective. However, as discussed before and will be shown later in this section, PAT com-putes and materializes much fewer aggregate cells than BUC. Thus, the space overhead for storing a PAT is much smaller than the space for all the aggregate cells computed by BUC. Moreover, BUC needs to scan the base table multiple times to compute a complete data cube. Therefore, PAT is a nice tradeoff between space and query answering time. 5.2 Building prefix aggregate trees We tested the size of the data cube computed by BUC, the size of PAT, the number of aggregates computed, the memory usage (the highest watermark of memory usage during the running of the programs) and scalability (runtime) of the three methods. Four factors are considered: the Zipf factor, the dimensionality, the car-dinality of the dimensions and the number of tuples in the current sliding window. The results are consistent. Some results are shown in Fig. 9 .
 since it simply maintains the tuples in the current sliding window and does not pre-compute any aggregate. Both BUC and PAT are sensitive the the Zipf factor: the smaller the Zipf factor, the more distinct aggregates exist in the data set. However, PAT runs faster than BUC since it computes much less aggregate cells than BUC. the size of the current sliding window in Baseline. The size of PAT counts both the prefix aggregates, infix aggregates and links. The number of tuples varies from 500 thousand to 2 . 5 million so that the scalability of the methods are tested. All three methods are roughly linear on the number of tuples, but PAT and the baseline method generate much smaller results than BUC. In other words, Baseline does not generate any aggregates but only the base tuples are maintained. PAT generates the prefix aggregates and the infix aggregates, which form a substantially small subset of all the aggregates generated by BUC. Even when the whole data cube is over 2 . 5 GB, the PAT including the links occupies less than 500 MB, which is less than three times of the size of all base tuples and can be easily accommodated in main memory.
 gates and infix aggregates) in PAT is linear in the cardinality of the dimensions. The trend is mild. When the cardinality increases, the data set becomes sparse and thus the total number of aggregates also increases. BUC computes all aggregates. As shown in the same figure, the increase of all aggregates is sub-linear in our experiments, but the number of all aggregates is much larger than the number of prefix aggregates and infix aggregates computed by PAT.
 stores the aggregates on disk. It only maintains the base table in main memory. Thus, it uses the same amount of main memory as the baseline method. Since the PAT resides in main memory, the memory usage of constructing a PAT increases as the dimensionality increases. When there are many dimensions, PAT has many levels. structing a PAT is also much faster than computing a data cube using BUC. 5.3 Incremental maintenance When testing the performance of incremental maintenance, we always set the number of tuples at each instant to a constant. A sliding window of 10 instants was used. We set the number of tuples in the original PAT to 500K. We only com-pare the PAT method and the baseline method. For BUC, there is not incremental maintenance algorithm. Thus, to incrementally maintain all the aggregates, we have to compute the data cube on the new data and merge the new aggregates with the existing ones. It has a similar performance as shown in Sect. 5.2 . Some results are shown in Fig. 10 .
 tent with Fig. 9 a. With a lower Zipf factor, the data set is sparser and thus PAT com-putes more prefix aggregates and infix aggregates. The baseline method is con-stant since it does not compute any aggregates. However, by comparing Figs. 9 a and 10 a, we can see that the incremental maintenance time is much shorter, since many paths in the existing PAT can be reused in the incremental maintenance. as the dimensionality increases, since the tree becomes larger and taller on high dimensional data sets. The maintenance time of Baseline also increases, but it is linear.
 and Baseline with respect to the number of new tuples at each instant. The result shows that both PAT and Baseline have an approximately linear scalability. This is consistent with the analysis of the PAT incremental maintenance algorithm. tuples at each instant. Interestingly, we observed that, under a given data distribu-tion, the size of the PAT is stable and insensitive to the number of tuples in the incremental part. In other words, the size of PAT mainly depends on the number of tuples in the sliding window, and is stable during the incremental maintenance. This is a nice property for data stream processing: no matter how large the data stream is, we will have an index structure of a stable size.
 when the Zipf factor is small, the data is sparse and thus not many prefixes can be shared. When the Zipf factor becomes larger, the data becomes more skewed, and the PAT becomes smaller due to the more sharing of the prefixes. The baseline uses constant memory in the maintenance, since it only needs to load the current sliding window into main memory.
 space usage with respect to the size of sliding window. 5.4 The effect of the order of dimensions PATs and the runtime of PAT construction. We made up a synthetic data set of 10 dimensions, Zipf factor 3 and 1 million tuples. The i -th dimension ( 1  X  i  X  10 ) has a cardinality of i . We tested the effects of the following 4 orders of dimensions:  X  R 1 : cardinality ascending order;  X  R 2 : cardinality descending order;  X  R 3 : D 5 -D 6 -D 4 -D 7 -D 3 -D 8 -D 2 -D 9 -D 1 -D 10 ;and  X  R 4 : D 1 -D 10 -D 2 -D 9 -D 3 -D 8 -D 4 -D 7 -D 5 -D 6 . the order of dimensions, since the number of tree node accesses is basically the same no matter which order is used.
 the orders. With order R 1 , putting dimensions of low cardinality ahead strongly facilitates the prefix sharing, and leads to the smallest number of nodes. As dis-cussed at the end of Sect. 3.1 ,atlevel i , the size of the tree nodes is proportional to the sum of cardinalities in dimensions D i + 1 to D n . Therefore, the average size of tree nodes using order R 1 is the largest. However, the advantage of reduction on number of nodes well overcomes the disadvantage of large tree node size. Thus, order R 1 achieves the smallest tree. Order R 2 suffers from deficiency in sharing the prefixes. Although its average tree node size is the smallest, the tree size turns out to be the largest. Orders R 3 and R 4 stay in between.
 nality ascending order to explore possible sharing of prefixes. However, there is no theoretical guarantee that the cardinality ascending order always leads to the smallest tree. 5.5 Results on the weather data set We also tested the PAT construction using the well-accepted weather data set [ 19 ], which contains 1 , 015 , 367 tuples and nine dimensions. The dimensions with the cardinalities of each dimension are as follows: station-id ( 7037 ) , longitude ( 352 ) , change-code ( 10 ) , hour ( 8 ) , and brightness ( 2 ) . Eight data sets with 2 X 9 dimen-sions are generated by projecting the weather data set on the first k dimensions ( 1  X  k  X  9 ) . Figure 11 shows the results.
 parable to the size of QC-tree (241 . 2MBasreportedin[ 26 ]), a recently developed data cube compression method. However, to construct a QC-tree, the base table has to be scanned and sorted multiple times, and the incremental maintenance of a QC-tree is more costly than PAT. The PAT construction is also much faster than computing the whole cube using BUC but slower than Baseline (Fig. 11 b). As indicated in [ 25 ], construction of a quotient cube is slower than BUC, since extra work is needed to achieve compression.
 the observations that we obtained from the experiments on the synthetic data sets. 5.6 Summary Based on the above experimental results, we have the following observations. makes the PAT feasible in space for data streams. Second, our algorithms for con-structing and incrementally maintaining a PAT are efficient and highly scalable for data streams. The construction and maintenance cost is dramatically smaller than the cost of materializing the whole cube. Third, query answering using a PAT is comparable to the best cases using a full cube. It is much faster than the base-line method. The PAT approach can be regarded as a good tradeoff between the construction/maintenance cost and the query answering performance. 6 Conclusions Online warehousing data streams and answering ad hoc aggregate queries are in-teresting and challenging research problems with broad applications. In this paper, we propose a novel PAT data structure to construct an online data warehouse. Ef-ficient algorithms are developed to construct and incrementally maintain a PAT over a data stream, and answer various ad hoc aggregate queries. We present a systematic performance study to examine the effectiveness and the efficiency of our design.
 References Author Biographies
