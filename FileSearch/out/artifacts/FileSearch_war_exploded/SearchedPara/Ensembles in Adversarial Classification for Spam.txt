 The standard method for combating spam, either in email or on the web, is to train a classifier on manually labeled instances. As the spammers change their tactics, the perfor-mance of such classifiers tends to decrease over time. Gath-ering and labeling more data to periodically retrain the clas-sifier is expensive. We present a method based on an ensem-ble of classifiers that can detect when its performance might be degrading and retrain itself, all without manual interven-tion. Experiments with a real-world dataset from the blog domain show that our methods can significantly reduce the number of times classifiers are retrained when compared to a fixed retraining schedule, and they maintain classification accuracy even in the absence of manually labeled examples. I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation ; I.5.4 [ Pattern Recognition ]: Applications X  text processing Spam, Weblogs, Ensembles, Adversarial Classification, Non-stationarity, Retraining Algorithms, Experimentation
As we build more robust classifiers to detect spam, the methods used by spammers evolve so that they can continue to get their messages through. This leads to changes in the distribution of the data seen by classifiers used to weed out spam. Such changes can also occur naturally, for example, as topics change in a blog or one company is bought by an-other and the internal email traffic changes in form and con-tent. Unless this non-stationarity of the data is taken into account, classifier performance will decrease over time. For example, one common method used by spammers is to add text from legitimate sources (e.g., emails, blogs, newswires, books) to their communications in an attempt to get past the classifiers as false negatives [4]. When many of these messages get through, classifiers are often updated, which leads to an arms race of sorts.

Two natural questions arise. First, how does one deter-mine when classifier performance has degraded? Second, how are the classifiers retrained? The most common answers to these questions involve a continual stream of labeled ex-amples, such as current emails labeled according to whether or not they are spam. These labeled examples can be used to test the accuracy of the classifiers and to retrain them when accuracy becomes too low. The problem with this approach is that obtaining labeled examples is expensive.

In this paper we present a method based on an ensemble of classifiers that automatically, without human intervention, deals with the two questions above. First, we use mutual agreement between classifiers in the ensemble to detect pos-sible changes in classifier accuracy. The mutual agreement between a pair of classifiers is the fraction of time they as-sign an instance the same class label. Note that the true label need not be known to compute mutual agreement. We show, using real data from the blog domain, that changes in mutual agreement are indicative of decreased classifica-tion accuracy. Second, we use the output of the ensemble as a proxy for the true label for new instances and retrain individual classifiers identified as possibly weak via mutual agreement.

There is significant work on using ensembles to tackle con-cept drift. These approaches are motivated by the fact that it is often useful to have multiple (weighted) opinions be-fore making a decision. [6] presented an ensemble method for concept drift by assigning weights to classifiers based on their accuracies. [3] extended this work by dynami-cally creating and removing weighted experts in response to changes in performance. Both methods rely on a stream of labeled examples, whereas ours does not. [5] presented another ensemble-based method for concept drift, claiming it to have low computational cost and still be comparable with other ensemble methods. [1] presented a random deci-sion tree ensemble-based engine to mine data streams. They demonstrated the ability to detect concept drift on the fly and discussed ways to combine old data with new data for computing optimal models. All the data they used was syn-thetic.
Spam blogs (splogs) are typically constructed automati-cally by combining legitimate content taken from the web with ads or links to other sites in an attempt to boost their rank in search engines. Spam blogs are a tremendous prob-lem for intermediate servers and blog search engines as dis-cussed and studied in [2].

The empirical results reported in Section 4 are all based on two blog datasets, called SPLOG2005 and SPLOG2006, that we manually constructed. SPLOG2005 was obtained by sampling the update ping streams at a blog search en-gine (Technorati, http://technorati.com) in the year 2005. 700 positive examples (splogs) and 700 negative examples (blogs) were identified and labeled by hand. SPLOG2006 was created by processing the update ping streams at a ping server (http://weblogs.com) in the year 2006. 750 positive and 750 negative examples were also manually labeled for SPLOG2006. Training and testing on SPLOG2006 in gen-eral resulted in better classifier performance given that it was sampled at a ping server. SPLOG2005, which was sam-pled through a blog search engine, contained splogs that had already passed one set of filters and were thus possibly more difficult to automatically identify as such.

The features extracted from data are crucial for the suc-cess of machine learning methods. Features that work in the email and web spam domains may not work in the blog domain. Therefore, we used a variety of features, some of which are novel, as described below.
The utility of using an ensemble of classifiers is that the ensemble can perform well even if some of its member classi-fiers perform poorly. A good ensemble has classifiers that are diverse, perhaps having disjoin t feature sets. An adversary, such as a spammer, is unlikely to change all the features at the same time, so only the classifiers based on the fea-tures that change will be affected, not the others. The other classifiers can still keep the overall ensemble performance high. Eventually, though, the underperforming classifiers will have to be retrained to prevent further changes to the features from reducing the accuracy of the ensemble.
Whenever two different classifiers give the same label to an instance, we say they agree. By doing this test on a suffi-ciently large number of instances, we can estimate their mu-tual agreement. Let f i ( x k ) be the output of the i th on the k th instance. Let  X  ( p ) evaluate to 1 if p ,apredi-cate, is true. Otherwise, it evaluates to 0. Then the mutual agreement between classifiers i and j can be estimated as follows: Over a period of time, if the mutual agreement between a pair of classifiers decreases, it may indicate that one or both the classifiers are performing poorly. Using mutual agreement as a potential indicator of performance, we were able to do the following. This section describes two experiments with our method. The first shows the usefulness of mutual agreement in re-ducing retraining time. The second experiment shows how mutual agreement can be used to improve the accuracy of an ensemble as compared to performing no retraining and that it is as good as frequent retraining with significantly less computation.
The first experiment demonstrates how mutual agreement can be used to reduce retraining time and track performance of individual classifiers in an ensemble. The following cases were considered in this experiment. For the cases that follow, the classifiers were trained on SPLOG2005, and then selectively retrained based on differ-ent criteria. 95% threshold 79.48 1.2 100% threshold 80.44 2 Always retrain 82.74 14 Table 1: Experiment 1a: Retraining to maintain mutual agreement using ensemble labels 95% threshold 80.48 1 100% threshold 81.5 1.6 Always retrain 5 84.26 14 Table 2: Experiment 1b: Retraining to maintain mutual agreement using true labels
In addition, cases 3, 4, and 5 were performed in the set-ting with true labels instead of ensemble labels. Also, these cases were repeated 5 times with different random orders of the SPLOG2006 dataset instances. The average results are provided in Table 1. All of the agreement values are given in percentages and the  X % X  sign has been dropped.  X  X e-trainings X  means the number of times the classifiers were retrained.  X  X nsemble labels X  means retraining was done us-ing samples labeled by the ensemble.  X  X rue labels X  stand for the case of using true labels for retraining.
 Clearly, with no retraining, the agreement value goes down. Experimental results show that by using mutual agreement for retraining we reduce retraining time drastically while maintaining the initial agreement between classifiers. We next show how mutual agreement can be used in an ensem-ble setting for dynamic retraining of base classifiers.
To demonstrate how members of an ensemble can be trained dynamically based on mutual agreement as a trigger, we considered 5 classifiers in all possible pairs. Performance is measured in terms of the accuracy of the ensemble. The 5 classifiers were based on the text, character, outlink, anchor, and tag feature sets. Classifiers based on character-gram and word-gram feature sets were left out as they are similar to the classifier based on text features, and therefore contribute little to ensemble diversity. All of the classifiers were trained on SPLOG2005 and initial agreements were determined by performing 10 fold cross-validation as in the earlier experi-ment. This gave rise to a fully connected graph of 5 clas-sifiers where the nodes are the classifiers and the arc labels are the agreement values between the classifiers. The classi-fiers were then exposed to the SPLOG2006 dataset and, as in the earlier experiment, the agreement values for the clas-sifiers were checked after every 100 instances. So with every iteration we update the edges of the graphs with agreement values for that iteration. Unlike the previous experiment, we make definite claims about accuracy here. This experiment was carried out in the following settings, mainly differing in the retraining algorithm.
 Table 3: Experiment 2a: Impact on ensemble accuracy, retraining using ensemble labels Table 4: Experiment 2b: Impact on ensemble accuracy, retraining using true labels
Cases 2, 3, and 4 were also repeated with true labels in-stead of ensemble labels. Each case was repeated 5 times with different random orderings of the SPLOG2006 dataset instances. Some of the experiments were also performed with a 100% threshold but no significant improvement in accuracy was observed. Therefore, we only report results with a 95% threshold. All of the results are shown in Table 4 where the columns are the same as Table 1. The only dif-ference is that, instead of agreement values, we report the accuracies of the ensemble. The accuracy values are given in percentages and the  X % X  sign is dropped.

Figure 1 shows a plot of accura cies for no retraining, and for full retraining using both true and ensemble labels. This plot shows lower and upper bounds on performance. Two additional accuracy plots are shown -one for experiments using true labels, Figure 3, and the other for experiments Retraining Ensemble Labels True Labels Weak pairs 93 91 Retraining Ensemble Labels True Labels Weak pairs None Tag,Anchor Table 5: Summary of agreement graph after two iterations Figure 1: Full retraining accuracy using ensemble vs. true labels and no retraining
Figure 2: Accuracy with ensemble labels for retraining using ensemble labels, Figure 2. Note that the data series line for the  X  X o Retraining X  case has been added just for comparison, it is the same for both cases. All these exper-iments show how mutual agreement can be used to reduce the retraining time, while maintaining the accuracy close to that of frequent retraining.
The experimental results in the previous section showed how mutual agreement can be used for dynamic retraining of base classifiers to handle adversarial classification. The main idea is that decreasing mutual agreement is an indica-tor that one or more classifiers are performing poorly. Said differently, when mutual agreement is above threshold, we assume that the classifiers are performing well. However, it could very well happen that, though the classifiers agree, they are both wrong. In short, the relation between mu-tual agreement and accuracy of the classifiers is not perfect. Thiscanbemoresevereifweconsiderjusttwoclassifiers at a time rather than a larger set of them. The probability of both the classifiers succumbing to the spammers chang-ing tactics in the same way cannot be neglected. But in an ensemble setting where we have more then two classifiers, the probability that most of the classifiers will be wrong at a given time decreases. The more classifiers there are, the lower the probability of them all going wrong at the same time. But, for this statement to hold true, we need to keep the classifiers diverse. So if the adversary changes some of the features, only classifiers based on those features may be affected. The rest should still perform well. [1] W. Fan. Streamminer: a classifier ensemble-based [2] P. Kolari, A. Java, and T. Finin. Characterizing the [3] J. Z. Kolter and M. A. Maloof. Dynamic weighted [4] D. Lowd and C. Meek. Good word attacks on statistical [5] M. Scholz and R. Klinkenberg. An ensemble classifier [6] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining
