 Anomaly detection (also known as outlie r detection) plays a key role in data mining for detecting unusual patterns or events in an unsupervised manner. In particular, there is growing interest in collaborative anomaly detection [1,2,3], where multiple data sources submit thei r data to an on-line service, in order to detect anomalies with respect to the wider population. For example, in partic-ipatory sensing networks (PSNs) [4], participants collect and upload their data to a central service to detect unusual eve nts, such as the emergence of a source of pollution in environmental sensing, or disease outbreaks in public health mon-itoring. A major challenge for collaborative anomaly detection in this context is how to maintain the trust of participants in terms of both the accuracy of the anomaly detection service as well as the privacy of the participants X  data. In this paper, we propose a random perturbation scheme for privacy-preserving anomaly detection, which is resilient to a variety of privacy attacks while achiev-ing comparable accuracy to anomaly detection on the original unperturbed data.
There have been several studies on colla borative anomaly d etection, where a number of participants want to build a global model from their local records, while none of the participants are willing to disclose their private data. Most existing work relies on the use of Secure Multiparty Computation (SMC) to generate the global model for anomaly detection [1,2]. While this approach can achieve high levels of privacy and accu racy, SMC incurs a high communication and computational overhead. Moreover, SMC based methods require the simul-taneous coordination of all participants during the entire training process, which limits the number of participants. Thus, an open research challenge is how to improve scalability while achieving high levels of accuracy and privacy.
To address this challenge, we propose a privacy-preserving scheme for anomaly detection called Random Multiparty Perturbation ( RMP ). RMP supports the scenario where participants contribute their local data to a public service that trains an anomaly detection model from the combined data. This model can then be distributed to end-users who want to test for anomalies in their local data. In this paper, we focus on the use of an auto-associative neural network (AANN, also known as an autoencoder) as our anomaly detection model, although our scheme is also applicable to other types of anomaly detectors.

In order for the participants of RMP to maintain the privacy of their data, we propose a form of random perturbation to be used by each participant. Pre-vious approaches to random perturbation in this context [5,6,7,3] require all participants to perturb their data in the same way, which makes this scheme potentially vulnerable to breaches of privacy if collusion occurs between rogue participants and the server. In contrast, RMP proposes a scheme in which each participant first perturbs their contributed data using a unique, private random perturbation matrix. In addition, any end-user can apply the resulting anomaly detection model to their own local data by using a public perturbation matrix. This provides a scalable collaborative approach to anomaly detection, which en-sures a high level of privacy while still achieving a high level of accuracy to the end-users.

The main contributions of this paper are as follows: (i) We propose a privacy-preserving model of collaborative an omaly detection bas ed on random pertur-bation, such that each participant in training the anomaly detector can have their own unique perturbat ion matrix, while the resulting anomaly detection model can be used by any number of users for testing. (ii) In contrast to pre-vious methods, we give the first privacy-preserving scheme for auto-associator anomaly detectors that does not require computationally intensive cryptographic techniques. (iii) We show analytically the resilience of our scheme to two types of attacks X  X ayesian Estimation and Independent Component Analysis (ICA). (iv) We demonstrate that the accuracy of our approach is comparable to non-privacy preserving anomaly detection on various datasets.
 In general, privacy-preserving data mining (PPDM) schemes can be classified as syntactic or semantic . Syntactic approaches aim to prevent syntactic attacks, such as the table linkage attack. Semantic approaches aim to satisfy semantic privacy criteria, which are concerned with minimising the difference between adversarial prior knowledge and adversarial posterior knowledge about the individuals rep-resented in the database. While both approaches have the same goal, i.e., hiding the real values from the data miner, syntactic approaches typically take the en-tire database as input, whereas semantic approaches do not. Since in participa-tory sensing the participants are responsible for masking their own data before outsouring them to the data miner, semantic approaches are the only option here.
Inthefollowingwereviewexistingworkonprivacy-preservingback-propagation and anomaly detection in the context of collaborative learning, and highlight the major differences with our work. The majority of semantic PPDM approaches, either in the context of privacy-preserving back-propagation [8,9] or privacy-preserving anomaly detection [1,2], use Secure Multiparty Computation (SMC). SMC approaches suffer from high computational complexity. Moreover, they require the cooperation of all participants throughout the whole process of building the data mining models, and hence suffer a lack of scalability.
Another approach to semantic PPDM is data randomisation , which refers to the randomised distortion or perturbation of data prior to analysis. Perturbing data elements, for example, by introducing additive or multiplicative noise, al-lows the server to extract the statistical properties of record s without requiring the participants to allocate substantial resources or coordinate with the server during the training process. We now outline the main types of data perturbation methods: additive, multiplicative, and nonlinear transformation.

Additive perturbation adds noise to the data [10,11]. Since independent ran-dom noise can be filtered out [12,13], the general principle is that the additive noise should be correlated with the data [14,15].

Multiplicative perturbation multiplies the data with noise. A typical approach is to use a zero-mean Gaussian random matrix as the noise for a distance-preserving transformation [5]. However, if the original data follows a multivariate Gaussian distribution, a large portion of the data can be reconstructed via at-tacks to distance-preserving transformations as proposed in [16,17].
In general, distance-preserving transformations are good for accuracy but are susceptible to attacks that exploit distance relationships. In comparison, the random transformation approach [7], where the noise matrix is a random matrix with elements uniformly distributed betw een 0 and 1, is not dis tance-preserving and hence not susceptible to these attacks.

Nonlinear transformations canbeusedtobreakthedis tance-preserving effect of a distance-preserving transformation on some data points. For example, the function tanh approximately preserves the distance between normal data points, but collapses the distance between outliers [3] X  X t is hence suitable for anomaly detection. In RMP ,weusethe double logistic function for this purpose and for conditioning the probability density function (pdf) of the transformed data.
The aforementioned works satisfy privacy requirements in the case where all participants are assumed to be semi-honest (i.e., are not colluding with other parties) and agree on using the same perturbation matrix. As discussed in Sec-tion 3.1, this assumption is restrictive in applications such as participatory sens-ing. A few works have tried to extend current randomisation approaches to a collaborative learning architecture [6,18]. For example, Chen et al. X  X  framework [6] is a multiparty collaborative mining scheme using a combination of additive and multiplicative perturbations. This scheme requires the participants to stay in touch for an extended period to generate the perturbation matrix, which is impractical for large-scale participatory sensing. Liu et al. [18] build a frame-work on top of [5], and allow each participant to perturb the training data with a distinct perturbation matrix. This scheme then regresses these mathematical relationships between training samples in a unique way, thereby preserving clas-sification accuracy. Although participants use private matrices, the underlying transformation scheme is still vulnerable to distance inference attacks [16,17].
In summary, data randomisation is the most promising approach to PPDM in the paradigm of PSN. Unlike distance-preserving transformations, random transformation does not preserve Euclidean distances and inner products be-tween data distances, hence it thwarts dis tance inference attacks. However, if this method is applied to collaborative learning, all the participants must agree on the same perturbation m atrix, and then collusion attacks may succeed. Fur-thermore, the mining models generated from the perturbed records are specific to the participants. For these reasons our RMP scheme uses a random transfor-mation and provides each participant with a private perturbation matrix. The perturbation matrices are tailored so that both data privacy and accuracy of the mining models are achieved. In addition, RMP generates models that can be adopted by any user, i.e, data contributors and non-contributors. In this section we present our Random Multiparty Perturbation ( RMP ) scheme, which considers a general participatory sensing architecture comprising three types of parties, namely, participants, the data mining server and end-users. 3.1 Problem Statement and Design Principles We consider the case of a participatory sensing network that comprises a set of participants, C = { c i | i =1 ,...,q } , a mining server S , and an arbitrary number of end-users U , as can be seen in Fig. 1. Each participant c i is an individual who captures data records for the same set of attributes, i.e., a horizontal parti-tion, and contributes the sampled records to S for training purposes. The server S is a third-party providing a data mining service to the participants. After receiving the contributed data records, the server t rains an anomaly detector that generates a global classification model M from the locally collected data. The end-users, U , could be the participants themselves or third parties such as analysts trying to learn about the monitored phenomena. We share a similar underlying assumption with [4], in which the computational demands on the par-ticipants should be minimised, while the anomaly detection model should be in a form that can be disseminated and used by an arbitrary number of end-users, and not limited to the original participants or customised for a small number of end-users. In addition, we adopt a well established assumption in the literature that regards all parties as semi-honest entities, i.e., they follow the protocol. To make the scenario realistic, we assume that a small subset of the participants might collude with the server to infer other participants X  submitted records. De-signing a collaborative data mining scheme that fulfills these requirements with low communication and computation costs while preserving the participants X  privacy is an open problem.

For privacy, we need to ensure that the attribute values in the participants X  contributed records are pr operly masked: given the masked values, the server cannot infer the original values. However, this must be done without over-sacrificing accuracy, i.e., the results of anomaly detection based on the masked data should be close to the corresponding result using the original data. Multi-plicative perturbation projects data t o a lower dimensional space. The perturbed data matrix has a lower rank than the original data matrix, thereby forcing the attacker to solve an undetermined system of linear equations. However, this is not enough, and the following design principles are pertinent.
 Resilience to Distance Inference Attacks: The review of existing multiplica-tive perturbation schemes in Section 2 reveals that distance-preserving transfor-mations are susceptible to distance inference attacks [16,19]. The challenge is to find a non-distance preserving transform that is suitable for certain data mining tasks. Random transformation [7] qualifies as such a transform in that it does not preserve the dot product or Euclidean distance among transformed data points, yet it is suitable for anomaly detection.
 Resilience to Bayesian Estimation Attacks: Bayesian Estimation is a gen-eral attack that exploits the pdf of the original data. Gaussian data is particularly exploitable because it reduces a maximum a posteriori estimation problem into a simple convex optimization problem [17]. A suitable defense is to prevent this reduction by conditioning the pdf through a nonlinear transformation. Resilience to Collusion: Let X i  X  R n  X  m i be participant c i  X  X  dataset, and T  X  R w  X  n be a random matrix shared by all participants ,where w&lt;n .The participant c i perturbs its records as Z i = TX i . Since the constructed anomaly detector is perturbed, the perturbation matrix needs to be shared with all end-users. This approach of using a common T poses a serious privacy risk. If a rogue participant or end-user colludes w ith the server, the server can recover any participant X  X  original data using the breached perturbation matrix. The naive solution of generating an arbitrarily different perturbation matrix T i for each participant c i does not work, because building an accurate mining model requires consistency among the perturbation matrices. To overcome this challenge, RMP generates participant-specific perturbation matrices by perturbing T . 3.2 The Scheme RMP is designed to address the design principles in the previous subsection. Let T be a w  X  n matrix ( w&lt;n ) with U (0 , 1)-distributed elements. Each participant c generates a unique perturbation matrix where each element in i is drawn from U (  X  , ), and 0 &lt; &lt; 1. Experimental results show that for small values of , the accuracy loss in anomaly detection is small. Next, we describe RMP in full detail.
 In RMP , a participant transforms X i to Z i in two stages: Stage 1: The participant transforms X i to Y i , by applying a double logistic function to X i element-wise: for k =1 ,...,n , l =1 ,...,m i , where sgn is the signum function. For suitable values of  X  and normalised values of x k,l (i.e., | x k,l | X  1), the double logistic function approximates the identity function y k,l = x k,l well. To maximise this approximation, we equate the optimal value of  X  to the value that minimises the integral of the squared difference between the two functions: The role of this nonlinear transformation is to condition the pdf of Y i to thwart Bayesian Estimation attacks, as detailed in Section 5.
 Stage 2: Using  X  T i generated earlier, the participant transforms Y i to Z i : The participant then sends Z i to the mining server S , which will receive the following from all the participants:
The role of S is to learn an anomaly detection model encoding the underlying distribution of Z all . End-users given access to the model and T can then detect anomalies in their data with respect to the model. RMP is independent of the anomaly detection algorithm used, but the auto-associative neural network is used for our study and is discussed next. In our collaborative framework, the role of the server S is to learn an anomaly detection function, which can then be disseminated for use by the end-users. In particular, the learned anomaly detection function should encode a model of the underlying distribution of the (perturbed) training data provided by the partic-ipants. A given data record can then be tested using this model for anomalies.
While our general framework can potentially use a wide variety of anomaly de-tection models, in this paper we use an auto-associative neural network (AANN) [20], also known as an auto-encoder, as the basis for our anomaly detection func-tion. We choose to use an AANN for our anomaly detection function because: (i) it can be trained in an unsupervised manner on either normal data alone, or a mixture of normal data with a small but unspecified proportion of anomalous data; (ii) it is capable of learning a wide variety of underlying distributions of training data; and (iii) the resulting anomaly detection function is compact and computationally efficient X  X ence practical for dissemination to end-users.
An AANN is a multi-layer feed-forward neural network that has the same number of output nodes as input nodes. Between the input and output layers, a hidden  X  X ottleneck X  layer of a smaller dimension than the input/output layers captures significant features in the inputs through compression. Training the AANN means adjusting the weights of the network for each training record, so that the outputs closely match the inputs. In this way, the AANN learns a nonlinear manifold that represents the underlying distribution of the data. In a trained AANN, the reconstruction error (integrated squared error between the inputs and outputs) should be high for anomalies, but low for normal data. Let e i be the reconstruction error for training dataset X i . If the reconstruction error for a test record is larger than the threshold  X  =  X  ( e i )+3  X  ( e i ), where i =1 ,...,q , the record is identified as an anomaly. Due to space constraints, the interested reader is referred to [20] for the details of the AANN training algorithm. In the statistical disclosure control and privacy-preserving data publishing liter-ature, the most popular semantic privacy criterion is differential privacy . Differ-ential privacy is for answering queries to a database containing private data of multiple individuals . For the participatory sensin g scenario where participants are data owners who publish data (instead of answering queries) about themselves alone , differential privacy is not a good fit. Below, we propose an alternative (in-formal) privacy criterion.
Linear multiplicative perturbation schemes aim to project a data matrix to a lower dimensional space so that an attacker has only an ill-posed problem in the form of an underdetermined system of linear equations  X  T x = y to work with, where y is a projection of data vector x and the projection matrix  X  T is assumed known in the worst case. An underdetermined system cannot be solved for x exactly, but given sufficient prior information about x , an approximation of the true x might be attainable. In a known input-output attack , the attacker has some input samples (i.e., some samples of x ) and all output samples (i.e., all samples of y ), and knows which input sample corresponds to which output sample [21,22,19]. In the collaborative learning scenario where the data miner may collude with one or more participants to unravel other participants X  data, the known input-output attack is an immediate concern. In the following, our privacy analysis is conducted with respect to two known input-output attacks, one based on Bayesian estimation, and one based on Independent Component Analysis (ICA). Suppose the attacker is targeting a particular participant by trying to solve Z =  X  TY =( T + ) Y for Y . In the analysis below, let z  X  Z represent a column of Z ,and y  X  Y represent a column of Y . 5.1 Attacks Based on Bayesian Estimation We consider two scenarios where  X  T is known and where  X  T is unknown. Scenario where  X  TisKnown: For a worst-case analysis, we assume the at-tacker somehow knows  X  T exactly but not X . In a Bayesian formulation, the maximum a posteriori (MAP) estimate of y ,given  X  T and z ,is where Y = { y : z =  X  T y } [17]. Note that MAP estimation is a more general approach than maximum likelihood estimation because the former takes a prior distribution (which in our case is p Y ) into account. If p Y is an n -variate Gaus-sian with a positive definite covariance matrix, then (4) becomes a quadratic programming problem with solution [17, Theorem 1]: where  X  y and  X  Y are the sample mean vector and sample covariance matrix of Y respectively, and  X  Z is the sample covariance matrix of Z .Notethat  X  Y is positive definite, provided the covariance matrix is full rank and there are more samples of Y than dimensions of Y [23]. In this case, we can write  X  Y = QQ , where Q is a nonsingular matrix. Furthermore,  X  Z =  X  T X  Y  X  T =  X  TQQ  X  T .Since  X  T is nonsingular,  X   X  1 Z and therefore the solution (5) exists.

The analysis above suggests that to thwart MAP estimation, we cannot hope to generate  X  T such that  X  Z is singular. Instead, it is more productive to pre-vent MAP estimation from being reducible to a quadratic programming problem solvable by (5) in the first place. RMP achieves this by nonlinearly transforming the original data. If X is a standard Gaussian random variable, then based on [24, Section 4.7], the pdf of Y =sgn( X )[1  X  exp(  X   X  X  2 )] is Unlike a standard Gaussian which is continuous everywhere and has a global maximum at x =0, p Y  X  X  X  at y =  X  1 , 0 , 1. Using p Y or ln p Y in (4) renders the optimisation problem non-convex, and numerically unstable near y =  X  1 , 0 , 1, which is problematic for numerical methods such as hill climbing and simulated annealing. Applying the same nonlinear transformation to Laplace-distributed data (sparse data) has the same effect. Therefore, RMP  X  X  nonlinear transfor-mation converts a potentially Gaussian (Laplace) data distribution to a non-Gaussian (non-Laplace) one that hampers the attacker X  X  solution of (4).
The double logistic function is better than tanh, which is used in [3], in terms of thwarting MAP estimation attacks. If X is a standard Gaussian random vari-able, then the pdf of Y = tanh( X )is p Y ( y )= 1 For | y | X  0 . 9, the pdf above is convex. This means the attacker can solve (4) as a convex optimisation problem, when the data are perturbed using tanh. Scenario Where  X  T is Unknown: In the previous scenario, the attacker knows T and the relationship between T and  X  T (see Equation 1). We also consider the scenario where the a ttacker does not know  X  T . Note that even with precise knowledge of T and , without further information, any matrix value between T  X  1 and T + 1 can be an estimate of the victim X  X  matrix  X  T . According to Lemma 1, for every element of  X  T , there is a 50% chance of guessing its value wrong by at least (2  X  Lemma 1. Let D be the difference between two U (  X  , ) -distributed random variables. Then for 0  X  d  X  2 , Pr[ | D | X  d ]=(2  X  d ) 2 / (4 2 ) . Proof. Let A and B be two U (  X  , )-distributed random variables. Then the pdf of D = A  X  B is given by the convolution To get Pr[ | D | X  d ]where0  X  d  X  2 , we integrate the expression above from  X 
MAP estimation can be used to estimate both Y and  X  T .TheMAPestimates p (  X  T | Z ) p Y ( Y | Z ). The optimisation problem can be written as or the following when p  X  T is substituted with U w  X  n ( T  X  1 , T + 1 )and p Y is assumed to be zero-mean Gaussian: In (6), y j ( j =1 ,...,m ) are columns of Y . Note that in the equality constraint, both  X  T and Y are optimisation variables, so even the Gaussian assumption does not reduce (6) to a convex problem. As previously explained, RMP  X  X  nonlinear transformation converts a potentially Gaussian (Laplace) data distribution to a non-Gaussian (non-Laplace) one. This hampers the attacker X  X  solution of not only (4) but also (6), which is a harder problem than (4). 5.2 Attacks Based on Independent Component Analysis (ICA) ICA can be used to estimate  X  T  X  R w  X  n and Y  X  R n  X  m , knowing only their product Z =  X  TY ,provided (i) w = n (the even-determined case) or w&gt;n (the over-determined case); ( ii) the attributes (rows of Y ) are pairwise independent; (iii) at most one of the attributes are Gaussian; (iv)  X  T has full column rank.
However, RMP enforces w&lt;n .When w&lt;n , the problem of ICA becomes overcomplete ICA (or underdetermined ICA). In this case, the mixing matrix  X  T is identifiable but the independent components are not [22]. Furthermore, when w  X  ( n +1) / 2, no linear filter can separ ate the observed mixture Z into two or more disjoint groups, i.e., recover any row of Y [5]. In this section we evaluate the quality of our proposed privacy-preserving anomaly detection scheme RMP when used with an AANN. The main objective of our experiment is to measure the trade-off in accuracy of our AANN anomaly de-tection algorithm as a result of maintaining the participants X  privacy. Note that Lemma 1 provides a theoretical relationship between and the privacy that can be achieved in terms of an attacker X  X  ability to estimate the value of a target victim X  X  perturbation matrix  X  T . Thus, in our empirical evaluation, we consider the effect of different levels of privacy in terms of on the overall accuracy of anomaly detection. We use the Receiver O perating Characteristic (ROC) curve and the corresponding Area Under the Curve (AUC) to measure the perfor-mance of RMP . The effectiveness and change in accuracy of RMP are evaluated by comparing against a non-privacy-preserving neural network, in which the raw data records are fed to the AANN for training and testing.

Experiments are conducted on four real datasets from the UCI Machine Learn-ing Repository (all collected from sensor networks except the fourth): (i) Hu-man Activity Recognition using Smartphones (HARS), (ii) Opportunity activity recognition (Opportunity), (iii) Gas Sensor Array Drift (Gas), (iv) Abalone. We also use the Banana synthetic dataset, generated from a mixture of overlap-ping Gaussians to resemble a pair of bananas in any two dimensions. We ran the experiment on the first 1000 records of each dataset. Feature values in each dataset are normalised between [0 , 1] and merged with 5% anomalous records, which are randomly drawn from U (0 , 1). In each experiment a random subset of the dataset is partitioned horizontally among the participants in batches of 30 records and submitted to the server for training.

We deploy a three-layer AANN with the same number of input and output units, i.e., the number of units is set corresponding to the dataset attributes n . The number of hidden units for each dataset is set to about half of the input units (empirically we found that increasing the number of hidden units causes overfitting). All weights and biases in the neural network are initialised randomly in the range of [  X  0 . 1 , 0 . 1]. The learning rate and momentum are set to 0 . 25 and 0 . 85, respectively, and the number of training epochs range from 300 to 1000.
Table 1 compares the results using an AANN anomaly detector on the un-perturbed data records ( X  X aw X ) along with the corresponding results using the privacy preserving scheme of RMP . Accuracy in RMP is affected by its two stages of transformation, i.e., applying the double logistic function and the random transformation, and the level of added noise . As can be seen from the table, when data are perturbed with a marginal level of noise =0 . 01, the accuracy decreases slightly (about 1%). Hence, it shows that the transformations do not exert a significant impact on the accuracy of anomaly detection. In the reported results in Table 1, the dimensionality of the datasets is reduced by r =1,where r = n  X  w . Our empirical experiments show th at the accuracy on datasets with a larger number of attributes n are less affected by an increase of r , e.g., reducing n by 40% only decreases accuracy by about 1%. However, that is not the case for datasets with small n , e.g., the Abalone and Banana datasets, where reducing the data dimensionality by half might result in a 10% reduction in accuracy. The level of added noise to the perturbation matrices has a major influence on RMP accuracy. As increases, so does the loss in a ccuracy, especially in datasets with smaller numbers of attributes. Since the accuracy loss is generally small, RMP is a highly effective approach for privacy-preserving anomaly detection. In a typical participatory sensing scenario, participants send data to a data min-ing server; the server builds a model of the data; and end-users download the model for their own analyses. Collabora tive anomaly detection refers to the case where the model is for anomaly detection. RMP is a privacy-preserving collabo-rative learning scheme that masks the participants X  data using a combination of nonlinear and linear perturbations, while maintaining detection accuracy. RMP protects the private data of participants using individual perturbation matrices, imposes minimal communication and computation overhead on the participants, and scales for an arbitrary number of participants or end-users. We show an-alytically how RMP is resilient to two common types of attacks: Bayesian Es-timation and ICA. Our experiments show that RMP yields comparable results to non-privacy preserving anomaly detection using AANN on a variety of real and synthetic benchmark datasets. As follow-up to this preliminary work, we are in the process of establishing a mathematical framework that relates to accuracy loss and privacy level X  X his will allow us to determine basedonthe intended trade-off between accuracy and p rivacy. We are also investigating the resilience of RMP , in conjunction with other supervised or unsupervised learning algorithms, to attacks exploiting sparse datasets, such as overcomplete ICA. Acknowledgments. NICTA is funded by the Australian Government as rep-resented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Ex-cellence program. Yee Wei Law is partly supported by ARC DP1095452 and the EC under contract CNECT-ICT-609112 (SOCIOTAL).

