 Optical Character Recognition (OCR) has emerged as a useful technology for bridging the gap between paper and paperless work [Fujisawa 2008]. Many professional OCR systems for printed Western text are available, but the work is still in progress for OCR systems for various Indian scripts including Gujarati [Chaudhuri 2007; Govindaraju and Setlur 2009]. Major challenges in the development of such technologies are the diversity of Indian scripts, the cursive nature of writing, and complex character set which consists of base characters, conjuncts, modifiers, and special symbols [Kompalli et al. 2005]. Character classification plays a vital role in the success of any OCR system. Therefore, it is desirable to investigate those features that are effective with cursive script, robust against variations, and that work well with a large-scale classification for various Indian scripts.

Gujarati is an Indo-Aryan language derived from the ancient Devanagari script. It is the official language of the state of Gujarat and the seventh most popular Indian language in terms of the number of native speakers out of 22 official Indian languages [GOI 2003; GOI 2001]. The Gujarati language is written in the Gujarati script, which is a phonetic and syllabic script written from left to right [Mehta and Dholakia 2004]. The Gujarati script character set consists of 10 numerals, 34 consonants, and 12 vowels (as shown in Figure 1(a), (b), and (c), respectively). Each vowel also corresponds to some modifier symbol (shown in Figure 1(c)). Usually, the base character gives the basic sound, and the modifier gives an additional phoneme to change the basic sound slightly. Apart from the basic characters, Gujarati script also has conjuncts or joint characters (shown in Figure 1(d)). The symbols in Gujarati writing are divided into three zones: upper, middle, and lower. The upper and lower zones contain modifier symbols, whereas the middle zone contains base characters and conjuncts (see Figure 2(a)). All symbols in Gujarati script are made up of highly cursive and elongated high-level strokes (see Figure 2(b)) that are in turn made up of multiple low-level strokes (sub-stroke) like endpoints, junction points, lines, and curves. The current proposal describes a template matching-based approach for extracting these low-level strokes from a one pixel-wide thinned, binary character image. Unlike other template matching methods [Kato et al. 1999], which mostly capture line elements of different orientation, the current method effectively captures line elements as well as junction points, endpoints, and curve elements with different orientation and slope using various templates. The method is generalized and can be extended to any script. However, it will be more useful with curvilinear scripts, where the number of endpoints, junction points, and curve elements plays a crucial role in the identification of symbols. The majority of Indian scripts, including Gujarati, have symbols made of cursive and elongated high-level strokes connected with each other at some junction points. Hence, the proposed features are expected to work well for the majority of Indian scripts. Because few efforts have been made in the recognition of characters from Gujarati script compared to other Indian scripts, it is used as a case study in our current experiments. Moreover, no other Gujarati character recognition system deals with low-level stroke features like endpoints, junction points, and line and curve elements. The lack of resources, such as a standardized symbol-level database, is another issue in OCR research for Gujarati text. Through this work, we have generated a moderately size symbol-level database with font, size, and style variations that provides a kickstart to young researchers in this domain. Thus, the current proposal is expected to improve the state-of-the-art in Gujarati script character recognition. The performance of the proposed low-level stroke feature is compared with other contour-based structural features like chain code, directional element, and the histogram of oriented gradients (HoG). The experiment shows that the proposed feature gives better results than chain code and directional elements and gives comparable results with the histogram of oriented gradients.
The rest of the article is organized as follows: Section 2 provides an overview of related work in the field of Indian text character classification and OCR, with an emphasis on Gujarati script. Section 3 discusses preprocessing, low-level stroke feature extraction, feature vector generation, and classification. Section 4 describes databases used, our experimental setup, the results, and comparison with existing work. Finally, the work is concluded in Section 5 with some discussion on future research scope. The majority of Indian scripts can be broadly classified into two categories based on their origin; namely, North and South Indian scripts. The North Indian scripts are derived from an Indo-Aryan origin, whereas the South Indian scripts are derived from a Dravidian origin [Ishtiaq 1999]. Much work is found in the literature for the classifi-cation of characters from both script families. For example, Chaudhuri and Pal [1998], Sinha and Mahabala [1979], Lehal and Singh [2000], and Chaudhuri et al. [2001] discuss work on the development of OCR and character classification for Devanagari, Gurmukhi, Bengali, and Oriya scripts, respectively, whereas the work on major Dravid-ian languages such as Tamil, Telugu, and Kannada is available in Jawahar et al. [2003], Aparna and Ramakrishnan [2002], Lakshmi and Patvardhan [2002], and Manjunath et al. [2006], respectively.

The work found for the classification of characters and OCR for Gujarati script is relatively young compared to many other Indian languages. The first reference can be traced back in 1999 by Antani and Agnihotri [Antani and Agnihotri 1999] who used moment features with minimum Hamming distance and a k-Nearest Neighbor (kNN) classifier. The accuracy obtained was 67% on a small database of 800 samples. After that, no significant work was done until 2006, when S. K. Shah and A. Sharma proposed a complete design of OCR for Gujarati script that uses a fringe distance map as a features-and template-matching classifier [Sharma and Shah 2006]. They claim an accuracy of 78% on a database of approximately 4,500 symbols in 250 different classes. In 2009, Dholakia et al. attempted to use wavelet features with Neural Network and kNN classifiers [Dholakia et al. 2007]. The overall accuracy claimed was 96 X 97% on a database of 4,173 symbols in 119 classes. The authors have also proposed a zone identification method for Gujarati script [Dholakia et al. 2009]. In 2011, Goswami et al. used a Self-Organizing Map (SOM) projection with a kNN classifier and claimed an accuracy of 84% on a small database of around 3,000 symbols in 32 middle-zone character classes [Goswami et al. 2011]. In 2012, Mandar et al. used Locality Preserving Projection (LPP) with a Back Propagation Neural Network (BPNN) for the classification of similar-looking Gujarati characters [Mandar et al. 2012]. The accuracy claimed was 95% on a database containing samples of 10 similar-looking symbols from printed Gujarati text. Finally, in 2014, Hassan et al. used a Multiple Kernel Learning-based Support Vector Machine (MKL-SVM) classifier with multiple features, namely fringe distance map (FM), shape descriptor (SD), and HoG. They claimed a 97 X 98% accuracy on a database of 16,000 symbols from approximately 250 different classes including middle-, upper-, and lower-zone symbols as well as conjunct characters [Hassan et al. 2014]. The majority of the work reported until now uses transform domain, geometrical, or statistical features, and structural features are rarely featured. Figure 3 outlines the proposed work. Like any other classification task, there are three basic steps: preprocessing, feature vector generation, and classification. The following section describes each step in detail. The process begins by taking a binarized, skew-corrected, pre-segmented character symbol image as input and applies various preprocessing operations as follows.  X  Noise Removal : The binarized character images are quite often subject to salt-and-pepper noise. Therefore, a 2D median filtering [Huang et al. 1979] with a 3x3 mask is used to remove any salt-and-pepper noise present in the image. Removal of salt-and-pepper noise is crucial because it will introduce small holes into the object region, which may destroy the original shape of a character stroke while performing thinning in a later step.  X  Resizing : In order to get uniformly sized feature vector, the images are resized to 56x56 size using bicubic interpolation [Keys 1981].  X  Thinning : A parallel thinning algorithm [Zhang and Wang 1996] is applied next to convert the input image into a one pixel-wide thinned image that preserves the original structure of strokes. Thinning is essential because the shape of the stroke can be well described by a one pixel-wide image. Thinning is one of the widely used pre-processing steps in many structural feature extraction methods. However, thinning also introduces additional spurious edges in the structure. The amount of spurious edges generated by a thinning algorithm depends on the font style, thickness of ink, and cursive nature of the script. Thus, based on the empirical evaluation of various thinning algorithms on printed Gujarati text [Suthar et al. 2014], the ZW algorithm [Zhang and Wang 1996] was selected for the thinning task because it generates the minimum number of spurious edges and is robust against boundary deformation. Researchers have proposed many different features for character classification in the course of time [Trier et al. 1996], and these can be broadly classified into three groups: (i) transform domain features (like DCT, DWT, PCA, ICA, etc.), (ii) statistical and geometrical features (like moments and histograms), and (iii) shape-or structure-based features (like contours and regions). Out of these, shape-or structure-based features are the most natural and significant visual features to describe the character symbols in any script. A detailed survey of various shape-based features can be found in Zhang and Lu [2004]. The current proposal focuses on low-level stroke (also called sub-stroke) features, which are part of contour-based structural features. Some of the widely used contour based stroke feature extraction methods are briefly described next. 3.2.1. Chain Code (Directional Code). This is the most common technique for extracting sub-stroke features from a thinned image. In the basic approach, a directional code value at a given object pixel will be determined by the relative position of the current object pixel with respect to the next pixel in the 3x3 neighborhood [Freeman 1974]. Either four (East, North, West, and South) or eight (East, North-East, North, North-West, West, South-West, South, and South-East) directional code values are possible, as shown in Figure 4. A contour tracing algorithm is used to determine the pixel order (current and next pixel). Pixels in thinned binary images are scanned in some predefined order starting from some predefined start point.
 Encoding of any arbitrary curve using chain codes is described in Kaneko and Okudaira [1985], whereas chain code-based features for the handwritten word and writer recognition is described in Govindaraju [1999] and Siddiqi and Vincent [2009], respectively. 3.2.2. Directional Element Features. This is a technique for finding structural line seg-ments, such as horizontal, vertical, left, and right slant, using a 3x3 template pattern mask [Ejima et al. 1985]. Later, the features were improved by adding dot orientation information in Sun et al. [1995]. The features can be computed from both a one pixel-wide thinned image [Ejima et al. 1985] or a boundary image [Kato et al. 1999]. Figure 5 shows 3x3 template patterns used to capture four different line elements. Elements (a) X (d) are assigned a single pattern code, whereas elements (e) X (l) are assigned two pattern codes since these are considered as a combination of two line strokes. The method captures line element only and does not pay attention to endpoints, junction points, and curve elements. 3.2.3. Gradient-based Features. Gradient-based methods work by computing the gradi-ent components in both axes using various derivative operators [Srikantan et al. 1996] such as Sobel, Robert, Krish, and the like. These gradient components are then used to compute gradient directions and strength. HoG is one of the more popular gradient-based methods for object detection in grayscale images [Dalal and Bill 2005]. HoG features are obtained by dividing the image into non-overlapping blocks or cells and computing the HoG in each block. HoG features are more effective when computed on a grayscale image, giving different result when applied to binarized character images. Since the gray level of all object pixel is identical, and change in gray level occurs only at boundary pixels, the gradient information is only captured at boundary pixels, and it is zero for all other pixels in the character image. 3.2.4. Low-Level Stroke Features. This section describes proposed Low-Level Stroke (LLS) feature extraction. LLS features are an extension of directional element [Ejima et al. 1985] features, as discussed in Section 3.2.2, used to accommodate junction points, endpoints, and curve elements.

As discussed in the Section 1, all Gujarati characters consist of cursive and elongated high-level strokes, which are in turn made up of multiple line and curve strokes. Each high-level stroke starts and/or ends with either a junction point or an endpoint (see Fig-ure 6(a)). Therefore, capturing endpoints, junction points, and curve and line elements plays a vital role in improving character recognition for scripts like Gujarati. LLS features are designed to capture these basic elements of geometry (i.e., points, lines, and curves) from one pixel-wide thinned character image using 3x3 template pattern masks. The template-matching approach gives a direct representation of the underly-ing structural pattern without any intermediate values like angle, slope, orientation, or gradient. Also, the only mathematical operation required is a binary comparison; hence this is computationally efficient compared to other mathematical techniques.
Figure 6(b) shows the categorization of different low-level strokes captured by the proposed method. A total of 34 different template patterns are designed to capture 12 different low-level strokes. Each low-level stroke is assigned a unique pattern code. Low-level strokes corresponding template patterns and pattern codes are described as follows.  X  End Points : An endpoint has only two object pixels in the 3x3 neighborhood. There are a total of eight different endpoint orientations. Figure 7 shows 3x3 pattern masks for all different endpoint orientations. However, the current proposal ignores the orientations of endpoints and assigns only a single pattern code to all endpoint patterns.  X  Line Elements : Line elements are the most common structural elements. There are four possible line elements: horizontal, vertical, right slant, and left slant. Figure 8(a) shows 3x3 pattern templates and pattern codes for all different line elements.  X  Curve Elements : Curve elements indicate the transition between various line ele-ments to form a curvature. For example, Figure 9 shows the simple case of how curvatures are formed. The transition from the vertical to a left slant to the horizon-tal makes a left-hand curvature, and from the horizontal line to right slant to the vertical line makes a right-hand curvature. Transition from a horizontal (flat) line to left and right slants, as well as a vertical (deep) line to left and right slant are repre-sented by left flat curve (Figure 10(a)), right flat curve (Figure 10(b)), left deep curve (Figure 10(c)), and right deep curve (Figure 10(d)), respectively. Figure 8(b) shows 3x3 pattern templates and pattern codes for all four curve elements. The categorization is sufficient to capture different orientations and slopes of curve elements.  X  Junction Point Elements : Junction points are the structural elements where more than one high-level stroke meets. The number of object pixels in the 3x3 neighbor-hood of a junction point can be four or more (including the junction point pixel itself), which makes it a difficult structural element to design because it has many possible combinations. Fortunately, in Gujarati script, the junction points are generated us-ing two or three line crossings only. Thus, there are three possible types of junction point: T-Junction (i.e., , , ,  X  ), Y-Junction (i.e., Y , Y , (i.e.,  X  , + ). Figure 11 shows all possible junction point patterns, corresponding tem-plates, and pattern codes.
 The formation of any arbitrary high-level stroke using low-level strokes is shown in Figure 12. The Number of On Pixels (NOP ) in the 3x3 neighborhood of an object pixel (including the object pixel itself) can be used to determine whether a low-level stroke is an endpoint, a junction point, a line, or a curve element. Let NOP I be the NOP pixel in the 3x3 neighborhood of an object pixel I ; then, the following facts are evident:  X  X f NOP I = 1 , then I is an isolated object pixel and should be ignored because it is very likely to be a noisy pixel.  X  X f NOP I = 2 , then I is an endpoint.  X  X f NOP I = 3 , then I is either a line or curve element.  X  X f NOP I = 4 , then I is either a T-junction or Y-junction.  X  X f NOP I = 5 , then I is a cross-junction.  X  X f NOP I &gt; = 6 , then I is an unknown low-level stroke and will be denoted by some special don X  X  care pattern code (  X  1 in this case).

Algorithm 1 gives an iterative procedure for low-level stroke extraction. The algo-rithm takes an MxN thinned, binary character image as an input and goes through every object pixel one by one. It finds the 3x3 neighborhood of each object pixel and matches it with known pattern templates. Finally, the object pixel is replaced with the pattern code corresponding to the observed low-level stroke. The step-by-step proce-dure for matching low-level stroke patterns is given in Algorithm 2. The process begins by computing the NOP I value for each object pixel I in a thinned character image. Depending on the value of NOP I , it tries to match corresponding template patterns and replace the object pixel with low-level stroke pattern code. Due to the limitations of the thinning method, it may happen that the pixel pattern in the 3x3 neighborhood of I does not match any template patterns, especially in the case of junction point elements. In such a scenario, the object pixel will be replaced by a special don X  X -care symbol  X  1. These don X  X -care strokes (  X  1) are rectified during the second scan and replaced by the closest matching low-level stroke pattern template without don X  X -care. Algorithm 3 pro-ceeds by finding the 3x3 neighborhood, N 3 ( d ), for each don X  X -care symbol d and replaces all low-level strokes as well as current don X  X -care symbol d in N 3 ( d ) with 1. All other don X  X  care symbols(  X  1) in N 3 ( d ) are replaced by 0. Thus, N 3 ( d ) gives an approximation of a low-level stroke without considering don X  X -care symbols (see Figure 13). Finally, N 3 ( d ) is matched with template patterns using Algorithm 2 to replace the value of the low-level stroke at d .

In summary, the MxN thinned character image is converted into an MxN matrix of template pattern codes corresponding to the low-level stroke observed at the given pixel location. Figure 14 shows the sample output of Algorithm 1.
 The feature extraction step will convert an MxN binarized, thinned character image into an MxN matrix of pattern codes. Every element ( x , y ) in the matrix gives the value of the pattern code corresponding to the low-level stroke pattern observed at a given object pixel in the character image. Since all the features discussed in the previous section (i.e., directional element, chain code, gradients, and low-level strokes) are cat-egorical features, the direct comparison of values may not be efficient. Therefore, the feature matrix is converted into a numerical feature vector by dividing the feature matrix into non-overlapping blocks and computing the normalized histogram of fea-tures for each block and concatenating them (as shown in Figure 15). The normalized block-wise histogram of features has a smoothing effect that helps in averaging out the effect of small spurious edges in the thinned image. The large spurious edges, however, tend to destroy the actual histogram of features within the block. The division of the feature matrix into blocks allows us to capture features at different levels of general-ization. In the simplest form, we consider the entire matrix as a single block that gives very generalized features and may not be useful. The specific features are obtained by subsequently dividing the feature matrix into 4(2x2), 9(3x3), 16(4x4), 25(5x5), and 36(6x6) blocks. As the number of block increases, the size of the feature vector will also increase, subsequently increasing the computational cost of the classifier. However, a large feature vector will be able to capture small variations in high-level strokes. The size of the feature vector is determined by the number of blocks multiplied by the histogram size, which in turn is given by the number of distinct values of the feature under consideration. Table I shows the number of distinct values of all features used in the experiments as well as the size of the feature vector for different block sizes. Special care is taken while normalizing the endpoint and junction point elements in LLS features because the number of endpoints and junction points is small as compared to line and curve elements. Therefore, dividing endpoints and junction points by the total number of elements in the block will reduce their importance. Also, the number of endpoints and junction points are independent of the size of the high-level strokes. Thus, to normalize the endpoints and junction points, we divide them by the total number of endpoints and junction points in the entire character image, whereas the curve and line elements are normalized by dividing them by the total number of curve and line elements in the given block only. The objective of the experiments is to check the discriminating strength of our proposed features; hence, a k-Nearest Neighbor (kNN) classifier is used to perform classification of printed Gujarati characters. kNN is the most widely used tool for any classification task and has produced good results in many practical problems [Duda et al. 2000]. In this section, we briefly discuss the kNN classifier; the details, however, can be found in Cover and Hart [1967], Duda et al. [2000], and Kevin [2012]. kNN is a non-parametric model used for classification and regression tasks. It is a knowledge-based supervised learning algorithm; hence, it does not require training. It remembers the entire training database as templates, computes the distance of an unknown sample with respect to all samples in the template database, and estimates the class label (or value) of an unknown sample from the class labels (or values) of the most similar or closest samples in the template database, called the nearest neighbors. Thus, the posterior probability of a class label of an unknown sample is obtained from the frequency of class labels of kNN, as given by Equation (1) [Kevin 2012]. where N K ( x , D ) are the K nearest points to an unknown sample x in template database D and I is an indicator function.

The performance of kNN on any database depends on two important design decisions: selection of distance measure and the appropriate value of the neighborhood parameter ( K ). The type of distance measure function depends on the type of features used (i.e., categorical or numerical). As discussed in Section 3.3, the feature vector generated is numerical; therefore, Euclidean distance, which is one of the most common numerical distance measures, is used to compute the distance between feature vectors. The value of K interprets the number of neighbors upon which the estimation of the class label of an unknown sample depends, with typical values of K being 1, 3, 5, or 7. The selection of neighborhood parameter K is crucial and depends on the nature of the database under consideration. If the database is dense and has tighter class boundaries with a minimum overlap between class samples, then K = 1 gives better performance since, for any given unknown sample, it is almost always guaranteed to get some nearest point that best describes the unknown sample (see Figure 16(a)). But if the database is sparse with overlapping class boundaries, then it may not be possible to get any nearest point that best describes the unknown sample (an unknown sample may be an outlier), or there may be more than one points at an equal distance from a given unknown sample (see Figure 16(b) and (c)). In both scenarios, estimation of an unknown sample using a single sample will be erroneous, while K &gt; 1 may give better results. However, a larger value of K indicates comparing an unknown sample with many samples from a template database, and some of them may not be closer to the unknown sample (see Figure 16(d)). Thus, a larger K often adversely affects the performance of kNN. Details on the performance of kNN with respect to K and the derivation of an error bound can be obtained from Duda et al. [2000].

In practice, the nature of the database is rarely known in advance; hence, the only way to obtain an optimum value of K is by empirical evaluation. The experimental setup includes a database consisting of 42 middle-zone symbol classes without modifiers. Nearly 16,782 samples were collected from three different sources: machine-printed books, newspapers, and laser-printed documents. The primary objec-tive of this division is to ensure that enough variation exists in terms of boundary deformation, font type, style, and size in the database. 4.1.1. Machine-Printed Book Symbol Database. The book database (BOOKDB) consists of 7,418 samples collected from 80 pages of 4 different machine-printed books. In machine printing, the character symbols are often subjected to an uneven spread of ink and pressure while printing. Therefore, machine-printed characters are subject to boundary deformations and the presence of holes, cuts, and dark spots due to excessive ink. Figure 17 shows sample images of the symbols from BOOKDB. It can be seen that all the samples in Figure 17 are subject to uneven boundaries; sample (2) contains holes, sample (6) contains additional structure, sample (10) has a minor cut, and sample (3) has a portion of the character missing as compared to sample (8).
 4.1.2. Laser-Printed Document Symbol Database. Unlike machine printing, laser-printed symbols have smooth boundaries, an even spread of ink, and do not contain holes or cuts. But laser-printed documents have a variety of fonts and styles. The laser-printed database (LASERDB) consists of 4,928 samples collected from laser-printed documents written using seven different Gujarati fonts: Saral, Ghanshyam, Gopika, Varun, Harikrishna, Nilkanth, and Shruti . Figure 18 shows the sample images from the laser database. 4.1.3. Newspaper Symbol Database. The newspaper database (NEWSDB) consists of 4,436 samples collected from leading Gujarati newspapers such as Sandesh, Gujarat Samachar, Divya Bhaskar ,and Akila . Similar to machine-printed symbols, newspaper symbols are also subject to holes and boundary deformations. Apart from this, they also have large size variations and ink thickness, as shown in Figure 19. 4.1.4. External Database. The experiments are also performed on the external database (EXTDB) used in Hassan et al. [2014] to compare the results. The database consists of approximately 13,000 symbols from 239 classes. There are 12 different modifier (upper-zone and lower-zone) classes (see Figure 20(b)) and 229 middle-zone classes. The middle-zone symbols are further divided into conjuncts (141) and base character classes as shown in Figure 20(a) and (c), respectively. The database is the largest publicly known database for isolated Gujarati character symbols in terms of the number of classes.
 In all the experiments, a 3-fold cross validation technique is used to make the results more authentic and realistic. The database is divided into three equal, non-overlapping folds, each containing (1 / 3) rd of total samples. Each experiment is repeated three times, using 2 out of 3 folds for training and 1 for testing in a round-robin order (as shown in Table II). The division makes sure that every sample gets a chance to be part of training and testing once, and it also avoids the factor of chance in estimating the results. Average test accuracy over all three runs is used as a primary performance measurement. The experiments were performed to classify the symbols from BOOKDB, LASERDB, NEWSDB, and a combined database (ALLCOMBINED) using a kNN classifier with various features (i.e., DEF, CC, HoG, and LLS). The results of the experiments on BOOKDB, LASERDB, and NEWSDB are shown in Tables III, IV, and V, respec-tively. Figures 21, 22, and 23 shows a performance comparison of various features on BOOKDB, LASERDB, and NEWSDB, respectively. It is evident from the graph that the results obtained by applying LLS features are comparable with HoG and better than DEF and CC in all three experiments. Table VI shows the summary of the best re-sults obtained by DEF, CC, HoG, and LLS features on BOOKDB, LASERDB, NEWSDB as well as the combined database (ALLCOMBINED). The experiments show that all the features give best results on BOOKDB, followed by LASERDB and NEWSDB since the former database has only ink variations, and the boundary deformation, font size, and type remains the same. However, in the case of LASERDB and NEWSDB, due to large size variations as well as font type and style variations, the accuracy drops by 0.28% to 2.9%. The LLS features outperform DEF and CC features on all databases and give an average improvement of 0.45% and 1.07% on DEF and CC, respectively. Also, the performance of LLS is marginally better than HoG over BOOKDB (0.053%) and ALLCOMBINED (0.040%), where the size of BOOKDB and ALLCOMINED are 7,418 and 16,782, respectively. In the case of LASERDB and NEWSDB, HoG outperforms LLS by 0.162% and 0.298%, respectively; however, the size of LASERDB (4,928) and NEWSDB (4,436) is much smaller than the BOOKDB and ALLCOMINED databases.
 The analysis shows that there is a higher possibility of structural noise (i.e., spurious edges or hairs during thinning step) in the sample images with large size and font thickness [Suthar et al. 2014]. Therefore, the samples from NEWSDB will have higher structural noise followed by LASERDB and BOOKDB. Since DE, CC, and LLS features are computed from the thinned binary images, they are more likely to be affected by structural noise than are HoG features that do not use thinned images. Thus, HoG features are expected to perform better on NEWSDB and LASERDB, which can be ver-ified from Table VI. The performance of LLS is very close to HoG and better than CC and DEF features in the presence of structural noise (i.e., LASERDB and NEWSDB).
Binary character images are quite often subjected to salt-and-pepper noise. To see the effect of salt-and-pepper noise on the accuracy obtained using various features, an experiment was performed adding 5%, 10%, and 15% noise to actual sample images. The drop in the accuracy as compared to the best result obtained by each feature on all three databases is recorded in Table VII. Since the best results were obtained using a 5x5 feature vector size and the kNN classifier with K = 1 and a Euclidean distance measure, the same experimental setup is repeated for noisy images as well. It can be seen from the results in Table VII that the average drop in accuracy over all three databases is minimal for HoG (1.760%), followed by LLS (3.464%), CC (9.472%), and DEF (15.46%), which shows that the robustness of the proposed LLS features is slightly lower than HoG but much better than DEF and CC. A comparison of the drop in accuracy for the various features versus the distribution of noise in all three databases is shown in Figure 24.

In summary, the experiments show that the LLS features are robust against font type, size, style, and boundary deformation. However, the performance drops slightly below the state-of-the-art in cases of excessive size and thickness of stroke, where there is a higher possibility of structural noise. Also, LLS can only handle a small amount of skew. Thus, binarized, skew-corrected images are a mandatory condition for better performance (as discussed in Section 3.1). The assumption seems reasonable because there are effective methods available in the literature for correcting skew in printed documents during the OCR preprocessing stage [Hull 1998; Pal and Chaudhuri 2004]. The next experiment was performed on the external database (EXTDB) used in Hassan et al. [2014]. The EXTDB is the largest known database of upper-, middle-, and lower-zone symbols from Gujarati script. The authors in Hassan et al. [2014] performed classification of symbols from EXTDB using a kNN classifier with a Euclidean distance measure and three different features: Shape Descriptor (SD), Fringe Map (FM), and HoG. The HoG feature was discussed in Section 3.2.3. The FM features are computed by finding a distance transform of a binarized character image [Brown 1994]. The SD features are based on shape context features [Belongie et al. 2002]; Belongie et al. has produce the major work in shape-based object recognition. The SDs are defined as the magnitude of the Fourier transform of a normalized shape context log-polar histogram. The results were further improved by taking the various combination of features with an MKL-SVM classifier. Since the objective of the current experiment is to compare the discriminating strength of proposed LLS feature with SD, FM, and HoG features, we only considered the results of individual features with the kNN classifier and omitted other experiments of multiple features with the MKL-SVM classifier as mentioned in Hassan et al. [2014]. However, we do show the highest accuracy obtained with multiple features and MKL-SVM in Table IX. Table VIII shows a comparison of the accuracy obtained by applying the LLS feature (with block size 5x5) with the results reported in Hassan et al. [2014] for SD, FM, and HoG features. In both cases, the kNN classifier with Euclidean distance was used. The optimum value of the neighborhood parameter reported in Hassan et al. [2014] for SD, FM, and HoG is K = 5, whereas the LLS features give optimum results with K = 3 on the same database. It is evident that LLS features perform better than SD and FM features in all the experiments and that they give comparable performance with HoG features. These results are aligned with our observations in the previous experiments. Usually, the classification of conjunct symbols is harder than the base character symbols due to high shape similarity between symbols (as can be seen in Figure 20(a)). However, no experiment was reported in Hassan et al. [2014] to validate this hypothesis. Thus, two separate experiments were performed to check the performance of LLS features using the kNN classifier on conjunct symbols and base character symbols. It can be seen in Table VIII that the accuracy drops by 3.42% in the case of conjunct symbols as compared to base character symbols.

Table IX shows the comparison of proposed methods with different approaches (dis-cussed in Section 2) for the classification of printed Gujarati characters. The major-ity of the works reported so far have used a machine-printed book symbol database only, whereas the current experiment uses a database that consists of symbols from machine-printed books, laser-printed documents, and newspapers. The experiments were also performed on the large machine-printed book symbols database obtained from Hassan et al. [2014]. Thus, two different entries are shown in Table IX for the proposed method to reflect two different experiments. It is evident from Table IX that the results obtained are comparable with the best results reported on reasonably sized databases. This article presents an elegant way of extracting low-level stroke-based features like endpoints, junction points, and line and curve elements from a thinned binary image and demonstrates its strength in the classification of a subset of printed Gujarati symbols. The experimental database consists of samples with wide variations in terms of font style, size type, and ink thickness. The experimental results show that the features are quite robust against variations and give excellent accuracy (96 X 99%). The experiments are also performed to compare results with the best existing work [Hassan et al. 2014] and other contour-based structural features. It is apparent from the results that the proposed features give better results than many existing features like chain code, directional element features, shape descriptors, fringe maps and the like and give comparable results with HoGs. Moreover, the unique thing about the low-level stroke feature is its ability to represent high-level strokes present in the character as a sequence of low-level strokes (as shown in Figure 6(a)).

In future, the work can be extended to handwritten as well as printed symbol recog-nition from Gujarati as well as other Indian scripts since the low-level stroke features are general and can be extended to any script. The low-level stroke features give a direct representation of high-level strokes; therefore, the identification of high-level strokes from the sequence of low-level strokes can also be investigated further. These will allow us to find high-level stroke-based shape similarity between characters and words in cursive Indian scripts. Another interesting further development could be to learn low-level strokes automatically from data instead of handcrafted ones.
