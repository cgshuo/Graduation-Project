 Clustering plays a fundamental role in computer vision, machine learning, pat-tern recognition, and data mining. There are a variety of clustering algorithms available, one classic algorithm of which is k -means that works well for discover-ing spherical or near-spherical clusters in relatively small-scaled databases. Sin-gle, average, and complete linkages are the hierarchical agglomerative algorithms, which take the minimal, average, and maximum distances between points as the inter-cluster similarity measurement, respectively. Shi and Malik [ 20 ]proposed the Normalized Cuts (N-Cuts) algorithm that casts the clustering process as a graph partition problem; they partitioned the graph into N parts by eigenvec-tors of a graph-Laplacian matrix, providing N clusters to be identified. The non-parametric clustering method Affinity Propagation (AP) [ 6 ] updates the affin-ity between points via a messaging process. Algorithms based on graph theories have the appealing property that a graph can portray the inherent structures of given data. Inspired by PageRank [ 15 ], Minsu and Kyoung defined graph authority scores [ 3 ], and used authority node traversals and authority propagation to iden-tify clusters. Then authors of [ 4 ] re-defined the graph representation to strengthen relevancy between neighbors and weaken that among non-neighbors; and its per-formance is eventually improved by applying the authority-ascent kernel to this kind of graph representation. In spite of the aforementioned algorithms, there still remain two major difficulties in handling data with complex structures:  X  X ata noise sensitivity X  and  X  X luster center variance X . Data noise sensitivity affects algorithms such as k -means, Linkage, N-Cuts, AP, and Authority-Ascent Shift (AAS) [ 3 ], making them fail to correctly separate clusters from noise (Figs. 4 (c), (d), (e), (f) and (g)). Cluster center variance requires multiple execution of k -means to obtain precise and stable results due to its dependence on initial partitions. Rodriguez and Laio [ 18 ] proposed a simple-and-effective clustering algorithm: Density Peaks (DP), which used simple ideas instead of complicated mathemat-ics to tackle the aforementioned problems. They defined the concepts of local density and the minimum distance to the higher density neighbor, and illustrated the relationship between these two concepts in the form of decision graph .The decision graph provides a visualization of candidates of cluster centers, shedding light on the research of clustering from noise. However, the weak algorithmic assumptions make their approach constantly result in incorrect clusters. They built their algorithm on the belief that cluster structures and centers are charac-terized by the distances between points. In fact, measuring similarity according to distance can be easily affected by data structure and distribution, which fails to identify centers from the decision graph. Moreover, the criteria for deciding the cluster border can also be easily affected by the pre-defined cutoff distance (or the distances between clusters), jeopardizing the overall robustness. In this paper, we propose a novel robust clustering approach to effectively address the challenges of data noise sensitivity and cluster center variance. It has been found that points are aggregated to form homophilic layers if they are re-organized by their high-order in-degrees and corresponding sorted out-degrees, and noisy points generate the weakest layer due to low densities, making it easy to separate clusters from noise [ 24 ]. Comparing to the state-of-the-art methods, we highlight the main advantages of our approach as follows:  X  established upon degree homophily [ 24 ], we use a robust cluster-to-noise esti-mator to separate clusters from noise, leading to a potentially superior clus-tering quality;  X  with extracted clusters, our method takes advantage of graph kernel to provide an accurate global similarity measurement between points as opposed to [ 18 ];  X  the minimum distance to higher density neighbor is re-defined with global similarity, thus providing a much better visualization of center candidates in decision graph.
 With all cluster centers identified from the decision graph, the remaining points are categorized to the same clusters as their most similar neighbors with higher densities. The rest of this paper is organized as follows. In Sect. 2 , we review and evaluate the DP algorithm in detail. Sections 3 and 4 present our proposed way of integrating degree homophily and graph kernel , respectively. A comprehensive evaluation on both synthetic and real-world databases is conducted in Sect. 5 ; conclusions are drawn in Sect. 6 .
 The assumption of the DP algorithm is that local densities of cluster centers are higher than those of their neighbors, and cluster centers are far apart from each other [ 18 ]. The point i embodies two variables,  X  i its structural attributes.  X  i is the local density of that point, which is defined as follows: where  X  ( x )=1if x &lt; 0; otherwise  X  ( x )=0[ 18 ]. D is the distance matrix, and d c is a cutoff distance [ 18 ].  X  i denotes the distance between point i and its nearest neighbor with higher local density (Eq. ( 2 ) Left) [ 18 ]; for the point which has the highest local density,  X  i is assigned with the maximum in distance matrix D (Eq. ( 2 ) Right) [ 18 ]: The variables  X  i and  X  i serve the purpose of finding cluster centers and border regions. 2.1 Clustering Process According to Rodriguez and Laio X  X  assumptions, cluster centers are more dis-tinctive with larger  X  and  X  . As described in Eq. ( 2 ),  X  and this relationship is shown in a decision graph (Fig. 1 (e)), which visualizes the candidates of cluster centers. Colored points with relatively larger  X  and  X  are recognized as cluster centers, while points near axes are either with lower densities or close with neighbors with higher densities. The remaining points are categorized into the clusters as their nearest neighbors with higher densities [ 18 ]. We formally define r as the vector containing the nearest neighbors with higher density, r i as nearest neighbor with higher densities than i , points, b i as the label of point i , C center as indexes of centers, and n ber of clusters. We summarize the specific steps for obtaining the decision graph in Algorithm 1 , and the procedure of cluster-label assignment in Algorithm 2 . The border region [ 18 ], or cluster halo (a.k.a. outliers ), is the set of points which are categorized to one cluster but close to other clusters within a cutoff distance d c [ 18 ]. Points whose densities are higher than the maximum density (denoted by  X  b ) of border region are considered as members of clusters, while others are treated as noise. We summarize the procedure of labeling the halo in Algorithm 3 , where we further denote B as the cluster border region and as the labels of halo which is labeled as zero. The clustering result is shown in Fig. 1 (a).
 2.2 Analysis of DP Algorithm The appealing novelties of DP are the definition of  X  and the decision graph which provides us a useful tool to visualize candidates of cluster centers. The computation of  X  depends on D and local density. Therefore,  X  will be affected by distances between points, shapes or distributions of clusters, and the local densities. Figure 1 (b) shows that DP fails to recognize five cluster centers if the distance between centers of clusters 1 and 2 is shortened; from its correspond-ing decision graph (Fig. 1 (f)), we observe that there are only four points with relatively higher  X  and  X  ; and the center of cluster 1 becomes indistinguishable in decision graph as its  X  i decreases. Besides, in the case which is demonstrated in Fig. 1 (d), another center candidate shows up in decision graph if there exist two local density maxima in cluster 3. These examples indicate that it is diffi-cult to identify cluster centers without sufficiently large  X  we perceive that for point i , the assignment for its nearest neighbor with higher density depends on its  X  i . As shown in Fig. 4 (b), even if centers are identified correctly from decision graphs, clusters in strip-shaped and circle-shaped are divided into several parts, meaning that points are assigned to wrong neighbors. Therefore,  X  i is important not only in locating cluster centers but also in clus-tering process. Furthermore, the estimator  X  b for cluster border region is easily affected by d c and distances between clusters. If we manually locate five cluster centers in Fig. 1 (g), clusters 1 and 2 shrink severely (Figs. 1 (b) X (c) because many points are categorized to border region and recognized as noise.
 rithm based on graph techniques to eliminate data noise sensitivity and cluster center variance. Although homophily is a social concept which depicts the individual behavior preference, it has been adopted by computer science to solve many problems, such as modeling evolving networks [ 16 ]. In [ 24 ], homophilic degree is used as a type of centralized measure of points. Meanwhile, a robust estimator is proposed to eliminate noise based on degree homophily [ 24 ]. Therefore, we make use of degree homophily to separate clusters from noise so as to guarantee a superior clustering quality. 3.1 Homophilic In-Degree Figure The basis of degree homophily is a directed asymmetric graph. If we construct a directed graph G by creating links from points to their k nearest neighbors (denoted by N ), then G is also asymmetric owing to the asymmetric neighbor-hood of points: point i is possibly not contained in N j even if point j is one of k nearest neighbors of point i . The weights of links can be measured by the distance exponential e  X  D 2 ij / X  2 (parameterized by  X  ), or other similarity mea-surements. Thus, each element W ij in similarity matrix W unless: Each point in a directed graph G has two attributes describing its connections with others: in-degree and out-degree; we denote the t -order in-degrees and out-degrees with d in = W t 1 and d out =( W t ) 1 (the scripts  X  t  X  X n have been left out), respectively. Here comes the most interesting part of degree homophily . Points can be re-organized according to the following steps:  X  define a coordinate whose horizontal and vertical axes are indexes and degrees of points, respectively;  X  X ort d out in descending order and plot the sorted d out  X  X oreach d out i , draw its corresponding d in i as a point in the same coordinate. Figs. 2 (a), (b), (c) and (d) present the re-organized results. As we can see, there is nothing particular when t = 1. However, with the increasing t , in-degrees generate transparent homophilic layers gradually, which is explained as degree homophily in [ 24 ]. This visualization of Homophilic In-degree is referred to as  X  X I figure X  [ 24 ]. For the selection of t , it is recommended to choose a t that makes in-degree layers maximally uniform. For more details about degree homophily , please refer to [ 24 ]. 3.2 Noise Elimination In Figs. 2 (a), (b) and (c), points are decorated with three colors: red, yellow and black. Particularly, red points are the cores of clusters and their in-degrees are higher than out-degrees. We can see from Fig. 2 (e) that red and yellow points can roughly represent clusters but still contain many outliers when t = 1. With the growth of t , clusters represented by colorized points gradually become clean and clear (Figs. 2 (f)-(g)). There are two types of noise. The first type of noise generates connections between clusters and noise. This noise always lie at the bottom of the HI figure due to low in-degree links to these noisy points. Another one is uniformly distributed among points. Connections formed by these kind of noisy points are so weak that the homophilic layer formed by them will be the first one to fall below the out-degree curve with the growth of t . They will finally lie on the bottom together with the first type of noise for a moderate t . Although noise will not disappear by increasing t , we can separate clusters from them easily with local densities of points.
 neighbors [ 14 , 22 ]. Rodriguez and Laio defined the local density using the number of neighbors (Eq. ( 1 )) [ 18 ], which is similar to the ideas of using k nearest neighbors in density estimation. We estimate the local density  X   X  the average of similarities [ 24 ], i.e. where  X   X  i is decoupled from d c , and hence becomes more amendable to control. Next, we look for the minimum local density in the set of largest cores (red points) of clusters (denoted by C max ) whose in-degrees are higher than out-degrees: Then the set of clusters can be obtained by extracting points whose densities are larger than  X   X  min C where X is the set of points. Here, yellow points are identified and merged into C cluster . Thus, the remaining points are categorized to the set of noise naturally. The clusters separated from noise shown in Fig. 2 (h) demonstrate that  X   X  clusters from noise is presented in Algorithm 4 . Though DP uses  X  to  X   X  distance d c and sometimes affected by distances between clusters (Figs. 1 (b)-(c)). Algorithm 4. Detect Clusters from Noise with Homophily (init.: initialize) After separating clusters from noise by degree homophily , we can divide sub-graphs G cluster and G noise , whose corresponding similarity matrices are W W cluster and obtain the global similarity matrix to re-compute  X  for each point. 4.1 Similarity Propagation by Diffusion Kernel In a graph, the similarity between points can be estimated by the linkage along the paths connecting them, which is regarded as long-range relationship. It has been verified that the long-range relationships can be used to enhance the affini-ties in network communities [ 11 , 13 ]. The graph kernel technique is useful in cap-turing the long-range relationships between points induced by the local graph structure [ 12 ]. Exploiting the fact that W t ij (we have left out the script of  X  X lus-ter X  in W t ij )of W cluster in t -order represents the semantic similarity along the paths of length t , we can take advantage of such properties of diffusion kernel. Here, we use the von Neumann diffusion kernel : where  X &lt; 1 / X  ,and  X  is the spectral radius of W cluster parameter that controls the long-range relationships. The von Neumann diffu-sion kernel can also be written in the form of infinite series of matrix powers. Therefore, K ij presents the semantic similarity, which is calculated by summat-ing the similarities of all possible paths in different lengths from point i to j . In this way, points in the cluster become more similar and pairs in the same cluster with long-range relationships can be drawn together from the viewpoint of structural similarity.
 4.2 Center Location and Cluster Aggregation One of the appealing novelties in [ 18 ] is the definition of  X  , which essentially measures the similarity between point i and its nearest neighbor with higher density directly by distance. However, as mentioned in Sect. 2.2 ,  X  enlarge the gap between centers and the remaining points in the decision graph. By the virtue of graph kernel , we re-define  X  i to make it more plausible and robust. For point i , we compute a quantity  X  s i that is the maximum similarity between itself and other points with higher densities: where S i is the set of points that have relation with point i . After noise elimi-nation by degree homophily , there are no connections between clusters, and is a block matrix, meaning that K ij or K ji is non-zero only when points i and j are connected directly or indirectly. Note that N i  X  X  i also associated with indirectly connected points (via long-range relationships) accordingtoEq.( 7 ). Although K is asymmetric as well, our concern is the sim-ilarities between pairs of points instead of the direction. Hence, the asymmetry of K ij and K ji is simultaneously taken care of in computing  X  s 0 for the point with the highest density. Here, we compute of  X  s i added with a small number to avoid diving by zeroes: where  X   X  i is regarded as a measure of structural length of the path from point i to its most similar neighbor with higher density in graph trate at the upper right corner, which shows that centers are well characterized by relatively higher  X   X  i and  X   X  i , making it easy to identify all centers unambigu-ously. There are larger margins between center candidates and the remaining points in decision graphs, clearly validating the superiorities of homophily . Clustering is straightforward based on the recognized centers: the remaining points are categorized into the same clusters as their most similar neighbors with higher densities, which resembles the Algorithm 2 .
 where n is the number of points; and n s is the number of noisy points. After noise elimination, the number of points to handle is reduced to ( n i in G cluster is generally related to points of the same cluster in contain the entire points in that cluster in the extreme case. For t -order degrees, the time complexity is O ( tn ), where t is much lower than n in practice. The label assignment stage can be completed in linear time O ( n ). We evaluate our approach in relation to several state-of-the-art methods on two 17 , 19 ], which represent different clustering problems with various complexities. 5.1 Synthetic Data We first apply our approach to the same synthetic database (Figs. 1 (a), (b) and (d)) to compare decision graphs with DP algorithm. In Fig. 3 (b), although two clusters are close to each other, their centers can be conveniently identified from the decision graph in Fig. 3 (h). With degree homophily , clusters and noise can be separated completely, thereby producing better decision graphs. Meanwhile, Fig. 3 (b) demonstrates again that  X   X  min C noise in comparison with  X  b and d c for DP algorithm (Fig. 1 (c)). For the cluster that contains two local density maxima, our algorithm can handle it as well, as demonstrated in Figs. 3 (c) and (i).
 terns for our experiments. Different from [ 3 ], 20 % noise is added to test the robustness of algorithms. Besides DP, we also compare our algorithm with other mainstream algorithms such as k -means, Linkage, N-Cuts, AP, and AAS; and our approach is hereafter referred to as  X  X K X  (Homophily-degree + graph-Kernel). Observed from the first row in Fig. 4 , we find that methods working on the measure of distance directly (i.e. k -means, Linkage, AP, and DP) fail to cap-ture the line-shaped structure, thus resulting in incorrect results. However, the graph-based algorithms (i.e. AAS, N-Cuts, and HK) produce much better results. Particularly, the result of HK turns out to be the best of all in clustering and noise elimination. A similar conclusion can also be drawn from the results on the second and third rows. We plot the decision graphs of HK and DP algorithms in Fig. 5 , illustrating the evident superiority of HK. 5.2 Real-World Problems We also carry out performance evaluation of clustering on six real-world data-bases. The FRGC (Face Recognition Grand Challenge) database [ 17 ] contains 466 persons/clusters embodying 16,028 facial images; the number of members ranges from 2 to 80 in each cluster. The Yale face database [ 7 ] contains 165 gray-scale images belonging to 15 individuals. The ORL database [ 19 ] contains 400 face images of 40 subjects. The MNIST [ 8 ] and USPS [ 2 , 10 ] databases are collected for handwritten-digit recognition; MNIST includes 10,000 handwritten digits of 10 classes; USPS involves 8-bit gray-scale images from  X 0 X  to  X 9 X  (each class has 1,100 samples). Besides, we also test on the Isolet [ 2 , 5 ], which is a spoken letter recognition database. We empirically find that the parameters of AP are difficult to control, which forces us leaving it out and merely performing the comparison with other algo-rithms. The performance of algorithms is evaluated by Normalized Mutual Infor-mation (NMI) [ 21 ]. As we can see from Table 1 , on relatively simple databases (e.g. FRGC), all algorithms except Single Linkage manage to perform well; and HK obtains the best result of all. However, when performed on Yale, MNIST, and USPS databases that are more challenging, HK algorithm is significantly superior than DP. Generally, points in high-dimensional spaces or on curved manifolds are far from each other, hence  X  of centers are generally not big enough to gen-erate large margins on decision graphs for DP. On the opposite, our algorithm based on graph kernel shows satisfying robustness quality on these databases, and yields the best results across all six databases.
 To address the difficulties of  X  X ata noise sensitivity X  and  X  X luster center vari-ance X  in mainstream clustering algorithms, we propose a novel approach that can identify cluster centers unambiguously from data contaminated with noise. The degree homophily is applied to eliminate outliers, which effectively sim-plifies the clustering process greatly. Built upon the concepts of graph kernel , our approach can capture long-range relationships in data and discover clusters of manifold-structured shapes. The comprehensive experimental evaluations on synthetic and real-world databases verify the effectiveness and robustness of our algorithm comparing to several mainstream state-of-the-art methods.

