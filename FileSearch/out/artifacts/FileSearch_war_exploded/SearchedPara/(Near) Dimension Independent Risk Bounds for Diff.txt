 Microsoft Research Stanford University and Microsoft Research Recently, there have been growing concerns regarding po-tential privacy violation of individual users X /customers X  data by modern systems that employ learning and statistical analysis methods. Motivated by such concerns, several re-cent works have proposed and analyzed privacy preserving learning algorithms (Chaudhuri et al., 2011; Pathak et al., 2010; Kifer et al., 2012; Jain et al., 2012; Duchi et al., 2012; Smith &amp; Thakurta, 2013). All these works use differential privacy (Dwork et al., 2006b) as the notion to define privacy of each individual training data point. Furthermore, they show that not only their methods are differentially private, but they also have bounded excess risk or bounded regret that improves with larger number of training instances. Most of these existing methods use the standard technique of adding noise to either the learned model or some inter-mediate construct of the algorithm (Chaudhuri et al., 2011; Kifer et al., 2012; Jain et al., 2012). Subsequently, they provide excess risk/generalization error bounds that scale as a polynomial of the dimensionality ( p ) of the input data. One of the achievement of machine learning has been that for several learning problems that can be modeled using generalized linear model , the excess generalization risk can be shown to be independent of the dimensionality of input data points ( p ), provided that the input data points and the output model are constrained to have bounded L 2 -norm. Similarly, for several other classes of problems, the bounds are known to grow only logarithmically with p (Shalev-Shwartz et al., 2009; Negahban et al., 2009).
 Hence, a long-standing open problem in the domain of differential privacy has been: Can computationally effi-cient differentially private learning algorithms be designed which have excess risk/generalization error that is either independent or only logarithmically dependent on the di-mensionality of the problem? Recently, Kifer et al. (2012); Smith &amp; Thakurta (2013) obtained risk bounds for the sparse regression problem that scale logarithmically with p . However, their method needs additional restrictive as-sumptions like restricted strong convexity (RSC). In this paper, we provide the first dimension independent risk bounds for generalized linear model based learning methods. In particular, we show that while the  X  X istance X  (such as Euclidean distance) between the differentially pri-vate learned model and the optimal non-private learned model can depend polynomially on p , the excess risk can still be independent of the dimensionality of the input data, as long as the feature vectors and the underlying parame-ter vector are bounded in the L 2 -norm. Furthermore, we show that if the feature vectors are bounded in the L  X  -norm and the underlying parameter vector is bounded in the L 1 -norm, we can get risk bounds which only depend logarithmically on the dimensions.
 We propose to use Gaussian noise based perturbation with the output and objective perturbation algorithms by Chaud-huri et al. (2011). While the privacy of such perturba-tion was analyzed by Chaudhuri et al. (2011); Kifer et al. (2012), their excess risk analysis for empirical risk mini-mization (ERM) in the generalized linear models (GLM) were loose and led to polynomial dependence on p . We show that their analysis can be tightened to remove explicit dependence on p and have dependence only on the L 2 norm of the input data points and the output parameter vector. Our results hold for all L 2 regularized unconstrained ERMs with 1 -Lipschitz smooth convex function. We would like to stress that this class of ERM is a popular class and includes important problems such as logistic regression. Further-more, for analysis of output perturbation based algorithms we don X  X  need smoothness assumption as well and hence can provide excess risk bounds for private-SVM as well. We then study a class of problems where the parameter vec-tor is bounded to be on a scaled simplex. Recently, such problems have gained a lot of importance as they provide excess risk bounds that scale only logarithmically in di-mensions and linearly with the L 1 -norm of parameter vec-tor, hence are well-suited for high-dimensional learning. For these problems, we propose a novel privacy preserving algorithm that draws multiple samples from a distribution defined by the non-private optimal parameter vector and outputs their average. We show that the excess risk for this algorithm scale as O (log p/n 1 / 3 ) , where n is the number of training examples. In comparison, existing approaches (Chaudhuri et al., 2011; Kifer et al., 2012) incurs a poly ( p ) dependence on the error. We would like to stress that our algorithm fundamentally deviates from existing approaches for private ERM as the existing techniques require O ( p ) randomized operations to output the parameter vector while we perform only sub-linear (in n ) number of randomized operations to give the output parameter vector. Hence, our method requires significantly lesser randomness for p  X  n . Furthermore, most of the existing differential private algo-rithms either add explicit perturbation (Chaudhuri et al., 2011; Kifer et al., 2012) or uses a well-known exponen-tial mechanism (McSherry &amp; Talwar, 2007). In contrast, our algorithm uses a novel sampling approach which might in itself be of interest for designing novel differentially pri-vate algorithms. Our algorithm also ensures that the output is in fact a sparse vector, hence it not only provides privacy but also enables efficient computation.
 As a direct application of our private algorithm over the simplex, we provide a privacy preserving variant of the Follow the Regularized Leader (FTRL) algorithm com-monly used in online learning (Hazan et al., 2007; Shalev-Shwartz, 2011). We show that if the cost functions (in the online learning setting) are linear and the optimization is performed over the simplex, then our proposed algorithm achieves the optimal regret bound of O ( T refers to the time horizon of the online learning algo-rithm and p -refers to the dimensionality of the problem. A similar result was obtained by Dwork et al. (2010b) that guarantees only for a weaker model of privacy where the adversary cannot distinguish between the presence or absence of one coordinate in the linear cost function .
 Finally, we provide empirical evaluation of our proposed methods and compare them against the objective/output perturbation methods of Chaudhuri et al. (2011) over benchmark data sets. We show that the methods of Chaud-huri et al. (2011) indeed incur test error that grows with p , while our method is able to obtain accurate predic-tions even for high-dimensional data sets. Similarly, we also evaluate our proposed sampling based method for pri-vacy preserving learning over simplex by simulations over a benchmark data set.
 Contributions : 1. We show that by sampling the perturbation from a Gaus-sian distribution, instead of Gamma distribution as pro-posed by Chaudhuri et al. (2011), we can obtain dimen-sion independent excess risk for the well-known output and objective perturbation algorithms (Chaudhuri et al., 2011) when applied to the maximum-margin based problems. 2. We provide a sampling based differentially private algo-rithm for solving a large class of ERMs over scaled simplex and show that the obtained risk bound scales logarithmi-cally in p . However, our excess generalization error rate has a worse dependence of 1 /n 1 / 3 on the size of the data set ( n ), as compared to the optimal rate of 1 / 3. We provide a differentially private version of the Fol-low The Regularized Leader algorithm for online learning, whose regret scales as O ( the simplex. Our regret bound matches the non-private re-gret bound (under similar setting) up to factors depending only on differential privacy parameters  X  and log(1 / X  ) . 4. We provide empirical evaluation of our methods on benchmark data sets. Our evaluation clearly shows that the proposed techniques not only provides significantly tighter error bounds but also provide significantly more accurate predictions on benchmark data sets. The problem of differentially private ERM has been stud-ied extensively in the literature. Starting with (Chaudhuri &amp; Monteleoni, 2008; Chaudhuri et al., 2011; Rubinstein et al., 2009), there has been extensive work both in the low-dimensional setting as well as the high-dimensional setting . In the low-dimensional setting, where the dimensionality of the problem ( p ) is smaller than the size of the training data set ( n ), most of the existing methods provide an ex-cess error bound that has a polynomial dependence on p (Chaudhuri &amp; Monteleoni, 2008; Chaudhuri et al., 2011; Rubinstein et al., 2009; Kifer et al., 2012; Jain et al., 2012). In the high-dimensional setting , where the dimensionality of the problem exceeds the size of the data set, the existing methods provide logarithmic dependence on p , but require strong statistical assumptions like Restricted Strong Con-vexity (Kifer et al., 2012; Smith &amp; Thakurta, 2013). An ex-ception to the above is the work by Jain &amp; Thakurta (2013) that provides a differentially private method for learning with kernels, however their methods assume a stronger model where a few samples from the unlabeled test data set are also available.
 In contrast, we show that for generalized linear models with bounded feature space (either in the L 2 -norm or L  X  norm), we can use differentially private regularized ERM that also guarantees excess risk bound which is indepen-dent of p or depends logarithmically on the dimensions ( p ). Chaudhuri &amp; Hsu (2011) provided a differentially private classifier where the excess generalization error is depen-dent only the doubling dimension of the hypothesis space. They also provided a matching lower bound. However, their method is defined only for 0  X  1 loss function and in general can take exponential time in the number of train-ing points. We also note that the doubling dimension of regularized linear learning models can be shown to be ei-ther independent or logarithmically dependent on p (Zhang, 2002). Hence our results do not contradict the lower-bound result of Chaudhuri &amp; Hsu (2011). Risk Minimization and Excess Generalization Error: Given a data domain X , an unknown but fixed distri-bution Dist over X , a fixed convex set C  X  R p , and a risk (loss) function  X  : C  X  X  X  R , the objective is solve the following stochastic minimization problem : arg min excess generalization error (or risk) is defined as an upper bound on: To minimize the excess risk, we use the standard regular-ized Empirical Risk Minimization (ERM) method: where, the training data ( D ) is drawn i.i.d. from the dis-tribution Dist . Also, r : C  X  R is a twice differentiable convex regularizer.
 Now the goal of this work is to design differentially private ERM with small excess risk bound. We focus on the gener-alized linear models (GLMs), where each data point d is of the form ( x , y ) with x  X  R p and y  X  R , and the loss func-regression, linear regression and support vector machines are some of the classic examples of GLM.
 Differential Privacy: Differential privacy (Dwork et al., 2006b;a) ensures that the amount of information an adver-sary can obtain about an individual from the output of an algorithm A running on the data set D is roughly the same irrespective of that individual X  X  presence or absence in the data set D . Formally, Definition 1 (Differential privacy (Dwork et al., 2006b;a)) . A randomized algorithm A is (  X ,  X  ) -differentially private if for any two data sets D and D  X  of size n drawn from the domain X n with d H ( D , D  X  ) = 1 ( d H being the hamming distance), and for all (Borel) sets O  X  Range ( A ) the fol-lowing holds: Pr[ A ( D )  X  O ]  X  e  X  Pr[ A ( D  X  )  X  O ] +  X . Choice of  X ,  X  privacy parameters: Smaller values of  X  and  X  imply stronger privacy guarantees. Typically  X  is set to be a small constant (say 0 . 1 ) and  X  should be o (1 /n 2 where n is the number of records in the data set. See (Ka-siviswanathan &amp; Smith, 2008) (Lemma 3.3) for a rigorous justification of the above choices of these parameters. Ka-siviswanathan &amp; Smith (2008) also show that the seman-tic notion privacy (see Definition 2.3 in (Kasiviswanathan &amp; Smith, 2008)) is invariant to the size of the hypothesis space and hence, to provide a fixed level of privacy to a data point,  X  is not required to depend on p .
 A common approach for designing a differentially private algorithm is via the global sensitivity framework defined below. In a lot of the algorithms discussed in this paper, this forms the basic building block. Let X n be a domain of data sets (with n data points) and let f : X n  X  R p be a function to be evaluated on a data set D  X  X n . Global sensitivity of the function f is defined as in (2). Here the operator d H refers to the hamming distance and kk q refers to the L q -norm for a specific q .
 Let b  X  R p be a random variable sampled from the distri-2006b) showed that for a given data set D , an algorithm that outputs f ( D ) + b is  X  -differentially private . Assumptions and notation: Throughout this paper we will assume that the loss function  X  : R  X  R  X  R is in the generalized linear model, L -Lipschitz continuous in its first parameter, and twice-continuously differentiable. The feature vectors are bounded in the L q -norm with the bound being denoted with R q . We will set q = 2 or q =  X  based on the context of the problem. For a fixed distribution Dist over the data domain X , we denote  X   X  = arg min ameter of the convex set C in L q -norm as kCk q (when the diameter is finite). For the domain of data points X , R q denotes an L q bound on the norm of any x  X  R p in X . In this section, we present two privacy preserving ERM algorithms with dimension independent excess generaliza-tion error bounds for L 2 -norm bounded data points and L norm bounded parameter space. We also assume that the ization parameter) and the convex set C equals R p . One way to interpret the assumption on the convex set is in the improper learning setting. Although the true risk min-imizer lies in a bounded convex set, the algorithm is al-lowed to produce a hypothesis from R p . Settings as above are common in several machine learning formulations, es-pecially ones that try to find maximum margin learner such as SVM, L 2 regularized logistic regression etc. Chaudhuri et al. (2011) proposed two popular algorithms for the problem of privacy preserving ERM, namely out-put perturbation and objective perturbation . Roughly, the output perturbation method perturbs the true minimizer of the ERM (1),  X   X  , to preserve privacy. While the objec-tive perturbation method perturbs the objective function J (  X  ) = 1 n In the following, we provide tighter utility analyses for both output and objective perturbation for GLMs. 4.1. Output Perturbation The output perturbation method (Chaudhuri et al., 2011) first computes the minimizer b  X  of (1) and then adds noise scaled according to the global sensitivity of b  X  . A formal description of the algorithm is given in (3).
 Here b  X  ( D ) maps a data set D to the corresponding min-imizer b  X  of (1), and b  X  R p is a random vector whose L -norm ( v ) is distributed according to the Gamma distri-Chaudhuri et al. (2011) showed that if the bound on the double derivative of  X  is c 2 (w.r.t. its first parameter), the excess generalization error scales Thakurta (2013) showed that instead of adding Gamma is added, then the generalization error improves to proved dependence on the dimensionality compared to the bound by Chaudhuri et al. (2011). However, the privacy guarantee is weaker, i.e., the algorithm now satisfies (  X ,  X  ) -differential privacy compared to  X  -differential privacy guar-antee provided by Chaudhuri et al. (2011).
 In our work we improve the earlier analysis and show that with the same Gaussian noise, one can get generalization error guarantees that are independent of any explicit de-pendence on p . Our result also has improved dependence on parameters L , R 2 and k  X   X  k 2 . We would like to note that our results hold only for the generalized linear model (GLM) and when the regularization function is the squared L 2 norm.
 Theorem 1. Let D = { ( x 1 , y 1 ) , , ( x n , y n ) } be i.i.d. samples drawn from a fixed distribution Dist over the do-main X . Also, let  X   X  = arg min randomness of the training data set D and the randomness of the noise vector b , the following is true: where  X  priv is the output of the output perturbation method Proof Idea: The main idea in our proof is that since the learning model is a GLM, the prediction for a feature vector x  X  R p with a parameter vector  X  depends only on h x ,  X  i . Now for the two parameter vectors b  X  (from (1)) and  X  priv (the output of the output perturbation algorithm), and any data point ( x , y ) , the difference in the loss is bounded by the following. Since we are interested in bounding the excess generaliza-tion error, we only need to bound the right hand side in expectation over b (and then use Markov X  X  inequality). Re-call that our noise vector b is a symmetric Gaussian ran-dom vector. So, we have E the standard deviation of b . Notice that a naive Cauchy-Schwarz argument would result in a bound of The fact that we can bound the expectation with a quantity that does not have any explicit dependence on the dimen-sionality allows us to get our desired result in Theorem 1. See Appendix A for a detailed proof.
 4.2. Objective Perturbation In this section we discuss the objective perturbation algo-rithm that perturbs the objective function in (1) by a random linear term to guarantee differential privacy. That is,
Objective Perturbation:  X  priv = where  X  is a parameter to the algorithm and b is a zero-mean perturbation. This algorithm was first proposed by Chaudhuri et al. (2011) and was subsequently improved by Kifer et al. (2012). Chaudhuri et al. (2011) showed that the algorithm is  X  -differentially private when the noise vector b is drawn from the Gamma distribution with kernel e  X   X  k b k and  X  = 2 c 2 R 2 2  X  , where |  X   X  X  ( h  X  , x i ; y ) |  X  c X ,  X   X   X  R p . (The double derivative is w.r.t. the first pa-rameter of  X  .) While, when the noise vector b is drawn from the algorithm is (  X ,  X  ) -differentially private. In terms of excess risk bounds, Chaudhuri et al. (2011) showed that under suitable choice of parameter  X  , the gen-eralization error of the private algorithm with Gamma noise scales as: Later, Kifer et al. (2012) improved the excess risk bound to: where the perturbation vector b is sampled from the Gaus-sian distribution mentioned above and where  X  parameter is selected appropriately.
 We now present our dimension independent excess risk bound of the objective perturbation algorithm with Gaus-sian noise. The dimension independent analysis for objec-tive perturbation is significantly trickier than output per-turbation, since unlike output perturbation the exact distri-bution of  X  priv is hard to evaluate. Our analysis uses a novel reduction of the analysis of objective perturbation al-gorithm to the analysis of output perturbation as given in Section 4.1.
 Theorem 2. Let D = { ( x 1 , y 1 ) , , ( x n , y n ) } be i.i.d. samples drawn from a fixed distribution Dist over the do-main X . Also, let  X   X  = arg min If the regularization coefficient  X  = LR 2 the following holds with probability at least 7 / 10 over the randomness of the training data set D and the randomness of the noise vector b : where  X  priv is the output of the objective perturba-tion method (4) where b  X  N 0 ,  X  2 I p and  X  2 = Note that the generalization error for objective perturba-tion is almost identical to that of output perturbation except an extra poly log n factor and an extra factor of LR 2 . We conjecture that extra poly log n factor is an artifact of our analysis and leave further tightening of our bound as topic of future research.
 Proof of Theorem 2. Let, J Note that, Recall that  X  priv = arg min arg min  X  itively think that  X   X  priv is obtained after executing the output perturbation algorithm with the objective function J priv We now artificially increase the dimensionality of the prob-lem from p to p +1 . For every feature vector x  X  R p , define x coordinate. For the vector  X   X  priv , define  X   X   X  priv the vector  X   X  priv appended with one in the last coordinate. Using the definition of  X   X  priv , we have,  X  Now, by using the observation of the previous section,  X   X  gence theorem of Shalev-Shwartz et al. (2009) (re-stated in Theorem 5 of Appendix A) , the following holds with prob-ability  X  9 / 10 over the randomness of the sampling of the data set D :
E where R 2  X  = equation and the observation that  X   X   X  priv = [  X  priv + we get:
E Now, by using Lipschitz property of  X  , we get:
E x ,y [  X  ( h  X  priv , x i ; y )  X   X  ( h  X   X  , x i ; y )]  X  Since, the noise vector b is a vector of i.i.d. Gaus-sian random variables with standard deviation  X  , by the tail probability of Gaussian random vectors we can conclude that with probability at least 9 / 10 , R 2  X  = O R 2 probability at least 9 / 10 over the randomness of b , E Combining the above observations with (7), we get (w.p.  X  9 / 10 ):
E x ,y [  X  ( h  X  priv , x i ; y )  X   X  ( h  X   X  , x i ; y )]  X  =
O (1)( LR 2 ) 2 Theorem now follows by setting  X  according to the theorem statement and by selecting  X  = LR 2 In this section we present a differentially private ERM al-gorithm for L  X  -bounded data points and parameter vec-tors restricted the simplex, with excess generalization error bound that scales logarithmically in the dimensionality. As a price for improved dependence on the dimensionality, our excess generalization error has worse dependence on the data set size ( n ). Our error scales as 1 /n 1 / 3 , compared to the non-private optimum of 1 / As a corollary of our result, we derive improved regret bound for differentially private online learning with lin-ear costs considered in (Dwork et al., 2010b). Our regret guarantee depends logarithmically on the dimensionality, compared to the polynomial dependence of (Dwork et al., 2010b). However, our privacy guarantees are weaker, i.e., we guarantee (  X ,  X  ) -differential privacy as compared to  X  -differential privacy of (Dwork et al., 2010b). 5.1. Private ERM with Entropy Regularization Consider the following ERM problem over simplex  X  = {  X   X  R p : b  X  = arg min where ( x i , y i )  X  Dist,  X  i . If we choose  X  = O ( then using standard Rademacher or covering number argu-ments one can show that the excess generalization error of b  X  is bounded by O (Kakade et al., 2008; Shalev-Shwartz et al., 2009). If we use either output or objective perturbation algorithm from Section 4 to obtain a differentially private variant of the above, then the excess generalization error will scale as O (  X  non-private case.
 In this section, we present a novel differentially private algorithm for solving ERM over simplex such that the excess risk scales as O (log n log pR  X  /n 1 / 3 ) . Our al-gorithm heavily exploits the fact that the optimization is over a simplex, and involves sampling non-uniformly from probability vectors from the simplex. In our method we first computes the non-private ERM solution  X   X  us-ing (8). Now, we treat b  X  as a discrete probability dis-tribution over { 1 , 2 , . . . , p } and sample m i.i.d. points [ a 1 a 2 . . . a m ] , a j  X  [ p ] , from  X  priv is given by: where e a j denotes the a j -th canonical basis vector and m is a parameter that we specify later in the theorems for privacy and excess risk.
 In the following, we provide formal privacy guarantee (Theorem 3) and excess generalization risk bounds (The-orem 4) for  X  priv (9). See Appendix B for proofs. Theorem 3 (Privacy guarantee) . Let  X  : R  X  R  X  R be a differentiable smooth function. Let L g be the Lipschitz constant of  X   X  X  gradient and let L be its Lipschitz constant. Then:  X   X  priv is  X  -differentially private when m =  X   X  priv is (  X ,  X  ) -differentially private when m = Theorem 4 (Utility guarantee) . Let D = { ( x 1 , y 1 ) , , ( x n , y n ) } be i.i.d. samples drawn from a fixed distribution Dist over the domain X . Also, let  X  = Then, the following holds with probability at least 2 / 3 over the randomness of the training data set D and the randomness of the algorithm: where  X   X  = arg min obtained by (9) with m as given above.
 Similarly, with a proper choice of  X  and m , we can obtain an  X  -differentially private algorithm whose excess risk scale as O ( poly (log n log p ) / ( 5.2. Private Online Learning over Simplex In this section, we study the problem of private online learning over the simplex. The main goal of this section is to demonstrate that the techniques developed in the pre-vious section can be related to existing methods for private online learning. Furthermore, it leads to improved regret bound for differentially private online learning over sim-plices as compared to the existing works of Dwork et al. (2010a); Jain et al. (2012). In particular, similar to the of-fline learning setting, existing privacy preserving methods incur additional O ( In contrast, our proposed method is able to bring down the multiplicative factor to linear costs over the simplex.
 We assume that the cost functions  X  t (  X  ) provided at each step are linear, i.e.,  X  t (  X  ) = h x t ,  X  i . Also, we assume a  X  X eak X  adaptive adversary which cannot see the prediction at the t -th step beforehand. Now, we first consider the pop-ular Follow-the-regularized-leader (FTRL) algorithm with entropy regularization for this problem (Shalev-Shwartz, 2011). Using FTRL, the t -th step parameter vector is given by: b where  X  -th step loss function is given by:  X   X  (  X  ) = h x Notice that the optimization problem in (10) at every step t , is equivalent to (8). Hence, we can use the same method as given in the previous section for computing the privacy preserving update  X  priv t +1 . In particular, we select m = 1 and sample one index a from the probability distribution b  X  t +1 Hence, where a  X  { 1 , 2 , . . . , p } is sampled from the discrete dis-tribution b  X  t +1 .
 In online learning the objective is to bound the regret , given by the following:
Regret ( T ) = E From the standard online learning literature (Corollary 2.14 from (Shalev-Shwartz, 2011)), it follows that (11) is up-per bounded by  X  log p + T R 2  X   X  . And from the privacy analysis of Theorem 3, it follows that the above algo-rithm is (  X ,  X  ) -differentially private (over all the T -steps) as long as  X   X  R  X  differentially private FTRL algorithm which has the fol-lowing regret bound in (12).

Remark: The regret bound of our private algorithm matches the non-private optimal regret bound up to factors of log(1 / X  ) / X  . Our result directly improves on the result of Dwork et al. (2010b) (adapted to our privacy model), where the regret implied by the analysis of (Dwork et al., 2010b) is O we provide a weaker (  X ,  X  ) -differential privacy guarantee as opposed to  X  -differential privacy guarantee of Dwork et al. (2010b). By adapted to our privacy model, we mean the following: Dwork et al. (2010b) guarantees the indistin-guishability of the presence or absence of one coordinate of a single linear cost function, where as in this paper we ensure the indistinguishability of one complete linear cost function. In fact in the privacy model considered in (Dwork et al., 2010b), they managed to get a regret guarantee that scales as O ( In this section, we first validate our theoretical anal-ysis of the normal distribution based output and objective-perturbation methods (denoted as Output-Gauss , Objective-Gauss respectively) for L 2 regularized ERM. Next, we show that our sampling based method (denoted as Sampling ) for entropy regularized ERM also achieves similar accuracy to the non-private ERM. For our first set of experiments, we apply SVM based clas-sifiers on two benchmark datasets: URL and Cod-RNA. We use a subset of the URL dataset which has 100 , 000 data points and its dimensionality is around 20 M. Cod-RNA has around 60 K data points and its dimensionality is 8 . We use 70% of the data for training and the remaining 30% for test. All the results presented are averaged over 20 runs and our code uses a modification of the LIBLINEAR method for solving the perturbed SVM problem.
 We evaluate the standard Output-perturbation (denoted as Output ) and Objective-perturbation (denoted as Objec-tive ) methods by (Chaudhuri et al., 2011) against Output-Gauss , Objective-Gauss methods. We first show that as shown by our theoretical analysis, the test error of Output-Gauss and Objective-Gauss is indeed independent of the training data X  X  dimensionality. Moreover, the test er-ror of the gamma-distribution based perturbation used by (Chaudhuri et al., 2011) indeed increases with the dimen-sionality.
 We first apply all the four methods mentioned above to the URL and the Cod-RNA dataset. We set the regulariza-tion parameter  X  = 0 . 001 and  X  = 10  X  3 . Figures 1 (a), (b) shows accuracy achieved by different algorithms on the URL, Cod-RNA dataset, with varying privacy parameter  X  . Clearly, Objective perturbation methods are significantly better than the Output perturbation based methods. More-over, normal distribution based perturbation is able to ob-tain significantly higher accuracy.
 We now study how the accuracy of various methods vary with the dimensionality of the dataset (Figure 1 (c)). To this end, we use Cod-RNA dataset and artificially blot up its di-mensionality by adding zeros to the feature space vectors. Note that this does not effect the baseline classifier and its accuracy. Clearly, as predicted by our analysis, accuracy of the Objective-Gauss and the Output-Gauss does not change with large increments in the ambient dimensionality p . In contrast, accuracy of both Objective and Output perturba-tion algorithms suffer heavily for larger p .
 Above experiments suggest that our theoretical analysis of objective perturbation (Theorem 2) may be loose. One open problem is to investigate the tightness of the current analysis.
 Finally, we study our Sampling method for entropy reg-ularized ERM. To this end, we first solve a entropy regu-larized least squares problem. We then threshold predicted values to obtain class values. We conduct experiments on Cod-RNA dataset with  X  = 10 ,  X  = 10  X  3 and by using 70% of the data for training and the remaining for test. Fig-ure 1 (d) shows accuracy achieved by various methods with artificially bloated dimensionality of the data. Clearly, ac-curacy achieved by our Sampling method is similar to the non-private classifier and is significantly better than both Objective and Objective-Gauss for larger values of p . Our dimension independence analysis for objective pertur-bation holds only if the optimization is over the uncon-strained space R p (see Theorem 1). In fact, it can be easily shown that both output and objective perturbation approach will fail to provide dimension independent risk bound if the solution of the ERM is contrained to lie in an arbitrary con-vex set C  X  R p . For example, say optimal solution  X  is constrained to lie in the positive orthant and let the optimal solution to the ERM is 0 . In this case, it is easy to see that both objective and output perturbation methods will give excess risk that scales as Another limitation of our analysis is that we need to as-sume that the regularization function of the ERM is either squared L 2 norm or the negative entropy function. Tight excess risk analysis (in terms of dependence on p ) for other regularization functions (for example, L 1 norm) is still an open problem and is left as a topic of future research. Finally, our algorithm for privacy preserving entropy regu-larized ERM uses a sampling based technique that is signif-icantly different from the existing differential privacy learn-ing techniques. For future research, we want to explore this technique in more detail and possibly, apply the technique to other similar problems.
 Chaudhuri, Kamalika and Hsu, Daniel. Sample complexity bounds for differentially private learning. Journal of Ma-chine Learning Research -Proceedings Track , 19:155 X  186, 2011.
 Chaudhuri, Kamalika and Monteleoni, Claire. Privacy-preserving logistic regression. In NIPS , 2008.
 Chaudhuri, Kamalika, Monteleoni, Claire, and Sarwate,
Anand D. Differentially private empirical risk minimiza-tion. JMLR , 12:1069 X 1109, 2011.
 Duchi, John C., Jordan, Michael I., and Wainwright, Mar-tin J. Privacy aware learning. In NIPS , 2012.
 Dwork, Cynthia, Kenthapadi, Krishnaram, McSherry,
Frank, Mironov, Ilya, and Naor, Moni. Our data, our-selves: Privacy via distributed noise generation. In EU-ROCRYPT , 2006a.
 Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private data analysis. In TCC , 2006b.
 Dwork, Cynthia, Naor, Moni, Pitassi, Toniann, and Roth-blum, Guy N. Differential privacy under continual ob-servation. In STOC , 2010a.
 Dwork, Cynthia, Naor, Moni, Pitassi, Toniann, and Roth-blum, Guy N. Differential privacy under continual ob-servation. In Proceedings of the 42nd ACM symposium on Theory of computing , 2010b.
 Dwork, Cynthia, Rothblum, Guy N, and Vadhan, Salil. Boosting and differential privacy. In FOCS , 2010c. Hazan, Elad, Agarwal, Amit, and Kale, Satyen. Loga-rithmic regret algorithms for online convex optimization. Machine Learning , 2007.
 Jain, Prateek and Thakurta, Abhradeep. Differentially pri-vate kernel learning. In ICML , 2013.
 Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep. Differentially private online learning. In COLT , 2012. Kakade, Sham M, Sridharan, Karthik, and Tewari, Ambuj.
On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In NIPS , 2008. Kasiviswanathan, Shiva Prasad and Smith, Adam. A note on differential privacy: Defining resistance to arbitrary side information. CoRR , arXiv:0803.39461 [cs.CR], 2008.
 Kifer, Daniel, Smith, Adam, and Thakurta, Abhradeep.
Private convex empirical risk minimization and high-dimensional regression. In COLT , 2012.
 McSherry, Frank and Talwar, Kunal. Mechanism design via differential privacy. In FOCS , 2007.
 Negahban, Sahand, Ravikumar, Pradeep, Wainwright,
Martin J., and Yu, Bin. A unified framework for high-dimensional analysis of $m$-estimators with decompos-able regularizers. In NIPS , 2009.
 Pathak, Manas A., Rane, Shantanu, and Raj, Bhiksha.
Multiparty differential privacy via aggregation of locally trained classifiers. In NIPS , 2010.
 Rubinstein, Benjamin IP, Bartlett, Peter L, Huang, Ling, and Taft, Nina. Learning in a large function space:
Privacy-preserving mechanisms for svm learning. arXiv preprint arXiv:0911.5708 , 2009.
 Shalev-Shwartz, Shai. Online learning and online con-vex optimization. Foundations and Trends R in Machine Learning , 2011.
 Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and Sridharan, Karthik. Stochastic Convex Optimization. In COLT , 2009.
 Smith, Adam and Thakurta, Abhradeep. Differentially pri-vate feature selection via stability arguments, and the ro-bustness of the lasso. In COLT , 2013.
 Thakurta, Abhradeep Guha. Differentially private convex optimization for empirical risk minimization and high-dimensional regression. PhD Dissertation , 2013. Zhang, Tong. Covering number bounds of certain regular-ized linear function classes. Journal of Machine Learn-
