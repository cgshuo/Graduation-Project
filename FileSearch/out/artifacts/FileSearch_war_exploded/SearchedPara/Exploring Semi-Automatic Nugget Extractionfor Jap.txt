 Building test collections based on nuggets is useful evaluating sys-tems that return documents, answers, or summaries. However, nugget construction requires a lot of manual work and is not feasible for large query sets. Towards an ef fi cient and scalable nugget-based evaluation, we study the appli cability of semi-a utomatic nugget extraction in the context of the ongoing NTCIR One Click Ac-cess (1CLICK) task. We compare manually-extracted and semi-automatically-extracted Japanese nuggets to demonstrate the cov-erage and ef fi ciency of the semi-automatic nugget extraction. Our fi ndings suggest that the manual nugget extraction can be replaced with a direct adaptation of the English semi-automatic nugget ex-traction system, especially for queries for which the user desires broad answers from free-form text.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval evaluation; information units; NTCIR; nuggets; summaries; test collections
For over half a century, information retrieval research has fo-cussed on document retrieval. However, in many search tasks, what the user wants is information rather than a list of documents. Ac-cordingly, the task of returning relevant information in response to a query, and methods for evaluating such tasks based on nuggets have received attention recently (e.g. [2]). Building test collections based on nuggets is useful not only for evaluating systems that re-turn direct answers or summaries (e.g. [7, 9]), but also for handling novelty and redundancy in document retrieval [1] and for ef fi cient relevance assessments [8].

The ongoing NTCIR One Click Access (1CLICK) task is an ex-ample of an information retrieval task, which can be described as: Given a search query, provide a short summary that fi ts a mobile phone screen. Put relevant pieces of information fi rstsoastomin-marization and question answering evaluation, the task was novel in that the positions of nuggets found in the system output were lever-aged to evaluate systems [9]. In the evaluation of the fi rst Japanese 1CLICK task at the NTCIR-9 conference, two types of manual ef-fort were required: (a) extracting nuggets from relevant documents; and (b) identifying the matches (and their exact positions) between the system output and the list of gold-standard nuggets. While the manual efforts enable highly reliable and robust evaluation, mea-sures for (semi-)automating some of the processes are essential for enhancing evaluation ef fi ciency and scalability. While Task (b) has been tackled to some extent in the summarization and question an-swering communities using automatic text segmentation techniques such as N-grams [5, 6], the present study addresses the question of whether Task (a), i.e., extracting nuggets, can be done semi-
Recently, Rajput et al. [8] proposed a framework for conducting document relevance judging and nugget judging simultaneously. The framework is based on an online mutual reinforcement algo-rithm designed to dramatically reduce the assessor effort. That is, the judged nuggets are used for selecting new documents to be judged; the judged documents are used for selecting new nuggets to be judged. However, their work considered English information access only, and it is an open question whether their approach trans-fers well to other languages. In particular, the Japanese language is radically different from European languages: there are no spaces between words; several different character sets (both ideograms and phonograms) are used together; and the grammar generally al-lows more fl exible word ordering within a sentence than those of European languages. Therefore, in this study, we address the ques-tion of whether the mutual reinforcement framework of Rajput et al. can be extended for the purpose of semi-automatic extraction of nuggets for the Japanese 1CLICK task. We are currently running the second 1CLICK (1CLICK-2) task at NTCIR-10, which com-prises English and Japanese subtasks. In this paper, we leverage the data constructed at 1CLICK-2 to pursue this research question. 1 1CLICK homepage: http://research.microsoft. com/en-us/projects/1click/ 1256172. Speci fi cally, we examine which types of queries are suited to the system as a direct adaptation from English versus fully manual ex-traction and conclude by hypothesizing changes to improve general applicability.
For the NTCIR 1CLICK-2 task, the organizers provided a set of queries and the baseline search results, which consisted of top-ranked Yahoo! API search results returned in response to each query, and expected participants to generate system output based on the provided search results for each query. Having received partic-ipants X  system outputs, the organizers evaluated them based on nuggets prepared for each query in advance. In the following sub-sections, we explain the queries and nuggets used for the NTCIR 1CLICK-2 task.
The NTCIR 1CLICK-2 test collection includes 100 Japanese and 100 English queries. Based on a study on mobile query logs [4], eight query types were considered: ARTIST, ACTOR, POLITI-CIAN, ATHLETE, FACILITY, GEO, DEFINITION, and QA. For each query type, it was assumed that the user has the following information needs: ARTIST, ACTOR, POLITICIAN, ATHLETE (10 each) FACILITY (15) user wants access and contact information for a GEO (15) user wants access and contact information for entities DEFINITION (15) user wants to look up a phrase, etc.; QA (15) user wants to know factual (but not necessarily factoid) The number of queries for each type is shown in parentheses.
To allow for cross-language comparison, we created 15 queries which overlapped between Japanese and English. Table 1 shows the overlap query set, which comprises one query from ARTIST and ACTOR, two queries from POLITICIAN and ATHLETE, and three queries from FACILITY, DEFINITION, and QA. Note that there is no overlap GEO query as it was dif fi cult to fi nd GEO queries that are used both in English and Japanese. The Japanese version of those 15 overlap queries were used in our evaluation.
A nugget at 1CLICK-2 is de fi ned as a sentence relevant to the information need for a query, and was used to evaluate the qual-ity of system output by identifying which nuggets are present in its content. At NTCIR 1CLICK-2, native Japanese speakers in the or-ganizer team identi fi ed relevant documents from the provided base-line search results, and manually extracted relevant sentences as nuggets. For example, a sentence  X  X chiro Suzuki (born October 22, 1973) is a professional baseball player X  was extracted as a nugget for query  X  X chiro suzuki. X  In total, 3,927 nuggets were extracted for 100 Japanese queries (39.2 nuggets per query on average).
Section 3 describes how the nugget extraction can be semi-automated, and Section 4 then discusses the performance of the semi-automatic nugget extraction.

As described in [8], the nuggets system uses a mutual, iterative reinforcement feedback system between documents and nuggets automatically extracted from the documents. An assessor judges the relevance of documents and the system updates beliefs about the relevance of unjudged documents and nuggets. This procedure is illustrated in Figure 1.
 Figure 1: The iterative assessment procedure: documents are selected and assessed, nuggets are extracted and [re]weighted. This procedure has been modi fi ed to both allow for processing of Japanese text and to allow for judgments on nuggets in addition to documents. As the nugget system is highly modular, the English-speci fi c text processing tasks can be directly substituted with those of another language. Speci fi cally, the two instances in which text is automatically processed are when nuggets are extracted from doc-uments and when text is matched between nuggets and documents. For nugget extraction, we maintain sentences as the text unit. For Japanese, we use a regular expression to match sentence endings, as these patterns are more well de fi ned than in English. Documents are segmented into sentences and all sentences from relevant docu-ments are used as nuggets in the learning procedure.

For matching, we maintain the shingle matching system previ-ously described. Brie fl y, for each shingle, a sequence of secutive words, in a piece of text and minimum span S of words in a document which contains all shingle words in any order, we calculate the shingle score as where  X  is a fi xed decay parameter,  X  =0 . 95 in our case. How-ever, this requires some knowledge of Japanese morphemes to seg-ment text into word units. Raw text is split into these word units
Figure 2: Screenshot of the Nugget Extractor for Japanese. using a Japanese morphological analyzer, MeCab 3 . Only nouns, verbs, adjectives, and adverbs were used in the shingle matching, while the other part-of-speech, such as particles and conjunctions (or function words), were excluded. From here, the shingle match-ing system proceeds as before, in which we accumulate all shingles for a nugget-document pair ( n, d ) :
The adaptability of th e Hedge algorithm[3], used by the nugget X  X  iterative update procedure, leads to a nugget score being increased as long as it matches relevant documents (positive feedback) and reduced when the nugget matches non-relevant documents (nega-tive feedback). For a nugget n and new document d : This is highly bene fi cial for fi nding new and diverse relevant doc-uments, but sometimes problematic for fi nding the global set of relevant information.

In the document judgment interface shown in Figure 2, we dis-play nuggets sorted by their quality score q n to give the user an idea of what information is deemed relevant at this stage of the nugget procedure. We allow the user to mark nuggets as relevant or nonrel-evant at any point in the process in order to explicitly overwrite any implicit feedback from the algorithm and to select what portion of information is relevant to that user. This explicit information can be incorporated into the feedback system, allowing for more targeted document evaluation. This is done by automatically assigning the maximum score given to any nugget to all judged relevant nuggets, and the minimum score to all judged nonrelevant nuggets: where R and NR are the set of judged relevant and nonrelevant nuggets respectively. Explicit nuggets judgments have a strong in-fl uence on the update procedure, but they do not entirely dominate, as compared to assigning a fi xed score or one outside the ranges automatically given. http://mecab.googlecode.com/svn/trunk/ mecab/doc/index.html
Additionally, as shown in our previous work, we can evaluate global relevance by examining t he total ability of a nugget to bring in relevant vs nonrelevant documents during the iterative proce-dure. This provides evidence as to the general ability of the chosen nuggets and our matching system to automatically sort information.
Finally, we have modi fi ed the system to allow a user to inject manually-created nuggets into the system. These nuggets are au-tomatically matched to all documents, and function just as auto-matically extracted nuggets in the update procedure. This allows primarily for the injection of seed information to improve the di-versity of the results, for instance if an aspect of a query is under-represented. This can be thought of as a form of query expansion, but requires no modi fi cation to the nuggets procedure.
We manually extract nuggets for the Japanese version of the 15 overlap queries listed in Table 1, and independently semi-automatically extract nuggets for the same query set as described in Section 3. It can be dif fi cult to judge which nuggets contain the same informa-tion when they were obtained using different methods as nuggets can include multiple pieces of information and the granularity of each nugget highly depends on the document from which the nugget was extracted.

Thus, nuggets were manually broken down into smaller units, known as information units (or iUnits ). An iUnit is a factual state-ment which is relevant (satis fi es the information need behind the query partially or wholly) and atomic (cannot be broken down into multiple iUnits without the compone nts losing meaning). For ex-ample, a nugget  X  X chiro Suzuki (born October 22, 1973) is a pro-fessional baseball player X  was broken down into two iUnits: (1)  X  X orn October 22, 1973, X  and (2)  X  X  professional baseball player. X  In fact, for the NTCIR-10 Japanese 1CLICK-2 subtask, iUnits were utilized instead of nuggets for evalu ating system output due to their maximal granularity.

Once the nuggets from both the manual and semi-automatic sys-tem are broken down into iUnits, they can be directly compared to assess the commonalities and differences between the two sets. We compare the two sets for all queries, grouped into four meta-categories based on the type of query and desired answers. FAC-ITLITY queries look for exact facts about location and contact in-formation, and CELEBRITY queries (ACTOR, ATHELETE, MU-SICIAN, and POLITICIAN) look for speci fi c facts about a celebrity, while DEFINITION and QA queries allow for broader answers. Figure 3: Distinct and overlapping iUnits extracted manually vs semi-automatically for the different query types.
Figure 3 shows the proportion of overlap of manual vs semi-automatic iUnits for the different queries. Manual iUnits are clearly superior for the fact-based queries, while semi-automatic iUnits perform better for the broader queries.

Two primary assumptions of our current system are that nuggets are sentences and that partial sentence matches imply partial in-formation overlap. These assumptions were made speci fi cally for broader informational-based queries, but can plainly cause prob-lems when searching for speci fi c types of answers. A human, for instance, is good at picking out dates and other numbers from the text, but under our fi rst assumption, small facts are not easily repre-sented by nuggets in the form of an entire sentence. The automated system, however, is designed to branch out based on similar infor-mation and similar contexts, so it fi nds a much larger space of in-formation. For instance, with CELEBRITY query  X  X chiro suzuki X , some example iUnits are  X  X o. 31 X  and  X  X eight: 180.3cm, X  which do not have correct partial matches and are not commonly found as sentences, whereas with DEFINITION query  X  X eothermal energy X , iUnits such as  X  X he fi rst geothermal energy plant was developed in Italy X  and  X  X onsidered to be sustainable energy X  match exactly the target form.

These assumptions could be modi fi ed based on query catego-rization to search for and extract alternate candidate nuggets and to perform text matching using differing unit sizes based on the type of nugget, both of which are planned areas of future study. For now, however, these results help classify times when manual extraction is necessary versus when the semi-automatic method is bene fi cial.
In order to evaluate the extraction ef fi ciency of the system, we examine the number of iUnits found over time as document judg-ments occurred. Figure 4 shows this information averaged over the four meta-categories. There is a clear trend of fi nding a great deal of relevant information early in the process, with an expected di-minishing return as more documents are judged. This is especially true for de fi nition and facility queries, where the system is either es-pecially good at fi nding information, or there is a small amount of primary relevant information. Using these fi ndings, we can assess useful heuristics for stopping conditions durin g the judging process based on how much information is believed to be left or to allow the user to decide if continuing is likely to be worth the effort. iUnit Recall (Category Avg) Figure 4: iUnit set growth VS effort on document assessing, averaged over queries for each category.

Finally, as with English queries in our previous work [8], we can examine the ability of the system to accurately infer the quality of a particular nugget. We use the same scoring method based on the ability of a nugget to correctly rank the judged documents by its Figure 5: MAP value of the inferred nugget score vs the judged relevance for each category. match to these documents. The MAP value of the inferred nugget scores vs the judged relevance is produced for each of the meta-categories and shown in Figure 5. However, in this task, the aver-age number of documents judged was only 39, so a lower overall score is expected, especially for queries categories with few rel-evant documents. The score remains useful enough to provide a great reduction in the number of nuggets requiring manual assess-ment in order to fi nd the relevant information.
We have introduced an adaptation of our nugget extractor sys-tem for Japanese, demonstrated its ability to extract information on Japanese, and examined the cases under which it performs better and worse than manual text extraction. We have seen that, as de-signed, the nugget system is effective at fi nding larger quantities of information written as free-form text.

A primary area for future work is examining the effects of using varying sizes of nuggets and the ability of both oracle and auto-mated query classi fi ers to choose the desired types of nuggets. Ad-ditionally, we plan to examine th e utility of thes e nuggets in directly creating summaries. We believe that the redundancy and overlap of information will help a summarizer with both fl ow and importance of information, two important facets of abstractive summarization.
