 The email services are becoming more and more important in our modern life. Nowa-days, a typical user receives about 40-50 email messages every day[5]. For some working time on email processing. With the rapid growth of World Wide Web[10], the popularity of email communication is growing; and the time spent on reading and replying emails is increasing. Despite their usefulness, however, email systems nowa-days suffer one major problem: information overloading. In other words, people re-ceive a lot of junk or irrelevant emails such as advertisements everyday. The number get rid of them to save space. Generally, the main tool for email management to solve this problem is text classification [10]. However, emails are always short documents with noise information which could decrease the performance of text classification text data is highly desired. (PCA)[6] and Linear Discriminant Analysis (LDA) are traditional linear approaches which could clean the noise features of data [8, 14]. Their goal is to obtain compact representations of the original data that are essential for higher-level analysis while eliminating unimportant or noisy factors. On the other hand, nonlinear dimension reduction algorithms such as Local Linear Embedding (LLE)[9, 11] and Isomap [2] which linear algorithms do not work well [16]. However, little work has focused on the data cleaning problem by nonlinear dimension reduction approaches for text well for very high dimension text data. In contrast to the traditional text data or web pages, email data are always lower dimensional vectors in Vector Space Model (VSM)[1]. Thus the effective nonlinear dimension reduction algorithms are possible to be applied on the email data. rithm called as Supervised Semi-definite Embedding (SSDE) to clean the noise fea-tures of email data. Through this supervised learning procedure, the new arrived email data could be classified into different classe s. Then the user could ignore the emails in the irrelevant class. Furthermore, the experimental results on synthetic data showed algorithm with data structure preserved. This property could help the user to select the most important emails and ignore the irrelevant emails intuitively. tion can be formulated as discovering a low-dimensional embedding of high-dimensional data assumed to lie on a nonlinear manifold. It is highly desirable that this embedding preserved local geometry of the original data, i.e., close points in the high-dimensional space must remain close in the embedded space. Recently, a new nonlin-ear dimensionality reduction technique based on semi-definite programming, namely Semi-definite Embedding (SDE)[17, 18], is proposed for unsupervised nonlinear di-mensionality reduction of image manifold. SDE is based fundamentally on the notion of isometry . Like Isomap and LLE, it relies on efficient and tractable optimization that is not plagued by spurious local minima. Isomap estimates geodesic distances between local angles and distances. Comparing the algorithms, the theoretical and experimental results of SDE showed that it overcomes certain limitation of previous works interest-ingly. However, the original SDE algorithm is unsupervised and was originally in-tended for multidimensional image data visualization. The unsupervised algorithms ignore the valuable label information used in classification problems. Our contribution in this paper is a supervised variant of SDE for email data. To learn a mapping from a high-dimensional space into low-dimensional space, a nonlinear regression analysis [7, 12] is used, i.e. mapping new arrival email data points into the embedding space, where the classification of these points is done by the K Nearest Neighbor (KNN) classifier. Our experimental results on various benchmark data sets form the UCI Ma-LLE, and SDE. Besides this, there is an interesting observation: SSDE could visualize the information of the different classes of data in the reduced space. necessary background knowledge of dimension reduction. In Section 3, we present our proposed Supervised SDE algorithm mathematically. In Section 4, we demon-strate the experimental results on some benchmark data sets and the real world email data. Conclusion of this paper is given in Section 5. reduction (manifold learning) algorithm to clean the noise features of email data and help to visualize these data for users. Our algorithm originated from an unsupervised manifold learning algorithm called as Semi-definite Programming. For better com-prehension, the mathematical definitions of manifold learning problems are given firstly. Following that, some necessary basi c ideas of unsupervised SDE are shown in this section. 2.1 Dimension Reduction for Data Cleaning The world is made of a huge amount of complex data. Discovering the hidden dimen-reduction problem. The dimension reduction problem could be mathematically de-features. Here D R denotes the space of all the D dimensional vectors. The problem is with the input X , that provides a faithful embedding in dD &lt; dimensions. Here XR  X   X  is a matrix with each line a data point. However, many real data sets such as web documents and short emails contain essen-tial nonlinear manifold that are invisible to PCA and LDA [9]. In the last few years, researchers have uncovered a large family of algorithms for computing such nonlinear problem, named as manifold learning. tics, geometry, and computation. Given high-dimension data sampled from a low-dimension manifold and a prescription for identifying  X  X eighboring X  inputs, the prob-lem is how to efficiently compute a nonlinear embedding such that the outputs pre-serve local geometry of the original data. 2.2 Semi-definite Embedding To propose our Supervised SDE algorithm for email data, in this section, we would like to introduce the basic idea of SDE algorithm for manifold learning firstly: isome-try . Then we present the detail steps of the SDE algorithm in the following subsection. 2.2.1 Isometry As a novel approach of manifold learning , SDE preserve more local information of the original data than the other nonlinear dimension reduction approaches. SDE util-izes the definition of isometry while mapping a high-dimensional data into a low dimensional space. As the theory of Riemanni an manifolds, two manifolds are said to be isometric if they can be bent (or transformed in anyway) one onto the other without changing distances as measured along the surfaces. We can translate the above defini-tion into mathematical form as below. information is. Using the symbols proposed in section 2.1, Consider two data sets 
XR  X   X  and Nd YR  X   X  that are in one-to-one correspondence XY  X  . Let matrix is, x y  X = X  ), we have: Let ij i j GXX = X  and ij i j KYY = X  denote the Gram matrices of the inputs and outputs, respectively. Then we can rewrite eq. (1) as by simple linear algebra transformations: Eq. (2) expresses the conditions for local isometry purely in terms of Gram matrices; learning that we will introduce in the next section. 2.2.2 Semi-definite Embedding The recently proposed SDE algorithm is proposed to maximize the sum of pairwise squared distances between outputs while the input data and outputs are locally isomet-without any furling or fold. Mathematically, SDE obtains: In addition, SDE also constrain the outputs i Y to be centered on the origin: Then, Eq. (3) can be translated into the following form by adding the constraint: to the constraints that they are centered on the origin and locally isometric to the in-puts D i XR  X  . The optimization problem can be written as an instance of semi-definite programming problem below: SDP problem, as well as a number of general-purpose toolboxes. The experimental results in this paper were obtained using the SeDuMi and CSDP 4.7 toolbox[3, 13] to solve the semi-definite programming in a supervised manner. eigenvalue  X   X  . Then the Gram matrix can be written as: A d-dimensional embedding that is locally isometric to the inputs D i XR  X  is ob-tained by identifying the th  X  element of the output i Y as: The three steps of the SDE algorithm are summarized as following, using the unla-beled data: Input: data matrix N D XR  X   X  Step 1 : (Nearest Neighbors) compute the k nearest neighbors of each input. Step 2 : (Semi-definite Programming) compute the Gram matrix of the maximum variance embedding centered on the origin, the preserves the character of locally isometric. Step 3 : (Spectral Embedding) extract a low dimensional embedding from the domi-nant eigenvector of the Gram matrix learned by semi-definite programming. Output: new reduced data matrix Nd YR  X   X  . SDE is an unsupervised dimensionality reduction algorithm. It aims at taking a set of high dimensional data and mapping them into a low dimensional space while preserv-categorization. To complement the original SDE with the additional class informa-tion, we propose a supervised SDE algorithm (SSDE) which utilizes the classes label name of this proposed algorithm implies that membership information is employed to chosen only from representatives of the same class as i X . In other words, the idea of belongs to. This nearest neighbor finding procedure is possible to be conducted since we assume that all the training data are labeled. the dataset {, 1,2,,} i Xi N  X = = " includes all the labeled training sample data. First, the  X = X   X  X   X  X  ... and , and m is the total number of classes known a priori. we look for its K nearest neighbors also belongs to 1  X  , i.e., both i X and its neighbors have the same class membership. When applied to all data points, this procedure leads to a construction of the neighborhood matrix x  X  . Thus, whenever j X is the neighbor 2.2.2 are carried out just as in case of the unsupervised SDE. process. For the testing process after embedding, to learn a mapping from a high-Training process: using the training data X and the label. i  X  with each i X Get the binary matrix  X  , such that j X is the neighbor of i X if and only if 1 ij  X  = . 
Step 2 compute the Gram matrix K through the following optimize problem with 
GXX = X  : Step 3 extract a low-dimension embedding from the dominant eigenvector of the Gram matrix K . Testing process: using unlabeled data 
Project all of the unlabeled data u to a low-dimensional representation by nonlinear regression. Then, the classification of these points is done by the K 
Nearest Neighbor (KNN) classifier. dimensional space into low dimensional space, a nonlinear regression analysis ap-proach [13] is used, i.e. mapping unseen points into the embedding space, where the classification of these points is done by th e K Nearest Neighbor (KNN) classifier. The detail steps of the SSDE algorithm are summarized in Table 1. as to clear the noise features. Moreover, we could visualize these email data by projecting them into a two or three dimens ional space for the users to classify them intuitively. To illustrate the property of our proposed Supervised Semi-definite Embedding teers. The data were labeled by the volunteers themselves. To give experimental re-sults on public dataset, we conduct our proposed algorithm on some benchmark data-sets of UCI and we take PCA, LDA, LLE and the original unsupervised SDE as the baselines in the third subsection. The results of experiments show that the supervised SDE performs very well on text data which exhibits a manifold structure. Our pro-posed SSDE achieves average 8% improvemen t on the precision of classification on the UCI dataset .Due to the nonlinear structure of the email data, our proposed SSDE improved 6%. 4.1 The Synthetic Data For better comprehension of our proposed SSDE algorithm, we give a group of intui-algorithms to illustrate the nonlinear property of it in Figure 1. The picture on the left panel of Figure 1 is the original dataset which is composed of four classes, i.e. dots, tri-angles, circles and stars respectively. They are mixed together in this 2 dimensional picture. The picture on the right panel is these four classes of data after calculated by our algorithm. It can be seen that they are separated into different groups clearly. 4.2 The Real Email Data To demonstrate the classification performance of the proposed algorithm, the real email data was used here. Since it is hard to find public email data on the Web to the best of our knowledge, we choose the real email data of six volunteers which contains the dataset. Given an email as input, we ask these volunteers to identify the class label of it by themselves. We predefined the number of classes which is three and they contain: emails about daily work, emails for fun and junk emails (including adver-tisements) as irrelevant data. Then, we use the Bag of Words algorithm [10] to trans-are the average of the ten runs. Moreover, a nonlinear regression analysis algorithm to project the test data and a KNN classifier with k=3 are used. Figure 2 shows the error rate on the test data. The X-axis denote s the algorithms used for comparison. experimental results show that our proposed SSDE significantly reduced the error rate to 6% while the traditional linear approach even increased the error rate of classifica-tion results. This demonstrates that SSDE can extract nonlinear manifold on the email dataset and outperform the unsupervised approaches such as LLE and SDE. 4.3 The UCI Data The readers may argue that all our experiments are conducted on the synthetic data or the data of our own. What should the performance of the SSDE algorithm be on some public datasets? To answer this question, we conduct it on the UCI machine learning are used by the machine learning community for the empirical analysis. data sets were obtained from the UCI repository [4]. Table 2 gives a detailed descrip-tion of all the datasets used in this paper. For later comparisons, the number of dimen-sions needed by global PCA to retain 90% of the variance, the global intrinsic dimen-sionality L D , is shown in the table as well. To compare the SSDE method with more traditional techniques, we still use PCA (unsupervised linear algorithm), LDA (super-vised linear algorithm), LLE (unsupervised nonlinear algorithm) and original SDE as our baseline systems here since they covers almost all the research areas of dimension reduction by feature extraction. randomly separated into 10 splits and we take training set (80%) and a test set (20%). To learn a mapping from a high dimens ional space into low dimensional space, a nonlinear regression analysis is used, i.e. mapping unseen points into the embedding space. And then the classification of these points is done by the K Nearest Neighbor (KNN) classifier with k=3. performance than SDE and other mapping technique used by us. This is to be ex-pected, as SSDE can extract nonlinear manifold in a supervised way. In this paper, we propose a novel supervised learning of short text manifold by semi-definite programming. We use this algorithm to clean the email data and visualize them for the email service users. Unlike the original unsupervised SDE algorithm, we studied this algorithm when initial dataset were drawn from several classes. It aims at taking a set of high dimensional data and mapping them into a low dimensional space information of the data. In contrast to other dimension reduction algorithms, which SDE, experiments on the real email data and a number of benchmark datasets which clearly exhibit manifold structure demonstr ated that SSDE is a powerful embedding. the reduced space. on more different datasets. Further research will address the problem of choosing D in a more well-founded way for SSDE. 
