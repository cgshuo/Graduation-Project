 1. Introduction
Model predictive control (MPC) refers to a computer control strategy in which an explicit dynamic model of the process is used to predict its future behaviour over some time horizon ( Camacho and Bordons, 1999 ; Maciejowski, 2002 ; Rossiter, 2003 ; Tatjewski, 2007 ). Various MPC algorithms have been successfully used for years in numerous advanced industrial applications ( Qin and Badgwell, 2003 ), in particular in chemical engineering. It is because MPC algorithms have a unique ability to take into account constraints imposed on both process inputs (manipulated variables) and outputs (controlled variables). Constraints are very important in practice, they usually determine quality, economic efficiency and safety. Moreover, MPC techniques are very efficient when applied to process with many inputs and many outputs.
To maximise economic profits MPC algorithms cooperate with set-point optimisation, the purpose of which is to calculate online optimal set-points for MPC ( Blevins et al., 2005 ; Brdys and Tatjewski, 2005 ; Engell, 2007 ; Tatjewski, 2007 , 2008 ). Usually, the multilayer (hierarchical) structure is used in which the control layer keeps process at given operating points and the optimisation layer calculates these set-points ( Findeisen et al., 1980 ). It is also possible to integrate set-point optimisation and MPC optimisation into one optimisation problem ( Tvrzska de Gouvea and Odloak, 1998 ; Tatjewski, 2007 ; Zanin et al., 2000 , 2002 ). Alternatively, an integrated predictive optimiser and con-the control layer with set-points calculated for both optimality and constraint handling.

When the classical multilayer control structure is used, it is usually assumed that disturbances are slowly varying when com-pared to the dynamics of the process ( Tatjewski, 2007 ). In such a case, the steady-state nonlinear set-point optimisation problem can be solved reasonably less frequently than the MPC controller executes. Provided that the dynamics of disturbances is much slower than the dynamics of the plant, such an approach gives good or satisfactory results. In many practical cases, however, dynamics of disturbances is comparable with the process dynamics. Very often disturbances, for example flow rates, properties of feed and energy streams, etc., vary significantly and not much slower than the dynamics of the controlled process. In such cases operation in the classical structure with frequency of set-point optimisation much lower than that of MPC may result in significant loss of set-point optimisation should be repeated online as often as MPC is activated. Because of high computational complexity, it is usually not possible. Moreover, nonlinear optimisation may terminate in local minima. Hence, nonlinear set-point optimisation is rarely used online.
 case for set-point optimisation a constant linear steady-state model derived from the dynamic model used in MPC can be used ( Kassmann et al., 2000 ; Qin and Badgwell, 2003 ; Tatjewski, 2007 , (SSTO) structure in which set-points are calculated from an easy to solvelinearprogrammingproblem.Itcanbesolvedonlineas frequently as MPC is activated. Nat urally, for nonlinear processes such an approach may give economically wrong operating points. take into account the uncertainty in steady-state gain in the frame-work of a robust steady-state target calculation ( Kassmann et al., (ASSTO) structure the comprehensive nonlinear steady-state model is linearised online and set-point optimisation becomes a linear programming task (  X  awryn  X  czuk et al., 2008a ; Qin and Badgwell, 2003 ; Tatjewski, 2007 , 2008 ). A yet another approach is to use 2008b ; Tatjewski, 2008 ; Tatjewski et al., 2006 ). The set-point optimisation layer can be also replaced by a neural network which approximates the solution to the set-point optimisation problem (  X  awryn  X  czuk and Tatjewski, 2010 ).
 cient neural ASSTO structure which cooperates with a suboptimal MPC algorithm. Two neural models of the process are used online.
For set-point optimisation, a steady-state neural model is line-arised online and the set-point is calculated from a linear pro-gramming problem. For MPC, a dynamic neural model is linearised online and the control policy is calculated from a quadratic programming problem. In consequence of linearisation of neural models, the necessity of online nonlinear optimisation is elimi-nated. The proposed structure is applied to a simulated yeast fermentation process. Fermentation is one of the most important biochemical processes. Because properties of the process are non-linear, the classical PID controller and the MPC algorithm based on a linear model are unable to control the process efficiently as demonstrated in  X  awryn  X  czuk (2008) and Nagy (2007) .Results obtained in the ASSTO structure are comparable with those achieved in a computationally demanding structure with nonlinear optimisation used for set-point optimisation and MPC. advantages, can be successfully used for modelling and control of nonlinear processes, e.g. Hussain (1999) and N X rgaard et al. (2000) . Neural networks are universal approximators ( Hornik et al., 1989 ), hence neural models are usually very precise. More-over, unlike fundamental models ( Luyben, 1990 ; Marlin, 1995 ) (which are composed of algebraic and differential equations), neural models have a simple structure and relatively a limited number of parameters. Neural models directly describe input X  output relations of process variables, complicated systems of equations do not have to be solved online in set-point optimisa-tion and MPC. The literature concerned with MPC based on various types of neural models is very rich, e.g. Alexandridis and Sarimveis Yu and Gomm (2003) and references therein.
 classical multilayer control structure, both set-point and MPC opti-with the ASSTO layer which cooperates with the suboptimal MPC algorithm is presented, linearisation of dynamic and steady-state neural models is discussed. In Se ction 4 development of neural models of the yeast fermentation biochemical reactor is thoroughly described. The proposed structure is compared in terms of accuracy and computational burden with the structure in which nonlinear optimisation is used online. Finally, Section 5 concludes the paper. 2. The classical multilayer control structure access to input (manipulated) variables of the process. The supervisory control layer (also named the advanced control layer) calculates online set-points for the basic control layer. The local steady-state optimisation (LSSO) layer calculates online econom-ically optimal set-points for the supervisory control layer in such a way that the production profit is maximised and constraints are satisfied. Each layer has a different frequency of intervention, the basic feedback control layer is the fastest. Plant-wide optimisa-tion of complex processes is not considered in this paper, the reader is referred to Skogestad (2000 , 2004) . 2.1. Set-point optimisation of presentation a single-input single-output process is assumed) ( Tatjewski, 2007 , 2008 ) min subject to y ss  X  f ss  X  u ss , h ss  X  X  1  X  where u is the input of the process (the manipulated variable), y is the output (the controlled variable) and h is the measured (or estimated) disturbance, the superscript  X  X s X  refers to the steady-state. The function f ss : R 2 -R denotes a steady-state model, c represent prices resulting from economic considerations, u u max , y min , y max denote constraints imposed on input and output variables, respectively. 2.2. Model predictive control optimisation
MPC algorithms are usually used in the supervisory control layer. It is because in MPC all necessary constraints can be easily taken into account. Moreover, in case of multivariable processes with strong cross-coupling, a model-based MPC algorithm is a natural choice.
 for the current sampling instant k (or for some previous instant and used for the current instant). Using the nonlinear steady-state model, the value y ss lsso  X  k  X  corresponding to u ss lsso next passed as the desired set-point to the predictive control optimisation problem ( Camacho and Bordons, 1999 ; Maciejowski, tion hard output constraints are used) min subject to u min r u  X  k  X  p j k  X  r u max , p  X  0 , ... , N u 1
D u max r D u  X  k  X  p j k  X  r D u max , p  X  0 , ... , N u y min r ^ y  X  k  X  p j k  X  r y max , p  X  1 , ... , N  X  2  X  In MPC at each sampling instant k (algorithm iteration) future control increments are calculated D u  X  k  X  X  X  D u  X  k j k  X  D u  X  k  X  1 j k  X  ... D u  X  k  X  N purpose of MPC is to minimise differences between the optimal dynamic model of the process. Only the first element of the determined sequence (3) is actually applied to the process u  X  k  X  X  D u  X  k j k  X  X  u  X  k 1  X  At the next sampling instant, k  X  1, output measurements are updated and the whole procedure is repeated.

In the standard multilayer control structure two optimisation problems are solved online: the LSSO task (1) and the MPC task (2). When a comprehensive nonlinear steady-state model is used, the first optimisation problem is nonlinear. Sometimes, for non-linear processes linear MPC algorithms based on linear models give quite good results. In such cases the MPC optimisation task is in fact an easy to solve quadratic programming problem. Unfor-tunately, for many processes, for example for the yeast fermenta-tion reactor, linear MPC algorithms are inefficient (slow) as demonstrated in  X  awryn  X  czuk (2008) . If a nonlinear dynamic model is used for control, the MPC task is also a nonlinear optimisation problem. As a result, one obtains two nonlinear optimisation tasks which must be solved online. 3. The multilayer structure with adaptive steady-state target optimisation and efficient MPC based on neural models 3.1. The structure
The multilayer neural adaptive steady-state target optimisation (ASSTO) structure ( Tatjewski, 2007 , 2008 ) which cooperates with MPC is depicted in Fig. 2 . It consists of the nonlinear set-point optimisation (LSSO) layer, the adaptive steady-state target optimi-sation layer and the MPC layer. Two neural models are used online. A steady-state neural model is used in LSSO and in ASSTO layers, whereas a dynamic model is used in MPC. Nonlinear LSSO set-point optimisation, in which the nonlinear optimisation problem (1) is solved, is activated infrequently. At each sampling instant ASSTO and MPC problems are solved. The ASSTO layer recalculates the optimal set-point when the LSSO layer is not activated. Both neural models are linearised online around the current operating point. As a result, set-point optimisation (ASSTO) is a linear programming task, MPC optimisation is a quadratic programming task. ASSTO and MPC layers have the same frequency of interven-tion. In different words, the ASSTO layer, unlike the LSSO layer, is activated as frequently as the MPC algorithm is executed, i.e. at each sampling instant.

In the classical (i.e. nonadaptive) SSTO structure a constant linear steady-state model is used for set-point calculation, whereas in the LSSO layer a comprehensive nonlinear steady-state model is used. On the one hand it makes it possible to formulate a linear programming SSTO optimisation problem, on the other hand the linear model is usually only a rough estimate of the nonlinear one. In the presented structure the same steady-state model is used in LSSO and ASSTO layers.
 3.2. Neural models feedforward neural network with one hidden layer and a linear output ( Haykin, 1999 ) is used. The output of the model is where the sum of inputs of the i th hidden node is
Weights of the steady-state neural model are denoted by w , i  X  1, y , K ss , j  X  0,1,2, and w i 2,ss , i  X  0, y , K ss second layer, respectively, K ss is the number of hidden nodes, j : R -R is the nonlinear transfer function of hidden nodes (e.g. hyperbolic tangent). From (4) and (5), the steady-state neural model is model is described by the general discrete-time equation y  X  k  X  X  f  X  x  X  k  X  X  X  f  X  u  X  k t  X  , ... , u  X  k n B  X  , realised by the second feedforward neural network the structure of which is shown in Fig. 4 . Its output is y  X  k  X  X  w 2 0  X  where K is the number of hidden nodes and z i ( k ) is the sum of inputs of the i th hidden node. From (7) one has
Weights of the dynamic neural model are denoted by w i , j i  X  1, y , K , j  X  0 , ... , n A  X  n B  X  n C t t h  X  2, and w the first and the second layer, respectively, I u  X  n B t
I h  X  n C t h  X  1. 3.3. Adaptive steady-state target optimisation based on neural models point optimisation problem (1) is also nonlinear. In order to reduce computational complexity in the ASSTO layer the model is linearised online. Linearisation makes it possible to calculate the set-point from a linear programming task.
 the current state of the process determined by most recent measurements u ( k 1) and h ( k ). Using the Taylor series expan-sion, the linear approximation of the nonlinear model is where
H  X  k  X  X  d f ss  X  u ss , h ss  X  d u ss
In contrast to the standard SSTO approach with a constant linear model ( Kassmann et al., 2000 ; Qin and Badgwell, 2003 ; Tatjewski, 2007 , 2008 ), the discussed formulation uses a linearised model derived online from the steady-state neural model. Taking into account the structure of the steady-state neural model given by (4) and (5), one has H  X  k  X  X 
If hyperbolic tangent is used as the nonlinear transfer function j in the hidden layer of the model @ j  X  z ss i  X 
Taking into account (6), (10), (11), (12), the linearised steady-state neural model is
Using the linearised steady-state model, from the nonlinear set-point optimisation (LSSO) problem (1), one obtains the equivalent linear programming ASSTO problem min subject to u y y Let the solution to the ASSTO problem for the current sampling used to calculate the optimal set-point y ss assto  X  k  X  . 3.4. Efficient nonlinear MPC-NPL algorithm based on neural models
If for prediction and optimisation of the control policy (3) a nonlinear functions of calculated increments D u  X  k  X  . In conse-quence, the MPC optimisation problem (2) is a nonlinear task which has to be solved online in real time. To eliminate the necessity of nonlinear optimisation, the MPC algorithm with nonlinear prediction and linearisation (MPC-NPL) (  X  awryn  X  czuk, 2007 , 2009 ) is adopted (in this work the model and the algorithm take into account the disturbance signal h ). At each sampling instant k a local linear approximation of the nonlinear neural dynamic model is calculated online. In consequence of linearisa-tion, predictions depend in a linear way on the calculated control policy. Hence, the algorithm needs solving online a quadratic programming problem, which can be easily done within a fore-seeable time period.

Linearisation of the nonlinear dynamic model (7) derived at the current sampling instant k is y  X  k  X  X  b t  X  k  X  u  X  k t  X  X  X  b n B  X  k  X  u  X  k n B  X  arised model. It can be shown (  X  awryn  X  czuk, 2009 ) that if the linear approximation (16) of the original nonlinear model (7) is used for prediction in MPC, the output prediction vector ^ y  X  k  X  X   X  ^ y ^ y  X  k  X  X  G  X  k  X  D u  X  k  X  X  y 0  X  k  X  X  17  X  The first part depends only on the future (on future increments D u  X  k  X  which are calculated from the MPC optimisation problem (2) for the current sampling instant k ), the second part is a depends only on the past. The dynamic matrix G  X  k  X  of dimension-ality N N u G  X  k  X  X  is composed of step-response coefficients of the linearised model. the linearised model s  X  k  X  X  Taking into account the structure of the dynamic neural model given by (8) and (9), coefficients of the linearised model (16) are calculated online from a  X  k  X  X  @ f  X  x  X  k  X  X  @ y  X  k l  X   X  where l  X  1, y , n A , and b  X  k  X  X  @ f  X  x  X  k  X  X  @ u  X  k l  X  The linearisation point x  X  k  X  is determined by input, disturbance and output signals (measurements) corresponding to the argu-ments of the nonlinear model (7). If hyperbolic tangent is used as the nonlinear transfer function j in the hidden layer of the neural dynamic model @ j  X  z i  X  x  X  k  X  X  X  @ z i  X  x  X  k  X  X   X  1 tanh
Because the future output prediction (17) is a linear function of the optimised control policy D u  X  k  X  , the MPC optimisation problem (2) becomes the following quadratic programming task: subject to u min r J D u  X  k  X  X  u  X  k 1  X  r u max
D u max r D u  X  k  X  r D u max y min e min r G  X  k  X  X  y 0  X  k  X  D u  X  k  X  r y max  X  e max e min Z 0 , e max Z 0  X  23  X  where y y y are vectors of length N , u u D u u  X  k 1  X  X  X  u  X  k 1  X  ... u  X  k 1  X  T are vectors of length N u , K  X  l diag  X  1 , ... , 1  X  and J  X  are matrices of dimensionality N u N u . If at the current sampling instant the nonlinear LSSO layer is used, y ref  X  k  X  X  X  y ss To prevent infeasibility problems output constraints are softened by means of slack variables (vectors e min and e max of length N ), r min and r max 4 0 are weights ( Maciejowski, 2002 ; Tatjewski, 2007 ).

In the MPC-NPL algorithm for optimisation of the future control policy a linearised model (16) is used (it makes it possible to obtain a quadratic programming task (23)). On the other hand, for free trajectory calculation the full nonlinear neural model can be used. It is reflected in the name of the algorithm. From (8) the nonlinear prediction is ^ y  X  k  X  p j k  X  X  w 2 0  X  where p  X  1, y , N and d  X  k  X  X  y  X  k  X  y  X  k j k 1  X  X  y  X  k  X  w 2 0  X  is the unmeasured disturbance estimate. Using (9) recurrently one obtains z i  X  k  X  p j k  X  X  w 1 i , 0  X 
Auxiliary integer numbers are denoted by I uf  X  p  X  X  max  X  min  X  p t  X  1 , I u  X  X  , I hf  X  p  X  X  max  X  min  X  p t h  X  1 , I calculated recurrently online from the general nonlinear predic-tion equation (24) assuming only influence of the past y 0  X  k  X  p j k  X  X  w 2 0  X 
For free trajectory calculation no changes in the control signal from the current sampling instant k are assumed (i.e. replaced by the free trajectory. From (26) one obtains z 0 i  X  k  X  p j k  X  X  w 1 i , 0  X 
Since future values of the disturbance are usually not known at for p Z 1. 3.5. Summary of calculations
Fig. 5 . The ASSTO layer calculates the set-point as frequently as the MPC-NPL algorithm is activated (at each sampling instant).
The LSSO layer is activated infrequently or for verification. Neural models are linearised online: the ASSTO layer uses a linearised steady-state model (it solves a linear programming problem), the
MPC-NPL algorithm uses a linearised dynamic model (it solves a quadratic programming problem).
 1. Linearisation of the steady-state neural model: H  X  k  X  is found 3. If the calculated set-point is verified, the set-point u ss 4. Linearisation of the dynamic neural model: coefficients a 6. The MPC-NPL quadratic programming problem (23) is solved to determine the optimal control policy D u  X  k  X  . As the set-point layers are used. 7. The first element of the calculated vector is applied to the process, i.e. u  X  k  X  X  D u  X  k j k  X  X  u  X  k 1  X  . 4. Simulation results 4.1. Yeast fermentation reactor The considered yeast fermentation reactor is shown in Fig. 6 . The reactor is modelled as a continuous stirred tank with the constant substrate feed flow, the outlet flow from the reactor containing the product, the substrate and the biomass is also constant. The reactor contains three distinct components: the biomass, which is a suspension of yeast fed into the system and evacuated continuously, the substrate, which is the solution of glucose feeding the micro-organism ( Saccharomyces cerevisiae ) and the product (ethanol), which is evacuated together with other components. Together with yeast, inorganic salts are added. It is necessary for the formation of coenzymes. Inorganic salts have also a strong influence on the equilibrium concentration of oxygen in the liquid phase.

The comprehensive fundamental model of the process is thoroughly described in Nagy (2007) , here the model is given in a compact form. State variables are: V  X  the volume of the mass of the reaction (l), c X  X  concentration of the biomass (yeast) (g/l), c  X  concentration of the product (ethanol) (g/l), c S  X  concentra-in the liquid phase (mg/l), T r  X  temperature of the reactor ( T  X  temperature of the cooling agent in the jacket ( 1 C). The reactor is described by the following continuous-time funda-mental model containing seven nonlinear ordinary differential equations: d V d t  X  F i F e d c d t  X  m X C X d c d t  X  m P C X d c d t  X  d c d t  X  X  k la  X  X  c O 2 c O 2  X  r O 2 d T d t  X  d T d t  X  The equilibrium concentration of oxygen in the liquid phase is c 2  X  X  14 : 6 0 : 3943 T r  X  0 : 007714 T where the global effect of ionic strengths is X The mass transfer coefficient for oxygen is  X  k The rate of oxygen consumption is r The maximum specific growth rate is  X  A 1 exp Parameters of the fundamental model are given in Table 1 , nominal operating conditions of the process are given in Table 2 .
The fundamental model of the process is implemented in Matlab, it is treated as the real process during simulations. Because model
Runge X  X utta 45 method, need small steps, and, in consequence, a huge number of iterations). The f undamental steady-state model is derived from the dynamic model by neglecting time and equating all time-derivatives to zero. Obtained steady-state model is com-posed of a set of nonlinear equations. They are solved using the Newton X  X aphson method.
 algorithm, the fermentation process has one input (manipulated in Nagy (2007) , two main disturbances of the process can be considered: changes in the substrate concentration c S and in temperature of the substrate flow entering the reactor T in only the second one has a significant effect on the process. nonlinear. The steady-state characteristics T in ( F ag , T
Fig. 7 . In consequence of the nonlinear nature of the reactor, the classical PID controller and the MPC algorithm based on a linear 2008 ; Nagy, 2007 ). 4.2. Process modelling for set-point optimisation and MPC comprised of nonlinear equations which are solved iteratively, for set-point optimisation and MPC simpler neural models are used.
Neural models can be easily used online because they are direct models, for a given input the output is calculated without the necessity of solving any equations. 4.2.1. Steady-state modelling random data from the whole domain of interest (0 l = h r F
Accuracy of models is defined by the sum of squared errors (SSE) performance function SSE  X  of the fundamental models, S ss is the number of steady-state samples. Since input, disturbance and output variables have a different order of magnitude, they are scaled as u  X  0 : 01  X  F ag F ag , nom  X  y  X  0 : 1  X  T r T r , nom  X  spond to the nominal operating conditions of the process (Table 2 ). 2000 samples. The training data set is used only for neural network training, the SSE performance index is minimised on this set. As a result, parameters (weights) of neural networks are optimised. The value of the SSE index for the validation data set is monitored during training. To avoid overfitting training is terminated when the validation error increases. Such an approach makes it possible to obtain models which have good generalisation properties. Model selection is performed taking into account only the value of SSE for used to assess generalisation abilities of the chosen model. It is model selection. Therefore, the error on the test set gives an unbiased estimate of the generalisation error.
 Neural models are trained using the BFGS (Broyden X  X letcher X  Goldfarb X  X hanno) optimisation algorithm ( Bazaraa et al., 1993 ). Weights of networks are initialised randomly and training is repeated 10 times for each model configuration (the multi-start approach). Results presented next are the best obtained.
Three classes of steady-state models are considered: linear, polynomial and natural models. Table 3 compares obtained models in terms of accuracy and complexity. Due to nonlinearity of the process, the linear model is very inaccurate. Polynomial models are more precise, but they need many parameters. Since neural networks are universal approximators ( Hornik et al., 1989 ), they make it possible to approximate the steady-state characteristics with very high accuracy. Finally, as a compromise between accuracy and complexity, the model with K ss  X  3 hidden nodes is chosen. Although it has only 13 parameters, its accuracy is comparable with that of the polynomial model of the 10th order which has as many as 121 parameters. Finally, the chosen neural model is tested. The SSE test error is small, comparable with the error for training and validation data sets. As the test set is used neither for training nor for model selection, such a small value means that the model generalises well. 4.2.2. Dynamic modelling
To generate data for finding a dynamic model, the fundamental dynamic model is simulated open-loop in order to obtain training, validation and test data sets shown in Fig. 8 . Each set contains 4000 samples. The output signal contains small measurement noise. For dynamic models the SSE performance index is SSE  X  where y mod  X  k j k 1  X  denotes the output of the dynamic model for the sampling instant k calculated using signals up to the sampling instant k 1, y ( k ) are targets recorded during simulations of the fundamental model, S is the number of samples.
 Models of different orders of dynamics have been considered. Finally, second-order neural models are used Table 4 shows accuracy and complexity of models with different number of hidden nodes. Because data sets contain noise, when the model has too many weights ( K 4 3), the SSE error for the validation data set increases. Hence, the model with K  X  3 hidden nodes is chosen. The model has 25 weights. For the test data set the SSE test error is also small, comparable with the error for training and validation sets. Fig. 9 shows the output of the process vs. the output of the neural model for training, validation and test data sets. Because the model is very precise, model errors are also depicted.

To show high accuracy of the neural model, a linear model is also found, it has the same arguments as the neural one (29). Unfortunately, due to nonlinearity of the process, its accuracy is very low. For the linear model SSE validation  X  6.9683 10 for the chosen neural models SSE validation  X  2.3435 10 1 shows the output of the process vs. the output of the linear model for training and validation data sets, no comparison for the test data set is given since the linear model is not finally selected. In 4.3. Online set-point optimisation and MPC
Three systems structures are compared: 1. LSSO  X  MPC-NO : the ideal classical multilayer structure with nonlinear set-point optimisation (LSSO) repeated at each sampling instant and the MPC-NO algorithm with full non-linear optimisation. 2. LSSO 100  X  ASSTO  X  MPC-NPL : the proposed structure with the
ASSTO layer and the MPC-NPL algorithm, the LSSO layer is executed 100 less frequently than the MPC-NPL algorithm. 3. LSSO 100  X  MPC-NO : the realistic classical structure with non-linear set-point optimisation repeated 100 times less fre-quently than the MPC-NO algorithm.
 In the first case at each sampling instant two nonlinear optimisa-tion problems are solved online at each sampling instant. For this purpose sequential quadratic programming optimisation algo-rithm is used ( Bazaraa et al., 1993 ). In the second case a linear programming set-point optimisation is solved in the ASSTO layer and a quadratic programming MPC-NPL task is solved. Both tasks are solved at each sampling instant. The LSSO layer uses nonlinear optimisation, but it is executed for verification very infrequently. Finally, in the third structure the set-point is calculated by the LSSO layer by means of nonlinear optimisation (the ASSTO layer is switched off). In order to illustrate usefulness of the ASSTO layer, in this structure potentially the better MPC-NO algorithm rather than the suboptimal MPC-NPL one is used.

Three models of the process are used during simulation experiments. The fundamental model is used as the simulated process, for set-point optimisation the steady-state neural model with K ss  X  3 hidden nodes is used, for MPC the dynamic neural model with K  X  3 hidden nodes is used.
 To maximise production, in set-point optimisation (LSSO and ASSTO layers) the minimised objective function is J The same constraints imposed on the manipulated variable are used in set-point optimisation and MPC F ag  X  0l = h , F max ag  X  100 l = h Additionally, in set-point optimisation and MPC a constraint is imposed on the controlled output to guarantee that T r r T min where the value of T min r can be adjusted by the operator of the process. The disturbance scenario is T in  X  k  X  X 
The disturbance changes sinusoidally from the sampling instant k  X  100, its amplitude is 5. The disturbance signal is shown in Fig. 11 .

N  X  10, N u  X  2, l  X  2, the same as in  X  awryn  X  czuk (2008) , r max  X  20. Fig. 12 depicts simulation results of three compared systems structures for T r min  X  25. At the beginning of simulations the LSSO layer calculates the optimal set-point ( F ag min
T r min  X  25 1 C). Because the process is started using nominal conditions (Table 2 ), the MPC algorithm needs some time (approxi-mately 70 sampling instants) to steer the process to the desired set-point. In the LSSO 100  X  ASSTO  X  MPC-NPL structure the LSSO remaining iterations the set-point is calculated using the ASSTO Fig. 14 for T r min  X  26 1 C.
 be made: 1. Trajectories obtained in the proposed structure with the 2. The realistic classical structure with infrequent nonlinear set-point optimisation (LSSO 100  X  MPC-NO) gives significantly dif-ferent trajectories. It means that the ASSTO layer is really necessary.

For the whole simulation horizon (600 sampling instants) the economic performance index J  X  is calculated after simulations. Table 5 gives values of the index J obtained in the ideal multilayer LSSO  X  MPC-NO structure and in the proposed LSSO 100  X  ASSTO  X  MPC-NPL structure for different constraint values T r min .As Figs. 12 X 14 suggest, both structures yield similar economic performance. For example, for T r min the LSSO  X  MPC-NO structure J E  X  2.2778 10 6 , in the LSSO ASSTO  X  MPC-NPL structure J E  X  2.3137 10 6 .
 Although trajectories obtained in the ideal multilayer LSSO  X  MPC-NO structure and in the proposed LSSO 100  X  ASSTO  X  MPC-NPL structure are vary similar, the difference in computa-tional burden is very big. Table 6 shows computational complex-ity of these two structures in MFLOPS (millions of floating point T min  X  25, in the LSSO  X  MPC-NO structure the computational cost is 659.89 MFLOPS, in the LSSO 100  X  ASSTO  X  MPC-NPL structure it is reduced to 61.32 MFLOPS. 5. Conclusions
The described structure with adaptive steady-state target optimisation (ASSTO) and the MPC-NPL algorithm is computa-tionally efficient and gives trajectories very similar to those obtained when for set-point optimisation and MPC nonlinear optimisation is used online. For set-point optimisation and MPC steady-state and dynamic neural models are linearised online. In consequence, set-point optimisation is an easy to solve linear programming task, MPC optimisation is a quadratic programming problem. These optimisation tasks can be efficiently solved online, the necessity of nonlinear optimisation is eliminated.
This paper recommends using neural models for set-point optimisation and MPC. Development of steady-state and dynamic neural models is detailed. Although a dynamic fundamental (first-directly used in MPC because it is composed of a set of stiff differential equations which must be solved online. The neural model is very precise and has a limited number of parameters. Moreover, its structure is simple, regular. In consequence, it can be easily used online in the computationally efficient MPC-NPL algorithm, complicated systems of nonlinear differential equations do not have to be solved online. Analogously, for set-point optimisation a neural approximate model is used, it is not necessary to solve online a set of nonlinear algebraic equations comprising the comprehensive fundamental steady-state model.
 Acknowledgement national budget funds for science.
 References
