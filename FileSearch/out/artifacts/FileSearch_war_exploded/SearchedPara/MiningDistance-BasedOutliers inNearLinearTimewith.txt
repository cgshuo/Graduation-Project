 De ning outliers by their distance to neigh boring examples is a popular approac h to nding unusual examples in a data set. Recen tly, much work has been conducted with the goal of nding fast algorithms for this task. We sho w that a sim-ple nested loop algorithm that in the worst case is quadratic can giv e near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with mil-lions of examples and sho w that the near linear scaling holds over sev eral orders of magnitude. Our average case analy-sis suggests that much of the eciency is because the time to pro cess non-outliers, whic h are the ma jorit y of examples, does not dep end on the size of the data set.
 H.2.8 [ Database Managemen t ]: Database Applications| data mining Outliers, distance-based operations, anomaly detection, disk-based algorithms Detecting outliers, examples in a database with unusual prop erties, is an imp ortan t data mining task. Recen tly re-searc hers have begun focusing on this problem and have at-tempted to apply algorithms for nding outliers to tasks suc h as fraud detection [7], iden tifying computer net work in-trusions [10, 18], data cleaning [21], and detecting emplo yers with poor injury histories[17].
 Outlier detection has a long history in statistics [3, 13], but has largely focussed on data that is univ ariate, and data with a kno wn (or parametric) distribution. These two lim-itations have restricted the abilit y to apply these typ es of metho ds to large real-w orld databases whic h typically have man y di eren t elds and have no easy way of character-izing the multiv ariate distribution of examples. Other re-searc hers, beginning with the work by Knorr and Ng [16], have tak en a non-parametric approac h and prop osed using an example's distance to its nearest neigh bors as a measure of unusualness [2, 10, 17, 19].
 Although distance is an e ectiv e non-parametric approac h to detecting outliers, the dra wbac k is the amoun t of com-putation time required. Straigh tforw ard algorithms, suc h as those based on nested loops, typically require O ( N 2 ) dis-tance computations. This quadratic scaling means that it will be very dicult to mine outliers as we tac kle increas-ingly larger data sets. This is a ma jor problem for man y real databases where there are often millions of records. Recen tly, researc hers have presen ted man y di eren t algo-rithms for ecien tly nding distance-based outliers. These approac hes vary from spatial indexing trees to partitioning of the feature space with clustering algorithms [19]. The common goal is dev eloping algorithms that scale to large real data sets.
 In this pap er, we sho w that one can mo dify a simple al-gorithm based on nested loops, whic h would normally have quadratic scaling beha vior, to yield near linear time mining on real, large, and high-dimensional data sets. Sp eci cally , our con tributions are: The remainder of this pap er is organized as follo ws. In the next section, we review the notion of distance-based outliers and presen t a simple nested loop algorithm that will be the focus of this pap er. In Section 3, we sho w that although our simple algorithm has poor worst case scaling prop erties, for man y large, high-dimensional, real data sets the actual performance is extremely good and is close to linear. In Section 4, we analyze our algorithm and attempt to explain the performance with an average case analysis. In Section 5, we presen t examples of disco vered outliers to giv e the readers a qualitativ e feel for how the algorithm works on real data. Finally , we conclude this pap er by discussing limitations and directions for future work. A popular metho d of iden tifying outliers is by examining the distance to an example's nearest neigh bors [2, 16, 17, 19]. In this approac h, one looks at the local neigh borho od of poin ts for an example typically de ned by the k nearest examples (also kno wn as neigh bors). If the neigh boring poin ts are rel-ativ ely close, then the example is considered normal; if the neigh boring poin ts are far away, then the example is consid-ered unusual. The adv antages of distance-based outliers are that no explicit distribution needs to be de ned to deter-mine unusualness, and that it can be applied to any feature space for whic h we can de ne a distance measure.
 Giv en a distance measure on a feature space, there are man y di eren t de nitions of distance-based outliers. Three popu-lar de nitions are 1. Outliers are the examples for whic h there are few er 2. Outliers are the top n examples whose distance to the 3. Outliers are the top n examples whose average distance There are sev eral minor di erences between these de ni-tions. The rst de nition does not pro vide a ranking and requires specifying a distance parameter d . Ramasw am y et al. [19] argue that this parameter could be dicult to deter-mine and ma y involv e trial and error to guess an appropri-ate value. The second de nition only considers the distance to the k th neigh bor and ignores information about closer poin ts. Finally , the last de nition accoun ts for the distance to eac h neigh bor but is slo wer to calculate than de nition 1 or 2. Ho wever, all of these de nitions are based on a near-est neigh bor densit y estimate [11] to determine the poin ts in low probabilit y regions whic h are considered outliers. Researc hers have tried a variet y of approac hes to nd these outliers ecien tly. The simplest are those using nested loops [16, 17, 19]. In the basic version one compares eac h example with every other example to determine its k nearest neigh-bors. Giv en the neigh bors for eac h example in the data set, simply select the top n candidates according to the outlier de nition. This approac h has quadratic complexit y as we must mak e all pairwise distance computations between ex-amples.
 Another metho d for nding outliers is to use a spatial in-dexing structure suc h as a KD-tree [4], R-tree [12], or X-tree [5] to nd the nearest neigh bors of eac h candidate poin t [16, 17, 19]. One queries the index structure for the closest k poin ts to eac h example, and as before one simply selects the top candidates according to the outlier de nition. For low-dimensional data sets this approac h can work extremely well and poten tially scales as N log N if the index tree can nd an example's nearest neigh bors in log N time. Ho w-ever, index structures break down as the dimensionalit y in-creases. For example, Breunig et al. [8] used a varian t of the X-tree to do nearest neigh bor searc h and found that the index only work ed well for low dimensions, less than 5, and performance dramatically worsened for just 10 or 20 dimen-sions. In fact, for high-dimensional data they recommended sequen tial scanning over the index tree.
 A few researc hers have prop osed partitioning the space into regions and thus allo wing faster determination of the near-est neigh bors. For eac h region, one stores summary statistics suc h as the minim um bounding rectangle. During nearest neigh bor searc h, one compares the example to the bounding rectangle to determine if it is possible for a nearest neigh bor to come from that region. If it is not possible, all poin ts in the region are eliminated as possible neigh bors. Knorr and Ng [16] partition the space into cells that are hyp er-rectangles. This yields a complexit y linear in N but exp o-nen tial in the num ber of dimensions. They found that this cell based approac h outp erformed a nested loop algorithm, whic h is quadratic in N , only for four or few er dimensions. Others use a linear time clustering algorithm to partition the data set [19, 10]. With this approac h, Ramasw am y et al. demonstrated much better performance compared with the nested loop and indexing approac hes on a low-dimensional syn thetic data set. Ho wever, their exp erimen ts did not test how it would scale on larger and higher-dimensional data. Finally , a few researc hers have adv ocated pro jections to nd outliers. Aggra wal and Yu [1] suggest that because of the curse of dimensionalit y one should focus on nding out-liers in low-dimensional pro jections. Angiulli and Pizzuti [2] pro ject the data in the full feature space multiple times onto the interv al [0,1] with Hilb ert space lling curv es. Eac h successiv e pro jection impro ves the estimate of an example's outlier score in the full-dimensional space. Their initial scal-ing results are promising, and app ear to be close to linear, however they have rep orted results on only two syn thetic domains.
 In this pap er, we sho w that the simplest typ e of algorithm based on nested loops in conjunction with randomization and a pruning rule giv es state-of-the-art performance. Ta-ble 1 sho ws our variation of the nested loop algorithm in more detail. The function distance computes the distance between any two examples using, for example, Euclidean distance for con tin uous features and Hamming distance for and upp ercase variables represen ts sets.
 discrete features. The sco re function can be any monoton-ically decreasing function of the nearest neigh bor distances suc h as the distance to the k th nearest neigh bor, or the av-erage distance to the k neigh bors.
 The main idea in our nested loop algorithm is that for eac h example in D we keep trac k of the closest neigh bors found so far. When an example's closest neigh bors achiev e a score lower than the cuto we remo ve the example because it can no longer be an outlier. As we pro cess more examples, the al-gorithm nds more extreme outliers and the cuto increases along with pruning eciency .
 Note that we assume that the examples in the data set are in random order. The examples can be put into random order in linear time and constan t main memory with a disk-based algorithm. One rep eatedly shues the data set into random piles and then concatenates them in random order.
 In the worst case, the performance of the algorithm is very poor. Because of the nested loops, it could require O ( N distance computations and O ( N=blocksize N ) data ac-cesses. In this section, we examine the empirical performance of the simple algorithm on sev eral large real data sets. The primary question we are interested in answ ering is \Ho w does the running time scale with the num ber of data poin ts for large data sets?" In addition, we are also interested in understanding how the running time scales with k , the num ber of nearest neigh bors.
 To test our algorithm we selected the ve real and one syn-thetic data sets summarized in Table 2. These data sets span a range of problems and have very di eren t typ es of features. We describ e eac h in more detail.
 We obtained the data sets Corel Histogram, Co vert yp e, and KDDCup 1999 from the UCI KDD Arc hiv e [14] and the census data from the IPUMS rep ository [20].
 We pro cessed the data by normalizing all con tin uous vari-ables to the range [0,1] and con verting all categorical vari-ables to an integer represen tation. We then randomized the order of examples in the data sets. Randomizing a le can be done in O ( N ) time and constan t main memory with a disk-based shuing algorithm as follo ws: Sequen tially pro-cess eac h example in the data set by randomly placing it into one of n di eren t piles. Recom bine the piles in random order and rep eat this pro cess a xed num ber of times. We ran our exp erimen ts on a ligh tly loaded Pentium 4 com-puter with a 1.5 GHz pro cessor and 1GB RAM running Lin ux. We rep ort the wall clo ck time, the time a user would have to wait for the output, in order to measure both CPU and I/O time. The rep orted times do not include the time needed for the initial randomization of the data set and rep-resen t one trial. Preliminary exp erimen ts indicated that al-ternate randomizations did not have a ma jor e ect on the running time. To measure scaling, we generated smaller data sets by taking the rst n samples of the randomized set. Unless otherwise noted, we ran exp erimen ts to return the top 30 anomalies with k = 5, a blo ck size ( j B j ) of 1000 examples, and we used the average distance to the nearest k neigh bors as the score function.
 Our implemen tation of the algorithm was written in C++ and compiled with gcc version 2.96 with the -O3 optimiza-tion ag. We accessed examples in the data set sequen tially using standard iostream functions and we did not write any special routines to perform cac hing. The total memory footprin t of the executing program was typically less than 3 MB.
 Figure 1 sho ws the total time tak en to mine outliers on the six data sets as the num ber of examples varied. Note that both the x and y axes are in a logarithmic scale. Eac h graph sho ws three lines. The bottom line represen ts the theoreti-cal time necessary to mine the data set giv en a linear algo-rithm based on the running time for N = 1000. The middle line sho ws the actual running times of our system. Finally , the top line sho ws the theoretical time needed assuming a quadratic algorithm based on scaling the running time for N = 1000.
 These results sho w that our simple algorithm giv es extremely good scaling performance that is near linear time. The scal-ing prop erties hold for data sets with both con tin uous and discrete features and the prop erties hold over sev eral or-ders of magnitude of increasing data set size. The plot-ted poin ts follo w nearly straigh t lines on the log-log graphs whic h means that the relationship between the y and x axis variables is of the form y = ax b or log y = log a + b log x , where a and b are constan ts. Th us, the algorithm scales with a polynomial complexit y with an exp onen t equal to the slop e of the line. Table 3 presen ts for eac h data set the slop e of a regression line t to the poin ts in Figure 1. The algorithm obtained a polynomial scaling complexit y with exp onen t varying from 1.13 to 1.32.
 Table 3: Slop e b of the regression t relating log t = log a + b log N (or t = aN b ) where t is the total time (CPU + I/O), N is the num ber of data poin ts, and a is a constan t factor.
 We also examined how the total running time scales with k , the num ber of neigh bors and the results for Normal 30D and Person (with N = 1 ; 000 ; 000) are sho wn in Figure 2. In these graphs, both the x and y axes are in a linear scale and the measured times fall appro ximately on a straigh t line. This suggests that the running time scales linearly with k . In this section, we explain with an average case analysis wh y randomization in conjunction with pruning performs well, esp ecially when much of the past literature rep orted that nested loop designs were extremely slo w because of the O ( N 2 ) distance computations. In particular, both Knorr and Ng [16] and Ramasw am y et al. [19] implemen ted ver-sions of the nested loop algorithm and rep orted quadratic performance. Ho wever, Knorr and Ng did not use pruning or randomization in their algorithm, and Ramasw am y et al. only incorp orated pruning.
 Consider the num ber of distance computations needed to pro cess an example x . For now we assume that we are using outlier de nition 2, rather than de nition 3 whic h we used in our exp erimen ts, for ease of analysis. With this de nition an outlier is determined by the distance to its k th nearest neigh bor. In order to pro cess x we compare it with examples in the data set until we have either (1) found k neigh bors within the cuto distance d , in whic h case we eliminate it as it cannot be an outlier, or (2) we have compared it with all N examples in the data set and failed to nd k neigh bors within distance d , in whic h case it is classi ed as an outlier. the observ ed time at N = 1000 . Person (with N = 1 ; 000 ; 000 ).
 We can think of this problem as a set of indep enden t Bernoulli trials where we keep dra wing instances until we have found k successes ( k examples within distance d ) or we have ex-hausted the data set. Let ( x ) be the probabilit y that a randomly dra wn example lies within distance d of poin t x , let Y be a random variable represen ting the num ber of trials until we have k successes, and let P ( Y = y ) be the probabil-ity of obtaining the k th success on trial y . The probabilit y P ( Y = y ) follo ws a negativ e binomial distribution: The num ber of exp ected samples we need to dra w to pro cess one example x is: The rst term is the exp ectation of concluding a negativ e binomial series within N trials. That is, as we are pro cess-ing an example, we keep dra wing more examples until we have seen k that are within distance d , at whic h poin t we eliminate it because it cannot be an outlier. The second term is the exp ected cost of failing to conclude the negativ e binomial series within N trials, in whic h case we have ex-amined all N data poin ts because the example is an outlier (less than k successes in N trials).
 The exp ectation of a negativ e binomial series with an in nite num ber of trials is, This is greater than the rst term in Equation 2. Com bining Equations 2 and 3 yields, Surprisingly , the rst term whic h represen ts the num ber of distance computations to eliminate non-outliers does not de-pend on N . The second term, whic h represen ts the exp ected cost of outliers (i.e, we must compare with everything in the database and then conclude that nothing is close) does de-pend on N, yielding an overall quadratic dep endency to pro-cess N examples in total. Ho wever, note that we typically set the program parameters to return a small and possibly xed num ber of outliers. Th us the rst term dominates and we obtain near linear performance.
 One assumption of this analysis is that the cuto distance is xed. In practice, the cuto distance varies during program execution, and the nal cuto required to return the top n outliers changes with N . Ho wever, the relationship between cuto value and percen tage of the data set pro cessed often sta ys the same for di eren t values of N . For example, Fig-ure 3 sho ws the plot of cuto value against the percen tage of the data set pro cessed for di eren t values of N . In general, we exp ect that if the nal cuto distance in-creases with larger N , then scaling will be better as ( x ) is larger and any randomly selected example is more likely to be a success (neigh bor). Con versely , if the cuto distance decreases, the scaling will be worse. In Figure 4 we plotted the relationship between b , the empirical scaling factor, and c 50 K =c 5 K , the ratio of the nal cuto s for N = 50000 and N = 5000 for the six data sets used in the previous sec-tion. We also plotted results for two additional data sets, Uniform 3D and Mixed 3D, whic h we believ ed would be resp ectiv ely extremely dicult and easy . Uniform 3D is a three-dimensional data set generated from a uniform distri-bution between [-0.5,0.5] on eac h dimension. Mixed 3D is a mixture of the uniform data set (99%) com bined with a Gaussian (1%) cen tered on the origin with covariance matrix equal to the iden tity matrix.
 The results indicate that for man y data sets the cuto ra-tio is near or greater than 1. The only data set with an extremely low cuto ratio was Uniform3D. The graph also Figure 3: Value of the cuto versus the percen tage of the data set pro cessed for N = 50K, 100K, 1M, and 5M. indicates that higher values of the cuto ratio are asso ciated with better scaling scores (lo wer b ). This supp orts our the-ory that the primary factor determining the scaling is how the cuto changes as N increases. Figure 4: Empirical scaling factor b versus c 50 K =c 5 K the ratio of cuto scores for N = 50 ; 000 and N = 5 ; 000 .
 Figure 5 sho ws the running time plot for Uniform 3D and Mixed 3D. We exp ected Uniform 3D to have extremely bad scaling performance because it has no true outliers as the probabilit y densit y is constan t across the entire space. In-creasing N simply increases the densit y of poin ts and drops the cuto score but does not rev eal rare outliers. In con trast, the results for Mixed3D were extremely good ( b = 1 : 11). In this data set, as we increase N we nd more extreme outliers from the Gaussian distribution and the cuto distance in-creases, thus impro ving pruning eciency . Finally , we note that data sets with a true uniform distribution are probably rare in real domains. Although the use of distance-based outliers is well estab-lished, in this section, we sho w results from the census data to giv e the readers a qualitativ e idea of the typ es of outliers found when large data sets are mined. We also compare the disco vered outliers with examples agged as unusual by GritBot, a commercial program from RuleQuest Researc h that was designed to nd anomalies in data [21].
 As we have limited space in this pap er, we presen t only selected results. The full list of outliers on the Household and Person data sets for both our algorithm and GritBot are available online 1 and we encourage the readers to view this list directly .
 We emphasize that we are not claiming that one set of results is better than another, but rather we feel these results sho w that distance-based outlier detection nds unusual examples of a qualitativ ely di eren t nature than GritBot. We rep ort selected results from running our outlier detection algorithm on the full set of 5 million examples to return the top 30 outliers with k = 5.
 The top outlier in the household database is a single fam-ily living in San Diego with 5 married couples, 5 mothers, and 6 fathers. In the census data, a family is de ned as a group of persons related by blo od, adoption, or marriage. To be considered a mother or father, the person's child or children must be presen t in the household. The house had a rep orted value of $85K and was mortgaged. The total re-ported income of the household was appro ximately $86K for the previous year.
 Another outlier is a single-family rural farm household in Florence, South Carolina. The house is owned free and clear by a married couple with no children. This example is un-usual because the value of the house is greater than $400K (not including the land), and they rep orted a household in-come of over $550K.
 In the person data set one of the most extreme outliers was a 90+ year old Blac k Male with Italian ancestry who does not speak English, was enrolled in school 2 , has a Do ctorate degree, is emplo yed as a bak er, rep orted $110K income of whic h $40K was from wages, $20K from business, $10K from farming, $15K from welfare, and $20K from investmen ts, has a disabilit y whic h limits but does not prev ent work, was a veteran of the U.S. armed forces, tak es public transp orta-tion (ferry boat) to work, and immigrated to the U.S. 11-15 years ago but mo ved into his curren t dw elling 21-30 years ago. Clearly , there are inconsistencies in this record and we believ e that this record represen ts an improp erly completed form. http://www.isle.o rg/ sba y/pap ers/kdd03/
Taking a course that a high school or college would accept for credit would coun t under Census de nitions. 3D ( b = 1 : 11 ).
 A second outlier was a 46 year old, White, wido wed female living with 9 family mem bers, two of whic h are her own chil-dren. She has a disabilit y that limits but does not prev ent her work as a bookk eep er or accoun ting clerk in the theater and motion picture industry . She tak es public transp orta-tion to work (bus or trolley) and it tak es her longer than 99 min utes to go from home to work.
 A third outlier was a 19 year old, White, female with Asian ancestry and Mexican Hispanic origin with a disabilit y that limits but does not prev ent work. She earned $123K in business income, and $38K in retiremen t income (whic h ma y include paymen ts for disabilities), and is also enrolled in school. GritBot nds records that have a surprising value on one attribute giv en the values of other attributes. For example, an outlier GritBot found on the Person data set was case 481942: This means that 98.94% of people who have African Amer-ican ancestry and who speak English, listed their race as Blac k. Case 481942 is unusual because the race listed was White.
 We were not able to run GritBot on the household and per-son data sets with ve million examples because of memory limitations. GritBot's requiremen ts exceeded the available main memory as it loaded the entire data set and then allo-cated additional memory during program execution. Ho w-ever, we were able to run GritBot on smaller data sets, and speci cally , we ran GritBot using the default settings on appro ximately one million household records and one half million person records.
 Since GritBot and our algorithm compute two di eren t sets of outliers, precise comparisons of their running times are not very meaningful. Ho wever, to giv e the reader a rough idea of their performance, GritBot took appro ximately 70 min utes to pro cess one million household records and 170 min utes to pro cess one half million person records on a 600 MHz MIPS R14000 with 4 GB of memory . In comparison, our algorithm took 87 and 18 min utes resp ectiv ely to pro cess similar amoun ts of data on a 1.5 GHz Pentium 4 with 1 GB of memory . 3 In con trast to the results from distance-based outliers, Grit-Bot found qualitativ ely di eren t outliers. For example, on the household data GritBot found a total of 266 anomalies. These anomalies could be divided into roughly three groups: On the person data set, GritBot found a total of 1407 anoma-lies. Unlik e the household data, we could not place the ex-amples into neat categories, but as before GritBot found records with unusual com binations of attributes whic h in-cluded
The data sets were not exactly iden tical as they con tained di eren t samples of Census records. In general, GritBot tended to nd examples in whic h a small num ber of attributes made the example unusual. This is not surprising as by default GritBot is set to examine four or less conditions. Ho wever, GritBot often did not use all four conditions and man y outliers had only one or two terms. The main goal of our exp erimen tal study was to sho w that our algorithm could scale to very large data sets. We sho wed that on large, real, high-dimensional data sets the algorithm had near linear scaling performance. Ho wever, the algorithm dep ends on a num ber of assumptions, violations of whic h can lead to poor performance.
 First, our algorithm assumes that the data is in random order. If the data is not in random order and is sorted then the performance can be poor. For example, the Census data as retriev ed from the IPUMS rep ository [20] came with the examples sorted by state. This can cause problems when our algorithm considers a person from Wy oming. It will try to eliminate it by nding the k nearest neigh bors who are also likely to be from Wy oming. To nd these neigh bors, the algorithm will rst scan all examples from states Alabama to Wisconsin giv en the sequen tial manner in whic h it accesses the data.
 Second, our algorithm dep ends on the indep endence of ex-amples. If examples are dep enden t in suc h a way that they have similar values (and will likely be in the set of k near-est neigh bors) this can cause performance to be poor as the algorithm ma y have to scan the entire data set to nd the dep enden t examples.
 An extreme version of this problem can occur when the data set originates from a attened relational database For exam-ple, if there are two tables X and Y , with eac h example in X poin ting to sev eral di eren t ob jects in Y , our attened database will have examples with form ( X 1 ;Y 1 ), ( X 1 ( X 1 ;Y 3 ), ( X 2 ;Y 4 ), ::: and so forth. As it is likely that the closest neigh bors of ( X 1 ;Y 1 ) will be the examples ( X and ( X 1 ;Y 3 ) our algorithm ma y have to scan the entire data set until it nds them to obtain a low score.
 Ho wever, our algorithm ma y still perform acceptably on data sets with less sev ere violations. For example, the exam-ples in the Person data set are not completely indep enden t as they are tied together by a common household. 4 Ho w-ever, the performance on this data set ( b = 1 : 16) was still very good.
 The third situation when our algorithm can perform poorly occurs when the data does not con tain outliers. For exam-ple, our exp erimen t with the examples dra wn from a uniform distribution had very poor scaling. Ho wever, we believ e data sets of this typ e are likely to be rare as most physical quan-tities one can measure have distributions with tails. We are interested in extending our work in this pap er in sev eral ways. First, we are interested in speeding up the algorithm even further. In Section 4 we sho wed that the scaling performance dep ended on how the cuto changes as we pro cess increasingly larger data sets. The algorithm starts with a cuto threshold of zero whic h increases as bet-ter outliers are found. One mo di cation is to start the al-gorithm with a pre-de ned cuto threshold belo w whic h we would consider any example to be unin teresting. In prelim-inary exp erimen ts, a good initial guess could cut time to a third. There ma y also be automatic ways to get a good cuto early . For example, we could rst pro cess the exam-ples with a small data set to get an idea of the examples that are most unusual. We then place these examples at the beginning of the data le.
 Another pressing limitation is that our work has only ad-dressed nding outliers in the data sets that can be repre-sen ted with a vector space or equiv alen tly a single table in a database. Man y real data sources will be in the form of re-lational databases with multiple tables that relate di eren t typ es of information to eac h other.
 To address relational data, the simplest solution is to at-ten the database with join operators to form a single table. While this is a con venien t solution it loses much of the infor-mation available. For instance, a attened database cannot easily represen t households that have a variable num ber of individuals. We also found that attening a database could create dep endencies between examples and, as we explained above, this can reduce the e ectiv eness of randomization and pruning.
 We are curren tly investigating how we can extend our al-gorithm to handle relational data nativ ely . There are two researc h questions that arise. First, how does one de ne a distance metric to compare ob jects whic h ma y have a vari-able num ber of link ed ob jects? There has been some work on de ning metrics for relational data [6, 9, 15]. The cen tral idea is to apply a recursiv e distance measure. That is, to compare two ob jects one starts by comparing their features directly , and then mo ves on to compare link ed ob jects and so on. Second, how does one ecien tly retriev e an ob ject and its related ob jects to compare them in the con text of searc hing for outliers? Retrieving related ob jects ma y in-
The Census micro data is based on cluster samples, i.e., the samples are made of households or dw ellings from whic h there ma y be multiple individuals. Individuals from the same household are not indep enden t. volv e extracting records in a non-sequen tial order and this can greatly slo w database access.
 Finally , there are man y practical issues with algorithms for mining distance-based outliers that we did not investigate suc h as determining how to set algorithm parameters suc h as k , the blo ck size, the distance measure, and the score function. Eac h of these parameters can have a large e ect on the disco vered outliers (or running time for the blo ck size). In sup ervised classi cation tasks one can set these param-eters to maximize predictiv e performance by using a hold out set or cross-v alidation to estimate out of sample perfor-mance. Ho wever, outlier detection is unsup ervised and no suc h training signal exists. In our work applying outlier detection algorithms to large, real databases a ma jor limitation has been scaling the al-gorithms to handle the volume of data. In this pap er, we addressed the scaling problem with an algorithm based on randomization and pruning whic h nds outliers on man y real data sets in near linear time. This ecien t scaling al-lowed us to mine data sets with millions of examples and man y features. We thank Thomas Hink e and Da vid Roland of NASA Ames for reviewing a draft of this pap er. This work was supp orted by the CICT Program at NASA Ames Researc h Cen ter un-der gran t NCC 2-5496. [1] C. C. Aggarw al and P. S. Yu. Outlier detection for [2] F. Angiulli and C. Pizzuti. Fast outlier detection in [3] V. Barnett and T. Lewis. Outliers in Statistic al Data . [4] J. L. Ben tley . Multidimensional binary searc h trees [5] S. Berc htold, D. Keim, and H.-P . Kreigel. The X-tree: [6] G. Bisson. Learning in FOL with a similarit y measure. [7] R. J. Bolton and D. J. Hand. Statistical fraud [8] M. M. Breunig, H. Kriegel, R. T. Ng, and J. Sander. [9] W. Emde and D. Wettsc herec k. Relational [10] E. Eskin, A. Arnold, M. Prerau, L. Portno y, and [11] E. Fix and J. L. Ho dges. Discriminatory analysis: [12] R. Guttmann. A dynamic index structure for spatial [13] D. Ha wkins. Identi c ation of outliers . Chapman and [14] S. Hettic h and S. D. Ba y. The UCI KDD arc hiv e. [15] T. Horv ath, S. Wrob el, and U. Bohneb eck. Relational [16] E. M. Knorr and R. T. Ng. Finding intensional [17] E. M. Knorr, R. T. Ng, and V. Tucak ov.
 [18] T. Lane and C. E. Bro dley . Temp oral sequence [19] S. Ramasw am y, R. Rastogi, and K. Shim. Ecien t [20] S. Ruggles and M. Sob ek. Integrated public use [21] Rulequest Researc h. Gritb ot.
