 Most existing recommender systems focus on modeling the rat-ings while ignoring the abundant information embedded in the re-view text. In this paper, we propose a unified model that com-bines content-based filtering with collaborative filtering, harness-ing the information of both ratings and reviews. We apply topic modeling techniques on the review text and align the topics with rating dimensions to improve prediction accuracy. With the in-formation embedded in the review text, we can alleviate the cold-start problem. Furthermore, our model is able to learn latent top-ics that are interpretable. With these interpretable topics, we can explore the prior knowledge on items or users and recommend completely  X  X old X  items. Empirical study on 27 classes of real-life datasets show that our proposed model lead to significant im-provement compared with strong baseline methods, especially for datasets which are extremely sparse where rating-only methods can-not make accurate predictions.
 H.3.3 [ Information Storage and Retrieval ]: Information filtering; I.2.6 [ Artificial Intelligence ]: Learning-parameter learning Collaborative Filtering; Content-based Filtering; Cold-start Prob-lem
With the ever growing number of choices available online, rec-ommender systems are becoming more and more indispensable. We rely on them to select favorite songs from millions of collec-tions in music streaming services like Spotify and iTunes Radio. We depend upon them to suggest interesting movies in movie rat-ing website such as IMDb and video streaming providers such as Netflix. Amazon uses recommender systems to suggest products to potential users. Now the company even takes it to another level by Figure 1: Percentage of items having less than 10 ratings and more than 30 words in various Amazon datasets shipping the products to the warehouse near the customer based on speculated order produced by recommender systems 1 .

Although recommender systems employed in industry seem to perform well in practice, there are some deficiencies with exist-ing approaches. The first problem confronted with most recom-mender system is their inability to deal with so called cold-start problem [28]. When a new user joins a recommender system, there is little data available for the system to learn the preferences of the user accurately. Without an accurate representation of the user, the system cannot make recommendations confidently. Similarly, the systems defer the recommendations for newly included items as well. The cold-start problem leads to poor experience for new users and also when recommending new items. In real-life recommender systems, the cold-start problem is a severe problem. Shown in Fig-ure 1 are the statistics of 5 categories in Amazon datasets [16]. Across the 5 listed datasets, over 80% of items have few ratings (less than 10). While at the same time, over 70% of items have review text at length (over 30 words). The ratings alone are in-adequate to learn the preferences accurately. Review comments complement the ratings by providing rich knowledge of the items and preferences of the users. Harnessing the information embed-ded in the review text is the key to successful recommendation in such scenarios.

Another drawback of existing recommender systems is their poor interpretability, making further understanding of users X  preference as well as items X  properties impossible. For example in matrix-factorization [25] based methods, we learn two latent feature ma-trices corresponding to users X  latent features and items X  latent fea-tures. The dot product between a user X  X  and an item X  X  feature vector is used to predict the rating that the user would assign to the item. It is challenging to associate these real valued features with con-http://on.rt.com/c8v82n ceivable physical meanings. We know that a user might like an item due to a particular latent feature since they both have a large positive (or negative) value on that feature. But we have no clue of the feature X  X  physical meaning. Does it mean that the user is fond of Sci-Fi and the movie belongs to Sci-Fi genre? Or is it that the user loves the leading actor of the movie? We do not know. In fact, it is possible that each feature corresponds to a combination of the human interpretable features, rendering the feature interpretation problem more difficult.

Both of the above problems can be solved or at least alleviated by combining content-based filtering and collaborative filtering . In collaborative filtering, we make predictions on a user X  X  preferences over items based on all users X  past ratings. Collaborative filter-ing rooted in the keen observation that users who shared similar preferences in the past tend to rate similarly in the future. A col-laborative filtering model uses only the past rating information and does not take the contents of the items into consideration. On the other hand, content-based filtering approaches the recommendation problem by analyzing the content of the items and matches it with the preference of a user.

In a recommender system, apart from an integer score, users are often allowed to write text reviews about the item to complement the rating. The review text contains a source of rich information explaining the reason why the user assigns such a rating to the item. These reviews provide text contents of the items, which can be leveraged to alleviate cold-start problem when the ratings are sparse. This is because the information embedded in the text review is much richer that an integer rating. When we have few ratings, it is nearly impossible to learn an accurate feature of the concerned user/item. However, the text review might allow us to better es-timate the features. To solve the interpretation problem, we align latent topic spaces with the rating spaces. Each latent dimension is tagged with a word cloud, explaining the physical meaning of the dimension. For example, when we see that a user and a movie have large positive value on the third feature, which has text la-bel  X  X hriller, sci-fi, nolan X , we know that this user likes the science fiction thriller movie directed by Christopher Nolan.

Interpretability and the cold-start problem are not two isolated problems. Learning an interpretable model could help alleviate the cold-start problem [1, 13]. We can leverage prior knowledge of items and suggest completely  X  X old X  items with confidence. For example, if we know that a user assigns high scores for the topic tagged with  X  X antasy, adventure, peter, jackson X , a recommender system can confidently recommend  X  X he Hobbit: There and Back Again X  (a fantasy adventure movie directed by Peter Jackson) to the user even if this movie is not being shown yet.

The contribution of this paper is three-fold. First, we propose a novel method to combine content-based filtering seamlessly with collaborative filtering, modeling the reviews and ratings simultane-ously. Secondly, we derive an efficient collapsed Gibbs sampling method to learn the model. Thirdly, we demonstrate our model X  X  advantage in prediction accuracy compared with previous work, es-pecially under the cold-start setting, on large real-life datasets with millions of users and items. We also show the interpretability of the model using a few examples.
Recommender systems can broadly be classified into two types: content-based filtering [21] and collaborative filtering . Content-based filtering recommends an item to a user by matching up the features of the item with the preferences of the user, both of which are learnt by analyzing the contents and profiles. Collaborative filtering (CF), on the other hand, approaches the recommendation problem by analyzing the co-occurrence patterns of user-item pair, which is often attached with an integer rating. There are extensive investigations on collaborative filtering, from neighborhood-based methods [27, 11] to model-based methods [9, 26]. Recently, some methods concentrated on ranking [10, 24] the items better. Other approaches leveraged social [14, 15] and side information [32] to improve the performance.

Due to the advantage of taking the review text into considera-tion, there are a few efforts [2, 4, 16, 17] explored the combina-tion of content-based filtering and collaborative filtering. In the early work [17], the authors cast the content-based filtering as a classification problem, using which they filled out some of the un-observed user item rating matrix and apply collaborative filtering methods on this denser matrix. The authors of [2] cast the recom-mendation problem as an ordinal regression problem and applied a combination of kernels to handle the side information. In [4], the authors found that there were a few aspects that affect how users rate items. They harnessed the information embedded in the review text to learn how a user weights these aspects and how an item dis-tributes on these aspects. However, their method required human annotators with expert domain knowledge to pre-define these as-pects rather than learning them automatically from the reviews.
In the recent work [16], the authors proposed the Hidden Factors and Hidden Topics (HFT) model, which learnt a Latent Dirichlet Allocation (LDA) [3] model for items using the review text and a matrix factorization model to fit the ratings. To bridge the gap between the stochastic vector obtained from LDA and the real-valued vector in MF model, the authors proposed a transforma-tion to link the two. Their method demonstrated significant im-provement over baseline methods that use ratings or reviews alone. However, the transformation function they employ, the exponential function, fixed the relationship between latent vector in MF and the topic distribution. Although a parameter is employed to maintain a more flexible relationship, it is still difficult to ensure that this transformation is correctly scaled.

In [8, 29, 30], the authors also considered the interpretable as-pects to make better predictions. However, their approaches differ from ours in that either they had explicit ratings per aspect (i.e., multiple ratings on prescribed aspects per user item pair), or these aspects were inferred from other context than review text. In an-other line of research called sentiment analysis [6, 12], a positive or negative label rather than an integer score is learnt for a short text. Our work also differs from [18], which recommends person-alized reviews. In [31], the authors proposed Collaborative Topic Regression (CTR) to suggest scientific articles to potential readers. Later work [22] extended it to take the social network among users into consideration. As pointed out in [16], the latent dimensions they discovered were not necessarily correlated with ratings.
Our model, titled  X  X atings Meet Reviews X  (RMR), is a proba-bilistic generative model that combines a topic model seamlessly with a rating model. We describe it as follows. Suppose there are N users U = { u 1 ,u 2 ,  X  X  X  ,u N } , M items V = { v 1 ,v 2 ,  X  X  X  ,v M } , a set of observed indices Q = { i,j } , where { u i ,v j } X  X  X V defines the observed ratings X = { x each of which is optionally associated with a review r i,j V } of length L i,j , where V is the set of vocabulary used in the re-view text. Alternatively, let U j denote the indices of users who have rated item v j . Let K denote the number of topics.
RMR operates on items 2 , which has an intrinsic distribution  X  on topics. This distribution describes the proportion that the item belongs to each topic.

We present the generative process below: 1. For each user u  X  X  : 2. For each latent topic dimension k  X  [1 ,K ] : 3. For each item v  X  X  :
From the generative process, we can identify that the text re-views are generated similarly as the LDA model. We use a mixture of Gaussian rather than matrix factorization based methods [16, 31] to model the ratings. These user-topic specific Gaussian distribu-tions have clear interpretations. They describe how a user values the aspects denoted by each latent topic. The item is modeled as a distribution of topics, which together with the user-topic specific Gaussian distributions determine how a user would rate the item. The ratings and review text are connected by the same item topic distribution  X  . The more a user talks about certain aspects con-cerning an item, the higher the distribution will be on these topics, which in turn affects the rating that the user would assign to the item.

We choose to model ratings using mixture of Gaussians for two reasons. First, we can avoid the difficult choice of the transfor-mation function employed in [16]. As discussed above, the trans-formation function is restrictive and the scaling parameter is non-trivial to select. Secondly, we can retain the interpretability of the topics with no compromise. The interpretability of the latent di-mensions is an important factor to solve the cold start problem. Take book recommendation for example, when a user showed strong interest in dimension with high probability on words  X  X a vinci code dan brown X . We can confidently recommend Dan Brown X  X  new book  X  X nferno X . We are able to associate the latent dimensions with the prior knowledge (for example, Meta data) that is available with-out ratings or reviews.

Given the generative process, the probability of observing the re-view text and the ratings given the model parameters  X  = {  X , X , X  }
RMR is symmetrical in that the topic distribution  X  can also be user specific. We found, however, that item specific  X  performs better in practice. is P ( w , x |  X ;  X , X , X  0 , X  2 0 , X  2 ) =  X   X 
X If we take the log of Eq. (1), we get the log-likelihood of model pa-rameters. However, because of the summation inside the log, direct optimization is not feasible. We subsequently develop an efficient collapsed Gibbs sampling method to learn the model parameters in Section 3.3. We now take a deeper look at RMR and compare it with HFT and CTR.
Shown in Figure 2 are the graphical models of RMR and several related work. As is clear from the figure, the left parts of CTR, HFT and RMR resemble LDA, which was originally proposed by David Blei et al. to learn the latent topics in a corpus of documents (items in our setting) in an unsupervised manner. The LDA algo-rithm assumes there are K latent topics in the corpus, which are K multinomial distributions over the vocabulary. Each document in the corpus is a mixture of these topics. A document specific topic distribution over the K latent topics,  X  j with Dirichlet prior  X  , gov-erns how much weight each topic takes in document j . This  X  a length-K stochastic vector with non-negative entries which sums up to 1 .

Both HFT and CTR adopt the matrix factorization method to model the ratings. Arrange users X  ratings on items in a partially observed matrix X  X  R N  X  M . The matrix factorization model as-sumes that X has a low rank structure and thus can be decomposed in to the product of two matrices U T V , where both U and V are of rank K min ( M,N ) . The columns of U and V can be in-terpreted as the latent features of users and items respectively. The dot product between a user X  X  feature vector and an item X  X  feature vector approximates the rating that the user would assign to the item. To regularize the value that the latent features can assume, often zero-mean isotropic Gaussian priors are placed on both the user and item latent features. The objective function of a matrix factorization model can be formulated as follows, in which the first term is the difference between observed and pre-diction and the rest are regularization terms.

Clearly, there is a discrepancy between the item topic distribu-tion  X  j in LDA and the item feature vector V j in MF model. The former is a distribution which is all positive and sums up to 1 while the later can assume any real value. Both HFT and CTR try to align the item features with the item topic distribution and hence the rich text review can be exploited to better model the item features. The main difference between HFT and CTR model lies in the way they align the topic distribution  X  j and the item feature V j item feature V j is assumed to be a Gaussian random variable with mean  X  j and precision c . In other words, the  X  j is taken as the de-fault value of V j , but the later can adapt to match the ratings. If an item receives a lot of ratings, it is possible that V j  X  significantly. The interpretation of the latent topics in such case (c) HFT might be distorted. CTR model was proposed to recommend scien-tific articles to potential readers, which is a one-class collaborative observed ratings ( X i,j = 1 ) and unobserved ratings ( X In our setting, we try to match only the observed ratings with the given rating scales (for example, 1 to 5 ). On the other hand, HFT adopts a fixed transformation function to map one-to-one between  X  and V j . In Figure 2(c), we use a dashed line to represent this relationship. The HFT model is effectively a matrix factorization model with the item feature regularization replaced by the corpus likelihood. This fixed transformation function is difficult to select and restrictive in modeling capability.

RMR adopt a mixture of Gaussian to model the ratings. The mixture proportion is assumed to have the same distribution as the topic distribution. Thus when there are few ratings for an item, the text review can still allow us to learn the topic distribution  X  accu-rately. We avoid the difficult choice of the transformation function in HFT and retain the interpretability of the latent topics.
To develop a Gibbs sampler for RMR, we need to specify the conditional probability of the hidden variables z and f , which are the hidden topics associated with the observed words w and ob-served ratings x , P ( z , f | w , x ) . This conditional probability does not have a closed form and is difficult to sample directly. The col-lapsed Gibbs sampler runs a Markov chain that uses the full con-ditional in order to simulate it. In our case, we need to specify the following two conditional probabilities We will briefly derive the expression for the second probability in Eq. (3). Using Bayes X  theorem and the conditional independence, we obtain Now we will derive the expression for the two terms in Eq. (4). Let the index i denote the rating assigned by user u to item v . The second term in Eq. (5) is a Gaussian distribution, because Since P (  X  u,j ) is Gaussian N (  X  0 , X  2 0 ) and conjugate to P ( x the posterior distribution P (  X  u,j | f  X  i , x  X  i ) will be Gaussian N (  X  where The predictive posterior in Eq. (5) is Gaussian N (  X  i , X  Similarly, the expression for the second term in Eq. (4) is of which the second term is Again, since P (  X  v ) is Dirichlet(  X  ) and conjugate to P ( f P ( z |  X  v ) , the posterior is also a Dirichlet distribution and the pos-terior predictive of Eq. (9) is Combine the result in Eq. (5) and Eq. (9), we get the expression for the full conditional for the first probability in Eq. (3) and by employing a similar procedure, we get the expression for the first probability in Eq. (3) We summarize the notations used in the derivation process in Ta-ble 1. Note that we omit the  X  i subscription in some of the nota-tions to save space. With this notation, it means that when counting the respective values, we exclude current word w i or current rating x .
Once the sampling process is finished, we can readily readout the model parameters and The notations denote the same meaning as is in Table 1, except that the counters now count all effective samples.
Our collapsed Gibbs sampler mainly use the following coun-ters to keep track of current states: counter n v z,j of size M  X  K , counter n v f,j of size M  X  K , counter n w  X  i,j of size V  X  K , counter P than the above listed counters, there are summation counters that are one order smaller than the above counters and thus negligible. So the total Space complexity is O (( M + N + V )  X  K ) .
To sample z or f conditioned on everything else, we only need to calculate the conditional probabilities in Eq. (3). This operation requires O ( K ) operations. Given a fixed K , our sampler scales linearly with the length of the observed review text and the number received ratings. Usually the number of latent topics K is small, making our sampler scales up well. Note that training a RMR model is faster than training a HFT model [16]. This is because the later one requires an additional step to learn the feature vectors for all the users.
In order to make a prediction that user u assigns to item v , we compute the expected value In practice, we compute the empirical global mean g , user bias b and item bias b v for all users and all items from the training set. We feed the x 0 u,v = x u,v  X  g  X  b u  X  b v to the sampler and when making predictions, add g , b u , and b v back.
We conduct an empirical study of RMR and various baseline models to show the following facts: 1. Our model leads to significant improvement on prediction ac-2. Our model learns latent topic dimensions that are clearly in-3. Our model performs better in datasets which are extremely
We use the Amazon Review dataset collected by [16]. This dataset is a collection of 27 datasets corresponding to various types of items that are available on Amazon 3 . This is the largest rat-ing dataset with text reviews publicly available, to the best of our knowledge. We show the statistics of the datasets in Table 2. Refer to Table 2; there are two facts that are evident immediately. First, the datasets are extremely sparse. The sparseness would clearly de-teriorate the performance of most existing recommender systems which only model the ratings. Secondly, a review contains 116.87 words on average across all categories. As will be apparent in the results shown later, these review texts are key to model the ratings accurately. We compare our model with four baseline models MF, LDAMF, CTR and HFT.
The statistics of category Baby we calculated differs from the de-scription provided on the webpage and we exclude it from consid-eration. We use Mean Squared Error (MSE) to evaluate various models. For each of the dataset, we randomly select 80% as training set up to 2 million reviews. The remaining reviews are split evenly into validation set and testing set. The initial latent variables z and f are uniformly randomly assigned. We run 2500 iterations with a thinning of 50 iterations to get samples and MSE readout. We report the MSE of the testing set which has the lowest MSE on the validation set. The training of the baseline methods MF, LDAMF, CTR and HFT follow the same routine described in [16]. We use K = 5 for all models. We set hyperparameters 4  X  = 0 . 1 ,  X  = 0 . 02 ,  X  0 = 0 ,  X  2 0 = 1 and we use the empirical variance of x as  X  2 . In practice, the time required to train the RMR model is about half the time spend on training the HFT model on the same machine.
Shown in Table 3 are the MSE results. The best MSE of each dataset is in bold. We listed the performance of various models on the datasets and the average improvement. The standard deviations of MSE results are shown in parenthesis. Out of the 27 datasets, RMR performs the best on 19 datasets among all considered meth-ods.
 Compared with matrix factorization (MF column in Table 3), RMR performs better on 26 out of the 27 datasets with an average improvement on MSE of nearly 8%. Matrix factorization method usually performs well in practice [10, 23] and is a strong baseline method. However, as is shown in our case, in datasets which are extremely sparse, MF is unable to learn an accurate representation of users/items and thus under-performs other methods which take the review text into consideration. However, in the datasets such as Music, Movies and TV and Books, which are relatively denser
We searched through the parameters linearly and reported hyper-parameters which performed the best. compared with other datasets; the MF method still performs very well.

The baseline method LDAMF, which was proposed as a base-line method in [16], is probably the simplest model that combines review text and ratings. This baseline method takes the item topic distribution produced by LDA as the feature vectors for the items and then learns the user feature vectors by fitting the observed rat-ings with item features fixed. The feature vectors of items are learnt using only the reviews, which might be sub-optimal to fit the rat-ing data. The expressiveness is thus restricted and we think this restriction caused the nearly 8% improvement produced by RMR.
Compared with CTR, which take the full advantage of the com-bined information of both the reviews and ratings, our proposed model still leads to an average improvement of 3.28% and performs better on 25 out of the 27 datasets. Similar to LDAMF, CTR takes the item topic distribution produced by LDA as the initial item fea-tures. However unlike LDAMF, during the training period, CTR alters both the user features and item features to fit the ratings. The regularization parameter  X  V controls how much the item features can deviate from the item topic distribution vectors. It performs better due to the more flexible modeling capability. However, the CTR does not perform as well as RMR in the extremely sparse datasets such as Arts and Jewelry. We observe that during the ex-periment, CTR can learn a model that fits the data with a small training error. But the generalization of the learnt model to the unobserved rating is not as good. Note that we report the perfor-mance of CTR on the test set by setting  X  V and  X  U to the value which gives best performance on validation set. So the issue of under-regularization is minimized. The performance of CTR on the relatively dense datasets is very competitive.

Compared with HFT, another recommendation method that takes review text into consideration, RMR is still able to improve the per-formance by 1.22% on average and performs better or equally well in 21 out of 27 datasets. As discussed in previous sections, we think
Figure 3: Gain in MSE for user with limited training data the fixed one-to-one mapping between the item topic distribution and item feature vector impose restrictions of the expressiveness of HFT and allow RMR to out-perform it. Due to large size of the datasets, the improvements reported are significant at 1% level. We consider the improvements of RMR over CTR (3.28%) and HFT (1.22%) significant because both of these two baselines are full-fledged models that take both the ratings and reviews into con-sideration. Also, these improvements are verified on 27 real-life datasets. In a real system where recommendation plays a central role, e.g. Amazon, Netflix, these improvements could lead to bet-ter revenue and profit.
An interesting phenomenon we found in the results is that the improvement of RMR over the traditional collaborative filtering methods (matrix factorization) is more significant for datasets that are sparse. For classes such as Arts, Industrial Scientific, RMR show substantial improvement. In such cases, the number of rat-ings is too scarce to model the items and users adequately. The text in the review associated with the ratings come as rescue, which al-low our model to learn a more accurate topic distribution. Whereas for classes such as Music, Movies and Books, which are the largest 3 datasets with larger reviews per user and reviews per item, the traditional methods tend to produce accurate predictions. We fur-ther verify this finding by comparing the performance of RMR with MF on users with limited training data. Shown in Figure 3 is the gain of RMR compared with MF for users with limited training items. We show the result on two datasets due to space limit and the phenomenon repeats across all the datasets. As we can see, our model gains the most when the user has few training items. The performance gain starts to decrease with the number of training items available for each user. This further demonstrates that RMR is valuable for the cold-start settings.
Apart from being more accurate at prediction, another advantage of RMR is that it learns interpretable latent topics. We show two examples of the top words in each topic learnt in RMR in Table 4 and Table 5. Table 4 shows the top words for topics learnt with software dataset. Note that Roxio is software for burning DVDs and Quicken is personal financial software. Leopard and Tiger are the code name of Mac OS X and Parallels is a popular virtual ma-chine on OS X. The fourth topic is about the company Microsoft and its products and the last topic is related to Linux. Table 5 shows the top words for topics learnt with Movie and TV dataset. The first topic is dedicated to workout related videos. The second topic con-tains commonly used words to describe TV series. Batman, Matrix trilogy, Alien and Harry Potter are either science fiction, adventure or fantasy movies. Godzilla is a disaster thriller and Hitchcock is a famous director of psychological thrillers. Nicole Kidman is the leading actress of the classic thriller  X  X yes Wide Shut X .
Clearly these interpretable topics would help us understand items and users better. For items, the top topic words can be employed as extended tags attached to the item and may improve the prediction accuracy in a tag-aware recommender system [5]. We may also gain better understanding of items by analyzing the topic distri-bution similarities. For users, once obtaining the topic preferences, we can recommend  X  X old X  items which have few or no ratings to the users with confidence. For example, if we know that a user tends to rate high for topic three and five in Table 5, we can confidently recommend the movie  X  X nterstellar X  (a Sci-Fi Thriller movie) even if this movie is not being shown yet. Our prior knowledge of items therefore can help alleviate the cold-start problem.
In this paper, we propose a model that combines content-based filtering with collaborative filtering seamlessly. By exploiting the information in both ratings and reviews, we are able to improve the prediction accuracy significantly across various classes of datasets over existing strong baseline methods, especially under the cold-start settings where the data are extremely sparse. We develop an efficient collapsed Gibbs sampler for learning the model parame-ters. Our model also learns topics that are interpretable, enabling us to exploit prior knowledge to alleviate the cold start problem. We plan to explore RMR X  X  ability in discovering user communities and new genres in future work.
 The work described in this paper was fully supported by the Na-tional Grand Fundamental Research 973 Program of China (No. 2014CB340401 and No. 2014CB340405), the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK 413212 and CUHK 415113), and Microsoft Research Asia Regional Seed Fund in Big Data Research (Grant No. FY13-RES-SPONSOR-036).
