 Sparseness of user-to-item rating data is one of the major factors that deteriorate the quality of recommender system. To handle the sparsity problem, several recommendation techniques have been proposed that additionally consider auxiliary information to im-prove rating prediction accuracy. In particular, when rating data is sparse, document modeling-based approaches have improved the accuracy by additionally utilizing textual data such as reviews, ab-stracts, or synopses. However, due to the inherent limitation of the bag-of-words model, they have difficulties in effectively utiliz-ing contextual information of the documents, which leads to shal-low understanding of the documents. This paper proposes a novel context-aware recommendation model, convolutional matrix fac-torization (ConvMF) that integrates convolutional neural network (CNN) into probabilistic matrix factorization (PMF). Consequently, ConvMF captures contextual information of documents and further enhances the rating prediction accuracy. Our extensive evaluations on three real-world datasets show that ConvMF significantly out-performs the state-of-the-art recommendation models even when the rating data is extremely sparse. We also demonstrate that Con-vMF successfully captures subtle contextual difference of a word in a document. Our implementation and datasets are available at http://dm.postech.ac.kr/ConvMF.
 Collaborative Filtering; Document Modeling; Contexual Informa-tion
The exploding growth of the number of users and items in e-commerce services increases the sparseness of user-to-item rating data. Eventually, this sparsity deteriorates the rating prediction ac-curacy of traditional collaborative filtering techniques [5, 8]. To en-hance the accuracy, several recommendation techniques had been proposed that consider not only rating information but also auxil-Corresponding author iary information such as demography of users, social networks, and item description documents [14, 15, 17, 19, 23, 24].

Recently, approaches based on document modeling methods such as Latent Dirichlet Allocation (LDA) and Stacked Denoising Auto-Encoder (SDAE) have been proposed to additionally utilize item description documents such as reviews, abstracts, or synopses [15, 17, 23, 24]. Specifically, Wang et al. proposed collaborative topic regression (CTR) that combines topic modeling (LDA) and col-laborative filtering in a probabilistic approach [23]. Variants of CTR were proposed, which also integrates LDA into collabora-tive filtering to analyze item description documents with differ-ent integration approaches [15, 17]. Most recently, Wang et al. proposed collaborative deep learning (CDL) that integrates SDAE into probabilistic matrix factorization (PMF) [20], thereby gener-ating more accurate latent model in terms of the rating prediction accuracy [24].

However, existing integrated models do not fully capture doc-ument information, as they assume the bag-of-words model that ignores contextual information of documents such as surrounding words and word orders. For example, suppose that the following two sentences are given in a document:  X  people trust the man.  X ,  X  people betray his trust finally.  X  Since LDA and SDAE consider the document as a bag of distinguished words, they cannot distin-guish each occurrence of term  X  trust  X . Precisely, although each oc-currence of  X  trust  X  seems to have almost the same meaning, there is a subtle syntactic difference between these words  X  a verb and a noun, respectively. Such subtle difference within a document is also a nontrivial factor for deeper understanding of the document, and further such understanding facilitates improvements in the rat-ing prediction accuracy.

To address the aforementioned issue, we utilize convolutional neural network (CNN), which is the state-of-the-art machine learn-ing methodology that shows high performance for various domains such as computer vision [13], natural language processing (NLP) [2, 10, 11], and information retrieval [4, 21]. CNN effectively captures local features of images or documents through modeling components such as local receptive fields, shared weights, and sub-sampling [13]. Thus, the use of CNN facilitates deeper understand-ing of documents, and generates better latent model than LDA and SDAE do, especially for items that resort to their description docu-ments due to the lack of ratings. Moreover, CNN is able to take ad-vantage of pre-trained word embedding models such as Glove [18] for deeper understanding of item description documents. Note that LDA and SDAE cannot exploit pre-trained word embedding mod-els because they adopt bag-of-words model.

However, existing CNNs are not suitable for recommendation task, as their objectives are different from the objective of recom-mendation. Specifically, conventional CNNs mainly solve classifi-cation task that is to predict labels of words, phrases, or documents. On the contrary, the objective of recommendation is regarded as a regression task aiming at accurately approximating ratings of users on items. Thus, the existing CNNs cannot be directly applied to our task of recommendation.

To handle the technical issue, we propose a document context-aware recommendation model, convolutional matrix factorization (ConvMF), which captures contextual information of item descrip-tion documents by utilizing convolutional neural network (CNN) and further enhances the rating prediction accuracy. Precisely, Con-vMF seamlessly integrates CNN into PMF, which is commonly used for recommendation tasks. Consequently, the integrated model follows the recommendation objective, and eventually effectively utilizes both collaborative information and contextual information. As a result, ConvMF accurately predicts unknown ratings even when the rating data is extremely sparse.

To demonstrate the effectiveness of ConvMF, we evaluate Con-vMF on three different real-world datasets. Our extensive exper-iments over various sparsenesses of rating datasets demonstrate that ConvMF significantly outperforms the state-of-the-art mod-els. The superiority of ConvMF verifies that ConvMF generates item latent models that effectively reflect contextual information of item description documents even when the rating data is extremely sparse. We also qualitatively demonstrate that ConvMF indeed captures subtle contextual differences of a word in a document. Furthermore, we investigate whether pre-trained word embedding model helps improve the rating prediction accuracy of ConvMF. Detailed experiment results are also available at http://dm.postech. ac.kr/ConvMF besides our implementation and datasets.

Our contributions are summarized as follows.
The remainder of the paper is organized as follows. Section 2 briefly reviews preliminaries on the most representative collabora-tive filtering technique and CNN. Section 3 introduces an overview of ConvMF, explains our CNN architecture of ConvMF, and de-scribes how to optimize ConvMF. Section 4 experimentally evalu-ates ConvMF and discuss the evaluation results. Section 5 summa-rizes our contributions and gives future work.
In this section, we briefly review matrix factorization (MF) (the most popular collaborative filtering technique) and convolutional neural network (CNN).
Traditional collaborative filtering techniques are categorized into two categories [5]: memory-based methods (e.g. nearest neigh-borhood) [3, 7, 12] and model-based methods (e.g. latent factor model) [12, 20]. In general, model-based methods are known to generate more accurate recommendation results [12]. Thus in this section, we describe MF, which is the most popular model-based method.

The goal of MF is to find latent models of users and items on a shared latent space in which the strengths of user-item relation-ships (i.e., rating by a user on an item) are computed by inner prod-ucts [12]. Suppose that we have N users, M items and a user-item rating matrix R  X  R N  X  M . In MF, the latent models of user i and item j are represented as k -dimensional models, u i  X  v  X  R k . The rating r ij of user i on item j is approximated by the inner-product of corresponding latent models of user i and item j (i.e. r ij  X   X  r ij = u T i v j ). A general way of training latent models is to minimize a loss function L , which consists of sum-of-squared-error terms between the actual ratings and the predicted ratings and L 2 regularized terms that try to avoid the over-fitting problem as follows: L = where I ij is an indicator function such that it is 1 if user i rated item j and 0 otherwise.
Convolutional neural network (CNN) is a variant of feed-forward neural networks with the following components: 1) convolution layer for generating local features , 2) pooling (or sub-sampling) layer for representing data as more concise representation by se-lecting only several representative local features (i.e., features hav-ing the highest score via the activation functions) from the previous layer, which is usually a convolution layer.

Even though CNN has been originally developed for computer vision [13], the key idea of CNN has been actively applied to in-formation retrieval and NLP such as search query retrieval [4, 21], sentence modeling and classification [10, 11], and other traditional NLP tasks [2]. Although CNN for NLP tasks requires a significant amount of modification on the architecture of CNN, it eventually helps enhance the performance of various NLP tasks.

However, CNN has not yet been actively adopted to the field of recommender system. To the best of our knowledge, van den Oord et al . were first to apply CNN to music recommendation [22], where they analyzed songs in acoustic analysis point of view via CNN, and proposed a model that predicts the ratings based on the item latent model obtained by acoustic CNN. However, their CNN model, designed for acoustic signal processing, is not suitable for processing documents. Documents and acoustic signals have an in-herent difference on the quality of surrounding features. A signal at a certain time is inherently similar to its surrounding signals, i.e., the signals that have slight time difference, while a word at a certain position in the document has a large semantical difference from the surrounding words. Such difference in the degree of similarity be-tween surrounding features affects the quality of local features, and eventually requires different CNN architectures. Furthermore, the model does not fully reflect the collaborative information. In par-ticular, the item latent models are mainly determined by the results of audio signal analysis via CNN rather than collaborative infor-mation. Thus, the performance of overall recommendation even does not achieve that of weighted matrix factorization (WMF) [9], which is one of the conventional MF-based collaborative filtering techniques dealing with implicit feedback dataset.
In this section, we provide details of the proposed model, con-volutional matrix factorization (ConvMF), through three steps: 1) We introduce the probabilistic model of ConvMF, and describe the Figure 1: Graphical model of ConvMF model: PMF part in left (dotted-blue); CNN part in right (dashed-red) key idea to bridge PMF and CNN in order to utilize both ratings and item description documents. 2) We explain the detailed ar-chitecture of our CNN, which generates document latent model by analyzing item description documents. 3) Finally, we describe how to optimize latent variables of ConvMF.
Figure 1 shows the overview of the probabilistic model for Con-vMF, which integrates CNN into PMF. Suppose we have N users and M items, and observed ratings are represented by R  X  R matrix. Then, our goal is to find user and item latent models ( U  X  R k  X  N and V  X  R k  X  M ) whose product ( U T V ) reconstructs the rating matrix R . In probabilistic point of view, the conditional dis-tribution over observed ratings is given by where N ( x |  X , X  2 ) is the probability density function of the Gaus-sian normal distribution with mean  X  and variance  X  2 , and I indicator function as mentioned in Section 2.1.

As a generative model for user latent models, we place conven-tional priori, a zero-mean spherical Gaussian prior on user latent models with variance  X  2 U .
However, unlike the probabilistic model for item latent models in conventional PMF, we assume that an item latent model is gen-erated from three variables: 1) internal weights W in our CNN X j representing the document of item j , and 3) epsilon variable as Gaussian noise, which enables us to further optimize the item latent model for the ratings. Thus, the final item latent model is obtained by the following equations. For each weight w k in W , we place zero-mean spherical Gaussian prior, the most commonly used prior.
Accordingly, the conditional distribution over item latent models is given by
Detail of W of CNN will be explained in Section 3.2 where X is the set of description documents of items. A document latent vector obtained from the CNN model is used as the mean of Gaussian distribution and Gaussian noise of the item is used as the variance of Gaussian distribution which plays an important role as a bridge between CNN and PMF that helps to fully analyze both description documents and ratings.
The objective of our CNN architecture is to generate document latent vectors from documents of items, which are used to compose the item latent models with epsilon variables. Figure 2 shows our CNN architecture that consists of four layers; 1) embedding layer, 2) convolution layer, 3) pooling layer, and 4) output layer. Embedding Layer The embedding layer transforms a raw document into a dense nu-meric matrix that represents the document for the next convolution layer. In detail, regarding the document as a sequence of l words, we represent the document as a matrix by concatenating word vec-tors of words in the document. The word vectors are randomly ini-tialized or initialized with pre-trained word embedding model such as Glove [18]. The word vectors are further trained through opti-mization process. Then, the document matrix D  X  R p  X  l becomes: where l is the length of the document, and p is the size of embed-ding dimension for each word w i .
 Convolution Layer The convolution layer extracts contextual features. As we X  X e dis-cussed in Section 2.2, documents are inherently different from sig-nal processing or computer vision in the nature of contextual in-formation. Thus, we use the convolution architecture in [2, 11] to analyze documents properly. A contextual feature c j i  X  tracted by j th shared weight W j c  X  R p  X  ws whose window size ws determines the number of surrounding words: where  X  is a convolution operator, b j c  X  R is a bias for W is a non-linear activation function. Among non-linear activation functions such as sigmoid, tanh and rectified linear unit (ReLU), we use ReLU to avoid the problem of vanishing gradient, which causes slow optimization convergence and may lead to a poor local minimum [16, 6]. Then, a contextual feature vector c j  X  R of a document with W j c is constructed by Eqn.(1): However, one shared weight captures one type of contextual fea-tures. Thus, we use multiple shared weights to capture multiple types of contextual features, which enable us to generate contex-tual feature vectors as many as the number n c of W c . (i.e. W where j = 1 , 2 ,...,n c ).
 Pooling Layer The pooling layer extracts representative features from the convo-lution layer, and also deals with variable lengths of documents via pooling operation that constructs a fixed-length feature vector. Af-ter the convolution layer, a document is represented as n tual feature vectors, where each contextual feature vector has vari-able length (i.e., l  X  ws + 1 contextual feature). However, such representation imposes two problems: 1) there are too many con-textual features c i , where most contextual features might not help enhance the performance, 2) the length of contextual feature vec-tors varies, which makes it difficult to construct the following lay-ers. Therefore, we utilize max-pooling, which reduces the repre-sentation of a document into a n c fixed-length vector by extracting only the maximum contextual feature from each contextual feature vector as follows. where c j is a contextual feature vector of length l  X  ws +1 extracted by j th shared weight W j c .
 Output Layer Generally, at output layer, high-level features obtained from the previous layer should be converted for a specific task. Thus, we project d f on a k -dimensional space of user and item latent models for our recommendation task, which finally produces a document latent vector by using conventional nonlinear projection: b
Eventually, through the above processes, our CNN architecture becomes a function that takes a raw document as input, and returns latent vectors of each documents as output: where W denotes all the weight and bias variables to prevent clut-ter and X j denotes a raw document of item j , and s j denotes a document latent vector of item j .
To optimize the variables such as user latent models, item latent models, weight and bias variables of CNN, we use maximum a posteriori (MAP) estimation as follows.
By taking negative logarithm on Eqn.(5), it is reformulated as follows.
 where  X  U is  X  2 / X  2 U ,  X  V is  X  2 / X  2 V , and  X  W is  X 
We adopt coordinate descent, which iteratively optimizes a latent variable while fixing the remaining variables. Specifically, Eqn.(6) becomes a quadratic function with respect to U (or V ) while tem-porarily assuming W and V (or U ) to be constant. Then, the opti-mal solution of U (or V ) can be analytically computed in a closed form by simply differentiating the optimization function L with re-spect to u i (or v j ) as follows. where I i is a diagonal matrix with I ij , j = 1 ,...,M as its diago-I and R j are similarly defined as I i and R i , respectively. Eqn.(8) shows the effect of document latent vector of CNN in generating the item latent model v j , where  X  V is a balancing parameter as in [23].

However, W cannot be optimized by an analytic solution as we do for U and V because W is closely related to the features in CNN architecture such as max-pooling layers and non-linear activation functions. Nonetheless, we observe that L can be interpreted as a squared error function with L 2 regularized terms as follows when U and V are temporarily constant.
 To optimize W , we use back propagation algorithm. (Recall that W is the weights and biases of each layer.)
The overall optimization process ( U,V and W are alternatively updated) is repeated until convergence. With optimized U , V , and W , finally we can predict unknown ratings of users on items: Recall that v j = cnn ( W,X j ) + j .
 Time Complexity Analysis For each epoch, all user and item latent models are updated in O ( k 2 n R + k 3 N + k 3 M ) , where n R is the number of observed ratings. Note that document latent vectors are computed while up-dating W . Time complexity for updating W is dominated by the computation of convolution layer, and thus all weight and bias vari-ables of CNN are updated in O ( n c  X  p  X  l  X  M ) . As a result, the total time complexity per epoch is O ( k 2 n R + k 3 N + k 3 M + n and this optimization process scales linearly with the size of given data.
In this section, we evaluate the empirical performance of Con-vMF on real-world datasets. Our extensive experiment results demon-strate that 1) ConvMF significantly outperforms other competitors when dataset is extremely sparse, 2) the improvements of ConvMF over the competitors increase further even when dataset becomes dense, 3) pre-trained word embedding model helps improve the performance of ConvMF when dataset is extremely sparse, 4) the best performing parameters verify that ConvMF well alleviates data sparsity, and 5) ConvMF indeed captures subtle contextual differ-ences. Datasets To demonstrate the effectiveness of our models in terms of rating prediction, we used three real-world datasets obtained from Movie-Lens 2 and Amazon 3 . These datasets consist of users X  explicit rat-ings on items on a scale of 1 to 5. Amazon dataset contains reviews on items as item description documents. Since MovieLens does not include item description documents, we obtained documents (i.e., plot summary) of corresponding items from IMDB 4 .

Similar to [23] and [24], we preprocessed description documents for all datasets as follows: 1) set maximum length of raw docu-ments to 300, 2) removed stop words, 3) calculated tf-idf score for each word, 4) removed corpus-specific stop words that have the document frequency higher than 0.5, 5) selected top 8000 distinct words as a vocabulary, 6) removed all non-vocabulary words from raw documents. As a result, average numbers of words per docu-ment are 97.09 on MovieLens-1m (ML-1m), 92.05 on MovieLens-10m (ML-10m) and 91.50 on Amazon Instant Video (AIV), respec-tively.

We removed items that do not have their description documents in each dataset, and specifically for the case of Amazon dataset, we removed users that have less than 3 ratings. As a result, statistics of each data show that three datasets have different characteristics (Table 1). Precisely, even though several users are removed by pre-processing, Amazon dataset is still extremely sparse compared to the others.
 Competitors and Parameter Setting We compared two versions of ConvMF with the following base-lines. http://grouplens.org/datasets/movielens/ http://jmcauley.ucsd.edu/data/amazon/
Plot summaries are available at http://www.imdb.com/
We set the size of latent dimension of U and V to 50 (as reported in [24]) and initialized U and V randomly from 0 to 1. Table 2 shows the best performing values of common parameters (  X  of each model found by grid search. Since we use explicit datasets, we set the precision parameter of CTR and CDL to 1 if r ij served and 0 otherwise. The rest of the CDL parameters were set as reported in [24]. 5 Implementation Detail We implemented ConvMF using Python and Keras [1] library with NVidia Geforce TitanX GPU. To train the weights of CNN, we used mini-batch based RMSprop, and each mini-batch consists of 128 training items. As for the detailed CNN architecture, we used the following settings: 1) we set the maximum length of docu-ments to 300. 2-1) ConvMF: we initialized word latent vectors randomly with dimension size of 200. These word latent vectors will be trained through the optimization process. 2-2) ConvMF+: we initialized word latent vectors by pre-trained word embedding models with dimension size of 200. These word latent vectors will be trained through the optimization process. 3) In the convolution layer, we used various window sizes (3, 4, and 5) for shared weights to consider various length of surrounding words, and we used 100 shared weights per window size. 4) Instead of the L 2 regularizer related to weights of CNN, we used dropout and set dropout rate to 0.2 to prevent CNN from over-fitting.
 Evaluation Protocol To evaluate the overall performance of each model on the real world datasets, we randomly split each dataset into a training set (80%), a validation set (10%) and a test set (10%). The training set con-tains at least a rating on every user and item so that PMF deals with all users and items. As the evaluation measure, we used root mean squared error (RMSE), which is directly related to an objec-tive function of conventional rating prediction model.
 We reported test errors of each model, which gives the lowest val-idation errors within 200 iterations with early-stopping. For relia-bility of our results, we repeated this evaluation procedure 5 times from data split process and we reported mean test errors. 1) Quantitative Results on MovieLens and Amazon Datasets Table 3 shows the overall rating prediction error of ConvMF, Con-vMF+, and three competitors. Note that  X  X mprove X  indicates the
We tried different settings but the performance was almost the same. Ratio of training set to the entire dataset (density)
Figure 3: Skewness of the number of ratings for items on each dataset relative improvements of  X  X onvMF X  over the best competitor. Com-pared to three models, ConvMF and ConvMF+ achieve significant improvements on all the datasets.

For MovieLens datasets, which are relatively dense datasets, the improvements of ConvMF over the best competitor, CDL, are 3.92% on ML-1m dataset and 2.79% on ML-10m dataset. We also ob-serve that the performance differences between PMF and the two competitors are marginal on ML-1m dataset. It implies that given enough ratings to generate user and item latent models, document analysis that fails to capture contextual information does not help generate more accurate latent models. However, significant per-formance gap between ConvMF and PMF indicates that deeper understanding of documents helps adjust latent models more accu-rately even when enough ratings are given .

For Amazon dataset, which is an extremely sparse and skewed dataset as shown in Figure 3 and 4, the improvement of ConvMF over the best competitor, CDL, is 16.60%. This improvement is more substantial compared to the improvements on relatively dense and balanced MovieLens datasets, which indicates that ConvMF constructs accurate item latent models by effectively analyzing doc-uments even with sparse and skewed data. Note that the RMSE of CTR is higher than that of PMF although CTR additionally uses item documents. It implies that on such sparse and skewed dataset, it is likely that item latent models built by the LDA part of CTR and user latent models built by the PMF part of CTR do not reside in the same latent space. 2) Quantitative Results Over Various Sparseness on ML-1m We generate seven additional datasets of different sparsenesses by randomly sampling from ML-1m dataset. As shown in Table 4, ConvMF significantly outperforms three competitors over all ranges of sparseness. Specifically, we observe that improvement of Con-vMF over the best competitor (CDL) increases consistently from 2.98% to 3.92% when data density increases from 0.93% to 3.71%. It implies that ConvMF produces more accurate latent models by exploiting ratings when the number of ratings of each item evenly increases  X  the rating data becomes dense whereas the skewness of
Figure 6: Relative improvements of ConvMF+ over ConvMF the rating data is almost consistent, which is supported by Figure 3 and 4. To be precise, while ML-1m (sparse) and ML-1m (dense) dataset whose training set consists of 20% and 80% of the entire dataset, respectively, have almost the same skewness over items, the former has relatively smaller number of ratings on items than the latter one. The performance of ConvMF consistently increases as the dataset gets denser, which indicates that CNN of ConvMF is well integrated into PMF for recommendation task. 3) Impact of Pre-trained Word Embedding Model
Unlike CTR and CDL, ConvMF is able to take advantage of us-ing pre-trained word embedding models such as Glove [18]. Thus, we investigate the impact of pre-trained word embedding model on our recommendation task by initializing the embedding layer of CNN of ConvMF using the pre-trained word embedding model of Glove [18].

Table 3 shows marginal changes of ConvMF+ over ConvMF on three datasets: -0.22%, 0.35% and 0.51% on ML-1m, ML-10m and AIV dataset, respectively. Note that the sparsity of datasets is in increasing order of ML-1m, ML-10m and AIV. In spite of and  X  V on three dataset the marginal changes, we observe the consistent tendency that as the rating data gets sparser, the pre-trained word embedding model helps improve the performance of ConvMF. It is because the se-mantic and syntactic information of pre-trained word embedding model complement shortage of ratings. We also observe that given sufficient number of ratings to train latent models, pre-trained word embedding model rather deteriorates the performance of ConvMF. In other words, since ratings directly reflect the relationships be-tween users and items, it is better to fully leverage ratings rather than to additionally utilize pre-trained word embedding model ob-tained from external documents when rating data is relatively dense.
Moreover, we investigate relative improvements of ConvMF+ over ConvMF on various  X  V , a parameter that balances between the importance of ratings and description documents in ConvMF. In Figure 6, for MovieLens datasets, since the dataset has relatively large number of ratings, pre-trained word embedding model does not have much impact on the performance of ConvMF on various  X  . However, for extremely sparse Amazon dataset, pre-trained word embedding model indeed affects the performance of Con-vMF. Specifically, when  X  V is 100 and 1000, the improvement of ConvMF+ over ConvMF reaches almost 0.51% and 2.74% with the same parameter setting whereas the performance of ConvMF+ sud-denly drops with other  X  V values. In other words, relatively low or high  X  V with pre-trained word embedding model fails to achieve high performance. This phenomenon implies that latent models are under-fitted or over-fitted by the word embedding model when rat-ing data is extremely sparse. Nevertheless, as shown in Figure 6, given a proper value of  X  V , adopting pre-trained word embedding model increases the performance of ConvMF+ when the number of ratings is insufficient. 4) Parameter Analysis We investigate the impact of three parameters on the performance of ConvMF: p (dimension size of word latent vectors),  X  U
Figure 5 shows RMSE changes of ConvMF and ConvMF+ ac-cording to various p on Amazon dataset. Interestingly, in case of ConvMF, the increase of p from 100 to 300 does not boost the per-formance of ConvMF, but rather shows degradation. However, in case of ConvMF+, the increase of p reduces RMSE significantly. This demonstrates that when dataset is extremely sparse, the per-formance of ConvMF+ is improved by adopting pre-trained word embedding model in which the information contained gets richer as p gets larger.

Figure 7 shows the impact of  X  U and  X  V on three real-word datasets. Regarding the changes of the best performing values of  X 
U and  X  V from Figure 7(a) to (c), we observe that when the rating data becomes sparse,  X  U decreases while  X  V increases to produce the best results. Precisely, the values of (  X  U ,  X  V ) of ConvMF are (100, 10), (10 and 100) and (1 and 100) on ML-1m, ML-10m and AIV, respectively. Note that when  X  U is high, user latent model is hardly updated. In other words, a high value of  X  U implies that item latent model tend to be projected to the latent space of user latent model (same applies to  X  V ). Thus, when  X  U decreases and  X 
V increases, user latent models are projected to the latent space of item latent model whose space is mainly built by description docu-ments rather than by ratings. These best performing values demon-strate that ConvMF well alleviates data sparsity by balancing the importance of ratings and description documents.

Phrase captured by people trust the man 0.0704 betray his trust finally 0.1009
Test phrases for people believe the man 0.0391 betray his believe finally 0.0682 people faith the man 0.0374 betray his faith finally 0.0693 people tomas the man 0.0054 betray his tomas finally 0.0480 5) Qualitative Evaluation The performance of ConvMF is affected by the contextual features extracted by shared weights W c , and each contextual feature is as-sociated with a phrase. In this section, we verify whether ConvMF is able to distinguish subtle contextual differences by comparing each contextual meaning of phrases captured by the shared weights.
Specifically, as a case study, we select W 11 c and W 86 c model trained on ML-10m dataset, and compare the contextual meaning of phrases captured by the shared weights (Table 5). The meaning of  X  trust  X  in the two phrases captured by the two shared weights seem to be similar to each other. However, there is a sub-tle difference on contextual meaning of the term  X  trust  X  in the two phrases. Indeed, the  X  trust  X  in the phrase captured by W as a verb whereas the  X  trust  X  in the phrase captured by W as a noun. To verify this, we slightly change the term  X  trust  X  in the phrases, and investigate the change of the feature values as shown in the phrases captured by W 11 c , the feature value of the former phrase is higher than that of the latter phrase. However, for the case of W 86 c , the feature value of the latter phrase is higher than that of the former phrase. This matches with our expectation that  X  believe  X  is syntactically more similar to the contextual meaning of  X  trust  X  as a verb captured by W 11 c whereas  X  faith  X  is syntactically more similar to the contextual meaning of  X  trust  X  as a noun cap-tured by W 86 c . When we replace  X  trust  X  with  X  tomas  X , which has a completely different meaning from  X  trust  X ,  X  tomas  X  returns lower values for both W 11 c and W 86 c than  X  believe  X  and  X  faith  X .
This comparisons imply that ConvMF distinguishes a subtle con-textual difference of the term  X  trust  X . As a result, we conclude that ConvMF captures contextual meaning of words in documents and can even distinguish subtle contextual difference of the same word via different shared weights.
In this paper, we address that considering contextual information such as surrounding words and word orders in description doc-uments provides deeper understanding of description documents, and we develop a novel document context-aware recommendation model, ConvMF, that seamlessly integrates CNN into PMF in order to capture contextual information in description documents for the rating prediction. Extensive results demonstrate that ConvMF sig-nificantly outperforms the state-of-the-art competitors, which im-plies that ConvMF well deals with the sparsity problem with con-textual information. Moreover, since ConvMF is based on PMF, which is the standard MF-based recommendation model, ConvMF is able to be extended to combining other MF-based recommenda-tion models such as SVD++ [12] that only consider ratings.
As a next research direction, since it is widely known that un-supervised pre-training on deep neural network has much impact on performance, we try to develop convolutional autoencoder for description documents. By unsupervised way, it enables us to pre-train not only the weight variables of the embedding layer but also all the remaining weight variables of the CNN part of ConvMF. We expect that this unsupervised pre-training by the autoencoder sig-nificantly boosts the performance of recommendation when rating data is extremely sparse. This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education, Science and Technology (No. 2012M3C4A7033344) and the ICT R&amp;D program of MSIP/IITP [B0101-15-0307, Basic Software Re-search in Human-level Lifelong Machine Learning (Machine Learn-ing Center)] and the Industrial Core Technology Development Pro-gram (10049079, Development of Mining core technology exploit-ing personal big data) funded by the Ministry of Trade, Industry and Energy (MOTIE, Korea)
