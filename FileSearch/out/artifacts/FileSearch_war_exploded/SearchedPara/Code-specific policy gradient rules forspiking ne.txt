 Neural implementations of reinforcement learning have to solve two basic credit assignment prob-lems: (a) the temporal credit assignment problem, i.e., the question which of the actions that were taken in the past were crucial to receiving a reward later and (b) the spatial credit assignment prob-lem, i.e., the question, which neurons in a population were important for getting the reward and which ones were not.
 Here, we argue that an additional credit assignment problem arises in implementations of reinforce-ment learning with spiking neurons. Presume that we know that the spike pattern of one specific neuron within one specific time interval was crucial for getting the reward (that is, we have already solved the first two credit assignment problems). Then, there is still one question that remains: Which feature of the spike pattern was important for the reward? Would any spike train with the same number of spikes yield the same reward or do we need precisely timed spikes to get it? This credit assignment problem is in essence the question which neural code the output neuron is (or should be) using. It becomes particularly important, if we want to change neuronal parameters like synaptic weights in order to maximize the likelihood of getting the reward again in the future. If only the spike count is relevant, it might not be very effective to spend a lot of time and energy on the difficult task of learning precisely timed spikes.
 The most modest and probably most versatile way of solving this problem is not to make any as-sumption on the neural code but to assume that all features of the spike train were important. In this case, neuronal parameters are changed such that the likelihood of repeating exactly the same spike train for the same synaptic input is maximized. This approach leads to a learning rule that was derived in a number of recent publications [3, 5, 13]. Here, we show that a whole class of learning rules emerges when prior knowledge about the neural code at hand is available. Using a policy-gradient framework, we derive learning rules for neural parameters like synaptic weights or threshold parameters that maximize the expected reward.
 Our aims are to (a) develop a systematic framework that allows to derive learning rules for arbitrary neural parameters for different neural codes, (b) provide an intuitive understanding how the resulting learning rules work, (c) derive and test learning rules for specific example codes and (d) to provide Finally, we argue that the learning rules contain two types of prediction problems, one related to reward prediction, the other to response prediction. 2.1 Coding features and the policy-gradient approach The basic setup is the following: let there be a set of different input spike trains X  X  to a single post-synaptic neuron, which in response generates stochastic output spike trains Y  X  . In the language of partially observable Markov decision processes, the input spike trains are observations that provide information about the state of the animal and the output spike trains are controls that influence the action choice. Depending on both of these spike trains, the system receives a reward. The goal is to adjust a set of parameters  X  i of the postsynaptic neuron such that it maximizes the expectation value of the reward.
 Our central assumption is that the reward R does not depend on the full output spike train, but only on a set of coding features F j ( Y ) of the output spike train: R = R ( F ,X ) . Which coding features F the reward depends on is in fact a choice of a neural code, because all other features of the spike train are not behaviorally relevant. Note that there is a conceptual difference to the notion of a neural code in sensory processing, where the coding features convey information about input signals, not about the output signal or rewards.
 The expectation value of the reward is given by  X  R  X  = P F ,X R ( F ,X ) P ( F | X, X  ) P ( X ) , where P ( X ) denotes the probability of the presynaptic spike trains and P ( F | X, X  ) the conditional proba-bility of generating the coding feature F given the input spike train X and the neuronal parameters  X  . Note that the only component that explicitly depends on the neural parameters  X  i is the condi-tional probability P ( F | X, X  ) . The reward is conditionally independent of the neural parameters  X  i given the coding feature F . Therefore, if we want to optimize the expected reward by employing a gradient ascent method, we get a learning rule of the form If we choose a small learning rate  X  , the average over presynaptic patterns X and coding features F can be replaced by a time average. A corresponding online learning rule therefore results from dropping the average over X and F : This general form of learning rule is well known in policy-gradient approaches to reinforcement learning [1, 12]. 2.2 Learning rules for exponentially distributed coding features The joint distribution of the coding features F j can always be factorized into a set of conditional function that is characteristic for the distribution and depends only on the parameters C i . Note that the NEF is a relatively rich class of distributions, which includes many canonical distributions like the Poisson, Bernoulli and the Gaussian distribution (the latter with fixed variance). Under these assumptions, the learning rule (3) takes a characteristic shape: where  X  i and  X  2 i are the mean and the variance of the conditional distribution and the parameters  X  . Note that correlations between the coding features are implicitly accounted for by the dependence of  X  i and  X  i on the other features. The summation over different coding features arises from the factorization of the distribution, while the specific shape of the summands relies on the assumption of normal exponential distributions [for a proof, cf. 12].
 There is a simple intuition why the learning rule (4) performs gradient ascent on the mean reward. The term F j  X   X  j fluctuates around zero on a trial-to-trial basis. If these fluctuations are positively lead to higher reward, so that the mean of the coding feature should be increased. This increase is implemented by the term  X   X  i  X  j , which changes the neural parameter  X  i such that  X  j increases. In this section, we illustrate the framework by deriving policy-gradient rules for different neural codes and show that they can solve simple computational tasks.
 The neuron type we are using is a simple Poisson-type neuron model where the postsynaptic firing rate is given by a nonlinear function  X  ( u ) of the membrane potential u . The membrane potential u , in turn, is given by the sum of the EPSPs that are evoked by the presynaptic spikes, weighted with the respective synaptic weights: where t f i denote the time of the f -th spike in the i -th presynaptic neuron. ( t  X  t f i ) denotes the shape of the postsynaptic potential evoked by a single presynaptic spike at time t f i . For future use, we have introduced PSP i as the postsynaptic potential that would be evoked by the i -th presynaptic spike train alone, if the synaptic weight were unity.
 The parameters that one could optimize in this neuron model are (a) the synaptic weights and (b) pa-rameters in the dependence of the firing rate  X  on the membrane potential. The first case is the standard case of synaptic plasticity, the second corresponds to a reward-driven version of intrinsic plasticity [cf. 10]. 3.1 Spike Count Codes: Synaptic plasticity dow [0 ,T ] and that the reward is delivered at the end of this period. The probability distribution for the spike count is a Poisson distribution P ( N ) =  X  N exp(  X   X  ) /N ! with a mean  X  that is given by the integral of the firing rate  X  over the interval [0 ,T ] : The dependence of the distribution P ( N ) on the presynaptic spike trains X and the synaptic weights w i is hidden in the mean spike count  X  , which naturally depends on those factors through the postsynaptic firing rate  X  . Because the Poisson distribution belongs to the NEF, we can derive a synaptic learning rule by using equation (4) and calculating the particular form of the term  X  w i  X  : This learning rule has structural similarities with the Bienenstock-Cooper-Munro (BCM) rule [2]: The integral term has the structure of an eligibility trace that is driven by a simple Hebbian learning rule. In addition, learning is modulated by a factor that compares the current spike count ( X  X ate X ) with the expected spike count ( X  X liding threshold X  in BCM theory). Interestingly, the functional role of this factor is very different from the one in the original BCM rule: It is not meant to introduce selectivity [2], but rather to exploit trial fluctuations around the mean spike count to explore the structure of the reward landscape.
 We test the learning rule on a 2-armed bandit task (Figure 1A). An agent has the choice between two actions. Depending on which of two states the agent is in, action a 1 or action a 2 is rewarded ( R = 1 ), while the other action is punished ( R =  X  1 ). The state information is encoded in the rate pattern of 100 presynaptic neurons. For each state, a different input pattern is generated by drawing the firing rate of each input neuron independently from an exponential distribution with a mean of 10Hz. In each trial, the input spike trains are generated anew from Poisson processes with these neuron-and state-specific rates. The agent chooses its action stochastically with probabilities that are proportional to the spike counts of two output neurons: p ( a k | s ) = N k / ( N 1 + N 2 ) . Because the spike counts depend on the state via the presynaptic firing rates, the agent can choose different actions for different states. Figure 1B and C show that the learning rule learns the task by suppressing activity in the neuron that encodes the punished action.
 In all simulations throughout the paper, the postsynaptic neurons have an exponential rate function g ( u ) = exp (  X  ( u  X  u 0 )) , where the threshold is u 0 = 1 . The sharpness parameter  X  is set to either  X  = 1 (for the 2-armed bandit task) or  X  = 3 (for the spike latency task). Moreover, the postsynaptic neurons have a membrane potential reset after each spike (i.e., relative refractoriness), so that the assumption of a Poisson distribution for the spike counts is not necessarily fulfilled. It is worth noting that this did not have an impeding effect on learning performance. 3.2 Spike Count Codes: Intrinsic plasticity Let us now assume that the rate of the neuron is given by a function  X  ( u ) = g (  X  ( u  X  u 0 )) which depends on the threshold parameters u 0 and  X  . Typical choices for the function g would be an exponential (as used in the simulations), a sigmoid or a threshold linear function g ( x ) = ln(1 + exp( x )) .
 By intrinsic plasticity we mean that the parameters u 0 and  X  are learned instead of or in addition to the synaptic weights. The learning rules for these parameters are essentially the same as for the synaptic weights, only that the derivative of the mean spike count is taken with respect to u 0 and  X  , respectively: Here, g 0 =  X  x g ( x ) denotes the derivative of the rate function g with respect to its argument. 3.3 First Spike-Latency Code: Synaptic plasticity As a second coding scheme, let us assume that the reward depends only on the latency  X  t of the first spike after stimulus onset. More precisely, we assume that each trial starts with the onset of the presynaptic spike trains X and that a reward is delivered at the time of the first spike. The reward depends on the latency of that spike, so that certain latencies are favored. Figure 1: Simulations for code-specific learning rules. A 2-armed bandit task: The agent has to choose among The probability distribution of the spike latency is given by the product of the firing probability at time  X  t and the probability that the neuron did not fire earlier: Using eq. (3) for this particular distribution, we get the synaptic learning rule: In Figure 1D, we show that this learning rule can learn to adjust the weights of two neurons such that their first spike latencies approximate a set of target latencies. 3.4 The Full Spike Train Code: Synaptic plasticity Finally, let us consider the most general coding feature, namely, the full spike train. Let us start with a time-discretized version of the spike train with a discretization that is sufficiently narrow to allow at most one spike per time bin. In each time bin [ t,t +  X  t ] , the number of spikes Y t follows a Bernoulli distribution with spiking probability p t , which depends on the input and on the recent history of the neuron. Because the Bernoulli distribution belongs to the NEF, the associated policy-gradient rule can be derived using equation (4): This is the rule that should be used in discretized simulations. In the limit  X  t  X  0 , p t can be approximated by p t  X   X   X  t , which leads to the continuous time version of the rule: Here, Y ( t ) = P t proposed by Xie and Seung [13] and Florian [3] and, slightly modified for supervised learning, by Pfister et al. [5].
 Following the same line, policy gradient rules can also be derived for the intrinsic parameters of the neuron, i.e., its threshold parameters (see also [3]). Obviously, the learning rule (16) is the most general in the sense that it considers the whole spike train as a coding feature. All possible other features are therefore captured in this learning rule. The natural question is then: what is the advantage of using rules that are specialized for one specific code? Say, we have a learning rule for two coding features F 1 and F 2 , of which only F 1 is correlated with reward. The learning rule for a particular neuronal parameter  X  then has the following structure: Of the four terms in lines (19-20), only the first term has non-vanishing mean when taking the trial average. The other terms are simply noise and therefore more hindrance than help when trying to maximize the reward. When using the full learning rule for both features, the learning rate needs to be decreased until an agreeable signal-to-noise ratio between the drift introduced by the first term and the diffusion caused by the other terms is reached. Therefore, it is desirable for faster learning to reduce the effects of these noise terms. This can be done in two ways: These considerations suggest that the spike count rule (7) should outperform the full spike train rule (16) in tasks where the reward is based purely on spike count. Unfortunately, we could not yet substantiate this claim in simulations. As seen in Figure 1B, the performance of the two rules is very similar in the 2-armed bandit task. This might be due to a noise bottleneck effect: there are several sources of noise in the learning process, the strongest of which limits the performance. Unless the  X  X ode-specific noise X  is dominant, code-specific learning rules will have about the same performance as general purpose rules. estimate is one that takes only the relevant coding features into account and subtracts the trial mean of the reward: This learning rule has a conceptually interesting structure: Learning takes place only when two con-ditions are fulfilled: the animal did something unexpected ( F j  X   X  i ) and receives an unexpected reward ( R  X  R (  X  1 , X  2 ,... ) ). Moreover, it raises two interesting prediction problems: (a) the predic-tion of the trial average  X  j of the coding feature conditioned on the stimulus and (b) the reward that is expected if the coding feature takes its mean value. 5.1 Prediction of the coding feature In the cases where we could derive the learning rule analytically, the trial average of the coding feature could be calculated from intrinsic properties of the neuron like its membrane potential. Un-fortunately, it is not clear a priori that the information necessary for calculating this mean is always available. This should be particularly problematic when trying to extend the framework to coding features of populations, where the population would need to know, e.g., membrane properties of its members.
 An interesting alternative is that the trial mean is calculated by a prediction system, e.g., by top-down signals that use prior information or an internal world model to predict the expected value of the coding feature. Learning would in this case be modulated by the mismatch of a top-down by a  X  X ottom-up X  approach. This interpretation bears interesting parallels to certain approaches in sensory coding, where the interpretation of sensory information is based on a comparison of the sensory input with an internally generated prediction from a generative model [cf. 6]. There is also some experimental evidence for neural stimulus prediction even in comparably low-level systems as the retina [e.g. 8].
 Another prediction system for the expected response could be a population coding scheme, in which a population of neurons is receiving the same input and should produce the same output. Any neuron of the population could receive the average population activity as a prediction of its own recently proposed for reinforcement learning in populations of spiking neurons [11]. 5.2 Reward prediction The other quantity that should be predicted in the learning rule is the reward one would get when the coding feature would take the value of its mean. If the distribution of the coding feature is sufficiently narrow so that in the range F takes for a given stimulus, the reward can be approximated by a linear function, the reward R (  X  ) at the mean is simply the expectation value of the reward given the stimulus: The relevant quantity for learning is therefore a reward prediction error R ( F )  X  X  R ( F )  X  F | X . In classical reinforcement learning, this term is often calculated in an actor-critic architecture, where some external module -the critic -learns the expected future reward either for states alone or for state-action pairs. These values are then used to calculate the expected reward for the current state or state-action pair. The difference between the reward that was really received and the predicted reward is then used as a reward prediction error that drives learning. There is evidence that dopamine signals in the brain encode prediction error rather than reward alone [7]. We have presented a general framework for deriving policy-gradient rules for spiking neurons and shown that different learning rules emerge depending on which features of the spike trains are as-sumed to influence the reward signals. Theoretical arguments suggest that code-specific learning rules should be superior to more general rules, because the noise in the estimate of the gradient should be smaller. More simulations will be necessary to check if this is indeed the case and in which applications code-specific learning rules are advantageous.
 For exponentially distributed coding features, the learning rule has a characteristic structure, which allows a simple intuitive interpretation. Moreover, this structure raises two prediction problems, which may provide links to other concepts: (a) the notion of using a reward prediction error to reduce the variance in the estimate of the gradient creates a link to actor-critic architectures [9] and (b) the notion of coding feature prediction is reminiscent of combined top-down X  X ottom-up approaches, where sensory learning is driven by the mismatch of internal predictions and the sensory signal [6]. The fact that there is a whole class of code-specific policy-gradient learning rules opens the interest-ing possibility that neuronal learning rules could be controlled by metalearning processes that shape the learning rule according to what neural code is in effect. From the biological perspective, it would be interesting to compare spike-based synaptic plasticity in different brain regions that are thought to use different neural codes and see if there are systematic differences.

