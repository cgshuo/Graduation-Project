 Multi-label classification is supervised learning, where an in-stance may be assigned with multiple categories (labels) si-multaneously. Recently, a method called Probabilistic Clas-sifier Chain (PCC) was proposed with numerous appealing properties, such as conceptual simplicity, flexibility, and the-oretical justification. Nevertheless, PCC suffers from high inference complexity. To address this problem, we propose a novel inference method with gibbs sampling . An acceler-ation scheme is proposed to accelerate this method further. Our proposed method is based on our claim that PCC is a special case of Bayesian network. This claim may inspire more inference algorithms for PCC. Experiments with real-world data sets show effectiveness of our proposed method. I.5.1 [ Pattern Recognition ]: Models X  statistical,structural Algorithms, Experimentation Multi-label Classification, Probabilistic Classifier Chain, Gibb-s Sampling
Multi-label classification is supervised learning, where an instances is usually assigned with multiple labels simultane-ously. In recent years, the multi-label classification attract-s increasing attentions from various domains, such as text  X  corresponding author classification [4, 8], media categorization [9] and genomic-s [6]. In multi-label classification setting, X X X = R d the instance space, and Y Y Y = { 0 , 1 } m denotes label space with m labels. An instance x x x  X  X  is associated with a label y = 1 denotes the instance has the j -th label and 0 other-wise.

Recent Probabilistic Classifier Chain (PCC) [1] is an at-tractive solution for multi-label classification. PCC trains a probabilistic classifier with respect to each label. The clas-sifier corresponding to the j -th label takes x x x plus y as features. Hence the classifier is trained to estimate the all classifiers, formulated as Eq.(1). During inference, PCC calculates the probabilities of all pos-sible label combinations, and chooses the label combination with the highest probability, formulated as Eq.(2).
PCC is attractive for several reasons. First, it converts multi-label classification to some binary classification prob-lems, which allows us to leverage existing researches. Sec-ond, it is a principled probabilistic model, and there is a theoretical guarantee that PCC produces Bayes optimal pre-dictions. Third, it is computationally inexpensive to train. However, during inference, PCC calculates the probabilities of all possible label combinations (totally 2 m ), resulting in high inference complexity. To reduce the inference complex-ity of PCC, we propose a novel inference method Probabilis-tic Classifier Chain via Gibbs sampling (P2CG). An accel-eration scheme is proposed to further reduce the inference complexity. Actually, we propose our method inspired by that PCC is a special case of Bayesian network. Our exper-iments with real-world data sets show our proposed method dramatically extends the practical viability of PCC.
The remainder of this paper is organized as follows. P2CG is introduced in section 2. we claim PCC is a special case of Bayesian network in section 3. Experiments are conducted in section 4. Section 5 concludes this paper.
P2CG employs gibbs sampling to PCC inference. Gibbs sampling algorithm samples each label variable successively or randomly from the conditional distributions p ( y j | x x x, y The conditional distribution p ( y j | x x x, y  X  j y  X  j p ( y j = 1 | x x x, y  X  j y  X  j y  X  j ) = p ( y j = 1 , y  X  j = ables but the sample target label and x x x denotes the instance feature. With the formula of joint conditional distribution, the procedure of P2CG is shown in Algorithm 1.
 Algorithm 1 P2CG
Initialize y y y for all i  X  1 , 2 , .., nIter do end for return y y y
The label combinations generated by gibbs sampling al-the label combination with higher joint probability is more likely to appear in the sample set. Hence, it is reasonable to choose the label combination with the highest joint proba-bility in the sample set as output. where S denotes the sample set generated by gibbs sampling algorithm.
 How to choose the number of iterations is important in P2CG. Since large number of iteration will causes high in-ference time, we suggest to choose small number. In experi-ments of this paper, we will see P2CG achieves satisfactory performance even when small number of iterations.
We introduce the acceleration scheme when generalized linear model [5] is utilized as base classifier, and show the acceleration scheme works for other types of base classifi-er. Generalized linear model (linear regression, logistic re-gression, poisson regression, etc) produces probabilistic pre-diction with the dot product of the feature vector and the weight vector. Hence, the j -th classifier in the PCC model produces probabilistic prediction with Eq.(5). where w w w j and b j are the j -th model X  X  parameters. f is non-linear function of converting the dot product to probability. For example, when the general linear model is logistic re-gression, f is the logistic function, f ( s ) = 1 1+ exp (  X  s ) denote the dot product vector, we have If we can maintain s s s and update it fastly, the classifiers in the PCC model will take less time to produce predictions and thus we will speed up the inference. Since P2CG changes only one label every time, we can update s s s fastly. When we change the k -th label, the dot products for latter labels ( s , j &gt; k ) will change according to Eq.(8). where w j k is the j -th classifier X  X  weight w.r.t the k -th label (When j &gt; k , the k -th label is a feature of the j -th classifier. w j,k is the weight with respect to this feature).

By maintaining and updating s s s , we develop a acceleration scheme, as shown in Algorithm 2. The inference complexity will be reduced with this acceleration scheme further. With-out the acceleration scheme, as Algorithm 1 shows, each iteration of P2CG inference samples O ( m ) labels succes-sively, each sampling needs O ( m ) predictions by classifiers, the time complexity of producing probabilistic prediction is O ( d + m ). Hence, the inference complexity per iteration is O ( dm 2 + m 3 ). With s s s , the time complexity of producing pre-diction is reduced from O ( d + m ) to O (1). Hence, the infer-ence complexity per iteration is reduced from O ( dm 2 + m to O ( m 2 ).
 Algorithm 2 P2CG with the acceleration scheme Initialize y y y
Calculate s s s with Eq.(6). for all i  X  1 , 2 , .., nIter do end for return y y y
It is obvious that this acceleration method still works when Naive Bayes and Maximum Entropy utilized. General-ized linear models, Naive Bayes and Maximum Entropy are the most popular models for probabilistic outputs. Hence, this acceleration method works for most cases. In this section, we claim that PCC is a special case of Bayesian network. The claim is made based on the follow-ing three reasons. First, PCC and Bayesian network can have the same structure. Figure 1 shows a special structure of Bayesian network. In this Bayesian network structure, all label variables have common cause x x x and the former label variables depend the latter label variables. This structure is exactly the structure of PCC. Second, their learning process-es are equivalent. In the above Bayesian network, maximum likelihood estimation is to learn the parameters by estimat-ing p ( y 1 | x x x ), ... , p ( y m | x x x, y 1 , ..., y C are trained to estimate p ( y 1 | x x x ) , ..., p ( y Therefore, they are equivalent. Third, both of them are to find the instantiation (label combination) with the highest joint probability during inference.
 Figure 1: Bayesian network with a special structure
We propose to use gibbs sampling to reduce the inference complexity in this paper. Actually, this method is inspired by that gibbs sampling is an efficient inference algorithms for Bayesian network. The claim, that PCC is a special case of Bayesian network, may inspire more inference algorithms. There exist numerous inference algorithms for Bayesian net-work, such as stochastic MCMC simulation [2], variational methods [3] and so on. All of them can be employed to PCC inference. Since we have developed P2CG as the example, details of these extensions are omitted in this paper.
We compare P2CG to the following methods: Binary Rel-evance (BR), Classifier Chains (CC) [7], Probabilistic Clas-sifier Chain (PCC). BR learns a binary classifier with re-spect to each label, which suffers from ignoring label depen-dencies. CC [7] organizes the classifiers along a chain, and takes predictions produced by the former classifiers as the latter classifiers X  additional features. CC exploits the label dependencies however CC usually plunges into local opti-ma [1]. In our experiments, we use CC X  X  result as initializa-tion label combination of P2CG. Experiments are done in ten-fold cross validation (CV). We choose logistic regression as the base classifier for all algorithms in our experiments, since logistic regression is an effective probabilistic classifi-cation model. .

We perform experiments on four real-world data sets The statistics of these data sets are shown in Table 1, where n denotes the size of the dataset, d denotes the dimension of instance, and m denotes the number of labels.
Available at http://mulan.sourceforge.net/datasets. html
We compare P2CG to BR, CC and PCC. We choose small number of iterations and set it to 20. Evaluation is done in terms of th evaluation metrics. The first one is 0/1 loss. The 0/1 loss is ratio of exact match of predicted label set p p p and true label set y y y . where I ( true ) = 1 and I ( f alse ) = 0. n denotes the size of dataset. The lower 0/1 loss, the better. The second metrics is hamming loss, which is defined as follows.
 where  X  denotes the symmetric difference of two sets. The lower hamming loss, the better.
 Scene .485  X  .012 .441  X  .013 .386  X  .021 .386  X  .021 Slashdot .789  X  .015 .628  X  .027 .557  X  .028 .557  X  .028 Enron .854  X  .029 .847  X  .027 INF .833  X  .023 Corel5k .997  X  .002 .996  X  .003 INF .986  X  .003 Table 2: Performance (mean  X  std) of each algorith-m in terms of 0/1 loss.
 Scene .107  X  .005 .105  X  .006 .096  X  .007 .096  X  .007 Slashdot .056  X  .002 .047  X  .027 .041  X  .002 .041  X  .002 Enron .045  X  .002 .048  X  .002 INF .048  X  .002 Corel5k .014  X  .000 .013  X  .000 INF .010  X  .000 Table 3: Performance (mean  X  std) of each algorith-m in terms of hamming loss.

Table 3 shows the detailed results ( INF means the algo-rithm is not suitable for the corresponding data set). The P2CG as well as PCC show clear superiorities to BR and CC. It shows the advantages of PCC and P2CG. When PC-C is suitable (the Scene and Slashdot data set), P2CG has similar performance to PCC. The reasonable explanation for these experiment results is that P2CG (almost) reaches the global optimum. When the number of labels exceeds 30, it is hard to apply PCC (the inference complexity of PCC exceeds 2 30 ). But this isn X  X  a problem with P2CG. Even the number of labels is 374 (the corel5k data set), P2CG is still applicable, which shows P2CG can reduce the inference complexity dramatically.
To compare P2CG with PCC in term of efficiency, we record test time of PC2G and PCC in the above experi-ments, which are reported in Table 4. The test times are similar when the number of labels is small (the label num-ber of the scene is 6). As the number of labels grows, the test time of PCC grows rapidly. The label number of the slashdot is 22, the test time of PCC is 2.8h! However the test time of PC2G is still small (9.2s). The number of labels exceeds 30, PCC isn X  X  applicable, but P2CG works. For the enron and corel5k, the number of labels are 53 and 374, PC-C isn X  X  able to complete the test procedure in a week, P2CG completes the test procedure in 40.3s and 150.4s respective-ly.

Convergence rate is an important property of P2CG. To examine the convergence rate of P2CG, we keep track of the Due to the page limit, we only report the results on the Scene and Corel5k data set.

Figure 2 shows the average value of probabilities of the result label combinations when performing experiments on the Scene data set. In figure (2), P2CG converges after around iteration 7. Figure (3) shows the average value of probabilities of the result label combinations when perform-ing experiments on the Corel5k data set. In figure 3, P2CG converges after around iteration 70. The experiment results show convergence rate of P2CG is influenced by the number of labels. This is because convergence rate of gibbs sampling is influenced by the number of nodes. But even when the number of labels reaches 374, P2CG still converges fastly (after around iteration 70).
 Figure 2: The average value of p ( y y y | x x x ) as a function of iterations number, when applying P2CG on the Scene dataset
PCC achieves a good performance in multi-label classifi-cation. Nevertheless, PCC suffers from high inference com-plexity. In this paper, we are inspired by that PCC is a Figure 3: The average value of p ( y y y | x x x ) as a function of iterations number, when applying P2CG on the Corel5k dataset special case of Bayesian network, and propose P2CG to re-duce the inference complexity. Experiments with real-world data sets show effectiveness of P2CG.
This research was partly supported by National High Tech-nology Research and Development Program of China (863 Program) (No.2012AA011101), National Natural Science Foun-dation of China (No.91024009), Major National Social Sci-ence Fund of China (No. 12&amp;ZD227). [1] K. Dembczynski, W. Cheng, and E. H  X  ullermeier. Bayes [2] D. Gamerman and H. F. Lopes. Markov chain Monte [3] T. S. Jaakkola and M. I. Jordan. Bayesian parameter [4] I. Katakis, G. Tsoumakas, and I. Vlahavas. Multilabel [5] J. A. Nelder and R. W. Wedderburn. Generalized linear [6] Y. Peng, G. Kou, Y. Shi, and Z. Chen. A multi-criteria [7] J. Read, B. Pfahringer, G. Holmes, and E. Frank. [8] A. N. Srivastava and B. Zane-Ulman. Discovering [9] K. T. G. Tsoumakas, G. Kalliris, and I. Vlahavas.
