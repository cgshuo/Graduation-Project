
Retrieving similar data has drawn many research ef-forts in the literature due to its importance in data mining, database and information retrieval. This problem is chal-lenging when the data is incomplete. In previous research, data incompleteness refers to the fact that data values for some dimensions are unknown. However, in many practical applications (e.g., data collection by sensor network unde r bad environment), not only data values but even data di-mension information may also be missing, which will make most similarity query algorithms infeasible. In this work, we propose the novel similarity query problem on dimen-sion incomplete data and adopt a probabilistic framework to model this problem. For this problem, users can give a distance threshold and a probability threshold to specify their retrieval requirements. The distance threshold is us ed to specify the allowed distance between query and data ob-jects and the probability threshold is used to require that the retrieval results satisfy the distance condition at lea st with the given probability. Instead of enumerating all pos-sible cases to recover the missed dimensions, we propose an e ffi cient approach to speed up the retrieval process by leveraging the inherent relations between query and dimen-sion incomplete data objects. During the query process, we estimate the lower / upper bounds of the probability that the query is satisfied by a given data object, and utilize these bounds to filter irrelevant data objects e ffi ciently. Fur-thermore, a probability triangle inequality is proposed to further speed up query processing. According to our ex-periments on real data sets, the proposed similarity query method is verified to be e ff ective and e ffi cient on dimension incomplete data.
Multidimensional data, such as time series and feature vectors extracted from images, are widely used in various applications. Similarity query on multidimensional data (i.e., to retrieve similar data objects from multidimensio nal database given a data object as an input query) has attracted many research interests as it plays an important role in many data mining, database and information retrieval tasks . This problem is challenging when the data is incomplete [1, 4, 23], which may be caused by various reasons. For example, in sensor network applications, the data collecte d may become incomplete when sensors do not work properly or when errors occur during data transfer process. well researched (e.g., see [17, 5]). In these works, data in-completeness usually refers to missing value : data values for some dimensions are unknown (or uncertain), but it is known that, for each dimension, if the corresponding data value is missing or not. However, in practice, it is quite common that we do not know which dimensions (or posi-tions) have data loss [9]. In other words, the dimensionalit y of the collected data is lower than its actual dimensionalit y, and we lose the correspondence relationship between data dimensions and their associated values. This is regarded as dimension incompleteness in this work.
 database usually contains time series data objects, each of which is represented by a sequence of values x 1 , x 2 ,..., x The dimension information (e.g., time stamp) associated with data elements can be implicitly inferred from the order of data arrival. This schema of data collection and storage i s very common in resource-constrained applications since ex -plicitly maintaining dimension information will cause add i-tional costs. Therefore, even missing a single data element will destroy the dimension information of the entire data object. Besides, in some applications where dimension in-formation is explicitly maintained, the dimension indicat or itself may also be lost which will cause the data to become dimension incomplete. More generally, for some data sets containing time series with various lengths, it can be as-sumed that these data sets are originally generated with the same dimensionality and then some dimensions are lost. So the dimension incomplete data is quite common in practical applications.
The incompleteness of data dimension brings challenges to similarity query task, as dimension information is es-sential for existing techniques used to handle uncertain data [6, 10]. Given a query object and a dimension in-complete data object, the similarity measurement between them is fundamental for similarity query task but becomes impractical because their dimensions do not match. For instance, the widely used L p -norms distance, L p ( x , y ) = ( P cause it does not allow shifting of data dimension. Other similarity computation methods like DTW [13] and LCSS [3] are mainly proposed to better measure the similarity be-tween two data objects without considering dimension in-completeness. They can not be used to deal with dimension incomplete data, as their underlying matching strategies a re not designed to capture the characteristics of data in dimen -sion incomplete scenario. One straightforward solution is to consider all possible dimension missing cases and cal-culate the similarity accordingly. But this procedure may become extremely time consuming. Assume the original dimensionality of data objects and query is m , but only n dimensions of a data object are observed ( n &lt; m ), there are C m possible dimension combinations to be examined when the similarity is computed. Thus better solutions are neede d to deal with this problem.

In this paper, we model the problem of similarity query on dimension incomplete data with a probabilistic frame-work. Based on our framework, users can give a distance threshold to specify the allowed distance between the query and dimension incomplete data objects, and a probability confidence threshold to specify the requirement that the re-trieval results should satisfy the distance condition at le ast with the given probability. Our query approach is based on the fact that the relationship between query and dimension incomplete data objects can be referred. Such relationship information may provide helpful guides for performing sim-ilarity query task. An e ffi cient method is proposed to find lower and upper bounds of the probability that a data object satisfies the query. These bounds can be used to (1) elimi-nate data objects that are judged as dismissals, and (2) keep qualified ones in O ( n ( m  X  n ) 2 ) time. Furthermore, based on the proposed probability triangle inequality, an approa ch with time complexity O ( m ) is introduced to further speed up the similarity query process. Our proposed method is proved to be theoretically correct. Experiments on two real data sets indicate that our method is promising in doing sim-ilarity query on dimension incomplete data.
The problem of missing data values has been well re-searched in the literature [5, 8, 18, 22]. In their research, the dimensions with data missing are known and their cor-responding values are estimated [15]. There are also some works that deal with data uncertainty (e.g., [20, 6, 7, 14, 21 , 11, 19, 2]), which is related to but di ff erent from dimen-sion incompleteness problem. These works consider the uncertainty of data values and estimate a probability den-sity function (pdf) to model the uncertainty of values for data elements. For example, it is addressed in [7] that the recorded value is likely to be di ff erent from the actual value. In such cases, queries may also be uncertain. These works di ff er from ours in that they only consider the uncertainty of data values, while we consider the uncertainty of dimension as well. In [9], missing data elements in symbolic sequence is addressed. In this work, a more general and more chal-lenging problem is studied. We consider real-valued multi-dimensional data and address the probabilistic query task, both of which are of essential di ff erence from [9]. plete data cannot be solved by algorithms like Dynamic Time Warping (DTW) [13, 12] or Longest Common Subse-quence distance (LCSS) [3]. DTW and LCSS are designed to target a goal di ff erent from ours. They match two multi-dimensional data objects focusing on their common things, not for measuring the similarity between objects with miss-ing dimension information. dimensional data. A data object X from D is a real-valued vector: ( x 1 , x 2 ,..., x M ), where x m (1  X  m  X  M ) is the data value for the m -th dimension of X . | X | = M denotes the dimensionality of X .
 to have missing values or dimensions. Otherwise, D is com-plete . In this work, one data object is regarded as dimen-sion incomplete , if it satisfies that (a) at least one of its data elements is missing; (b) the dimension of the miss-ing data element cannot be determined. For example, given a complete data object X , if its k data elements are miss-ing, the resulting dimension incomplete data is of the form X database is defined as: given a database D containing N data objects of M dimensions, an M -dimensional query Q , a query threshold r and a distance function  X  , to retrieve all the data objects in the database D that have a distance away from Q less than r : Apparently, it is not practical to measure the exact similar -ity between dimension incomplete data object and query, because the associated dimensions cannot be well aligned. Thus the similarity score is uncertain and will depend on both the dimension alignment and the values estimated for the missing dimensions.

In this work, we address the probabilistic similarity query problem on dimension incomplete database: to re-trieve data objects from the database with high probability of satisfying the input query. This problem can be formu-lated as follows: Definition 1 (Probabilistic Similarity Query on Dimension Incomplete Data (PSQ-DID)) . Given a database D con-taining dimension incomplete multidimensional data ob-jects X obs whose underlying complete version is denoted by X, a query Q that is complete data, a distance threshold r, a confidence threshold c, an imputation method  X  indicat-ing the distribution of missing data values, and a distance function  X  ,
P[  X  ( Q , X ) &lt; r ], termed confidence in this paper, indicates the probability that the underlying complete data object X satisfies the requirement of query. Its calculation depends on both the imputation strategy  X  and the distance function  X  . Consider a dimension incomplete data object X obs . With-out knowing the corresponding complete data object X , we can construct the possible complete version of X obs by 1. assigning a dimension combination { n 1 ,..., n | Q | X  X  X 2. imputing the assigned dimensions according to the im-In this work, we use X rv and X mis for representing the re-covery version (i.e., the constructed complete version) an d the imputed part respectively.

Example 1: Given X obs = (12 , 9 , 40), assume the com-plete form of X obs is known to be of dimensionality 5, and the dimension combination indicating that the data miss-ing positions are { 2 , 4 } . Then we know 12, 9, 40 correspond with the first, third, and fifth dimension of X . If the specified imputation strategy is: the missing elements follow a certa in distribution with given expectation and variance, then X rv is a random vector (12 , x i where x i given distribution.

Obviously, there are C | X mis | | Q | possible dimension combina-tions for the missing data elements, each of which could de-rive a recovery version X rv . Since we have no prior knowl-edge about the dimension combination or the possible re-covery result, we assume the probability of using each re-covery result is equal. Therefore, we have
In Equation 3, if X rv is generated by imputing random variables that follow a given distribution,  X  ( Q , X rv ) is a new random variable and P [  X  ( Q , X rv ) &lt; r ] will be a real value belonging to [0 , 1].

Now we will give some detailed discussions on the im-putation strategy  X  and the distance function  X  . Without loss of generality, in this paper, we assume all imputed random variables are mutually independent and follow nor-mal distribution. The mean of each random variable is de-cided according to the dimension to be imputed. If x i in X is missing, we impute random variables with expectation ( x nearest existing data element is used instead). For instanc e, if we impute data to X obs = (1 , 3 , 5) on the the first, third and fourth dimension of X to form a 6-dimensional data object, the recovery version of X will be ( 1 , 1 , 2 , 2 , 3 , 5), where the circled values denote the specified mean values of the random variables. In this paper, all random variables are as -signed the same variance in one dimension incomplete data object, denoted by  X  2 . Specifically, we choose to use the variance of X obs as the variance of imputed random vari-ables in this paper. For many multidimensional data set, this strategy is reasonable since the value of missing data element tends to be related to its neighbor elements [16], while the variance reflects the property of the whole data object. Other strategies for setting mean value and varianc e can also be adopted in our approach. The imputation strat-egy depends on specific application scenarios and is inde-pendent of our method. For instance, we can use the mean value of X obs as the expectation value of the random vari-ables. For distance function  X  , we adopt Euclidean distance which is widely used as similarity metrics in the literature . However, the approach proposed in this paper can be ex-tended to handle other similarity measurements easily.
In this section, we will introduce an e ffi cient approach for probabilistic similarity query on dimension incomplete data . Recall from Section 3, the key point of this task is how to evaluate P[  X  ( Q , X ) &lt; r ] e ffi ciently by avoiding enumerating all possible cases. In order to speed up the query process, we utilize a gradual refinement search strat-egy. Specifically, we propose two pruning methods to help: (1) lower / upper bounds of confidence, and (2) probability triangle inequality, both of which are computationally e ffi -cient and proved to be correct. The overall framework is shown in Figure 1. We first use probability triangle inequal-ity to evaluate data objects in the database. In this step, some data objects are judged as true query results and some will be filtered out. Next, we use confidence lower / upper bounds to further evaluate the remaining candidates, from which some are determined as true query results or true dis-missals. Only those data objects that can not be judged in the former two steps are evaluated by a naive verification algorithm, which is relatively slow but can guarantee both completeness and correctness of query results.
This section provides the definition of the lower and up-per bounds of the probability confidence, the proof of their correctness, and an e ffi cient algorithm for calculating them.
To find the bounds of confidence, we need to treat the missing part and the observed part of the dimension incom-plete data separately. Given a query Q and a certain recov-ery version X rv for dimension incomplete data X obs , we have where Q obs and Q mis are the values on dimensions the same as those of X obs and X mis respectively. Since r  X  0, we have
P [  X  ( Q , X rv ) &lt; r ] = P [  X  2 ( Q mis , X mis ) &lt; r
Obviously,  X  2 ( Q obs , X obs ) is a real value for given X while  X  2 ( Q mis , X mis ) is a random variable depending on the imputation method. From Q , there are totally C | X obs | plete versions with dimensionality | X obs | that can be derived by removing values on some dimensions, denoted by Q obs . Then we can find the lower and upper distance bounds be-tween the observed elements of X and Q : Similarly, we can find the lower bound and upper bound distances for the missing elements: =  X  (argmin Q =  X  (argmax Q value assigned by the imputation method on the k -th dimen-sion of X mis .
 X obs = (2 , 8 , 7). For a query Q = (1 , 4 , 5 , 6 , 7),  X  will be (2-1) 2 + (8-6) 2 + (7-7) 2 = 5 corresponding to the recovery version (2 , ? , ? , 8 , 7), and  X  2 UB be (2-1) 2 + (8-4) 2 + (7-5) 2 = 21 corresponding to the re-covery version (2 , 8 , 7 , ? , ?), where  X ? X  denotes the imputed random variable. For the imputed ran-dom variables X mis = { x 1 , x 2 } , according to our imputa-tion policy, E ( x 1 ) and E ( x 2 ) rely on the dimensions to be imputed. Then  X  2 LB  X  sponding to X rv = (2 , 8 , 7.5 , 7.5 , 7).
 lower and upper bounds of P [  X  ( Q , X rv ) &lt; r ] according to the following theorem.
 Theorem 1 (Confidence Lower and Upper Bounds) . Given a query Q, threshold r and c, for an incomplete multidimen-sional data X obs whose complete form is denoted by X, we have (a) P [  X  ( Q , X ) &lt; r ]  X  P [  X  2 LB (b) P [  X  ( Q , X ) &lt; r ]  X  P [  X  2 UB Proof. (a)For any recovery version X rv of X obs , according to Eq. 5, we have According to Eq. 6 we know Thus  X  ( Q mis , X mis ) / X  2 obeys noncentral chi-square distribution with noncentrality parameter  X  X  X  bution with noncentrality parameter denoted by  X  LB cording to Eq. 8, we know  X  LB these two random variables have the same degree of free-dom | X mis | . According to the property of noncentral chi-square distribution, we know Also considering Eq. 12, we get (b)The proof is similar to that of (a).

For simplicity, we have: According to Theorem 1, P [  X  LB ( Q , X ) &lt; r ] and P [  X  UB ( Q , X ) &lt; r ] are the upper bound and lower bound of P [  X  ( Q , X ) &lt; r ] respectively. These two probability bounds can be used for filtering purpose in the query process. Specifically, we can select data objects with objects with P [  X  LB ( Q , X ) &lt; r ]  X  c as true dismissals. The correctness of this pruning process is guaranteed by the above theorem.

To utilize this pruning process, we need e ffi cient algo-values for the observed part and two distance bounds for the missing part.

The first sub-problem can be solved easily. Since  X  central chi-square distribution, these two probabilities can be calculated with the cumulative distribution function (c df) of noncentral chi-square distribution or by a table lookup approach.

Consider the second sub-problem. The naive method to compute any one of the four bounds is extremely time-consuming since we have to enumerate all the C | X obs | | Q | covery versions. Below we introduce a dynamic program-ming based algorithm to get these four bounds in O ( | X obs ( | Q |  X  | X obs | ) 2 ) time. Algorithm 1 is for calculating  X  and  X  LB element in the 2 n -th column of T is  X  LB  X 
LB mis ( Q , X mis ) can be inferred from the assistant array S . In order to calculate  X  UB is needed: replace function min in line 17, 21 to max and replace argmin in line 11 to argmax . It can be found that the algorithm does not require building the entire table T . Algorithm 1 Calculate  X  LB INPUT: Query Q , | Q | = m and dimension incomplete data object X obs , | X obs | = n (0 &lt; n &lt; m ).
 OUTPUT:  X  LB assistant array S ).
 X Construct two m  X  (2 n + 1) matrices T and S , where the ( i th , j th ) element of T is initialized to ( Q i  X  X is an assistant array initialized with (0 , 0) for each ele-ment. 1: for j = 1 to 2 n + 1 do 2: if j = 1 then 3: for i = 1 to m  X  n do 4: S [ i ][ j ]  X  ( i -1,1) 5: if i &gt; 1 then 6: T [ i ][ j ]  X  T [ i ][ j ] + T [ i  X  1][ j ] 7: end if 8: end for 9: else if j &gt; 2 and jmod 2 = 1 then 11: p  X  argmin 12: T [ i ][ j ]  X  T [ i ][ j ] + T [ i  X  p ][ j  X  2( p  X  1)] 13: S [ i ][ j ]  X  ( i  X  p , j  X  2( p  X  1)) 14: end for 15: else if j &gt; 2 and jmod 2 = 0 then 16: for i = j 2 to j 2 + m -n do 18: end for 19: end if 20: end for Thus its computation complexity is O ( | X obs | ( | Q | X  X  X Compared with the naive method which has to enumerate improvement.
 X obs = (2 , 4 , 8). Then X  X  = ( 2 , 2 , 3 , 4 , 6 , 8 , 8 ) , where the circled elements are added by the imputation strat-egy. The initialized T is shown in Figure 2(a). The algo-rithm starts the calculation from the bottom of the first col-umn to top right. In step.1, T [1][1] = 1 remains unchanged. T [1][2] = 25 is replaced with T [1][1] + T [2][1] = 25 + 1 = 26. In the second column, we do nothing. In Step.2, we deal with the third column of T . T [3][3] = 4 is replaced with maining steps are shown in Figure 2. Finally, from the 6-th column in Figure 2(g), we find  X  2 LB which is the minimal element in the column. In order to find  X 
LB mis ( Q , X mis ), we first find the minimal value among those in the top of column 1,3,5,7 of T that is min { 26,5,1,10 } . We find T [4][5] = 1 is the minimal. Thus the imputed vari-able with mean value 6 will be matched to 6 in Q . Then row and first column of T , where the corresponding impu-tation is with mean value 2 and is matched to 3 in Q . Thus,  X  puted random variables with mean value 2 and 6 respec-tively.
This section presents a probability triangle inequality which is also used for results pruning during query process. Theorem 2 (Probability Triangle Inequality) . Given a query Q and a multidimensional data object R ( | Q | = | R | ). For a dimension incomplete data object X obs whose under-lying complete version is X, we have: Proof. (a) From Theorem 1, we have Thus, for any recovery version X rv of X , we have In metric space, the triangle inequality holds, thus Thus we have Therefore, data object R , some data objects in database can be deter-Since the required dynamic programming computation can be finished in advance without knowing the query, this eval-uating process can be done in O( | Q | ) time.
 X obs = (2 , 4 , 8), r = 2, and c = 0 . 2. Refer to Exam-ple 3, we know  X  LB x ) 2 + (6  X  x 2 ) 2 , where x 1 and x 2 are imputed random variables with mean value 2 and 6 respectively. The variances (  X  2 ) of them are both 6.222 (the variance of X distribution with degree of freedom 2 and noncentral-ity parameter [(3  X  2) 2 + (6  X  6) 2 ] / 6.222 = 0.161. Then, P [  X  LB ( R , X )  X   X  ( Q , R ) &lt; r ] = P {  X  2 LB 2) 2  X  14] / X  2 } = P [  X  2 Therefore, based on Theorem 2, we know P [  X  ( Q , X ) &lt; r ] &lt; c , indicating X obs is not a result of query Q . ues need to be stored for each ( R , X obs ) pair. One is  X  trality parameters of random variable  X  2 LB  X  controls the tradeo ff between query processing time and storage. problem of probabilistic similarity query on dimension in-complete data is described in Algorithm 2.
 ing strategy with aforementioned pruners to speed up the query process. Specifically, we first use assistant objects i n S
R to examine data objects in the database based on prob-ability triangle inequality. Then confidence lower and up-per bounds are used to further evaluate the remaining can-didates. Only those data objects that the former two steps cannot judge will be evaluated by the naive verification al-gorithm.

Recall from Eq. 3 in Section 3, the straightforward way for confidence evaluation is to examine all possible recover y versions of the incomplete data. Here we also provide an optimized enumeration process that prunes out some cases safely. From Eq. 4, we find that if  X  ( Q obs , X obs )  X  r , if we can judge  X  LB r , there will be no need to examine X obs . Furthermore, ac-cording to Eq. 6, obviously, for a given query Q and an rived from Q ( | Q  X  |  X  | X obs | ) that satisfies  X  LB r , then for all Q  X  obs derived from Q  X  ( | Q  X  obs | X | X  X 
LB obs ( Q  X  obs , X obs )  X  r . Thus, we only need to evaluate part sions with a recursive procedure: for Q obs derived from Q ( | Q obs |  X  | X obs | ), if  X  LB from Q obs will not be evaluated. Due to space limitation, we will not discuss the details of this recursive procedure.
When our algorithm is used, usually a large portion of time will be consumed by the naive verification process. In order to have higher e ffi ciency, however, we can avoid the naive verification process and simply regard the remaining candidates as query results (or dismissals, depending on th e requirements of query precision and recall). Such a strateg y is reasonable for some applications where the two probabil-ity confidence bounds are e ff ective for pruning results. In this case, this simplified algorithm will not cause remark-able decrease in query results quality. Our experimental re -sults shown in Section 5.2.3 justify the e ff ectiveness of such a simplified strategy.
In this section, we present the experimental results. The goal of our experiments is to (a) evaluate the e ff ectiveness and the e ffi ciency of the overall method for probabilistic similarity query on dimension incomplete data and various key techniques proposed, (b) study the influence of di ff er-ent parameters and data sets on our method, (c) compare the performance of our approach with other solutions for handling dimension incomplete data.
Two real data sets are used in our experiments. The first one is the Standard and Poor 500 index historical stock data 1 . This data set contains stock prices of about 541 companies collected over one year. We use the opening Algorithm 2 Probabilistic Similarity Query INPUT: the dimension incomplete database D , query Q , the set of assistant data objects S R .
 OUTPUT: the results set S result . 1: for all X in D do 2: for all R in S R do 3: if P [  X  LB ( R , X )  X   X  ( Q , R ) &lt; r ]  X  c then 5: goto 1 to evaluate next X in D 6: else if P [  X  UB ( R , X ) +  X  ( Q , R ) &lt; r ]  X  c then 8: goto 1 to evaluate next X in D 9: end if 10: end for 11: if P [  X  LB ( Q , X ) &lt; r ]  X  c then 13: else if P [  X  UB ( Q , X ) &lt; r ] &gt; c then 15: else 16: do naive confidence evaluation 17: end if 18: end for price data of each stock, which is a vector of 251 dimen-sions. Since the final step of query process is very time-consuming, we need to sample the original data and have a lower dimension data set. Thus, we construct a new one with 30 dimensions by segmenting the data in Standard and Poor 500 index historical stock data set, resulting in total ly 541  X  8 = 4 , 328 data objects with 30 dimensions (denoted by S&amp;P500). Another data set contains 32-dimensional im-age features extracted from 68,040 images (denoted by IM-AGE) 2 .
 plete. Similarity query results on the complete data are use d as  X  X round truth X  in evaluating precision and recall of our approach. We construct the dimension incomplete data set by randomly removing some dimensions of each data ob-ject. The number of missing data elements are controlled by missing ratio which is the ratio of the number of missing di-mensions over the original dimensionality. Totally 100 dat a objects, which are randomly sampled from the data set, are used as queries. 5.2.1 E ff ectiveness of probabilistic similarity query on Experiments in this section is to evaluate the e ff ectiveness of our method on various data sets, and with various param-
Figure 3. Query precision on S&amp;P500 data set eter settings. Figure 3, 4, 5 and 6 show the quality of query result measured by precision and recall.

It can be observed from the results that our method (PSQ) achieves a satisfactory performance in querying di-mension incomplete data. Particularly, though both preci-sion and recall decrease with the increase of missing ratio, even when 15% dimensions are missing, our method (PSQ) achieves more than 0.9 precision and recall on S&amp;P500 data set. For image histogram data set, if distance threshold and confidence threshold are well chosen, a good query quality can also be achieved. This justifies the usefulness of our ap-proach in real applications. We also compare our approach with a simple method ( simpleL 2), which (1) randomly re-moves some elements of the query to construct a new query with the same dimensionality as the dimension incomplete candidates in database, and (2) uses Euclidean distance to measure whether their distance is lower than threshold r . From the results, we can see that our method is able to better reflect the distance and thus achieves better query quality.
Moreover, precision and recall on S&amp;P500 data set are higher than those on the image histogram data set. This is due to the intrinsic characteristics of these two data sets. S&amp;P500 data set holds the typical characteristics of time series and has an excellent correlation between the consec-utive data elements, while image histogram data does not have this property. Therefore, the imputation method used in our experiment fits better for S&amp;P500 data set. This also shows the importance of assigning a suitable imputa-tion method in handling the dimension incomplete data. 5.2.2 E ff ect of the confidence threshold It can be observed from Figure 3, 4, 5 and 6 that recall value decreases and precision value increases with the increase of the confidence threshold. To make it clearer, Figure 7 shows the relationship between the confidence threshold c and precision / recall ( missing ratio = 0.1, r = 60 for S&amp;P500 and r = 0.7 for IMAGE). This experiment also indicates that by setting a proper c , our method is able to achieve both good precision and good recall on real data sets. 5.2.3 E ff ectiveness of di ff erent pruners In this section, we study the usefulness of the four pruners proposed in this paper by examining their pruning power. number of the data objects in the database, and N de finite the number of data objects in the database judged as dis-missals or search results by the pruner.
 gle inequality with various number of assistant data object s ( c = 0 . 2). For S&amp;P500 data, when only 10 assistant data ob-jects were used, the pruning power is more than 60%. For image histogram data, in most cases, the pruning power of the probability triangle inequality is more than 20% with 20 assistant data objects.
 has good pruning power in query process by involving only
Figure 8. Pruning power of probability trian-gle inequality a few assistant data objects. Moreover, the performance wil l improve when more assistant data objects are available. But after the pruning power reaches a certain level, the increas e of assistant data objects has no significant further impact o n the pruning power.

We also examined the pruning power of the four pruners proposed in this paper, including probability triangle in-equality using confidence lower bound and confidence up-per bound (denoted by pruner 1 and pruner 2 respectively), confidence lower and upper bounds (denoted by pruner 3 and pruner 4 respectively). Figure 9 shows the pruning power of each pruner with various r ( missing ratio = 10%, c = 0.1, 20 assistant objects). Firstly, this justifies the use-fulness of each pruner proposed in this work. Secondly, we can find for S&amp;P500 data set, about 90% data in total can be pruned, which means only a small part of data need to do naive verification. For image histogram data set, in the worst case, there will be 50% data need to do naive veri-fication. Thirdly, the pruning power of these four pruners are influenced by threshold r significantly. When specify-ing a smaller r (i.e., the user wants to get relatively a small amount of query results), more data are pruned by two lower bound based pruners pruner 1 and pruner 3. In contrast, a larger distance threshold produces a larger pruning power for pruner 2 and pruner 4.

Since the time complexity of naive verification is poor, we study if naive verification is necessary. We try two sim-plified verification strategies: for data objects that the fo r-mer four pruners cannot judge, strategy Pos simply outputs them as query results, and strategy Neg , by contrast, judges them as dismissals. Obviously, Pos might result in more false positives, while Neg might produce more false nega-tives. The query quality of these two strategies and doing naive verification (denoted by DoN ) are shown in Table 1 ( c = 0.1). From the results, we can find for S&amp;P500 data set, the precision and recall without doing naive verifica-tion are very close to those doing naive verification. For image histogram data, however, the query quality depends more heavily on the naive verification. It can be concluded from this result that for some data sets, a high query quality can be achieved without utilizing the slow naive verificatio n process. 5.2.4 Performance analysis There are mainly three steps in our approach: (1) pruning with probability triangle inequality; (2) pruning with con -fidence lower and upper bounds; (3) naive confidence ver-ification. We test time costs of these steps respectively us-ing S&amp;P500 data set on a computer with 3.0GHz CPU and 1.0GB RAM, and average them over all queries. Results are shown in Figure 10. Particularly, naive confidence ver-ification takes much longer time than the other two steps. However, Table 1 in section 5.2.3 indicates that the naive confidence verification process is not so necessary for han-dling S&amp;P500 data set, which means the e ffi ciency of the overall query processing system can be improved signifi-cantly without losing much in performance.
This paper addresses the similarity query problem on dimension incomplete data, which is of both practical im-portance and technical challenge. We adopt a probabil-ity framework to model this problem. In order to solve this problem e ffi ciently, an approach is introduced based on the proposed lower / upper confidence bounds and the probability triangle inequality. The proposed methods are proved to be theoretically correct. Given a query Q and a database containing dimension incomplete data X obs , com-pared with the brute force method whose time complex-improvement where most data objects can be handled in O ( | X obs | ( | Q | X  X  X obs | ) 2 ) or even O ( | Q | ) time.
Experiments are conducted on two real data sets. The re-sults indicate: (1) our approach achieves satisfactory per for-mance in querying dimension incomplete data; (2) both the probability triangle inequality and the confidence bounds have a nice pruning power and improve query e ffi ciency significantly. This verifies that our method is promising in handling dimension incomplete data.
 Our future work will focus on the following aspects. Since a probability triangle inequality holds, we plan to de -velop an index structure to make the query process faster. Besides that, it will be interesting and useful to extend our query strategy to fit for other similarity measurements.
The work was supported by NSFC 90924003, 60973103 and 863 funding 2007AA01Z156.

