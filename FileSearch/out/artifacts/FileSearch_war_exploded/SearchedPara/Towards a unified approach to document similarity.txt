 1. Introduction
Document similarity search (i.e. query by example) is to find documents similar to a query document in a text corpus or on the Web. 1 A ranked list of similar documents is required to be returned to users. The typical kind of similarity search is K-nearest neighbor search, namely K-NN search, which is to find K documents most similar to the query document.

Similarity search is widely used to improve traditional document search engines by allowing the user to use a document as a query and thus releasing the burden of extracting keywords from the document to formulate the query for the user. Traditional search engines take a query of several terms as input and return a set of relevant documents that match the query terms. However, the query terms are sometimes difficult to define and prone to be inaccurate because not every user is a search expert. Note that it happens very often that when users already have an interesting document, they just want to see more relevant documents, and they can use the document as a query and perform similarity search to get similar documents. A few search engines have provided the functionalities of document similarity search or recommendation. For example, Google form an advanced search with  X  X  X elated X  X  option to find-similar web pages with a user-specified web page and
CiteSeer.IST 3 provides a list of similar papers with the currently browsed paper. Moreover, document simi-larity search can be used for relevance feedback in search engines, where the retrieved documents can be re-ranked by their similarity with the documents the user shows interest in.

Different from short-query based search, similarity search usually takes a full document as query and the query is much longer than keyword-based short queries. The query of full document is usually characterized by a series of subtopics and contains more redundant and ambiguous information and even greater noise effects stemmed from the presence of a large number of words unrelated to the overall topic in the document.
Document similarity search can be intuitively considered as a particular kind of text retrieval and thus various text retrieval functions can be attempted for this task.

In most previous approaches to similarity search ( Cruz, Borisov, Marks, &amp; Webb, 1998; Haveliwala, Gio-considered as a single unit for similarity calculation. However, a document as a whole may not be appropriate to represent a single topic because a document usually contains multiple subtopics. Specifically, a text docu-occur in the context of a few main topic discussions. And a web page usually contains various contents such as navigation, decoration, interaction, contact information, which may be unrelated to the main topic of the web page. Furthermore, a web page often contains multiple topics that are not necessarily relevant to one another.
Therefore, detecting the semantic content structure of a document could potentially improve the performance of document similarity search, as well as other standard IR tasks. Moreover, most approaches find relevant documents to a query document only by the pairwise comparison between the document and the query, thus ignoring the intrinsic global manifold structure of the whole set of documents. In order to address the above two limitations, we evaluate document similarity at a finer granularity of document block (or passage) instead of at the coarse granularity of the whole document, each block representing a semantic unit with coherent text fold-ranking process is employed to make full use of the relationships between the document blocks. The prior assumption of manifold-ranking is: (1) nearby points are likely to have the same ranking scores; (2) points on the same structure (typically referred to as a cluster or a manifold) are likely to have the same ranking scores, similar to the cluster hypothesis in the IR context ( Hearst &amp; Pedersen, 1996; van Rijsbergen, 1979 ). In the manifold-ranking process, document blocks can spread their ranking scores to their nearby neighbors via a weighted network.

In more details, the proposed approach consists of the following two processes: initial ranking and re-rank-ing. In the initial ranking process, a small number of documents are initially retrieved based on the popular
Cosine function. In the re-ranking process, the query document and the initially retrieved documents are seg-mented into blocks by using either the TextTiling algorithm ( Hearst, 1994, 1997 ) and the VIPS algorithm ( Cai, blocks and each block obtains its ranking score. Lastly, a document gets its final retrieval score by fusing the ranking scores of the blocks in the document. The initially retrieved documents are re-ranked and the re-ranked list is returned to users. Experimental results on the TDT data and the ODP data show the improved performance of the proposed approach over baseline approaches for similarity search of text documents and web pages, respectively. Document block is validated to be a more suitable unit than the whole document in the manifold-ranking process.

The main contributions of this paper are two-fold: (1) The manifold-folding algorithm is applied for the first time on document blocks instead of whole documents to improve the retrieval results; (2) The proposed approach is applied not only to text retrieval but also to web retrieval, and the evaluation results on both text documents and web pages show the robustness of the approach.

The rest of this paper is organized as follows: Section 2 introduces related works. The proposed approach is described in detail in Section 3 . Sections 4 and 5 give the experiments and results on the TDT data and the
ODP data, respectively. Lastly, we present our conclusion and future work in Section 6 . 2. Related works
The retrieval performance of a similarity search engine relies heavily on the retrieval function for evaluating document similarity. The popular retrieval functions (including similarity measures) used in current text retrie-1999; van Rijsbergen, 1979 ), the BM25 function in the Okapi system ( Robertson &amp; Walker, 1994; Robertson,
Walker, &amp; Beaulieu, 1999 ), the vector space model with document length normalization in the Smart system
Lee, &amp; Domshlak, 2005 ), among which the BM25 function is one of the best models for short-query based search, and the standard Cosine function is considered as one of the best models for document similarity search because of its good ability to measure the similarity between two full documents. More features have been investigated for similarity search of web pages. Haveliwala et al. (2002) propose a new evaluation strat-egy using Web hierarchies, such as Open Directory, in place of user feedback to evaluate the task of finding pages on the Web that are similar to a query page. Web page structure denoted by HTML tags has also been the factors of the textual content and the structural information when evaluating web page similarity. Another different source widely-used to determine similarity is the link structure between web pages, such work includ-
Chen, &amp; Yu, 2004 ). Smucker and Allan (2006) examine find-similar, the feature provided by search systems to request documents similar to a given document, as a search tool, like relevance feedback, for improving retrie-val performance. In this study, we do not make use of the link structure between web pages.
In recent years, a number of graph-based methods have been proposed to improve text or web retrieval results. For web retrieval, the graph is built based on explicit links between web pages. PageRank ( Page, Brin,
Motwani, &amp; Winograd, 1998 ) and HITS ( Kleinberg, 1999 ) are the most popular algorithms that make use of web link structure to improve retrieval performance. PageRank makes use of the link structure between web pages and judge the importance of web pages according to the  X  X  X otes X  X  or  X  X  X ecommendations X  X  from their neighboring pages. HITS differs from PageRank in that HITS differentiates two kinds of salient web pages: texts. Zhang et al. (2005) propose a novel ranking scheme named Affinity Ranking (AR) to re-rank search results by optimizing both information richness and diversity. The two metrics are calculated using the ran-dom walk model and the greedy algorithm, respectively, from a directed link graph named Affinity Graph (AG), which models the structure of a group of documents based on the asymmetric content similarities between each pair of documents. Diaz (2005) exploits the cluster hypothesis ( van Rijsbergen, 1979 ) directly by a score regularization process, which adjusts ad hoc retrieval scores from an initial retrieval so that topi-cally related documents receive similar scores. Kurland and Lee (2005) propose an approach to improving the retrieval precision by adapting the PageRank algorithm for defining and computing centrality within a direc-ted graph and then re-ranking non-hyperlinked document sets. Later, they present an approach ( Kurland &amp;
Lee, 2006 ) to utilize cluster information based on the HITS algorithm. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents and clusters, on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them. Qin, Liu, Zhang, Chen, and Ma (2005) propose a generic relevance propagation framework and provide a comparison study on the effectiveness and efficiency of various representative propagation models. The above methods have not been explored for document similarity search.
 Graph-based methods have also been proposed for document summarization ( Erkan &amp; Radev, 2004;
Mihalcea &amp; Tarau, 2004 ) and question answering ( Otterbacher, Erkan, &amp; Radev, 2005 ), where sentences are ranked based on the graph with sentence-to-sentence relationships. Other close related works include pas-sage retrieval ( Callan, 1994; Kaszkiel &amp; Zobel, 1997 ), which ranks passages instead of whole-documents. It can provide convenient units of text to return to the user, avoid the difficulties of comparing documents of different length, and enable identification of short blocks of relevant material amongst otherwise irrelevant text. 3. The proposed approach 3.1. Overview
The aim of the proposed approach is two-fold: one is to evaluate the similarity between the query document and a document at a finer granularity by segmenting the documents into semantic blocks, which addresses the limitation of present similarity metrics based on the whole document, usually characterized as a series of blocks with semantically coherent content; the other is to evaluate the similarity between the query document and a document by exploring the relationships (i.e. the intrinsic manifold structure) between all the obtained blocks in the feature space, which addresses the limitation of present similarity metrics based only on pairwise comparison.

The proposed approach first segments all documents (including the query document) into blocks, and then applies the manifold-ranking process on the document blocks. All the blocks of a document obtain their rank-ing scores and the ranking score of the document is obtained by fusing the ranking scores of its blocks.
Note that it is of high computational cost to apply the manifold-ranking process to all the documents in the collection, so the above manifold-ranking process is taken as a re-ranking process. First, we use a popular retrieval function (e.g. Cosine) to efficiently obtain an initial ranking of the documents, and then the initial k documents are re-ranked by applying the above manifold-ranking process.

Formally, given a query document q and the collection C , the proposed approach consists of the following four steps: ranked list in response to the query document q , and the set of top k documents in the list is denoted as
InitScore ( d i ). 2. Document segmentation : By using the TextTiling algorithm or the VIPS algorithm, the query document q is segmented into a set of blocks v q ={ x 1 , x 2 , ... , x and the total set of blocks for D init is v D 3. Manifold-ranking : The manifold-ranking process in applied on the whole set of blocks: v  X  v each block x j ( p +1 6 j 6 n )in v D 4. Score fusion : The final score FinalScore ( d i ) of a document d ranking scores of its blocks. The documents in D init are re-ranked according to their final scores and the re-ranked list is returned.

The steps 2 X 4 are key steps in the re-ranking process and they will be illustrated in detail in next sections, respectively. 3.2. The document segmentation process
This step aims to decompose the text documents or web pages into text blocks, with each block representing a single coherent topic. Due to different characteristics of the text documents and web pages, different algo-rithms are employed for block segmentation, i.e. the TextTiling algorithm ( Hearst, 1994, 1997 ) is employed for text segmentation and the VIPS algorithm ( Cai et al., 2003; Song et al., 2004; Yu et al., 2003 ) is employed for web page segmentation. The details of the two algorithms are, respectively, described as follows: 3.2.1. The TextTiling algorithm for text segmentation
There have been several methods for division of text documents according to units such as sections, para-graphs, or fixed length sequences of words, or semantic passages given by inferred shift of topic ( Hearst &amp;
Plaunt, 1993 ). In this study, we adopt semantic passages to represent subtopics in a document. As mentioned context of a few main topic discussions. For example, a news text about China X  X S relationship, whose main topic is the good bilateral relationship between China and the United States, can be described as consisting of the following subdiscussions (numbers indicate paragraph numbers): 1 Intro-the establishment of China X  X S relationships . 2 X 3 The officers exchange visits . 4 X 5 The culture exchange between the two countries . 6 X 7 The booming trade between the two countries . 8 Outlook and summary .

We expect to acquire the above subtopics in a document and use them in the manifold-ranking process instead of the whole document. The most popular TextTiling algorithm is used to automatically subdivide text into multi-paragraph units that represent subtopics.

The TextTiling algorithm detects subtopic boundaries by analyzing patterns of lexical connectivity and word distribution. The main idea is that terms that describe a subtopic will co-occur locally, and a switch to a new subtopic will be signaled by the ending of co-occurrence of one set of terms and the beginning of the co-occurrence of a different set of terms. The algorithm has the following three steps: (1) Tokenization : The input text is divided into individual lexical units, i.e. pseudo-sentences of a predefined (2) Lexical score determination : All pairs of adjacent lexical units are compared and assigned a similarity (3) Boundary identification : The resulting sequence of similarity values is graphed and smoothed, and then is
For TextTiling, subtopic discussions are assumed to occur within the scope of one or more overarching main topics, which span the length of the text. Since the segments are adjacent and non-overlapping, they are called TextTiles. In this study, we denote a TextTile as a block.

The computational complexity is approximately linear with the document length, and a few efficient imple-mentations are available, such as Kaufmann ( Kaufmann, 1999 ) and JTextTile ( Choi, 1999 ). 3.2.2. The VIPS algorithm for page segmentation
Several kinds of methods have been proposed for web page segmentation, among which the most popular ones are DOM-based segmentation ( Chen, Zhou, Shi, Zhang, &amp; Qiu, 2001 ), location-based segmentation ( Kovacevic, Diligenti, Gori, &amp; Milutinovic, 2002 ) and Vision-based Page Segmentation (VIPS) ( Cai et al., 2003; Song et al., 2004; Yu et al., 2003 ). Compared with other segmentation algorithms, VIPS excels in both an appropriate partition granularity and coherent semantic aggregation. In this study, we adopt the VIPS algorithm to segment web pages into semantic blocks. Note that other segmentation algorithms can also be explored as well, which is however not the focus of this study.

The VIPS algorithm makes full use of page layout features such as font, color and size and takes advantage of visual cues to obtain the vision-based content structure of a web page. The algorithm can successfully bridge the gap between the DOM structure and the semantic structure. The page is partitioned based on visual separators and structured as a hierarchy closely related to how a user will browse the page. Content related parts could be grouped together even if they are in different branches of the DOM tree.

The VIPS algorithm first extracts all the suitable nodes from the HTML DOM tree, and then finds the sep-arators between these nodes. Here, separators denote the horizontal or vertical lines in a web page that visually do not cross any node. Based on these separators, the semantic tree of the web page is constructed. A value called degree of coherence (DoC) is assigned for each node to indicate how coherent it is. Consequently, VIPS can efficiently keep related content together while separating semantically different blocks from each other.
Each block in VIPS is represented as a node in a tree. The root is the whole page; inner nodes are the top level coarser blocks, and all leaf nodes consist of a flat segmentation of a web page. The granularity of seg-mentation in VIPS is controlled by a predefined degree of coherence (PDoC), which plays a role as a threshold of the most appropriate granularity for different applications (usually set to 5). The segmentation only stops when the DoCs of all blocks are no smaller than the PDoCs. Fig. 1 shows the result of using VIPS to segment a sample CNN web page ( Song et al., 2004 ).

VIPS is also very efficient. Since we trace down the DOM structure for visual block extraction and do not analyze every basic DOM node, the algorithm is totally top-down. Furthermore, the PDoC can be pre-defined, which brings significant flexibility to segmentation and greatly improve the performance. 3.3. The manifold-ranking process
Manifold-ranking ( Zhou, Bousquet, et al., 2003; Zhou, Weston, et al., 2003 ) is a universal ranking algo-rithm initially used to rank data points along their underlying manifold structure. Text retrieval experiments using manifold-ranking of documents have been performed and promising results have been obtained on the 20-newsgroups dataset ( Zhou, Weston, et al., 2003 ). An intuitive description of manifold-ranking is as follows:
A weighted network is formed on the data, and a positive rank score is assigned to each known relevant point and zero to the remaining points which are to be ranked. All points then spread their ranking score to their nearby neighbors via the weighted network. The spread process is repeated until a global stable state is achieved, and all points obtain their final ranking scores.

In our context, the data points are denoted by the document blocks in the query document q and the doc-uments in D init . The manifold-ranking process in our context can be formalized as follows:
Given a set of data points v  X  v q [ v D ment blocks in the query document q and the rest n p points are the document blocks in the documents in
D init . Let f : v ! R denote a ranking function which assigns to each point x can view f as a vector f =[ f 1 , f 2 , ... , f n ] T . We also define a vector y =[ y in D init , where x j 2 d i , d i 2 D init , and InitScore ( d ument is used as the initial ranking scores of the blocks in the document. The manifold-ranking algorithm goes as in Fig. 2 .

In the above iterative algorithm, the normalization in the third step is necessary to prove the algorithm X  X  convergence. The fourth step is the key step of the algorithm, where all points spread their ranking score to their neighbors via the weighted network. The parameter of manifold-ranking weight a specifies the relative contributions to the ranking scores from neighbors and the initial ranking scores. Note that self-reinforcement is avoided since the diagonal elements of the affinity matrix are set to zero.

The theorem in ( Zhou, Weston, et al., 2003 ) guarantees that the sequence { f ( t )} converges to where b =1 a . Although f * can be expressed in a closed form, for large scale problems, the iteration algo-rithm is preferable due to computational efficiency. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any point falls be-low a given threshold (0.0001 in this study).

Using Taylor expansion, we have
From the above equation, if we omit the constant coefficient b , f * can be regarded as the sum of a series of the blocks in the documents is gradually incorporated into the ranking score. 3.4. The score fusion process
The final retrieval score of a document d i 2 D init is computed by fusing the ranking scores of its blocks as follows: where k j measures the importance of the block x j in document d block is. j d i j represents the number of blocks in document d with more blocks.

For text documents, k j = sim cosine ( x j , d i ) is the Cosine similarity between the block (i.e. TextTile) x associated document d i , which measures the importance of the block (i.e. TextTile) x block x j in the web page d i ( Cai, He, Wen, &amp; Ma, 2004 ), where r is a normalization factor to make the sum of k j for blocks in d i to be 1, i.e.

Finally, the documents in D init are re-ranked according to their final scores and the re-ranked list is returned. Note that only the top k documents (i.e. the documents in D ranked and the rest documents in the initial ranked list still hold their initial ranks. 4. Evaluation on TDT data
This section aims to evaluate the proposed approach to similarity search of text documents on a text corpus. 4.1. Experimental setup 4.1.1. Baseline approach
In the experiments, the manifold-ranking based approach ( X  X  X R + TextTile X  X ) is compared with the follow-ing baseline approaches:  X  X  X M25 X  X ,  X  X  X VSM X  X ,  X  X  X osine X  X  and  X  X  X R + Document X  X . The  X  X  X M25 X  X ,  X  X  X VSM X  X  and  X  X  X osine X  X  baselines do not apply the manifold-ranking process and directly ranks the documents by their similarity with the query document based on respective similarity functions. The  X  X  X R + Document X  X  baseline is adopted in ( Zhou, Weston, et al., 2003 ), which uses the whole document instead of TextTile in the manifold-ranking process, and thus it does not need the steps of text segmentation and score fusion. The manifold-rank-ing process is also applied on top-retrieved documents by other retrieval functions.

The Cosine function is one of the most popular measures for evaluating document similarity and it is based on the vector space model (VSM) ( Baeza-Yates &amp; Ribeiro-Neto, 1999; Salton, Wong, &amp; Yang, 1975 ). Each document d is represented by a vector with each dimension referring to a unique term. The weight w ciated with term t is calculated by the tf d , t . idf t formula, where tf document d and idf t = 1 + log( N / n t ) is the inverse document frequency, where N is the total number of doc-between the query document q and any document d , can be defined as the normalized inner product of the two corresponding vectors as follows:
The BM25 function ( Robertson &amp; Walker, 1994; Robertson et al., 1999 ) is one of the most popular retrieval models in a probabilistic framework and is widely-used in the Okapi system. Given the query document q , the similarity score for document d is defined as follows 4 : where dltf d is the sum of term frequencies in d ; avedltf is the average of dltf are tuned on a training corpus and they are set as follows: K = 2.0, b = 0.8.

The vector space model with document length normalization (NVSM) ( Salton, 1991; Singhal et al., 1996 )is also a popular retrieval model and is used in the Smart system ( Salton, 1991 ). Given the query document q , the similarity score for document d is defined as follows: where dlb d is the number of unique terms in d ; avetf d is the average of term frequencies in d (i.e.  X  X  dltf avedlb is the average of dlb d in the collection; S is tuned and set to 0.2.
For  X  X  X R + Document X  X  and  X  X  X R + TextTile X  X , the Cosine function is heuristically used for initial ranking and manifold-ranking. Note that other similarity measures can also be explored, but this study focuses only on the widely-used Cosine measure. 4.1.2. Dataset A ground truth dataset is required to perform the experiments. We built the ground truth dataset from the
TDT-3 corpus, which has been used for evaluation of the task of topic detection and tracking ( Allan, Carbo-nell, Doddington, Yamron, &amp; Yang, 1998 ) in 1999 and 2000. TDT-3 corpus is annotated by Linguistic Data
Consortium (LDC) from 8 English sources and 3 Mandarin sources for the period of October through Decem-ber 1998. 120 topics are defined and about 9000 stories are annotated over these topics with an  X  X  X n-topic X  X  table presenting all stories explicitly marked as relevant to a given topic. According to the specification of
TDT, the on-topic stories within the same topic are similar and relevant. After removing the stories written in Chinese, there remain 8458 English stories. We randomly chose 40 topics as a test set, while the others were used as a training set for parameter tuning. 6
Sentence tokenization was firstly applied to all documents. Stop words were removed and Porter X  X  stemmer ( Porter, 1980 ) was used for word stemming. The JTextTile tool ( Choi, 1999 ) with default setting was employed to segment each document into TextTiles. The total stories are used as the document collection for search, the first document within the topic is simply used as the query document without any tuning, and all the other documents within the same topic are the relevant (similar) documents, while all the documents within other topics are considered irrelevant (dissimilar) to the query document. A ranked list of 500 documents was required to be returned for each query document based on each retrieval approach. The higher the document is in the ranked list, the more similar it is with the query document. For the proposed manifold-ranking pro-cess, the number of re-ranked documents is heuristically set to 50, i.e. j D 4.1.3. Evaluation metric
As in TRECT 7 experiments, we use the average precisions at top N results, i.e. P @5 and P @10, and the mean average precision (MAP) as evaluation metrics.

The precision at top N results for a query is calculated as follows: where R is the set of top N retrieved documents, and C is the set of similar documents defined above for a given query document. The precision is calculated for each query and then the values are averaged across all queries.

The non-interpolated average precision (AP) for a query is a number averaged over all precision values cal-culated after each relevant document is retrieved. If a relevant document is not retrieved, the corresponding precision value is 0.0. The mean average precision (MAP) is the mean AP over all queries.

Note that the number of documents within each topic is different and some topics contain even less than 5 documents or 10 documents, so its corresponding P @5 or P @10 may be low. 4.2. Experimental results
The precision values of the proposed approach ( X  X  X R + TextTile X  X ) and the baseline approaches (i.e.  X  X  X osine X  X  and  X  X  X R + Document X  X ) are compared in Table 1 , where the manifold-ranking weight a is set to 0.3, which is tuned on the training set. Seen from Table 1 , the proposed approach significantly outperforms all the baseline systems over P @5 and P @10 metrics. We can also see that the  X  X  X R + Document X  X  baseline achieves almost the same P @5 value with the  X  X  X osine X  X  baseline and the higher P @10 and MAP values than the  X  X  X osine X  X  baseline, which demonstrates that manifold-ranking process can benefit document ranking.
Among the three baselines without using manifold-ranking (i.e.  X  X  X M25 X  X ,  X  X  X VSM X  X ,  X  X  X osine X  X ), the BM25 function and the NVSM function perform poorly, while the Cosine function achieves a comparatively high performance, especially over the MAP metric. Note that both the BM25 function and the NVSM function work well for short-query based text retrieval. The result shows that measuring the similarity between full doc-uments is different from measuring the similarity between a short query and a full document (i.e. the keyword search in TREC experiments). The query of a full document is different from the relatively short query in that the full document contains more redundant and ambiguous information and even greater noise effects stem-ming from the presence of a large number of words unrelated to the overall topic in the document, and thus measuring the similarity between full documents and measuring the similarity between the short query and a full document are not identical.

The performances of the two MR-based approaches (i.e.  X  X  X R + TextTile X  X  &amp;  X  X  X R + Document X  X ) with different manifold-ranking weight a are shown and compared in Figs. 3 X 5 . Seen from the figures, with appro-significantly outperform the approach of  X  X  X R + Document X  X  over P @5 and P @10 metrics ( t -test: p -value &lt; 0.05), which demonstrates that TextTile is a more appropriate unit than the whole document for the manifold-ranking process. This result can be explained by that a document is usually characterized as a sequence of subtopical discussions that occur in the context of a few main topic discussions and each TextTile can represent a subtopic with coherent text, and thus the manifold-ranking process can work at a finer granularity.

We now compare the performances of the proposed approaches ( X  X  X R + TextTile X  X ) with manifold-ranking ( a = 0.3) and without manifold-ranking ( a = 0). Seen from the figures, the P @5 and P @10 values of  X  X  X R + TextTile X  X  with a = 0.3 are slightly better than those of  X  X  X R + TextTile X  X  with a = 0, while the
MAP value of  X  X  X R + TextTile X  X  with a = 0.3 is significantly better than that of  X  X  X R + TextTile X  X  with a =0 ranking approach. The results show that in addition to the contribution of using TextTiles instead of whole documents, the manifold-ranking process does contribute to the final retrieval performance.
Fig. 6 explores the influence of the number of initially retrieved documents (i.e. k ) on the performance of the proposed approach (i.e.  X  X  X R + TextTile X  X ). Seen from the figure, when k is larger than 75, the system per-formances almost do not alter any more. This shows that a small number of initially retrieved documents work well in the re-ranking process and it does not improve the retrieval performance by increasing the number of initially retrieved documents for re-ranking. In the TDT dataset, the number of the relevant (similar) docu-ments for a query document is not large, and the top 50 documents in the initial ranked list usually include most relevant documents. Most irrelevant documents are likely to be added into the re-ranked document set when k is increased, which does not benefit to improve the retrieval performance at all. 5. Evaluation on ODP data
This section aims to evaluate the proposed approach to similarity search of web pages on the Web. 5.1. Experimental Setup
Similar to the experiments in Section 4.1 , the proposed approach ( X  X  X R + Block X  X ) is compared with the following baseline approaches:  X  X  X M25 X  X ,  X  X  X VSM X  X ,  X  X  X osine X  X  and  X  X  X R + Page X  X . The  X  X  X R + Page X  X  baseline uses the whole web page instead of page block in the manifold-ranking process, and thus it does not apply the steps of page segmentation and score fusion as in the proposed approach. For  X  X  X R + Page X  X  and  X  X  X R + Block X  X , the Cosine function is heuristically used for initial ranking and manifold-ranking. As in ( Haveliwala et al., 2002 ), we built the ground truth dataset from the directory hierarchy of Open
Directory Project (ODP). 8 The ODP maintains hierarchical directories with a large number of web pages, and all the web pages within a directory belong to the same category. Document similarity is implicitly encoded in these hierarchical directories, e.g. for a web page in the directory of  X  X  n Computers n Program-ming n Databases X  X , another web page in the same directory is assumed to be more similar to this page than any web page in the directory of  X  X  n Computers n Programming n Graphics X  X . We downloaded 9610 web pages are on average about 25 web pages in each directory. We used the VIPS tool age number of blocks in a web page is 6.59. We extracted plain texts from the whole web page and each block by removing all tags. Stop words were removed and Porter X  X  stemmer was used for word stemming. 170 web pages were randomly collected as queries in test set. Two kinds of relevance are defined as follows: (dissimilar) pages. irrelevant (dissimilar) pages.
Based on the above relevance levels, two relevance lists for a query page were built, respectively, and the retrieval performance can be obtained on either relevance list.

The total web pages were used as the collection for search, a ranked list of 500 web pages was returned in response to each query page based on a specified retrieval approach. For the proposed manifold-ranking pro-cess, the number of re-ranked web pages was set to 50, i.e. j D
Similarly, we used P @5, P @10 and MAP as evaluation metrics. Note that based on different relevance lists, we can obtain respective P @5, P @10 and MAP values. Usually the precision values based on the 2-level rel-evance list are higher than those based on the 1-level relevance list. 5.2. Experimental results
The precision values of the proposed approach ( X  X  X R + Block X  X ) and the baseline approaches are compared in Table 2 , where the manifold-ranking weight a is also set to 0.3, the same value with the evaluation on the TDT dataset. Seen from Table 2 , the proposed approach significantly outperforms the baseline approaches.
We can also see that the  X  X  X R + Page X  X  baseline achieves higher precision values than the  X  X  X osine X  X  baseline relevance, which shows that the manifold-ranking process on the granularity of the whole web page can not significantly improve the retrieval performance. Similarly, the Cosine function outperforms the BM25 func-tion and the NVSM function over all metrics, which further demonstrates that document similarity search is different from short-query based search.

The performance values of two MR-based approaches (i.e.  X  X  X R + Block X  X  &amp;  X  X  X R + Page X  X ) with different manifold-ranking weight a are shown and compared in Figs. 7 X 12 . Seen from the figures, with appropriate values of the manifold-ranking weight ( a &lt; 0.5), the proposed approach (i.e.  X  X  X R + Block X  X ) can always out-perform the approach of  X  X  X R + Page X  X . The observations demonstrate that page block is a more appropriate unit than the whole web page in the manifold-ranking process, which can be explained by that a web page usually contains various contents and multiple topics and is not appropriate to be considered as a single unit, manifold-ranking process.

We now compare the performances of the proposed approaches ( X  X  X R + Block X  X ) with manifold-ranking ( a = 0.3) and without manifold-ranking ( a = 0). Seen from Figs. 7 X 9 , the P @5, P @10 and MAP values (based on 1-level relevance) of  X  X  X R + Block X  X  with a = 0.3 are slightly better than those of  X  X  X R + Block X  X  with a = 0, while seen from Figs. 10 X 12 , the P @5, P @10 and MAP values (based on 2-level relevance) of  X  X  X R + Block X  X  with a = 0.3 are all significantly better than those of  X  X  X R + Block X  X  with a =0 ( t -test: p -value &lt; 0.05). The results show that in addition to the contribution of using page blocks instead of whole web pages, the manifold-ranking process does contribute to the final retrieval performance.
Fig. 13 explores the influence of the number of initially retrieved documents (i.e. k ) on the performances of our proposed approach (i.e.  X  X  X R + Block X  X ). Seen from the figure, when k increases from 50 to 150, the performances almost do not alter any more. This shows that a small number of initially retrieved documents work well in the re-ranking process and it will not significantly improve the retrieval performance by increas-ing the number of initially retrieved documents, similar to the results on the TDT dataset. 6. Conclusion and future work
In this paper, we propose a novel retrieval approach for document similarity search. The proposed approach re-ranks a small number of initially retrieved documents based on manifold-ranking of document blocks. The manifold-ranking process can make use of the relationships among document blocks to improve the retrieval performance. Experiments on the TDT data and the ODP data have been performed separately and the experimental results demonstrate the favorable performance of the proposed approach to both sim-ilarity search of text documents and similarity search of web pages.

In this study, we employ the TextTiling algorithm for text segmentation and use the VIPS algorithm for web page segmentation. We will investigate how different document segmentation methods affect the retrieval performance in future work. Furthermore, more diversified data sets will be used in the experiments to thor-oughly investigate the robustness of the proposed approach. Retrieval efficiency is important for real search engines, so we will explore faster algorithms to propagating the ranking scores between document blocks. References
