 Jean Honorio jhonorio@cs.sunysb.edu Stony Brook University, Stony Brook, NY 11794, USA Structure learning aims to discover the topology of a probabilistic network of variables such that this net-work represents accurately a given dataset while main-taining low complexity. Accuracy of representation is measured by the likelihood that the model explains the observed data, while complexity of a graphical model is measured by its number of parameters.
 One challenge of structure learning is that the num-ber of possible structures is super-exponential in the number of variables. For Ising models, the number of parameters, the number of edges in the structure and the number of non-zero elements in the ferro-magnetic coupling matrix are equivalent measures of model com-plexity. Therefore a computationally tractable ap-proach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; H  X ofling &amp; Tibshirani, 2009).
 One additional challenge for Ising models (and Markov random fields in general) is that computing the likeli-hood of a candidate structure is NP-hard. For this reason, several researchers propose exact optimiza-tion of approximate objectives, such as ` 1 -regularized logistic regression (Wainwright et al., 2006), greedy optimization of the conditional log-likelihoods (Jalali et al., 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (H  X ofling &amp; Tibshirani, 2009). Several convex upper bounds and approximations to the log-partition function have been proposed for maximum likelihood estimation, such as the log-determinant relaxation (Banerjee et al., 2008), the cardinality bound (El Ghaoui &amp; Gueye, 2008), the Bethe en-tropy (Lee et al., 2006; Parise &amp; Welling, 2006), tree-reweighted approximations and general weighted free-energy (Yang &amp; Ravikumar, 2011).
 In this paper, we focus on the stochastic optimiza-tion of the exact log-likelihood as our motivating prob-lem. The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximat-ing the gradient. For restricted Boltzmann machines (a very related graphical model) researchers have pro-posed a variety of approximation methods, such as variational approximations (Murray &amp; Ghahramani, 2004), contrastive divergence (Hinton, 2002), persis-tent contrastive divergence (Tieleman, 2008), tem-pered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al., 2010).
 Empirical results in (Marlin et al., 2010) suggests that stochastic maximum likelihood is superior to con-trastive divergence, pseudo-likelihood, ratio matching and generalized score matching for learning restricted Boltzmann machines, in the sense that it produces a higher test set log-likelihood, and more consistent clas-sification results across datasets.
 Learning sparse Ising models leads to the use of stochastic optimization with biased estimates of the gradient. Most work in stochastic optimization as-sumes the availability of unbiased estimates (Duchi &amp; Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Ne-mirovski et al., 2009). Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d X  X spremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level. Similarly, Devolder (2012) analyzed the case of stochastic errors with fixed bias and variance and show convergence up to a constant level.
 Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander &amp; Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximal-gradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex func-tions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse mod-els under the ` 1 regularization. Friedlander &amp; Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander &amp; Schmidt (2011) pro-poses a growing sample-size strategy for approximat-ing the gradient, i.e. by picking an increasing number of training samples in order to better approximate the gradient. In contrast, our work is for NP-hard gradi-ents and we provide bounds with high probability, by taking into account the bias and the variance of the errors. Duchi et al. (2011) analyzed mirror descent (a generalization that includes forward-backward split-ting) and show convergence rates in expected value and with high probability with respect to the mixing time of the sampling distribution. We argue that prac-titioners usually terminate Markov chains before prop-erly mixing, and therefore we motivate our analysis for a controlled increasing number of random samples. Regarding our contribution in optimization, we pro-vide a convergence-rate analysis of deterministic errors for three different flavors of forward-backward splitting (FBS): robust (Nemirovski et al., 2009), basic and ran-dom (Duchi &amp; Singer, 2009). We extend our analy-sis to biased stochastic errors, by first characterizing a family of samplers (including importance sampling and MCMC) and providing a high probability bound that is useful for understanding the convergence of not only FBS, but also PG (Schmidt et al., 2011). Our analysis shows the bias/variance term and allow to derive some interesting conclusions. First, FBS for deterministic or biased stochastic errors requires only a logarithmi-cally increasing number of random samples in order to converge (although at a very low rate). More interest-ingly, we found that the required number of random samples is the same for the deterministic and the bi-ased stochastic setting for FBS and basic PG. We also found that accelerated PG is not guaranteed to con-verge in the biased stochastic setting.
 Regarding our contribution in structure learning, we show that the optimal solution of maximum likelihood estimation is bounded (to the best of our knowledge, this has not been shown before). Our analysis shows provable convergence guarantees for finite iterations and finite number of random samples. Note that while consistency in structure recovery has been established (e.g. Wainwright et al. (2006)), convergence rates of parameter learning for fixed structures is up to now un-known. Our analysis can be easily extended to Markov random fields with higher order cliques as well as pa-ularizer instead. In this section, we introduce the problem of learning sparse Ising models and discuss its properties. Our dis-cussion will motivate a set of bounds and assumptions for a more general convergence rate analysis. 2.1. Problem Setup An Ising model is a Markov random field on binary variables with pairwise interactions. It first arose in statistical physics as a model for the energy of a phys-ical system of interacting atoms (Koller &amp; Friedman, 2009). Formally, the probability mass function (PMF) of an Ising model parameterized by  X  = ( W , b ) is de-fined as: where the domain for the binary variables is x  X  { X  1 , +1 } N , W  X  R N  X  N is symmetric with zero di-agonal, b  X  R N and partition function is defined as Z ( W , b ) = vergence rate analysis, we also define  X   X  R M where M = N 2 .
 In the physics literature, W and b are called ferro-magnetic coupling and external magnetic field respec-tively. W defines the topology of the Markov ran-dom field, i.e. the graph G = ( V , E ) is defined as V = { 1 , . . . , N } and E = { ( n 1 , n 2 ) | n 1 &lt; n 2  X  w It is well known that, for an Ising model with arbitrary topology, computing the partition function Z is NP-hard (Barahona, 1982). It is also NP-hard to approx-imate Z with high probability and arbitrary precision (Chandrasekaran et al., 2008).
 The number of edges |E| or equivalently the cardinal-ity (number of non-zero entries) of W is a measure of model complexity, and it can be used as a regu-larizer for maximum likelihood estimation. The main disadvantage of using such penalty is that it leads to a NP-hard problem, regardless of the computational complexity of the log-likelihood.
 Next, we formalize the problem of finding a sparse Ising model by regularized maximum likelihood esti-mation. We replace the cardinality penalty by the ` 1 -norm regularizer as in (Wainwright et al., 2006; Baner-jee et al., 2008; H  X ofling &amp; Tibshirani, 2009). Given a complete dataset with T i.i.d. samples ` -regularized maximum likelihood estimation for the Ising model in eq.(1) becomes: where the negative (average) log-likelihood L ( W , b ) =  X  b  X 
T b , the empirical second-order moment b  X  =
P b  X  = 1 The objective function in eq.(2) is convex, given the convexity of the log-partition function (Koller &amp; Fried-man, 2009), linearity of the scalar products and con-vexity of the non-smooth ` 1 -norm regularizer. As dis-cussed before, computing the partition function Z is NP-hard, and so is computing the objective function in eq.(2). 2.2. Bounds In what follows, we show boundedness of the opti-mal solution and the gradients of the maximum like-lihood problem. Both are important ingredients for showing convergence and are largely used assumptions in optimization. In this paper, we follow the origi-nal formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; H  X ofling &amp; Tibshi-rani, 2009), which does not regularize b . We found interesting to show that this problem has bounds for k b  X  k 1 unlike other stochastic optimization problems, e.g. SVMs (Shalev-Shwartz et al., 2007).
 First, we make some observations that will help us derive our bounds. The empirical second-order mo-ment b  X  and first-order moment b  X  in eq.(2) are com-puted from binary variables in { X  1 , +1 } , therefore k b  X  k  X   X  1 and k b  X  k  X   X  1.
 Assumption 1. It is reasonable to assume that the empirical first-order moment of every variable is not equal to  X  1 (or +1 ), since this would be equivalent to observe a constant value  X  1 (or +1 ) for such variables in every sample in the dataset, i.e. (  X  n ) | b  X  n | = 1  X  k b  X  k  X  &lt; 1  X  (  X  n )  X  1 &lt; b  X  n &lt; +1 . Given those observations, we state our bounds in the following theorem. For clarity of the convergence rate analysis, we also define the bound D of the optimal solution.
 Theorem 2. The optimal solution  X   X  = ( W  X  , b  X  ) of the maximum likelihood problem in eq. (2) is bounded as follows: where D 2 = Proof Sketch. Claim i and ii follow from the fact that the function evaluated at ( W  X  , b  X  ) is less than at ( 0 , 0 ). Additionally, Claim i follows from non-negativity of the negative log-likelihood in eq.(2), while Claim ii follows from non-negativity of the regu-larizer and from Assumption 1. Claim iii follows from norm inequalities and Claims i and ii. (Please, see Appendix C for detailed proofs.) If we choose to add the regularizer  X  k b k 1 in eq.(2), it Claim i of Theorem 2.
 The gradient of the objective function of the maximum likelihood problem in eq.(2) is defined as: where P is the probability distribution with PMF p ( x ). The expression in eq.(4) uses the fact that E It is well known that computing the gradients  X  log Z / X  W and  X  log Z / X  b is NP-hard. The com-plexity results in (Chandrasekaran et al., 2008) imply that approximating those gradients with high proba-bility and arbitrary precision is also NP-hard. Next, we state some properties of the gradient of the exact log-likelihood. For clarity of the convergence rate analysis, we also define the Lipschitz constant G . Lemma 3. The objective function of the maximum likelihood problem in eq. (2) has the following Lipschitz continuity properties: where G 2 = N 2 max((1+ k b  X  k  X  ) 2 + 1 N (1+ k b  X  k  X  Proof Sketch. Claims i to iii follow from the fact that the terms  X  log Z / X  W and  X  log Z / X  b in eq.(4) are the second and first-order moment of binary variables in { X  1 , +1 } . Claim iv follows from the definition of subgradients. Claim v follows from norm inequalities and Claims ii to iv. 2.3. Approximating the Gradient of the Suppose one wants to evaluate the expression E P [ xx T ] in eq.(4) which is the gradient of the log-partition func-tion. Let assume we know the distribution p  X  ( x ) up trial distribution with PMF q ( x ), calculates the impor-estimate ( the distribution p  X  ( x ) based on constructing a Markov chain whose stationary distribution is p  X  ( x ). Thus, the estimate becomes 1 S In what follows, we characterize a family of sam-plers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001).
 Definition 4. A ( B, V, S, D ) -sampler takes S random samples from a distribution Q and produces biased es-timates of the gradient of the log-partition function  X  log Z / X   X  +  X  , with error  X  that has bias and vari-ance: for B  X  0 , V  X  0 and (  X   X  ) k  X  k 2  X  D .
 Note that a ( B, V, S, D )-sampler is asymptotically un-biased with asymptotically vanishing variance, i.e. S  X  +  X   X  B S  X  0  X  V S  X  0. Unfortunately, ana-lytical approximations of the constants B and V are difficult to obtain even for specific classes, e.g. Ising models. The theoretical analysis implies that such con-stants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC. We argue that this apparent disadvantage does not diminish the relevance of our analysis, since we can reasonably expect that more refined samplers lead to lower B and V . Note that Definition 4 does not contradict the com-plexity results in (Chandrasekaran et al., 2008) that show that it is likely impossible to approximate Z (and therefore its gradient) with probability greater than 1  X   X  and arbitrary precision  X  in time polyno-mial in log 1  X  and 1  X  . Definition 4 assumes biasedness and a polynomial decay instead of an exponential de-cay (which is a more stringent condition) and cannot be used to derive two-sided high probability bounds that are both O (log 1  X  ) and O ( 1 S ). Therefore, Defini-tion 4 cannot be used to obtain polynomial-time al-gorithms as the ones considered in (Chandrasekaran et al., 2008).
 Assumption 5. It is reasonable to assume that the estimates of the gradient of the log-partition function are inside [  X  1; +1] since they are approximations of the second and first-order moment of binary variables in { X  1 , +1 } . Furthermore, it is straightforward to en-force Lipschitz continuity (condition i of Lemma 3) for any sampler (e.g. importance sampling, MCMC or any conceivable method) by limiting its output to be inside [  X  1; +1] . More formally, we have: In this section, we analyze the convergence rates of forward-backward splitting . Our results apply to any problem that fulfills the following largely used assump-tions in optimization:  X  the objective function is composed by a smooth  X  the optimal solution is bounded, i.e. k  X   X  k 2  X  D  X  each visited point is at a bounded distance from  X  both L and R are Lipschitz continuous, i.e.  X  the non-smooth regularizer vanishes at zero, i.e. We additionally require that the errors do not change the Lipschitz continuity properties, i.e. k  X  L / X   X  +  X  k 2  X  G (as discussed in Assumption 5). 3.1. Algorithm We analyze forward-backward splitting (Duchi &amp; Singer, 2009) for deterministic as well as biased stochastic errors, for non-increasing step sizes of the alent to basic proximal gradient (Schmidt et al., 2011) for r = 0 (constant step size). We point out that FBS has O ( 1  X  PG has O ( 1 K ) convergence, and accelerated PG has O ( 1 K 2 ) convergence. Thus, PG methods have faster convergence but they are more sensitive to errors. FBS performs gradient descent steps for the smooth part of the objective function, and (closed form) pro-jection steps for the non-smooth part. Here we assume that at each iteration k , we approximate the gradient with some (deterministic or biased stochastic) error  X  ( k ) . For our objective function in eq.(2), one itera-tion of the algorithm is equivalent to: gradient approximation. Step ii is a projection step for the non-smooth regularizer R (  X  ). For the regularizer in our motivating problem R ( W ) =  X  k W k 1 , Step ii decomposes into N 2 independent lasso problems. 3.2. Convergence Rates for Deterministic In what follows, we analyze three different flavors of forward-backward splitting: robust which outputs the weighted average of all visited points by using the step sizes as in robust stochastic approximation (Nemirovski et al., 2009), basic which outputs the average of all vis-ited points as in (Duchi &amp; Singer, 2009), or random which outputs a point chosen uniformly at random from the visited points. Here we assume that at each iteration k , we approximate the gradient with some deterministic error  X  ( k ) . Our results in this subsection will allow us to draw some conclusions regarding not only FBS but also proximal gradient.
 In order to make our bounds more general for different use generalized harmonic numbers H r,K = H Additionally, we define a weighted error term that will be used for our analysis of deterministic as well as biased stochastic errors. Given a sequence of errors  X  (1) , . . . ,  X  ( K ) and a set of arbitrary weights  X  k such that First, we show the convergence rate of robust FBS. Theorem 6. For a sequence of deterministic errors  X  (1) , . . . ,  X  ( K ) , step size  X  k =  X  point  X  (1) = 0 , the objective function evaluated at the weighted average of all visited points converges to the optimal solution with rate: where  X  =  X  ( K ) = error term A  X  ,  X  is defined as in eq. (9) , and the error Proof Sketch. By Jensen X  X  inequality L (  X  ) + R (  X  )  X  P k  X  k ( L (  X  a technical lemma for bounding consecutive steps (Please, see Appendix B).
 Second, we show the convergence rate of basic FBS. Theorem 7. For a sequence of deterministic errors  X  (1) , . . . ,  X  ( K ) , step size  X  k =  X  point  X  (1) = 0 , the objective function evaluated at the average of all visited points converges to the optimal solution with rate: where  X  = A  X  ,  X  is defined as in eq. (9) , and the error weights  X  Proof Sketch. By Jensen X  X  inequality L (  X  ) + R (  X  )  X  P k ( L (  X  cal lemma for bounding consecutive steps (Please, see Appendix B).
 Finally, we show the convergence rate of random FBS. Theorem 8. For a sequence of deterministic errors  X  (1) , . . . ,  X  ( K ) , step size  X  k =  X  point  X  (1) = 0 and some confidence parameter 0 &lt;  X  &lt; 1 , the objective function evaluated at a point k chosen uniformly at random from the visited points converges, with probability at least 1  X   X  , to the optimal solution with rate:
L (  X  ( k ) ) + R (  X  ( k ) )  X  X  (  X   X  )  X  X  (  X   X  ) where the error term A  X  ,  X  is defined as in eq. (9) , and the error weights  X  k = 1 K such that Proof Sketch. Since the distribution is uniform on k , the expected value of the objective function is equal to the average of the objective function evaluated at all visited points, i.e. the average regret  X  ( K ). The final result follows from Markov X  X  inequality and the upper bound of  X  ( K ) given in Theorem 7.
 The convergence rates in Theorems 6, 7 and 8 lead to an error term A  X  ,  X  that is linear, while the error term is quadratic in the analysis of proximal gradient (Schmidt et al., 2011). In basic PG, the error term can be written as: where the error weights  X  k = 1 K such that In accelerated PG, the error term can be written as: where the error weights  X  k = k/ Note that both PG methods contain terms K and K 2 , which are not in our analysis. As noted in (Schmidt et al., 2011), errors have a greater effect on the accel-erated method than on the basic method. This ob-servation suggests that, unlike in the error-free case, accelerated PG is not necessarily better than the basic method due to a higher sensitivity to errors (Devolder et al., 2011).
 Intuitively speaking, basic PG is similar to basic FBS in the sense that errors from all iterations have the same effect on the convergence rate, i.e.  X  k is con-stant. In robust FBS, errors in the last iterations have a lower effect on the convergence rate than errors in the beginning, i.e.  X  k is decreasing. In accelerated PG, errors in the last iterations have a bigger effect on the convergence rate than errors in the beginning, i.e.  X  k is increasing.
 The analysis of Schmidt et al. (2011) for determinis-tic errors implies that in order to have convergence, the errors must decrease at a rate k  X  ( k ) k 2  X  X  ( 1 for some  X  &gt; 0 in the case of basic PG, and O ( 1 k 1+  X  for accelerated PG. In contrast, our analysis of FBS show that we only need logarithmically decreasing er-rors O ( 1 log k ) in order to have convergence. Regarding robust FBS requires errors O ( 1 required for convergence in basic PG). Table 1 summa-rizes the requirements for different convergence rates of the error term A  X  ,  X  of FBS as well as the error terms of basic PG in eq.(13) and accelerated PG in eq.(14). For an informal (and incomplete) analysis of the re-sults in (Schmidt et al., 2011) for biased stochastic optimization, consider each error bounded by its bias and variance k  X  ( k ) k 2  X  B/S k + c and an increasing number of random samples S k that allows to obtain decreasing errors. Without noting the possible need of  X  X niform convergence X  of the bound for all K iterations (making c a function of K ), the number of random samples must increase (at least) at a rate that is quadratic of the rate of the errors. For instance, in order to have O ( 1 K ) convergence, basic PG requires errors to be O ( 1 k 1+  X  ) and therefore it would re-quire (at least) an increasing number of random sam-ples S k  X  O ( k 2+  X  ) for some  X  &gt; 0. Accelerated PG would require (at least) S k  X  O ( k 4+  X  ) in order to ob-tain O ( 1 K 2 ) convergence. If we include the fact that c is a function of K , then the required number of ran-dom samples would be  X  X orse than quadratic X  of the required rate of the errors. Fortunately, a formal anal-ysis in the next subsection shows that this is not the case for all methods except accelerated PG. 3.3. Bounding the Error Term for Biased In what follows, we focus in the analysis of stochas-tic errors in order to see if better convergence rates can be obtained than the ones informally outlined in the previous subsection. A formal analysis of the er-ror terms show that forward-backward splitting for bi-ased stochastic errors requires only a logarithmically increasing number of random samples in order to con-verge, i.e. S k  X  X  (log k ). More interestingly, we found that the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG. On the negative side, we found that accelerated PG is not guaranteed to converge in the biased stochastic setting.
 Next, we present our high probability bound for the er-ror term for biased stochastic optimization. One way to bound the error term A  X  ,  X  would be to rely on  X  X ni-form convergence X  arguments, i.e. to bound the error of each iteration k  X  ( k ) k 2 and then use the well-known union bound. We chose to bound the error term it-self, by using the fact that errors become independent (but not identically distributed) conditioned to the pa-number of random samples S k for each iteration k . Theorem 9. Given K ( B, V, S k , D ) -samplers each producing estimates with an error  X  ( k ) , and given a set of arbitrary weights  X  k such that some confidence parameter 0 &lt;  X  &lt; 1 , with probability at least 1  X   X  , the error term is bounded as follows: where the bias term  X  1 = min(2 the variance term  X  2 = min(4 M, V Proof Sketch. The bias and variance for each k  X  ( k ) k 2 are bounded by B S 3 and Assumption 5 we have k  X  ( k ) k 2  X  2 is the maximum bias, and its square is the maximum variance. By the definition of marginal distribution, identically distributed) conditioned to the parameters  X  (1) , . . . ,  X  ( K ) . We then invoke Bernstein inequality for properly defined variables such that it applies to the weighted average A  X  ,  X  .
 It is interesting to note what happens for a fixed num-ber of random samples S k  X  O (1). In this case, the bias term  X  1  X  O (1) and therefore FBS will not converge. For robust FBS, the variance term  X  2  X  O ( H 2 r,K / ( H r,K ) 2 ) which for instance for r = we have  X  2  X  O ( log K K ). For basic FBS, the variance term  X  2  X  X  ( 1 K ). Therefore, for the constant number of random samples, the lack of convergence of FBS is explained only by the bias of the sampler and not its variance.
 Table 2 summarizes the requirements for different con-vergence rates of the error term A  X  ,  X  of FBS as well as the error terms of basic PG in eq.(13) and accel-erated PG in eq.(14). Note that convergence for FBS is guaranteed for a logarithmically increasing number of random samples S k  X  O (log k ). Moreover, in order to obtain convergence rates of O ( 1  X  required number of random samples is just the inverse of the required rate of the errors for the deterministic case, and not  X  X orse than quadratic X  as outlined in our informal analysis of the previous subsection. One important conclusion from Theorem 9 is that the upper bound of the error term is  X ( 1 K ) independently of the bias term  X  1 and the variance term  X  2 . This implies that the error term is O ( 1 K ) for any setting of error weights  X  k and number of random samples S . The main implication is that the error term in accelerated PG in eq.(14) is constant and therefore the accelerated method is not guaranteed to converge. We illustrate our theoretical findings with a small syn-thetic experiment ( N = 15 variables) since we want to report the log-likelihood at each iteration. We per-formed 10 repetitions. For each repetition, we generate edges in the ground truth model W g with a 50% den-sity. The weight of each edge is generated uniformly at random from [  X  1; +1]. We set b g = 0 . We finally generate a dataset of 50 samples. We used a  X  X ibbs sampler X  by first finding the mean field distribution and then performing 5 Gibbs iterations. We used a step size factor  X  = 1 and regularization parameter  X  = 1 / 16. We also include a two-step algorithm, by first learning the structure by ` 1 -regularized logistic regression (Wainwright et al., 2006) and then learning the parameters by using FBS with belief propagation for gradient approximation. We summarize our results in Figure 1.
 Our experiments suggest that stochastic optimiza-tion converges to the maximum likelihood estimate. We also show the Kullback-Leibler divergence to the ground truth, and more pronounced effects for impor-tance sampling (Please, see Appendix D).
 Concluding Remarks. Although we focused on Ising models, the ideas developed in the current paper could be applied to Markov random fields with higher order cliques. Our analysis can be easily extended to regularizer instead. Although we show that acceler-ated proximal gradient is not guaranteed to converge in our specific biased stochastic setting, necessary con-ditions for its convergence needs to be investigated. Acknowledgments. This work was done while the author was supported in part by NIH Grants 1 R01 DA020949 and 1 R01 EB007530.

