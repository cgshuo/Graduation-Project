 For a large and complex software system, the project team could receive a large number of bug reports. Some bug reports could be duplicates as they essentially report the same problem. It is often tedious and costly to manually check if a newly reported bug is a duplicate of an already reported bug. In this paper, we propose BugSim , a method that can automatic ally retrieve duplicate bug reports given a new bug report. B ugSim is based on learning to rank concepts. We identify textual and statistical features of bug reports and propose a similarity function for bug reports based on the features. We then construct a training set by assembling pairs of duplicate and non-duplicate bug reports. We train the weights of features by applying the stoc hastic gradient descent algorithm duplicate reports using the traine d model. We evaluate BugSim using more than 45,100 real bug repor ts of twelve Ec lipse projects. The evaluation results show that the proposed method is effective. On average, the recall rate for the top 10 retrieved reports is 76.11%. Furthermore, BugSim outperforms the previous state-of-art methods that ar e implemented using SVM and D.2.7 [ Software Engineering ]: Distribution, Maintenance, and Enhancement X  Restructuring, reverse engineering, and reengineering; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing. Bug reports, duplicate bug retrieval, duplicate documents, software maintenance, learning to rank. With the rapid development of in formation technology, software systems are getting more and mo re complex. Although a range of measures have been taken to assu re software quality, in reality released software systems still contain bugs. For a large-scale, widely-used software system, the project team could receive a large number of bug reports over time. For example, 49,422 bugs were reported for the Eclipse projects in 2010, in average 135 bugs every day. Software bugs are typically maintained by a bug tracking system such as BugZilla 1 , which stores the reported bug reports and tracks their status. After a bug report is created by the bug tracking system, the project team could retrieve it, examine it, verify it, and assign it to a developer to work on. The bug reporting process is essentia l an uncontrolled, distributed process. Users and testers at different sites can report bugs whenever they encounter them. Therefore, often some bug reports are duplicates because they actually describe the same problem. For example, among all Eclipse bugs reported in 2010, 3799 of them are duplicates. Studies also show that as many as 36% of bug reports could be duplicates or invalid [1]. The existence of duplicate bug reports can exacerbate the already high cost of software maintenance, and can even lead to wasted maintenance efforts (imaging a developer spends days on verifying and debugging a bug, and finally realizes that the same problem has been reported before by a different user and fixed by another developer). To reduce software maintenance effort, users/developers are required to check whether a new bug report is a duplicate of an existing one or not before reporting/handling a new bug. As the number of bug reports maintained by a bug tracking system could be very large, a manua l process could be tedious and error-prone. In recent years, researchers have proposed automated methods for detecting duplicate bug reports. Given a new bug report, these methods analyze the existing bug reports stored in a bug track system, and return the top N related bug reports using certain information retrieval techniques . For example, [20] uses a SVM-based discriminative model to compute the probability of two bug reports being duplicate and then retrieval the top N most similar bug reports as the cand idate duplicate bug reports. [21] uses an extended BM25F model to retrieve related bug reports. In this paper, we propose BugSim , a duplicate bug retrieval method based on the concept of learning to rank . We first collect a training set, which consists of triples ( bug, dup bug, non-dup bug ), where dup bug represents a duplicate of the bug, non-dup bug represents a non-duplicate of the bug. We then create a ranking model to rank the dup bug higher than the non-dup bug. To construct the ranking model, we identify 9 textual and statistical features of bug reports, and learn the weights of the features by applying the stochastic gradient descent algorithm over the training dataset. For a new bug report, we retrieve and rank the duplicate bug reports us ing the trained model. We evaluate our method using more than 45,100 real bug reports of twelve Eclipse projects. The evaluation results show that the proposed method is effective. The recall rates for the top 10 retrieved reports are above 61%, with an average of 76.11%. http://www.bugzilla.org/ Furthermore, BugSim outperfo rms the previous SVM-based method by 15.41% and the BM25F ext -based method by 3.71% on the recall rate of the top 10 results. The organization of the paper is as follows. In Section 2, we describe the background of this wo rk. In Section 3, we describe the proposed BugSim approach. Section 4 describes our experimental design, and Sec tion 5 shows and discusses the experimental results. Section 6 gi ves the threats to validity. We discuss the related work in Sect ion 7 and conclude the paper in Section 8. During software testi ng and maintenance, a large number of bugs could be discovered and reported. Different users/testers may report the same problem at differ ent time, thus causing duplicate bug reports. Figure 1 gives an example of duplicate bug reports. Bug #214445 2 and Bug #214451 3 are real bug reports for the Eclipse project. Each bug report contains a summary and a description , and is written in natural language. We can see that the two bug reports contain many similar words such as select , value , filter , table , etc. Usually, duplicate bug reports exhibit high degree of text similarity as they report the same problem. Therefore, information retrieval (IR) techni ques can be applied to help developers detect duplicate bug reports. Bug #214445 , reported by ysun, on 2008-01-06 S ummar y : &lt;Select value...&gt;cannot select any value. D escri p tion : &lt;Select value...&gt;cannot select any value [00]. table. 4. Add a filter/map/highlight. 5. Select a column and open the select value dialog. 6. Se lect one value and click OK. Actual result: Nothing value is selected. Bug #214451 , reported by Wen Lin, on 2008-01-07 S ummar y : Select value in table f ilter condition panel does not take any effect. take any effect... Step to reproduce: 1. New a table binding to a dataset in a report. 2. Add a filter in filters panel. 3. Set a filter condition as row "xxx" not equal to a value; select this value from select value panel. Expect result: The value selected by user should be set to the condition combo. Actual result: The select value operation does not take any effect. Recently, researchers have proposed some IR-based methods for automated retrieval of duplicate bug reports. A general process of these methods is shown in Figure 2. The existing bug reports are stored at a repository managed by a bug tracking system. These bug reports are first preprocessed (including tokenization, https://bugs.eclipse.org/bugs/show_bug.cgi?id=214445 https://bugs.eclipse.org/bugs/show_bug.cgi?id=214451 stemming, and stop words remova l). The features of the bug report are extracted and used to c onstruct a retrieval model. When a new bug report arrives, the retrie val model compares its features to the features of the existing reports stored in the repository. The existing reports are then ranked according to their similarities to the new bug report, and are returned to the users. The users can examine the top N returned results and mark if the new bug report is a duplication. The differences among the existi ng duplicate bug report detection methods are mainly in the types of features being selected and in the information retrieval techniques being applied. In this section, we briefly introduce two major state-of-art methods: SVM: In [20], the authors proposed a SVM-based discriminative model to determine how likely two bug reports are duplicates. They bucket contains a set of confirme d duplicate bug reports. Pairs of duplicate and non-duplicate bug reports can be extracted from the buckets and used for training the model. As a bug report consists of two textual fields: summary and description, they identify three bags of words: summary, description, and both (summary + description). For each bag, an idf measure is computed, so there are three types of idf ( idf-summary, idf-description, and idf -both). Each idf combination is considered as a feature. They also consider bigrams besides unigrams, therefore making the total number of features 54. After identifying the features, they then build a SVM-based discriminative model, which can produce a probability of two bug reports be ing duplicates of each other. When a new bug report comes, the trained model is applied to retrieve a ranked list of candidate reports of which the new bug is likely a duplicate. Once developers confirm a duplication, the new bug report is placed into the corresponding bucket. Otherwise, a new bucket is created for the new bug. In [21], the authors proposed to use BM25F ext , an extended BM25F model [17], to measure the textual similarity between two bug reports. They also organize the existing bug reports as a list of buckets and pairs of duplicate and non-duplicate bug reports are used for model training. They cons ider bug reports as structural documents that are composed of two fields (summary and description) with different degrees of importa nce, thus applying a BM25F model for structured doc ument retrieval. Also, they consider using the entire bug report as a query, therefore they extend the BM25F model to incorporate the effect of long query. For a document d and a query q , the score of their extended model BM25F ext is computed as follows: , where 
B MFdq IDFt W , where f represents a field (either bug summary or bug average_length f is the average length of f across all documents in effect of term frequency. Unli ke the classic BM25F function, BM25F ext considers the weight of terms in queries by introducing an additional parameter k 3 . In [21], the authors also propose a retrieval function called REP. In REP, they identify seven features of bug reports, including textual similarity computed using BM25F ext and several categorical related features. The we ight of each feature is tuned using the gradient descent algorith m. Finally, the reports in all buckets are ranked by the weighted sum of feature scores and are returned to developers. However, many of the categorical features (such as bug priority, buggy component, type, and version) may be unknown at the time of reporting bug, especially to end users. Therefore, we only consider the BM25F ext model in this paper. As a relative new form of statis tical learning, learning to rank techniques have received much attention in recent years. Learning to rank is a supervised (or semi-supervised) learning method, which learns a model from ordered objects in a training set and uses the model to rank unknown objects. The information retrieval (IR) problem can be transformed as the problem of ranking. All documents in the corpus are ranked according to their relevancy to the query, highly relevant documents should be ranked highe r than the irrelevant ones. Learning to ranking algorithms such as RankBoost [8], RankSVM [10], and RankNet [4] construct pairs between documents and use machine learning technique to minimize the number of disordered pairs. These algorithms are often called pairwise approaches and have been applied to retrieve documents [4, 5, 13]. More recently, researchers also proposed Listwise approaches [6, 28, 29], which aim to directly minimize the lo ss function defined on a list of objects. In our approach, we treat existing bug reports stored in a bug tracking system as doc uments and a newly arrived bug report as a query. We design methods to retrieve duplicate bug reports by using learning to rank techniques. We apply the pairwise approach, as it is difficult to obtain th e complete order of duplicate bug reports, but it is easier to know th e partial orders between two bug reports. In this section, we present a method called BugSim for ranking duplicate bug reports for a softwa re project. Figure 3 shows an overall structure of BugSim. Following [20], we organize the existing bug reports as a list of buckets, where each bucket contains a set of confirmed duplicate bug reports. A bug report cannot belong to any two buckets thus each bucket is distinct. If a bug does not have duplicates, it corresponds to a bucket that only contains itself. From the buckets, we collect a tr aining set consisting of triples duplicate of the bug, non-dup bug represents a non-duplicate of the bug. We first identify features of bug reports and propose a similarity function to measure the similarities between two bug reports based on these features. Ba sed on the training set, we then construct a retrieval model, which ranks the dup bug higher than the non-dup bug and minimizes the loss function.
 Given a new bug report, the retrie val model can rank the existing bug reports according to the similarity function, and present the ranked results to users as candida te duplicate bug reports. If the new bug report does not belong to any existing bucket, a new bucket is created for this bug. Ot herwise, the bug is assigned to the bucket where the duplicate bug report is. In the following subsections, we de scribe the feature selection, the model training, and the ranki ng processes in more details. As described in Section 2, a b ug report consists of two parts: summary and description. Inspired by the work on discriminative models for information retrieval [16], we identify 9 features based on bug summary and bug description as shown in Table 1. These features use statistics such as the word count and idf (inverse document frequency) and their combinations. We use these features to discriminate the bug reports. In Table 1, S 1 and D 1 represent the summary and descriptions of a bug report B 1 , respectively. S 2 and D 2 represent the summary and descriptions of a bug report B 2 , respectively. idf s (w) represents the idf (Inverse Document Frequency) value of a word w over the summaries of all existing bug reports. idf D (w) represents the idf value of a word w over the descriptions of all existing bug reports. c(w,D 2 ) represents the number of times the word w appears in D |D | means the length of D 2 . |C D | means the total number of unique words that appear in the descriptions of all existing bug reports. c(w,C D ) means the number of times the word w appears in the descriptions of all existing bug reports. 
Table 1. The identified features between two bug reports 3 4 7 8 9 Feature 1 describes the idf values of the words that are common to the summary of the two bug reports. Feature 2 describes the similarity between the summary of the two bug reports (measured in terms of the number of common words). Feature 3 describes the number of times words used in the summary of B 1 the descriptions of B 2 . Feature 4 desc ribes the frequency of words used in the summary of B 1 appearing in the descriptions of B Feature 5 gives the multiplication of the term frequency and the idf values ( tf * idf ). Feature 6 gives the idf values of words that are used in both the summary of B 1 and the descriptions of B 7 describes the weights of words that are used in both the summary of B 1 and the descriptions of B 2 , in the entire document collection. Feature 8 describes th e inverse collection frequency of words that are used in both the summary of B bug reports, measured using VSM (Vector Space Model). For some features, the log function is used to dampen the effects of large numbers. Based on the identified features, we define a metric BugSim for measuring the similarity between two bug reports B follows: In (2), f i represents the i th feature; w i represents the weight of the between the two bug reports. To apply (2), 9 weights should be gi ven. It is difficult to obtain the values of these weights subjectivel y. In our approach, we adopt a learning process to tune these weights. In our training set, each instance is a triple ( bug, dup bug, non-dup bug ), where dup bug represents a duplicate of the bug, non-dup bug represents a non-duplicate of the bug. Clearly, if we want the dup bug to be ranked higher than the non-dup bug, the followings should hold: We also utilize the Fidelity loss function ( F ij ) [25] to measure the cost of the BugSim on an instance I : 
F In (4),  X   X   X  X  X  is the target probabilities that sample i is to be ranked higher than sample j . In the case of duplicate bug detection, duplicate bug reports should be always ranked higher than non-duplicate ones. So equation (4) can be simplified as follows: The Fidelity loss function was originally a distance metric in physics and was recently used by [25] in the probabilistic ranking framework. It has been shown that the Fidelity loss function exhibits properties that are helpful for ranking. From (5) we can see that the value of F ij increases when o ij increases. To better differentiate dup bug and non-dup bug , the values of w minimized. To obtain the optimal values of w i , we apply the stochastic gradient descent algorithm [15]. The stochastic gradient descent algorithm iterates N time. In each iteration, for each instance in the training set, the algorithm adjusts each weight as follows: , where  X  is the tuning rate, which is usually a very small number minimum F ij value are the optimal weight values. In (6), the partial derivative of the Fidelity loss function with respect to w derived as follows: o BugSim bug non dup bug BugSim bug dup bug ww  X   X  X   X  X  X  In summary, Figure 4 shows the algorithm we used to perform training. The algorithm includes two parts: the selection of training instances and the learning of weights. After the ranking model is traine d, we can use it for duplicate bug report retrieval. When a new bug report arrives, BugSim computes the similarity between a new bug re port and a bucket, which is the maximum similarity value betwee n the new bug report and all the existing bug reports in the bucket . To compute the similarity between the new report and an ex isting one, BugSim first extracts the features between them (as described in Section 3.2), and then calculates the similarity using e quation (2). BugSim ranks all the buckets by their similarity valu es to the new report. For each users can then check if the new bug is a duplicate of a bug in the contains the bug it duplicates to. If it is not a duplicate bug, a new bucket is created for the new bug report. TRAINING ( Buckets, S ) 
Buckets: The list of buckets that contain duplicate bug reports. 
S: The size of the training set // Training set selection 1. Initialize the training set T 2. Randomly select two bug reports from the Buckets 3. Randomly select a bucket that is different from the 4. Create a triple I(bug, dup bug, non-dup bug) , and add 5. Repeat the steps 2-4 until the size of T reaches S. // Learning weights 6. Applying the stochastic gradient descent algorithm on 7. Return the tuned weights w To evaluate the effectiveness of the proposed approach, we choose the Eclipse 4 projects as our subject . Eclipse is an open source community whose projects are focused on building extensible software developm ent platforms and frameworks. Software tools produced by the Eclipse projects are very popular and have millions of users worldwide 5 . The bugs reported against Eclipse are stored and tracked by the BugZilla tool 6 . Like in other projects, users who would like to post an Eclipse bug report are encouraged to check whether it has been posted already 7 . In our evaluation, we investig ate 12 relatively independent projects (categories) in Eclipse , as shown in Table 1. These projects contain 45,170 bugs reported from January 1, 2008 to December 31, 2008. Among the 45,170 bugs, 2,949 are duplicates. Following the previous work [21], we construct a small training set, which contains total 5,863 bugs and 200 duplicate bugs (from http://www.eclipse.org/ http://www.numberof.net/number-of-eclipse-users/ https://bugs.eclipse.org/bugs/ http://wiki.eclipse.org/FAQ_Ho w_can_I_search_the_existing_lis t_of_bugs_in_Eclipse%3F 2008-01-01 to 2008-02-22). The remaining data is used as the testing dataset. We use Java to implement the research prototype. The experiments are carried out on a Windows 2008 Server R2 with Intel Xeon(R) CPU E5506@2. 13GHz and 8G memory. 
Eclipse IDE Foundation 63 1497 RQ1: How effective is BugSim? RQ1 evaluates the effectiveness of our approach described in Section 3. To answer RQ1, we evaluate BugSim on the projects described in Table 2. We use th e training set to train a ranking model, following the algorithm de scribed in Figure 4. For each bug report in the testing set, B ugSim retrieves the candidate duplicate bug reports and returns a ranked list to users. The performance of retrieval is measured using the recall rate and the MRR metrics. RQ2: Can BugSim outperform the related methods? RQ2 compares the proposed met hod with the two major related methods that are described in Section 2.2:  X  SVM [20]: We use the libsvm tool 8 as the SVM  X  BM25F ext [21]: We use the parameters given in [21] to build RQ3: Does the Fidelity loss function outperform the related RNC loss function? Libsvm: http://www.csie.n tu.edu.tw/~cjlin/libsvm. In our approach, we use the Fidelity function as the loss function. Other loss functions could be al so adopted in learning to rank approaches. In RankNet [4], a pairwise cross entropy loss function is used to penalize ordering error, which is defined as: The above loss function is also used in [21, 23] and is referred to as the RNC function. This RQ compares the effectiveness of the Fidelity function with that of the RNC function. We evaluate the performance of BugSim using the following metrics:  X  Recall@ N : The percentage of bug reports whose relevant  X  MRR (Mean Reciprocal Rank), which is a statistic for RQ1: How effective is BugSim? Table 3 shows the performance of BugSim on 12 projects, when recall rate is concerned. For the STP project, BugSim can first returned result). Therefore its Recall@1 is 1. For the Eclipse IDE project, it has 9872 bugs a nd 928 of them are duplicates. Using BugSim, 45.58% dup licate bug reports can be detected at top 1, 71.77% can be detected within top 10, and 76.51% can be detected within top 20. For all the projects, the Recall@1 values are above 33% (with an averag e of 49.48%), the Recall@10 values are above 61% (with an average of 76.11%), and the Recall@20 values are above 67% (w ith an average of 82.03%). These results are considered satisfactory. Table 3 also shows the performance of BugSim measured in terms of MRR. For the STP and Modeling projects, the MRR values exceed 0.7. For the rest of the projects, the MRR values are above 0.42. In general, the evaluation results show that BugSim is effective in retrieving duplicate bug reports. It can allow developers detect duplicate bug reports by examining a small number of bug reports. The effort spent on bug handling can be reduced. DataTools 41.38% 68.72% 82.76% 86.21% 0.5434 Foundation 44.44% 68.25% 76.19% 84.13% 0.5576 Eclipse IDE 45.58% 64.55% 71.77% 76.51% 0.5447 Modeling 71.72% 85.86% 88.89% 91.92% 0.7636 Technology 46.02% 66.37% 73.01% 80.97% 0.5574
WebTools 43.81% 61.50% 68.14% 75.66% 0.5226 average 49.48% 69.92% 76.11% 82.03% 0.5871 RQ2: Can BugSim outperform the related methods? Table 4 shows the comparison between BugSim and SVM and BM25F ext based methods, when MRR is concerned. BugSim performs better than the SVM-base d discriminative model on all experimental projects. The MRR values SVM achieves are improvements of BugSim over SV M range from 4.87% to 76.47%, with an average of 31.49%. 
Table 4. The comparison between BugSim and the related Project BugSim Eclipse Foundation 0.5576 0.4678 19.21% 0.4897 13.86% Eclipse IDE 0.5447 0.3987 36.62% 0.5211 4.53% Technology 0.5574 0.4002 39.29% 0.5146 8.32% 
Figure 5. The comparisons be tween BugSim and the related BugSim also outperforms BM25F ext on 10 projects. The improvement is from 1.5% to 20%. BugSim only performs slightly worse than BM25F ext on two projects (1.7% and 0.7%, respectively). The average improvement of BugSim over BM25F ext is 6.84%. We also conduct a paired t-test and the result confirms that BugSim performs statistically significantly better than BM25F ext , at the significance level 0.05 (with p value 0.017). We also find that BugSim outperforms SVM and BM25F ext, when performance is measured in terms of recall rate. For example, Figure 5 shows the results on Eclip se IDE and RT projects, at different top N lists. For the two proj ects, BugSim improves the SVM method by 14.15%  X  53.26% and 9.75%  X  29.00%, respectively, and improves BM25F ext by 2.59%  X  5.47% and 4.49%  X  12.00%, respectively. Table 5 give s the details on the percentage of improvement BugSim over SVM and BM25F ext on Recall@10 values, for all experimental projects. The average improvement of BugSim over SVM and BM25F ext is 15.41% and 3.71%, respectively. RQ3: Does the Fidelity loss function outperform the related loss functions? Figure 6 shows the impact of two different loss functions (Fidelity loss and RNC) on BugSim. Fidelity loss outperforms RNC when recall rate is concerned. For example, on the Eclipse IDE project, Fidelity loss function leads to 14.89%  X  31.37% higher recall values than RNC. On the RT project, Fidelity loss function leads to 18.05%  X  31.34% higher recall values than RNC. Similarly, Fidelity loss function also outperforms RNC when the effectiveness is measured in te rms of MRR (Figure 7). Fidelity loss leads to 4.07%  X  27.78% higher MRR values on 10 projects. On STP and DSDP projects, the two loss functions obtain similar performance. A paired t-test confirms that the results are statistically significant, at the significance level 0.05(with p value 0.0002). 
Table 5. The comparison between BugSim and the related Project Bug-Sim BIRT 70.94% 50.74% 39.81% 67.98% 4.35% DataTools 82.76% 62.07% 33.33% 75.86% 9.10% DSDP 78.85% 71.15% 10.82% 75% 5.13% Eclipse Foundation 76.19% 73.02% 4.34% 73.02% 4.34% Eclipse Modeling 88.89% 80.81% 10.00% 86.87% 2.33% Technology 73.01% 57.52% 26.93% 70.80% 3.12% Tools 78.90% 69.32% 13.82% 76.16% 3.60% TPTP 62.65% 59.04% 6.11% 61.45% 1.95% 
WebTools 68.14% 57.52% 18.46% 69.03% -1.29% average 76.11% 65.95% 15.41% 73.39% 3.71% Figure 6. The impact of Fide lity Loss and RNC functions on 
Recall Rate
Recall Rate
Recall Rate
Recall Rate Figure 7. The impact of Fide lity Loss and RNC functions on In Section 5.1, we have show n that BugSim outperforms SVM and BM25F ext based methods for detecting duplicate bug reports. We have identified the following major differences between BugSim and the two previous methods: 1) Feature selection: Inspired by the work on discriminative 2) Learning method: BugSim is essentially a pairwise learning Although our experiments show th at BugSim can effectively retrieve duplicate bug reports, it may still fail to identify some duplicate bug reports. BugSim assumes that duplicate bug reports have high degree of text similarity, which is a valid assumption for most of duplicate bug reports. However, we found that some duplicate bug reports may use different words to describe the same problem. For example, when reporting the problem of a software crash, different reporters may use different words such as  X  X xception X ,  X  X rash X ,  X  X ailure X ,  X  X rror X ,  X  X own X , etc. Currently BugSim does not consider the se mantic similarity among words. Therefore, it may treat these reports as dissimilar texts. Another reason of missing cases is that some bug reports are very brief, as the users did not spend much time on writing detailed problem scenarios. This also causes difficulty in statistical analysis. We will address these two issues in our future work and to further improve the performance of the proposed approach. BugSim and the related met hods (such as the SVM-based discriminative model and the BM25F ext model) basically detect near-duplicate documents by co mparing the text similarity between pairs of bug reports using information retrieval techniques. Another major approach to the detection of similar documents is a te chnique known as fingerprinting , which represents and compares documents using compact values Fingerprint-based methods include Shingling [3, 9] and SimHash [7, 9, 19]. We compare the performance of B ugSim to that of SimHash on the Eclipse projects. The results are shown in Figure 8. The MRR values of SimHash are ranging from 0.15 to 0.57 (with an average of 0.29), while the MRR values of BugSim are ranging from 0.43 to 1.00 (with an average of 0.59). A paired t-test also confirms the statistical significance (p = 0.00). Clearly, BugSim is more capable of detecting duplicate bug reports than SimHash. According to the literature, SimH ash is effective in detecting duplicate web pages at a very la rge scale [9]. The duplicate web pages are identical/near-identical web pages stored at different locations. While in the cases of bug reports, although duplicate bug reports describe the same problem, they could differ in the detailed natural langua ge descriptions. 
Figure 8. The comp arison between BugSim and SimHash There are potential threats to the validity of our work:  X  The performance of our approach relies on bug report quality.  X  The dataset used in our expe riments is collected from the  X  In our experiments, we evalua te BugSim on 45,100+ real bug Duplicate documents are often found in large corpus. Many researchers have proposed met hods for detecting duplicate or near-duplicate documents. For exam ple, Broder et al. [3] used word sequences (shingles) and a cl ustering algorithm to efficiently detect near-duplicate web pages. Brin et al. [2] proposed to use word sequences to detect copyright violations. Theobald et al. [24] proposed SpotSig, an algorithm for extracting and matching signatures for near duplicate dete ction in large Web crawls. Sood and Loguinov [19] used SimHash [7] to detect similar document pairs in large-scale collections. The above methods are based on documents X  fingerprints. Hoad and Zobel [11] also experimented with information retrieval based method for identifying versioned and plagiarized documents. They proposed several term-weight based similarity measures (called identity measures) to rank similar documents. Their experiments on two document colle ctions showed that both the identity measures and the fingerprint-based methods can identify duplicate documents, and in genera l identity measures outperform fingerprint-based methods in terms of accuracy. In this work, we focus on a special form of documents  X  bug reports. Unlike duplicate web pa ges or plagiarized documents, duplicate bug reports are not identical. The scale of bug reports for a software system is usually much smaller than the scale of a Web corpus. We identify statisti cal features of bug reports and apply learning to rank techniques to detect duplicate bug reports. Recently, some researchers have proposed methods for detecting duplicate bug reports. We have already discussed and compared the SVM-based discrimina tive method and the BM25F ext in previous sections. Runeson et al. [18] also treated bug reports as text documents and applied the Vector Space Model to retrieve duplicate bug reports. They com puted a term X  X  weight as 1+log(frequency) , and applied three measures (cosine measure, dice coefficient, and Jaccard coefficient) to measure text similarity. Their experiments on Sony Ericsson mobile systems showed that about 40% of the duplicates could be found. Sun et al. [20] found that their SVM-based disc riminative method outperforms Runeson et al. X  X  method. Wang et al. [27] proposed to detect duplicate bug reports using both natural language text and execution information. However, their approach is limited in that not all bug reports come with runtime program execution information. Jalbert and Weimer [12] built a classifier to detect duplicate bug reports. They used bug reports X  surface features, text si milarity metrics, and graph clustering algorithms to identif y duplicate documents. Their evaluation on 29,000 bug reports from the Mozilla project showed that 8% of duplicate reports can be successfully detected. Recently, Sureka and Jalote [22] proposed to use n-grams based features to characterize a bug report. Their experiment on Eclipse projects showed that th eir approach is able to detect duplicate bug reports with reasonable accuracy . They randomly selected 1100 duplicate Eclipse bug reports and found that their approach can achieve Recall@10 value 21% an d Recall@20 value 25%. Sun et al. [21] reported that their BM25F ext method outperforms Sureka and Jalote X  X  method. As described in previous s ections, BugSim outperforms the existing state-of-art SVM and BM25F ext based methods (therefore other related methods described a bove). On Eclips e projects, the Recall@10 values BugSim achieve s are between 61% and 100%. Although quality control measures ha ve been taken, in reality, software systems contain bugs. Fo r a large-scale, widely-used software system, the project team could receive a large number of bug reports over time. The exis tence of duplicate bug reports could greatly increase software main tenance efforts. In this paper, we have proposed BugSim , a duplicate bug retrieval method based on learning to rank techniques. B ugSim constructs a training set by assembling pairs of duplicate and non-duplicate bug reports. A training model is built using textual and statistical features of bug reports and a similarity function based on these features. Our experiments on 12 Eclipse projects show that BugSim is effective in retrieving duplicate bug reports. The recall rates for the top 10 retrieved reports are above 61%, with an average of 76.11%. Our method also outperforms the relate d state-of-art methods (SVM and BM25F ext ). In the future, we will consider the semantic similarity between bug reports, aiming to further improve the performance of the proposed approach. We will evaluate the proposed approach on industrial projects. We also plan to integrate our approach into an existing bug tracking system. This research is supported by the NSFC grant 61073006, and the Tsinghua University project 2010THZ0. We thank the authors of [21] for providing us the tool and the Eclipse dataset used in their experiments. [1] J. Anvik, L. Hiew, and G. C. Murphy. 2006. Who should fix [2] S. Brin, J. Davis, and H. Garcia-Molina. 1995. Copy [3] A. Z. Broder, S.C. Glassman, M.S. Manasse, and G. Zweig. [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. [5] Y. Cao, J. Xu, T-Y Liu, H. Li, Y. Huang, and H-W Hon. [6] Z. Cao, T. Qin, T-Y Liu, M-F Tsai, and H Li. 2007. [7] M. S. Charikar. 2002. Similarity estimation techniques from [8] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 2003. An [9] M. Henzinger. 2006. Finding near-duplicate web pages: a [10] R Herbrich, T. Graepel, and K. Obermayer. Large margin [11] T. C. Hoad and J. Zobel. Methods for identifying versioned [12] N. Jalbert and W. Weimer. 2008. Automated Duplicate [13] T. Joachims. 2002. Optimizing search engines using click [14] T-Y Liu. 2009. Learning to Rank for Information Retrieval. [15] T . M. Mitchell. Machine Learning . New York: McGraw [16] R. Nallapati. 2004. Discrimina tive models for information [17] S. Robertson, H. Zaragoza, and M. Taylor. 2004. Simple [18] P. Runeson, M. Alexanderson, O. Nyholm. 2007. Detection [19] S. Sood and D. Loguinov. 2011. Probabilistic near-duplicate [20] C. Sun, D. Lo, X. Wang, J. Jiang, and S-C. Khoo. 2010. A [21] C. Sun, D. Lo, S-C. Khoo, an d J. Jiang. 2011. Towards more [22] A. Sureka and P. Jalote . 2010. Detecting duplicate bug [23] M. Taylor, H. Zaragoza, N. Cr aswell, S. Robertson, and C. [24] M. Theobald, J. Siddharth, and A. Paepcke. 2008. SpotSigs: [25] M -F. Tsai, T-Y. Liu, T. Qin, H-H. Chen, and W-Y. Ma. [26] E.M. Voorhees. 1999. Question Answering Track Report. In [27] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. 2008. An [28] F. Xia, T-Y. Liu, J. Wang, W. Zhang, and H. Li. 2008. [29] J. Xu and H. Li. 2007. AdaRank: a boosting algorithm for 
