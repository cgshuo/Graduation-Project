 1. Introduction
Monitoring streamed data is very important in many different scenarios. Examples of such cases are: tele-sary to approximate aggregate answers to user queries over data stream windows [4,16,20,19] , or to estimate the data frequency peaks or join sizes [2,3] .
Thus, the massive streams of data generated by these applications, sometimes, need to be summarized through mind have to be able to adapt to the heavy-tailed distributions that these streams usually present.
As a whole, this is a complex problem and several data structures have been proposed to keep approximate counts of items using the smallest amount of memory possible and a fast per-item response time. The data structures proposed in this area are the counting bloom filters (CBF) [17] , and their dynamic extensions: the spectral bloom filters (SBF) [10] and the dynamic count filters (DCF) [1] . CBF are a counter-oriented
CBF may saturate and fail in their mission of keeping an accurate track of counts with skewed data. As an alternative, the SBF were designed to dynamically adapt the size of the counters to the characteristics of access times because of the indexing structures they require. Finally, DCF allow for dynamic environments with unlimited size counters and fast access times. However, they waste a large percentage of the memory they allocate in the presence of data skew, and fail to provide fast average response times because of the heavy reconstruction phases that they require.

In this paper, apart from reviewing and analyzing all the data structures mentioned above thoroughly, we propose a partitioning strategy that, once applied to a dynamic approach such as SBF and DCF, solves all the problems mentioned above. On one hand, it helps to make SBF and DCF insensitive to changes in the char-implementations where memory may be a restriction. Second, it reduces the average response time because it simplifies the painful reconstruction phases of the previous non-partitioned methods, reducing the worst case response times, and adapting better to high data streamed frequencies. Third, it assures and, under some cir-cumstances, it improves the accuracy of previous approaches, making it possible to use it in environments where accuracy is important but memory is again a restriction.

On the other hand, under stress situations, like changes in the intensity of the data flux, our partitioning strategy allows for fast response and high accuracy. The degradation shown by SBF in such situations, is reduced significantly by our partitioned strategy combined with DCF. 1.1. Contributions
In order to make this paper self-contained and to perform a thorough analysis of the methods studied and proposed, we make the following contributions:
We analyze the streamed data monitoring problem in perspective, reviewing and understanding the previ-ous literature on the topic, and in particular CBF, SBF and DCF. The analysis allows us to understand the benefits and drawbacks of these structures.

We propose a generic partitioning strategy that can be easily adapted to dynamic approaches in general at a low cost, significantly improving their characteristics as mentioned above. Our partitioning strategy tackles the drawbacks of SBF and DCF with clear benefits in all the aspects mentioned above.

We present mathematical models that allow for two different analysis: (i) a memory comparison of the four approaches and (ii) a comparison of the complexity of the insert/delete/query/rebuild operations for each of the data structures. The models that we propose are very helpful in order to show the most interesting fea-tures of each data structure and to understand the results obtained through our real tests. We evaluate all the strategies analyzed in the paper, including the partitioned and non-partitioned SBF and
DCF, which is the first profound evaluation of such monitoring data structures. We do that in a set of very different scenarios to give a complete view of their characteristics.

As a general view of our contributions, our results show that our partitioning strategy is robust because it satisfies the constraints imposed by data stream environments, improving significantly in terms of memory space, response time, accuracy and versatility compared to the use of SBF and DCF alone. 1.2. Organization of the paper
This paper is organized as follows: In Section 2 we enumerate the different structures used for monitoring data streams and explain a set of scenarios in which the techniques proposed and evaluated in this paper can 5 , we seek the optimal number of partitions for PSBF and PDCF. In Section 6 we model and compare PSBF,
PDCF, SBF, and DCF. In Section 7 , we present the experimental results for the different scenarios and, finally, we conclude in Section 8 . 2. Applications and evolution of data set monitoring
Data set monitoring has a wide range of application areas. The need to detect the presence of data items in a set or to approximate aggregate answers for a query, makes it necessary to use structures such as bloom where the bloom filters and the counting structures are necessary.

The bloom filter has been widely used for membership monitoring purposes: in multi-join queries [9,21] , where the objective of their use is to save I/O in Hash Join and Merge Join operations; in generalized hash teams [22] aimed at avoiding the partitioning of relations that are intermediate results between the nodes of of the joining keys during the execution of semi-joins [5,24] ; or to save both data traffic communication and computational time in the centralized relational database Gamma machine [13,14] . In all the cases, the filter structure can be used and sent in a compressed way, as proposed in [27] .

With the need for real-time data set monitoring applications ranging from network processing to streamed over multisets. In [17] the authors propose the counting bloom filters (CBF) in order to overcome this limi-tation. In their research, for a proxies network environment, a CBF represents the summary cache directory of a particular participant proxy. Thus, each proxy has a copy of all the CBF in the system. This way, when looking for a specific Web page, the CBF are checked for potential hits before sending any query. Each proxy hence, all proxies have an updated summary cache of all the components in the system.
CBF have also been investigated for their use in network environments to summarize the content of peer-to-peer systems, in resource routing, in packet routing, and for network performance improvement [8] . CBF routing lookups [15] .

The spectral bloom filters (SBF) [10] and the dynamic count filters (DCF) [1] , are alternative representa-tions of CBF. In [10] , besides presenting SBF, the authors also present new methods for reducing the prob-ability and magnitude of errors. In their work, the authors propose multiple uses of the SBF within the [25] operation in order to simplify and shorten the execution time of distributed joins. Also, Bloom histo-grams, a further compressed view of SBF, are used to keep counting statistics for paths in XML data [30] . 3. Data structures for data set monitoring
In this section, we review the data structures proposed for data set monitoring. In order to make the paper for the common variables used for the different data structures that we explain in this paper. Note that, throughout the paper, we use the terminology data element and value interchangeably. 3.1. Bloom filters
A bloom filter [6] is a bit-vector of m bits that represents a set of n different values, S = s value ranging from 1 to m : hence for each value s 2 S , d positions h filter. Assuming that all those hash functions are perfectly random, the d hash functions may map different values of S onto the same position of the bloom filter with probability P .
 hash functions to t and test the values of the corresponding positions in the bloom filter. Only if " i : that t is not in S .
 probability, p , that a particular bit is 0 as
Thus, the probability of a false positive in the bloom filter is: P =(1 p )
The analysis presented in [28,29] shows that the number of hash functions d that minimizes the probability of a false positive is given by although, practically, a smaller d may be preferred because it results in a smaller computational cost. number of false positives is proportional to n and d , m = n d (log usage is approximately a factor of n 1.44. 3.2. Counting bloom filters
A counting bloom filter (CBF) [17] is an extension of the bloom filter data structure, where each bit of the bit-vector is substituted by a counter. Thus, a CBF consists of a sequence of C M = n l , where l is the multiplicity factor.
 simply set to one, in CBF the counter in each of the d entries is incremented by one. In an analogous way, to
The usual representation of a CBF is a data structure where counters have a fixed size over time. Such a representation has two major drawbacks: (i) whenever an insertion of a new element results in a counter over-in memory waste if data do not have a uniform distribution.

Dharmapurikar et al. [15] address the problem of overflowed counters. In their approach, the CBF struc-
However, the refresh of this data structure may be very costly because all data elements must be re-inserted again and the number of elements mapped onto the saturated counters may be large. 3.3. Spectral bloom filters Cohen and Matias [10] proposed the spectral bloom filters (SBF), which are a compact representation of
CBF. The main goal of SBF is to achieve an optimum counter space allocation. SBF consist of a compact base array of a sequence of m counters, which represents a set of M values as with CBF. At any point in time, the goal of SBF is to keep the size of the base array as close to N bits as possible, where N  X  Note that from now on, we assume log to be log 2 .

To achieve its goal, each counter C j dynamically varies its size such that it has the minimum necessary bits
SBF includes e m slack bits that are placed among the counters. A slack bit is added every 1 0&lt; e 6 1.

While the counter space in SBF is kept close to the optimum value, in order to support the flexibility of having counters with different sizes, SBF require complex index structures. In the context of this paper, we identify the index structures described in [10] as
Coarse vector ( CV ): a bit-vector index that provides offset information for the beginning of a subgroup of counters. Offsets are provided using counters of a fixed size length in bits.
 CV.
 the subgroup of counters that fulfills providing more detailed information about the offsets for each counter C that a subgroup of counters fulfills used that contains the exact offset for each counter. For simplicity, and without loss of generality, the
SC in chunks of loglog N counters (SC 0 ), and holds a total of log 3 N , each offset can be represented with 3loglog N bits, totalling 3log N per each SC position of C j . The OV consists of loglog N offsets of 3loglog N bits each offset, totalling 3(loglog N ) subgroup SC 0 .

The offset vector associated with SC 0 can also be substituted by a lookup table depending on a threshold based on the length of SC 0 . More details about this approach can be found in [10] .
The main drawbacks of the SBF approach are (i) the need for traversing the index structures in order to locate a counter, which derives into large item access times and (ii) the large number of rebuild operations that need to be performed, i.e. one rebuild operation is needed for each overflowed counter. Moreover, the counter space involved in a rebuild operation may be considerable depending on the positions of the over-flowed counter and the first available slack bit in the base array. Note that these penalties become worse as the size of the base array and the index structures grow because of the need to represent a large number of distinct values. 3.4. Dynamic count filters
Dynamic count filters (DCF) [1] are composed of two different vectors. The first vector is a basic CBF with each entry being a counter of fixed size x  X  1  X  log M
C , ... , C m , the CBF vector, hereby named CBFV, accounts for a total of m x bits.
 The second vector is the overflow counter vector (OFV), which also has the same number of entries as the ing entry in the CBFV overflowed. The size in bits of each counter in the OFV ( y ) may vary dynamically depending on the distribution of the values in the data set and depends on the largest value stored in the
Fig. 2 is a representation of DCF. There, it is possible to observe that DCF are composed of m entries with m counters split in pairs of counters, h OF 1 , C 1 i , ... , h OF the CBFV. All the counters in DCF have the same size, equal to x + y bits, where y may vary its bit length dynamically. As an example, when a data element has to be inserted, in the case that a counter C then the value of C j is set to zero and the corresponding counter in OFV, OF the case that the overflow counter OF j has to be incremented from 2 be performed, it is necessary to rebuild the OFV such that one bit must be added to all the counters in the OFV.

The decision of having the same size for all the counters implies that, first, many bits in the DCF structure will not be used unless data are homogeneously distributed and, second, the access to both vectors is fast write mechanism that has an asymptotic cost of O (1), avoiding the use of complex indexing structures, and saving memory space in most of the cases compared to other approaches [1] .

The main drawback of the DCF approach is that it adds one bit per counter whenever an OFV counter overflows. This fact is not important for the representation of small data sets. However, when the number of distinct values to be represented increases, it may cause: (i) memory waste because it may create a lot of unused counter space, especially for skewed data and (ii) time-consuming rebuilds of the structure because work on DCF. 3.5. The importance of rebuild operations
Rebuild operations are important because they stop the access to data while being executed. During a rebuild, neither reads nor updates can be performed, which degrades the per-item response time. In order to save the reads or updates performed during a rebuild, it is necessary to keep some extra storage, a buffer area for pending requests. 3.6. Qualitative comparison of CBF, SBF and DCF Table 2 shows a comparison of the three bit-counter-oriented approaches explained above: CBF, SBF, and DCF.

CBF are static and may saturate their counters when data are skewed or have a significant number of repeated values, which turns into a very low accuracy and poor versatility.

SBF, which improve the accuracy and versatility of CBF because they adapt dynamically to the changing nature of data, have a large per-item access time and significant memory requirements. The large average per-when traversing the indices to access the data items. The memory requirements are because of the amount of indexing structures required.

Finally, DCF takes the best qualities of the two other techniques, the dynamic counters from SBF and the fast access from CBF. However, because of the expensive and memory inefficient rebuild operations, DCF achieve neither fast average per-item access times nor low memory budget. 4. Partitioning the dynamic data structures
We propose a simple and powerful partitioning strategy for the dynamic data structures used in monitoring data streams. This is an interesting solution because any dynamic approach may benefit from this. Our strat-egy solves the space and time penalties of SBF and DCF. We achieve this goal by clustering the streamed data set into c different partitions. This way, each insert, delete and query operation only interacts with one partition, as opposed to the whole data structure. In addition, only the partitions where an overflow occurs tenance costs, and improves both space and execution performance, as we will show later.
Fig. 3 shows the general idea for the partitioning strategy. This consists in decomposing the m original counters of the counter structure into c groups that we call partitions. Each partition holds a total of m ing on the situation.

Each partition is accessed through a simple index data structure, the partitioning vector (PV), and each implement the counters in the partitions.

In the partitioning-oriented approach, looking up for a certain data element s results in checking d filter entries using hash functions, h 1 ( s ), h 2 ( s ), ... , h hash function returns a value that ranges from 1 to m and determines the counter to be accessed. Then, we is a simple division operation K i  X  h i  X  s  X  calculated through a modulo operation, j = h i ( s )mod m entry within a given partition consists of using the access methods designed for the original strategy, SBF or DCF. Note that the partitioning does not harm the access time incurred by the original data structures other than by the fact that we also access the PV.

In the following sections, we present the details of the partitioning approach applied to SBF and DCF, called partitioned SBF (PSBF) and partitioned DCF (PDCF), respectively. 4.1. Partitioned spectral bloom filters (PSBF)
As shown in Fig. 4 , our partitioning scheme splits SBF into c partitions (SBF sequence of C 1 ; C 2 ; ... ; C m 0 counters in each partition.

All data structures and access methods within each SBF partition are as in the original SBF [10] . Hence, similar to what we explained in Section 3 , the aim of the SBF data structure within a partition is to keep the size of the base array as close to N 0 as possible, where N nificant reduction in the memory usage because the counter space and the amount of slack bits for PSBF and SBF are the same.

Overall, we can say that the partitioning strategy used on SBF leads to: (i) a reduction of the total rebuild item access time caused by the smaller size of the index structures used in each partition. 4.2. Partitioned dynamic count filters (PDCF)
The structures required for the implementation of the c partitions of PDCF are based on the data structures presented for the DCF approach.

As shown in Fig. 5 , PDCF consist of three sets of data structures: c partitions of the CBFV (CBFV 1 6 i 6 c ) and OFV (OFV i , for 1 6 i 6 c ), and the PV. Each partition of CBFV consists of m  X  C c m 0 x  X  m x bits. In the same way, each partition of the OFV consists of m  X  OC 1 ; OC 2 ... ; OC m 0  X  of size y i = b log(max( OC j the PV stores two fields: (i) a pointer to the corresponding OFV determine its size in bits.

Overall, PDCF achieves a better use of the memory space by clustering contiguous counters into groups that expand and shrink homogeneously. As such, the partitions grow as a consequence of the overflow in their counters, and overflow operations become more efficient for PDCF than for DCF. 4.3. Example
Both SBF and DCF obtain clear benefits when being partitioned with the technique described above. How-ever, given the nature of both methods, an example for DCF will make it clearer how the memory usage for the actual counters change when different numbers of partitions are implemented. We do not show this for
SBF because it does not really save memory with partitioning, and clear examples of SBF X  X  use of memory can be found in [10] .

Fig. 6 shows the benefits we obtain by using our partitioning strategy for a DCF that summarizes the num-ber of accesses to the addresses stored in a cache directory for a particular proxy. For simplicity, assuming only one hash function and no false positives, this subset of DCF counters would provide the number of acces-partitions. The gain for 6 partitions is obvious. Analogously, for PSBF we would have less counters to manage per partition, and, thus, smaller indices and rebuild times. 5. Optimum number of partitions ( c )
At this point, it is important to define the trade-off between the amount of memory saved by the partition-ing strategy, the space occupied by the PV and the amount of buffer area required during the rebuilt opera-tions. This will allow us to obtain the optimum number of partitions c for a partitioned data structure. among the m positions of the bit vectors. We also assume a total number of distinct values n and a total num-ber of data elements M . Further below, we extend the model to tune c under skewed data.
We use the following definitions: c number of partitions. t R rebuild time when there is only one partition.
 S p size of a pointer in bytes.
 S c size of a counter in bytes.

S e size of a data element in bytes. freq incoming frequency of the streamed data, requests second d size of the meta-data to manage an SBF partition.

PV entry size of an entry in the partitioning vector (PV). S
We build a simple model of the memory saved when using multiple partitions compared to the non-parti-tioned data structures. The model is built on the following assumptions:
The memory difference between the space occupied by SBF and the addition of the space occupied by the multiple partitions is not significant, close to zero, as we explained above.

In the case of PDCF, the difference is also close to zero because, for uniform distributions, the amount of overflows will be very similar for all the partitions.

Given those assumptions, we define fmem as a function to estimate the memory saved by the smaller buffer area compared with the size of the PV data structure required by the partitioning approach. We calculate fmem as partitioned approaches to keep incoming requests. In other words, t titioning approach for only one partition if we do not want to miss incoming streamed data due to rebuild same purpose. PV entry c is the extra space needed for PV.

Thus, the value of c that maximizes fmem is given by fmem then, for fmem 0 ( c ) = 0, we isolate c which tells us that c is proportional to the square root of the rebuild time t partitions have to be, and consequently, the larger the throughput that can be reached. This can also be ex-the buffer area t R c freq S e Fig. 7 , where we show how c (vertical axis) varies depending on t values for t R and freq , the larger the number of partitions needed to minimize the memory used. In practice, PV entry and S e are known values. The values of t observed values from previous executions. 5.1. Dynamic partition tuning under a data skewed input stream
In this section, we tune the optimum number of partitions for skewed data. Note that this can only be done for PDCF because the amount of memory used by SBF and PSBF is always the same regardless of the incom-ing data distribution. So, the skewed case for PSBF is restricted to that explained above for uniform data distributions.
Tuning c under data skew makes sense when it is a result of an unexpected large number of instances of a stream. We define the following parameters: occ is the fraction of partitions that monitor values with a large number of instances. For instance, occ = 1.0 means that the skew originated because of a large number of repeated values is evenly distributed among partitions. This is the worst case for PDCF because all partitions are oversized. average, we only use 10% of the counter space of the oversized partitions occ c . csize is the average counter size in bits for the occ c partitions.

We assume that we may know the parameters defined above, which is not an unreasonable assumption since we can extract these data from historical statistics after several executions. Thus, we tune c for PDCF to minimize the wasted space within each partition.

The savings in memory because of the unused counter space by the oversized partitions can be defined as: c occ csize (1 inn _ occ ).

By introducing this new variable in function fmem ,wehave and calculating its derivative, we have
Then, for fmem 0 ( c ) = 0, we isolate c
Note that this equation assumes that occ csize (1 inn _ occ )&lt;PV PV entry is at least 10 bytes for a 64-bit machine, divided into an 8 byte pointer plus 2 byte counter.
As a more complex approach we could monitor each partition during execution time, and dynamically split oversized partitions such that each partition has its own intra-partitioning schema. This approach requires a second partitioning vector within each partition. We do not contemplate this technique in this paper and we address its implementation and analysis for future work. 5.1.1. Example
As an example for PDCF, Fig. 8 shows results for the average rebuild time and maximum memory usage form and skewed data (using a Zipfian distribution [7] with h = 2.0 thus we may have an idea of what would happen with intermediate data distributions.

In this case, the memory usage plot in Fig. 8 a, takes into account the buffer area defined above. For this example, we observe that PDCF used on uniform distributions increase the memory requirements compared to the case of only one partition, starting from 175 partitions, while PDCF used on skewed data require more memory than DCF for the same data set starting at 975 partitions.

Fig. 8 b shows that the rebuild time is significantly reduced as the number of partitions increases. The rebuild time is reduced up to two orders of magnitude compared to the case with only one partition (equiv-alent and DCF). 6. Comparing SBF, DCF, PSBF, and PDCF
We use models to compare the four data structures from different points of view: the memory resources needed, the time to access and update a counter and the time to rebuild the data structures when a counter overflows. We evaluate the practical issues of these implementations in Section 7 . For the analysis, we use the same terminology defined in Section 3 . 6.1. Memory resources
We start modeling the differences between the approaches in terms of their memory usage depending on different issues such as the skew of the data set, or the number of different values. The dynamic buffer area defined in Section 5 is not taken into account for the following discussions. For the following analysis we where the number of repeated values of one or more counters exceeds by 100% the expected counter size. Note a large number of repeated values, that is to say a value of alpha greater than zero. determine that SBF needs m bits for CV1, 3 m bits for CV2, and 3 m loglog N for the OV. Thus, we can define the memory used by SBF as a function of the initial number of counters m : Mem glog( N ), where N = m + e m . Note that, although the SBF base array is very compact and equal to N bits, a large amount of extra memory is required because of the size of the indices.
 Given the definitions above, we define the memory used by PSBF as bytes. With this, it is easy to show that Mem PSBF is very similar to Mem compare SBF and PSBF and we only compare DCF, PDCF and PSBF.
 The memory used by DCF is determined by Finally, we define the memory occupied by PDCF as a function of Mem where  X  1 occ  X  m used by PV assuming that pointers are 8 bytes and counters 2 bytes long. Note that, as we pointed out in Sec-tion 5 , c is proportional to the square root of n .
 Fig. 9 a shows how the ratio Mem DCF
Given a number of distinct values represented by n , M varies proportionally from 10 to 100,000 times n repeated values. This shows how the number of elements in the data set and the different values represented there relate. In any case, DCF always uses more memory than PDCF, and data sets with larger skew benefit PDCF more than DCF (small occ ).

Given that PDCF always require less memory than DCF, we will only compare PSBF and PDCF using PDCF, where little skew would benefit PSBF more significantly. So, for the scenarios analyzed here, Fig. 9 b shows that
PDCF, as opposed to DCF, behaves very well on skewed data: i.e. when data is skewed, PDCF expands just a few partitions, which results in memory savings that increase as data are geared towards skew.
PDCF always outperforms PSBF (ratio over 1) except for extreme cases where we have a large number of repeated values with a uniform distribution. 6.2. Access and update operations The cost of accessing a counter in the SBF structure Lookup beginning of SC 0 within subgroup SC, with cost O (1); and (iii) calculating the exact offset for C through OV, O (loglog N ). Hence, we get an accumulated cost of: Lookup
In contrast, the cost of accessing a counter in DCF does not need the use of index structures and it has a cost Lookup DCF = O (1).
 The partitioning approaches only add one division and one modulo operation, as we explained above. These two operations can be considered negligible in the overall access time and we can conclude that asymp-totically Lookup PSBF = O (loglog N 0 ) and Lookup PDCF = Lookup
N  X  m  X  _ m for the partitioning approach, being N 0  X  N c
Each time we query, insert or delete a data element s in SBF, DCF, PSBF, and PDCF, we must lookup and, in some cases, update counters h 1 ( s ), h 2 ( s ), ... , h tion of each counter in the base counter array that has an asymptotic cost of Lookup Lookup PSBF d , respectively. For DCF and PDCF the access to the counters is direct and has a cost of
Lookup DCF d . Once the counters are located, it is necessary to perform simple operations to read, increment or decrement the counter by one. 6.3. Structure rebuilds because of overflows in the counters. We model the cost of such rebuilds here.

In particular, the rebuild of an SBF is performed by adding available slack bits to it. These slack bits are placed among the counters but may not be adjacent to the counter requiring them. Because of that, when a between in order to make space where required. Thus, the cost of a rebuild depends on the closest position
In [10] , the authors make assumptions about the maximum number of inserts that have to be done to give an asymptotic cost to the operation.

Letting C j represent the counter that needs to be expanded, and C to it, then the number of counters k involved in the rebuild are l j +1if l P j ,or j l otherwise. Therefore, for each counter we must locate its position in the base array ( Lookup and in the worst cases, the asymptotic cost is O ( m ) Lookup
PSBF, compared to SBF, have a smaller rebuild cost because of the smaller counter space for each SBF partition, m 0  X  m c respectively.

Rebuilds on the DCF structure are O ( m ). They have the advantage that iterating through a set of entries to tation of the data locality in the memory hierarchy. Note also that the number of rebuilds to be performed is far smaller than the number of rebuilds performed by SBF. This is because each rebuild only affects the over-or PSBF approaches. This was already shown in [1] .

PDCF has a smaller rebuild time than DCF because only those partitions that contain overflowed counters
DCF during rebuild operations. In addition, for non-uniform distributions, PDCF has to reallocate less coun-ter memory space than DCF, which is also reflected in a reduction of the overall processing time during the rebuild operation.

Overall, the partitioning approach improves the rebuild cost significantly, both in time and memory, espe-
PDCF: hence, considering that Lookup PSBF is also smaller as the number of partitions increases, the rebuild time is expected to be smaller for PSBF. However, as in DCF, for each rebuild operation, PDCF adds one bit the PDCF approach. 7. Experimental results
We are going to evaluate and compare the techniques described and analyzed up to now. The objective is to shed light onto the real practical issues of SBF, DCF, PSBF, and PDCF. The goal of our tests is to show the response of each approach to important constraints in the data stream environment like memory budget, per-item response time, accuracy, and versatility.

All the approaches have been programmed in C. The implementations of SBF and DCF have followed the exact specifications given in [10,1] , respectively. We ran our tests on a 750 MHz IBM PowerPC _RS64-IV processor with 16 GB of main memory. The operating system is AIX version 5.1. Programs have been com-piled with full optimization-O3 1 .
 sites, IP traffic, word frequencies in natural text, etc. [12,11] .

Data streams for these applications are rarely uniform, and present significant skew. For Zipfian distribu-tions, the skew is defined by parameter h that may range from 0 to 2. Values for h  X  0 represent uniformly our evaluations range from uniformly distributed to very skewed, showing the extreme cases and allowing us to test the real behavior of the techniques tested.

Unless otherwise specified, the number of partitions c for PSBF and PDCF are calculated using the opti-mum partitioning coming from Eq. (3) , as explained in Section 5 . Values for the rebuild time t freq , are obtained based on the values observed for real executions. The number of counters m for the four of hash functions used d . We set by default P = 0.05 and d = 3. The number of distinct values n may change, and it is specified for each experiment we describe. Also, the total number of values M may vary for the dif-ferent tests.
 ters matches the real number of times s that it has been inserted in the data set.

We first show results regarding the access times and rebuild times for a static scenario, which extend the results shown in [1] . Then we execute the four techniques in a dynamic scenario where streamed items are skewed data and unpredicted values in the streamed items. The two sets of tests mentioned are newly designed for the techniques evaluated here and represent a new approach to understanding the behavior of dynamic data structures for data set monitoring. 7.1. Static evaluation: read, write and rebuild time Fig. 10 shows the average time to perform access and rebuild operations for the SBF, DCF, PSBF, and amount of M = 100 n values, uniformly distributed, have been first inserted, then queried, and finally, ran-plete execution of the test.

From the results in Fig. 10 it is possible to conclude that PSBF and PDCF show a significant reduction in the rebuild time compared to SBF and DCF, respectively (more than two orders of magnitude). In addition, while the rebuild time for DCF and SBF grows linearly with respect to the number of distinct values n , the rebuild time of PSBF and PDCF grows slower. In Section 6 , we showed that the rebuild costs for SBF and
DCF are strictly proportional to m , and for PSBF and PDCF that they are proportional to m values of c for PSBF and PDCF, thereby decreasing their rebuild time.

Although PSBF improves its access time significantly over SBF, the access time of DCF and PDCF is still for PSBF is less than that for SBF because of the smaller counter space within each partition. 7.2. Dynamic analysis
We want to analyze the behavior of the four techniques in a dynamic scenario, where three million inserts, updates and queries at a random order. For data insert operations, h is randomly chosen in each time step, thus we may have skewed groups of data at execution time, mimicking real scenarios where there are trends in the data set. With the same objective, the ratio between inserts, deletes and queries changes over time at random. We have performed the tests with a total number of distinct possible values set between n = 10,000 and n = 100,000.

Fig. 11 shows a plot and a table. The plot shows the memory usage for the proposed approaches for the case of 100 thousand distinct values. The memory usage is determined by the maximum memory needed dur-tom of the same figure.
 After analyzing the results in Fig. 11 , we can point out the following important aspects:
Given a set of operations, the partitioning strategies complete their execution much faster than their non-partitioned counterparts. In particular, for n = 100,000, while PSBF and PDCF complete their execution in 59 and 3.5 s, respectively, SBF and DCF required 930 and 32 s. Therefore, the execution time of PDCF was approximately 10 times faster than that of DCF, and the execution time of PSBF was 17 times faster than that of SBF. For a total of 10,000 distinct values, the differences are not so big because rebuild operations become cheaper for SBF and DCF (see Fig. 10 ).

The partitioned approaches consume less memory than their counterparts. For SBF, the memory consump-tion is static during the whole execution, while for DCF, the consumption is dynamic and has some vari-ability introduced by the automatically reconfigurable threshold [1] .

PDCF require less than 5 times the memory required by PSBF. Data are inserted and deleted over time, thus, while PSBF have the same static size right from the beginning, PDCF dynamically adapt the size of their structures according to the data requirements.

Although we observed that PSBF show a faster rebuild time than DCF, the DCF approach still has better performance. The rebuild of one counter in DCF, automatically rebuilds the rest of the counters, growing the whole OFV vector by one bit. On the other hand, each SBF rebuild involves only one counter at a time resulting in a larger number of rebuilds. 7.2.1. Trade-off between memory and accuracy
In order to compare all the data structures under the same circumstances, we fix the amount of memory. In particular, we fix the memory used by PDCF to the largest size used by PSBF. The extra space is used to decrease the probability of false positives P , increasing the number of counters for PDCF. The results obtained for this setup showed that PDCF achieves a larger accuracy of 98.7%, which is at least 8% more accu-rate than any other approach. In order to be fair, we also run experiments where DCF used the same amount of memory than SBF. For those experiments the accuracy for DCF was 95%, which is still 3% less than PDCF.
Note that another possibility would be to use the memory saved by the partitioning strategy to make more partitions than the optimum obtained through Eq. (3) . This could be necessary in the case that we needed to tune the data structure to improve its performance.
 7.3. Varying the data streamed frequency
We want to understand the effect of different incoming data frequencies, and how the partitioned structures adapt by increasing the number of partitions c . In this section, we concentrate on PDCF and skip PSBF because the results are similar.

Table 3 shows results of a particular example for the PDCF approach where, according to the model presented mum value using Eq. (3) . Data elements are considered to be 4 bytes, S
S = 2 bytes and the size of a pointer to S p = 4 bytes. t R improvement because of the need to have more partitions. Under these circumstances, the rebuild time t shown in the table) and the buffer area size are reduced proportionally to the number of partitions. 7.4. Skewed data streams
Skew is important in some scenarios where the hash functions map input values into a few counters. This approach when there is skew in the streamed data. The plot shows the maximum memory usage (vertical bars factors (from h = 0.1 to h = 2.0).

For this scenario, we set the number of distinct values n to 100 thousand. In total, M =10 n values are PDCF, c is tuned for each different value of h using Eq. (4) .
 We can observe that the partitioned approach shows substantial benefits when we face scenarios with skew:
PSBF shows a better average per-item response time than SBF because of the smaller per-partition rebuild times. Note that the higher the skew factor, the lower the average per-item response time is for PSBF. This is expected since fewer counters are overflowed and, consequently, less rebuilds are performed. The memory usage is slightly improved by PSBF and remains the same for all tests because, as explained through the paper, the size of PSBF and SBF does not depend on the incoming data distribution.

PDCF improve in all the aspects compared to DCF. In terms of memory usage, only a few groups of coun-ters are expanded because of an unexpected high number of repeated values ( h = 1.0, ... ,2.0). Thus, PDCF saves most of the unused counter space found in DCF. In terms of per-item response time, PDCF improves because of the improvement in the rebuild time, just as for PSBF.
 Note that the memory results are in accordance with the model presented in Section 6 . 7.5. Underestimated data set size expected when the data structure was created. We created SBF, DCF, PSBF, and PDCF to support a data set scenario represents a special case of an unexpected peak in the number of data elements in the set at some point in time. For clarity, the accuracy of DCF is not shown as it is the same as the one obtained for PDCF.
Fig. 13 a depicts the memory usage and Fig. 13 b the representation accuracy for the three approaches at each time step. Through these results we can see that PDCF borrows the flexibility of the DCF structure com-pared to SBF and PSBF. In the scenario of this test, SBF and PSBF have a constant memory usage, as it is inserts surpasses the expected number of data elements. At this point SBF and PSBF counters saturate and, consequently, the accuracy starts to drop sharply. In contrast, PDCF X  X  accuracy is nearly constant and always at a high-level (above 95%). This is achieved because of the ability of DCF and PDCF to dynamically increase the overflow space. This fact is a consequence of the increase of memory usage for DCF and PDCF.
The saturation of counters for SBF and PSBF is caused by the lack of slack bits , which due to the unex-pected large number of data elements inserted into the structures. This saturation could be avoided somehow if we dynamically added more slack bits as they get exhausted. However, this solution is not contemplated in rebuild the base array and we might also have to rebuild the index data structures. In addition, the PSBF counters may saturate sooner than the SBF counters. This is because the PSBF counters cannot benefit from the case of SBF. In contrast, PDCF, compared to DCF, shows to be very flexible and therefore capable of gracefully handling peaks in the number of data elements stored in the data set. In addition, PDCF keeps
DCF. 8. Conclusions
In this paper, we review different data structures for data set monitoring and propose a partitioning scheme oriented to improve the qualities of previously proposed data structures. Low memory budget, fast per-item response time, accuracy and versatility are typical constraints in many data stream environments. Our study allows us to thoroughly compare the existing solutions for the problem we address and shows that thanks to the partitioning scheme that we propose, it is possible to comply with those constraints.
In our study, we have applied the partitioning scheme to the spectral bloom filters (SBF) and to the dynamic count filters (DCF), which we call PSBF and PDCF, respectively. Table 4 summarizes the qualities of all the techniques evaluated in this paper.
 time, more than one order of magnitude compared to SBF and DCF, (ii) diminishing the amount of memory proportionally to the inverse of the number of partitions.
 Based on our results, we showed that PDCF is the best strategy after applying our partitioning strategy.
PDCF adapts to skew and dynamic data changes in a simple way, allowing only small access time and memory variations over time, in a significantly better way than PSBF, SBF, and DCF.

References
