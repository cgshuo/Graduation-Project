 Question Classification(QC) is the basic and important module of question an-swering which task is to assign one or several classes to a given question. The errors of question classification will prob ably result in the failure of question answering. The experiments[8] show that about 36.4% of the errors of the open domain question answering result from the question classification module. fication systems[1]. But it is time-consuming and labor-intensive. Recently, re-searches on question classification have been focused on the machine-learning approaches such as SVM[2,3], SnoW[4], Statistical LM[6], MEM[7], and etc[5]. from the approach and semantic views, and a SVM classification algorithm based on multiple features and hybrid feature weighting. The experiments show that: high performance question classification; (2) If we cannot get high accuracy de-pendency parser, Bi-gram features can replace the dependency relation without performance declining, which is inconsistent with our anticipation. (3) Our pro-posed feature weighting is effective for question classification. Numerous question taxonomies[1,4] have been defined according to question an-swering techniques, relevant resources and NLP tools available. Within in most of these question taxonomies, question types are defined from the semantic view-point of the answer for reducing the number of answer candidates. We think that the following principles should be abided in building question taxonomy. answering. This is the approach classifier.
 as questioning the author of book, someone X  X  birthday and so on; while retrieval technique is more suitable for the questions querying about the description of something or somebody; but for other questions, the approaches based on natural language processing may be more competent than others. egory of the answer(such as PERSON, PLACE, TIME, etc), and thus restricting the space of answer candidates. This is the semantic classifier.
 from all of noun phrases than from some specific named entities. on current natural language processing resources and tools.
 above principles and the observation of our question collection[10] (2800 ques-tions of which are from HIT-IRLab).
 teristic of our Chinese question taxonomy is that we have two parallel question categories which can make up their disadvantages, whereas other question tax-onomies are hierarchical taxonomies which include coarse and fine categories. For example, given question  X  X  X  X  X  X  X  X  X  X  X  /In what year did Mozart born?, the approach class BIRTHDAY suggests that pattern matching approach is suit-able for this question and semantic class shows that the semantic type of answer is YEAR. So we can use effective pattern matching approach while avoiding the mechanical string matching because of the semantic restriction of answer. In this paper, three kinds of features, i.e. basic features (word and POS), struc-tural features (Bi-gram or dependency relation) and lexical semantic features (thesaurus and named entity types) , are used. Intuitively, it is more reasonable if we replace the Bi-Gram features with the dependency relations.
 ing to their contribution to the classifier. As we know, TF  X  IDF is the classical weighting method. However, in QC task, TF is equal to 1 in most cases. There-fore, we need to find a more suitable measure to modify TF  X  IDF.
 One is the local attribute of features like MI, CHI, DWF, the other is the global attributeoffeatureslikeIG,DF.Inthispaper,wepresentahybridmodelof feature weighting which combines  X  2 max with TF  X  IDF. So the weight of a term where  X  is the balance factor between  X  2 max and TF  X  IDF score, d is the distance between the interrogative and the other words,  X  denotes the distance penalty. In this paper, we will conduct experiments to answer the following questions. (1) Will the hybrid feature weighting be effective for QC Task? (2) Which feature has the biggest contribution to classifiers? (3) Will the precision of classifiers based on dependency relation be higher than that of classifiers based on Bi-gram? used for training, and conduct evaluations in terms of precision which is the proportion of the correctly classified questions among all test questions. of features. The larger  X  , the smaller contribution of local attribute. The smaller  X  , the larger contribution of local attribute. The task of this experiment is to find the best value of  X   X  which maximizes the precision of classifiers. Gram only) and lexical semantic features. The results are shown in Fig.1. TF  X  IDF by  X  2 max is appropriate in QC task. In the early stage, the precisions of open and close test are improved with the increasing of  X  . But the precisions decline in the ending. The precisions reache the top when  X   X  is about 0.2. is to combine the different attributes of features.
 through incorporating various kinds of features. In this experiment, we expect to compare the contributions of these feat ures to the question classifiers which is shown in the right figure in Fig.1 semantic features and structural features into the classifiers. The performance of the semantic classifier increases about 0.6% and 7% respectively after named entities and lexical semantic features are integrated. Meanwhile, Bi-gram feature is helpful not only to the approach classifier but also to the semantic classifier. However, the contribution of POS is very slight and even declines the precisions. The reason we speculate lies in that the helpful POS has been included in named entities, the rest POS such as nouns, verbs are not helpful to classification. than that of classifier based on Bi-gram? better for classifiers than Bi-gram, because the dependency relation is not re-stricted by long distance. And this experiment is conducted to validate the above idea. Table 2 is the precision comparisons in detail.
 FSET6  X  and FEST6  X  are improved by 3.8% and 2.7% respectively compared to classifier that based on FEST6. While, t he precision of approach classifier based on FSET6  X  and FEST6  X  are improved by 3.8% and 1.8% respectively compared to classifier based on FEST6. This shows that whether dependency relation or Bi-gram features has significant contribution to semantic and approach classifiers. However, the contribution of dependency relation extracted from our current parser is no better than that of Bi-gram. This phenomenon is inconsistent with our speculation, which can be explained as follows. 1. The precision of the de-pendency parser for questions is about 90%, which means that it may output wrong dependency relation to the classifier. Therefore, these errors may result in the wrong classification for the question. 2. Though Bi-gram features are less reasonable than features extracted by dependency parser, such  X  X nreasonable X  features may not consequentially result in misclassification.
 tracted by only high accuracy parser can achieve our anticipation; Otherwise, Bi-Gram can replace the dependency relation without performance declining. This paper puts forward a new Chinese question taxonomy from approach and semantic viewpoints. A support vector machine classification algorithm is also presented for question classification based on multiple features including basic features (word &amp; POS), structural features (Bi-Gram &amp; dependency relation) and lexical semantic features (named entity &amp; thesaurus). are the guarantee of high performance of question classification. Regrettably, the contribution of dependency relation extracted from our current parser is no better than that of Bi-Gram which is inconsistent with our anticipation. error classification, wherever using machine learning algorithm or handcrafted rules. Three kinds of misclassifications in our current system are as follows: 1. Misclassifications resulting from inherently difficult questions. 2. Misclassifi-cations resulting from mistakenly recognized Focus Words 3. Misclassifications resulting from the errors of word segmentation and Named entity recognition. the question classification in our Chinese Question Answering System. This work was supported by the Natural Sciences Foundation of China(60372016) and the Natural Science Foundation of Beijing(4052027).
