 Uri Shalit uri.shalit@mail.huji.ac.il Daphna Weinshall daphna@cs.huji.ac.il Gal Chechik gal.chechik@biu.ac.il In the past few years, significant research has been invested in learning to organize and classify music, with the goal of allowing users to retrieve and dis-cover new music ( Turnbull et al. , 2009 ; Su et al. , 2010 ; McFee et al. , 2012 ). This includes classifying songs into musical genres ( Scaringella et al. , 2006 ), creating playlists and recommending music ( Logan , 2002 ; 2004 ; McFee &amp; Lanckriet , 2011 ) and even searching music by humming or notes ( Lu et al. , 2001 ). The growing availability of music tracks and of methods to capture their unique acoustic signatures now opens new possi-bilities to study the structure of the music collective itself. Specifically, this paper provides a quantitative modeling approach to study musical influence. Mu-sical influence is often discussed, but has never been studied quantitatively and at a large scale before. Although central to understanding musical creation, the concept of musical influence is loosely defined, and its role debated among scholars of art history and cul-tural critics. For instance Bloom claimed that im-portant artistic work results when an artist creates original work against existing influence ( X  X he Anxi-ety of Influence X , Bloom , 1997 ), while Lethem claimed that  X  X riginality and appropriations are as one X  in all artistic endeavor, ( X  X he Ecstasy of Influence X , Lethem , 2007 ). In a cultural-artistic landscape that is very much shaped by sampling, remixing, and copy-pasting, the question of the role of influence in art is always present ( Reynolds , 2011 ). Unfortunately these ques-tions were never studied in a data-driven way. The challenge in modeling a whole musical corpus is two-fold: The audio signal itself is a complex continu-ous signal, with meaningful structure on multiple time-scales; and there exist intricate and evolving relations between artists, songs, and genres.
 By learning probabilistic models of the influence that a song has on later songs, this paper offers a quantitative measure of influence, that can be used to engage the ongoing discussion about influence with scientific and data-driven arguments. We use the learned models to detect influential songs automatically, and study the relation between influence and innovation.
 Our model of music influence is based on Dynamic Topic Model (DTM) ( Blei &amp; Lafferty , 2006 ) and Doc-ument Influence Model (DIM) ( Gerrish &amp; Blei , 2010 ). These models were originally developed in the context of analyzing text documents and used to analyze how the language of scientific papers evolves. Under the DIM, an influential scientific paper is one whose lan-guage is adopted by its successors in its scientific field. In our case, of music influence, the audio content of songs replaces the text of a scientific paper, and we con-sider a song to be influential if its  X  X usical-language X  (or sound-content) has been adopted by later songs in related genres.
 We find that the DIM successfully captures known his-torical dynamics of popular music, as validated using manually curated data. For example, it clearly shows the lineage leading from Reggae, Disco and Funk to modern electronic musical genres on one hand, and Hip hop and Rap on the other. The model also agrees with other measures of musical influence inferred from a large human curated musical website, allmusic.com . Finally, it reveals interesting connections between in-fluence and innovation. We first discuss the question of how to model musical influence. Then we present the dataset used to conduct this research. 2.1. How To Model Musical Influence Influence relations in the corpus of popular music have complex structure, at several aspects. First, musical influence can be modeled at a hierarchy of levels, rang-ing from a sound segment  X  like an electronic distor-tion, to individual songs, to albums, to artists and musical bands. Second, the relation between these lev-els is  X  X oft X : many songs are created in collaboration by several artists, many artists take part in several bands, and many songs were published in several ver-sions, sometimes spanning a few decades. Finally, a well known thorny issue is that there exist no consis-tent metadata system which contains the above infor-mation for all music, and mapping music across meta-data systems is hard.
 With these considerations in mind, we chose to model influence on the basis of individual songs, since a song is typically a clearly delineated unit in terms of its acoustic data and metadata.
 A second critical design choice is about the scope of in-fluence. An artist may be influenced by another artist, or by an individual song. A single song may influence many artists, or even originate a musical style. Here we model influence as a process where one song affects the  X  X usical language X  of a musical stream, or  X  X opic X . Such an approach was previously taken for modeling how one text document may influence an entire topic ( Gerrish &amp; Blei , 2010 ). This song-to-topic approach is expected to generalize better than direct song-to-song modeling, since it allows to control the model complex-ity by the number of topics.
 This idea of song-to-topic influence hinges on the basic idea of topic modeling: each song has a distribution across a set of genres, and influences an entire topic (i.e. genre), in proportion to its membership in that topic. The goal of the model is to assign this song-level topic-influence score, and is described in detail in Section 3 .
 In our model we use only the acoustic data of a song, along with its year of release. We do not use any meta-data such as genre, leaving this kind of information for validating out model.
 Several previous works took different approaches to the question of measuring and modeling musical influence. Collins ( 2010 ) model influence as audio similarity, fo-cusing on the influence of a single Depeche Mode song on several hundred British synth-pop songs. Later, the same authors ( Collins , 2012 ) took a more elaborate ap-proach to measure song influence, by building a proba-bilistic model of the audio based on earlier songs, and evaluating the same model on later songs. Early songs that gave high likelihood for later songs were consid-ered influential. This work focused on 248 from the early days of electronic dance music. Lastly, a differ-ent methodology was used by Bryan &amp; Wang ( 2011 ). Using a large dataset indicating which songs sampled from which songs, they modeled influence as derived from the graph structure of which songs, artists, and genres sampled from whom. This method of course does not directly account for influence other than ex-plicitly using an audio sample from an earlier song, a practice which is much more common in genres such as Hip hop and electronic music. 2.2. The Dataset Conducting our study became possible with the publication of the Million Songs Dataset ( Bertin-Mahieux et al. , 2011 ) (MSD) in 2011. MSD is the first truly large scale, diverse and epoch-spanning dataset of songs ever made publicly available. MSD includes detailed audio features for  X  1,000,000 songs along with rich (albeit sometimes inconsistent and missing) metadata including genre tags, artist location and artist familiarity. The audio features are described in Section 4 .
 Out of the 1 million songs, about half are not tagged with a year of release, hence not suitable for our pur-pose. After removing these untagged songs along with duplicates, very short songs, and items which are not musical in nature, such as comedy sketches, we are left with 455,123 songs from the years 1922-2010, with most songs from later years. In general only a few songs are available from any given album. We sampled 24941 songs, by 9222 artists. We biased our sample to include a relatively larger portion of earlier songs since our model revolves around modeling historical trends, and since the dataset itself is heavily skewed towards later year. We also biased our sample to include songs by more familiar artists, in order to aid non-musical-experts in appreciating some of the results. The  X  25K songs were divided into 28 time epochs, with all songs of the same epoch treated as concurrent. We used 2-years epochs for the years 1963-2010, and longer epochs for earlier years, to compensate for the sparse data available before 1963. See Figure 1 for the distri-bution of songs over the years. Contemporary music has a strong  X  X opic-like X  struc-ture in the form of musical genres (like Hip hop, Metal, or Electronic), but at the same time, it exhibits nearly endless mixtures and interactions between gen-res. There is a clear sense of temporal evolution within and between these genres, which is fundamental to the modeling of influence ( Holt , 2007 ; Fabbri , 1982 ). To capture these structures and analyze the flow of mu-sical influence across the music corpus, we use the tools of topic modeling. Specifically, we use the Dynamic Topic Model (DTM) ( Blei &amp; Lafferty , 2006 ) and the Document Influence Model (DIM) ( Gerrish &amp; Blei , 2010 ). Topic models provide a nuanced view of the structure of the musical corpus, enabling soft topic membership, and dynamic topic models have been shown to discover meaningful temporal structure in the evolution of heterogeneous texts ( Blei &amp; Lafferty , 2006 ; Hall et al. , 2008 ).
 Adopting these concepts to the evolution of music, we view influential songs as those songs which changed the  X  X usical-language X  of songs in their musical genres. The model we use consists of three interacting layers, with inference performed jointly. 1. A classical topic model applied to each time epoch 2. A time-dependent model: Each topic evolves with 3. A topic-dependent influence factor. Each song is Formally, each song d  X  { 1 . . . D } is comprised of a set of N d musical words , w d 1 , . . . , w d N ulary of size W . These words reflect both local and global audio structure, and are discussed in the next section. Each song belongs to one of T time epochs, and we assume the existence of K topics.
 The topic model assigns a single topic k from 1 . . . K to the word w d n , and we indicate the assignment by an indicator variable z d n,k . The normalized sum In addition, we assign to each song a scalar nor-mally distributed topic-influence score l d k controlling how much the topic k should later drift in direction of song d .
 The following relations define the probabilistic model that we use: For each epoch t and topic k the probability distri-bution of the words is governed by a W -dimensional parameter vector  X  k,t .
 The probability distribution is: The temporal evolution of the topic-word distribution vectors  X  k t is given by:  X  2 controls the rate of the topics X  evolution, and : z song X  X  topic-influence score ,  X  ( d ) is the time of song d and  X  ( t,  X  ( d )) is a kernel function controlling the time-decay of the influence scores. Each epoch evolves from a starting point that is the sum of two compo-nents: the topic X  X  distribution in the previous time-epoch, and the sum of the songs in the previous epochs, scaled by their influence score and a time-delay kernel. Computing the posterior distribution of the topics and influence scores for this model is intractable. In their paper, Gerrish &amp; Blei present a variational approx-imation and derive an algorithm for maximizing a lower bound on the marginal probability of the ob-served data. See Gerrish &amp; Blei ( 2010 ) and the code at The variables of interest to us are the topic-influence scores l d k , and the aggregate topic-word assignments P a song and how much it influences each of the topics. The influence of each song is defined as l d  X  max (using the mean across topics gives similar results). We set the time-kernel  X  to a log-normal distribution. Topic models were originally conceived for textual data, where each document is represented as a bag-of-words ( Blei et al. , 2003 ). Music however, is natu-rally represented as a single continuous variable, with structure on multiple time scales from less than a mil-lisecond to the entire song length. To convert the con-tinuous acoustic signals into a dictionary of discrete musical-signature, we applied a widely-used two-stage procedure: First, we extract short time-scale features on the scale of 0.25-1 seconds; then we quantize them using K-means. The clusters formed by K-means are treated as musical-words , and the histogram of their occurrence in a song gives us the required bag-of-words representation.
 We compounded the short-scale audio features with long time-scale features such as tempo and key, and quantized these as well.
 All of the raw audio features we used are available as part of the Million Songs Dataset, provided by The Echonest, and described in detail in the Echonest API documentation ( Jehan , 2010 ).
 More specifically, each song is partitioned into non-overlapping segments, typically under a second long, such that each segment is relatively uniform in tim-bre and harmony. For each segment we use 25 double precision features:  X  max. loudness : A single number representing  X  chroma : A 12-component vector corresponding  X  timbre : A 12-component vector describing the These three types of features cover three primary and complementary musical facets ( Serr`a et al. , 2012 ; Ball , 2010 ). Concatenating the chroma and timbre features together gives a richer description of each segment. It has been recently shown ( Serr`a et al. , 2012 ) that while the overall distribution of chroma features is relatively stable over the years, the use of timbre features has been more variable. After concatenating and z-scoring the features, we applied K-means using K = 5000 to a set of 10 million descriptors, sampled randomly from songs in the MSD. Then, given a song, we make a hard-assignment of each segment of the song to one of the 5000 words, giving us a musical bag-of-words representation for each song with a vocabulary of 5000 musical-words.
 In addition, we used 4 global audio variables:  X  tempo : The overall estimated tempo of a track  X  time signature : The estimated meter of the  X  key : Identifies the tonic triad, the chord, major  X  mode : Indicates whether the song is major or Overall, adding up the local and global features, we describe each song by a bag-of-words with a vocabu-lary size of 5033 . In the supplementary section we describe an experiment evaluating the contribution of each of the features. We applied 1, 5, 10 and 20-topic models to the 24941 songs data. 5.1. Comparison To Known Genres We first looked at the matching of the topics with known musical genres. Our data includes 4803 genre tags, with a median number of 36 tags per artist. The genre-tags are weighted to indicate the strength of the genre-artist association. For each topic learned by our model, we summed up the artist-genre scores weighted by the topic proportions of each song. We found that the topics broadly match widely accepted genres such as Metal, Electronic, Hip hop, Indie Rock and Acous-tic, especially for the later years where the dataset is larger and more varied.
 We then investigated the temporal evolution of the topics, using the same genre scores. Table 1 shows the genre tags associated with 6 of the 20 topics of a 20-topic model, in select years from 1957 to 2009. We found several well known genre temporal dynam-ics. for example, a topic containing Jazz, Blues and Hard-bop songs in the late 50 X  X  evolved into a topic with Jazz, Funk, Disco and Soul in the 70 X  X , Hip hop, Electro and Techno in the 80 X  X , Hip hop, Electronica and Trip-hop in the 90 X  X , and Techno and House in the 2000 X  X . Note that these genre tags were in no way used in training or selecting the model. 5.2. Influential Songs Table 2 shows some of the top ranking influential songs per topic and epoch. We chose examples that show di-verse epochs and topics and highlight both well-known and less-known artists.
 Table 3 presents several mistakes made by the model. We also consistently found that songs with wrong early time-stamps were given high influence scores by the model. For example, three 1992 songs by the Jamaican artist Barrington Levy were mislabeled as being from 1922. Every model we ran (until this mistake was dis-covered) found these songs to be extremely influential. This is because the model viewed them as predicting the acoustic language of their original, correct time, and scored them as influential. 5.3. Evaluating Against A Human Curated The model we learned is unsupervised, and predicts the overall influence each song has on songs that fol-lowed it.
 To assess the validity and quantify the performance of our model, we compared our results with the database of the music site allmusic.com . This site includes en-cyclopedic data about  X  100,000 artists, with a graph-like structure indicating artist-to-artist influenceas de -termined by the human editors of the site. For ex-ample, allmusic.com asserts that Beyonc  X e was influ-enced by Madonna, and that Dizzy Gillespie was influ-enced by Louis Armstrong. The music dataset we used consists of 24941 songs by 9222 artists; building the artist-to-artist influence graph for these artists using allmusic.com  X  X  data, we found 5601 of these artists to have at least one edge in the graph.
 To bring allmusic.com  X  X  artist-to-artist relation and our song influence measures in line, we performed two procedures. First, we summed the number of artists each artist has influenced according to allmusic.com . Thus, according to this dataset we have the Beatles as the most influential artists with 556 influenced artists. Second, we had to transform our model from the song level to the artist level. We used a similar approach to that of ( Bryan &amp; Wang , 2011 ), averaging the influence scores of each artist, and yielding an artist influence We found that according to the allmusic.com influ-ence measure, earlier artists tend to be much more influential than later artists (having more time to ac-quire a larger following), making overall comparisons of influence mostly time related. We thus evaluated our influence measure separately for each time-epoch, and averaged the results.
 The mean Spearman rank correlation across epochs between the scores obtained from a 10-topic DIM and the allmusic.com data is 0 . 15 ( p &lt; 0 . 05). Figure 2 plots the per-epoch Spearman correlations and their respective negative log p-values for the 10-and 1-topic DIM, and a baseline method explained below. The 10-topic model performs best for songs from the mid 70 X  X  to early 90 X  X , as well as the 2000 X  X .
 Figure 3 shows the mean Spearman correlations with allmusic.com  X  X  data of 1, 5, 10 and 20-topic models, along with that of the best baseline model. We see that the 10-topic model performs best, and surprisingly bet-ter than the 20-topic model. This might stem from the fact that the greater granularity of the 20-topic model prevents the model from identifying the glob-ally most-influential artists, as they are represented in allmusic.com  X  X  influence measure.
 Future-Past Baseline As a baseline for comparing the DIM performance, Gerrish &amp; Blei ( 2010 ) proposed an easy to calculate heuristic influence measure. In this baseline, each word is assigned a weight for each time epoch by: w t = the time windows into the future and past, respectively. The influence of each song is taken as the mean over its word scores w t as defined in above.We obtained a mean Spearman r of 0 . 07 with allmusic.com  X  X  dataset ( p &gt; 0 . 05), maximized over all possible values of f and b . The correlations were only significant for the earliest and latest epochs, as shown in Figure 2 . We use our model to explore the relation between being musically innovative and musically influential. This issue has only scarcely been looked at before ( Noyes et al. , 2010 ), and has never been approached using the audio content of the songs themselves. Having established a valid computational model of mu-sical influence, we are left to the task of modeling mu-sical innovation. The DIM itself gives us a way to mea-sure innovation. Since the model is probabilistic, each song is assigned a posterior likelihood. We propose to use this likelihood score as a measure of innovation, since more innovative songs will be harder to account for by the model, and thus assigned a lower likelihood. Innovation is of course always relative to the past, and so to measure the innovation of a song from 1960, we use a model fitted using only songs up to 1960. We will call this measure time-restricted likelihood . To validate that indeed time-restricted likelihood corre-lates with innovativeness, we conducted a qualitative survey on the single least likely songs from each time-epoch, as well as a comparable random selection of songs from the dataset. We found that 17 of 27 least likely songs are from artists or albums described as in-novative or  X  X xperimental X  during the relevant period: examples include songs from by Grandmaster Flash, considered by allmusic.com to be  X  X ip-hop X  X  great-est innovator X , and by the band Deerhoof, described as  X  X n experimental spirit... challenging and utterly dis-tinctive music X ; unsuccessful examples include a song from Country singer Don Williams,  X  X ever known as an innovator X . For a random song selection, we have found 8 out 27 can be considered innovative, 6 of them from the earlier periods of the dataset up to 1970. We also found time-restricted likelihood to correlate well with other measures of innovation such as the use of rarer musical-words relative to the epoch. We will thus refer to time-restricted likelihood as innovation score. Before comparing influence to innovation, we have two dataset trends we need to consider. First, overall in-fluence scores decline over the years. This results from the model being able to assign more credit for earlier songs as opposed to later songs, which have not yet had the chance to manifest their effect. Second, over-all innovation scores increase with time. This is likely a result of the dataset including more songs and more diverse songs in later years. We account for these two trends by standardizing both the influence and innova-tion scores per each epoch, using order statistics. The most influential song in epoch t is given a normalized influence score of 1; the least influential a score of 1 with D t the number of songs in epoch t ; the same goes for the innovation score.
 After standardizing the influence and innovation scores, we find that overall across the dataset there is no monotonic correlation between the two (Spear-man r =  X  0 . 019 , p &gt; 0 . 05). However, there are more subtle relations between the two. A closer look shows that the correlation fluctuates over the years. Figure 4 shows the median of the standardized innovation score for the top 10% most influential songs in each epoch. That is, we look at how innovative were the most influ-ential songs, where innovation and influence are both measured relative to the period. We see two periods in which influential songs tended to be more innovative: the early 70 X  X , and the mid-90 X  X , and perhaps a third such period in the last few years. The rise at the mid-90 X  X  stems mainly from electronic and hip-hop artists who were given both high innovative and high influ-ence scores; examples include Cypress Hill, Outkast, Tricky and Mad Professor. All are indeed considered both original and influential by critics. We presented the first large-scale machine-learned quantitative model of musical influence based on the low-level sound content of popular songs from many decades. The learned influence scores are in agreement with manually curated data of artist-artist influence, providing a quantitative way, based on a principled probabilistic model, to study properties of the evolu-tion of popular music.
 Ball, P. The Music Instinct: How music works and why we can X  X  do without it . Random House, 2010. Bertin-Mahieux, T., Ellis, D.P.W., Whitman, B., and
Lamere, P. The million song dataset. In Proceed-ings of the 12th International Conference on Music Information Retrieval (ISMIR) , 2011.
 Blei, D.M. and Lafferty, J.D. Dynamic topic models.
In Proceedings of the 23rd international conference on Machine learning (ICML) , pp. 113 X 120. ACM, 2006.
 Blei, D.M., Ng, A.Y., and Jordan, M.I. Latent dirich-let allocation. the Journal of machine Learning re-search , 3:993 X 1022, 2003.
 Bloom, H. The anxiety of influence: A theory of poetry . Oxford University Press, USA, 1997.
 Bryan, N. J. and Wang, G. Musical influence network analysis and rank of sampled-based music. In Pro-ceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR) , 2011. Collins, N. Computational analysis of musical influ-ence: A musicological case study using mir tools. In
Proceedings of the International Symposium on Mu-sic Information Retrieval (ISMIR) , 2010.
 Collins, N. Influence in early electronic dance music: an audio content analysis investigation. In Proceed-ings of the International Symposium on Music In-formation Retrieval (ISMIR) , 2012.
 Fabbri, F. A theory of musical genres: Two applica-tions. Popular Music Perspectives , 1:52 X 81, 1982. Gerrish, S. and Blei, D.M. A language-based approach to measuring scholarly impact. In Proceedings of the 26th International Conference on Machine Learning (ICML) , 2010.
 Hall, D., Jurafsky, D., and Manning, C. D. Studying the history of ideas using topic models. In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP) , pp. 363 X 371, 2008.
 Holt, F. Genre in popular music . University of Chicago Press, 2007.
 Jehan, T. The Echo Nest Analyze documentation. Technical report, The Echo Nest, 2010.
 Lethem, J. The Ecstasy of Influence. Harper X  X  Maga-zine , pp. 59 X 71, 2007.
 Logan, B. Content-based playlist generation: Ex-ploratory experiments. In Proceedings of the In-ternational Symposium on Music Information Re-trieval (ISMIR) , 2002.
 Logan, B. Music recommendation from song sets.
In Proceedings of the 5th International Conference on Music Information Retrieval (ISMIR 2004) , pp. 425 X 428, 2004.
 Lu, L., You, H., Zhang, H.J., et al. A new approach to query by humming in music retrieval. In Pro-ceedings of the IEEE International Conference on Multimedia and Expo , 2001.
 McFee, B. and Lanckriet, G. R. G. The natural lan-guage of playlists. In Proceedings of the 12th Inter-national Conference on Music Information Retrieval (ISMIR) , October 2011.
 McFee, B., Barrington, L., and Lanckriet, G.R.G.
Learning content similarity for music recommenda-tion. IEEE Transactions on Audio, Speech, and Lan-guage Processing , 20(8):2207 X 2218, October 2012. Mermelstein, P. Distance measures for speech recogni-tion, psychological and instrumental. Pattern recog-nition and artificial intelligence , 116:91 X 103, 1976. Noyes, E., Allen, I.E., and Parise, S. Artistic influences and innovation in the popular music industry. Fron-tiers of Entrepreneurship Research , 30(15):3, 2010. Reynolds, Simon. Retromania: Pop Culture X  X  Addic-tion to Its Own Past . Faber &amp; Faber, 2011. Scaringella, N., Zoia, G., and Mlynek, D. Auto-matic genre classification of music content: a survey.
Signal Processing Magazine, IEEE , 23(2):133 X 141, 2006.
 Serr`a, J., Corral,  X  A., Bogu  X n  X a, M., Haro, M., and Ar-cos, J.L. Measuring the evolution of contemporary western popular music. Scientific Reports , 2, 2012. Su, J.H., Yeh, H.H., Yu, P.S., and Tseng, V.S. Music recommendation using content and context informa-tion mining. Intelligent Systems, IEEE , 25(1):16 X 26, 2010.
 Turnbull, D.R., Barrington, L., Lanckriet, G., and Yaz-dani, M. Combining audio content and social con-text for semantic music discovery. In Proceedings of the 32nd international ACM SIGIR conference on
Research and development in information retrieval ,
