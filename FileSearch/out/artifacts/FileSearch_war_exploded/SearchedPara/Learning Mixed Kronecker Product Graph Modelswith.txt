 There has recently been a great deal of work focused on de-veloping statistical models of graph structure X  X ith the goal of modeling probability distributions over graphs from which new, similar graphs can be generated by sampling from the estimated distributions. Although current graph models can capture several important characteristics of social network graphs (e.g., degree, path lengths), many of them do not gen-erate graphs with sufficient variation to reflect the natural variability in real world graph domains. One exception is the mixed Kronecker Product Graph Model (mKPGM), a gener-alization of the Kronecker Product Graph Model, which uses parameter tying to capture variance in the underlying dis-tribution [10]. The enhanced representation of mKPGMs enables them to match both the mean graph statistics and their spread as observed in real network populations, but un-fortunately to date, the only method to estimate mKPGMs involves an exhaustive search over the parameters.
In this work, we present the first learning algorithm for mKPGMs. The O ( | E | ) algorithm searches over the contin-uous parameter space using constrained line search and is based on simulated method of moments , where the objective function minimizes the distance between the observed mo-ments in the training graph and the empirically estimated moments of the model. We evaluate the mKPGM learning algorithm by comparing it to several different graph models, including KPGMs. We use multi-dimensional KS distance to compare the generated graphs to the observed graphs and the results show mKPGMs are able to produce a closer match to real-world graphs (10-90% reduction in KS dis-tance), while still providing natural variation in the gener-ated graphs.
 I.2.6 [ Artificial Intelligence ]: Learning X  Parameter learn-ing Link analysis, statistical graph models, Kronecker models, method of moments estimation.

Graphs are a natural representation to use for the analysis of complex systems. As such, there has been a great deal of research focusing on the development of models that accu-rately reflect the characteristics of real-world networks. This work includes stochastic algorithms (e.g., [16, 2, 6]) that are capable of generating graphs with particular properties of interest (e.g., short geodesic distance, high local clustering). It also includes statistical models that are capable of mod-eling probability distributions over graphs. For example, Exponential Random Graph Models (ERGMs) [15], Chung-Lu models (CL) [3], and Kronecker Product Graph Models (KPGM) [8, 9]. Statistical models such as these offer the advantage that model parameters can be estimated from observed networks and, once learned, new graphs can be generated from the estimated distribution by sampling.
Of the statistical models of graphs, the KPGM method [9] is intuitively appealing for its elegant fractal structure and fast sampling algorithms (i.e., O ( | E | )). However, recent work has identified a few limitations of KPGMs. Seshadhri et al. [13] showed via mathematical analysis that graphs gen-erated from the KPGM have 50-75% isolated vertices with no incident edges and Moreno et al. [11] showed empirically that graphs generated from the KPGM do not capture the level of clustering observed in many network datasets.
Moreover, while recent graph models adequately capture several important characteristics of social network graphs (e.g., skewed degree, short path lengths), many of them do not generate graphs with sufficient variation to reflect the natural variability of real world graph domains [11]. Specif-ically, when learned from real-world social network samples, the models appear to place most of the probability mass on a relatively small subset of graphs with very similar charac-teristics. This clearly limits the applicability of the models for representing and reasoning about graph populations.
Recently, we proposed a mixed -KPGM (mKPGM) [10] to address these issues. The mKPGM is a generalization of the KPGM that uses parameter tying to model dependencies among edges. These dependencies enable the model to more accurately capture the clustering observed in real-world net-works. The dependencies also increase the variance of the es-timated distribution while preserving the expectation X  X hus mKPGMs are able to more accurately capture the natural variation observed in real-world network populations . How-ever, the parameter tying makes it more difficult to learn the model from observed data (due to edge dependencies) and to date the only method to determine mKPGM parameters from data involves exhaustive search.
In this work, we present the first tractable learning al-gorithm for mKPGMs. The algorithm is based on simu-lated method of moments where the distance between the observed moments in the training data and the empirically estimated moments of the model is minimized using con-strained line search for continuous optimization. We eval-uate the proposed mKPGM learning algorithm by compar-ing to several alternative graph models, including KPGMs, on six real-world network domains. The results show that mKPGMs are able to capture the characteristics of the real-world graphs more accurately, while still providing natural variation in the generated graphs.

The contributions of the paper include:
This section describes the Kronecker Product Graph Model (KPGM) and two algorithms to learn its parameters based on (i) maximum likelihood estimation (MLE) [9] and (ii) method of moments (MoM) [4]. We also describe the mixed-KPGM (mKPGM) [10] and include a brief discussion about the choice of initiator matrix size.
Let  X  be a b  X  b initiator matrix of parameters, where  X  i,j  X  ij  X  [0 , 1], and let K be a parameter that determines the size of the graph (i.e., | V | = b K ). Then the KPGM al-gorithm generates a graph G K = ( V K , E K ), where V K and E K are the set of nodes and edges respectively, as follows. First, the model computes the K th Kronecker power of the initiator matrix  X  via K  X  1 Kronecker products of  X  with itself. This produces a b K  X  b K matrix P K =  X  [ K ] , where P
K ( i,j ) represents the probability of an edge existing be-tween nodes i and j . P K is used to generate a graph G K with b K nodes, by sampling each edge independently from a Bernoulli( P K ( i,j )) distribution (i.e., if the trial is successful, the edge e ij is added to E K ).
 Given an observed training network G ? = ( V  X  , E  X  ), the KPGM likelihood of the graph is: Here  X  i denotes the position of node i according to a per-mutation  X  . MLE learning algorithm finds the parameters  X  that maximizes the likelihood of the observed graph given a permutation (  X  ) of the rows and columns of the adjacency matrix [9]. In practice, the true permutation is unknown and the learning algorithm uses a Metropolis-Hasting sam-pling approach to search over the factorial number of possi-ble permutations of the network. The algorithm then uses a gradient descent approach to update the parameters  X , where the derivative of the likelihood is approximated given the current  X  and  X .

A second learning algorithm for KPGMs was developed in [4], which is based on method of moments . The strength of this approach is that it is permutation invariant X  X hus it avoids the difficulty of search over permutation space. The MoM learning algorithm searches for parameters  X  that minimize the following objective function: Here F i is a function over a network G = ( V , E ) that cal-culates a statistic of the graph, e.g., for number of edges: F = | E | . Then, F  X  = { F  X  1 ,F  X  2 ,  X  X  X  ,F  X  N m } corresponds to a set of N m sample moments of the training network G ? and E [ F i |  X ] is the expected value of those statistics (i.e., distri-butional moments ) given particular values of  X . The MoM learning algorithm [4] considers four moments: the num-ber of (i) edges, (ii) 2-stars, (iii) 3-stars, and (iv) triangles. These moments were selected because their expected values can be analytically calculated for any  X , provided b = 2.
The mKPGM is a generalization of the KPGM, which uses parameter tying to capture the clustering and natural vari-ation observed in real-world networks more accurately [10]. The marginal probabilities of edges in P K are preserved but the edge probabilities are no longer independent.

Specifically, given  X , K , and a parameter `  X  [1 ,  X  X  X  ,K ] that specifies the level of parameter tying, the mKPGM generation process samples a network of size b K as follows. First, the model uses the standard KPGM algorithm with initiator matrix  X  to calculate a probability matrix P ` =  X  and then a graph G ` is sampled from P ` . Next a subsequent Kronecker product is computed from G ` to produce a new probability matrix P ` +1 = G `  X   X . Then a graph G ` +1 is sampled from P ` +1 for further Kronecker products. The process of sampling a graph before computing subsequent Kronecker products produces dependencies among the sam-pled edges. This process is repeated K  X  `  X  1 times to generate the final network G K . For more details see [10].
The parameter ` controls the level of tying and thus im-pacts the variance and clustering of the model. Lower val-ues of ` produce larger dependencies among the edges and greater clustering among the nodes. When ` = K the model is equivalent to KPGM and this produces lower clustering and lower variance.

The mKPGM likelihood has two parts: the untied part (i.e., G ` ) is calculated as in the original KPGM, while the tied part is based on the K  X  ` Kronecker products where edges share parameters. The mKPGM likelihood of an ob-served graph G  X  (given a permutation  X  ) is based on the matrix G  X  ` that was generated to produce the permuted net-work G  X  under  X  : P ( G  X  |  X  , X  ) = P ( G  X  ` |  X , X  ` )
Y Here P ( G  X  ` |  X , X  ` ) is the likelihood of the untied part given by Eq. 1, where  X  ` = 1 ,  X  X  X  , | V  X  ` | and G  X  ` ( i,j ) = 1 if there is at least one edge in the submatrix of G  X  defined by ( i  X  1)  X  b K  X  ` + 1 ,  X  X  X  ,i  X  b K  X  `  X  ( j  X  1)  X  b K  X  ` + 1 ,  X  X  X  ,j  X  b under the permutation  X  , and G  X  ` ( i,j ) = 0 otherwise. In ( i,j ) to its corresponding cell in the adjacency matrix G and results in an edge probability of zero when the cell in G where  X  i k j k corresponds to the probability of a successful Bernoulli trial for ( i,j ) at scale k in the hierarchy X  X hich is determined by the entry of P 1 corresponding to the k th bits of ( i,j ) (see [10] for a detailed explanation of the hierarchical bit representation).
To investigate the impact of the size of the initiator ma-trix, we generated networks over a wide range of parameter values in  X  and measured the characteristics of the resulting graphs. Specifically, we considered 9,239 different values of  X  with b = 2. For each  X , we generated 50 networks with K = 11 from KPGM ( ` = K ) and mKPGM ( ` = 6). This produced networks with 3,500-15,200 edges. We also gener-ated the same quantity of networks, with similar number of nodes, edges and equivalent levels of parameter tying for  X  of size b = 3. For b = 3, we considered 18,816 different val-ues of  X  and generated networks with K = 7 from KPGMs ( ` = K ) and mKPGM ( ` = 4). This produced networks with 3,400-16,300 edges. For the generated networks, we calculated two moments: (i) average clustering coefficient, and (ii) average geodesic distance (see Figure 1).
Figure 1 shows that mKPGM and KPGM are able to pro-duce networks with higher clustering coefficient when using a larger initiator matrix ( b = 3). Moreover, the increased size of the initiator matrix does not change considerably the average geodesic distance of the resulting networks. Conse-quently, since a slightly larger initiator matrix increases the coverage of the graph space without significantly increasing the number of parameters that need to be estimated (4 vs. 9), in this paper we use b = 3 for the initiator matrix size. Figure 1: Variation of graph properties for synthetic networks using generator matrix of size b=2 and b=3 for KPGM (left) and mKPGM (right)
The simulation experiments in [10] demonstrate the ad-vantages of the mKPGM representation for capturing im-portant characteristics of real world networks (i.e., clustering and variation). However, previous experiments used exhaus-tive search to identify model parameters X  X o date there is no training algorithm to learn mKPGM parameters automati-cally from an observed network. Here we describe the first tractable learning algorithm for mKPGMs. In Section 5 we will apply the algorithm to learn mKPGM parameters from real-world network populations and show that the learned models are more accurate than competing models.

First we note that even though the mKPGM likelihood (Eq. 3) is similar to that of KPGMs (Eq. 1), it can not easily be used as an objective function to estimate the parameters of the model. In principle, if the level of tying ( ` ) is known apriori, a MLE algorithm could alternate between sampling permutations and estimating the parameters of the untied and tied parts of the model. However, we found that MLE is not likely to be successful in this case, since local search (i.e., swapping a single pair of nodes) in permutation space is ex-tremely unlikely to discover the block structure that results from parameter tying. Since it is necessary to recover the block structure permutation to accurately estimate the pa-rameters with MLE, in practice MLE only works well when the search starts from very close to the true permutation X  which is not the case for real datasets.

Next, we note that the MoM approach to learn KPGMs [4] is difficult to apply to mKPGMs since analytical expressions for even simple moments are difficult to derive for mKPGMs due to the complex dependencies between the edges (the key characteristic of the model). For example, let A ij = 1 when there is an edge between nodes i and j, and 0 otherwise. Then for nodes that have common (tied) parameters in the network generation of G K  X  ` : E [ A ij A kl ] 6 = E [ A This makes it difficult to calculate distributional moments analytically and directly minimize f ( X  , F  X  ) in Eq. 2.
However, a strength of MoM estimation is that it is permu-tation invariant X  X .e., it successfully avoids the search over the factorial permutation space which is even more difficult in the space of mKPGMs. At the same time, a strength of both mKPGMs and KPGMs is their ability to generate sample networks in sub-quadratic time. To exploit these two strengths we developed a simulated method of moments (SMM) learning algorithm that approximates the objective function in Eq. 2 with empirically estimated moments. Sim-ulated method of moments (see e.g., [12]) is often used to estimate models where the moments are complicated func-tions that cannot easily be evaluated analytically (e.g., in econometric models). In SMM methods, simulation experi-ments are used to empirically estimate the moments and/or their derivatives.

In our SMM method, we replace the analytical expression of E [ F |  X ] with an empirical estimation b E [ F |  X ] based on sim-ulated networks from  X . Algorithm 1 outlines the function estObjFunc where we estimate f ( X  , F  X  ) empirically. Specif-ically it estimates b E [ F |  X ] from a set of sampled networks S . Each network G i ( i  X  { 1 ,  X  X  X  ,N s } ), where N is generated with the mKPGM algorithm using the input parameters  X , K , and ` and the specified moments are cal-culated in each G i . Each expected moment is then esti-mated from the median of the moments observed in the set of the objective function is calculated using the estimated distributional moments. The complexity of this function de-pends on both the complexity of sampling N s networks from the model and the complexity of calculating the moments in Algorithm 1 Function estObjFunc 2: Generate network G i using mKPGM with ( X  ,K,` ) 3: Calculate moments F i for G i . each network. While generating a network from mKPGM is O ( | E | ), the time needed for moment estimation depends on the chosen moments. For example, computing average node degree is O ( | E | ) but computing average geodesic distance could be O ( | V || E | log | V | ). In this work, we approximate the calculation of moments to make them at most linear in E (e.g., by sampling shortest paths). Thus, the complexity of estimating the objective function is O ( N s  X | E | ).
The use of SMM avoids the difficulties of determining an analytical expression for each moment and facilitates the incorporation of a wider range of moments in the learning algorithm. However, since the objective function in Algo-rithm 1 is not convex, we need a way to search over the parameter space to minimize f ( X  , F  X  ). Moreover, without a closed form expression for the moments, we also can not estimate their gradients, thus a gradient descent-type opti-mization method is not applicable. To offset these issues, we developed a line search optimization method, using the empirically estimated moments from SMM.

To consider whether linear search would be reasonable to pursue, we explored whether the objective function is lo-cally convex. We varied each parameter  X  ij , while keeping the rest of parameters constant, and evaluated f ( X  , F  X  different  X  ij (see Figure 2, left). In all cases, we observed a locally convex error function with a minimum around the value of  X  ij such that S K  X  = ( P i P j  X  ij ) K  X  X  E  X  ber of edges of the training network). This implies that a one dimensional linear search (i.e., changing a single param-eter) will not be effective X  X nce it reaches a local minima that matches | E  X  | it will not explore any further because changing a single parameter in isolation will always change the expected number of edges. However, a two dimensional (2D) search, where we keep S K  X  constant through a simulta-neous increase in one parameter and equal decrease in an-other, could enable a successful search to improve f ( X  , F To test this, we varied combinations of two parameters in  X  and evaluated f ( X  , F  X  ) for different values of the parame-ters, while keeping the rest of parameters constant (figure 2, right). Again, in all cases, the 2D curve was locally convex.
Thus, under the assumption that the 2D space is likely to be locally convex, our algorithm approximates a full search over parameter space by performing a linear search in 2D, while constraining the parameters to match S K  X  = | E  X  | .
Algorithm 2 outlines our learning algorithm which com-bines SMM with a constrained line search in two dimensions (2D) to identify the best set of parameters. The algorithm ber of nodes of the training network G ? = ( V  X  , E b  X  b is the size of the parameter matrix  X . It continues with the initialization  X  i,j  X  ij = K p | E  X  | /b 2 , which ensures the constraint S K  X  = | E  X  | is met. With the initial set of parameters, the initial error EF  X  is calculated with estOb-F igure 2: Error function with respect to the varia-tion of one (left) and two (right) parameters.
 A lgorithm 2 mKPGM training algorithm 7: for i = 1; i + +; i  X  iter do 13:  X  =  X  20:  X  =  X  23:  X  =  X / 2 24: return  X  j Func . The algorithm continues with the generation of the set of N c = b 2 2 possible pairs of parameters  X  pairs to con-sider in the 2D search. In case of undirected networks the symmetric relationship in  X .

The algorithm then begins the search over the parameter space, which consists of three loops. The first loop iterates iter times over the step sizes  X  , which determines the pa-rameters value changes. The second loop iterates over the set  X  pairs , selecting two parameters in each iteration, which determine the part of parameter space that is searched. The two indexes of the selected parameters are given by the pair index =  X  pairs ( mod ( j 1 ,N c ) + 1), where index (1) and index (2) correspond to the parameters indexed by the first and second element of index respectively. The third loop (over j 2 ), implements the restricted linear search, by iter-ating from  X  3  X  to 3  X  with a step size of  X  . The loop be-gins with a copy of the original set of parameters ( X  =  X ), over the two dimensional parameter space while constrain-ing S K  X  = | E  X  | . If all the parameters are in the range [0 , 1], then the value EF 0 is calculated by estObjFunc using  X . If EF 0 &lt; EF  X  , then  X  is accepted, the error is updated, and the search extended for the next N c iterations.
The complexity of the mKPGM learning algorithm is O ( c  X  estObjFunc ), where c = iter  X  N c depends on the number of iterations needed for learning and the size of the initiator matrix (i.e., number of parameters). Since the objective function can be estimated in O ( N s  X | E | ), the overall com-plexity of learning is O ( c  X  N s  X | E | ).

Our SMM learning algorithm has three important advan-tages over the previous MoM learning method for KPGMs. First, our algorithm is not limited to moments that can be calculated analytically. The only consideration for including additional moments is the additional time needed to calcu-late them empirically in network samples. Second, the SMM approach facilitates learning for different sizes of initiator matrices (i.e., b  X  2). Third, our algorithm is not limited to undirected networks, since the SMM approach can handle the complexity of directed networks.

In this work, we use b = 3 for our initiator matrix and consider five moments in our training algorithm: (i) average number of edges, (ii) average cluster coefficient, (iii) average geodesic distance (approximated by a sample of nodes), (iv) size of the largest connected component, and (v) number of nodes with degree greater than zero (to solve the KPGM problem of isolated nodes [13]).
We compare mKPGMs, learned with our SMM algorithm, to four alternative statistical models of networks and eval-uated their ability to model both synthetic and real-world network populations. To assess whether the generated net-works capture the properties we observe in real network pop-ulations, we use four evaluation measures and visual compar-ison of the properties of networks generated from the learned models. We also apply our learning algorithm to three sin-gle network datasets, where due to the lack of information about population variance, we train the mKPGM algorithm with ` = K (which is equivalent to a KPGM). The results in-dicate that our new SMM approach can learn more accurate parameters than current learning methods for KPGMs.
We considered one synthetic dataset and six real-world network datasets in this paper. All datasets were trans-formed to undirected graphs without self loops. For the syn-thetic data, we generated 50 networks utilizing the mKPGM algorithm with K = 7 and ` = 4 and the  X  orig parameters specified in Figure 2.
 The first real dataset (Facebook) is drawn from the public Purdue Facebook network. Facebook is a popular online so-cial network site with over 845 million members worldwide. We considered a set of over 400,000 Facebook wall links in a year-long period among over 50,000 Facebook users belong-ing to the Purdue University network. From this data, we sampled 50 networks based on the same process describe by Moreno [10]. Each network has 2,187 nodes ( b = 3) ( K =7) the edges were collected over time windows of 60 days.
The second dataset (Email) is drawn from a Purdue email network which was constructed from anonymized email logs on the Purdue mail-servers. The email traffic was recorded over 189 days from August 22, 2011 to February 28, 2012 and is comprised of all email transactions from one Purdue user to another. In order to remove the effects of mailing lists and automated emails, we drop any node that has an incoming or outgoing degree of 0. In our analysis we focus on the 137 daily snapshots of networks that occur during during class periods (i.e., weekdays). The resulting networks have an average of 10,946 nodes and 26,562 edges per day.
The third dataset (AddHealth) consists of a set of social networks from the National Longitudinal Study of Adoles-cent Health [5]. The AddHealth dataset consists of survey information from 144 middle and high schools, collected (ini-tially) in 1994-1995. The survey questions queried for the students X  social networks along with myriad behavioral and academic attributes. In this work, we considered a set of 25 school networks with sizes ranging from 800 to 2,000 nodes.
These three real datasets are illustrative examples of graph populations  X  X ets of networks that exhibit similar graph struc-tures with natural variation. Each set of graphs is likely to be drawn from the same underlying distribution (e.g., email communication patterns are generated by similar so-cial processes). While the networks population are small to medium-sized networks, the empirical performance of mKPGMs on larger-sized single networks indicates that re-sults will generalize to larger graphs as well.
 Three single large networks data were obtained from the Stanford Network Analysis Project 1 and [14]. We learned mKPGM models with ` = K from these single networks and compared against KPGM training methods. The first net-work is the Gnutella peer-to-peer network (Nutella), which is a sequence of snapshots of the Gnutella peer-to-peer file sharing network from August 2002 with 6,301 nodes and 20,777 edges. The second dataset is the Arxiv General Rel-ativity and Quantum Cosmology (GRQC) collaboration net-work, where each of the 5,242 nodes represent authors, and the 28,980 edges indicates a publication between two au-thors. The third dataset based on Facebook friendship links in New Orleans (FBOR) as described by [14], consists of 46,952 nodes and 183,412 edges. We compare mKPGMs to four alternative models: Kronecker Product Graph Model MLE (KPGM MLE).
 The KPGM algorithm using the maximum likelihood train-ing method described in section 2.2.
 Kronecker Product Graph Model MoM (KPGM MoM).
 The KPGM algorithm using the method of moments train-ing method described in section 2.2.
 Chung Lu Model (CL) [3]. The CL algorithm models the expected degree distribution via a set of weights w i portional to degree of node i . To generate a sample graph from the model, each edge is sampled independently with a Bernoulli distribution with P ( i,j ) = w i w j .
 Exponential Random Graph Model (ERGM) [15].
 ERGMs represent probability distributions over graphs with an exponential linear model that uses feature counts of local graph properties (e.g., edges, triangles).
Our evaluation investigates whether the models capture three important graph characteristics of real network datasets: degree, clustering coefficient, and hop plot (see e.g., [11] for a description of the characteristics). To evaluate the ability of the models to capture these characteristics, we compare the cumulative distribution functions (CDFs) of networks gen-http://snap.stanford.edu/data/ erated from the learned models to the CDFs of the original data. We plot each CDF independently and use those for visual comparison, but we would also like to evaluate the relationships among the distributions of characteristics to determine if the models are able to jointly capture the char-acteristics through the network. To measure this quantita-tively, we extend the Kolmogorov-Smirnov distance (KS), which measures distributional distance in a single variable, to multiple distributions. In addition to these results, we consider three more network characteristics that were not used as moments during mKPGM learning to assess how the models perform on non-optimized measures.
 The CDF provides a more complete description of the net-work structure compared to a single aggregate statistic (e.g., average degree). To compare the algorithms, we generated 50 sample graphs from the learned models. From these sam-ples, we estimated the empirical sampling distributions for degree, clustering coefficient, and hop plots. We plot the me-dian and error bars corresponding to the interquartile range for the set of observed network distributions. Solid lines cor-respond to the median of the distributions and error bars to the 25th and 75th percentiles.
 Since network characteristics are correlated, independent evaluation of multiple CDFs can make it difficult to accu-rately evaluate the models. For example, when two or more variables are correlated (e.g., degree and clustering), an al-gorithm may generate networks that match each (marginal) distribution but do not capture the joint distribution accu-rately. At the same time, another algorithm may generate networks that do not accurately match the marginal distri-butions (e.g., one is overestimated and another is underesti-mated), but the joint distribution may be a closer match.
To quantitatively measure the differences between mul-tidimensional discrete CDFs, we outline a new distance measure X  X he KS 3 D which is based on the Kolmogorov-Smirnov ( KS ) distance. The KS distance between two one dimensional CDFs is the maximum absolute difference be-tween the CDFs: KS ( CDF 1 ,CDF 2 ) = max x | CDF 1 ( x )  X  CDF 2 ( x ) | . This distance varies between 0 and 1, where zero indicates a perfect match of the two distributions.
The KS 3 D distance captures the correlation among graph measures in multiple dimensions, calculating the maximum difference between two three-dimensional CDFs. To calcu-late the KS 3 D , we represent every node as a three-dimensional point Po i = &lt; d i ,c i ,g i &gt; where d i is the degree, c clustering coefficient, and g i is the average geodesic distance, for node i respectively. We then calculate the maximum per-centage difference between two distributions of Po . Specif-ically, given two graphs G 1 = ( V 1 , E 1 ) and G 2 = ( V the KS 3 D distance is defined as: where cp i ( Po x ) is the percentage of points from network G i that are less than or equal to the reference point Po (in all dimensions). The KS 3 D distance varies between 0 and 1, with 0 indicating a perfect match between the two distributions. Although we describe the distance in 3D, gen-eralization to higher dimensions is straightforward.
The code for the KS 3 D distance is presented in Algo-rithm 3. Given G 1 = ( V 1 , E 1 ) and G 2 = ( V 2 , E 2 Algorithm 3 KS 3 D distance algorithm 2: maxDist = 0 3: for node i in V do 5: for j = 1; j + +; j &lt; = 2 do 10: cp j + + 14: return maxDist V = V 1  X  V 2 is defined. For every node i in V , we use Po i to calculate cp 1 and cp 2 for G 1 and G 2 , if the abso-lute difference between them is greater than the current maximum distance, we save the new difference and con-tinue with the next point. Once that all points are consid-ered, the maximum distance is return. An approximation of the KS 3 D can be calculated using random selected nodes from each network and using a 3D-grid for the comparison of the 3D-grid) We also evaluate three different characteristics not utilized as moments in the mKPGM learning algorithm, to asses whether the learned mKPGMs can capture other network characteristics that were not explicitly optimized. K-core: The K-core of a network is the largest induced subgraph where each node has minimum degree k in the subgraph. The distribution of k-core sizes (for varying k ) demonstrates the connectivity and community structure of the graph [1]. Considering that two k-core distributions could differ in size, we use skew divergence to measure the divergence of the two distributions PDF 1 and PDF 2 . The skew divergence distance is defined as KL [  X PDF 1 + (1  X   X  ) PDF 2 , X PDF 2 + (1  X   X  ) PDF 1 ] with  X  = 0 . 99. eigVal: The eigenvalues of the adjacency matrix of the net-work G measure the centrality of each node in the network. We compare the largest 25 eigenvalues of each network uti-lizing the standardize absolute distance between them, a dis-tance of zero implies a similarity among the importance of the nodes in the network. netVal: The first eigenvector of an adjacency matrix con-tains important information for data analysis [7]. We calcu-late the euclidean distance between the largest 100 network values of each first eigenvector, where a value of zero im-plies that important component characteristics are modeled by the networks.
 Average: To summarize the results of these three measures, we normalized each measure to the range [0 , 1] and averaged the three for each model on each dataset.
We evaluated our training algorithm on one synthetic dataset and six real datasets. For each dataset, we selected a single network to use as the training set. To control for variation in the samples, we selected the network that was closest to the median of the degree distribution on synthetic (network 27, 13,460 edges) and Facebook data (network 11, 5,634 edges). For Email and AddHealth we selected the network which is the closest to 3 8 = 6 , 561 and 3 7 = 2 , 187 nodes (network 36 with 14,756 edges and network 72 with 15,484 edges respec-tively). The other real datasets consist of a single network. Table 1: Standard deviation of the number of edges, for real data and mKPGM algorithm.

To determine the best value of ` , we compared the stan-dard deviation of the number of edges observed in the real data against the standard deviation generated by the learned parameters. We chose the value of ` to be the value with closest match to the real data. The standard deviation for the number of edges and the selected values of ` (in bold ) are included in Table 1.

Using each selected network as a training set, we learned each of the described models. For mKPGM and KPGM-MLE we used b = 3; for KPGM-MoM we used b = 2 since the algorithm is specific to that size of initial matrix. For mKPGMs, we used  X  = 0 . 15 ,iter = 9 , and N s = 10 (which is sufficient considering that we use the median of the char-acteristics for learning). From each learned model, we gen-erated 50 sample graphs and calculate the minimum of the measures mentioned in Section 4.3, to control for the effect of network variation.
The synthetic data experiments are intended to evaluate whether our proposed mKPGM estimation algorithm is able to learn parameters successfully in networks generated from mKPGM distributions. Table 2 reports the parameters learned by each of the KPGM-type models. KPGM-MLE does not learn the original parameters, which is also reflected in the CDFs and the evaluation metrics. Even though our mKPGM training algorithm does not recover the exact pa-rameters, the learned parameters can emulate the proper-ties of the original synthetic dataset X  X s is confirmed by the results in Figures 3-6. When MoM is used, we note that models are not identifiable if the number of moments is less than the number of model parameters. In our experiments, we used five moments to estimate six parameters, and com-bined this with constrained search X  X ith quite accurate re-sults. In future work, we will explore whether additional moments can be used to improve estimation without incur-ring additional computational costs.

Figures 3-5 (first column) show the CDFs for all meth-ods on the synthetic data. Noticeably, mKPGM is the only method able to capture the observed variance. The large error for ERGMs may be due to degeneracy problems, even  X   X  0.90 0.50 0.10 -0.90 0.10 --0.70 Table 2: Original and learned parameters for Kro-necker algorithms in synthetic data.
 Table 3: K-core, eigVal and netVal distances over synthetic data for all models.
 though we used some prescribed solutions to avoid it. We expected that KPGMs would generate graphs with less vari-ance than mKPGM, and this is confirmed by the results. Finally, the lack of variance in the CL graphs can also be explained by the models assumption of edge independence, as is demonstrated in Appendix B. The conclusions from visual CDF comparisons are further confirmed by the quan-titative KS 3 d results (see Figure 6) and the non-optimized measures (see Table 3). The low error for mKPGMs con-firms the ability of the SMM learning algorithm to identify a good set of parameters.
Similar to results on synthetic data, the only model that can capture the variance observed in the CDFs of real net-works is the mKPGM (see Figures 3-5). The CL model is the closest method to the median of the degree distribution in most datasets (figure 3), however, it fails considerably in the other characteristics. Moreover, the low variance of the method makes it difficult for it to match entire distribu-tions. The ERGM can only model the Email data, but while it matches the median of the CDFs fairly well, it still does not capture the variance. Moreover, it produces large errors in the other two datasets. The KPGM cannot model any of the CDFs well and critically lacks the ability to capture any of the clustering in the data (see Figure 4). Finally the mKPGM can model all datasets fairly well. In particular, the mKPGM is the best model for the Facebook and Ad-dHealth data, matching not only the median of the CDFs but also their variance.

These results are further supported by the KS 3 d distance (Figure 6, left plot), where mKPGM obtains the lowest er-ror in two of the three datasets and the second lowest on the third dataset. Notably, the mKPGM model results in up to a 77% reduction of KS 3 d distance compared to the KPGM models. These results confirm that the SMM training algo-rithm not only learns the average of the selected moments but also can learn parameters which capture the correlations among the characteristics. It is important to observe the high error of other models on some of the real datasets. For example CL and KPGM MoM obtain the highest possible error on Email data.
The results for the non-optimized measures of K-core, eigVal and netVal (see Table 4) confirm that the training method for mKPGM is not overfitting to the moments used in the objective function and the learned parameters pro-duce graphs that also capture other characteristics. Specifi-cally, even though the training used different characteristics, mKPGM produces the best match for the three characteris-tics on the Facebook and AddHealth data, where it obtains the lowest average error. The only exception is the Email data, where the mKPGM model produces networks with a high value in the K-core distance. This is due to the skew-ness of the K-core distribution in the email data (the highest K-core is three). Beside the ERGM, which is the best model for Email data, the other models can only capture one or two characteristics at the same time, obtaining most of the time an average distance over 0.4.
 Given that KPGMs are a special case of mKPGMs ( ` = K ), we apply our training method to three single network datasets and compare our learning algorithm with ` = K , against the current KPGM training algorithms. From the learned models we generated 50 networks to compare against the real data (except by FBOR where we generate a smaller number of samples because of the network size). For these experiment, we only present the KS 3 D distances (Figure 6, right plot), and omit other results due to space constraints.
In spite of the fact that mKPGM generates networks equiv-alent to KPGMs ( ` = K ), the results show that mKPGMs obtain the lowest KS 3 D error among all the models X  X p to 36% of reduction in KS error. This demonstrates that Table 4: K-core, eigVal and netVal distances over network populations for all models. our SMM training algorithm also improves on the previous KPGM training algorithms X  X ven in large networks such as FBOR (46,952 nodes). While other KPGM models can cap-ture some aspects of the networks, CL is the worst model in single datasets (except for FBOR where ERGM cannot be learned due to its size).
In this paper, we presented the first tractable learning al-gorithm for mixed Kronecker Product Graph Models, mak-ing it feasible to learn these models from data for the first time. Our empirical evaluation demonstrates that mKPGMs, learned with our simulated method of moments training al-gorithm (SMM), outperform several competing statistical models of graphs X  X ot only by matching the characteristics of the networks, but also by capturing the variability ob-served in network populations.

Specifically, we compared against CL, ERGM, KPGM (MLE and MoM) and showed that mKPGMs are signifi-cantly more accurate, resulting in 10-90% reduction in KS distance. The improved fit is due to the ability of mKPGMs to jointly capture the clustering coefficient and the hop plot distribution. Moreover, the learned parameters also pro-duce a fairly good match on characteristics that were not included in the objective function (eigenvalues, network val-ues, and K-core). We note that the SMM method is flexible enough to include these additional measures into the objec-tive function directly if they are deemed important enough to explicitly optimize and computational resources allow.
We also demonstrated that SMM learning can be applied to a single network by setting ` = K . mKPGMs learned with SMM offer a significant improvement over current KPGM learning algorithms, not only by reducing the error over the measures (10-40% reduction in KS 3 D distance) but also by avoiding the difficulties of search over factorial permutation space and the complexities of deriving analytical moments. For evaluation, we proposed a new measure X  X he 3-D Kolmogorov-Smirnov distance. This measure considers the correlation among graph distributions (e.g., multiple graph features per node) and enables a more accurate assessment of the characteristics of generated networks by considering the empirical joint distribution rather than independently evaluating marginal distributions.

In the future, we will extend the mKPGM learning algo-rithm to consider the use of additional moments, including higher order moments (i.e., variance) that can be used to learn the most appropriate way to tie parameters (e.g., by varying levels throughout the graph).
This research is supported by NSF, ARO, and DARPA un-der contract number(s) IIS-0916686, IIS-1219015, IIS-1017898, W911NF-08-1-0238, and N660001-1-2-4014. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the of-ficial policies or endorsements either expressed or implied, of NSF, ARO, and DARPA or the U.S. Government.
