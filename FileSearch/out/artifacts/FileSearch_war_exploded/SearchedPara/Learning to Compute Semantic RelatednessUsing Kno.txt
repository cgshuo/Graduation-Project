 Computing semantic relatedness (SR) between terms is an important task for many natural language processing applications, such as information retrieval [4], word sense disambiguation [11] and entity linking [10,18] . Many approaches for computing SR have been proposed in these y ears. Some approaches use statistical analysis of large corpora to get SR between words; other approaches make use of hand-crafted lexical resources for computing SR. However, both of these two groups of approaches have their limitations. Corpora-based methods only use collections of texts, which do not make any use of available structured knowledge, and their results might be influenced by noise in the texts. The other group of approaches utilize lexical resources such as WordNet [8], Roget X  X  Thesaurus [13]; but these resources are usu ally created by human expe rts and therefore usually cover a small part of language lexicon.

Recently, lots of work has shown that Wi kipedia can be used as the resource for computing SR between words or text fragm ents. Wikipedia is one of the largest online encyclopedias on the web, which contains more than 20 million articles written in 285 languages by March 2013. Wikipedia contains large number of hypertext pages that are interconnected by links. A page that has encyclopedic information in it is called a Wikipedia article, or entry. Articles are the most important elements of Wikipedia, each of them identifies a notable encyclopedic topic and summarizes that topic comprehen sively. Except article pages, there are other meta pages which are used for administrative tasks, including Category page, Statistic page and Help page, etc. Fig. 1 is an example of a Wikipedia article page.
 Different kinds of information in Wikipedia has been used for computing SR. The category system of Wikipedia is used in [16,12], links among Wikipedia articles are used in [17], links between articles and categories are used in [1]. Information of the same kind can also be used in different ways to compute the SR. For example, inlinks of two articles are compared as two sets in [17], but they are compared as two vectors in [5]. We have observed that different combination of Wikipedia information and SR measure s usually result in different SR results. Table 1 shows the top 10 entities that have the highest relatedness with the entities Mathmatics for three different SR measures based on articles X  inlinks, including Inlink-DC (Dice Coefficient of inlinks), Inlink-GD (Google Distance of inlinks), and Inlink-VB (Vector based d istance of inlinks). Entities listed in each column in Table 1 are ranked in descending order of SR. It shows that most the 10 entities have different ranks in three columns. Table 2 shows the top 10 most related articles Mathematics based on the the same SR measure Dice Coefficient, but with three different kinds of Wikipedia information, including the inlinks, outlinks and categories. It shows that using the same SR measure but with different kinds of information will also generate different results. Therefore, how to use different kinds of information in Wikipedia to accurately compute SR between entities becomes a challenging problem.

In this paper, we propose a supervised learning approach for computing SR between entities based on Wikipedia. Our approach uses articles X  inlinks, out-links, and categories in Wikipedia, and em ployes three different SR measures to compute raw SRs between entities. A supe rvised learning algorithm is proposed to learn the weights of different raw SRs to get the final SR. We evaluate our approach on three standard datasets, the results show that our approach can get better results than the comparison approaches.

The rest of this paper is organized as follows, Section 2 discusses some re-lated work; Section 3 describes the proposed approach; Section 4 presents the evaluation results and Section 5 concludes this work. There have been several approaches that compute SR based on Wikipedia. Strube and Ponzetto [16,12] firstly proposed to compute SR based on Wikipedia. Their approach WikiRelate first maps the given words to Wikipedia pages, and then hooks these pages to the category tree by extracting the categories the pages belong to. The SR between the given words is computed by path-based measures based on the category taxonomy.

Gabrilovich and Markovitch [5] proposed Explicit Semantic Analysis (ESA) for computing SR. ESA first represents each Wikipedia concept as an attribute vector of words that occur in the corresp onding article, and assigns weights to the words in vectors by using TF-IDF method [15]. For any given words or long texts, ESA then represents them as weighted vectors of concepts based on the words-concept associations in the former step. And ESA finally computes SR of texts or words as the cosine distances between their vectors.

Milne and Witten [17] proposed a approach, Wikipedia Link-based Measure (WLM), which computes SR using the hyperlink structure of Wikipedia instead of its category hierarchy or textual content. Given two terms, WLM fist iden-tifies the corresponding Wikipedia articles of them. Then it uses two measures to compute the SR: the first measure represents each article as the vector of weighted inlinks and then computes the cosine similarity between the vectors of two articles; the second measure represents each article as a set of inlinks, and then computes the Normalized Google Dis tance [3] between the feature sets of two articles.

Hassan and Mihalcea [7] proposed to measure the SR of words by using their concept-based profiles. A profile is constructed using the co-occurring salient concepts found within a given window size in a very large corpus. A word is defined by a set of concepts which share its context and are weighted by their point-wise mutual information. To compute the SR, a modified cosine similarity is computed between words X  profile vectors.

Yeh et al. [19] proposed to compute SR by using personalized PageRank on a graph derived from Wikipedia. Their approach first converts Wikipedia into a graph, and then maps the input texts into the graph. Random walks on the graph based on Personalized PageRank are performed to obtain stationary distribu-tions that characterize each text. An d the SR between two texts is computed by comparing their distributions.

Hassan and Mihalcea [6] also extended the ESA approach and proposed a method for computing cross-lingual SR between words. By using the cross-lingual links in Wikipedia, their method can compute the cosine similarity between ESA concept vectors in different languages. Several modifications of ESA are also done to improve the performance of their method.
Chan et al. [2] solve a problem that words with comparatively low frequency cannot always be well estimated based on ESA. They proposed a method for using not only word frequency but also layout information in Wikipedia articles by regression to better estimate the relevance of a word and a concept. Empirical evaluation shows that on the low frequency words, this method achieves better estimate of SR over ESA.

Table 3 summarizes the key features of the above approaches. Being differ-ent from these approaches, our approach uses several different components of Wikipedia article and several measures t ogether; instead of empirical designed measures, our approach use supervised learning algorithm to predict the SR. In this section, we introduce our proposed approach. Given two entities, our approach first maps them to the articles in Wikipedia; then it computes several different raw SRs between the corresponding articles, and uses a supervised learning algorithm to get the final SR. 3.1 Raw Semantic Relatednesses Our approach utilizes 3 features of Wikiped ia articles, which are then combined with 3 different relatednesses measures to form 9 raw SRs.
 Article Features. Here we first define three features of Wikipedia articles. Definition 1. Outlinks. For a Wikipedia article a , the outlinks of a is the set of articles O a that a links to by hyperlinks.
 Definition 2. Inlinks. For a Wikipedia article a ,theinlinksof a is the set of articles I a that link to a by hyperlinks.
 Definition 3. Categories. For a Wikipedia article a , the categories of a is the set of categories C a that that a belongs to.
 Relatedness Measures. Our approach uses 3 measures for computing SR. Definition 4. Dice Coefficient. Given two feature sets A and B of two Wikipedia article a and b , Dice Coefficient computes the relatedness of a and b as Definition 5. Google Distance. Given two feature sets A and B of two Wikipedia article a and b , Google Distance computes the SR of a and b as where W is the set of all articles in Wikipedia.
 Definition 6. Vector-based Similarity. The vector-based similarity is calcu-lated between the feature vectors of articles X  features. Before the similarity com-putation, a virtual document is generated for the feature of each article. Then the virtual document of each article is represented as a vector, where the elements in the vector are weights assigned to the words in the virtual document using TF-IDF method [15]. For a word i in an article X  X  virtual document j , the weight of the word is computed as where tf ij is the number of occurrences of i in j , df i is the number of virtual documents that contain i ,and N is the total number of virtual documents. The vector-based similarity between two article is computed as the cosine value be-tween their virtual documents: where M is the total number of distinct words in all of the virtual documents. Raw Semantic Relatedness. Based on the above features and relatedness measures, 9 different raw SRs are computed for each pair of entities. These raw SRs are outlined in Table 4. 3.2 Learning to Compute Relatednesses To get the final SR, our approach computes the weighted sum of 9 raw SRs between two articles by the following function: One challenge problem here is how to determine the proper weights  X  i , ( i = 0 ,..., 9) for each raw SR. We propose a supervised learning algorithm to learn the optimum weights. The goal of learning algorithm is to make the predicted SR to be close to human judgement as much as possible.

In order to learn the weights of different raw SRs, training data need to be built by human experts. Instead of asking people to decide the SR values of given entity pairs, we ask human experts to tell whether a given entity e 1 is more related to a referent entity e 2 than another reference entity e 3 . Because we think it is natural and easy for people to answer such kind of questions. After collecting a number of the answers, we are able to build a training dataset D indicates SR ( e j 1 ,e j 2 ) &gt;SR ( e j 1 ,e j 3 ).

Given training dataset D = { e j 1 ,e j 2 ,e j 3 } m j =1 , our algorithm learns the weights to ensure where  X  =  X  1 ,..., X  9 and SR (  X  )= SR 1 (  X  ) ,...,SR 9 (  X  ) . If we define the proba-bility of R ( e j 1 ,e j 2 ) &gt;R ( e j 1 ,e j 3 )as 0 . 5and P ( R ( e j 1 ,e j 3 ) &gt;R ( e j 1 ,e j 2 )) &lt; 0 . 5.

In this case, the weights  X  can be determined by the MLE (maximum like-lihood estimation) technique for logistic regression. We generate a new dataset put vector and y j represents the class label (positive or negative). For each triple t a logistic regression model is trained on D , the learned weights  X   X  is used to compute the SR by formula (8). We implemented our proposed approach based on the English Wikipedia data that is archived in August 2012. After parsing the Wikipedia XML dump and removing redirect pages, we obtained 7.5 GB of text in 4 million concept pages and 889,527 category pages. Each article linked to 19.262 articles and be linked from 16.914 articles on average. 4.1 Datasets We evaluated our proposed approach on three datasets of entity pairs and man-ually defined SR, Table 5 shows the detail information of these datasets. M&amp;C30. Miller and Charles [9] replicated the experiment with 38 test sub-jects judging on a subset of 30 pairs called M&amp;C30 taken from the original 65 pairs (i.e. R&amp;G65). We argue that evaluations restricted to those datasets were limited with respect to the number of word pairs involved, the parts of speech of word pairs, approaches to select word pairs (manual vs. automatic, analytic vs. corpus based), and the kinds of semantic relations that hold between word pairs. However, an evaluation involving t he aspects described above is crucial to understand the properties of a specific measure. A significant evaluation of SR measures requires a higher number of word pairs such as the next datasets. R&amp;G65. Rubenstein and Goodenough [14] obtained similarity judgments from 51 test subjects on 65 noun pairs written on paper cards. These noun pairs range from the high synonyms to the irrelevant words on semantics. Test subjects were instructed to order the cards accordin g to the similarity of meaning and then assign a continuous similarity value [0,4] to each card, where 0 represents the fact that two words art not related, and 4 indicates that both are synonymous. The final dataset called R&amp;G65 contains 65 English noun pairs, which is commonly used for semantic computation. Fin353. Finkelstein et al. [4] created a larger dataset for English called Fin353, which is a significant famous manual dataset of SR containing 353 word pairs including also the 30 word pairs from M&amp;C30. To assess word relatedness, we use the Fin353 benchmark dataset, available online, which contains 353 word pairs. Each pair was judged, on average, by 13-16 human annotators. This dataset, to the best of our knowledge, is the largest publicly available collection of this kind, which is used by the most works (detailed in related works paragraph) in their evaluation. 4.2 Evaluation Metric We evaluated performance by taking th e Pearson product-moment correlation coefficient between the relatedness meas ure scores and the corresponding human judgments. It is computed as: variable X and the relatedness in the evaluation dataset as another variable Y , and then compute their correlation. High correlation indicates that the computed SRs are closed to the relatedness defined by humans. 4.3 Results Comparison Table 6 shows the experimental results of our approach and five comparison approaches. The results of comparison approaches were reported in already pub-lished papers, we compare the results of our approach with these ones. It shows that our approach achieves the best results on all the three datasets. Our ap-proach outperforms the second best approaches on three datasets by 0.18, 0.13 and 0.03 respectively. It is obvious that combining different features and relat-edness measures by supervised learni ng algorithm can improve the accuracy of computed SRs.
 In this paper, we propose a supervised learning approach for computing SR between entities based on Wikipedia. Our approach first maps entities to articles in Wikipedia, and then computes 9 different raw SRs between them. A learning model based on logistic regression is used to obtain the optimal weights for each raw SR, and the final SR is computed as the weighted average of 9 original ones. The experimental results show that our a pproach can accurately predict the SR, and it performs better than the comparison approaches.

Currently, computing SR between entities in one language is widely studied, but the problem of computing cross-lingual SR has not been well solved. There-fore, we want to extend our approach to the cross-lingual domain to compute cross-lingual SR based on Wikipedia in the future work.
 Acknowledgement. The work is supported by NSFC (No. 61202246, 61003225, 61170203, 61171014, 61272475, 61371185), NSFC-ANR(No. 61261130588), and the Fundamental Research Funds for the Central Universities (2013NT56, 2013NT57), and by SRF for ROCS, SEM.

