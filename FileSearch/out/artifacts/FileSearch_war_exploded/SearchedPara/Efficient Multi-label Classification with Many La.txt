 Wei Bi weibi@cse.ust.hk James T. Kwok jamesk@cse.ust.hk Many real-world classification problems involve mul-tiple label classes. In multi-class classification, each sample can belong to one and only one label; whereas in multi-label classification, each sample can be asso-ciated with multiple labels. For example, in text cate-gorization, a document can belong to the categories of  X  X iracy X ,  X  X opyright X  and  X  X oftware X . Similarly, in bioinformatics, a gene may be associated with the functions of  X  X ranscription X ,  X  X etabolism X  and  X  X ro-tein synthesis X . Image annotation is also a multi-label learning problem. Nowadays, in many social network-ing websites, billions of digital images, each often as-sociated with multiple tags (e.g.,  X  X lephant X ,  X  X ungle X  and  X  X frica X ), are available for free download, sharing and research. For example, Flickr already has 5 bil-lion photos and more than 20 millions unique tags as of 2010. While many of these tags may be redundant, it has been suggested that humans can still recognize between 10,000 and 100,000 unique object classes. The Dmoz data set, which is constructed by crawling web-pages from the Open Directory Project, also has more than 30,000 labels. Obviously, how to handle such a large number of labels in multi-label learning is an im-portant research problem.
 A basic approach to multi-label classification is binary relevance (BR) (Tsoumakas et al., 2010), which sim-ply trains a classifier for each label independently. In recent years, many approaches have been proposed to further improve classification performance by incorpo-rating the label correlations (Dembczynski et al., 2010; Hariharan et al., 2010) or exploiting the label hierarchy (Bi &amp; Kwok, 2011). However, as the number of labels ( d ) in many domains keeps growing, even the simple BR approach (in which the number of classifiers to train is equal to d ) can easily become computationally infeasible, not to mention the more sophisticated and computationally demanding approaches.
 Some recent approaches have been proposed to ad-dress the multi-label classification problem with many labels. A first attempt is by Hsu et al. (2009), who projects the d -dimensional label vector using compressed sensing, and performs training with the much lower-dimensional projected label vectors. Sub-sequently, many variants have been developed along this line, which use different projection mechanisms including principal component analysis (Tai &amp; Lin, 2012), canonical correlation analysis (Zhang &amp; Schnei-der, 2012) and other singular value decompositions (Chen &amp; Lin, 2012). A common characteristic is that they all reduce the possibly large number of labels to a more manageable set of transformed labels. Yet, a major limitation is that the transformed labels, though fewer in quantity, may be more difficult to learn. Instead of using label transformation, Balasubrama-nian &amp; Lebanon (2012) proposed to train only a small subset of the labels. Since this subset come from the original labels, their learning problems will not be made more difficult. Obviously, the key issue is how to select this label subset. A good candidate should allow the non-selected labels to be faithfully and easily constructed from the selected ones. Bal-asubramanian &amp; Lebanon (2012) formulated this as a a group-sparse learning problem. However, even with the recent advances in learning with structured sparsity (Bach et al., 2011), the involved optimization problem is still computationally expensive, especially when there are a lot of labels to select from. In this paper, we alleviate this problem by proposing an efficient label selection method based on random-ized sampling. Following the assumption in (Balasub-ramanian &amp; Lebanon, 2012), we design the sampling probability of each label using its leverage score in the best rank-k subspace of the label matrix. Theoretical analysis shows the efficiency of this sampling scheme and its performance when coupled into the multi-label classification framework.
 The rest of this paper is organized as follows. Sec-tion 2 first gives a brief review on label transformation / selection and the column subset selection problem (Drineas et al., 2006). Section 3 then presents the proposed learning algorithm. Experimental results are presented in Section 4, and the last section gives some concluding remarks.
 Notation : In the sequel, n denotes the number of training samples, m is the number of features, and d is the number of labels. X  X  R n  X  m is the input (training) data matrix, and Y  X  { 0 , 1 } n  X  d is the cor-responding label matrix. Moreover, the transpose of vector/matrix is denoted by the superscript T , A  X  de-notes the Moore-Penrose pseudo-inverse of a matrix A , k A k 2 is its spectral norm and k A k F is the Frobenius norm, and A ( i ) is the i th column of A . 2.1. Label Transformation Hsu et al. (2009) proposed a three-step approach to address classification problems with a large number of labels. First, the high-dimensional label vector is projected to a low-dimensional space using random transformation. Next, a regression model is built for each dimension of the transformed label vector. Fi-nally, given a test sample, the estimated label vector is projected from the low-dimensional space back to the original label space.
 Recently, various improvements have been proposed. Tai &amp; Lin (2012) found the random transformation in (Hsu et al., 2009) to be ineffective, and proposed the principal label space transformation (PLST) which uses principal component analysis (PCA) on the la-bel matrix Y . As PCA only minimizes the encod-ing error between Y and its low-dimensional represen-tation, Chen &amp; Lin (2012) proposed the conditional PLST (CPLST) that simultaneously minimizes both the encoding error and training error in the reduced-dimensional space. Similarly, Zhang &amp; Schneider (2011) proposed to use canonical correlation analy-sis (CCA) that also takes both the input and output matrices into consideration. They further proposed a maximum margin formulation to learn an output cod-ing that is predictive (based on the estimated predic-tions in the reduced-dimensional space) as well as dis-criminative (such that different label vectors have dif-ferent transformed label vectors) (Zhang &amp; Schneider, 2012). However, its optimization relies on the cutting plane algorithm (Tsochantaridis et al., 2005), which may not be efficient when there are many labels. Zhou et al. (2012) used the Gaussian random projection to form the transformed labels. During preprocessing, it extracts an auxiliary distilled label set containing the frequently appearing label subsets. However, empiri-cally this step is expensive. 2.2. Label Selection Recently, Balasubramanian &amp; Lebanon (2012) pro-posed the multiple output prediction landmark selec-tion method (MOPLMS) based on the assumption that all the output labels can be recovered by a small sub-set. In other words, Y ' YW , where W  X  R d  X  d is the coefficient matrix with only a few nonzero rows. Mathematically, we have where  X  1 , X  2 are regularization parameters, k W k 1 , 2 P izer that encourages row sparsity of W , and k W k 1 = P i,j =1 | W ij | is the traditional ` 1 -regularizer that en-courages sparsity over the whole W . However, when the number of labels d is large, W becomes large and problem (1) is computationally expensive. Besides, the size of the label subset cannot be explicitly con-trolled, and can only be indirectly varied by changing the  X  1 , X  2 parameters in (1). 2.3. Column Subset Selection Problem (CSSP) Given a matrix A  X  R n  X  d and a positive integer k , the CSSP seeks to find exactly k columns of A so as to span A as much as possible. In other words, we want to find an index set C with cardinality k such that the residual k A  X  A C A  X  C A k F is minimized. Here, A denotes the submatrix consisting of the C columns in A , and A C A  X  C is the projection matrix onto the k -dimensional space spanned by the columns of A C . A brute-force search will need to enumerate O ( d k ) pos-sible solutions. Even for a small k , this can be pro-hibitive when d is large. Recently, randomized sam-pling schemes have been proposed to find approximate solutions of the CSSP more efficiently (Drineas et al., 2006; Boutsidis et al., 2009).
 A popular algorithm is the exact subspace sampling scheme (Drineas et al., 2006), which has also been in-corporated by other CSSP algorithms (Boutsidis et al., 2009). For a given , it samples O ( k 2 / 2 ) columns from A , where the probability of selecting the i th column is and V A ,k is the matrix containing the top k right singular vectors of A . Intuitively, p i corresponds to the leverage score of A ( i ) on the best rank-k subspace of A (Boutsidis et al., 2009). In statistical diagnos-tic regression analysis, leverage scores can be used to measure outliers. Thus, (2) provides a bias to-ward columns that are outlying, which play an im-portant role in spanning the subspace. With proba-bility 1  X  1 e , the approximation error of the resultant approximation k A  X  A C A  X  C A k F is upper-bounded by (1 + ) k A  X  A k k F , where A k is the best rank-k ap-proximation of A .
 Recently, Boutsidis et al. (2009) proposed to first sam-ple  X ( k log k ) columns from A with the probabilities in (2), and then perform the rank-revealing QR (RRQR) decomposition (Gu &amp; Eisenstat, 1996) on a scaled ver-sion of the sampled columns of V T A ,k . This allows the extraction of exactly k columns from A . With prob-ability 0.8, the error bound is k A  X  A C A  X  C A k F  X ( k log 1 2 k ) k A  X  A k k F . Recall that CSSP aims to find k columns from a given matrix A so that the reconstruction error is mini-mized. Obviously, this shares the same goal as MO-PLMS (Balasubramanian &amp; Lebanon, 2012), with the label matrix Y playing the role of A , i.e., While MOPLMS relies on an expensive optimization problem to select the labels (columns), here we will use an efficient CSSP variant (Section 3.1). Once this subset of k labels are selected, a binary classifier is trained for each of them (alternatively, the k labels can be learned jointly as in multi-task learning). In total, k binary classifiers are needed. In contrast, a di-rect application of BR requires the training of d binary classifiers, where d k .
 As label selection is now considered as a CSSP, in prin-ciple one can use any of the algorithms in Section 2. However, this may not be entirely satisfactory. While we aim at selecting exactly k columns of Y , the al-gorithm in (Drineas et al., 2006) selects a lot more than k columns. As for the algorithm in (Boutsidis et al., 2009), though it can output exactly k columns, it needs to first select c =  X ( k log k ) columns. Empiri-cally, a proper choice of c can be sensitive to the label matrix (Mahoney, 2011). Moreover, performing the RRQR decomposition in its deterministic step takes O ( c 2 k log demonstrated in Section 4.3, this can be even more computationally expensive than the sampling step it-self.
 In Section 3.1, we propose a novel variant of (Drineas et al., 2006; Boutsidis et al., 2009) which directly se-lects k different columns in Y , while ensuring efficiency and a good approximation error bound. Section 3.2 discusses the prediction mechanism. Section 3.3 pro-vides an error analysis, and Section 3.4 discusses how this can be extended to the use of kernels. 3.1. Proposed CSSP Randomized Sampling The proposed procedure is shown in Algorithm 1. As in (Drineas et al., 2006; Boutsidis et al., 2009), we first perform partial SVD on Y and pick the top k right singular vectors V Y ,k  X  R m  X  k . For notational simplicity, we will use V k for V Y ,k in the sequel. The columns in Y are sampled with replacement, with the probability for selecting the i th column being as in (2). However, instead of performing a fixed num-ber of sampling trials, we continue sampling until k different columns are selected (steps 4-9).
 Similar to (Boutsidis et al., 2009), the following Propo-sition shows that the ( V T k ) C matrix sampled is full rank with high probability.
 Proposition 1. If the WHILE loop stops in T trials, where and c 0 is a constant in Theorem 3.1 of (Rudelson &amp; Algorithm 1 Multi-label classification via CSSP (ML-CSSP). 1: Compute V k , the top k right singular vectors of 2: Compute the sampling probability p i for each col-3: C  X  X  X  . 4: while | C | &lt; k do 5: Select an integer from { 1 , 2 ,...,m } where the 6: if i /  X  C then 7: C  X  C  X  X  i } . 8: end if 9: end while 10: Train the classifier f ( x ) from { x ( n ) , y ( n ) 11: Given a new test point x , obtain its prediction h Vershynin, 2007), then ( V T k ) C is full rank with proba-bility 1  X  4 2 .
 Moreover, one can obtain the following error bound, which shows that the obtained approximation error is close to that based on the best rank-k approximation, with a factor of  X (1).
 Corollary 1. With probability 0 . 9  X  4 2 , k Y  X  Y rank-k approximation of Y .
 The following Proposition shows that, with probability at least 0.9, k different columns will be selected in O ( k log k ) trials. Thus, in the worse case, we do not need to sample more columns than the algorithm in (Boutsidis et al., 2009).
 Proposition 2. With probability at least 0.9, k differ-ent columns are selected in O ( k log k ) sampling trials. Though the above results suggest that O ( k log k ) columns may still need to be sampled in the worst case, empirical results in Section 4.3 show that the T obtained is much smaller than k log k . 3.1.1. comparison with other methods Table 1 compares the proposed algorithm with the existing CSSP algorithms in (Drineas et al., 2006) and (Boutsidis et al., 2009). To compute the sam-pling probabilities, all three CSSP-based algorithms have to first obtain the top k singular vectors of the label matrix Y . This takes O (min { nd 2 ,n 2 d } ) time in general, but can be reduced to O ( ndk ) time by using Lanzcos/Arnoldi algorithms as Y is typi-cally sparse. The second term in the time complex-ity comes from the number of sampling trials (which is O ( k 2 / 2 ) for (Drineas et al., 2006), O ( k log k ) for (Boutsidis et al., 2009) and ours). Finally, the algo-rithm in (Boutsidis et al., 2009) involves an additional RRQR decomposition after the sampling step, which takes O ( c 2 k log c =  X ( k log k ).
 For easy comparison, we also show the time complex-ities for PLST and CPLST in Table 1. PLST is fast, as it only requires a single SVD on the label matrix, which takes O ( ndk ) time. CPLST needs to compute the pseudoinverse of the input matrix X (which is as expensive as computing its SVD) and a SVD on the matrix Y T XX  X  Y . In general, these two matrices are dense, and so CPLST takes O (min { nm 2 ,n 2 m } + d 3 ) time for these two operations, and is much more expen-sive. The time complexity of MOPLMS cannot be di-rectly compared as it involves numerical optimization. Empirically, as will be demonstrated in Section 4.2, this is much more expensive. 3.2. Prediction On prediction, we first apply the k learned classifiers on a new test sample to obtain its k -dimensional pre-diction vector h . Note from (3) that Y ' Y C Y  X  C Y . Each row of Y (which corresponds to the d labels of a particular sample) can thus be approximated as the product of the corresponding row in Y C (which cor-responds to the k selected labels of the same sample) with Y  X  C Y . Given h , a d -dimensional label vector  X  y can be recovered as h T Y  X  C Y . This is further rounded to produce a binary classification output. 3.3. Error Analysis In this section, we analyze the root mean square error (RMSE) on the training examples, which is defined as where  X  Y is the estimated label matrix. As  X  Y , Y are binary, the squared RMSE is also proportional to the commonly used Hamming loss 1 nd k Y  X   X  Y k 2 . Denote the estimated label matrix using f ( x ) in the selected label dimensions as H . Let Y k be the best rank-k approximation of Y . The following Proposition bounds the training RMSE for Algorithm 1.
 Proposition 3. With the conditions in Proposition 1, we have, with probability 1  X  4 2 , RMSE  X  Proof. Let  X  Y = round( HY  X  C Y ) be the reconstructed label vector after rounding, where round(  X  ) rounds each element of the matrix argument to the nearest 0 or 1. Since both  X  Y and Y are binary, (  X  Y  X  Y ) ij is either 1 or 0. 1. (  X  Y  X  Y ) ij = 1: This happens iff ( HY  X  C Y ) ij 2. (  X  Y  X  Y ) ij = 0: In this case, (8) also holds trivially. Thus, k  X 
Y  X  Y k F = Using the triangle inequality for norms and the fact that k AB k F  X k A k F k B k 2 , we have From Proposition 1, ( V T k ) C is full rank (and thus has rank k ) with probability 1  X  4 2 . As ( V T k ) C is a sub-matrix of ( V T ) C , rank(( V T ) C )  X  rank(( V T k ) C ( V T ) C has k different columns, rank(( V T ) C )  X  k . Let the SVD of Y be Y = U X V T . Then Y C = U X  ( V T ) C , which is full rank with rank k . Assuming that V T is of rank d , we have Y  X  C = (( V T ) C ) T  X   X  1 Result follows by combining (9), (10), (11).
 The error bound in (7) consists of two parts. Intu-itively, the first term k H  X  Y C k F represents the train-ing error between the learned label submatrix and the target label submatrix Y C selected by the algorithm; while the second term k Y  X  Y C Y  X  C Y k F represents the encoding error in projecting the full label matrix Y onto the selected labels Y C .
 Combining with Corollary 1, we immediately obtain the following.
 Corollary 2. With the conditions in Proposition 1, we have, with probability 0 . 9  X  4 2 , that RMSE  X  This is similar to the error bound for PLST in (Tai &amp; Lin, 2012), namely, RMSE  X  2  X  n k H  X  YV k k F ing error between the learned label submatrix and the projected label matrix YV k . 3.4. Kernel Extension Instead of assuming that the columns of Y are spanned by a small subset of its columns, we can first map the columns to some kernel-induced feature space before taking the approximation. In other words, we try to label matrix.
 As in Section 3.1, the probability for selecting the i th column can be similarly defined as p i = 1 k k (  X  V T k where  X  Y =  X  U  X   X   X  V T . It can be easily seen that the are the same as the top k eigenvectors of the kernel matrix K =  X  Y T  X  Y , as K =  X  Y T  X  Y =  X  V  X   X   X   X  V  X  On testing, the prediction  X  y of a pattern x can be ob-tained by directly minimizing k  X  y  X   X  h T  X  Y  X  C  X   X  y is the kernel-mapped vector of  X  y . For the RBF ker-nel, it can be shown that the optimal  X  y can be easily obtained in a component-by-component manner. In this section, we perform experiments on a number of benchmark real-world data sets 1 (Table 2).  X  cal500 (Turnbull et al., 2008): It contains songs  X  corel5k (Duygulu et al., 2002): It contains images  X  delicious (Tsoumakas et al., 2008): It contains the  X  Eur-Lex (Menc  X  X a &amp; F  X urnkranz, 2008): It contains  X  dmoz : It is constructed by crawling webpages Using linear regression as the learner, the proposed ML-CSSP and its kernel version ML-CSSP-Knl (with the RBF kernel) are compared with various recent multi-label output coding methods: (i) PLST (Tai &amp; Lin, 2012); (ii) CPLST (Chen &amp; Lin, 2012); (iii) MOPLMS (Balasubramanian &amp; Lebanon, 2012); (iv) compressed labeling (CL) (Zhou et al., 2012). We also compare with the standard baseline of binary relevance (BR), which trains a classifier for each label indepen-dently. All the methods are implemented in MatLab. We do not compare with the compressed-sensing-based method in (Hsu et al., 2009), as its performance is al-ready shown to be inferior to PLST (Tai &amp; Lin, 2012). The number of selected (or transformed) labels is set to k = 0 . 1 d , except for dmoz in which we use k = 300 (i.e., k ' 0 . 01 d ). Note that MOPLMS cannot set k explicitly. Thus, we try different settings of its regu-larization parameter, and pick the one whose resultant k is closest to 0 . 1 d .
 For performance evaluation, we will use two popular measures: (i) the RMSE as defined in (6); and (ii) the micro-averaged area under the precision-recall curve (AUPRC) (Vens et al., 2008). Following (Menc  X  X a &amp; F  X urnkranz, 2008), we perform 10-fold cross-validation, except on the large dmoz data set (for which we ran-domly select 40,000 samples for training and the rest for testing, and repeat 3 times). All experiments are run on a PC with quad-core 3.40 GHz Intel i7-3770 CPU and 32 GB RAM. 4.1. Accuracy Table 3 shows the RMSE results obtained. Recall that MOPLMS is very computationally expensive, and so cannot be run on most of the larger data sets. For the largest dmoz data set, ML-CSSP-Knl also runs out of memory as it has to perform a partial SVD on the dense 35K  X  35K kernel matrix. Similarly, CPLST fails as it has to compute the pseudoinverse of the 350K  X  800K input data matrix. For CL, computing the distilled label set in its preprocessing step already takes more than 72 hours. Moreover,  X  The performance of CPLST is comparable with  X  CL does not perform well in our experiments. Em- X  ML-CSSP-Knl is better than ML-CSSP on the Overall, ML-CSSP and ML-CSSP-Knl achieve the best accuracy on five of the six data sets.
 Table 4 shows the AUPRC results, 2 which is obtained by varying the rounding threshold for each of the methods from 0 to 1. As can be seen, ML-CSSP out-performs the others on 5 of the 6 data sets. Note that PLST sometimes performs much worse than the other methods (e.g., on the data sets of delicious , EUR-Lex (desc) and dmoz ). 4.2. Time In this section, we compare the time performance of the various multi-label output coding methods. With our experimental setup, they all have the same number of learning tasks after label transformation/selection. Hence, we will only compare their encoding time, i.e., the time to perform label transformation/selection. Results are shown in Table 5. As can be seen,  X  CL has the shortest encoding time as it uses ran- X  On encoding, both ML-CSSP and PLST have  X  CPLST has to compute the pseudoinverse  X  ML-CSSP-Knl is much slower than CSSP, as it  X  MOPLMS has the longest encoding time as it Overall, the encoding speeds of ML-CSSP and PLST are reasonably fast. Both of them take less than half an hour even on the largest data set of dmoz . Moreover, they reduce the number of learning tasks by 99% when compared to BR. In combination with the accuracy comparison in Section 4.1, we can conclude that ML-CSSP is both fast and accurate. 4.3. Comparison with (Boutsidis et al., 2009) Here, we compare the proposed method with the CSSP algorithm in (Boutsidis et al., 2009). As suggested in (Boutsidis et al., 2009), we set the number of sampled columns to s = 2 k log k , and the RRQR decomposition is then used to extract exactly k columns (labels) from these s columns.
 Table 6 shows the number of sampling trials and en-coding time. As can be seen, the proposed method always samples a much smaller number of columns, and its encoding is also faster. Recall that the RRQR decomposition in (Boutsidis et al., 2009) operates on the matrix ( V T k ) S , where S is the subset of indices selected in the sampling step (with | S | = s ). This ma-trix is of size k  X  s = k  X  2 k log k , which is around 400  X  5K for EUR-Lex (desc) and 300  X  3.5K for dmoz . These are too large for the RRQR algorithm, which cannot finish in an hour. Thus, the CSSP algorithm in (Boutsidis et al., 2009) is not scalable enough for prob-lems with a large number of labels. Table 7 compares the two methods in terms of the approximation ra-both methods are comparable.
 Overall, both algorithms have similar accuracy but the proposed method is more efficient and can be used on problems with much larger number of labels.
 We further test the probability of ( V T k ) C to be full rank in Proposition 1. Table 8 shows that using the sampling probabilities in (4), the ( V T k ) C obtained is always full rank in all the repetitions. In contrast, if we use the more naive uniform sampling scheme, the probabilities of obtaining a full-rank ( V T k ) C drop sig-nificantly. Thus, empirically, our sampling can obtain full rank ( V T k ) C with high probability, thus Corollary 1 holds with high probability.
 4.4. Variation with the Number of Selected In this section, we demonstrate the tradeoff between training error and encoding error as discussed in Sec-tion 3.3. The number of selected labels k is varied from 0 . 1 d to d on the smallest data sets cal500 . For each k , we show the training error ( k H  X  Y C k F ML-CSSP and MOPLMS, and k H  X  YV k k F for PLST and CPLST) and the encoding error ( k Y  X  Y C Y  X  C Y k F for ML-CSSP, k Y  X  YW k F for MOPLMS, and k Y  X  Y C V k V T k k F for PLST and CPLST).
 Figure 1 shows the results. As can be seen, the train-ing error increases with k , as there are more tasks to be learned. Moreover, ML-CSSP and MOPLMS have lower training errors than PLST and CPLST, as the selected labels are in general easier to learn than the transformed labels. The encoding error, on the other hand, decreases with k as expected. In particular, PLST, which transforms Y to its best rank-k repre-sentation, yields the smallest encoding error. In this paper, we proposed an efficient approach for handling multi-label classification problems with many labels. Using a label selection approach, we sample the more important labels by considering it as a column subset selection problem (CSSP). Instead of using a pre-determined number of sampling trials as in exist-ing CSSP algorithms, the number of trials used in the proposed algorithm is adaptive. Empirically, a much smaller number of sampling trials is needed. Theo-retical analysis shows that the proposed sampling ap-proach is highly efficient. It can also obtain a good approximation of the label matrix, and a good multi-label classification performance bound. Experiments performed on a number of real-world data sets with large number of labels demonstrate that the proposed algorithm is effective and efficient as compared to the various recent multi-label learning algorithms. This research was supported in part by the Research Grants Council of the Hong Kong Special Administra-tive Region (Grant 614012).

