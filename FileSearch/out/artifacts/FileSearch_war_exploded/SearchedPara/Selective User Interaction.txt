 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval[query formulation, search process] General Terms: Experimentation, Human Factors Keywords: User Interaction, Query Expansion, Query Re-laxation, Query Potential
Query expansion [12] refers to the process of including related terms in the original query to produce expanded queries, while query relaxation [8] refers to the dropping or down-weighting of terms from the original query to produce sub-queries . The automatic versions of both query expan-sion (AQE) and query relaxation (AQR) are known to fail in a large fraction of queries, and overall (average) improve-ments in performance can be attributed to high gains on a smaller fraction [7].

The potential to address the mistakes made by automatic techniques by involving the user [6] motivates interactive versions of these techniques (IQE, IQR). Previous research has shown that involving users in selection [4, 5, 10, 1] or rejection of terms or sets of terms [8] suggested by an auto-matic method has the potential to further improve perfor-mance. However, the same problems that plague automatic techniques are prevalent in interactive techniques: i.e. user interaction has the potential to lead to improvements only for a subset of queries. Further, a second problem has gener-ally been ignored: frequently none of the options selected by the automatic procedures and presented to the user are any better than the original query. In this paper we develop and present procedures for determining when to interact with a user to obtain explicit feedback in the IQR and IQE set-tings. We show that by using these procedures we can avoid interaction for almost 40% of TREC queries without com-promising significant improvements over the baseline. We also develop procedures to rank queries by their potential for improvement through user interaction, enabling systems to interact with users working under time and cognitive load constraints.
 Figure 1: Query Relaxation: The utility of inter-action on a per-query basis. Values less than zero (to the right) indicate that none of the sub-queries presented to the user were better than the baseline query The motivation for this paper is best summarized through Figure 1. It shows the distribution of the absolute potential improvements in mean average precision (MAP) due to IQR for 249 queries in the TREC Robust 2004 collection. If one were to consider a minimum improvement of 0.025 to be worth interacting to achieve, then we can see that user in-teraction for close to 150 queries is unnecessary. Identical trends are observed for IQE as well. The overall improve-ments in MAP (from 0.235 to 0.332, and 0.261 to 0.341 respectively) mask the minuscule improvements contributed by these queries.
 In this paper we seek to address the following questions, Given a long query, is it possible to infer the potential utility of invoking user interaction to select a relaxed version of the same query? and Given a short query, is it possible to infer the potential utility of invoking user interaction to select a better set of expansion terms?
We used version 2.3.2 of the Indri search engine, developed as part of the Lemur 1 project. The pseudo-relevance feed-back mechanism we used was based on relevance models [9]. We reprised our earlier maximum spanning tree based al-gorithm [8] to rank sub-queries/expanded queries. We used http://www.lemurproject.org the TREC Robust 2004, Robust 2005, TREC 5 ad-hoc and HARD 2003 document collections for our experiments. All collections were stemmed using the Krovetz stemmer pro-vided as part of Indri. We also used a manually-created stoplist of twenty terms ( a, an, and, are, at, as, be, for, in, is, it, of, on, or, that, the, to, was, with and what ). 249 queries from the TREC Robust 2004 track were ana-lyzed to determine and fine tune the procedure we devel-oped to determine the utility of interaction. The remaining 150 queries, 50 each from the three remaining tracks, were used to test the effectiveness of our interaction-utility deter-mining procedure. We measured performance using mean average precision (MAP)and geometric mean average preci-sion (GMAP).

There is a large body of previous and related work on procedures to determine the quality of queries [13, 3, 2]. The goal of that work was to predict in advance if a query will result in acceptable values of precision, and take appropriate steps if the query was predicted to fail (have a low AP). The procedures were thus tuned to accurately predict MAP. Our goal is different. We wish to determine if an interaction mechanism will lead to an improvement in MAP. From the perspective of a user, expending interaction effort to improve precision from 0.1 to 0.11 is of the same utility as improving precision from 0.8 to 0.81 i.e. little utility. Hence we tuned our procedure to target improvements in MAP, and not just MAP values themselves.
Our investigation of potential features for predicting im-provement was guided by the following hypotheses about potentially good question sets for interaction. By question sets , we mean the set of top ten sub-queries or expanded queries presented to the user. 1. When the original query is very long, a large number 2. The average score [8] of the question sets will be high, 3. The scores of the sub-queries/expanded queries in the
For each query, we started with the top ten sub-queries / expanded queries ranked by the selection procedure we developed in [8]. We used the scores assigned to them by the selection procedure to investigate several features based on measures of central tendency, measures of dispersion, and measures involving query lengths. In this paper we report only those features that had a high coefficient of correlation (  X  ) with MAP. Table 1 provides a list of the top features we found correlating with potential improvements in AP in the case of IQR and IQE.

The feature with the highest correlation in IQR was orig-inal query length (QL). The negative value indicates that high values of initial query length translate to low-quality sub-queries, while lower values of initial query length are
Identifying and selectively weighting such terms is a con-tinuing challenge Table 1: Features with the highest correlation co-efficient with respect to potential improvement in AP predictive of high-quality sub-queries. This is intuitive as identifying all the concepts in longer queries is more diffi-cult. Longer queries also tend to induce more errors into the sub-query ranking procedure. The feature with the second highest correlation was a dimensionless quantity, coefficient of variation, CV = s x  X  x ,where s x is the standard deviation of asetofsamples x i ,and  X  x its mean. CV can be considered as a measure of the scatter of a set of values. The posi-tive correlation indicates that question sets that have high dispersal are more likely to contain sub-queries that lead to improvements in AP. This is consistent with our hypothesis that question sets with varied sub-queries are more likely to cover concepts the user is interested in. Interestingly, the coefficient of correlation between QL and CV is -0.361. Volatility log change is the standard deviation of the natural base logarithms of the differences of successive ordered val-ues of a set X i.e. VC =  X  ( Y )where y i =log x i +1 x i the volatility measure had very high correlation with CV, we chose to use only CV as a predictive measure for IQE.
Using training instances we learned a simple decision tree thresholded on feature values to determine when to interact with a user. Table 2 reports the change in potentially achiev-able MAP as well as the percentage of queries requiring user interaction for IQR when simultaneous threshold sweeps on both features, QL and CV, were performed. Every MAP value in the table is a statistically significant improvement over the baseline of 0.235.

It is apparent from the table that a wide selection is avail-able for determining appropriate thresholds for the two fea-tures. We chose values of 16 for QL, and 2 for CV (see boxed number in Table 2). For the training set, it meant obviating interaction for 97 i.e. (1.0-0.61)  X  249 queries in lieu of a 2 % reduction in potential MAP improvement .

Table 3 reports the change in potentially achievable MAP and the number of queries requiring user interaction as a threshold-sweepisperformedonCVinthecaseofIQE.The transition to non-significant improvements over the baseline as the threshold is increased shows the limit to which we can avoid user interaction without impacting performance seriously. We chose a CV value of 6 as the final threshold. In Table 4 we provide an overview of results for simulated IQR when the system makes a decision to either interact with the user or go with the baseline query. We can see that when selective interaction was performed there was an signed-rank test, with  X  set to 0.05.
 provided are &lt; MAP, % queries requiring interaction &gt; tuples. An italicized score implies that it was not a statistically significant improvement over the baseline MAP of 0.261 Baseline 0.160 0.142 0.227 Upper Bound 0.283 0.217 0.351 Auto Select 0.162 0.122 0.223 User Select 0.190 0.158 0.267 Thresholded Select 0.180 0.153 0.253 %dropinMAP 5.5 3.1 5.2 % queries dropped 42 40 44 Table 4: Final results for query relaxation. The reported values are those of MAP average drop of 40% in the number of queries the user had to interact with, leading to an average drop in performance of 4.6%. In spite of the reduction, the final MAP was sig-nificantly better than the baseline (Wilcoxon text,  X  =0.05). However, in the case of Robust 2005 and HARD 2003, there was a significant drop in performance from what would have been achieved if the user interacted with all the queries ( X  X ser Select X ). For a user with only enough time to in-teract for 60% (or not interact with 40%) of the queries the significant improvement over the baseline is still worth it.
The results for our simulated IQE experiments are given in Table 5. Again, we observed statistically significant im-provements over the baseline for all three collections. The greatest reduction in the number of queries requiring inter-action was for HARD 2003. However the MAP achieved by our system was statistically less than that potentially achieved by interacting with all queries. As mentioned be-fore, we believe the impact of this result is subjective.
We now extend the procedures we have developed for se-lective user interaction to the scenario where a user presents the system with a set of queries along with a condition that she is willing to only interact say, for x % of the queries. Such situations are not impossible to imagine as users frequently have constraints on the time and effort they are willing to spare. To maximize the benefit from user interaction, it is apt for the system to determine the x % of queries that would have most potential for improvement. The trends in Tables 2 and 3 indicate that higher values of potential improvements Baseline 0.239 0.159 0.315 Upper Bound 0.305 0.210 0.371 Auto Select 0.244 0.162 0.319 User Select 0.266 0.170 0.333 Thresholded Select 0.260 0.165 0.325 %dropinMAP 2.2 2.9 2.4 % queries dropped 22 32 52 Table 5: Final results for query expansion. The re-ported values are those of MAP in AP correlate with higher values of CV. Guided by this observation, we sorted the question sets for each query in the descending order of CV values, and presented them to the simulated user.

Figure 2 provides an overview of the performance on Ro-bust 2005 when the user accedes to interact for 10%, 20%, 30% and so on of the query set. The lowest curve shows the gradual improvements with i ncreased user interaction when query subsets are chosen at random for interaction. The highest curve tracks the improvement when the sys-tem makes the best choice (highest potential improvement in AP) on queries for interaction each time. In between the two is the curve that conveys the effect of presenting the question sets in descending order of CV. While the po-tential for improvement does not rise as rapidly as in the upper bound case, it clearly is much better than presenting the user with queries in random order. The discrepancy in correspondence between the MAP at 60% interaction in the graph and the value reported in the table is because the lat-ter X  X  ordering of queries involves the second feature QL too. For the same user with time to spare for 60% of the queries, we can observe that using CV-based selection helps obtain better performance with the same effort , when compared to randomly selecting queries.

Figure 3 shows the potential gains obtained by increased user interaction through IQE on the Robust 2005 corpus. We notice that in the ideal case upper bound performance can be achieved by interacting with only 50% of the queries. Figure 2: Trajectories of potential improvements in MAP using various question-selection techniques for Robust 2005 in IQR Figure 3: Trajectories of potential improvements in MAP using various question-selection techniques for Robust 2005 in IQE In other words there was no utility in interaction for 50% of the queries. This explains the occasional  X  X lattening X  of the CV-based selection and Random selection curves. The lower portion of the CV-based selection curve has a higher slope than the upper portion. This indicates that the selec-tion process had done a good job of presenting queries with higher potential ahead of those with less.
We have discussed an important problem concerning inter-active information retrieval systems. While user interaction is a promising way to improve retrieval effectiveness, its ef-ficiency needs to be considered too. Inefficient interactive systems that force a user to interact on every instance can cause disenchantment. We have shown that it is possible to predict the utility of interaction with reasonable accuracy, and use it without compromising much on effectiveness. The use of a single feature measuring scatter for both interaction mechanisms implies that interaction mechanisms that pro-vide a wide range of choices have more utility. In other words, showing the user the different parts of the search space her query could lead her to is advantageous.
Shen and Zhai [11] presented work whose motivation is similar to ours. They performed simulated user studies for interaction involving document-level feedback, with the goal of developing procedures that chose the best documents from a pool to present to the user for feedback. The procedures they developed for and results from such active feedback showed that showing users a diverse set of documents was most effective. However unlike our work on query refor-mulation, they did not extend theirs to determine when to interact with the user, or how to handle a user with time and cognitive load constraints.

Some extensions to our work include working with multi-ple interaction mechanisms and learning to select the most appropriate one based on a number of factors. Determining the optimal number of options to present to a user warrants further investigation too. Improving predictive accuracy by exploring new features is also an area of further interest.
