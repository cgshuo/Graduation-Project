 Republished article finding is the task of identifying instances of ar-ticles that have been published in one source and republished more or less verbatim in another source, which is often a social media source. We address this task as an ad hoc retrieval problem, using the source article as a query. Our approach is based on language modeling. We revisit the assumptions underlying the unigram lan-guage model taking into account the fact that in our setup queries are as long as complete news articles. We argue that in this case, the underlying generative assumption of sampling words from a document with replacement, i.e., the multinomial modeling of doc-uments, produces less accurate query likelihood estimates.
To make up for this discrepancy, we consider distributions that emerge from sampling without replacement: the central and non-central hypergeometric distributions. We present two retrieval mod-els that build on top of these distributions: a log odds model and a bayesian model where document parameters are estimated using the Dirichlet compound multinomial distribution.

We analyse the behavior of our new models using a corpus of news articles and blog posts and find that for the task of repub-lished article finding, where we deal with queries whose length ap-proaches the length of the documents to be retrieved, models based on distributions associated with sampling without replacement out-perform traditional models based on multinomial distributions. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Experiment, Theory Language models, Hypergeometric, Multinomial, Linking, Online news, Social Media
Republished article finding (RAF) is the task of identifying in-stances of articles that have been published in one source and re-published more or less verbatim in another. A common instance of the phenomenon occurs with news articles that are being repub-lished by bloggers. The RAF task is important for a number of stakeholders. Publishers of news content are a prime example. For us, the motivation for considering the RAF task comes from the area of online reputation management.

Over the past decade, the web has come to play an increasingly important role in the overall communication strategy of organisa-tions. It continues to offer new opportunities for organisations to di-rectly interact with their customers or audience but it also contains possible threats as online conversations are impossible to control, while the potential impact on an organisation X  X  reputation may be deep and long-lasting. Online reputation management (ORM) is aimed at monitoring the online reputation of an organization, brand or person, by mining news, social media and search engine result pages.

A key aspect of ORM is early detection of news topics that may end up harming the reputation of a given company, brand or person ( X  X ustomer X ), so that public relations activities can be launched to counter such trends. For this purpose it is important to track news stories that talk about an issue that affects the customer. In the blogosphere news stories may be republished for a number of rea-sons. In our data sets (see Section 4 for details), we have come across instances where bloggers want to share a news item with colleagues or students 1 or where a blogger aims to kick off a dis-cussion around the original news article within his own online com-munity, 2 or where someone uses excerpts from a news article as references in a post where they discuss their own opinion. dition to this  X  X trict X  interpretation of the RAF task (where most or all of a source article is being republished), ORM analysts are also interested in a somewhat looser interpretation, where a key part of a source article (e.g., its lead) is being republished in social media. Republished articles matter to ORM analysts as they may become springboards where intense, possibly negative discussions flare up. E.g., a very large part of NYT article  X  X  Boy the Bullies Love to Beat Up, Repeatedly X  http://www.nytimes.com/2008/ 03/24/us/24land.html?_r=1 was republished verbatim in The Kentucky School blog written by the school X  X  teachers, at http://theprincipal.blogspot.com/2008/03/ boy-bullies-love-to-beat-up-repeatedly.html .
E.g., all of  X  X inancial Russian Roulette X  by NYT jour-nalist Paul Krugman was reposted by Mark Thoma (Pro-fessor of Economics at University of Oregon) at http: //economistsview.typepad.com/economistsview/ 2008/09/paul-krugman-fi.html , with a one sentence commentary by Thoma, followed by about 110 follow-up comments.
See, e.g.,  X  X hat parts of the agenda would you sacrifice to try to put bushies in jail for torture? X  http://nomoremister.blogspot.com/2009/01/ what-parts-of-agenda-would-you.html .
Having motivated the task of finding republished news articles in the blogosphere, we now turn to addressing the task. At first glance the strict version of the task looks like a duplicate detection task. As we show in Section 5 below, on a strict interpretation of the RAF task, state-of-the-art duplicate detection methods show very reason-able performance in terms of MRR but in terms of MAP they leave room for improvement. Under the more liberal interpretation of the RAF task, the performance of state-of-the-art duplication detection methods drops rapidly, on all metrics.

These initial findings motivate the use of standard information retrieval methods for the RAF task, viewing the original news arti-cle as a query to be submitted against an index consisting of, say, blog posts [19, 36]. We follow the latter and focus on language modeling (LM) techniques for the RAF task. Language model-ing in IR is usually based on distributions that emerge from sam-pling with replacement , e.g., 2-Poisson, bernoulli, binomial, multi-nomial [32]. This allows a generative model of language to serve its purpose, namely, to produce infinite amounts of word sequences from a finite word population. However, in the particular case of the RAF task, we are dealing with long (document-size) queries. Here, sampling with replacement can lead to overgeneration of un-seen terms; when paired with the long query length, this can have a cumulative and negative effect on performance. It is well-known from general statistics that when the sample size grows close to the population size, i.e., when it is less than 10 times the population, models based on sampling with replacement become less and less accurate [30]. In our case, we consider documents and queries as bags of word level unigrams; unigrams from the document form the population, and unigrams from the query form the sample. In the standard ad hoc retrieval setting, queries tend to be much shorter than documents, i.e., the sample is much smaller than the popula-tion. For example, title queries in the TREC Robust 2004 test set have 3 words, while documents are on average 500 words long [37]. However, in the case of our RAF task, the assumption that docu-ments (blog posts) are at least 10 times longer than queries (source news articles) is blatantly violated: in our data set, the former are 800 words long, the latter as many as 700 words: the two are of comparable length.
 Our main contribution is an LM-based retrieval model for the RAF task that builds on statistical distributions that emerge from sampling without replacement . Documents and queries are consid-ered as urns that contain terms where multiple examples of each term can coexist simultaneously. A document X  X  relevance to an information need, translates into the probability of sampling the query (the source news article) from the document (blog posts). Then, documents are ranked by this probability [33]. A suitable statistical distribution for this model is the hypergeometric distri-bution which describes the number of successes in a sequence of n draws from a finite population without replacement, just as the bi-nomial/multinomial distribution describes the number of successes for draws with replacement.

Our approach to the RAF task consists of deriving a document model and a retrieval model. The document model is based on one of the two multivariate hypergeometric probability distributions we present here: (a) the central hypergeometric distribution and (b) the Wallenius X  hypergeometric (also called non-central) distribu-tion. Both can take into account local term weights (such as raw term frequency (TF), while the model based on the Wallenius X  dis-tribution also allows one to also incorporate global term weights (such as inverse document frequency (IDF)).

The main research question that we seek to answer is whether distributions based on sampling without replacement provide for a more effective retrieval model for the RAF task than (the usual) distributions based on sampling with replacement. We provide a positive answer to this question and complement the answer with a thorough experimental analysis of our proposed models plus a comparison to existing retrieval models tuned for the RAF task.
The rest of the paper is organized as follows. We present two hy-pergeometric distributions in Section 2, two retrieval models based on those distributions in Section 3 We present our experimental setup in Section 4, report on results and analysis in Section 5, dis-cuss alternatives in Section 6, present related work in Section 7. and conclude in Section 8.
We present two hypergeometric distributions which we will use later for sampling a query from a document: (a) the central hy-pergeometric, and (b) non-central hypergeometric (also known as Wallenius X  hypergeometric distribution). The difference between the two is in how we perform sampling and whether bias in sam-pling is involved. Under the non-central distribution the probability of drawing a term depends on the outcome of the previous draw, while under the central hypergeometric distribution, terms can be sampled independently [3, 18, 28].

Let us first describe the specific form of language model that we consider in this paper, which builds on the query unigram model model proposed in [42]. This model postulates that the relevance of a document to a query can be measured by the probability that the query is generated by the document .

Consider a query q and a document collection C of N docu-ments, C := { d l } l =1 ,...,N , with both queries and document being represented as vectors of indexed term counts: where q i is the number of times the term i appears in the query and V is the size of the vocabulary. Let us also define the length of a query ( n q ) and of a document ( n l ) as the sum of their components: n
The multivariate central hypergeometric distribution is derived from the observation that since the sampling is done without re-placement, the unordered sample is uniformly distributed over the combinations of size n q chosen from d l : Terms are sampled independently or simultaneously, reflecting the term independence assumption.
What if terms had an additional property that affected their prob-ability of being sampled, for example, in how many documents they occur? In the urn model, we can think about objects that, except of their different color, can be heavier or bigger than others. This ad-ditional property can bias sampling and can be modeled as a weight for each object type. We call this weight  X  i for the i th term in the vocabulary.

Under the multivariate non-central hypergeometric distribution the probability of sampling a term depends on the terms sampled so far and also on the remaining terms in the urn. Further, it sup-ports biased sampling allowing for the incorporation of global term weights directly in the probability calculation. The following for-mula describes the distribution: where  X  =  X   X  ( n l  X  n q ) = P V i =1  X  i ( d l,i  X  q i bias of q i after every draw, and the integral stands for the recursive sampling from time t = 0 until all terms are sampled at t = 1 .
The mathematical derivation, properties and efficient computa-tion methods of the Wallenius X  distribution are beyond the scope of this paper. Wallenius [38] provides in-depth information on the characteristics of the non-central distribution and Fog [13] presents efficient methods for sampling from it. The central and non-central hypergeometric distributions are connected in that when  X  all i , then bias is cancelled and the non-central hypergeometric dis-tribution degenerates into the central hypergeometric distribution.
Now that we have presented the hypergeometric distributions, let us look at an illustrative example on how sampling with and without replacement can lead to different results when the sample size is close to the population size. We start with a query (sample) and we need to calculate the probability of a document (population) to generate the query. In the case of sampling with replacement , the probability of sampling a query term t from a document D follows the binomial distribution: 4 with parameters n,p where n is the number of trials (query size), quency of t in D , and k is the number of successes, i.e., the term frequency of t in Q . In the case of sampling without replacement the probability of sampling a query term t from a document D fol-lows the hypergeometric distribution: with parameters m,n,N where m is the term frequency of t in D , n the number of draws (query size), N the population size (doc-ument size), and k the number of successes, namely the term fre-quency of t in Q .

For our example, we let query Q have 4 terms, each occurring once, and also we define two documents A and B of length 1,000, and 15, respectively which share at least one common term with Q . Let also that query term t occurs 1 time in A and B . The probabil-ity of sampling t from A or B when we sample with replacement is given by (3) with k = 1 , n = 4 , p A = 1 / 1 , 000 , and p The calculations result in values of 0.003988 for document A and 0.216810 for B . Similarly, when sampling without replacement, we use (4) and set k = 1 , m = 1 , n = 4 , N A = 1 , 000 , and N
B = 15 . This results in values of 0.004000 for A and in 0.266666 for B . These numbers show that the difference in probability from the two models for document A is negligible ( 1 . 2  X  10  X  5 the population is close to the sample size (document B ), the differ-ence grows three orders of magnitude reaching 0.049. The example illustrates that when queries are of comparable size to the retrieved
We use the binomial distribution instead of the multinomial for simplifying the calculations in our example. documents, sampling with replacement can lead to poor likelihood estimates with a cumulative negative effect in the multivariate case, i.e., we calculate probabilities for all query terms.

What is the upshot? It is known that the multinomial approx-imates the central hypergeometric as the population size remains many times larger than the sample size, i.e., when the document is much longer than the query. In the RAF task, this assumption is vi-olated as queries and documents are expected of roughly the same size. This motivates us to derive retrieval models based on hyper-geometric modeling of documents instead of multinomial models.
Before deriving retrieval models based on hypergeometric mod-eling of documents, we revisit (1) and (2). We identify three con-straints emerging from these equations, which relate to smoothing and play a role in the design of a retrieval model: 1. only query terms that occur in the document contribute to the 2. the query should be shorter than the document, 3. the frequency of a query term should be lower than or equal The first constraint is obvious. The other two stem from the fact that is impossible to draw more terms than currently exist in the urn. The second constraint is imposed from the denominator n l becomes zero when n q &gt; n l and results in infinite probability. The third constraint roots in d l,i q results in zero probability. In general, P ( q ) is positive only if To address the three constraints listed above, we consider two types of smoothing. The performance of retrieval models that build on top of hypergeometric distributions is sensitive to the employed smoothing strategy, just like other retrieval models are that build on the multinomial or other distributions. In the following two subsec-tions we present two approaches to smoothing. The first approach is somewhat related to relevance feedback and an estimated doc-ument model is trained on text from both the query and the doc-ument; this approach works for both the central and non-central hypergeometric distribution. The second approach is more elabo-rate and is based on bayesian inference; this approach works only for the central hypergeometric distribution, as we explain below.
Our first approach to overcome the limitations on q i , n document, is basic in terms that no sophisticated smoothing meth-ods are involved for estimating the parameters of the document model. In a sense, it is remotely related to pseudo-relevance feed-back but instead of re-estimating the query model from pseudo-relevant documents, the documents models are complemented with information from the query. One way to visualize the process is to think of a bag with query terms from which we sample the query. Obviously, the probability of sampling the query from the bag is 1. Now, for a document in the collection we add the document terms in the bag and sample the query again. Documents with high vocabulary overlap with the query will result in high probability while documents with only few common terms will result in low probability.

In particular, instead of sampling the query directly from the doc-ument, we derive a hypothetical document d 0 which is a mixture of the query q and the document d : d 0 := ( d 0 where r q ,r d are parameters for regulating the mixture. The length of this hypothetical document is: n 0 l = P i r q q i + r r some of the terms are always sampled from d 0 l (i.e., those originat-ing from the query), but never all of them because n q &lt; n inition of d 0 l . The extreme case of P  X  ( q ; n q ,n 0 when there is no vocabulary overlap between d l and q and r however, this case is hardly encountered in practice because docu-ments without common terms are excluded from ranking.

Document length and the vocabulary intersection between the query and the document both play an important role in the proba-bility outcome, as in other retrieval models. To this end, we normal-ize the probability given the observation that the probability should maximize when the document is an exact duplicate of the query, i.e., q = d l : Given this observation, documents able to generate the query with probability close to the maximum should be favored. We express this in the following ranking function: The denominator can be ignored for ranking since it is constant for all documents. The expression of P max  X  holds when we look at finding near or exact duplicates of a query. Under this scenario, query terms are expected to occur in a candidate  X  X uplicate X  docu-ment in relatively similar frequencies. However, this is hardly true in other settings where retrieved documents can deviate consider-ably from the query in both vocabulary and term frequencies.
In this respect, the assumption we made for deriving (6), can be too strict. The assumption can be relaxed if we only take into ac-count terms common to the query and to the document and compute the maximum probability based on those. Similarly as before, first we derive a hypothetical document: with length n 00 l = P i d 00 l,i . Further, we also reduce the original query to a hypothetical query q 00 that consists of terms common to q and d l : This results in the following definition of maximum probability, previously defined in (6): and the ranking function in (7) becomes: In this representation, P 0 max  X  cannot be ignored because it is de-pendent on the vocabulary overlap of the query and the document.
A second approach to overcome the limitations on q i , n q at the start of this section, is to use bayesian inference. Recall that when documents are modeled as a multinomial distribution of terms, and we apply bayes X  rule, the conjugate prior distribution to the multinomial is the Dirichlet distribution [41, 42]. Setting the parameters of the Dirichlet distribution accordingly, leads to the well known Dirichlet smoothing method. Here, we follow the same line of reasoning for the multivariate central hypergeomet-ric, and arrive at the Dirichlet compound multinomial distribution (DCM, also known as the multivariate Polya distribution) for es-timating the parameters of a document model. To the best of our knowledge no closed form is known for the conjugate priors of the non-central hypergeometric distribution; hence, we do not offer a bayesian non-central hypergeometric model.
 Now, let us consider that terms t = ( t 1 ,...,t i ,...,t from a multivariate central hypergeometric process where parame-ter n N , the vocabulary length, is known ( n N = P N l =1 and n N &gt; 0 ) and  X  l = (  X  l, 1 ,..., X  l,V ) , the vector of term frequen-cies in the vocabulary that make up the population, are unknown ( 0  X   X  l,i  X  n l and P i  X  i = n l ).

Under this model, the probability of generating a particular query q with counts q is given by: In the case where documents consist of all vocabulary terms, we can obtain the point estimate  X  l,i = d l,i . However, such documents rarely exist. Rather than find a point estimate for the parameter vector  X  l , a distribution over  X  l is obtained by combining a prior distribution over the model parameters P (  X  l ) with the observation likelihood P ( d l |  X  l ) using Bayes X  rule: where the observation likelihood is given by: The conjugate prior of a multivariate hypergeometric process is the DCM with hyperparameters H , an integer greater than zero, and  X  = (  X  1 ,..., X  i ,..., X  V ) where  X  i &gt; 0 and P V i =1 where  X  i &gt; 0 and P V i  X i = n l . The resulting posterior distribution is also DCM: with  X  i &gt; 0 and P V i =1  X  i = H .
 The query likelihood then becomes: A standard approximation to the Bayesian predictive distribution P ( q | d l ) is the use of the maximum posterior (MP) distribution. The approximation consists of replacing the integral in (15) with its maximum value [41, 42]: Although, there is no closed form solution for the maximum like-lihood estimate  X  i of DCM [40], we can use the expected value of  X  [20, p.80]: Following [42], we assign  X  i =  X P ( i | C ) where  X  is a parameter and P ( i | C ) is the probability of the i th term in the collection and the equation above becomes:
The derivation of DCM from the central hypergeometric distri-bution is important, because it establishes a similar link to that be-tween multinomial and Dirichlet smoothing. In this respect, the use of DCM is expected to result in positive performance differ-ences over Dirichlet when the sample size is close to the population size but these differences will become smaller when the sample is a small fraction of the population. Indeed, Elkan [12] compared the performance of DCM and the multinomial for document clustering (sample and population are expected to be of comparable size) with results favoring DCM. Xu and Akella [40] introduced a probabilis-tic retrieval model with experiments on ad hoc retrieval (when the sample is just a fraction of the population size) using Dirichlet and DCM smoothing with results that favour DCM, but small, although statistically significant, differences.
We present our research questions, experiments, dataset and eval-uation method. For the purpose of finding instances of articles that have been published in one source and republished more or less verbatim in another, we choose to focus on a single target source in our experimental evaluation, namely the blogosphere. This choice is based on the fact that blog posts unlike status updates or mi-croblog posts, can be of arbitrary length and therefore they can be verbatim copies of a news article.
In addressing the RAF problem in both its strict and loose inter-pretation, we concentrate on the retrieval effectiveness of the hy-pergeometric retrieval models for finding how news content propa-gates in the blogosphere. In this respect our goals are comparable to those of [19, 22, 23, 34]. In particular, we want to know the ef-fectiveness of our log odds hypergeometric retrieval models and of bayesian hypergeometric retrieval model, both for finding repub-lished articles.

To answer these research questions, we compare our methods to seven state-of-the-art retrieval methods listed in Table 1. Among them, simhash is one of the best-performing near-duplicate detec-tion methods [17, 26]; kl has proven successful in plagiarism detec-tion [5]; cosine , probabilistic, and language modeling based meth-ods have performed well in the related topic detection and track-ing [2] task.
 In our experiments we use the Indri framework for indexing. Each experimental condition returns maximum 1,000 results. For parametric retrieval models we find parameter values that optimize their performance for our dataset. We set  X  = 1120 for kl , lm , indri , hgm-central-bayes , r q = 1 ,r d = 1 for hgm-central , and hgm-noncentral , and k 1 = 2 . 0 ,b = 0 . 75 for bm25f . For hgm-noncentral we set  X  i , to the term X  X  inverse document frequency (IDF).

The data set that we use as our target social media collection is the Blogs08 collection provided by TREC; the collection consists of a crawl of feeds, permalinks, and homepages of 1.3M blogs dur-ing early 2008 X  X arly 2009. This crawl results in a total of 28.4M blogs posts (or permalinks). We only used feed data, the textual content of blog posts distributed by feeds and ignored the perma-links. Only using feed data is common practice and requires almost no preprocessing of the data. Extracting posts from the feed data gave us a coverage of 97.7% (27.8M posts extracted). As a second preprocessing step we perform language identification and remove all non-English blog posts from the corpus, leaving us with 16.9M blogs posts. Our index is constructed based on the full content of blog posts.

Our news article dataset is based on the headline collection from the top stories task in TREC 2009. This is a collection of 102,812 news headlines from the New York Times that includes the article title, byline, publication date, and URL. For the purposes of our experiments we extended the dataset by crawling the full body of each of the articles.
As there is no standard test collection for the republished article finding task, we created our own. 5 The ideal ground truth for our task would consist of tuples ( n,s ) consisting of a news article and a social media utterance, where s is a republication of n .
As a proxy, we follow [15, 27, 29] and use blog posts that are explicitly linked to a given news source. Our ground truth is assem-bled in two phases. First, for each news article we find blog posts that include the article X  X  URL. Second, for each discovered blog post we look for other blog posts that include its URL. The pro-cess continues recursively until no more blog posts are discovered. For our experiments we sample headlines with more than ten ex-plicit links and where social media possibly plays a role. For each news article, we take only explicitly linked blog posts within  X  1 day from the article X  X  publication date to reduce the search space.
In the second phase, we removed the explicit links and for each (backlinked) blog post we manually examined whether it is a re-publication of the news article. In the strict interpretation of the RAF task, the blog post needs to be a copy all of the material from the source news article, possibly interleaved with comments etc.
The ground truth may be retrieved from http://ilps. science.uva.nl/resource/hypergeometric-lm Table 2: Relevance assessments for strict and loose interpreta-tions of the RAF task.
 Table 3: System performance for the strict interpretation of the RAF on 160 news articles using three hypergeometric models, and seven other retrieval methods. Significance tested against simhash . In the loose interpretation our assessors made sure that a key part of the source news article was republished in the blog post (e.g., a highly informative title, the news articles X  X  lead or a central para-graph). Two assessors created this ground truth and discussed any differences they encountered until agreement was reached. See Ta-ble 2 for details of the resulting test collection; recall that in this paper, news articles are the queries that are submitted against an index of blog posts.

We report on standard IR measures: precision at 5 (P@5), mean reciprocal rank (MRR), mean average precision (MAP), and r-pre-cision (Rprec). Statistical significance is tested using a two-tailed paired t-test and is marked as N (or H ) for significant differences for  X  = . 01 , or M (and O ) for  X  = . 05 .
In this section, we report on the results of our experiments and conduct an analysis of their outcomes.
 Strict interpretation. In our first experiment we study the retrieval effectiveness of our methods with regards to the strict interpretation of the RAF task. To this end, we choose simhash , the state-of-the-art for near-duplicate detection, as our baseline. The performance of three hypergeometric models, and seven retrieval models is listed in Table 3. We see that hgm-central and hgm-noncentral out-perform the baseline with statistically significant differences in all metrics. Second and third best (in terms of MAP) come bm25f and cosine similarity with small differences between them; kl , hgm-central-bayes , lm , and indri follow with performance that hovers at the same levels. In general, all methods show strong performance in all metrics, with an exception for tf  X  idf .

Turning to individual metrics, we find of particular interest Rprec and MAP. For hgm-central Rprec peaks at 0.8160, 20% more than for simhash . In terms of MAP, hgm-central achieves the best score at 0.8874, a 14% improvement over the baseline. With re-gards to other language modeling based methods, hgm-central outperforms kl , lm , indri (statistically significantly so, in MRR, Rprec, and MAP). In terms of early precision (P@5), all meth-ods show similar performance, which is mainly due to the small number of relevant documents per news article.

To better understand the differences between hgm-central and simhash , we look at per topic differences in average precision. Fig. 1 shows that out of 160 articles, 45 favor the use of hgm-central , and 9 simhash . Manual inspection of the results revealed that hgm-central is able to account for small changes in language: For example, if the title of the republished article had been changed in the blog post, then, according to hgm-central , this blog post will rank lower than a blog post where the title was kept the same as the original. simhash seems unable to capture these differences. This is partially due to its nature which although allows document com-pression which improves efficiency, it looses in precision. Another finding was the robust ranking capabilities of hgm-central even in lower ranks: blog posts there used only a couple of sentences from the original article. In contrast, ranked lists from simhash were polluted quite early (rank 10), with long documents that are irrele-vant to the article, but that do share language with the article; this is in line with findings in [34].

Turning to hgm-central and lm , we find no striking differences in the resulted ranked lists. Differences in MAP are mainly due to how the ground truth is constructed. More specifically, there exist topics for which either method is penalized because the first rank-ing document is not assessed, however, found relevant after man-ual inspection. In general, lm was found to rank blog posts higher that contain either short excerpts of the article without commen-tary, or blog posts that are verbatim copies of the article with lots of commentary. This behavior can be explained by the accumulation of term probabilities using Dirichlet smoothing: probability mass is assigned to terms occurring in the original article. We see that hgm-central counters this problem with the use of P 0 max ensures that documents are ranked by how much the blog post  X  X e-viates X  from the original article. (a) hgm-central vs. simhash Figure 1: Per topic difference in average precision (AP) for the strict RAF task.
 Loose interpretation. In our second experiment we test retrieval methods with regards to the loose interpretation of the RAF task. We set our baseline to hgm-central as it proved the best perform-ing method in the previous experiment. Results in Table 4 show that when we move away from near-duplicates, retrieval effective-ness drops for all methods. hgm-central achieves the best scores overall, followed by bm25f in MRR, and lm , indri , kl in MAP. In this interpretation of the RAF task, simhash , our previous base-line, is one of the least effective along with tf  X  idf .
Looking at the results in more detail, hgm-central shows ro-Table 4: System performance for the loose interpretation of the RAF task of 404 news articles using three hypergeometric models, and seven other retrieval methods. Significance tested against hgm-central . bust performance in MRR which is statistically significant over the rest of retrieval methods. hgm-noncentral shows marginally bet-ter results in terms of P@5, MRR, and Rprec over hgm-central at the cost of MAP. Finally, we find interesting that bm25f used to outperform language modeling based methods in our first ex-periment, however, in the current scenario we observe the oppo-site. This change can be ascribed to the parameter estimation of the models, which is related to the nature of the relevant documents. hgm-central , and hgm-noncentral as parameter free models are not as sensitive to changes in the notion of  X  X elevance. X  Document length. Finally, we examine our hypothesis on the ef-fect of document length (population size) and query length (sam-ple size) in retrieval effectiveness between modeling documents as hypergeometric and multinomial distributions of terms. Fig. 2 il-lustrates the correlation of MAP, MRR, and the length of relevant documents over query length, for hgm-central and lm . Hyperge-ometric document modeling shows to have strong positive effects in both metrics when document length is up to 0.1 times the query length. As the query and the document length become equal, the differences between the hypergeometric and the multinomial di-minish.

Our experimental results demonstrate the utility of hypergeomet-ric retrieval models for the republished article finding task in both its strict and loose interpretation. Figure 2: Moving average (window of 30) of MRR (left), and of MAP (right) over the ratio of average relevant document length and query length.
So far we have examined how different retrieval models perform on the two interpretations of the RAF task. In this section, we Table 5: System performance on the loose interpretation of the RAF task using the log odds retrieval model and changing the underlying distribution to: multinomial, multivariate central hypergeometric, and multivariate non-central hypergeometric distribution. The parameters r q ,r d are set to 1. Significance tested against the multinomial . take a closer look at the distributions used for document modeling, namely, the multinomial and the hypergeometric and conduct a di-rect comparison of them by keeping the retrieval model the same and changing the underlying distribution. Further, we study the log odds retrieval model by experimenting with document/query repre-sentations, such as TF and TF  X  IDF, and with different mixture ratios r ,r d (see Section 3). Finally, we explore the use of hgm-central , hgm-noncentral and hgm-central-bayes in ad hoc retrieval.
We are interested in exploring the validity of our hypothesis that hypergeometric document models are superior to multinomial ones when the query size is comparable to document length. We proceed as follows. For each of the two retrieval models we presented, i.e., log odds and bayesian, we create two runs, one using the multivari-ate hypergeometric distribution and one using the multinomial dis-tribution. Keeping the same retrieval model and smoothing method and varying the underlying distribution, ensures that any observed differences in performance are solely due to the change in the un-derlying distribution. For our experiments we use the dataset from loose interpretation of the RAF task.
 Log odds. We use the log odds retrieval model with the param-eters r q ,r d set to 1, and different underlying distributions: multi-nomial ( multinomial ), multivariate central hypergeometric ( hgm-central ), and multivariate non-central hypergeometric ( hgm-non-central ). Results in the top half of Table 5 validate our hypothesis. Log-odds document models built on hypergeometric distributions outperform models built on the multinomial distribution. In partic-ular, both hgm-central , and hgm-noncentral outperform multi-nomial in all metrics with statistically significant differences. Dirichlet vs DCM. We compare the performance of Dirichlet smoothing on the multinomial distribution (unigram language model) and of DCM on the multivariate central hypergeometric. The smooth-ing parameter  X  was found to peak at 1120 for both models when optimized for MAP. Table 5 (bottom) lists the results. Performance hovers at the same levels for both models, with DCM showing bet-ter r-precision with statistically significant difference. This can be attributed to the ability of DCM to capture word burstiness better than the Dirichlet [40] which leads to high early precision.
We look at different mixture ratios r q ,r d for hgm-central and hgm-noncentral ; see (5). Table 6 shows that, on average, perfor-mance degrades as we deviate from r q = 1 ,r d = 1 . When r Table 6: System performance using log odds retrieval model and tf  X  idf for document and query representation, and several mixture ratios r q ,r d . Significance testing against hgm-central with TF, and r q ,r d set to 1. runID Weight r q r d P@5 MRR Rprec MAP Mixture ratios r q ,r d hgm-central TF 1 1 0.5446 0.7612 0.4642 0.4413 hgm-central TF 1 2 0.5525 0.7576 0.4721 N 0.4382 O hgm-central TF 2 1 0.5198 H 0.7251 H 0.4189 H 0.3611 H hgm-central TF 3 5 0.5356 0.7338 H 0.4436 H 0.3908 H hgm-noncentral TF 1 2 0.5515 0.7536 0.4670 0.4238 hgm-noncentral TF 2 1 0.5173 H 0.7261 H 0.4172 H 0.3620 hgm-noncentral TF 3 5 0.5351 0.7307 H 0.4428 H 0.3886 tf  X  idf representation hgm-central TF  X  IDF 1 1 0.4238 H 0.7097 H 0.2912 H 0.2435 hgm-noncentral TF  X  IDF 1 1 0.4861 H 0.7297 H 0.3581 H we observe a slight increase for some metrics at the cost of a lower MAP. In particular, hgm-central shows a statistically significant increase in Rprec.

Next, we explore the effect on performance of using global term weights, such as TF  X  IDF, instead of TF, for the representation of the hypothetical document d 0 ; see (5). The results in Table 6 (bot-tom) show that the use of TF  X  IDF leads to a significant decrease in performance for all metrics. Manual inspection reveals that the returned documents are very short, nearly one sentence long. The document size remains small, and comparable to two or three sen-tences until the end of the rank list. For the topics we examined at, the top ranked document is usually relevant, however, in most cases it is not assessed.
Finally, we look at the performance of our log odds and bayesian retrieval models in ad hoc retrieval. For our experiments, we use TREC-Robust 2004. We formulate our queries using content from the title of each topic. The Dirichlet smoothing parameter  X  is set to 1000. Table 7 shows results for indri (baseline), hgm-central-bayes , hgm-central , and hgm-noncentral . We see that hgm-central-bayes shows the same performance as the baseline. Runs based on the log odds retrieval model prove least effective. The rea-son lies in the value of P 0 max  X  , which becomes 1 when the query and the document share only one common term X  X hich is common for short queries. Without the normalization factor, and enough in-formation from the query, the performance of the log odds model depends on d 0 which is mainly estimated from the document (given the negligible effect from the query due to its short length). To this end, the more elaborate smoothing methods, used in indri , and hgm-central-bayes prove most effective.
 The three analyses that we performed in this section establish the following. The hypergeometric distributions are a better choice over the multinomial for modeling documents, when the system has to respond to document long queries. The Dirichlet and DCM smoothing show similar performance, with the later producing bet-ter early ranking. Further, retrieval effectiveness benefits the most from document representations that use raw term frequencies (TF), and equal mixture ratios r q , r d . Finally, with regards to ad hoc retrieval, retrieval models based on bayesian inference deliver the best performance.
 Table 7: System performance on the TREC-ROBUST 2004 col-lection. Significance tested against indri .
Our task, republished article finding, is parallel to the tasks of text reuse which, in turn, relates to near-duplicate detection. Near-duplicate detection. Garcia-Molina et al. [14] introduces the problem of finding document copies across multiple databases. Manku et al. [26] adopt simhash, a document fingerprinting method and hamming distance for efficient near-duplicate detection in web crawling; we used simhash as a baseline in our comparisons. Chang et al. [9] focus on finding event-relevant content using a sliding window over lengths of sentences. Muthmann et al. [31] discover near-duplicates within web forums for grouping similar discussion threads together. They construct a document X  X  fingerprint from a four dimensional vector which consists of domain (in-)dependent text-based features, external links, and semantic features. Kolak and Schilit [23] find popular quoted passages in multiple sources, and use them to link these sources. Abdel-Hamid et al. [1] de-tect the origin of text segments using shingle selection algorithms. Zhang et al. [43] use two stage approach for finding partial dupli-cates with applications to opinion mining and enhanced web brows-ing: sentence level near-duplicate detection (Jaccard distance) and sequence matching; the tasks considered in this paper are similar to ours, however, the authors focus on pruning techniques, while we aim at discovering effective and robust methods, the output of which needs little, if any, further processing.
 Text re-use. Broder [8] introduces the mathematical notions of  X  X esemblance X  and  X  X ontainment X  to capture the informal notions of  X  X oughly the same X  and  X  X oughly contained X  and propose ef-ficient methods using document fingerprinting techniques. These notions correspond to our  X  X trict X  and  X  X oose X  interpretations of the republished article finding task. Seo and Croft [34] compare a set of fingerprinting techniques for text reuse on newswire and blog collections. One of their findings, which we also share, is how text in blogs layout affects the performance of fingerprinting methods. Kim et al. [22] propose an efficient overlap and content reuse de-tection in blogs and news articles. They find that blog posts contain large amount of exact quotations from the news articles. However, for the particular task, they find that blog posts raise significant challenges against retrieval [21]. Bendersky and Croft [6] consider the issue of text reuse on the web. They address the task using three methods: word overlap, query likelihood, and mixtures mod-els. This work is of particular interest to us, as we focus on better understanding the effectiveness of query likelihood using hyperge-ometric document models.
 Hypergeometric distributions. The univariate central hypergeo-metric distribution has been firstly used in the past to provide a theoretical framework for understanding performance and evalua-tion measures in IR [11, 35], and for proving the document-query duality [10].

Wilbur [39] was the first to use the central hypergeometric distri-bution in a retrieval setting. The vocabulary overlap of two docu-ments is modeled as a hypergeometric distribution for determining the relevance to each other. Wilbur X  X  model initially ignored lo-cal and global term weights, such as term frequencies within doc-uments or term document frequency. Term weights are integrated into the final score only later through multiple iterations of the main model. Our retrieval models are able to support local and global term weights in a straightforward manner.

More recently, Bravo-Marquez et al. [7] derived a query reduc-tion method for document long queries using the extended hyper-geometric distribution. Amati [3] used the central hypergeometric distribution within the Divergence from Randomness (DFR) frame-work for deriving the binomial distribution, a readily accepted dis-tribution for the generative model. Amati X  X  model has applications in query expansion [16], pseudo-relevance feedback [4], and enter-prise search [24].
We looked at the task of republished article finding (RAF), to dis-cover springboards of discussion in social media related to a news article. Our approach is to find verbatim or near-verbatim copies of the news article building on the language modeling paradigm. Our task is related to near-duplicate detection with the additional chal-lenge that in our scenario, users can inject comments in between excerpts from the original article. To this extent the documents to be retrieved can deviate considerably from the original article.
In the process of tackling the problem, we revisited the assump-tions made in unigram language model, namely, using the multi-nomial distribution for modeling documents. We presented two retrieval models using the hypergeometric distributions, one task-driven (log odds), and one more elaborate using Bayesian infer-ence. In the later, we found that the Dirichlet compound multi-nomial distribution (DCM) arises naturally for estimating the pa-rameters of a document model. This is an important finding be-cause it links central hypergeometric to DCM as multinomial is linked to Dirichlet. DCM has been derived in the past from hier-archical bayesian modeling techniques as a better model to Dirich-let [12, 25, 40].

Our experiments on finding republished news articles in the blo-gosphere demonstrate the utility and effectiveness of modeling doc-uments using hypergeometric distributions. We found that our log odds retrieval model is most useful for documents whose size is similar to the query size.

In future work, we envisage to study more in depth different smoothing methods suitable for the hypergeometric distributions and compare them to the multinomial case. Such methods can be challenging to find as they need to meet the requirements set by the hypergeometric distribution, namely, the smoothed estimates need to be larger than those sampled. With regards to the noncentral hy-pergeometric distribution, we aim at exploring more elaborate ways of incorporating term bias, such as term co-occurence between the document and query. In the long term, we believe that our meth-ods based on hypergeometric distributions hold promise to support grouping of individual news stories into topics, providing support for impact analysis.

Finally, our republished article finding task was formulated in the setting of online reputation management (ORM). ORM is re-lated to search engine optimization, but the two do not coincide and their goals differ widely. ORM gives for a number of recall-oriented retrieval tasks: republished article finding is one, dealing with  X  X reative X  name variants and implicit references to a given target in social media is another important example.
 Acknowledgments. This research was partially supported by the European Union X  X  ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, the PROMISE Network of Excellence co-funded by the 7th Framework Programme of the Eu-ropean Commission, grant agreement no. 258191, the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.061.814, 612.061.815, 640.004.802, 380-70-011, the Center for Creation, Content and Technology (CCCT), the Hyperlocal Service Platform project funded by the Service Innova-tion &amp; ICT program, the WAHSP project funded by the CLARIN-nl program, and under COMMIT project Infiniti. [1] O. Abdel-Hamid, B. Behzadi, S. Christoph, and M. R. [2] J. Allan, editor. Topic detection and tracking: event-based [3] G. Amati. Frequentist and bayesian approach to information [4] G. Amati. Information theoretic approach to information [5] A. Barr X n-Cede X o, P. Rosso, and J.-M. Bened X . Reducing the [6] M. Bendersky and W. B. Croft. Finding text reuse on the [7] F. Bravo-Marquez, G. L X  X uillier, S. R X os, and J. Vel X squez. [8] A. Broder. On the resemblance and containment of [9] H.-C. Chang, J.-H. Wang, and C.-Y. Chiu. Finding [10] L. Egghe and R. Rousseau. Duality in information retrieval [11] L. Egghe and R. Rousseau. A theoretical study of recall and [12] C. Elkan. Clustering documents with an exponential-family [13] A. Fog. Calculation methods for wallenius X  noncentral [14] H. Garcia-Molina, L. Gravano, and N. Shivakumar. dscam: [15] S. Geva and A. Trotman. Inex 2010 Link-The-Wiki Track, [16] B. He and I. Ounis. Combining fields for query expansion [17] M. Henzinger. Finding near-duplicate web pages: a [18] D. Hiemstra and W. Kraaij. Twenty-one at TREC-7: ad-hoc [19] D. Ikeda, T. Fujiki, and M. Okumura. Automatically linking [20] N. L. Johnson, S. Kotz, and N. Balakrishnan. Discrete [21] J. Kim, K. Candan, and J. Tatemura. Organization and [22] J. W. Kim, K. S. Candan, and J. Tatemura. Efficient overlap [23] O. Kolak and B. N. Schilit. Generating links by mining [24] C. Macdonald and I. Ounis. Using relevance feedback in [25] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word [26] G. S. Manku, A. Jain, and A. Das Sarma. Detecting [27] R. Mihalcea and A. Csomai. Wikify!: linking documents to [28] D. R. H. Miller, T. Leek, and R. M. Schwartz. A hidden [29] D. Milne and I. H. Witten. Learning to link with Wikipedia. [30] D. S. Moore. The Basic Practice of Statistics . W. H. [31] K. Muthmann, W. M. Barczy  X  nski, F. Brauer, and A. L X ser. [32] J. M. Ponte and W. B. Croft. A language modeling approach [33] S. E. Robertson. The probability ranking principle in IR , [34] J. Seo and W. B. Croft. Local text reuse detection. SIGIR [35] W. M. Shaw, R. Burgin, and P. Howell. Performance [36] E. Tsagkias, M. de Rijke, and W. Weerkamp. Linking online [37] E. M. Voorhees and L. P. Buckland, editors. Proceedings of [38] K. T. Wallenius. Biased sampling; the noncentral [39] W. J. Wilbur. Retrieval testing with hypergeometric [40] Z. Xu and R. Akella. A new probabilistic retrieval model [41] H. Zaragoza, D. Hiemstra, and M. Tipping. Bayesian [42] C. Zhai and J. Lafferty. A study of smoothing methods for [43] Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Efficient
