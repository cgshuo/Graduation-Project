 1. Introduction
With the advent of the vast pools of information and documents in large enterprise organisations, collaborative users reg-ularly have the need to find not only documents, but also people with whom they share common interests, or who have spe-cific knowledge in a required area. In an expert search task, the user X  X  goal is to identify people who have relevant expertise to a given topic of interest. This can be accomplished, for instance, by inspecting the organisation X  X  intranet for documents relevant to this topic. Indeed, modern expert search engines usually work as a two-layer retrieval process, with an initial search for relevant expertise evidence for a topic of interest from a document collection, and a subsequent ranking of can-didate experts associated to the documentary evidence retrieved in the initial step ( Bailey, Craswell, de Vries, &amp; Soboroff, 2007; Balog et al., 2008; Craswell, de Vries, &amp; Soboroff, 2005; Soboroff, de Vries, &amp; Craswell, 2006 ).
An intranet corpus, however, generally reflects the view of the organisation that it serves X  X ontent generation often tends to be autocratic or bureaucratic rather than democratic ( Fagin et al., 2003 ). Such a centralised publishing behaviour may pose additional difficulties for finding experts in a particular subject area among all employees within the organisation ( Mukherjee &amp; Mao, 2004 ). On the other hand, nowadays, many employees may publish content outside the domains of their organisation. For instance, employees may contribute articles to conference proceedings or journals; alternatively, they may participate in public forums or submit posts or comments to blogs. This means that there may be sufficient expertise evidence available on the Web for searching for company experts.

A natural source for looking for experts on the Web is to query existing Web search engines (WSEs) for evidence support-ing the expertise of a set of candidates on a particular topic of interest. Unfortunately, however, WSEs are not tailored expert search systems, and hence we cannot measure their performance at an expert search task directly. An alternative would be to use the WSE result listing as the first layer in an expert search system. However, this is difficult because we cannot make  X 
WSEs directly target subsets of the Web (e.g., those containing expertise evidence for a set of candidates) through their public interfaces. Moreover, WSEs do not provide relevance scores for their retrieved documents, which are often necessary order to leverage the full potential of WSEs for expert search, we propose to simulate their use as expert search systems themselves.
 In this article, our major contributions are two-fold:
Firstly, we introduce a novel approach for expert search that builds upon the assumed quality of WSEs X  ranking mecha-nisms by explicitly trying to mimic their rankings of suitable documentary expertise evidence.

Secondly, we conduct a comprehensive experimental investigation on the effectiveness of this approach for expert search in the context of the Expert Search task of the TREC 2007 and 2008 Enterprise tracks.

In contrast with previous approaches, we not only make use of the expertise evidence identified by WSEs, but also of how important this evidence is considered to be based on how it is ranked by individual WSEs. In particular, we experiment by querying seven different WSEs to gather expertise evidence for a set of candidate experts from the TREC CERC collection in dence is then indexed locally and scored using four distinct document ranking approaches so as to try to mimic each WSE X  X  underlying scoring mechanism, building what we call a pseudo -WSE. By assigning scores to individual documents, we can estimate the extent to which a given documentary evidence is on-topic, and how much it contributes to the relevance of a given candidate. By training the pseudo-WSEs in order to refine the mimicking of the corresponding WSEs, this estimation can be further improved. Finally, this enriched expertise evidence is aggregated into a ranking of candidate experts using an effective expert search model.

Our results using TREC data attest the effectiveness of employing mimicked WSEs X  document rankings in replacement to or in combination with existing intranet evidence for expert search. Moreover, by training the pseudo-WSEs to better repro-duce the original WSEs X  rankings, we find that additional improvements can be attained, often significantly. Finally, by using the titles and snippets extracted from the WSEs X  results instead of downloading the corresponding full documents, we show that higher retrieval performances can be achieved while drastically reducing the network overhead incurred by our approach, was it to be deployed in a typical enterprise setting.

The remainder of this article is organised as follows. In Section 2 , we describe related work. In Section 3 , we show how expertise evidence from WSEs X  document rankings can be mined and effectively mimicked by pseudo-WSEs. In Section 4 ,we describe the experimental setup used in our investigations. In Section 5 , we assess the impact of different mimicking strat-egies for reproducing the document rankings of different WSEs. In Section 6 , we evaluate our approach for leveraging exper-tise evidence from different WSEs for expert search. In Section 7 , we evaluate the effectiveness of combining this external evidence with intranet expertise evidence. In Section 8 , we investigate the usefulness of titles and snippets as an alternative to downloading the full content of the WSEs X  retrieved results. Finally, in Section 9 , we present our concluding remarks and directions for future work. 2. Related work
In the following, we overview existing approaches to expert search. Motivated by our proposed approach to leverage Web search engines for expert search, we describe an expert search framework that is agnostic to the underlying document rank-ing component. We then discuss how external evidence has been previously used in general search tasks and how it can be effectively used within this general framework. 2.1. Expert search in the enterprise
Expert search has emerged as a knowledge management task for finding people with relevant expertise in a given topic ( Yimam-seid &amp; Kobsa, 2003 ). While several enterprise organisations have developed their own expert search systems, the problem has also gathered considerable interest from the information retrieval (IR) research community, with the advent of the expert search task within the TREC 2005 Enterprise track ( Craswell et al., 2005 ). This task ran until TREC 2008, pro-2005; Soboroff et al., 2006 ).

Modern expert search systems work around the notion of profiles. The profile of a candidate contains documentary evi-dence representing the expertise of this candidate (e.g., documents, articles, project reports, or e-mails authored by or that contain some sort of identification of the candidate). The profiles of all candidates within an organisation can be used to rank these candidates in response to a query according to their expertise to the topic of the query. To this end, a common ap-proach relies on probabilistic models. For instance, Balog et al. (2006) proposed two generative language models for expert search. While their Model 1 directly builds a language model from a candidate X  X  profile, their Model 2 estimates document language models and their association to a candidate. In the same vein, Fang and Zhai (2007) proposed candidate and topic generation models for expert search. Serdyukov and Hiemstra (2008b) also proposed a generative model to represent the documents retrieved for a query as a mixture of candidate langua ge models. Recent works have also focused on discriminative probabilistic models. In particular, Fang, Si, and Mathur (2010b) proposed a learning framework for expert search and derived effective discriminative models for finding experts.

Different from the aforementioned probabilistic approaches, Macdonald and Ounis (2006) proposed an expert search ap-proach inspired by social choice theory and data fusion techniques. In their Voting Model, documents retrieved for a given query that belong to the profile of a candidate are considered as votes for the relevant expertise of that candidate to the topic completely agnostic to the underlying document ranking component. More importantly for our study, it can operate on top of any document search engine, which makes it particularly suitable for experimenting with rankings produced by WSEs.
The Voting Model defines many voting techniques, which convert a single ranking of documents into a single ranking of candidates. Note that this is different from traditional data fusion techniques, which combine multiple rankings of docu-ments ( Macdonald &amp; Ounis, 2006 ). In this article, we use one of the most effective voting techniques of the Voting Model, namely expCombMNZ ( Macdonald &amp; Ounis, 2008 ), which takes into account the number of voting documents associated to a candidate as well as the scores of these documents, transformed by an exponential function. Applying the exponential function has two effects: it removes the logarithm present in many document weighting models and, in doing so, it places more emphasis on the highly scored documents. It is defined as: score ( d , Q ) corresponds to the score of document d with respect to the query Q , as given by the underlying document weight-ing model. Documents in both R ( Q ) and P ( C ) are usually taken from an intranet collection. In the next section, however, we describe related works on expert search that rely on expertise evidence gathered from outside the enterprise sphere. More-over, we discuss how we can improve over these by not only considering this external expertise evidence as such, but also the estimation of its importance as determined by how it is ranked by WSEs. 2.2. External evidence in expert search
External evidence has been traditionally employed in IR for enriching local collections with the primary goal of enhancing the localcollectionsmaynotcontainsufficientrelevantinformationforsomequeriesormaypresenttoonoisytermstatistics,inwhich case retrieving documents from a larger, higher-quality external corpus can provide better feedback documents for expanding such difficult queries. The queries expanded from the external pseudo-relevance feedback documents are then used to retrieve documents from the local collection. Collection enrichment has been also shown to be effective in document search within an enrichment a suitable alternative to traditional query expansion techniques ( Hawking, 2004 ).
 In the context of expert search within an organisation, exploiting external evidence can be particularly advantageous.
Organisations create intranets to facilitate communication and access to information. However, intranet development differs substantially from the Web, which grows democratically. In contrast, an intranet collection is inherently limited in the sense that it serves ( Fagin et al., 2003; Mukherjee &amp; Mao, 2004 ). In such controlled environments, there may be insufficient doc-umentary evidence for ranking relevant candidates for a topic of interest. In this scenario, resorting to external resources can bring useful evidence for finding experts within the enterprise ( Hawking, 2004 ).

In this article, we categorise related approaches on using external evidence for expert search into two main classes: can-didate-centric and document-centric. Candidate-centric approaches mine expertise evidence around the name of a candi-date, whereas document-centric approaches narrow the expertise mining by targeting occurrences of a candidate X  X  name in the context of a set of topics of interest.

While candidate-centric approaches have the advantage of allowing expertise evidence to be mined off-line, they may not be able to target useful evidence from the Web. Indeed, except for candidates with very distinctive names, these approaches are likely to mine misleading evidence associated to homonyms of the actual candidates. Document-centric approaches, on the other hand, perform expertise mining in an on-demand basis, by targeting the relevant portions of the Web for an unseen expertise query, and incrementally build a local expertise base that can be used to answer future queries.

Two main candidate-centric approaches were proposed recently. Jiang, Han, and Lu (2008) investigated the usefulness of expertise evidence gathered from Google. A breakdown of the results retrieved by this WSE for a list of candidate names ex-tracted from the TREC CERC collection confirmed that much more evidence can be retrieved from the Web than from the intranet collection. Moreover, they showed that this evidence can be effectively used in combination with the intranet evi-dence in order to retrieve candidate experts. Their approach was later adapted by Balog and de Rijke (2008) with an alter-native model for aggregating the expertise evidence derived from result snippets retrieved using the Yahoo! WSE.
A document-centric approach was proposed by Serdyukov and Hiemstra (2008a) and later extended by Serdyukov, Aly, and Hiemstra (2008) . In their approach, a query was submitted to six different WSEs and several expertise indicators were derived based on features extracted from the retrieved results. These comprised query-independent features X  X uch as URL depth and domain size X  X nd a simple query-dependent feature, namely, the frequency of the query terms in the URL, title, or snippet of each of the retrieved results. Using any of these features, the ranking produced by the WSE and that produced on the local enterprise collection were then aggregated using the Borda count method from metasearch ( Aslam &amp; Montague, 2001 ). A document-centric approach was also used by He, Macdonald, Ounis, Peng, and Santos (2008) for gathering expertise evidence from the full content of results retrieved using the Yahoo! WSE, however with a limited success.

Table 1 summarises the main characteristics of all these approaches as well as our proposed one. These characteristics include the sources of external evidence, the features considered from each source, and the feature integration strategies deployed by each approach: query-independent (based on result counts or other query-independent features extracted from the WSEs X  retrieved results), query-dependent (based on re-scoring the WSEs X  retrieved results using standard retrieval ap-proaches), or mimicking (based on fitting the retrieval approaches to replicate the WSEs X  underlying ranking mechanism). Additionally, Table 1 organises all approaches as either candidate or document-centric.

Asshownin Table1 ,ourapproachcanalsobeclassifiedasdocument-centric.However,differentlyfromtheaforementioned approaches, we wish to fully exploit the capabilities of WSEs for tackling the expert search problem. Indeed, the major contri-bution of this article is a comprehensive investigation of the several dimensions involved in integrating expertise evidence from WSEs to an expert search system. Our aim is to answer the general question: how well can WSEs do at expert search? As
WSEs are not tailored expert search systems, however, this question cannot be answered directly X  X .g., WSEs do not rank spe-they provide relevance scores to enable an improved usage of such evidence. Instead, in order to benefit from the full potential of WSEs for this task, we propose to simulate their use as expert search systems themselves. In particular, besides mining expertise evidence from WSEs, we make use of the valuable information provided by their underlying ranking mechanisms in order to answer a given expertise information need. By appropriately scoring the results retrieved by WSEs, we can generate therelevancescoresnotprovidedbythem.Moreimportantly,byexplicitlytryingtomimictheiroriginalrankings,wecanbuild an improved expert search system upon their assumed highly effective document rankings. 3. Mimicking Web search engines for expert search
In this section, we describe our approach to expert search, centred on mimicking WSEs X  document rankings for a given expertise information need. Following a general expert search flow, as discussed in Section 2.1 , we first need to build a set of candidate profiles, each profile comprising documentary evidence of the expertise of a candidate for a given set of top-ics. An efficient alternative for mining Web information for a given set of topics is to use WSEs X  Application Programming
Interfaces (APIs) to directly query WSEs. Various WSEs provide programmatic APIs to allow developers to postulate queries and retrieve the associated rankings of URLs which would have been returned by the WSE as for a normal user.
However, if we were to issue an expert search query to a WSE directly, it is likely that no documents related to a particular candidate would be retrieved, primarily due to the large size of the Web. Moreover, the WSEs X  APIs do not provide methods to only rank arbitrary subsets of the Web, i.e., those with documents relevant to assessing the expertise of the candidate to the topics of interest. Instead, by formulating appropriate queries, we can use WSEs X  APIs for deriving an external profile of the expertise of a given candidate to a predefined set of topics. In this work, we follow the document-centric query formulation strategy suggested by Serdyukov and Hiemstra (2008a) in order to generate what we call evidence identification queries . In particular, each evidence identification query contains: the candidate X  X  full name in quotes: e.g.,  X  X  X ohn smith X  X  ; the name of the organisation: e.g., csiro ; the query terms without any quotations: e.g., genetic modification ; a directive prohibiting any results from the actual organisation Web site: e.g., -site:csiro.au .

The use of the name of the organisation helps in name disambiguation by preventing the matching of any content not related to the candidate expert in question (however, this will also prevent the matching of evidence for a candidate from a previous employer). The prohibitive -site directive, on the other hand, ensures that the acquired expertise evidence does not overlap with the intranet collection.

Given a WSE, we issue an evidence identification query through its API and obtain a list of search results. For each re-turned result, we download the corresponding full-text document (as identified by the result URL) and extract its relevant metadata, such as its associated title and descriptive snippet. This content comprises part of the profile of a candidate with respect to a specific topic, as given by the issued evidence identification query. This profile can be enriched on-demand by submitting additional queries involving the candidate X  X  name and different topics.

After building the external candidate profiles, we proceed to ranking the candidates in the organisation. At this stage, our the valuable information provided by the WSEs X  underlying ranking mechanism. In contrast, we propose an approach more in spirit with the Voting Model (described in Section 2.1 ), where the external evidence of each candidate X  X  expertise is ranked in response to an expert search query (i.e., the original query without the candidate X  X  name or organisation) in light of the estimated relevance of such evidence, as conveyed by the WSEs X  ranking mechanism.
 In order to enable our approach, we form pseudo -Web search engines (pseudo-WSEs), each of which corresponds to a
WSE with indices restricted to the documents contained in the profiles of the candidates as obtained above. To facilitate the creation of the pseudo-WSEs, the documents in the profiles of all candidates are downloaded and indexed locally. Using this index, we can then apply a document retrieval approach in order to mimic the real WSE. In this way, a pseudo-WSE at-tempts to simulate the ranking mechanism of the corresponding WSE as if the latter was only permitted to retrieve from the documents previously identified in the profiles. Moreover, by using pseudo-WSEs, we can actually assign a score to each of their ranked documents, as WSEs X  APIs do not provide this information. The generated ranking of documents can then be used as input to the Voting Model to produce a ranking of candidates. Additionally, as we have control over the document ranking approach applied by each pseudo-WSE, we can explore different ranking strategies in order to better mimic the rankings produced by the corresponding real WSEs. 4. Experimental setup
To ascertain how good WSEs are at expert search, we aim to answer the following research questions: 1. Can we accurately mimic the WSEs X  rankings as pseudo-WSEs? 2. Can we effectively use the generated pseudo-WSEs for expert search? 3. Can we effectively combine the evidence mined from WSEs with the existing intranet evidence? 4. Can we leverage useful alternative evidence from WSEs instead of having to download their full results?
Sections 5 X 8 investigate each of these research questions in turn. In the remainder of this section, we describe the exper-imental setup aimed at supporting all these investigations. 4.1. Collection and topics
In this work, we experiment with the TREC CSIRO Enterprise Research Collection (CERC) ( Bailey et al., 2007; Balog et al., 2008 ), a corpus of 370,715 documents crawled from the public domain of the Australian Commonwealth Scientific and
Industrial Research Organization (CSIRO) in March 2007. This collection was introduced in TREC 2007 as a more realistic rep-development phase involved actual members of CSIRO engaged in their work tasks ( Bailey et al., 2007 ). Moreover, CERC com-prises a timely corpus with respect to the Web evidence used in our experiments, as described in Section 4.3 . In our exper-iments, we index both the TREC CERC collection and the acquired Web evidence using the Terrier IR platform ( Ounis et al., 2006 ), 1 after applying Porter X  X  stemmer and removing standard English stopwords.

The TREC CERC collection was used in the expert search task of the TREC 2007 and 2008 Enterprise track. During these parable to those reported in the expert search literature, in our experiments, we split these topics into two sets: EX07, com-prising the 50 TREC 2007 topics (numbered CE-001 X  X E-050), and EX08, comprising the 77 TREC 2008 topics (numbered CE-051 X  X E-127). 4.2. Baseline intranet ranking
Out of the 3,475 candidates experts identified by their corporate e-mail address in the CERC test collection ( Macdonald, 2009 ), we build a set of unique candidates from the top 100 ones suggested by our intranet baseline expert search engine for each of the 127 topics from the TREC 2007 and 2008 Enterprise track. This baseline uses the expCombMNZ voting technique (Eq. (1) ) on top of a document ranking produced using the Divergence From Randomness DLH13 document weighting model ( Amati, 2006; Macdonald, He, Plachouras, &amp; Ounis, 2005 ). In particular, through an extensive experimentation, this baseline has been shown to deliver an effective expert search performance ( Macdonald, 2009 ). 4.3. Web evidence acquisition
In order to improve our initial intranet baseline, for each candidate it retrieved for each of the EX07 and EX08 topics, we build an evidence identification query, as described in Section 3 , including the candidate X  X  name and the title field of the to-pic. We then submit these queries to major WSEs using their public APIs, which will allow Web documents specific to the query topic and to the candidate to be retrieved. We build 4834 evidence identification queries for the 50 EX07 topics and 7234 of such queries for the 77 EX08 topics. In total, 12,068 evidence identification queries are issued to each of the follow-ing WSEs: 1. Google : A general WSE, to identify Web documents relating the candidate to the query in question. 2. Yahoo! : Another general WSE, to provide comparative results. 3. Google-pdf : As Google, but only PDF documents are retrieved. 4. Yahoo!-pdf : Analogously, as Yahoo!, but only PDF documents are retrieved. 5. Google Scholar : To identify any academic publications by the candidate about the topic area. 6. Google Blogs : To identify any blog posts linking the candidate to the query. 7. Google News : To identify any news stories linking the candidate to the query. A candidate cited or quoted in a news article is likely to be very authoritative in the topic of the article.

For each WSE, the evidence identification queries are issued and the search result listings obtained. From these, we ex-tract a list of results associated to each candidate. A maximum of 24 results per query are extracted and the corresponding
Web pages downloaded. 3 , 4 These pages form the external profiles of the candidates. Note that these profiles are query-biased, as only documents which are related to the query topic(s) are associated to each candidate.

Table 2 details the statistics of the results found and downloaded from the result lists provided by the WSEs for evidence identification queries based on both EX07 and EX08 topic sets. queries which retrieved at least one result. As most WSEs use some form of conjunctive querying, in which all query terms must be found in a document for it to be retrieved, not retrieving documents for every evidence identification query is expected X  X n-deed, not every candidate expert considered will have on-topic documents for every evidence identification query. We also re-port the number of documents retrieved, the number of candidates for whom some evidence was found (out of the 3475 in the
CERC test collection), and the average number of documents identified per candidate. For example, in the first row of Table 2 ,we detail the statistics of the evidence identification queries based on the EX07 topics submitted to the Google WSE: 3464 out of the 4834 queries issued retrieved 1 or more documents; in total, 19,225 documents were retrieved; this provides expertise evi-dence for 1498 unique candidates from the CERC collection (about 43% of all candidates); this amounts to an average profile size of around 23 documents per candidate. From the table, we note that Google and Yahoo! produce the most evidence, while, as expected, restricting their searches to only PDF documents reduces the number of documents identified. Google Blogs and
Google News produce little evidence, while Google Scholar provides evidence for around 50 X 60% of the candidates covered by Google. 4.4. Mimicking strategies
Besides acquiring expertise evidence from WSEs, we want to ensure that our pseudo-WSEs produce document rankings that are as accurate as possible. For such, we assume that the rankings produced by the real WSEs are of high quality. This is an acceptable assumption, even if purely on the basis that they have many people employed to ensure that their search re-sults are of high quality. Therefore, we want to have each pseudo-WSE produce rankings that are as similar as possible to the real WSE that it is mimicking. However, the ranking strategies adopted by commercial search engines are a closely guarded secret: we cannot know which weighting model they apply, and which additional features are taken into account.
In order to investigate different alternatives for impersonating the WSEs X  underlying ranking mechanisms, we apply four standard weighting models: BM25 ( Robertson, Walker, Hancock-Beaulieu, Gatford, &amp; Payne, 1995 ), Hiemstra X  X  language modelling (LM) ( Hiemstra, 2001 ), and two Divergence From Randomness ( Amati, 2003 ) models, namely, PL2 and DLH13. For each of these models, we deploy two mimicking strategies, aimed at reproducing the WSEs X  rankings:
The UNTRAINED mimicking strategy applies the considered weighting models with their default parameter settings. In par-ticular, we use the often suggested settings of BM25 X  X  b = 0.75 ( Robertson et al., 1995 ), LM X  X  k = 0.15 ( Hiemstra, 2001 ), and PL2 X  X  c = 1.0 ( Amati, 2006 ). Note that the DLH13 weighting model has no parameters to train ( Amati, 2003 ). Besides the
TRAINED mimicking strategy, we investigate another strategy to improve the mimickings of BM25, LM, and PL2. In particular, our
TRAINED mimicking strategy optimises the parameter settings of each of these models using the training queries that we have available. As discussed in Section 3 , for each WSE, we have a list of the evidence identification queries that it answered and the ranking of documents produced for these queries. From Table 2 , we can see that, for most WSEs, this amounts to over 2000 queries. We can then train each of our pseudo-WSEs to reproduce the corresponding WSE X  X  ranking as accurately as
The next issue is how the effectiveness of the pseudo-WSEs should be ascertained during training. If we restrict the doc-uments retrieved for a given query to the same documents that the real WSE retrieved, then all standard IR measures will give 1.0, as all and only relevant documents are retrieved. However, we are not interested in the precision and recall of our pseudo-WSEs but, instead, in the extent to which their rankings correlate with the real WSEs. In particular, we use three different measures to quantify the extent to which our pseudo-WSEs achieve the correct ranking of documents: nDCG, , and q .

Discounted cumulative gain (DCG ( J X rvelin &amp; Kek X l X inen, 2002 )) is a standard IR evaluation measure that uses graded (i.e., non-binary) relevance labels. It is computed by summing the relevance labels of all retrieved documents, penalised by the logarithm of the rank position of each document: order to make the DCG values for different queries comparable, we employ its normalised version, nDCG ( J X rvelin &amp; Kek X l X i-nen, 2002 ): where DCG is given by Eq. (2) and iDCG is the ideal DCG value, obtained by producing a perfect ranking for the same query.
While nDCG is normally applied with up to five levels of relevance, we apply it with up to 24 levels (i.e., the maximum num-ber of results retrieved per evidence identification query), with the highest level denoting the top-ranked document for a given query. The final nDCG value is then calculated over the ranking of documents up to the number of relevant (retrieved) documents.

Average precision (AP) correlation ( s ap ( Yilmaz, Aslam, &amp; Robertson, 2008 )) is based on average precision and extends the traditional Kendall X  X  s rank correlation coefficient ( Kendall, 1938 ) by performing a  X  X  X ank-aware X  X  correlation, thus distin-guishing between errors towards higher ranks and those towards lower ranks. It is defined as: where con( i ) is the number of concordant pairs of documents with respect to a reference ranking above rank i . In our exper-iments, the reference rankings are those obtained from the different WSEs.

Finally, we also use Spearman X  X  rank correlation coefficient (Spearman X  X  q ( Wackerly, Mendenhall, &amp; Scheaffer, 2002 )), which is equivalent to computing a linear correlation using the ranks (rather than the scores) of the retrieved documents: is produced by a WSE, while y is produced by the corresponding pseudo-WSE. 4.5. Evidence combination
Besides evaluating a pseudo-WSE in comparison to a baseline intranet ranking, we also evaluate the combination of these two rankings. In particular, to combine the results of the intranet and external expert search engines X  X enoted int and ext , respectively X  X e apply a weighted sum: and may be unbounded, while w int and w ext combine the roles of normalising candidate scores and weighting the importance of the intranet and external evidence. Moreover, by combining separate candidate rankings, we do not mix statistics of dis-tinct collections. In our experiments, we combine the rankings produced by an expert search engine based on the intranet evidence with each of the rankings produced using pseudo-WSEs, in both their untrained and trained versions. In all cases, the intranet and external engines apply the same document weighting model and the same voting technique (i.e., ex-pCombMNZ). In order to avoid temporal disparities between the Web evidence acquired for the EX07 and EX08 topic sets, we train the weights w int and w ext for each set independently, by performing a simulated annealing ( Kirkpatrick, Gelatt, &amp;
Vecchi, 1983 ) through a five-fold cross validation. 5. Mimicking WSEs X  document rankings
In this section, we address our first research question, regarding the accuracy of our mimicking strategies. In particular, we want to assess the extent to which our pseudo-WSEs can reproduce the document rankings provided by the different
WSEs. In this investigation, we apply the aforementioned document weighting models (i.e., BM25, LM, PL2, and DLH13) for our pseudo-WSEs in their default and trained settings. Table 3 reports the increase in nDCG tings of each of these models according to the three considered measures (i.e., nDCG, s untrained setting. Significance between the untrained (i.e., baseline) and trained settings is calculated using the Wilcoxon signed-rank matched-pairs test and denoted by one of five symbols: =, denoting no significant difference from the baseline; decreases or increases compared to the baseline with p &lt; 0.01. The best achieved mimicking of each WSE is shown in bold.
From Table 3 , we note, as expected, that nDCG can be improved by training. Moreover, while there is no clearly preferred training measure, all three measures bring significant mimicking improvements across most settings, with the exception of
Google Blogs and Google News, which provide the least evidence in Table 2 . Indeed, significance is likely to occur for very small improvements on potentially large sets of queries. Examining the best settings, we note that, when no training is ap-plied, DLH13 is the most effective weighting model in 5 out of 7 cases. Moreover, when training is applied, it remains best for three WSEs. 7 Among the other weighting models, LM seems to perform best overall, with and without training. Further improvements towards better reproducing the underlying WSEs X  ranking mechanisms might be possible by the use of addi-tional features by the corresponding pseudo-WSEs, such as anchor text or linkage information. However, this is made difficult because the real WSEs can leverage all anchor text identified for each document from the entire Web. In this sense, our pseudo-
WSEs can never behave identically to the corresponding WSEs, due to their lack of knowledge of the whole Web surrounding the documents that they act on. Nevertheless, the very high values reported for nDCG in Table 3 attest the accuracy of our mim-icking strategies. Recalling our first research question, this confirms the feasibility of building pseudo-WSEs that almost clone the original WSEs X  document rankings, while providing our approach with scoring information for each ranked document. 6. Leveraging Web search results for expert search
In this section, we evaluate the expert search retrieval performance of using our pseudo-WSEs in comparison to a baseline expert search engine based on intranet evidence only. By doing so, we address our second research question. Additionally, we want to quantify the impact of further refining the mimicking of WSEs X  rankings through training on the final expert search performance. As in Section 5 , we build pseudo-WSEs using four document weighting models, in their untrained and trained versions. From these document rankings, up to rank 1000, we then apply the expCombMNZ voting technique (Eq. (1) ).
Table 4 presents the results of our experiments using the TREC EX07 and EX08 topics (CE-001 X  X E-050 and CE-051 X  X E-127, respectively) in terms of standard mean average precision (MAP). Statistical significance between each of the default and trained settings of our pseudo-WSEs and a baseline expert search engine based on intranet evidence is denoted with one of the symbols described in Section 5 . A second symbol denotes the significance of each trained setting with respect to the best performing among the default and trained settings, which is itself marked with an asterisk (*). For example, for the EX07 topics, both the untrained and trained settings of BM25 + Yahoo!-pdf significantly underperform when com-pared to the intranet baseline (MAP 0.3576), while three of them do not significantly differ from the best trained setting ( s , MAP 0.2325). The best performance using each WSE is shown in bold. A row with the performance reported by Jiang et al. (2008) using external results retrieved by Google is also included. Note that a direct comparison to the works by
Serdyukov et al. (2008) and Balog and de Rijke (2008) is not possible, as both use summaries (snippets) of search results exclusively from outside the domain of the organisation under consideration. The performance reported by Serdyukov and
Hiemstra (2008a) using titles and snippets is included as a reference value in the investigation described in Section 8 .
From the results in Table 4 , we firstly note that some of the considered WSEs can be effectively applied for identifying relevant experts in the CERC collection. In particular, pseudo-WSEs mimicking Google and Yahoo! markedly outperform the intranet baseline for most settings for the EX07 topics, albeit not significantly. Moreover, these results show that, using exactly the same document ranking techniques, it can be more effective to mine the Web than the intranet of the actual orga-nisation. This is somewhat expected as, given the size of the Web, it is possible that some prominent experts will have useful expertise evidence outside of their organisation X  X  intranet. This is typical of research organisations such as CSIRO, as researchers write papers and give talks outside the organisations, which leads to their name and some evidence of their expertise appearing on Web sites other than their own. A comparison to the performance of the candidate-centric approach of Jiang et al. (2008) using results retrieved by Google (last row in the table) further attests the effectiveness of our mimick-ing strategy. As for the remaining WSEs, restricting Google and Yahoo! to retrieve only PDF documents degrades the retrieval performance, while still not significantly differing from the intranet baseline. The next most effective source is Google Scho-lar, while Google Blogs and Google News have much lower performances, vided the least evidence, as previously discussed in Section 4 .

A similar scenario is observed for the EX08 topics, however with a lower performance on most settings, and significantly lower than that of the intranet baseline. This suggests that the expertise needs from the EX08 topic set are apparently less likely to be answered on the Web when compared to those from the EX07 topic set. Given the longer time elapsed since the development and release of the EX07 topics, it seems plausible that more evidence is available on the Web for this topic set compared to the EX08 one. Indeed, using the procedure described in Section 3 for obtaining expertise evidence from WSEs, the median WSE (among the seven considered) retrieves an average of 334.20 documents per EX07 topic while only 268.97 are retrieved for each EX08 topic. Nonetheless, the performances attained by LM + Google and LM + Yahoo! are notable exceptions, as they do not significantly differ from the intranet baseline, once again showing that useful evidence can be mined from WSEs, even when the apparently more difficult EX08 topic set is considered. Overall, these results show that we can effectively leverage the general Google and Yahoo! WSEs for expert search, hence answering our second research question.

As for the considered mimicking strategies, the (untrained) DLH13 model performs generally best for the EX07 topics, while LM is overall the best for EX08. When mimicking is refined by training the pseudo-WSEs, a corresponding improve-ment in retrieval performance over untrained pseudo-WSEs is observed for the majority of cases. For the EX07 topic set, out of 21 possible cases, training BM25, LM, and PL2 improves in 11, 16, and 18 cases, respectively. For the EX08 topic set, improvements are attained in 15, 10, and 18 out of 21 cases for BM25, LM, and PL2, respectively. Spearman X  X  q yields the best results in most cases (11) for the EX07 topics, followed by s effective training measure (eight cases), followed closely by nDCG (7) and Spearman X  X  q (6). As a whole, refining the mim-icking of WSEs using any of nDCG, s ap ,or q is beneficial. Improvements are particularly marked for PL2 on both topic sets, and can also be significant, particularly for the EX08 topics. This further confirms our findings in Section 5 , regarding the potential of refining our mimicking approach. 7. Combining pseudo-WSEs with intranet evidence
Compared to a typical enterprise search scenario, where only intranet evidence is used, the retrieval performance achieved by the pseudo-WSEs in Section 6 is impressive, particularly for the EX07 topics. Hence, a natural question that arises is whether this external evidence can be combined with an existing expert search engine operating with intranet data.
As the two sources of expertise evidence do not overlap, they should be independent and their combination should result in an increase in retrieval performance. In this section, we address our third research question, by investigating whether the external and intranet evidence can be successfully combined. Additionally, similarly to the previous section, we assess the impact on the expert search performance of this combination after refining the mimicking of the WSEs X  underlying rank-ing, by training the corresponding pseudo-WSEs.

Table 5 presents the results of our experiments for both EX07 and EX08 topics and additionally includes the intranet base-lines from Table 4 . Significant increases over the baseline and the best among the untrained and trained settings are shown using the symbols previously introduced in Section 6 . As an additional baseline, a row with the performance reported by
Jiang et al. (2008) using Google is included. Note, however, that they do not perform an explicit integration of internal and external evidence. Instead, they simply allow Google to also retrieve results from inside the CERC collection. Neverthe-less, this is the closest attempt to combine full content evidence gathered from both inside and outside this collection that we are aware of.

With respect to our third research question, we note that combining the rankings based on mimicked WSEs with that based on existing intranet evidence can markedly improve both the intranet baseline and the corresponding pseudo-WSEs alone (see Table 4 ). Indeed, for both the TREC EX07 and EX08 topic sets, except for Google News, all mimicked WSEs can bring improvements to the existing intranet-based expert search engine. On the EX07 topic set, marked improvements are attained by integrating expertise evidence from Google, Yahoo!, and Yahoo!-pdf. These gains can be significant, in par-ticular for Google using LM, and for Yahoo! using both BM25 and LM. However, for the EX08 topic set, less marked improve-ments are observed, which are not significant with respect to the baseline intranet performance. These results confirm our observations in Section 6 , regarding the increased difficulty of the EX08 topic set compared to EX07.

As for the untrained weighting models used for mimicking the document rankings produced by the different WSEs, LM performs the best, improving the intranet baseline in 6 out of 7 possible cases, followed by BM25, DLH13, and PL2, with improvements in 4, 4, and 1 out of 7 cases, respectively. On the EX08 topics, a different trend is observed, with BM25 improv-ing the intranet baseline in 5 out of 7 cases. LM, PL2, and DLH13 bring improvements in 3, 2, and 2 out of 7 cases, respec-tively. When compared to the performance reported by Jiang et al. (2008) using Google, we note that marked improvements are attained using all four untrained weighting models, again attesting the effectiveness of our approach.

By examining the effect of refining the mimicking of a pseudo-WSE on the effectiveness of the combined expert search engine, we note that the retrieval performance of the latter is not consistently enhanced compared to using an untrained pseudo-WSE. For the EX07 topics, out of 21 possible cases, training BM25, LM, and PL2 improves over their untrained version in 7, 11, and 14 cases, respectively. For the EX08 topics, improvements are observed in 8, 13, and 11 cases, respectively. In particular, the additional layer of training involved in combining internal and external expert search rankings seems to attenuate the benefits of adopting a trained mimicking strategy, in contrast to the observations in Section 6 .
Besides being effective on its own, as shown in Section 6 , the results in Table 5  X  X articularly those on the EX07 topics X  show that our approach for mimicking WSEs X  rankings can be successfully integrated to an existing intranet-based expert search engine, hence answering our third research question. Additional improvements could be achieved if we further en-hanced the mimicking and training procedures by integrating features such as candidate query term proximity or candidate homepage detection, which were shown to be effective on this task ( Macdonald, Hannah, &amp; Ounis, 2008 ), e.g., within a learn-ing-to-rank setting ( Joachims et al., 2007 ). Moreover, enhancing the mimicking of WSEs by extracting additional features from the available documentary evidence of expertise could also help overcome the sparsity of this evidence for the more recent EX08 topics. 8. Leveraging alternative expertise evidence from WSEs
In the previous sections, we showed that mimicking WSEs by trying to reproduce their rankings for evidence identifica-tion queries can be an effective approach for expert search. By doing so, we have answered the hypothetical question what if
WSEs did expert search? Since indexing documents and scoring these documents in response to a query are an integral part of any WSE X  X  searching mechanism, efficiency would not be a concern if our approach was to be deployed in a Web search scenario. Nonetheless, in this section, we address the practical problem of deploying our approach in a real enterprise scenario. Despite the costs incurred in building an initial expertise base using evidence from WSEs (which is even more problematic for candidate-centric approaches, as discussed in Section 2 ), maintaining our approach in a typical expert search environment should not represent a significant burden for an enterprise. For instance, the initial base could be updated on-demand as new employees entered or leaved the enterprise, or as new topics became of interest to its users. As a fall-back plan, if no external evidence is available for a given topic or candidate, the intranet alone could still be used to effectively answer users X  queries, as denoted by the strong performance of our baseline intranet expert search engine used in the previous sections.

Nevertheless, for a given unseen expert search request, the system would have to query a WSE in order to identify exper-tise evidence and build the profile of every candidate. Moreover, the full documents that comprise these profiles would have to be downloaded. For instance, in our current setting, as detailed in Section 4 , we download up to the top 24 documents retrieved for each expertise identification query. Additionally, these profiles would have to be updated from time to time X  X gain, a problem also faced by candidate-centric approaches. These costs, however, could be overcome by the use of an alternative form of expertise evidence extracted from the WSEs X  result pages.

In this section, we address our fourth and last research question, by investigating the effectiveness of leveraging the meta-data associated to the results retrieved by WSEs, such as their titles and descriptive snippets, instead of downloading the corresponding full documents. In particular, for each result for an evidence identification query, we extract its title and snip-pet, combining them together as a single document of expertise evidence to compose the profile of a candidate. Such evi-dence is then indexed and scored by our pseudo-WSEs, in the same way as done for the full documents.

In Table 6 , analogously to the results presented in Table 3 using full documents, we contrast the nDCG performances at-tained by the pseudo-WSEs using the combined indices of titles + snippets under different training settings. In particular, we train our pseudo-WSEs using the same set of 12,068 evidence identification queries used in Section 4.4 , however using the derived rankings of titles + snippets. Relative to indexing the full documents retrieved by the WSEs, the evidence retrieved in the form of result titles and snippets is more sparse X  X or instance, titles and snippets do not always contain all the query terms used to retrieve them, as these terms are only present in the corresponding full documents. Additionally, the statistics derived from a collection of snippets are usually less refined, since the summarisation of full documents into snippets often reproduce the original WSEs X  rankings, which is reflected in the lower nDCG values reported in Table 6 when compared to those in Table 3 using the full documents.

Nevertheless, we compare the retrieval effectiveness of using titles + snippets to the counterpart pseudo-WSEs in Table 4 , which use the full content of each retrieved result. Table 7 presents the results of applying the expCombMNZ voting tech-nique on top of the pseudo-WSEs using either the combined index of titles + snippets ( T + S ) or the full retrieved documents (F) for the 127 topics from the EX07 and EX08 tasks. As we are mostly interested in comparing the effectiveness of these alternative document representations for expert search, we report the results obtained by mimicking WSEs using either rep-resentation without further training. 10 Significance is given by the Wilcoxon signed-rank matched-pairs test and denoted by the usual symbols. Additionally, the last row includes the results reported by Jiang et al. (2008)  X  X  candidate-centric approach using Google X  X nd Serdyukov and Hiemstra (2008a)  X  X  document-centric approach using Yahoo! X  X n the EX07 topics. Results on the EX08 topics are not reported in these papers. Also note that the results reported by Serdyukov and Hiemstra (2008a) for
WSEs other than Yahoo! are not restricted to external evidence only, and hence are not comparable to ours. Similarly, Balog and de Rijke (2008) and Serdyukov et al. (2008) do not report their results in isolation from the intranet evidence. Moreover, the results of the latter two approaches are based on either titles or snippets, but not both.

From Table 7 , we observe that mimicking WSEs using titles + snippets performs even better than using the full content of each retrieved result. Indeed, for the EX07 topics, improvements over the full content baseline are obtained in 26 out of 28 possible cases. For the EX08 topics, the use of titles + snippets is preferable in 18 cases. Improvements are significant in sev-eral cases (10 for EX07, 13 for EX08). The highest improvements are observed for PL2, raising its observed poor performance on full documents to a comparable level to the other considered weighting models when titles + snippets are used. One rea-uments, the expertise evidence from the titles + snippets is more numerous. Indeed, differently from full documents, they are extracted directly from the WSEs X  result listings and hence are not prone to network errors, etc. during crawling. An inter-esting observation, however, is that LM shows a noticeable preference for the full content documents on the EX08 topics. By comparing our results to those reported by Jiang et al. (2008) and Serdyukov and Hiemstra (2008a) and reproduced in
Table 7 , we observe a marked improvement over the former and a comparable performance to the latter. Recalling our last research question, these results attest the feasibility of using this alternative evidence from WSEs, drastically reducing the incurred costs, particularly in terms of network traffic ( Serdyukov &amp; Hiemstra, 2008a ). On average, for each evidence iden-tification query, the downloaded content was reduced from 550KB (i.e., the result pages and up to 24 full documents) to only 8KB (i.e., the result pages only)! Moreover, the results in Table 7 also show that leveraging titles + snippets can be even more effective than using the corresponding full documents. Indeed, DLH13 + Yahoo! using the titles + snippets evidence provided the best expert search retrieval performance across all individual strategies investigated in this article. 9. Conclusions
In this article, using a uniform experimental setting, we have conducted a thorough investigation of the usefulness of Web search engines (WSEs) as expert search systems. In particular, we have shown that WSEs can be effectively used for expert search by explicitly trying to mimic their underlying ranking mechanisms.

Using a standard enterprise test collection and two topic sets from the TREC 2007 and 2008 Enterprise Expert Search tasks, we investigated the effectiveness of leveraging external evidence using seven different WSEs. Our results showed that this evidence can be successfully used for finding experts within an organisation. Moreover, by combining it with the exist-ing intranet-based evidence, we showed that the expert search effectiveness can be further improved on the 2007 topic set.
On the apparently more difficult 2008 topic set, although less evidence could be mined from the Web, we showed that marked improvements could still be attained, hence demonstrating the consistency of our approach.

Finally, we investigated the effectiveness of using the titles and snippets rather than downloading the full content of the results retrieved by WSEs. Our results showed that mimicking WSEs X  rankings using this alternative evidence is unexpect-edly more effective for expert search than using the corresponding full documents, while providing a drastic reduction in the incurred network costs. This demonstrates that our approach can effectively and efficiently be deployed in a typical enter-prise setting.

Overall, our mimicking experiments show that, should a WSE start offering expert search services for enterprise organ-isations like CSIRO, they would likely be effective. Moreover, they confirm that exploiting the WSEs X  underlying ranking mechanism through mimicking can be an effective approach to estimate the relevance of expertise evidence gleaned from outside the enterprise sphere. As future work, we wish to investigate the impact of different voting techniques ( Macdonald &amp;
Ounis, 2006 ) on the effectiveness of using this external evidence, as well as different training procedures for further enhanc-ing the mimicking of WSEs X  document rankings, including the simulation of such rankings ( Macdonald &amp; Ounis, 2009 ). Finally, another direction of investigation is the study of combining multiple sources of expertise evidence ( Fang, Si, &amp; Mathur, 2010a ).
 References
