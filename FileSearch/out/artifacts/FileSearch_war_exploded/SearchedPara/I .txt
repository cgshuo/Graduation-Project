 Data mining research has long concentrated on the five main areas: clustering, association discovery, classification, forecasting and sequential patterns. Web data mining projects are concerned mainly with text mining, user segmentation, forecasting web usage and analyzing users' clickstream patterns. We present a new type of web usage mining called funnel analysis or funnel report mining. A funnel report is a study of the retention behavior among a series of pages or sites. For example, of all hits on the home page of www.msn.com, what percentages of those are followed by hits to moneycentral.msn.com? What percentage of www.msn.com hits are followed by moneycentral.msn.com, and then www.msnbc.com? What are the most interesting funnels starting with www.msn.com? Where does the greatest drop off rate occur after a user has hit MSNBC? Funnel reports are extremely useful in e-business because they give product planners an idea of how usable and well-structured their site is. From our experience performing web usage mining for the MSN network ol y sites, funnel reports are requested even more than user segmentation analyses, site affiliation studies and classification exercises. In this paper, we define a framework for funnel analysis and provide a tree-based solution we have been using successfully to extract all relevant funnels using only one scan of the data file. Over the last few years, a number of data mining problems have been discussed. Classification, association discovery, sequential patterns, outlier detection, time series forecasting and clustering are among the most common ones. Data mining techniques have been applied to both market basket and web data. Application of data mining techniques to web, i.e., web data mining, has been following three main directions: web content mining, web structure mining, and web usage mining. Web usage mining is the process of applying data mining techniques to the discovery of usage patterns from web data [2]. In web usage mining research, literature on mining the retention behavior among a sequence of pages or sites is yet to appear. We now propose a new type of personal or classroom u~ is granted without lee provided that copies are not made o1" disu'ibuted for profit or commercial advantage and that requires prior speci tic permission and/or a I~e. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 web usage mining application called funnel analysis that tackles this specific problem. In our decision support practice, we perform different kinds of web usage mining for all the sites in the MSN network, where a site in the MSN network is any site with a *.msn.com domain, i.e. Carpoint, MoneyCentral, the MSN Portal, Communities, Chat, etc. We have used association discovery to identify site affiliations, clustering to segment the user populations in our worldwide properties, and forecasting to give site managers estimates of future web traffic. However, one of the most common types of mining requests we get from business managers is not on this list, and that is funnel reports. A funnel report is a study of the retention behavior among a series of pages or sites. Funnel reports are extremely useful in web business because they give product planners an idea of how usable and well-structured their site is. For instance, funnel reports can be utilized to measure how well an on-line sign-up process works. Suppose a site has three specific steps (pages) in its sign-up process, and it needed to know the number of users who hit the first page of the sign-up process, how many then proceeded to the second step, and finally, the number that ended up at the last page and successfully subscribed to the service. The end result of this analysis is a picture of a funnel where the width at the top of the funnel represents all the users who had been to the first step of the sign-up process. The width in the middle of the funnel represents the number of users who had seen both the first and second pages of the sign-up process, and the bottom of the funnel reveals the number of users who ended up at the last page and finally subscribed to the service. A significant drop-off rate between one step and the next may indicate that the site should restructure its sign-up process to increase retention. For different Internet properties and services, we have seen extremely different funnel shapes. Drop off rates between funnel steps can be as high as 98% or as low as 2%. 
Figure 1 shows a sample funnel with five steps in the subscription process. 800 users hit step 1. Of those 800, 500 of them hit step 2 after viewing step 1. 400 users performed the first three steps, 300 did the first four steps and only 200 finished all five steps. In effect, 25% of users who hit Step 1 reached the last step of the funnel. The process of finding the funnel slope (drop off rates) for a set of required funnel points in clickstream data is not difficult. That is, when we are given the actual steps themselves, then all we have to do is scan the entire data set once looking for and counting sequences of the steps in the clickstream, and that has been our practice. This example itself is not a data mining application per Instead of relaying specific funnel points (pages) of interest at each step, business managers want to know all "interesting" funnels starting with given funnel points. For users who hit the MSN Portal Home Page, what is the most common behavior after reading the page? Where do we start losing users? When do users abandon the network? These all translate into the questions: What are the widest funnels? What are the narrowest funnels? What are the funnel points? In these cases, we are not given the funnel points and asked to find the funnel drop off rates. We are given only the top point (page) of the funnel (or no points of interest at all) and asked to find the most interesting funnels. Here we enter into the realm of data mining and automated knowledge extraction. In this paper, we tackle the problem by presenting a framework for funnel analysis and by providing a tree-based solution that will extract all relevant funnels using only one scan of the data file. 
Reactions to our funne! reports have generally followed one of two routes: 1) re-negotiation of deals with content providers to better reflect the worth of the site content and 2) re-design of the sites to better meet the likes and dislikes of end users. In the case of deal re-negotiations, when certain types of content generate lots of abandonment, the deal is either cancelled or the link to the content is not given dominant promotion locations in the network. 
When re-designing sites to better meet users' interests, product managers may decide to add extra categories of content, placing pages that were previously unknown to be significant into the new content areas. To re-design the sites taking into account users' dislikes, designers may decide to cut certain steps of sign-up processes that prevent end users from finishing all the steps required. 
We have applied our solution successfully on many of our projects and write about these experiences in this paper. The rest of this paper is organized as follows. Section 2 presents our framework for funnel report mining, where we discuss the criteria that make for interesting funnels. In Section 3, we describe one primary algorithm and another alternate algorithm that have worked for us successfully in practice. Section 4 reports on results of running funnel analysis on some real life MSN domain level clickstream data. In Section 5, we briefly list some related work in the field of web usage mining. Section 6 concludes the paper and describes some future projects. 
To answer the question of what makes for an interesting funnel, we need measures of interestingness. This can be achieved through depth and width criteria. The depth of a funnel is the number of levels (funnel steps) a funnel has. We can specify that interesting funnels have exactly X levels or &lt;= X levels. A funnel with the points A, B, C, D, E has a depth of five. The width criterion refers to each step in the funnel. The width of a point can be measured in terms of both absolute and relative frequency. The width of a funnel point, measured relatively, is the frequency of the full path ending at that point in the funnel divided by the frequency of the first (top) page of the funnel. For example, the funnel A, B, C, D, E with respective values 200, 126, 100, 76, 50 would have widths of 100% at level 1, 63% at level 2, 50% at level 3, 38% at level 4 and 25% at level 5. The absolute frequencies are 200, 126, 100, 76 and 50. We can specify width criteria at each level of the funnels we are interested in. Another way to specify width is to look at only the last level's width. Requiring that the last level width of interesting funnels be i.e. &lt;= 10% would imply that we are looking for narrower funnels. A last level width criterion of &gt;---60% may indicate interest in wider funnels. For simplicity's sake, we will assume from here on in that when we talk about width, we are referring to last level relative width. The next diagram demonstrates the width and depth concepts of a funnel. Using these criteria, we refine our requirements to read the following:  X  for a set of users' clickstreams, report all funnels with This is the funnel analysis framework in its m0kst general sense. However, site managers are usually not interested in funnels starting with all possible pages; they are only interested in funnels starting with a specific set of pages. We will call this type of criteria starting page criteria. In these cases, we can further refine our requirements to read:  X  for a set of users' clickstreams, report all funnels starting Our solution to the funnel analysis problem is tree-based, and requires only one scan of the data file. The idea behind the algorithm is to store all subpaths of each clickstream satisfying the depth and starting page criteria in tree structures, and then read through the trees to find funnels satisfying the width requirements. The algorithm works by creating a tree for each top funnel point, and then creating branches under the trees for each subpath or full path that begins with the top point. Each path in a tree represents all the click paths found that start with the page at the root of the tree. Counters at each node in a tree keep track of how many sessionized clickstreams contain the subpath that ended in that node. We can describe the process in more detail with an example. We name our clickstream file D. Each row in D contains the clickstream data for a user-session. Clickstream data is a sequence of hits, such as "www.msn.com/default.asp, moneycentral.msn.com/home.asp, search.msn.com/results.asp". This cliekstream represents a user-session where the user hit the MSN home page first, proceeded on to the MoneyCentral home page and then searched for something on the MSN Search page. This algorithm, which we have called SPNAV for Sub Path NAVigation, starts by looking at the first clickstream record CSv CSI contains pageviews PI, P2,...,P,,,...P,,, where n is the number of pages viewed in the record, and the plaeeholders PI,P2,...,P, represent the first, second, ..., X ..... and last pages in the clickstream of the user-session. We will be storing unique click paths in branches of tree structures, where each node in a tree has a page name and a support value. The name field is the name of the page contained in the node and the support is the number of times we have seen the particular subpath (beginning at the root and ending at the node in question) in the data file. Given the criteria that we are only interested in funnels with depth = X or &lt;= X, we read through CSI starting with Pl up to Px. We create a tree with root node containing the page P1, and descendant nodes consisting of P2-..Px where P2 is the child node Px-v As we create these, we also increment each node's support by one. In essence, the support counter indicates the number of times we have seen the subpath (ending at that node). If X is greater than N, then we only run each subpath down to PN. ~ After we have finished incrementing the counters for the tree rooted at P1, we then proceed to the subpath of CS1 starting with P2 and ending at Px+x. We create a new tree rooted at P2, increment the node's support, create the descendent nodes and increment all their supports. We are finished with the clickstream CSI when we have created trees for each X-item subpath of CSI starting with PbP2,...,Pn-~+t. For the subpaths starting with P,. x+2...P n, we create trees rooted at each page Pn.x+2,...,Pn and then store the subpaths that start at each page (and ending at Pn) in the i To extract funnels that reveal network abandonment behavior (i.e. After users hit www.msnbc.corn, 9% of the time they abandon the network on the next click), treat the end of a clickstream as a "page", thereby increasing n by 1 respective tree. We only create nodes and new trees when they are not already present. By the end of the processing of CSi, we should have n trees where each tree gives us all the subpaths beginning with the node at the root of the tree. We process the next clickstream CS2 in a similar manner; after a node has been created or if it is already present, we increment its support counter. If there are starting page criteria on the funnels, then entered by the user. In this case, we would only have s trees ifs was the number of starting pages inputted. After processing the entire clickstream data set, we will have a number of trees, where each tree represents all funnels with a particular starting page. The algorithm then proceeds to the second phase where it runs through all branches of each tree, extracting and outputting only branches that satisfy the depth and width criteria. For example, a branch with nodes in the order {www.msn.com, moneycentral.msn.com, search.msn.com} and counters {20,10,5} represents the funnel {www.msn.com, moneycentral.msn.com, search.msn.com}. The funnel values indicate that 20 clickstreams (user-sessions) in the data file contained a hit on www.msn.com. 50% of user-sessions that saw the MSN Portars home page hit MoneyCentral immediately afterwards, and 25% of user-sessions that hit MSN had MoneyCentral and MSN Search as their second and third page views. If the last level width criterion was set to "&gt;= 23%", then this funnel would satisfy the requirements. If the last level width criterion was set to "&gt;= 30%" or "&lt; 20%", then this funnel would not be valid. If width criteria had been entered for each level, the algorithm will check the validity of the width at each level. In this case, if a node Nodei at any one level does not satisfy the width criteria at that level, then the algorithm ceases to read all nodes under Nodei. In some cases, an absolute frequency width criterion might be more appropriate. For example, if we are interested in funnels starting with the MSN Portal, and we know that the number of hits on the Portal completely dominates the number of hits on other MSN sites, then we may find it more suitable to specify an absolute frequency width criterion such as "funnel point 2 &gt;= 10000 hits" rather than a relative frequency criterion. Another interesting alternative is to take into consideration the number of outgoing branches on a node at any one level. We can specify width criterion relative to aggregate measures, for example, where we require that the absolute or relative widths of an "interesting funnel" be greater or less than the average width across all its adjacent nodes. The SPNAV algorithm is very efficient in the sense that it reads the data file only once. Memorywise, for a clickstream file containingp different ~ages and given the depth criteria &lt;= d, we need a maximum ofp + pa-i + ... + p nodes to hold all the trees in memory. However, in reality, the actual number of nodes used is much less. This maximum only holds if users have clickstreams containing every possible combination of pages, which is never the case. In a majority of situations, analysts only want to see funnels with smaller depth to get to the root of the underlying business problem. Therefore, d is usually quite small. Past experience has also told us that site managers are not always interested in pages that get low numbers of hits; P, the number of different pages in the clickstream file, is usually reduced by analyzing only pages that are hit most frequently by users. In addition, the number of trees created is diminished drastically when starting page criteria are specified. For s different starting page criteria, we essentially reduce the number of trees created by p -s. For example, if our clickstream file was composed of 1000 different unique pages and the site was only interested in funnels starting with ten specific pages, we would be creating 10 trees rather than 1000. With the SPNAV algorithm, we read through every X-item subpath of each clickstream record in the data file (where X is the maximum depth). This limits the number of nodes we build down each tree. Starting page criteria limits the number of actual trees we create. In the rare case when an analyst does not want to apply any starting page criteria and needs to extract very deep funnels, we can apply another tree building method that will store fewer nodes than SPNAV. SPNAV creates one tree for each possible starting point and stores each X-item subpath of every clickstream record. The alternate method, which we call FPNAV for Full Path NAVigation trees, creates one tree for each actual starting point of a clickstream record and stores only the full path of each clickstream record. For example, for clickstream CSt with pages {PI, P2 create only the tree rooted at Pt and store its descendant nodes P2...P, as one branch in the tree. Unlike SPNAV where we would then create a tree rooted at P2, we instead proceed to the next clickstream record CS2 and create a tree whose root node contains the first page of CS2. We subsequently store the full clickpath of CS2 as a full branch of the tree. With FPNAV, we are not creating extraneous nodes and trees for each possible subpath of a clickstream record. We only store the full record itself; therefore, the number of nodes required to hold all the necessary information is less than what SPNAV would need if there were no starting point or depth criteria. After we have created all the trees, we then enter phase two of the algorithm where, for each possible starting page, we search through all trees looking for subpaths that start at the given starting page and satisfy the depth requirement. As we find these subpaths, we store them in a temporary tree that will hold the support for all funnels beginning with the particular starting point in question. After we have finished searching through all the trees, we then recurse through the temporary tree and output funnels that satisfy the width criteria. FPNAV is an alternative to SPNAV when the analyst wants to find very long funnels and is interested in funnels starting with any page on the web site. In this case, the amount of nodes created by SPNAV will be more than FPNAV because SPNAV stores several subpaths of each user-session's clickstream record. FPNAV only stores the full subpath of a user-session's clickstream. FPNAV is not without faults itself; although FPNAV stores fewer nodes in the extreme case, it requires more processing time than SPNAV to generate all the funnels. FPNAV is offered as an alternative here for the extreme case; we ourselves have never used it in practice because our customers have always been interested in narrow ranges of funnels for which SPNAV is the optimal algorithm. In the next session, we focus on SPNAV and report on its run time with a real life application. We have used our SPNAV implementation for a number of different site projects, and have found many interesting funnels as a result. For the purposes of this paper, we ran SPNAV on a particular non-random group of 332,970 users who came to the MSN network of sites on Thursday, January 18, 2001. We extracted their day's sessionized clickstream data at the domain level. In total, there were 1,019,341 sessions and 9,512,431 page views in the final data set. On average, there were 9.3 page views per session. The following table displays some sample clickstreams. search.msn.com, search.msn.com, search.msn.com, search.msn.com www.msn.com, www.msn.com, entertainment.msn.com, entertainment.msn.com www.bcentral.com, www.msn.com, www.msn.com www.msn.com, www.msnbc.com, www.msnbc.com www.msn.com, www.msn.com www.msn.com, www.msn.com, slate.msn.com, slate.msn.com, www.msn.com, www.msn.com communities.msn.com, communities.msn.com, chat.msn.com Our clickstream data file consisted of the 1,019,341 clickstreams for each of the user sessions in the day. It was 23MB in size after decoding the raw domain names to domain IDs. We implemented SPNAV in Perl and ran it on this clickstream file using a machine with a 550MHz Intel processor and having four gigabytes of memory. We had twenty test cases containing different width, depth and starting page criteria. Table 2 shows the different criteria used in each case, the performance of the algorithm with each criteria and the number of nodes created (LL in the Width Crit. Column stands for "Last Level", L2 in the Width Crit. Column stands for "Level 2", etc.) Case Depth Width 
No. Crit. Crit. 10 &lt;--4 LL&gt;=I 0% Case Depth Width Start Run # of No. Crit. Crit. Page Time Nodes Our results above show that depth and starting page criteria have the greatest effect on the number of nodes created and the amount of time required for the program to complete. For example, in case no. 2, we look for all funnels with depth &lt;= 4. This run took 650 seconds to complete and occupied 199,612 nodes. Decreasing the depth to &lt;= 3 (case no. 1), the run took only 540 seconds to finish and occupied 56,293 nodes (26% of nodes occupied with depth of 4): The effect of limiting the starting pages is even more dramatic. Supplying a depth criteria of &lt;= 4 and applying starting page criteria of five pages (case no. 3), the algorithm took 221 seconds to complete and occupied only 36,247 nodes. Here we see quite a substantial improvement when starting page and depth criteria are applied. An exact measure of improvement depends on the quality of the data set, the number of pages in the clickstream and the number of unique click paths found. We also experimented with width criteria; the results demonstrated that width did not have much of an effect on processing time. Some interesting funnels found using SPNAV on our data set were: MSNBC Abandon Entertainment MSNBC bCentral ~ISN Entertainment MSN Search MSN Abandon bCentral MSN MSN Entertainment MSN MSN Entertainment VISN MSNBC Entertainment MSN Underwire MSNBC MSN MSNBC Search MSN Search Carpoint MSN MSNBC Entertainment MSN Underwire MSNBC MSN MSNBC The first record in the table represents a two level funnel revealing that 9.3% of all hits to MSNBC were followed by no other hits. The second record in the table represents a two level funnel showing that 3% of hits on MSN Entertainment pages in the day were followed by a hit on MSNBC next. The second three level funnel (record six) indicates that of all hits to MSN bCentral, 51% of them were followed by hits to the MSN Portal and 38% were subsequently followed by another hit to MSN. The first four level funnel (record thirteen) reveals that of all page views on the MSN Entertainment site, 29% of them were trailed by a hit to www.msn.com, 1.3% were then followed by a visit to underwire.msn.com and finally 0.55% returned to the MSN Portal Home Page. These are extremely interesting findings from a business point of view. The first record tells us that after this group of users hit MSNBC, over 9% of the time they abandoned the network in their current session. For the funnels starting with hits on the bCentral Service, we see that 51% of the succeeding hits did not stay inside the site; rather, they went to the MSN Home Page. These are indications that for this particular group of users, the MSNBC and bCentral sites could restructure some pages in order to achieve greater user stickiness. Similarly, 29% of hits on the MSN Entertainment site revert back to the MSN Portal immediately at~erwards. Research related to our work here falls in the area of web usage mining, and in particular, sequential pattern discovery in web usage mining. One of the first sequential pattern mining algorithms was the Apriori-based GSP written by Srikant and Agrawal in [1]. The hash-tree data structure discussed in [1] sparked the core concept behind SPNAV of running all subpaths with different starting points through trees to generate interesting funnels. Srivastava et al [2] presented a good survey of web usage mining up to January 2000. We note below several research projects in the area of sequential pattern discovery in web usage mining. Wu et al [3] presented SpeedTracer as a tool for discovering most frequented traversal paths and most frequently visited page groups. Borges [4] modeled a collection of user navigation sessions as a hypertext probabilistic grammar, where higher probability strings correspond to the navigation trails preferred by the user. Heuristics were given to compute small subsets of rules containing the best strings from a larger set of strings with probability above the cut-point, as well as to compute relatively small set of longer rules composed of links having high probability on average. Gaul and Thieme [5] presented an apriori type algorithm for mining all generalized subsequences of user navigation paths with prescribed minimal occurrence. Berendt [6] presented a tool, STRATDYN, that builds on top of and complements an older well known tool WUM (Web Utilization Miner). WUM helps analyst to discover user navigation patterns and to generate hypothesis. STRATDYN tests differences between navigation patterns described by a number of measures of success and strategy for statistical significance. Kato et al [7] proposed a tool for discovering the gap between web site publisher expectations and user behavior. The site publisher expectations are measured by the inter-page conceptual relevance, whereas user behavior is measured by inter-page access co-occurrence. The user paths are extracted based on the maximal forward reference as did Wu et al [3]. A visualization tool is presented for interactive analysis of user paths. In our service of data mining for all of MSN's worldwide properties, funnel analysis has been gaining more and more visibility and we have been receiving constant requests for funnel reporting. We have done funnel reports for the subscription and registration processes in a number of MSN's Internet properties. 
This has all been made possible through the funnel analysis framework we have architected and the tree-based solution created to extract all interesting funnels based on depth, width and starting page criteria. The tree based solution maps out all relevant funnels starting only with pages the business managers are interested in, and the funnels generated can be regularly written out to a database in order to allow for interactive querying. 
We are investing more work into improving our funnel analysis algorithms. In particular, we are researching ways to make the algorithm more scalable to large data set sizes. Much work is still left to be done in this area, with the vision of eventually incorporating into our decision support systems a tool that takes in data sets of any size and automatically generates interesting funnels in matter of seconds. 
Our thanks to all the MSN iDSS team members who have supported us in writing this paper. [1] R. Srikant, R, Agrawal, "Mining Sequential Patterns: [2] J. Srivastava, R. Cooley, M. Deshpande, P-N Tan, "Web [3] K-L Wu, P. S. Yu and A. Ballman. "Speedtraeer: a web [4] J. Borges, "A Data Mining Model to Capture User Web [5] W. Gaul and L. Thieme, "Mining web navigation path [6] B. Berendt, "Web usage mining, site semantics, and the [7] H. Kato, T. Nakayama, Y. Yamane, "Navigation Analysis 
