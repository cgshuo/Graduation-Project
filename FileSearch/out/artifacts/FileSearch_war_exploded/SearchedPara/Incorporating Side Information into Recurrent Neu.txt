 Neural network approaches to language modelling (LM) have made remarkable performance gains over traditional count-based n gram LMs (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2011). They offer several desirable characteristics, includ-ing the capacity to generalise over large vocabular-ies through the use of vector space representation, and  X  for recurrent models (Mikolov et al., 2011)  X  the ability to encode long distance dependencies that are impossible to include with a limited context windows used in conventional n gram LMs. These early papers have spawned a cottage industry in neu-ral LM based applications, where text generation is a key component, including conditional language models for image captioning (Kiros et al., 2014; Vinyals et al., 2015) and neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).

Inspired by these works for conditioning LMs on complex side information, such as images and for-eign text, in this paper we investigate the possibility of improving LMs in a more traditional setting, that is when applied directly to text documents. Typi-cally corpora include rich side information, such as document titles, authorship, time stamp, keywords and so on, although this information is usually dis-carded when applying statistical models. However, this information can be highly informative, for in-stance, keywords, titles or descriptions, often in-clude central topics which will be helpful in mod-elling or understanding the document text. We pro-pose mechanisms for encoding this side informa-tion into a vector space representation, and means of incorporating it into the generating process in a RNNLM framework. Evaluating on two corpora and two different languages, we show consistently sig-nificant perplexity reductions over the state-of-the-art RNNLM models.

The contributions of this paper are as follows: 1. We propose a framework for encoding struc-2. We introduce a new corpus, the RIE corpus, 3. We provide empirical analysis showing consis-We first review RNNLM architecture (Mikolov et al., 2011) before describing our extension in  X 2.2. 2.1 RNNLM Architecture The standard RNNLM consists of 3 main layers: an input layer where each input word has its embedding via one-hot vector coding; a hidden layer consisting of recurrent units where a state is conditioned recur-sively on past states; and an output layer where a target word will be predicted. RNNLM has an ad-vantage over conventional n-gram language model in modelling long distance dependencies effectively.
In general, an RNN operates from left-to-right over the input word sequence; i.e., where f ( . ) is a non-linear function, e.g., tanh , ap-plied element-wise to its vector input; h t is the cur-rent RNN hidden state at time-step t ; and matrices W and vectors b are model parameters. The model is trained using gradient-based methods to optimise a (regularised) training objective, e.g. the likelihood function. In principle, a recurrent unit (RU) can be employed using different variants of recurrent struc-tures such as: Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Gated Recur-rent Unit (GRU) (Cho et al., 2014), or recently deeper structures, e.g. Depth Gated Long Short Term Memory (DGLSTM)  X  a stack of LSTMs with extra connections between memory cells in deep layers (Yao et al., 2015). It can be regarded as being a generalisation of LSTM recurrence to both time and depth. Such deep recurrent structure may capture long distance patterns at their most general. Empirically, we found that RNNLM with DGLSTM structure appears to be best performer across our datasets, and therefore is used predominantly in our experiments. 2.2 Incorporating Side Information Nowadays, many corpora are archived with side in-formation or contextual meta-data. In this work, we a) b) argue that such information can be useful for lan-guage modelling (and presumably other NLP tasks). By providing this auxiliary information directly to the RNNLM, we stand to boost language modelling performance.

The first question in using side information is how to encode these unstructured inputs, y , into a vector representation, denoted e . We discuss several meth-ods for encoding the auxiliary vector: BOW additive bag of words, e = average the average embedding vector, bigram convolution with sum-pooling, RNN a recurrent neural network over the word se-From the above methods, we found that BOW worked consistently well, outperforming the other approaches, and moreover lead to a simpler model with faster training. For this reason we report only results for the BOW encoding. Note that when using multiple auxiliary inputs, we use a weighted combi-nation, e = The next step is the integration of e into the RNNLM. We consider two integration methods: as input to the hidden state (denoted input ), and con-nected to the output softmax layer ( output ), as shown in Figure 1 a and b, respectively. In both cases, we compare experimentally the following in-tegration strategies: add adding the vectors together, e.g., using x t + stack concatenating the vectors, e.g., using mlp feeding both vectors into an extra perceptron Note that add requires the vectors to be the same di-mensionality, while the other two methods do not. The stack method can be quite costly, given that it increases the size of several matrices, either in the recurrent unit (for input ) or the output mapping for word generation. This is a problem in the latter case: given the large size of the vocabulary, the matrix able effect on training time (and presumably also propensity to over-fit). The output+stack method does however have a compelling interpretation as a jointly trained product model between a RNNLM and a unigram model conditioned on the side in-formation, where both models are formulated as softmax classifiers. Considered as a product model (Hinton, 2002; Pascanu et al., 2013), the two com-ponents can concentrate on different aspects of the problem where the other model is not confident, and allowed each model the ability to  X  X eto X  certain out-puts, by assigning them a low probability. Datasets. We conducted our experiments on two datasets with different genres in two languages. As the first dataset, we use the IWSLT2014 MT track iary information, including: title, description, key-words, and author related information. We chose statistics of the training set is shown in Table 1. We used dev2010 (7 talks/817 sentences) for early stop-ping of training neural network models. For evalu-ation, we used different testing sets over years, in-cluding tst2010 (10/1587), tst2011 (7/768), tst2012 (10/1083).

As the second dataset, we crawled the entire Euro-sions. Such sessions contain useful structural in-formation, namely multilingual texts divided into speaker sessions and topics. We believe that those texts are interesting and challenging for language modelling tasks. Our dataset contains 724 plenary sessions over 12.5 years until June 2011 with mul-dataset by RIE 5 ( R ich I nformation E uroparl). We randomly select 200/5/30 plenary sessions as the training/development/test sets, respectively. We be-lieve that the new data including side information pose another challenge for language modelling. Fur-thermore, the sizes of our working datasets are an or-der of magnitude larger than the standard Penn Tree-bank set which is often used for evaluating neural language models.
 Set-up and Baselines. We have used cnn 6 to im-plement our models. We use the same configura-tions for all neural models: 512 input embedding and hidden layer dimensions, 2 hidden layers, and vocabulary sizes as given in Table 1. We used the same vocabulary for the auxiliary and modelled text. We trained a conventional 5  X  gram language model using modified Kneser-Ney smoothing, with the KenLM toolkit (Heafield, 2011). We used the Wilcoxon signed-rank test (Wilcoxon, 1945) to mea-sure the statistical significance ( p &lt; 0 . 05 ) on dif-ferences between sentence-level perplexity scores of improved models compared to the best base-line. Throughout our experiments, punctuation, stop words and sentence markers (  X  s  X  ,  X  /s  X  ,  X  unk  X  ) are fil-tered out in all auxiliary inputs. We observed that this filtering was required for BOW to work rea-sonably well. For each model, the best perplexity score on development set is used for early stopping of training models, which was obtained after 2-5 and 2-3 epochs on TED Talks and RIE datasets, respec-tively.
 Results &amp; Analysis. The perplexity results on TED Talks dataset are presented in Table 2 and 3. RNNLM variants consistently achieve substan-tially better perplexities compared to the conven-tional 5  X  gram language model baseline. 7 Of the ba-sic RNNLM models (middle), the DGLSTM works consistently better than both the standard RNN and the LSTM. This may be due to better interactions of memory cells in hidden layers. Since the DGLSTM experiments. For TED Talks dataset, there are three kinds of side information, including keywords, ti-tle, description. We attempted to inject those into different RNNLM layers, resulting in model vari-ants as shown in Table 2. First, we chose  X  X ey-words X  (+k) information as an anchor to figure out which incorporation method works well. Comparing input +add+k, input +mlp+k and input +stack+k, the largest decrease is obtained by output +mlp+k con-sistently across all test sets (and development sets, not shown here). We further evaluated the addition of other side information (e.g.,  X  X escription X  (+d),  X  X itle X  (+t)), finding that +d has similar effect as +k whereas +t has a mixed effect, being detrimental for one test set (test2011). We suspect that it is due to often-times short sentences of titles in that test, af-ter our filtering step, leading to a shortage of useful information fed into neural network learning. Inter-estingly, the best performance is obtained when in-corporating both +k and +d, showing that there is complementary information in the two auxiliary in-puts. Further, we also achieved the similar results in the counterpart of English part (in French) using output +mlp with both +t and +d as shown in Ta-ble 3. In French data, no  X  X eywords X  information is available. For this reason, we run additional exper-iments by injecting English keywords as side infor-mation into neural models of French. Interestingly, we found that  X  X eywords X  side information in En-glish effectively improves the modelling of French texts as shown in Table 3, serving as a new form of cross-lingual language modelling.

We further achieved similar results by incorpo-rating the topic headline in the RIE dataset. The consistently-improved results (in Table 4) demon-strate the robustness of the output +mlp approach. We have proposed an effective approach to boost the performance of RNNLM using auxiliary side infor-mation (e.g. keywords, title, description, topic head-line) of a textual utterance. We provided an empir-ical analysis of various ways of injecting such in-formation into a distributed representation, which is then incorporated into either the input, hidden, or output layer of RNNLM architecture. Our ex-perimental results reveal consistent improvements are achieved over strong baselines for different datasets and genres in two languages. Our future work will investigate the model performance on a closely-related task, i.e., neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). Fur-thermore, we will explore learning methods to com-bine utterances with and without the auxiliary side information.
 Acknowledgements The authors would like to thank the reviewers for valuable comments and feedbacks. Cong Duy Vu Hoang was supported by research scholarships from the University of Melbourne, Australia. Dr Trevor Cohn was supported by the ARC (Future Fellow-ship).

