 The internet has grown to become ubiquitous in our daily lives. From a humble beginning in the early to mid 1990 X  X , the internet has sustained a tremendous growth rate of approximately 1 million web page per day and it currently contains over 3 billion web pages.
 esting case. It is one of the largest data repositories known to man, constantly growing and changing. Each web page may contain text, images, sound, video clips. Very few web pages contain intentionally coded metadata 1 which may fa-cilitate their easy retrieval. Each web page is coded in a common format and language, known as the hypertext markup language (HTML). The specification of HTML is quite loose, and  X  X orgiving X . Thus, web pages containing some non-critical errors nevertheless still can be displayed when viewed by a user using a browser. Most web pages contain links to other web pages, which serve as  X  X ontainers X  of information. Such pointed web pages may contain the informa-tion itself, or they may in turn point to other web pages, which may serve as their  X  X ontainers X  of information. Thus, from a graph-theoretic point of view, each web page may be considered as a node, with incoming links from other nodes, and outgoing links pointing to other nodes. This is commonly known as web graphs.
 (a) matching the query to the content of web pages, and (b) through links. In approach (a), one simple way would be to match keywords from the user query with those occurring in the text in the web page; the one with the most matches will be ranked closest to the query. A common example is the Shared Point Portal c of Microsoft, in which sophisticated ways were used to weigh the words according to their occurrence in the total number of documents/web pages in the repository [2]; whether the word is a commonly occurred word, etc. In approach (b), an incoming link is considered as a  X  X ote X  of confidence from another web page. Thus, a web page with many incoming links, each of these links is coming from pages which themselves may have many incoming links would be considered as important as it is  X  X ost popular X  from a  X  X oting X  point of view. One of the most celebrated formulations of the second approach is the work of Brins and Page [1] which is one of the mechanisms underlying the way in which the popular search engine: Google arranges query results and presents them to users. In this paper we will concentrate on approach (b) and we will not discuss approach (a) further.
 brief introduction to the page rank determination as given in [1]. In Section 3, we will show how Google X  X  page rank equation can be modified to find personalized page ranks by the introduction of a forcing function term which can be considered as a basis expansion function, each basis being an identified topic which a person wishes to consider. In Section 4, we will show how the generalized linear matrix stochastic equation introduced in [1] discussed in Section 3 can be modified to become a multi-layered feedforward neural network. In Section 5 we will show a number of indicative experiments based on the web repository WT10G which demonstrate the potential of the proposed modifications to the Google page rank equation. Some conclusions are drawn in Section 6. The page rank, or page importance, as provided in [1], is independent of the content of the web pages, but solely dependent on the links among the web pages, and thus their topological structure. According to [1]: Here x i is the page rank of the i -th web page, i =1 , 2 ,...,n , the total number of web pages in the web repository is assumed to n . pa [ i ] is the set of pages pointing to i , e i is the default authority assigned to page i , h j is the outdegree of j ,and d  X  (0 , 1) is a dumping factor. When stacking all the x i into an n-vector X , we obtain where W  X  X  n  X  n is a matrix called transition matrix with elements w i,j =1 /h j if there is a hyperlink from node j to node i ,and h j is the total number of outlinks of node j ,and w i,j = 0 otherwise. Thus, W is a non X  X ull stochastic matrix, where each column either sums to 1 or to 0. In [1], the forcing factor E =[ e 1 ,...,e n ] is a vector having all the elements equal to 1, i.e. e i =1for each i ,and denotes the transpose operator.
 and exists provided d W 1 &lt; 1 holds, where  X  1 denotes the norm 1 operator, i.e. W 1 = max j ( i | w i,j ) | . When the W satisfies the above hypothesis, the so-lution is given by X =(1  X  d )( I n  X  d W )  X  1 E ,where I n denotes the n  X  X imensional identity matrix. The page rank as determined by Eq(1) is valid for the entire web repository. Once the topology of the web repository is given, and d fixed, then the page ranks are fixed. There are circumstances where a user may wish to modify such page ranks. This is often referred to as personalized page ranks. The main idea underlying the approaches to personalized page ranks is that the concept of  X  X age importance X  is not absolute, but it depends on the particular needs of a user or a search engine. For example, the homepage of a large directory may be authoritative for a general purpose search engine, but not important for a portal specialized on the topic  X  X ine X . Thus, the suggested solution consists of building different page ranks, each one specialized on a topic [3], a user [4] or a query [5].
 rameterize the page rank in Eq(1). We assume that a set of specialized page ranks x 1 ,..., x m is available. These page ranks, which may be built using differ-ent kinds of features (e.g. the page content, the URL, etc.), should capture some of the page properties which are useful to estimate the authority of a document for a given user 2 . We will assume that a parameterization of the page rank can be obtained using a linear combination of these specialized page ranks. In other words, a parameterized page rank x ( p ) is constructed as follows: where  X  h  X  X  are parameters which satisfy m h =1  X  h =1,and p =[  X  1 ,..., X  m ] . In fact, x ( p ), called adaptive page rank, can be personalized by varying the parameter set p . In the following, we will denote by M  X  X  n  X  m the matrix [ x 1 ,..., solution of (1) when the forcing function is e h and the transition matrix is the original W used in the page rank equation (1). Since x h =( I n  X  d W )  X  1 e h , x Thus, x ( p ) is by itself a specialized rank whose default authorities are a linear combination of those used for computing x 1 ,..., x m .
 problem [6], involving a quadratic cost function, e.g., expressing the square of difference of the distance between the page ranks as given by Eq(1), and those of Eq(4); a set of linear constraints which expresses user requirements, and the linear dynamical constraint Eq(4). Such solutions can be obtained quite quickly using common quadratic programming solution software [6]. It is interesting to ask the question: is it possible to replace the linear equation in Eq(1) by a nonlinear equation, e.g., a multi-layered feedforward neural network. The intuition is that a nonlinear forcing function may produce a better result than those provided by Eq(4), as there is no inherent reason why the underlying graph mapping function is linear. Indeed there are reasons to suspect that the underlying mapping function is nonlinear. It is noted that the page importance of each web page is a function of the page importance of those pages pointing to the page. The topology of the internet is such that there are relatively only a very small number of web pages pointing to a particular web page. In turn, these web pages are pointed to by a relatively small number of web pages, and so on. Hence, intuitively, it would be useful if we encode this limited neighborhood 3 dependency information in terms of a graph-dependent model. The page ranks as determined by Eq(1) on the other hand, does not have this graph-dependent notion. It assumes that potentially all web pages in the web graph can influence the page rank of an individual web page, or more generally one another. It depends on the topology of the web graph as encoded in the transition matrix to limit the influence of the other web pages which may not have any direct connections to the particular web page concerned or its parents, grand-parents, etc. Thus instead of relying on the transition matrix W , in the graph-dependent approach, it explicitly encodes the information in a graph-dependent fashion by only considering the parents, grand-parents, and generations before. In other words, the page importance is a function of where the web page is located in relation to its parents, grand-parents, etc. As we will implement the nonlinear dependency using a multi-layered feedforward neural network, we will call this a graph neural network.
 where N (  X  ) denotes a matrix nonlinear function. The implementation of this function can be realized using a multilayered feedforward neural network. In general, there is only one hidden layer, as it is known that such an architecture is a universal approximator to an arbitrary degree of accuracy provided that there is a sufficient number of hidden layer neurons [7].
 tained by minimizing a quadratic cost function which, e.g., may express the square of the distance of the difference between the page ranks as determined by Eq(1) and those obtained by Eq(5), while satisfying the nonlinear equa-tion Eq(5). The derivation is tedious though easy conceptually and hence omit-ted here. In this section, we will show a number of experiments using the web repository WT10G. The WT10G data set is provided by CSIRO, Australia and was pre-pared as a benchmark for the Text Retrieval Conference (TREC). The WT10G presents a snapshot of a portion of the World Wide Web. The collection paid special attention to the connectivity among the Web pages. Hence this set of Web collection is suitable for evaluations of search engine algorithms based on connectivity concepts. There are 1,692,096 documents from 11,680 servers con-tained in the dataset.
  X  X port X ,  X  X ennis X ,  X  X ockey X ,  X  X urgery X , and  X  X ancer X . The association of pages to topics was carried out by a naive Bayesian classifier [8, 9]. Bayesian text classi-fication consists of a learning and a production phase. During the learning phase some documents (about 20 in our experiments) which are known to address a topic are collected in order to produce a statistical analysis of the word occur-rences. In the production phase, a classifier estimates, for each document in the dataset, the probability that it belongs to the topic by comparing the statistical data and the words of the document. The classifier assigns a document to a class when the probability is larger than a predefined threshold. In our case, the topic thresholds are defined by human inspection of the highest scored documents returned by the classifier. A more general approach was suggested in [10]. ity of the statistical analysis, both learning and production phases may in-clude a feature extraction process which decreases the number of words con-sidered for each document, selecting only the most useful ones [2]. In our case, feature extraction consists of three parts: (a) most common words (i.e. stop words) were removed; (b) a stemming procedure was applied 4 ; and (c) during the production phase only the 20 most promising words were considered by selecting from the document those having the highest frequency in the train-ing set 5 .
 uments which discuss a number of topics at the same time, because the presence of many irrelevant words for the topic does not impair the extraction of the most relevant ones. Multi X  X opic documents are usually news wires, directories and other general content pages. Since, they play a different role with respect to documents dealing with a single topic, a special class  X  X eneral X  was built for them. We simply decided to include into topic  X  X eneral X  all the pages that were previously inserted in 3 or more topics. Finally, the topic  X  X ther pages X  was considered which includes all the documents not belonging to any classes indicated.
 the mean of the page rank for each topics. Page rank is maximal on the class  X  X eneral X , where the average is much higher than those on other topics. This observation confirms the claim that the class of  X  X eneral X  pages plays a different role on the Web and contains directories and other well referenced documents. denotes the percentage of pages of topic i which have been classified also in topic j . Notice that some apparently unrelated topics have a non void intersection, e.g. Windows and Surgery. A manual inspection of the web pages concerned shows that the pages in these intersections are partially due to documents that actually discuss the two topics (e.g. pages about software for medical applications) and partially due to general documents which our method was not able to move to the class  X  X eneral X  6 .
 computed, one for each topic. For each specialized rank, we used the same transi-tion matrix W as Google X  X  page rank and a forcing function E i =[ e i, 1 ,...,e i,n ] , where and Q is a normalizing factor such that E i 1 = N and N is the number of pages in the topic dataset. The normalization of the e i allows to derive ranks with comparable values and simplifies the optimization process.
 page ranks. Each row of the table compares a specialized page rank to the page rank as obtained in Eq(1), while the columns specify the set of pages where the comparison was carried out. For example, the cell of row  X  X ine X  and column  X  X indows X  displays the ratio of the average of the specialized page rank  X  X indows X  on the pages of class  X  X ine X  by the mean of page rank. Formally, cell in row i and column j is r i,j pr page rank on the pages of j -topic and pr j is the mean of page rank on the pages of j -topic. Thus, the cells displaying values larger than one define the classes where the specialized ranks are larger than page rank.
 scores of the pages about the corresponding topic and about related topics (see, for example, wine and cooking). The data also confirms that the improvement is large for smaller topics, where the rank is concentrated in a few pages. In the limiting case of the class  X  X ther X , which contains most of the pages of the dataset, the improvement is very close to 1. Note also the scores of documents in apparently unrelated topics may be affected: the magnitude of the change, which is however much smaller than the modification which affects the related topics, is probably due to the web connectivity and the  X  X loseness X  of the topics on the web graph. For example, the rank of pages on  X  X inux X  was improved because the rank on pages of  X  X indows X  was improved.
 the pages were sorted from the most important to the least important. Figure 1 shows the distribution of the values of the page rank and the specialized page ranks on  X  X indows X ,  X  X inux X ,  X  X ooking X  and  X  X ther X  pages 7 .
 intuitive idea underlining the concept of specialized page rank: it depends on the number of the pages of the topic. In fact, X  X inux X  is the topic with the smallest number of pages (see Table 1),  X  X ooking X  and  X  X indows X  are topics with moderate number of pages. On the other hand, Google X  X  page rank can be seen as the specialized page rank of the topic which includes all the pages ( e =[1 ,..., 1] ), and,  X  X ther X , which contains most of the pages, has similar magnitude as page rank. In topics with a small number of pages, the rank is concentrated in few pages, while in topics with larger number of pages, it is more widespread. The rank of the most important pages of topics with small number of pages may be up to 10 times larger than the corresponding ones of topics with large number of pages (see Figure 1).
 becomes soon close to 0. In fact, specialized page ranks may be 0, since the pages not belonging to the topics have been assigned a zero default rank, i.e. e h =0.On the other hand, Google X  page rank is never very close to zero, because e h =1for each h , and the behavior of specialize page rank in the category of  X  X ther X  pages is similar to that of Google X  X  page rank (the two plots overlap almost everywhere). posed method of obtaining personalized page ranks. Acting as a user, interested in wine, we selected four pages on wine and designed three constraints to in-crease their scores. The constraints consisted of inequalities which require the adaptive page rank to be at least a 50% larger than their respective Google X  X  page rank 8 . Moreover, in order to keep the scores of the documents that are not about wine as close as possible to their Google X  X  page ranks, the optimization problem contained also the linear function and the constraints.
 confirms that the scores of the three pages were actually augmented by 50%. Moreover, for each page, the absolute position in the ordering established by the adaptive page rank versus the position determined by Google X  X  page rank is shown in Figure 2. Each point in the graph represents a page. The diagonal dashed line corresponds to the line y = x , and the points above such a line represent pages whose adaptive page rank is greater than the corresponding Google X  X  page rank, whereas points under it represent pages whose adaptive page rank has decreased.
 the pages which belong to the individual topics of interest. The  X  X ine X  plot shows that most of the pages on this topic gained a higher rank. On the other hand, for  X  X ooking X , which is a related topic, there are different kinds of behav-iors. There is a set of pages whose distribution closely resemble those observed on  X  X ine X . In fact, some pages on  X  X ooking X  belong also to the class  X  X ine X  and received the same scores. Moreover, there are pages displayed above the line y = x which are not related to the  X  X ine X  pages. Finally, some documents are not about  X  X ine X , but they are pointed to by pages on  X  X ennis X . In fact, they have an intermediate behavior and lay just between the diagonal dashed line and the pages similar to  X  X ooking X .
 pages which fall within a selected topic using the neural network approach. A training set of 20 pages is selected from the topic of interest and 3880 web pages are selected randomly outside the topic of interest. This forms a training set with just 3900 web pages. The neural network version of the page rank equation Eq(5) is then trained for 2500 epochs.
 on the test set which consists of 1 , 688 , 196 web pages. We consider a node is on target if the computed page rank differs from the target page rank by less then 5%. For example, a computed page rank of 9 . 6 is considered on target with the target rank as 10.
 respectively and increased the training iterations to 5000 epochs to investigate the impact of some of the training parameters. The results are shown in Table 5. This shows that variation of these parameters does not have a significant impact on the final results.
 Section 4. In this paper, we considered the page rank equation underlying the Google search engine [1] and considered two concepts, viz., modifying the forcing function by introducing a basis decomposition, and using a nonlinear version of such an equation. Through simple experiments using the web repository, WT10G, we have demonstrated that our proposed approaches work well.
 example, it would be interesting to consider other types of neural network ar-chitectures in the place of multi-layered feedforward neural networks, e.g., Ko-honen X  X  self organizing map [12]. The self organizing map is one way in which training examples can be clustered into groups which are topologically close in the high dimensional feature space. It may be possible to replace the naive Bayesian method introduced in Section 3 by the self organizing map.

