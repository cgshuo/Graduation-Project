 fi nd the optimal number of clusters. Within the ACGSA, a variable fi 1. Introduction
Data clustering is an unsupervised technique that distributes unlabeled data into groups based upon dissimilarity measures ( Jain et al., 1999 ; Bezdek, 1981 ). Clustering techniques are applied to a wide variety of applications such as image segmentation, data compression, pattern recognition, and machine learning ( Jain et al., 1999 , 2000 ; Ball and Hall, 1967 ; Das et al., 2009a) . These can be classi fi ed into two main categories: hierarchical and partition clustering. The former clustering constructs a tree like nested structured partition of datasets ( Xu and Wunsch, 2009 ).
However, it has some drawbacks such as it is computationally expensive and it fails to separate the overlapping clusters (Das et al., 2009a) . Whereas the latter category that is partition clustering technique divides data points into some pre-speci number of clusters without the hierarchical structure ( Xu and
Wunsch, 2009 ). Partition clustering is widely used in pattern recognition than the hierarchical clustering as reported in litera-ture ( Jain et al., 2000 ). In many clustering problems, the number of clusters may be unknown or dif fi cult to estimate. Recently, researchers used the metaheuristic techniques to solve partition clustering problem. A comprehensive review of metaheuristic algorithms for clustering problems can be found in Abraham et al. (2007) and Hruschka et al. (2009 ). However, in most of the clustering techniques based on metaheuristic algorithms, it has been observed that we need to specify the number of clusters as an input prior to running the algorithm. We would like to determine the number of clusters during run time.
 approach, called Automatic Clustering Using Gravitational Search
Algorithm (ACGSA), for fi nding the number of clusters in a given dataset automatically. It simultaneously fi nds both the number of clusters and the corresponding partitioning. The two new concepts i.e. threshold setting and weighted cluster centroid computation have been introduced for fi nding the optimal number of clusters accurately and ef fi ciently. The new fi tness function has also been proposed to guide the search accurately as it plays a vital role for performance evaluation of metaheuristic based clustering techni-ques. The performance of ACGSA has been evaluated on real-life datasets with respect to three clustering metrics namely the number of clusters, inter-cluster and intra-cluster distances. The results have been compared with state-of-the-art clustering tech-niques such as Automatic Clustering using Modi fi ed Differential Evolution (ACDE) ( Das et al., 2008b ), Dynamic Clustering using Particle Swarm Optimization (DCPSO) ( Omran et al., 2006 ) and
Genetic Clustering with an unknown number of clusters (GCUK) ( Bandyopadhyay and Maulik, 2002 ). The ACGSA has further been applied on fi ve well-known grayscale and color images for segmentation. The experimental results demonstrate that the number of clusters determined using ACGSA, is almost equal to the number of actual classes present in real-life datasets. The inter-cluster and intra-cluster distance values are better than those obtained using other existing techniques.

The rest of the paper is organized as follows. In Section 2 , related work done in the fi eld of partitional clustering techniques based on metaheuristic is described brie fl y. Section 3 presents basics of clustering and gravitational search algorithm. Section 4 presents the motivation and mathematical foundation for the proposed work. Section 5 presents real-life datasets, experimental setups and the experimental results. Section 6 covers the applica-tion of proposed approach in image pixel classi fi cation followed by conclusions in Section 7 . 2. Related works
Finding an optimal number of clusters in a given dataset is a challenging task. Recently, researchers focused on this clustering problem and tried to develop dynamic clustering techniques. Lee and Antonsson ( 2000 ) used an evolutionary strategy to cluster a dataset dynamically. They implemented variable length genomes to search for both centroids and the number of clusters.
Bandyopadhyay and Maulik (2002) proposed a variable string length genetic algorithm to solve the clustering problem using a single fi tness function. Their algorithm was called as Genetic
Clustering for Unknown K (GCUK). Jarboui et al. ( 2007 ) developed a discrete PSO algorithm to solve partitional clustering problems.
Omran et al. ( 2006 ) developed an automatic hard clustering scheme called as DCPSO. It started partitioning the dataset into relatively a large number of clusters in order to reduce the effects of initial conditions. They used PSO to select the optimal number of clusters. Ye and Chen (2005) used the hybridization of PSO and
K-means algorithm for detecting the cluster centers of geometrical structure datasets automatically. Abdule-Wahab et al. ( 2006 ) used a scatter search for automatic clustering. Das et al. (2008b) presented a new DE-based strategy, named as Automatic Cluster-ing using DE (ACDE), for hard partitioning problems. Das et al. (2008a) used the same concept as used in ACDE except that multi-elitist PSO instead of DE. Das and Konar (2009) used fuzzy concept in ACDE for image segmentation. Das and Sil (2010) extended
ACDE with a kernel induced similarity measure for image seg-mentation. Quadfel et al. ( 2010 ) used the same concept as used in
ACDE except that PSO was used in place of DE. Pan and Cheng (2007) proposed a framework for automatic clustering using Tabu
Search. They used the number of clusters, as a variable, and evolved it to an optimal number. Saha et al. proposed a technique for automatic evolution of clusters. They developed a variable string length genetic algorithm based clustering technique, which utilized the symmetry based distances ( Bandyopadhyay and Saha, 2008 ; Saha and Bandyopadhyay, 2010 ; Saha and Maulik, 2011 ).
Das et al. ( 2009b ) used Bacterial Evolutionary Algorithm for automatic clustering. Lee and Chen (2010) proposed improved
Differential Evolution algorithm with cluster number oscillation for automatic crisp clustering. Cai and Gong (2011) used the differential evolution with modi fi ed point symmetry based cluster validity index for automatic clustering. Hatamlou et al. (2012) used gravitational search for fi xed number of clusters in data clustering.
Recently, Masoud et al. ( 2013 ) used the combinatorial PSO for dynamic clustering. Sarkar and Das ( 2013 ) presented a 2-D histogram based multi-level thresholding approach for image segmentation.

The hybridization of metaheuristic algorithms has also been in use in clustering problems. Niknam and Amiri (2010) proposed a cluster optimization algorithm based on the combination of PSO, Ant Colony Optimization and K-Means and so did Siriporn and Kim ( 2009 ) using combination of GA, ACO and fuzzy C-Means. There are other techniques such as cuckoo search ( Gandomi et al., 2013a ), Bat ( Gandomi et al., 2013b ), Krill Herd ( Gandomi and Alavi, 2012 ) and fi re fl y( Gandomi et al., 2011 ) algorithms, which can be used in data clustering. However, the Gravitational Search Algorithm is yet to be applied to automatic clustering of real-life datasets as well as image segmentation. 3. Background
This section describes the clustering problem and gravitational search algorithm. 3.1. Clustering problem
The clustering algorithms aim to minimize within-cluster variation, called intra-cluster distance and maximize the between-cluster variation, called inter-cluster distance. The math-ematical description of partitional clustering is as follows:
Let the set of n input data points be X  X f x 1 ; x 2 ; ... ; x  X  X  x j 1 ; x j 2 ; ... ; x jd  X  A R d , and each measure x Clustering algorithms try to fi nd out K partitions of X , 2009 ): C a  X  ; i  X  1 ; 2 ; ... ; K  X  1  X  C \ C j  X   X  ; i ; j  X  1 ; 2 ; ... ; K and i a j  X  2  X  [ In general data points are assigned to clusters based on some similarity measures. The most commonly used similarity measure is the Euclidean distance, between data point x j and cluster center m of cluster C i and is de fi ned as d  X  3.2. Gravitational search algorithm Gravitational search algorithm (GSA) was fi rst introduced by Rashedi et al. (2009) , which is inspired by the laws of gravitation and motion. In this algorithm, the agents are considered as objects and their performance is measured by their masses. All these objects attract each other by the gravity force, and this force causes a global movement of all objects towards the objects with heavier masses. The heavy masses correspond to good solutions of the problem. In GSA, each mass has four speci fi cations: position, inertial mass, active gravitational mass and passive gravitational mass. The position of the mass corresponds to a solution of the problem, and its gravitational and inertial masses are determined by using fi tness function ( Cai and Gong, 2011 ).

During the search process, the agents are moved according to the following equations: x v i  X  rand i v t i  X  a t i  X  6  X  agent at iteration t and it is de fi ned as a  X  F
Here M t ii and F t i denote the inertial mass and total force acting on agent is a weighted sum of the forces exerted from K best agents having best fi tness) agents and is as given below:
F  X   X  where F t ij represents the force acting on agent i from agent j at iteration t and rand j is a random number in the interval  X  0 given as follows:
F  X  G  X  t  X  where M pi and M aj are the passive and active gravitational masses related to agent i respectively, G  X  t  X  is a gravitational constant at time t,  X  is a small constant, and R t ij is the Euclidean distance between two agents x i and x j at iteration t . The gravitational constant, G , is a function of initial value, G 0 , and iteration, t :
G  X  t  X  X  G 0 exp 1 t T  X  10  X  where G 0 is a constant and T is the total number of iterations. The gravitational and inertial masses are calculated by the fi function evaluation.

In this paper, we assume that the gravitational and inertial masses are equal and these are computed using the map of fi of an agent, the termination criterion is checked (i.e., the number of iterations or adequate fi tness function value). If the criterion is satis fi ed, the best solution is retained; otherwise these operations (Eqs. (5)  X  (9) )arerepeated.

The main strengths of GSA lie in the fact that it is memory-less and only the current positions of the agents participate in the updating procedure, in contrast to other metaheuristic algorithms such as PSO, GA ( Rashedi et al., 2009 ). 4. Proposed work
This section fi rst describes the motivation and mathematical foundation of proposed work followed by proposed approach for automatic clustering technique. 4.1. Motivation
The major contribution of this paper is a novel approach for the automatic clustering scheme. In this paper, a variable string length based clustering technique is proposed which is used to auto-matically fi nd the optimal number of clusters and partitions. The variable string encodes the centers of clusters with the corre-sponding threshold values. For this, a new method of threshold value setting for each cluster centroid is proposed. This enables the proposed algorithm to consider the variance of given dataset, which has not so far been considered in other contemporary techniques. Weighted Euclidean distance has been preferred for assignment of data-points to different clusters in place of Eucli-dean distance. The reason behind this is to make the proposed algorithm include the effect of threshold values corresponding to the cluster centers during the distance computation for approx-imate grouping of points as a cluster for all types of clusters. A new fi tness function is proposed here and is used for computing the fi tness of the agents. The GSA metaheuristic has been preferred.
The reason for adopting GSA is that it requires a few parameters for fi ne-tuning. For example, differential evolution (DE) requires many parameters such as crossover, mutation probability. In GSA, only one constant is required to be set.
 major shortcomings. First, it uses fi xed threshold cutoff value for all types of datasets whereas the fact is that it is dataset dependent. So, this was not acceptable as it should be made to change according to the dataset. Second, the threshold value has not been taken into consideration during the distance computa-tion. These two major weaknesses of ACDE motivated us to propose the new approach explained as under: 4.2. Automatic clustering using gravitational search algorithm tional search algorithm (ACGSA) consists of following four steps.
Step 4. The best agent will yield the optimal cluster centers
The main concepts of ACGSA and its steps are described in the following subsections 4.2.1. Cluster encoding in agents
ACGSA uses the cluster centroid-based encoding to represent the clustering solutions. In the proposed method, for n data points, each having d dimensions, for user speci fi ed maximum number of clusters
K , an agent is a vector of real numbers of dimension
K  X  K max d .The fi rst K max entries are positive real numbers in the range  X  0 ; 1 , each of which controls whether the corresponding cluster is to be activated or not during the process of clustering. The remaining entries are used for K max cluster centers, each having d dimensions. The vector ( V i  X  t  X  )ofanagent i is illustrated as where m i ; j is the j th cluster center of i th agent and Th threshold value of corresponding cluster centroid m i ; j
The Th i ; j s are the selection thresholds for selecting active cluster centers. These selection thresholds are assigned to each of the corresponding cluster centers. The value of selection threshold is set to the corresponding intra-cluster variation. If
Th 4 Threshold Specified , then m i ; j is selected for partitioning the given dataset otherwise not. This means that the j th cluster center in the i th agent is active and is selected for partitioning the dataset. Here Threshold Specified indicates speci fi ed threshold or cutoff value, which is based on mean standard deviation of given dataset. The rule for selecting the number of clusters speci the chromosome is as follows: If Th i ; j 4 Threshold Specified then m ij is active Else m ij is inactive.

For example, in two-dimensional data, the agent encodes the cutoff value is 0.4. Then, according to the rule mentioned above, only two thresholds are higher than 0.4 (i.e., 0.8 and 0.6) and been selected (shown in circle) for partitioning the dataset.
When the agents are updated during the search process of GSA, there are chances that none of the thresholds is greater than the cut-off threshold value. In such a situation, we randomly select two thresholds and reinitialize them to the values greater than the cut-off threshold to ensure that there are a minimum of two clusters. 4.2.2. Fitness evaluation
The partitioning of a dataset is achieved by optimizing the speci fi ed clustering criterion. A large number of clustering criteria have been reported in literature ( Jain et al., 1999 ; Xu and Wunsch, 2009 ). Most of the criteria are based on the within-cluster and between-cluster scatter matrices. This paper uses trace  X  S The within-cluster variation  X  S W  X  measures how much scattered are the data points from their cluster centroid. S W is de S W  X   X  where v ij is a partition matrix. v ij  X  1if X j A cluster i otherwise zero.

The between-cluster variation  X  S B  X  measures how much scat-tered are the cluster centroids from the mean of whole dataset. S is de fi ned as S  X   X  K criterion is required for high quality of clustering solutions. Sheng et al. (2008) suggested a penalty term for this criterion as it is biased towards increasing the number of clusters. They included the user speci fi ed maximum number of clusters in their penalty term. The penalty function has been modi fi ed in this paper and is mathematically reformulated as Fit  X  trace  X  S 1 W S B  X  n 1  X  K 1  X   X  14  X  where 1 =  X  K 1  X  is used as a penalty function and K represents the number of clusters. The penalty function has been introduced to ensure that at least two clusters exist in the given dataset. 4.2.3. Cluster center validation
There is a possibility that the number of data points assigned to a cluster center is less than two. This may be attributed to the fact that the selected cluster center (s) is (are) outside the boundary of the distribution of data points. To cope up with this problem, cluster center positions of particular agents are reinitialized by an average computation method ( Das et al., 2008b) . 4.3. Complexity analysis In this section, the complexity analysis of ACGSA is presented. We have analyzed both time and space complexities of the proposed technique. 4.3.1. Time complexity 4.3.2. Space complexity
ThemajorspacerequirementofACGSAclusteringtechniqueisdue toitsnumberofagents( agent size ). Thus, the total space complexity of ACGSA clustering technique is O agent size string length  X  X  . 5. Experimentation and results
To evaluate the performance of ACGSA, fi ve real life datasets mentioned in Section 5.1 have been used for experimentation. We have compared the performance of ACGSA with the three well established automatic clustering algorithms such as ACDE, GCUK and DCPSO. The results are evaluated and compared using some acceptable cluster quality measures such as the optimal number of clusters found, inter-cluster and intra-cluster distances ( Jain et al., 1999 ). Inter-cluster distance is a measure of separation between clusters whereas the intra-cluster distance denotes a measure of homogeneity within a cluster. For better clustering the values of intra-cluster and inter-cluster distances must be as small as possible and as large as possible respectively. 5.1. Real-life datasets used datasets of UCI database ( Blake and Merz, 1998 ). Table 1 presents the details of these datasets. 5.2. Experimental setup in the proposed approach (number of agents, maximum Iterations, K max , and G 0 ). These parameters were fi xed that are as follows: The number of agents and K max are set to 30 and 15 respectively. The maximum number of iterations is fi xed to 200. G 0 is set to 100.
For each dataset, clustering algorithm is run for 40 times for the comparison. 5.3. Implementation results 2008b ), DCPSO ( Omran et al., 2006 ), GCUK ( Bandyopadhyay and
Maulik, 2002 ), and classical DE ( Das et al., 2008b ). Three quality metrics have been used i.e. optimal number of clusters obtained, inter-cluster and intra-cluster distances. The results have been calculated in terms of mean and standard deviation over 40 independent runs in each case. Table 2 shows the optimal number of clusters obtained. The results reveal that DCPSO, GCUK and classical DE produce two clusters for Iris dataset. ACGSA and ACDE both produce three clusters. For Wine dataset, ACGSA produces three clusters in each run. For Glass dataset, ACGSA, ACDE and
DCPSO provide six clusters in almost each run. For Breast Cancer dataset, we have found that ACGSA, ACDE and GCUK fi nd two clusters in almost each run. For Vowel dataset, ACGSA produces six clusters in each run. However, ACDE produces near optimal number of clusters, but not in each run. DCPSO, GCUK and classical
DE do not produce the optimal number of clusters. Hence, ACGSA performs better than other well known techniques in respect of the optimal number of clusters.
 by the best and the second best clustering algorithms. We have taken 40 as the sample size for unpaired t -tests. Table 3 shows the results of unpaired t -tests based on number of clusters presented in Table 2 . For all the datasets except Glass dataset, ACGSA is statistically signi fi cant as compared to other clustering techniques. distances obtained from clustering algorithms. For all the datasets except Breast Cancer, ACGSA provides compact clusters as com-pared to other clustering algorithms. ACDE produces clusters that have minimum distance between data points for Breast Cancer dataset. It has also been noted from Table 5 that for Iris, Wine, and Breast Cancer datasets ACGSA creates well-separated clusters as compared to other techniques. For Glass and Vowel data-sets, Classical DE and ACDE generate well separated clusters respectively.

The results also depict that the clusters formed by ACGSA are well separated and cohesive in nature as demonstrated above. The inter-cluster distance obtained from ACGSA is almost three times of intra-cluster distance. None of the existing clustering algorithms generates much well separated and compact cluster as are produced by the proposed algorithm. Tables 4 and 5 also indicate that clusters formed by ACGSA exhibit one important property of clustering. That is, intra-cluster distance is not larger than the inter-cluster especially in these datasets. But other techniques violate this property. Fig. 2 (a) shows the ratio of inter-cluster to intra-cluster distances among above mentioned fi ve datasets. The results indicate that ACGSA produces compact and well separated clusters as compared to those obtained by using other existing clustering techniques.

The number of fi tness function evaluations is used to compare the speed of above mentioned clustering techniques. The number of fi tness function evaluations for ACDE, DCPSO, GUCK and Classical DE are quoted from ( Das et al., 2008b ). Fig. 2 (b) shows the performance evaluation of ACGSA over other existing cluster-ing techniques in terms of function evaluations. The results obtained (refer Fig. 2 (b)) indicate that ACGSA is faster than the other clustering techniques. Besides fi tness function evaluations, we have also computed the execution time for all the datasets. The mean execution time for Iris, Wine, Glass, Cancer, and Vowel datasets are 17.3337 s, 23.1006 s, 27.9689 s, 81.0526 s and 92.8483 s respectively. 5.4. Sensitivity analysis of parameters for ACGSA
The ACGSA has three control parameters that affect the per-formance of clustering technique. We discuss the impact of the parameters such as number of agents, maximum number of user-speci fi ed clusters ( K max ), and G 0 on the ACGSA algorithm. (1) Number of agents: To investigate the effect of number of (2) Maximum number of user-speci fi ed clusters ( K max ): Keeping the intra-cluster distance increases and inter-cluster decreases as K max increases. The inter-cluster and intra-cluster distances are least effected as K max varies for Wine dataset. For Glass,
Breast Cancer and Vowel datasets intra-cluster distance decreases with the increase in value of K max . The inter-cluster distance increases as K max increases for Glass and
Breast Cancer datasets. It has been found that, ACGSA performs better for K max  X  15 for most of the datasets. (3) The constant G 0 : The ACGSA was run for different values of G keeping other parameters fi xed as speci fi ed in Section 5.2 . The values of G 0 used in experimentation are 35, 50, 100, 150, and 200. Fig. 5 (a) depicts that the number of clusters decreases with the increase in G 0 for Iris, Glass and Vowel datasets. For
Wine and Breast Cancer datasets, the number of clusters remains same as G 0 changes. Fig. 5 (b) and (c) illustrates the effect of G 0 over inter-cluster and intra-cluster distances. The inter-cluster and intra-cluster distances are least effected as G varies for Wine dataset. For other datasets, the values of inter-6. Application to image segmentation
Image segmentation can be de fi ned as the process of dividing an image into constituent regions and each region should have similar features. The similarity of features is measured using some image property (e.g. pixel intensity). Image segmentation can be modeled as a clustering problem. Several clustering techniques have been suc-cessfully applied in image segmentation ( Jain et al., 1999 ). Here each pixel corresponds to a pattern and image region corresponds to a cluster. The image segmentation is mathematically de fi ned as follows:
Let I be the set of all image pixels. By applying segmentation on I , different non-overlapping regions R 1 ; R 2 ; ... ; such that [
Every pixel of the image must be exhibited in one and only one segmented region. The proposed automatic clustering technique has been applied to both types of images; grayscale and color images for segmentation. 6.1. Implementation 1: grayscale image segmentation
The fi ve-grayscale images, each of size 256 256, are used for experimentation. For clustering, the intensity of each pixel is taken as a feature. For each image, the number of data points is 65 536.
The same parameters setting is used for image segmentation as mentioned in Section 5.2 except that the user speci fi ed maximum number of clusters, K max , is set to 10 as it is the common practice. The results are compared with ACDE, DCPSO, GUCK and Classical
DE. Fig. 6 shows original images and segmented images obtained from ACGSA.
 number of clusters computed over 40 runs, are tabulated in
Table 6 . For Clouds image, DCPSO and GCUK provide fi ve clusters which is not the optimal number of clusters. ACGSA produces three clusters and corresponding segmented image is shown in
Fig. 6 (b). ACGSA produces the number of clusters that fall in the optimal range in almost every run for Peppers, Magazine and Robot images and corresponding segmented images are shown in
Figs. 6 (d), (f) and (j) respectively. ACGSA produces number, in the optimal range, of clusters for Mumbai image and corresponding segmented image is shown in Fig. 6 (h). Therefore, results in
Table 6 depict that the ACGSA fi nds the number of clusters for grayscale images in optimal cluster range. Table 7 shows the results of unpaired t -tests based on the number of clusters of
Table 6 between the best two algorithms. The results reveal that the ACGSA is statistically signi fi cant than other techniques for image segmentation. 6.2. Implementation 2: color image segmentation
The proposed technique has also been applied to four well-known color images; Lena, Mandrill, Jet and Peppers. The size of each image is 256 256. For each image, the number of data points is 65 536. The RGB color scheme is used. The experimental setup is same as used in grayscale images. The original images, labeled images formed by cluster labels and segmented images are shown in Fig. 7 .

The optimal range for number of clusters for above-mentioned images is speci fi ed in ( Celenk, 1990 ; Turi, 2001 ). The results of
ACGSA are shown in Table 8 . For Lena and Mandrill images, ACGSA produces six clusters. ACGSA produces fi ve and seven clusters in case of Jet and Peppers images respectively. The results are compared with DCPSO. The results produced by both the algo-rithms are in the optimal range. Further the signi fi cance of ACGSA is proved through unpaired t -test mentioned in Table 9 . The unpaired t -test is not feasible for Peppers image as there is no variation in the number of clusters. These results show the supremacy of ACGSA over DCPSO. ACGSA fi nds the number of clusters in the optimal range. 7. Conclusions
In this paper, a GSA based automatic clustering technique has been proposed. The proposed approach uses two new concepts namely a dynamic threshold setting and weighted cluster centroid computation. The experimental results clearly show that the proposed ACGSA is able to determine the correct number of clusters. The newly proposed optimization criterion helps in fi nding the optimal number of clusters and best partitions from the given dataset. The clusters produced by ACGSA are well separated and are more compact as claimed on the basis of inter-cluster and intra-cluster distances. The proposed algorithm has further been applied on fi ve grayscale and four color images for segmentation. On comparing the results of the proposed technique with the recently developed techniques, it has also been observed that ACGSA outperforms the recently proposed automatic clustering techniques. On the basis of results obtained, we can conclude that the proposed algorithm is well applicable for real life datasets, grayscale and color images.
 References
