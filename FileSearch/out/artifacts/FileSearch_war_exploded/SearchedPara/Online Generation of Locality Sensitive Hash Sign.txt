 There has been a surge of interest in adapting re-sults from the streaming algorithms community to problems in processing large text collections. The term streaming refers to a model where data is made available sequentially, and it is assumed that resource limitations preclude storing the entirety of the data for offline (batch) processing. Statis-tics of interest are approximated via online, ran-domized algorithms. Examples of text applica-tions include: collecting approximate counts (Tal-bot, 2009; Van Durme and Lall, 2009a), finding top-n elements (Goyal et al., 2009), estimating term co-occurrence (Li et al., 2008), adaptive lan-guage modeling (Levenberg and Osborne, 2009), and building top-k ranklists based on pointwise mutual information (Van Durme and Lall, 2009b).
Here we revisit the work of Ravichandran et al. (2005) on building word similarity measures from large text collections by using the Locality Sensi-tive Hash (LSH) method of Charikar (2002). For the common case of feature updates being addi-tive over a data stream (such as when tracking lexical co-occurrence), we show that LSH signa-tures can be maintained online, without additional (1995) for an unrelated application) that the an-gle between any two vectors summarized in this fashion is proportional to the expected Hamming distance of their signature vectors. Hence, we can retain length d bit-signatures in the place of high dimensional feature vectors, while preserving the ability to (quickly) approximate cosine similarity in the original space.

Ravichandran et al. (2005) made use of this al-gorithm to reduce the computation in searching for similar nouns by first computing signatures for each noun and then computing similarity over the signatures rather than the original feature space. In this work, we focus on features that can be maintained additively, such as raw frequencies. 1 Our streaming algorithm for this problem makes use of the simple fact that the dot product of the feature vector with random vectors is a linear op-eration. This permits us to replace the v i  X  r i op-eration by v i individual additions of r i , once for each time the feature is encountered in the stream (where v i is the frequency of a feature and r i is the randomly chosen Gaussian-distributed value asso-ciated with this feature). The result of the final computation is identical to the dot products com-puted by the algorithm of Charikar (2002), but the processing can now be done online. A simi-lar technique, for stable random projections, was independently discussed by Li et al. (2008).
Since each feature may appear multiple times in the stream, we need a consistent way to retrieve the random values drawn from N (0 , 1) associated with it. To avoid the expense of computing and storing these values explicitly, as is the norm, we propose the use of a precomputed pool of ran-dom values drawn from this distribution that we can then hash into. Hashing into a fixed pool en-sures that the same feature will consistently be as-sociated with the same value drawn from N (0 , 1) . This introduces some weak dependence in the ran-dom vectors, but we will give some analysis show-ing that this should have very limited impact on the cosine similarity computation, which we fur-ther support with experimental evidence (see Ta-ble 3).

Our algorithm traverses a stream of words and features. Our algorithm is superior in terms of memory (because of the pooling trick), and has the benefit of supporting similarity queries online. 3.2 Pooling Normally-distributed Values We now discuss why it is possible to use a fixed pool of random values instead of generating unique ones for each feature. Let g be the c.d.f. of the distribution N (0 , 1) . It is easy to see that picking x  X  (0 , 1) uniformly results in g  X  1 ( x ) be-ing chosen with distribution N (0 , 1) . Now, if we select for our pool the values for some sufficiently large m , then this is identical to sampling from N (0 , 1) with the caveat that the accuracy of the sample is limited. More precisely, the deviation from sampling from this pool is off from the actual value by at most
By choosing m to be sufficiently large, we can bound the error of the approximate sample from a true sample (i.e., the loss in precision expressed above) to be a small fraction (e.g., 1%) of the ac-tual value. This would result in the same relative error in the computation of the dot product (i.e., 1%), which would almost never affect the sign of the final value. Hence, pooling as above should give results almost identical to the case where all the random values were chosen independently. Fi-nally, we make the observation that, for large m , randomly choosing m values from N (0 , 1) results in a set of values that are distributed very similarly to the pool described above. An interesting avenue for future work is making this analysis more math-ematically precise. 3.3 Extensions Decay The algorithm can be extended to support temporal decay in the stream, where recent obser-vations are given higher relative weight, by mul-tiplying the current sums by a decay value (e.g., 0 . 9 ) on a regular interval (e.g., once an hour, once a day, once a week, etc.).
 Distributed The algorithm can be easily dis-tributed across multiple machines in order to pro-cess different parts of a stream, or multiple differ-ent streams, in parallel, such as in the context of the MapReduce framework (Dean and Ghemawat, tures. This gave a stream of 773,185,086 tokens, with 1,138,467 unique types. Given the number of types, this led to a (sparse) feature space with dimension on the order of 2.5 million.

After compiling signatures, fifty-thousand  X  x,y  X  pairs of types were randomly sampled by selecting x and y each independently, with replacement, from those types with at least 10 to-kens in the stream (where 310,327 types satisfied this constraint). The true cosine values between each such x and y was computed based on offline calculation, and compared to the cosine similarity predicted by the Hamming distance between the signatures for x and y . Unless otherwise specified, the random pool size was fixed at m = 10 , 000 .
Figure 1 visually reaffirms the trade-off in LSH between the number of bits and the accuracy of cosine prediction across the range of cosine val-ues. As the underlying vectors are strictly posi-tive, the true cosine is restricted to [0 , 1] . Figure 2 shows the absolute error between truth and predic-tion for a similar sample, measured using signa-tures of a variety of bit lengths. Here we see hori-zontal bands arising from truly orthogonal vectors leading to step-wise absolute error values tracked to Hamming distance.

Table 1 compares the online and batch LSH al-gorithms, giving the mean absolute error between predicted and actual cosine values, computed for the fifty-thousand element sample, using signa-tures of various lengths. These results confirm that we achieve the same level of accuracy with online updates as compared to the standard method.
Figure 3 shows how a pool size as low as m = 100 gives reasonable variation in random values, and that m = 10 , 000 is sufficient. When using a standard 32 bit floating point representation, this is just 40 KBytes of memory, as compared to, e.g., the 2.5 GBytes required to store 256 random vec-tors each containing 2.5 million elements.

Table 2 is based on taking an example for each of three part-of-speech categories, and reporting the resultant top-5 words as according to approx-imated cosine similarity. Depending on the in-tended application, these results indicate a range Thanks to Deepak Ravichandran, Miles Osborne, Sasa Petrovic, Ken Church, Glen Coppersmith, and the anonymous reviewers for their feedback. This work began while the first author was at the University of Rochester, funded by NSF grant IIS-1016735. The second author was supported in part by NSF grant CNS-0905169, funded under the American Recovery and Reinvestment Act of 2009.

