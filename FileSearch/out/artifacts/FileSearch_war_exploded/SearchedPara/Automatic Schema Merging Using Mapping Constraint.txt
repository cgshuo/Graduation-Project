 Schema merging is the process of consolidating multiple schemas into a unified view. The task becomes particularly challenging when the schemas are highly heterogeneous and autonomous. Clas-sical data integration systems rely on a mediated schema created by human experts through an intensive design process.

In this paper, we present a novel approach for merging multi-ple relational data sources related by a collection of mapping con-straints in the form of P2P style tuple-generating dependencies (tgds). In the scenario of data integration, we opt for minimal mediated schemas that are complete regarding certain answers of conjunctive queries. Under Open World Assumption (OWA), we characterize the semantics of schema merging by properties of the output mapping system between the source schemas and the me-diated schema. We propose a merging algorithm following a re-dundancy reduction paradigm and prove that the output satisfies the desired logical properties. Recognizing the fact that multiple plausible mediated schemas may co-exist, a variant of the a priori algorithm is employed to enumerate alternative mediated schemas. Output mappings in the form of data dependencies are generated to support the mediated schemas, which enables query processing. We have evaluated our merging approach over a collection of real world data sets, which demonstrate the applicability and effective-ness of our approach in practice.
 H.2.1 [ Database Management ]: Logical Design X  Schema and subschema ; H.2.5 [ Database Management ]: Heterogeneous Databases; H.2.4 [ Database Management ]: Systems X  Relational databases Algorithms, Design schema merging, data integration, model management, schema mappings
Modern data intensive applications often involve a multitude of heterogeneous data sources. Schema merging is the process to con-solidate multiple related heterogeneous schemas to provide a uni-fied user view called the mediated schema . In order to support data loading from the sources to the mediated schema (e.g., in data warehousing), or to enable querying of sources through the me-diated schema (e.g., in data integration [24] or dataspaces [31]), mappings revealing the relationship between the mediated schema and the source schemas have to be established in the merging pro-cess. Classical data integration systems [24] nowadays still rely on a mediated schema created by an intensive manual design pro-cess by human experts, which is costly and inflexible in a dynamic evolving environment such as dataspaces.

In vision of the importance of schema merging, Merge is pro-posed as one of the major operators in Model Management [7]. Nevertheless, as retrospected by Bernstein and Melnik in [9], the original vision of Model Management 1.0 is not semantic but struc-tural, i.e., not relating schema and data. In other words, operators are interpreted in terms of schemas, while lacking connection to the underlying data of the schemas. A semantic merging approach is in need, not only for the sake of expressiveness but also for ex-ecutability reasons. Merging using logical schema mappings, e.g., tgds, is inevitable for realizing a model management engine to ad-dress real-world data programmability problems. In order to be executable, a semantic merging algorithm not only consumes data dependencies as input, but also produces data dependencies as out-put, so that query processing or data migration can be performed.
Although differing a lot in terms of mapping language, seman-tics, data models, and methodologies, most existing schema merg-ing techniques are binary, i.e., merging two schemas at a time. We deem a native n-ary merge as interesting for generating mediated schemas for a multitude of data sources. Though binary merging al-gorithms can be applied iteratively to merge multiple schemas, the process need a full human supervision, i.e., in each iteration an ex-pert is required to generate mapping between a new source schema and the intermediate merged result from previous steps. Moreover, in a scenario of ad hoc P2P environments, only some particular mappings are available and nobody has the complete knowledge to produce arbitrary mappings. Therefore, a native n-ary merging algorithm is more suited to exploit all the available mappings for multiple data sources. In the survey by Batini et al. [6], they con-tribute the popularity of binary merge to a complexity reason, that is, by involving less input the problem of merging will become less complicated. However, as we have already described, constructing schema mappings is also quite expensive and requires a lot of hu-man supervision, which compensates for the increased computation efforts of a native n-ary merge.
In this paper, we propose a native n-ary semantic merging ap-proach. Without human supervision, it generates a series of candi-date minimal mediated schemas that are complete regarding certain answers of conjunctive queries. It takes source integrity constraints and P2P style tgds as input and generates logical output mappings in the form of data dependencies. Our contributions are as follows:
As a running example, consider the schemas illustrated in Fig. 1, adapted from [18]. Given are two schemas, each one with three relations. In schema S 1 , grant represents a many-to-many relation-ship between companies and projects. Similarly, funding represents a many-to-many relationship between organization and financial in S 2 . The corresponding foreign key constraints for these rela-tionships are indicated by  X  -lines, while keys are underlined. In addition, we show the value correspondences of the attributes of the schemas in straight lines.

The relationship between source schemas can be captured pre-cisely in tgds: Tgd M 1 states that the relations company and org are equivalent, except for the additional column city in company . Such simple one-to-one correspondences can be represented in most other ap-proaches, too. However, more complex relationships can only be formalized by mappings involving joins between several relations. For example, the tgd M 2 states that the join of grant and project in S is equivalent to the join of funding and financial in S 2 similar but it states that the relation financial is equivalent to a pro-jection of the join of grant and project . With the inclusion of M we know that there are no dangling tuples of financial that do not join with some tuple of funding . Finally, M 4 states that all tuples from project are contained in financial . The details of the mapping language will be given in Section 2.

As in the running example, source schemas usually contain im-portant information in the form of integrity constraints, such as keys and foreign keys. Integrity constraints reveal inner structure of a schema and hence are a significant source of information which should be taken into consideration in schema merging.

The remaining of the paper is organized as follows. Sec. 2 de-scribes some background definitions. We characterize the seman-tics of schema merging in the concrete scenario of designing a me-diated query interface in Sec. 3. The main aspects of our approach and the algorithms are presented in Sec. 4. Experimental results are reported in Sec. 5. Related works are covered in Sec. 6, before we conclude and discuss future work in Sec. 7.
Data Dependencies: A tuple generating dependency (tgd) [1], is a query containment constraint in the form of:  X  ~ X [  X   X  ~
Z  X  ( ~ X , ~ Z )] , where  X  and  X  are conjunctions of atoms and ~ Z are mutually disjoint variables. It is full , if there are no existen-tial variables on the right hand side, otherwise it is an embedded tgd. An equality-generating dependency (egd) [1] has the form  X  ~ and X i , X j are variables in ~ X . A set of tgds is said to be weakly acyclic , if there is no recursive implication of existential variables [19]. In the following, we omit the quantifiers for brevity. We also write  X  ( ~ X , ~ Y )  X   X  ( ~ X , ~ Z ) to denote both,  X   X  ( ~
X , ~ Y )  X   X  ( ~ X , ~ Z ) in a mapping. For a source schema S and a target schema T , a source-to-target tgd (s-t tgd) is a tgd such that the antecedent contains only atoms from S and the consequent con-tains only atoms from T . For a schema with relations r 1 a set of tgds are called copy tgds , if they send each source relation to a distinct target relation with the same arity, i.e., in the form of {  X  r r . For a relation r with arity n , we say an identity query is a query q ( X 1 , X 2 , . . . , X n )  X  r ( X 1 , X 2 , . . . , X
Schemas and Mappings: A schema S is a sequence of relation I of a schema S , denoted by I  X  Inst ( S ) , is the union of relation instances over r i where r i  X  S . An instance I is legal wrt. a set of data dependencies  X  formulated as tgds and egds [1] over S , if it satisfies the dependencies, i.e., I | =  X  . A binary mapping between two schemas S 1 and S 2 is a triple M = ( S 1 , S 2 ,  X  ) where of dependencies. The semantics of a binary mapping is a binary relation with instances of the source schema as the domain and in-stances of the target schema as the range, i.e., Inst ( M position of two mappings M 13 = M 12  X  M 23 is then defined as Inst ( M 13 ) = { ( I , K ) :  X  J ( I , J )  X  Inst ( M 12 For a mapping M from S to T , the possible worlds, called solu-tions , of T wrt. an instance I of S are Sol M ( I ) = { J  X  Inst ( T ) : ( I , J )  X  Inst ( M ) } . We denote it by Sol ( I ) when the mapping in-volved is clear from context. A solution is called a universal solu-tion if there is a homomorphism from it to any other solution [19]. A universal solution that has no homomorphism to a subset of it-self is called a core [20]. It is known that the core is unique up to isomorphism when it exists.

Chase and Certain Answers: The chase procedure [13] is an indispensable tool for reasoning with data dependencies. Let a set of tgds and egds with terminating chase, we use chase denote the result of the chase using  X  over a database I . When is a set of s-t tgds and target dependencies. We also use chase to denote the target instance J such that ( I , J ) = chase a schema mapping specified by a finite set of s-t tgds set of target dependencies  X  t consisting of a finite set of weakly any source instance I , chasing I against  X  st  X   X  t yields a univer-sal solution if the chase succeeds. The certain answer of a query q wrt. a set of database instances P over the same schema is: then defined using certain answers. Two sets of database instances P and P 2 under the same schema are said to be L-equivalent wrt. a query language class L , denoted by P 1  X  L P 2 , if for any query in L they have the same certain answer. In this paper, we are partic-ularly interested in equivalence wrt. conjunctive queries, i.e., CQ-equivalence.
In [26], Miller et al. first use formal schema equivalence results to characterize the semantics of schema merging. However, as we understand it today, the semantics of a mediated schema is better characterized using not only the structure of the schema itself but also the mapping relating the mediated schema to data sources that host extensional data. Based on such an observation, the semantics of schema merging in our approach is characterized as logical prop-erties of the output mapping system between the source schemas and the mediated schema, targeting at the scenario of creating a me-diated query interface over incomplete data sources. We first state the problem of n-ary schema merging in Section 3.1, where the inter-schema mapping constraints and source integrity constraints are unified. In Section 3.2, we propose the completeness criteria aiming at retaining not only all the ground data, but also certain answers ensured by data dependencies. Section 3.3 formalizes the desiderata that data asserted to be equivalent in the input mapping should be integrated via the output mapping system. In Section 3.4, we formulate minimality of a mediated schema such that no column in the schema is redundant.
Consider the scenario of merging multiple data sources to create a single unified query interface. Since the data sources are indepen-dently developed, their extensions, i.e., explicitly stored data, usu-ally do not conform to any inter-schema logical constraints. That is, under Closed World Assumption, logical constraints usually cannot be asserted among data sources. This is one reason why Pottinger and Bernstein [28] interpret their input mapping as specification of overlap between schemas instead of direct logical constraints over data instances. In this paper, we take the OWA by interpreting input mapping as constraints that are expected to hold over the integrated global database, which is also inline with the common assumption in data integration that sources are sound but incomplete.
Definition 1. Given an incomplete database I of schema S , the semantics of I wrt. a set of data dependencies  X  over S is Sem { I from context, we simply write Sem ( I ) for brevity.

Consider n source schemas S 1 , S 2 , . . . , S n with no two relations sharing the same name. Each source schema S i has a set of in-tegrity constraints  X  i as a union of tgds and egds. The input map-ping is specified by a set of inter-schema tgds  X  in among the source schemas.

Definition 2. The joint source schema is the disjoint union of the n source schemas, while a joint source instance is the disjoint union of n instances with one instance for each source schema. The merge input is then a pair ( S ,  X  ) , with S being a joint source schema and  X  =  X  in  X  S i  X  i . The semantics of a joint source instance I is then Sem  X  ( I ) .

In the following of the paper, we only consider sound data sources, that is, they are consistent with the specified data depen-dencies and the possible world set they present is not empty.
Definition 3. For a merge input ( S ,  X  ) , a merge output is a binary mapping M o = ( S , G ,  X  o ) , called the output mapping , in which G is the mediated schema and  X  o is a set of data dependencies.
It is now commonly observed that for a given merging scenario, there may be multiple plausible mediated schemas [12, 31]. With-out human intervention, our approach will produce a series of plau-sible outputs.
In the survey by Batini et al. [6], completeness of a mediated schema is described as containing all concepts in the union of the source schemas X  application domains, which is rather representa-tive of the approaches they surveyed, since they mostly employ a conceptual model such as EER and do not explicitly consider querying of the mediated schema.

Pottinger and Bernstein [28] concretize Hull X  X  notion of query dominance [23] in the scope of schema merging as retaining in the mediated schema the data of each source relation. We extend their formalization in two aspects. First, we consider retaining answers of all conjunctive queries besides each source relation. Second, we take source incompleteness into consideration and hence require the retainment of both, extensively stored data and inferred data, for which we use the notion of certain answers.
 Definition 4. Given a merge input ( S ,  X  ) , an output mapping M o = ( S , G ,  X  o ) is complete wrt. a mapping language L , if there ex-ists a mapping M w = ( G , S ,  X  w ) with  X  w specified in L such that for any joint source instance I , we have: Sem  X  ( I )  X  CQ Sol M M w is called a witness mapping .

The requirement of completeness ensures that an output map-ping system has a mapping backward to the joint source schema, which is a witness of the retainment of certain answers of queries. If the witness mapping allows CQ rewriting, e.g., specified in s-t tgds, then for each q  X  CQ over S , the rewriting against duces a query over the mediated schema producing the same certain answer.

E XAMPLE 1. Consider a subset of our example containing only the relations com pany and org and the mapping M com pany ( Cid , Name , City )  X  org ( Cid , Name ) . The mediated schema { com pany_org ( cid , Name , city ) } is complete with the out-put mapping: com pany_org ( Cid , Name , City )  X  org ( Cid , Name ) com pany_org ( Cid , Name , City )  X  com pany ( Cid , Name , City ) A witness mapping is: com pany ( Cid , Name , City )  X  com pany_org ( Cid , Name , City )
Completeness alone is not sufficient to indicate the quality of an output mapping system, as illustrated in the following example.
E XAMPLE 2. Consider the same input as in Ex. 1. A copy of the source schema with the following output mapping is also com-plete: com pany 0 ( Cid , Name , City )  X  com pany ( Cid , Name , City ) and org 0 ( Cid , Name )  X  org ( Cid , Name ) . A possible witness map-ping is: A problem with the above output mapping system is that, although in the input mapping company and org are asserted to be equiva-lent when projected to cid and name , their images in the mediated schema are still separate. This phenomenon reveals that complete-ness does not guarantee that equivalent data are integrated in the mediated schema.

When creating a mediated query interface, semantically equiv-alent data should be provided in a seamless way in the merged schema, while the users do not need to concern about either the ori-gin or the structural heterogeneity. We formalize below the require-ment that integration is actually performed via the output mapping. Definition 5. Given a merge input ( S ,  X  ) , an output mapping M o = ( S , G ,  X  o ) is integrated if for any joint source instance I , the following holds:
Besides mixing equivalent data, integratedness has another im-plication under OWA. Since data sources are incomplete, two dif-ferent joint source instances may have the same semantics, i.e., the same possible world set. Integratedness requires that the output mapping does not distinguish these equivalent joint sources regard-ing query answering.

E XAMPLE 3. Consider a source instance I = { com pany ( 1 , IBM , Armonk ) , org ( 2 , Microso f t ) } for Example 2. The query q ( Y )  X  com pany ( X , Y , Z ) will give only IBM as certain answer. However, the same query has { IBM , Microso f t } integrated.

We have to point out that integratedness alone is not sufficient either, since a constant output mapping producing a constant target side always satisfies integratedness. Hence, integratedness has to be used together with completeness.
In the scenario of creating a mediated query interface for data integration systems, we make the assumption that a smaller query interface (still retaining all the query capabilities) is better and head for a minimal schema with no redundant column. Redundancy of columns is defined wrt. a given output mapping.

Definition 6. For a target schema G in an output mapping M the induced mapping wrt. a projection M p of G is the composition M o  X  M p .

An induced mapping is actually an adaptation of the output map-ping after removing some columns from the mediated schema.
E XAMPLE 4. Consider a fragment of the running exam-ple with the source schema S consisting of the relations grant(grantee,pi,amount,sponsor) , funding(pi,oid, aid) and finan-cial(aid,amount,project,year) . The input mapping is Let G be a replica of S and denote the predicates in G by grant X  , funding X  and financial X  . A complete and integrated output mapping M o consists of the following dependencies: Let M p be a projection that removes the pi column from grant X  . The subschema G 0 contains grant X  , financial X  and funding X  . The induced mapping is M = ( S , G 0 ,  X  ) with  X  represented by the fol-lowing dependencies: f inancial ( Aid , A , Pj , Y r )  X  f unding ( Pi , O , Aid )  X  grant f inancial ( Aid , A , Pj , Y r )  X  f inancial 00 ( Aid , A , Pj , Y r )
Definition 7. An output mapping M o = ( S , G ,  X  o ) is minimal wrt. a set of output mappings P , if for any projection of G that is not an identity, the induced mapping does not belong to
The set of output mappings P may be the set of all mappings having certain properties, e.g., the properties completeness and in-tegratedness as introduced above. In the following, we are always interested in the minimal mappings that are complete and integrated and simplify say minimal or minimality.

Intuitively, an output mapping is minimal, if removing any columns will cause loss of some property (e.g., completeness). E XAMPLE 5. Consider the induced mapping M in Example 4. It is no longer complete because the information over pi of grants is lost due to the projection. In fact, the original output mapping M o is minimal wrt. completeness.

The next proposition suggests we only need to focus on com-pleteness for induced mappings of an integrated output mapping.
P ROPOSITION 1. Any induced mapping of an integrated output mapping is also integrated.

For instance, the induced mapping in Example 4 is still inte-grated.
In this section, we describe an algorithm producing complete and integrated mediated schemas that have no redundant columns. The algorithm consists of two stages: a constraint repairing stage and a mediated schema minimization stage. Sec. 4.1 describes a constraint repairing procedure which produces an initial medi-ated schema with an output mapping that is both, complete and integrated, and which we call the canonical mediated schema. In Sec. 4.2, we provide a procedure for testing completeness after re-moving columns from the canonical mediated schema schema (or equivalently, whether some columns are redundant), which is at the core of the mediated schema minimization stage described in Sec. 4.3. Finally, we describe the complete algorithm in Sec. 4.4.
In this section we show that there always exists a merge output that is both, complete and integrated, which we call the canonical output mapping . The mediated schema is called the canonical me-diated schema . Given a merge input ( S ,  X  ) , we create the canonical output mapping as follows: 1. Initialize G to be a replica of S . Let  X  co py be the copy tgds, 2. Let  X  be the renaming of predicates from S to their images in 3. The canonical output mapping is M o = ( S , G ,  X  co py
The output mapping consists of two parts: a set of s-t tgds trans-(
 X  co py ) and a set of target dependencies over the mediated schema (  X  G ).

L EMMA 1. The canonical output mapping M o described above is both complete (wrt. full s-t tgds) and integrated.
P ROOF . Completeness: for any source instance I , we have  X  ( Sem ( I )) = Sol M o ( I ) with  X  being the renaming as in the cre-ation of the canonical output mapping. Using a set of copy tgds (the inverse of  X  co py , which exists as G is a 1-1 copy of S ) as the witness mapping, we obtain a solution set co-initial (i.e., with the same minimal elements regarding set inclusion) with Sem ( I ) . Co-initiality implies CQ-equivalence, since CQ is monotone.
Integratedness: for each complete ground source I 0  X  Sem ( I ) we clusion, because  X  ( I 0 ) is a solution and it is a subset of any other initial. Considering  X  ( Sem ( I )) is identical to Sol M ness is straightforward.

Interestingly, Melnik describes in [25] a straightforward algo-rithm creating a mediated schema for view integration, which dif-fers from our canonical mediated schema only in the direction of the output mapping. However, the size of the schema does not matter for view integration and hence he does not head for mini-mization of mediated schemas.

E XAMPLE 6. The output mapping M o in Example 4 is the canonical output mapping obtained from the input using the pro-cedure described.

We have shown that a complete output mapping is able to be expressed in the language of s-t tgds and target dependencies. The following proposition reveals that without the target dependencies in the output mapping, completeness cannot be achieved.

P ROPOSITION 2. There exists an input mapping specified as a finite set of full tgds, such that no output mapping defined as a finite set of s-t tgds is complete wrt. s-t tgds.

P ROOF SKETCH . Consider the following input mapping, in which relations from source schemas S 1 and S 2 are labeled with corresponding subscripts: e represents the edges in a graph, c 1 and c 2 express the transitive closure of e 1 . A query asking for the certain answers of c ness of the incompleteness, which simply follows from the inability of first order queries to express transitive closure [1].
We introduce in Section 4.2.1, a procedure testing whether the answer of a CQ is still obtainable after removing some columns from a schema. This is then employed by the algorithm in Section 4.2.2, which tests schema redundancy.
We first introduce in this section a useful notion which we call query recoverability . Intuitively, it is a formalization that the an-swer of a query is still obtainable after a projection.

Definition 8. For a given projection mapping M p and a given query class L :
E XAMPLE 7. Consider two relations company(cid, cname, city) and org(oid, name) with the dependencies  X  that contain a tgd com pany ( Cid , Cname , city )  X  org ( Cid , Cname ) and an egd org ( Oid , Name 1 )  X  org ( Oid , Name 2 )  X  Name 1 = Name be the projection that projects out the cname column of com pany. Then the query  X  cname ( com pany ) is recoverable wrt. denote the projected relation by company X (cid, city) , the query  X  cname ( com pany 0 1 org ) can be used to recover the original an-swer to  X  cname ( com pany ) .

We describe in Figure 2 a test for conjunctive query recoverabil-ity. The correctness is given in Theorem 1. By freezing a query as a database, we mean sending each distinct variable to a fresh new constant following the query containment tests described in [33].
T HEOREM 1. Let  X  be a set of tgds and egds with terminating chase. A conjunctive query (CQ) q is recoverable wrt.  X  , a pro-jection M p and the query class CQ, if and only if q passes the recoverability test. recoverable ( q ,  X  ,  X  p )
Input: A conjunctive query q , a set of dependencies  X  , and a projection  X  p
Output: Returns true if q is recoverable after projection 01 If chasing q against  X  fails, return true; 02 else let q 0 = chase  X  ( q ) . 03 Freeze q 0 as a database d 1 04 Perform the following: 05 d 2 = chase  X  p ( d 1 ) 06 Reverse the arrows in  X  p resulting in  X   X  1 p ; 07 d 3 = chase  X   X  1 08 d 4 = chase  X  ( d 3 ) 09 If the frozen head of q 0 is contained in q 0 ( d 4 10 else return false
P ROOF SKETCH . The case when the chase on line 1 fails is straightforward. We prove in the following the case when the chase succeeds.  X  : Assume there exists a query q 1 over the projected schema that recovers q . Let q 2 be the unfolding of q 1 against the projection, we have q 2  X   X  q 0 . We also know q 2 ( d 1 )  X  q 2 ( d not make use of any source data not exported by the projection. Therefore, q 0 . f rozenHead  X  q 0 ( d 1 ) = q 2 ( d 1 )  X  q i.e., the test succeeds.  X  : We define a melting transformation, denoted by f  X  1 , which sends each frozen constant to its original variable but keeps vari-ables introduced during chase unchanged. In this way, a frozen database is transformed into a CQ. We claim that f  X  1 ( d a recovery query.
 E XAMPLE 8. We perform the recoverability test over Example 7. Chasing against  X  leads to the query q 0 =  X  cname ( com pany 1 org ) . After freezing we get: d 1 = { com pany ( id 0 , name 0 , city 0 ) , org ( id 0 , name ter the expanding following projection, we get d 3 = { org ( id 0 , name 0 ) , com pany ( id 0 , Name 1 , city being a new variable. Chasing d 3 against  X  gives d 4 = { org ( id 0 , name 0 ) , com pany ( id 0 , name 0 , city name 0  X  q 0 ( d 4 ) , the original query passes the recoverability test.

The recoverability test can be extended to the language of union of CQs (UCQs):
P ROPOSITION 3. A UCQ, with no component CQ contained in another, is recoverable wrt. UCQ if and only if each component CQ is recoverable wrt. CQ. In particular, a CQ is recoverable wrt. UCQ if and only if it is recoverable wrt. CQ.

T HEOREM 2. Let  X  be a set of weakly acyclic tgds and egds, and q be a CQ, the recoverability is NP-complete wrt. | q | . More-over, when each dependency in  X  has a bounded length, and q is either of bounded length or project-free, then the recoverability test is in PTIME wrt. the size of  X  , q, and S.

P ROOF SKETCH . Since the data complexity of chasing against weakly acyclic tgds is PTIME [19], the database d 4 is of polyno-mial size. It is straightforward that embedding the body of q a homomorphism to d 4 is in NP. The NP-hardness is due to a re-duction from rewriting CQ using conjunctive views. When each reducible ( M p , M w )
Input: a projection M p = ( G , G 0 ,  X  p ) to be tested and a map-ping M w = ( G , S ,  X  w ) .  X  G denotes the dependencies in G .
Output: returns true if the projection is reducible. 01 for each r  X  S 02 q r = unfolding of r against  X  w 03 endfor 04 return V r  X  S recoverable ( q r ,  X  G ,  X  p ) dependency has a bounded length, the chase result is again of poly-nomial size. Hence the final database is of polynomial size. When q is of bounded length or project-free, testing embedding can be done in PTIME.
Given a complete initial output mapping, a projection is re-ducible with respect to a mapping language L , if the induced map-ping is complete with respect to L . The existence of a reducible projection suggests that the mediated schema in the original out-put mapping has redundant columns. Therefore, testing projection reducibility is in fact testing schema redundancy.

The following theorem states that projection reducibility can be tested by query recoverability tests, which we have already devel-oped in Section 4.2.1.

T HEOREM 3. Let M = ( S , G ,  X  sg  X   X  g ) be the canonical out-put mapping with a witness mapping M w . The following two state-ments are equivalent: 1. A projection M p of G is reducible wrt. full s-t tgds; 2. For each relation r  X  S, unfolding of the identity query
P ROOF SKETCH . It is obvious that if all identity queries are re-coverable, we just take the union of the recoverable queries of all source relations to get a witness mapping for the induced mapping system. The other direction holds because each valid instance of the target schema of the canonical mediated schema is a ground core [20] of some source instance, and in this case retaining certain answers of CQs implies the identity queries are recoverable.
Taking into consideration that an identity mapping is a witness mapping for the canonical output mapping and proposition 3, we are now able to develop a procedure for testing projection reducibil-ity over the canonical mediated schema, which is described in Fig. 3.

L EMMA 2. For inputs with terminating chase, a projection of the canonical mediated schema is reducible wrt. full s-t tgds if and only if it passes the reducibility test.

E XAMPLE 9. Consider the mediated schema G and the output mapping M o as in Ex. 4. We test whether the projection M ducible. Recall that the witness mapping for the canonical output mapping system is in a simple form, i.e., a set of copy tgds. The cor-responding unfolding of the identity query of each source predicate r ( ~ X ) will be r 0 ( ~ X ) . Since only grant 0 is affected in the projection M p , all other unfoldings are trivially recoverable. Therefore, in order to test whether M p is reducible, we only need to test whether coverability test returns false, which is in line with our previous discussion. enumMaxPro jection ( I l , C l , l , MP , M w )
Input: a projection set of columns I l , a set C l of possible columns to be further projected, the recursion level l , the set
MP of maximal projections computed already, a set of depen-dencies  X  , and a witness mapping M w .

Output: a set of maximal sets of projections MP . 01 for each x  X  C l 02 I l + 1 = I l  X  X  x } 03 P l + 1 = { y  X  C l | y &gt; x } 04 if  X  I 0  X  MP such that I l + 1  X  P l + 1  X  I 0 05 return 06 endif 07 C l + 1 = /0 08 for each y  X  P l + 1 09 if reducible ( I l + 1  X  X  y } , M w ) 10 C l + 1 = C l + 1  X  X  y } 11 endif 12 endfor 13 if C l + 1 = /0  X  X  X  I 0  X  MP such that I l + 1  X  I 0 14 MP = MP  X  X  I l + 1 } 15 else 16 enumMaxPro jection ( I l + 1 , C l + 1 , l + 1 , MP , M 17 endif 18 endfor
With the schema redundancy test at hand, we are now able to search for minimal mediated schemas. A naive way of finding min-imal subschemas of the canonical mediated schema that retain com-pleteness is to enumerate all possible projections and test for re-ducibility. If a projection is maximal in terms of set inclusion rela-tionship, namely any superset will not retain completeness, the cor-responding induced output mapping is a desired minimal mediated schema. Obviously, the above enumeration procedure is exponen-tial. We present a more efficient enumeration algorithm by making use of the following property: if a projection is reducible, then pro-jecting out any subset also results in a reducible subschema. This is known as the A-priori property [2]. We use here a variant of the depth-first GenMax algorithm [22], which makes use of both superset pruning (as in the original A-priori) and subset pruning. Fig. 4 depicts the procedure to compute the maximal projections. As input it receives a set I l of columns projected out, a set of can-didate columns C l that possibly can be projected out additionally, and a witness mapping M w . After termination MP contains all the maximal projections representing minimal induced mappings. For brevity, we denote in the algorithms in Fig. 4 and Fig. 5 a projec-tion by the set of positions to be projected out instead of explicitly constructing a projection mapping.
 E XAMPLE 10. Consider the full running example specified in Figure 1, with all integrity constraints and mappings (M 1 The merging algorithm reveals four different ways to reduce redun-dancy in the mediated schema: 1. remove the whole project relation and company.cname ; 2. remove the relations project and org ; 3. remove company.cname and financial.year ; 4. remove the relation org and financial.year . merge(S,  X  ) Input: joint source schema S , input dependencies  X 
Output: a series of output mappings Result = { M } 01 compute the canonical output mapping M o = ( S , G , 02 construct the witness mapping M w for M o 03 C = /0 04 let C 0 be the set of all columns in any relations in G 05 for each c  X  C 0 06 if reducible ( { c } , M w ) 07 C = C  X  X  c } 08 endif 09 endfor 10 MP = /0 11 enumMaxPro jections ( /0 , C , 0 , MP , M w ) 12 if MP = /0 return { M o } 13 for each P  X  MP 14 construct subschema G 0 defined by P 15 construction projection mapping M p determined by P 16 M = M o  X  M p 17 Result = Result  X  X  M } 18 endfor 19 return Result
The complete merging algorithm is depicted in Figure 5. The algorithm starts by creating the canonical mediated schema, tak-ing into consideration all the egds and tgds in the input mapping and integrity constraints incorporated in the source schemas, if any. An initialization step finds all the projections of size one that are reducible. The initial candidate set is then fed to the enumeration algorithm. The reducibility test is employed by the enumeration algorithm to detect redundancy in the canonical mediated schema. Each maximal projection determining a minimal output mapping is added to the output.

T HEOREM 4. Let  X  be a set of tgds and egds over S with ter-minating chase. The algorithm merge ( S ,  X  ) produces all and only the subschemas of the canonical mediated schema with an output mapping that is complete (with respect to full s-t tgds), integrated, and minimal.
 P ROOF . The result simply follows the correctness of the Gen-Max algorithm [22], Lemma 1, Lemma 2, and Proposition 1. Language for Output Mappings The output mapping M gener-ated by the algorithm is the composition of M o , which is a union of a set of full s-t tgds and a set of target dependencies, and is a special form of full s-t tgds. A recent result by Arenas et al. [3] shows that composition of two mappings specified by s-t tgds with target dependencies is able to be expressed using source-to-target second order dependencies (s-t SO dependencies). Therefore, s-t SO dependencies can be used as the syntax for the output map-ping. An alternative is to use predicates in the canonical mediated schema G 0 as helper predicates and express the output mapping as a set of egds and tgds over ( S , G 0 , G ) . The latter is our current im-plementation for the output mapping language. Implementation of s-t SO dependencies is in our research agenda.
 Processing Queries Over Mediated Schema Given a user query against the mediated schema, there are two strategies for query pro-cessing. The first way is query answering using a universal solu-tion [19], while the second is query rewriting. When the input data dependencies admit a terminating chase, the output mapping gen-erated by our approach is guaranteed to admit also a terminating chase. Therefore, query answering can be performed by chasing the source instances against the output mapping and then perform query evaluation over the materialized instance of the mediated schema. However, the expressive mapping language we allow im-poses challenges on query rewriting, since the output mapping may involve recursion of relations. We detail here an algorithm which is able to rewrite a conjunctive query into a Datalog program when the input mapping consists of weakly acyclic tgds [19], which is an extension of the inverse rule algorithm for query rewriting in Local-As-View systems [16]. The rewriting algorithm proceeds in three stages. In the first stage, bottom-up generation of functional pat-terns of predicates are performed until a fixpoint is reached. This stage can be shared by rewriting of different queries against a given mediated schema. In the second stage, for a given user query, we check the reachability of patterned predicates backward from the given query. The third stage takes as input the reachable patterned rules obtained in the previous stage and employs the predicate-split [16] technique to produce a function-free Datalog program. The merging algorithm is implemented using Java SE 6 and SWI-Prolog 5.8.0. A parallel chase [13] is implemented in Pro-log for reducibility tests. A-priori enumeration is implemented in java. The experiments have been carried out on a 2GHz dual core computer. The maximal heap size is set to 512M, while the initial heap size is 40M. Disk I/O costs are excluded from profiling, while communication costs between Java and Prolog are included. Two sets of experiments are carried out. The first are performed over real data sets to evaluate effectiveness for practical merging scenar-ios. The second are carried out over a workload of various degrees of complexities to demonstrate the scalability. Experiments are carried out over the real world data sets from Illinois Semantic Integration Archive ( http://pages.cs.wisc. edu/~anhai/wisc-si-archive/ ). Three out of five data sets are used: Courses , Real Estate II , and Inventory . The remaining data sets are Faculty and Real Estate I . The former has an identical schema for all data sources and hence is of little interest for schema merging. The latter is left out because it is a variant of Real Estate II , with less complicated mappings.
 Expressiveness of Mapping Language Complex relationships in-volving joins of relations arise in the data sets, which confirms the necessity of tgds as the mapping language. Furthermore, we see it is crucial to be able to specify integrity constraints over source schemas. Without presence of keys in the source, e.g., house in RealEstate II , no attribute is projectable, even many attributes are asserted to be equivalent. This is in line with completeness: when there is no functional dependency, a tuple is only retrievable when all components are kept. The Courses data set demonstrates the strength of our approach to perform n-ary merge. The Courses data set consists of five heterogeneous sources for courses. Instead of consecutive binary merging, we utilize tgds from four binary mappings among the five data sources and perform only one merg-ing. This is particularly suitable in scenarios where only few fixed mappings are available, such as in a P2P setting. Value conver-sion functions and arithmetic expressions arising in the data sets are not directly expressible in tgds. We handle the problem by using skolem functions as in SO tgds [21]. Non-invertible func-tions are expressed as skolem terms in the head of tgds, while invertible functions are handled using helper predicate and rules to avoid recursive nesting of skolem functions during chase. An example is the concatenation of names, for which a ternary pred-icate concat ( FirstName , LastName , F ullNmae ) is introduced to-conclude that the language of tgds as in our approach is rich enough and necessary to capture real world relationships among data sources.

Comparison to Manually Created Schemas Our merging al-gorithm removed a large number of redundant attributes from the mediated schema for all three data sets: 31 over 70 (Courses), 16 over 57 (RealEstate II), 32 over 89 (Inventory). There is a man-ually created mediated schema available for Course . Our medi-ated schema differs from the referential schema in several aspects. First, some source attributes occurring in only one source are left out in the referential schema while our algorithm retains them due to the requirement of completeness. Second, our generated schema is more normalized than the referential schema, which is probably due to the fact that we follow a reduction based approach. Third, since we perform multi-way merging of five course schemas using only four binary mappings among them, some correspondences of attributes are not transitively captured in the mapping simply be-cause the intermediate schema in-between does not have an equiv-alent attribute. An example is course_rice and course_wsu have no direct mapping and are mapped independently to course_reed . course_reed does not have an attribute for comment . Therefore, comments from the two schemas are not revealed to be equivalent. The conclusion is our approach has successfully reduced schema level redundancy in real world scenarios.
Workload Generator A random workload generator has been implemented to provide inputs of varied complexities. The work-load generator consists of three components: a schema generator, a match generator, and a mapping generator. Given a set of config-urations, the schema generator generates a universal relation and a set of functional dependencies, which are then fed to a top-down decomposition based schema normalizer. The schema normalizer takes as input a universal relation and a set of functional depen-dencies, performs a normalization, and creates a database schema in BCNF with functional dependencies and acyclic inclusion de-pendencies. The match generator uniformly selects one position from each schema to form a value correspondence. Each value cor-respondence pair is selected independently of previously selected pairs. A cyclicity test is performed before each candidate value cor-respondence is admitted into a match, i.e., a set of correspondences, so that the implied mapping is weakly acyclic. Given a match, the mapping generator generates a tuple generating dependency using a simplified version of query discovery [18].
 Scalability Experiment Result We generated 166 random inputs ranging from 10 to 200 attributes, with the size of the dependency graph (covering mapping and ICs) from 10 to 5000. The running time wrt. the size of the dependency graph is demonstrated in Fig-ure 6. The time grows following a polynomial trend line. A close look at the generated workload reveals that most of them have only a small number of redundant attributes (165 out of the 166 inputs have less than 3 redundant attributes). Therefore, the enumeration of the candidate position sets does not take much time, which in worst case can be exponential. Therefore, the running time is deter-mined by the reducibility test, which is in PTIME when the query recoverability test is in PTIME. This is in line with our previous analysis.
Schema merging has a long history in database research. We do not head for a detailed survey, but instead focus on the evolution of schema merging methodologies and compare those works to our logical approach.

Batini et al. [6] provide an early survey covering a lot of classical view/database integration approaches. As the integration tasks are usually carried out in a schema design scenario, schemas are repre-sented in a variant of ER model or Object-Oriented model. Those approaches differ a lot on how inter-schema relationships between source schemas are represented. So called inter-schema assertions [32, 29] are a popular language specifying set-based relationships (e.g., inclusion, disjoint, and equal) between possible extensions of concepts in different schemas. Most of the approaches undergo a two phase procedure: first collapse equivalent elements in the source schemas and then resolve the conflicts arising in collaps-ing. Spaccapietra et al. [32] is a well known representative, which common shortcoming of this line of work is that no output map-ping in the form of logical mappings (e.g., tgds) are generated, al-though attribute correspondences between source schemas and tar-get schemas are an implicit result.

Schema merging using expressive logical mappings is consid-ered to be largely unexplored [14, 8]. [10] and [11] are pioneers using logical constraints in merging. Similar to us, they con-sider source integrity constraints and head for a minimal mediated schema. However, their input mapping language is a special class of mappings in the form of one-to-one relation-wise implications and keys are required to be present in each implication. Our ap-proach can be deemed as an extension of their work in the sense that we consider tuple generating dependencies which is much more expressive. Another distinction is that we consider source incom-pleteness, while they do not. In [28], Pottinger and Bernstein ex-tend their early work [27] to a merging algorithm working with relational schemas and generating output mappings. Similar to us, they also head for minimal mediated schemas in data integration. Our approach differs from theirs in several ways. First, their in-put mapping language is a special class of GLAV mappings using conjunctive queries to specify overlap between schemas. In con-trast, we consider arbitrary query containment constraints in the form of tuple generating dependencies with the only restriction that they admit a terminating chase. Second, their merging semantics is based on preserving source information and overlap. Since we do not have the concept of overlap, there is no overlap preservation in our requirements for schema merging. They assume that the ex-tensions of the sources do not conform to any direct constraints and hence their completeness requirement is based on preserving all extensionally stored data. In contrast, we consider source in-completeness as a basic assumption and take the input mapping as expected constraints over the integrated global database. Therefore, our completeness requires preserving not only extensional data but also inferred data in the form of certain answers. Third, the queries used in their approach to witness the complete preserving of source information do not contain joins, i.e., over a single relation, while our approach uses conjunctive queries over the mediated schema to reconstruct source information, which probably leads to smaller mediated schemas. Last but not least, source integrity constraints made use of in our approach are not exploited in theirs.
As clarified in [26], view integration is a closely related but semantically different problem from data integration. View inte-gration aims at creating a backend storage schema supporting the source schemas as views, which results in quite different require-ments on the merging algorithm. Melnik [25] proposes a straight-forward algorithm for view integration of logical schemas. The mediated schema is taken to be a disjoint union of the source schemas, with source dependencies and input mapping encoded as constraints. Output mappings are identity mappings copying part of the mediated schema to a corresponding source schema. Are-nas et al. [4] extend the work to achieve a smaller instance for the mediated schema by adding denial constraints. The two works dif-fer fundamentally from our work in that they head for creating a backend storage schema to support the views satisfying the input mapping. That X  X  why their output mapping is from the mediated schema to the source schemas while we create a mapping from the sources to the mediated schema. They are more concerned with creating a smaller mediated instance of the mediated schema while the schema X  X  size is insignificant. To the contrary, we aim at gener-ating a minimal query interface instead of a minimal instance.
Chiticariu et al. [12] propose an interactive schema merging ap-proach using schema matches as input. Concepts are extracted from logical schemas and each possible configuration of concept collaps-ing results in a plausible mediated schema. The space of plausible collapsing of concepts is then navigated by the user in an interac-tive manner. Since each extracted concept has a particular join path in the source schemas, two concepts and value correspondences be-tween them comprise an implicit GLAV mapping. Following this point of view, a schema match is a representation of a collection of uncertain mapping constraints. The work is extended in [30] to generate only top-k mediated schemas, with a ranking of qual-ity of candidate mediated schemas. A mediated schema is consid-ered more desired if it collapses concepts with higher similarity or higher sub-element coverage. Sarma et al. [31] provide another un-certain schema merging approach using also schema matches. They represent alternative mediated schemas as a probability distribution over different clustering of attributes. A probabilistic mapping [15] is produced for each possible mediated schema. In our approach, the input mapping is constrained by logical formulas and bears no uncertainty. However, uncertainties still arise from the fact that the same piece of information can be structured in different ways. Similarly to [12], we allow multiple plausible mediated schemas as output.

We make use of the notion of witness mapping in our defini-tion of completeness to support the recovery of certain answers. The witness mapping is from the mediated schema backward to the joint source schema. At a first glance, it is similar to the notion of mapping inverse [17] or mapping recovery [5]. However, the witness mapping is neither an inverse nor a recovery mapping of the output mapping. The composition of the output mapping and the witness mapping only need to be CQ-equivalent to the map-ping ( S , S ,  X  ) with  X  being the union of input tgds and source in-tegrity constraints, and it does not need to contain the pair ( I , I ) in the solution space,which is common for both, mapping inverse and mapping recovery, when I is an incomplete joint source instance.
In this paper, we have presented a novel approach to n-ary schema merging for the relational model. We extend existing work by considering source integrity constraints and using a much broader class of mapping language, namely tgds with terminating chase. Under OWA, we opt for minimal mediated schemas that are complete wrt. certain answers of CQs. We have developed an algorithm producing all desired mediated schemas as results of re-moving redundant columns from the joint source schema. We have also described the feasibility of query processing in our framework. Finally, the evaluation has shown the applicability of our approach to real world data sets.

In future work, we will study the various requirements for schema merging under different scenarios, i.e., by investigating the semantics for other applications than virtual data integration. One possible direction is data warehousing, in which a schema is cre-ated to materialize reconciled data. Furthermore, our approach for the relational model should be extended to other popular metamod-els, such as XML. Last but not least, merging schemas when the underlying data sources are not consistent is also of practical sig-nificance.
 Acknowledgements: The authors are grateful to the anony-mous referees for comments helping to improve the paper. The work is supported by the DFG Research Cluster on Ultra High-Speed Mobile Information and Communication UMIC ( www. umic.rwth-aachen.de ). [1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] M. Arenas, R. Fagin, and A. Nash. Composition with target [4] M. Arenas, J. P X rez, J. L. Reutter, and C. Riveros. [5] M. Arenas, J. P X rez, and C. Riveros. The recovery of a [6] C. Batini, M. Lenzerini, and S. B. Navathe. A comparative [7] P. A. Bernstein, A. Y. Halevy, and R. Pottinger. A vision for [8] P. A. Bernstein and H. Ho. Model management and schema [9] P. A. Bernstein and S. Melnik. Model management 2.0: [10] J. Biskup and B. Convent. A formal view integration method. [11] M. A. Casanova and V. M. P. Vidal. Towards a sound view [12] L. Chiticariu, P. G. Kolaitis, and L. Popa. Interactive [13] A. Deutsch, A. Nash, and J. Remmel. The chase revisited. In [14] A. Doan and A. Y. Halevy. Semantic integration research in [15] X. L. Dong, A. Y. Halevy, and C. Yu. Data integration with [16] O. M. Duschka and M. R. Genesereth. Answering recursive [17] R. Fagin. Inverting schema mappings. In Proc. PODS , pages [18] R. Fagin, L. M. Haas, M. A. Hern X ndez, R. J. Miller, [19] R. Fagin, P. Kolaitis, R. J. Miller, and L. Popa. Data [20] R. Fagin, P. G. Kolaitis, and L. Popa. Data exchange: getting [21] R. Fagin, P. G. Kolaitis, L. Popa, and W. C. Tan. Composing [22] K. Gouda and M. J. Zaki. Genmax: An efficient algorithm [23] R. Hull. Relative information capacity of simple relational [24] M. Lenzerini. Data integration: A theoretical perspective. In [25] S. Melnik. Generic Model Management: Concepts and [26] R. J. Miller, Y. E. Ioannidis, and R. Ramakrishnan. The use [27] R. Pottinger and P. A. Bernstein. Merging models based on [28] R. Pottinger and P. A. Bernstein. Schema merging and [29] C. Quix, D. Kensche, and X. Li. Generic schema merging. In [30] A. Radwan, L. Popa, I. R. Stanoi, and A. A. Younis. Top-k [31] A. D. Sarma, X. Dong, and A. Y. Halevy. Bootstrapping [32] S. Spaccapietra, C. Parent, and Y. Dupont. Model [33] J. D. Ullman. Information integration using logical views. In
