 remedy leads to improved classification performance. { v 1 , . . . , v m } each node v randomly draw a set of n indices Z replacement. We then manually label the n nodes v as accurately as possible. We encode the label y that of generating an estimation vector f recover the label y each y one. Given f corresponding label estimation  X  y y , then the classification error is err ( f indicator function.
 In order to estimate f = [ f given kernel matrix K  X  R m , the quadratic regularization f T Q f denotes the kernel matrix, and regular K denotes the number of classes. by a loss function  X  ( f set Z where  X  &gt; 0 is an appropriately chosen regularization parameter. P Theorem 1 Let  X  ( f a , constant b when  X  samples Z where  X  Z introduce the following notation. let i let Z data in Z k = 1 , . . . , K : where  X  [ f In fact, if  X  f ( Z holds. Otherwise, assume that  X  f ( Z y  X  ( x, 0)  X  a } ) / 2 , then either  X  f i d , it follows that there exists k = k  X  f or b K i We are now ready to prove Theorem 1 using (3). For every j  X  Z subset of n samples in Z  X  (  X  f j, ( Z n +1 ) , y j ) + b c X n K j,j p . We thus obtain for all f  X  R mK : cation. For the SVM loss  X   X  a = 1 / 16 , b = 0 . 5 , c = 0 . 5 in Theorem 1. Consider an undirected graph G = ( V, E ) defined on the nodes V = { v edges E  X  X  1 , . . . , m } X { 1 , . . . , m } , and weights w simplicity, we assume that ( j, j ) /  X  E and w Definition 1 Consider a graph G = ( V, E ) of m nodes with weights w The unnormalized Laplacian matrix L ( G )  X  R m  X  m is defined as: L j S -normalized Laplacian matrix is defined as: L regularization is based on: f T L . The idea is natural: we assume that the predictive values f S = D ) so that diagonals of L S become all one [3, 4, 7, 2]. Definition 2 Given label y = { y regularizer in Definition (1), which encourages f an in-class pair ( j, j  X  ) , we want to have S which we will investigate later in the paper. For unnormaliz ed Laplacian (i.e. S identical to the standard graph-theoretical definition: cut ( L , y ) = P We consider K in (1) defined as follows: K = (  X  S  X  1 + L parameter to make K strictly positive definite. This parameter is important. L where C Proof. Let f This rate of convergence is faster when p increases. However in general, tr we normalize the diagonal entries of K such that K p of Definition 3 A subgraph G is induced by restricting E on V  X  a pure component. Denote by  X  a pure subgraph (but not the only one). For each pure componen t G always zero. The second eigenvalue  X  Theorem 3 Let the assumptions of Theorem 2 hold, and G  X  =  X  q performance of (1), E where m Proof sketch . We simply upper bound tr L graph. Then S = I and cut ( L are balanced, while the convergence may behave like O ( p q/n ) otherwise. 3.1 Near zero-cut optimum scaling factors within each pure component ( S Let us define cut ( G  X  , y ) = P use cut ( L with the choice  X  s where u ( G  X  ) = min degree-based normalization method S and all other nodes of the same pure component with edge weigh t w model, the degree-based normalization can fail because the deg G ized Laplacian. Let v and thus  X  K for all j . We call this method of normalization ( S this paper as it scales the kernel matrix K so that each K degree-based normalization ( S of formance. We shall first introduce dimension reduction with normalized Laplacian L by P r r . In such context, the choice of K well approximated by its projection onto P q Theorem 4 Let G  X  =  X  q  X  (encoding of the true labels) for class k ( j = 1 , . . . , m ). Then k P r where  X  squares loss  X  ( f b = 0 . 5 , c = 0 . 5 , we have E Z over  X  , we obtain is replaced by kL caused by S . The 2-norm of the symmetric error matrix L be at an order of m times more than kL the smallest r eigenvalues and regularizes with f T K  X  1 f if P r least squares loss  X  Controlled data experiments does well even when L -scaling rather underperforms the unnormalized baseline. Real-world data experiments edge weight between the i -th and the j -th data points X the bag-of-word vectors and then set w loss function, where we use the oracle  X  which is optimal.
 K t K statistically significant ( p  X  0 . 01 ) in both Figure 2 (b-1) and (b-2). however, when dimension is reduced, kL smaller (Section 4), which suggests that K -scaling should improve performance. problem. Experiments confirm the superiority of the this nor malization scheme.
