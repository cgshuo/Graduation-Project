 In the context of document retrieval in the biomedical do-main, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. We demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. Arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with dif-ferent browsing strategies. With broader implications to IR, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility eval-uation. Simulation-based studies provide researchers with an additional evaluation tool to complement interactive and Cranfield-style experiments.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation, Measurement Keywords: related article search, find-similar
This work was motivated by a seemingly simple question:  X  X ow do users find things with PubMed? X  PubMed R  X  is a large, publicly-accessible Web-based search engine that pro-vides access to MEDLINE R  X  , the authoritative repository of abstracts from the medical and biomedical primary lit-erature. Both are maintained by the U.S. National Library of Medicine (NLM). MEDLINE currently contains over 17 million abstracts, covering a wide range of disciplines within the health sciences (broadly interpreted), from biochemistry to public health. As the primary access point to MEDLINE, PubMed is an indispensable tool for clinicians and scientists.
There is substantial evidence to suggest that PubMed can be difficult to use, since it is a pure Boolean retrieval engine that returns results sorted in reverse chronological order. A number of studies have demonstrated the superiority of best-match ranked retrieval over comparable Boolean techniques (e.g., [26], but see [9] for contrary results). Since almost all commercial Web search engines implement some sort of best-match algorithm, users have grown accustomed to using ranked retrieval systems. In contrast, the query formulation process in PubMed feels quite foreign. Empirical support comes from PubMed transaction logs, where long chains of repeated query formulations are frequently found X  X hey sug-gest that users often struggle coming up with the right query terms. In fact, approximately a fifth of all PubMed queries return zero results. 1 Related to these challenges is the dif-ficulty associated with controlling the result set size, which is another characteristic of Boolean retrieval. For example, adding an additional term to a query that retrieves 1000 hits might yield zero hits.

The importance of access to the primary literature for clin-icians and scientists lends merit to our motivating question. Abstracting from this specific instance into a more general problem, we explore the complex relationship between the quality of initial query results and the overall utility of an interactive system. In particular, we examine the contri-butions of a browsing tool based on content similarity: we hypothesize that such a feature can compensate for poor re-trieval results. Through strategy simulations, we uncover a non-linear relationship between utility and quality of the initial results. These findings highlight the need to evalu-ate an interactive IR application as a whole, and not simply component-wise.

More generally, this work represents a case study in the application of user simulations to automatically measure utility. We argue that simulation-based evaluations provide researchers with an alternative to existing methodologies, as well as a powerful formative tool that combines advantages of both interactive and Cranfield-style evaluations.
Given that query formulation in PubMed is a major issue, how do users go about finding relevant documents? We be-lieve that related article suggestions contribute significantly to a user X  X  overall search experience. This section overviews the feature and discusses our hypothesis.

Whenever the user examines an abstract in PubMed, the right panel of the browser is automatically populated with titles of articles that may also be of interest, as determined Figure 1: Typical PubMed screenshot showing a MEDLINE abstract and  X  X elated Links X . by a probabilistic content-similarity algorithm [20] (see Fig-ure 1). That is, each abstract view triggers a related article search: the top five results are integrated into a  X  X elated Links X  panel in the display. 2 This feature is similar to what Smucker and Allan [23] call find-similar ; cf. [11, 29]. Re-lated article suggestions provide an effective browsing tool for PubMed users, allowing them to navigate the document collection without explicitly issuing queries.

We hypothesize that related article suggestions compen-sate for cases where PubMed results are poor (whether due to difficulties in query formulation or lack of relevance rank-ing). As long as the initial results contain one relevant docu-ment, related article suggestions can help the user find more relevant documents by browsing around. Thus, the quality of the initial results is only one factor affecting the overall utility of the system.

Prima facie support for this argument comes from the cluster hypothesis [27]. Since relevant documents tend to be clustered together, a browsing tool based on content similar-ity should be effective in helping users gather additional rele-vant documents once one has already been encountered. The bulk of this paper focuses on simulations of search strategies, which appear to support this claim. However, we also relate experimental findings to a recent analysis of transaction logs from PubMed, lending further credibility to our conclusions (see Section 7).
IR evaluations generally fall into one of two categories: batch-style, system-centered evaluations in the Cranfield tra-dition [6] (as exemplified by ad hoc evaluations in TREC), and interactive, user-centered evaluations of searcher perfor-mance ([16] provides a classic example). Researchers have long acknowledged that Cranfield-style evaluations are lim-ited in examining only one aspect of information seeking X  the quality of a ranked list generated by a one-shot query. To measure the utility of interactive retrieval systems, one must typically turn to carefully-orchestrated user studies that ex-amine human search behavior. Many studies over the years have confirmed that the first is not a substitute for the sec-ond [3, 10, 24, 25], since a significantly  X  X etter X  retrieval al-gorithm (as measured by batch evaluations) might not lead to significantly better utility. Given these facts, why aren X  X  there more interactive evaluations that focus on utility?
The short answer is that interactive evaluations are dif-ficult to conduct, not for lack of trying [8]. Compared to batch evaluations, the high cost and time-consuming nature of interactive evaluations limit the speed at which hypothe-ses can be explored and the statistical significance of results. Due to the conflation of numerous user, task, and contextual factors that are unavoidable, even with the adoption of best practices in study design, results are difficult to compare and often do not support convincing generalizations.

Simulation-based evaluations have recently emerged as a promising methodology for bridging interactive and batch evaluations [18, 21, 22, 28]; see also similar work in the HCI community [5, 12]. Although details vary, they are all based on a common idea: instead of studying actual users, simulate what they might do and assess the likely outcomes. In other words, simulation-based evaluations examine the behavior of an idealized user who adopts a particular (known) strategy for interacting with retrieval systems.

User simulations can be viewed as a compromise between interactive and batch evaluations. They preserve the advan-tages of batch evaluations in that experiments remain easily and rapidly repeatable, while at the same time they begin to characterize the utility of an interactive system, poten-tially modeling user, task, and context factors. Naturally, the validity of simulation results is contingent on the real-ism of the simulations X  X ut it is possible to have meaningful debates over the realism of different user models, informed by results of user studies [11], eye-tracking experiments [14], log analysis [13], etc.

In fact, the Cranfield methodology can be viewed as a primitive user simulation: it models a user who types in a query and then examines the results sequentially. Many of the criticisms leveled against it speak to the poor assump-tions it makes about users: one shot retrieval goes against what we know about the iterative nature of information-seeking behavior; the assumption of binary, independent relevance judgments is an over-simplification of the com-plex nature of relevance; etc. With user simulations, we can begin to address these deficiencies in a principled fashion.
One potential concern with simulation-based evaluations is comparability of results. Characterizations of utility de-pend both on the system and the  X  X imulation module X , so the latter must be distributed in the same way that topics and qrels are widely available today. We envision the evolution of standard test suites, of which Cranfield test collections represent one specific type. Naturally, the community as a whole would need to converge on what these standard test suites might contain. We are hopeful that such a consensus is possible X  X n the same way that mean average precision and other evaluation methods became standard practice af-ter much debate in the early days of IR.

Of course, the emergence of user simulations as an eval-uation methodology does not obviate the need for interac-tive evaluations X  X here can ultimately be no replacement for users when the goal is to develop systems that are useful for human beings. We advocate simulations as a formative tool, replacing user studies in situations where they are simply too slow or cumbersome (e.g., for rapid prototyping). With the distribution of standard test suites, simulation-based evalu-ations should be no more difficult to conduct than current Cranfield-style experiments, and hence they represent a su-perior alternative. 3 In our view, traditional user studies will likely remain important for summative evaluations.

Finally, user simulations might be used prescriptively as well as descriptively. That is, results of user simulations could be used as a basis for educating users on effective search strategies. This is not an unrealistic scenario in the context of PubMed: due to the nature of its users and their work, PubMed searchers are often willing to learn effective search techniques and advanced features. 4
We began by abstracting pertinent elements of the prob-lem into a more controlled experiment, while preserving the overall goal of the study: to characterize the impact of a content-similarity browsing tool on utility.

The general setup was as follows: starting from an initial ranked list in response to an information need, we simulated user behavior under different browsing strategies. Each sim-ulation is characterized by a sequence of documents, which represents the order in which the user would examine docu-ments in the collection. Since both the input and output of the simulation are ordered lists of documents, we can assess and compare their quality using standard ranked retrieval metrics X  X his is similar to the strategy used by Aalbers-berg [1] for evaluating relevance feedback. In what follows, we describe the test collection, the initial results, the simu-lation procedure, and metrics used to capture utility.
Evaluations were conducted with data from the TREC 2005 genomics track [7], which employed a ten-year subset of MEDLINE (1994 X 2003). The collection contains approx-imately 4.6 million records, or about a third of the entire database at the time it was collected in 2004 (commonly known as the MEDLINE04 collection).

One salient feature of this TREC evaluation was its use of generic topic templates (GTTs), which consist of semantic types, such as genes and diseases, embedded in prototypical information needs, as determined from interviews with bi-ologists and other researchers. In total, five templates were developed, each with ten fully-instantiated topics; examples are shown in Table 1. For each topic, relevance judgments were provided by an undergraduate student and a Ph.D. re-searcher in biology. No relevant documents were found for one topic, which was discarded from our experiments.
As input to the user simulations, we wished to consider initial results that range widely in terms of quality. Since our hypothesis concerns situations where browsing compensates for poor results, we especially needed realistic samples of ranked lists with few relevant documents. Note that we are careful not to equate poor results with a poor retrieval
Information describing standard [methods or protocols] for doing some sort of experiment or procedure. methods or protocols: purification of rat IgM
Information describing the role(s) of a [gene] involved in a [disease]. gene: PRNP disease: Mad Cow Disease
Information describing the role of a [gene] in a specific [biological process]. gene: casein kinase II biological process: ribosome assembly
Information describing interactions between two or more [genes] in the [function of an organ] or in a [disease]. genes: Ret and GDNF function of an organ: kidney development
Information describing one or more [mutations] of a given [gene] and its [biological impact or role]. gene with mutation: hypocretin receptor 2 biological impact: narcolepsy Table 1: The five templates used in the TREC 2005 genomics track (with sample instantiations). system, since query formulation may play an important role (as in the case of PubMed). In addition, variations in topic difficulty, as well as variations in performance exhibited by even the best retrieval systems, contribute to poor query results.

Although Turpin and Scholer [25] present a technique for synthetically generating ranked lists that attain a specific mean average precision, we rejected their method since it does not yield results that correspond to any real system. Instead, we used as input all 62 runs submitted to the TREC 2005 genomics track (58 of which contributed to the pool). This gave us an accurate sampling of the types of results generated by modern retrieval engines. For the submitted runs, MAP ranged from 0.302 to 0.054 (mean of 0.197); P10 ranged from 0.474 to 0.176 (mean of 0.358).
We examined two different content-similarity algorithms and two different browsing strategies, yielding a two-by-two matrix experiment. Much of our procedure is similar to that used by Smucker and Allan [23], to which we refer the reader. Here, we provide only an overview.

One experimental variable was the algorithm for suggest-ing related articles. We considered two: Quite explicitly, our goal was not to compare Lemur with PubMed, but rather to examine the effects of different con-tent-similarity algorithms, given that the two have different theoretical foundations.
In terms of browsing behaviors, we examined the two proposed by Smucker and Allan [23]: the greedy pattern represents an abstraction of depth-first behavior in exam-ining a ranked list and the breadth-like pattern represents an abstraction of breadth-first search behavior. Both were adapted from the findings of an eye-tracking study con-ducted by Kl  X  ockner et al. [15], and are consistent with the results of Aula et al. [4]; cf. [14].

Under the greedy strategy, the simulated user starts with the initial ranked list and examines documents in rank or-der. Whenever a relevant document is encountered, the user applies content-similarity search and pulls up its list of re-lated documents (in the current PubMed interface, this is equivalent to clicking on the  X  X ee all Related Articles X  link). The user ceases to examine documents in a list after exam-ining 2 contiguous non-relevant documents. After stopping, the user hits the  X  X ack button X  and returns to the previous list and continues examining documents in that list (unless the user is already examining the initial results, in which case the user simply continues down the list).

Under the breadth-like strategy, the simulated user also examines documents in rank order. Unlike the greedy pat-tern, the breadth-like browser only begins to examine re-lated article suggestions when the ranked list X  X  quality be-comes too poor. As the user examines relevant documents, documents are placed in a first-in first-out queue local to the current list. When the precision at N , where N is the rank of the current document, drops below 0 . 5 or when 2 contiguous non-relevant documents have been encountered, the user applies content-similarity search to the first rele-vant document in the queue. When the user returns to the current list, the user applies content-similarity search to the next document in the queue until the queue is empty. The browser never applies content-similarity search on a relevant document more than once. The breadth-like strategy mod-els a user who delays exploration until the current list seems to have gone  X  X old X . The user stops examining a ranked list in the same manner and with the same criteria as the greedy browser, i.e., hitting the  X  X ack button X  after encountering 2 contiguous non-relevant documents.

In both simulations, the user never examines a document more than once, even though it is likely that a hit appears in several result lists (given the tendency for documents to clus-ter together). We see no good reason why a user would want to re-examine documents previously encountered. In a Web interface, visited links are typically marked (e.g., change in link color), providing a prominent visual cue to help users remember where they have already been.
Since ranked lists serve as inputs to our simulations and their outputs consist of ordered document sequences, it is ap-propriate to evaluate both using standard ranked retrieval metrics. We argue that measurements made on the simula-tion outputs quantify the overall utility of the system, since they capture the quality of documents that a user might encounter with the system. Therefore, we are not merely measuring one-shot retrieval effectiveness, but rather the usefulness of the system given a particular usage scenario (under a simple utility model that tracks the total amount of relevant material found).

As with most Web search engines, early precision is very important, as studies have shown that users focus primar-ily on top results [2, 14]. Thus, we settled on P20 as one metric X  X he cutoff is meaningful since PubMed presents 20 results per page. The downside of precision at fixed cutoff, of course, is its inability to capture recall-oriented perfor-mance. On the other hand, we feel that MAP at 1000, the most common ranked retrieval metric for IR experiments, does not accurately characterize utility in our case since typi-cal users are unlikely to examine that many consecutive hits. MAP at lower cutoffs (e.g., MAP at 20) is also problematic, since there may be more relevant documents than the size of the results list X  X n which case, a MAP of 1.0 would be impossible to achieve. This causes problems since unequal score ranges make meaningful cross-topic comparisons more difficult. Ultimately, we settled on interpolated precision at recall 0.5, which we feel captures a reasonable tradeoff between precision and recall.
For each of the conditions in our two-by-two matrix exper-iment, we ran a total of 3038 separate trials (62 runs  X  49 topics). In the graphs in Figure 2, each trial is plotted as a point in a scatter graph: the x coordinate represents interpo-lated precision at recall 0.5 (IPR50) for the initial ranked list (baseline), while the y coordinate represents IPR50 for the simulation (utility). The left graph shows the greedy brows-ing strategy and the right graph the breadth-like browsing strategy, both with Lemur. Plots for the PubMed content-Figure 3: Distribution of P20 scores for the initial ranked lists across 3038 trials (62 runs  X  49 topics). similarity algorithm look similar and are not included for space considerations.

For both plots, points above the line y = x represent instances where the content-similarity browsing tool would help users gather more relevant documents than could be ob-tained with the ranked list alone. For both strategies, we see that substantial improvements are possible at low baseline performance levels X  X his appears to confirm our hypothesis that a content-similarity browsing tool can compensate for poor query results. As a note, points on the left edge of the plots ( x = 0) are simply artifacts of the interpolation, since our simulations cannot possible improve initial results containing zero relevant documents.

It is interesting that with good initial results, browsing related article suggestions can actually be detrimental, par-ticularly with the greedy browsing strategy, which exploits content-similarity search at the earliest possible moment without regard to the quality of the current results list. Note that in the breadth-like browsing strategy, the user will not even consider related article suggestions if the initial results are good enough (precision above 0.5, no two contiguous non-relevant documents). Thus we see the linear relation-ship between baseline and utility at high IPR50 values. In general, we are not particularly concerned with the cases of decreased performance because real users generally ex-hibit some type of lightweight lookahead behavior, which is not captured in our simulations. Eye-tracking studies have shown that before clicking on a link, users often  X  X ook ahead X  a couple of hits to see if they might be relevant [14]. We be-lieve that this type of behavior would suppress browsing in cases where the initial results were very good. Furthermore, we find that initial results are more often poor than they are good (more on this below).

Since there are only 21 possible P20 values, the metric supports aggregation in a straightforward manner. In Fig-ure 3, we show the distribution of P20 scores for all initial results (3038 trials). There appears to be a bimodal distri-bution, corresponding to  X  X asy X  topics (i.e., high P20 scores) and  X  X ard X  topics (i.e., low P20 score). For 65% of the tri-als, the P20 score of the initial results is 0.35 or lower. This underscores the importance of improving bad results and further lessens the impact of cases where the browsing tool might be detrimental (see above).

We see from Figure 3 that in 23% of all cases, no rele-vant documents were found in the top twenty results, which indicates that on the whole the topics were quite challeng-ing for modern retrieval systems. Note that the content-similarity browsing tool cannot improve on cases where no relevant documents are found in the initial results, since the underlying premise of the tool X  X  effectiveness is that relevant documents cluster together (in our simulations, users only apply content-similarity search to relevant documents).
In Figure 4, we compute the mean P20 utility at each base-line P20 value, showing the results as line graphs: the left fig-ure shows simulations with Lemur as the content-similarity algorithm, while the right graph shows simulations with the PubMed algorithm. Both graphs focus on the 65% of cases where P20 is 0.35 or lower.

These results support our hypothesis that a browsing tool based on content similarity can compensate for poor query results. As an example, starting from a ranked list with 15% precision, a user stands to gain about eight absolute per-centage points on average from browsing Lemur-suggested related articles. Although obviously exaggerated, this trans-lates into a relative improvement of approximately fifty per-cent! We see similarly large improvements with the PubMed content-similarity algorithm. The fact that consistent gains are achieved, independent of the browsing strategy and con-tent-similarity algorithm, suggests that our results are not merely artifacts of experimental design.

The downside of the graphs in Figure 3 is that they hide per-topic variations in simulated utility scores. A more de-bars represent  X  [0 . 10 , 0 . 30] , darker bars represent  X  X  0 . 35 tailed analysis is shown in Figure 5, where we graph the fraction of trials at each baseline P20 level that resulted in increases or decreases in simulated P20 utility. Lighter bars represent absolute increases or decreases of between 0.10 and 0.30 (inclusive); darker bars represent absolute increases or decreases of 0.35 or more (although no such decreases were observed). To improve clarity in presentation we ignored cases where P20 changed only slightly (  X  0 . 05), i.e., differ-ence of one document. The left graph shows results for the greedy strategy with Lemur and the right graph shows re-sults for the breadth-like strategy with Lemur. Results with PubMed appear similar and are not shown for space consid-erations. As an example of how to understand these graphs: for the breadth-like strategy with Lemur (right graph), of all trials whose initial result set contained 2 relevant documents (P20 of 0.1), about a quarter of the time the simulated user encountered at least two additional relevant documents (P20 gain of at least 0.1), and nearly six percent of the time the user encountered at least seven addition relevant documents (P20 gain of at least 0.35). At that particular baseline P20 level, in no cases did content-similarity browsing decrease precision by more than 0.05.

We see from these results that at baseline P20 scores below 0.25, browsing related article suggestions rarely hurts (less than five percent of the time). These bar graphs further underscore our finding that content-similarity browsing is especially helpful when initial results are poor.

Another significant finding from these experiments is the non-linear relationship between initial result quality and util-ity. We can divide the utility curves in Figure 4 into two general regions:
Similar non-linear relationships between result quality and utility have previously been noted [3, 25]. These observa-tions hold important implications for work on retrieval algo-rithms or any attempt to improve retrieval performance: the marginal gain in utility depends not only on the marginal improvement in quality of retrieved results, but also on the absolute quality itself. At low precision levels, the marginal gain in utility is quite high. For example, compare a system that returns one relevant document to a system that returns none; to the extent that the cluster hypothesis holds, a user could browse the document collection to find more relevant documents starting with one relevant document. At higher precision levels, the marginal gain in utility could be quite low. For example, more relevant documents returned in the retrieved results would have been found by browsing any-way, thereby lessening the impact of improvements in one-shot retrieval effectiveness. These observations suggest that emphasis in research should be placed on improving poor results, and that averaging per-topic scores hides important variations in performance.
To recap the results thus far: we hypothesized that in an interactive retrieval application, a content-similarity brows-ing tool can compensate for poor quality results returned by the system. User simulations appear to support this claim, independent of the initial retrieval algorithm, the content-similarity algorithm, or specifics of the browsing strategy. More broadly, we intend for this work to highlight the po-tential advantages of simulation-based evaluations. In this section, we discuss a number of insights that would not have been possible with either a user study or a Cranfield-style experiment.

User studies at scales comparable to our simulations are not practical X  X ur two-by-two matrix design contains 3038 trials for each condition. Needless to say, a large num-ber of trials makes trends much more apparent and facil-itates quantitative comparisons. Furthermore, the simu-lations have taught us valuable lessons about the experi-mental design space: for example, that the specific content-similarity algorithm is not as important as one might think. Without this knowledge, it would be natural for a researcher to explore different algorithms in a user study, thereby mak-ing less effective use of subjects X  time. This represents a great example of leveraging simulations in a formative ca-pacity, to inform the design of summative evaluations. By eliminating less important factors, researchers can explore the design space more efficiently.

The simulation module aside (which can be developed into a reusable evaluation component with additional engineer-ing), our evaluations are no more difficult to conduct than traditional Cranfield-style experiments. Retaining the most important characteristic of such evaluations, simulation runs can be rapidly conducted and repeated as often as necessary to support tight development cycles. Yet, the simulations re-veal insights that are not possible in the standard Cranfield methodology. Simulations begin to characterize utility X  X ot only a system X  X  one-shot retrieval effectiveness, but also ways in which a searcher would use the system. In this specific study, we uncovered a complex relationship between initial retrieval quality and utility, which can be helpful in guiding the development of future retrieval systems.
Let us now return to our original motivating question:  X  X ow do users find things with PubMed? X  Instead of directly addressing the question, we abstracted a more general prob-lem and explored the effectiveness of a content-similarity browsing tool. In this section, we discuss how experimental findings directly relate to PubMed.

We are able to demonstrate that browsing related arti-cle suggestions is an effective information-seeking strategy. This, however, says nothing about whether users actually take advantage of the PubMed feature. Fortunately, inde-pendent evidence provides a more complete characterization of user behavior. A recent analysis of PubMed transaction logs indicates that searchers click on suggested article titles with significant frequency [19]. Data gathered during a one-week period in June 2007 indicate that approximately 5% of page views in non-trivial user sessions (discarding, for ex-ample, sessions that consist of one page view) are generated from users clicking on related article links. Approximately one fifth of all non-trivial user sessions involve at least one click on a related article link. Furthermore, there is evidence of sustained browsing using this feature: the most frequent action following a click on a related article link is a click on another related article link (about 40% of the time). Thus, browsing related articles appears to be an integral part of PubMed searchers X  activities.

As a final experiment, we ran our strategy simulations on actual PubMed results. One of the co-authors manu-ally formulated PubMed queries interactively for each of the topics in the TREC 2005 genomics track test collection (49 topics). Using the topic templates as a starting point, the  X  X earl growing X  strategy was adopted: the aim was to find a relevant document, and then use it as a basis for learning more about the topic (for query expansion, refinement, etc.). Although the co-author was not a domain expert, the rele-vance judgments made the task manageable. Approximately 5 X 10 minutes were spent on each topic: queries were refor-mulated until at least one relevant document was found in the top 1000 results, or until the allotted time had expired. No specific attention was made to crafting a precise query, nor one that obtained high recall. Figure 6: Schematic representation of the top 20 document retrieved by manual PubMed queries; rel-evant documents shown as darker bars.

Due to the strict Boolean nature of PubMed, the number of results varied widely: 264 mean, 58 median, 367 stan-dard deviation (the number of results returned was capped at 1000). In Figure 6, we show the number of relevant docu-ments retrieved in the top twenty (darker bars). The lighter bars represent the total number of retrieved documents if less than 20, which was the case for 16 topics. Due to these variations, P20 does not completely capture retrieval perfor-mance; for example, in the case where only one document was retrieved (and it was relevant), P20 would only be 0.05, yet the result cannot be any more precise.

Acknowledging that searchers vary widely in terms of in-formation-seeking behavior, we are careful not to draw any generalizations from this particular set of queries. Instead, these results are intended to provide one example of how PubMed users might realistically behave. Our searcher might be characterized as an expert searcher in general (being an IR researcher), but one who is not specifically trained in the PubMed query language. We might imagine other classes of users who are domain experts and highly skilled in the inter-face (e.g., medical librarians), in which case higher perfor-mance would be expected. In these situations, there might be less of a need for browsing. Other users might be do-main experts, but not specifically trained in search tech-niques (e.g., physicians), in which case performance might be comparable or lower. In these situations, browsing would certainly provide a useful tool.

We applied the breadth-like browsing strategy to the re-sults of the manual PubMed queries. Simulated utility is shown in Figure 7. Instead of measuring P20, we counted the absolute number of relevant documents in the first 20 hits (which better accounts for cases where the initial result set contained fewer than 20 documents). Topics are arranged in ascending order of number of relevant documents in the ini-tial results (shown as squares). The vertical bars denote the results of the simulations. For one topic, starting from a sin-gle relevant document, browsing related article suggestions would yield 15 additional relevant documents. The graph does not show the 11 topics for which the 20 initial results contained zero relevant documents.

The simulation output appears consistent with the results in Section 5. On the whole, topics are difficult, and the ini-tial results contain few relevant documents most of the time. Also, we see that topics with fewer relevant documents in Figure 7: Results of applying the breadth-like browsing strategy to manual queries. Squares repre-sent initial results; bars represent simulation results. the initial results get a bigger boost from browsing. Over-all, this confirms the generality of our findings about the content-similarity browsing tool, independent of the actual search engine.
The contributions of this work are twofold: First, our ex-periments provide a deeper understanding of how PubMed users find information. We demonstrate that a browsing tool based on content similarity is able to compensate for poor retrieval quality, illustrating the complex relationship between retrieval performance and utility. Second, and more broadly, this work represents a case study in the application of user simulations for automatic utility evaluation. As a complement to both interactive user studies and Cranfield-style experiments, simulation-based evaluations provide a powerful and flexible tool for formative studies.
The first author was supported in part by the National Li-brary of Medicine; the second author, in part by the Center for Intelligent Information Retrieval and in part by DARPA under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. The first author wishes to thank Es-ther and Kiri for their kind support, as well as Doug Oard and Ryen White for discussions related to  X  X he RAT X .
