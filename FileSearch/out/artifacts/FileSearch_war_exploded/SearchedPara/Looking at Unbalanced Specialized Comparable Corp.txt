 The bilingual lexicon extraction task from bilin-gual corpora was initially addressed by using par-allel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lex-icons, parallel corpora are scarce resources, es-pecially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has fo-cused on another kind of bilingual corpora com-prised of texts sharing common features such as domain, genre, sampling period, etc. without hav-ing a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora , have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and Che-ung (2004), who range bilingual corpora from par-allel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e. a kind of filiation).

The bilingual lexicon extraction task from com-parable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach , dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data).

In this paper we want to show that the assump-tion that comparable corpora should be balanced for bilingual lexicon extraction task is unfounded. Moreover, this assumption is prejudicial for spe-cialized comparable corpora, especially when in-volving the English language for which many doc-uments are available due the prevailing position of this language as a standard for international scientific publications. Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the un-founded assumption of the balance of the spe-cialized comparable corpora. In specialized do-mains, the comparable corpora are traditionally of small size (around 1 million words) in comparison with comparable corpus-based general language (up to 100 million words). Consequently, the ob-servations of word co-occurrences which is the ba-sis of the standard approach are unreliable. To make them more reliable, our second contribution is to contrast different regression models in order to boost the observations of word co-occurrences. This strategy allows to improve the quality of ex-tracted bilingual lexicons from comparable cor-pora. In this section, we first describe the standard ap-proach that deals with the task of bilingual lexi-con extraction from comparable corpora. We then present an extension of this approach based on re-gression models. Finally, we discuss works related to this study. 2.1 Standard Approach The main work in bilingual lexicon extraction from comparable corpora is based on lexical con-text analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts. The basis of this obser-vation consists in the identification of  X  X irst-order affinities X  for each source and target language:  X  First-order affinities describe what other words are likely to be found in the immediate vicinity of a given word  X  (Grefenstette, 1994, p. 279). These affinities can be represented by context vec-tors, and each vector element represents a word which occurs within the window of the word to be translated (e.g. a seven-word window approxi-mates syntactic dependencies). In order to empha-size significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association mea-sure. Then, the translation is obtained by compar-ing the source context vector to each translation candidate vector after having translated each ele-ment of the source vector with a general dictio-nary.

The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D  X  ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the Transferring context vectors Using a bilingual Finding candidate translations For a word to be i a = cooc ( i, j ) b = cooc ( i,  X  j )  X  i c = cooc (  X  i, j ) d = cooc (  X  i,  X  j )
This approach is sensitive to the choice of pa-rameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010).
The standard approach is used by most re-searchers so far (Rapp, 1995; Fung, 1998; Pe-ters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D  X  ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; References Domain Languages Source/Target Sizes Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words Rapp (1999) Newspaper GE/EN 135/163 million words D  X  ejean et al. (2002) Medical GE/EN 100,000/100,000 words Morin et al. (2007) Medical FR/JP 693,666/807,287 words Otero (2007) European Parliament SP/EN 14/17 million words Bouamor et al. (2013) Financial FR/EN 402,486/756,840 words -Medical FR/EN 396,524/524,805 words Bouamor et al., 2013, among others) with the im-plicit hypothesis that comparable corpora are bal-anced. As McEnery and Xiao (2007, p. 21) ob-serve, a specialized comparable corpus is built as balanced by analogy with a parallel corpus:  X  Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora.  X . For instance, Ta-ble 2 describes the comparable corpora used in the main work dedicated to bilingual lexicon extrac-tion for which the ratio between the size of the source and the target texts is comprised between 1 and 1.8.

In fact, the assumption that words which have the same meaning in different languages should have the same lexical context distributions does not involve working with balanced comparable been paid to the problem of using unbalanced comparable corpora for bilingual lexicon extrac-tion. Since the context vectors are computed from each part of the comparable corpus rather than through the parts of the comparable corpora, the standard approach is relatively insensitive to dif-ferences in corpus sizes. The only precaution for using the standard approach with unbalanced cor-pora is to normalize the association measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its associ-ation scores). 2.2 Prediction Model Since comparable corpora are usually small in spe-cialized domains (see Table 2), the discrimina-tive power of context vectors (i.e. the observa-tions of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013). This consists in as-signing to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus.

In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013), one strategy consists in ad-dressing this problem through regression: given training corpora of small and large size (abun-dant in the general domain), we predict word co-occurrence counts in order to make them more reliable. We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the standard approach. Our work differs from Hazem and Morin (2013) in two ways. First, while they experienced the linear re-gression model, we propose to contrast different regression models. Second, we apply regression to unbalanced comparable corpora and study the impact of prediction when applied to the source texts, the target texts and both source and target texts of the used comparable corpora.

We use regression analysis to describe the rela-tionship between word co-occurrence counts in a large corpus (the response variable) and word co-occurrence counts in a small corpus (the predictor variable). As most regression models have already been described in great detail (Christensen, 1997; Agresti, 2007), the derivation of most models is only briefly introduced in this work.

As we can not claim that the prediction of word co-occurrence counts is a linear problem, we con-sider in addition to the simple linear regression model ( Lin ), a generalized linear model which is the logistic regression model ( Logit ) and non lin-ear regression models such as polynomial regres-sion model ( P oly n ) of order n . Given an input vector x  X  R m , where x 1 ,..., x m represent fea-tures, we find a prediction  X  y  X  R m for the co-occurrence count of a couple of words y  X  R us-ing one of the regression models presented below: where  X  i are the parameters to estimate.

Let us denote by f the regression function and by cooc ( w i , w j ) the co-occurrence count of the words w i and w j . The resulting predicted value of following equation: 2.3 Related Work In the past few years, several contributions have been proposed to improve each step of the stan-dard approach.

Prochasson et al. (2009) enhance the represen-tativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also sug-gest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for fil-tering the noise of the context vectors. In another way, Rubino and Linar ` es (2011) improve the con-text words based on the hypothesis that a word and its candidate translations share thematic similari-ties. Yu and Tsujii (2009) and Otero (2007) pro-pose, for their part, to replace the window-based method by a syntax-based method in order to im-prove the representation of the lexical context.
To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a stan-dard general language dictionary with a special-ized dictionary, whereas D  X  ejean et al. (2002) use the hierarchical properties of a specialized the-saurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by us-ing identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis).
The rank of candidate translations can be im-proved by integrating different heuristics. For in-stance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symme-try. From the ranked list of candidate translations, the standard approach is applied in the reverse direction to find the source counterparts of the first target candidate translations. And then only the target candidate translations that had the ini-tial source word among the first reverse candidate translations are kept. Laroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms. Here, candidate translations which are cognates of the word to be translated are ranked first among the list of trans-lation candidates. In this section, we outline the different textual re-sources used for our experiments: the comparable corpora, the bilingual dictionary and the terminol-ogy reference lists. 3.1 Specialized Comparable Corpora For our experiments, we used two specialized French/English comparable corpora: Breast cancer corpus This comparable corpus is Diabetes corpus The documents making up the
The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of dis-tinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparabil-ity measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [ breast cancer corpus i ] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i ( i  X  [1 , 14] ). 3.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 French Part 1 7,376 4,982 English Part 1 8,214 (79.2) 5,181 (75.2) Part 2 7,788 (78.8) 5,446 (75.9) Part 3 8,370 (78.8) 5,610 (76.6) Part 4 7,992 (79.3) 5,426 (74.8) Part 5 7,958 (78.7) 5,610 (75.0) Part 6 8,230 (79.1) 5,719 (73.6) Part 7 8,035 (78.3) 5,362 (75.6) Part 8 8,008 (78.8) 5,432 (74.6) Part 9 8,334 (79.6) 5,398 (74.2) Part 10 7,978 (79.1) 5,059 (75.6) Part 11 8,373 (79.4) 5,264 (74.9) Part 12 8,065 (78.9) 4,644 (73.4) Part 13 7,847 (80.0) 5,369 (74.8) Part 14 8,457 (78.9) 5,669 (74.8) Table 3: Number of distinct words (# words) and degree of comparability (comp.) for each compa-rable corpora source is a general language dictionary which con-tains only a few terms related to the medical do-main. 3.3 Terminology Reference Lists To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part single words were extracted for the breast can-cer corpus and 244 French/English single words were extracted for the diabetes corpus. It should be noted that the evaluation of terminology ex-traction using specialized comparable corpora of-corpora ten relies on lists of a small size: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2007), 125 and 79 in Bouamor et al. (2013). In this section, we present experiments to evaluate the influence of comparable corpus size and pre-diction models on the quality of bilingual termi-nology extraction.

We present the results obtained for the terms be-longing to the reference list for English to French direction measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008) as fol-lows: where | Ref | is the number of terms of the refer-ence list and r i the rank of the correct candidate translation i . 4.1 Standard Approach Evaluation In order to evaluate the influence of corpus size on the bilingual terminology extraction task, two ex-periments have been carried out using the standard approach. We first performed an experiment using each comparable corpus independently of the oth-ers (we refer to these corpora as balanced corpora). We then conducted a second experiment where we varied the size of the English part of the compara-ble corpus, from 530,000 to 7.4 million words for the breast cancer corpus in 530,000 words steps, and from 250,000 to 3.5 million words for the di-abetes corpus in 250,000 words steps (we refer to these corpora as unbalanced corpora). In the ex-periments reported here, the size of the context window w was set to 3 (i.e. a seven-word window that approximates syntactic dependencies), the re-tained association and similarity measures were the discounted log-odds and the Cosine (see Sec-tion 2.1). The results shown were those that give the best performance for the comparable corpora used individually.

Table 4 shows the results of the standard ap-proach on the balanced and the unbalanced breast cancer and diabetes comparable corpora. Each column corresponds to the English part i ( i  X  [1 , 14] ) of a given comparable corpus. The first line presents the results for each individual com-parable corpus and the second line presents the re-sults for the cumulative comparable corpus. For instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [ breast cancer corpus 3 ] (MAP of 21.0%), and ii) of [ breast cancer corpus 1, 2 and 3 ] (MAP of 34.7%).

As a preliminary remark, we can notice that the results differ noticeably according to the compa-rable corpus used individually (MAP variation be-tween 21.0% and 29.6% for the breast cancer cor-pora and between 10.5% and 16.5% for the dia-betes corpora). We can also note that the MAP of all the unbalanced comparable corpora is al-ways higher than any individual comparable cor-pus. Overall, starting with a MAP of 26.1% as provided by the balanced [ breast cancer corpus 1 ] , we are able to increase it to 42.3% with the un-balanced [ breast cancer corpus 12 ] (the variation observed for some unbalanced corpora such as [ diabetes corpus 12, 13 and 14 ] can be explained by the fact that adding more data in the source language increases the error rate of the translation phase of the standard approach, which leads to the introduction of additional noise in the translated context vectors). sides of the comparable corpora) 4.2 Prediction Evaluation The aim of this experiment is two-fold: first, we want to evaluate the usefulness of predicting word co-occurrence counts and second, we want to find out whether it is more appropriate to apply predic-tion to the source side, the target side or both sides of the bilingual comparable corpora.
 Table 6: Results (MAP %) of the standard ap-proach using different regression models on the balanced breast cancer and diabetes corpora 4.2.1 Regression Models Comparison We contrast the prediction models presented in Section 2.2 to findout which is the most appropri-ate model to use as a pre-processing step of the standard approach. We chose the balanced corpora where the standard approach has shown the best results in the previous experiment, namely [ breast cancer corpus 12] and [ diabetes corpus 7] .
Table 6 shows a comparison between the standard approach without prediction noted N o prediction and the standard approach with pre-diction models. We contrast the simple linear re-gression model ( Lin ) with the second and the third order polynomial regressions ( P oly 2 and P oly 3 ) and the logistic regression model ( Logit ). We can notice that except for the Logit model, all the regression models outperform the baseline ( N o prediction ). Also, as we can see, the results obtained with the linear and polynomial regres-sions are very close. This suggests that both linear and polynomial regressions are suitable as a pre-processing step of the standard approach, while the logistic regression seems to be inappropriate according to the results shown in Table 6.
That said, the gain of regression models is not significant. This may be due to the regression pa-rameters that have been learned from a training corpus of the general domain. Another reason that could explain these results is the prediction pro-cess. We applied the same regression function to all co-occurrence counts while learning mod-els for low and high frequencies should have been more appropriate. In the light of the above results, we believe that prediction can be beneficial to our task. 4.2.2 Source versus Target Prediction Table 5 shows a comparison between the standard approach without prediction noted N o prediction and the standard approach based on the predic-tion of the source side noted Source pred , the tar-get side noted T arget pred and both sides noted Source pred + T arget pred . If prediction can not re-place a large amount of data, it aims at increasing co-occurrence counts as if large amounts of data were at our disposal. In this case, applying pre-diction to the source side may simulate a config-uration of using unbalanced comparable corpora where the source side is n times bigger than the target side. Predicting the target side only, may cancer and the diabetes corpora leads us to the opposite configuration where the target side is n times bigger than the source side. Finally, predicting both sides may simulate a large comparable corpora on both sides. In this experi-ment, we chose to use the linear regression model ( Lin ) for the prediction part. That said, the other regression models have shown the same behavior as Lin .

We can see that the best results are obtained by the Source pred approach for both comparable cor-pora. We can also notice that predicting the tar-get side and both sides of the comparable corpora degrades the results. It is not surprising that pre-dicting the target side only leads to lower results, since it is well known that a better characterization of a word to translate (given from the source side) leads to better results. We can deduce from Ta-ble 5 that source prediction is the most appropriate configuration to improve the quality of extracted lexicons. This configuration which simulates the use of unbalanced corpora leads us to think that using prediction with unbalanced comparable cor-pora should also increase the performance of the standard approach. This assumption is evaluated in the next Subsection. 4.3 Predicting Unbalanced Corpora In this last experiment we contrast the standard approach applied to the balanced and unbalanced corpora noted Balanced and U nbalanced with the standard approach combined with the predic-tion model noted Balanced + P rediction and U nbalanced + P rediction .

Figure 1(a) illustrates the results of the exper-iments conducted on the breast cancer corpus. We can see that the U nbalanced approach sig-nificantly outperforms the baseline ( Balanced ). The big difference between the Balanced and the U nbalanced approaches would indicate that the latter is optimal. We can also notice that the prediction model applied to the balanced corpus ( Balanced + P rediction ) slightly outperforms the baseline while the U nbalanced + P rediction approach significantly outperforms the three other approaches (moreover the variation observed with the U nbalanced approach are lower than the U nbalanced + P rediction approach). Overall, the prediction increases the performance of the standard approach especially for unbalanced cor-pora.

The results of the experiments conducted on the diabetes corpus are shown in Figure 1(b). As for the previous experiment, we can see that the U nbalanced approach significantly outperforms the Balanced approach. This confirms the unbal-anced hypothesis and would motivate the use of unbalanced corpora when they are available. We can also notice that the Balanced + P rediction approach slightly outperforms the baseline while the U nbalanced + P rediction approach gives the best results. Here also, the prediction increases the performance of the standard approach especially for unbalanced corpora. It is clear that in addi-tion to the benefit of using unbalanced comparable corpora, prediction shows a positive impact on the performance of the standard approach. In this paper, we have studied how an unbalanced specialized comparable corpus could influence the quality of the bilingual lexicon extraction. This as-pect represents a significant interest when working with specialized comparable corpora for which the quantity of the data collected may differ depend-ing on the languages involved, especially when in-volving the English language as many scientific documents are available. More precisely, our dif-ferent experiments show that using an unbalanced specialized comparable corpus always improves the quality of word translations. Thus, the MAP goes up from 29.6% (best result on the balanced corpora) to 42.3% (best result on the unbalanced corpora) in the breast cancer domain, and from 16.5% to 26.0% in the diabetes domain. Addition-ally, these results can be improved by using a pre-diction model of the word co-occurrence counts. Here, the MAP goes up from 42.3% (best result on the unbalanced corpora) to 46.9% (best result on the unbalanced corpora with prediction) in the breast cancer domain, and from 26.0% to 29.8% in the diabetes domain. We hope that this study will pave the way for using specialized unbalanced comparable corpora for bilingual lexicon extrac-tion.
 This work is supported by the French National Re-search Agency under grant ANR-12-CORD-0020.
