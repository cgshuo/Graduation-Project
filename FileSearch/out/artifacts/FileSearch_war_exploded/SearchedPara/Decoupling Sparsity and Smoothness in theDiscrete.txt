 The hierarchical Dirichlet process (HDP) [ 1 ] has emerged as a powerful model for the unsupervised analysis of text. The HDP models documents as distributions over a collection of latent components, over terms associated with that topic. The per-document distributions over topics represent systematic regularities of word use among the documents; the per-topic distributions over terms encode the randomness inherent in observations from the topics. The number of topics is unbounded. Given a corpus of documents, analysis proceeds by approximating the posterior of the topics and topic proportions. This posterior bundles the two types of regularity. It is a probabilistic decomposition of the corpus into its systematic components, i.e., the distributions over topics associated with each document, and a representation of our uncertainty surrounding observations from each of those components, i.e., the topic distributions themselves. With this perspective, it is important to investigate how prior assumptions behind the HDP affect our inferences of these regularities. In the HDP for document modeling, the topics are typically assumed drawn from an exchangeable Dirichlet, a Dirichlet for which the components of the vector parameter are equal to the same scalar parameter. As this scalar parameter approaches zero, it affects the Dirichlet in two ways. First, the resulting draws of random distributions will place their mass on only a few terms. That is, the resulting topics will be sparse . Second, given observations from such a Dirichlet, a small scalar parameter encodes increased confidence in the estimate from the observed counts. As the parameter approaches zero, the expectation of each per-term probability becomes closer to its empirical estimate. Thus, the expected distribution over terms becomes less smooth . The single scalar Dirichlet parameter affects both the sparsity of the topics and smoothness of the word probabilities within them. When employing the exchangeable Dirichlet in an HDP, these distinct properties of the prior have consequences for both the global and local regularities captured by the model. Globally, posterior inference will prefer more topics because more sparse topics are needed to account for the observed words of the collection. Locally, the per-topic distribution over terms will be less smooth X  X he posterior distribution has more confidence in its assessment of the per-topic word probabilities X  X nd this results in less smooth document-specific predictive distributions.
 The goal of this work is to decouple sparsity and smoothness in the HDP. With the sparse topic model (sparseTM), we can fit sparse topics with more smoothing. Rather than placing a prior for the entire vocabulary, we introduce a Bernoulli variable for each term and each topic to determine whether or not the term appears in the topic. Conditioned on these variables, each topic is represented by a multinomial distribution over its subset of the vocabulary, a sparse representation.
 This prior smoothes only the relevant terms and thus the smoothness and sparsity are controlled through different hyper-parameters. As we will demonstrate, sparseTMs give better predictive performance with simpler models than traditional approaches. Sparse topic models (sparseTMs) aim to separately control the number of terms in a topic, i.e., use, represented as a distribution over the fixed vocabulary of the collection. In order to decouple smoothness and sparsity, we define a topic on a random subset of the vocabulary (giving sparsity), and then model uncertainty of the probabilities on that subset (giving smoothness). For each topic, we introduce a Bernoulli variable for each term in the vocabulary that decides whether the term appears in the topic. Similar ideas of using Bernoulli variables to represent  X  X n X  and  X  X ff X  have been seen in several other models, such as the noisy-OR model [ 4 ] and aspect Bernoulli model [ 5 ]. We can chooses the terms for the topic; the  X  X lab X  only smoothes those terms selected by the spike. Assume the size of the vocabulary is V . A Dirichlet distribution over the topic is defined on a V  X  1 -simplex, i.e., where 1 is a V -length vector of 1 s. In an sparseTM, the idea of imposing sparsity is to use Bernoulli variables to restrict the size of the simplex over which the Dirichlet distribution is defined. Let b be a V -length binary vector composed of V Bernoulli variables. Thus b specifies a smaller simplex through the  X  X n X  X  of its elements. The Dirichlet distribution over the restricted simplex is Singer use this type of distributions for language modeling.
 Now we introduce the generative process of the sparseTM. The sparseTM is built on the hierarchical Dirichlet process for text, which we shorthand HDP-LDA. 1 In the Bayesian nonparametric setting the number of topics is not specified in advance or found by model comparison. Rather, it is inferred through posterior inference. The sparseTM assumes the following generative process: Figure 1 illustrates the sparseTM as a graphical model. The distinguishing feature of the sparseTM is step 1, which generates the latent topics in such a way that decouples sparsity and smoothness. For each topic k there is a corresponding Beta random variable  X  k and a set of Bernoulli variables b kv s, one for each term in the vocabulary. Define the sparsity of the topic as This is the proportion of zeros in its bank of Bernoulli random variables. Conditioned on the Bernoulli parameter  X  k , the expectation of the sparsity is topic k is represented by those terms with non-zero b kv s, and the smoothing is only enforced over these terms through hyperparameter  X  . Sparsity, which is determined by the pattern of ones in b k , is controlled by the Bernoulli parameter. Smoothing and sparsity are decoupled.
 We next compute the marginal distribution of  X  k , after integrating out Bernoullis b k and their parameter  X  k : components are defined over simplices of different dimensions. In total, there are 2 V components; each configuration of Bernoulli variables b k specifies one particular component. In posterior inference we will need to sample from this distribution. Sampling from such a mixture is difficult in general, due to the combinatorial sum. In the supplement, we present an efficient procedure to overcome this issue. This is the central computational challenge for the sparseTM.
 Step 2 and 3 mimic the generative process of HDP-LDA [ 1 ]. The stick lengths  X  come from a Griffiths, Engen, and McCloskey (GEM) distribution [ 8 ], which is drawn using the stick-breaking construction [9], Note that P k  X  k = 1 almost surely. The stick lengths are used as a base measure in the Dirichlet process prior on the per-document topic proportions,  X  d  X  DP (  X ,  X  ) . Finally, the generative process for the topic assignments z and observed words w is straightforward. Since the posterior inference is intractable in sparseTMs, we turn to a collapsed Gibbs sampling butions  X  and term selectors b analytically. The latent variables needed by the sampling algorithm are stick lengths  X  , Bernoulli parameter  X  and topic assignment z . We fix the hyperparameter s equal to 1 .
 To sample  X  and topic assignments z , we use the direct-assignment method, which is based on an analogy to the Chinese restaurant franchise (CRF) [ 1 ]. To apply direct assignment sampling, an auxiliary table count random variable m is introduced. In the CRF setting, we use the following notation. The number of customers in restaurant d (document) eating dish k (topic) is denoted n dk , and n d  X  denotes the number of customers in restaurant d . The number of tables in restaurant d serving dish k is denoted m dk , m d  X  denotes the number of tables in restaurant d , m  X  k denotes the number of tables serving dish k , and m  X  X  denotes the total number of tables occupied. (Marginal counts are represented with dots.) Let K be the current number of topics. The function n ( v ) k denotes the number terms have been assigned to topic k . Index u is used to indicate the new topic in the sampling process. Note that direct assignment sampling of  X  and z is conditioned on  X  .
 The crux for sampling stick lengths  X  and topic assignments z (conditioned on  X  ) is to compute the conditional density of w di under the topic component k given all data items except w di as, The derivation of equations for computing this conditional density is detailed in the supplement. 2 We summarize our findings as follows. Let V , { 1 ,...,V } be the set of vocabulary terms, B k , { v : n k,  X  di &gt; 0 ,v  X  V} be the set of terms that have word assignments in topic k after excluding w di and | B k | be its cardinality. Let X  X  assume that B k is not an empty set. 3 We have the following, where  X  conditional probability in Equation 6 which depends on the selector variables and selector proportions. We now describe how we sample stick lengths  X  and topic assignments z . This is similar to the sampling procedure for HDP-LDA [1].
 Sampling stick lengths  X  . Although  X  is an infinite-length vector, the number of topics K is finite at every point in the sampling process. Sampling  X  can be replaced by sampling  X  , [  X  1 ,..., X  K , X  u ] [1]. That is, Sampling topic assignments z . This is similar to the sampling approach for HDP-LDA [ 1 ] as well. Using the conditional density f defined Equation 5 and 6, we have If a new topic k new is sampled, then sample  X   X  Beta (1 , X  ) , and let  X  k new =  X   X  u and  X  u new = (1  X   X  )  X  u . Sampling Bernoulli parameter  X  . To sample  X  k , we use b k as an auxiliary variable. Note that b k was integrated out earlier. Recall B k is the set of terms that have word assignments in topic k . (This time, we don X  X  need to exclude certain words since we are sampling  X  .) Let A k = { v : b kv = 1 ,v  X  V} be the set of the indices of b k that are  X  X n X , the joint conditional distribution of  X  k and b k is super set of B k , there must be a term, say v in B k but not in A k , causing  X  kv = 0 , a.s., and then we iteratively sample b k conditioned on  X  k and  X  k conditioned on b k to ultimately obtain a sample from  X  k .
 Others. Sampling the table counts m is exactly the same as for the HDP [ 1 ], so we omit the details here. In addition, we can sample the hyper-parameters  X  ,  X  and  X  . For the concentration parameters  X  and  X  in both HDP-LDA and sparseTMs, we use previously developed approaches for Gamma priors [1, 10]. For the Dirichlet hyper-parameter  X  , we use Metropolis-Hastings.
 Finally, with any single sample we can estimate topic distributions  X  from the value topic assignments z and term selector b by where we can smooth only those terms that are chosen to be in the topics. Note that we can obtain the samples of b when sampling the Bernoulli parameter  X  . In this section, we studied the performance of the sparseTM on four datasets and demonstrated how sparseTM decouples the smoothness and sparsity in the HDP. 5 We placed Gamma (1 , 1) priors over the hyper-parameters  X  and  X  . The sparsity proportion prior was a uniform Beta, i.e., r = s = 1 . For hyper-parameter  X  , we use Metropolis-Hastings sampling method using symmetric Gaussian proposal with variance 1.0. A disadvantage of sparseTM is that its running speed is about 4-5 times slower than the HDP-LDA. 4.1 Datasets The four datasets we use in the experiments are: For all data, stop words and words occurring fewer than 10 times were removed. 4.2 Performance evaluation and model examinations We studied the predictive performance of the sparseTM compared to HDP-LDA. On the training documents our Gibbs sampler uses the first 2000 steps as burn-in, and we record the following 100 samples as samples from the posterior. Conditioned on these samples, we run the Gibbs sampler for test documents to estimate the predictive quantities of interest. We use 5-fold cross validation. We study two predictive quantities. First, we examine overall predictive power with the predictive The predictive perplexity is Lower perplexity is better.
 Second, we compute model complexity . Nonparametric Bayesian methods are often used to sidestep model selection and integrate over all instances (and all complexities) of a model at hand (e.g., the number of clusters). The model, though hidden and random, still lurks in the background. Here we study its posterior distribution with the desideratum that between two equally good predictive distributions, a simpler model X  X r a posterior peaked at a simpler model X  X s preferred.
 To capture model complexity we first define the complexity of topic. Recall that each Gibbs sample contains a topic assignment z for every observed word in the corpus (see Equation 9). The topic complexity is the number of unique terms that have at least one word assigned to the topic. This can be expressed as a sum of indicators, where recall that z d,n is the topic assignment for the n th word in document d . Note a topic with no words assigned to it has complexity zero. For a particular Gibbs sample, the model complexity is the sum of the topic complexities and the number of topics. Loosely, this is the number of free parameters in the  X  X odel X  that the nonparametric Bayesian method has selected, which is We performed posterior inference with the sparseTM and HDP-LDA, computing predictive perplexity and average model complexity with 5-fold cross validation. Figure 2 illustrates the results. Perplexity versus Complexity. Figure 2 (first row) shows the model complexity versus predictive perplexity for each fold: Red circles represent sparseTM, blue squares represent HDP-LDA, and the dashed line connecting a red circle and blue square indicates the that the two are from the same fold. These results shows that the sparseTM achieves better perplexity than HDP-LDA, and at simpler models. (To see this, notice that all the connecting lines going from HDP-LDA to sparseTM point down and to the left.) Figure 2: Experimental results for sparseTM (shortened as STM in this figure) and HDP-LDA on four datasets. First row . The scatter plots of model complexity versus predictive perplexity for 5-fold cross validation: Red circles represent the results from sparseTM, blue squares represent the results from HDP-LDA and the dashed lines connect results from the same fold. Second row . Box plots of the hyperparameter  X  values. Third row . Box plots of the number of topics. Fourth row . Box plots of the number of terms per topic.
 Hyperparameter  X  , number of topics and number of terms per topic. Figure 2 (from the second to fourth rows) shows the Dirichlet parameter  X  and posterior number of topics for HDP-LDA and sparseTM. HDP-LDA tends to have a very small  X  in order to attain a reasonable number of topics, but this leads to less smooth distributions. In contrast, sparseTM allows a larger  X  and selects more smoothing, even with a smaller number of topics. The numbers of terms per topic for two models don X  X  have a consistent trend, but they don X  X  differ too much either.
 Example topics. For the NIPS data set, we provide some example topics (with top 15 terms) discovered by HDP-LDA and sparseTM in Table 1. Accidentally, we found that HDP-LDA seems to produce more noisy topics, such as, those shown in Table 2. These results illuminate the issue with a single parameter controlling both sparsity and smoothing. In the Gibbs sampler, if the HDP-LDA posterior requires more topics to explain the data, it will reduce the value of  X  to accommodate for the increased (necessary) sparseness. This smaller  X  , however, leads to less smooth topics that are less robust to  X  X oise X , i.e., infrequent words that might populate a topic. The process is circular: To explain the noisy words, the Gibbs sampler might invoke new topics still, thereby further reducing the hyperparameter. As a result of this interplay, HDP-LDA settles on more topics and a smaller  X  . Ultimately, the fit to held out data suffers. For the sparseTM, however, more topics can be used to explain the data by using the sparsity control gained from the  X  X pike X  component of the prior. The hyperparameter  X  is controlled separately. Thus the smoothing effect is retained, and held out performance is better.
 Acknowledgements. We thank anonymous reviewers for insightful suggestions. David M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, and grants from Google and Microsoft.

