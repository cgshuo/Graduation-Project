 Traditional probabilistic relevance frameworks for informa-tional retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the suc-cessful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region : a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collec-tions, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effective-ness.
 H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Performance, Experimentation, Ranking Query processing, ranking, query segmentation, BM25
Modern information retrieval ranking functions, like the ones used in today X  X  search engines, employ a large num-ber of features derived from different sources of evidence: matches of query terms in documents, document query-in-dependent quality measures, and click-through information among others. Prevalent among those signals, and central to most standard retrieval approaches such as BM25 and lan-guage models, is the term frequency (tf) information (i.e., the number of times a term appears in a document). Ap-proaches derived from the probabilistic retrieval model are implemented as a summation of X  X eights X  X f the query terms that appear in the document, where the weight is essentially a normalized version of term frequency.

Traditional probabilistic relevance frameworks for infor-mational retrieval [30] refrain from taking positional infor-mation into account, both because of the hurdles of develop-ing a sound model while avoiding an explosion in the number of parameters and because positional information has been shown (somehow surprisingly) to have little effect on aver-age [34]. Recently, though, it has been proved that consider-ing sequences of terms that form query concepts is beneficial for retrieval [6]. Those extensions build up on the Markov Random Field retrieval model (MRF) and use a linear com-bination of different concepts and query-term scores in or-der to derive a final score for a document. Furthermore, the well-known BM25F extension of the successful Okapi rank-ing function (the latter of which we are aiming on building upon) can be seen as an embryonic attempt in the same di-rection: the basic idea there is that each document is made of regions (fields) and some fields may provide stronger evi-dence of relevance than others.

In this paper, we proceed along the same line, defining the notion of virtual region : a virtual region is a part of the document that, like a BM25F-field, can provide a different evidence of relevance of the document (the amount of evi-dence is, like in BM25F, expressed by a weight). Differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable operators to the query: such operators may (and typically will) use positional informa-tion.

The techniques we propose can be seen as a two-stage ranking: in the first stage, a number of operators are ap-plied to the incoming query to individuate virtual regions within the document; in the second stage, the regions are ranked much in the same way as with BM25F, using the weights attached to each operator. The idea is that there are  X  X tricter X  operators, that give a stronger evidence (e.g.,  X  X  want that at least three of the query terms appear in a span of at most ten words X ), but are less likely to appear in document, and other that are  X  X eaker X  (e.g.,  X  X  want at least one of the query terms appearing somewhere X , that is the standard bag-of-words requirement) and contribute to re call.

The operator-based technique that we propose fits nicely in the usual eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but we believe it has a more gen-eral appeal.

Abstracting from the positional aspect, we can think of our method as an attempt to understand more deeply the user X  X  intent underlying a query [2], following the increas-ing interest in extracting features and learning about the query itself. For example, all major search engines are able to detect entities in queries in order to shortcut the user to an appropriate vertical (e.g., for e-commerce or news), to trigger different visualization schemes or simply to help ranking better the results that are being produced. The basic idea is that of being able to pre-process a plain set of query terms that a user submitted to build a model of the query (possibly with the help of contextual informa-tion, e.g., about the query session and/or the user X  X  pro-file). This model might simply contain spelling corrections, term annotations or may exhibit more sophisticated expan-sions, obtained through gazetteers, synonym dictionaries or query-logs, just to name a few.

In general, we observe that information retrieval is moving from a document-centric to a query/user-centric approach, and modern search engines are investing large amounts of re-search in building better, more comprehensive query models. The question that we address in this paper is whether it is possible to extend the classical probabilistic formulation so as to accommodate in a natural way these extended query models for enhancing ranking, in both Web search and more classical TREC-like retrieval settings.

Summing up, this paper aims at proposing an extension of the probabilistic retrieval framework [30] that accounts for the information coming from queries and documents. Our approach can be seen as a principled way of integrating query-document features into a BM25-based model [32], by extending the event space using a number of operators that derive from a query model [7]. For instance, both BM25 and BM25F [33] could be regarded as a special case of the method presented here. The framework operates in a general manner by means of a set of operators that are materialized using query-derived information; each operator determines a (possibly empty) virtual region within the document, that is treated as a (weighted) field; query-term frequencies in each virtual region are used to compute the final score of the document. We shall frame our technique as an extension of BM25(F) and describe how it can be implemented efficiently. Finally, we provide experimental evaluation of our approach showing that the usage of operators outperforms state-of-the-art ranking that use just matching of query terms and is especially helpful for difficult queries. The software im-plementing our technique and used for the experiments will be made available at http://mg4j.dsi.unimi.it/ .
Some lines of research attempt at addressing the issue of adding semantic information to documents and queries [22]. The model that we present (which could indeed encode [22] as a special case) can be seen as a template for grounding different graphical model instances, in the spirit of Markov Logic Networks [18, 29], even though in this paper we make no attempt to generalize the learning procedure of the prob-abilities involved in the model, and the inference we perform is restricted to one particular formulation and combination. However, the very structure of our method allows one to extend it to different combinations and aggregation func-tionals over probability distributions. Robertson et al. [33] introduced BM25F, a variation of BM25 that is able to deal with matches of query terms in different fields of the docu-ment, boosting them differently. Our framework stems from the same fundamental notions of BM25F and BM25 (term eliteness, re-weighting of term matches) and it is able to extend/accommodate both.

There are several recent papers that deal with spans of terms in ranking. Svore et al. [35] show that introducing spans of terms as a further feature for machine learning to rank model gives improvements over BM25. Other au-thors have dealt with the issue of incorporating these spans of terms into the language model framework, the first one being the Markov Random Field (MRF) model of Metzler and Croft [26], extending the language modeling framework for information retrieval [27, 39] to handle term dependen-cies. Some other approaches [11] compute the aggregated distance of matches and add it to the BM25 score, or define a kernel-like distance [23, 24] that can be successfully used for ranking by plugging it into a language-model divergence between the query and document estimation. One remark-able model close to ours is that of Bendersky et al. [5], who weight different query concepts using a parameterized com-bination of diverse importance features: those concepts can be single query keyword, phrases matching in the document, or matches of keywords that span a window of a certain size. The amount of matching concepts in the document are later integrated into the MRF ranking model. In our case we focus on extending BM25 and not the language modeling framework, the core difference being in the way information is aggregated for each term during ranking.

Besides taking spans into account, it may be beneficial to adopt some additional query segmentation technique, try-ing to grasp which words in a query should appear in shorter spans (or even consecutively), as successfully attempted in [28, 19].
As explained in the introduction, the basic approach of this paper is to extract, from a given query, a number of regions in the document using suitable operators. Alterna-tively, you can think that a given input query is refined in a number of different ways using some refinement operators, that may (and typically will) use positional information; vir-tual regions are then the regions matching each of the refined queries.

As a concrete example, consider the following two opera-tors: One could think of them as query-refinement operators; for e xample, using the query language syntax of MG4J [9],  X  1 applied to the query young nice girl gives rise to the query: When a document is considered against this query, all the areas where at least two of the three queried word appear either consecutively or one word apart are selected. This will determine a (possibly empty) sub-document, in which we can count the frequency of each of the three query words.
Operators  X  2 , instead, applied to the same query will pro-duce and would just extract the occurrences of either of the three query words from the document.

Clearly, a large frequency in the virtual region determined by  X  1 would be far more predictive of relevance than that de-termined by  X  2 (the latter would amount to actually count-ing the usual term frequency in the whole document).
To gain some experimental support for this intuition we performed the following experiment: we considered the top-ics 701-850 from the TREC GOV2 collection, and built que-ries using the words in their title. To each query, we applied three operators: the plain or operator (corresponding to the usual bag-of-words interpretation of the query), the 2-and operator (satisfied only by documents that contain at least any two of the query terms) and the 2-gram operator (sat-isfied by documents that contain two terms consecutively).
Then, for each matching document, we computed the fre-quency of the query terms within the virtual region and we determined if the document was relevant or not; the frac-tion of relevant documents is plotted against term frequency. Figure 1 shows the results of the experiment, and provides two fundamental evidences: first of all, stricter operators (2-grams, for instance) provide for the same term frequency a larger probability of relevance, as expected; secondly, the behavior shows in all cases the well-known phenomenon of saturation  X  as the frequency increases, relevance also in-creases but at a slower and slower pace, giving rise to the typical sigmoid-shaped function.

Our approach is blind with respect to the operators be-ing considered, which is part of its generality. In our ex-periments, among other operators, we employ a supervised phrase and entity detection algorithm and feed the differ-ent query chunks through the model in order to produce a document score.

Most previous works have addressed the combination of scores in a linear fashion: Bendersky et al. [5, 6] and Li et al. [20] focused on extensions of the language modeling framework. Linear combinations of features are able to bring increased performance; however, when taking into account evidence coming from the same source of information it is beneficial to understand the distributional properties of the signal the model has to deal with. In these cases, the in-formation employed for ranking is always taken from the number of times one term or a sequence of terms matches a document. In contrast, if one was to incorporate other features, like query-independent document quality [17], or click-based information [1], a linear combination might be
I n MG4J, the  X  operator restricts matches to a span of words of a given maximum length. Figure 1: Term frequency versus probability of be-i ng relevant, depending on the operator applied to the query. Curves are obtained fitting the points through a spline with three degrees of freedom. good enough, as long as the features integrated into the model are not correlated. Note however that (somehow sur-prisingly) even link-based features such as PageRank [10] and BM25 [32] turn out to be non-independent, given the presence of query terms in the anchor text of Web pages [17].
Not assuming feature independence is especially impor-tant when devising more complex ranking models that em-body a large number of features, such as those employed in learned ranking functions [21]. In case of machine learn-ing frameworks, this dependence is somehow captured by the complexity of learned functions, which in general might incorporate an over-engineering of features.
Traditional probabilistic models, like BM25, assume that the relevance of a document to a query can be determined by aggregating individual contributions of the query terms. That is, given the binary random variable R representing relevance, and the vectors of random variables representing the document D and query Q , we want to rank documents according to their increasing odds-probability [30]. Here Q is (or can be thought of as) a set of terms, while D t is a multi-state variable that encodes the features about the oc-currence of term t in D (term frequency, position, etc.); we assume that those features contain a natural zero, corre-sponding to the absence of t and represented by 0 . We let d and q denote two actual realizations of D and Q , and r ( r ) represent the event R = 1 ( R = 0, respectively), i.e., the document being relevant (irrelevant, respectively). Then, within the probabilistic framework, documents are ranked Figure 2: BM25 plate diagram representing the as-s umptions over variable independence according to p ( r | Q = q, D = d ), or equivalently where w t,d is the weight assigned for term t in document d , and you can think of d t as being the term frequency of t in d , later on denoted by tf t,d .

In the derivation above, we assumed the terms to be con-ditionally independent , that is, p ( D t = x, D t  X  = y | r, Q ) = p ( D t = d t | r, Q ) p ( D t  X  | r, Q ) and p ( D t = x, D and t  X  ; this is a weaker assumption than term independence, in that we only require terms to be independent for each fixed query and relevance; this assumption is fundamental in practice, because it leads to tractable models, but it also has a deeper justification: as [16] proved, conditional term independence can be obtained when, for any given query, terms are statistically correlated but the correlation is the same on relevant and on non-relevant documents. Empir-ical evidence on retrieval performances of BM25 suggests that this indeed is often the case. For example, it is true that query terms New and York are correlated in relevant documents for the query New York pizza , but they are also correlated in the whole collection.

Further, the derivation imposes a vague prior assumption over terms not appearing in the query ( p ( D t = x | r ) = p ( D t = x | r ) if t /  X  Q ). This can be weakened in the case of query expansion by explicitly linking unseen query terms to relevance.

The final arithmetic trick in the above derivation, known as removing the zeroes , is used to eliminate from the final score calculation terms that are not present in the document.
The determination of term weights in BM25 is based on the assumption that there is an Elite random variable, which can be cast as a simple topical model that perturbs the dis-tribution of words over the text. That is [30], the author is assumed first to choose which topics to cover, i.e., which are the elite terms and which are not. Furthermore, it is assumed that frequencies of terms on both the elite and the non-elite set follow a Poisson distribution, with two different means; in other words, for a given term t and for e  X  X  0 , 1 } (denoting whether we are considering the term to be in the elite or not), there is a random variable E t,e that expresses the distribution of the frequencies of the (elite or non-elite) term t in a document, and E t,e  X  Poisson(  X  t,e ); clearly, we expect  X  t, 1 &gt;  X  t, 0 (i.e., a single term will appear more frequently if it is elite than if it is not). Plugging this as-sumption in the general formula derived above determines a monotonic weighting function with an horizontal asymp-tote that may be interpreted as a form of saturation: the probability of relevance increases with term frequency, but the amount of increase is ultimately close zero when the fre-quency becomes large. In practice, this is well approximated by where w idf t is the inverse document frequency for term t (that determines the asymptotic behavior when the frequency goes to infinity), k 1 is a parameter and  X  tf t,d is a normalized term frequency with respect to the document length, i.e., where | d | is the length of document d , avdl the average length of documents in the collection, and b  X  [0 , 1] is a tunable parameter. Putting things together, the weight derived from the 2-Poisson elite assumption w BM25 t,d is U BM25 (  X  tf
In the following derivation we start again from the above-mentioned estimation of Here, the only features that we need to observe are term frequencies; this is a quite mild assumption and stems from the idea that documents are a single body of text with no structure whatsoever. Some earlier refinements of the prob-abilistic model, however, already introduced the idea that documents may have some structure. In BM25F [33], for ex-ample, a notion of region was introduced: each document is made up of regions, or fields, (e.g., title, body, footnotes etc.) and it is possible to observe term frequencies separately in each region. This extension accounts for the fact that some fields may be more predictive for relevance than others (for example, title will be more predictive than footnotes), and fits well in the eliteness model. The idea is that eliteness of terms is decided beforehand, for every given document, and it is the same across fields; term-frequency, instead, will be again modeled as in standard BM25, although it will be in-fluenced by the length of each field X  X f course, shorter fields (such as title) are expected to contain more elite terms than longer ones.
As said, the present paper extends this idea by defining w hat we call virtual regions . Ideally, suppose that you have some way (as yet unspecified) to single out some parts of the document that you know will provide high-quality informa-tion about relevance; then, you may think of that area as a single virtual region, and apply to that region the evaluation described for BM25F. Those virtual regions are actually ob-tained by the query itself, through a process that we shall now describe.

An operator is a function that associates, to a given query and a given document, another document 2 called a  X  X irtual region X . We are given a set of such operators, each endowed with a weight, {h  X  j , w j i} j  X  X  : for each query q and doc-ument d , we individuate the virtual regions  X  j ( d, q ) (the region of document d matching query q according to opera-tor  X  j ), and for each of them we count the term frequency of each term t  X  q in that virtual region; such a frequency is denoted by tf q,j t,d (the term frequency of term t in the virtual region of document d matching query q according to opera-tor  X  j ); as it is customary in the RSJ model [31], we shall omit the query q , and just write tf j t,d .

To capture this idea, as with BM25F, we proxy the depen-dence of the eliteness of the occurrences using a set {  X  of Bernoulli random variables, which reflect the probability of occurrences in each region generated from a particular operator. De Finetti [8] proved that any set of exchangeable random variables has a representation as a mixture distribu-tion, in general an infinite mixture. Therefore we represent the operators as p ( tf j t,d = x | r ) = X
X three factors in the summation is zero if  X  = 0, otherwise it is  X  x t,e e  X   X t,e /x ! (for the Poisson-mixture assumption). The second factor for  X  = 1 is just  X  j , the parameter that governs the j -th Bernoulli  X  j . The last factor is the probability that the document is elite for the term if it is relevant.
Let us write  X  (  X  ) for  X  t, 1 (  X  t, 0 , respectively) and let p ( q ) be the probability that a document is elite for the term, given that it is relevant (irrelevant, respectively).
So and similarly Now following Robertson [30], we can divide both probabil-ities by  X  x e  X   X  /x ! getting Observe that, since  X  &gt;  X  , the latter tends to p/q as x  X  X  X  , as in [30].
A s explained in the next section, technically the operator produces a set of queries that is then matched against the document to obtain a virtual region, but the difference is immaterial for the moment. Figure 3: Plate diagram with the extended event s pace.

The treatment for the case x = 0 is slightly more involved, because and similarly for the case of irrelevant documents: the last summand depends on the fact that a term can have zero occurrences in a region simply because of  X  j ; so or equivalently T his term behaves like (1  X  q ) / (1  X  p ), under the usual as-sumption [30] that  X   X   X  is small, and assuming further that  X  j is sufficiently close to 1.
We observe that our underlying retrieval system has a rich query language, the set of whose queries is denoted by Q . Every document d can be matched against the query q  X  Q producing a sub-document M ( d, q ) (i.e., a subsequence of the words the document d is made of). 3 This function is naturally extended to sets of queries, by letting M ( d, A ) be the union of all sub-documents M ( d, q ) for q  X  A .
Conversely, a raw query is just a sequence of terms r = h t , . . . , t u i : this is what we suppose that the user inputs to the system; the set of all raw queries is denoted by R .
An operator is a function  X  : R  X  2 Q mapping a raw query to a set of queries. Here are some simple examples of operators that we will be using in our experiments:
T he exact semantics of the match depends on the query language and on the retrieval system used and will be not described further, but see [9, 13] for two examples. Let now {h  X  j , w j i} j  X  X  be a set of operators and weights; for a fixed raw query r = h t 1 , . . . , t u i , let tf j of occurrences of term t in M ( d,  X  j ( r )). The average term frequency of term t in document d is then defined to be 5 The score of document d for query q is then computed as where the latter is the standard term idf.

Note that using a single bag-of-words operator reduces our scoring formula to the usual BM25 score. Conversely, suppose your collection has G fields, and let  X  1 , . . . ,  X  operators that work like a standard bag-of-words, but where  X  i tries to find matches only in field i . So, for example, M ( d,  X  title ( t )) would return the sub-document of the title made only by the occurrences of term t . Then, an appli-cation of the above formula would reduce to the standard BM25F score.
BM25, following the original probabilistic relevance frame-work, adopts a disjunctive semantics, that is, there is no need for a document to contain every query term in order to receive a non-zero score. The key point behind this as-sumption is that we need an external mechanism to decide which eliteness models we want to take into account for each query, and this will simplify the number of estimations and scores we need to compute. After that, it is important to
B oth the p -gram, the phrasal and the segment operators can be endowed with an enlargement factor that allows for some extra word to sneak in X  X lso this point will be fully explained in Section 5.
The length-normalization factor here might actually be dif-ferent for each virtual region, but this solution turns out to be extremely expensive to implement, because the average region length is unknown unless the whole collection is ex-amined. A good approximation can be obtained by using the standard document length as a measure of  X  X erbosity X  of all the virtual regions it contains. decide what is an appropriate shape of the functional esti-mating relevance probability as a function of term scores: in Section 3 we showed empirically that conjunctive and prox-imity operator produce the same shape as in the 2-Poisson eliteness model.

An issue raised by our model, and that we must take into account, is the fact that the very same occurrence of a term within a document will be counted more than once, because virtual regions (differently from BM25F fields) may over-lap. For instance, if we want to score separately matches of stemmed query terms and matches of unstemmed query terms we would be double-scoring some of the occurrences. This remark calls for discounting signals coming from the same source; one way to obtain this result would be to establish some dependence between the operators  X  j : in the example above, we might correct the estimation using a p ( X  st j |  X  ex j , r ) correction factor (here, and in the following, the superscripts st and ex stand for  X  X temmed X  and  X  X xact X , respectively).

In practice, however, we can avoid this estimation by re-calibrating the different weights. We empirically know that both exact and stemmed matches should contribute to the score on which the saturation function is applied, and we want to aggregate those contributions together, using the proper weights. We can then correct the double counting and substitute accordingly in equation (1) as  X  tf
Some observations about possible variants of our model are worth being remarked here:
Albeit today X  X  search engines offer a limited number of op-erators (phrasal, proximity etc.) most users tend to stick at introducing plain queries, typically a sequence of few words (about 3 . 08 on average, according to recent studies [36]). Nonetheless, as observed previously [37], many queries are actually made up by some basic conceptual units, each of them being formed possibly by many words. For exam-ple, the query indian summer victor herbert is clearly formed by two conceptual elements ( X  X ndian summer X  and  X  X ictor herbert X , the former referring to a meteorological phenomenon, the second to a person); breaking such concep-tual units apart, or inverting the order of the words that la-bel them, would produce information loss X  X f course, many documents will contain both the words  X  X ndian X  and  X  X um-mer X  without referring to  X  X ndian summer X ; also  X  X ummer indian X  will also probably appear in some document, for ex-ample in reference to summer indian food, without any rele-vance to the concept sought. In some cases, it may be wise to allow for one spurious word to be inserted within a segment (so that, for example, san jose airport can also match the sequence  X  X an Jose international airport X , or george bush match  X  X eorge W. Bush X ).

Among our goals, we would like the model to accommo-date for query term-dependence, or concept-detection, at the query level. For example, if there is evidence that the query san jose aiport consists of two concepts, one re-ferring to a city and the other one denoting a place or an action, we would like the model to be able to weight dif-ferently documents that only match one concept from those that match both, taking also into account the distance be-tween the terms making up the concepts. To this end, one of our operators will employ query segmentation, an emerging NLP task that aims at identifying sub-sequences of strings that refer to a single unit or concept [7, 37], as described above. There have been limited attempts to integrate seg-ments into ranking [5, 6, 20] in the context of the language modeling framework [39], but to the best of our knowledge this is the first time that a similar attempt is being applied to BM25-like techniques.

In the experimental section, we use both a segmentation operator based on generative language models and Wikipedia (as described in [37]) and a simple operator extracting p -grams of consecutive terms: the latter is of course less pre-cise (because some p -grams do not correspond to concepts), and is given a lower weight  X  it serves the purpose of identi-fying possible word-sequences that for some reason the seg-mentation algorithm failed to guess. Moreover, for both types of operators we allowed for a variety of amount of spurious words appearing in the segment: this is obtained by introducing different enlargement factors  X  (a multiplica-tive factor determining the maximum allowed ratio between the length of the span found and the length of the segment); an enlargement factor  X  = 1 corresponds to accepting only exact matches, whereas for example allowing for an enlarge-ment  X  = 2 corresponds to accepting at most one extra word for every single word in the segments (e.g., if the segment is made up of two words, it is still acceptable if we find them two words apart). Different enlargement factors are used in the experiments, of course assigning larger weights to smaller enlargements.
 Collection Size Documents Topics TERA04 436G 25M TREC 701-750 T ERA05 436G 25M TREC 751-800 T ERA06 436G 25M TREC 801-850 WEB 100G 10M 1000 W EB-Phrasal 100G 10M 400
We tested the usefulness and accuracy of the operators described in Section 4.1 and 5 in a series of experiments. We posit that query segments will only affect to a limited number of queries in the data-set; however the working ques-tion is whether combining them has some positive retrieval effect or not. Retrieval performance is optimized iteratively using MAP as a target metric. We sweep one parameter at a time over the allowed parameter range, holding every other parameter fixed, in a similar fashion to the method of Metzler and Croft [26]. Each operator introduces two pa-rameters: a weight w j controlling the relative importance of the operator, and a factor b j determining the impact of term-frequency normalization with respect to the corresponding operator. Different enlargement factors  X  (for segments, p -grams and phrasal operators) are reported in separate rows of the table, as well as different values for the number p of consecutive terms that are considered while building p -grams.

We report on MAP and P@10 and check for statistical significance using a one-sided t-test with p &lt; 0 . 05. The op-erators are trained on two different Web collections: GOV2, TREC X  X  Terabyte track collection [14, 15, 12], and a sub-sample of the Web from 2011. The collections are described in Table 1. Training and testing is performed on different topic sets; for TREC topics, we train and test on differ-ent years (train on TERA04, test on TERA05/06; train on TERA05, test on TERA04), and for the Web collection we perform 10-fold cross validation across the whole topic set. For the TREC collection, we used the topic title as raw query.

Table 2 presents the results of applying one single oper-ator, combined with BM25 (that is, with the bag-of-words operator). Experiments show that all the operators are able to improve the performance on their own to a reasonable ex-tent. The impact of single operators, specially the segments, is limited. However, it is worth noting that the methods us-ing a single operator that restrict the semantics of the match-ing (like AND or segments) perform comparably to the best values reported at TREC for early precision (P@20) [14, 15, 12]. Table 3 presents the results of combining segment and p -gram operators with BM25 and BM25F. The different op-erators have been chosen using various enlargement factors; we further enriched the segment combination with a 2-gram respect to BM25, MAP only). operator. The purpose of this experiment is to determine whether the sigmoid-like term-frequency normalizing func-tion is able to accommodate for different features which stem from the same source of evidence (matches of the query term in the document). Results are significantly better than the baseline and outperform state-of-the-art ranking functions that just use matching of query terms (note we are not adding query-independent evidence, like link information, click-through data, etc.). For instance, the best MAP value for TREC 2004 at TERA04 [14] was 0.2844, the sequential dependence model of the Markov random field for IR peaks at MAP 0.2832 [26] and the two-stage segmentation model of Bendersky et al. had an average MAP of 0.2711 over the 150 topics [5].

In order to explore further the usefulness of query segmen-tation and p -grams for difficult queries, we selected a sub-sample of 1000 queries taken from Yahoo! Search; the data corpus was a 100GB sub-sample of the Web. The queries had been evaluated by a trained editorial team, with about 130 judged documents per query on average: relevance was assigned on a 4-level scale, from Bad to Excellent ; given that we had graded relevance available, we report on NDCG (gain values at the relevance level, from 0 to 4) and MAP. The average query length was 3.14. We report the perfor-mance of BM25 and BM25F as baselines. In addition, we selected a sub-sample of 400 queries where the user herself had employed query segments (i.e., phrasal queries), and re-moved the segmentation information. This experiment was aimed at establishing if a more elaborate query interpreta-tion mechanisms is able to be of help in these cases.
Table 4 shows that segmentation and p -grams, when mixed with the operator combination are able to improve the per-formance of a large number of Web queries. When looking at the differences between the regular and phrasal queries, we observe that gains, even if consistent over the two differ-ent sets, are slightly higher in the second group (  X  12 . 3% vs.  X  7 . 1% in MAP for p -grams vs. BM25F). This fact in-dicates that the method is helpful for queries that contain difficult concepts, as they have been expressed by users man-ually, whereas maintaining the performance of queries in which identifying concepts is not so critical and standard bag-of-word approaches perform as well.

It is useful to look into the reasons behind the increased retrieval performance we observed in our experiments; with this goal, we considered separately the mean average pre-cision on each of the TREC topics 701-850 and examined the ten queries that produced the largest difference in pre-cision between our system and the BM25 baseline. Table 5 shows the queries that obtained the greatest benefit from the use of segmentation and/or p -gram operators: in the right-most column of the table, we identified the segments (or the p -grams) that most contributed to the increased precision, allowing to retrieve relevant documents that BM25 missed (because they were ranked too low).
In this paper we proposed a way to extend the proba-bilistic relevance framework with a notion of virtual region based on the use of operators applied to the query. Our experiments (both on standard collections, such as TREC, and on other Web-like repertoires) show that the use of vir-tual regions is especially beneficial for hard queries where positional information is actually precious. The method has room for improvements and further study should be under-taken to understand which operators are more useful and under which circumstances. Even if we explored a reason-able number of combinations, we have not made any sys-tematic attempt to develop a method to select the individ-ual best operators; we just limited ourselves to handpick a few X  X t was out of the scope of this paper to analyze auto-mated operator selection. In contrast, a machine learning approach [3] would derive features for as many operators as possible and try to combine them optimizing a loss function of MAP or NDCG [38]. One might even envisage the adop-tion of a query-classification tool to decide which operators should be used, based on the presumed nature of the query. In any case, our experiments show the practical usefulness of the non-linear operator score combination for retrieval [33]. As a final remark, it is interesting to observe that most of the studied operators (actually, all of them except for seg-ments) do not employ any source of external information, and still produce a significant performance improvement. [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] R. Baeza-Yates. Query intent prediction and  X  = { 1 , 2 , 3 } .
 { 1 , 2 , 3 } , and for segment is  X  = { 1 , 2 , 3 } . [3] B. Bai, J. Weston, D. Grangier, R. Collobert, [4] A. Banerjee. An analysis of logistic models: [5] M. Bendersky, B. Croft, and D. a. Smith. Two-stage [6] M. Bendersky, D. Metzler, and W. Croft. Learning [7] S. Bergsma and Q. Wang. Learning noun phrase query [8] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [9] P. Boldi and S. Vigna. MG4J at TREC 2005. In E. M. [10] S. Brin and L. Page. The anatomy of a large-scale [11] S. B  X  uttcher, C. L. A. Clarke, and B. Lushman. Term [12] S. B  X  uttcher, C. L. A. Clarke, and I. Soboroff. The [13] C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski. [14] C. L. A. Clarke, N. Craswell, and I. Soboroff. [15] C. L. A. Clarke, F. Scholer, and I. Soboroff. The [16] S. Cooper. Some Inconsistencies and Misnomers [17] N. Craswell, S. Robertson, H. Zaragoza, and [18] P. Domingos, S. Kok, H. Poon, M. Richardson, and [19] M. Hagen, M. Potthast, B. Stein, and C. Braeutigam. [20] Y. Li, B.-j. P. Hsu, C. Zhai, and K. Wang.
 [21] T.-Y. Liu. Learning to Rank for Information Retrieval. [22] Y. Lu, F. Peng, G. Mishne, X. Wei, and B. Dumoulin. [23] Y. Lv and C. Zhai. Positional language models for [24] Y. Lv and C. Zhai. Positional relevance model for [25] Y. Lv and C. Zhai. A log-logistic model-based [26] D. Metzler and B. Croft. A Markov random field [27] J. Ponte and W. B. Croft. A language modeling [28] L. Ramshaw. Text chunking using [29] M. Richardson and P. Domingos. Markov logic [30] S. Robertson. The probability ranking principle in IR. [31] S. Robertson. On event spaces and probabilistic [32] S. Robertson and S. Walker. Some simple effective [33] S. Robertson, H. Zaragoza, and M. Taylor. Simple [34] S. E. Robertson and H. Zaragoza. The probabilistic [35] K. Svore, P. Kanani, and N. Khan. How good is a [36] M. Taghavi, A. Patel, N. Schmidt, C. Wills, and [37] B. Tan and F. Peng. Unsupervised query segmentation [38] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [39] C. Zhai and J. Lafferty. A study of smoothing
