 1. Biographical introduction
Although William (Bill) Goffman died several years ago, this article is intended to serve as a timely remem-brance of his career and scholarly contributions, which remain relevant to contemporary progress in informa-tion science. He is remembered today as a leading pioneer in the establishment of mathematical information science. During his career as a researcher, Professor and Dean at Case Western Reserve University, he devel-oped general mathematical models for several areas: epidemiology models to explain and predict the genera-tion, growth and spread of scientific and other ideas; simple but direct and efficient information storage and retrieval approaches and algorithms; measures of information relevance in the retrieval process; the use of
Bradford distributions to explain and prescribe core literature development and rank-ordered retrieval file organization.

He and his co-authors were among the very first critics of the wide spread deployment of Boolean logic, despite its sub-optimality, as the basis for information retrieval X  X  practice that became chronically entrenched and institutionalized, and endures in most of our present-day systems. Goffman and colleagues also extended Bradford distribution probability principles to the realms of literature organization, retrieval effectiveness and efficiency, and core collection development. They helped pioneer the emergent fields of medical informatics, including public health informatics and clinical decision-making by finding remarkable similarities between the literatures describing the etiology and epidemiology of vastly different diseases.
And, they applied the emergent logic of clinical decision-making in medicine (centered on true or false positive or negative diagnoses) to illustrate corresponding similarities in information retrieval processes.
Goffman can thus be remembered in several ways. He was a versatile and creative mathematical informa-tion scientist, an imaginative and resourceful polymath, a logician, a catalyst for the later emergence of bib-liometrics and informetrics, and a unique kind of epidemiologist. In addressing and resolving information science problems, he seldom failed to surprise his colleagues and students. While others would gambit about the periphery of what they thought were authentic information science problems, he would quietly and quickly reframe the problem at hand, push aside irrelevant variables, quickly isolate the critical ones, and derive a solution. He was truly a master of convergent thinking, but would surprise others by diverging from the topic under discussion to bring in seemingly irrelevant but appropriate metaphors.

Bill Goffman was born in Cleveland, Ohio on January 28, 1924. He was the fourth of four children and the second son born to Sam and Mollie Goffman. Bill X  X  father, Sam Goffman, was born in 1889 in Kuschenev,
Russia and migrated to Montreal, Canada and then to Cleveland, Ohio in 1907, where he lived until his death in 1975. Sam spent most of his life in Cleveland, where he worked as a stonemason, contractor and employer of many construction workers. Sam X  X  work in stonemasonry and construction gave him an affinity for num-bers and measurement, and motivated him to study mathematics. He encouraged his children to do likewise.
Bill once remarked that the intricate nature of his father X  X  stone measurement and construction problems pro-vided an intuitive foundation for such topics as algebra, plane and solid geometry, and set theory. This intu-itive foundation was conveyed especially to sons Casper, who pursued a career in theoretical mathematics, and Bill, who became an applied mathematician.
 Bill X  X  mother, Mollie (Stein) Goffman, was born in Ryegrud, Poland, near the border with Russia, in 1895.
In 1907, she migrated to Baltimore, Maryland, a point of entry, and then to Cleveland, where she wedded Sam and lived until her death in 1976. Four children were born to Sam and Mollie Goffman: Casper, Ruth, Sarah, and Bill (the youngest by 11 years).

Bill attended local schools and graduated from Glenview High School in 1943. During World War II, he joined the US Army, went through flight training in Texas, and became a commissioned officer and pilot in the US Army Air Corps. He flew combat missions in the Pacific theater and attained the rank of Major.
He attended the University of Michigan at Ann Arbor and received a B.S. degree in mathematics in 1950 and a Ph.D. degree in mathematics in 1954. During and after his graduate studies, he served as a part-time pilot for Pan American World Airways, flying Lockheed Constellations out of New York X  X  Idlewild Airport (now John F. Kennedy International Airport). He also worked as a mathematical consultant following his graduate studies and continued to reside in Cleveland.

In 1959, during one of his frequent trips to the New York area, Bill had the good fortune of being intro-duced by a research colleague to Patricia Ann Mc Loughlin, a registered nurse, who was working at the Jersey
City Medical Center, New Jersey. They continued their relationship, and Bill and Pat were married on Feb-ruary 7, 1964 in New York City. Pat then joined Bill in Cleveland in 1964.

He also joined the staff of the Center for Documentation and Communications Research (CDCR) in 1959 as a staff mathematician and later a research associate. The CDCR had been established in 1955 as a research arm of the School of Library Science at Western Reserve University (Western Reserve University was feder-ated with Case Institute of Technology in 1967 to become Case Western Reserve University). As a staff math-ematician and associate at CDCR, he joined such notable information retrieval pioneers as James W. Perry,
Allan Kent, Jesse H. Shera, Tefko Saracevic, Jack Belzer, Allan Rees, and Al Goldwyn. Bill immersed himself in mathematical modeling for information retrieval system design, effective file organization, machine searching heuristics and algorithms, and the application of epidemiology concepts to the spread of scientific ideas. Through the years, he also developed good working relations with notable colleagues outside of CDCR, such as A.D. Booth, Eugene Garfield, Vaun A. Newill, Tomas G. Morris, and Ken S. Warren.

The CDCR was an especially exciting place to be in the 1960s, since many seminal information retrieval concepts were framed there, along with a significant amount of the US effort to develop computer-based retrie-val during the Cold War with its military mobilization and the US-Russian space race. CDCR developed the
Rapid Selector, an early machine-based retrieval system. The Center also pioneered new approaches to abstracting, indexing and current feedback systems. And the Center established the Comparative Systems
Laboratory, headed by Tefko Saracevic, for the innovative testing of different retrieval systems and approaches. It was there that Bill Goffman developed a reputation for resolving the toughest and most com-plex information retrieval problems. He served as a sort of  X  X  X upreme Court  X  for handling relatively intracta-ble cases through his ability to restate retrieval problems clearly and precisely through his mathematical postulations, rigorous proofs, and mobilization of multi-disciplinary metaphors.

Bill also served as an exemplary teacher and role model for students. Over the years, he developed and offered such graduate courses as information retrieval theory, mathematics for information scientists, general communications theory, systems analysis, bibliometrics, and seminars for doctoral students. He also super-vised numerous doctoral committees and dissertations. In 1968, he became a full professor in the School of Library Science, where he continued to specialize in communication theory and information retrieval.
He was appointed Dean of the School of Library Science at Case Western Reserve University in 1971 and served in that role until 1977. Despite heavy administrative duties, he continued to be an active researcher, teacher and dissertation supervisor. As Dean, he promoted multidisciplinary problem solving and education.
He was principal investigator for National Institute of Health and National Science Foundation grants. He obtained foundation funding to establish a University Complex Systems Institute, and served as its Director from 1972 to 1975. In 1978, he returned to full-time teaching and research and from 1981 through 1988 served as Director of the Training Program on Computer Applications in Health Sciences at the Case Western Reserve School of Medicine. He also served as a Professor in the Department of Biometry of the School of Medicine from 1980 to 1986.

Bill received a number of honors during his career. He was elected Fellow of the American Association for the Advancement of Science in 1982. He had the honor of serving three times as an invited, resident scholar at the Rockefeller Foundation Bellagio Study and Conference Center, Bellagio, Italy during the 1970s and early 1980s. Bill is listed in Who X  X  Who in the World (4th ed., 1978/1979), Who X  X  Who in America (34th ed., 1976/ 1977), and Who X  X  Who in Science and Engineering (1st ed., 1992/1993). In the 1960s and 1970s, he also held visiting professorships at the University of Montreal, Georgia Institute of Technology, the City University of London, and the University of Denver.
 He retired from active teaching in 1986, and was named Professor Emeritus and Dean Emeritus of the
School of Library and Information Science. In retirement, he continued to do research, part-time teaching and consulting, and pursued his hobbies. Following his father X  X  craftsmanship propensity, he enjoyed refur-bishing his beautiful home in nearby Bratenahl on the shores of Lake Erie, a suburb of Cleveland. Bill had many other hobbies. He enjoyed classical music, such as string quartets X  X articularly those played by the
Budapest String Quartet; he had a huge collection of tapes, records and discs. He was an avid baseball fan, and would buy season tickets to watch the Cleveland Indians. His strong reading interests involved ancient to modern history, numerous classics, and various mystery novels, particularly those by Dick Fran-cis, Rex Stout, Dashiell Hammett, Raymond Chandler, and Agatha Christie. Bill especially liked to travel with Pat on cruise ships, particularly on the Queen Elizabeth. On one trip to Panama with colleague Donald
Booth and wife, they rode out an Atlantic hurricane, which Bill found tremendously exhilarating. This experience prompted more cruise trips. Bill and Pat also enjoyed air travel, including yearly flights on the
Concorde between New York and London. He and Pat took several other trips throughout and around the world before and after his retirement. Bill died of lung cancer at his home on February 29, 2000, a month after his 76th birthday. His wife Pat survived him, along with Casper Goffman, his very close older brother.

Casper Goffman, incidentally, was also a brilliant mathematician; he spent most of his career teaching and conducting pure mathematical research at Purdue University. But Bill X  X  frequent communications with him tended to focus on their strong mutual interests in baseball players and teams, or on various classical music works and artists.

The following sections summarize the key thrusts of Bill Goffman X  X  research and briefly discuss its signif-icance. Coverage is necessarily selective; works cited represent most but not all of his published works. 2. Early contributions in information storage and retrieval
When Goffman started working at the CDCR in 1959, there existed a very general and intense interest in furthering, refining and even perfecting earlier documentation practices for information storage and retrieval, and for automating them. Efforts were typically based on the assumption that methods for the manual orga-nization of files and document representations could and should be applied to automated methods of indexing and retrieval. This tendency was somewhat reminiscent of earlier phenomena whereby new technologies were viewed as extensions of older ones, despite the emergence of a quite different technology. It was at this juncture that skeptical mind of Bill Goffman kicked in by questioning a priori concepts. Goffman and colleagues dem-onstrated mathematically that the well-entrenched reliance on Boolean logic was problematic. They concluded that the use of Boolean operators for manual and automated information retrieval was inherently sub-optimal and inefficient. For example, retrieval of the intersection of subject  X  X  X   X  and  X  X  X   X  risks giving too much irrel-evant information, while retrieving  X  X  X   X  or  X  X  X   X  risks leaving out relevant material ( Verhoeff, Goffman, &amp; Belzer, 1961 ).

Goffman challenged another basic information retrieval mantra X  X he notion that relevance could be used as a measure of information carried by a single document in relation to a given query. He noted that for relevance to qualify for as a measure, one must define relevance in relation to the entire set of docu-ments, rather than to only one document. That is, relevance as a measure must be additive (based on the aggregate of documents in a set) rather than on non-additive mappings to individual documents ( Goffman, 1964b ). Although the information retrieval community largely ignored Goffman X  X  challenge for many years, his paper did anticipate and pave the way for later retrieval practices that incorporated aggregate relevance judgments.

Likewise in 1964, Goffman noted that the manual and automated retrieval processes of the time tended to be remarkably inefficient and cumbersome. Information retrieval typically involved searching through the entirety of a large file of documents or document representations to obtain a relevant set of useful documents.
He proposed that instead we should treat each query as a set of indicators that has a probability distribution and could be mapped onto an entire file of documents stored in a system. Thus, on the basis of trial probes, we can partition out a subset or sector of documents that might be highly relevant to our query and then search intensively within that subset. Such a direct search of high relevance documents could increase searching effi-ciency dramatically ( Goffman, 1964a ). To further efficiency, Goffman, Verhoeff, and Belzer (1964) proposed the use of abstract and powerful meta-languages in information searching to replace the use of numerous, highly specific terms being laboriously used to frame queries.

Goffman also argued against the almost exclusive use of two-valued logic and championed the use of multi-valued logic in information retrieval. Both manual and machine retrieval operations in the 1960s tended to be based on the classical, two-valued Boolean prepositional calculus. Either a document was relevant to a given query (having a value of 1) or it was not relevant (having a value of 0). He demonstrated that retrieval is far too complex to be based on either-or logic and called for the use of infinitely valued logic. He was thus an early proponent of the use of fuzzy logic in information retrieval ( Goffman, 1965a ).

In 1966, Goffman and his Western Reserve University Medical School colleague Vaun A. Newill evaluated retrieval systems through the combined use of effectiveness measures (the system X  X  ability to perform its des-ignated purpose) and efficiency measures (the relative cost of task performance). They further incorporated analogous medical diagnostic test criteria into information retrieval. Medical test sensitivity , a test X  X  ability to detect and identify the actual presence of a disease or disorder, could be applied to the retrieval of highly relevant documents, and these retrieved documents could be regarded as  X  X  X rue positives  X  . Conversely, a med-ical test X  X  tendency to sometimes inaccurately indicate the presence of a disease when it was actually not pres-ent is regarded as a false positive. A retrieval system X  X  identification of non-relevant documents as ones that were relevant could be referred to as  X  X  X alse positive  X  retrieval. In contrast, specificity was defined as the system X  X  ability to identify non-relevant documents (true negatives). Both specificity and sensitivity were deemed to be essential to a system X  X  effectiveness. But Goffman and Newill also noted that some user queries do not represent the users X  actual, underlying information needs. They thus went on to differentiate between the concept of relevance, (the relation between a user query and a document set) and pertinence (the relation between a need and a document set). They then proposed a remarkable, holistic approach to retrieval system testing and evaluation that would incorporate the above notions of effectiveness, efficiency, sensitivity, spec-ificity, relevance, and pertinence into a unified model ( Goffman &amp; Newill, 1966 ).

In 1969, Goffman followed through on his earlier, direct method of information retrieval (isolating poten-tially relevant subsets and searching intensively therein) by proposing a counterpart concept, an indirect method of retrieval. Because searches were based on what was known about a subject, the documents that were retrieved and judged to be relevant were based on what was already known, and could be used to direct further search probes. He proposed exploiting this retrieval practice by treating a given scholarly paper as a query itself and then using the paper X  X  cited references as relevant query  X  X  X nswers  X  . In turn, the citations in these cited references could be further pursued as answers of a second order. When put to the test, this indirect method of information retrieval yielded far better results than the earlier direct method of identifying and probing file subsets ( Goffman, 1969 ). In doing this, Goffman was proposing the formalization of the then rel-atively new method of citation-based retrieval that Eugene Garfield and his associates were developing at the Institute for Scientific Information in Philadelphia.

In conclusion, Goffman X  X  early contributions in information retrieval were based on a healthy skepticism about some of the leading assumptions of the time. He observed the sub-optimal results obtained from the use of Boolean logic, particularly the severe tradeoffs between being too inclusive or too exclusive. He advo-cated the use relevance judgments based on holistic rather than singular analyses of information representa-tions, and the use of highly selective information retrieval probes. He developed sophisticated ways to evaluate information retrieval performance through the use of multiple measures, such as effectiveness, efficiency, sen-sitivity, and specificity. And he was an early proponent of citation-based information retrieval. It is probably fair to say that those who worked with Bill Goffman regarded him as a subtle but highly effective iconoclast.
He would deftly challenge basic information retrieval premises and protocols of the time and quickly replace them with more accurate, holistic and powerful models. 3. Bradfordian distributions and core literature identification
Goffman was intrigued with laws of literature dispersion, particularly Bradford X  X  law, and took great delight in pointing out to students and colleagues its broad implications. Bradford X  X  law states basically that there are predictable regularities in the way that a very few journals in a given topical area contain about a third of the available articles in that area. It requires a significantly larger number of journals to produce the next set of a third of all related articles, and a still much greater number of peripheral journals to produce the remaining third of all available articles on the topic. Bradford expressed these ratios as 1: n : n Goffman and his close colleague, Kenneth Warren, reported in 1969 on their extensive National Library of
Medicine-funded efforts to isolate general laws related to the dispersion of medical literature. It was and con-tinues to be important to sift through a vast output of medical literature to identify essential, core literature related to basic and clinical research topics ( Goffman &amp; Warren, 1969 ). They selected two very unlike litera-tures (on mast cell research and schistosomiasis) to search for consistent relationships that would identify core concentrations of literature in specific areas of health science and medicine. Despite differences in content and size of these two different literatures, they found first that the ratio of journals to articles in both fields was nearly identical; second, the average number of papers by authors in each area did not vary significantly within each 5-year period through eight decades; and third, the paper/author ratio was about equal to the minimal constant of Bradford X  X  law of dispersion. Goffman and Warren thus adapted Bradford X  X  law to identify core medical literatures, a potentially invaluable guide for formulating journal acquisition policies for research groups and libraries. Such core literatures could be identified through citation analysis or actual journal usage data to designate a minimum nucleus, and successively larger groups of journals identified to add secondary and tertiary collections of journals around the core. Each grouping could be optimized to fulfill needs, conserve finances, and adjust to other circumstances.
To further test these findings, Goffman joined with Tomas Morris, Cleveland Medical Association Library director, to conduct a detailed analysis of periodical circulation and to designate a list of medical journals rank-ordered into successively larger zones according to apparent usage. Here too, the most highly circulated journals represented a minimal but essential, user-defined core for that setting at that time period. It required a dramatically larger number of journals to produce the number of generally used articles equal to the number in the minimal core set. They proposed that journals should thus be acquired according to their relative infor-mation usage value, budgetary constraints, and other local factors ( Goffman &amp; Morris, 1970 ).
Goffman and Warren then extended their application of Bradford X  X  law to a broad analysis of the ecology of medical literatures and related Bradfordian principles. They analyzed how knowledge is communicated in ways resembling the epidemic spread of diseases. They debunked the then-current notion of the existence of a literature explosion; while there was indeed an exponential growth of literatures in various medical fields, the ratios of papers to authors and that of high quality papers to the total number of papers (as indicated by cita-tion analysis) remained constant over long periods of time. They developed a case against the professionally popular calls of the time to alter the basic ecology of medical literature to deal with a supposed literature explosion. They argued that altering the overall ecology of medical literature would probably destroy mech-anisms essential to the natural selection of high quality literature along with the destruction of literature qual-ity. Ecology alteration would also interfere with the cyclical output of literature in various areas of investigation. Instead, both literature dissemination services and individual scientists should apply Bradford X  X  law to identify core nuclei of highly cited and otherwise significant literature, and then identify secondary and tertiary cores according to need. These insights could be particularly useful for financially restricted medical research groups and libraries here or abroad ( Warren &amp; Goffman, 1972 ).
 One of Goffman X  X  key applications of Bradford X  X  law is illustrated in a significant, jointly authored article.
The article illustrated how medical libraries and other research organizations in the world X  X  developing coun-tries could afford to acquire essential, core medical literature in the face of rapidly increasing numbers of journals being published and their escalating costs ( Bruer, Goffman, &amp; Warren, 1981 ). In the late 1970s, for example, Latin American medical school libraries each held from fewer than 300 up to about 1200 jour-nals, but the great majority (about 90%) each held fewer than 300 journals. In contrast, major US medical libraries held 2000 X 3000 journal titles each. Goffman and colleagues used Bradford X  X  law and bibliometric analysis to identify minimal core collection lists consisting of 35 general nucleus journals and 92 specialized journals that could be portioned into 31 medical specialty classes. These minimal and specialty cores could be adjusted to local user demands and other circumstances to conserve funds, bring about substantial sav-ings and improve access to higher quality journals. Goffman and colleagues noted that a geometric increase in the quantity of literature results in only a small arithmetic increase in utilization, and that it was neither practical nor cost-effective to acquire more than a small threshold number of journals, since utilization capacity could easily be surpassed. But larger collections could be developed incrementally by lowering the required citation frequency of candidate journals to acquire to build needed specialty collections, and to participate in regional resource sharing. Thus, journal supply and demand relationships could economi-cally be optimized to satisfy educational, clinical and research needs, especially in developing nations and regions.
 Goffman extended these concepts by proposing an additional approach to literature selection ( Goffman &amp;
Warren, 1980 ). He was critical of the then-typical library approaches that sought to acquire and preserve everything printed rather than to make key contributions available to advance personal or scientific knowl-edge. He likewise objected to anecdotal approaches to journal and other document acquisitions based on tra-dition, consensus, and casual relevance assessments. He also criticized information retrieval approaches that divorced retrieval from their broader knowledge communications contexts and their uncritical use of Boolean logic. He again proposed that instead we try to build highly relevant Bradfordian core sets of literature geared to local user populations and conditions. Regional resource sharing could be used to augment such core sets.
Last, he advocated going beyond the rigidities of Boolean retrieval by developing relevance networks based on citation linkages among related papers, especially for the strategic advancement of knowledge.

While the Internet and web have obviously changed knowledge access patterns, Goffman X  X  proposed adap-tation of Bradfordian principles, citation networking, and going beyond Boolean retrieval logic all remain applicable to this day.

In September, 1993 Goffman applied for a US patent that would apply his application of Bradford X  law and his bibliometric methodology for selecting core collections of journals and other objects in various settings.
The patent (US Patent Office number 5,594,897) was granted on January 14, 1997. Thus one might add the word  X  X  X nventor  X  to describe Goffman X  X  extensive realm of talents. 4. Epidemic spread of knowledge
Goffman is probably best known for his mathematical work in the realm of communicable disease epide-miology and its application to the growth and spread of knowledge. He and his colleagues studied similarities between the genesis, spread and decline of various diseases across populations over time. They then drew par-allels with how human individuals and populations become  X  X  X nfected  X  with ideas and knowledge fashions, and successfully modeled these processes against empirical data.
 In the early 1960s, Goffman and Newill started drawing parallels between disease and knowledge epidemics.
For a disease to originate and spread, there must exist infective material and a vulnerable population. In turn, the population is made up of infectives (hosts to the infective material), susceptibles (those who can be infected), and removals (those who are susceptible or infected but leave the population because of immunity, hospitalization, or death). Exposed individuals are either immune to the disease or can become infected through contact with a disease host or vector. The time required for an infective to develop the disease after contact with infectious material is known as a latency or incubation period. An epidemic may be said to occur when a disease outbreak involves an abnormally high number of individuals. Goffman and Newill extended the above epidemic concepts by discussing various knowledge epidemics, such as the spread of major religious ideas and such scientific concepts as those of Newton, Darwin and Freud. But they noted that while most idea epidemics might be regarded as desirable, disease epidemics are generally not desired. Vulnerable members of a scientific or lay population could, for example, be infected with a concept published in a book or journal (a vector), while other members, being immune, might reject that concept. Or, immune individuals might trans-mit the concept without being infected (the  X  X  X yphoid Mary  X  phenomenon). Once an abnormally large num-ber of population members become infected with the concept, a knowledge epidemic can be said to occur.
But could such knowledge epidemics be successfully modeled to explain or predict the actual spread of sci-entific or other concepts based on historical data? Goffman and Newill attacked this question by developing a general deterministic model based on differential equations to show how infectives are involved in the spread of mathematical or medical concepts, especially in large populations. They briefly explored a contrasting sto-chastic modeling approach, based on finite state Markov processes and particularly applicable to smaller pop-ulations, to derive the probability of new levels of infective cases. Mathematical epidemic approaches could thus be used to depict the dynamics of knowledge epidemics, despite their necessary limitations ( Goffman &amp; Newill, 1964 ).

Goffman elaborated on these epidemic models by specifying epidemic processes in open populations. If epi-demic processes are treated stochastically, they can be represented with finite state Markov models using either discrete or continuous parameters. Discrete parameters are applicable where the latency period is constant, in which case infectives occur through multiple generations. Continuous parameters are applicable when latency periods are variable. Past deterministic and stochastic treatments of epidemic processes, he noted, tended to deal with closed populations, wherein population N remains constant throughout the epidemic. But in actual epidemics the total number N tends to vary considerably, since the number of susceptibles, infectives, and rem-ovals change throughout the epidemic process. New supplies of infectives tend to be introduced continuously. An epidemic will peak and decline if and only if the rate of change of removals is constant. ( Goffman, 1965b ).
It is remarkable that in one short article in the journal Nature , he demonstrated that most epidemic modeling in the 1960s addressed only the special case of closed populations. In effect, Goffman became an epidemiol-ogist per se, without reference to knowledge epidemiology, by introducing a more general epidemic model per-tinent to open populations.
 Next, he looked at the relative stability of epidemic processes by extending Russian mathematician A.M.
Lyapunov X  X  theory related to differential equations. Sub-population numbers in open epidemic populations tend to remain in flux or in state transitions, owing to different degrees of potency of infectious material, exposure, immunity, etc. Consequently, populations can be either in an increasing, stable or declining disease state. While closed populations tend toward a relatively predictable stable or equilibrium states, the size of open populations is variable, given the differential rate of introduction of new susceptibles and infectives.
Thus, outcomes are less predictable in open populations, and there is no guarantee that the epidemic will reach a maximum and then transition to a stable or declining state. To clarify the conditions under which an open population epidemic might stabilize or decline, Goffman extended Lyapunov X  X  differential equation stability theorem and a uniqueness theorem to specify points along a growth or decline trajectory at which epidemic stability might occur ( Goffman, 1966a ). Again, he produced an original contribution to mathematical epidemiology.

He then returned to the problem of exploring the epidemic-like spread of knowledge. But he was then inter-ested in explaining the more general problem of the growth of science itself. As a case in point, he explored the history of mast cell research. Goffman felt that it was fortunate that the acclaimed stress theorist, Hans Selye, had assembled a complete bibliography on the topic (which covered the period from Erlich X  X  discovery of mast cells in 1877 X 1963). Selye also made his personal library available to Goffman. Bill Goffman then defined authors of mast cell works as  X  X  X nfectives  X  , who would later become  X  X  X emovals  X  , a year following their last publication on the topic. From 1877 through World War II, mast cell publication output was minimal and sporadic, but it reached epidemic proportions following World War II and could be projected to increase well into the future. Goffman noted that this epidemic model was  X  X  X ompletely general  X  and could be applied in any scientific area to assess the comparative importance of existing and emergent lines of inquiry and anticipate new ones ( Goffman, 1966b ).

About the same time, he developed a model to explain the conditions of disease co-occurrence. Two co-occuring diseases in a population might either be independent of one another, or one might augment or inhibit the other. Goffman put forth an extensive set of proofs to explain the probability of individuals being infected with two diseases simultaneously or sequentially ( Goffman, 1966c ).

Goffman and Newill resumed their investigation of knowledge epidemiology by looking beyond the rather narrow information retrieval paradigm of the time that centered on file organization and query matching.
They addressed the wider problem of achieving optimal, effective and timely knowledge diffusion throughout specified populations. In a lengthy, elaborate mathematical paper published in the Proceedings of the Royal
Society of London ( Goffman &amp; Newill, 1967 ), they proposed that both the transmission of diseases and of ideas are special cases of a more general communication process. In both cases, individuals are exposed and are either immune or infected, and might or might not transmit infection to others. Eventually, the epidemic might become dormant.

An information retrieval system might thus be regarded as something embedded in a population to stim-ulate an epidemic. The retrieval system X  X  purpose would be to provide effective (relevant) contact between infectives (document authors) and susceptibles (searchers or inquirers) so that ideas are transmitted and knowledge is acquired. They proposed a number of definitions and theorems dealing with several topics: pop-ulation stability; information relevance relations and transformations; one-to-many relevance mappings or closures between susceptibles and infectives; and the ordering of documents and exposures. For a given doc-ument set to be relevant, it must have compatible relevance with other documents in its set; in contrast, a par-tially relevant document in the set might inhibit the relevance of others in the set. It is noteworthy that
Goffman and Newill explained these concepts also in terms of systems theory; they pointed out that systems consist of sets of elements that interact optimally to perform specific functions (procedures) to fulfill specific purposes (requirements). An information retrieval system X  X  output consists of documents (agents) that convey relevant (infectious) information in response to queries posed by users (susceptibles). Systems theory thus aug-mented their eclectic use of communications theory, mathematics, epidemiology and information retrieval.
With Ken Warren, Goffman applied the Kermack X  X cKendrick model of epidemics, a model particularly applicable to schistosomiasis since it applies to cyclical and multi-stage diseases involving intermediate hosts.
Schistosomiasis was and continues to be a serious, widespread disease outside of the US and is transmitted by parasitic worms carried by snails. Goffman and Warrren determined three kinds of required threshold num-bers and density levels: those above which snail populations must exist to bring about schistomiasis epidemics in human populations; those at which epidemics would stabilize; and those below which epidemics would decline or cease to exist ( Goffman &amp; Warren, 1970 ). They thereby provided critical parameters for public health officials to control snail populations and consequently to control schistosomiasis epidemics.
In the 1960s, Goffman was pleased to find published, exhaustive bibliographies that encompassed the origination and development of symbolic logic ( Church, 1936, 1938 ). To further test his epidemic theory of the growth of science, he observed that the long-term growth of symbolic logic was characterized by continual evolution and diffusion. Boole and DeMorgan had published their seminal formulation of symbolic logic in 1847. Prior to that time, works on symbolic logic were scattered, rudimentary, non-cumulative, and spurred by some persistent stimulus that originated in the remote past, Goffman observed. After 1847, in contrast, publications in symbolic logic were outgrowths of related publications preceding them. From the bibliogra-phy, it became clear that scholars would juxtapose and synthesize ideas from the past to produce new con-cepts. New concepts would then diffuse through a population and result in production cycles of yet newer syntheses. Thus, Goffman reasoned that epidemic theory could be applied to establish quantitatively the rel-ative importance of past lines of inquiry, and to predict the behavior of current lines of investigation and the probable emergence of new research directions. He provided saw-tooth-like graphs of the cyclical development of symbolic logic from 1847 onward. Initial publication output was sporadic and limited, with peaks occurring in 1867, 1877 and 1892. Symbolic logic then entered successive epidemic states, with peak points occurring at 25-year intervals (1907, 1932 and 1957). Goffman observed that initial, original works in the foundations of mathematics stimulated discoveries on the paradoxes of set theory, and later stimulated developments in prep-ositional calculus, meta-mathematics, intuitionism, and other sub-fields. Applications of epidemic theory dem-onstrated that once there was a reasonably successful culmination of work in one area, new questions would be evoked and new lines of inquiry would then build to form newer periods of activity of different sizes and intensities. He then applied a stochastic (Markov chain) model to explain further the evolution of symbolic logic. Apparently, the seven sub-fields of symbolic logic that emerged from 1847 to 1932 had developed as a somewhat predictable sequence of overlapping epidemics. But he could find no predictable pattern of migra-tion of researchers from one sub-field to another; once researchers left an area, they were unlikely to return to make further contributions ( Goffman, 1971 ). 5. Scientific discovery and growth of knowledge
Over the years, Goffman developed a keen interest in the scientific discovery process. He viewed the growth of specific areas of knowledge as the cumulative result of a succession of related, epidemic-like discoveries. As my dissertation supervisor, he challenged me in 1968 to write a dissertation that would provide a model for predicting the occurrence of scientific discoveries. Under Goffman X  X  guidance, this dissertation effort produced a systems model and general equation that would illustrate how previous discoveries were made: a series of key contributions in a given area would over time aggregate into a discovery (a disciplinary sub-sub-system). In turn, these discoveries would eventually aggregate into disciplinary sub-systems (sub-fields), and these sub-sys-tems would aggregate into disciplinary systems. Disciplinary systems would form into disciplinary super-sys-tems (such as the biological, physical or social sciences). There were chronological regularities in the making of key contributions and their manner of aggregation into discoveries. In all cases, human short-term memory limits of about seven informational chunks required that contributions be summarized and aggregated peri-odically; otherwise, a logjam of contributions would tend to occur and stop progress ( Harmon, 1970, 1973 ).
Following dissertation completion, I remodeled the above discovery process in terms of set theory. A dis-covery itself could be seen as a complete, finite and ordered set of about seven cognitive elements or informa-tion chunks formed within the discoverer X  X  short-term memory. Such an ordered set served to provide a meaningful organization of knowledge within a realm of scientific discourse. That is, the discovery process involved a series of cognitive set transformations involving successive mental iterations through four states or stages: Possession of (I) insufficient and unordered information elements; (II) insufficient but ordered infor-mation; (III) sufficient but unordered information; (IV) sufficient and ordered informational elements X  X he discovery state. A two-by-two matrix represented these four states. The discovery event thus reflected a suc-cessful culmination of a series of efforts to gather the necessary information elements, to define an appropriate set-defining criterion, and to organize the set. The set was regarded as finite, consisting of about seven ele-ments, because the number of elements had to be restricted to a cognitively manageable number to accommo-date the human working (short-term) memory capacities of both the discoverer and each individual who might seek to understand or use that discovery. Once the discovery was made, the set could be further elaborated, tested and refined. We then further modeled this discovery process as a four-state Markov chain, in which the stages or states could be represented as probabilities of movement of an incipient discovery system between the four different states. The previously mentioned Church bibliographies on the development of symbolic logic ( Church, 1936, 1938 ) were applied to test the model. Church had marked key contributions in the bibliogra-phy with single asterisks, and significant breakthroughs with double asterisks. The Markov model showed that symbolic logic discoveries resulted from forward and backward oscillations through states I X  X II until a discov-ery emerged as state IV. The mean recurring times for states I X  X V were, respectively, 2.2, 4.0, 4.4 and 12.5 years. But each discovery state apparently posed new and different questions and the probability of the dis-covery set remaining in discovery state IV was 0. Throughout the development of symbolic logic, a discovery could be predicted to occur every 12.5 years. The Markov transition possibilities was irreducible, since each or the four states could be reached from each of its other states. Each state was a function of its preceding state, and all states were representative of the others (ergodic), so the limited memory of the Markov model was sufficient to explain transitions. In summary, these discoveries resulted from a predictable pattern of oscilla-tions through three states to acquire the necessary information elements and order them appropriately to arrive in state IV, the discovery state ( Goffman &amp; Harmon, 1971 ).

Goffman later teamed with his colleague and former doctoral student, Tefko Saracevic, in an attempt to understand the mechanisms involved in forecasting information user and use requirements for subject litera-tures. They did an analysis of the structure and behavior of sample scientific literatures through extended time periods. They reported their findings at the 1977 International forum for Information and Documentation, noting that Goffman and Warren X  X  theory of epidemics and holistic literature ecology concepts could be invoked to furnish qualitative and quantitative literature developmental parameters that would serve to fore-cast needed capabilities for large information services. The old ritual of building exhaustive collections was no longer feasible or even possible, given the huge numbers of journals in most scientific areas and the trend for scientists to demand quality filters. They further observed that future research should explore how to provide high quality literature, but with attention to qualitative X  X uantitative tradeoffs ( Saracevic &amp; Goffman, 1977 ).
Goffman worked with another former doctoral student and colleague, Miranda Lee Pao, on several pro-jects. For example, they sought to refine a citation impact measure to assess the quality of a large, 20-year body of schistosomaisis literature. Their statistically rigorous analysis revealed that a small core of 351 of nearly 10,000 researchers in this field (3.51%) had produced a third of the field X  X  literature over a 15-year per-iod. They also found that the most productive half of grant awardees over the same period produced the high-est quality research, as indicated by citation impact factors and average influence weightings (the latter developed by Computer Horizons Inc.). The more productive researchers tended also to link parasitic diseases with the broader realm of modern biology. Sustained funding of schistosomiasis research over extended peri-ods of time had served to enhance the high quality research, they concluded ( Pao &amp; Goffman, 1990 ). Their remarkably timeless article is worth reading today.

While mentoring a succession of doctoral students through their dissertations, Goffman frequently expressed concern that information science itself was slow in defining its problems rigorously and in developing the abstract body of knowledge required to qualify it as a true science. In his noteworthy paper,  X  X  X nformation Sci-ence: Discipline or Disappearance  X  ( Goffman, 1970 ), he observed that both library and computer science were excessively rooted in their respective institutional and technological realms. Thus, neither area had been able to produce neither abstract modeling of key concepts nor rigorous theoretical formulations. The aim of informa-tion science, he argued, should be to develop a unified approach to the study of information phenomena throughout the biological, physical, social and technological realms. Through the deployment of such approaches as the modeling of knowledge growth patterns, including citation graphing and relevance mapping, information science might develop into an authentic scientific area. Otherwise, it would become extinct.
But Goffman did not confine himself to information science when it came to analyzing scientific discovery and knowledge growth. For example, he joined with colleague Michael Katz, to explore the preformation of ontogenetic patterns in biology. Preformation theory asserted the existence of pre-determined, prototypical genetic and embryonic models that govern the development of organisms. Modern biology had abandoned eighteenth and nineteenth century preformation doctrine in favor of epigenetics, which holds that adult organ-isms possess features that are not pre-existent in germ cells; such features are acquired through a process of complex, synergistic interaction during development. But Katz and Goffman argued that preformation explanations still have validity, and can be seen in DNA formations of the genome and within cellular and multi-cellular development. They analyzed the ontogenetics and growth of fresh water snails and proposed that preformed, precursor patterns were indeed present and had been conserved during ontogeny. They applied mathematical typology and discrete element pattern analysis to show how intra-cellular generators influence the course of cellular development. They demonstrated that generator configurations in given cells serve as skel-etal representations of cellular cytoplasm organization in both unicellular and multi-cellular organisms. There-fore, many final organism features could generally be traced to preformed, precursor elements through conserved typology, despite some variations in the process ( Katz &amp; Goffman, 1981 ). In retrospect, these find-ings appear to be fundamental and revolutionary in modern biology. The authors had proposed and validated a more qualified, middle ground theory between the pure extremes of preformation and epigenesis. 6. Scientific communication: A synthesis
In the late 1970s, Goffman collaborated with Ken Warren to co-author a book, Scientific Information Sys-tems and the Principle of Selectivity ( Goffman &amp; Warren, 1980 ). Their joint effort integrates and extrapolates much of their previous work to produce a general model of the development of scientific communication sys-tems and their ecologies.

Their book begins by evaluating a number of differing perspectives and problems related to scientific com-munication processes. They point out that even the comprehensive Index Medicus in the late 1970s covered only about an eighth of the world X  X  extant biomedical literature. Key contributions, like Mendel X  X  1865 paper on heredity, were often ignored, largely because they were paradigmatically premature. Insignificant  X  X  X ewage  X  literature often clogs scientific communication channels and obscures significant literature. Roughly only a half of the then-existing scientific papers were ever cited at all, or were only cited once, and slightly more than a half of library collections went used. Yet, the then-popular conceptualizations of a  X  X  X iterature explosion cri-sis  X  and frequent calls to completely revamp the scientific communication system both tended to oversimplify the problems and preclude their resolution.

In contrast, Goffman and Warren argued that formal and informal formal scientific communication sys-tems and ecologies had evolved in a complementary and selective fashion over time and had developed their own adaptive mechanisms. Accordingly, what was needed was not to radically change these systems, but to adjust their quality filtering mechanisms through optimal literature selectivity and retrieval. They provided a comprehensive and original mathematical foundation to model the structures and dynamics of scientific communication systems and their ecology, including the synthesis of scientific findings. Their general model masterfully incorporates elements of the classical Shannon theory of communications; Bradford distribution principles; Garfield X  X  citation indexing; knowledge epidemiology; and the mathematical axioms of identity, permutation, transitivity, and self-distribution. Their model explains how findings are grouped into concep-tual syntheses.

They tested their general model by marshalling data from comprehensive bibliographies of quite different literatures: schistosomiasis, mast cell research, symbolic logic, biochemistry, nutrition, immunology, neopla-sia, and other areas. Despite the diversity of these literatures, they graphed remarkably similar cyclical pat-terns of growth and decline of literature production for each area through extended time periods, thus supporting their basic model.
 Goffman and Warren further explored the qualitative dimensions of scientific literature through the use of
Bradfordian distribution analysis. They noted that small circles of elite scientists tend to produce the most sig-nificant (highly cited) literature in their respective fields, and that the contributions of the great majority of scientists tend to be relatively minor. Cost-effective filtering can be achieved by centering on the more fre-quently cited literature worthy of publication in highly ranked journals, state-of-art reviews and abstracts.
Last, they proposed a comprehensive policy for improving scientific communications. They noted that sci-entific communications systems evolve over time, and develop their own regulatory mechanisms and ecolog-ical relations. While knowledge systems in some areas might become sub-optimal or even fade away, their operation can be explained and predicted through the use of epidemiological modeling. Some communication systems might best be left to natural evolution and survival of the fittest. Others might be improved through optimal filtering, storage, and retrieval to support the much-needed synthesis of scientific findings.
Goffman and Warren X  X  book provides a noteworthy consolidation and extension of their separate, previous works and details numerous significant findings. They also acknowledge the parallel efforts of colleagues
Eugene Garfield and Vaun Newill. Overall, this book is a synergy of two prodigious minds; here, one plus one equals far more than two, which epitomizes the old observation that wholes represent more than the sum of their parts. 7. An unfolding legacy
Bill Goffman X  X  legacy is obviously diverse and far reaching, particularly in the areas of knowledge epide-miology, recognition of patterns in the statistical distribution of literatures and direct knowledge representa-tions, and information storage and retrieval. He also made or helped make original contributions in disease epidemiology and helped to revitalize pre-formation theory in evolutionary biology. A 2002 historiography compiled by Eugene Garfield from his HistCite Guide ( http://garfield.library.upenn.edu/histcomp/goffman-w_citing/index-tl.html ) reveals that Goffman had been cited over 550 times by colleagues throughout the world, particularly those from the US, UK, Canada, and Germany. Scholars from France, Belgium, Hungary,
India, Nigeria, Japan, Israel, and other countries also cited his works. Colleagues Ken Warren, Vaun Newill, and Tom Morris cited him often, along with former students Tefko Saracevic, Miranda Pao, Glynn Harmon and others. Such information science pioneers as Eugene Garfield, B.C. Brooks, Robert A. Fairthrone and Brian C. Vickery acknowledged his works as well.

Close to a half of the citations of Goffman X  X  works by others relate to his formulations of knowledge epi-demiology or to his original contributions in disease epidemiology itself. A quarter of the citations relate to his statistical distributions of literature collections and usage; and nearly another quarter relate to his works on information storage and retrieval. But over the years, some of Goffman X  X  ideas appear to have become indi-rectly absorbed or  X  X  X ain-streamed  X  into such areas as search engine technology (particularly page-ranking algorithms and relevance rankings), citation pattern analysis, disease epidemiology, the epistemology of sci-entific discovery, applied mathematics, and even evolutionary biology. It might be said that over time the riv-ers of knowledge absorb flows from many tributaries. In this sense, Goffman X  X  legacy is ongoing, and the formation of new knowledge epidemics can serve to revitalize that legacy. As long as information is retrieved and used, or new knowledge develops, Goffman X  X  explanations come into play. 8. Reflections of colleagues
Don Cleveland, one of Goffman X  X  former doctoral students, submitted the following four paragraphs, which accurately describe Bill X  X  working relations with students and his lasting impact:
Likewise, I recall similar feelings about him and an identical respect and admiration. When I first saw Goff-man, I noted that co-workers would take their most difficult, seemingly intractable information retrieval prob-lems to him for an opinion. He would then mull problems over, step to a blackboard, and reframe the problem mathematically. He would then say to his stymied colleagues that they were addressing either the wrong prob-lem or a special case of a more general problem, or that they might have it right. But if he looked uninterested or indifferent about the problem, he would tend to leave the scene emotionally and attend to more interesting things X  X n unstated form of negative feedback that signaled to the colleagues that they should redefine or rethink the problem.

When I later would hear Goffman lecture about his research findings, particularly those about Bradford distributions or parsimonious information retrieval, I sensed that he possessed a sort of uncanny brilliance.
He thrived on detecting the mysteries of hidden patterns among numbers and their interrelations, and doing so would often bring a child-like gleam to his eyes, and sometimes the subtle smile of the stereotypical mad scientist. For the first time, I developed an appreciation for the clean simplicity and power of mathematical reasoning and, as he would say, the desirability of developing  X  X  X legant  X  solutions. While he liked to have something in writing from others before reviewing their ideas, he did not like verbiage but wanted something terse and translatable into a mathematical ontology or equation. Nevertheless, he had a keen perception for the presence of sheer intellectual excitement in others X  X f anyone had stumbled across a great idea and was ebullient, he would make himself available to share that excitement. 9. Conclusion
In summary, Goffman made substantial, pioneering contributions in mathematical information science and even contributed to the areas of disease epidemiology and evolutionary biology. His initial, early efforts focused on information storage and retrieval problems and brought about better ways to frame queries and search effectively and efficiently by escaping the severe constraints of Boolean logic and through the use of multi-valued logic. He created better file organization schemes and developed methods of searching sub-sets of files rather than total files. He was an early advocate of indirect, citation-based searching. A fascination with Bradford-like statistical distributions led him into developing authentic core research literature collec-tions, particularly in the biomedical areas. His subsequent efforts brought about the creative application of disease epidemiological models to the transmission and spread of ideas and knowledge through populations, which led to accurate modeling of the growth of entire knowledge systems and their respective ecological dimensions. He was later successful in understanding the dynamics of scientific discovery and the epistemo-logical dimensions of large bodies of scientific knowledge. As one driven by curiosity, he also delved into such areas as biometry, clinical decision-making in medicine.

Bill Goffman can thus be remembered as a gifted and creative mathematician and eclectic, multi-disciplinary scientist, teacher and administrator. To his immediate colleagues, his presence was never boring and often full of surprise. He was simultaneously a mature and wise colleague, but one who exhibited humorous and playfully child-like streaks of satire, mischief, and anti-authoritarian skepticism. He always had a keen nose for the ironies of conventional life. He could be characterized as a mathematician, artist, scientist, scholar, administrator, intel-lectual leader, and even as a mystic who was fascinated with the implicit dynamics of nature and society. His past students and colleagues would likely all agree that his intellectual presence is indelible and remains with them to this day, and that his mathematical explorations produced durable, significant results. Acknowledgements
The author is most grateful to Patricia Goffman, who continually provided detailed biographical informa-tion and access to Bill Goffman X  X  writings and other related materials. Eugene Garfield originally proposed this work, provided ongoing encouragement and kindly compiled Goffman X  X  citation data from his Histcite Guide and library. Likewise, Tefko Saracevic and Chistinger Tomer provided data and encouragement. Don Cleveland X  X  keen reflections about Bill Goffman are truly appreciated.
 References
