 We propose a cross-language retrieval model that is solely based on Wikipedia as a training corpus. The main contri-butions of our work are: 1. A translation model based on linked text in Wikipedia and a term weighting method asso-ciated with it. 2. A combination scheme to interpolate the link translation model with retrieval based on Latent Diric h-let Allocation. On the CLEF 2000 data we achieve improve-ment with respect to the best German-English system at the bilingual track (non-significant) and improvement against a baseline based on machine translation (significant). H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models Algorithms, Experimentation CLIR, Wikipedia, LDA, language modeling
Translation lexica and parallel corpora are often only ac-cessible for some European language pairs. And even if they exist their vocabulary is inherently limited in contrast to an ever growing wide-coverage ressource like Wikipedia, wher e corresponding articles are connected across languages. At -tempts have therefore be made to extract information for CLIR from article-level co-occurrence statistcs, with mod -erate succes [6]: Coarse thematical relationships alone ca n arguably not capture the specific meaning contained in a query, a word-to-word translation is necessary. The questi on therefore arises how word-specific mappings can be obtained from freely available large-scale knowledge sources such a s Wikipedia.
 Several approaches in this direction have been undertaken. Often, a Wikipedia title in the source language is associate d with a word and the corresponding title in the target lan-guage is used as a translation [4]. However, [5] note that the vocabulary distribution of titles has a skew to certain word s. Because the titles of Wikipedia articles are specifically ta i-lored to be unique identifiers rather than representative te xt samples, translations of large classes of words might be pro b-lematic when a variation is used. In [2] a bilingual dictiona ry is extracted from Wikipedia by supervised classification.
The method we use is unsupervised and based on the an-chor text of links. In Wikipedia, any text can be linked to any page. For example, the German texts  X  X n W  X  aldern gelegte Br  X  ande X  and  X  X uschbrand X  may be valid contexts to be linked to the article with the English counterpart  X  X ild-fire X . Three items of information are necessary to build a probabilistic translation model based on linked text:
If, for the sake of simplicity, a unigram model is used for translation this amounts to the probablities P ( l | w ), P ( a | w, l ) and P ( w | a, l ). In a bilingual setting l is a variable indicating whether the source word is linked, a is the bilingual article the source word is linked to, and w E and w F are words in the source and the target languages respectively. The prob-ability of translating a source word into a target word is: We focus on the linked case. Assuming that the translation of linked source words does only depend on the articles they are linked to, one gets: We note that the probability P ( l true | w E ) can function as a term weighting in the source documents, assuming that the importance of terms is correlated with their probabilit y of being linked. Translation and linking are assumed to be independent of the document, given a source word. In the following we write D for a source document in language E , l for l true and n ( w, Q ) for the count of w in a query Q .
In a query likelihood model a document provides a prob-abilistic model for a query. The ranking is usually done by log P ( Q | D ) = P w  X  Q n ( w, Q ) log P ( w | D ).
In principle, all components are provided by the link model to perform retrieval in such a setup. The probability that Wikipedia article a is the link target when a linked word is picked from document D (making the same independence assumptions as before) is and P ( w F | a, l ) is the probability that, given a source word is linked to article a , it is translated to word w F . Together the elements of the link model provide us with the distribution: It is practical to think of this distribution as combined in that way, because it separates the per-document estimates from the vocabulary estimates. The atomic probabilities are estimated from relative frequencies: P ( a | l, w E ), P ( l | w and P ( w F | a, l ) from Wikipedia, P ( w E | D ) from the current document.

This formulation poses three problems: The zero-proba-bility problem: Because the components of the model are estimated from relative frequencies, to many events zero probability is assigned. The summation problem: If the probability distributions are smoothed and are never zero, summation might for every word go over all Wikipedia ar-ticles, which would be prohibitively expensive to compute. The training basis problem: The model only considers words likely to be linked. Especially high frequency or func -tion words could have skewed distributions.

The model is hence not immediately applicable. We tackle these problems by interpolating the link model with a lan-guage model based on LDA and by considering the proba-bility of being linked for the query term weights.
One possible combination scheme of LDA and link model is to interpolate word distributions given a document. We use Wikipedia as a bilingual training corpus for LDA by cutting articles at 100 words, discarding shorter ones and concatenating both language sides. We trained models with 125, 250, 500 and 1000 topics (parameterized as suggested in [3]) and interpolated them with equal weight to avoid local maxima. After inference on the retrieval collection, one ha s: Having the word probabilities, the question arises how to weight the query word counts, as a weighting according to their probability of being linked seems reasonable for the link model, but is not justified for LDA. Hence, we use two model parameters  X  and  X  to interpolate weightings and dis-tributions respectively. The query log-likelihood become s: log P  X , X  ( Q | D ) = X The LDA-distributions are smooth by defininition, so for 0  X   X  &lt; 1 there is no zero-probability problem. For efficiency reasons, we did not smooth the link component and summed only over the 1000 most probable articles per document. In an ad-hoc parametrization we let both weightings and both models contribute equally strongly. The probability of a word being linked is very low with p ( l ) = . 06, to get equal influence of both weightings we require  X   X  p ( l ) = 1  X   X  , which results in  X  = . 94,  X  is set to . 5.

We evaluated on the German-English CLEF 2000 bilin-gual track 1 (title+description) and achieve map = . 291 which is better than any of those reported for the same language pair [1], this difference is however statistically not signi ficant. The model is significantly ( p &lt; 0 . 05, paired t-test) better than a base-line using Moses machine translation trained on Europarl with tf.idf vector retrieval.

Table 1: Results on German-English CLEF2000
We have introduced a CLIR method that is based solely on information extracted from Wikipedia. It combines documen t-level information (captured by LDA) and word-specific in-formation (captured by a link model) in a clear language modeling setup. There is much room for the exploration of different smoothing and combination schemes. We tried a simple one on word level. This model did not only outper-form a base-line obtained with Moses machine translation, i t also produced results that compare favorably against value s reported for the CLEF 2000 bilingual track. [1] M. Braschler. CLEF 2000-Overview of results. In [2] M. Erdmann, K. Nakayama, T. Hara, and S. Nishio. [3] T. Griffiths and M. Steyvers. Finding scientific topics. [4] D. Nguyen, A. Overwijk, C. Hauff, R. Trieschnigg, [5] J. Sjobergh, O. Sjobergh, and K. Araki. What types of [6] P. Sorg and P. Cimiano. Cross-lingual information
