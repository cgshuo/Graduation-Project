 Named Entity Recognition (NER) has earned an important place in Natural Language Processing (NLP) as an enabling process for other tasks. When explicitly taken into account, research shows that it helps such applications achieve bet-ter performance levels (Babych and Hartley, 2003; Thompson and Dozier, 1997). NER is defined as the computational identification and classification of Named Entities (NEs) in running text. For in-stance, consider the following text: A NER system should be able to identify Barack Obama and Middle East as NEs and classify them as Person (PER) and Geo-Political Entity (GPE), respectively. The class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature 1 .
 According to (Nadeau and Sekine, 2007), opti-mization of the feature set is the key component in enhancing the performance of a global NER sys-tem. In this paper we investigate the possibil-ity of building a high performance Arabic NER system by using a large space of available feature sets that go beyond the explored shallow feature sets used to date in the literature for Arabic NER. Given current state-of-the-art syntactic processing of Arabic text and the relative small size of man-ually annotated Arabic NER data, we set out to explore a main concrete research goal: to fully ex-ploit the level of advancement in Arabic lexical and syntactic processing to explore deeper linguis-tic features for the NER task. Realizing that the gold data available for NER is quite limited in size especially given the diverse genres in the set, we devise a method to bootstrap additional instances for the new features of interest from noisily NER tagged Arabic data. We use our state-of-the-art NER system described in (Benajiba et al., 2008) as our baseline sys-tem (BASE) since it yields, to our knowledge, the best performance for Arabic NER . BASE em-ploys Support Vector Machines (SVMs) and Con-ditional Random Fields (CRFs) as Machine Learn-ing (ML) approaches. BASE uses lexical, syn-tactic and morphological features extracted using highly accurate automatic Arabic POS-taggers. BASE employs a multi-classifier approach where each classifier is tagging a NE class separately. The feature selection is performed by using an in-cremental approach selecting the top n features (the features are ranked according to their individ-ual impact) at each iteration and keeping the set that yields the best results. In case of conflict -a word is classified with more than one class/tag si-multaneously -the global NER system selects the output of the classifier with the highest precision.
The following is the feature set used in (Bena-jiba et al., 2008) and accordingly in the BASE sys-tem. 1. Context: a  X  / +1 token window; 2. Lex-ical: character n  X  grams where n ranges from 1  X  3 ; 3. Gazetteers: automatically harvested and manually cleaned Person NE class (PER), Geopo-litical Entity NE class (GPE), and Organization NE class (ORG) lexica; 4. POS-tag and Base Phrase Chunk (BPC) : automatically tagged us-ing AMIRA (Diab et al., 2007) which yields F-measures for both tasks in the high 90 X  X ; 5. Mor-phological features: automatically tagged using the Morphological Analysis and Disambiguation for Arabic (MADA) tool to extract information about gender, number, person, definiteness and as-pect for each word (Habash and Rambow, 2005); 6. Capitalization: derived as a side effect from running MADA. MADA chooses a specific mor-phological analysis given the context of a given word. As part of the morphological information available in the underlying lexicon that MADA ex-ploits. As part of the information present, the un-derlying lexicon has an English gloss associated with each entry. More often than not, if the word is a NE in Arabic then the gloss will also be a NE in English and hence capitalized.
 We devise an extended Arabic NER system (EX-TENDED) that uses the same architecture as BASE but employs additional features to those in BASE. EXTENDED defines new additional syn-tagmatic features.

We specifically investigate the space of the sur-rounding context for the NEs. We explore gener-alizations over the kinds of words that occur with NEs and the syntactic relations NEs engage in. We use an off-the-shelf Arabic syntactic parser. State-of-the-art for Arabic syntactic parsing for the most common genre (with the most training data) of Arabic data, newswire, is in the low 80%s. Hence, we acknowledge that some of the derived syntactic features will be noisy.

Similar to all supervised ML problems, it is de-sirable to have sufficient training data for the rele-vant phenomena. The size of the manually anno-tated gold data typically used for training Arabic NER systems poses a significant challenge for ro-bustly exploring deeper syntactic and lexical fea-tures. Accordingly, we bootstrap more NE tagged data via projection over Arabic-English parallel data. The role of this data is simply to give us more instances of the newly defined features (namely the syntagmatic features) in the EXTENDED sys-tem as well as more instances for the Gazetteers and Context features defined in BASE. It is worth noting that we do not use the bootstrapped NE tagged data directly as training data with the gold data. 2.1 Syntagmatic Features For deriving our deeper linguistic features, we parse the Arabic sentences that contain an NE. For each of the NEs, we extract a number of features described as follows: -Syntactic head-word (SHW): The idea here is to look for a broader relevant context. Whereas the feature lexical n-gram context fea-ture used in BASE, and hence here for EX-TENDED, considers the linearly adjacent neigh-boring words of a NE, SHW uses a parse tree to look at farther, yet related, words. For instance, in the Arabic phrase  X  X rH Ams An Figure 1: Example for the head word and syntactic environment feature bArAk AwbAma ytrAs X , which means  X  X e-clared yesterday that Barack Obama governs ... X , glossed  X  X rH/ declared Ams/ yesterday An/ that bArAk/ Barack AwbAmA/ Obama ytrAs/ governs ... X , is parsed in Figure 1. According to the phrase structure parse, the first parent sub-tree headword of the NE  X  X ArAk AwbAmA X  is the verb  X  X trAs X  (governs), the second one is  X  X n X  (that) and the third one is the verb  X  X rH X  (declared). This exam-ple illustrates that the word  X  X ms X  is ignored for this feature set since it is not a syntactic head. This is a lexicalized feature. -Syntactic Environment (SE): This follows in the same spirit as SHW, but expands the idea in that it looks at the parent non-terminal instead of the parent head word, hence it is not a lexicalized fea-ture. The goal being to use a more abstract repre-sentation level of the context in which a NE ap-pears. For instance, for the same example pre-sented in Figure 1, the first, second, and third non-terminal parents of the NE  X  X ArAk AwbAmA X  are  X  X  X ,  X  X BAR X  and  X  X P X , respectively.
 In our experiments we use the Bikel implementa-tion (Bikel, 2004) of the Collins parser (Collins, 1999) which is freely available on the web 2 . It is a head-driven CFG-style parser trained to parse En-glish, Arabic, and Chinese. 2.2 Bootstrapping Noisy Arabic NER Data Extracting the syntagmatic features from the training data yields relatively small number of instances. Hence the need for additional tagged data. The new Arabic NER tagged data is derived via projection exploiting parallel Arabic English data. The process depends on the availability of two key components: a large Arabic English parallel corpus that is sentence and word aligned, and a robust high performing English NER system. The process is as follows. We NE tag the English side of the parallel corpus. We project the automatically tagged NER tags from the English side to the Arabic side of the parallel corpus. In our case, we have access to a large manually aligned parallel corpus, therefore the NER projection is direct. However, the English side of the parallel corpus is not NER tagged, hence we use an off-the-shelf competitive robust automatic English NER system which has a published performance of 92% (Zitouni and Florian, 2009). The result of these two processes is a large Arabic NER, albeit noisy, tagged data set. As mentioned earlier this data is used only for deriving additional instances for training for the syntagmatic features and for the context and gazetteer features. 3 Given this additional source of data, we changed the lexical features extracted from the BASE to the EXTENDED. We added two other lexical features: CBG and NGC, described as follows: -Class Based Gazetteers (CBG): This feature focuses on the surface form of the NEs. We group the NEs encountered on the Arabic side of the parallel corpus by class as they are found in different dictionaries. The difference between this feature and that in BASE is that the Gazetteers are not restricted to Wikipedia sources. -N-gram context (NGC): Here we disregard the surface form of the NE, instead we focus on its lexical context. For each n , where n varies from 1 to 3, we compile a list of the  X  n , + n , and  X  / + n words surrounding the NE. Similar to the CBG feature, these lists are also separated by NE class. It is worth highlighting that the NCG feature is different from the Context feature in BASE in that the window size is different + /  X  1  X  3 for EXTENDED versus + /  X  1 for BASE. 3.1 Gold Data for training and evaluation We use the standard sets of ACE 2003, ACE 2004 and ACE 2005. 4 The ACE data is annotated for many tasks: Entity Detection and Tracking (EDT), Relation Detection and Recognition (RDR), Event Detection and Recognition (EDR). All the data sets comprise Broadcast News (BN) and Newswire (NW) genres. ACE 2004 includes an additional NW data set from the Arabic TreeBank (ATB). ACE 2005 includes a different genre of Weblogs (WL). The NE classes adopted in the annotation of the ACE 2003 data are: Person (PER), Geo Political Entity (GPE), Organization (ORG) and Facility (FAC). Additionally for the ACE 2004 and 2005 data, two NE classes are added to the ACE 2003 tag-set: Vehicles (e.g. Rotterdam Ship) and Weapons (e.g. Kalashnikof). We use the same split for train, de-velopment, and test used in (Benajiba et al., 2008). 3.2 Parallel Data Most of the hand-aligned Arabic-English parallel data used in our experiments is from the Language Data Consortium (LDC). 5 . Another set of the par-allel data is annotated in-house by professional an-notators. The corpus has texts of five different gen-res, namely: newswire, news groups, broadcast news, broadcast conversation and weblogs corre-sponding to the data genres in the ACE gold data. The Arabic side of the parallel corpus contains 941,282 tokens. After projecting the NE tags from the English side to the Arabic side of the paral-lel corpus, we obtain a total of 57,290 Arabic NE instances. Table 1 shows the number of NEs for each class.
 Table 1: Number of NEs per class in the Arabic side of the parallel corpus 3.3 Individual Feature Impact Across the board, all the features yield improved performance. The highest obtained result is ob-served where the first non-terminal parent is used as a feature, a Syntactic Environment (SE) fea-ture, yielding an improvement of up to 4 points over the baseline. We experiment with different sizes for the SE, i.e. taking the first parent versus adding neighboring non-terminal parents. We note that even though we observe an overall increase in performance, considering both the { first, sec-ond } or the { first, second, and third } non-terminal parents decreases performance by 0.5 and 1.5 F-measure points, respectively, compared to consid-ering the first parent information alone. The head word features, SHW, show a higher positive im-pact than the lexical context feature, NGC. Finally, the Gazetteer feature, CBG, impact is comparable to the obtained improvement of the lexical context feature. 3.4 Feature Combination Experiments Table 2 illustrates the final results. It shows for each data set and each genre the F-measure ob-tained using the best feature set and ML approach. It shows results for both the dev and test data us-ing the optimal number of features selected from the all the features except the syntagmatic ones ( All-Synt. ) contrasted against the system in-cluding the semantic features, i.e. All the features, per class All . The baseline results, FreqBaseline , assigns a test token the most frequent tag observed for it in the gold training data, if a test token is not observed in the training data, it is assigned the most frequent tag which is the O tag. Individual feature impact results show that the syntagmatic features are helpful for most of the data sets. The highest improvements are obtained for the 2003 BN and 2005 WL data-sets. The im-provement varies significantly from one data-set to another because it highly depends on the num-ber of NEs which the model has not been able to capture using the contextual, lexical, syntactic and morphological features.
 Impact of the features extracted from the paral-lel corpus per class : The syntagmatic features have varied in their influence on the different NE classes. Generally, the LOC and PER classes ben-efitted more from the head word features, SHW), than the other classes. On the other hand for the syntactic environment feature (SE), the PER class seemed not to benefit much from the presence of this feature. Weblogs : Our results show that the random contexts in which the NEs tend to ap-pear in the WL documents stand against obtain-ing a significant improvement. Consequently, the features which use a more global context (syntac-tic environment, SE, and head word, SHW, fea-tures) have helped obtain better results than the ones which we have obtained using local context namely CBG and NGC. Projecting explicit linguistic tags from another language via parallel corpora has been widely used in the NLP tasks and has proved to contribute sig-nificantly to achieving better performance. Dif-ferent research works report positive results when using this technique to enhance WSD (Diab and Resnik, 2002; Ng et al., 2003). In the latter two works, they augment training data from parallel data for training supervised systems. In (Diab, 2004), the author uses projections from English into Arabic to bootstrap a sense tagging system for Arabic as well as a seed Arabic WordNet through projection. In (Hwa et al., 2002), the authors report promising results of inducing Chi-nese dependency trees from English. The ob-tained model outperformed the baseline. More re-cently, in (Chen and Ji, 2009), the authors report their comparative study between monolingual and cross-lingual bootstrapping. Finally, in Mention Detection (MD), a task which includes NER and adds the identification and classification of nom-inal and pronominal mentions, (Zitouni and Flo-rian, 2008) show the impact of using a MT sys-tem to enhance the performance of an Arabic MD model. The authors report an improvement of up to 1.6F when the baseline system uses lexical fea-tures only. Unlike the work we present here, their approach requires the availability of an accurate MT system which is a more expensive process. In this paper we investigate the possibility of building a high performance Arabic NER system by using lexical, syntactic and morphological fea-tures and augmenting the model with deeper lexi-cal features and more syntagmatic features. These extra features are extracted from noisy data ob-tained via projection from an Arabic-English par-allel corpus. Our results show that we achieve a significantly high performance for almost all the data-sets. The greatest impact of the syntagmatic features (1.64 points of F-measure) is obtained for the ACE 2004, BN genre. Also, the WL genre yields an improvement of 1.16 F1 points absolute.
