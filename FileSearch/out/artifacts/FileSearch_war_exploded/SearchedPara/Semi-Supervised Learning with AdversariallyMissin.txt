 Semi-supervised learning algorithms use both labeled and u nlabeled examples. Most theoretical analyses of semi-supervised learning assume that m + n labeled examples are drawn i.i.d. from a distribution, and then a subset of size n is chosen uniformly at random and their labels are erased [1]. This missing-at-random assumption is best suited for a situation where the labels ar e acquired by annotating a random subset of all available data. But in ma ny applications of semi-supervised over which examples were labeled.
 For example, pictures on popular websites like Facebook and Flikr are tagged by users at their dis-cretion, and it is difficult to know how users decide which pic tures to tag. A similar problem occurs when data is submitted to an online labor marketplace, such a s Amazon Mechanical Turk, to be manually labeled. The workers who label the data are often po orly motivated, and may deliberately skip examples that are difficult to correctly label. In such a setting, a learning algorithm should not assume that the examples were labeled at random.
 Additionally, in many semi-supervised learning settings, the partial label information is not provided on a per-example basis. For example, in multiple instance learning [2], examples are presented to regularization [3], a learning algorithm is given information about which e xamples are likely to have the same label, but not necessarily the identity of that label. Recently, there has been much interest in algorithms that learn from labeled features [4]; in this setting, the learning algorithm is given information about the expected value of several featu res with respect to the true distribution on labeled examples.
 To summarize, in a typical semi-supervised learning proble m, label information is often missing in an arbitrary fashion, and even when present, does not always have a simple form, like one label per example. Our goal in this paper is to develop and analyze a learning alg orithm that is explicitly designed for these types of problems. We derive our learning algorithm within a framework that is expressive enough to permit a very general notion of label in formation, allowing us to make minimal assumptions about which examples in a data set have been labe led, how they have been labeled, and why. We present both theoretical upper and lower bounds f or learning in this framework, and motivated by these bounds, derive a simple yet provably opti mal learning algorithm. We also provide experimental results on several standard data sets, which s how that our algorithm is effective and robust when the label information has been provided by  X  X azy  X  or  X  X nhelpful X  labelers. Related Work: Our learning framework is related to the malicious label noise setting, in which the labeler is allowed to mislabel a small fraction of the tra ining set (this is a special case of the even more challenging malicious noise setting [5], where an adverary can inject a small number of arbitrary examples into the training set). Learning with this type of label noise is known to be quite difficult, and positive results often make quite restr ictive assumptions about the underlying data distribution [6, 7]. By contrast, our results apply far more generally, at the expense of assuming a more benign (but possibly more realistic) model of label no ise, where the labeler can adversarially erase labels, but not change them. In other words, we assume that th e labeler equivocates, but does not lie. The difference in these assumptions shows up quite c learly in our analysis: As we point out in Section 3, our bounds become vacuous if the labeler is allo wed to mislabel data. In Section 2 we describe how our framework encodes label info rmation in a label regularization function, which closely resembles the idea of a compatibility function introduced by Balcan &amp; Blum [8]. However, they did not analyze a setting where this funct ion is selected adversarially. Let X be the set of all possible examples, and Y the set of all possible labels, where |Y| = k . Let D be an unknown distribution on X  X Y . We write x and y as abbreviations for ( x and ( y i.i.d. from the distribution D on X  X Y , and x  X  X  m to denote that each x marginal distribution of D on X .
 labels  X  y , and in most theoretical analyses, the missing components o f  X  y are assumed to have been selected uniformly at random.
 We make a much weaker assumption about what label informatio n is available. We assume that, the examples  X  x and to a label regularization function R . The function R encodes some information A label regularization function R maps each possible soft labeling q of the training examples  X  x to a about how the labeler selects R . We give examples of label regularization functions in Sect ion 2.1. Let  X  denote the set of distributions on Y . A soft labeling q  X   X  m of the training examples  X  x is a doubly-indexed vector, where q ( i, y ) is interpreted as the probability that example  X  x when its argument is true and 0 otherwise; we overload notation and write  X  y to denote the correct soft labeling.
 restricts the choices the labeler can make. We are intereste d in designing learning algorithms that other minima and near-minima as well . This is the sense in which label information is  X  X issing X   X  it is difficult for any learning algorithm to distinguish amo ng these minima.
 We emphasize that, while our algorithms work best when  X  y is close to the minimum of each R  X  bounds degrade gracefully as this condition is violated. We are interested in learning a parameterized model that pre dicts a label y given an example x . Let the development in this paper will apply to generic loss func tions, but two loss functions that will particularly interest us are the negative log-likelihood o f a log-linear model where  X   X   X  ( x, y )  X  R d is the feature function, and the 0-1 loss of a linear classifie r ing algorithm is to find a parameter  X  that minimizes the expected loss E denotes expectation with respect to ( x, y )  X  X  .
 Let E random from the training examples  X  x and  X  supposing that this is example  X  x from the distribution q ( i, ) . Accordingly, E 2.1 Examples of Label Regularization Functions To make the concept of a label regularization function more c lear, we describe several well-known learning settings in which the information provided to the l earning algorithm is less than the fully labeled training set. We show that, for each these settings, there is a natural definition of R that captures the information that is provided to the learning al gorithm, and thus each of these settings can be seen as special cases of our framework.
 expressed in our framework. In the supervised learning sett ing, the label of every example in the contains a single function R In the semi-supervised learning setting, the labels of only some of the training examples are revealed. In this case, there is a function R if and only if the soft labeling q agrees with  X  y on the examples in I . This implies that R independent of how q labels examples not in I  X  these are the examples whose labels are missing. In the ambiguous learning setting [9, 10], which is a generalization of semi-supervis ed learning, the labeler reveals a label set  X  Y for each training example, the learning algorithm is given a set of possibile labels the example can have (semi-supervised learning is the special case where ea ch label set has size 1 or k ). Letting  X  each possible  X  Y such that R Here q zero if and only if the soft labeling q is supported on the sets  X  Y The label regularization functions described above essent ially give only local information; they spec-ify, for each example in the training set, which labels are po ssible for that example. In some cases, we may want to allow the labeler to provide more global inform ation about the correct labeling. One example of providing global information is Laplacian regularization , a kind of graph-based regularization [3] that encodes information about which ex amples are likely to have the same labels. cian regularizer is defined to be R semi-definite matrix defined so that R the same label are assigned different label distributions b y q .
 may or may not be related to the model features  X   X   X  defined in Section 2. As noted by several authors [4, 11, 12], it is often convenient for a labeler to provide in formation about the expected value of the form R expected value of f . This term penalizes soft labelings q which cause the expected value of f on the training set to deviate from b .
 Label regularization functions can also be added together. So, for instance, ambiguous learning can be combined with a Laplacian, and in this case the learner is g iven a label regularization function of the form R in Section 5.
 Note that, in all the examples described above, while the cor rect labeling  X  y is at or close to the Again, this is the sense in which label information is  X  X issi ng X .
 It is also important to note that we have only specified what information the labeler can reveal to the this framework. To see why, consider the example of semi-sup ervised learning. Using the notation defined above, most analyses of semi-supervised learning as sume that R random subset I of the training examples [13, 14]. By constrast, we make no as sumptions about how R In this section, we state upper and lower bounds for learning in our framework. But first, we provide a definition of the well-known concept of uniform convergenc e.
 Definition 1 (Uniform Convergence) . Loss function L has  X  -uniform convergence if with probability 1  X   X  where (  X  x ,  X  y )  X  X  m and  X  ( , ) is an expression bounding the rate of convergence. loss function L lows from standard results about Rademacher complexity and covering numbers. Other commonly used loss functions, such as hinge loss and 0-1 loss, also hav e  X  -uniform convergence under similar boundedness assumptions on  X   X   X  and  X  .
 We are now ready to state an upper bound for learning in our fra mework. The proof is contained in the supplement.
 Theorem 1. Suppose loss function L has  X  -uniform convergence. If (  X  x ,  X  y )  X  X  m then with proba-bility at least 1  X   X  for all parameters  X   X   X  and label regularization functions R  X  X  (  X  x ,  X  y ) Theorem 2 below states a lower bound that nearly matches the u pper bound in Theorem 1, in certain cases. As we will see, the existence of a matching lower bound depends strongly on the structure R ( x , y ) essentially constrains what information the labeler can re veal to the learning algorithm, thereby encoding our assumptions about how the labeler will behave. We make three such assump-tions, described below. For the remainder of this section, w e let the set of all possible examples X = {  X  x 1 , . . . ,  X  x N } be finite.
 the set of possible labelings under R is separable over examples. Assumption 1 (  X  -Separability) . For all labeled training sets ( x , y ) and R  X  R ( x , y ) there ex-ists a collection of label sets { Y P i =1  X  { supp( q i )  X  Y x i } + F ( q ) is true and  X  otherwise, and F ( q ) &lt;  X  for all q  X   X  m .
 satisfy Assumption 1. Also note that Assumption 1 allows the finite part of R (denoted by F ) to depend on the entire soft labeling q in a basically arbitrarily manner.
 denote a labeling function that maps examples X to labels Y . Also, for any labeling function h and unlabeled training set x  X  X  m , we let h ( x )  X  X  m denote the vector of labels whose i th component is X , whose i th component is p  X  X lose X  (by which we mean that they are consistently labeled and k p label regularization functions available to the labeler fo r each training set are the  X  X ame X , in the sense that the sets of possible labelings under each of them a re identical.
 k p such that R ( h ( x )) &lt;  X  if and only if R  X  ( h ( x  X  )) &lt;  X  , for all labeling functions h . possible labelings under R is the correct one only by examining R .
 then R  X  X  ( x , y  X  ) .
 Of all our assumptions, reciprocity seems to be the most unna tural and unmotivated. We argue it is Section 2.1 satisfy this assumption, and secondly, in Theor em 3 we show that lifting the reciprocity assumption makes the upper bound in Theorem 1 very loose.
 We are nearly ready to state our lower bound. Let A be a (possibly randomized) learning algorithm that takes a set of unlabeled training examples  X  x and a label regularization function R as input, and outputs an estimated parameter  X   X  . Also, if under distribution D each example x  X  X is associated with exactly one label h  X  ( x )  X  Y , then we write D = D the marginal distribution of D on X . Theorem 2 proves the existence of a true labeling function h  X  such that a nearly tight lower bound holds for all learning al gorithms A and all data distributions D data distributions significantly complicates the analysis , but this generality is important: since D is typically easy to estimate, it is possible that the learni ng algorithm A has been tuned for D proof of Theorem 2 is contained in the supplement.
 Theorem 2. Suppose Assumptions 1, 2 and 3 hold for label regularization function family R , the A and data distributions D 2.
 Obviously, Assumptions 1, 2 and 3 restrict the kinds of label regularization function families to which Theorem 2 can be applied. However, some restriction is necessary in order to prove a mean-ingful lower bound, as Theorem 3 below shows. This theorem st ates that if Assumption 3 does not hold, then it may happen that each family R ( x , y ) has a structure which a clever (but computation-ally infeasible) learning algorithm can exploit to perform much better than the upper bound given in Theorem 1. The proof of Theorem 3, which is contained in the su pplement, constructs an example of such a family. Theorem 3. Suppose the loss function L is 0-1 loss. There exists a label regularization function family R that satisfies Assumptions 1 and 2, but not Assumption 3, and a learning algorithm A such that for all distributions D if (  X  x ,  X  y )  X  X  m then with probability at least 1  X   X  for some R  X  X  (  X  x ,  X  y ) , where  X   X  is the parameter output by A .
 Whenever lim 2 approaches R (  X  y )  X  min are asymptotically matching if the labeler always chooses a label regularization function R such that R (  X  y ) = min Several of the example learning settings described in Secti on 2.1, such as semi-supervised learning and ambiguous learning, meet this criteria. On the other han d, if R (  X  y )  X  min this sense, our framework is best suited to settings in which the information provided by the labeler is equivocal , but not actually untruthful , as it is in the malicious label noise setting [6, 7]. Finally, note that if lim Assumption 3. Given the unlabeled training examples  X  x and label regularization function R , the bounds in Section 3 suggest an obvious learning algorithm: Find a parameter  X   X  that realizes the minimum The objective (1) is simply the minimization of the upper bou nd in Theorem 1, with one difference: for algorithmic convenience, we do not minimize over the set  X  , but instead add the quantity  X  k  X  k 2 to the objective and leave  X  unconstrained (here, and in the rest of the paper, kk denotes L If we assume that  X  = {  X  : k  X  k  X  c } for some c &gt; 0 , then this modification is without loss of generality, since there exists a constant  X  In order to estimate  X   X  , throughout this section we make the following assumption a bout the loss function L and label regularization function R .
 Assumption 4. The loss function L is convex in  X  , and the label regularization function R is convex in q .
 in Sections 2 and 2.1 satisfy Assumption 4.
 Instead of finding  X   X  directly, our approach will be to  X  X wap X  the min and max in (1) , find the abbreviate the function that appears in the objective (1) as F (  X  , q ) , E theoretic minimax theorem in its proof of correctness  X  is gi ven in Algorithm 1; the implementation details for each step are given below Theorem 4.
 Algorithm 1 GAME: Game for Adversarially Missing Evidence 1: Given: Constants  X  1 ,  X  2 &gt; 0 . 2: Find  X  q such that min  X  F (  X  ,  X  q )  X  max q  X   X  m min  X  F (  X  , q )  X   X  1 3: Find  X   X  such that F (  X   X  ,  X  q )  X  min  X  F (  X  ,  X  q ) +  X  2 4: Return: Parameter estimate  X   X  .
 In the first step of Algorithm 1, we modify the objective (1) by swapping the min and max, and then find a soft labeling  X  q that approximately maximizes this modified objective. In th e next step, we labeling  X  q . The next theorem proves that Algorithm 1 produces a good est imate of  X   X  , the minimum of the objective (1). Its proof is in the supplement.
 Theorem 4. The parameter  X   X  output by Algorithm 1 satisfies k  X   X   X   X   X  k X  q 8 We now briefly explain how the steps of Algorithm 1 can be imple mented using off-the-shelf algo-rithms. For concreteness, we focus on an implementation for the loss function L = L also the loss function we use in our experiments in Section 5.
 The second step of Algorithm 1 is the easier one, so we explain it first. In this step, we need to the definition of F , and we see that this minimization amounts to maximizing the likelihood of a log-linear model. This is a very well-studied problem, and t here are numerous efficient methods available for solving it, such as stochastic gradient desce nt.
 The first step of Algorithm 1 is more complicated, as it requir es finding the maximum of a max-min objective. Our approach is to first take the dual of the inn er minimization; after doing this the function to maximize becomes G ( p , q ) , H ( p )  X  1 have max authors; see [15] for more details. Note that G is concave function, and we need to maximize it over simplex constraints. Exponentiated-gradient-style algorithms [16, 15] are well-suited for this kind of problem, as they  X  X atively X  maintain the simplex con straint, and converged quickly in the experiments described in Section 5. We tested our GAME algorithm (Algorithm 1) on several standa rd learning data sets. In all of our experiments, we labeled a fraction of the training examples sets in a non-random manner that was designed to simulate various types of difficult  X  even advers arial  X  labelers.
 Our first set of experiments involved two binary classificati on data sets that belong to a benchmark suite 1 accompanying a widely-used semi-supervied learning book [ 1]: the Columbia object image library (COIL) [17], and a data set of EEG scans of a human subj ect connected to a brain-computer interface (BCI) [18]. For each data set, a training set was fo rmed by randomly sampling a subset of of p  X  [0 , 1] and for each training set, we labeled only the p -fraction of examples with the highest outlier score. In this way, we simulated an  X  X nhelpful X  labe ler who only labels examples that are exceptions to the general rule, thinking (perhaps sincerel y, but erroneously) that this is the most effective use of her effort.
 We tested three algorithms on these data sets: GAME, where R (  X  x ,  X  y ) was chosen to match the [3]; and Transductive SVM [19]. When constructing the Laplac ian matrix and choosing values for 21.2.1 and 21.2.5]. The results of our experiments are given in Figures 1(a) and 1(b). We also tested the GAME algorithm on a multiclass data set, na mely a subset of the Labeled Faces in the Wild data set [20], a standard corpus of face photograp hs. Our subset contained 500 faces of the top 10 characters from the corpus, but with a randomly s kewed distribution, so that some faces appeared more often than others. The feature represen tation for each photograph was PCA (see Section 2.1 for a definition of ambiguous learning). We l abeled trainined examples to simulate a Figure 1: (a) Accuracy vs. fraction of unlabeled data for BCI data set. (b) Accuracy vs. fraction of unlabeled data for COIL data set. (c) Accuracy vs. fraction o f partially labeled data for Faces in the Wild data set. In all plots, error bars represent 1 standard d eviation over 10 trials. label y with respect to their distance, in feature space, from the ce ntroid of the cluster of examples this list. The net effect of this procedure is that examples o n the  X  X order X  of the two clusters are given both labels y and y  X  in the training set. The idea behind this labeling procedure is to mimic a (realistic, in our view) situation where a  X  X azy X  labeler d eclines to commit to one label for those examples that are especially difficult to distinguish.
 We tested the GAME algorithm on this data set, where R (  X  x ,  X  y ) was chosen to match the ambiguous learning setting with a Laplacian regularizer (see Section 2.1). We compared with two algorithms from [9]: UNIFORM, which assumes each label in the ambiguous label set is equally likely, and learns a maximum likelihood log-linear model; and a discrim itive EM algorithm that guesses the true labels, learns the most likely parameter, updates the g uess, and repeats. The results of our experiments are given in Figure 1(c).
 Perhaps the best way to characterize the difference between GAME and the algorithms we compared it to is that the other algorithms are  X  X ptimistic X , by which we mean they assume that the missing labels most likely agree with the estimated parameter, whil e GAME is a  X  X essimistic X  algorithm that, because it was designed for an adverarial setting, ass umes exactly the opposite. The results of our experiments indicate that, for certain labeling styles , as the fraction of fully labeled examples decreases, the GAME algorithm X  X  pessimistic approach is su bstantially more effective. Importantly, Figures 1(a)-(c) show that the GAME algorithm X  X  performanc e advantage is most significant when the number of labeled examples is very small. Semi-supervis ed learning algorithms are often pro-moted as being able to learn from only a handful of labeled exa mples. Our results show that this ability may be quite sensitive to how these examples are labe led. Our framework lends itself to several natural extensions. F or example, it can be straightforwardly extended to the structured prediction setting [21], in which both examples and labels have some internal structure, such as sequences or trees. One can show that both steps of the GAME algorithm both the loss function and label regularization function de compose appropriately over the structure. of successively more informative label regularization fun ctions, with the aim of extracting the most interesting to design Amazon Mechanical Turk experiments t hat test whether the  X  X nhelpful X  and assumptions we introduced in Section 3 to aid our analysis, w e only proved (in Theorem 3) that one of them is necessary. We would like to determine whether the o ther assumptions are necessary as well, or can be relaxed.
 Acknowledgements Umar Syed was partially supported by DARPA CSSG 2009 Award. B en Taskar was partially sup-ported by DARPA CSSG 2009 Award and the ONR 2010 Young Investi gator Award.
