 Internet display advertising is a critical revenue source for publishers and online content providers, and is supported by massive amounts of user and publisher data. Targeting dis-play ads can be improved substantially with machine learn-ing methods, but building many models on massive data becomes prohibitively expensive computationally. This pa-per presents a combination of strategies, deployed by the online advertising firm Dstillery, for learning many models from extremely high-dimensional data e ciently and with-out human intervention. This combination includes: (i) A method for simple-yet-e  X  ective transfer learning where a model learned from data that is relatively abundant and cheap is taken as a prior for Bayesian logistic regression trained with stochastic gradient descent (SGD) from the more expensive target data. (ii) A new update rule for au-tomatic learning rate adaptation, to support learning from sparse, high-dimensional data, as well as the integration with adaptive regularization. We present an experimental anal-ysis across 100 di  X  erent ad campaigns, showing that the transfer learning indeed improves performance across a large number of them, especially at the start of the campaigns. The combined  X  X ands-free X  method needs no fiddling with the SGD learning rate, and we show that it is just as e  X  ec-tive as using expensive grid search to set the regularization parameter for each campaign.
 I.5.4 [ Computing Methodologies ]: [Pattern Recognition -applications] Transfer Learning; Stochastic Gradient Descent; Adaptive Learning Rates; Adaptive Regularization
Online display advertising, served through real-time bid-ding systems, is a major source of revenue for web publishers. As media consumption continues to shift to web-hosted con-tent, advertising dollars continue to pour into the system. Global display advertising revenues exceed 40 billion USD annually are growing at double-digit rates [7]. Real-time bidding systems are run by ad exchanges : auction platforms connecting sellers of ad placements (usually web publishers with ad space to monetize) and buyers (usually indepen-dent firms like Dstillery, 1 operating on behalf of consumer brands and their agencies). The goals of the buyers vary. At Dstillery, our ultimate goal is to identify the best pos-sible audience for each of our individual advertising clients and advertise to this audience. What exactly  X  X est X  means depends upon the goals of the advertiser, but usually we optimize some form of post-view conversion rate . The defi-nition of conversion is again campaign-specific, but always requires taking some action (such as visiting a site, buying a product, or signing up for a service). The post-view qualifier means the conversion is observed in some interval following an ad exposure but without the necessity of an ad click. The data that we use for modeling is discussed in detail in Section 2, but the foundation of our targeting methodology is a suite of methods for high-dimensional, sparse classifica-tion (ranking) that builds multiple models for each advertis-ing campaign individually. Training a single large-scale pre-dictive model is no longer a rare feat. Our machine learning system trains, deploys, and continually retrains thousands of predictive models simultaneously. The performance re-quirements of display advertising customers and the need to operate many models simultaneously pose a number of tech-nical challenges, which when combined motivate an e cient  X  X ands-free X  operation X  X obust performance with minimal intervention by the data science team. The specific chal-lenges we consider are: 1. Cold Start : Targeting must perform well from the 2. Non-Stationarity : Human browsing behavior, the 3. Consistency : Since we restrict ourselves from pooling
Formerly Media6degrees 4. Scale and Robustness : The system builds and main-
Our current system addresses these issues with a number of di  X  erent components, amply described in prior work (see [11] and the citations therein). The current paper explores an alternative, novel solution to the same goal by tightly in-tegrating transfer learning [10] and scalable predictive mod-eling. Recent developments in adaptive learning-rate sched-ules [14] and adaptive regularization [13] allow for incre-mental training of linear models in millions of dimensions without exhaustive hyper-parameter searching. Our com-plications are (i) the need of transfer learning to support cold start and few available positive examples, and (ii) the application to ultra-high-dimensional modeling. This work extends the prior work first by incorporating the transfer learning steps directly into the large-scale linear models us-ing Bayesian logistic regression (section 3) and second by combining and extending state-of-the-art methods to en-able hands-free, scalable stochastic gradient descent (SGD, Section 4). The combination of these methods demonstra-bly improves campaign performance, especially in the early stages of the campaign, and this improvement is achievable within the throughput limitations of the system. We will dis-cuss more specific related work in the relevant sections. We are not aware of a prior paper that describes and evaluates a production-ready combination of current state-of-the-art learning methods for e cient, hands-free transfer learning of large numbers of predictive models from extremely sparse data.

Specifically, the contributions of this paper are: 1. We develop a transfer learning system based on these 2. We derive a new update rule for the adaptive learning-3. We provide an algorithm for using both adaptive learn-4. We use production data to show the e  X  ect of transfer
The remainder of the paper shows how these and other developments in linear modeling can be combined into a con-tinuously updating, hyperparameter-agnostic ad-targeting system that transitions from learning on a proxy population ( all conversions) to the true population ( post-impression conversions) as more data becomes available. Our results show that this strategy is indeed e  X  ective for improving campaign performance, especially in the early stages of the campaign, and this improvement is more scalable than al-ternative methods.
The data which informs our proprietary ad-targeting mod-els comes primarily from multiple continuous data streams. One stream records general browsing events of users as they consume content across the web. Another similar stream records interactions between users and the brands we rep-resent. These interactions include home page visits and conversions on their owned websites. The last data stream we consider is the actual ad serving process. We serve ads to users based on various criteria and following the ads we record whether or not these users convert. From these data streams we can construct di  X  erent but related classification (ranking) tasks.

Our most obvious data asset for campaign optimization is the campaign data itself. If we want to draw training data directly from the target distribution, we can randomly tar-get users with impressions, subsequently labeling a user as positive if she converts soon after exposure to the ad and negative otherwise. However, this poses serious challenges. Serving ads randomly at a necessary scale X  X o avoid the se-lection bias inherent in training on data resulting from tar-geted impressions X  X s something our stakeholders are often reluctant to do. Moreover, conversion rates are generally quite low (often 10 6 or less), which leads to the need to serve massive number of randomly targeted ads just to ob-tain a moderate number of conversions for training.
This paper focuses on a method to leverage other data sources for cheap and reliable  X  X djunct X  training data. The Bayesian transfer learning approach provides a principled method for combining these data with a smaller amount of randomly sampled post-impression data.

Before a client initiates an advertising campaign, we place tracking pixels on various parts of the client X  X  web site. Vis-its to these pixeled pages get logged (anonymously) for anal-ysis and optimization. In parallel, we collect web browsing data on these same anonymous users from data partners across the web. This data is stored in our system as a set of anonymized (hashed) URL strings linked to a user X  X  cookie. These two mechanisms, which are independent of actually serving ads, when combined give us both features and labels that we can use to build the adjunct training data. Specifi-cally, we build training instances in a backward looking fash-ion. For a given period of time, we select all pixel events coming from the client X  X  website and label the correspond-ing users as positive instances. We then select a random subset of users that were active in the same period but did not incur a pixel event and label them as negative instances. This design choice is very practical: while post-impression conversion events are typically rare, general visits or conver-sions on the client X  X  website are orders of magnitude more abundant.

Thus we have two data sets for each campaign X  X ne drawn from the target application X  X  distribution, but which com-prises relatively few positive instances, and another that is naturally more abundant and cheaper, but doesn X  X  exactly represent the problem we are trying to solve. The challenge addressed in this paper is to use these data to build models to estimate the relative likelihood that a previously unseen user will convert after seeing an ad. The rest of the paper describes and evaluates a Bayesian transfer learning frame-work implemented to achieve this, as well as the modern machine learning methods we have combined and enhanced to do so at scale.
Our goal, in a nutshell, is to improve the performance of conversion-rate modeling at all points of the campaign. We do this by first initializing our model with a learned prior and then incrementally improve the prediction task by incorpo-rating post-impression conversions as they become available. Prior work uses the term transfer learning [10] to refer to any situation where a model learned on one classification task (called the source task) is used to improve a predictive model applied to a di  X  erent task, the target task. We adopt the language and notation from [10] to present our version of transfer learning more formally.

Formally, a classification task { , P ( X ) , ,P ( Y | X ) } is com-posed of a feature space , a probability distribution P ( X ), where X = { x 1 ,x 2 ,...,x d } 2 is a feature vector, a la-bel space , where Y 2 is the outcome of interest and lastly, a corresponding objective function P ( Y | X ). In our particular situation, we refer to the campaign data as the target and the auxiliary data described in section 2 as the source . We generally assume that the source and target tasks share the same feature space but generally assume that P S ( X ) 6 = P T ( X ) for all X . 2 We also have di  X  erent out-come spaces for both data sets, though they are related. In our domain S represents conversion labels derived from observing general tra c on the client X  X  website whereas T is derived from post-impression conversions.

Our main estimation problem is to learn P T ( Y T | X ) from the target data. We do this by first learning P S ( Y S | X ) from the source data and then use that estimator as a prior on our estimator for P T ( Y T | X ). We generally assume that P
T ( Y T | X ) 6 = P T ( Y S | X ), but that the structure of the esti-mators (specifically the weight vectors from linear models) are similar and that is why we can use the source model as a prior on the target.

To put this more formally, consider a logistic regression model for a data set D S = ( X 1 ,y S 1 ) ,..., ( X N ,y S use the standard log-likelihood formulation and denote the sample logistic loss function as where p t ( )=(1+ e X t ) 1 . Again we use superscript S and T to di  X  erentiate between the source and target data. Given the source data, we define what we call the source model: where r (  X  ) is a suitable regularization function and S regularization weight.
When referring to constructs within target or source data, we use the superscripts S and T to di  X  erentiate
We use  X  in this equation instead of to distinguish it as the initialization model learned on source data
We expect that the source model is relatively e  X  ective for identifying a high-performance audience even though the training data are not drawn from the  X  X roper X  (target) dis-tribution. We want a model that will rank highly a set of users who are likely to convert following the presentation of an ad. The source model instead identifies users who are similar to a client X  X  customer set with respect to their web browsing history, independent of the advertising process.
We can directly model the target task by first creating an appropriate dataset. Let the target data set D T = { ( X 1 ... ( X N ,y T N ) } , where X t is defined as above and y as a post-impression conversion. In order to induce transfer learning from P S ( y S | X )to P T ( y T | X ), we add a non-zero Gaussian or Laplace prior on the logistic regression coe -cients that estimate P T ( y T | X ) . Our resulting optimization problem then looks like: where again we are using the standard logistic loss. The dif-ferences between equations (2) and (3) are that in (3) we are training over the target data and have modified the regular-ization function in (3) to incorporate a non-zero informative prior. This form of knowledge transfer is highly scalable, because although we do incur the additional cost of learning  X  , the cost of learning with equation 3 is no more than had we used a zero-mean prior. This is a major benefit in sup-port of this method over more complicated transfer learning methods [10] and a motivating factor for putting it to use in our system.
 Incorporating a source model for transfer learning as a Bayesian prior is not new. In a text-mining application, Chelba et al [5] successfully incorporate a prior model as a Bayesian prior into a maximum entropy Markov model to recover the correct capitalization of uniformly-cased text. Similarly, Arnold et al [2] use the hierarchical structure of features to learn informative priors over features to use in training conditional random fields. For transfer learning with logistic regression, Raina et al [12] perform text clas-sification using a learned Gaussian prior. However, the for-mulation is quite di  X  erent; we view our formulation as much more straightforward (they focus on the prior variance). In advertising applications, Chapelle et al [4] use informative priors in logistic regression models but not explicitly as a transfer learning mechanism. In their application they build models incrementally and use the previous iteration X  X  model as the prior. We are not aware of any prior work that ex-plicitly uses Bayesian transfer learning in a large-scale pro-duction application.
Recall that our goal is twofold. As described above, we would like to take advantage of both the plentiful source data and the expensive-but-more-appropriate target data. We also would like to do this e ciently with minimal human curation. A popular and e  X  ective algorithm for building lin-ear models with large-yet-sparse, data is Stochastic Gradient Descent (SGD) [4, 9, 15]. The main drawback of SGD is that it includes various hyper-parameters (learning rates, regular-ization parameters, etc.) that need to be carefully tuned in order to achieve good performance. Parameter tuning ulti-mately complicates and lengthens the model training phase. Firms that need to build hundreds or thousands of models simultaneously, in production, face a daunting task setting these hyper-parameters separately for each campaign.
To enable building many models with few data scientists, and to reduce the training load on our system, we have de-veloped a  X  X ands-free X  approach that does not require ei-ther manual parameter setting or expensive grid-search. The methods we present here are well suited for large and sparse consumer behavior data and they integrate well with the transfer-learning described above. In particular, we present a hyper-parameter-free SGD logistic regression training al-gorithm based on combining and enhancing two recent de-velopments: the adaptive learning rates [14] (which we call NoPesky ) and adaptive regularization [13].

We made several enhancements to make NoPesky learn-ing rates and adaptive regularization work together and ef-ficiently in the current context. Specifically, we adapted the NoPesky learning rate method so that we can 1) take advan-tage of the sparse structure of the data to run e cient SGD updates, 2) explicitly integrate the regularization term in order to make NoPesky work with adaptive regularization, and 3) give the NoPesky algorithm a more robust starting point when positive samples are rare.

It is helpful to start by considering the update step of a standard SGD algorithm: at time t , we observe a new training sample with per-sample loss where comprises the model coe cients; ` t is the per-sample logistic loss function as defined in Section 3; is the regularization factor, and r is the regularization function having the form r (  X  )= 1 2 the average loss function by f = 1 N logistic loss function by ` = 1 N sample, and for each dimension i of the model, we calculate coe cient via where  X  t,i is the learning rate at time t for dimension i . Because our algorithm works with both source and target models, we drop the superscripts S and T throughout this section.
The NoPesky learning rate method sets the SGD learning rates in a greedy manner by choosing a rate that minimizes the expected value of the average loss function f after each SGD update. It is generally impossible to solve this mini-mization problem exactly, but an approximate solution can be obtained and works well in practice. The method utilizes the second-order derivatives of the per-sample loss function, making it suitable for learning models with a smooth loss function, such as linear and logistic regression. Our experi-ments validate prior research [14], showing that SGD algo-rithms trained with the NoPesky learning rates perform as
Our algorithm uses di  X  erent learning rates for di  X  erent dimensions, rather than a global learning rate for all di-mensions. Multiple studies suggest that dimension-specific learning rates yield superior performance to global learning rates, particularly for data sets with sparse features like ours [6, 9]. well as state-of-the-art adaptive learning rate schedules such as AdaGrad [6]. The di  X  erence is that NoPesky learning re-quire no systematic parameter tuning and thus can promote faster learning.

We first sketch the core steps of the original NoPesky learning rate method that are necessary for presenting our adaptations. The method treats the stochastic gradient r t,i as a random variable, and chooses a learning rate to min-imize the expected regularized loss after one SGD update, that is: where e i is a unit vector in dimension i . In order to solve the above minimization problem, three approximations are made. First, the average loss function f is approximated by a quadratic function in the neighborhood of t . This is achieved by approximating its first-order derivative by g t,i  X  E [ r t,i ], and the second-order derivative by h t,i E [ approximated by g t,i  X  E [ r t,i ]. And lastly, the second-order moment of r t,i is approximated by v t,i  X  E [ r 2 t,i One can then solve (6) and obtain an approximate solution, which is set as the per-sample, dimension specific learning rate. The three approximate variables are calculated as running averages where  X  t,i is an averaging window, that can be heuristically set by  X  1 ,i =1and The use of a dynamic averaging window determined by equa-tion (9) to update equation (8) enables this method to adapt to non-stationary data. When the data distributions change,  X  is automatically reset to a lower number which increases the learning rate and enables more exploration.

To work within our system, we have made the following enhancements to the NoPesky method.

As discussed above, online webpage visitation data is ex-tremely sparse. It is desirable to take advantage of this ex-treme sparsity to speed up the SGD updates X  X pecifically, we would like to enhance the NoPesky method such that at step t , we only update dimension i if x t,i 6 =0.
In regularized logistic regression, the per-sample loss func-tion has two components: the logistic loss function and the regularization function. The logistic loss function ` t (de-fined in (1)) has a convenient property that its gradient in dimension i is 0 if x t,i = 0, thus allowing for sparse updates. Unfortunately, the regularization term 2 ally does not have zero gradients. In fact, the regularization term is independent of x t and has non-zero gradient in di-mension i as long as t,i 6 =  X  i . This regularization term prevents us from running e cient sparse updates.

Our solution is to rewrite the per-sample loss function as the following: here N is the total number of training samples, and N i is the total number of times that dimension i is present in a sample. We essentially reassigned the regularization term to the per-sample loss function. Instead of assign-ing equal regularization strength to every data sample, we choose to only regularize a dimension when that dimension is present, and correspondingly increase the regularization strength of that dimension in inverse proportion to the num-ber of samples that have this dimension. In so doing, we have kept the total regularization strength unchanged, i.e.,
Our learning method operates on the rewritten per-sample loss function  X  f t instead of f t .Atstep t , for dimension i such updates on these dimensions and only run updates (7) (8) (9) on dimensions i with x t,i 6 =0.

The original NoPesky method doesn X  X  explicitly handle regularization terms, e  X  ectively putting the regularization as part of the loss function. Hence the regularization term also contributes to the running estimates g t,i , v t,i and h t,i inhibits the integration of NoPesky with adaptive regularization X  we would like it to be capable of handling changing regular-ization factors.

Fortunately, since the regularization term is deterministic and can be calculated precisely, we can exclude them from the calculation of the running estimates. Thus, instead of (8), we have We can then modify our three approximations to , E " larization factor ( t comes from adaptive regularization be-low). This leads us to  X  Algorithm 1 Hyper-parameter Free Learning Input :  X  init , e ,  X  , 0
Data variables : i ,  X  g i ,  X  v i ,  X  h i 0,  X  i 1, 0 for t =1to N do end for Finally, we address one more limitation to the original NoPesky method, when applied to our context. The NoPesky method relies on a number of approximations that get in-creasingly better as it runs more updates. Unfortunately these approximations can be inaccurate in the initial stages of the training when there are few updates and the coe -cients are far from the optimal solution. The original paper [14] proposed a slow-start method to process the first 0.1% of the data with a small constant learning rate in order to obtain a stable estimate of the running averages. While this method works well in their setting, where data have dense rows, it doesn X  X  work well with data having sparse rows be-cause many dimensions might not show up at all in the first 0.1% of the data samples!
Thus, we introduce an alternative slow-start method. We use a small learning rate (  X  ) for updates in a particular di-mension until we have seen at least  X  init samples having that dimension . E  X  ectively we use a fixed learning rate SGD al-gorithm for the first  X  init updates in a dimension, and only after that do we start to engage the NoPesky method. We set  X  init = 100 and e =10 6 and use this for all of our experiments 5 .
A common strategy for selecting optimal regularization weights is to perform a grid search: train multiple models using di  X  erent regularization factors, and select the one that yields the best performance on the validation data. As dis-
While the slow start mechanism does in fact create hyper-parameters, we found that the same settings work for a large class of similar problems (i.e., all sparse campaign data sets) and no tuning is needed once an appropriate configuration is found for that class cussed above, this is time-consuming and prohibitive when training very large numbers of models.
 Adaptive regularization [13] o  X  ers an alternative solution. It adapts the regularization factor iteratively on the vali-dation data while learning the model on the training data. Adaptive regularization works with L 2 -regularized models and can be integrated into an SGD algorithm. Instead of training multiple models with di  X  erent but fixed regulariza-tion factors, with adaptive regularization we train one model with regularization factors that change over time. This is achieved by running a gradient descent algorithm on the regularization weight.

Adaptive regularization works as follows: alternatively draw one sample from the training data and one sample from the validation data then run the SGD update for the training sample. Let ` val t be the logistic loss function of the validation sample. Adaptive regularization then runs the following update where  X  is a fixed learning rate. The max is necessary be-cause we would like to keep the regularization factor always a non-negative quantity.

There are a few undesirable features of the update in (12). First, it is di cult to choose a learning rate  X  that works well across multiple magnitudes of the regularization factor. Second, the need to truncate at 0 in (12) seems artificial. Finally, the dimension of the update equation doesn X  X  match, because the dimension of @` @ equals the dimension of ` , which is a logistic loss function and hence dimensionless. But is not dimensionless and has dimension (dimension of 2 ) 1 based on (4).

We fix the above issues by slightly modifying the update equation to By inserting t to the gradient term, we restored the di-mension homogeneity of the update formula. We found this modified update formula performs more robustly in practice. We illustrate adaptive regularization at work in Figure 1. In this example, we take a one-day sample from our training data stream (the source data defined in the previous section) and perform several training runs, with each one having a di  X  erent initial starting point for the regularization factor. The plot shows the path that takes as the optimization sees more data. We see that all paths converge to the same factor, though the speed to convergence is sensitive to the initial starting point.

In practice, we set the fixed learning rate  X  =0 . 001. Be-cause we will perform many updates on the regularization factor, and the model changes slowly around the optimal regularization factor, the performance of our model is not sensitive to the choice of this learning rate.
In this section we demonstrate empirically the improve-ments conferred by transfer learning, as well as the robust-Figure 1: Adaptive regularization with dimension-homogeneous update formula (13) converges to the same regularization factor starting from a wide range of initial values. ness of the  X  X ands-free X  design. The analysis is conducted experimentally across 100 di  X  erent campaigns, across a wide variety of advertisers from di  X  erent industries, with di  X  er-ent base rates, and with di  X  erent inherent predictability. A quick summary goes as follows: 1) using the transfer learn-ing from section 3 achieves better results than not using transfer learning, and 2). doing this  X  X ands-free X  via the methods detailed in section 4 does not perform much worse than doing a full-blown grid search that is (at least) an order of magnitude less e cient. In all of these experiments, we report results on experiments conducted on data from indi-vidual campaigns, either in full detail or in aggregate. In every case, the source and target data sets used are specific to that particular campaign (as described above).
Our first set of experiments was conducted on data from a single time frame spanning two time periods. We chose 100 advertisers and for each advertiser created 3 data sets: (1) a source data set D S 0 in period  X  0 , (2) a target data set D in period  X  0 , and (3) a target data set D T 1 in period  X  this scenario periods  X  0 and  X  1 are disjoint but consecutive, and  X  0 and  X  1 span fourteen and seven days, respectively.
For this set of experiments we use a 2x2 factorial design for examining the modeling. One experimental factor is whether or not we use transfer learning. For the  X  X rans-fer Learning X  variant we first learn a weight vector  X   X  s using equation 2 on the source data D S 0 . We then use D T 0 , and equation 3 to learn a source-domain-adapted model on the target data. For the  X  X o Transfer X  (control) variant, we again use D T 0 and equation 3 but without the informative prior  X   X  s . The second design factor represents how we tune the hyper-parameters when training the models. For the  X  X daptive X  variant, we use the adaptive regularization with the NoPesky learning rate schedule. For the  X  X rid Search X  variant we use the AdaGrad learning rate formula and speci-fiy a fixed regularization weight. We search over a 4x9 grid of values, where we use all powers of ten between 10 0 and 10 3 for the initial AdaGrad learning rate and 10 2 to 10 for the regularization weight. In all training scenarios we select hyper-parameters by splitting the training data into training and validation sets. We choose the hyper-parameter configuration that performs best on the validation set and then retrain on the entire training set using the selected hyper-parameters.

Figure 2 shows 6 the results of multiple tests on the dif-ferent training scenarios. In figure 2 (a) we first establish that transfer learning improves performance under tradi-tional hyper-parameter selection methods. In both axes, the models were trained using our Grid Search variant and the only di  X  erence is that transfer learning was used to produce the results in the vertical axis. Overall, we see that 91% of points fall above the identity line, meaning transfer learn-ing improves performance for an overwhelming majority of cases.

In figure 2 (b) we hold the transfer learning variant con-stant and test whether our adaptive hyper-parameter se-lection strategies fare better or worse than traditional grid search. Overall we see a balanced and tight scatter around the identity line, with the exception of two campaigns. This suggests that in the context of our transfer learning strat-egy, adaptive hyper-parameter search is as e  X  ective as more traditional grid search methods. This is a powerful result, as it suggests we can indeed have a  X  X ands-free X  approach to training our models while keeping the training costs low. Our grid search optimization required 36 runs through the training data whereas the adaptive approach only takes one. In a more realistic setting, we may be able to reduce the grid space, but even then, we X  X  expect to still require 5-10 train-ing runs to find an optimum. Thus, the adaptive approach significantly speeds up our ability to build transfer-enhanced models.

In figure 2 (c) we show the variants of transfer learning with adaptive hyper-parameter search against no transfer with grid search. This is our closest comparison between
Note that all 3 plots in figure 2 evaluate generalization performance using lift. Using AUC instead shows the same results qualitatively. See Table 1 for summary results. our proposed approach and what might be done without access to the source data. We see that 2 (a) and 2 (c) are qualitatively similar, with the di  X  erence being that (c) can be achieved up to 10 times faster because of the adaptive learning.

The results of these three comparisons are summarized in table 1. In each row we report the mean of di  X  erences when training according to the strategies specified in the first col-umn. For AUC we measure the di  X  erence between the two strategies and for Lift we measure the ratio between the two. In our experimental design, we set our null hypothesis to be that using one variant produces no di  X  erence in performance over the other variant. We report the mean AUC di  X  erence and Lift ratio, in the 2nd and 5th columns, respectively (we take 1 st 2 nd for AUC and 1 st / 2 nd for lift). We also report the p-Values using a paired t-test on these statistics. If we think about these methods as global strategies for all cam-paigns, we can conclude with strong confidence that transfer learning improves campaigns on average and adaptive learn-ing does not hurt.
In this set of experiments we explore a scenario that re-flects an additional aspect of production training and model use at a firm like Dstillery. In particular, data arrives incre-mentally and continually from the moment a new campaign is initiated until it runs its course, and (hopefully) is replaced by a new campaign.

Since SGD trains via incremental updates, it can run naturally as an incremental learning system. For a given campaign, at any point in time we have a targeting model that has been trained using all data available up until that point; this model will then be used to target the follow-ing time period X  X  ads. We simulate this process by cre-ating daily samples from the advertising system for each campaign, and then incrementally updating the campaign X  X  model using some experimental learning-algorithm variant. performance.
 or Control B variants to the Control A variant.
 We then evaluate the resultant model on the following day X  X  sample.

We explore three variants in this set of experiments: (1)  X  X ransfer Learning X   X  using equation 3 with  X   X  s derived from the source data; 7 (2)  X  X ontrol A X   X  equation 3 with  X   X  s =0 (the standard regularization case), and (3)  X  X ontrol B X   X  equation 3 with  X   X  s specified as the prior day X  X  model. This final X  X ontrol B X  X ariant is the methodology used by [4], and
In this set of experiments we learn one estimate of  X   X  s for each campaign and hold that fixed throughout the 60 days of training. is the method most similar to our work in design. 8 For each variant, on each day, we train on just the given day X  X  data using the chosen prior with adaptive regularization and op-timize using SGD with the NoPesky adaptive learning rates. When performing incremental updates using SGD, we found that performance generally improves when each day X  X  model is initialized with the prior day X  X  model and that the state of all parameters used to compute the learning rate are per-sisted and carried over to the next day (let X  X  call this the  X  X arm start X ).
The work by [4] was not explicitly attempting knowledge transfer, but can be interpreted as an instance of Bayesian transfer, like ours except transferring across time periods. combined.

Figure 3 shows aggregated results of running incremen-tal training with daily performance evaluation across 30 randomly sampled campaigns from our production system (sampled with the one constraint that the target data had at least 5 post-impression conversions for each day of anal-ysis). In the top chart we show the average relative Lift ratio between the variants Transfer Learning and Control B over Control A. In the bottom chart we show the av-erage di  X  erence in AUC. Thus, in each chart a black dot above the reference line means that Transfer Learning out-performed Control A for that time period. An X  X  X  X bove the line means that Control B outperformed Control A for that time period. A black dot above an  X  X  X  means that Trans-fer Learning outperformed Control B for that time period. And vice versa. From these results we can see multiple ef-fects: (1) when comparing to Control A, the postive e  X  ect of transfer learning wears o  X  over time; (2) when comparing to Control A, the transfer learning benefit is more pronounced for AUC than for Lift, although for both metrics there is a general benefit, and (3) Control B tends to underperform both variants, and this is more dramatic in AUC.

The first trend mentioned above is something we ought to expect -as more data is introduced into the system, and because the models do have some form of a memory built into them, we should expect the weights learned just with the target data will start to converge toward an optimal point. The second trend can possibly be explained by dif-ferent properties of the metrics themselves. In general, Lift can be a very high variance metric in data scenarios with a low absolute number of positive outcomes. This variance comes from both (i) the model, because it is more di cult to fit the tails of the distribution, and (ii) the evaluation itself: since we are updating the models one day at a time, we have less target data per model-building and evaluation step and this increases the expected variance. As a policy, we always review both AUC and Lift, even though Lift is the more appropriate metric.

The third trend mentioned above might be the most un-expected (to us at least), and the sub-optimality of Control B warranted additional analysis. Figure 5 shows the aver-age L2-norm of the weight vectors learned for each variant for each day. The general trend we see is that the average Figure 5: Average L2-norms for the models aggre-gated and reported in figure 3 norm of weight vectors trained using the Control B variant is bigger than that of the other two. This trend is likely an artifact of regularizing towards existing models. The issue is two-fold and manifests from the fact that the regulariza-tion prior is a moving target. Over time, the weight vector gets closer to an optimal solution and this often coincides with an increasing norm. Eventually the process begins to overfit the data and the regularization is too restrictive to let the model escape from a specific solution. Also, this reg-ularization strategy makes it di cult to escape optimization valleys in non-stationary data. The NoPesky learning rate schedule has a built-in method for adapting to changing data distributions (i.e., by  X  X orgetting X  most of the past), but for-getting the past is di cult when the regularizer forces the model to remember it. Thus, the Transfer Learning prior might be better in our application because it doesn X  X  in-crease over time and is biased enough to avoid overfitting. It is anchored close enough to a specific day X  X  optimal so-lution to be useful but not too close that it over-fits in a generally noisy learning environment.
As additional, auxiliary support that the hands-free trans-fer method works well, we present its actual production per-formance. In the experiments above, we took care to control for the di  X  erent factors that a  X  ect performance on a hold-out set so that we could isolate the di  X  erent e  X  ects of our design choices. Once released  X  X nto the wild X  there are many elements that prevent a pure apples-to-apples comparison (when not doing a pure A/B test on them). Nonetheless, we find in particular that our implementation of hands-free transfer learning is in aggregate our best-performing learn-ing method.

Figure 4 shows a screen shot taken from our internal KPI dashboard. We benchmark every targeting algorithm for ev-ery campaign against a default baseline that runs as part of the campaign. We normalize the conversion rates of each algorithm in each campaign against its respective baseline and call this our  X  X ift KPI. X  Figure 4 here shows the median Lift KPI across all campaigns for the fourth quarter of 2013. The top line (called  X  X 99 X  9 ) represents our transfer learning algorithms in action. The bottom line (called  X  X N-ALL X ) is a blend of all our algorithms combined. While this  X  X 99 X  group might not perform best for every campaign, in general it is our best and demonstrably performs well above aver-age across all campaigns. Many of the fluctuations in rela-tive performance between these two lines are driven by the internal campaign allocation optimizations of the system. Each campaign starts with a set of competing algorithms. As we get live feedback we reallocate impressions to the bet-ter performing algorithms until the algorithm X  X  performance converges to the mean of the campaign. Thus, as with pro-duction numbers generally, these should not be taken as an exact comparison between algorithms under identical condi-tions.
We have presented a set of techniques that when applied together present a robust and scalable solution to several challenges often encountered in online display advertising. Our transfer learning approach shares intuition with prior work [1], [8], in that one can look outside of the target data set for sources of signal that can be passed to the target problem. An advantage to our formulation is that (in retro-spect) it is very straightforward, which is a virtue when de-signing and implementing large-scale, completely automated machine learning systems. Our focus here is on producing a scalable, accurate, and robust system. We have achieved that through the combination of Bayesian transfer learning and the extension and integration of methods for adaptive, parameter-free learning. Looking forward, adaptive regular-ization is a recently developed strategy and more research needs to be done to understand its performance bounds and general limitations. On the transfer learning side, the e  X  ec-tiveness of the strategy depends on specifying an appropriate prior. This work presents a straightforward mechanism for how to transfer; more sophisticated methods could increase the advantages from inductive transfer. Additionally, within this framework, understanding when to transfer is necessary for developing a fully robust solution.
We often use obscure terminology to refer to our internal products [1] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota. [2] A. Arnold, R. Nallapati, and W. W. Cohen. Exploiting [3] M. Broadie, A. Zeevi, and D. Cicek. Multidimensional [4] O. Chapelle, E. Manavoglu, and R. Rosales. Simple [5] C. Chelba and A. Acero. Adaptation of maximum [6] J. Duchi, E. Hazan, and Y. Singer. Adaptive [7] Interactive Advertising Bureau. Q3 2013 earnings [8] Y. Liu, S. Pandey, D. Agarwal, and V. Josifovski. [9] H. B. McMahan, G. Holt, D. Sculley, M. Young, [10] S. J. Pan and Q. Yang. A survey on transfer learning. [11] C. Perlich, B. Dalessandro, T. Raeder, O. Stitelman, [12] R. Raina, A. Y. Ng, and D. Koller. Constructing [13] S. Rendle. Learning recommender systems with [14] T. Schaul, S. Zhang, and Y. LeCun. No more pesky [15] W. Xu. Towards optimal one pass large scale learning [16] M. D. Zeiler. ADADELTA: An Adaptive Learning
