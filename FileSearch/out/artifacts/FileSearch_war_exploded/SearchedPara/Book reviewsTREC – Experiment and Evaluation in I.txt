 TREC  X  Experiment and Evaluation in Information Retrieval. Ellen M. Voorhees, Donna K. Harman (Eds.), The MIT Press, Cambridge, MA (2005). 462 pp., ISBN: 0-262-22073-3
The success of Information Retrieval (IR) is often attributed to the interplay of theory, practice and experi-ment. Unlike many other disciplines, IR has a rich tradition of experimental work at its core. New theoretical models for information representation, techniques for relevance estimation, cognitive models of interaction, and ideas for user interface design are thoroughly tested by the research community before they can gain acceptance.

The two main approaches to IR, systemic or algorithmic on the one side, and cognitive or user-centered on the other, have determined the approaches to evaluation and experimentation. The problem addressed by the systemic camp is:  X  X  X ssuming that we know what the searcher wants, find the most relevant documents X  X . Therefore, the evaluation is centered on testing of indexing, weighting and matching models, based on the re-trieval effectiveness that they yield for a well-specified information need.

The cognitive camp addresses the complementary problem:  X  X  X hat are the best ways to support the infor-mation seeker in clarifying, refining and formulating an information need, and how can the exploration of the information need best be assisted? X  X . Therefore, the evaluation is based on task-oriented user studies.
While people X  X  different backgrounds and interests usually put them in one camp or the other, it is widely accepted that the two approaches are complementary and that both types of evaluation are necessary. TREC falls in the former category of systemic approaches.

Despite its name, TREC (Text Retrieval Conference) is more than a conference. It can be described as a hands-on workshop on IR evaluation, or as a workbench exercise for testing new models and algorithms. Although with some concessions made to researchers interested in user-centered task-oriented, TREC is steeped in the  X  X  X ranfield paradigm X  X  of systemic laboratory experimentation: the human searchers are simu-lated via recorded expressions of information need (or  X  X  X opics X  X ) and via relevance judgments produced by information analysts, indicating if the retrieved documents are relevant or not to the topics. Various assump-tions are made in order to provide a context, according to the real-world tasks that are simulated.
The TREC experiment is organized annually by the National Institute for Standards and Technology (NIST) and its annual proceedings are available online (http://trec.nist.gov). They contain not only the papers submitted by the participants, but also overviews of the different tracks, each dealing with a certain kind of task, and overview of the conference.
 While not bringing new content or new results,  X  X  X REC  X  Experiment and Evaluation in Information Retrieval X  X , edited by Ellen Voorhees and Donna Harman, who have been running the workshop since its incep-tion, is a welcome volume that summarizes results from over 10 years of experimentation, and attempts to distill the lessons learned. It follows several special issues of the  X  X  X nformation Processing and Management X  X  (IP&amp;M) journal which reviewed TREC work: volume 31, number 3, 1995, focused on TREC-2, volume 36, number 1, 2000 focused on TREC-6, while volume 37, number 3, 2001, focused on the interactive retrieval experiments in TREC. Compared to these journal issues, the book is much more substantial, covering all the annual events to date, as well as all the main tracks, and the most important contributions over the years.

The book has three parts. The first part,  X  X  X ntroduction X  X , contains three chapters that should be read by anyone unfamiliar with TREC, before reading the rest of the book, as the other chapters make frequent ref-erences to them. They describe the TREC experimental framework in a historic perspective, describe the test collections and the procedure to build pools of judged documents, and justify the experimental design, the evaluation methodology and the measures used to estimate success. Once the reader has a good grasp of the introductory part, the rest of the chapters can be read in any order; they are written by different contrib-utors and are quite independent of each other.
 experimental design details employed in various tracks of TREC, summarize the results and discuss the knowl-edge accumulated in these experiments. Here are the tracks covered:  X  The ad-hoc track was the prototypical document retrieval task: given a static collection of documents, rel- X  The routing task dealt with the complementary situation: given a stream of documents, route each of them  X  The interactive track explicitly studied the human searchers X  interaction with the system, and investigated  X  Dealing with languages other than English started in tracks considering documents and topics in Spanish or  X  Retrieval of documents recorded in other media than text was successfully tested in tracks that used noisy  X  The attempt made by TREC to be relevant in the context of modern Web searching is most visible in the  X  The Question-Answering track (QA) is based on the assumption that searchers would often prefer to get
The third part of the book contains another seven chapters contributed by research groups that have been steady participants in TREC and have made a significant impact based on the novel models that they devel-oped, on the particularly impressive results that they obtained, or on the widespread acceptance of the ideas that they proposed. These are groups from the Center for Intelligent Information Retrieval of the University of Massachusetts at Amherst (the INQUERY project), City University, London (the Okapi project), Cornell
University (the SMART project), the City University of New York (the PIRCS project), University of Water-loo (the MultiText project), University of Twente (the Twenty-One project) and IBM Research (GURU, JURU, and other projects).
 ing in TREC, their excitement of facing new challenges and finding solutions to problems year after year by adapting and extending their theoretical models and finding heuristics to improve them, and their struggle to adapt and improve their experimental systems to deal with ever larger collections and more varied media or types of data. Some of these chapters are truly inspiring! original proposers of the very idea of an effort to build a framework and infrastructure for IR evaluation on large document collections, which materialized in TREC. As in her previous reviews of TREC, included in the special issues of IP&amp; M, Sparck Jones discusses the successes and failures of TREC, its impact on IR research and on commercial search engines and, more importantly, its challenges for the future. This includes the need to engage with the heterogeneity of the Web, the need to deal with new data types and media, the need to consider new tasks, and the need to explore the integration of IR tools in the context of information manage-ment services and personalized delivery of information.

TREC has had a significant impact on IR research by being a test-bed for new ideas and models, and through contributions from some of the most creative and active IR researchers and research groups. Signif-icantly, it is one of the few venues where bad results are as welcome as good ones, and where the purpose is to understand what works and what does not, and why. Therefore, the reader of this book can find out not only what the state of the art is, but also what was tried and did not work.

I recommend this book to a variety of readers. Researchers new to IR or unfamiliar with TREC can read the entire book in order to better understand their field of research, to find out what the hot issues are, and who are the leading researchers in this field. Moreover, experimental settings and evaluation methodologies widely adopted in the field are described in some detail, so the new researchers do not need to re-invent the wheel. Previous TREC participants who want to move to new tracks can also benefit from the book, by studying the appropriate chapters. Even for seasoned IR researchers the book is extremely useful, serving as a reference source that can be more easily consulted than the entire collection of TREC proceedings. Final-ly, this can be an interesting read for any researchers in Computer Science or Information Science with an interest in search engines, in the functionality that these are expected to support and in ways in which these should be evaluated.
 Handbook of Evaluation Methods for Health Informatics, Jytte Brender (2005). ISBN: 0-12-370464-2,  X 69.95
Identifying the right evaluation techniques is an area of considerable interest to both academics and prac-titioners in the area of healthcare informatics. With an increasing reliance on healthcare information systems, appropriate evaluation is essential to the continuing development of usable and useful technologies. Many healthcare studies point to failures in evaluation as a primary reason for information system failures (Ander-son, Aydin, &amp; Jay, 1994). Furthermore, the diversity of the healthcare information technology precludes any one method from dominating the field. Therefore, we need to understand the different variety of evaluation methods that are available to us.

Against this backdrop, we read with some interest The Handbook of Evaluation Methods for Health Infor-matics by Jytte Brender. The book is divided into three main parts. The four chapters in part 1 serve as the foundation for the book. In this part, the author introduces evaluation and discusses the conceptual under-pinnings of evaluation. Part 1 attempts to provide readers a basic understanding of evaluation. Part 2 is the  X  X  X eart X  X  of the book and focuses on methods and techniques of evaluation. The author discusses thirty-six different evaluation techniques. Clearly, these are not all the known evaluation techniques but the author does a more than adequate job of providing coverage of different types of techniques. They range from qual-itative techniques (e.g. field studies) to quantitative techniques (e.g. randomized control trials). Finally, part 3
