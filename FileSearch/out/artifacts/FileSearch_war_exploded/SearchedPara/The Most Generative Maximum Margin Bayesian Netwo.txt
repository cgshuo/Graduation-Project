 j  X  dataset ML MCL MM ML-BN-SVM Linear SVM SVM abalone 57 . 70  X  1 . 58 57 . 92  X  1 . 65 57 . 78  X  0 . 96 58 . 69  X  1 . 86 58 . 42  X  1 . 77 59 . 29  X  1 . 40 adult 85 . 70  X  0 . 66 86 . 65  X  0 . 64 86 . 54  X  0 . 65 86 . 76  X  0 . 64 86 . 86  X  0 . 64 86 . 87  X  0 . 64 australian 81 . 67  X  2 . 66 81 . 97  X  3 . 70 85 . 49  X  3 . 40 84 . 76  X  3 . 78 85 . 78  X  1 . 69 86 . 80  X  2 . 34 breast 95 . 56  X  2 . 06 95 . 56  X  1 . 45 96 . 59  X  0 . 50 96 . 00  X  2 . 31 96 . 15  X  1 . 51 97 . 19  X  0 . 41 car 94 . 24  X  1 . 50 98 . 08  X  0 . 75 97 . 79  X  0 . 79 98 . 08  X  1 . 07 93 . 84  X  0 . 65 99 . 65  X  0 . 30 chess 92 . 19  X  1 . 62 97 . 65  X  0 . 81 97 . 43  X  0 . 79 97 . 99  X  0 . 92 97 . 02  X  0 . 82 99 . 50  X  0 . 25 cleve 79 . 43  X  6 . 34 77 . 74  X  7 . 53 79 . 09  X  7 . 56 80 . 79  X  7 . 58 83 . 57  X  5 . 29 82 . 19  X  6 . 37 corral 97 . 53  X  4 . 61 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 93 . 36  X  4 . 55 100 . 00  X  0 . 00 crx 84 . 04  X  4 . 64 80 . 32  X  5 . 20 83 . 89  X  5 . 89 84 . 20  X  4 . 56 85 . 75  X  3 . 20 85 . 75  X  2 . 65 diabetes 74 . 35  X  4 . 23 74 . 22  X  5 . 50 73 . 31  X  5 . 71 74 . 35  X  5 . 42 73 . 96  X  4 . 46 74 . 48  X  4 . 65 flare 81 . 57  X  1 . 27 81 . 48  X  1 . 91 84 . 45  X  0 . 28 83 . 30  X  1 . 06 84 . 45  X  0 . 28 84 . 45  X  0 . 28 german 71 . 90  X  1 . 83 69 . 50  X  3 . 54 73 . 20  X  4 . 01 72 . 60  X  2 . 89 76 . 10  X  1 . 11 75 . 80  X  2 . 80 glass 72 . 68  X  5 . 29 68 . 55  X  4 . 03 71 . 71  X  10 . 88 72 . 61  X  6 . 35 71 . 61  X  5 . 50 73 . 24  X  5 . 33 glass2 81 . 38  X  9 . 20 82 . 00  X  8 . 05 80 . 75  X  10 . 51 80 . 75  X  10 . 51 79 . 38  X  4 . 27 79 . 96  X  8 . 90 heart 80 . 74  X  10 . 36 77 . 04  X  10 . 61 77 . 41  X  9 . 81 81 . 48  X  9 . 34 84 . 81  X  4 . 11 81 . 85  X  9 . 40 hepatitis 86 . 17  X  10 . 00 86 . 08  X  11 . 48 86 . 08  X  3 . 38 86 . 17  X  6 . 31 87 . 42  X  10 . 89 88 . 67  X  6 . 37 iris 94 . 00  X  1 . 85 94 . 00  X  1 . 85 92 . 67  X  4 . 53 94 . 00  X  1 . 85 93 . 33  X  2 . 93 93 . 33  X  2 . 93 letter 86 . 21  X  0 . 84 87 . 65  X  0 . 80 89 . 58  X  0 . 74 88 . 57  X  0 . 77 90 . 07  X  0 . 73 94 . 07  X  0 . 58 lymphography 80 . 77  X  7 . 36 75 . 38  X  10 . 86 80 . 66  X  11 . 11 76 . 92  X  10 . 54 83 . 57  X  10 . 44 86 . 48  X  9 . 99 mofn-3-7-10 92 . 62  X  1 . 37 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 mushroom 100 . 00  X  0 . 07 100 . 00  X  0 . 07 100 . 00  X  0 . 07 100 . 00  X  0 . 07 100 . 00  X  0 . 07 99 . 82  X  0 . 19 nursery 92 . 96  X  0 . 77 98 . 31  X  0 . 40 98 . 84  X  0 . 33 98 . 68  X  0 . 35 93 . 31  X  0 . 76 100 . 00  X  0 . 04 satimage 85 . 79  X  1 . 92 81 . 52  X  0 . 95 86 . 82  X  2 . 66 86 . 98  X  1 . 30 88 . 36  X  1 . 58 90 . 59  X  1 . 59 segment 94 . 89  X  1 . 02 94 . 37  X  1 . 57 96 . 02  X  1 . 21 95 . 76  X  0 . 62 96 . 19  X  0 . 73 96 . 84  X  1 . 17 shuttle 99 . 88  X  0 . 05 99 . 84  X  0 . 06 99 . 91  X  0 . 05 99 . 92  X  0 . 04 99 . 96  X  0 . 03 99 . 96  X  0 . 03 soybean-large 91 . 88  X  1 . 28 82 . 66  X  4 . 59 90 . 77  X  2 . 16 91 . 87  X  2 . 26 91 . 15  X  3 . 72 93 . 54  X  1 . 19 spambase 92 . 97  X  0 . 85 92 . 99  X  1 . 10 93 . 62  X  0 . 80 94 . 03  X  0 . 84 94 . 27  X  0 . 72 95 . 04  X  0 . 37 TIMIT4CF 90 . 70  X  0 . 42 87 . 25  X  0 . 48 91 . 70  X  0 . 40 91 . 59  X  0 . 40 92 . 05  X  0 . 39 92 . 38  X  0 . 39 TIMIT4CM 90 . 47  X  0 . 43 88 . 57  X  0 . 46 85 . 62  X  0 . 51 92 . 58  X  0 . 38 92 . 88  X  0 . 38 93 . 16  X  0 . 37 TIMIT6CF 83 . 18  X  0 . 52 80 . 92  X  0 . 54 84 . 27  X  0 . 50 84 . 89  X  0 . 49 85 . 57  X  0 . 48 85 . 74  X  0 . 48 TIMIT6CM 83 . 05  X  0 . 52 80 . 98  X  0 . 54 85 . 45  X  0 . 49 85 . 91  X  0 . 48 86 . 66  X  0 . 47 86 . 56  X  0 . 47 USPS 91 . 20  X  0 . 93 90 . 46  X  0 . 97 95 . 98  X  0 . 65 95 . 98  X  0 . 65 95 . 82  X  0 . 66 91 . 80  X  0 . 90 vehicle 70 . 60  X  2 . 00 69 . 64  X  3 . 69 69 . 04  X  4 . 30 69 . 88  X  2 . 41 70 . 12  X  1 . 26 69 . 76  X  2 . 43 vote 94 . 37  X  2 . 62 94 . 15  X  2 . 04 96 . 01  X  2 . 45 95 . 31  X  2 . 74 94 . 85  X  2 . 20 95 . 54  X  3 . 18 waveform-21 82 . 36  X  0 . 71 80 . 55  X  1 . 00 82 . 86  X  0 . 51 83 . 48  X  0 . 56 84 . 78  X  1 . 77 85 . 16  X  1 . 29 dataset ML MCL MM ML-BN-SVM Linear SVM SVM abalone 53 . 64  X  1 . 45 59 . 12  X  1 . 71 56 . 62  X  0 . 88 59 . 12  X  1 . 69 58 . 42  X  1 . 77 59 . 29  X  1 . 40 adult 83 . 37  X  0 . 71 86 . 90  X  0 . 64 86 . 92  X  0 . 64 86 . 94  X  0 . 64 86 . 86  X  0 . 64 86 . 87  X  0 . 64 australian 85 . 92  X  2 . 92 84 . 02  X  2 . 76 85 . 34  X  2 . 64 87 . 24  X  2 . 86 85 . 78  X  1 . 69 86 . 80  X  2 . 34 breast 97 . 63  X  1 . 01 95 . 56  X  1 . 45 95 . 85  X  2 . 22 97 . 04  X  1 . 45 96 . 15  X  1 . 51 97 . 19  X  0 . 41 car 85 . 64  X  1 . 59 93 . 43  X  1 . 76 93 . 78  X  1 . 63 92 . 73  X  1 . 14 93 . 84  X  0 . 65 99 . 65  X  0 . 30 chess 87 . 45  X  2 . 57 97 . 11  X  1 . 02 97 . 58  X  0 . 86 97 . 68  X  1 . 21 97 . 02  X  0 . 82 99 . 50  X  0 . 25 cleve 82 . 87  X  6 . 79 82 . 52  X  6 . 36 82 . 17  X  6 . 94 82 . 53  X  7 . 64 83 . 57  X  5 . 29 82 . 19  X  6 . 37 corral 89 . 16  X  8 . 67 93 . 36  X  4 . 55 93 . 36  X  4 . 55 93 . 36  X  4 . 55 93 . 36  X  4 . 55 100 . 00  X  0 . 00 crx 86 . 84  X  3 . 29 85 . 13  X  4 . 10 84 . 82  X  3 . 71 86 . 06  X  3 . 54 85 . 75  X  3 . 20 85 . 75  X  2 . 65 diabetes 73 . 96  X  4 . 17 75 . 40  X  5 . 41 74 . 61  X  4 . 94 74 . 87  X  3 . 47 73 . 96  X  4 . 46 74 . 48  X  4 . 65 flare 76 . 58  X  1 . 04 83 . 40  X  1 . 02 82 . 63  X  1 . 79 83 . 11  X  0 . 82 84 . 45  X  0 . 28 84 . 45  X  0 . 28 german 74 . 20  X  3 . 58 75 . 10  X  1 . 42 76 . 50  X  1 . 52 75 . 30  X  3 . 12 76 . 10  X  1 . 11 75 . 80  X  2 . 80 glass 71 . 66  X  3 . 58 68 . 05  X  0 . 63 68 . 03  X  1 . 91 70 . 61  X  3 . 63 71 . 61  X  5 . 50 73 . 24  X  5 . 33 glass2 81 . 29  X  10 . 50 82 . 63  X  8 . 12 80 . 09  X  9 . 96 82 . 63  X  8 . 12 79 . 38  X  4 . 27 79 . 96  X  8 . 90 heart 81 . 85  X  9 . 40 82 . 59  X  5 . 77 81 . 85  X  5 . 73 83 . 33  X  5 . 14 84 . 81  X  4 . 11 81 . 85  X  9 . 40 hepatitis 88 . 58  X  6 . 57 86 . 08  X  3 . 38 84 . 92  X  8 . 69 92 . 33  X  6 . 75 87 . 42  X  10 . 89 88 . 67  X  6 . 37 iris 93 . 33  X  2 . 93 92 . 67  X  3 . 46 93 . 33  X  2 . 93 93 . 33  X  2 . 93 93 . 33  X  2 . 93 93 . 33  X  2 . 93 letter 74 . 95  X  1 . 05 85 . 97  X  0 . 84 82 . 53  X  0 . 92 85 . 79  X  0 . 85 90 . 07  X  0 . 73 94 . 07  X  0 . 58 lymphography 84 . 23  X  5 . 60 84 . 23  X  4 . 47 82 . 80  X  5 . 54 82 . 80  X  4 . 39 83 . 57  X  10 . 44 86 . 48  X  9 . 99 mofn-3-7-10 87 . 31  X  1 . 94 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 100 . 00  X  0 . 00 mushroom 98 . 04  X  0 . 54 100 . 00  X  0 . 07 100 . 00  X  0 . 07 99 . 78  X  0 . 20 100 . 00  X  0 . 07 99 . 82  X  0 . 19 nursery 89 . 97  X  0 . 91 92 . 38  X  0 . 80 92 . 98  X  0 . 77 93 . 03  X  0 . 77 93 . 31  X  0 . 76 100 . 00  X  0 . 04 satimage 81 . 56  X  1 . 80 87 . 29  X  1 . 11 88 . 82  X  1 . 26 88 . 41  X  1 . 33 88 . 36  X  1 . 58 90 . 59  X  1 . 59 segment 92 . 68  X  1 . 78 94 . 29  X  0 . 77 94 . 98  X  1 . 66 95 . 37  X  0 . 86 96 . 19  X  0 . 73 96 . 84  X  1 . 17 shuttle 99 . 62  X  0 . 09 99 . 91  X  0 . 05 99 . 94  X  0 . 04 99 . 95  X  0 . 04 99 . 96  X  0 . 03 99 . 96  X  0 . 03 soybean-large 93 . 35  X  1 . 91 92 . 98  X  3 . 88 92 . 79  X  1 . 59 91 . 50  X  3 . 81 91 . 15  X  3 . 72 93 . 54  X  1 . 19 spambase 90 . 03  X  1 . 11 93 . 73  X  0 . 95 94 . 01  X  0 . 97 94 . 08  X  0 . 75 94 . 27  X  0 . 72 95 . 04  X  0 . 37 TIMIT4CF 87 . 88  X  0 . 47 92 . 04  X  0 . 39 91 . 90  X  0 . 40 91 . 95  X  0 . 39 92 . 05  X  0 . 39 92 . 38  X  0 . 39 TIMIT4CM 88 . 86  X  0 . 46 93 . 04  X  0 . 37 92 . 88  X  0 . 38 92 . 71  X  0 . 38 92 . 88  X  0 . 38 93 . 16  X  0 . 37 TIMIT6CF 82 . 20  X  0 . 53 85 . 50  X  0 . 49 85 . 20  X  0 . 49 85 . 49  X  0 . 49 85 . 57  X  0 . 48 85 . 74  X  0 . 48 TIMIT6CM 82 . 43  X  0 . 53 86 . 24  X  0 . 48 86 . 04  X  0 . 48 86 . 50  X  0 . 47 86 . 66  X  0 . 47 86 . 56  X  0 . 47 USPS 86 . 89  X  1 . 11 94 . 37  X  0 . 76 95 . 44  X  0 . 69 95 . 08  X  0 . 71 95 . 82  X  0 . 66 91 . 80  X  0 . 90 vehicle 61 . 57  X  1 . 44 68 . 67  X  3 . 03 69 . 76  X  2 . 56 67 . 95  X  6 . 00 70 . 12  X  1 . 26 69 . 76  X  2 . 43 vote 90 . 16  X  4 . 70 94 . 61  X  2 . 21 95 . 78  X  2 . 21 94 . 61  X  3 . 19 94 . 85  X  2 . 20 95 . 54  X  3 . 18 waveform-21 81 . 14  X  1 . 05 85 . 10  X  1 . 53 85 . 43  X  1 . 34 85 . 14  X  1 . 52 84 . 78  X  1 . 77 85 . 16  X  1 . 29 ML-BN-SVM NB 24 / 19 21 / 11 15 / 9 21 / 14 14 / 7 20 / 8  X  15 / 4 9 / 3 7 / 1 ML-BN-SVM TAN 19 / 18 21 / 15 13 / 8 21 / 16 12 / 8 15 / 3 12 / 6  X  10 / 4 3 / 2 ML-BN-SVM NB 12 / 6 7 / 1 25 / 19 18 / 12 23 / 15 22 / 13  X  7 / 2 11 / 4 11 / 6 ML-BN-SVM TAN 18 / 11 13 / 3 25 / 19 24 / 20 26 / 22 27 / 17 18 / 11  X  10 / 4 10 / 6 ML-BN-SVM NB 1 / 0 2 / 0 18 / 11 14 / 7 19 / 9 20 / 13  X  6 / 2 16 / 6 14 / 7 ML-BN-SVM TAN 5 / 3 3 / 1 19 / 14 20 / 11 22 / 14 20 / 14 17 / 10  X  23 / 11 23 / 11 dataset NB TAN NB TAN NB TAN NB TAN
 Robert Peharz* robert.peharz@tugraz.at Sebastian Tschiatschek* tschiatschek@tugraz.at Franz Pernkopf pernkopf@tugraz.at In machine learning, there are two primary ap-proaches: generative and discriminative learning. In generative learning, the aim is to estimate an underly-ing and unknown probability distribution from data. Therefore, generative models represent probability dis-tributions and the objective is some form of likelihood. In discriminative learning, the aim is to find a repre-sentation of a function for mapping features to targets. Here, the objectives are more versatile than in the gen-erative case; dependent on the scenario, one aims to minimize some form of error, or maximize the condi-tional likelihood, some form of margin or the classifi-cation rate. When generative models do not capture the true distribution well, discriminative approaches tend to outperform their generative counterparts. Bayesian networks (BNs) represent distributions and are therefore well-suited for generative learning. On the other hand, they also represent conditional dis-tributions and classification functions, and can be trained also discriminatively ( Friedman et al. , 1997 ; Ng &amp; Jordan , 2001 ; Wettig et al. , 2003 ; Greiner et al. , 2005 ; Guo et al. , 2005 ; Sha , 2007 ; Pernkopf et al. , 2012 ). When a BN is trained discriminatively, its gen-erative semantics is abandoned, i.e. its interpretation as joint distribution. The BN is optimized to infer the class value from the features, while other inference tasks are implausible and yield poor results. How-ever, a discriminative BN still represents some spuri-ous marginal feature distribution, which does not ful-fill any modeling purpose. Why should we then use a BN, when we are actually interested in the condi-tional distribution only? One reasonable ramification is to use models which explicitly model conditional distributions, but not the marginal feature distribu-tion, such as conditional random fields ( Lafferty et al. , 2001 ). The motivation in this paper is different: Even when the conditional distribution obtained by discrim-inative training is unique, the representation as a BN might be not unique. A natural approach is to use this degree of freedom to improve the generative aspect of the model, i.e. to select the representation with high-est likelihood. This describes a domain of likelihood-aware discriminative models, justifying a generative usage, such as sampling new examples , versatile in-ference scenarios , and consistent treatment of missing features during test time . A similar philosophy can be found in maximum entropy discrimination (MED) ( Jebara , 2001 ) which combines discriminative estima-tion with generative models in a principled way. In this work, we consider a SVM-type maximum margin approach for BNs ( Cortes &amp; Vapnik , 1995 ; Guo et al. , 2005 ; Pernkopf et al. , 2012 ). We intro-duce a weighted  X  1 -norm in the objective, where the weights correspond to the likelihood counts obtained from training data. The motivation for the weighted  X  -norm is not that a better classifier is learned; lit-erature provides several alternatives to the classical  X  -norm SVMs ( Zhu et al. , 2004 ; Zou &amp; Yuan , 2008 ) and no general preference can be assessed for any norm. We merely assume that the weighted  X  1 -norm does typically not perform worse than any other norm regularizer. However, we show that for specific net-work structures the resulting network parameters are automatically normalized, which gives the weighted  X  -norm the additional interpretation as likelihood-term . Therefore, we can interpret our model as a likelihood-aware SVM . When the SVM-trade-off pa-rameter is zero, the solution of our formulation coin-cides with maximum likelihood parameters. When the parameter tends towards infinity, the sample-margins are emphasized. Our model is related to hybrid generative-discriminative models ( Raina et al. , 2003 ; Bouchard &amp; Triggs , 2004 ; Bishop &amp; Lasserre , 2007 ), but there is a substantial difference: Although the objective of our formulation is a trade-off between a likelihood term and a margin term, the objective is not a blend of a  X  X enerative X  and a  X  X iscrimina-tive X  term. The margin term alone is not a discrim-inative objective, just as a standard SVM without norm-penalization has little discriminative meaning. Rather, the likelihood-term has to be viewed as norm-penalization, while the generative semantics are a de-sired side-effect .
 We introduce our notation in Section 2 . In Section 3 , we present our formulation as convex optimization problem and state Theorems 1 and 2 which guarantee correctly normalized BN parameters, permitting the additional likelihood-interpretation. In Section 4 , we propose a projected gradient method which is scalable to large datasets. In Section 5 we report results on benchmark datasets. Section 6 concludes the paper. Throughout the paper, we assume discrete random variables (RVs), where plain capital letters denote sin-gle RVs and capital boldface letters represent sets of RVs. Lower-case plain letters represent states of RVs and lower-case boldface letter represent joint states of variable sets. When y is a state of Y , and X  X  Y , then y ( X ) denotes the corresponding state of X . Further-more, when X and Y are disjoint, then [ x , y ] denotes a state of set X  X  Y . The set of states which can be assumed by RV X is denoted as val ( X ), and similarly we use val ( X ) for a set X . For notational ease, we represent (unconditional) distributions as conditional distributions of the form P( X ) := P( X | X  ). A Bayesian network (BN) is defined as a tuple B = ( G , P ), containing an acyclic directed graph G and a set of conditional probability distributions P . The nodes of G correspond to RVs X = { X 0 , . . . , X N } and the edges describe direct depen-dencies among these RVs. For each node in G , the ditional distributions, where Pa i denotes the set of parents of X i according to G . Similarly, we define Ch i as the set of children of X i . A BN repre-sents the joint distribution P B ( X ) = Q N i =0 P( X i | Pa For discrete data, a general representation of P is a collection of conditional probability tables (CPTs)  X  = {  X  0 , . . . ,  X  N } , with  X  i = {  X  i j | h | j  X  val ( X BN distribution can then be written as where  X  i j | h is the indicator function 1 ( x i = j  X  x ( Pa i ) = h ). We represent the BN param- X   X  as a vector, whose elements are addressed as  X  i j | h . We say that  X  is sub-normalized , iff log X and  X  is normalized , iff ( 2 ) holds with equality. A vector  X  is strictly sub-normalized, iff it is sub-normalized, but not normalized. In order to represent valid BN parameters,  X  has to be normalized. We de-fine a vector-valued function  X  ( x ) of the same length in  X  . In that way, we can express the log of ( 1 ) as log P B ( X = x ) =  X  ( x ) T  X  .
 Assume that we have M i.i.d. samples { x 1 , . . . , x M } , drawn from an unknown distribution P  X  ( X ). For a fixed BN structure G , the (smoothed) maximum like-lihood (ML) parameters are given as where Here,  X   X  0 is a smoothing parameter with the in-terpretation of a virtual sample count, which biases the ML estimates towards a uniform distribution. The  X  X irtual samples X  are distributed consistently among the CPTs. We say that the likelihood-counts are con-sistent , when for all X k , j  X  val ( X k ), X i  X  Ch k , and h  X  val ( Pa k  X  Pa i ) it holds that where A = Pa k \ Pa i and B = Pa i \ ( Pa k  X  X  X k } ). For  X  &gt; 0, Equation ( 3 ) is also the MAP solution using Dirichlet priors according to ( Buntine , 1991 ; Heckerman et al. , 1995 ).
 In this paper, we consider classification problems, where w.l.o.g. we assume that the class variable C = X 0 and the features are Z = { X 1 , . . . , X N } . Our discussion will concentrate on structures satisfying the following condition, as identified in ( Wettig et al. , 2003 ): Condition 1. Each child of the class-node has a cov-ering parent.
 We call node Y a covering parent of node X iff Y is a parent of X and Pa ( X )  X  Pa ( Y )  X  X  Y } . Structures satisfying Condition 1 are denoted as C1-structures . The class of these structures is quite rich, containing, amongst others, the naive Bayes (NB) structure, the tree-augmented naive Bayes (TAN) ( Friedman et al. , 1997 ), and diagnostic networks ( Wettig et al. , 2003 ). C1-structures facilitate discriminative learning, since for each unnormalized parameter vector there exists also a normalized parameter vector, specifying the same conditional distribution P B ( C | Z ). Wettig et al. ( 2003 ) provided a constructive proof, by proposing Algorithm 1 (shown in the Appendix) for normaliz-ing a set of unnormalized BN parameters, while leav-ing P B ( C | Z ) unchanged. Condition 1 allows a convex relaxation of our optimization problem, presented in Section 3 , i.e. a globally optimal solution can be ob-tained. However, in principle our methods can also be applied to arbitrary structures, by applying a normal-ization maintaining parameter transformation such as in ( Pernkopf et al. , 2012 ). The probabilistic margin  X  m of the m th sample is de-fined as ( Guo et al. , 2005 ; Pernkopf et al. , 2012 ) Clearly, when  X  m &gt; 1, then the m th sam-ple is correctly classified, and when  X  m &lt; 1, it is wrongly classified. By defining  X  c ( x ) :=  X  ([ c, x ( Z )]), we can express the log of ( 8 ) as log  X  m = min interpret  X  c ( x m ) as (class-dependent) feature trans-formation, we can formulate the following multiclass SVM-type training for BNs ( Cortes &amp; Vapnik , 1995 ; Crammer &amp; Singer , 2001 ; Guo et al. , 2005 ): min . Here, k  X  k denotes some norm,  X  = (  X  1 , . . . ,  X  M ) is a vector of margin slacks, and  X  is a trade-off pa-rameter, set by cross validation. We call formu-lation ( 9 ) the BN-SVM . In general, a solution of the BN-SVM will not be normalized, i.e. typically we consider C1-structures, we can simply apply Al-gorithm 1 (see Appendix), and obtain valid BN pa-rameters, with the same class conditional distribution (i.e. the same classifier) as the unnormalized, optimal solution.
 Although this approach allows to marry SVM-type training with BNs, the following questions naturally rise: Why should we even care about renormalized parameters, corresponding to the same classifier as the solution of ( 9 )? Why should we use a BN at all, when, by training it like an SVM, we abandon any probabilistic interpretation? The answer we give here, is that discriminative training in BNs can be meaningful, when we (partly) maintain a generative interpretation. To this end, we modify ( 9 ) and use the following weighted  X  1 -norm for the BN-SVM norm n j | h are the likelihood-counts according to ( 4 ), col-lected in a vector n . Furthermore, we subject the vec-tor  X  to sub-normalization constraints ( 2 ). These con-straints restrict the parameters to a smooth approxi-mation of the negative orthant, but do not severely restrict the solution space, since an arbitrary con-stant can be added to a solution vector  X  , yielding the same classifier. However, for the BN-SVM ac-cording to ( 9 ), we are allowed to arbitrarily assume a function margin of 1, since an optimal solution vec-tor simply scales with this value. By introducing the sub-normalization constraints, this does not hold true any more. Therefore, we introduce a model parame-ter  X  for the function margin, which is set by cross-validation. Since constraints ( 2 ) imply  X  i j | h  X  0, Finally, we get the modified convex problem: min . Our first interpretation of ( 10 ) is that of a special in-stance of an BN-SVM, with (exotic) weighted  X  1 -norm term nL n (  X  ) and an arbitrary (but not limiting) sub-normalization constraint on the solution vector. On the other hand, nL n (  X  ) =  X  n T  X  is formally the neg-ative log-likelihood of  X  . Therefore, although ( 10 ) is a discriminative formulation , we see that as a side effect , it aims to maximize the data likelihood . However, there is still a major problem about this generative inter-pretation: the solution vector  X  might be strictly sub-normalized. In this case,  X  does not represent valid BN parameters, and strictly speaking, nL n (  X  ) can not be interpreted as negative log-likelihood. When Algo-rithm 1 is applied to obtain normalized parameters, the discriminative character is left unchanged. But how does the generative character change under Algo-rithm 1 ? Fortunately, as shown in Lemma 1 , for C1-structures the log-likelihood can only increase when Algorithm 1 is applied to sub-normalized parameters. The proofs for Lemma 1 and Theorems 1 and 2 can be found in the Appendix.
 Lemma 1. Let G be a C1-structure,  X   X  be a sub-normalized parameter-vector for G , and n be a non-negative vector of consistent likelihood-counts. Then the likelihood is non-decreasing under Algorithm 1 , i.e. when  X  is the output of Algorithm 1 for input G ,  X   X  , then nL n (  X  )  X  nL n (  X   X  ) .
 Using Lemma 1 , it is easy to show that ( 10 ) always has a normalized solution, as stated in Theorem 1 . Theorem 1. Let G be a C1-structure, { x 1 , . . . , x M } be an arbitrary data set, and n be an element-wise non-negative vector of consistent likelihood-counts. Then problem ( 10 ) (for  X   X  0 ) always has an optimal solu-tion  X  ,  X  , such that  X  is normalized.
 Furthermore, for positive likelihood-counts (e.g. when  X  &gt; 0 in ( 4 )), the solution is unique and normalized . Theorem 2. Assume the same conditions as in The-orem 1 , but where n is element-wise positive. Then problem ( 10 ) has a unique, normalized solution. Lemma 1 and Theorems 1 and 2 show that for C1-structures, we can always interpret nL n (  X  ) as nega-tive log-likelihood. Due to this generative interpreta-tion, we call formulation ( 10 ) the maximum-likelihood BN-SVM (ML-BN-SVM). Problem ( 10 ) is convex and can be addressed by standard solvers. However, this restricts learning to medium sized data sets. In the following section we describe an optimization method which scales better to large datasets. The main limitation in ( 10 ) is that we have M ( | val ( C ) |  X  1) linear constraints, which restricts application currently to some thousand samples. Therefore, we slightly modify the problem and pro-pose a scalable gradient-based optimization method. First, we eliminate the margin constraints, and substi-tute the slack variables via the parameters  X  , by using a hinge function. In order to obtain a differentiable objective we use the soft-hinge h R ( ), defined as h
R (  X  ) = R is the radius of a fitted circle-segment, smoothing the discontinuity of the hinge, and = R (1  X  In our experiments we set R = min(1 ,  X  ). Now, the slack variables are (approximately) expressed as  X  where smax is the soft-max, defined as smax tends towards infinity, the soft-max converges to the max. In our experiments we set  X  = 10. We obtain the following modified problem min . s . t . log X Problem ( 11 ) is convex, with continuous differen-tiable objective. We use a projected gradient descent method, i.e.  X  is projected onto the set of subnor-malized vectors after each gradient step. This can be done independently for each CPT, i.e. for each combination of i  X  { 0 , . . . , N } and h  X  val ( Pa i ). Projecting an arbitrary vector  X   X  = (  X   X  1 , . . . ,  X   X  onto the set of subnormalized vectors is formulated as min . lem has no closed form solution, but can be addressed by the iterative algorithm proposed in ( Lin , 2003 ). This algorithm neatly meets our requirements, since we can use the solution of the previous projected gradi-ent step as initialization, and then perform only some few iterations of the projection algorithm, without needing to iterate until convergence. The proposed projected gradient method scales nicely to large data sets; the evaluation of the objective and its gradient is linear in ( | val ( C ) | X  1) M N . It is also straightforward to implement parallel and stochastic versions of this method. Further details can be found in the supple-mentary material. In this section we present experiments for illustrative purposes (Sections 5.1 and 5.2 ) and a comparison on real-world datasets (Section 5.3 ). We considered 30 datasets from the UCI repository ( Frank &amp; Asuncion , 2010 ), TIMIT ( Pernkopf et al. , 2012 ) and USPS data ( Hastie et al. , 2003 ). Datasets containing more than 5000 samples were split into training and test set; Otherwise 5-fold cross-validation was used for test-ing. More details on the datasets can be found in the supplementary material. For discretizing contin-uous attributes, we used the algorithm described in ( Fayyad &amp; Irani , 2003 ). The smoothing parameter  X  in ( 4 ) was constantly set to 1. Although  X  can have a great impact on classification ( Friedman et al. , 1997 ; Silander et al. , 2007 ), its evaluation is out of the scope of this paper. 5.1. Generative-Discriminative Trade-off The parameter  X  in problem ( 10 ) allows to control the trade-off between the generative and discrimina-tive character of the model. Choosing  X  = 0, the ML-BN-SVM parameters coincide with the ML solution. When  X  tends towards infinity, a large margin separa-tion of training samples is emphasized. Intermediate choices of  X  correspond to a generative/discriminative crossover. To illustrate the effect of parameter  X  , we learned ML-BN-SVMs with varying  X  , assuming NB structure, using the car dataset. The results are shown in Figure 1 . With increasing  X  , the negative log-likelihood increases, while the sum of slacks de-creases. Qualitatively, the classification rate increases correspondingly. Similar behavior can be observed on other datasets.
CR 15.75 5.2. Classification with Missing Features Although the ML-BN-SVM is primarily trained for classification, its generative character justifies other inference tasks, e.g. marginalizing out missing fea-tures. The assumption is that the more generative the model is, the more robust the classifier is against missing data. To this end, we conducted an experi-ment with missing features in test data, using the ve-hicle dataset. We trained ML-BN-SVMs for different values of  X  , cross-validating  X  . In the test set, we var-ied the number of missing features, selected uniformly at random. For classification, missing features were marginalized out using junction-tree message passing. Classification results are shown in Figure 2 , where re-sults are averaged over 100 independent runs. While the purely generative model has the worst performance when no features are missing, its classification rate is almost constant until about 40% of missing features, and degrades slowly over the whole range of missing features. In contrast, models that are more discrimi-native (i.e. larger  X  ) show a better performance when all features are used, but their classification rates de-grade rapidly with increasing percentage of missing features. This effect can be controlled; for  X  = 1 and using all available features, the classification rate is almost as good as for classifiers trained with larger values of  X  . Furthermore, the results are better than for the purely generative classifier for almost the whole range of missing features.
 5.3. Benchmark Classification Results We compared ML-BN-SVMs with ML, maximum conditional likelihood (MCL) and maximum margin (MM) parameters using the algorithm proposed in ( Pernkopf et al. , 2012 ). In order to enable a fair com-parison, MM was executed without early stopping. Experiments with early stopping are provided in the supplementary material. Furthermore, we compared with linear SVMs and SVMs equipped with Gaus-sian kernels ( Chang &amp; Lin , 2011 ). For ML-BN-SVMs for  X  and  X  , uniformly spaced in the intervals [0 . 01 , 0 . 5] and [0 . 01 , 1], respectively (see ( Pernkopf et al. , 2012 ) for details). For SVMs we validated the trade-off parameter  X   X  { 2  X  2 , 2  X  1 , . . . 2 10 } and, for kernel-ized SVMs, the kernel width  X   X  { 2  X  5 , 2  X  4 , . . . , 2 For the classifiers based on BNs, we used NB and maximum-likelihood TAN structures ( Friedman et al. , 1997 ). Classification results for the compared meth-ods are shown in Table 1 . Due to limited space, we omit the results for NB, averaged results for TIMIT, and show only a subset of the datasets. Further re-sults are provided in the supplementary material. We see that ML-BN-SVM parameters clearly outperform both ML and MCL parameters. Furthermore, ML-BN-SVM performs better than MM in 17 out of 27 datasets. ML-BN-SVM also compares well to linear SVMs. We observe a slight preference for kernelized SVMs, which can be attributed to the kernel trick, and its implicit high dimensional feature transform. How-ever, generally we see that the ML-BN-SVM delivers satisfying classification results.
 To demonstrate the generative character of the ML-BN-SVM, we compare the likelihoods of the trained BN models. In Figure 3 we plot the likelihood (nor-malized by the sample size) of ML parameters against the likelihood of MCL, MM, and ML-BN-SVM pa-rameters, respectively. The results for NB and TAN are combined. For cross-validated results, each fold is used as individual dataset, i.e. one dot in the scatter plot. Since ML parameters maximize the likelihood, no points on the left hand side of the 45  X  -line are possi-ble. We observe that the scatter plot for ML-BN-SVM is clearly more concentrated in the vicinity of the 45  X  -line than for MCL and MM parameters, constituting the generative character of the ML-BN-SVM. A simi-lar result is achieved for the likelihood on the test sets. Averaged over all datasets, the ML-BN-SVM achieved a likelihood of 91 . 09% relative to maximum likelihood (89 . 84% on the test sets); on the other hand, MCL training achieved on average a likelihood of 67 . 23% (61 . 47% on the test sets) and MM 39 . 99% (39 . 10% on the test set), relative to ML.
 Furthermore, we performed missing feature experi-ments on the UCI datasets. We randomly removed features from the test sets, were we varied the percent-age of missing features between 0 and 90%. Classifiers based on BNs treated missing features by marginal-ization. For the SVM (here we only considered the Gaussian kernel), K-nearest-neighbor (K-NN) impu-tation (with K = 5) was used to replace missing val-ues. For all BN-classifiers, TAN structures were used. We also provide results for logistic regression (LR), using K-NN imputation. The result, averaged over all UCI datasets, are shown in Figure 4 . As expected, the ML solution shows the most robust behavior against missing features, and for a percentage larger 60%, it performs best of all compared methods. However, ML-BN-SVMs perform better than ML in the case of no or little missing features, and are almost as ro-bust against missing features as the ML solution. The purely discriminative BN parameters, MCL and MM, show a quick drop-off in performance when the per-centage of missing features is increased. For large por-tions of missing features ( &gt; 60%) also SVMs perform poorly compared to ML and ML-BN-SVM. This ex-periments indicates that ML-BN-SVMs are favorable in conditions where many features might be missing, and where the percentage of missing features varies strongly. A BN distribution is a log-linear model, en-abling SVM-type training for BNs ( Guo et al. , 2005 ; Pernkopf et al. , 2012 ), which we call BN-SVM. For a large class of network structures ( Wettig et al. , 2003 ), one can always obtain correctly normalized parame-ters, i.e. a formally valid BN. In this paper, we pro-posed the maximum-likelihood BN-SVM, where during discriminative training the log-likelihood of the model is maximized as a desired side-effect, partly maintain-ing a generative interpretation. In experiments we showed that in terms of classification our models out-perform standard generative and discriminative learn-ing methods for BNs (i.e. ML, MCL and MM), com-pete with linear SVMs, and are in range with kernel-ized SVMs. Furthermore, our models achieve likeli-hoods close to the ML solutions. We demonstrated the benefit of the generative character in missing fea-ture experiments. In future work, we will extend the ML-BN-SVM to treat missing data during learning . In the BN framework, this naturally includes learning with missing features and semi-supervised learning . This work was supported by the Austrian Science Fund (project number P22488-N23 ).
 Algorithm 1 ( Wettig et al. , 2003 ) Input: G , unnormalized parameters  X   X  Output: Normalized parameters  X  , with 1:  X   X   X   X  2: Find a topological order (  X  0 , . . . ,  X  N ), i.e. any edge 3: for i = 0 . . . N do 4: for h  X  val ( Pa  X  i ) do 7: if X  X  i is a class-child then 8: Let X k i be a covering parent of X  X  i 11: for a  X  val ( A ) do 13: end for 14: end if 15: end for 16: end for Proof of Lemma 1 . First note that in Algorithm 1 ,  X  always remains sub-normalized: If  X  is sub-normalized, then  X   X  0 in step 5. In step 6 a CPT becomes normalized, and in step 12,  X  is added to some CPT entry, which again yields a sub-normalized CPT. By induction,  X  remains sub-normalized and  X   X  0. Algorithm 1 iterates over all X  X  val ( Pa  X  fore some modification, and  X   X  X  the vector afterwards. We show, that nL n (  X   X  X  )  X  nL n (  X   X  ), and therefore nL n (  X  )  X  nL n (  X   X  ).
 First,  X  is modified in step 6, where  X   X  X   X  i j | h =  X   X   X   X  j  X  val ( X  X  have nL n (  X   X  X  )  X  nL n (  X   X  ) =  X  n T  X   X  X  + n T  X   X  =  X  Therefore, when X  X  nL n (  X   X  ). When X  X  i is a class-child, we additionally have in step 12  X  where X k common parents, and A are the extra parents of X k i . Since G is a C1-structure, it holds that consistent likelihood-counts (cf. ( 7 )), we have that P thus We see that nL n (  X   X  X  )  X  nL n (  X   X  ), and by induction nL n (  X  )  X  nL n (  X   X  ).
 Proof of Theorem 1 . Let  X   X  ,  X   X  be an optimal solution of ( 10 ). When we apply Algorithm 1 to  X   X  , obtaining the normalized  X  as output, we see that  X  ,  X  , with  X  =  X   X  , is feasible, since the class-conditional distribution is invariant under Algorithm 1 . Fur-thermore, since  X   X  is sub-normalized, we have by Lemma 1 that nL n (  X  )  X  nL n (  X   X  ). Therefore, nL n (  X  ) + C P m  X  m  X  nL n (  X   X  ) + C P m  X   X  m , which implies that  X  ,  X  is optimal.
 Proof of Theorem 2 . We first prove by contradic-tion, that under the conditions of Theorem 2 , all solu-tions are normalized. Assume that  X   X  ,  X   X  are optimal for ( 10 ), where for some X  X  corresponding CPT in  X   X  is strictly sub-normalized . Let  X  be the output of Algorithm 1 for input G ,  X   X  . Let  X   X  be the vector before the strictly sub-normalized CPT is processed, and  X   X  X  be the vector afterwards. When X  X  tive log-likelihood is strictly decreased in step 6, i.e. nL n (  X   X  X  ) &lt; nL n (  X   X  ). Since the negative log-likelihood is never increased afterwards,  X   X  ,  X   X  can not be optimal.
 When X  X  likelihood is compensated in step 12 (cf. ( 14 )). How-ever, at the same time, some entries of some CPTs of the covering parent are strictly decreased, i.e. they become strictly sub-normalized. Due to the topologi-cal ordering, these CPTs are processed at a later step. By induction, some CPTs of the class node become strictly sub-normalized, since the class node has to be the covering parent for some class child. Finally, when the CPTs of the class node are normalized, the nega-tive log-likelihood is strictly decreased, which contra-dicts that  X   X  ,  X   X  are optimal.
 Now we show that the solution is unique. Assume two optimal solutions  X   X  ,  X   X  and  X   X   X  ,  X   X   X  ,  X   X  6 Since ( 10 ) is a convex problem, the convex combi-nation  X  = 0 . 5  X   X  + 0 . 5  X   X   X  ,  X  = 0 . 5  X   X  + 0 . 5  X  is also optimal. Since all solutions are normalized, However, since log P exp is a strictly convex function,  X  is strictly sub-normalized, which contradicts that  X  ,  X  is optimal.
 Bishop, C. M. and Lasserre, J. Generative or discrim-inative? Getting the best of both worlds. Bayesian Statistics , 8:3 X 24, 2007.
 Bouchard, G. and Triggs, B. The trade-off between generative and discriminative classifiers. In COMP-STAT , pp. 721 X 728, 2004.
 Buntine, W. Theory refinement on bayesian networks.
In Uncertainty in Artificial Intelligence (UAI) , pp. 52 X 60, 1991.
 Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines.
ACM TIST , 2:27:1 X 27:27, 2011. URL http://www.csie.ntu.edu.tw/ ~ cjlin/libsvm . Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning , 20(3):273 X 297, 1995.
 Crammer, K. and Singer, Y. On the algorithmic im-plementation of multiclass kernel-based vector ma-chines. Journal of Machine Learning Research , 2: 265 X 292, 2001.
 Fayyad, M. U. and Irani, B. K. Multi-interval dis-cretization of continuous-valued attributes for clas-sification learning. IJCAI , pp. 1022 X 1029, 2003. Frank, A. and Asuncion, A. UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences, 2010. URL http://archive.ics.uci.edu/ml .
 Friedman, N., Geiger, D., and Goldszmidt, M.
Bayesian network classifiers. Machine Learning , (29):131 X 163, 1997.
 Greiner, R., Su, X., Shen, B., and Zhou, W. Structural extension to logistic regression: Discriminative pa-rameter learning of belief net classifiers. Machine Learning , 59(3):297 X 322, June 2005.
 Guo, Y., Wilkinson, D., and Schuurmans, D. Maxi-mum margin Bayesian networks. In Uncertainty in Artificial Intelligence (UAI) , pp. 233 X 242, 2005. Hastie, T., Tibshirani, R., and Friedman, J. The El-ements of Statistical Learning: Data Mining, Infer-ence, and Prediction . Springer, August 2003. Heckerman, D., Geiger, D., and Chickering, D. M.
Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning , 20:197 X 243, 1995.
 Jebara, T. Discriminative, Generative and Imitative learning . PhD thesis, MIT, 2001.
 Lafferty, J., McCallum, A., and Pereira, F. Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data. In Interna-tional Conference on Machine Learning (ICML) , pp. 282 X 289, 2001.
 Lin, A. A class of methods for projection on a convex set. Advanced Modeling and Optimization (AMO) , 5(3), 2003.
 Ng, A. Y. and Jordan, M. I. On Discriminative vs.
Generative Classifiers: A comparison of logistic re-gression and naive Bayes. In Advances in Neural Information Processing Systems (NIPS) , 2001. Pernkopf, F., Wohlmayr, M., and Tschiatschek, S. Maximum margin Bayesian network classifiers.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence , 34(3):521 X 531, 2012.
 Raina, R., Shen, Y., Ng, A., and McCallum, A. Classi-fication with hybrid generative/discriminative mod-els. In Advances in Neural Information Processing Systems (NIPS) , 2003.
 Sha, F. Large margin training of acoustic models for speech recognition . PhD thesis, University of Penn-sylvania, 2007.
 Silander, T., Kontkanen, P., and Myllym  X aki, P. On sensitivity of the MAP bayesian network structure to the equivalent sample size parameter. In Proceed-ings of UAI , pp. 360 X 367, 2007.
 Wettig, H., Gr  X unwald, P., Roos, T., Myllymaki, P., and Tirri, H. When discriminative learning of
Bayesian network parameters is easy. In Interna-tional Joint Conferences on Artificial Intelligence (IJCAI) , pp. 491 X 496, 2003.
 Zhu, J., Rosset, S., Hastie, T., and Tibshirani, R. 1-norm support vector machines. Advances in Neural Information Processing Systems , 16:49 X 56, 2004. Zou, H. and Yuan, M. The f  X  -norm support vector
