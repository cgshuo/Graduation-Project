 It is often expensive to acquire data in real-world data mining applications. Most previous data mining and machine learning research, however, assumes that a fixed set of training examples is given. In this paper, we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns, and to decide the ideal number of examples needed to minimize the total cost. We also propose a new strategy for Partial Example Acquisition ( PAS ), in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost. Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition. I.2.6 [ Artificial Intelligence ]: Learning  X  induction .
 Algorithms, Measurement, Performance, Economics. Data acquisition, induction, cost-s ensitive learning, data mining, machine learning, active learning, active cost-sensitive learning, interactive and online data mining. It is often expensive to acquire da ta in real-world data mining and machine learning applications. Most of previous works on data mining and machine learning appli cations, however, assume that a fixed set of training examples is given to build learning models. However, in reality, there may not be enough data to begin with. Data acquisition is the first, and one of the most important steps in machine learning and data mining. It is well recognized that data acquisition is time consuming and co stly. Thus, it is important for the learner to be able to acquire a proper set of training examples during learning. As data acquisition incurs cost, it is natural to study it as a part of the cost-sensitive learning process. We assume that acquiring an additional example incurs a certain cost. This cost is the sum of attribute costs (given) if their values are needed, plus the label cost (also given). We can further assume that the learner can decide if an example to be acquired is  X  X omplete X  or  X  X artial X . A complete example contains values of all attributes, thus it costs more to acquire than acquiring a partial example which contains values of a subset of attributes. The learner must decide what attribute values are needed in the partial example acquisition. Furthermore, partial example acqui sition can take several steps to acquire attribute values. It allows acquiring other unknown attribute values for the acquired partial examples. For example, assume that a problem domain has only three acquiring the values of these attributes and the class label are C C , C 3 , and C L respectively. If the learner decides that a complete example is needed, a complete example (such as a a =1 , c=true ) is drawn randomly according to a certain distribution governing the generation of training examples, with the cost of C 1 +C 2 +C 3 +C L . If the learner decides to acquire an example with only the first attribute value and the class label first, and later to acquire the second attribute value, the cost is reduced to be C 1 +C 2 +C L . This is partial exampl e acquisition. Thus, the cost C 3 is saved. On one hand, the learner shoul d try to avoid acquiring more examples (or more attribute values) than necessary. On the other hand, the learned model is usually more accurate, and thus has lower misclassification cost, when it is built with more examples and examples with fewer missing values. The goal of the learner is thus to minimize the total cost of data acquisition and misclassification on test examples. We propose and study a novel partial example acquisition strategy (called PAS ) in this paper. The rest of paper is organized as follows. We first review the relate d work in Section 2. In Section 3, we describe our data acquis ition framework and the evaluation method. Section 4 discusses th e complete example acquisition strategy (called COM ) and partial example acquisition strategy (called PAS ). We conduct experiments to compare COM and PAS , and analyze the results in Section 5. Finally we conclude the work in Section 6. Cost-sensitive learning is an active research topic in recent years. Turney [20] gives an excellent survey on a variety of costs that may be considered in learning, such as misclassification costs, data acquisition cost (including ex ample costs and attribute costs), active learning costs, comput ation cost, human-computer interaction cost, and so on. Most previous works on cost-sensitive learning only consider minimizi ng misclassification costs (e.g., [5, 20, 6, 18]). A few works do attempt to minimize the sum of misclassification costs and attr ibute costs [21, 4, 24, 11]. However, these works do not consid er data acquisition cost. That is, they assume that a fixed set of training examples is given, and the leaner cannot acquire additional information during learning. Some previous works study data ac quisition cost, such as [10, 9, 23, 13, 14]. Among them, some [10, 9] study how to acquire attribute values to build an op timal classifier with a certain budget. The others [23, 13, 14] study how to achieve a desired accuracy of a classifier by acquiring missing values in training examples with minimum cost. However, they do not minimize the total cost, which combined the acquisition cost and the misclassification cost. In our work, the cost of attribute acquisition and misclassification is unified as the same currency of cost, and the task is to minimize the total cost. We can also put a  X  X udget X  on the total cost, thus our proposed algorithms can be easily adapted to meet such total budget. Active learning can be viewed as a specific form of data acquisition. The most popular type of active learning is called  X  X ool-based X  active learning. Many works have been published in recent years on pool-based active learning, including, for instance, [19, 22, 16, 17, 12, 1] (See [1] for a good review of active learning approaches). All these works assume that a pool of unlabeled examples is given, and the learner can choose which ones to acquire their labels duri ng learning. In our work, we do not have this assumption. We do not assume that there is a pool of examples given. In addition, almost all of these works on active learning are evaluated by accuracy (or learning curves with accuracy) and  X  X eficiency X  of active learning [1], as well as the number of examples acquired. Ho wever, we propose a new cost-sensitive acquisition framework (based on the actual cost misclassification cost. In additi on, our learning framework can also acquire partial examples. Our cost-sensitive acquisition frame work and two data acquisition strategies (to be presented in S ections 3 and 4) utilize the cost-sensitive decision tree ( CSDT in short) as a base learning algorithm; thus we briefly review it here. CSDT [11] is similar to C4.5 [15], but it uses the total cost of attributes and misclassifications, instead of entropy, as the attribute split criterion. At each step, CSDT always chooses an attribute from the available attribute set with the maximum cost reduction on training data, similar to maximu m entropy reduction, to build decision trees. Cost reduction (E X  X  A  X  X  C ) is the difference between the expected cost E before splitting and the sum of the expected cost E A after splitting with attribute A and the total attribute costs T C . The attribute with the largest cost reduction, if it is greater than 0, is chosen to split the training data (otherwise it is not worth to build the branch further, and a leaf is formed). The same procedure is applied recursiv ely to build subtrees. However, CSDT and C4.5 assume that all examples are available to build the tree. In our work, we assume that it is possible for the learner to acquire more examples in the tree building. In this section we will describe our cost-sensitive data acquisition framework and algorithms. The framework of our cost-sens itive data acquisition algorithm is quite simple. It is like a  X  X rapper X  and can be applied to any cost-sensitive learning algorithm, such as ICET in [21], MetaCost in [5], csNB in [4], and CSDT in [11], as the base learner. At a high level, the data acquisition is a simple on-line process. That is, it acquires examples at cost gradually while monitoring an evaluation criterion until it is met. The evaluation computes the sum of the acquisition cost and misclassification cost of test examples to see when it reaches the minimum (details are presented in next subsection). Ex amples given to the learner are drawn randomly according to a certain fixed distribution. As the effect of one extra example is often too small, the learner always acquires units of examples in each acquisition. In this paper we set a unit to have 10 examples. More specifically, each time, the learner acquires one unit or a number of units of examples, a nd includes them in the training set. Then a new cost-sensitive learning model is built by the base learner from the expanded training se t, and is evaluated to see if the total cost of the example ac quisition and misclassification of the learner stops acquiring more units of examples, and the current learned model is produced. We re-implement and use cost-sensitive decision tree (called CSDT ) as the base learner in the rest of the paper. This is because CSDT (reviewed in Section 2) is itsel f cost sensitive, and it is also very fast in building many decision trees needed in the data acquisition process. The pseudo-c ode of the cost-sensitive data acquisition algorithm is presented below. We assume that the learner is given an initial set T of examples ( T can be empty), and that each time some units of training examples are acquired and added into the training set. Algorithm Input : an initial training set T , and a stopping criterion. While stopping criterion is not met a. Acquire training examples with certain attribute values b. Call CSDT to build a cost-sensitive tree on T c. Evaluating the tree Output : a cost-sensitive decision tree We will first discuss the evaluation method in the next subsection. One might think that it would be easy to calculate the total cost of misclassification and example ac quisition  X  just sum them up. There are quite a few intriguing issues to be resolved, as we discuss as follows. To evaluate the tree (or any learned model) built in the acquisition procedure for future test performance, we must use a part of available training examples as a held-out set. These examples are not used in building the models. However, from the cost-sensitive point of view, holding out some examples for testing excludes them from building the model, making some acquiring examples wasted. To reduce the waste of acquired training examples used for testing, we use leave-one-out cross-validation ( LOO in short) to evaluate the learned model, so only one example is  X  X asted X . As the decision tree learning algor ithm is quite efficient, this would not be a major problem in most real-world applications. Thus all available examples except one are used to train the decision tree to estimate the average misclassification cost of a test example in LOO . The following procedure describes details on how to estimate the misclassification cost of one test example (in LOO ). For binary classification (used in this pape r), we use the following notations: TP and FP are the cost of true and false positive, TN and FN are the cost of true and false negative, tp and fn are the number of true positive and false negative examples, and tn and fp are the number of true negative and false positive examples. For a leaf in the cost-sensitive decision tree, let C P (= tp  X  total misclassification cost of being a positive leaf, and C (=tn  X  TN+fn  X  FN) be the total misclassification cost of being a negative leaf. Then the probability of a leaf being positive is estimated by the relative cost of C P and C N ; the smaller the cost, the larger the probability (as minimum cost is sought). Thus, the thus: being a negative is used directly in estimating misclassification costs, because the number of training examples in l eaves is usually very small, especially at the beginning of ex ample acquisition. To reduce the effect of extreme probability estimations, we apply the Laplace correction [8, 3] to smooth probability estimates in leaves. We modify the original Laplace based on accuracy for estimation with misclassification cost. The original Laplace correction for accuracy can be expressed as examples which belong to class C, N is the number of training examples, and n is the number of classes. As we consider misclassification costs now, the probability of a leaf being positive is is negative example is FP C The next issue is how to integrate the misclassification cost of one test example (obtained above) with the cost of training examples acquired. A simple sum of the tw o, as in [12], would not be reasonable, as it depends on how many future test examples (or how often) the model will be used to predict. Clearly, if the model built will be seldom used (only on ce or twice), we can reduce the total cost through building a rough model with acquiring only a few examples. On the other hand, if the model built will be used very frequently (for instance, millions of times), it would be worthwhile to acquire more examples to build a highly accurate model with very low misclassifi cation cost. As we may not know the number of future test exam ples during the model building process, we introduce a variable t to represent the number of future test examples. As the cost of acquiring training examples is shared by all the test examples, each test example has to burden example, and Tr is the number of acqui ring examples. The total cost, which is the sum of the share of the cost of acquiring training examples and the misclassification cost, is thus: where MisCost is the average misclassification cost of one example. Section 5.5 studies the effect of different t values for the cost-sensitive data acquisition algorithm. The next issue is how to use th e total cost as a stopping criterion for the incremental cost-sensitive active learners. Ideally, we can expect the total cost to decreas e initially, and it would then reach a minimum before it goes up. This is because the gain in reduced misclassification cost would be deducted as more and more examples or labels are acquired (with a constant cost for each complete example, or with a vari ant cost for each partial example, see Section 4). Thus, we can obtai n a learning curve in terms of the total cost and the number of examples required (see Figure 3). If the curve is smooth, then indeed the learning algorithm has found the optimal number of traini ng examples needed. However, as we will see in Section 5, the cu rve of the total cost may not be smooth, and the local minimum ma y not be the global one. Thus it is necessary to  X  X ook ahead X  and acquire a few more (units of) examples to ensure that the local minimum is global. The extra examples in LOO and look-ahead will be extra (or wasted) on top of the minimum total cost found. We will describe the look-ahead strategy obtained empirically in Section 5.1. There are different strategies to acquire training data. In this section, we first describe the si mplest data acquisition strategy, that is, the complete data acquisition strategy ( COM ). COM acquires training examples with all attribute values and their labels. It will be used as th e baseline to compare with the PAS (partial data acquisition strategy) in Section 4.2. The complete attribute strategy , which acquires additional training examples with complete a ttribute values, is quite simple. In the pseudo-code presented in Figure 1, the step 1(a) simply stipulates that all attributes should be acquired during example acquisition. That is, this strategy does not consider omitting some attribute values for reduced cost; in the next section, we consider the partial attribute strategy which does. If the learner is able to acquire partial examples with a subset of attribute values at reduced cost, it may help to reduce the total cost in the end. Intuitively, the attributes which do not appear in the final decision tree would not help in the tree construction. A simple idea is to simply acquire partial examples w ith attributes appearing in the cost-sensitive decision tree. For th e example described in Section 1, if only attributes a 1 and a 2 appear in the final decision tree, we only need acquire the values of th ese two attributes with the class label for each example. However, we do not know the final cost-sensitive decision tree at the beginning; thus, we do not know which attributes will appear in the final cost-sensitive decision tree. PAS  X  X uesses X  the attributes in the final decision tree based on the current tree, by  X  X xtending X  the current tree with more potentially useful attributes, and acquire examples with those attributes. Hopefully the attributes in the current tree plus the extended attributes can  X  X over X  attributes in the final decision tree. More specifically, PAS consists of the following four major steps (as the Steps 1(a) and 1(b) in the framework in Figure 1). Details of these steps will be explained later. We will provide details of each step below. In the first step, it is possible that when the tree is re-constructed, a different attribute is chosen as the root (or internal node), and some examples may not have values on this attribute. In this case, PAS will acquire unknown values on the path of classifying them to a leaf in the tree built. Though this may be cumbersome to acquire attribute values of some previously acquired examples in real-world applications, it does save the overall acquisition cost. Figure 2. An example of an intermediate cost-sensitive decision tree. In the second step, to save the acquisition cost, PAS uses a conservative  X  X equential strategy  X , explained using the example shown in Figure 2. Supposed there is an intermediate tree built by CSDT shown in Figure 2, PAS utilizes the intermediate tree structure to guide the sequence of attributes of the example to be acquired in the following way: PAS first acquires the value of the attribute in the root of the tree. That is, the value of attribute A Figure 2. According to the value of attribute A 2 acquires the value of attribute A 3 or A 4 . For example, if the attribute A 2 has a value of 1, PAS will acquire the value of attributes on the path one by one until it classifies this example into a leaf. In the third step, for each leaf, PAS determines the potentially useful attributes, which are possibly used to extend the current intermediate cost-sensitive d ecision tree. Naturally, all the remaining attributes that do not appear on the path of the leaf are candidates. For example, the remain ing attributes of the left-most leaf L 1 in Figure 3 are attributes A 1 , A 4 , A 5 and A complete attribute set is (A 1 , A 2 , A 3 , A 4 , A leaf, PAS computes the cost reduction of its remaining attributes and chooses the attributes whose cost reduction is not less than a lower bound value L B as the potentially useful attributes. The lower bound value corresponds to the expected total misclassification cost E before splitting, the expected total misclassification cost E A after splitting on attribute A and the total cost T C of acquiring the values of attribute A for all examples in the leaf. It is defined as where f is a variation coefficient with a small given value, controlling the size of potential attributes of the leaves. The larger the value of f , the more attributes are chosen as potentially useful attributes; however, the more attribute values acquired could be wasted. In this paper, we let f be 0.05 unless otherwise stated. We chooses an attribute as a potentially useful attribute that has the property: (1+f) X E-(1-f) X (E A +T C )  X  0 . PAS uses the same formula used in building CSDT to compute the expected total misclassification costs before a nd after splitting for each attribute not on the path to the leaf. PAS can be regarded as a beam-search strategy with an adaptive beam size. The beam size is automatically determined by PAS according to the examples in each leaf. PAS localizes the attribute selection in each leaf. It is possible that PAS chooses zero or several attributes as the potentially useful attributes for a leaf. As the example in Figure 2, A 4 and A 6 are the 2 potentially useful attributes from the left-most leaf L 1 . However, there are no extended attributes for the right-most leaf L 4 . In the fourth step, PAS further acquires values of those extended attributes for all examples in each leaf. For example, PAS further acquires the values of the two extended attributes (A examples in leaf L 1 . Note that some unknown attribute values of some previously acquired examples may be acquired again in this step. Again, though this may be cumbersome in real-world applications, it does save the overall acquisition cost. In sum, PAS uses the heuristics of guessing the attributes in the final tree using the intermediate trees, and only acquires the values of these attributes (part of attributes) for each example. Thus, the acquisition cost can be saved significantly, comparing with COM . If the misclassification cost does not increase too much, the total cost of PAS would be less than that of COM . We investigate this in the next section (Section 5). We conduct experiments with PAS and COM on 10 real-world datasets downloaded from the UCI Machine Learning Repository [2]. These datasets are chosen because they have at least some discrete attributes, binary cla ss, and a good number of examples. The numerical attributes in datasets are discretized first using minimal entropy method [7] as CSDT can currently only deal with discrete attributes. We also tr y to choose datasets with very different number of attributes. Th e number of attributes and other features of these datasets are listed in Table 1. If misclassification costs of these datasets are not available, we assign them with values in a reasonable range, followi ng [5, 11, 24, 4]. This is fair and reasonable as all experime ntal comparisons are conducted with the same cost assignments . We assign random values from 0 to 100 to test costs. We assign th e labeling cost to be 100, and the misclassification costs as FP/FN = 1000/3000 ( TP=TN=0 ). We also assume that the learned model will be tested on 1,000 test examples (i.e., t=1,000 ) for now. Table 1. Features of the 10 Datasets used in the experiments. To compare the performance of COM and PAS , we set the initial training set with 30 examples. The original datasets listed in the Table 1 are the natural pool of examples to be acquired. COM and PAS randomly acquire units of extr a examples from these original datasets to make their own training sets. For COM , all attribute values of each fresh example are available. However, for PAS , the attribute values of each fresh example will be given only when it is requested. In addition, we re peat all our experiments 10 times (randomly selecting the initial training set), and only average values are displayed in following figures and tables. We first conduct experiments for th e complete attribute strategy COM (Section 4.1). The experiment results are shown in Figure 3, (Mushroom). Again, the unit m is chosen to be 10 in all experiments. The horizon axis is the number of acquired examples, and the vertical axis is the total cost, which is sum of the share of the cost of acquiring training examples and the average misclassification cost of t future test examples, assuming similar curves). From Figure 3, we can see clearly that the LOO curve produced by COM has a desirable trend: it goes down at the beginning (after a few units of training examples are acquired), and then it goes up after a certain point, fo rming a global minimum (the minimal total cost). All datasets produce similar curves as shown in Figure 3. Figure 3. The ideal total cost curves with COM . The vertical axis is the total cost, and the horizontal axis is the number of example units acquired. This is actually the experimental results on the dataset Mushroom.
 However, the LOO curve is not always smooth, thus look-ahead is needed to find the global minimum. We have calculated the minimum length of look-ahead needed for each dataset in Table 2, and have found that the number of look-ahead can be set to be 2. This is relatively small for the to tal cost wasted in searching for the global minima. Note that the total costs shown in the paper have included the 2 units of look-ahead. Table 2. The minimum length of look-ahead needed for COM to find the true global minimum in the LOO curve for each dataset. We first conduct experiments to investigate the performance of PAS , and then compare it with COM . Again, in these experiments, PAS and COM acquire 30 complete examples as the initial training data. The cost of this initialized acquisition is counted into the total acquisition cost. Th e experimental results are shown in Figure 4, which displays the learning curves for each dataset. For each curve, the horizon axis is the number of acquisition times, and the vertical axis is the average total cost. The smaller the average total cost is, the better. Note that the segments of the curves after the global minima are computed and plotted for demonstrating the increment of the total cost only. In reality, COM and PAS would stop soon after the minimum (due to look-ahead). From Figure 4, we can draw severa l interesting conclusions. First, all curves of PAS and COM have a similar trend: they go down at the beginning after a few units of training examples are acquired, and then they go up, forming global minima. That is, the performance of PAS and COM become better at the beginning of acquisition; and they become wors e after more acquisitions occur. Figure 4. Comparing PAS with COM on the 10 datasets. The smaller the average total cost is, the better. Second, PAS outperforms COM significantly on seven out of the ten datasets (i.e., the first seven datasets in Figure 4). This is because PAS indeed reduces the acquisition cost significantly, comparing with COM . The most interesting thing is the cost-sensitive decision tree built on the partial examples acquired by PAS performs similar to or better than the tree built on the complete examples acquired by COM on the seven datasets. We further explain this by analyzing the acquisition cost and misclassification cost separa tely in Section 5.3. Third, PAS often achieves its global minimum through acquiring more examples. As PAS spends less acquisition cost for acquiring partial examples than COM does for acquiring complete examples in each acquisition, this allows PAS to acquire more partial examples with less total cost. In the subsection above (Section 5.2), the experimental results show that PAS performs better than COM on seven out of the ten datasets in terms of the average to tal cost. In this subsection, we further analyze the details of why PAS performs better or worse than COM . As the total cost is the sum of the acquisition cost and misclassification cost, we investigate their performances in terms of the acquisition cost and the misclassification cost respectively, to see which cost is the most important contributor. The environment parameters are the sa me as the subsection above. As the trend of the performance of PAS and COM is similar in some datasets, particularly the curves of the acquisition cost for each dataset, we only display the curves of the acquisition cost and the misclassification cost of PAS and COM on three typical datasets (i.e., Ecoli, Mushroom, and Voti ng). The three typical datasets represent three cases: the misclassification cost of PAS is lower than that of COM ; the classification cost of PAS is slightly higher than that of COM ; the misclassification cost of PAS is much higher than that of COM . The experimental results of the three datasets are shown in Figure 5. Similar to Figure 4, for each curve, the horizon axis is the num ber of acquisition times, and the vertical axis is the average acquisition cost and misclassification cost respectively. Again, the smaller the average total cost is, the better. 
Figure 5. Acquisition cost and misclassification cost of three typical datasets (i.e., Ecoli, Mushroom, and Voting). The smaller the average total cost is, the better. From Figure 5, we can see that the acquisition cost increases as more information (including pa rtial examples and the extra attribute values for existing ex amples) is acquired. This is expected. However, the rate of increment of acquisition cost is different between COM and PAS . We can see that the rate of increment of COM is much higher than that of PAS . For the misclassification cost, we can see that the misclassification cost decreases after more information is acquired. However, the extent of the decrement of the misclassification curves under PAS and COM is also different. For example, the misclassification cost curve of PAS goes down faster than that of COM on the dataset Ecoli. However, the misclassification cost curve of PAS goes down slower than that of COM on the dataset Mushroom, and much slower on the dataset Voting. For the first two cases, PAS performs better than COM in terms of the total cost, since the acquisition cost is the major factor that determines which is better between PAS and COM . This is the reason why PAS performs better than COM on the first seven datasets. However, for the last case, the misclassification cost plays the important role in the total cost. This is the reason why PAS performs worse than COM on the other three datasets (i.e., Kr-vs-kp, Voting, and Cars). In the subsections above, we do a ll experiments with the variation coefficient value f=0.05 (see equation 2) in PAS . Again, the variation coefficient is a small value. In this subsection, we investigate the performance of PAS with different variation coefficients, such as, f=0.01, f=0.05 , and f=0.1 . The experimental results of the three different variations of PAS on the typical dataset (Mushroom) are shown in Figure 6. 
Total cost Figure 6. The total cost of the typical dataset ( Mushroom ) under different variation coefficients in PAS . From Figure 6, we can see that the performance of PAS is slightly different under the different values of the variation coefficient on the dataset Mushroom. Particularly, their global minima are very close. In general, their total costs are almost the same before the global minimum point. After that, th e total cost decreases slightly when the value of the variation coefficient decreases, as the acquisition cost decreases when the value of the variation coefficient decreases. This is exp ected, as the smaller the value of the variation coefficient, PAS is more strict to choose potentially useful attributes. Thus, th e information acquired by PAS is less possible to be wasted. Figure 6 only shows the performance of PAS with different variation coefficients on the ty pical dataset (Mushroom). As PAS performs similar on other datasets , we do not show the learning curves of PAS for them. We summarize the performance of PAS for all the ten datasets (including Mushroom) in terms of the global minimum total cost in Table 3.
 Table 3. Summary of the global minima for each dataset under the three variation coefficients with the assumption the number of future test examples t =1,000. Tic-tac-toe 657.5 641.5 641.2 Mushroom 236.2 234.5 231.6 From Table 3, we can conclude that the value of the variation coefficient does not affect the performance of PAS significantly, although the minimum average total cost of PAS decreases slightly when the variation coeffi cient becomes smaller. This is because the acquisition cost decreas es slightly when the variation coefficient is smaller. Thus, PAS can work well if its variation coefficient is assigned a small value. In the subsections above, we assume that there are t=1,000 future test examples. In this subsecti on, we further investigate the performance of PAS (with f =0.05) and COM under different sizes of future test examples. The experimental results of PAS and COM with the different sizes ( t=250, 500, and 1,000 ) of the future test examples for the ty pical dataset (Mushroom) are shown in Figure 7. 
Figure 7. The total cost of the typical dataset ( Mushroom ) under different sizes of future test examples. From Figure 7, we can conclude that the average total cost drops down when the number of future test examples increases. This is because there are more test exampl es to share the acquisition cost, for the same amount of inform ation acquired. It is most interesting that PAS can achieve a much lower average total cost than COM does with all the three sizes of future test examples. When the size of future test examples is smaller, the difference between PAS and COM is greater. Figure 7 shows the learning curves of PAS and COM under different sizes of future test examples on the typical dataset (Mushroom). For other datase ts, the learning curves of PAS and COM show the similar relationship. We do not display them here. Instead, we also summarize the global minimum total costs for the 10 datasets under the three sizes of future test examples in Table 4 Table 4. Summary of the global minima for each dataset under the three different sizes of future test examples. Mushroom 496.0 419.1 401.8 302.3 317.6 234.5 From Table 4, we can conclude that the size of future test examples affects the performance of PAS and COM . When the size of future test examples increases, both PAS and COM can achieve lower global minimum on a ll the 10 datasets. We also can see that PAS consistently performs better than COM on the seven datasets, and worse than COM on the other three datasets. In the subsections above, the mi sclassification costs are assigned as FP = 1,000 and FN = 3,000 . (That is, the cost ratio is 1:3.) In this subsection, we inve stigate the performance of PAS under more extreme ratios of misclassification costs. Besides the ratio of the misclassification costs ( FP = 1,000, FN = 3,000 ), we set the pair of misclassification costs with more extreme ratios, such as, ( FP = 1,000, FN = 10,000 ) and ( FP = 1,000, FN = 20,000 ). The experimental results on the typi cal dataset (Mushroom) under the three different ratios are shown in Figure 8. From Figure 8, we can see that the total costs of PAS and COM increase when the sum of the false positive and false negative costs increases. And when the sum of the pair of the misclassification costs increases, both PAS and COM intend to acquire more information (i.e., taking more iterations) to reach its global minimum points. This is what we expected, as both PAS and COM intend to acquire more information to build a more accurate model to reduce the probability of misclassification when the misclassification cost increases. In addition, PAS always misclassification costs. Again, the lower the total cost, the better the performance is. Figure 8. The total cost of the typical dataset (Mushroom) under different ratios of misclassification costs. Figure 8 shows the learning curves of PAS and COM with different ratios of misclassifica tion costs on the typical dataset (Mushroom). For other datasets, the experimental results show that both PAS and COM hold the same corresponding relationships as the dataset (Mus hroom). Again, we do not show them here. Instead, the global minimum of all the 10 datasets under the three different cost ratios are summarized in Table 5. Table 5. Summary of the global minima for each dataset under the three different ratios of misclassification costs. Ecoli 189.7 153.6 360.4 267.3 543.8 399.3 Breast 358.2 335.5 1010.5 837.8 1927.0 1504.6 Heart 559.4 556.6 1061.9 830.5 1514.2 920.0 Thyroid 310.7 258.5 855.5 683.1 1433.6 983.7 Australia 493.8 492.3 1218.7 900.9 1850.6 1117.1
Tic-tac-toe Mushroom 317.6 234.5 740.9 576.1 1085.4 926.9 Kr-vs-kp 485.7 609.0 908.7 942.7 1310.9 1059.2 Voting 227.9 295.7 542.7 572.1 887.4 851.3 Car 480.6 591.4 1206.8 888.7 1856.2 1053.4 Average 411.5 416.9 875.5 724.7 1333.3 963.0 From Table 5, we can conclude that both PAS and COM have higher global minima on all the 10 datasets when the sum of the misclassification costs increases. PAS performs consistently better than COM on the first seven datasets under all the cost ratios. The difference between PAS and COM becomes greater and greater when the sum of misclassification costs increases. It is also interesting that PAS performs better than COM on the other three datasets (Kr-vs-kp, Voting, and Car), when the ratio of the misclassification costs increases, pa rticularly on the dataset Car. In all, PAS performs much better than COM when the ratio of the misclassification costs is extreme. In this paper, we propose tw o data acquisition strategies: complete attribute strategy ( COM ) and partial attribute strategy ( PAS ) in the cost-sensitive framework. The simple complete attribute strategy ( COM ) acquires examples with all attribute values and labels. However, partial attribute strategy ( PAS ) acquires only a subset of attribute values for each example. It saves the acquisition cost. Our experimental results and comparative studies from the UCI datasets show that the partial a ttribute strategy is an effective method. The higher the misclassifi cation costs, the greater the improvement of PAS is, comparing with COM . In our future work, we will investigate how PAS acquires only once the attribute values of the partial examples acquired. [1] Baram, Y., El-Yaniv, R., and Luz, K. 2004. Online Choice of [2] Blake, C.L., and Merz, C.J. 1998. UCI Repository of [3] Cestnik, B. 1990. Estimating probabilities: A crucial task in [4] Chai, X., Deng, L., Yang, Q., and Ling,C.X.. 2004. Test-Cost [5] Domingos, P. 1999. Meta Cost: A General Method for [6] Elkan, C. 2001. The Foundati ons of Cost-Sensitive Learning. [7] Fayyad, U.M., and Irani, K.B. 1993. Multi-interval [8] Good, I.J. 1965. The estimation of probabilities: An essay on [9] Kapoor, A., and Greiner, R. 2005. Learning and Classifying [10] Lizotte, D., Madani, O., and Greiner R. 2003. Budgeted [11] Ling, C.X., Yang, Q., Wa ng, J., and Zhang, S. 2004. [12] Margineantu, D.D. 2005. Active Cost-Sensitive Learning. In [13] Melville, P., Saar-Tsechansk y, M., Provost, F., and Mooney, [14] Melville, P., Saar-Tsechansk y, M., Provost, F., and Mooney, [15] Quinlan, J.R. 1993. C4.5: Programs for Machine Learning . [16] Roy, N., and McCallum, A. 2001. Toward optimal active [17] Saar-Tsechansky, M. and Provost, F. 2004. Active sampling [18] Ting, K.M. 1998. Inducing Cost-Sensitive Trees via Instance [19] Tong, S., and Koller, D. 2001. Support vector machine active [20] Turney, P.D. 2000. Types of cost in inductive concept [21] Turney, P.D. 1995. Cost-Sensitive Classification: Empirical [22] Zhang, T., and Oles, F. 2000. A probability analysis on the [23] Zhu, X. and Wu, X. 2005. Cost-constrained Data Acquisition [24] Zubek, V.B. and Diette rich, T. 2002. Pruning improves 
