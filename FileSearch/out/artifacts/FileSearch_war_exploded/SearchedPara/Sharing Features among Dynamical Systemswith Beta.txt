 encourage objects to share similar subsets of the large set o f possible behaviors. matrix F = { f among the series.
 let B continuous, we define the following L  X  evy measure on the product space [0 , 1]  X   X  : Here, c &gt; 0 is a concentration parameter ; we denote such a beta process by BP ( c, B B  X  BP ( c, B 0 ) is then described by where (  X  process with rate measure  X  . If there are atoms in B provide our sought-for featural representation. A realiza tion X particular, f X In many applications, we interpret the atom locations  X  Bernoulli process realization X given N samples X Here, m reordered the feature indices to list the K Computationally, Bernoulli process realizations X binary indicator variables f a previously tasted dish k with probability m m to SLDS [17]. Let y ( i ) dynamical mode. Assuming an order r switching VAR process, denoted by VAR( r ), we have standard HMM with Gaussian emissions arises as a special cas e of this model when A all k . We refer to these VAR processes, with parameters  X  As in Sec. 2, let f exhibits behavior k for some t  X  X  1 , . . . , T each object i we define a doubly infinite collection of random variables: Here,  X  denotes the element-wise vector product. This constructio n defines  X  ( i ) positive integers, but assigns positive mass only at indice s k where f Dirichlet distribution of dimension K The  X  hyperparameter places extra expected mass on the component of  X   X  ( i ) transition  X  ( i ) which is summarized in Fig. 1, as the beta process autoregressive HMM (BP-AR-HMM). on previous exact samplers for IBP models with non-conjugat e likelihoods. 4.1 Sampling binary feature assignments Figure 1: Graphical model of the BP-AR-HMM. The beta process distribu ted measure B | B these behaviors are indexed by { 1 , . . . , K  X  i transition variables  X  ( i ) =  X  ( i ) f Here, the IBP prior implies that p ( f ciency [14] than standard Gibbs samplers. To update f of Eq. (10) to evaluate a MH proposal which flips f To compute likelihoods, we combine f with object i . Let K f where H is the gamma prior on transition variables,  X  features, and  X  the existing unique features by n  X   X  or eliminate one of the existing features in f Let n moves: a new feature is created with probability 0 . 5 , and each of the n with probability 0 . 5 /n where b with n  X  parameters, the Jacobian term normally arising in RJMCMC al gorithms simply equals one. 4.2 Sampling dynamic parameters and transition variables fied if we instantiate the mode sequences z ( i ) Given feature-constrained transition distributions  X  ( i ) and dynamic parameters {  X  the observation sequence y ( i ) messages m Because Dirichlet priors are conjugate to multinomial obse rvations z ( i ) generated from feature-constrained transition distribut ions, n ( i ) Thus, to arrive at the posterior of Eq. (18), we only update  X  ( i ) inverse-Wishart (MNIW) prior [26] on { A on  X  ing sufficient statistics based on the sets Y observations and lagged observations, respectively, asso ciated with behavior k : Following Fox et al. [5], the posterior can then be shown to eq ual 4.3 Sampling the beta process and Dirichlet transition hype rparameters parameter  X  . Let F = { f where, as before, K Gamma ( a Transition hyperparameters are assigned similar priors  X   X  Gamma ( a fixed variance  X  2 Here,  X  =  X  2 / X  2 switched between AR(1) models with a by a a k  X  X  X  0 . 4 , 0 . 8 } assuming all objects are performing minor variations of the same dynamics. on  X  . The gamma proposals used  X  2 blocks, with feature labels unique to that sequence. our performance both to the HDP-AR-HMM and to the Gaussian mi xture model (GMM) method to discover the shared feature structure. Acknowledgments
