 We introduce smoothing of retrieval effectiveness scores, which balances results from prior incomplete query sets against limited additional complete information, in order to obtain more refined system orderings than would be possible on the new queries alone. Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and software  X  performance evaluation General Terms: Measurement, Performance, Experimentation Keywords: Score aggregation, system ordering, smoothing, partial information When selecting an information retrieval system, the three factors that influence the decision are the storage occupied by the system and its files, the speed of index construction and querying, and the effectiveness of the retrieval results.

To evaluate effectiveness, and allow system comparisons to be made, it is usual to use a test collection, comprising a set of doc-uments, a set of queries deemed to be somehow representative of what the retrieval service will deal with in practice, and a set of relevance judgments that indicate which documents are relevant to which query. If each of the test systems is used to index the docu-ment collection and execute each of the queries, and scored using an effectiveness metric, an overall average score for each system can be computed, and used to order the systems. Typically, the more queries that are available as part of the test collection, the more stable the overall system ranking is likely to be, providing a direct tension between experimental cost, measured in terms of the number of relevance judgements to be carried out; and experi-mental stability, measured as the lik elihood of having generated the  X  X orrect X  system ordering.

In this paper we describe a method for blending partial prior sys-tem scores into extended experiments, so as to increase experimen-tal stability, without increasing expe rimental cost. The scenario we consider is this: we suppose that two different sets of retrieval sys-tems have already been ordered using different associated sets of queries, and that we now wish to prepare an overall system ordering that combines the two. What process should be adopted to merge the two separate orderings? And can the prior different-queries sys-tem scores be used to advantage, to reduce the cost of creating the overall ordering?
More precisely, we suppose that a set of retrieval systems S x has been evaluated on a set of topics Q a , and that a disjoint set of
Smoothing is a process that allows approximate statistics to be computed from insufficient data [1]. To allow blending of effec-tiveness scores between prior knowledge and new observations, a smoothing term is introduced into the effectiveness metric. This allows the new observations to be tempered by other information, even if that other information is not provided with the same cover-age of systems as the new. For example, we propose that smoothed average precision for a system s on query q in the context of prior information for a query set Q be computed as: where MAP s,Q is the mean average precision of system s on query set Q ;AP s,q is the average precision of system s on an additional query q ;and  X   X  [0 , 1] is a constant.

Once smoothed average precision for Q c in the context of Q can be cal-culated, and used in place of mean average precision. Those ad-justed scores can then be used to obtain a system ordering in which the system behavior on the new queries Q c is moderated by the prior scores obtained by the systems on Q . Note that the query set Q c is same for all systems being compared, so that the score com-parisons are fair; but that the smoothing query set Q does not have this restriction, and might differ on a system-by-system basis. Note also that smoothing can be applied to any effectiveness metric, and in our experiments we have used both smoothed average precision and smoothed standardized average precision [3]. To examine the effect of smoothing on the accuracy of the sys-tem ordering, we used the 249 topics prepared for the 2004 TREC Robust track [2], and the 110 systems that submitted runs against them. To perform each experiment, we randomly sampled three sets of 25 mutually exclusive topics to form Q a , Q b and Q c ,and randomly split the 110 systems S into 55 systems for S x and 55 systems for S y . Scores were then computed for S  X  Q c ,for S x  X  Q ,andfor S y  X  Q b , as shown in Figure 1. We also computed, as a baseline, the experiment S  X  ( Q a  X  Q c ) , to measure how stable the system orderings would be if 50 topics could be used against the full set of systems.

The various system orderings resulting from each experiment (baseline, plus  X   X  X  0 , 0 . 5 , 0 . 8 , 1 } ) were then compared against the  X  X hole population X  system ranking obtained using all of the 249 queries, and Kendall X  X  tau rank correlation scores computed. This experiment was then repeated 10 , 000 times, with two differ-ent effectiveness metrics used to generate the scores.

Figure 2 shows the  X  distribution arising when AP is used as the effectiveness metric. The inclusion of the two sets of background scores means that a system ordering is generated that on average is closer to the presumed ground truth than the system ordering generated by the background queries only (when  X  =0 ), or by the set Q c only (when  X  =1 ). In this experiment, the best value of  X  was approximately 0 . 8 , with the prior scores given one quarter of the weight of the all-systems scores on the Q c query set. The system ranking that results is still not as accurate as can be obtained if 50 queries are used against all of the systems (the baseline in the graph), but nor is it as expensive to compute.

Figure 3 shows the same experiment, but with standardized aver-age precision used as the underlying effectiveness metric, with the standardizing factors computed according to the subset of queries and topics being used in each case. The same overall pattern of behavior can be observed, providing further evidence of the useful-ness of smoothing. Now the best value of  X  is 0 . 5 .
