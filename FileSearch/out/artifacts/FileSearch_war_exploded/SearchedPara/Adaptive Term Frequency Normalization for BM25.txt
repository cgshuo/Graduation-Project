 A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula. The scale and shape of this TF normalization component is controlled by a parameter k 1 , which is generally set to a term-independent constant. We hypothesize and show em-pirically that in order to optimize retrieval performance, this parameter should be set in a term-specific way. Following this intuition, we propose an information gain measure to directly estimate the contributions of repeated term occur-rences, which is then exploited to fit the BM25 function to predict a term-specific k 1 . Our experiment results show that the proposed approach, without needing any training data , can efficiently and automatically estimate a term-specific k and is more effective and robust than the standard BM25. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms BM25, Term frequency, adaptation, information gain
The Okapi BM25 retrieval function [9, 11] has been the state-of-the-art for nearly two decades. The BM25 formula, as presented in [3], scores a document D with respect to query Q as follows: where c ( q, Q ) is the count of term q in Q , N is the total number of documents, df ( q ) is the document frequency of q , and k 3 is a parameter.

A key component of BM25 contributing to its success is its sub-linear term frequency (TF) normalization formula: where is the pivoted normalization method [12] for document length normalization. Here b  X  [0 , 1] is the slope parameter, | represents document length, avdl stands for average docu-ment length, and c ( q, D )istherawTFof q in D .

The scale and shape of this TF normalization component is controlled by a parameter k 1 , which is generally set to a term-independent constant [9, 6, 3]. Although many studies have attempted to improve BM25 from various perspectives, e.g., [10, 4, 8, 7], in all these studies, the sub-linear TF normalization formula with a fixed and term-independent parameter setting remained the same as the original BM25 [11], and no work has attempted to improve the TF nor-malization of BM25 through adaptively normalizing TF for individual terms. In this paper, we propose to automatically set k 1 in a per-term basis.

Our work is motivated by the observation that k 1 is intu-itively related to the rate of increase in score contribution from matching an additional occurrence of a term. We no-tice that the optimal rate of score increase for a term gen-erally should depend on the global tendency of the term to be repeated. Specifically, the increase of score from oc-currences t to occurrences t + 1 is related to how many of those documents with at least t occurrences of the term in the collection actually have at least t + 1 occurrences of the term. Intuitively, if almost all documents containing at least t occurrences of the term actually contain at least t +1 occur-rences, the score increase should be smaller, whereas if very few of those documents actually contain at least t +1 occur-rences (i.e., they mostly contain exactly t occurrences), the score increase from t to t +1 should be larger. Our intuition that k 1 should be set in a per-term basis is also confirmed in the empirical plots in Figure 1.

Following this intuition, we develop an unsupervised ap-proach to adaptively predict the optimal k 1 value for each term based on the counts of documents containing different levels of TF for a query term. Specifically, we propose to measure the contribution of the occurrences of a query term using the information gain of  X  X bserving one more token of the same query term X , which can be estimated directly based Figure 1: Comparison of different strategies for op-timizing k 1 for TREC query topic 422, where we optimize k 1 for the whole collection ( X  X ollection X ), for this particular query ( X  X uery X ), and for each in-dividual term (while we use the setting of  X  X uery X  for the other two terms), respectively. on term statistics, and then we can fit the BM25 function with these contribution scores to solve a k 1 for each term.
Our experiment results on multiple test collections show that the proposed approach can effectively and efficiently estimate a term-specific k 1 , and that it not only achieves better retrieval performance than a well-tuned BM25, but also is more robust than the standard BM25.
We propose to measure the contribution of the observed t occurrences of a query term q in document D using the in-formation gain (denoted as IG t q ) of  X  X ncountering one more occurrence of q  X  in the current document D . Following the divergence from randomness (DFR) framework [1], we also assume that the probability that the observed term contribu-tions to select a relevant document is high, if the probability of encountering one more token of the same term in a rele-vant document is similarly high, i.e., if D really talks about q , q would be likely to appear again. The probability of a further success in encountering a term is thus a conditional probability that gradually increases as t increases. However, on the other hand, if successes were brought about by pure chance, then the conditional probability would be constant no matter how t changes, but this constant could still be very large. Considering these intuitions and analysis, we hypoth-esize that the information gain of  X  X ncountering one more token of the same term X  should be an appropriate measure of the contribution of the observed t term occurrences: Next, we introduce how to compute the information gain IG t q and how to estimate k 1 .
The probability of a further success in encountering a term q conditioned on the current observed t occurrences is de-noted as p ( t +1 | t, q ) in our paper. The use of this proba-bility is not entirely novel: the Laplace X  X  law of succession was used to estimate a similar probability in [1]. However, their methods do not consider the characteristics of q and are term-independent . In order to estimate a term-specific probability, we exploit term statistics of q . Specifically, we approximate p ( t +1 | t, q ) with the probability of randomly picking a document that will contain at least t + 1 occur-rences of q from a document set of which each document contains at least t occurrences of q , formally, where 0 . 5 and 1 are used to smooth the probability to avoid zero probabilities (can be interpreted as hyperparameters of a beta prior distribution). Similarly, we have an  X  X nitial occurrence X  probability p (1 | 0 ,q ), which is the probability of randomly picking a document that contains q :
Hence, the information gain of  X  X ncountering one more token X  is the change of information content from an initial state to the current state of observing t occurrences: IG t q =  X  log 2 df ( q )+0 . 5 where the definition of informative content as  X  log 2 P was given in semantic information theory [5].

However, it is well-known that longer documents tend to have higher TF. Following BM25, we also choose the piv-oted normalization method [12], as shown in Equation 3, to obtain the normalized version of TF as t = c ( q, D ). Yet there is another problem after document length normaliza-tion: t = c ( q, D ) is usually no longer an integer, and thus it may not make sense to talk about the probability of en-countering  X  X ne X  more token conditioned on c ( q, D ). Our solution is to simply calculate a list of IG t q values only for integers t  X  X  0 , 1 , 2 ,  X  X  X } , and then use a few sample IG values to fit BM25. To estimate the information gain for an integer t , we define df t ( q ) in a more formal way. df t ( q )= Here we use the rounding technique, which intuitively makes sense. For example, it is not reasonable to exclude a doc-ument D from df 2 ( q )if c ( q, D )=1 . 99. Empirical analysis also shows that this rounding technique works very well.
It is observed that, when t becomes very large, the esti-mated information gain values are often noisy due to docu-ment sparseness (i.e., df t ( q ) may be too small). To address this problem, we only calculate IG t q for t  X  X  0 , 1 ,  X  X  X  where T is chosen heuristically as the smallest integer that
It is interesting to see that, besides document length nor-malization, the information gain measure also achieves both an IDF effect and a term-specific TF normalization effect. traditional IDF formula. And according to the word bursti-ness phenomenon [2], as t increasesandbecomeslarge,the second component of IG t q generally would also increase, and the increasing rate should be term-dependent .
Empirical evidence shows that it is beneficial to set k 1 a per-term basis. However, how can we achieve this goal? Intuitively, k 1 is related to the  X  X core gain X  of matching a term t times, so we can try to measure  X  X nformation gain X  and connect k 1 with the proposed IG t q , i.e., we can now estimate k 1 by relating the BM25 TF component to IG t q .
Specifically, in Formula 4, we assume that the score dis-tribution of the information gain measure is proportional to the score distribution of BM25. Now we align the two distri-butions at a special point, t = 1. We know that when t =1, theleftsideandtherightsideoftheFormula4appeartobe idf ( q )and IG 1 q respectively. So we can align two distribu-tions by normalizing both sides of Formula 4 to make their score be 1 . 0when t = 1, formally, Because we have already obtained values { IG 0 q ,IG 1 q , we can estimate the optimal value of k 1 ( q ), i.e., the q -specific k , using a curve-fitting technique and the least square method:
The estimation of  X  k 1 ( q ) only relies on a few IG t q which can be calculated very quickly given the inverted in-dex. So the estimation process is very efficient. Moreover, because  X  k 1 and IG 1 q are generally not affected by online fac-tors, they can also be pre-computed offline.

Now we can solve IG t q by substituting Equation 10 into 9, and then use IG t q to replace the left side of Formula 4, which leads to a BM25-like retrieval function, called BM25-adpt, It is clear that BM25-adpt has fewer parameters than the standard BM25, since a term specific parameter  X  k 1 ( q )can be estimated automatically using Formula 10.

Moreover, comparing BM25-adpt with the standard BM25, there is also another interesting difference in the implemen-tation of IDF: BM25-adpt uses a novel IDF formula, IG 1 q From the definition of IG t q inFormula7,wecanseethat IG 1 is essentially a second-order IDF, which not only considers also captures randomness of the repeated occurrences (i.e., worksaseffectivelyasthetraditionalIDF. We used several standard TREC collections in our study: TREC8, Robust04, WT2G and WT10G. Queries were taken from the title field of the TREC topics. The preprocess-ing of documents and queries included Porter stemming and stopword removing using a total of 418 standard stopwords.
We used two variations of the standard BM25 function as our baselines,  X  X M25-basic X  and  X  X M25 X . They differ from each other only in the parameter setting. Specifically, in  X  X M25-basic X , k 1 was fixed to 1 . 2 as suggested in [6], while b was well tuned; this tuning strategy has been frequently used in many studies, e.g., [3], in order to reduce the effort of parameter tuning. In X  X M25 X , both b and k 1 were optimized;  X  X M25 X  represents the upper bound of the standard BM25 function, which, however, requires significantly more tuning effort than  X  X M25-basic X . In the proposed method, which Table 2: Performance comparison using cross vali-dation. is labeled as  X  X M25-adpt X , we only tuned parameter b since k was eliminated, so it requires the same amout of effort as  X  X M25-basic X  for parameter tuning. In all runs, k 3 = 1000.
In our experiments, the top-ranked 1000 documents for eachrunwerecomparedintermsoftheirmeanaveragepre-cisions (MAP). Note that the superscripts + and  X  indicate that the MAP improvements of BM25-adpt over BM25-basic and BM25 are statistically significant respectively.
We compare the optimal performance of different meth-ods in Table 1. It is observed that  X  X M25-adpt X  works effec-tively: its improvements over  X  X M25-basic X  are statistically significant in most cases, and it even outperforms  X  X M25 X  consistently. It suggests that our way of automatically set-ting a term-specific k 1 ( q ), needing significantly less effort for parameter tuning, outperforms the standard way of manu-ally tuning a term-independent parameter.

Comparison of optimal performance alone may raise some concern of over-fitting. So we further use a 5-fold cross-validation method to verify our observations on Robust04 and WT10G. The results are reported in Table 2. Over-all, the comparison results are consistent with our previous observations.

In practice, we may not have appropriate training data in the same domain for all queries. So we are also interested in the robustness of a retrieval algorithm when the training and test sets are from different domains. In this experiment, we simulate such a scenario: TREC, WT10G, and WT2G col-lections are used as the training sets respectively to train the retrieval functions, which are then evaluated on Robust04. The results are presented in Table 3. It shows clearly that the proposed method  X  X M25-adpt X  is more robust, and it significantly outperforms both  X  X M25-basic X  and  X  X M25 X . This suggests that our algorithms, although with less pa-rameters, perform more robustly.
Parameter b in BM25 controls the influence of document length in TF normalization. So we are interested in how the setting of b affects the retrieval performance of our method. We set k 1 =1 . 2 [6] in the standard BM25 algorithm so as to focus on the sensitivity of parameter b .Notethat b is the only parameter that we need to tune in the proposed al-gorithm  X  X M25-adpt X . The sensitivity curves are drawn in Figure 2 (left). We can see that the setting of b affects the re-trieval performance of both methods significantly. However, our method appears consistently more effective and robust Table 3: Performance comparison on Robust04 us-ing other collections for parameter training. than the standard BM25 when we vary b .Wealsoseethat when b deviates from its optimal setting, the MAP of the standard BM25 often drops more quickly than our method; one possible explanation is that our method can be able to adjust k 1 adaptively for each b .
 In the above analysis, we set k 1 =1 . 2 in the standard BM25. Since a better-tuned k 1 can often lead to better performance as we have shown in Figure 1, so we want to know how critical this k 1 is and how likely a standard BM25 with manually tuned k 1 can achieve performance similar to our method. We thus set the parameter b in each method to its corresponding optimal value, and then vary k 1from 0 . 2to2 . 0 to examine the sensitivity of k 1 ,whichisshown in Figure 2 (right). We can see that the standard BM25 is quite sensitive to k 1 , and even with the optimal setting for k , its performance is still lower than  X  X M25-adpt X .
BM25 has been a state-of-the-art retrieval function for a long time. The important parameter k 1 has so far been man-ually set to a term-independent constant based on a training data set. In this paper, we proposed a novel unsupervised approaches to automatically and adaptively set parameter k in BM25 for each term and each collection. Specifically, we developed an information gain measure to directly estimate the contributions of repeated term occurrences, which was explored to fit the BM25 function to solve a term-specific k without needing any training data. Our experiment results on multiple test collections show that the proposed approach works more effectively and robustly than the standard BM25 with a manually tuned k 1 . As a  X  X y product X  of the derived formulas, we effectively eliminated the k 1 parameter from BM25. Thus with the proposed adaptive BM25 formula, we only need to tune parameter b , which can reduce significantly the effort for parameter tuning.
 There are many interesting problems for future research. For example, how to also adaptively set another important parameter in BM25, i.e., b ? how to adaptively set k 1 at the query level instead of term-level, since terms in the same query should interact with each other in TF normalization?
We thank the anonymous reviewers for their useful com-ments. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0713581, CNS-0834709, and CNS-1028381, a Sloan Research Fellowship, and a Yahoo! Key Scientific Challenge Award.
