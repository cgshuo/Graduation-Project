 This paper studies document ranking under uncertainty. It is tackled in a general situation where the relevance pred ic-tions of individual documents have uncertainty, and are de-pendent between each other. Inspired by the Modern Port-folio Theory, an economic theory dealing with investment in financial markets, we argue that ranking under uncertainty is not just about picking individual relevant documents, bu t about choosing the right combination of relevant documents . This motivates us to quantify a ranked list of documents on the basis of its expected overall relevance (mean) and its variance; the latter serves as a measure of risk, which was rarely studied for document ranking in the past. Through the analysis of the mean and variance, we show that an opti-mal rank order is the one that balancing the overall relevanc e (mean) of the ranked list against its risk level (variance). Based on this principle, we then derive an efficient document ranking algorithm. It generalizes the well-known probabil ity ranking principle (PRP) by considering both the uncertaint y of relevance predictions and correlations between retriev ed documents. Moreover, the benefit of diversification is math-ematically quantified; we show that diversifying documents is an effective way to reduce the risk of document ranking. Experimental results in text retrieval confirm the theoreti cal insights with improved retrieval performance.

H.3.3 [ Information Search and Retrieval ]: Retrieval models, Search process, and Selection process Algorithms, Experimentation, Measurement, Performance
Modern portfolio theory, Mean-variance analysis, Proba-bility ranking principle, Ranking under uncertainty
Information retrieval (IR) concerns how to retrieve docu-ments for a user information need. The process of retrieving documents may be divided into two stages. In the first stage, the relevance between the given user information need and each of the documents in a collection is calculated. Proba-bilistic retrieval models that have been proposed and teste d over decades are primarily focusing on this task, aiming at producing a  X  X est guess X  at a document X  X  relevance. Exam-ples include the RSJ model [15] (a further development of that model led to the BM25 term weighting function [16]), and the language modelling approaches [23]. The second stage focuses on how to present (normally rank) documents to the user. The probability ranking principle (PRP) [6] forms the basis in this stage, stating that the system should rank documents in order of decreasing probability of rele-vance; it has been shown that, following the principle, the overall effectiveness of an IR system, such as expected Pre-cision, is maximized [13].

If we make an analogy with the field of finance, our rank-ing task resembles the investment problem in financial mar-kets; for example, suppose that an investor needs to select a set (portfolio) of n stocks that will provide the best distri-bution of future return, given his or her investment budget  X  an analogy with IR is that we invest ranking positions in documents. The PRP of IR might suggest that, for optimal selection, one should first rank stocks in order of decreasin g future returns and then choose the top-n most  X  X rofitable X  stocks to construct the portfolio. Such a principle that es-sentially maximizes the expected future return was, how-ever, rejected by an economist Harry Markowitz in his Nobel Prize winning work, the Modern Portfolio Theory (MPT) of finance, in 1952 [11]. As one of the most influential economic theories dealing with finance and investment, the MPT was motivated on the basis of the following two observations [11 ]. 1) The future return of a stock is unknown and cannot be calculated with absolute certainty. Investors have differe nt preferences of the risk associated with uncertainty. There -fore, it is highly desirable to have a method of quantifying this uncertainty or risk, and reflect them and incorporate users X  risk preferences when selecting stocks. 2) Since in practice the future returns of stocks are correlated, assum -ing independence between the returns and selecting them independently to construct a portfolio is not preferable.
Realizing the two fundamental issues, the MPT empha-sizes that risk (uncertainty) is an inherent part of future return, and quantifies it by using the variance (or the stan-dard deviation) of the return. The theory suggests that, for a risk-averse decision, an investor should both maximize the return as a desirable thing and minimize the variance of the return as an undesirable thing. Under such a formulation, the MPT mathematically shows that diversification, known as  X  X ot putting all of your eggs in one basket X , is an effective way to reduce the risk of the portfolio.

Going back to our IR problem, we have two similar criti-cal issues: 1) during retrieval, the relevance of documents is unknown and cannot be estimated with absolute certainty from IR models. There are many sources of uncertainty such as ambiguity in the query, specific user preferences, and deviations between the scoring function and the  X  X rue X  probability of relevance. 2) The relevance estimates of in-dividual documents are also correlated, either positively or negatively [8]. Thus it is of great interest to see how we can follow the school of thinking in the field of finance to address the ranking problem in IR.

In this paper, we focus on the theoretical development of the portfolio theory of document ranking. We formulate the ranking problem as a portfolio selection problem. That is, in response to a user information need, a top-n ranked list (portfolio) of documents is selected as a whole, rather than ranking documents independently. To characterize a ranked list, we employ two summary statistics, mean and variance. The mean represents a best  X  X uess X  of the overall relevance of the list, while the variance summarizes the uncertainty o r risk associated with the guess. Our analysis provides new insights into the way we rank documents, and demonstrates that a better and more general ranking principle is to select top-n documents and their order by balancing the overall relevance of the list against its risk (variance). An efficien t ranking algorithm is then introduced to trade off between efficiency and accuracy, and leads to a generalization of the PRP, where both the uncertainty of the probability estima-tion and diversity of ranked documents are modelled in a principled manner. The new ranking approach has been ap-plied to the ad hoc text retrieval and sub-topic retrieval. The experiments demonstrate that our approach can adapt to different risk preferences of evaluation metrics, and as a result significant performance gains have been achieved.
The remainder of the paper is organized as follows. We will discuss the related work in Section 2, present our theo-retical development in Section 3, give our empirical invest i-gation in Section 4, and conclude in Section 5.
Gordon and Lenk have discussed the two underlying as-sumptions of the PRP: independent assessment of relevance by the user and certainty about the estimated probabilities of relevance [8]. To deal with the assumption of the indepen-dence, Chen and Karger in [4] argued that the PRP, which ranks documents in descending order of probability of rele-vance, is not always optimal for different user information needs (or risk preferences we may say). In some scenarios users would be satisfied with a limited number of relevant documents, rather than requiring all relevant documents. The authors therefore proposed to maximize the probabil-ity of finding a relevant document among the top n under the assumption of binary relevance. By treating the previ-ously retrieved documents as non-relevant ones, their algo -rithms naturally introduced diversification into the proba -bilistic ranking.

Unlike in [4] that concerns only the dependence of doc-uments X  relevance, our proposed mean-variance paradigm considers that the two assumptions of the PRP are highly connected and address them together in a more general set-ting. One of the theoretical contributions of our paradigm is that we mathematically demonstrate that diversifying th e top-n documents is a way to reduce the variance and there-fore risk of the ranked list. The greedy algorithm proposed i n [4], which considers only the correlation between two neigh -boring documents, is in fact a special case in our proposed ranking method. Our paradigm is a general one, indepen-dent of the retrieval model that is being used, and has the advantage of tuning the risk via a single parameter.
Previous studies on integrating diversity has been focused on document re-ranking. Heuristically, Carbonell and Gold -stein [3] proposed the Maximal Marginal Relevance (MMR) criterion to reduce redundancy by re-ranking retrieved doc -uments under the Vector Space setup. Lafferty and Zhai in [10] presented a risk minimization framework. In the frame-work, documents are ranked based on an ascending order of the expected risk of a document. The MMR criterion has also been employed in the risk framework in resolving the subtopic retrieval problem [24], by modelling not only rel-evance but also redundancy, novelty, and subtopics. But, nonetheless, when coming to the practical algorithm, the studies [10, 24] still resolve to take point estimation, and use mode of the posterior as opposed to integrating out model parameters. Therefore, the uncertainty of the estimation is still not properly addressed. This is different from our mean-variance paradigm where the document ranking relies on both the mean and variance of the probability estimation of document relevance.

Our preliminary study on collaborative filtering has demon-strated that ranking derived from the analysis of mean and variance improves recommendation performance significant ly [20]. We now provide a comprehensive treatment, looking at a more general application: text retrieval. Our formulatio ns in this paper are flexible for both users X  risk-averse and risk-loving behaviors, whereas our previous work focused only on risk-averse behaviors in collaborative filtering. 3.1 Relevance Return of a Ranked List
The task of an IR system is to predict, in response to a user information need, which documents are relevant. Sup-pose, given the information need, the IR system returns a ranked list consisting of n documents from rank 1 to n  X  in an extreme case, all the documents need to be ordered when n equals the number of documents in the collection. Let r be the estimated relevance score of a document in the list during retrieval, where i = { 1 , ..., n } , for each of the rank po-sitions. We intentionally keep the discussion general, whi le bearing in mind that the exact definition of the relevance score, either degree of relevance or probability of relevan ce [14], relies on the system X  X  assumption of the relevance and adopted retrieval model.

Our objective is to find an optimal ranking that has the maximum effectiveness in response to the given user infor-mation need. There are many ways of defining the effective-ness of a ranked list. A straightforward way is to consider the weighted average of the relevance scores in the list as: where R n denotes the overall relevance of a ranked list. We assign a variable w i , where P n i =1 w i = 1, to each of the rank positions for differentiating the importance of rank po -sitions. This is similar to the discount factors that have be en applied to IR evaluation in order to penalize late-retrieve d relevant documents [9]. It can be easily shown that when w 1 &gt; w 2 ... &gt; w n , the maximum value of R n gives the rank-ing order r 1 &gt; r 2 ... &gt; r n . This follows immediately that maximizing R n , by which the document with the highest relevance measure is retrieved first, the document with the next highest is retrieved second, and so on, is equivalent to the PRP. By contrast, in finance, R n is the overall future return of a portfolio having n stocks; r i is the return of in-dividual stock i , while w i is the percentage of the budget invested in the stock i .

However, the overall relevance R n cannot be calculated with certainty. It relies on the estimations of relevance sc ores r of documents from retrieval models. As we discussed, uncertainty can arise through the estimations. To address such uncertainty, we make a probability statement about the relevance scores, assuming the relevance scores are ran -dom variables and have their own probability distributions . Their joint distribution is summarized by using the means and (co)variances. Mathematically, let E [ r i ], i = { 1 , ..., n } be the means (the expected relevance scores), and let C n be the covariance matrix. The non-diagonal element c i,j in the matrix indicates the covariance of the relevance scores between the document at position i and the document at position j ; the diagonal element c i,i is the variance of the in-dividual relevance score, which indicates the dispersion f rom the mean E [ r i ]. The calculations of the mean and variance in text retrieval are discussed in Section 4.1.

Introducing E [ r i ] and c i,j gives the expected overall rele-vance of a ranked list and its variance as follows: where V ar ( R n ) denotes the variance of the ranked list. For the derivation of Eq. (3), we refer to [11]. By contrast, in finance, E [ R n ] is regarded as the expected overall return of a portfolio containing n stocks; V ar ( R n ) is the variance of the overall return of the portfolio, a measure of the volatility (or risk) associated with the portfolio [7]. Notice that we adopt variance for mathematical convenience, while it is al so possible to measure the risk by the standard deviation. 3.2 Expected Relevance vs. its Variance
The mean and variance summarize our belief about the effectiveness of a ranked list from the following two aspects . The mean measures the overall relevance returned from the ranked documents as a whole, and for optimal retrieval it seems intuitively obvious to maximize the mean. This is es-sentially what the PRP has suggested. But, on the other hand, the variance measures the likelihood that we have under-or over-estimated the expected relevance. That is it represents the level of a risky prospect if we produce an optimal rank order by maximizing the mean. If it is un-derestimated, the user will likely be pleased with the out-put, whereas if it is overestimated, the user will likely be displeased with the output. Thus, for risk-averse users or systems, the variance should stay as small as possible, but, for risk-loving users or systems, a large variance might be a preferable attribute.

For the risk-averse case, consider the following example of movie recommendation, a popular application in IR. The task is to suggest top-n ranked movie items that the user is most likely to like, given the user X  X  past ratings (a repre -sentation of information needs). In this example, the movie items X  relevance scores have multiple values 1-6, with 1 bei ng the lowest rating and 6 being the highest one. Suppose that the system returns a top-10 ranked list of movie items as a recommendation solution. Fig. 1 plots the randomly sam-pled recommendation solutions, marked by circles, each of which contains top-10 ranked items. Their means and vari-ances are calculated based on Eq. (2) and Eq. (3). The item-based model [17] was used to predict the individual items X  relevance, and the covariance matrix is estimated from the historic rating data. For a risk-averse decision, the graph shows that, given a mean value (the expected relevance), one can find an efficient ranking solution that has the min-imal variance (risk). Varying the mean value, we obtain a set of efficient ranking solutions; they are geometrically located on the upper left boundary. In finance, the bound-Figure 1: The relationship between the expected overall relevance and variance of the top-10 ranked list. The curve is the Efficient Frontier . ary is called the efficient frontier [11]. In IR, it represents the set of ranking solutions that have maximal mean (the expected overall relevance) given an upper bound on the variance (risk).

Therefore, mathematically, we have the following criteria for risk-averse ranking: 1. Maximize the mean E [ R n ] regardless of its variance 2. Minimize the variance V ar ( R n ) regardless of its mean 3. Minimize the variance for a specified mean t (parame-4. Maximize the mean for a specified variance h (param-5. Maximize the mean and minimize the variance by using The first two criteria provide the two simplest cases, opti-mizing either of the quantities; the first criterion is what the PRP has optimized, while the second one gives min-imum variance solutions, which might be suitable for the most cautious users or system setup.

The important ones are the ranking criteria 3, 4, and 5, and they are mathematically equivalent [2]. Here, we focus on the formulation of Eq. (6) as it is the common objective function used in practice. For a risk-averse solution, the parameter b &gt; 0. The efficient frontier plotted in Fig. 1 is a set of the solutions that maximize the objective function as b ranges from 0 (the right side) to 40 (the left side). Note that the frontier cannot tell us which one is the single best ranked list for a given user information need; it has to be dependent on the user X  X  risk preference, and can be tuned for the specified evaluation metric, shown in Section 4. In finance, investors are usually assumed to be risk-averse. But in IR risk-loving behaviors may be useful in some sit-uations. For instance, pseudo relevance feedback is a risky solution since it assumes the first few retrieved documents are relevant. It is often reported to have an ability to im-prove MAP (mean average precision) [12, 19]. In this regard, it is beneficial to study the effectiveness of the risk-loving so-lutions when we set b &lt; 0 in the objective function. In fact, by applying the utility theory, one can give a more general justification of the objective function in Eq. (6) [22]. For readability, its detailed derivation is given in Appendix. 3.3 Diversification vs. Uncertainty
This section discusses diversification, and formally deriv es its relationship with the uncertainty of a ranked list. A further decomposition of the variance in Eq. (3) gives where  X  i =  X  c i,i is the standard deviation, and  X  i,j = is the correlation coefficient.  X  i,j = 1 means that there is an exact positive relationship between two documents,  X  i,j = 0 means no relationship between the two documents, and  X  i,j =  X  1 indicates an exact negative relationship be-tween the two documents. As shown in Eq. (7), to reduce the uncertainty of the relevance prediction for the returne d documents, we need to have small correlation coefficients (preferable negative correlations) between documents. Th is means diversifying the documents in the ranked list will re-duce the variance and therefore the uncertainty of the ex-pected overall relevance of the returned documents.
To understand this, consider two extreme cases: in the first case, suppose we have a ranked list consisting of two documents, where the correlation coefficient  X  between them is  X  1. This means that their estimated relevance scores change in the exact opposite direction in response to differ-ent information needs. The volatility (the change) of the documents X  relevance cancels one another completely and leads to a situation where the ranked list has no volatility a t all. As a result, a certain amount of relevance for any kind of user information needs is maintained. Conversely, when we have two documents that are perfectly correlated (  X  = 1) in the list, the relevance returns of the two documents move in the perfectly same direction in response to different in-formation needs. In this case, the returned relevance of the list mimics that of each of the two documents. As a result, the list contains the same amount of uncertainty (risk) as each of the two documents alone. In this case, risk is not reduced. 3.4 Document Ranking -A Practical Solution
Unlike in finance, the weight w n in IR, representing the discount for each rank position, is a discrete variable. The re-fore, the objective function in Eq. (6) is no-smooth, and there is no easy solution for directly optimizing it. In this section, we present an efficient document ranking algorithm by sequentially optimizing the objective function. It is ba sed on the observation that the larger the rank of a relevant doc-ument, the less likely it would be seen or visited by a user. An economical document selection strategy should first con-sider rank position 1, and then add documents to the ranked list sequentially until reaching the last rank position n . For each rank position, the objective is to select a document tha t has the maximum increase of the objective function. Notice that such a sequential update may not necessarily provide a global optimization solution, but it provides an excellen t trade-off between accuracy and efficiency.

The increase of the objective function from position k  X  1 to k is: Table 1: Overview of the six tested collections.
 where k  X  X  2 , ..., n } . The final equation is derived as Since w k is a constant for any document in rank k , dropping it gives the following ranking criterion: select a document at rank k that has the maximum value of 3.5 Discussions
Ranking principle: Eq. (9) extends the PRP into a more general situation. It contains three components. The first component concerns the point estimate of the relevance E [ r k ], which is essentially equivalent to the PRP. The second component generalizes the PRP by considering the uncer-tainty of the point estimate. It concerns the variance of the estimates of individual documents. The third component ex-tends it further by looking at the correlations between the estimates. A positive b produces risk-averse ranking where negatively correlated (with previously retrieved documen ts) documents should be given high ranking scores. In this case, diversification, which is quantified by the weighted average of the correlations between the ranked documents (see the second component in Eq. (7)), is effectively incorporated into the document ranking. The smaller the parameter b is, the less risk-averse the ranking is. When b = 0, it goes back to the PRP, which only considers the point estimate E [ r k ]. When b &lt; 0, the ranker intends to take more risk. The impact of b and its relations with IR metrics are studied in Section 4.

Higher moments: The discussions so far rely on a Gaus-sian assumption about the distribution of relevance scores . Most probabilistic retrieval models are, however, not Gaus -sian. Strictly speaking, using the first two moments (the mean and variance) may not be entirely adequate to describe the distribution, and the third moment might be needed to indicate the skewness (asymmetry to the mean) if any. But in practice as an approximation the analysis of the mean and variance is fair enough to trade-off between complexity and speed.

Relations with prior work Our ranking approach is a general one. When b &gt; 0, the last component in Eq. (9) re-sembles the MMR (Maximal Marginal Relevance) re-ranking method [3]. As discussed, the MMR re-ranking, as a heuris-tic method, linearly combines relevance and novelty using a parameter between 0 and 1. It judges a document to have high  X  X arginal relevance X  if it is both relevant to the query and contains minimal similarity to already selected docu-ments. Thus, our probabilistic approach provides a theoret -ical justification. Also, our formulation is less computati on-ally expensive as it does not need to find minimal similarity. The empirical comparison between them is in Section 4.3.2.
The ranking criterion in Eq. (4) gives an alternative for-mulation for the objective at which Chen and Karger in [4] have aimed: fixing the amount of relevance the user intends to receive in Eq. (4) (by setting the parameter t ) is similar to optimizing the number of relevant documents in the ranked list, proposed in [4]. The merit of our mean-variance formu-lation is that the resulting ranking principle is a general o ne and can be applied to any IR models, whereas the formu-lation in [4] is only suitable for binary relevance IR models as it explicitly relies on the assumption of binary relevanc e, and is coupled with the IR model during ranking.
Our evaluation focuses on text retrieval, where ad hoc and subtopic retrieval [5] are studied; we report results on five TREC test collections for ad hoc retrieval and one TREC collection for subtopic retrieval. These collections are d e-scribed in Table 1. Our main goal is to validate our theo-retical development, and investigate the effectiveness of t he various risk preference settings. 4.1 Calculation of Mean and Variance in IR
Different probabilistic retrieval models result in differen t estimators of E [ r i ] and C n . E [ r i ] can be determined by a point estimate from the specific text retrieval model that ha s been applied. In this paper, three widely-adopted retrieva l models, namely, the Dirichlet and Jelinek-Mercer smooth-ing language models [23], and the BM25 model [16] are used to calculate the expected relevance scores. For the two lan-guage models, we employ the posterior mean of the query-generation model as the estimator. Strictly speaking, the BM25 scores are not calculated in a probabilistic way, but it is reasonable to assume that its output scores are random variables and have uncertainty associated with them.
The covariance matrix C n represents both the uncertainty and correlation associated with the estimations. Although they are largely missing in current probabilistic retrieva l models, there are generally two ways of estimating them in practice. Formally, they should be determined by the second moment of the relevance scores. For instance, one can estimate the (co)variances of individual document mod-els (parameters) by adopting the Bayesian paradigm [1, 25]. Alternatively, for given two documents, the covariance be-tween their relevance scores can be approximated by the covariance with respect to their term occurrences. This is similar to use historic data of two stocks to calculate the correlation between their future returns in finance.
In this paper, for the two language models, the relevance scores are assumed to follow the Dirichlet distribution, an d their variances  X  2 are thus conveniently calculated [25]. Since the BM25 is not a probability model, we set the variances as a constant for all documents. This allows us to study the effectiveness of the correlations solely when using the BM25 scores. The correlation  X  is approximated by using the Pearson X  X  correlation coefficient between each pair of documents X  term vectors. Ranking is based on the sequen-tial update formulated in Eq. (9), and only the correlations with the previously retrieved documents are needed. Thus, the computational load of calculating covariances has been reduced significantly. The weights of rank positions w i are chosen according to the discount factors in [9]. 4.2 Ad Hoc Text Retrieval 4.2.1 Parameter: As studied by Thom and Scholer in [18], the IR evaluation metrics generally have two distinct categories: those strongly biased towards early-retrieve d doc-uments, such as Mean Reciprocal Rank (MRR), and those trying to capture a broader summary of retrieval perfor-mance, including Mean Average Precision (MAP). Let us first evaluate the impact of the risk preference parameter b toward the two categories.

Fig. 2 (a) and (b) plot the percentage of improvements against a varying b from -6 (risk-loving) to 6 (risk-averse). The fitted curves are based on the data points, and the per-centage of improvement on the MRR and other metrics is based on the improvement over the setting where b = 0 (equivalent to the PRP). In this experiment, the Dirichlet smoothing language model (where  X  =2000, a typical set-ting) is adopted for obtaining the relevance scores. From Fig. 2 (a), we can see that positive values of b , i.e, diversify-ing search results, helps improve the MRR metric. This ex-plains that by  X  X nvesting X  into different kinds of documents , the actual chance of returning the first relevant documents as early as possible can be actually increased.

By contrast, for a metric capturing a broader summary of retrieval performance such as MAP, Fig. 2 (b) shows that negative values of b , which emphasize a document positively correlated with the early-retrieved documents, help impro ve the performance.  X  X nvesting X  in the same  X  X ype X  of docu-ments is a risky action (big variance), and might hurt the MRR metric. But, on average, it does increase the perfor-mance of the entire ranked list (in this setting, n = 1000). This is similar to the effectiveness of pseudo relevance feed -back in ad hoc retrieval, i.e., the top ranked documents are generally likely to be relevant, and to find other documents similar to these top ranked ones will help improve MAP [19].
To further understand these risk behaviors, we then study how the parameter behaves under a risk-sensitive metric called k -call at 10, or k -call for simplicity, proposed in [4]. Given a ranked list, k -call is one if at least k of the top-10 documents returned for a query are relevant. Otherwise, k -call is zero. Averaging over multiple queries yields mean k -call. The two extremes are 10-call, an ambitious metric of perfect precision: returning only relevant documents, a nd 1-call as a conservative metric that is satisfied with only one relevant document. Thus, a risk-averse approach, which can reliably find one relevant document, is preferred for 1-call , while a risk-loving approach is favored for 10-call [4]. are marked with  X  .
The relationship between the optimal value of b and k -call ( k =1,...,10) is plotted in Fig. 2 (c). The figure shows that when k is small such as 1 and 2, the optimal b is posi-tive for all collections. This means that diversifying top-10 search results reduces the risk of not returning any relevan t documents. When k increases, the optimal b becomes neg-ative. This shows that a risk-loving approach will increase the chance of finding many relevant documents. 4.2.2 Performance: We now test the performance against various setups and metrics. 5-fold cross validation is car-ried out on the four ad hoc test collections. Queries in each collection were randomly partitioned. For each parti-tion, model parameters were trained with all the other parti -tions and performance for the partition is evaluated with th e trained parameters. We evaluated the concatenated ranked lists from all 5 partitions, and report the results in Table 2 . When compared with the PRP via the Dirichlet smoothing language model in Table 2 (a), out of the 60 reported re-sults, 57 improvements are positive, and 27 improvements are statistically significant. When compared with the PRP via the Jelinek-Mercer smoothing language model in Table 2 (b), out of the 60 reported results, all the improvements are positive, and 48 improvements are statistically significan t. When compared with the PRP via the BM25 model in Ta-ble 2 (c), out of the 60 reported results, 58 improvements are positive, and 22 improvements are statistically significan t.
Overall, our approach largely outperformed the PRP in our experiments. As different IR metrics may reflect differ-ent risk-taking preferences, e.g., risk-loving or risk-av erse, by tuning the parameter, our approach provides an effective way for optimizing different IR metrics. 4.3 Subtopic Text Retrieval
Subtopic retrieval is concerned with finding documents that cover many different subtopics of a general query topic. In subtopic retrieval, the utility of a document is dependen t on other documents in the ranking. To study the effective-ness of our ranking approach in this task, we compare our approach with the PRP and the MMR ranking method [3]. We also study the relationship between the parameter b and a range of subtopic specific metrics. We used the TREC interactive track subtopic collection, which, to our knowl -edge, is the only publicly available subtopic collection. T he collection consists of 20 topics adapted from TREC ad hoc retrieval topics. The number of subtopics for these topics ranges from 7 to 56 with an average length of 20. For a topic, the relevance judgment for each document is a vector, whose length is the number of subtopics. The vector con-sists of 1 and 0, which represents relevant and not relevant for a subtopic, respectively.

We report the metric called  X  -NDCG (Normalized Dis-counted Cumulated Gain) proposed by [5], which takes into account both novelty and relevance of documents. A pa-rameter  X  between 0 and 1 balances novelty and relevance in  X  -NDCG, and when  X  = 0,  X  -NDCG is equivalent to standard NDCG [9]. The larger the  X  value, novelty is re-warded more over relevance, and vice versa. We fixed  X  as 0.5 for a balance between novelty and relevance.

We also extended the traditional Recall at n and MRR metrics to define two new subtopic retrieval metrics, namely , subtopic Recall (sub-Recall) at n and subtopic MRR (sub-MRR). These two new metrics emphasize novelty, and have simpler definitions than  X  -NDCG, therefore, will likely help us gain a more direct view of the effect of parameter b on subtopic retrieval. Suppose there are N subtopic for a topic, we define sub-Recall at n as the number of differ-ent subtopics covered by the top n documents divided by N . Given a topic, we define sub-MRR as the inverse of the rank of the first position where documents covering all the subtopics have been retrieved. Therefore, sub-MRR awards a system which can retrieve all subtopics as close to the top of a ranked list as possible. We average sub-Recall at n and sub-MRR over a number of topics to get their means, respectively. 4.3.1 Parameter: We plot the relationship between sub-Recall at n and the corresponding optimal value of b in Fig. (3). In Fig. (3), when the cut-off points are beyond 20, the optimal b is around 0.0, i.e., little or no diversification is employed. This tells us that for top 20 or more documents, the PRP can perform as well as our risk-aware approach, i.e., a sufficient number of relevant documents retrieved by the PRP can cover different sub-topics well. However, for lower cut-off points from 2 to 15, the optimal b is always between 4.0 and 12.0, showing that a risk-averse approach helps choose documents on different aspects of a topic. 4.3.2 Performance: We compared our approach with the PRP and MMR [3] ranking criterion. We again used 5-fold cross-validation for subtopic retrieval on the TREC subtopic collection to optimize the parameters, and the re-sults are shown in Table 3.

We can see from Table 3 that our approach can largely outperform both the PRP and the MMR method. Com-pared with the PRP, out of 30 reported results, all the per-formance gains by our approach are positive, and 15 perfor-Figure 3: Relation between optimal b and sub-Recall@ n . mance gains are statistically significant. Compared with th e MMR method, out of 30 results, all the performance gains by our approach are positive, and 12 performance gains are statistically significant.

The MMR method can slightly outperform the PRP when the cut-off points of sub-Recall and  X  -NDCG are below 10, but performed worse than the PRP when the cut-off points are above 10; while our approach consistently outperformed the PRP.
 We think the good performance of our approach over the MMR method is due to the reason that our approach pro-vides a more principled way for taking into account both variance and diversification in document ranking. Besides, in our approach, correlations between a new document and all top ranked documents are considered, while the MRR method only considers the maximum similarity between a new document and one top ranked document. We think that the use of only one pair of documents X  similarity in the MMR method may result in unstable results when the ranked list is long.
To address the ranking uncertainty, we have followed the school of thinking from the Modern Portfolio Theory in fi-nance, and presented the mean-variance paradigm for doc-ument ranking in text retrieval. The analysis of the mean and variance of a ranked list led to a new and generalized document ranking principle.

Handling uncertainty is critical for IR modelling. There are fruitful avenues for future investigations into the pro -posed mean-variance paradigm, including 1) the analysis of mean and variance of IR evaluation metrics. 2) Variance as an indicator of the risk does not distinguish a bad surprise from a good surprise. It is worthwhile investigating  X  X own-side risk X  in finance that considers only bad surprises. 3) Large numbers of documents make the estimation of cor-relations between all documents a great challenge. How to effectively and efficiently calculate the variance (risk) and correlation of the estimation remains an open question. 4) It is of great interest to study the mean-variance analysis in other IR applications such as filtering [21], multimedia retrieval, and advertising.
In Section 3.2, we have given Eq. (6) on the basis of our mean-variance analysis. Here we present an additional jus-tification from a Bayesian view point. The intuition is that the loss function for estimating the returned relevance of a ranked list is asymmetric. To model this, we adopt the LINEX asymmetric loss function [22]: where b is the parameter to balance the loss. When b &gt; 0, the loss of over-estimate is larger than that of under-estimate, and when b &lt; 0, otherwise.

From the Bayesian point of view, the returned overall rel-evance of a top-n ranked document list is a random vari-able. The posterior probability of the R n can be written as p ( R n | r i , ..., r n ). Integrating out the unknown hidden vari-able R n gives the expected loss as: where E denotes the expectation.  X  R n is the Bayes estimator of R n with respect to the cost function L . The optimal estimator of R n should minimize the expected loss function. Minimizing Eq. (11) gives the optimal Bayesian estimator as follows (for detailed information, we refer to [22]): If the overall relevance R n is assumed to be a normal distri-bution, one can derive the estimation analytically as follo ws: where E [ R n ] is the posterior mean and V ar ( R n ) is the pos-terior variance. Replacing b/ 2 with b gives Eq. (6). Our derivation shows that, for selecting an optimal top-n ranked list, maximizing the objective function in Eq. (6) is equiva -lent to the Bayesian estimator of returned overall relevanc e that minimizes the asymmetric loss.
