 minimizes the ` 1 -penalized sums of squares and Yu, 2006; Wainwright, 2006).
 Hastie and Tibshirani (1999) introduced the class of additi ve models of the form of the additive model is the functional ANOVA model sample size n .
 straints are placed on the component functions { m j } 1 that support its good empirical performance. this optimization problem in the finite sample setting.
 E ( f j ( X j )) = 0 and E ( f j ( X j ) 2 ) &lt;  X  , furnished with the inner product population setting, is cation of this problem that imposes additional constraints : sparse, the estimated additive function f ( x ) = P p j by rescaling.
 convex in  X  and { g j } separately, it is not convex in  X  and { g j } jointly. However, consider the following related optimization prob lem: defined by C = ( f 11 , f 12 , f 21 , f 22 ) T  X  R 4 : and third components is an ` 1 ball. In this way, it can be seen that the constraint P j f j the norm f j this constraint could be replaced by P j f j ( f (
P to that of the COSSO for smoothing spline ANOVA models (Lin an d Zhang, 2006); however, our scalable and easy to implement even when p is much larger than n . we observe Y = m ( X ) + , where is mean zero Gaussian noise. We write the Lagrangian for the optimization problem ( Q ) as Let R j = Y  X  P k derivative  X  L as for any  X  f j  X  H j satisfying E ( X  f j ) = 0, where v j  X   X  satisfying stationary condition (9) implies and f j = 0 otherwise. Condition (11), in turn, implies Thus, we arrive at the following multiplicative soft-thres holding update for f j : where [  X  ] Tibshirani, 1999), we estimate the projection E [ R j | X j ] by a smooth of the residuals: q
E [ P 2 j ]. A simple but biased estimate is the SpAM backfitting algorithm given in Figure 1.
 and Zhang, 2006) for smoothing splines, the SpAM backfitting algorithm decouples smoothing and implement and scales to high dimensions. 3.1 SpAM for Nonparametric Logistic Regression cation. The additive logistic model is iteratively computes the transformed response for the curr ent estimate f 0 with weights w . The weighted smooth is given by To incorporate the sparsity penalty, we first note that the La grangian is given by element of the subgradient  X  E w( X )( f ( X )  X  Z ) | X j +  X v j = 0. When E ( f 2 j ) 6= 0, this implies the condition In the finite sample case, in terms of the smoothing matrix S j , this becomes If solved explicitly, so we propose to iterate until convergen ce: given in Section 5. 4.1 SpAM is Persistent that an estimator b m n is persistent relative to a class of functions M n if where m  X  n = argmin f L n = o (( n / log n ) 1 / 4 ) . We show a similar result for SpAM. class of additive models M n = 4.2 SpAM is Sparsistent show a similar result for SpAM with the sparse backfitting pro cedure. d submatrix in the natural way. The SpAM optimization problem can then be written as s variables from the minimizer b  X  n of (25).
 Theorem 4.2. Suppose that 9 satisfies the conditions Let the regularization parameter  X  n  X  0 be chosen to satisfy Then SpAM is sparsistent: P b S n = S  X  X  X  1 . statistic, which is defined as where S j is the smoothing matrix for the j -th dimension and b  X  2 is the estimated variance. 5.1 Simulations is generated from the following 200-dimensional additive m odel:  X  remaining components are zero. 5.2 Boston Housing original ten covariates, so that they have the same empirica l densities. The full model (containing all 10 chosen covariates) for the Boston Housing data is: path, the important variables are seen to be rm , lstat , ptratio , and crim . The importance 5.3 SpAM for Spam best C p score. Right: Additive fits for four relevant variables. This research was supported in part by NSF grant CCF-0625879 and a Siebel Scholarship to PR.
