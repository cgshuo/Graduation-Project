 Dwell time on Web pages has been extensively used for vari-ous information retrieval tasks. However, some basic yet im -portant questions have not been sufficiently addressed, e.g. , what distribution is appropriate to model the distribution of dwell times on a Web page, and furthermore, what the dis-tribution tells us about the underlying browsing behaviors . In this paper, we draw an analogy between abandoning a page during Web browsing and a system failure in reliabil-ity analysis, and propose to model the dwell time using the Weibull distribution. Using this distribution provides be t-ter goodness-of-fit to real world data, and it uncovers some interesting patterns of user browsing behaviors not previ-ously reported. For example, our analysis reveals that Web browsing in general exhibits a significant  X  X egative aging X  phenomenon, which means that some initial screening has to be passed before a page is examined in detail, giving rise to the browsing behavior that we call  X  X creen-and-glean. X  I n addition, we demonstrate that dwell time distributions can be reasonably predicted purely based on low-level page fea-tures, which broadens the possible applications of this stu dy to situations where log data may be unavailable.
 H.1.2 [ Information Systems ]: Models and Principles  X  User/Machine Systems Algorithms, Measurement, Human Factors Weibull analysis, User behaviors, Web browsing, Dwell time
Real-world information retrieval (IR) heavily relies on ef -fective usage of implicit feedback, which comes in various forms such as document clickthrough, viewing, scrolling, and bookmarking. Many researchers have studied the cor-relations between implicit feedback and document relevanc e ( e.g. , [6, 21, 5, 10]), and revealed that document dwell time ( i.e. , the length of time a user spends on a document), is gen-erally the most significant indicator of document relevance besides clickthrough, although the extent of the relations hip may vary depending on the information seeking task [14, 15]. Because of the correlation between dwell time and document relevance, dwell time has been successfully used in various applications, such as learning to rank [2, 3], query expan-sion [5], and inferring query-independent page importance [19]. Specifically, Agichtein et al. [2, 3] demonstrate that user browsing features, a major component of which is Web page dwell time, significantly improve the retrieval perfor -mance of a competitive search engine, even with the presence of other important features such as BM25 and search-result clickthrough. Although post-query browsing is intuitivel y more relevant to IR than general browsing, general browsing is still an important component in information seeking [20, 25]. Indeed, some of aforementioned studies ( e.g. , [19]) and real-world search engines also leverage the general browsi ng activities for improved efficacy and coverage.

Although dwell time has been extensively studied, some important questions have not been sufficiently addressed. For example, what distribution is appropriate to model the dwell time t on a Web page 1 d across all visits, i.e. , what does P r ( t  X  d ) look like? Furthermore, how does the distribution depend on the features of d ? And finally, what does the distribution tell us about users X  general browsing behavio rs? These questions are not only interesting in themselves, but are also useful for various IR applications, as we now explai n.
First, accurately modeling P r ( t  X  d ) would enable the con-struction of generative models involving dwell time for Web text analysis. For example, when dwell time is properly modeled, topic discovery can be guided by considering both P r ( t  X  d ) and content. Second, P r ( t  X  d ) can readily answer questions such as  X  X hat is the probability that a user will stay longer than t 1 on the page? X  (answer: P r ( t  X  t 1 or  X  X hat is the expected remaining time that a user will spend on a page that he has dwelled on for t 1 ? X  (answer: E ( t  X  t  X  t 1 , d )). Answers to such questions could help pub-lishers optimize advertising and content placement. Third , understanding P r ( t  X  d ) would help us gain insights into user browsing behaviors that can help inform the design of search
We use page, Web page, URL and document interchange-ably in this paper and advertising technologies, as we will explain later in th e paper.

Precise modeling of dwell time is not straightforward since duration on a Web page depends on many factors, some of which may not even be fully captured by log data ( e.g. , the mood of the user). In addition, the distribution family may vary with different information seeking tasks in different se t-tings ( e.g. , time of day). As the first step towards precise modeling of dwell time, we choose to model the overall dis-tribution of user dwells on each Web page, which we believe will help us better understand the dwell time distributions in general across all users.

In this paper, we draw an analogy between abandoning a page during Web browsing and a system failure in reliabil-ity analysis, and use Weibull analysis techniques which are commonly used in reliability engineering [1] to characteri ze general browsing behaviors. Furthermore, we demonstrate that it is possible to predict the dwell time distribution ba sed on page-level features. We make the following contribution s in this study:
The remainder of this paper is organized as follows. We first discuss the related work in Section 2, and then examine the goodness-of-fit of the Weibull distribution in Section 3 . We present the Weibull analysis results in Section 4, and elaborate on the predictive model in Section 5. With ex-tensions and future work discussed in Section 6, Section 7 concludes this study. This paper is related to work on implicit feedback within IR. Research on implicit feedback has sought to address the high cost of soliciting explicit feedback from users by un-obtrusively observing their natural interactions and buil d-ing models for activities such as query expansion and user profiling [17]. Although implicit feedback may be less ac-curate than explicit feedback [22], it is available in signi f-icantly greater quantity than explicit feedback. Implicit measures include document retention ( e.g. , printing, sav-ing, bookmarking) and document interaction ( e.g. , view-ing, scrolling, dwell time) [21, 6, 14]. Morita and Shinoda [21] measured the relationship between dwell time, saving, following-up and copying of a document and users X  explicit ratings, and showed that there was a relationship between dwell time and interest, but no relationship between inter-est and any other measures. Claypool et al. [6] examined mouse clicks, scrolling, dwell time, and requested explici t ratings, and found that dwell time and the amount of mouse scrolling had a strong positive correlation with explicit r at-ings. Studies by Kelly and Belkin [14, 15] further found that special attention is needed to interpret dwell time as rele-vance because of the implications of different tasks. On the application side, besides being incorporated into learnin g to rank for Web IR [2, 3, 19], dwell time is also widely used in other information seeking tasks ( e.g. , [16, 5]). In this paper, we do not focus on particular search and retrieval tasks as done in most previous work, but instead try to model the dwell time distribution across all users engaged in general Web browsing.

This work is also related to online user behavior model-ing, which has been attracting significant attention in rece nt years ( e.g. , [24, 25, 9]). There are two main complementary approaches to uncovering user behavior models: one based on controlled user studies ( e.g. , [12, 13, 24]), and the other based on large-scale log analysis ( e.g. , [25, 9, 26, 4, 19]). This work falls into the second category, and is mostly related to BrowseRank [19], which tries to infer a query-independent score for each page from page dwell time in general browsing. In particular, BrowseRank assumes (mainly for tractabilit y) that the dwell time for a given page follows an exponential distribution. In this paper, we show that the Weibull dis-tribution is more versatile than the exponential distribut ion used in [19]; it better fits the real-world dwell time data and provides insights on browsing behaviors.

This work also relates to research on Weibull analysis, which has been extensively and successfully applied in near ly all scientific disciplines, such as biological, demographi cal, reliability sciences (c.f. [1, 23]). This paper therefore a dds a new application area to the rich literature of Weibull analy -sis, and meanwhile introduces a disciplined method for an-alyzing temporal data on the Web, e.g. , time-to-first-click on search result pages and the session length in time, in addition to the page dwell time as studied here.
In this section, we fit the dwell time data with exponen-tial and Weibull distributions (Section 3.2), and compare their goodness-of-fit in Section 3.3. The data used for this comparison and throughout the paper are discussed in Sec-tion 3.1.
We collected two-weeks of log data from a popular Web browser plug-in operating in the English (US) market, which records the searches and browsed pages for opted-in users. The log data is organized in sessions, each of which is defined as a series of Web page visits that extends until either the browser is closed or a period of 30-minutes of inactivity. Based on the visit time of consecutive page visits within sessions, the dwell time of each page visit is calculated. We do this for all pages apart from the last page in the session, which is then discarded from the analysis because we do not have a succeeding page visit from which to calculate its dwell time. For accurate parameter estimation, only pages with 10,000 or more visits are used. This results in a set of 205,873 URLs, each of which is accompanied by at least 10,000 dwell time observations.
The probability density function (PDF) of Weibull distri-bution is given by with E ( t  X  , k ) =  X (1+1 /k ). and k are the scale and shape parameters, respectively. Figure 1 plots the PDFs of some typical parameterizations, which exemplify the versatili ty of the Weibull distribution, and how parameters and k affect the scale and shape of the distribution, respectively.
When k = 1, the Weibull distribution reduces to the ex-ponential distribution with PDF and E ( t  X  ) = .

Given a sample of n observed dwell time for a page, { t i we choose to fit the model through maximum likelihood es-timation (MLE), and denote the fitted model with Weibull by M W and that with exponential by M E . While fitting M is as simple as fitting M W is nontrivial because the MLE of ( , k ) has no closed form. Instead, we need to use an iterative approach proposed in [7]. For completeness, we briefly outline the estimation.
 Given the likelihood function as we set the partial derivative w.r.t. and k to 0, i.e. ,  X  X n ( L ) Eqn 4 gives which, once plugged into Eqn 3, renders which only involves k and can be solved through Newton-Raphson iterations. Specifically, let then g  X  ( k ) = Then the MLE of k , denoted by  X  k , is obtained by We terminate the iterations when the change of k is less than 10  X  6 . Once  X  k is obtained,  X  immediately follows from Eqn 5. We try initial value k (1) = 0 . 1 , 1 , 10, and choose the final (  X  ,  X  k ) with the largest likelihood. Readers interested in a thorough treatment of parameter estimations (besides MLE) for Weibull distributions are referred to [23]. The above estimation can be trivially parallelized across URLs for distributed computing, which affords Web-scale dwell time data analysis.
We use the log-likelihood (LL) and the Kolmogorov-Smirnov distance (KS-distance) [8] to evaluate the goodness-of-fit of M W and M E . In general, a better fit corresponds to a bigger LL and/or a smaller KS-distance.
 The KS-distance as defined below is the test statistic for Kolmogorov goodness-of-fit test, w hich tests whether a random sample X 1 , X 2 ,  X  X  X  X  , X n , whose em-pirical cumulative distribution function (eCDF) is descri bed by S ( x ), comes from a completely specified hypothesis dis-tribution whose cumulative distribution function (CDF) is given by F  X  ( x ).

Because the exponential distribution is a special case of the Weibull distribution, M W is guaranteed to be no worse than M E if fitted and evaluated on the same dataset. To be fair, the dwell time observations for each page are randomly split into training and testing portions with a ratio of 4:1. In other words, the data are split within the dwell time observations for each page rather than across pages. M W and M E are fit using the training portion and evaluated on the testing portion for each page. LL and KS-distance on the testing portion are used to determine which model wins. The number of pages on which a model wins are listed in Table 1, which clearly shows the superiority of M W to M E Specifically, M W outperforms M E on more than 85% of the pages in terms of both metrics, and a sign test for each result gives a p-value that is very close to zero. In this section, we discuss the implications of a fitted Weibull distribution for understanding user browsing beha v-iors (Section 4.2). We first provide a brief introduction to Weibull analysis in Section 4.1.
Weibull analysis dates back to 1937 when Waloddi Weibull invented the Weibull distribution. It has been successfull y applied to nearly all scientific disciplines, such as biolog ical, environmental, health, physical and social sciences, but, to the best of our knowledge, not in the Web data analysis domain. By fitting time-to-failure data to Weibull distri-butions, Weibull analysis enables principled failure inte rpre-tation, risk assessment, failure forecasting, and plannin g of corrective actions. Since a full introduction to Weibull an al-ysis is neither realistic nor necessary here, we will highli ght those aspects that pertain to our analysis and referring in-terested readers to [23] and [1] for a thorough treatment of Weibull analysis and applications.

The most popular characteristic function of a Weibull dis-tribution is the Hazard function, which is defined as If an item that has survived time x is called an x -survivor, the hazard function gives the probability that an x -survivor fails immediately at time x , and it is also known as the instantaneous failure rate or the hazard rate . Usually, the hazard rate is interpreted as the amount of risk associated with an x -survivor at time x in reliability study and as the force of mortality in demography and actuarial science.
The hazard function of a Weibull distribution is given by whose first-order derivative is
When k  X  (0 , 1), the first-order derivative,  X   X  t ( x ), is less than 0, so the hazard rate monotonically decreases w.r.t. x . This phenomenon is often termed  X  negative aging , X  which means that the longer one survives, the less likely it would fail instantaneously. Since the hazard rate is high at the onset, it is also called the  X  X nfant mortality X  phenomenon. In abstract terms, negative aging means that a screening is taken place at the early stage so that weak items with hid-den defects are sorted out while leaving robust and healthy ones in the population, or as Lehman [18] suggests  X  X o once the obstacle of early youth have been hurdled, life can con-tinue almost indefinitely. X  We will reveal the implication o f negative aging for Web browsing in Section 4.2.

In contrast, k &gt; 1 corresponds to the  X  positive aging  X  phe-nomenon, which means that the longer one survives, the more likely it fails instantaneously. Finally, k = 1 results Figure 3: Distributions of the Fitted and k Values in a constant hazard function, indicating a constant failur e rate, which is the physical model of the exponential distri-bution.

The hazard functions of some example Weibull distribu-tions are plotted in Figure 2, which illustrates different ty pes of aging. Note that when k  X  (0 , 1), we see negative aging, or a decrease in the failure rate over time. In the context of Web browsing this would mean a decrease in Web page abandonment rate over time. Conversely, when k &gt; 0, we see positive aging, or an increase in the failure rate over time.
Using the data set as described in Section 3.1, we now ex-amine the fitted and k values on the training portion for each page. Figure 3 plots the empirical cumulative distri-bution function (eCDF) for the fitted and k values. Fig-ure 3(a) shows the eCDF for the scale parameter of the es-timated dwell time distribution. We see that the dwell time is no more than 70 seconds on 80% of the 205,873 pages, which gives us an overall estimate of the dwell time scales across pages. Figure 3(b) shows the eCDF for the shape parameter, k . We see that k is less than 1 on 98.5% pages. Recalling that k &lt; 1 indicates a negative aging effect. Thus, Figure 3(b) suggests that Web browsing exhibits a strong  X  X egative aging X  phenomenon, that is, some  X  X creening X  is carried out at the early stage of browsing a page, and the rate of subsequent abandonment decreases over time.
This discovery agrees well with the intuition about how a user browses a page: upon landing on a Web page, the user would first skim through the page, assessing the potential benefit of further reading, before delving into it and gleani ng needed information. During the screening, the probability of abandoning the page is high ( i.e. , a high hazard rate), but once the page survives the screening ( e.g. , is regarded 0.05 0.15 0.25 0.35 0 0.5 1 1.5 k as useful by the user), the abandonment rate decreases. We therefore suspect that users do in general adopt a  X  X creen-and-glean X  type browsing behavior, which gives rise to the dwell time distribution showing the observed negative agin g effect.

We now examine whether and how the  X  X egative aging X  phenomenon relates to the topic of the page, as defined by category membership, i.e. , do people impose equal screen-ing on pages of different categories? For this purpose, we employed a proprietary document classifier that assigned each page into one of 23 top-level categories in a taxonomy similar to dmoz 2 . The categorization succeeded on 136,395 pages. We then analyzed how category information relates to the aging effect from two complementary aspects: first, we compare P r ( Category  X  k &lt; 1) with P r ( Category  X  k &gt; 1), and second, we examine P r ( k  X  Category ) for different cate-gories. In order to have sufficient data in each category, only the top-10 categories were retained, which included 106,16 9 (77.8%) pages.

Figure 4(a) compares the category distributions for Web pages with k &lt; 1 and k &gt; 1. We show the category distri-butions for each of these two types of pages. We see that Entertainment , Recreation , Relationships , Travel and Vehi-cles have a proportionally greater presence when k &gt; 1 than when k &lt; 1 (recall the sets of pages with k &gt; 1 and k &lt; 1 are highly imbalanced). Thus pages exhibiting positive ag-ing ( k &gt; 1) are more likely to fall into these categories, which we can characterize as more entertaining, than those showing negative aging effect. Conversely, we see that the presence of Computers and Education is stronger in k &lt; 1 than k &gt; 1, indicating that people are more likely to screen pages in these two categories before examining them in more details. This observation leads to a hypothesis that nega-tive aging is more common on less-entertaining pages than on fun pages, which in turn suggests that people tend to screen less-entertaining pages more harshly.

Figure 4(b) shows boxplots of P r ( k  X  Category ) for the 10 categories. The line in the middle of each box is the median http://www.dmoz.org/ of the data, and the lower and upper lines of the box repre-sent the 25 t X  and 75 t X  percentiles of the data, respectively. The categories are ordered in ascending order of the me-dian values from left to right, which median value of 0.6506 for Education and a median value of 0.7979 for Vehicles . Again, we observe that less-entertaining categories appea r on the left of the figure, supporting the hypothesis that less -entertaining pages may be more harshly screened.
In this section, we investigate the feasibility of predicti ng dwell time distribution from page-level features. A succes s-ful prediction from page features will not only enable third -parties without access to browsing logs to use dwell time information, but will also provide us with an opportunity to identify page features that are most related to dwell time distributions. We describe the experimental setup and page features in Section 5.1, report on the prediction results in Section 5.2, and inspect the learned model in Section 5.3.
We randomly sampled 5000 pages from the set of pages whose test KS-distance is less than 0.05 from the ex-periments in Section 3.3. By choosing pages with a high goodness-of-fit to the Weibull distribution (small KS-distance), we can provide good training examples to the clas -sifier. For each sampled page, the and k values fitted on the training portion of data are taken as the learning la-bels. In order to extract page features, we crawled these pages using a dynamic crawler, which employs an Internet Explorer object to execute all dynamic components ( e.g. , flash, javascript, etc. ) and download the final rendered page. Pages containing the term X  X ogin X  X re excluded because thes e login pages are usually automatically loaded through a time -out redirection. This, together with failed crawling, gave us a set of 4,771 pages, which are randomly partitioned into training, validation and testing sets with a ratio of 7:1:2.
Since we want to inspect the learned model we use Mul-Feature Description PageSize Size (in bytes) of the rendered page PageHeight Height of the rendered page PageWidth Width of the rendered page DownloadCount Number of total downloaded URLs DownloadTime Time to download all URLs SecDownloadCount Number of secondary URLs SecDownloadTime Time to download secondary URLs ParseTime Time to parse all URLs RenderTime Time to layout and render the page tiple Additive Regression Trees (MART) [11], which pro-vide good interpretability and high accuracy. We used the validation set to locate the optimal parameters, which in-clude the maximum number ( L ) of leaf nodes of the base learner tree and the shrinkage parameter ( v ). We varied L  X  { 2 , 3 , 4 , 6 , 11 , 21 , 25 } , v  X  { 1 , 2  X  1 , 2  X  recorded the number ( m ) of iterations that achieved the minimum error. The tuple ( L, v, m ) that achieved the low-est error on the validation set was used in the final testing phase.

We constructed the following three sets of features, ranked in ascending order based on their closeness to what users would actually experience when viewing that page in a Web browser:
We intentionally chose not to include advanced features such as PageRank, number of inlinks and any log-based fea-tures in the feature set, because these features are general ly not available to researchers outside search engine compani es. By restricting to page-level features, anyone can crawl the page, construct the features, reproduce the result, and mor e importantly, utilize the predictive model to asses dwell ti me for any pages that can be downloaded. http://wordlist.sourceforge.net/
In order to determine how different sets of features in-teract, we tested seven feature configurations as listed in Table 3. Also listed in the table are the optimal parameters determined by the validation set. In particular, the MART parameters for predicting and k are denoted by (  X  ) and k (  X  ), respectively.

Log-likelihood (LL) and KS-distance are again used as the evaluation metrics. For each test page, we evaluate the LL and KS-distance on the test portion data with the predicted and k values, and compare it with the baseline model, which returns the mean value (  X  ,  X  k ) across all training pages, which resembles, and is stronger than, the exponential mode l (c.f. Eq. 2).

Results are presented in Table 3, in which  X  X redict Win X  means that the predictive model achieves a higher LL (or a smaller KS-distance) than the baseline model. As the two metrics give very similar results, we will focus on the resul t based on LL in the following.

First, we see that the prediction model outperforms the baseline method on all seven configurations with statisti-cal significance. Sign tests for  X  0 : predict  X  baseline all return p -values that are very close to zero for the seven con-figurations. This result shows that low-level page features do carry some prediction power that can be leveraged for effective dwell time prediction.

Second, HtmlTag is as effective as Dynamic when used in-dividually, and when combined, they bring further improve-ment. This observation indicates that the nine Dynamic fea-tures are as predictive as the 93 HtmlTag features, and their prediction power is complementary. Conversely, Content in itself outperforms Content + Dynamic , and adding HtmlTag to Content only provides some marginal improvement. Finally, the best performance is actually achieved by Content + Dynamic . This is reasonable in that Dynamic repre-sents what users would experience immediately after clicki ng through to a page while Content corresponds to what con-tent users would see once the page is loaded. Note that adding HtmlTag does not provide much benefit. Given the promising results from Content + Dynamic and the fact that only the top-1,000 frequent words are used in Content , we ex-pect further improvements from better feature engineering , for example, by choosing words with high inverse document frequencies rather than simply the most frequent ones, or by including the number of graphics or tables in the page. We will explore how to fully utilize the content together with other kinds of features in future work. For now, let us in-spect the learned models to understand what page features are the most useful for dwell time prediction.
By virtue of the interpretability of MART, we could es-timate the importance of each feature and sort them in descending order of the estimated importance. Figure 5 depicts the six most important features for predicting and k respectively under each configuration. The figure for  X  HtmlTag + Content  X  is dropped due to space constraints.
Figure 5(a) shows that Html tags about scripts and links are the most important ( X  &lt;!-- X  is for comments in Html). In Figure 5(b), it is unsurprising to see that the document length is the most relevant feature, followed by words relat ed to pornography, games and news. This looks reasonable as the dwell times for those topics are likely very different, si nce users may interact with pages on these topics in different ways. Similarly, in Figure 5(c) we see that the height of the rendered page is the top feature for , followed by the page size and width. Interestingly, the time to parse the page is the most relevant feature for predicting k in Figure 5(c). This may suggest when parsing takes a comparably long time, the page will have a lower chance to survive users X  screening. Finally, for the remaining three figures involvi ng feature combinations, we see that Dynamic , although only comprising nine features, always appears near the top. This confirms our belief that the nine dynamic features are strong predictors; but because of the limited number of features, complementary support from Content is necessary for the best performance. This paper presents the first step in Weibull analysis of Web page dwell time data, which can be extended in both breadth and depth. In breadth, there are many characteris-tic functions/quantities for Weibull analysis besides the haz-ard function, e.g. , the cumulative hazard rate function and the mean residual life, each of which has a natural correspon -dence to interesting aspects of Web browsing. In depth, it is interesting to investigate how sophisticated models ( e.g. , mixture of Weibulls) would bring better goodness-of-fit and more insights into understanding user browsing behaviors.
The predictive models as presented here demonstrate the possibility of predicting Web page dwell time distribution s. While better feature engineering and algorithm improve-ments would likely further improve performance, the curren t approach has an inherent shortcoming: it predicts and k separately whereas it is their combination that determines the goodness-of-fit of the predicted model. So instead of predicting and k separately, a more principled approach could be to optimize the likelihood directly, which would Likely provide a much better goodness-of-fit.
The Weibull analysis in this paper reveals some implica-tions for understanding the browsing behaviors of all users . Alternatively, the user population can be partitioned alon g explicit dimensions such as time-of-day and geographical locations or implicit dimensions such as user intent and domain expertise estimates. For the latter, we can par-tition the dwell time based on how users reach the page, e.g. , through a search clickthrough, an advertisement click-through, or a link from a general Web page. In this way, we would gain more detailed understanding of user browsing dwell time in different scenarios.
This paper has drawn an analogy between abandoning a browsed page and the failure of a system, and presented the first Weibull analysis on Web page dwell time data. We found that general Web surfing exhibits a significant  X  X egative aging X  phenomenon, suggesting that users adopt a  X  X creen-and-glean X  browsing behavior where they vet the page prior to more detailed examination. This study brings a new approach to analyzing implicit feedback involving dwell time, complementing previously conducted user stud-ies in that area. We have proposed some directions for building more sophisticated dwell time models and presente d some implications for understanding user browsing behavio r. Future work will build on our application of Weibull analysi s, as well as the numerous successes of it in other application domains, to improve search and advertising.
The authors would like to thank Yutaka Suzue for the dy-namic crawler, Krysta Svore and Qiang Wu for the MART implementation, Wen-tau Yih and Alice Zheng for the dis-cussion on analyzing dwell time, and Xiaoxin Yin for the proofreading and helpful comments. The authors also ap-preciate the anonymous reviewers who not only offered us detailed review comments but also insightful suggestions o n future work. [1] R. Abernethy. The New Weibull Handbook . fifth [2] E. Agichtein, E. Brill, and S. Dumais. Improving web [3] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [4] J. Attenberg, S. Pandey, and T. Suel. Modeling and [5] G. Buscher, L. van Elst, and A. Dengel. Segment-level [6] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit [7] A. C. Cohen. Maximum likelihood estimation in the [8] W. J. Conover. Practical Nonparametric Statistics . [9] D. Downey, S. Dumais, D. Liebling, and E. Horvitz. [10] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and [11] J. H. Friedman. Greedy function approximation: A [12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [14] D. Kelly and N. J. Belkin. Reading time, scrolling and [15] D. Kelly and N. J. Belkin. Display time as implicit [16] D. Kelly and C. Cool. The effects of topic familiarity [17] D. Kelly and J. Teevan. Implicit feedback for inferring [18] E. Lehman. Shapes, moments and estimators of the [19] Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, [20] G. Marchionini and B. Shneiderman. Finding facts vs. [21] M. Morita and Y. Shinoda. Information filtering based [22] D. Nichols. Implicit ratings and filtering. In [23] H. Rinne. The Weibull Distribution: A Handbook . [24] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. [25] R. W. White and S. M. Drucker. Investigating [26] R. W. White and S. T. Dumais. Characterizing and
