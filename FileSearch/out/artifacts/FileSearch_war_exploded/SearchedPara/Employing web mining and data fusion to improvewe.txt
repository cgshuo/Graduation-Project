 1. Introduction
Ad hoc retrieval is the fundamental operation in IR (information retrieval). Given a few-word query rep-resenting an information need, an IR system is expected to select, or rank to the top, the set of all relevant documents from a target collection. Ad hoc retrieval also serves as a pre-processing step of reducing a huge collection into a small, signal-rich set for other NLP-intensive tasks such as question-answering and summa-rization. Improving ad hoc retrieval has important ramifications not only for normal searching, but also for other processing downstream.

Over the history of TREC ad hoc experiments, one finds some queries give good to excellent results while others return mainly irrelevant answers. We name these queries and their retrievals strong and weak, respec-tively. Generally, if the query words are precise in meaning like TREC topic #312 (hydroponics), it has no ambiguity and documents containing it have high probability of satisfying user needs. However, others are too general or vague, like #379 (mainstreaming), and their returned results are unsatisfactory. There are also other factors such as topic aspects and requirements in a query that contribute to poor effectiveness. What makes a query easy or difficult is elusive and an unsolved issue. TREC initiated the Robust Track ( Voorhees, 2004, 2005 ) and explicitly focuses on how to improve the poor performers of a query set. New effectiveness measures were introduced. This paper explains how data fusion can improve weak query effectiveness based on extensive experimental results. Web mining is employed to define alternate queries for enhancing ad hoc retrieval, and various methods of salient term selection from longer queries to probe the web are used. There are many retrieval activities on closed collections or private intranets that TREC experiments simulate. They may in particular benefit from this study. The paper is organized as follows: Section 2 discusses some factors that make queries weak. Section 3 compares the WWW to other external resources that may aid retrieval. Sec-tion 4 presents our approach of using web mining to assist ad hoc retrieval. Section 5 discusses our experimen-tal results when applying our method to short  X  X itle X  queries, while experiments with longer queries, including salient term selection, are discussed in Section 6 . Section 7 has our conclusion. 2. Weak queries
What makes a query weak, i.e. difficult for retrieval? Some contributing factors (certainly not exhaustive, see also Arampatzis, van der Weide, van Bommel, &amp; Koster, 1998; Buckley, 2004 ) include the following: cur-rent retrieval models use a bag-of-words approach that does not account for word relationships or semantic inferences whether in query statements or in documents. This lack of language understanding capability is bound to cause irrelevant retrievals due to wrong interpretation or unsolved ambiguities. Polysemous words can lead to meaning mismatches, while synonyms may lead to missing of matches. General concepts in queries such as  X  X eather-related accidents X  also lead to retrieval difficulties because documents usually report instances of such concepts in specific wordings like  X  ... tornado caused destructions ...  X . Although one can use tools like
WordNet for resolution, such concept hierarchies however are practically open-ended. When a query is short, only a word or two, it also does not express the particular aspects of a topic, requirements or restrictions that a user has in mind. The bottom line is that many retrieval failures may be attributed to inadequate user need representation.

Three new measures were introduced in TREC2003-4 to study effects on weak queries ( Voorhees, 2004, 2005 ).  X #0p10 X  is a utility measure of the number of queries that have zero precision at top 10 retrieved.
Another is  X  X rea X , a weighted sum of the average precision of the 25% weakest queries in a set (e.g. 12 out of 50). The weight is calculated as
This weighs the weaker ones heavier. The weak queries in a set are not fixed. They may change for different retrieval methods. A third measure is  X  X map X , which essentially uses the geometric mean rather than the arith-metic mean to calculate the average precision of a query set. This also weighs the low performing queries hea-vier but has the advantage of using the full set of queries instead of only 25%. Using fewer queries may lead to issues of evaluation stability among different runs. In addition, we also employ the traditional mean average precision ( X  X ap X ) measure. This weighs all queries equally, favors high precision improvements, and therefore may be considered as a strong query measure.

The retrieval environment for our experiments consists of about 2GB of texts (TREC-8 collection without the Congressional Records documents), and three sets of queries named:  X  X ard50 X  (50 chosen from TREC que-ries #301 to #450 that are known to be more difficult),  X 601-50 X  and  X 651-49 X  (topic sets, of sizes 50 and 49, respectively, introduced during TREC2003-4. The properties of these query sets may be viewed from the initial retrieval results tabulated in Table 1 : t-ini . The  X  X ard50 X  set is indeed more difficult than the other two, with t-ini having  X  X ap X  value of .1074, an  X  X rea X  of only .0062 and #0p10 = 8. The other two query sets have  X  X ap X  values greater than .28, and  X  X rea X  values of about .03. All retrievals are done with our PIRCS retrieval system.
This acronym stands for Probabilistic Indexing and Retrieval  X  Components  X  System, and is based on Rob-ertson &amp; Sparck Jones X  (1976) probabilistic model, but applied to document components (content words) using the principle of document self-relevance ( Kwok, 1995 ).

We will emphasis on the  X  X ard-50 X   X  X rea X  results since our objective is to improve weak queries. Table 1 also shows that pseudo-relevance feedback (PRF: t-prf ) does not work for the new measures:  X  X ard50 X   X  X rea X  drops to .0036 compared to .0063 (t-ini) while #0p10 increases to 13 from 8, although overall  X  X ap X  improves to .1332 from .1074. For a weak query, initial retrieval usually does not obtain any, or very few, relevant doc-uments and hence defeats the assumption of PRF (recently re-studied in Harman &amp; Buckley, 2004 ). Query set  X 601-50 X  shows the best t-prf  X  X rea X  value .0439 among the three, and improves over its t-ini. Apparently queries in this set are sufficiently strong for PRF to work. 3. External resources for enhancing query and retrieval
Investigators in the past have sought external resources to help improve query representation, such as domain-specific thesaurus or synonym lists ( Rada &amp; Bicknell, 1989 ) and WordNet ( Voorhees, 1994 ) in order to alleviate mismatches or missing terms, or collection enrichment ( Kwok &amp; Chan, 1998 ) to aid in PRF. The general experience seems to be that thesaural aid can be useful when it matches the topical domain of a query; otherwise results may not be consistent. External resources employed appear limited in scale or scope. When the concepts/topics of a query are inadequately covered, one may result in noisy rather than helpful assistance.
The Web has been in existence for over a decade. It is vast, dynamic, up-to-date, free, and for practical pur-poses an all-domain thesaurus that may remedy the shortcomings of the other types and be useful to help ad hoc retrieval. The Web has been employed in other tasks, such as question X  X nswering (e.g. Lin, Fernandes, Katz, Marton, &amp; Tellex, 2003 ) or cross-lingual IR (e.g. Nie, Simard, Isabelle, &amp; Durand, 1999 ). In Grunfeld,
L, Kwok, Dinstl, &amp; Deng (2004) we introduced a general method of web-assisted ad hoc retrieval that is par-ticularly useful for weak queries and, unlike other schemes ( Amati, Carpineto, &amp; Romano, 2004; Yom-Tov,
Fine, Carmel, Darlow, &amp; Amitay, 2005 ), does not need identifying which query is weak. Liu, Sun, &amp; Yu (2005) who employed phrase matching for the robust track also found web assistance to be useful. In the following section, we summarize our approach of web assistance for weak queries. 4. Exploiting the web to form alternate queries
Our web mining procedure for aiding ad hoc retrieval is shown in Fig. 1 and consists of four steps: 1. Given an ad hoc query, define associated web queries for a specific search engine; 2. Probe the web for relevant/related snippets or pages; 3. From the returned web items, define alternate queries for target collection retrieval; 4. Perform data fusion of retrieval lists from the original and alternate queries, and form final result.
Since the web is vast, all-encompassing, one assumes a high probability that some relevant pages actually use the original query words to express the needed information intended in the query. These relevant pages would be ranked high by a search engine, and the related terms in these pages may be a good source for form-ing alternate expressions for our query. For Step 1, if the original query is short it should be used directly as a web probe. If it is longer (e.g. more than seven words), search engines may return few or null output because the query words are Boolean ANDED together. (Using the OR operator is generally not effective because of the noisy nature of the web.) For these longer queries one needs appropriate term selection to serve as key-words to probe the web. The process is not trivial, and will be discussed in Section 6 . Steps 2 and 3 are straight-forward, similar to steps for PRF. Here, we employ terms to create alternate queries rather than for query expansion. A fixed number of terms are chosen based on occurrence frequencies (&gt;10) in the web output. The use of multiple queries for an information need has been previously discussed by Ingwersen (1994) as  X  X ntentional redundancy as the principle of polyrepresentation X . Step 3 may involve web page struc-ture for mining terms but will not be investigated here. Step 4, data fusion of sufficiently dissimilar retrieval lists, is very useful for weak queries, and will be discussed in Sections 5.2 and 6.3 . In an effort to have suffi-ciently dissimilar retrieval lists, we aim at composing alternate queries from the web using differing web probes and/or web output. These are discussed in Sections 5 and 6 . 5. Experiments with short  X  X itle X  queries TREC topics express user needs with different sections such as Title, Description and Narrative sections.
We first studied applying the above approach for simulated user short queries with the Title section only. They are predominantly two or three words and none longer than five. These queries denoted as t-ini (with some 2-word phrases added), t-prf (90 term expansion from top 10 t-ini retrieved documents) and their retrieval results have been introduced in Section 2 and tabulated in Table 1 .

To obtain dissimilar alternate queries ( t -1 to t -4 ) from the  X  X itles X  by web mining, we employ the Google search engine and vary the web query composition, their retrieval units, the size of alternate queries that also leads to different occurrence term weights. t-1 (100, 60) employs the  X  X itle X  words directly as web probe (with stop words removed but no stemming) to return 100 snippets, and defines alternate queries of 60 terms. Google extracts snippets that are short contexts containing some query keywords. They should be less noisy than web pages if the embedded query words are relevant, but may lack sufficient variety of related words for composing alternate queries. t -2 (40, 60) has the same web probes as t-1 but uses top 40 full pages returned (restricted to text or html pages with size &lt;30K bytes), and defines alternate queries of 60 terms. t-3 (40, 90) is similar to t-2 except that both snippets and full pages are considered and the alter-nate query size is increased to 90. Snippet words are repeated 10 times so as to compete with those from the web page proper. The purpose is to have both advantages of a lesser chance of a noisy topic shift and more word variety for the alternate query output. t-4 (40, 90) is similar to t-3 except that the web probe has phrases identified by Lin X  X  (1994) MINIPAR parsing (see also Section 6.1 ). When phrases (two words or more) are used to probe the web, often much fewer (or even no) pages are returned. In the later case, no alternate query is defined.
 5.1. Retrieval results  X   X  X itle X  and alternate queries
The above alternate queries are defined in order to provide dissimilar retrievals. As shown in Table 1 , with bolded entries representing the highest values for each query set, they provide better or substantially better effectiveness compared to t-ini. For example, many of the  X  X rea X  values from the alternate queries are double the respective initial retrieval values  X  in particular,  X  X ard50 X   X  X rea X  increases from. 0062 to .0153 for t-3. Most of their  X  X ap X  values (indicator for strong queries) also improve over those of t-prf  X  for example,  X 651-49 X   X  X ap X  improves to .3875 from t-prf value of .3408 for t-4. It appears that the few words in these  X  X itles X  are quite effective as web probes. They bring back snippets or web pages whose co-occurring terms can define long alternate queries for better target retrieval. Changes in the  X  X map X  values do not necessarily follow the behav-ior of  X  X rea X . Their values are not averaged because they are not linear. Since  X  X map X  accounts for strong que-ries also while  X  X rea X  evaluates weak queries more directly, we will not show  X  X map X  in later tables.
Table 1 also shows that each alternate query leads to best results in different query sets. The average of the four alternate queries ( X  X vg.4 X ) still gives substantial improvements vs. t-ini. Table 2 tabulates the average pre-cision (AP) of the weakest 10 queries in the  X  X ard50 X  set for t-ini and the alternate queries. It shows that among the seven worst performing queries with t-ini AP less than .01, only two, Query #322 and #389, have AP remaining below .01 for four and three of the alternate queries, respectively. The other five have their effec-tiveness improved above .01 by at least two alternates. Which alternate query leads to this improvement how-ever is difficult to ascertain. This leads us to explore for better and more stable results by data fusion and combining the retrieval lists from these queries. 5.2. Data fusion of  X  X itle X  and alternate query lists
The situation in Table 2 shows that a short query that is weak originally for target retrieval may have a good chance of getting improved by some of its alternative forms obtained through web mining. It is however not known in advance which alternate form brings improvement and which does not. Our proposal is to com-bine the original retrieval list with those from the alternate queries to provide a resultant that may enhance its  X  X rea X  effectiveness. Assume that one uses two different representations or algorithms for a query, providing two sufficiently dissimilar retrieval lists R 1 and R 2 . Fox &amp; Shaw (1994), Lee (1997) &amp; Vogt &amp; Cottrell (1999) as well as other investigators have shown that when they are combined into one list R = C ( R the average precision of R , prec( R ), may improve above prec( R for this is that different methods are more likely to retrieve the same relevant documents, than the same non-relevant documents. Combining retrievals may synergistically yield better results than the components.
When this does not materialize, prec( R ) tends to lie around the average of prec( R been observed from past data fusion experiments. For this to happen, the representations should be faithful to the information need yet sufficiently dissimilar from each other. Weak queries with initial average precision close to zero would therefore benefit from data fusion when some of its alternate queries return improved effectiveness.

Data fusion introduces additional considerations such as which lists to combine first, how many lists and their combination coefficients ( Kwok, Grunfeld, Deng, &amp; Dinstl, 2005 ). Fox &amp; Shaw X  X  (1994) combination strategy CombMNZ avoids the use of combination coefficient and has been shown to be effective when  X  X ap X  improvement is the objective. It combines several lists by normalizing the retrieval status values of each list to a range of 0 X 1, sum them if the same document appears in multiple lists, and multiplied the sum by the number of occurrence lists. Here, we investigate the usefulness of CombMNZ when  X  X rea X  improvement for weak que-ries is the objective. Different numbers of retrieval lists from Table 1 are chosen. Results shown in Table 3 are divided into groups of 2-, 3-, 4-and 5-list combinations, and their averages. The individual runs with the best and worst  X  X rea X  values for the  X  X ard50 X  queries are also named for later use.

From Table 3 , it is seen that the  X  X ard50 X   X  X rea X  measure improves with the number of lists used for com-bination. Successive additional alternate query combinations synergistically help to enhance their results up to a certain point. For example, the use of 4-list combinations for  X  X ard50 X   X  X rea X  (average = .0166) has a good chance of outperforming 3-list combinations (average .0150).

Generally, combinations that are best for each of the three query sets are different. For the stronger query sets, 3-list combination appears to be near optimal; it has an edge in achieving lower #0p10 (number of queries with zero precision at 10) values of 2.7 X 2.8 compared to 4-list combination. High  X  X rea X  values seem to be cor-related with low #0p10. The single 5-list combination has slight improvements over the 4-list average for  X  X ard50 X , and worse for the other two sets. The average  X  X rea X  values of data fusion with 4-list achieve 29% ( X  X ard50 X ), 39% ( X 601-50 X ) and 48% ( X 651-49 X ) above those average values for individual alternate queries of Table 1 without fusion.

When compared with t-ini, the average 4-list combination has improved the  X  X rea X  values by 168%, 120% and 200%, respectively for the three query sets.  X  X ap X  values also improve over the t-prf values by 32%, 10% and 17%, respectively. This demonstrates that our approach of defining alternate queries through web mining and data fusion is quite successful for these short  X  X itle X  queries.

Fig. 2 plots the average precision of the weakest twelve queries for t-ini and compares it to the worst and best 4-list fusion retrievals t.0156 and t.0175 in Table 3 . It is seen that except for #322 and #389, every query in the original t-ini retrieval got substantial improvement because some one or more retrievals in the 4-list combination manage to contribute much better precision result than the original. Both t.0156 and t.0175 fusion improvements are statistically significant at &lt;1% level compared to t-ini according to the 2-tail sign test.
 6. Experiments with medium length  X  X escription X  queries
Some users issue queries that are more verbose, like a sentence or sometimes longer. We call them medium size queries, and can be simulated by using the  X  X escription X  section of a TREC topic. These vary from 2 to 28 content terms and average to about 6.8 per query for the TREC topics. Their d-ini (initial retrieval) and d-prf (PRF retrieval) results are tabulated in Table 4 . The  X  X rea X  values for  X  X ard50 X  are quite similar to the  X  X itles X : at .0063 for d-ini and lower at .0049 for d-prf. The other query sets however have much better results. Longer representation appears to help strong query results. For our approach, these medium size queries may contain more content words than a web search engine can handle, and therefore needs some strategy of word selection in order to employ web probing to define alternate queries. Boolean AND queries for search engines may return null results when there are more than about seven words. Sections 6.1 and 6.2 discuss differing ways of forming useable web probes when given a medium size query. 6.1. Forming web probes by term selection
An obvious way to reduce a query size for web probing is to select only content terms. Salient term selection has occurred in various contexts and is difficult. For example, Dorr, Zajic, &amp; Schwartz (2003) try to mimic editors in forming headlines for a news story. Its purpose is for human consumption and its requirements may be different from ours. In PRF, terms need to be selected from an initial retrieval term pool to expand a query. Current common approach is to use scores of terms. The technology to select precisely a few terms may not be reliable ( Harman &amp; Buckley, 2004 ). Cronen-Townsend, Zhou, &amp; Croft (2002) identify important terms using their clarity score. Clarity score depends on a good initial retrieval and hence may be useful for improving strong queries only. Since web-probing already incurs a time penalty, it will be useful to explore other methods of term selection without relying on an initial retrieval. The following summarizes methods that were tried in Kwok et al. (2005) . (1) Manual selection:  X  X itle X  terms .
 (2) Term selection by term weight .
 (3) Term selection by part-of-speech .
 (4) Term selection by SVM classification .

Feature-1 : w q k = log[ q k /( L q q k ) * ( N w F k )/ F Feature-2 :cf k /df k =(1+ a * q k ) * F k / D k Feature-3 : q k
Feature-4 : idf-ratio = log( N d /min q { D k })/log( N d
Here, q k is the frequency of term k in query, L q query length, N documents in the target collection, F k , D k are the collection and document frequencies of term k . w term weight used for our PIRCS retrieval algorithm: we assume that good weight for retrieval is also good for web probing. cf k /df k is an average term frequency factor ( Katz, 1996; Kwok, 1996 ) that is useful as term importance indicator; here we weighted it with a function of q observation that repeated use (in document/query) of a content term indicates its importance. Since training data are terms with no query identity, idf-ratio may restore some indication of a term X  X  relation with others in a query. These features are used successively as sets of 2, 3 and 4 for characterizing terms. Their results will be denoted as classification with two features c2, c3 and c4, respectively.

A test  X  X escription X  query after term classification may end up empty, or only 1 or 2 terms. We augment these queries by terms from a term weight method before doing web probing. If the classification query has greater than six terms, it is also truncated to six. More details of this method are discussed in Kwok et al. (2005) .

In Table 4 , rows characterized by c2, for example, mean using the first two features during classification. c2 i d, c3 i d and c4 i d denote concatenating the selected terms to the original  X  X escription X  (equivalent to double-weighting the selected terms) and lead to enhanced initial retrieval. For  X  X ard50 X , the  X  X rea X  effectiveness order sification. Using only the selected terms for retrieval (not shown) is worse than d-ini however.
Rows c2fpg, c2snpt show results of alternate queries formed by web probing with selected terms, and employing full page (fpg) or snippets (snpt). The  X  X ard50 X   X  X rea X  values for c2fpg (.0069), and c2snpt (.0094) improve over d-ini value of .0063. Training by the  X  X itles X  appears to help in focusing the enhanced query (c2 i d), or defining better web queries (c2fpg, c2snpt) for the  X  X ard50 X  set. The other two classifiers c3 and c4 however perform worse than d-ini.

The best  X  X pg X  and  X  X npt X  results of the three query sets via classification are highlighted in Table 4 . c4fpg and c4snpt are good for 651-49 set, but overall c2fpg and c2snpt seem to perform best. Unlike alternate query results derived from  X  X itles X  ( Table 1 ), many of these  X  X escription X  results are inferior to d-ini, reflecting the difficulties of forming effective web probes automatically. However, the usefulness of these runs is in their effect on data fusion to be presented in Section 6.3 . 6.2. Forming web probes by window rotation
In Kwok et al. (2005) , we proposed another method of probing the web for composing alternate queries called  X  X indow rotation X . This method avoids term selection and the difficulties associated with it. A window of size w rotates over a query to define web probes. A  X  X escription X  with m terms ( m &gt; w ) will give m such probes. They produce m lists of web pages after web searching. Returned web pages are ranked and selected based on their occurrence frequency in these m lists, and with frequency P 2. Queries with less than or equal to w terms default to the usual single web retrieval. The resultant list of web items then defines an alternate query as before. This method is more time consuming. If only snippets are used, the average time per query is about 10 X 15 s. If full page is used, it might take three times as much or longer depending on the query length and the net traffic. We formed three such queries all using a rotation window of size 5: d-1 (100, 60) produces final retrieval list of 100 snippets and form a maximum 60-term alternate query output; d-2 (40, 90) is similar to d-1 but with final retrieval list of 40 full pages and their snippets (like t-3); maximum 90-term alternate query output. d-3 (40, 90) is the same as d-2 except that MINIPAR is used to identify 2-word phrases which, together with other content words, defines the web probes.

Table 5 shows results of these alternate queries based on  X  X indow rotation X . Like term selection methods, their  X  X rea X  measures are also inferior to those of d-ini for all query sets. However, they are useful as dissimilar retrieval lists for data fusion. The bolded entries in Table 5 show the best results among the three alternate queries obtained for each query set. 6.3. Data fusion experiments for  X  X escription X  queries
As discussed in Section 5.1 , the chance of having some dissimilar alternate representations of the same weak query to produce better results is reasonably good for  X  X itles X . We assume similar effects also apply for a  X  X escription X  weak query even though on average the alternate queries under-perform compared to d-ini in  X  X rea X  measure for  X  X ard50 X . The enhanced  X  X escription X , the  X  X os X , SVM classification and window rotation are very dissimilar methods of defining alternate queries, and we investigate whether data fusion can work to improve retrieval effectiveness in this situation. 6.3.1. Data fusion of enhanced  X  X escription X  and alternate query lists Table 6 shows results of a set of 4-list combinations using enhanced  X  X escription X  c2 i d,  X  X OS X  and various
SVM alternate query retrievals (without window rotation). For  X  X ard50 X   X  X rea X , the worst, best and average values attained are, respectively, .0106, .0135 and .0122. The average almost doubles that of d-ini, and shows that although the alternate queries individually may not be as effective as d-ini, their combination can syner-gistically boost the weak performers (as well as those in the other two query sets). Like Table 3 , 2-list and 3-list results (not shown) are less effective than 4-list. The advantage of not using window rotation method is that alternative queries can be formed more efficiently.

However, if alternate queries from window rotation are also used for data fusion, Table 6 shows that greater  X  X ard50 X   X  X rea X  average effectiveness of .0150 can be achieved with just 3-list combinations (without SVM alter-nate queries). With 4-list combinations, further slight improvements are observed in  X  X ard50 X  and  X 601-50 X  query sets and more substantial improvement for  X 651-49 X  set. The 2-list shows that window rotation alternate queries are quite effective when combined with the enhanced  X  X escription X  query, giving an average  X  X rea X  value of .0118 and almost doubling that for d-ini. When compared with d-ini, the average 4-list combination has improved the  X  X rea X  values by 140%, 23% and 131%, respectively, for  X  X ard50 X ,  X 601-50 X  and  X 651-49 X  query sets.
Compared with d-prf, the 4-list average improves for the  X  X ard50 X  and  X 651-49 X  query sets, but lags for the  X 601-50 X  set. The  X 601-50 X  set has the best d-ini results among the three for both  X  X itle X  and  X  X escription X , and contains stronger queries. For them, PRF works better. These results demonstrate that our approach of using web min-ing and data fusion can enhance weak query effectiveness for longer  X  X escription X  queries.

Fig. 3 plots the average precision of the weakest 12 queries for d-ini and compares it to the two best and worst 4-list fusion retrievals d.0125 and d.0187 of Table 6 . Except for queries #389 and #401, every query in the original d-ini retrieval got substantial improvement because some one or more retrievals in the 4-list com-bination manage to contribute much better precision result than the original. Both d.0125 and d.0187 improvements are statistically significant at &lt;1% level compared to d-ini according to the 2-tail sign test. 6.3.2. Data fusion of  X  X escription X  and  X  X itle X  retrievals
Since the  X  X itle X  and  X  X escription X  of a topic carry the same information need of a user but in different word-ings, they can be considered as alternate queries to each other. It is worth exploring the effects of their com-bination on the weak queries. In a realistic ad hoc retrieval situation, one may design a system to ask users to highlight or underscore certain important terms when composing a (medium size) query. These may serve as the  X  X itle X , while the original statement as  X  X escription X . The 3rd and 4th data rows of Table 7 show the results of combining the initial  X  X itle X  and  X  X escription X  retrievals as well as their PRF retrieval lists. The fusion of t-ini and d-ini is quite effective, giving a value of .0142 for the  X  X rea X  of the  X  X ard50 X  X et, and only slightly worse than the .0166 average value of Table 3 using 4-list combination. Combining t-prf with d-prf gives a lesser improvement to .0112 for the  X  X rea X  of  X  X ard50 X . These are our new baseline.

In the next four rows of Table 7 are shown fusion results of the best and worst 4-list groups from Table 3 and from Table 6 . These runs are identified by their  X  X ard50 X   X  X rea X  values such as t.0175 or d.0187. These com-binations are quite stable. Irrespective of whether one uses the best or worst lists for fusion, the resultant attains a  X  X ard50 X   X  X rea X  value between .025 and .027. The average of .0261 is 84% better than the (t-ini, d-ini) baseline of .0142. Similar improvements for the  X 601-50 X  and  X 651-49 X  query sets are 42% and 79%, respec-tively. Fig. 4 plots the improvements for the 12 weakest queries of the best td.0268 and worst td.0254 fusion runs of Table 7 compared to (t-ini, d-ini). Query #389 is the only one left with average precision below .01. 7. Conclusion
We propose that for weak queries with precision close to zero, data fusion of sufficiently dissimilar retrieval lists can improve their effectiveness. This is supported by the experimental results in this paper: short  X  X itle X  queries of the  X  X ard50 X  query set has  X  X rea X  measure improved by an average of 168% from the initial retrieval value of .0062 to .0166, and  X #0p10 X , number of queries with zero precision at ten documents retrieved, drops from 8 to an average of 5.3 ( Table 3 ). For longer  X  X escription X  queries, this approach also improves the  X  X rea X  measure an average of 138% from .0063 to .0150, and  X #0p10 X  decreases from 7 to an average of 3.7 ( Table 6 ).
This represents improved quality of retrieval service to users whose queries need the most help. The combina-tion formula combMNZ is also effective for weak  X  X rea X  measure.

To realize such a strategy, we employ the web as an all-domain thesaurus, and mine it for related terms to help construct dissimilar alternate queries. Although the alternate queries may have mediocre performance individually, their ability to provide dissimilar retrieval lists contributes to substantial  X  X rea X  improvements via data fusion. Short queries can be used directly to probe the web. For longer queries, POS and SVM clas-sification methods can be used to select salient terms to be used for web searching. A more time consuming window rotation method produces even more effective alternate queries without term selection. In most cases, data fusion of four dissimilar alternate retrieval lists seems to give good results.

We have composed different alternate queries for use in fusion. It will be useful if one can identify which alternate query formation method is more effective. Our approach of web-assistance and data fusion is applied to all queries. It would be useful to have a method to identify which query is weak (or strong) with respect to a retrieval environment. One can then target individual queries with this or other specific method in a more indi-vidual fashion. It would also be useful to investigate other more effective ways of improving salient term selec-tion for web probing.
 Acknowledgements
This work was partially supported by a contract from the US Government and any opinions, findings, con-clusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the US Government.
 References
