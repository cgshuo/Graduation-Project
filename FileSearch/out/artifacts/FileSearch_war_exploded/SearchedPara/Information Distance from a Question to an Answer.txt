 We provide three key missing pieces of a general theory of information distance [3, 23, 24]. We take bold steps in for-mulating a revised theory to avoid some pitfalls in practical applications. The new theory is then used to construct a question answering system. Extensive experiments are con-ducted to justify the new theory.
 F.1.1 [ Models of Computation ]: Foundations of data mining; H.3.4 [ Systems and Software ]: Question-answering (fact retrieval) systems information distance, normalized information distance, ques-tion -answering system
Theory, Algorithms, Design, Experimentation
The Internet embodies every aspect of the human knowl-edge, from science to art, from literature to history, from medicine to entertainment, from travel to shopping, from our daily lives to international politics, from the elite to the hoi polloi, from the origin of the universe to the end of it, usually truthful but sometimes misleading, all languages, all cultures, and all people.

Fundamentally, one of the obstacles that stand between us and this universal but unstructured knowledge is a metric that measures what piece of information is the closest to a proper answer to a question. This  X  X iece of information X  may be an article, a webpage, a paragraph, a phrase, or just a word, or an approximate occurrence of any of above. Corresponding author
In the data mining community, some have argued that no such (universal) metric exists. Tan et al [39] studied an exhaustive list of 21 measures and concluded that (among these measures)  X  X here is no measure that is consistently better than others in all application domains X .

Over the past decade, it has been the goal of our (the last author and his colleagues X ) long term effort to develop precisely such a metric of an information distance [3, 23, 24] that is consistently and provably better than all other  X  X ea-sonable X  metrics in all application domains. These include all metrics listed in [39] that satisfy distance metric require-ments and when they are normalized to the range of 0 and 1 to be compared with the normalized information distance. See also [25]. This paper may be considered as a part of that continued effort.

The theory has been accepted and further studied by the theoretical community [43, 42, 36, 30, 29, 9]. It has also been partially accepted by the data mining community [18]. In [18], Keogh, Lonardi, and Ratanamahatana compared a variant of our approach in [23] to 51 measures from 7 data mining related conferences including SIGKDD, SIGMOD, ICDM, ICDE, SSDB, VLDB, PKDD, PAKDD, and have concluded that our information distance/compression based method was superior to all these parameter-laden methods on their benchmark data. In the meantime, the theory has found dozens of applications in many fields from weather broadcasting to software engineering, and to bioinformatics [1, 5, 8, 10, 11, 14, 15, 19, 20, 21, 22, 38, 31, 32, 33, 35, 2]. A complete list of references will be included in the 3rd edition of [25]. Despite of this success, three major pieces of the theory are still missing:
In this paper, we will answer these problems, while the emphasis of the paper is on the last two problems and to use this theory to build a real QA system. We will take bold steps to develop a new theory. We will also provide methodologies of how to unify various scoring systems in-cluding those introduced in [23, 24, 11] as well as those in [39] under one paradigm. We then describe a prototype QA system QUANTA based on our new theory. Extensive ex-periments show QUANTA has high potentials to become a practical system. Kolmogorov complexity was introduced in the 1960 X  X  by R. Solomonoff, A.N. Kolmogorov and G. Chaitin, see [25]. It defines randomness of an individual (finite or infinite) string. Kolmogorov complexity has been widely accepted as an information theory for individual objects parallel to that of Shannon X  X  information theory which is defined on an ensemble of objects. It has also found many applications in computer science such as average case analysis of algorithm s [25]. Fix a universal Turing machine U . The Kolmogorov complexity of a binary string x condition to another binary string y , K U ( x | y ), is the length of the shortest (prefix-free) program for U that outputs x with input y . It can be shown that for different universal Turing machine U 0 , for all x, y where the constant C depends only on U 0 . Thus we sim-ply write K U ( x | y ) as K ( x | y ). We write K ( x | ), where is the empty string, as K ( x ). For formal definitions and a comprehensive study of Kolmogorov complexity, see [25].
In the classical Newton X  X  world, we know how to measure  X  X hysical X  distances. Today, we live in an information soci -ety. Can we similarly measure the  X  X nformational X  distance in the cyber space between two objects: two documents, two letters, two emails, two music scores, two languages, two programs, two pictures, two systems, two genomes, or between a question and an answer? Such a measurement should not be application dependent. Just like in the clas-sical world, we do not measure distances sometimes by the amount of time a bird flies and sometimes by the number of pebbles lining up on the Santa Barbara beach.

A good information distance metric should not only be application independent but also universally minorizing a ll other  X  X easonable X  definitions.

The task of a universal definition of information distance is elusive. Traditional distances such as the Euclidean dis -tance or the Hamming distance obviously fail for even trivia l examples. Tan et al [39] have demonstrated that none of the 21 metrics used in data mining community is universal. In fact, for any computable distance, we can always find coun-terexamples. Furthermore, when we wish to adopt a met-ric to be the universal standard of information distance, we must justify it. It should not come out of thin air. It should not be from a specific application. It should be as good as any definition for any application.

From a simple and accepted assumption in thermodynam-ics, over the last decade, we have derived such a universal information distance in [3, 23, 24] and a general method to measure similarities between two sequences [23]. The theory has been initially applied to alignment free whole genome phylogeny [23], chain letter history [4], language history [5, 24], plagiarism detection [8], and more recentl y to music classification and clustering [10, 14], parameter-free data mining paradigm [18], internet knowledge discov-ery [11], protein sequence classification [21], heart rhyth m data analysis [37, 35], and many more.

What would be a good departure point for defining an  X  X nformation distance X  between two objects? To answer this question, in the early 1990 X  X , we [3] have studied the en-ergy cost of conversion between two strings x and y . Over half a century ago, John von Neumann hypothesized that performing 1 bit of information processing costs 1 KT of en-ergy, where K is the Boltzmann X  X  constant and T is the room temperature. Observing that reversible computations can be done for free, in early 1960 X  X  Rolf Landauer revised von Neumann X  X  proposal to hold only for irreversible com-putations. We proposed in [3] to use the minimum energy needed to convert between x and y to define their distance, as it is an objective measure. Thus, if one wishes to erase string x , then one can reversibly convert it to x  X  , x  X  X  short-est effective description, then erase x  X  . Only the process of erasing | x  X  | bits is irreversible computation. Carrying on from this line of thinking, we have defined in [3] that the en-ergy to convert between x and y to be the smallest number of bits needed to convert from x to y and vice versa. That is, with respect to a universal Turing machine U , the cost of conversion between x and y is: vation, and some other concerns, we have defined the sum distance in [3]: However, the following theorem proved in [3] was a surprise. Theorem 1. E ( x, y ) = max { K ( x | y ) , K ( y | x ) } . Thus, the max distance was defined in [3]: Both distances are shown to satisfy the basic distance re-quirements such as positivity, symmetricity, triangle ine qual-ity, in [3]. It was further shown that D max and D sum mi-norize (up to constant factors) all other distances that are computable and satisfies some reasonable density condition that within distance k to any string x , there are at most 2 k strings. Formally, a distance D is admissible if D max ( x, y ) satisfies the above requirement because of Kraft X  X  Inequality (with K being the prefix-free version of Kolmogorov complexity). It was proved in [3] that for any admissible computable distance D , there is a constant c , for all x, y , Putting it bluntly, if any such distance D discovers some similarity between x and y , so will D max .
 However, when we [23] tried to use information distances, D sum and D max , to measure similarity between genomes in 1998, we had a problem. E. coli and H. influenza are sis-ter species but their genome lengths differ greatly. The E. coli genome is about 5 megabases whereas the H. influenza genome is only 1.8 megabase long. D max or D sum between the two genomes are predominated by genome length dif-ference rather than the amount of information they share. Such a measure trivially classifies H. influenza to be closer to a more remote species of similar genome length such as A. fulgidus (2.18 megabases) than to E. coli .

In order to solve this problem, we introduced  X  X hared in-formation distance X  in [23]: where K ( x )  X  K ( x | y ) is mutual information between se-quences x and y [25]. We proved the basic distance met-ric requirements such as symmetry and triangle inequality, and have demonstrated its successful application in whole genome phylogeny in [23] and evolutionary history of chain letters [4]. It turns out that d share is equivalent to (using Symmetry of Information Theorem, Theorem 2.8.2 in [25], page 182) Thus, it can be viewed as the normalized sum distance. Hence, it becomes natural to normalize the optimal max distance in [24]: The distance d max ( x, y ) was called the  X  X ormalized infor-mation distance X  and its metricity properties were proved similar to that of normalized sum distance in [23].
We wish to develop a theory of information distance be-tween two concepts or between a question and an answer word/phrase for the purpose of developing a QA system. The new theory needs to solve three practical problems that trouble the d max ( x, y ) theory.

The first problem is how to remove the impact of irrelevant information. Consider the following QA example.  X  X hich city is Lake Washington by? X  (Question 1536) There are several cities around Lake Washington: Seattle, Kirkland, and Bellevue, which are all good answers. The most popu-lar answer Seattle contains overwhelmingly irrelevant inf or-mation, not related to the lake. The d max measure tends to choose a city with higher complexity (lower probability) such as Bellevue. Can we remove the irrelevant information in a coherent theory and give the most popular city Seattle a chance?
The second problem is: should an  X  X nformation distance X  really satisfy the triangle inequality? Imagine you and a stranger sitting next to you on the plane find out you share a common friend Joe Smith. The X  X istance X  X etween you the stranger gets suddenly much closer via Joe Smith! Consider a QA problem: The concept of  X  X arilyn Monroe X  is pretty far from the concept  X  X resident X . However, Marilyn Monroe is very close to  X  X FK X  and X  X FK X  is very close to the concept of  X  X resident X . In the academic world, this phenomenon is reflected by the  X  X rd  X  os number X . We all feel closely related via a third person Paul Erd  X  os. Think about your first date, did you cleverly find a conversation topic that subtly drew you and her closer?
An information distance must reflect what  X  X e think X  to be similar. And what  X  X e think X  to be similar apparently does not really satisfy triangle inequality.

Fagin and Stockmeyer gave an example of partial pattern matching where the triangle-inequality does not hold [17]. Veltkamp puts it vividly in [41]: under partial matching, th e distance between a man and a horse is larger than the sum of the distances between a man and a centaur and between a centaur and a horse, respectively. The QA problems often depend on partial information, and is in similar situation a s partial pattern matching.

Some objects are popular, such as Seattle mentioned above, and they are close to many other concepts. To model this phenomenon properly is our third problem. We need to relax the neighborhood constraints of Eq. 2 and Eq. 7 to allow some selected (very few) elements to have much denser neighborhoods. In fact, in the D sum ( x, y ) distance, we have already observed similar phenomenon many years ago, and proved a theorem about  X  X ough guys having fewer neigh-bors X , [25], Theorem 8.3.8, page 548. Note that this third problem is closely related to the first problem: only when we allow a few popular objects to have very dense neighbor-hoods, it is then possible that they are selected more often.
We will take care of all three problems naturally in one theory. Let us go back to the starting point of information distance. In Eq (1), we asked for smallest number of bits that must be used to convert between x and y . Keeping the original motivation in mind: some information in x or y are not relevant to this conversion, they can be kept aside in this conversion process. We thus define: with respect to a universal Turing machine U , the cost of conversion between x and y is:
E min ( x, y ) = min {| p | : U ( x, p, r ) = y, U ( y, p, q ) = x,
To interpret, the above definition separates r as the in-formation for x and q as the information for y . Define D min ( x, y ) = E min ( x, y ). In all of the following theorems and proofs, we omit O (log( | x | + | y | )) factors. Theorem 2. D min ( x, y ) = min { K ( x | y ) , K ( y | x ) } . Proof. Without loss of generality, assume that K ( y | x )  X  in [3, 25], it is known that there exists a program p of length K ( x | y ) and q of length K ( y | x )  X  K ( x | y ), such that We also need to show that q contains no information about x by proving K ( x | q ) = K ( x ). By the Symmetry of In-formation Theorem (Theorem 2.8.2 in [25], page 182), we we can construct y via x , p , plus K ( q | x ) bits. That is: K ( y | x ) &lt; | p | + | q | = K ( y | x ), contradiction. tice that if less than K ( x | y ) amount of information is present in p , then extra information of K ( x | y )  X  X  p | bits, in addition to K ( y | x )  X  K ( x | y ), will need to be given in both q and r , hence the sum will be more than E ( x, y ), violating the condition.

Observe the following interesting phenomena:
The above three properties naturally co-exist in the new theory. We can now normalize D min . Observe again that d min ( x, y ) is symmetric and positive, but it does not satisfy triangle inequality (proof omitted).
While it is clear that D min ( x, y )  X  D max ( x, y ), it is not clear if such a relationship would hold for d min vs d max following theorem shows that this holds after all. Theorem 3. For all x, y , d min ( x, y )  X  d max ( x, y ) .
Proof. Given a pair of x and y , without loss of gener-ality, assume that K ( x | y )  X  K ( y | x ). By the Symmetry of Information Theorem (Theorem 2.8.2 in [25], page 182), we know that up to an additive logarithmic factor, Thus, from K ( x | y )  X  K ( y | x ), we derive K ( x )  X  K ( y ). K ( x | y )  X  K ( x ), we have for any  X   X  0. Setting  X  = K ( y )  X  K ( x ), and using the Symmetry of Information Theorem again on the top, the rem hence follows.
Eq. 3 shows D max is universal. Hence D min is also univer-sal. However, only weak universality theorems of the nor-malized information distances, d max and d sum ( d share ), were proved. In [23], we proved: for any computable distance D , there is a constant c  X  2 such that, with probability 1, for all sequences x and y , d share ( x, y )  X  cD ( x, y ). It was not clear how to remove the phrase  X  X ith probability 1 X . The new MIN distance fixes this problem naturally.

In [24], we have tried to avoid this problem by rescaling the density conditions changing from in [23] to However, Formula (8) is so restrictive that no reasonable distances can satisfy such requirement except our own nor-malized information distance. Thus the universality state -ment is meaningless. Using the original Formula (7), it is possible to prove the following weak universality theorem. In fact, we actually show that this is the best we can do.
Theorem 4. (a) For any computable distance d , satis-fying density requirement (7), for all sequence x and y , if then, (b) For some nonrandom x and y , Inequality (9) does not hold.

Proof. Item (a) requires only a standard proof, using [23]. To see Item (b), consider normalized Hamming dis-tance H ( x, y ) = h ( x, y ) /n , where h is the Hamming dis-tance and x and y are binary strings of length n . Now, let x 0 = 0 n and y 0 = 10 n  X  1 . We have: H ( x 0 , y 0 ) = 1 /n , while d max ( x 0 , y 0 ) = O (1). Thus, H ( x 0 , y 0 ) &lt;&lt; d large n .

Can we normalize D max ( x, y ) differently so that the stronger universality holds for all x, y ? This is still an open question, but the obvious attempts do not work. Suppose we try to Then the universality holds, but d 0 max does not satisfy the triangle inequality any more.

Delightfully, since we did not want the min measure to satisfy the triangle inequality anyways, is actually universal under the original proper definition o f [23], Formula (7), without the X  X ith probability 1 X  X onditi on. This naturally resolves the universality problem.
Theorem 5. For any computable distance d satisfying (7), for all sequences x and y , where n = min {| x | , | y |} .

Proof. Using the density property of Formula (7) and the computability of d , we know K ( x | y )  X  d ( x, y ) | x | and d min ( x, y ) = For historical reasons, and to be consistent with d max , we have used d min , instead of d 0 min , in the experiments.
Both d min and d max are implemented in our QA system, with d max taking care of the more balanced matching situa-tions and d min designed for partial information matches for in-balanced situations. d min and d max are not computable. They must be approximated heuristically. Since these dis-tances are universal, theoretically, all computable dista nces, including all those in [39], and compression methods in [23] and frequency counts in [11] are all legitimate approxima-tions. It has been observed by many people that combining a strategy of searching more structured data with a statisti -cal strategy of searching unstructured data is important [2 8, 13, 7]. The distances d min and d max now provide a unifica-tion of all the methods. By definition, d min and d max are neither statistical methods nor structured pattern matchi ng methods, they are both, and more. They provide a natural way of combining these methods.
 Words are concepts encoded according to some language. The concept  X  X he top direction of the earth X  is encoded as  X  X orth X  with 5 letters in English, encoded as  X  X ord X  with 4 letters in French, and with  X  X ei X  1 character in Chinese. Now, let X  X  forget about such a priori encoding. Let us treat each word (or phrase) on the internet as a placeholder for the corresponding concept. Given a query, we want to find an answer that has the smallest information distance ( d min or d max ) to this query. The K terms in d min or d max can be approximated in several ways: 1. We can use approximate pattern matching with the 2. We can use frequency count, and use Shannon-Fano 3. Any of the methods, after being normalized, in [39]. 4. Mixed usage of above (1)  X  (3) for the K terms in
Thus different methods of approximating d max and d min are unified under one roof of the information distance.
This might be a proper point to compare d min and d max with the existing measures listed in [39]. D min may be con-in [39], if we insist on approximating K terms by statistics. Neither d min nor d max corresponds to any measure listed in [39], even restricted to pure probabilistic sense via Shann on-Fano encoding. In any case, other than being proved to be universal theoretically, the practical advantage of our no r-malized information distance measures is that they are nei-ther pure statistical measures (like the 21 measures in [39] ) nor pure pattern based measures. They are both, and more, naturally. They are very easy to use, and with a clear cri-teria: encoding length. This is particularly convenient to a QA system.

The measures of d max and d min need to be extended to conditional versions to be used in our system. Define: where c is the conditional sequence that is given for free to compute from x to y and from y to x .

This is important to a QA system. For example, for a polysemous word  X  X an X , Table 1 shows the d max distance from  X  X an X  to  X  X PU X  and from  X  X an X  to  X  X tar X , under three conditions: the empty condition,  X  X emperature X , and  X  X ol-lywood X .

All theorems proved for all variants of (normalized) in-formation distance still hold under condition c . This seem-ingly innocent definition actually faces some complication s in practice. For example, condition x, y, c are sometimes not given as separate entities. The condition c is often mixed in x and y . That is, we are often only given some encodings of x condition on c and y condition on c . At least we can show that for some reasonable encodings, The conditional versio n of d max and d min are still well-defined, as the following the-orem shows.

Theorem 6. For all sequences x, y, c , there exist x c and y , where x c is a shortest program computing x from c and y is a shortest program computing y from c , such that K ( x | y, c ) = K ( x c | y c , c ) modulo an additive O (1) term. Thus, up to an additive O (1) term, d max ( x c | y c , c ) = d max ( x | y, c ); d min ( x c | y
Proof. In [29], Muchnik proved that, there exists x c and y , such that K ( x c | x ) = O (1), K ( y c | y ) = O (1), K ( x | x O (1) and K ( y | y c , c ) = O (1), then At the same time, Thus, K ( x | y, c ) = K ( x c | y c , c ) + O (1).
A note on D min and D max vs d min and d max . The choice of using the normalized information distances, d min and d max was because they give more practical ranking. We have given one such example of genome-genome comparison. Consider another example of QA:  X  X ho is the greatest scientist of all? X  The statement  X  X ewton is the greatest scientist of all  X  has 7 precise matches on the internet;  X  X instein is the great -est scientist of all X  has 1 precise match; whereas  X  X od is the greatest scientist of all X  has 27 precise matches. Normaliz ed answer is Newton, and unnormalized answer is God.
We have described a complete theory: from a single ther-modynamic principle to universal metrics d min and d max , and to methods of how they can be used. We are now ready to apply our theory to develop a QA system.

The current search engines return a list of webpages, push-ing the burden of finding an answer to the users. This is not satisfactory, as a user may not want to or maybe unable to (a cell phone user), and should not be required to go through the pages to look for an answer.

Despite of their usefulness and successes, the wikipedia type of approaches or  X  X uman annotated knowledge bases X  have inherent disadvantages:
The goal is to extract succinct and correct answers from the internet to a question given by an English sentence. An elegant system for this purpose is the ARANEA system by Lin and Katz [26]. Similar systems at LCC (QAS), Power-set.com, ask.com, google.com are under development.
Guided by the theory described in this paper, a prototype of QUestion-ANswer-TAvern (QUANTA) system has been implemented. Initial experiments show high potentials of the QUANTA system. It answers many questions better than Google.com, ask.com, ARANEA, and START of MIT. The idea of using web information is certainly not new. The idea of combining a strategy of searching more struc-tured data with a statistical strategy of searching unstruc -tured data is also not new. See for example [28, 13, 7]. What X  X  new in our system is a natural integration of vari-ous measures under one roof: the information distance. The universal information distance gives a clean, natural, and provably best way of combining different ways of ranking the answers.

QUANTA consists of modules for query formulation, doc-ument and passage retrieval, candidate generation, and sco r-ing to generate the final exact answer to the original ques-tion, as shown in Figure 1. We briefly describe the main modules below.
The  X  X uestion normalization X  module normalizes ques-tions to the simpler forms. For example, questions like Tell me what ... are simply transformed to What ... form.
The  X  X uestion type classification X  module classifies the question according to the answer type. The answer type, such as PERSON or DATE , helps the system generate proper answer candidates.

Questions are further classified by their syntactic struc-tures. The syntactic structure, expressed as regular expres-sions, can help formulating queries, and telling the system what condition template to use in the later stages. An SVM, libsvm [6], is used to improve the classification.
The Part of Speech (POS) tagging is done by SS Tagger [40], with our own improvements.

The noun phrase chunking (NP chunking) module uses the basic NP chunker software from [34] to recognize the noun phrases in the question.
 The named entity recognition (NER) module uses the Stanford named entity recognizer software package from [16]. This software not only recognizes names, but also classify names into X  X erson X , X  X rganization X , X  X ocation X , and X  X isc X .
In this step of QUANTA, questions in natural language, after being processed by POS/NP chunking/NER modules, are transformed into queries, or requests, to search for the documents on the internet which may contain the answers.
Approximate queries are formed here to search for doc-uments in several forms: (a) approximate match; (b) co-occurrence in the same sentence, or neighboring sentences, or the same paragraph, or in the loosest case in the same webpage.

Because we did not have indexed web data, to perform the experiments, we had to use a commercial search engine for a temporary solution. Thus we suffer great loss in flexibility in our system and have to find ways to formulate queries with different strictness to maximally use a commercial search en-gine. For example, for each question, the most strict query requires documents exactly containing most part of the orig-inal text in the question, while a less strict query only looks for some important part of the question, and the most inex-act query only requires the isolated words occur in the docu-ment, independently In general, in QUANTA, many pattern Who was the first person to run the mile in less than four minutes? Figure 2: Queries for question  X  X ho was the first person to run the mile in less than four minutes? X  matching rules are applied to the question texts, its part-o f-speech tags, as well as the Named Entity chunking results, to generate different levels of queries. For example, when the pattern ( wh-word be [ noun chunk ] verb + ed ) is found, the exact queries such as ( X  X  noun-chunk ] be verb + ed  X ) and ( X  verb [ noun-chunk ] X ), inexact queries like ( X  X  noun-chunk ] X   X  be verb + ed  X ), as well as inexact query containing just all the words except for the wh-word are generated. An example is in Figure 2.
As a temporary solution, just to obtain experimental re-sults, QUANTA currently use a commercial search engine (such as Google, AltaVista or ask.com) as the document and passage retrieval engine. It does not matter which one, the results are approximately the same.

The queries, as shown in Figure 2, are sent to a search engine to retrieve top 100 snippets as the related passages. These summary texts are extracted in the summary extrac-tion module.
In the X  X -gram extraction X  X odule, all possible n -grams in the retrieved passages are extracted as raw candidates, wit h n = 1 , 2 , 3 , 4. Different scores are given to these candidates according to the corresponding queries which generate the source passage. Less strict queries will lead to smaller sco res. This part of QUANTA has been significantly benefited from the ARANEA system of Lin and Katz [26].
 Apparently there will be a large amount of raw candidates. A pre-filtering is performed to remove most of these can-didates in the  X  X ype filtering X  module. Firstly, duplicated items are merged and their scores are summed up. Secondly, all the candidates which are in the stop word list, or begin or end with such stop words are excluded. Finally, some heuristics are applied to take care of certain types of ques-tions, such as  X  X hat country X ,  X  X ow fast X ,  X  X hich year X . Only specified format or enumerations, filtered by various rules and dictionaries, are allowed to be the candidates for these questions.

After the above process, a simple combination module is performed to merge short candidates to long ones. For ex-ample, if  X  X ohn Wilkes Booth X  is found, then the candidates such as  X  X ooth X  or  X  X ilkes Booth X  are removed and their scores added to the integrated one.

With the above work done, reasonable answers will have high scores with high probabilities. Now we can come to the final step: scoring the candidates with d max and d min determine the final answer.
Only 10-20 candidates are left for the final information distance scoring. QUANTA system currently implemented two ways of approximating d max and d min (QUANTA com-bines d max and d min answers): (a) encoding via approximate match of question and answer sentences; (b) Shannon-Fano code to encode the probabilities to approximate K ( . ), see Example 1.11.2, page 67, in [25] for details of the encod-ing process. The minimum encoding via (a) and (b) is used for each K term in the d max and d min formulae. Item (a) requires an alignment and computing encoding overhead. Item (b) involves finding the probabilities of occurrences o f variables x and y under condition c . The powers of a struc-tured data search using alignment and a statistical strat-egy of searching unstructured data are thus organically in-tegrated.

As explained in Section 3.3, the conditional version of dis-scheme: to calculate the distance between one candidate and certain reference object, under certain conditions. There -fore, there are two tasks to do before the distance calcu-lation: to find an appropriate object as reference, and to formulate reasonable conditions.

It is usually simple to find a reference object, since it is very common for a question to contain some named entity, as a subject of the sentence in most cases and as an object in some other cases. For these cases, we can easily extract a proper entity as the reference object with the help of chunk-ing technique. There are indeed some questions which have no proper named entity to be used as a reference object. For such questions we X  X l use the unconditional version of d max
An important part of the system is the condition formu-lation module. The condition is very flexible to adapt for different contexts of various questions. This requires a loc al alignment of the condition to the sentence of paragraph that may contain the answer. However, in the current stage, in order to use a commercial search engine, this local align-ment method is temporarily replaced a hierarchy of condi-tions. The most strict condition is an exact sentence tem-plate which defines the texture of the sentence, including where x or y is, to be exactly matched, while the least strict condition is just some key words that must co-occur with x or y . This is similar to the query formulation part but is more complex.

To calculate the normalized information distance, we do the following.
Among other tests, we have evaluated our QUANTA sys-tem on a common factoid QA test set provided in [27]. The test set consists of 109 factoid questions, covering severa l domains including history, geography, physics, biology, e co-nomics, fashion knowledge, and etc., which are presented in various question forms. The open source factoid QA sys-tem ARANEA (downloaded from Jimmy Lin X  X  website in 2005) is used for comparison. The QA test set comes with a knowledge base which we have not used. Both ARANEA and QUANTA used the internet directly. ARANEA has 46 correct or 59 partially correct top 1 answers. QUANTA has 75 correct or 82 partially correct top 1 answers.
Some widely accepted evaluation measures, the top 1 an-swer percentage, and the mean reciprocal value of the rank (MRAR), are used in Table 2. Here, MRAR= 1 n  X  P i ( 1 rank in which the 1 rank first position; 0.5 if it firstly occurs in the second position ; 0.33 for the third, 0.25 for the fourth, 0.2 for the fifth and 0 if none of the first five answers is correct. Details of the ex-periment will be at http://www.cs.uwaterloo.ca/ ~ mli/QA-experiment. The performance of our system compared with ARANEA is listed in Table 2. Below are some case studies.
Example. Both d min and d max are implemented. As we have explained d min tends to favor the more popular an-swers, and d max tends to favor less popular answers (while they usually agree on problems with unique answers). For example, for question  X  X hich city is Lake Washington by? X  (Question 1536), d max  X  X  answer is Bellevue, a correct answer, whereas d min  X  X  answer is Seattle, a correct and a more pop-ular answer! For another example, for the question  X  X hen was CERN founded? X , d max  X  X  answer is X 52 years ago X , a cor-rect answer in 2006, whereas d min  X  X  answer is  X 1954 X , more accurate.

Example. Consider the example of  X  X hen was the tele-graph invented? X  (Question 1400) google.com gives a stan-dard answer from pubquizhelp website: 1838, crediting W.F. Cooke and Charles Wheatstone for their invention in 1838. Our answer is 1837 crediting Samuel Morse for the same invention. Interestingly, a slightly lower ranked answer i s 1774, referring to the time an impractical device of telegra ph was first built. This case study tells us that human anno-tated websites often contain human biases, whether they are correct or not. A QA system can sometimes provide more popular answers than wikipedia. Politically speaking, the QA systems may be more  X  X emocratic X .
 Example. To the question  X  X hat does KDD stand for? X , QUANTA answers:  X  X nowledge discovery in databases X .
We have further developed the theory of information dis-tance and solved several theoretical problems. We have pro-vided a framework so that such a theory can be fruitfully ap-plied to the QA systems. We have actually built a QUANTA system based on our new theory and demonstrated the po-tential and versatility of our system. Now we must answer some unsettling questions:
The experiments performed in this paper are severely hand-icaped by the fact that we do not have an indexed database of the internet (which is over 10 billion pages). Having to de -pend on commercial search engines 1 implies that we cannot do approximate matches and alignments effectively. Further study of this theory (for example, comparing the theory with
Note: we have not used any commercial search engine X  X  abilities of answering questions, often listed as the first i tem in the reply. all the other metrics listed in [39] in practice) will requir e sig-nificantly larger experiments and without such limitations . Despite of such limitations, we hope we have presented a novel theory of information distance accompanied by a vi-able system. We thank Paul Vit  X anyi for pointing out an error, Qiang Yang for pointers on data mining literature, and the KDD referees for their helpful comments. [1] C. An  X e and M.J. Sanderson, Missing the Forest for the [2] T. Arbuchle, A. Balaban, D.K. Peters, M. Lawford, [3] C.H. Bennett, P. Gacs, M. Li, P. Vitanyi, and W. [4] C.H. Bennett, M. Li and B. Ma, Chain letters and [5] D. Benedetto, E. Caglioti, V. Loreto, Language trees [6] C.C. Chang and C.J. Lin, LIBSVM: a library for [7] P. Cimiano, S. Staab, Learning by googling, ACM [8] X. Chen, B. Francia, M. Li, B. Mckinnon, A. Seker. [9] A.V. Chernov, An.A. Muchnik, A.E. Romashchenko, [10] R. Cilibrasi, P.M.B. Vit  X anyi, and R. de Wolf [11] R. Cilibrasi and P.M.B. Vit  X anyi, The Google [12] R. Cilibrasi and P.M.B. Vit  X anyi, Clustering by [13] C. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo, T.R. [14] M. Cuturi and J.P. Vert, The context-tree kernel for [15] K. Emanuel, S. Ravela, E. Vivant, C. Risi, A [16] J.R. Finkel, T. Grenager and C. Manning, [17] R. Fagin and L. Stockmeyer, Relaxing the triangle [18] E.J. Keogh, S. Lonardi, C.A. Ratanamahatana, [19] S.R. Kirk and S. Jenkins, Information theory-baed [20] A. Kraskov, H. St  X  ogbauer, R.G. Andrzejak, and P. [21] A. Kocsor, A. Kertesz-Farkas, L. Kajan, S. Pongor, [22] N. Krasnogor, D.A. Pelta, Measuring the similarity of [23] M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney, H. [24] M. Li, X. Chen, X. Li, B. Ma, P. Vit  X anyi, The [25] M. Li and P. Vit  X anyi, An introduction to Kolmogorov [26] J. Lin and B. Katz, Question answering from the web [27] J. Lin and B. Katz, Building a reusable test collection [28] J. Lin. The web as a resource for question answering: [29] An.A. Muchnik, Conditional comlexity and codes, [30] An.A. Muchnik and N.K. Vereshchagin, Logical [31] H.H. Otu and K. Sayood, Bioinformatics 19:6(2003), [32] H.K. Pao and J. Case, Computing entropy for [33] D. Parry, Use of Kolmogorov distance identification of [34] L. Ramshaw and M. Marcus, Text chunking using [35] C.C. Santos, J. Bernardes, P.M.B. Vit  X anyi, L. [36] A.K. Shen and N.K. Vereshchagin, Logical operations [37] A. Siebes, Z. Struzik, Mining using patterns, in: [38] W. Taha, S. Crosby, and K. Swadi, A new approach to [39] P.N. Tan, V. Kumar, and J. Srivastava, Selecting the [40] Y. Tsuruoka and J. Tsujii, Bidirectional inference [41] R.C. Veltkamp, Shape Matching: Similarity Measures [42] N.K. Vereshchagin and M.V. V X  X ugin, Independent [43] M.V. V X  X ugin, Information distance and conditional
