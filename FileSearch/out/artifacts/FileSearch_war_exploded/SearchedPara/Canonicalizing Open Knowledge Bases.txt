 Open information extraction approaches have led to the cre-ation of large knowledge bases from the Web. The problem with such methods is that their entities and relations are not canonicalized, leading to redundant and ambiguous facts. For example, they may store  X  Barack Obama, was born in, Honolulu  X  and  X  Obama, place of birth, Honolulu  X  . In this paper, we present an approach based on machine learning methods that can canonicalize such Open IE triples, by clus-tering synonymous names and phrases.

We also provide a detailed discussion about the different signals, features and design choices that influence the quality of synonym resolution for noun phrases in Open IE KBs, thus shedding light on the middle ground between  X  X pen X  and  X  X losed X  information extraction systems.
Recent advances in information extraction (IE) have led to the creation of large structured knowledge bases (KBs), such as NELL [5], YAGO [29], Freebase [4], DBpedia [1], Knowledge Vault [7], ReVerb [10], etc. These knowledge bases contain millions of entities (such as people, organi-zations, locations, or movies), and hundreds of millions of facts about them (such as which actor acted in which movie, which city is located in which country, etc.). This infor-mation is usually stored in the form of  X  subject, predicate, object  X  triples  X  a format known as RDF.

Techniques based on  X  X pen IE X  [3, 9], such as ReVerb [10], allow the subjects and objects to be arbitrary noun phrases, and the predicates to be arbitrary verb phrases. For exam-ple, we may extract  X  D.C., is capital of, United States  X  and  X  Washington, capital city of, U.S.  X  . However, it is not clear if these are talking about the same entities, or even about the same predicate. This means that when we query the KB for facts about an entity by one name, we cannot be sure that we get all facts about the entity. Conversely, we cannot be sure that all the facts we get are about the same entity.
At the other extreme, techniques based on  X  X losed IE X  re-quire that the subjects, predicates and objects are canonical, i.e., that they have unique ids. This is the approach taken by many KBs, such as YAGO, Freebase, DBpedia, and Knowl-edge Vault. The disadvantage of this approach is that the coverage is much lower than with Open IE. There are also systems such as NELL, in which the predicates are from a closed class, but the entity names are open.

In this paper, we present an approach that can take a large  X  X pen X  KB, such as produced by ReVerb or NELL, and con-vert it into a canonicalized form, where entity and relation names are mapped to canonical clusters. More precisely, our contributions are as follows:  X  We show how standard clustering techniques, with sim- X  We investigate an interesting interaction between the  X  We show how to extend the AMIE approach [12], which
Open IE systems extract triples of the form  X  subject, predi-cate, object  X  from natural language text. For example, given the sentence  X  X cCain fought hard against Obama, but fi-nally lost the election X , an Open IE system will extract two triples,  X  McCain, fought against, Obama  X  and  X  McCain, lost, the election  X  . Early systems [3, 31] typically restricted the subject and object to noun phrases. These can be named entities, such as Obama , but also common noun phrases, such as the election . The predicate can be any sequence of words that appear between the two arguments. This basic approach can harvest a huge number of triples from Web corpora. However, it will also extract uninformative triples, such as  X  Hamas, claimed, responsibility  X  (where it is not clear for what Hamas claimed responsibility).

The ReVerb approach [10] restricted the predicates to lightweight verbal phrases, which greatly improved the pre-cision of the triples. The Ollie approach [28] relaxed this restriction by expanding the syntactic scope of relation phrases to cover a much larger number of relation expres-sions, and by adding context information such as attribu-tion and clausal modifiers. Some approaches use dependency parsing [31], or employ hand-crafted patterns on the parse trees. ClausIE [6] can reason on the dependency trees, and thus extract more robust triples.

All of these approaches have in common that their rela-tionships are not canonicalized. The approaches cannot see that was born in and  X  X  birth place is denote the same seman-tic relationship. The approach used in the NELL project [5], in contrast, works with a predefined set of relationships. It will extract triples of the form  X  NP, relation, NP  X  , where the NP are noun phrases and relation is one of the prede-fined relation names. Thus, NELL is not strictly speaking an Open IE system. Still, it shares many properties of Open IE systems. Most notably, all of the Open IE approaches, and NELL, cannot extract canonical entities. They cannot see that Barack Obama and President Obama are two names for the same person. We discuss some solutions to this below.
One approach to resolving entity names is to try to map them to an existing list of known entities, such as Wikipedia or Freebase. This is known as entity linkage (or  X  X ikifica-tion X  if the target KB is Wikipedia). Typically each mention generates a candidate list of entities (based on string match-ing), and then this list is re-ranked using a machine learned model. There are several kinds of models: some link each entity mention in isolation using local features (e.g., [15]), some jointly link sets of mentions within a page using local features and global context (e.g., [26]), and some jointly link mentions across sets of pages (e.g., [19]).

One problem with the above approaches is that many pages may refer to new entities that are not already in a KB. This is particularly true for tail entities (i.e., ones that are not popular enough to have a Wikipedia entry), and/or for new or emerging entities. The standard approach to this is to allow each mention to either map to an entity on the shortlist, or to a special NIL or OOKB (out-of-KB) entity (see e.g., [17]). However, that still leaves the issue of how to cluster these NIL values to create new entities.

The problem of clustering equivalent entities has been widely studied, and is closely related to the problems of cross-document entity resolution in the NLP community (see e.g., [2, 30]) and record linkage in the DB community (see e.g., [8]). Most methods use some variant of the following basic technique: define (or learn) a pairwise similarity func-tion for comparing two candidate entities, and then apply hierarchical agglomerative clustering (HAC) to find the men-tion clusters. For example, this approach was used in the Resolver system of [32] to cluster the entities derived from TextRunner X  X  Open IE triples. They defined the similarity of two entities in terms of their string similarity, as well as the similarity of their attribute values. We use a similar technique in our paper; see Section 3 for the details.
We compare our technique to Concept Resolver [18], a state-of-the-art system that clusters entity mentions on NELL data. Concept Resolver operates in two phases. The first phase performs disambiguation under the one-sense-per-category assumption. This assumption states that within a given NELL category, noun phrases are unambigu-ous. For example, Apple can be a company and a fruit, but there cannot be two companies called Apple (nor two fruits). We can infer the type of a particular noun phrase by the type signature of the accompanying verb. For instance, for the triple  X  Apple, hasCeo, Tim Cook  X  , the domain of hasCeo tells us that Apple refers to the company. This triple is then rewritten as  X  Apple:company, hasCeo, Tim Cook:person  X  . The second phase clusters all these type-augmented men-tions. For this, they use HAC with a machine learned simi-larity metric, similar to our approach.
There has been less work on clustering synonymous rela-tions than on clustering synonymous entities. The database community has studied schema alignment, but this usually relies on knowing the type signature of the relations, which are unavailable for Open IE triples.

The Resolver system [32] used HAC to cluster Open IE relations in TextRunner data. They used the set of sub-jects and objects associated with each relation to define a feature vector; they then constructed a generative model (known as the  X  X xtracted Shared Property X  model) to com-pute the probability that two relations are equivalent, based on counting the number of entity pairs that they had in com-mon. Finally they used this similarity metric inside HAC.
The disadvantage of this approach is that it defines the feature vector for a relation in terms of the raw string names for the entities, and these can be very ambiguous. For ex-ample, suppose the dataset contains the triples  X  Indian Air-lines, is headquartered in, Mumbai  X  and  X  Indian Airlines, has headquarters in, Bombay  X  . We cannot determine that is headquartered in is equivalent to has headquarters in unless we know that Mumbai is equivalent to Bombay .

One solution to this is to jointly cluster the entities and relations at the same time; this has been called  X  X nowledge graph identification X  [25]. In this paper, we adopt a sim-pler two-stage approach. We first perform entity linkage on the triples, mapping the subject and object to a unique id (e.g., both Bombay and Mumbai map to the Freebase id /m/04vmp ). We then pass these partially-disambiguated triples to the AMIE rule mining system [13, 12], which can discover equivalences between synonymous relations. We then use these learned equivalences to create clusters of se-mantically equivalent relations. See Section 4 for the details.
The PATTY system [23] uses pattern mining techniques to find subsumption rules between syntactic patterns (e.g., daughter of &lt; child of ), extracted from a corpus. Like our approach, PATTY links the arguments of phrases to a KB (YAGO) to find subsumption rules. However, their goal is to construct a taxonomy of verbal phrases, whereas we are interested in finding equivalences between verbal phrases.
The WEBRE approach [22] can cluster verb phrases with close meaning in the presence of ambiguity. For instance, the verb phrase be part of holds different semantics in the sentences  X  X ew York is part of the United States X  ( location is part of location ) and  X  X un Microsystems is part of Oracle X  ( company is part of company ). WEBRE first disambiguates the relational concepts, producing a set of typed relations called type A relations (e.g. company is part of company ). Then, WEBRE performs synonym resolution on such con-cepts. For this purpose, WEBRE uses both the Open IE triples and the source text corpus to construct a hypernym graph, an entity similarity graph and verb phrase similar-ity graph. Such data structures are used to construct fea-tures for a clustering implementation based on HAC. Our approach also deals with ambiguous verbal phrases by en-forcing type constraints on the arguments of the equivalence mappings mined from the Open IE KB. However, unlike WEBRE, our methods rely solely on the Open IE triples.
Given an Open IE KB, our first goal is to canonicalize its noun phrases. For simplicity, we concentrate here on canonicalizing the subjects; the same approach can be used to canonicalize the objects. We note that the same string can have different meanings if it appears on two different pages. For example,  X  Obama, won, an award  X  can refer to Barack Obama or Michelle Obama, depending on where the triple was found. We assume, however, that the same sub-ject phrase on the same Web page will always refer to the same entity. For example, a news article that uses Obama to refer to the president may not use that word (without other qualification) to refer to his wife. This is a common assumption in linguistics [14].

With this in mind, we define a mention as a triple m = ( n,u, A ) where n is a subject noun phrase such as Barack Obama , u is the url of a Web document such as bbc.com where the mention was found, and A is the set of attributes about n that were extracted from u . Each attribute is a predicate-object pair such as  X  was born in, Honolulu  X  , or  X  won, an award  X  . Thus, a mention defines the profile of a noun phrase n in a particular Web source u .
Our goal is to partition the set of mentions, so that all mentions in one partition refer to the same real-world entity. This task can be seen as a clustering problem, where the real number of clusters, i.e., the number of different entities in the data, is unknown. To solve this problem, we use Hierarchical Agglomerative Clustering (HAC) on the set of mentions built from the Open IE triples.

In its general formulation, HAC has O N 3 time com-plexity, where N is the number of mentions. This makes it inadequate for large datasets. To alleviate this fact, we used token blocking [24], a method that resembles the canopies method introduced in [21]. This method first assigns each mention to one or several groups, called canopies . One stan-dard approach is to assign the noun phrases to canopies based on the words that the noun phrases contain. For example, a noun phrase President Obama will be assigned to the canopy for President and to the canopy for Obama . Then, standard HAC is applied within each canopy. This partitions each canopy into a set of clusters. Finally, the clusters that live in different canopies but share a mention are merged. In the example, the cluster { Barack Obama , President Obama } from the Obama canopy will be merged with the cluster { President Obama , US President } from the President canopy. This technique reduces the number of pairwise comparisons required by the standard HAC imple-mentation. Furthermore, it facilitates parallelism, because each canopy can run HAC on a smaller subset of data.
Canopies can drastically affect the recall of the clustering algorithm: If two mentions refer to the same real-world en-tity, but are assigned to different canopies, then they might never end up in the same cluster. For example, Mumbai and Bombay will go to different canopies, and unless there is a mention that connects them, they will remain in different clusters. Thus, we face a trade-off, where assigning a men-tion to a small number of canopies will improve efficiency (decrease the runtime), but hurt recall and precision.
We propose the following solution to this problem, which we call  X  X bject canopy expansion X : We assign two men-tions m 1 =  X  n 1 ,w 1 , A 1  X  and m 2 =  X  n 2 ,w 2 , A 2 canopy, if (1) n 1 and n 2 share a word that is not a stop-word, or if (2) there exist two objects o 1  X   X  object ( A  X  object ( A 2 ) that share a word. In this way, we can merge Mumbai and Bombay , if, e.g., they both appear in triples  X  Mumbai, is located in, the Republic of India  X  and  X  Bombay, is a city in, India  X  .
HAC requires a similarity function on the mentions. In this paper, we study different similarity functions, with the goal of determining which ones work best under which con-ditions. Many of the functions use the Jaccard coefficient: Given two mentions m = ( n,u, A ) and m 0 = ( n 0 ,u 0 , A define the following similarity functions (called features ): Attribute Overlap. The attribute overlap is the Jaccard coefficient of the set of attributes: Two attributes ( p,o )  X  A , ( p 0 ,o 0 )  X  A 0 are equal if p = p and o = o 0 .
 String Similarity. We use the Jaro-Winkler similarity be-tween n and n 0 as a feature String Identity. As a special case of the string similarity (and as a baseline), we consider the string identity: IDF Token Overlap. If two noun phrases share a word, they are more likely to be similar, but not if many other mentions share that word. For example, the fact that Rhine River and Ruhr River share the word River is not very sig-nificant, because this word also appears in a plethora of other river names. Therefore, we introduce a weighted word overlap, in which a word is given more importance if it ap-pears in fewer mentions. We follow the standard Inverse Document Frequency (IDF) approach: Here, w (  X  ) is the set of words of a string, excluding stop words. df ( w ) is the frequency of the word in the collection of all words that appear in the subjects and the objects of the OpenIE triples.
 Word Overlap. If two Web pages share many words, then this can be an indication that two mentions on these pages refer to the same entity. We define where t (  X  ) is the set of the top 100 words on a page, ranked in terms of TF-IDF [20]. Entity Overlap. Since words can be ambiguous, we also experimented with an overlap of entities. We applied a stan-dard entity linkage algorithm [11] on the pages, and identi-fied the set e ( u ) of linked Freebase entities on the page u . Then we define Type Overlap. Shared types can be a signal for merging two mentions. We used the results of the type induction algorithm [27] on the verbal phrases of the attributes (as provided by the authors of [27]), and define f tol ( m,m 0 ) = jaccard ( types (  X  pred ( A )) , types (  X 
So far, we have described how to compute the similarity between two mentions. The similarity between two clusters is calculated using the single linkage criterion [16], that is, the maximum of the intercluster pairwise similarities. Our experience suggests that the complete linkage criterion (the policy that uses the minimum intercluster similarity) and even the average linkage criteria are too conservative and therefore lead to low clustering recall.
In addition to the individual features, we also study a combined feature, which is a logistic function: Here, f sim ( m,m 0 ) is a linear combination of features: The f 1 ,...,f N are the similarity functions discussed in Sec-tion 3.3. Since the word overlap, the entity overlap, and the type overlap require a lot of preprocessing, we also study a simple combined feature that does not make use of these advanced measures, called f sml . In both cases, the weights c are determined by training a logistic regression classifier.
To train the combined similarity function, we need labeled training data, i.e., a set of mention pairs that are labeled as being equal or unequal . Such training data can be ob-tained, for instance, from approaches that perform entity disambiguation (Section 2.2), i.e., that map Open IE triple subjects to entities in a KB. In our case (i.e., in the case of ReVerb triples), half of the triples already have their sub-jects mapped to Freebase entities [11]. We use these map-pings for training, so that we can partition the remaining, yet-unmapped, mentions into synonymous clusters.

To learn robust weights for our features, we have to make sure that our training set contains hard cases, where the same entity appears with different names, because other-wise we will learn to put too much weight on just the string similarity. To do this, we randomly sample 200 Freebase entities that appear with different names in our triples. For example, our set of entities contains the Greek poet Homer, because he appears as Homer and as Homerus in our triples. Every one of these names, in turn, can refer to several en-tities (homonyms). For example, Homer does not just refer to the Greek poet, but also to Homer Simpson, the cartoon character from  X  X he Simpsons X . Hence, we add to our set also Homer Simpson, and all names that refer to Homer Simpson. This results in 436 entities with 714 names in to-tal. Next we collect all triples that contain one of these 714 names (in mapped form), and construct their mentions (us-ing the provenance information in the Reverb triples). This results in 43K mentions. From these mentions, we can con-struct a training set of pairs of mentions that are labeled as equal or unequal . We construct a set of 1137 pairs, balanced between equal and unequal pairs. We also make sure that each entity contributes with at least two examples, because standard random sampling would penalize entities with few mentions. This set of pairs is then used to train our weighted similarity function. In Section 5.1, we compare this learned similarity function to the baseline approaches.
Given a cluster of synonym mentions m = ( n,w, A ), the canonicalization consists of selecting a representative noun phrase  X n that will replace the other noun phrases in the canonicalized KB. We propose a simple approach that selects the noun phrase  X n with the highest number of different Web sources u . In case of a tie, an alternative is to select the longest noun phrase.
In this section, we describe our approach for clustering verbal phrases that have equivalent meaning. The basic idea is to learn rules that predict when one phrase is semantically equivalent to another, and then to perform clustering to en-force transitivity of this relation.
Our approach to verbal phrase clustering requires that the subjects and objects of the Open IE triples are already clustered. There are two ways to achieve this. Either we can use our noun phrase clustering algorithm of Section 3, or we can make use of the existing mappings to Freebase. We consider both approaches. In particular, for the latter case, we take a subset of all the Reverb triples where the subject was already mapped to Freebase (as provided in the data in [11]), and where we can unambiguously map the object to Freebase using a simple string matching technique.

With either of these techniques, we obtain a  X  X emi-canonicalized X  KB, where the subjects and objects are fully canonicalized, and the predicates are still uncanonicalized verbal phrases (the  X  X ual X  of NELL). This KB may contain, e.g.,  X  Barack Obama, was born in, Honolulu  X  and  X  Barack Obama,  X  X  birthplace is, Honolulu  X  .
Suppose we have the two Open IE relations r = was born in and r 0 =  X  X  birthplace is . We would like to discover that these are equivalent, i.e., that r &lt; r 0 and r 0 &lt; r &lt; r 0 means that r 0 subsumes (is more general than) r , i.e.,  X  x,y : r ( x,y )  X  r 0 ( x,y ). Unfortunately not all triples with r will necessarily appear with r 0 . Conversely, relations that are very sparse may occur with the same subject and object by chance, even though they are not equivalent. For exam-ple, if we find  X  Woody Allen, married, Soon-Yi Previn  X  and  X  Woody Allen,  X  X  stepdaughter, Soon-Yi Previn  X  , we should not deduce  X  X  stepdaughter &lt; married , even if all triples with the former verbal phrase appear with the latter. Therefore, we resort to a soft approach for subsumption detection that is based on statistical rule mining.
We decided to use the AMIE algorithm [13, 12], which can learn Horn rules such as the following: The left-hand-side of such a rule is called the body , and we abbreviate it by ~ B . All variables are implicitly universally quantified, so that Horn rules are a subset of first order logic statements. A relation subsumption r &lt; r 0 can be expressed as a Horn rule of the form r ( x,y )  X  r 0 ( x,y ). Learning such rules is called inductive logic programming. This requires positive and negative examples. One could take all facts that are absent in the KB as negative examples. However, this violates the Open World Assumption made by KBs. Instead, AMIE makes a more refined assumption called the Partial Completeness Assumption (PCA), which assumes that if we know any facts of relation r for a given entity x , then we know all facts of this relation for x , so missing edges are false, but if we do not know any facts of type r for entity x , then we do not assume missing edges are false. (In [7], PCA is called the  X  X ocal closed world as-sumption X .) For instance if a KB knows the nationality of a person and a rule predicts a different nationality, this is assumed as a negative example by the PCA. However, if no nationality is known for the person, PCA does not use this absence of information as counter-evidence.

For AMIE, the support of a rule is the number of positive examples covered by the rule: supp ( ~ B  X  r ( x,y )) := #( x,y ) :  X  z 1 ,...,z m : ~ B  X  r ( x,y ) Here, z 1 ,...,z m are the free variables of the body, and #( x,y ) : A is the number of pairs ( x,y ) that fulfill A . Based on the PCA, AMIE defines the following confidence measure: pcaconf ( ~ B  X  r ( x,y )) := supp (
The PCA confidence normalizes the support of the rule over the positive examples and the facts assumed as false according to the PCA, thus taking into account that KBs can be incomplete. Although the PCA does not hold for all relations, [13] shows that it leads to high precision and recall for rule mining.

We apply AMIE to our semi-canonicalized KB to mine subsumption rules of the form r ( x,y )  X  r 0 ( x,y ). As suggested in [12], we infer equivalence rules of the form r ( x,y )  X  r 0 ( x,y ) by merging subsumptions in both direc-tions. We score each subsumption with the PCA confidence, and score an equivalence with the minimum of the two sub-sumption scores. The output of the AMIE system is a set of equivalent verb phrases like be-named-after(x, y)  X  take-its-name-from(x, y)
The rule mining has given us a set of weighted equivalence relations between verbal phrases. Since the equivalence rela-tion is transitive, we iteratively merge equivalence mappings with at least one verbal phrase in common. For instance, given the equivalences stand-for(x, y)  X  be-an-acronym-for(x, y) be-short-for(x, y)  X  be-an-acronym-for(x, y) refer-to(x, y)  X  be-short-for(x, y) we merge the relations stand-for , be-an-acronym-for , short-for and refer-to into a single cluster.
We propose to canonicalize clusters of verbal phrases by mapping them to Freebase relations. To achieve this, we resort to the ROSA approach [12]. First, we restrict our set of triples to those that have a subject and an object linked to Freebase. Then we join this set with Freebase, so that we obtain a set with facts of the form vp ( x,y ) and fr ( x,y ). Here, x and y are Freebase entities, vp is a ver-bal phrase and fr is a Freebase relation. Then, we run AMIE on our coalesced KB in order to mine cross-ontology equivalence rules of the form vp ( x,y )  X  fr ( x,y ). The rule bought ( y,x )  X  acquiring company ( x,y ) is an example. For each cluster of Reverb verb phrases, we collected all the Free-base relations implied by the verbal phrases in these equiva-lences. In Section 5.2, we show that in some cases it is possi-ble to map clusters unambiguously to Freebase. If a cluster cannot be mapped to Freebase (e.g., because it represents a novel relation that is not part of the Freebase schema), a representative for the cluster can be chosen by selecting either the verb phrase that appears in most equivalences, or the phrase with the largest number of triples.
We conducted two groups of experiments, one to evaluate different entity clustering features, and one to evaluate the relation clustering.
To evaluate the quality of a clustering of mentions, we assume each mention m can be mapped to a correspond-ing entity e ( m ) from Freebase according to a gold standard clustering E . Each cluster e  X  E contains all mentions that map to the same entity. Given this, we can measure preci-sion and recall of the clustering in 3 different ways, which we call macro analysis, micro analysis and pairwise analysis.
We will explain these metrics using the example in Fig-ure 1, which illustrates a set of | M | = 7 mentions distributed across | C | = 3 clusters. There are | E | = 3 Freebase entities, all called  X  X omer X : the Greek poet, the cartoon character Homer Simpson, and the author Homer Hickam. (Each en-tity also has other aliases.) Macro-analysis. We define the macro precision of the clus-tering as the fraction of clusters that are pure, i.e., where all the mentions in the cluster are linked to the same entity: Figure 1: An example of a clustering of N=7 men-tions and a gold standard with 3 Freebase entities, labeled in parentheses below the mentions. Macro recall is calculated by swapping the roles of the ground truth and the resulting clustering, i.e., recall macro ( C,E ) = precision macro ( E,C ). This corre-sponds to the fraction of Freebase entities that get assigned to a unique cluster. In Figure 1, we can see that clusters A and C are pure as they do not mix mentions of different entities. Hence precision macro = 2 / 3 . Conversely, the car-toon character and the author are pure entities because they occur only in one cluster, therefore recall macro = 2 / 3 Micro analysis. Micro precision is defined as the pu-rity [20] of the resulting clustering. This is computed by assuming that the most frequent Freebase entity of the men-tions in a cluster is the correct entity. That is, we compute where N is the number of mentions in the input. Macro recall is symmetrically defined as recall micro ( C,E ) = precision micro ( E,C ). For example, in Figure 1, the most frequent entity for clusters A and B is the poet (2 mentions in each cluster) and for C is the author (2 mentions), so the micro precision is 6 / 7 . Analogously recall micro because the highest frequencies in a cluster for the entities are: 2 times for the poet, 2 times for the author, and 1 time for the cartoon character.
 Pairwise analysis. In the pairwise evaluation, we measure the precision and recall of individual pairwise merging de-cisions. To be more precise, let us say that two mentions from the same cluster produce a hit if they refer to the same Freebase entity. We define the pairwise precision as # pairs c = | c | X  ( | c | X  1) / 2 is the total number of mention pairs in a cluster. Likewise, we define recall as In Figure 1, clusters A and C produce 1 hit out of 1 pairwise decision, whereas cluster B produces 1 hit out of 3 pairwise decisions. Hence the pairwise precision is 3 5 . To compute recall pairwise , we calculate the total number of pairwise de-cisions in the gold standard E : # pairs poet + # pairs author
In all cases, the F1 measure is defined as the harmonic mean of precision and recall.
ReVerb 1 [10] is a state-of-the-art Open IE system that was run on the Clueweb09 corpus 2 . It produced 3M triples. Half of these triples have their subject linked to Freebase, as in [11]. To evaluate the different clustering features, we built a gold standard as follows: We sampled 150 Freebase entities that appear with at least 2 names in our dataset, and col-lected all their mentions in our triples. This resulted in 8.5K mentions. We call this dataset Base . We then enrich this dataset with all the mentions of homonym entities, as in Sec-tion 3.4. We name this dataset Ambiguous . This results in 446 Freebase entities and 34K mentions. For both datasets, http://OpenIE.cs.washington.edu/ http://www.lemurproject.org/clueweb09.php/ we constructed a gold standard clustering by grouping those mentions that are linked to the same Freebase entity. Results on Base . We ran our implementation of HAC on the 8.5K mentions (9.1K extractions) in the Base dataset. On a Intel Core i7 (8 logical cores, 2.40 GHz) with 16 GB of RAM, our implementation (using the full ML similarity function, plus expanded canopies) created 157 clusters in 54.3 seconds (averaged across 3 runs). Our memory foot-print reaches a peek of 5.7GB.

Next, we assess the quality of the different similarity fea-tures introduced in Section 3.2. We show the results in Table 1, using a confidence threshold that was chosen to maximize the F1 score. Our first observation is that all the features deliver very good precision. This means that they rarely produce a cluster than contains two different entities. Thus, the features behave rather conservatively.

Let us now look at recall. The macro-recall is very low in general, because as soon as one entity appears in more than one cluster, the metrics decreases (even if all other men-tions of the entity are clustered correctly). Let us therefore look at the micro-recall and the pairwise-recall. All features perform decently. This is mainly because all features can build on the pre-clustering that the canopies already estab-lished, i.e., every feature is applied only to pairs of mentions with overlapping words. When comparing the recall of the features, the attribute overlap stands out with lowest recall. This is because our triples are very sparse: It rarely happens that two different mentions of the same entity share many attributes. The type overlap, in contrast, works very well: The fact that two mentions with similar names share a type is a strong signal that they refer to the same entity. The word overlap performs well for a similar reason. Among the features that do not require preprocessing, the IDF token overlap is a clear winner.
 Rather surprisingly, the combined features (Full ML and Simple ML) are not the best performing methods. We thought that this would be because of the canopies, which make the resulting data distribution at test time quite dif-ferent from what was used to train the model. To assess this conjecture, we also ran HAC without canopies on this dataset (Table 2). We can observe that the string identity, the IDF tokens overlap, and the attributes overlap are insen-sitive to the canopies policy. The other features, in contrast, perform much worse without canopies. This suggests that they provide little or no extra evidence of synonymy. They are noisy signals that mainly mislead the ML methods. This, in turn, explains the performance of the ML methods with and without canopies.

Table 3 lists some examples of pure entity clusters that the Simple ML feature could find. We can for instance clus-ter the mentions  X  X hoenix Arizona X  and  X  X hoenix X  using as signals the tokens overlap and common attributes such as  X  located in, Arizona  X  . Moreover we can avoid mixing these mentions with the mythological creature of the same name. On the other hand, the sparseness of the attribute overlap still leads to losses in recall. For example, we could not clus-ter the mentions  X  X eijing National Stadium X  and  X  X ational Stadium X  together, even though they are the same entity. Results on Ambiguous . The Ambiguous dataset consists of 34K mentions and 37K triples. With the same setup as for Base , our implementation produces 823 clusters in 15.045 minutes on average with a peek memory footprint of 6.5GB. The results are shown in Table 4. Not surprisingly, preci-sion is lower on this dataset. This is mainly caused by the single linkage criterion for clusters. Under this policy, the similarity of a pair of clusters is determined by the highest intercluster similarity score. While this aggressive strategy was shown to improve recall significantly, it also propagates errors more easily. Furthermore, this phenomenon is am-plified by the reclustering phase and the ambiguity of the test set. To see this, consider a set of mentions with labels Barack Obama , Michelle Obama , and Obama , the latter re-ferring ambiguously to both Barack and Michelle Obama. A single clustering error on the ambiguous mentions ( Obama ) will move the three entities in the same cluster, and thus decrease precision. Conversely, the baseline approach is less sensitive to ambiguity in this particular case because the pre-cision will only penalize the merge of the ambiguous mention Obama , as Barack Obama and Michelle Obama will be never clustered together. Hence, all features produce lower preci-sion. The attribute overlap has highest precision  X  but this is mainly because it is a very sparse feature that hesitates to merge mentions. Let us therefore look at the micro-F1 and the pairwise-F1. We see that the IDF token overlap is the strongest feature, even in presence of ambiguity. It is fol-lowed by the combined features. For comparison, the table also shows the Simple ML without the  X  X bject canopy ex-Table 3: Some examples of successfull and unsuc-cessful synonym identification pansion X  (Section 3.2). We see that the expansion increases recall slightly, and has a negligible effect on precision. There-fore, we conclude that the technique is indeed useful. Table 3 shows some examples of impure clusters.
 Lessons Learned. Our study of features shows that, among the more advanced features, the type overlap and the word overlap produce significant leverage if combined with canopies  X  both on the random dataset and on the am-biguous one. They are closely followed by the IDF token overlap, which stands out as the strongest simple feature, with and without canopies. The combined features also per-form decently. All in all, the best features can cluster en-tities in the general case with nearly 100% precision, and a pairwise-recall of more than 98%. The NELL project [5] also extracts triples from the ClueWeb09 corpus. The Concept Resolver approach [18] aims to cluster the entities of these triples. Concept Re-solver operates under the one-sense-per-category assump-tion, which states that within one category, names are un-ambiguous. Once the type for a noun phrase has been ex-tracted, Concept Resolver collects all the type compatible triples about the noun phrase and builds a  X  X ense X  for that NP, the equivalent of what we call a  X  X ention X . The authors of [18] provided us with the data and the training examples used to evaluate Concept Resolver. As described in Sec-tion 3.4, we used the triples to construct mentions.
The challenge is to cluster together several names that re-fer to the same entity. Since not all the triples contained the source from which they were extracted, we defined a default source for those without provenance. We also restricted our dataset to the categories considered in the evaluation of Con-cept Resolver. The resulting dataset consists of 18K triples and 20K mentions. Gold Standard. Concept Resolver comes with a manu-ally compiled set of noun phrases that should be clustered together as one entity. We estimated our precision by sam-pling a random subset of 100 clusters from the output and checking them manually. For Concept Resolver we report the precision value from [18], averaged across all categories and weighted by the number of senses per category.

We note that our notion of precision is not fully compara-ble to theirs: the one-sense-per-category assumption merges all mentions of the same type into one mention  X  no matter if the triples come from different sources. This can cause problems. For example, Obama:Person is assumed to map to only one entity, whereas we allow it to map to multi-ple entities, depending on which page the mention was seen. Moreover, Concept Resolver removes singleton clusters from the evaluation of precision. For us, in contrast, such a sin-gleton cluster can correspond to several successfully merged mentions. Therefore, we restricted our manual evaluation to non-singleton clusters according to Concept Resolver and used the IDF token overlap as baseline. Concept Resolver does not report values for the macro dimension, we show results for the micro and pairwise evaluation.
 Results. We ran our synonym resolution machinery on the NELL triples and computed precision and recall. The results are shown in Table 5. The Simple ML feature can achieve decent precision and recall, albeit inferior to the values of Concept Resolver. We believe this is because the logistic regression model implemented by Concept Resolver lever-ages information from the ontology, which is not possible in an Open IE scenario. Whereas our Simple ML uses a Jaccard score on the attributes of mentions as signal for synonymy, Concept Resolver also builds features from the properties of the relations. For instance, their approach will penalize the similarity for the mentions Auburn and Auburn Hills as they have different values for the functional predi-cate cityLocatedInState (the first located in Maine and the second in Michigan) even if they have other attributes in common (e.g. both located in USA). Additionally, Concept Resolver makes use of inverse and quasi-inverse functions as they can identify mentions uniquely. The fact that the mentions Mumbai and Bombay share the inverse functional predicate  X  cityCapitalofState, Maharastra  X  is used as strong evidence for synonymy. Still, our Simple ML and the IDF token overlap deliver reasonable results even without this additional information.
 Lessons Learned. We can see here that the relation schema that Concept Solver uses improves the entity clus-tering results. If such data is not available, a synonym res-olution approach based on Open IE attributes and string similarity features can deliver reasonable results. Even a simple baseline such as the IDF token overlap should not be outright discarded for the task of synonym resolution.
Since the relations of NELL are already canonicalized, we run the relation clustering only on the ReVerb set of triples. As discussed in Section 4.1, the relation clustering requires a semi-canonicalized KB, in which the subjects and objects have been canonicalized. There are two ways to achieve this: either by our entity clustering ( X  X lustered KB X ) or by a mapping to Freebase ( X  X inked KB X ). The Clustered KB was constructed by sampling 25K freebase ids from the Linked KB and gathering all their triples. This results in 600K triples. Both the Clustered and the Linked KB are given as input to the AMIE system in order to mine relation equivalences. We ran AMIE using a support threshold of 5, meaning that two verbal phrases must have at least 5 pairs in common. This also implies that relations with less than 5 cases are discarded. The Linked KB has 33215 of such relations, the Clustered KB has 17259. [22] suggests that approximately 22% of the Reverb phrases are polysemous. The phrase belongs-to , e.g., con-veys different meanings in the sentences  X  X he Wii belongs to Nintendo X  ( invention created by organization ) and  X  X al-lorca belongs to Spain X  ( island belongs to country ). Poly-semy hurts precision, since phrases with unrelated meanings Precision
Verb phrases Freebase relation be an abbreviation-for, be known as, stand for, be an acronym for -be bought, acquire organization.organization.acquired_by will be transitively clustered as synonyms due to the poly-semic relations. To alleviate this effect, we also run AMIE on triples from Linked KB where the entities are augmented with types. This option makes AMIE enforce type con-straints on the arguments of the equivalence mappings when possible. Thus, a single verbal phrase can generate multiple specific relations that differ only in their signatures. Since the Freebase ontology has approximately 23K different data types, we allowed type enhacement only with the most com-mon types, i.e., person, organization, location, and string.
Since evaluating recall would require identifying all the relations in the set of 1.3M triples, we focus on measures of precision. As before, we can measure the precision of a relation cluster at the macro, micro or pairwise level. The pairwise precision measures the quality of a set of clusters as the ratio of correct pairwise merges. A pairwise merge is counted as correct if the corresponding verbal phrases mean the same, or if one of them is slightly more general than the other. For instance, the phrases be-spoken-in and be-the-official-language-of count as a correct merge. The micro-precision assumes the most frequent meaning in a cluster as ground truth. Hence, it is calculated by adding up the fre-quency of the most common meaning in each of the clusters and dividing this number by the total number of phrases. Conversely, the macro-precision is the ratio of pure clusters, i.e., the proportion of clusters where all the phrases belong to the same meaning. For micro and macro precision, phrases were labeled with the most general meaning.
On the Clustered KB, AMIE mined 3.5K equivalence mappings, whereas mining the Linked KB produced 4.3K equivalence rules. When the type enhancement is en-abled, the number rises to 22K mappings. For example, we find use-truck-for  X  use-van-for with confidence 1.0 and support 19, or stand-for  X  be-an-acronym-for with confidence 0.88 and support 44. With the type enhance-ment, AMIE can discriminate between a country changing location, e.g.,  X  X srael moved to Mont Hor X , a person visit-ing a place, e.g.,  X  X arack Obama moved to Los Angeles X , and an organization changing location, e.g.,  X  X ostoria Glass moved to Moundsville X . This results in different equivalence rules for the phrase move-to such as moved-to( location  X  location )  X  located-in( location  X  location ) (support 8, confidence 0.5) and moved-to( person  X  location )  X  move-permanently-to( person  X  location ) (support 5, confidence 0.5). In these examples our coarse-grained type constraints are enough to avoid mixing phrases that denote permanent location (e.g. situated-in ) with phrases that denote place of residence (e.g. now-lives-in ).

The mappings have different confidence scores, and there-fore we tested the phrase clustering at two confidence thresh-olds: 0.8 and 0.5. Table 6 shows the results, together with the number of clusters and phrases. As we see, the preci-sion of our clusters is very good, meaning that we do not merge phrases that do not belong together. Naturally, a higher confidence threshold always leads to fewer phrases being clustered, and to fewer clusters. Our results are bet-ter on the cleaner Linked KB than on the Clustered KB. We can also observe the benefit of the type enhancement. In general, we can cluster only a very small portion of the verbal phrases. However, our clusters are non-trivial and contain an average of 4-5 phrases.
 Comparison to WEBRE. We compare our results to the WEBRE system [22]. We use the precision on the Linked KB with types because the type-enhanced phrases resemble the type A relations introduced by WEBRE. To be com-parable, we report a weighted micro-precision, where the correct assignments of phrases to clusters are weighted by the number of triples with the verbal phrase. We get a score of 0.981, which is slightly better than WEBRE X  X  score of 0.897. Nevertheless, this comparison must be taken with a grain of salt because the evaluation performed in WEBRE is somewhat different from ours (see Section 5 in [22]). First, their method to generate typed verbal phrases is different. Second, we could not have access to their gold standard for precision and recall. Third, the micro-precision formula of WEBRE uses are more granular definition of synonymy: a phrase can be a synonym of the true relation of a cluster (score 1), somehow related (score 0.5) or unrelated (score 0). Nevertheless, it is safe to say that our approach is not far off from the state-of-the-art in terms of precision. Mapping to Freebase. As described in Section 4.4, we used AMIE and ROSA rules [12] to find equivalences be-tween verbal phrases and Freebase relations. We ran AMIE on a combination of Freebase and Reverb with support threshold 5, producing 5.1K cross-ontology mappings. We then applied the same confidence thresholds as for relation clustering (0.5 and 0.8) and used the rules to map the clus-ters of verb phrases to Freebase. We counted clusters that were mapped to one Freebase relation or to two mutually in-verse Freebase relations as  X  X orrectly mapped X . For example, Freebase expresses the fact that Steven Spielberg directed the Titanic movie with a pair of mutually inverse relations,  X  Steven Spielberg, directed, Titanic  X  and  X  Titanic, directed by, Steven Spielberg  X  . The last column in Table 6 shows the proportion of triples whose relation could be mapped. We find a large number of very interesting mappings, some of which are shown in Table 7. Going manually through the mappings, we find an average precision of 88% for the Clustered KB with threshold 0.8.
 Lessons Learned. We conclude that rule mining can be used to cluster verbal phrases, and to map them to canonical Freebase relations. A cleaner KB and the type enhancement both help. This method can produce such clusterings and mappings only for a small portion of the verbal phrases, but it can do so at a high precision and for a significant percentage of the Reverb triples.
We have shown that it is possible, using fairly simple and standard machine learning techniques, to identify synonym mentions in a reasonable fraction of the triples coming from standard Open IE systems, such as Reverb and NELL. Our results suggest that, even with a certain level of ambiguity, the IDF token overlap is the strongest signal of synonymy for noun phrases on the Web, whereas more sophisticated fea-tures extracted from the sources are insufficient for this task on their own. We also provided useful and novel insights about the impact of canopies in the performance of Hier-archical Agglomerative Clustering, a standard technique for record linkage and identification of synonyms. The resulting clusters of entities and relations are semantically meaningful; some of them correspond to existing entities and predicates in Freebase, but others are novel extensions. We believe this hybrid approach  X  whereby we use high recall extractors, followed by clustering methods to improve the precision  X  shows great promise for bridging the gap between Open and Closed IE methods for knowledge base construction.
