 We compare two recently proposed frameworks for combin-ing generative and discriminative probabilistic classifiers and apply them to semi-supervised classification. In both cases we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of la-beled and unlabeled data. While prominent semi-supervised learning methods assume low density regions between classes or are subject to generative modeling assumptions, we con-jecture that hybrid generative/discriminative methods allow semi-supervised learning in the presence of strongly overlap-ping classes and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specific classification task of interest. We apply both hybrid approaches within naively structured Markov random field models and pro-vide a thorough empirical comparison with two well-known semi-supervised learning methods on six text classification tasks. A semi-supervised hybrid generative/discriminative method provides the best accuracy in 75% of the experi-ments, and the multi -conditional learning hybrid approach achieves the highest overall mean accuracy across all tasks. I.2 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Semi-supervised learning, hybrid generative/discriminative methods, text classification
Most machine learning methods rely on the availability of large labeled datasets. However, human annotation is time-consuming, making labeled data costly to obtain in practice. Motivated by this problem, researchers have proposed semi-supervised learning methods that leverage large amounts of relatively inexpensive unlabeled data along with small amounts of labeled data. The increasing interest in apply-ing machine learning to new domains and the vast availabil-ity of unlabled data from the web and elsewhere are driving interest in semi-supervised learning.

Semi-supervised learning is especially relevant for applica-tions in data mining, as when initially analyzing data from a new domain, obtaining any labeled data requires labori-ous human annotation. The lowest effort approach to data mining would use unsupervised learning. However, super-vised learning methods typically provide better, more task-focused results.

For example, consider the problem of classifying messages as belonging to a mac hardware or pc hardware newsgroup. Although there are word features in the data relevant to this task (such as  X  X owerbook X  indicating mac , or  X  X ell X  indicat-ing pc ), because mac and pc postings have high word overlap, an unsupervised clustering algorithm could discover many different ways to partition this data. For example, messages about hard drives or networking may appear as clusters, but these clusters may not be directly relevant to the classifica-tion task of interest. If posed as a supervised classification task, however, with labeled examples from each newsgroup, the classifier will learn to focus on the features relevant to the mac  X  pc task, and make the desired separation.
Training methods for machine learning classifiers are often characterized as being generative or discriminative. Gen-erative training estimates the joint distribution of all vari-ables, both the classes and the  X  X nput X  data, while discrim-inative training is concerned only with the decision bound-ary. Classifiers trained discriminatively seem to have lower asymptotic error than analogous generatively-trained classi-fiers because, intuitively, they are able to focus limited rep-resentational capacity on predicting just the class variable. However, discriminative approaches do not always provide the highest accuracy. When the amount of training data is small, generative training can provide better accuracy even when the model is not a very good fit to the data. Ng and Jordan demonstrate this, comparing naive Bayes and logistic regression [15].
Motivated by these observations, several researchers have proposed hybrid methods that combine generative and dis-criminative training. These hybrid methods have delivered promising results in the domains of text classification [13, 18], pixel classification [10], and object recognition [21, 11], among others.

A variety of semi-supervised techniques have been de-veloped for both generative and discriminative models. A straightforward, generative semi-supervised method is the expectation maximization (EM) algorithm. The EM ap-proach for naive Bayes text classification models is discussed by Nigam et al. in [17]. Generative semi-supervised meth-ods rely on a model for the distribution of the input data, and can fail either when this model is wrong, or when the structure of the input data is not correlated with the clas-sification task (as illustrated in the mac -pc example above). Discriminative semi-supervised methods, including proba-bilistic and non-probabilistic approaches, such as transduc-tive or semi-supervised support vector machines (TSVMs, S3VMs) [8, 7] and a variety of other graph based meth-ods [22, 1] assume high density within class and low density between classes, and can fail when the classes are strongly overlapping. Hence, these approaches for semi-supervised learning in discriminative classifiers also use model assump-tions about the structure of the input data, but typically do not encode these assumptions as explicit models of input probability density.

In this paper, we apply hybrid generative/discriminative methods to semi-supervised learning. We compare two re-cently proposed approaches to combining generative and dis-criminative methods in detail. The first is multi-conditional learning [13], a class of training objective functions com-posed of the product of multiple likelihoods that share one set of parameters and are derived from an underlying joint model. We formulate the semi-supervised training problem in terms of the optimization of a multi-conditional objective function that is a weighted combination of a discriminative likelihood of labeled data and a marginal likelihood of both labeled and unlabeled data. We also consider a framework proposed by Lasserre et al. [11] which we henceforth refer to as the parameter coupling prior 1 method. In this approach, the discriminative and generative components derived from a common joint model have separate sets of parameters. These parameters are coupled by a prior distribution that specifies how one set of parameters influences the other.
Both of these hybrid approaches can be interpreted as dis-criminative classifiers trained using the marginal likelihood of the input data as parameter regularization. We conjec-ture that for many problems this form of regularization is more helpful than typical discriminative regularization ap-proaches penalizing decision boundaries passing through re-gions of high marginal density. In contrast, these gener-ative/discriminative hybrids are not constrained to avoid low-density regions between classes when placing decision boundaries. Additionally, they are able to balance between leveraging innate clusters in the input data (which may or may not be useful) and task-specific evidence from the la-beled data (which may or may not be representative). Hy-brid methods can avoid relying on generative modeling as-
We note that parameter coupling prior is a short name we devised to refer to the work of Lasserre, Bishop, and Minka. This term is not used in their paper. sumptions by emphasizing the discriminative likelihood dur-ing maximization. In summary, these methods allow us to avoid the assumptions of discriminative semi-supervised ap-proaches and mitigate the assumptions of generative semi-supervised methods. By emphasizing each component of the objective function appropriately, they allow semi-supervised learning in cases that other methods fail.

In addition to the motivation provided above, the contri-butions of this paper are:
First, we define the learning problem. Suppose we have data D = D L  X  D U , where D L and D U represent the labeled and unlabeled data, respectively. Each example in D L is a pair ( y , x ), where the vector y has length equal to the number of classes and a 1 in the position corresponding to the index of the correct class (other entries are 0). Vector x has length equal to the number of features of the input and each position contains the value of a particular feature for this example. In D U , each example is only ( x ), as the value of y is hidden. In the case of document classification, for example, each example corresponds to a document, and each position in x might contain the number of times a particular word occurs.
 Notice that x can be decomposed into P N i w i , where N = P | x | i x i , so that each w i corresponds to the event of observing a feature. Vector w i has a single 1 in one position and 0 elsewhere. For example, in document classification w i represents a word occurrence. Another occurrence of the same word in the document would correspond to separate event w k . This decomposition of x into individual events is useful for understanding the graphical model introduced in Section 3. First, we discuss two model-independent hybrid approaches.
Multi-conditional learning [13] is a class of training objec-tive functions composed of the product of multiple weighted likelihoods, each with parameters derived from the same un-derlying joint model. An advantage of the multi-conditional framework is the flexibility it allows to craft an objective function for a specific learning task. For example, an objec-tive function composed of the product of weighted discrimi-native likelihoods for multiple tasks is a natural framework for transfer learning or multitask learning [5].
McCallum et al. [13] combine discriminative and genera-tive likelihoods using the multi-conditional objective func-tion: Training text classification models with this objective func-tion was found to produce improvements in classification ac-curacy. Here, we express semi-supervised training in terms of a multi-conditional objective function by combining the weighted discriminative likelihood of the labeled data and the weighted marginal likelihood of labeled and unlabeled data. This objective function is: where X L and Y L denote the labeled data and the term P ( X ;  X ) includes both labeled and unlabeled data. It is convenient to maximize the natural log of O : We choose the model parameters  X   X  that maximize O :
In equation (1), increasing  X  gives more weight to the dis-criminative component during maximization, while increas-ing  X  gives more weight to the generative component.
A practical concern is that each component and its gra-dient may be different in scale. Notice that P ( Y L | X is a distribution is over the number of labels, and P ( X ;  X ) is a distribution over the number of features. This means that if the distributions were uniform the magnitude of the log-likelihood for the generative component would be much smaller than that of the discriminative component. Addi-tionally, in semi-supervised learning the number of labeled examples is typically much smaller than the number of un-labeled examples, so the sums inside each likelihood calcu-lation have a different number of terms. This makes it diffi-cult to choose values of  X  in an interpretable way. Choosing  X  = 0 . 6 and  X  = 0 . 4 does not correspond to maximizing with 60% of the weight on the discriminative component, as the discriminative gradient magnitudes tend to be larger than those of the generative component.

One potential solution to this problem is to normalize each of the components so that they have the same mag-nitude, and weight the normalized componenets. In non-log space, normalizing each component corresponds to raising each component to a power x . If x &gt; 1, then this makes the probability distribution more peaked, whereas if x &lt; 1, the probability distribution is flattened. Since P ( Y L | X L convex, stretching or flattening it should make little differ-ence in terms of the ability of a gradient-based optimizer to find the maximum. However, P ( X ;  X ) is not convex, and consequently flattening it could actually change the maxi-mum found by the maximizer, if x is small enough to suffi-ciently smooth the distribution. Because the generative like-lihood is smaller in magnitude and flattening it by raising it to a power x &lt; 1 may be detrimental, we avoid normaliza-tion and set  X  = 1 and  X  &gt;&gt;  X  .

The difference in the magnitude of the likelihoods could also cause maximization to appear to converge when one component conceals the changes in the other. To deal with this issue, we adapt convergence criteria so that training converges only when both components, considered indepen-dently, have converged.

In addition to the terms above, we use a standard zero-mean Gaussian prior over parameters:
In the approach of Lasserre, et al. [11], which again we refer to as the parameter coupling prior approach, the gen-erative and discriminative components have separate sets of parameters. The two sets of parameters are jointly trained and are coupled using a prior distribution. Following [11], we define the joint distribution of features X , classes Y , and parameters  X  D and  X  G (for the discriminative and genera-tive models, respectively) as: P ( X, Y,  X  D ,  X  G ) = P ( X  D ,  X  G ) P ( Y | X ;  X  D ) P ( X ;  X  Let us consider two special cases of priors. If the prior P ( X  D ,  X  G ) constrains  X  D =  X  G , then we have a genera-tive model based on the joint distribution.
 If the prior assumes that the two sets of parameters are independent P ( X  D ,  X  G ) = P ( X  D ) P ( X  G ), then we have: P ( X, Y,  X  D ,  X  G ) = P ( X  D ) P ( Y | X ;  X  D ) [ P ( X  In other words, if the underlying joint model is such that the parameters of the marginal model and the conditional model are completely independent, then the terms inside the brackets are constant with respect to  X  D , and hence play no role in classification. Therefore, this is a discriminative model.

A prior that imposes a soft constraint that the parameters must be similar allows blending of the generative and dis-criminative. As in [11], we couple the two sets of parameters by a prior of the form: Lasserre et al. noted that the generative component P ( X ;  X  can make use of unlabeled data, and can hence be used for semi-supervised learning. Experimental results on one dataset using the above prior demonstrated the potential for semi-supervised learning using this method.
It is important to note that the two approaches described above are model-independent because they only specify the form of the objective function. We can derive concrete versions of the objective functions for a specific graphical model. Here, we apply them to a Markov random field Figure 1: A factor graph for a naively-structured Makrov random field model. (MRF) model, an undirected graphical model. The model is structured so that the input variables w i are conditionally independent given the class y . This can be interpreted as an undirected analog to naive Bayes models. For this rea-son we refer to this specific structure as a naively structured MRF. The factor graph for this model is shown in Figure 1. We could also use these training objective functions in more complicated models with hidden topic variables or models for sequences.
We estimate the parameters of the model by finding the parameters that maximize the objective functions defined in Section 2. We use gradient methods to find the max-imum. Below we define these objective functions for the naively-structed MRF model, and compute the gradients. The components of each objective are derived from the joint distribution of the model given by: Where Z = P y P x exp(  X  T y y + y T  X  T xy x ) is a normalizing factor that ensures a true probability distribution. First, we derive the gradient for the discriminative component of the objective function, the conditional log-likelihood L y | x log P (  X  y |  X  x ), where  X  y and  X  x denote observations and where The gradient is then computed as where E x [  X  ] denotes the expectation under p ( x ) and E denotes an expectation under the empirical distribution of x . The gradient of L y | x with respect to parameters  X  y similar. The log marginal likelihood for P ( x ) is and has gradient For the multi-conditional objective function, the parameter matrices  X  xy and  X  y are the same for both the discrimina-tive and generative components. For the parameter coupling prior method,  X  xy and  X  y are different for each component and are coupled by (2). The derivative of this prior with respect to each set of parameters is:
If  X  = 0 in the multi-conditional approach or  X  =  X  in the parameter coupling priors approach, then the L x term drops out of the objective function and we only maximize L y | x which uses no unlabeled data. Maximizing only L y | x the naive MRF model yields a maximum entropy classifier [16]. We use maximum entropy classifiers as the supervised counterpart to our hybrid semi-supervised methods.
We use Limited-Memory-BFGS, a quasi-Newton optimiza-tion method that has been shown to work well for maxi-mum entropy models [12], in conjunction with the adapted converge criteria discussed in Section 2.1. Note that the marginal density P ( x ) is not convex, which means neither the multi-conditional nor the parameter coupling prior ob-jective functions are convex. Because L-BFGS is a convex optimizer, we converge to a local maximum. Empirically, we have found that L-BFGS requires fewer training iterations converges to better maxima than other convex optimizers. Using a method to specifically address the lack of convex-ity may be beneficial, but is an issue we leave for future research.
Although many semi-supervised methods have been pro-posed, they each fall into one of a small number of classes of methods, each of which make certain assumptions.
Co-training [2] and related multi-view learning methods [4, 20] assume that multiple classifiers are trained over multiple feature views (splits) of the same labeled examples. As ca-pacity control, these classifiers are encouraged to make the same prediction on any unlabeled example. However, multi-ple feature views often do not naturally exist in practice, and these methods resort to artificially creating random feature splits.

Graph based semi-supervised learning [22, 1] assumes that labeled and unlabeled examples are connected by a graph, where edges represent similarity between examples. The dis-criminant function is encouraged to vary smoothly with re-spect to the graph. As a result, connected nodes tend to have the same label. One interpretation of this it is that labels  X  X ropagate X  through unlabeled examples via graph edges. However, the graph is usually constructed from the distances in feature space, and is susceptible to overlap-ping classes. Indeed if unlabeled data from different classes strongly overlap, the graph will be wrong, and the method can be expected to be inferior to supervised learning.
Semi-supervised or transductive support vector machines (S3VMs, TSVMs) [8, 7] also assume that there is a wide margin in kernel induced feature space between unlabeled data from different classes. The margin may be different than the traditional margin of labeled examples. S3VMs attempt to place the decision boundary in the unlabeled margin. However, there are two issues: First, such margin may not exist if the classes strongly overlap even in the kernel induced feature space. Second, S3VMs involves a highly non-convex optimization problem which is difficult to solve [6].

Generative semi-supervised methods based on the expec-tation maximization (EM) algorithm assume a model for the input distribution. In [17] Nigam, et al. use the EM al-gorithm in conjunction with a mixture of multinomials, or naive Bayes, generative model. In the E step, each unlabeled example is assigned a label distribution according to its ex-pected value under the current model. In the M step, the multinomial parameters are re-estimated. In practice this model can fail when the assumption of independent input features is violated or when the best generative structure does not correspond to the decision boundary.
There have been several successful applications of hybrid generative/discriminative methods in addition to the two approaches that are the focus of this paper. In many ap-proaches, parameters are separated into two subsets, one of which is trained discriminatively and the other generatively.
Raina et al. [18], present a model for document classifi-cation in which documents are split into multiple regions. For a newsgroup message, regions might include the header and the body. In this model, each region has its own set parameters that are trained generatively, while the param-eters that weight the importance of each region in the final classification are trained discriminatively. Experimental re-sults show that this hybrid algorithm gives better classifica-tion accuracy than either naive Bayes or logistic regression (a generative/discriminative pair [15]) alone. Additionally, Raina, et al. show that because the number of discrimina-tive parameters in the model is small, only a small amount of training data is required to estimate these parameters.
Kang and Tian [9] extend naive Bayes by splitting fea-tures into two sets X 1 and X 2 . The directed graphical model for this approach has nodes X 1 as the parents of the class variable Y , and parameters X 2 as the children of Y . Param-eters of this model are estimated by maximizing P ( Y | X (the discriminative component), and P ( X 2 | Y ) (the genera-tive component). An iterative algorithm based on classifica-tion accuracy is used to decide which features go in X 1 .
Bouchard and Triggs [3] propose a method to trade-off generative and discriminative modeling that is similar to multi-conditional learning because it maximizes a weighted combination of two likelihood terms using one set of param-eters. Defining L gen = ln P ( Y, X ) and L disc = ln P ( Y | X ), Bouchard and Triggs present a combined objective function L  X  =  X  ln P ( X, Y ) + (1  X   X  ) ln P ( Y | X ). A subtle difference between this objective function and those presented here is that in their approach the generative component is the full joint distribution. As in other related work, experimental results show that highest accuracy is obtained somewhere between fully generative (in this case  X  = 1) and fully dis-criminative (  X  = 0).
Semi-supervised learning methods are rarely applied in practice, in part because there are few empirical compar-isons of multiple semi-supervised methods. Additionally, many semi-supervised experiments use binary datasets that have few features and are easily separable. Here we pro-vide a substantial comparison of two semi-supervised hy-brid generative/discriminative methods and two prominent semi-supervised learning methods, as well as their super-vised counterparts, on multi-class datasets with large num-bers of features.
We first discuss the supervised/semi-supervised pairs of methods we use in the experiments.
 Naive Bayes / EM Naive Bayes ( nb , emnb ) We use the naive Bayes implementation in the Mallet toolkit [14]. As in Nigam et.al [17], we use Laplace (plus-1) smooth-ing, so that unseen events do not get zero probability. SVM / Transductive SVM ( svm , tsvm ) In our experiments we use Universvm, an SVM implementa-tion introduced in [7] that uses the Concave-Convex Proce-dure (CCCP) to optimize transductive SVMs. Collobert et al. show that optimizing TSVMs with CCCP improves ac-curacy and decreases training time when compared to other heuristic methods. The TSVM introduces several hyperpa-rameters that need to be tuned. In our experiments, we tune C  X  X  10  X  5 , 10  X  3 , 10  X  1 , 10 , 10 3 , 10 5 } and C labeled and unlabeled data, respectively. Collobert et al. also tune the symmetric ramp loss for the unlabeled data, but doing a grid search over three parameters, each with several possible values, was not practical for large-scale com-parison. We set the symmetric ramp loss parameter to  X  0 . 5 in all experiments and use a linear kernel. Although we do not perform normalization of feature counts for other meth-ods, we find that this is very important to achieve reason-able TSVM results and therefore normalize feature vectors to have Euclidean length 1 for the SVM and TSVM experi-ments.
 Max Entropy / Multi-Conditional Method ( me , mcl ) We use the implementation of maximum entropy models in Mallet [14] and also use this framework to implement the multi-conditional method. For both we tune the Gaussian conditional method, we also tune the relative weighting of the discriminative component  X   X  [10 2 , 10 7 ] at every order of magnitude. We use  X  = 1 for all experiments. See Section 2.1 for some discussion of settings for  X  and  X  . Parameter Coupling Prior ( pcp ) We implement this model in Mallet [14] as well. Following Lasserre et al. [11], we tune the parameter  X  , which is trans-lated into a value for  X  , the strength of the coupling prior, 0 . 1. We use  X  2 = 1 for the Gaussian prior on parameters, because we find that tuning this value provides little benefit when compared to the extra time it requires (we later show that pcp requires more time to train than the other meth-ods). As above, the supervised counterpart for this method is the maximum entropy classifier.

We run experiments on five text classification datasets and one sliding-window sequence labeling classification dataset. For the text classification datasets, features correspond to word occurrence counts. For the NER task, features are bi-nary word occurrences and properties of those words (such as capitalization) within three time steps. Stopwords, HTML, message headers (where appropriate), and features that only occurred once are removed from all datasets. Where noted, low frequency features are also removed.
 Datasets
Although all datasets are labeled, we simulate unlabeled data by ignoring the labels for some examples. Specifically, we choose examples to remain labeled randomly, but en-sure that the number of labeled examples is the same for each class as in [17]. We treat the remaining examples as unlabeled, up to a maximum of 5,000 unlabeled examples. The success of semi-supervised learning is dependent on the quality of the  X  X eed X  set of labeled examples. Therefore, we average results over five random labeled sets. We report accuracy on a held-out test data, rather than reporting ac-curacy on the unlabeled data, as is done with TSVMs [7].
Many semi-supervised methods introduce hyperparame-ters including graph methods [22, 1] and TSVMs [7]. We discuss the issue of choosing hyperparameters and provide some heuristics that reduce the need for hyperparameter tuning for generative/discriminative methods in Section 5.3. For these experiments we use a grid search to find the best settings, and use parameters settings that give the best test set accuracy, as also in [7]. Therefore, these results can therefore be interpreted as an indication of the potential of these methods, though further research is needed for prac-tical parameter tuning. Classification accuracy results are presented in Table 1. Either MCL or PCP achieves the highest accuracy in 75% of the experiments, and MCL and PCP achieve the largest mean accuracy improvements over their supervised counter-part at 5.2% and 3.1%, respectively. Additionally, MCL is the only method to show semi-supervised improvements on every dataset. Figure 2 illustrates that for both MCL and PCP the best accuracies are attained in the space between purely generative and purely discriminative.

We argue in the introduction that hybrid approaches are able to avoid or mitigate the assumptions of other semi-supervised methods. We would like to determine if the cases in which the hybrid methods perform well empirically match our intuitive justifications. It is difficult to quantifi-ably compare the degree of overlapping classes or the degree to which model assumptions fail across text datasets, but we can gain some insight by considering the datasets on which hybrid semi-supervised methods do well and other methods do poorly.

First notice that naive Bayes performs worst on datasets sector and ner , in which EM naive Bayes gives lower ac-curacy than supervised naive Bayes. Additionally, despite semi-supervised improvements, EM naive Bayes gives much lower accuracy on blogs than other methods. These three datasets have the largest number of classes, most compli-cated and correlated features, and greatest number of fea-tures, respectively. Highly correlated features clearly violate the X  X aive X  X ssumption of feature independence. We also ex-pect that for these tasks the natural clusters in the data will not necessarily be correlated with the decision boundary. For example, both ner and blog involve a very specific task on text that includes a wide variety of words and topics. The hybrid methods are able to mitigate these generative assumptions using the discriminative component.

Relative to the other methods, TSVMs perform poorly on ner and sraa , cases in which we expect classes to be strongly overlapping. The sraa dataset contains messages from news-groups on simulated aviation, real aviation, simulated auto class. feature diff me mcl nbsp -.022 .237 .215 urllink -.012 .246 .234 arianna -.007 .249 .243 years -.007 .240 .234 wife -.006 .242 .235 frienz -.005 .261 .256 couple -.005 .259 .254 moved -.005 .245 .239 town -.005 .252 .248 bar -.004 .247 .243 renee -.004 .245 .241 world -.004 .249 .244 doesn -.004 .256 .251 city -.004 .253 .249 ago -.004 .254 .250 accuracy was 55.6. feature diff me pcp font -.092 .250 .158 autos -.089 .249 .160 div -.079 .250 .171 nextpart -.069 .250 .181 iso -.066 .250 .184 voodoo -.066 .244 .178 meta -.064 .250 .186 px -.063 .250 .187 printable -.063 .250 .187 mb -.062 .246 .184 racing -.061 .233 .172 gb -.061 .250 .189 ns -.061 .248 .187 cars -.061 .238 .177 version -.006 .232 .172 racing, and real automobiles. These classes have high word overlap, as both simulated and real automobile newsgroups will include words like  X  X ire X  and  X  X ngine X , for example. The ner dataset contains classes for the start of a token type, such as location, and the continuation of a token type. We expect that the  X  X egin X  and  X  X ontinuation X  classes for a to-ken type will not have a low density region between them, and we see in the results that the TSVM chooses poor lo-cations for the decision boundaries as the accuracy is much worse than supervised.

We note that we originally conducted the SVM/TSVM experiments without normalizing feature vectors, and the TSVM only improved accuracy over the supervised SVM in two cases (blog (10,25)). Interestingly, normalization seems to have a much larger impact on semi-supervised learning than on supervised learning. Excluding the ner dataset, the mean SVM accuracy increases by 1.8% using normaliza-tion, whereas the mean TSVM accuracy increases by 5.2%. Even when using normalization, TSVMs do not always give improvements over supervised SVMs. One possible explana-tion is that most published evaluations of TSVMs use sim-ple, two-class datasets. Additionally, TSVM experiments typically use the eventual test data as unlabeled data dur-ing training, whereas here we use a held-out test set that is separate from the unlabeled training data. A more compli-cated kernel may improve accuracy, but note that the other methods only use linear combinations of features.
Comparing the two hybrid generative/discriminative meth-ods, MCL achieves higher mean accuracy than PCP on seven of the twelve experiments, as well as higher overall mean ac-curacy across all tasks. Notice that the two methods rarely perform well on the same dataset. The fundamental differ-ence between the two methods is that MCL has one shared set of parameters while PCP has two coupled sets of parame-ters. An advantage of the PCP method is that the gradients of the two components do not directly compete to modify parameter values. This allows each component of the PCP objective to have more freedom in modeling the data. If some generative parameter needs to be set in a way which does not correspond to a good discriminative setting, this penalty can absorbed if it leads to an improved model. Ad-ditionally, the PCP method allows the inclusion of a prior that only effects the discriminative parameters. With MCL, a Gaussian prior on parameters helps prevent the discrimi-native component from overfitting, but because the param-eters are shared, this prior has a negative impact on the generative component. Namely, we observe that the penalty on large parameter values tends to prevent large changes in parameter value due to the generative component, as em-pirically the generative parameters need to be more spread out than their discriminative counterparts. However, it ap-pears that the extra modeling power afforded by the PCP method sometimes makes it more difficult to find good pa-rameter settings during training. We note that the cases in which MCL gives improved accuracy over PCP tend to be more complicated datasets in terms of the number of labels or features. This suggests that PCP may be preferable for less complicated datasets and MCL for more complicated datasets.

In Tables 2 and 3 we show how parameter values change after introducing the generative component that uses unla-beled data. These tables show differences in the discrimi-native power of features in a supervised maximum entropy model and a semi-supervised hybrid. The values for each feature can be interpreted as the probability that a single word document containing that word feature belongs to the class listed in the table caption. In Table 2, we see that for the blogs dataset and label age 10-19 , the discriminative power of  X  X m X ,  X  X ol X ,  X  X chool X  and  X  X ome X  increases, and the discriminative power of X  X ife X , X  X ouple X , and X  X ar X  X ecreases. In Table 3, we see that for dataset sraa and label real avia-tion , the discriminative power of  X  X irspeed X ,  X  X ltitude X , and  X  X reflight X  increases, while the power of  X  X utos X ,  X  X acing X , and  X  X ars X  decreases. Intuitively, we see that the addition of the generative component helps to boost the discrimina-tive power of features that the supervised model could not discover with limited labeled data.
Hyperparameter values are often important to the suc-cess of semi-supervised learning methods, but tuning hy-perparameters can be difficult in practice. In supervised learning, hyperparameter values are typically chosen using cross-validation. If we apply this method for semi-supervised learning, each fold test set ends up with a very small number of labeled examples, since the total number of labeled ex-amples is small. This means that the performance estimate obtained from this test set may be inaccurate. Addition-ally, optimal hyperparameter values may depend upon the specific set of labeled examples. Training with a subset of those labeled examples could result in drastically different ideal settings for the the hyper-parameters, as, for example, a poor seed set of labeled examples may require a hybrid classifier to put more weight on the generative component and rely on unlabeled data. Another option is to choose the hyperparameters that give the highest likelihood on the training data. However, we have found that these hyperpa-rameters produce the highest likelihood models because of overfitting. Finally, choosing hyperparameters using a vali-dation set is not practical, as if more labeled data is avail-able, it would almost certainly be more beneficial to use that data during training than to use it to tune hyperparameters.
Another practical issue in semi-supervised learning is that the added complexity of semi-supervised training algorithms increases the overall model training time, as shown in Ta-ble 4. On average, the PCP method takes the longest to train. In addition to having twice as many parameters as the MCL method, it also seem to take more iterations to converge. The EM naive Bayes semi-supervised learning is extremely fast because the maximum likelihood estimates can be computed in closed form.

We propose a heuristic for hybrid models to simultane-ously reduce training time and reliance on hyperparameter settings that involves ensuring reasonable initial parameter settings. For the PCP method, we initialize both sets of pa-rameters to the results of supervised training on the labeled data. We present results using supervised initialization of the PCP method in Table 5. This also reduces training time, as shown by the entry pcp-init in Table 4. For the MCL method, we propose to use the discriminative compo-nent exclusively in the first few iterations of training. Since the discriminative component is convex, this guarantees that we move into a reasonable region parameter space. At the same time, by avoiding going the whole way to the global maximum, we give the parameters room to move away from the discriminative maximum. Table 5: Classification results when using supervised initialization for parameter coupling prior model. Parenthesized values indicate the number of labeled documents per class.

These heuristics seem to make the algorithms more con-sistent, as the mean accuracy across all hyperparameter set-tings is higher. However, the maximum accuracies attained are sometimes lower. Intuitively, one of the real benefits of these methods is that the generative component can pull the discriminative component into regions of parameter space very different from those chosen by supervised discrimina-tive learning on the limited labeled data. With these initial-izations, we ensure that we do not converge to a poor local maximum, but sacrifice the potential to find drastically dif-ferent parameter settings.
We have considered hybrid generative/discriminative ap-proaches to semi-supervised classification in which the gen-erative component includes unlabeled data. We compare two methods for combining generative and discriminative likelihood in detail: a multi-conditional learning method and a method where each component has its own set of parame-ters that are coupled by a prior distribution. In a substantial empirical comparison, a hybrid method provides the best accuracy in eight of the twelve experiments. Intuitively, we conjecture that they perform well by mitigating the model-ing assumptions of generative semi-supervised methods and avoiding the low-density-between-class assumptions of dis-criminative semi-supervised methods.

In future work, we would like to apply hybrid genera-tive/discriminative approaches to transfer or multi-task learn-ing, consider heuristic and probabilistic methods to learn hyperparameters from data, and research alternative opti-mization techniques utilizing ideas from the multi-criteria optimization literature.
This work was supported in part by the Center for Intel-ligent Information Retrieval, in part by The Central Intel-ligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by NSF Nano #DMI-0531171, and in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Di-vision, under contract number NBCHD030010. CP appre-ciates support by Microsoft Research under the Memex and eScience funding programs and support from Kodak Re-search. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [2] A. Blum and T. Mitchell. Combining labeled and [3] G. Bouchard and B. Triggs. The tradeoff between [4] U. Brefeld, T. Gaertner, T. Scheffer, and S. Wrobel. [5] R. Caruana. Multitask learning. Machine Learning , [6] O. Chapelle, V. Sindhwani, and S. S. Keerthi. Branch [7] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Large [8] T. Joachims. Transductive inference for text [9] C. Kang and J. Tian. A hybrid [10] B. M. Kelm, C. Pal, and A. McCallum. Combining [11] J. A. Lasserre, C. M. Bishop, and T. P. Minka. [12] R. Malouf. A comparison of algorithms for maximum [13] A. McCallum, C. Pal, G. Druck, and X. Wang. [14] A. K. McCallum. Mallet: A machine learning for [15] A. Ng and M. Jordan. On discriminative vs. [16] K. Nigam, J. Lafferty, and A. McCallum. Using [17] K. Nigam, A. McCallum, S. Thrun, and T. M.
 [18] R. Raina, Y. Shen, A. Y. Ng, and A. McCallum. [19] J. Schler, M. Koppel, S. Argamon, and J. Pennebaker. [20] V. Sindhwani, P. Niyogi, and M. Belkin. A [21] D.-Q. Zhang and S.-F. Chang. A [22] X. Zhu, Z. Ghahramani, and J. Lafferty.

