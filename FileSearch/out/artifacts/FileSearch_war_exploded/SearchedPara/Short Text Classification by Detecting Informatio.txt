 Short text is becoming ubiquitous in many modern informa-tion systems. Due to the shortness and sparseness of short texts, there are less informative word co-occurrences among them, which naturally pose great difficulty for classification tasks on such data. To overcome this difficulty, this paper proposes a new way for effectively classifying the short texts. Our method is based on a key observation that there usu-ally exists ordered subsets in short texts, which is termed  X  X nformation path X  in this work, and classification on each subset based on the classification results of some pervious subsets can yield higher overall accuracy than classifying the entire data set directly. We propose a method to detect the information path and employ it in short text classifica-tion. Different from the state-of-art methods, our method does not require any external knowledge or corpus that usu-ally need careful fine-tuning, which makes our method easier and more robust on different data sets. Experiments on two real world data sets show the effectiveness of the proposed method and its superiority over the existing methods. I.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture evaluation and selection Short text classification; Information path; Anchor shackle term
With the explosion of online social network applications and e-commerce, short texts such as microblogging, prod-uct reviews and search snippets are becoming more popular on the Internet. Automatic classification serves as a use-ful way to explore short texts. Traditional text classifica-tion techniques are mainly based on common words among both labeled and unlabeled data that belong to the same category to measure text similarities. However, due to the shortness and sparseness of short text, such common words are insufficient. This gap makes short text classification a challenging problem [10]. To alleviate the sparseness of short text, state-of-art works mainly focus on expanding short texts with knowledge extracted from auxiliary long text corpus [5]. Nevertheless, this process is usually domain dependent and thus requires remarkable human efforts in collecting and tuning the data. On the other hand, utilizing inherent characteristics of short text other than the common consensus of shortness and sparseness, which might benefit the construction of more accurate classifier for short text data, however remains unexplored in the literature.
Based on the observation of several inherent characteris-tics of short text data sets, this paper proposes an effective and auxiliary-resource-free method for short text classifica-tion, which saves human efforts for collecting the auxiliary corpus remarkably. Specifically, an inherent characteristic of real world short text data is observed: usually, ordered sub-sets of short text (termed  X  information path  X ) can be found in test dataset and if we classify each subset using classi-fication results of previous subsets, it would achieve better classification results than classifying the entire dataset di-rectly. Figure 1 illustrates the main concept of information path. Essentially, it is a path that consists of sequential subsets in the test dataset. And according to this path, in-stances of former classified subsets can assist classification of later subsets.

The intuition behind information path can be explained as follows: though data shortness may yield less common words between training and test data that belong to the same category, some instances of test data are likely to share common discriminative terms with training data, and thus these instances can be labeled more accurately. In addition, these newly labeled texts bring in new common words. Thus a natural idea is to reuse these correctly labeled texts to fill the gap between training data and the other test data. Figure 2: Example of information path on paper titles
Figure 2 shows the insight intuition of information path by a more concrete example of short text classification on paper titles. Suppose the task is to predict the conference in which a paper is published given the title of the paper. Paper titles shown in this figure are all from one conference (SIGIR) and thus they should share the same label. In-stances of Group 1 are training data. If we train a classifier using instances in Group 1 and then classify the other three groups directly, the accuracy might be low. The reason is that the data in Group 1 has few if any common words with instances in Group 3 and 4. However, data in Group 2 can serve as a bridge to connect instances of training data and other test data in Group 3. Specifically, Group 2 shares a common word  X  X etrieval X  with data in Group 1, and  X  X ank X  with Group 3. Thus we can build a classifier on Group 1 and classify Group 2 firstly, and then utilize both Group 1 and 2 to train a new classifier to label instances in Group 3. This method can achieve better classification result as Group 2 brings in common word with Group 3. In a similar way, data in Group 4 can be labeled using a classifier built on previous labeled instances. In this way, Group 2, 3 and 4, which are all subsets of test data, are connected together to form the information path. And according to this infor-mation path, classification result of former subsets can help the classification of later subsets.

In this paper, we incorporate inherent characteristics of short text in semi-supervised learning framework. In general semi-supervised learning frameworks, instances with high classification confidence are moved from test data to training data, and then build a new classifier on the whole training data to classify other test data in each iteration. Based on this framework, our method considers the inherent charac-teristics of short text, which are as follows.

Firstly, common word that has good categorical discrim-inability can be found in selected groups of short texts, while that can be hardly found for the entire dataset. Such word is termed anchor shackle term in our paper (e.g., retrieval, rank, collect, in Figure 2). This is reasonable because when people talk about a specific topic, they tend to adopt the commonly used words in short texts for being understood easily. Paper titles serve as a good example to illustrate this observation.

Secondly, for two groups of short texts A and B that are from the same category but with seldom common discrim-inative terms, it is possible to find another group of short texts C from the same category that can bridge the gap be-tween A and B in the information path. That is, A can be used to classify C , and then the newly labeled C together with A in turn can be used to classify B .

Based on information path detection, the short texts can be classified sequentially where classification of each subset can get prearranged help from some previously labeled texts. This fills the gap of short texts without demanding auxiliary resources and human efforts. The main contributions of this paper are summarized below:
Existing techniques for short text classification are mainly based on feature expanding. The major difference between these techniques is how to obtain the extra features. One is based on search engine that integrates the search results into the short text [12, 13, 18, 2]. The other is to exploit an external corpus and utilize the topics as new features [10, 4, 11, 3, 16]. There is also a work[7] that uses the transfer learning methods to exploit the external data.

Although these methods can improve the accuracy of short text classification to some extent, they all need some exter-nal auxiliary data obtained through a time-consuming col-lection. These methods do not pay enough attention to the inherent characteristics of the short text itself. The classi-fication accuracy is improved by doing the topic correlation analysis of training and test data in [6]. There is also an-other work[14] improving the accuracy significantly by se-lecting some useful features. In [15], the proposed method yields similar accuracy compared to the baseline by using the representative query words of the short text to search the categories from the labeled short texts. These two works indicate that some inherent features of short text can play a more important role for classification.

In addition to the techniques for short text classifica-tion, our method also relates to some other existing machine learning techniques including: (1) semi-supervised learn-ing [8, 19] that improves the learning process by exploit-ing the unlabeled data in addition to the labeled training data, (2) transfer learning [9] that applies the knowledge learned from one domain to another domain, (3) curriculum learning [1] that learns the knowledge from easy to difficult gradually. Our method is different from these techniques. It is under the framework of self-training, but we reuse the data for tracking the inherent anchor shackle terms changes within the short texts and detecting the information path ac-cording to the characteristics of short texts. However, other techniques of semi-supervised learning aim to get more cor-rectly labeled data for a better classifier in each iteration. Our method can be seen as a knowledge transfer process through the information path, but there is no definite do-main information. The curriculum learning is a strategy to determine the sequence of instances to be learned when the classifier is trained (when the knowledge is learned), but our method can be seen as a strategy to determine the sequence of instances when they are classified (when the problem is solved). In our method, the sequence of instance is not de-termined by the difficulty of the classification, but by the changing of the detected anchor shackle terms. And there is not a definite concept of difficult knowledge or easy knowl-edge used in our work.
In this section, we formally define the problem of short text classification and denote the concepts of information path and anchor shackle term.

The problem of classifying short text is: given a training dataset D train = { &lt;x n ,y n &gt; } where x n and y n feature vector and label of the n -th training sample in D respectively and a testing dataset D test = { x n } where x the n -th feature vector in D test , the short text classification problem is to predict a class label y n for each x n  X  X  test Due to the sparseness of short text, specifically most dimen-sions of vectors x n and x n are zero values.

Information path is constituted by a sequence of mutu-ally inclusive data subset D n  X  X  test of the test data D where classification on each subset D n based on previous subsets D 1 ,..., D n  X  1 can yield an higher overall accuracy than classification on the entire test data directly. The con-cept of information path is illustrated in Figure 1 intuitively. The key point to detect the info rmation path is to construct all subsets D n of test data and determine the classified order of them.

An Anchor shackle term is the word that is commonly used in short text when talking about a specific topic. Gen-erally, these anchor shackle terms are presented as common words that have good categorical discrimination in the con-text of short texts.

Information path describes the subset that each instance belongs to and the order in which the subsets are classi-fied. Ideally, each subset on information path should have small gap with the subsets that it is directly connected to. Specifically, in short texts, if two texts have common anchor shackle term, they have large probability of talking about the same topic, and thus the gap between them is small. So the anchor shackle terms are the connections between subsets of the information path.
This section proposes our method for selecting the anchor shackle term and classifying the instances according to the detected information path.
The anchor shackle term is the core of information path detection. In addition to its strong categorical discriminabil-ity, it should also be commonly used. We define a precise process to decide which term is anchor shackle term. First, as a widely used word, it should be the common word be-tween D train and D test . So the anchor shackle term w sat-isfies the following criteria: w  X  W,W = { w | d D train ( w ) 1  X  d D test ( w )  X  1 } ,here d D ( w ) means the number of docu-ments in dataset D containing the anchor shackle term w . Next, we adopt entropy to measure its categorical discrim-inability since entropy is a very useful indicator for feature selection. We just use training set D train since instance labels are required when calculating the entropy. The for-mula of calculating the categorical discriminability of data is shown as follows:
Here d ( w,c i ) denotes the number of documents containing the word w with the label c i . We do not consider documents that do not contain the word w like the work [17]. After the words having been selected, we use those documents con-taining the selected word. So the categorical mixing degree of documents containing the word is more important to us. We consider the characteristic of wide usage of the anchor shackle term by using appearing time in short texts. As-suming there are two common words, if the numbers of doc-uments containing the words are either both very large or both very small, the appearing time cannot be a very impor-tant factor to determine which is to be selected as both of them are widely or rarely used. At this time, we should care more about the entropy. Only when the appearing times of two words are neither too large nor too small, the effect of appearing time and entropy should be considered equally. The sigmoid function can meet this requirement.

Based on this intuition, we design the following formula to score each word w  X  W andthenselectthewordwiththe highest score as the anchor shackle term in each iteration. Here f ( x ) is the sigmoid function that f ( x )= 1 1+ e subscript Tr or Te represent D train or D test respectively. d
Tr ( w )= d D train ( w ).  X  d Tr ( W Tr ) is the average appear-ing time of words in D train , W Tr = { w | d D train ( w )  X  d ber of elements in set W Tr . d Te ( w ) ,  X  d Te ( W Te terms calculated on D test .The Entropy ( w )canbecalcu-lated according to the formula 1.

As the formula 2 shown, one common word is selected, which meets two criterias: 1) it appears enough times in the short texts; 2) it has strong categorical discriminability. Detecting two or more anchor shackle terms in one iteration is also available in our method, but the quality of them might not be good enough. The main framework of our method is shown in Figure 3. Our method is executed by iterations of detecting following subsets on information path. The method is formally illus-trated as follows:
Step 1 . Select the anchor shackle term w according to
Step 2 . Generate the subsets d test from D test as the following subset on information path. According to the first characteristic of short text, the instances in training data that contain the selected anchor shackle term have small gap with test data. So, the generated subsets should satisfy: d w  X  x
Step 3 . Label the subset of test data d test on information path. Train a classifier C based on selected subset of training data d train , then yield y n for each x n  X  d test by C .Now, d
Step 4 . Move the data d test to the training data. D train D
Step 5 .If D test =  X  , the algorithm is terminated. Oth-erwise, go back to Step 1.

Note that those documents that have no common words with all other documents will be classified at last by the classifier trained on all the labeled data.
We evaluate the effectiveness of our method on two real-world short text dataset. Our experiments mainly focus on three aspects of our method: (1) To evaluate the perfor-mance by comparing with five baseline methods. (2) To offer the attributes by showing the anchor shackle terms. (3) To offer the insight of efficient-wise by giving the computational complexity and c onsuming time.
Two real-world short text dataset are used. One is the  X  X earch snippets X  1 , which had been commonly used in other works on short text classification. The other is X  X aper titles X  extracted from DBLP 2 , which contains thousands of paper titles collected by ourselves.

The Search snippets dataset is introduced in [10]. It consists of search snippets retrieved from Google. They used various phrases belonging to 8 different categories to do the Web search and selected top 20 (for training data) or 30 (for test data) snippets from the retrieval results to construct the dataset. Details of this dataset is summarized in Table 1.
The Paper titles dataset consists of paper titles be-longing to two different categories. One is multimedia , the other is network . The details of the conferences and transactions used to consist of the dataset are illustrated in Table 2. Since the amount of paper titles for different cate-gories are so unbalance, we randomly select 450 paper titles
Collected by Xuan-Hieu Phan, using JWebPro: http://jwebpro.sourceforge.net. http://www.informatik.uni-trier.de/  X  ley/db/.
 Table 1: Google search snippets as training &amp; test data Table 2: Research paper titles as training &amp; test data (uniform in different years) of each transaction or confer-ence in each time of experiment and do the experiment for 20 times.
In the experiments, we compare our method with five baseline methods. (1) PH is an enriching method intro-duced in [10], it extends short texts with single granularity topics. (2) CH is an enriching method proposed in [3], which extends short texts with multi-granularity topics. (3) Conf is a semi-supervised learning method which moves the in-stances with high classification confidence from test data to training data in one iteration. (4) NI is a semi-supervised learning method introduced in [8] which updates the label of texts by rebuilding the classifier according to both training data and test data in each iteration. (5) Conv is a con-ventional method which simply classifies the unlabeled test data by a classifier trained by all of the given training data.
Moreover, notice that two enriching methods need exter-nal text resources. For experiments on the Search snippets, an external dataset 3 is available. For experiments on the Paper titles, we collect an external corpus related to two fields (Multimedia and Network) from Wikipedia by using JWikiDocs following the same way as illustrated in [10].
Logistic Regression 4 is used as the classifier. Default pa-rameter settings of it are used. Note that each result on Pa-per titles dataset in the following figures (Figures 4(b) and
Collected by Xuan-Hieu Phan, using JWikiDocs: http://jwebpro.sourceforge.net Liblinear: http://www.csie.ntu.edu.tw/  X  cjlin/liblinear, L2-regularized logistic regression. 5(b)) is the average results of 20 times experiments with the corresponding parameter settings.
For both enriching methods PH and CH, two parameters need to be set: (1) the number of topics learned from exter-nal dataset, (2) the weight of topics. In addition, the method CH has another parameter, i.e. the number of granularity of topics, which is set to 3 on Search Snippets as in [3], and to 2 and 4 respectively on Paper Titles. Figure 4: Accuracy comparison with the enriching methods
The results are illustrated in Figure 4. The numbers after the PH and CH of the legend represent the number of topics mined for external corpus. We can see that our method outperforms the baseline methods on both dataset.
In addition, some details can explain the reason why our method works well. For example, there are texts talking about  X  X ank X  X  transactions processed by computer program X  in test data of Search Snippets, which belong to the  X  X usi-ness X  X ategory. However, no corresponding labeled text talk-ing about similar topics can be found in the same category. Thus the enriching methods simply extend the topics about computer programming and bank to the unlabeled texts. As the training dataset has a category  X  X omputers X , which contains large amount of texts talking about computer pro-gramming, the texts are classified incorrectly to the cate-gory  X  X omputers X . However, before classifying these texts, our method has automatically found and classified some un-labeled texts talking about  X  X ank X  in advance. As these texts also contain a lot of words such as stock and loans, which are widely existed in the labeled training data, these texts can be classified correctly. And then with the help of these correctly classified texts, the texts talking about  X  X ank X  X  transactions processed by computer program X  can be labeled correctly.
In method Conf, the amount of instances moved from training dataset to test dataset in each iteration is a pa-rameter that needs to be tuned. For the method NI, the parameter of iteration times is set to 50.

The results are illustrated in Figure 5. We discover that our method performs better than the baseline methods. For Search snippets, it consists of search results of synonyms ob-tained from Google. As the search words of same category share similar meanings, the search snippets are also related to each other in content for same category. Thus, the infor-mation path can be detected well by anchor shackle terms. For Paper Titles, though the texts in the same category are talking about similar topics, there are also some gaps be-Figure 5: Accuracy comparison with the semi-supervised learning methods tween paper titles because each paper can only talk about a sub-field of the category it belongs to. However, paper titles of the same conference are more likely to be related with each other as each conference always has a specific theme, which can be viewed as a sub-field of the category. More-over, the Conf method performs even worse than the Conv method mainly because of this gap. The label got with high confidence is not correct.

On Paper titles dataset, though the average performance of our method outperforms other methods, we provide some details here. Comparing with baseline methods in different parameter settings, our method gets the best accuracy in 10 out of 20 experiments. The second best method X  X H10305070 X  only achieves best accuracy in 6 out of 20 experiments (with its carefully parameter setting). Our method has dominant advantages on this dataset.
In order to offer the insights of our method, the relations of anchor shackle terms are given. Here, the relations of an-chor shackle terms in the first 20 iterations on two dataset are illustrated in Figure 6. We remove stop words and stem the words using TMG 5 for preprocessing, so some words in the two figures are different from their formal spellings. The anchor shackle terms chosen to be shown on the Paper titles are results of the 11th experiments as it achieves similar ac-curacy to the average level of our method on that dataset. The directed lines connecting two anchor shackle terms rep-resent the texts of test data containing the two terms. These texts are labeled when the term of starting point is detected and help classify the texts containing the term of terminal point. The more texts like this, the blacker the line is and closer the relation of the two anchor shackle terms, since the two terms are more likely to prompt in the same text. The terms with no directed lines pointed to indicate that the test data containing that term are classified without help of the former classified text in test data. The texts containing those terms have small gap with training data.

The anchor shackle terms that connect the subsets of in-formation path are quite reasonable on the two dataset. For example, there is an information path on Search snippets connected by a chain of words: lung  X  tobacco  X  prevent  X  epidem  X  infect  X  ... (The words after  X  X obacco X  is not illustrated in Figure 6(a) because they are detected in later iterations). Actually, most test documents containing the word  X  X nfect X  should be classified to the category  X  X ealth X . However, in the training data, texts containing  X  X nfect X  are http://scgroup20.ceid.upatras.gr:8000/tmg/. Figure 6: The relations of anchor shackle terms se-lected on two dataset in first 20 iterations in both the  X  X ealth X  and  X  X omputer X  categories, since some texts are talking about  X  X omputer virus X  in training data. So, if we classify the test data directly, texts those belong to the category  X  X omputers X  in training data would bring negative effect because such texts have word co-occurrences with texts in  X  X ealth X  category. If we use all the training data to build the classifier as the method Conf, texts of cate-gory  X  X omputer X  talking about  X  X omputer virus X  might also bring in negative effects. The classifier can classify the texts talking about  X  X ealth X  as  X  X omputers X  with a high confi-dence, while the given label is totally wrong. Our method detects the texts talking about  X  X ung X  first, which are more certain to the category  X  X ealth X  according to the training data. Then we distinguish the confusing texts later when their related texts are labeled.
The computational complexity of our algorithm for each iteration is O ( | V | +( |  X  L | +1)  X | D | + Tr ( C )+ Te ( is the size of the vocabulary, |  X  L | is the average amounts of common words appearing in each document, | D | is the size of instances of the whole dataset, Tr ( C )and Te ( C )arethe training and predicting complexity of the chosen classifier C respectively. Since the algorithm is terminated automat-ically when all the test data is labeled, iteration number is determined by the data itself. The more word co-occurrence of the dataset, the fewer the iteration times. With respect to quantitative analysis, average running time (20 experiments) on the real dataset is 135.58 seconds (on Search snippets) and 29.24 seconds (on Paper titles) respectively.
This paper proposes an auxiliary-resource-free method for short text classification by incorporating the inherent infor-mation path that is very common in various real world appli-cations. Since our method is based on the essential charac-teristics of the short text data, it is reasonable to anticipate better classification accuracy. But if the information path is not existed in the given dataset, our method may not perform well.

In future, we intend to involve some existing methods, in-cluding those leveraging auxiliary resource into our schemas to further improve its classification accuracy. This work was supported by National Natural Science Foundati on of China (60973103, 90924003). [1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. [2] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring [3] M. Chen, X. Jin, and D. Shen. Short text classification [4] X. Hu, X. Zhang, C. Lu, E. Park, and X. Zhou. [5] O. Jin, N. Liu, K. Zhao, Y. Yu, and Q. Yang.
 [6] L. Li, X. Jin, and M. Long. Topic correlation analysis [7] G. Long, L. Chen, X. Zhu, and C. Zhang. Tcsst: [8] K. Nigam. Using unlabeled data to improve text [9] S. Pan and Q. Yang. A survey on transfer learning. [10] X. Phan, L. Nguyen, and S. Horiguchi. Learning to [11] X.Quan,G.Liu,Z.Lu,X.Ni,andL.Wenyin.Short [12] M. Sahami and T. Heilman. A web-based kernel [13] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and [14] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, [15] A. Sun. Short text classification using very few words. [16] D. Vitale, P. Ferragina, and U. Scaiella. Classification [17] Y. Yang and J. Pedersen. A comparative study on [18] W. Yih and C. Meek. Improving similarity measures [19] X. Zhu. Semi-supervised learning literature survey.
