 The important proliferation of digital multimedia content requires tools for extracting useful knowledge from the content to enable intelligent and efficient multimedia or-ganization, filtering and retrieval. Vast amount of images are now available as digital format and many of them are being described either directly or indirectly by some textual information [1]. Such description is useful in many cases particularly for searching. Web images for instance have brought great challenges for search engines. While many search engines mainly focused on keyword spotting for indexing and searching, little have gone beyond the  X  X emantic meaning X  of images which are hid-den and embedded somewhere in the textual description. Semantically search of im-ages based on its content is highly difficult and little successful stories have been reported. An image in web documents, however, is provided with meaningful descrip-tion which can be exploited to support keyword searching. Relying on the surround-ing text alone however unable to provide meaningful description of the images. The questions of Who? What? When? and Where? are common among image searchers which are not capable of being answered by normal indexing. External sources par-ticularly in the form of ontology and some forms of annotation are required in order to provide such semantic facility. 
This paper reports our approach in supporting simple semantic search of web im-ages by exploiting lexical ontology. The scope of our work is on on-line newspaper images. Such images are usually provided with short description and elaborated some-where in the news content. The aim is to extract the  X  X ho X ,  X  X hat X ,  X  X hen X  and  X  X here X  information from the textual description which represent the images. In a broader sense, the idea is to extract semantic meanings of images based from the free (unstruc-tured) textual description provided by web authors and then subsequently constructs the semantic index. In our approach we employ the Named Entity Recognition (NER) technique and exploit the WordNet [2] and ConceptNet [3] lexical ontologies. NER plays an important role in extracting the information about person, location and time of the textual description. WordNet and ConceptNet on the other hand are used to seman-tically enhance the index of the stored images. The main purpose is to semantically index images beyond the terms provided in the attached textual description. 
This paper is organized into the following five sections. The next section provides some background and related research. Section three discusses our approach and technique to support semantic document retrieval and browsing and followed by sec-tion four which provides the evaluation activities. Finally we present the conclusions that may be drawn from our work. Search engines used textual information surrounding images as features [4]. In gen-eral terms, such information came be loosely considered as annotations containing textual keywords that can be thought of as resembling documents [5]. Such annota-tions can be semantically enhanced by exte nding to ontology or any lexical resources. Named Entity Recognition (NER), on the other hand, is one of the techniques in natu-ral language processing (NLP) that can semantically annotate important entities such  X  X erson X ,  X  X ocation X  and  X  X ime X . The remaining of this section briefly describes the aforementioned concepts and then proceed s with related works in this area. Few efforts in semantic retrieval for images are illustrated in the work of [1,6,7]. Benitez and Chang [1] focused on information extraction of semantic or relationship between a collection of images that has been annotated, such as nature images and news images. Among the heuristics considered are: words nearer to the images have semantic relations to the images and, high frequency of words indicates important meanings of images. Three stages involved in extracting such meanings, which are: text preprocessing; extraction of semantic concept and extraction of semantic rela-tionship. Text preprocessing uses syntactic analysis while semantic concept extraction process uses Wordnet and Word Sense Disambiguation (WSD) technique for text clustering. Weights are assigned to chosen concepts using tf  X  idf and log tf  X  entropy . Weights are also given based on synsets (from the WordNet), core meaning and ex-ample usage. Semantic relationships extraction of concepts on the other hand is based on the meanings derived from WordNet. Their study shows that the use WordNet gives better for nature images. Furthermore, the use of annotated news images that contain more textual description than those images that only contain keywords also give better result. 
The main objective of Gong et al [6] work is to develop indexing scheme of web images using texts that are available in the web. This research emphasizes the princi-ple that every website developer will use images to describe their website and the distance between the word and the image are also important. Therefore, texts that are available in the website will have semantic relations that are linked to the image. Their approach divided the text containing the images into three blocks of semantic segmentation, i.e. (i) TM (Title &amp; Meta); (ii) LT (image location, image name, image hyperlink or ALT); and (iii) BT (body of text). Results from the experiment showed that the texts situated nearest to the image gives recall measure. However, when tak-ing the whole text into account (i.e. by cons idering the three segments of TM, LT and BT), the recall measure is much better. The work of [6], however, does not consider external resources to provide additional semantic meanings of the indexed terms. 
The work of Hua et al [7] combined visual and textual feature for searching im-ages. Semantic information extracted from web pages are text summary, human re-lated information such as name, geographical information such as name of a place and telephone number. Apart from concept extrac tion as in the previous two approaches, their approach proposed four aspects of semantic information extraction namely: visual weight, total phrases, phrase weight and independent phrases. Result shows that 62% to 90% of web images capable of being semantically described. The proposed approach, however, still could not differentiate between geographical and human name such as McDonald X  X  which should be considered as geographical infor-mation and not human X  X  name. 
Previous work shows that annotations are important as means to describe the seman-tics of images. Two obvious forms of annotation exhibited in the related works: struc-tured annotation and unstructured annotation. Structured annotation can be regarded as comments made directly to sources by means of some tools and which are usually seen technically as metadata. This kind of annotation is exhibited by the work of [1]. How-language description attached to either the images, documents or artifacts. We may call this  X  X nstructured X  because it is not represented by means of meta data but usually freely provided by document or web authors. The WordNet has been a popular choice for enhancing semantic meaning of images. However, not all images descriptive terms can be semantically extended by WordNet. Therefore, some common-sense concepts are required. For instance mentioning the word  X  X ride and groom X  or seeing images with similar  X  X ontent X  will relate to terms such as  X  X edding X  and  X  X ing X  [8]. Such common-sense concepts can be derived from rich semantically processed resources such as the ConceptNet. This study, therefore, embarks on the possibility of representing semantic and common-sense meanings in images by exploiting the unstructured textual annota-tions and the surrounding text, and further extended by deriving the semantic and com-mon-sense concepts from the WordNet and ConceptNet respectively. Semantic search uses the science of meaning in language X  X nstead of just searching keywords, it checks the context of the words to return more relevant results. Therefore, we view semantic search as searching beyond searchable index. For instance searching for the term  X  X rimer minister X  will also result in the retrieval of images which contain similar terms such as  X  X ead of state X  and  X  X bdullah Badawi X  (the prime minister of Malaysia). However, the name  X  X bdullah X  can still be ambiguous as either represent-ing a name of a person, name of a street or even name of a company. Therefore, NER is one of the methods meant to solve such ambiguity. Our approach takes into consid-eration the aforementioned elements with the use of WordNet and ConceptNet, as well as NER patterns. We scope our work into on-line Malaysian English language news-paper under the nation category. The nation category is related to Malaysian political and local issue news. Our approach is as depicted in Fig. 1 of which containing a num-ber of stages as follows. 3.1 Extraction of Image Description, URL and Surrounding Text As mentioned earlier, textual information surrounding images can be regarded as unstructured annotation of images. Images in on-line newspapers contain such rich and useful annotations. In this stage, tags in HTML documents have to be  X  X leaned X  bookmark for extracting the surrounding text, image description and image loca-tion. The HTML documents will then be transformed to ASCII document file type. Image location will be stored directly into the database whereas the associated im-age X  X  description and surrounding text will be used in the next process. Throughout this paper, image X  X  description refers to textual information right under images as exhibited in many on-line newspaper images. Other textual information is consid-ered as surrounding text. 3.2 Syntactic Analysis Syntactic analysis is split into two main stages: syntax analysis and NER. The first stage is the syntax analysis that will be processed by the MontyLingua component which is part of the ConceptNet. Every sentence in description image will be token-ized using MontyTokenize classes:  X  X okenize X  and  X  X ag_tokenize X . The class tokenize transforms input sentences into sentences without hyphenation, whereas the class tag_tokenize performs the part-of-speech (POS) tagging process according to the This/DT is/VB a/DT sentence/NN ". 
After that the sentence will be tagged with a more detail tag in the MontyTagger class and the output sentence will be like this: " (NX He/PRP NX)(VX is/VB VX) (NX the/DT mailman/NN NX) ". Only noun phrases will be considered for the next process based on the assumptions that noun phrases are the best lexical category to describe images [9]. Weights for every noun phrases are calculated using the tf  X  idf weighting scheme. 
The second stage which is the NER stage contains two sub-processes: the sentence segmentation process and pattern matching process. In sentence segmentation, noun phrases from syntax analysis are used for sentence selection. Fig 2 illustrates an ex-ample of a word matching for choosing segmented sentence. In this example, the sentence:  X  Friendly visit: Anwar talking to Mohammad Nizar in Ipoh yesterday. With them are (from left) Gopeng MP Dr Lee Boon Chye, Anwar X  X  wife Datin Seri Dr Wan Azizah Wan Ismail and Perak PKR chief Zulkifly Ibrahim  X , will be segmented into  X  X nwar talking to Mohmmad Nizar X ,  X  X ohammad Nizar in Ipoh: and  X  X openg MP Dr Lee Boon Chye X  based upon the word matching of the set { X  X bdullah X ,  X  X poh X ,  X  X nwar X  and  X  X ungai Dua X  X . 
The next process of this second stage is the pattern matching process whereby the segmented sentences are matched with NER patterns developed from the analysis of existing documents. Overall there are 142 patterns meant to extract entities such as  X  X erson X ,  X  X ocation X ,  X  X vent X ,  X  X ime X ,  X  X itle X ,  X  X rganization X  and  X  X osition X . These enti-most common information acquired in news. Table 1 lists the example of the patterns used in this study and Fig. 3 illustrates an example of the pattern matching process. 
Based on Fig 3, the input for pattern matching are the segmented sentences. The sentences will be parsed to generate the POS tags (text pre-processing). For instance, the sentence  X  Anwar talking to Mohammad Nizar  X  will be tagged and tokenized as  X (NX Anwar/NX) X ,  X (VX talking to/VX) X ,  X (NX Mohammad Nizar/NX)  X . The tagged sentence is then matched with the appropriate pattern. For this example, it will be matched with the pattern  X  X person] talking to [person]  X  and subsequently semanti-cally tagged as  X  (Anwar [PERSON]) talking to (Mohammad Nizar [PERSON])  X . For process will continue by matching the verbs with the NER patterns. For example the tokenize segmented sentence of ( NX SK St Teresa/NX) (VXalong/VX)(NX Jalan Brickfields/NX)) will resulted in the generation of  X ( SK St Teresa[LOCATION]) along (Jalan Brickfields [LOCATION])  X . 3.3 Deriving Semantic Meanings The main aim of this stage is to add additional semantic information and common sense knowledge of the indexed images using the WordNet and ConceptNet. Noun phrases from the word list are matched with the terms in WordNet. If such matched exists, the hypernyms of the terms will be included as part of the indexed terms. For instance, the phrase  X  prime minister  X  will generate semantic information such as  X  Head of State, Chief of State, Representative, and negotiator . 
As mentioned earlier, ConceptNet is used to derive common-sense knowledge or meanings of sentences. Such meanings can be derived using the topic-jisting module. To achieve this, sentences are first fragmented into verb-subject-object-object (VSOO), of which then used to derive rela ted concepts (or topics) from the Concept-Net. ConceptNet provides weights (saliency weight) based on the relevancy of the topics to the submitted sentences. These weights are based upon lightweight syntactic cues and contextual intersection. It is not realistic, however, to consider all the topics derived from the ConceptNet. Therefore, we consider only top topics, i.e. those top n topics before the weight has converged to specific  X  value. The ConceptNet topic jisting module allows terms (or topics) associated with the images (based on the de-scription or surrounding text) to be derived from a large lexical resource. Liu and Lieberman [5] refer these additional terms as common sense knowledge. Initial evaluation was conducted by comparing our proposed approach to semantic search with the conventional bag-of-words vector space model (normal search). As such only 800 images from the Malaysian on-line newspaper (under the nation cate-gory) were indexed. On average, each document consists about 85 terms. Ten queries have been articulated, and for each query the relevant images is manually identified and labeled. The constructed queries resembled information related to  X  X erson X ,  X  X o-natural language sentence such as  X  Voter in the 12th general election  X . The popular precision and recall measures [11] were used. 
We divided the evaluation into eight groups in order to assess the significant contribution of the WordNet and ConceptNet. The eight groups represent the number of different indexes of which the results are shown in Table 2. The preci-sion-recall graph is shown in Fig. 4. S represents the standard index, S CNe t and S WNe t represent index expanded from extracting concepts from ConceptNet and WordNet respectively. S NER on the other hand represent index enhanced with NER tags. Therefore S CNet + S WNet represent index semantically expanded from both Concept-Net and WordNet. Cosine similarity is used to measure the similarity between que-ries and images. During retrieval, two document indexes involved i.e. the standard index and the expanded index. Therefore, the score for each images are combined weighted values for first and second index respectively, whereby c 1 + c 2 = 1. Table 2 illustrates the average precision for all tests which show that all the semantically enhanced index capable of increasing the av erage precision of the standard index. In this case c 1 = 0.2 and c 2 = 0.8. Fig. 5 illustrates the interpolated the precision-recall indexes. The results also show that the lexical ontology ConceptNet provides better semantic meaning expansion of the terms associated with each images. Furthermore NER approach was also seen as a useful for improving the precision and recall from its capability for discriminating different named entities found in the extracted terms. Semantically searched for images based on the low level features such as color and texture are still difficult to perform and little success development has been reported. The more practical approach is to use an notations or surrounding text for capturing the semantic meanings of images and subseq uently extends such meanings to lexical ontology or resources. In this paper, we described an approach for extracting such textual meaning of on-line newspaper images from the description (unstructured an-notation) and the surrounding text. The textual meaning is then semantically enhanced by mapping them to WordNet and common-sense concepts (or topics) which are derived from the ConceptNet lexical concepts. A set of NER patterns were designed in order to identify and differentiate important named entities in images. We scope our domain to the  X  X ation X  category of Malaysian on-line newspaper. Evaluation on the initial 800 images has shown promising result, but further evaluation is definitely required with more queries, larger data sets and on different domains. 
WordNet has been the preferred lexical resource among information retrieval re-searchers for supporting semantic search and query expansion. However, not all terms can be directly associated with WordeNet synsets. While WordNet can provide struc-tured semantic meanings for some terms or concepts, images can go beyond such meanings to common-sense concepts or knowledge. Our approach has shown that how topics or common sense concepts can be derived from the ConceptNet topic_jisting module in order to enhance the semantic search. 
