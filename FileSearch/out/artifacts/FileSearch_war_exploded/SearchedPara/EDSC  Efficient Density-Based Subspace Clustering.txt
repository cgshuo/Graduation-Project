 Subspace clustering mines clusters hidden in subspaces of high-dimensional data sets. Density-based approaches have been shown to successfully mine clusters of arbitrary shape even in the presence of noise in full space clustering. Exhaus-tive search of all density-based subspace clusters, however, results in infeasible runtimes for large high-dimensional data sets. This is due to the exponential number of possible sub-space projections in addition to the high computational cost of density-based clustering.

In this paper, we propose lossless efficient detection of density-based subspace clusters. In our EDSC (efficient density-based subspace clustering) algorithm we reduce the high computational cost of density-based subspace clustering by a complete multistep filter-and-refine algorithm. Our first hy-percube filter step avoids exhaustive search of all regions in all subspaces by enclosing potentially density-based clusters in hypercubes. Our second filter step provides additional pruning based on a density monotonicity property. In the final refinement step, the exact unbiased density-based sub-space clustering result is detected. As we prove that pruning is lossless in both filter steps, we guarantee completeness of the result.

In thorough experiments on synthetic and real world data sets, we demonstrate substantial efficiency gains. Our loss-less EDSC approach outperforms existing density-based sub-space clustering algorithms by orders of magnitude. Categories and Subject Descriptors: H.2.8 Database management: Database applications [Data mining] General Terms: Management Keywords: data mining, high-dimensional data, density-based clustering, subspace clustering, efficiency
Clustering is a data mining task for summarizing data such that similar objects are grouped together while dissim-ilar ones are separated. In high-dimensional data, clusters are typically hidden by irrelevant attributes and do not show across the full space. As relevance of attributes is not glob-ally uniform for all clusters, global dimensionality reduction approaches are not adequate.

Subspace clustering detects clusters and their locally rel-evant attribute projections [3]. The idea is to mine clusters in all subspaces of the high-dimensional data space. As the number of possible subspaces is exponential in the number of dimensions, this is a computationally challenging task.
For efficient subspace clustering, grid-based discretization of the space or other lossy approximations have been pro-posed [3, 16, 18]. While these algorithms show good run-times, they lose clusters which are cut apart by the grid or missed by the approximation. Apriori subspace cluster-ing algorithms prune based on a monotonicity assumption of subspace clusters with respect to the dimensionality [3, 10]. However, the incurred dimensionality bias leads to loss of high dimensional subspace clusters [4]. Consequently, ap-proximate, grid-based, and biased algorithms all fail to de-tect all subspace clusters.

Density-based approaches define clusters as dense areas separated by sparsely populated areas and have been shown to successfully mine arbitrarily shaped clusters even in the presence of noise [7, 10]. Their runtimes, however, are sig-nificantly higher due to repeated neighborhood density com-putations, making them infeasible for many practical appli-cations [10, 11].

In this work, we propose a concept for overcoming the ex-isting trade-off between accuracy and efficiency. We present a density-based subspace clustering algorithm EDSC (effi-cient density-based subspace clustering) in a multistep filter-and-refine architecture. Substantial efficiency gains are achie-ved by two novel filter steps. The first step reduces the search space by efficiently mining candidate subspace clus-ters in enclosing hypercubes. This hypercube filter reduces the number of database scans for density computations and, via our proven hypercube monotonicity, allows for effective and lossless pruning of hypercubes in many irrelevant sub-space projections. For the second step, a novel density fil-ter that further prunes subspace cluster candidates without loss of completeness, we prove a monotonicity property for unbiased density approaches. Both filter steps significantly reduce the number of subspace cluster candidates without loss of results. The final refinement step ensures that the exact density-based subspace clustering result is found, yet with a substantially lower runtime. Our multistep approach Aalborg University, Denmark thus efficiently detects density-based subspace clusters even in large high-dimensional data spaces.

Summarizing, EDSC shows two core characteristics:
In the literature, several different clustering paradigms ex-ist. Partitioning algorithms are limited to the detection of convex clusters where the number of clusters is known in ad-vance [14, 13]. Density-based algorithms such as DBSCAN are capable of detecting arbitrarily shaped clusters and have proven to work remarkably well in noisy settings [7, 8]. For any clustering paradigm, full space clustering algorithms do not scale to high-dimensional spaces. They suffer from the  X  X urse of dimensionality X , i.e. distances grow increasingly similar with increasing dimensionality and meaningful clus-ters can no longer be detected [5].

Dimensionality reduction techniques like PCA (principle components analysis) aim at discarding irrelevant dimen-sions [9]. In many practical applications, however, no glob-ally irrelevant dimensions exist, but only locally irrelevant dimensions for each cluster are observed. Consequently, re-search has focused on clustering in subspace projections. Projected clustering computes a partition into projected clus-terings [2, 1, 15]. However, overlapping clusters in different projections cannot be detected. ENCLUS, RIS and FIRES search for subspaces which might potentially contain clus-ters [6, 11, 12]. Clusters are mined in a second step us-ing any traditional full space clustering algorithm. As there is no connection between these two steps, they suffer from costly repeated cluster computations in unnecessary projec-tions and loss of subspace clusters in other projections.
Subspace clustering mines clusters directly in arbitrary, possibly overlapping, subspaces. As the number of sub-spaces is exponential in the number of dimensions, existing algorithms trade-off efficiency for accuracy. CLIQUE uses a grid to discretize the search space [3]. Grids greatly reduce the computational complexity, yet clusters which spread across cells are missed and results are sensitive to the position of the grid. SCHISM extends CLIQUE using a variable threshold adapted to the dimensionality of subspaces [18]. Heuristics and a grid-based discretization are used for pruning. Conse-quently, clusters are lost as well as in any existing grid-based discretization.

SUBCLU extends DBSCAN to subspace clustering to over-come grid-based cluster loss [10]. Runtimes are better than for naive re-runs of DBSCAN, yet still not feasible for practi-cal applications. Moreover, the assumption of density mono-tonicity still results in losing of subspace clusters in higher dimensions. DUSC overcomes this dimensionality bias by adapting density assessment to the dimensionality of the subspace [4]. Consequently, with its unbiased density model it is capable of detecting arbitrarily shaped subspace clus-ters of any dimensionality. As density monotonicity does not hold for unbiased density, it requires novel techniques for efficient computation.
For notational convenience, we assume the following ter-minology: we mine a database DB with N objects O = N number of data objects D = { 1 , ..., d } dimensions of full space S = { k 1 , ..., k | S | } subspace with | S | dimensions A i  X  [0 , v ] dimension with value range 0 ... v O S =( o k 1 , ..., o k | S | ) object in subspace S
N S  X  neighborhood with radius  X  minP oints minimum number of objects expDen ( s ) expected density for dimensionality s F density factor threshold ( o 1 ...o d )in d attributes A i  X  [0 , v ]. A | S | -dimensional subspace is denoted by S = { k 1 , ..., k | S | } X  D = { 1 , ..., d A projection of O to S is O S =( o k 1 , ..., o k | S | ). The distance between two objects O and P in subspace S is calculated by restricting the calculation to the dimensions of the sub-space: dist S ( O, P )= i  X  S ( o i  X  p i ) 2 . Our notations are summarized in Table 1.

In density-based clustering, clusters are defined as dense areas separated by sparsely populated areas [7]. Density is evaluated for every object. Objects are dense core objects if their neighborhood, specified by an  X  -range around the object, contains more than the threshold minP oints many objects. Chains of objects which are density-connected form the actual clusters. This is illustrated in Figure 1: e.g. the objects to the right form a cluster C 4 ,aseachofthemcon-tains many objects within its neighborhood (depicted as a circle centered at one of the objects). Dense objects are con-nected to clusters to detect arbitrarily shaped clusters even in noisy settings: Definition 1. Density and Connectivity.
 An object O is dense if at least minP oints objects are in its  X  -neighborhood N  X  ( O )= { P  X  DB | dist ( O, P )  X  Two dense objects O and P are density-connected if there is a chain of dense neighboring objects between them:  X  Q 1 ...Q n  X  DB : Q 1 = P, Q n = O,  X  i : dense ( Q i )
Density-connected objects thus are elements of chains of objects which are mutually included in one another X  X  neigh-borhoods. SUBCLU extends this model to subspace cluster-ing in a straightforward manner. In each subspace, clusters must exceed the minP oints threshold within the  X  neighbor-hood. In the SCHISM, RIS or DUSC approaches [11, 18, 4], the ensuing dimensionality bias is discussed: with increasing dimensionality, distances grow and typical densities drop. Figure 1 illustrates this effect: the 2d representation of the data shows a larger spread than the 1d projections at the left and bottom. In general in high dimensional subspaces, the expected density is smaller than in low dimensional spaces. Consequently, large thresholds are almost never exceeded in high dimensional spaces, resulting in cluster loss. On the other hand, small thresholds that find high dimensional clusters produce tremendous amounts of trivial low dimen-sional subspace clusters. Thus, density should be normalized by the expected density of the dimensionality [4]. The ex-pected density is simply the average number of objects in the  X  -neighborhood w.r.t. the dimensionality. This can be computed as the ratio of the volume of the  X  -sphere to the volume of the subspace. For details please refer to [4]. Definition 2. Unbiased Density.
 An object O is dense in subspace S of dimensionality s if its  X  -neighborhood in S , N S  X  ( O )= { P  X  DB | dist S ( O, P )  X  } , contains more than minP oints objects and exceeds the expected density in this subspace by a factor F : Density in subspaces is thus normalized with respect to the dimensionality simply by adapting the threshold to the ex-pected density.

The density measure may be further refined to assign more less weight to objects further away from the center of the neighborhood range. For a monotonously decreasing weight-ing function W :IR  X  IR , density ( O )= within the neighborhood according to their distance from the object O . DUSC uses Epanechnikov kernel, whereas the SUBCLU approach corresponds to the Rectangle kernel [4, 10]. Our algorithm works for any of these weighting func-tions.

Density-based subspace clusters are defined as maximal sets of objects that are density-connected via mutual in-clusion within one another X  X  neighborhood in the subspace projection. Moreover, they should be non-redundant, i.e. if the cluster appears in some dimensionality, we exclude its lower dimensional projections (e.g. in Figure 1, keep C 1 C , but not C 2 and C 3 ,whicharecoveredby C 4 )[4]: Definition 3. Subspace Cluster.
 A set of objects C  X  DB in subspace S  X  D with | S | &gt; 1 and | C | &gt;minSize is a subspace cluster if:  X 
Thus, subspace clusters are sets of density-connected ob-jects that are maximal, i.e. all density-connected objects are grouped together. They should be non-redundant, i.e. a subspace cluster that is repeated in a lower-dimensional projection is not considered to be novel information. Addi-tionally, we exclude trivial one-dimensional clusters.
Detecting all subspace clusters in any subspace is a highly complex task. The number of possible subspaces is expo-nential in the number of attributes. Naively searching all of these subspaces is hence not feasible in most applications and pruning the search space is crucial. In this section, we propose our efficient and lossless density-based subspace clustering algorithm EDSC (efficient density-based subspace clustering).

We propose a multistep filter-and-refine architecture as illustrated in Figure 2, for efficient subspace clustering. In-stead of running a complex subspace clustering algorithm on the entire database, we restrict the search space in a chain of filter steps. Each filter step reduces the search space to a successively smaller set of candidates, i.e. sets of objects that are potential density-based subspace clusters. EDSC uses two novel filters for both complete and selective prun-ing to ensure that the overall algorithm is accurate and ef-ficient. Completeness means that the filters do not produce any false dismissals, i.e. we guarantee that all subspace clus-ters are included in the candidate sets, and that the result set of the EDSC algorithm contains all subspace clusters. Selective pruning means that the algorithm achieves a large reduction in the search space, i.e. many irrelevant sets of objects and their higher-dimensional projections can be ex-cluded from further consideration for substantial speed-up. The combination of both filters in EDSC is illustrated in Figure 3: from the fact that a candidate fails one of the two filter tests, we infer that it is not a subspace cluster in any higher dimensional subspace.

The refinement step additionally ensures that there are also no false positives, i.e. only those candidates are actually reported as results that constitute subspace clusters.
Hypercube filter. Existing density-based subspace clus-tering algorithms suffer from the fact that for each sub-space and for each object, density has to be computed. This requires repeated neighborhood scans of the database. Our first filter step thus determines hypercubes that sur-round each potential subspace cluster, thus reducing the neighborhood evaluation to significantly fewer objects. The algorithm then processes only these candidate in higher-dimensional projections and thus can efficiently reduce the exponential search space.
Density filter. In the second filter, we build on the hy-percube restriction from the first step. The basic idea is to determine for any candidate hypercube not only if it poten-tially contains a density-based subspace cluster but also if any of its higher-dimensional projections may contain one. For unbiased density (cf. Def. 2), monotonicity does not hold and thus cannot be used for lossless pruning (see Sec-tion 4.3 for details). We determine a new monotonicity prop-erty that allows for lossless pruning even with respect to this unbiased density criterion. This new property, which determines a  X  X inimum X  density over all subspaces, ensures that we may safely prune a candidate hypercube and all its higher-dimensional projections, without loss of complete-ness. Our weak density filtering precedes the final refinement step.

Refinement. Subspace clusters which fulfill the  X  X in-imum X  density criterion (pass the weak density filter) are now iteratively processed in higher-dimensional subspaces to finally determine the exact result according to the unbi-ased variable density definition. The refinement step thus removes any remaining false alarms as it uses the stricter unbiased density.
To avoid repeated scans of the entire database to deter-mine the neighborhood of objects, we enclose potential sub-space cluster regions in hypercubes.
With the novel density-conserving grid we guarantee to enclose cluster regions and thus achieve a lossless and ef-ficient pruning. The density-conserving grid is devised to meet exactly the definition of density-based subspace clus-ters. Defined as sets of density-connected objects w.r.t. their  X  -neighborhoods, subspace clusters might spread across mul-tiple grid cells. To detect these clusters, we introduce novel connectivity borders of  X  width. Subspace clusters which are density-connected across cells can be detected along the connectivity border between cells. In the algorithm, these cells are merged until completely enclosing hypercubes for each subspace cluster are found. Figure 4: Traditional vs. density-conserving grid
Figure 4 illustrates the idea: on the left side, we show a traditional grid structure. Traditional grids loose clusters that spread across several cells. For example, by simply comparing the count of objects in each cell with an exem-plary threshold of 5 objects, the dark shaded cluster at the left hand side would be missed, and only the objects in cell (2 , 2) would be reported as a cluster. Our novel density-conserving grid, illustrated at the right side, has additional borders of the size of the neighborhood,  X  . Checking these borders, our EDSC algorithm detects all potential spreads of a cluster from one cell to another and merges the corre-sponding cells to build an enclosing hypercube. Thus, the dark shaded cluster is successfully detected.

Formally, our density-conserving grid is a partitioning of the data space into regular grid cells, plus novel connectivity borders that are exactly the size of  X  -neighborhoods that are used in the definition of density-connected objects (Def. 3). Each cell is identified by the indices of the cell intervals in each dimension. Borders are additionally marked for later processing in our algorithm: Definition 4. Density-conserving grid.
 A density-conserving grid is a regular grid with connec-tivity borders: Figure 5: Density-conserving grid and hypercubes
In Figure 5 we illustrate our approach by an example of a two-dimensional space where each attribute range v is 0 to 30, and the grid resolution g is set to 6. For exam-ple, cell C 4 , 3 contains two connectivity borders B 4 , B , 3 , one in each dimension. In the example, C 4 , 3 is a 2-dimensional cell which is restricted in interval 4 for di-mension 1 and in interval 3 for dimension 2. C 6 ,  X  is a 1-dimensional cell restricted in interval 6 for dimension 1 and not constrained in dimension 2. In general ( d  X  X  S | ) stars denotes the unconstrained dimensions of the cell. Hyper-cubes consist of merged cells, given by interval ranges per dimension. H [ 2 , 3 ][ 1 , 1 ] is a 2-dimensional example hypercube. |
H cube. For example, H [ 2 , 3 ][  X  ] contains 24 objects (all objects contained in the 2nd and 3rd interval), while its constrained projection to interval one in dimension one | H [ 2 , 3 ][ 1 tains 8 objects.
This density-conserving grid is now used to efficiently con-struct hypercubes that enclose potential density-based sub-space clusters. Thus, we avoid repeated clustering on indi-vidual objects in each subspace.
The hypercube filter on this density-conserving grid works as follows: all grid cells are accessed exactly once, by pro-cessing them in lexicographic order on their interval indices (e.g. in Figure 5, C 1 , 1 , C 1 , 2 ,..., C 6 , 5 , C 6 , empty cell, its borders are checked to see whether a subspace cluster might spread across the cell borders. As the size of the border is exactly the size of the neighborhood in the density-connectivity definition, any such spread is guaran-teed to be detected by checking all borders of the cell. Only objects in the borders could connect a cluster from one cell to the next, otherwise the density-connectivity property is not fulfilled. If the borders are empty, the hypercube is maximal and encloses the entire potential subspace cluster. We denote it as MICH ( maximal induced cluster hypercube ) and continue with the next filter step. If the borders of the cell are not empty, we repeatedly merge hypercubes (de-tails in Section 4.4.2) and check borders until the MICH is maximal. As our hypercube filter checks all cells and all connectivity borders between them, EDSC is guaranteed to find all MICHs, i.e. all potential subspace clusters. Theorem 1. Completeness of hypercube filter Each subspace cluster C is enclosed by a MICH.
 Proof.
 By Definition 3, C is a set of maximal density-connected objects Q 1 ,...,Q n .
 Case 1: All density-connected objects are within one cell. Thus an index vector  X  1 ,..., X  d exists such that all objects are in the corresponding grid cell  X  i  X  X  1 ,...,n } : Q i C all objects in C .
 Case 2: All density-connected objects are within two neigh-boring cells. Thus there are two directly density-connected objects Q i and Q i +1 positioned in two neighboring grid cells. Furthermore, at least one dimension l exists, in which these cells differ, w.l.o.g.  X  l =  X  l + 1. In our grid cell notation, we have:
The necessary condition for merging in EDSC is that there is an object in the connectivity-border.
 the upper limit of the grid cell intervall p  X  l is greater than  X  . Hence the overall distance of Q i to any object in the inter-val p  X  l +1 is also larger than  X  . Formally, dist ( Q i diction to the density-connectedness of Q i and Q i +1 . C  X  1 ,..., X  d , the merge is detected when processing C  X  1 EDSC will therefore merge the cells C  X  1 ,..., X  d and C  X  to an enclosing MICH.
 Case 3: For density-connected objects within more than two neighboring cells, the argument of Case 2 for directly connected neighboring Q i , Q i +1 can be inductively extended to all objects in the density-connected set. Therefore, the entire density-connected subspace cluster Q 1 ,...,Q n is de-tected through border checks, and the enclosing MICH M C is built.

For good runtime performance it is important to prune hypercubes that cannot contain subspace clusters. We pro-pose pruning those hypercubes that do not fulfill minimum size requirements for density-based subspace clusters. As we search for clusters with at least minSize objects, mono-tonicity with respect to the number of objects allows for safe pruning. We exploit monotonicity on the number of objects in hypercubes of different subspaces. Monotonicity means that as we restrict a hypercube in more dimensions the number of objects cannot increase. Therefore, a hyper-cube which does not contain enough objects in a subspace S does not contain enough objects in any higher-dimensional subspace T ( S  X  T  X  D ), either. Thus, we may safely prune this hypercube from further consideration without losing any subspace cluster.
 Theorem 2. Hypercube monotonicity.
 For any two subspaces with S  X  T , and any hypercube H identified by interval indices [ a 1 ,b 1 ] ... [ a d ,b d objects in T is bound by the number of objects in S :
Proof. | = |{ ( o 1 ,...,o d )  X  DB | X  i  X  S : o i  X  [ w  X  a i ,w  X   X |{ ( o 1 ,...,o d )  X  DB | X  i  X  T : o i  X  [ w  X  a i ,w  X  As S  X  T holds, their are more constraints on objects in subspace T . And therefore in the hypercube H T [ a 1 ,b 1 there are at most as many objects as in H S [ a 1 ,b 1 ] ...
As we have proven in Theorem 1, each density-based sub-space cluster is enclosed by a MICH. Thus our EDSC algo-rithm may safely prune sparse hypercubes and all its higher-this is the case, we continue with this candidate. Else, we have detected a spare region and we know for sure that we
MICHs are conservative approximations of potential sub-space clusters, i.e. all subspace clusters are enclosed by one MICH, but not all MICHs contain subspace clusters. To efficiently detect those sets of objects that are not dense and thus do not form subspace clusters, we devise our den-sity filter. It prunes those subspace clusters that do not fulfill minimum density in the current and also all higher-dimensional subspaces. As we prove completeness also for our second filter, overall lossless pruning is ensured.
In fixed threshold subspace clustering (more objects in the neighborhood than minP oints ), density is monotonous with respect to the number of dimensions which is used for prun-ing as in SUBCLU [10]. Those sets of objects that are not dense in a certain subspace, may not be dense in any higher-dimensional subspace, either, and are consequently pruned. In principle, we could use the same pruning idea. However, as shown in our previous work, this fixed threshold leads to dimensionality bias, as the expected density decreases rapidly with increasing dimensionality [4]. Thus, high di-mensional subspace clusters are almost never detected. We therefore use unbiased density in Definition 2 (density larger than F times the expected density). While this definition allows for detection of clusters in arbitrary subspaces, it is no longer monotonous, as the expected density decreases with growing dimensionality. For completeness, we there-fore propose using a weaker criterion for pruning based on monotonicity of the number of objects in the neighborhood with increasing dimensionality.
 Theorem 3. Density monotonicity.
 For any two subspaces with S  X  T , and any object O the number of objects in the neighborhood in subspace T is bound by the number of objects in subspace S : Proof.
 The proof is straightforward from the definition of the neigh-borhood. It uses the fact that with more dimensions in T , the distances of objects are larger than in S : dist S ( O, P )= i  X  S ( o i  X  p i ) 2 |
N S  X  ( O ) | = |{ P  X  DB | dist S ( O, P )  X   X  }| X  Pruning with this criterion for unbiased density measures (cf. Def. 2) would violate completeness. The expected den-sity decreases with increasing dimensionality. If we were to discard a set of objects that does not exceed the F  X  expDen ( s ) threshold for a dimensionality s , it can still ex-ceed the lower threshold F  X  expDen ( s )for s  X  s . Pruning based on s alone would therefore lose clusters. To allow for lossless pruning even for algorithms with unbiased density measurements, our new  X  X eak density X  criterion is based on the lowest possible expected density in the highest dimen-sionality d .
 Definition 5. Weak Density.
 An object O  X  DB is weak dense in S if: Thus, density in the subspace with the lowest expected den-sity leads to a complete filter: Theorem 4. Completeness of density filter For all subspaces T with S  X  T  X  D and | T | = t holds:
If an object is not weak dense in S it is not dense in T .

Proof. Using monotonicity property (Theorem 3) the left side of inequality (1  X  2) is monotonically decreasing with increas-ing dimensionality. As the expected density ( expDen )is calculated as the ratio of the volume of the  X  -sphere to the volume of the subspace, expDen is monotonously de-creasing with increasing dimensionality. Hence, the sub-space of the highest dimensionality d has the lowest density: expDen ( d )  X  expDen ( t ). Thus the maximum on the right side of inequality (2) is less than or equal to the maximum in inequality (3).

Following the weak density theorem, an object can be pruned if it violates the weak density condition, as it is not dense in any higher-dimensional subspace. Hence we check any candidate that passed the hypercube filter ac-cording to the weak density criterion in the density filter. If it is no subspace cluster with respect to both minP oints and F  X  expDen ( d ), we may safely discard all its objects, as we know for sure that they do not constitute a subspace cluster with respect to unbiased density F  X  expDen ( t )and minP oints in any dimensionality t .

This concludes the discussion of the two filters in our mul-tistep EDSC approach and the completeness proof. In the next section, we discuss the overall algorithm as illustrated in Figure 2 and provide more detailed information on effi-cient handling and merging of hypercubes.
Figure 6 outlines the complete algorithm that consist of hypercube filter, density filter and refinement (Fig. 2).
Recall that each MICH encloses a potential density-based cluster (a subspace cluster candidate). Our algorithm pro-cesses each such MICH in all subspace projections. Thus, the hypercube filter (Line 1-12) recursively calls the other filter steps of our multistep algorithm for each MICH. Using the hypercube monotonicity property in Theorem 2 a MICH and all its higher-dimensional subspace projections can be pruned if the MICH does not contain at least minSize ob-jects (Line 14). If a MICH is a cluster candidate, it is sub-sequently analyzed by the density filter of the EDSC al-gorithm (Line 15) based on the weak density in Theorem 3. If the hypercube does not contain any weak density con-nected subspace cluster the corresponding hypercube and all its higher-dimensional hypercube projections can be pruned. Additionally, redundant clusters are discarded according to the last part of the subspace cluster definition in Def. 3 (Line 17). The refinement step removes any false alarms: the method ExpDensityScan performs the actual subspace clus-tering w.r.t. the expected density of the subspaces.
In this section, we give algorithmic details and an illus-trative example on how to efficiently create enclosing hyper-cubes by merging of grid cells.

The algorithm starts on cells that are merged until an enclosing hypercube for each subspace cluster is found. For efficiency reasons we process cells in lexicographical order to ensure that each cell has to be processed only once. We mark future merges w.r.t lexicographic order as induced merges ; when these marked cells are actually processed, performed merges combine the cells. Lexicographically greater cells are processed in the future, hence we denote them as future cells , and lexicographically smaller cells as past cells .An enclosing hypercube is found if no more induced merges and performed merges are necessary from the current cell.
If a density-connected subspace cluster stretches from one cell into another, the connectivity border contains at least one object. Otherwise, as we set the border width exactly to the neighborhood range  X  , objects in one cell cannot be in the  X  -neighborhood of the other (see Definition 1). When a cell is processed, the connectivity borders of the cell are checked. For each border which contains an object a merge is induced into the corresponding adjacent cell. If both con-nectivity borders contain an object a cluster might also ex-tend into the diagonal cell, i.e. adjacent to the intersection of both borders, ensuring an induced merge to this diagonal cell.

When processing a cell, first merges into future cells are induced and then merges with past cells are performed. A merge is always performed with a past cell which induced a merge into the cell. During a merge process the number of objects (the objectCounter ) and the number of merges induced into future cells (the induceCounter ) are aggre-gated. We use the concept of induced merges to indicate whether an enclosing hypercube for a density-connected re-gion is found: if all induced merges of a hypercube are per-formed ( induceCounter = 0) a MICH is found.

Checking connectivity borders guarantees that the grid does not cut density-based clusters apart and that each sub-space cluster is enclosed by a MICH (see Theorem 1).
Figure 7 illustrates the merge steps performed to iden-tify a MICH enclosing the example cluster. We start by processing each cell of dimensionality two, beginning with cell C 1 , 1 . The number of objects is determined, and if the cell is not empty, its connectivity borders are tested. Cell C , 1 is empty and hence no merge is induced. Next, cell C , 1 is processed. As both connectivity borders contain an object, cell C 2 , 1 induces a merge into C 3 , 1 , C 2 , C , 2 .Next,cell C 3 , 1 performs the merge with C 2 , 1 ,creat-C C , 2 is performed (second part of Figure 7). This merge step creates the hypercube H [ 1 , 3 ][ 1 , 2 ] . Finally, cell C processed. As C 3 , 2 was already merged with both cells, only the induceCounter is decremented. After this step, no more merges are necessary and H [ 1 , 3 ][ 1 , 2 ] corresponds to a MICH. Figure 8: Extending a MICH to next dimension
After a MICH is found the EDSC algorithm checks if the hypercube contains more than minSize objects. If the re-gion does not contain enough objects, the hypercube and all higher-dimensional projections of that hypercube are pruned (monotonicity property, Theorem 2). Thus this hypercube filter step based on merges in the density-conserving grid reduces the number of time consuming database scans for density-connected clusters and to prune the search space.
Additionally our EDSC algorithm analyzes only one MICH at a time by successively extending the MICH in all dimen-sions. When a MICH is extended to a new dimension it is iteratively restricted to each grid cell. For example a two di-mensional hypercube embedded in a four dimensional space H four. We first extended it in dimension three and restrict cedure is applied recursively. Assume the merge step mines accordingly.

For simplicity, we demonstrate the extension step using a one-dimensional MICH H [ 1 , 3 ][  X  ] (see Figure 8). After ex-tending the hypercube H [ 1 , 3 ][  X  ] to dimension two and re-stricting it to the first grid cell H [ 1 , 3 ][ 1 , 1 ] the merge pro-cedure is applied again. As the corresponding connectivity border is not empty, a merge is induced and performed di-rectly afterwards, resulting in hypercube H [ 1 , 3 ][ 1 , again corresponds to a MICH in subspace S = { 1 , 2 } as all of its connectivity borders are empty.

This concludes the discussion of our EDSC algorithm. In the following, we demonstrate the efficiency and accuracy of our approach in the experimental evaluation.
We evaluate the efficiency of the EDSC algorithm as the first lossless but also efficient approach to density-based sub-space clustering. The most recent non-approximate density-based subspace clustering algorithm SUBCLU, which ex-tends DBSCAN to subspace clustering, is used for com-parison [10]. Additionally, we contrast our approach with SCHISM, a recent efficient but approximative grid-based al-gorithm. Experiments were run on Pentium 4 machines with 2.4 Ghz and 1 GB main memory. To evaluate scalability we use synthetic data sets. Further on we show the performance of EDSC on three real world data sets.
Setup and data sets. Based on properties of real world data sets we generate synthetic data for scalability exper-iments. We extend a method proposed in SUBCLU [10] to generate density-based clusters in arbitrary subspaces. Given the subspace and the number of objects for each clus-ter, dense regions separated by noisy regions are created. Objects can belong to multiple subspace clusters, just as in most real world data sets. We generate data of different dimensionalities and hide subspace clusters with a maximal dimensionality of 80% of the data dimensionality. Four sub-space clusters are hidden in the synthetic data with two of them overlapping in 10% of their objects.
 Scalability w.r.t. dimensionality. As the number of possible subspace clusters depends exponentially on the di-mensionality of the subspace, scalability in term of dimen-sionality is crucial for any subspace clustering algorithm. As depicted in Figure 9(a), SUBCLU does not scale w.r.t. dimensionality. For the 20-dimensional data set the algo-rithm did not even finish after six days. The reason is the tremendous amount of 1 and 2-dimensional subspaces that have to be investigated with density-based clustering before processing any higher-dimensional subspace. SCHISM as a grid-based approach has better runtimes, however, it is only an approximative technique as it loses density-based clusters due to its traditional grid structure. The EDSC algorithm overcomes the scalability problems of SUBCLU due to the efficient filter steps. And in contrast to SCHISM, the EDSC approach is lossless because of its novel density-conserving grid.

Scalability w.r.t. data base size. In the next exper-iment we generate 15-dimensional data sets with different approximate 500-5000 objects. As we can see in Figure 9(b), EDSC and SCHISM scale well w.r.t. the number of objects. SUBCLU also does not scale with increasing number of ob-jects because of the time consuming data base scans needed for density-based clustering. In contrast, EDSC uses in its first filter step the novel density-conserving grid to avoid these scans and thus is nearly not influenced by the data base size.

Selectivity. To illustrate the efficiency of the hypercube filter we analyze its selectivity compared to the number of density-based scans in SUBCLU. Recall that density-based clustering has quadratic complexity w.r.t. the number of objects. Thus avoiding density-based scans contributes sig-nificantly to the performance gains of EDSC. Figure 9(c) shows the number of required density-based clustering com-putations for the data from our first experiment with vary-ing dimensionality. The highest dimensionality shown is 15-dimensional because SUBCLU does not scale to higher-dimensional data. We see that indeed the main drawback of SUBCLU is the large number of regions that have to be clustered. Multistep filtering in EDSC substantially lowers the number of density-based clustering computations. Many regions that have to be processed by SUBCLU are pruned by EDSC. Pruning is possible without any data base scans in the first filter step, only based on the information of the grid cells and the novel border elements in the new density-conserving grid.
Detecting clusters in arbitrary subspaces may require te-dious parameter tuning in some algorithms. We evaluate pa-rameter setting for SUBCLU, SCHISM and EDSC to study their parameter sensitivity and demonstrate that the EDSC algorithm is quite robust in terms of parameter settings.
Figure 9: Scalability evaluation. Parameters: gridSize = 10; minSize =5% Fixed density thresholds. As already mentioned SUB-CLU is dimensionality biased due to a fixed density thresh-old M inP oints . For different dimensional subspaces the measured density is not comparable. To find high dimen-sional subspace clusters one has to use a lower value for M inP oints in SUBCLU. In EDSC this parameter is robust because of the unbiased density approach as defined in Sec-tion 3 following the DUSC approach [4]. Figure 10(a) illus-trates that with decreasing M inP oints the runtime of SUB-CLU increases. Finding high dimensional subspace clusters is virtually infeasible because already for M inP oints =8 SUBCLU did not scale.

Variable density thresholds. As EDSC uses a variable threshold its density measure is comparable for clusters in different subspaces. In Figure 10(a) we see that the choice of M inP oints has almost no effect on the runtime of the EDSC algorithm. As we can see in Figure 10(b) the second parameter F has almost constant runtimes, too. Using a un-biased density measure which automatically adapts to the dimensionality of the subspace, the variable density thresh-old is simply set with the fixed parameter F . F reflects the factor by which the variable expected density should be ex-ceeded. Thus EDSC is easily parameterized by a constant and thus robust factor, yet adapts to the dimensionality of the subspace. Additionally the EDSC approach achieves a lossless pruning for this robust variable thresholds due to the second filter step based on the weak density criterion.
Gridsize. Grid-based algorithms like SCHISM suffer from sensitivity to the grid resolution. In Figure 10(c), runtime of SCHISM and EDSC for varying gridsizes are shown. The performance of SCHISM depends largely on the grid struc-ture not only in terms of runtime as shown in Figure 10(c), but also in terms of accuracy (discussed later on, Fig. 12(b)). This is due to the sensitivity of SCHISM to grid cell reso-lution and positioning. EDSC is robust to positioning and resolution of the grid through its grid cell merges proposed in our novel density-conserving grid.

Weighting functions. EDSC uses a weighted density measure for density assessment in the neighborhood of an object (cf. Section 3). We show that its runtime is insensi-tive to the choice of kernels for weighting. Figure 11 illus-trates the runtimes for SUBCLU and SCHISM compared to EDSC with the Rectangle (REC) W ( x ) = 1, Epanechnikov (EPA) W ( x )=1  X  x 2 and Bisquare (BIS) W ( x )=(1  X  x 2 kernels, respectively. Figure 11: Three kernels vs. SUBCLU and SCHISM
Class labeled data (pendigits, vowel and glass) from UCI machine learning repository [17] are used as ground truth to evaluate the quality of subspace clustering as in other re-cent subspace clustering approaches [1, 18]. The data cover a variety of dimensionalities and data base sizes from 9 di-mensions in glass, 10 in vowel up to 16 in pendigits, and with 214 objects in glass, 990 in vowel and 7494 in pendig-its, respectively.

Runtimes for all data sets are given in Figure 12(a), the corresponding accuracy measurements in Figure 12(b). Ac-curacy is determined as the F1-value that is commonly used in evaluation of classifiers and recently also for subspace or projected clustering as well [19, 15]. It is computed as the harmonic mean of recall ( X  X re all clusters detected? X ) and precision ( X  X re the clusters accurately detected? X ) values, respectively. The F1-value of the whole clustering is simply the average of all F1-values. The class label assigned to any detected subspace cluster is its most frequent class label.
From the F1-value results we can see that the enormous efficiency gains of the EDSC algorithm are achieved for an effective density-based model, namely DUSC [4] that outper-forms competing approaches. Thus, EDSC is an algorithm that due to its efficient but also lossless multistep approach shows significantly better runtimes for a subspace cluster-ing model of very high accuracy, in two datasets even better runtimes than the approximate SCHISM approach.
We introduced EDSC, an efficient density-based subspace clustering algorithm. EDSC uses a data base inspired mul-tistep approach which allows powerful pruning of the search space by exploiting two monotonicity properties. In our first filter step EDSC efficiently prunes the search space with-out any density-based clustering. It is based on our novel density-conserving grid which ensures complete detection of density-connected regions by enclosing hypercubes. In our second filter step EDSC ensures lossless pruning for variable density thresholds by incorporating our weak density crite-rion. For both filter steps we prove lossless pruning and thus the completeness of our density-based subspace clustering. Overall EDSC achieves efficiency without jeopardizing ac-curacy, as our multistep algorithm is capable of losslessly detecting subspace clusters with respect to unbiased density and different kernel weighting functions. Our experiments on large and high-dimensional synthetic and real world data sets show that EDSC outperforms recent subspace clustering algorithms by orders of magnitude.
This research was funded in part by the cluster of excel-lence on Ultra-high speed Mobile Information and Commu-nication (UMIC) of the DFG (German Research Foundation grant EXC 89). [1] C. Aggarwal, J. Wolf, P. Yu, C. Procopiuc, and [2] C. Aggarwal and P. Yu. Finding generalized projected [3] R. Agrawal, J. Gehrke, D. Gunopulos, and [4] I.Assent,R.Krieger,E.M  X  uller, and T. Seidl. DUSC: [5] K.Beyer,J.Goldstein,R.Ramakrishnan,and [6] C.-H. Cheng, A. W. Fu, and Y. Zhang. Entropy-based [7] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [8] A. Hinneburg and D. Keim. An efficient approach to [9] I. Joliffe. Principal Component Analysis . Springer, [10] K. Kailing, H.-P. Kriegel, and P. Kr  X  oger. [11] K. Kailing, H.-P. Kriegel, P. Kr  X  oger, and S. Wanka. [12] H.-P.Kriegel,P.Kr  X  oger, M. Renz, and S. Wurst. A [13] S. Lauritzen. The EM algorithm for graphical [14] J. MacQueen. Some methods for classification and [15] G. Moise, J. Sander, and M. Ester. P3C: A robust [16] H. Nagesh, S. Goil, and A. Choudhary. MAFIA: [17] D. Newman, S. Hettich, C. Blake, and C. Merz. UCI [18] K. Sequeira and M. Zaki. SCHISM: A new approach [19] I. Witten and E. Frank. Data Mining: Practical
