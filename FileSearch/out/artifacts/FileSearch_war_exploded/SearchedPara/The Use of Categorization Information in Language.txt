 Community Question Answering (CQA) has emerged as a popular type of service meeting a wide range of information needs. Such services enable users to ask and answer ques-tions and to access existing question-answer pairs. CQA archives contain very large volumes of valuable user-generated content and have become important information resources on the Web. To make the body of knowledge accumulated in CQA archives accessible, effective and efficient question search is required. Question search in a CQA archive aims to retrieve historical questions that are relevant to new ques-tions posed by users. This paper proposes a category-based framework for search in CQA archives. The framework embodies several new techniques that use language models to exploit categories of questions for improving question-answer search. Experiments conducted on real data from Yahoo! Answers demonstrate that the proposed techniques are effective and efficient and are capable of outperforming baseline methods significantly.
 H.2.4 [ Database Management ]: Systems X  textual data-bases ; H.3.3 [ Information Storage and Retrieval ]: In-formation Search and Retrieval; H.3.4 [ Information Stor-age and Retrieval ]: Systems and Software; H.3.5 [ Infor-mation Storage and Retrieval ]: Online Information Ser-vices X  web-based services Algorithms, Experimentation, Performance question-answering services, question search, categorization, language models, category language models Figure 1: The Category Structure of Yahoo! An-swers
Community Question Answering (CQA) services are In-ternet services that enable users to ask and answer ques-tions, as well as to search through historical question-answer pairs. Examples of such community-driven knowledge mar-ket services include Yahoo! Answers (answers.yahoo.com) 1 Naver (www.naver.com), Baidu Zhidao (zhidao.baidu.com), and WikiAnswers(wiki.answers.com).

The success of Question Answering (QA) services mo-tivates research in question-answer search where the pre-existing, historical question-answer pairs that best match a user X  X  new question are to be retrieved [3, 7, 11, 12, 24, 25], as such functionality is an essential component of a CQA service. In addition, when a user chooses to ask a new ques-tion in a CQA service, the CQA service could automatically search and display pre-existing question-answer pairs that match the new question, if any. If good matches are found, the user needs not wait for other users to answer the ques-tion, thus reducing the waiting time and improving the user satisfaction. Hence, it is important the search service offers relevant results efficiently. This paper X  X  focus is to improve question search for CQA services.

When a user asks a question in a CQA service, the user typically needs to choose a category label for the question from a predefined hierarchy of categories. Hence, each ques-tion in a CQA archive has a category label and questions in CQA services are organized into hierarchies of categories. Figure 1 shows a small part of the hierarchy of Yahoo! An-swers.
Yahoo ! Answers dominate the answer site market share in U.S. according to a study by Hitwise (http://www.hitwise.com/press-center/hitwiseHS2004/question-and-answer-websites.php)
The questions in the same category or subcategory usually relate to the same general topic. For example, the questions in the subcategory  X  X ravel.Europe.Denmark X  mainly relate to travel in the country of Denmark. Although recent work has been done on question search in CQA data, we are not aware of any such work that aims to exploit the available categorizations of questions as exemplified above for ques-tion search.

To exemplify how a categorization of questions may be exploited, consider a user who enters the following question ( q ):  X  X an you recommend sightseeing opportunities for se-nior citizens in Denmark? X  The user is interested in sightsee-ing specifically in Denmark, not in other countries. Hence, the question ( d )  X  X an you recommend sightseeing opportu-nities for senior citizens in Texas? X  and its answers are not relevant to the user X  X  question although the two questions are syntactically very similar, making it likely that exist-ing question-answer search approaches will rank question d highly among the list of returned results.

In this paper we propose a new framework for exploiting categorization information in question search, and several approaches to realizing the framework. More specifically, the categorization information will be utilized in two respects.
First, the category information of all candidate histori-cal questions can be incorporated into computing the rele-vancy score of a historical question to a query question. The idea consists of two levels. 1) Comparing the relevancy of historical questions in different categories to a query ques-tion: If words in the query question are frequent in one category (i.e., it occurs in many questions in the category) while being neglectable in other category, this indicates that the questions in the former category are more likely relevant to question q than questions in the latter. For example, recall the query question q , where word  X  X enmark X  in q is frequent in category  X  X ravel.Europe.Denmark X  but not in category  X  X ravel.US.Texas X . Suppose that other words in q are not distinguishable in the two categories. This indi-cates that questions in category  X  X ravel.Europe.Denmark X  category are more likely to be relevant. 2) Comparing the relevancy of questions within the same category: frequent words in a category are less useful to distinguish questions in a category. For example, the word  X  X enmark X  will be unimportant when we compare the relevancy of two ques-tions in  X  X ravel.Europe.Denmark X  to query q , since nearly all questions in the category are about  X  X enmark X .
Second, we make use of the results of query question classi-fication to enhance the retrieval effectiveness. One straight-forward approach is to predetermine the category of a ques-tion q using a classifier, and then to search relevant ques-tions within that category. This approach is able to improve efficiency by pruning the search space. Unfortunately, we find that not only the category of a question q , but also other categories may contain relevant questions of question q . Thus, searching only within the category of question q will miss relevant questions even if the category of q could be correctly determined.

In contrast to searching questions within the category of query question q , we can compute the probability of the question q belonging to each category. The probability can be used to adjust the relevancy score of a question d in the category to question q . Historical questions from categories with high probability should be promoted.

Additionally, we can utilize query question categorization to prune search space to improve efficiency. Specifically, for each category we compute the probability that a query ques-tion belongs to the category and we search relevant questions only within the categories with probability values larger than a threshold.

In this paper, we explore the ideas outlined above in lan-guage model based question search. More specifically, the paper X  X  contributions are twofold.

First, we propose two approaches to enhancing question search with categorization information: 1) we use a category language model to smooth a question language model to ex-plore the first idea. And 2) we integrate the classification scores returned by a classifier built with historical question data into language models to explore the second idea out-lined above. We also explore a solution built on top of both ideas. To our knowledge, this is the first work that leverages categorization information for question search.

Second, we conduct experiments with a large real data set consisting of more than 3 millions of questions from Ya-hoo! Answers to empirically elicit pertinent properties of the techniques that make up the proposed framework. Experi-mental results show that the proposed technique is capable of significantly improving the baseline language model with-out using category information for question search in terms of both effectiveness and efficiency, i.e. category information is indeed useful for question search.

The remainder of this paper is organized as follows. Sec-tion 2 details the proposed techniques. Section 3 reports on the experimental study. Section 4 reviews related works. Fi-nally, Section 5 concludes and identifies research directions.
The questions are organized into hierarchical categories in Yahoo! Answers. This section first introduces language models and then presents the proposed techniques based on language models to exploit the category information in ques-tion retrieval.
Language models have performed quite well empirically in many information retrieval tasks [18, 17, 26], and also have performed very well in question search [12]. The basic idea is to estimate a language model for each document (resp. question), and then rank documents (resp. questions) by the likelihood of the query according to the estimated model. Given a query q and a document d , the ranking function for the query likelihood language model using Jelinek-Mercer smoothing method [26] is as follows: where w is a word in the query; P ml ( w | d ) is the maximum likelihood estimate of word w in d ; P ml ( w | Coll ) is the max-imum likelihood estimate of word w in the collection Coll ; tf ( w, d ) is the frequency of word w in document d ; and  X  is the smoothing parameter.
Each question in Yahoo! Answers belongs to a leaf cat-egory. This approach is to realize the first idea discussed in introduction. In this approach, category information of historical queries is utilized such that category-specific fre-quent words will play an important role in comparing the relevancy of historical questions across categories to a query, while category-specific frequent words are less important than category-specific infrequent words in comparing the rel-evancy of questions within the same category.
 This idea can be realized by two levels of smoothing. Namely, the category language model is first smoothed with the whole question collection, and then the question lan-guage model is smoothed with the category model. We next present the two levels of smoothing model and then show why this smoothing model meets the requirements.

Given a user search question q and a candidate question d (in a QA repository), we compute the probability P ( q | d ) of how likely q could have been generated from d . P ( w | d ) will be used as the retrieval model to measure how relevant a historical question d is to query question q . To compute P ( q | d ), we need to estimate language model P ( w | d ). In this approach Equation 1 will be modified as follows.
P ( w | d ) = (1  X   X  ) P ml ( w | d ) where w is a word in question q ,  X  and  X  are two differ-ent smoothing parameters, Cat ( d ) denotes the category of historical question q , P ml ( w | d ) is the maximal likelihood estimate of word w in question d and can be computed by the maximal likelihood estimate of word w in the Cat ( d ), lihood estimate of word w in the Collection. The ranking score for candidate question d using the query likelihood language model can be computed with Equation 2. We call the approach LM+L.

We proceed to show that category-specific frequent words will play an important role in ranking the relevancy of ques-tions across different categories in this model to a query q . We can define a model P cs for  X  X een X  words that occur in the category Cat ( d ) (i.e. tf ( w, Cat ( d )) &gt; 0), and P  X  X nseen X  words that do not occur in the category Cat ( d ) (i.e. tf ( w, Cat ( d )) = 0). The probability of a query q being generated from d can be written as follows: log P ( q | d ) = = = According to the leaf smoothing model, we have:
P cs ( w | d ) =(1  X   X  ) P ml ( w | d )+
P cu ( w | d ) =  X  X P ml ( w | Coll )
From Equation 3 and the two equations in Equation 4, we get: log P ( q | d ) =
Now we can see that the second term in the right hand of Equation 5 is independent of d , and thus can be ignored in ranking. We can also see from the first term in the right hand that for questions from different categories, the larger P ml ( w | Cat ( d )), the larger P ( q | d ), i.e. the more a word w in question q occurs in a category, the higher relevancy score the questions in the category will get. Hence, the category smoothing model will play a role in differentiating questions from different categories.

We next show that category-specific frequent words are less important in comparing the relevancy of questions within the same category in this model. As in [26], we define a model P s ( w | d ) used for  X  X een X  words that occur in pre-existing question d (i.e. tf ( w, d ) &gt; 0), and a model P is used for  X  X nseen X  words that do not (i.e. tf ( w, d ) = 0). The probability of a query q can be written as follows: In the leaf smoothing model, from Equation 2 we know:
P s ( w | d ) = (1  X   X  ) P ml ( w | d )+
P u ( w | d ) =  X  [(1  X   X  ) P ml ( w | Cat ( d )) +  X P ml
From Equation 6 and the two equations in Equation 7 we get: log P ( q | d ) = + As we can see, for the questions in the same category Cat ( d ), the second term in the right hand side of Equation 8 is the same, and thus will not affect the relative ranking of them; but the first term in the right hand side will be inversely proportional to the maximal likelihood estimate of word w in question Cat ( d ) P ml ( w | Cat ( d )). Hence, the leaf smoothing plays a similar role as the well known IDF for the questions in the same category. The more frequent a word occurs in a specific category, the less important it is for searching relevant questions in that category in the model LM+L.

As a summary of the above analysis: On one hand, for questions in different categories, leaf category smoothing will enable the questions in the category which is more rel-evant to the query to gain higher relevancy scores; On the other hand, for questions in the same category, leaf category smoothing plays a similar role as IDF computed with regard to the category.

This approach is inspired by the clustering based retrieval model CBDM for document retrieval in [14, 16]. However, previous work on clustering based retrieval model does not establish the above analysis and the analysis will also pro-vide insight for the cluster based retrieval model.
One straightforward method of leveraging the classifica-tion of a query question can be done as follows: first find out the category of the query using a classification model, and then rank questions in this category using the language model in Section 2.1 to retrieve relevant questions. Specif-ically, we build a classification model using historical ques-tions to classify a query question, and we compute the proba-bility P Cat ( q | d ) which represents how likely the query ques-tion q could have been generated from the historical ques-tion d with regard to the category Cat ( d ) containing d as Equation 9. The probability is used to rank the relevancy of historical questions to query q .
 where P ( q | d ) is computed by Equation 1, and CLS ( q ) rep-resents the category of the query q determined by the clas-sifier.

The simple approach can greatly improve the efficiency of question search since it can greatly prune the search space by limiting search in a category. The number of questions in a leaf category is usually not exceeding 5% and thus search-ing in a category will be much more efficient than in the whole collection. However, as to be shown in Section 3.2.2, this simple approach is not good in terms of effectiveness even if we assume that perfect classification results could be achieved. This is because not all the relevant questions come from the same category with the category of the query question. In addition, the effectiveness of question search will highly depend on the accuracy of classifier: if the query question is not correctly classified, then the retrieval results will be poor since we will search in a wrong category.
To alleviate the aforementioned problems, we consider the probability of query q belonging to the category Cat ( d ) of a historical question d . The probability is denoted by P ( Cat ( d ) | q ). According to Equation 9, under the condition CLS ( q ) = Cat ( d ) we have P Cat ( q | d ) = P ( q | d ), and under the condition CLS ( q ) 6 = Cat ( d ) we have P Cat ( q | d ) = 0. Actually P ( Cat ( d ) | q ) represents the probability of CLS ( q ) = Cat ( d ), and thus according to the total probability formula we have: where P ( Cat ( d ) | q ) is computed by classification model (to be discussed in Section 2.6). For a query question, the clas-sification model can return the probability of the query be-longing to each category.

Equation 10 suggests a way to rank questions by com-bining the query classification probability and the language model. We call this model LM+QC.

In this model, the ranking of a historical question d will be promoted if the probability of the query question q be-longing to the category Cat ( d ) of question d is high.
The approach LM+L in Section 2.2 establishes the connec-tion of a query and a category by smoothing with category language model, while the approach LM+QC in Section 2.3 establishes the connection of a query and a category by clas-sifying the query.

This section will present a new model combining the mod-els LM+L and LM+QC. It will benefit from both the two models. That is we enhance question language model using both category language model and query question classifi-cation for question search. In this model we will compute P ( q | d ) in Equation 10 using Equation 2, finally we have the following model: P Cat ( q | d ) = P ( Cat ( d ) | q ) where P ( Cat ( d ) | q ) is the same with that in the model LM+QC. We call this model LM+LQC.
Efficiency is important in question search since the ques-tion archive of popular community QA is huge and it keeps growing. 2
The results of query question classification are used to distinguish historical questions across different categories to improve the performance in the models LM+QC and LM+LQC. Actually query classification can also help to prune the question search space and save the runtime when efficiency is a main concern. We can introduce a threshold  X  and prune the categories if the probability of q belonging to them is smaller than the threshold  X  . In other words, the dif-ferent models including the baseline LM, LM+L, LM+QC, and LM+LQC will search questions only in the categories such that the probability of q belonging to them is larger than  X  .

However, the pruning might deteriorate the effectiveness of question search while saving the runtime. The balance
The Community QA in Baidu ZhiDao has more than 62.1 million resolved questions as of Aug. 18 2009 between effectiveness and the efficiency will be evaluated in Section 3.2.4.
Hierarchical classification approach has been shown to be more efficient and usually more effective than a flat classifi-cation model[8]. We use a top-down hierarchical classifica-tion approach [8] to build a hierarchical classification model for classifying new questions. Top-down hierarchical classifi-cation approach first learns to distinguish among categories at the top level, then lower level distinctions are learned only within the appropriate top level of the tree. Given a new question q to be classified, the hierarchical classifica-tion approach will traverse the classification tree, starting from the root node of the classification model tree; at each node, it check if the probability of q belong to the category at the node is larger than a threshold  X  : if it is, it will assign a probability to each category (child node) in the current node; otherwise the subtree rooted at the node will not be traversed.

Given a question q and a category Cat , the probability of q belonging to category Cat is defined as P ( Cat | q ), and can be computed as follows: where Path ( Cat ) refers to the path from category Cat to the root in the classification tree, which satisfies P ( q | c  X ,  X  c i  X  Path ( Cat ).

Some leaf categories might not be traversed due to the threshold  X  . In the LM+QC and LM+LQC models we will assign an untraversed category the probability of its nearest ancestor which has been traversed.

The classification task is essentially a text classification problem and using bags of words as features works well in text classification [21]. Hence we treat each question as a bag of words. There has been a lot of work on question clas-sification in Question Answering, e.g. [23], which classifies questions (mainly factoid questions) into a number of cate-gories mainly based on expected answer types, e.g. number category (the expected answer is a number). Although the proposed techniques there could be adapted for our classifi-cation task, we conjecture that they may not fit well our text classification task since the classification task considered in Question Answering is different.
To obtain classification information of each query ques-tion, we employ the hierarchical top-down method using the approach in [8], and the threshold  X  parameter is set at 0.01.
We evaluate the baseline approach, the Language Model (LM) and the category based retrieval approaches, namely the Language Model with leaf category smoothing (LM+L), the Language Model with query classification (LM+QC), the Language Model enhanced with question classification (LM+LQC). We also compare with the other two models that are briefly mentioned in Section 2.3, namely search in the Top-1 category determined by classifier (LM@Top1C) using LM and search in the correct category specified by users of Yahoo! Answers (LM@OptC) using LM. We also report the results of the Vector Space Model (VSM), the Okapi Model and the Translation Model (TR) that have been used for question retrieval in the previous work [12] (we use GIZA++ 3 for training the word translation proba-bilities). Note that they are reported as references and the main purpose of this experimental study is to see whether the three proposed category based approaches can improve the performance of baseline LM, i.e., whether the category information is indeed useful for question search .
We collected questions in all categories from Yahoo! An-swers and then divided the questions randomly into two data sets. The division maintains the distributions of questions in all categories. We get a data set containing 3,116,147 questions as the training data for classification and also the question repository for question search. We also get another data set containing 127,202 questions, which is used as the test set for classification. Note that we use a much larger question repository than those used in previous work for question search; a large real data set is expected to better reflect the real application scenario of CQA services. This also explains why the training data is larger than the test data: training data should come from the question reposi-tory used for question search and we would like to use most of the data for question search. Figure 2 shows the distri-bution of questions in the training data set across first-level categories. There are 26 categories at the first level and 1263 categories at the leaf level. Each question belongs to a unique leaf-level category.

We randomly select 300 questions from the test set (127,202 questions). We remove the stop words. For each model, the top 20 retrieval results are kept. We put all the results from different models for one query question together for annota-tion. Thus annotators do not know which results are from which model. Annotators are asked to label each returned question with  X  X elevant X  or  X  X rrelevant. X  Two annotators are involved in the annotation process. If conflicts happen, a third person will make judgement for the final result. We eliminate the query questions that do not have relevant ques-tions. Finally we get 252 queries that have relevant ques-tions, and they are used as the query set . 4 We evaluate the performance of our approaches using Mean Average Precision (MAP), Mean reciprocal rank (MRR), R-Precision, and Precision@n. MAP rewards approaches returning relevant questions earlier, and also emphases the rank in returned lists. MRR gives us an idea of how far down we must look in the ranked list in order to find a rele-vant question. R-Precision is the precision after R questions have been retrieved, where R is the number of relevant ques-tions for the query. Precision@n is the fraction of the Top-n questions retrieved that are relevant. Note that the recall base for a query question consists of the relevant questions in the top 20 results from all approaches. The recall base is http://www.fjoch.com/GIZA++.html
The query set is available in http://homepages.inf.ed.ac.uk/gcong/qa/. needed to compute some of the metrics that we used. Fi-nally, we use the tool trec eval from TREC [1] to compute the different metrics.

We measure the performance of hierarchical classifier us-ing Micro. F1-score, which is appropriate in the presence of large-scale categories. The Micro. F1-score is calculated by averaging the F1 scores 5 over all decisions. In our experiments, we need two smoothing parameters. Table 1 shows the results on a small data set, which contains 20 queries, when we vary both parameters to determine an optimal value in terms of MAP using LM+L. Finally we set  X  to 0.2 and  X  to 0.2. It is also shown in [26] that 0.2 is a good choice for the parameter  X  . In the Okapi Model, we follow the work presented in [20] to select the parameter.
In this section, we report the results of different question search approaches on different metrics. We also study the effect of pruning question search space on both efficiency and effectiveness. Before reporting question search results, we first present classification results.
The classification results provide indispensable informa-tion for the approaches utilizing question classification, in-cluding LM+QC, LM+LQC, and LM@Top1C. Note that approach LM+L does not need question classification. Ta-ble 3 gives the Micro. F1-scores for both the hierarchical classification model and the flat classification model. We do not see significant performance improvements of hierarchical classification over flat classification . However, we note that the classification time of hierarchical classification model is usually only about 40% of the time of the flat classification. The approaches LM+QC and LM+LQC use not only the Top-1 returned category, but also other returned categories. Even if the correct category is not the Top-1 returned cat-egory, our question search approach may still benefit from http://en.wikipedia.org/wiki/F score
Table 3: Performance of two classification models classification information if the correct category is contained in the Top-n returned categories. In order to see if the cor-rect category is contained in the Top-n returned categories, we compute the percentage of test query questions whose correct categories are contained in the Top-n categories re-turned by the classifier. We call the percentage X  X uccess@n. X  It is shown in Figure 3. We can see that the correct cate-gories of about 75% questions are in the Top-10 categories returned by the classifier Figure 3: Success@n: the percentage of questions whose correct categories are contained in Top-n re-turned categories by the classifier
Table 2 shows the performance of different models in terms of different metrics, including MAP, R-Precision, MRR, and Precision@n. Each column of Table 2 corresponds to an approach.

As shown in Table 2, LM+QC has better performance than the baseline method. LM+L and LM+LQC (utiliz-ing category information) achieve statistically significant im-provement (p-value &lt; 0.05) compared with the baseline model LM in terms of all the metrics. This clearly shows that the category information indeed is able to improve the perfor-mance of question search. We can also see in Table 2 that compared with LM) baseline approach LM outperforms VSM and Okapi, and performs slightly worse than TR.

Among the three approaches (LM@Top1C, LM@OptC, and LM+QC) leveraging classification results, LM+QC per-forms better than the other two. We can see that LM@Top1C performs very poorly. This is because this model searches relevant questions only from the questions in the Top-1 cate-gory determined by the classifier and highly depends on the reliability of classification results. However, LM@OptC is also not good, although it is better than LM@Top1C. This indicates that even if we could have a perfect classifier, the approach LM@Top1C still cannot achieve much better re-sults. The reason is that besides the category containing the query question, other categories also contain relevant questions and this model misses the relevant questions in other categories. LM+QC performs much better because it uses classification probabilities to adjust the ranking of questions and finds relevant questions in all the categories not only in the Top-1 category.

The LM+L model significantly outperforms the baseline approach LM. It is also shown that LM+L is more effec-tive than LM+QC in our experiments. This indicates that smoothing by category information helps more than query categorization in our models. The LM+LQC model outper-forms all the other approaches on all metrics. Recall that LM+LQC enhances the language model by combining the LM+L and LM+QC models. We also note that LM+LQC only slightly improves on LM+L and LM+QC. This is per-haps because both LM+L and LM+QC utilize the category information, although in different ways, and it could not gain much from the combination.

In Table 2 we also give the results for TransLM. It per-forms well, but is still worse than LM+L and LM+LQC because it does not consider the category information.
We scrutinize the results to understand the effect of cat-egory information on the performance of our models. Be-cause LM+LQC is a combination of the LM+L model and the LM+QC model, we only analyze the results of the latter two models. In this section we determine whether our mod-els outperform the baseline LM for a specific query based on the metric MAP. The analysis based on other matrics will be qualitatively comparable.

As discussed in Section 2.2, the LM+L model improves the baseline model LM in two respects. First leaf smooth-ing in the LM+L model enables the category-specific fre-quent words to play a more important role in distinguishing questions across categories. It will promote the rankings of historic questions from categories that are relevant to a query question. We find that about 90 query questions (out of 252) benefit from this. Second, leaf smoothing makes the rankings of questions from the same category more reason-able since it plays a role like IDF in distinguishing questions within a specific category. About 85 query questions bene-fit from this. Note that the performance of some queries is improved due to both of the two improvements.

We also notice that the LM+L performs worse than the baseline model LM on 45 queries. We investigate these cases and find the following reasons. Note that the performance of some queries is affected due to more than one reason. 1) Relevant questions come from the categories whose top-ics are not very relevant to query questions. The LM+L model will demote the rankings of the questions from  X  X on-relevant categories X ; thus, if those categories contain relevant questions, the performance becomes worse than the base-line. One reason for the problem is that a question may be submitted to a wrong category by the Yahoo! users. An-other reason is that many categories have an  X  X ther X  sub-category that may contain relevant questions, but such cat-egories cover various topics and are not quite relevant to the queries, and thus LM+L fails. We find that about 10 queries are affected by this. 2) The overlapping of categories leads to the worse per-formance of LM+L. Some queries may be contained in mul-tiple categories. For example, the query question  X  X ow many hours from portland,or to hawaii air time? X  is relevant to either  X  X ravel.United States.Honolulu X  or  X  X ravel.United States.Portland X . In our data set, the relevant question is contained in  X  X ravel.United States.Portland X , but LM+L ranks questions from X  X ravel.United States.Honolulu X  X igher. About 15 queries are affected by this. 3) Although the leaf smoothing usually helps to rank ques-tions from the same category since it plays a role like IDF computed with regard to a category, it may also lead to poor performance on some queries. For some queries, the smooth-ing with regard to the whole collection better describes the importance of words than the category. Hence, the smooth-ing in the baseline LM, which plays a role like IDF with regard to the whole collection, may perform better than the leaf smoothing. We find that about 25 queries are affected by this.

We proceed to analyze the results of LM+QC. This model benefits from the query classification. From Equation 10, we can see that the rankings of historical questions from cate-gories with high classification probability scores will be pro-moted. To further see if the promotion is useful, we compute some statistics as follows. For each query, we rank all the categories according to classification results and then count the number of relevant questions in every rank. We next ag-gregate the number of relevant questions in each rank across all queries and compute the percentage of relevant questions in each rank. Table 6 gives the results. It shows that about 86% relevant questions (returned by LM, the baseline model) come from the top-5 ranks. Hence it is reasonable for the LM+QC model to promote the rankings of questions from top-ranked categories. In the query set, about 120 queries have been improved by query classification. However for some queries that have obvious category features, most of the top-ranked historical questions in the results of LM are already from the correct category, and thus LM+QC fails to improve the retrieval.
 Table 6: Distribution of relevant questions in differ-ent ranks of category (LM results) LM+QC performs worse than the baseline for 52 queries. The reasons are as follows: 1) Query question classification errors. The performance of LM+QC relies on the accuracy of query classification. When a more relevant category has a relative low probability classification score, LM+QC will perform poorly. We notice that for some queries, LM+QC performs even worse than the baseline when the correct category of the query question is not contained in its Top-10 classification results. 2) The second reason is the same as the first reason for the failure of the LM+L model just analyzed. The LM+QC model also performs worse than baseline LM when relevant questions come from  X  X on-relevant X  categories.

To get a better understanding of our models, Table 4 gives part of the results of an example query question  X  X at is the best way to talk my mom into letting me get a snake??? X  that is originally in the category  X  X ets.Reptiles X . The ques-tions in bold are labeled as  X  X elevant. X  In the LM+L model, the category-specific frequent word  X  X nake X  makes the cate-gory  X  X eptiles X  and  X  X ther-Pets X  more relevant to the query through the leaf smoothing. Hence the rankings of the ques-tions in the two categories are promoted (from 12th to 7th and 13th to 8th, respectively). In the LM+QC model the category  X  X eptiles X  and  X  X ther-Pets X  are the Top-2 cate-gories judged by the classifier, and thus the rankings of ques-tions in the two categories are promoted by the LM+QC model (from 12th to 1st and 13th to 3rd, respectively).
In addition, recall that the leaf smoothing in the LM+L model also improves the results by enabling the rankings of questions from the same category more reasonable. To illustrate this, Table 5 gives an example for the query  X  X ow can I trim my parakeets beak? X  that is in the category  X  X ets.Birds X . In this category, word  X  X arakeets X  occurs 276 times and  X  X rim X  occurs only 5 times, and as a result, the word  X  X rim X  becomes more important than  X  X arakeets X  for the ranking of questions within this category. However, in the view of the whole collection, the word X  X arakeets X  X s more important than  X  X rim X  since it appears in fewer questions. This is why LM+L promotes the ranking of the question  X  X ow to Trim a Bird X  X  Beak ? X  compared with the baseline model LM and LM+QC (from 2nd to 1st).
In addition to retrieval effectiveness, efficiency is also very important considering that the sizes of CQA repositories are huge and keep increasing. In Section 2.5 we proposed a method of utilizing query classification to improve the ef-ficiency by introducing a threshold  X  . We prune categories that are less likely to contain relevant questions based on the probability scores returned by the classifier. This can greatly save running time of question search, though it might dete-riorate the effectiveness of question search. Note that the classification time is very little compared with the question search time.

We next evaluate the effect of threshold pruning using models LM, LM+L, LM+QC, and LM+LQC on both effec-tiveness and efficiency. Considering the probability value of the leaf categories returned by the classifier, we vary pruning threshold  X  from 10  X  7 to 10  X  1 . For all the four models, the runtime speed-up compared to the runtime of the baseline Figure 4: Relative running time of different mod-els using different pruning thresholds compared with the baseline model LM model LM is shown in Figure 4, and the MAP results are shown in Figure 5.

From Figure 4 and Figure 5, we can see that the pruning deteriorates the retrieval effectiveness while it can greatly improve the efficiency of all the four models. For example, the saving is about 85% when threshold  X  is set at 0.1 for all of them. As the threshold becomes smaller, the MAP results become better while the savings in runtime become less. However, even when the threshold  X  is set at 10  X  7 LM+LQC, the runtime is still 39% of the original runtime of the baseline while the MAP value is 0.455. LM+LQC with pruning still achieves a better MAP result than LM+QC without pruning, and it exhibits significant improvements over LM in terms of MAP, MRR, R-Precision, and P5.
The baseline model LM is the most efficient, and LM+QC and LM+L are a bit slower than LM because LM+QC needs to multiply the query classification results of a category and because LM+L needs one more level smoothing. LM+LQC is the slowest model because it needs not only to multiply the query classification results, but also the leaf smoothing. But it performs the best for all the threshold values. LM+L and LM+QC also perform better than the baseline model LM.

Hence, the threshold-based pruning offers us an option to make a compromise between the effectiveness and the efficiency. When efficiency is not the main concern, we can simply ignore the parameter.

As a summary, the experimental results show that the cat-egory information can be utilized to significantly improve the performance of question search in the proposed mod-els, LM+L and LM+LQC. Moreover, we can greatly save running time by pruning the search space.
Question Search. There has been a host of work on question search. Most of the existing work focuses on ad-dressing the word mismatching problem between user ques-tions and the questions in a QA archive. Burke et al. [4] combine lexical similarity and semantic similarity between questions to rank FAQs, where the lexical similarity is com-puted using a vector space model and the semantic sim-ilarity is computed based on WordNet. Berger et al. [2] study several statistical approaches to bridge the lexical gap Figure 5: Performance on MAP using different pruning thresholds for FAQ retrieval. Jijkoun and Rijke [13] use supervised learning methods to extract QA pairs from FAQ pages, and use a vector space model for question-answer pair retrieval. Riezler et al. [19] provide an effective translation model for question search; their translation model is trained on a large amount of data extracted from FAQ pages on the Web. Sori-cut and Brill [22] use one million FAQs collected from the Web to train their answer passage retrieval system.
Next, question search has recently been revisited on CQA data. Jeon et al. [11, 12] compare four different retrieval methods, i.e., vector space model, Okapi, language model, and translation-based model, for question search on CQA data. In subsequent work [25], they propose a translation-based language model, and also exploit answers for question search. Duan et al. [7] build a structure tree for questions in a category of Yahoo! Answers and then find important phrases in questions that are given higher weight in question matching. Bian et al. [3] propose a learning framework for factual information retrieval in CQA data. The approach needs training data, which is unfortunately difficult to get. Wang et al. [24] parse the questions into syntactic trees and use the similarity of the syntactic trees of the query question and the historical question to rank historical questions.
In contrast to all the previous work, our question search approaches exploit the question categories in CQA data, where all questions are organized into a hierarchy of cat-egories. To the best of our knowledge, no previous work attempts to utilize category information to improve ques-tion search, although this appears to be a natural idea.
Cluster-based document retrieval. In cluster-based retrieval, documents are grouped into clusters, and an a list of documents is returned based on the clusters that they come from. We can roughly classify cluster-based retrieval into two groups. The first group of approaches retrieve one or more clusters in their entirety in response to a query. For example, in early work [10], entire documents are clustered, and clusters are retrieved based on how well their centroids match a given query. In the second group, ranking scores of individual documents in a cluster are computed against a query, e.g., [6, 9, 14, 16]. No existing cluster-based retrieval approaches have been applied to question search.
 Classification for document retrieval. Chekuri and Raghavan [5] introduce the idea of utilizing classification for document retrieval. However, their focus is on the automatic classification of Web documents into high-level categories of the Yahoo! taxonomy; they do not consider how to lever-age the classification results for document retrieval. Lam et al. [15] develop an automatic text categorization approach to classify both documents and queries, and investigate the application of text categorization to text retrieval. Our ap-proaches are very different from the approaches in existing work for document retrieval. Question search is an essential component in Community Question Answer (CQA) services. In this paper, we propose a new framework that is capable of exploiting classifications of questions in CQA archives for improving question search. In doing so, the framework uses specific language models. It uses a local smoothing technique to smooth a question language model with category language models to exploit the characteristics of the categories. It also incorporates a technique that computes the probabilities of a user question belonging to each existing category and integrates the prob-abilities into the langauge model. Experiments conducted on a large QA data set from Yahoo! Answers demonstrate the effectiveness of the proposed framework.

Several promising directions for future work exist. First, it is of relevance to apply and evaluate other question search approaches, e.g., translation models (see the coverage of re-lated work for more approaches), in the proposed framework. Such approaches can be integrated into the framework. Sec-ond, it is relevant to evaluate the efficiency of the proposed approach more comprehensively. Third, additional empiri-cal studies of the performance of question search across cat-egories are in order. Fourth, we believe that it may be possi-ble to optimize the classification performance by combining or splitting categories so that similar subcategories are com-bined and incoherent categories are split. Finally, we believe it is interesting to consider answers in our framework. We would like to thank the anonymous reviewers for their constructive suggestions. [1] TREC . http://trec.nist.gov/. [2] A. L. Berger, R. Caruana, D. Cohn, D. Freitag, and [3] J. Bian, Y. Liu, E. Agichtein, and H. Zha. Finding the [4] R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. [5] C. Chekuri, M. H. Goldwasser, P. Raghavan, and [6] F. Diaz. Regularizing ad hoc retrieval scores. In [7] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching [8] S. T. Dumais and H. Chen. Hierarchical classification [9] M. A. Hearst and J. O. Pedersen. Reexamining the [10] N. Jardine and C. van Rijsbergen. The use of [11] J. Jeon, W. B. Croft, and J. H. Lee. Finding [12] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [13] V. Jijkoun and M. de Rijke. Retrieving answers from [14] O. Kurland and L. Lee. Corpus structure, language [15] W. Lam, M. E. Ruiz, and P. Srinivasan. Automatic [16] X. Liu and W. B. Croft. Cluster-based retrieval using [17] D. Petkova and W. B. Croft. Hierarchical language [18] J. M. Ponte and W. B. Croft. A language modeling [19] S. Riezler, A. Vasserman, I. Tsochantaridis, V. O. [20] S. Robertson, S. Walker, S. Jones, [21] F. Sebastiani. Machine learning in automated text [22] R. Soricut and E. Brill. Automatic question [23] E. M. Voorhees. Overview of the trec 2001 question [24] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree [25] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [26] C. Zhai and J. D. Lafferty. A study of smoothing
