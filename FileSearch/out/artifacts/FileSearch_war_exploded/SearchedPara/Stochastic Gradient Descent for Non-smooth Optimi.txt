 Ohad Shamir ohadsh@microsoft.com Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA Tong Zhang tzhang@stat.rutgers.edu This paper considers one of the simplest and most popular stochastic optimization algorithms, namely essentially non-smooth , even at the optimal solution. In general, for machine learning applications F may be non-smooth whenever one uses a non-smooth loss function, and thus a smoothness-based analysis is not appropriate.
 Without assuming smoothness, most of the existing analysis has been carried out in the context of online learning -a more difficult setting than our stochas-tic setting, where the subgradients are assumed to be provided by an adversary. Using online-to-batch con-version, it is possible to show that after T iterations, the average of the iterates, ( w 1 + ... + w T ) /T , has O (log( T ) /T ) optimization error for strongly-convex F (see precise definition in Sec. 2), and O (1 / for general convex F (Zinkevich, 2003; Hazan et al., 2007; Hazan &amp; Kale, 2011). However, (Rakhlin et al., 2011) showed that simple averaging is provably sub-optimal in a stochastic setting. Instead, they pro-posed averaging the last  X T iterates of SGD (where  X   X  (0 , 1), e.g. 1 / 2), and showed that this averag-ing scheme has an optimal O (1 /T ) convergence rate. In comparison, in the non-smooth setting, there are  X (1 / strongly-convex problems, respectively (Agarwal et al., 2012).
 These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate w T , often works quite well (e.g. (Shalev-Shwartz et al., 2011)). Un-less F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in (Rakhlin et al., 2011) only im-ply an O (1 / w
T with strongly-convex functions, and we are not aware of any results for the last iterate w T in the gen-eral convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where  X  g t = g t ), we do not know any existing results that can guarantee the performance of each in-dividual iterate w T . Second, the theoretically optimal suffix-averaging scheme proposed in (Rakhlin et al., 2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iter-ates w 1 ,..., w T in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick , but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in (Rakhlin et al., 2011).
 where  X  &gt; 0. For a general convex function, the above inequality can always be satisfied with  X  = 0.
 As discussed in the introduction, we consider the first-order stochastic optimization setting, where instead of having direct access to F , we only have access to an oracle, which given some w  X  W , returns a ran-dom vector  X  g such that E [  X  g ]  X   X  X  ( w ). Our goal is to use a bounded number T of oracle calls, and com-pute some  X  w  X  W such that the optimization error, that this framework can be applied to learning prob-lems (see for instance (Shalev-Shwartz et al., 2009)): given a hypothesis class W and a set of T i.i.d. ex-amples, we wish to find a predictor w whose expected loss F ( w ) is close to optimal over W . Since the ex-amples are chosen i.i.d., the subgradient of the loss function with respect to any individual example can be shown to be an unbiased estimate of a subgradient of F . We will mostly consider bounds on the expected error (over the oracle X  X  and algorithm X  X  randomness) for simplicity, although it is possible to obtain high-probability bounds in some cases.
 In terms of the step-size  X  t in the strongly-convex case, we will generally assume it equals 1 / (  X t ). We note that this is without much loss of generality, since if the step size is c/ X t for some c  X  1, then it is equivalent to tak-ing step sizes 1 / (  X  0 t ) where  X  0 :=  X /c  X   X  is a lower-bound on the strong convexity parameter. Since any  X  -strongly convex function is also  X  0 -strongly convex, we can use the analysis here to get upper bounds in terms of  X  0 , and if so desired, substitute  X /c instead of  X  in the final bound.
 When we run SGD, we let  X  g t denote the random vec-tor obtained at round t (when we query at w t ), and let g t = E [  X  g t ] denote the underlying subgradient of F . To facilitate our convergence bounds, we assume that E [ k  X  g t k 2 ]  X  G 2 for some fixed G . Also, when opti-mizing general convex functions, we will assume that the diameter of W , namely sup w , w 0  X  X  k w  X  w 0 k , is bounded by some constant D . We begin by considering the case of strongly convex F , and prove the following bound on the expected error of any individual iterate w T . In this theorem as well as later ones, we did not attempt to optimize constants. Theorem 1. Suppose F is  X  -strongly convex, and that E [ k  X  g t k 2 ]  X  G 2 for all t . Consider SGD with step sizes By the definition of S k and the inequality above, we have k
E [ S k  X  1 ] = ( k + 1) E [ S k ]  X  E [ F ( w T  X  k )]  X  ( k + 1) E [ S k ]  X  E [ S k ] + and dividing by k , implies
E Using this inequality repeatedly and summing from k = 1 to k = b T/ 2 c , we have It now just remains to bound these terms. E [ S T/ 2 ] is the expected average value of the last b T/ 2 c iterates, which was already analyzed in ((Rakhlin et al., 2011), Theorem 5), yielding a bound of for T &gt; 1. Moreover, we have P b T/ 2 c k =1 (1 /k )  X  1 + log( T/ 2). Finally, we have The result follows by substituting the above bounds into Eq. (4) and simplifying for readability.
 Using a similar technique, we can also get an individual iterate bound, in the case of a general convex function F that may be non-smooth. We note that a similar technique was used in (Zhang, 2004), but for a different algorithm (one with constant learning rate), and the result was less explicit.
 Theorem 2. Suppose that F is convex, and that for some constants D,G , it holds that E [ k  X  g t k ]  X  G 2 for all t , and sup w , w 0  X  X  k w  X  w 0 k  X  D . Consider SGD with step sizes  X  t = c/ Then for any T &gt; 1 , it holds that and upper bounding the norms by D , it is easy to calculate that
E [ S T  X  1 ]  X  F ( w  X  ) = Also, we have P T  X  1 k =1 1 /k  X  (1 + log( T )). Plugging these upper bounds into Eq. (6) and simplifying for readability, we get the required bound. The bounds shown in the previous section imply that individual iterates w T have O (log( T ) /T ) expected er-ror in the strongly convex case, and O (log( T ) / expected error in the convex case. These bounds are close but not the same as the minimax optimal rates, which are O (1 /T ) and O (1 / section, we consider averaging schemes, which rather than return individual iterates, return some weighted combination of all iterates w 1 ,..., w T , attaining the minimax optimal rates. We mainly focus here on the strongly-convex case, since simple averaging of all iter-ates is already known to be optimal (up to constants) in the general convex case.
 We first examine the case of  X  -suffix averaging , defined as the average of the last  X T iterates (where  X   X  (0 , 1) is a constant, and  X T is assumed to be an integer): In (Rakhlin et al., 2011), it was shown that this av-eraging scheme results in an optimization error of O ((1 + log( 1 T , but increases rapidly as we make  X  smaller. The following theorem shows a tighter upper bound of O (log( 1 more flexible in choosing  X  . Besides being of indepen-dent interest, we will re-use this result in our proofs later on.
 Theorem 3. Under the conditions of Thm. 1, and F ( w  X  )] is at most Proof. Suppose first that  X T  X  b T/ 2 c . The proof is mostly identical to that of Thm. 1, except that instead in advance. For example, if we do 1 / 2-suffix averag-ing, we need to  X  X now X  when we got to iterate T/ 2 and should start averaging. In practice, the stopping time T is often not known in advance and is deter-mined empirically (e.g. till satisfactory performance is obtained). One way to handle this is to decide in advance on a fixed schedule of stopping times T suffix-averages only for those times. However, this is still not very flexible. In contrast, maintaining the av-erage of all iterates up to time t can be done on-the-fly: we initialize  X  w 1 = w 1 , and for any t &gt; 1, we let Unfortunately, returning the average of all iterates as in Eq. (9) is provably suboptimal and can harm per-formance (Rakhlin et al., 2011). Alternatively, we can easily maintain and return the current iterate w t , but we only have a suboptimal O (log( t ) /t ) bound for it. In the following, we analyze a new and very simple running average scheme, denoted as polynomial-decay averaging , and show that it combines the best of both worlds: it can easily be computed on the fly, and it gives an optimal rate. It is parameterized by a number  X   X  0, which should be thought of as a small constant (e.g.  X  = 3), and the procedure is defined as follows:  X  w 1 = w 1 , and for any t &gt; 1, For  X  = 0, this is exactly standard averaging (see Eq. (9)), whereas  X  &gt; 0 reduces the weight of ear-lier iterates compared to later ones. Moreover,  X  w  X  t can be computed on-the-fly, just as easily as computing a standard average.
 We note that after this paper was accepted for publi-cation, a similar averaging scheme was independently proposed and studied in (Lacoste-Julien et al., 2012). Compared to our method, they consider a slightly dif-ferent step-size and a specific choice of  X  = 1, using a more direct proof technique tailored to this case. An analysis of our averaging scheme is provided in the theorem below.
 Theorem 4. Suppose F is  X  -strongly convex, and that E [ k  X  g t k 2 ]  X  G 2 for all t . Consider SGD initialized with w 1 and step-sizes  X  t = 1 / X t . Also, let  X   X  1 be an integer. Then E [ F ( w  X  T )  X  F ( w  X  )] is at most 58 1 + this bound and substituting in Eq. (11), we obtain F 0 (  X  w  X   X   X   X  where and
B = and Therefore we have Plugging this estimate into Eq. (13) and simplifying, we obtain an upper bound on E [ F ( w  X  T )  X  F ( w  X  )] of the form 17 1 + It remains to treat the case  X  = 1. In that case, the upper bound on  X  t  X   X  t  X  1 in Eq. (12) becomes and using the same derivation as before, we get that ran suffix averaging with  X  = 1 / 2, and simple aver-aging of all iterates. The results are reported in the figure below. Each graph is a log-log plot representing the training error on one dataset over 10 repetitions, as a function of the number of iterations. We also ex-perimented on the test set provided with each dataset, but omit the results as they are very similar.
 The graphs below clearly indicate that polynomial-decay averaging work quite well. Achieving the best or almost-best performance in all cases. Suffix averag-ing performs performs similarly, although as discussed earlier, it is not as amenable to on-the-fly computa-tion. Compared to these schemes, a simple average of all iterates is significantly suboptimal, matching the results of (Rakhlin et al., 2011). In this paper, we investigated the convergence behav-ior of SGD, and the averaging schemes required to ob-tain optimal performance. In particular, we consid-ered polynomial-decay averaging, which is as simple to compute as standard averaging of all iterates, but attains better performance theoretically and in prac-tice. We also extended the existing analysis of SGD by providing new finite-sample bounds on individual SGD iterates, which hold without any smoothness assump-tions, for both convex and strongly-convex problems. Finally, we provided new bounds for suffix averaging. While we focused on standard gradient descent, our techniques can be extended to the more general mir-ror descent framework and non-Euclidean norms.
 An important open question is whether the O (log( T ) /T ) rate we obtained on the individual iterate w T , for strongly-convex problems, is tight. This question is important, because running SGD for T iterations, and returning the last iterate w T , is a very common heuristic. If the O (log( T ) /T ) bound is tight, it means practitioners should not return the last iterate, since better O (1 /T ) rates can be obtained by suffix averaging or polynomial-decay averaging. Alternatively, a O (1 /T ) bound on the last iterate can indicate that returning the last iterate is indeed justified. For a further discussion of this, see (Shamir, 2012). Another question is whether high-probability versions of our individual iterate bounds (Thm. 1 and Thm. 2) can be obtained, especially in the strongly-convex case. Again, this question has practical implications, since if a high-probability bound does not hold, it might imply that the last iterate can suffer from high variability, and should be used with caution. Finally, the tightness of Thm. 2 Agarwal, A., Bartlett, P., Ravikumar, P., and Wain-wright, M. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimiza-tion. IEEE Transactions on Information Theory , 58 (5):3235 X 3249, 2012.
 Bach, F. and Moulines, E. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In NIPS , 2011.
 Hazan, E. and Kale, S. Beyond the regret minimiza-tion barrier: An optimal algorithm for stochastic strongly-convex optimization. In COLT , 2011.
 Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-gret algorithms for online convex optimization. Ma-chine Learning , 69(2-3):169 X 192, 2007.
 Kushner, H. and Yin, G. Stochastic Approxima-tion and Recursive Algorithms and Applications . Springer, 2nd edition, 2003.
 Lacoste-Julien, S., Schmidt, M., and Bach, F. A sim-pler approach to obtaining an o(1/t) convergence rate for projected stochastic subgradient descent. CoRR , abs/1212.2002, 2012.
 Ouyang, H. and Gray, A. Stochastic smoothing for nonsmooth minimizations: Accelerating sgd by ex-ploiting structure. In ICML , 2012.
 Rakhlin, A., Shamir, O., and Sridharan, K. Mak-ing gradient descent optimal for strongly convex stochastic optimization. CoRR , abs/1109.5647, 2011.
 Shalev-Shwartz, S., Shamir, O., Srebro, N., and Srid-haran, K. Stochastic convex optimization. In COLT , 2009.
 Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter,
A. Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming , 127(1):3 X 30, 2011.
 Shamir, O. Is averaging needed for strongly convex stochastic gradient descent? Open problem pre-sented at COLT, 2012.
 Zhang, T. Solving large scale linear prediction prob-lems using stochastic gradient descent algorithms. In ICML , 2004.
 Zinkevich, M. Online convex programming and gener-
