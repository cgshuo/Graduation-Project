 Songtao Guo  X  Xintao Wu  X  Yingjiu Li Abstract Additive randomization has been a primary tool for hiding sensitive private information. Previous work empirically showed that individual data values can be approxi-mately reconstructed from the perturbed values, using spectral filtering techniques. This poses a serious threat of privacy breaches. In this paper we conduct a theoretical study on how the an upper bound for the reconstruction error using matrix perturbation theory. Attackers who use spectral filtering techniques to estimate the true data values may leverage this bound to determine how close their estimates are to the original data. We then derive a lower bound for the reconstruction error, which can help data owners decide how much noise should be added to satisfy a given threshold of the tolerated privacy breach.
 Keywords Privacy preserving  X  Spectral filtering  X  Disclosure analysis  X  Error bound analysis 1 Introduction Randomization has been a primary tool to hide sensitive private information. The random perturbation techniques aim to distort the sensitive individual values while allowing estima-tion of the underlying distribution parameters. For all randomization based approaches, there are two fundamentally conflicting requirements: privacy for the individual data values and utility of the perturbed data values.

Consider a data set U with m records of n attributes and a noise data set V with same dimensions as U . The random value perturbation techniques generate a perturbed data matrix  X  U = U + V .Let  X  U denote an estimate of the original data which users (or attackers) can achieve. To preserve utility, certain aggregate characteristics (i.e., mean and covariance matrices for numerical data, or marginal totals in contingency table for categorical data) of U should remain basically unchanged in the perturbed data  X  U or can be restored from the reconstructed data  X  U . In other words, distributions of U can be approximately reconstructed from the perturbed data  X  U using distribution reconstruction approaches (e.g., [ 4 , 3 ]) when some a-priori knowledge (e.g., distribution, statistics etc.) about the noise V is available.
To preserve privacy, not only the difference between  X  U and U butalsothatbetween  X  U and U should be greater than some tolerated threshold. Here we follow the tradition of using the difference as a measure to quantify how much privacy is preserved. A key element in preserving privacy and confidentiality of sensitive data is the ability to evaluate the extent of all potential disclosures for the released data. In other words, we need to answer to what extent the confidential information in the perturbed data can be compromised by attackers. Hence, we should consider not only the perturbed data,  X  U , which is released directly, but also the reconstructed data,  X  U , which attackers may exploit various reconstruction methods to obtain.
 The reconstruction methods investigated in Agrawal and Agrawal [ 3 ], and Agrawal and Srikant [ 4 ] only focused on how to reconstruct the distribution of the original data from the perturbed data but did not consider the issue that attackers may reconstruct the indi-vidual data values through various means. The authors, in [ 17 ], argued that randomization schemes might not be secure as attackers may apply a random matrix based spectral filtering technique to retrieve original data values from the perturbed data. Recently, Huang et al. in [ 15 ], investigated a similar method based on the principal component analysis (PCA), which exploits correlations among attributes to reconstruct original data values. Their results show that accurate individual data values can be estimated from the perturbed data.

The previous work in [ 15 , 17 , 18 ] exploited spectral properties of the data and showed that the noise may be separated from the perturbed data and, as a result, privacy could be seriously compromised. Although they empirically assessed effects of the perturbation on the accuracy of the reconstructed individual values, one major question is what the explicit form of the relation between the reconstruction error and the noise may exist. In other words, what bounds of the reconstruction error can be achieved by attackers using spectral filtering based techniques.

In this paper we theoretically explore the problem which originates from the usage of additive noise for privacy preservation. We explicitly assess effects of perturbation on the accuracy of the reconstructed values and give an explicit relation on how the reconstruction error  X  U  X  U varies with the additive noise V . We bound the reconstruction error and the perturbations in terms of matrix norms. In particular, we first derive an upper bound for the tering techniques to estimate true data values may leverage this bound to determine how close their estimates are to the original data. We then derive a lower bound for the reconstruction error, which can help data owners decide how much noise should be added to satisfy a given threshold of tolerated privacy breach. Since the traditional matrix perturbation theory [ 21 ] mainly focused on how the eigenvalues and the angle between eigenvectors (or invariance subspaces) of a perturbed matrix  X  A are upper bounded by the perturbation, we cannot borrow their results directly to derive the lower bound. We present a singular value decomposition (SVD) based reconstruction method and derive a lower bound for the reconstruction error. Since the spectral filtering based approach is equivalent to the SVD based approach, as a result the achieved lower bound of SVD based approach can also be considered as the lower bound of the spectral filtering based approach.
The rest of this paper is organized as follows. In Sect. 2 we give definitions of matrix norms and introduce some preliminary background on matrix perturbation theory which will be used in our analysis. In Sect. 3 we revisit the spectral filtering reconstruction methods and present measures used in this paper for both privacy and utility. In Sect. 4 we present in detail the formal analysis on the upper bound and the lower bound with different types of additive noise. We present experimental results in Sect. 5 . Finally we discuss the related work in Sect. 6 and offer our concluding remarks in Sect. 7 . 2 Preliminaries We use the tilde conventions to denote perturbations and use the hat conventions to denote estimations. A symbol with a tilde (or hat) over it always denotes a perturbed (or estimated) quantity. The original quantity is denoted by the same symbol without a tilde or hat. Specifi-to matrices. For instance,  X  A = A + E denotes a perturbation of A .Let ( A ) ={  X  1 ,..., X  n } be the eigenvalues of A and let [ e 1 ,..., e n ] be their corresponding eigenvectors, where  X  1  X   X  2  X   X  X  X   X   X  n . Similarly, let ( respectively. Table 1 summarizes our notations used in this paper.
 Definition 1 Let A  X  R m  X  n . The Frobenius norm of A is the number The 2-norm of A is where x 2 is for the 2-norm (Euclidean norm) of a vector.

Definition 1 shows the mathematical form of the Frobenius norm and the 2-norm, which will be used in this paper. We will cast much of our analysis in terms of absolute and relative errors of the Frobenius norm, instead of point-wise bounds. The use of absolute and relative errors in the form of the Frobenius norm gives perturbation bounds a simplicity that makes them easier to interpret. Basically, the Frobenius norm is used to measure the magnitude of data values in total while the 2-norm is used to denote the largest singular value of a matrix.
We list some properties of matrix norms which will be used in our proofs as below. Refer to linear algebra books (e.g., [ 21 ]) for more details. 1. AB F  X  A F B F and AB 2  X  A 2 B 2 ,when A  X  R m  X  n and B  X  R n  X  q . 2. A 2  X  A F  X  3. A 2 =  X  max ( A T A ) , the square root of the largest eigenvalue of A T A . 4. if A is symmetric, then A 2 =  X  max ( A ) , the largest eigenvalue of A . 3 Spectral analysis of reconstruction methods 3.1 Spectral filtering revisited Consider a noise matrix V with same dimensions as U . The random value perturbation techniques generate a perturbed data matrix  X  U = U + V . The objective of the spectral filtering based approach is to derive the estimation  X  U of U from the perturbed data  X  U based on random matrix theory. An explicit filtering procedure is shown below. 1. Calculate the covariance matrix of  X  U by  X  A =  X  U T  X  U . 2. Since the covariance matrix is symmetric and positive semi-definite, we apply spectral 3. Derive information of the eigenvalues from the covariance matrix of the noise V . 4. Extract the first k components of  X  A as the principal components by comparing  X   X  i with 5. Obtain the estimated data set using  X  U =  X  UP  X   X  .

The authors, in [ 17 ], focused on the scenario where only a small number of instances exists in the data set. Furthermore, the noise matrix V considered in [ 17 ] is generated using one i.i.d. Gaussian distribution with zero mean and known variance. Based on the random matrix theory, we can derive the theoretical bounds of the eigenvalues corresponding to the noise matrix V as  X  V min =  X  2 ( 1  X  1 / to the ratio between the number of records and the number of attributes. As in most data mining applications, the number of records far exceeds that of attributes (hence Q is large), we can see  X  V min  X   X  V max  X   X  2 =  X  V . In this paper, we focus on scenarios with a large number of instances in data sets. 3.2 Other methods Several similar reconstruction methods have also been investigated. For example, a PCA based reconstruction method was investigated in [ 15 ] and a SVD based one was investigated in [ 14 ]. Since the SVD based reconstruction method can help to derive the lower bound of reconstruction error using the well-known Mirsky Theorem [ 21 ], we briefly present the SVD method and show the equivalence relationship between the spectral filtering and SVD based method.

Singular Value Decomposition decomposes a matrix U  X  R m  X  n (say m  X  n ) into the product of two unitary matrices, L  X  R m  X  m , R  X  R n  X  n , and a pseudo-diagonal matrix D = diag ( d 1 ,..., d n )  X  R m  X  n , such that U = LDR T or U = n elements d i of D are referred to as singular values , which are, by convention, sorted in descending order: d 1  X  d 2  X   X  X  X   X  d n  X  0. The columns l i and r i of L and R are, respectively, called the left and right singular vectors of U . Similarly let  X  U = U + V be left (right) singular vectors.
 as that from SVD  X  U SVD =  X  L k  X  D k  X  R T k .
 Proof We prove these two methods are equivalent. Since  X  R k = R I k 0 ,
Since the columns of right singular vectors (  X  R ) are the eigenvectors of  X  U T  X  U ,thatis  X  Q =  X  R .Then
The non-zero singular values for U are precisely the square roots of the non-zero eigen-values of the positive semi-definite matrix UU T , and these are precisely the square roots of the non-zero eigenvalues of U T U . Furthermore, the columns of L are eigenvectors of UU T and the columns of R are eigenvectors of U T U .

We can observe that all spectral based methods reconstruct the original data by projecting the perturbed data onto the projection subspaces which are determined by the first k eigen-vectors for the spectral filtering method or by the first k singular vectors for the SVD method. In Sect. 4 , we shall discuss the strategies of determining k . 3.3 Quantification of privacy and utility All the above spectral filtering based methods aim to reconstruct individual data directly. use a measure that defines privacy as follows: if the original value can be estimated with c of privacy at c confidence level. Since spectral based methods can recover individual data, attackers tend to use the reconstructed data value as an estimate of the original one. Definition 2 The absolute error of  X  U , which is regarded as an estimate of U ,isdefinedas If U F = 0, then the relative error of  X  U is defined as
The use of absolute and relative errors in terms of the Frobenius norm gives the evaluation of the perturbation a simplicity that makes them easier to interpret. In the remainder of this paper, we shall cast much of our bound analysis in terms of Frobenius norm with measures defined in Definition 2 and briefly discuss how to derive a probabilistic upper error bound for a tuple (row) of the data set in Sect. 4.1 .

The utility of the data, at the end of the privacy preserving process, is another important issue. The measure used to evaluate the utility usually depends on the specific data mining techniques with respect to which a privacy algorithm is performed. For example, for a clas-sification problem, the authors in [ 4 ] measure the inaccuracy in distribution reconstruction by examining the effects on the misclassification rate. In this paper, we apply the universal information loss defined in [ 3 ] as the metric for utility loss since the specific data mining task may be unknown. We know the more the noises are made to the data, the less the data reflects the domain of interest. Therefore, an evaluation parameter for the data utility can be the amount of information that is lost after the application of privacy preserving process.
Given the perturbed data, it is (in general) not possible to reconstruct the original density the lack of precision in estimating f U ( x ) in terms of distribution.

Note that the applied metric is universal in the sense that it can be applied to any re- X  f construction while with one as no overlap between the original density distribution and the reconstructed one. As spectral based methods have reconstructed individual data, we can estimate the density distributions f U and  X  f U by using multi-dimensional histogram on the original U and the reconstructed  X  U . 4 Bound analysis As the previous work in [ 15 , 17 ] only empirically assesses the effects of perturbation on the accuracy of the estimated individual value, in this section, we explore the explicit relation between  X  U  X  U and the noise V and give the upper and lower bounds of  X  U  X  U F in terms of V F . 4.1 Upper bound analysis The traditional matrix perturbation theory [ 21 ] focuses on how the perturbation E affects the matrix A . Specifically, it provides precise upper bounds on the eigenvalues, the angle between terms of the norms of the perturbation matrix E . In our scenario, A is the derived covariance matrix of the original data U while E is the derived perturbation on A caused by V . Hence, it is more significant to consider how the primary perturbation V affects the data matrix U rather than how the derived perturbation E affects the covariance matrix A .

Since the difference between the estimated data and the original one is determined by subspaces.
 Proposition 1 Let A  X  R n  X  n be a symmetric positive definite matrix, and let  X  1  X   X  2  X  X matrix [ XY ] X  R n  X  n is orthogonal and unitary. Given a perturbation E , let  X  A = A + E and Define eigengap  X  =  X  k  X   X  k + 1 . There exists a matrix P satisfying so that the columns of  X  X = ( X + YP ) form an orthonormal basis for the subspace spanned by the first k eigenvectors of  X  A Proof See Appendix.

The difference between the invariant subspace of the original data and that of the perturbed data shown in Eq. 2 depends on the eigengap  X  =  X  k  X   X  k + 1 whichisdeterminedbythe spectrum of the original data. Note that the spectrum of the original data is unknown to attackers. In the following, we show how to estimate this eigengap using the spectrums of the perturbed data and the noise.
 Proposition 2 Given a symmetric matrix A  X  R n  X  n and a symmetric perturbation E , let  X  A Proof From Corollary 4.9 in [ 21 ], we have: Since  X  =  X  k  X   X  k + 1 , Result 2 (Upper bound) Given a data set U  X  R m  X  n and a perturbation noise set V  X  R m  X  n , let  X  U = U + V and  X  U to denote the estimate obtained from the spectral filtering technique. We have where E = V T U + U T V + V T V is the derived perturbation on covariance matrix A = U T U . Proof Consider the covariance matrix of  X  U :
We denote  X  A as A + E where E = V T U + U T V + V T V .Since hence we have,
The upper bound given in Result 2 determines how close the estimated data achieved by attackers is to the original one when the spectral filtering technique is exploited. This represents a serious threat of privacy breaches as attackers know exactly how close their estimates are.

From Eq. 4 , we can observe that both the eigen gap  X   X  and the projection space P  X  depend on the determination of the number of principal components k . The original spectral filtering algorithm [ 17 ] suggested the following strategy to determine the first k eigen components. Strategy 1 k = max { i |  X   X  i  X   X  V } where  X  V denotes the largest eigenvalue of noise 1 .
Strategy 1 aims to include all significant eigen components (with  X  i &gt; 0) in the projection not be considered as a principal component.

The noise considered in the additive perturbation can be either independent or correlated with the original data. In the following, we show the upper bound of the reconstruction error for different cases.
 Corollary 1 When the noise is completely correlated with the original data, the upper bound of the reconstruction error can be expressed as When the noise is independent with the original data, the upper bound of the reconstruction error can be expressed as Proof When the noise is completely correlated with the original data, we have VP  X  F  X  V F as k represents the number of principal components. Then Eq. 4 becomes Eq. 6 . When the data and noise are uncorrelated, we have V T U = U T V = 0. Hence E = V T U + U T V + V T V can be simplified as E = V T V . In terms of Frobenius norm, we have E F = V T V F  X  V 2 F . By replacing E F with V 2 F in Eq. 4 ,wehaveEq. 7 .

In [ 17 ], the noise is assumed as following one i.i.d. Gaussian distribution N ( 0 , ) ,where the covariance matrix = diag ( X  2 ,..., X  2 ) . This represents the scenario where the noise is completely independent with original data. One example of this scenario is the online collection of customer X  X  individual data (as other customers X  data is unknown during the data collection). For i.i.d. noise, we have the following result.
 Corollary 2 When the noise is generated by an i.i.d. Gaussian distribution with zero mean and known variance  X  2 , the upper bound of the reconstruction error can be expressed as where || V || F = Proof When the noise matrix is generated by an i.i.d. Gaussian distribution with zero mean and known variance  X  2 , the square error of VP  X  is  X  2 =  X  2 k n [ 15 ]and || V || F is We have
Furthermore, when the noise is generated by an i.i.d. Gaussian distribution,  X  1  X   X  n .In other words,  X  E =  X  1  X   X  n is close to zero. By replacing VP  X  F and  X  E ,Eq. 7 becomes Eq. 8 .
 Upper bounds given in Eqs. 4 , 7 ,and 8 interpret privacy loss at the aggregate level. Attackers may be interested in exploring the error bound for each individual tuple. From Eq. 5 ,wecanhave where v i denotes the added noise on the u i . By incorporating Eq. 2 ,wehave Similarly we can derive the error bounds at the tuple level for the completely correlated noise and the i.i.d. Gaussian distributed noise, respectively. However, one problem here is that v i in Eq. 9 is not available to attackers. The Frobenius norm of this vector, as a function with n variables, represents the vector length in the Hilbert space. Therefore, the distribution of such function may be derived or approximated from the distribution of the noise. As a result, a probabilistic bound of || v i || F based on its corresponding distribution can be obtained by attackers. By replacing the || v i || F with the upper probabilistic bound, we can derive the probabilistic error bound for an individual tuple.

Strategy 1 can be generally used to determine the number of eigen components in the projection space. However, it cannot guarantee to achieve an optimal result. The reason is that it aims to include all significant eigen components (with  X  i &gt; 0) in the projection space for reconstruction. However, since the inclusion of one eigen component also brings eigen component may be diminished by the side effect due to the additional noise projected on this eigenvector.

For i.i.d noise, since the effect of the projection on any vector is the same, we can propose a better strategy (as shown in Strategy 2) which can achieve an optimal estimation. Strategy 2 only includes the i th eigen component when the benefit due to inclusion of the i th component the noise is independent to the data and also has no correlation among the noise, we have  X   X  =  X  i +  X  V  X  2  X  V .
 optimal by using k = max { i |  X   X  i  X  2  X  V } .
 Proof See Appendix for proof details. 4.2 Lower bound analysis In Sect. 3.2 , we have presented the SVD based reconstruction method and shown the equi-valence between the SVD based method and the spectral filtering method. Hence the derived lower bound from SVD based method can also be considered as the lower bound of the spectral filtering method. Recall that the SVD based reconstruction method simply estimates In this section, we derive a lower bound using the well-known Mirsky Theorem for SVD decomposition [ 21 ].
 data set U . The reconstruction error between  X  U and U has its lower bound: where U k = L k D k R k .
 Proof  X  U and U are matrices of the same dimensions with singular values Since  X  U k =  X  L k  X  D k  X  R T k ,weset By Mirsky X  X  theorem [ 21 ]
The relationship between the reconstruction error and perturbation (especially the lower bound) will, in turn, guide us to add noise into the original data set. The lower bound gives data owners the worst case security assurance since it is bounded by any matrix B of rank no greater than k derived by attackers. In order to preserve privacy, data owners need to perturbation.

Based on the derived lower bound,
Hence k which might be chosen by attackers can be determined by
For i.i.d. noise, based on strategy 2,  X  i  X   X  V , the data owner should generate V such that the eigenvalue of ( V T V ) satisfies where m is the number of rows in V . 5 Experimental results 5.1 Experiment setting In our experiment, we use two data sets. The first one is an artificial dataset, as specified similarly in [ 17 ]. We increase the size of instances from 300 to 30,000 since we focus on the scenario with a large number of instances in data sets. To better compare the difference of reconstruction error between Strategy 1 and 2, we add two more features which are independent with the previous four features. Specifically, U is a highly correlated data set with 35 variables which are generated from 6 independent features. Each feature has a specific trend like sinusoidal, square, or triangular shape and there is no dependency between any and 32,561 instances. To better illustrate the performance of the reconstruction method with range. In this paper, we consider three different types of additive noise.  X  Type 1. V is an additive noise following one i.i.d. Gaussian distribution N ( 0 , ) ,where  X  Type 2. V is an additive noise following one Gaussian distribution N ( 0 , ) ,wherethe  X  Type 3. V is an additive noise following Gaussian distribution N ( 0 , ) ,wherethecova-
Type 1 represents the scenario where the noise is completely independent with the original data. Type 2 represents the scenario where the variance of the original data is a-priori known while type 3 represents the scenario where the whole covariance matrix of the original data is used to generate noise. Note that in all the above three scenarios, we assume that the noise is generated with a Gaussian distribution and its associated mean vector is zero. This assumption is generally true in privacy preserving data mining applications as the change of the mean values will significantly affect the accuracy of data mining results.
In our following experiments, we perturb the original data by different levels of noise, which are generated by varying the covariance matrix . For each level, we keep the same noise-to-signal ratio ( V F / U F ). For type 1, based on || V || F  X   X  2 = V F / we can derive
For each perturbed data, we use our spectral filtering technique with Strategy 1 and 2 to reconstruct the point-wise data, respectively. We also show how the reconstruction error is affected by varying k , the number of eigen components included in the projection space. We use the relative error re ( U ,  X  U ) = data set is very sparse (35 features), we cannot accurately derive its density distribution by applying the multi-dimensional histogram technique. Here we only give evaluations on the utility loss for the Adult data set (6 features). 5.2 Artificial data set Table 2 shows our experimental results on the relative error re ( U ,  X  U ) with three types of additive noise (type 1, 2, and 3 noises) for the artificial data set. 5.2.1 Effect of varying k For i.i.d. noise, we have presented two strategies on how to determine k by examining the eigenvalues of the covariance matrix of the perturbed data and the eigenvalues of the There are two phases of reconstruction error changes when we increase k . We take column V1 from 0.821 to 0.260 when more principal components are included in reconstruction. This is because that the gain of inclusion of significant principal components are greater than the loss due to the inclusion of noise projected on those components. In the second phase (i.e., k  X  6), component (component with small eigenvalue or insignificant component) is diminished by the loss due to the inclusion of noise on that component. When we examine the original data, there exist 6 principal components as the data is highly correlated among 35 features. Since the added noise V1 is relatively small, both strategies incur the same reconstruction error by incorporating all six principal components. 5.2.2 Effect of varying noise In the next experiment, we vary the variance of the added noise from 0.213 (V1) to 4.814 (V9) as shown in Table 2 . We denote the values with  X  as the results following Strategy 2, while the values with  X  as the results following Strategy 1. For each noise data set, we also show all the relative reconstruction errors by varying k values. In each column, the value in bold font highlights the best result which could be achieved by comparing k reconstruction errors.
From Table 2 , we can see that Strategy 2 can achieve optimal results (least reconstruction error) for all perturbations from V1 to V9 while Strategy 1 suffers when relative large pertur-bations are added. The reason is that Strategy 1 always include all six principal components in the projection space across all nine noise data sets. On the contrary, Strategy 2 compares the magnitude of the principal components with the magnitude of additive noise to determine k . For example, the best k value for noise V4 is four as shown in Table 2 . We can observe that the magnitudes of the last two principal components are not as significant as those of noise projected on the corresponding components. Hence, the gain of inclusion of the last two (not very significant) principal components is diminished by the loss due to the inclusion of noise projected on those components.

Quality of the data reconstruction depends upon the relative noise contained in the pertur-bed data. As the noise added to the actual value increases, the reconstruction error increases. Figure 1 shows point-wise data distributions of reconstruction for feature two (we get a sample of 300 data records) when we vary noise levels. We can see when the noise-to-signal ratio V F / U F is 0.628, (the corresponding variance  X  2 = 0 . 213), the spectral filtering can achieve relatively accurate estimates because the effects due to the noise projection on the remaining 29 components are safely filtered. When we increase the noise-to-signal ratio to 1.366 (the corresponding noise variance is  X  2 = 1 . 007), the reconstruction error increases as shown in Fig. 1 (b). The reasons are twofold. First, much larger noise exists in the projec-tion space. Second, information contained in those principal components which are excluded from the projection space is lost since the large noise tends to affect the determination of k . 5.2.3 Effect of different types of noise Since Strategy 2 is not designed for scenarios (type 2 and 3 noises) with correlated noises, we only show the reconstruction errors of Strategy 1 in the second and third blocks of Table 2 . For type 2, the spectral filtering with Strategy 1 can achieve the optimal estimates. However, it generally cannot achieve good results for type 3 where the covariance matrix of the noise is linear with the covariance matrix of the signal. As the noise is not randomly generated, the spectral filtering technique, which is based on random matrix perturbation, cannot satisfactorily separate noise from data as they share the same distribution pattern. Figure 2 compares the reconstruction error for one single feature (attribute 2) in three cases. Each case has different type of the noise, however, with the same magnitude ( V F / U F = 0 . 628). We can see the spectral filtering performs best for the completely random perturbation (type 1) while performs worst for the completely correlated perturbation (type 3). 5.3 Adult data set Table 3 shows our experimental results on the relative error re ( U ,  X  U ) with three types of additive noise (type 1, 2, and 3) for the Adult data set. The values with  X  denote the results following Strategy 2, while the values with  X  denote the results following Strategy 1. The bold values indicate those best estimates achieved by the spectral filtering technique. We have similar observations as those on the previous artificial data set. For example, Strategy 2 can always achieve optimal estimates for the i.i.d. noise (type 1) while Strategy 1 usually incurs more inaccuracies since it tends to include all principal components (6 in this data set) without considering the side effect incurred by the inclusion of noise. We can also observe that the more noise we add, the greater the reconstruction error. This observation is held across all three types of noises.
 To measure the utility, we apply the universal information loss I ( f U ,  X  f U ) as defined in of the corresponding reconstructed data, we equally divide each dimension into 5 bins and compare the multidimensional histograms based on the frequency information contained in those 5 6 six-dimensional bins.

Table 4 shows our results on the utility loss of the reconstructed Adult data with different levels of type 1 noise (V1 X  X 9). The values with  X  denote the results following Strategy 2, while the values with  X  denote the results following Strategy 1. The bold values indicate those best estimates achieved by the spectral filtering technique. From Table 4 , we can observe that Strategy 2 always outperforms Strategy 1 in terms of utility preservation. Note that the 1 as no overlap between the original density distribution and the reconstructed one. Another when the magnitude of added noise increases.
To further evaluate how different types of noise (type 1, 2, and 3) affect the utility of the reconstructed data. We show one result on the relationship between the utility verses varying three types of noises in Fig. 3 . We can observe that the spectral filtering best preserves (completely correlated). This is because the completely correlated noise cannot be well filtered out by the spectral based reconstruction method although some statistical properties (e.g., the covariance matrix) can be fully preserved. 6 Related work The field of statistical databases has developed various perturbation based methods to prevent the disclosure of confidential individual data while satisfying requests for aggregate infor-mation. The perturbation family includes swapping values between records, replacing the original database by a sample from the same distribution, adding noise to the values in the are various approaches to assess risk of identity disclosure and most of them relate to the enough for many general environments which contain many numerical attributes. In most statistical database literatures, the privacy concerned is about the re-identification of some specific entries in the database.

A considerable amount of work on privacy preserving data mining has been reported in recent years. The random noise addition methods have been well investigated (e.g., [ 4 , 3 , 10 , 15 , 17 , 18 ]). The objective of randomization based privacy-preserving data mining is to prevent the disclosure of confidential individual values while preserving general patterns and rules. Agrawal and Srikant [ 4 ] proposed the development of data mining techniques that schema is to add a random number v i which is drawn from some known distribution, to u i , data mining. They also show an approach to recovering the distribution of the original data given the distribution of random noises and apply this approach for decision tree learning. Agrawal and Agrawal [ 3 ] have provided an expectation-maximization (EM) algorithm for reconstructing the distribution of the original data from perturbed observations. They provide informationtheoreticmeasurestoquantifytheamountofprivacyprovidedbyarandomization approach. Some recent work [ 3 , 13 , 16 ] has explored whether the reconstructed distribution or data mining results can be exploited by attackers to breach individual privacy.
The authors in [ 11 , 14 , 15 , 17 , 18 ] have investigated point-wise reconstruction methods (spectral filtering, PCA based, and SVD based), which may be exploited by attackers to breach individual privacy. Those techniques investigated how correlations among attributes affect the privacy of a data set disguised via the additive random perturbation scheme. In [ 15 ], Huang et.al. also introduced a Bayes approach based on maximum a posteriori (MAP) estimation, which considers both priori and posterior knowledge via Bayes X  theorem to estimate original data. However, strong assumptions are imposed on this method such as the original data and the noise are multi-variate normal distributed and both distributions are available to attackers.

In addition to the previous additive noise perturbation approach, the random rotation based perturbation approach ( Y = RX where R is an orthogonormal random matrix) has recently been investigated in [ 6 , 20 ]. The idea is to preserve the multidimensional geometric properties (vector length, inner products and distance between a pair of vectors) by perturbing the original data set through geometric rotation transformation. Hence, data mining results on the rotated data can achieve perfect accuracy. However, one important issue is that this approach is also subject to some specific attacks (e.g., a-priori knowledge PCA based attack [ 19 ] and ICA based attack [ 12 ]).

Thecondensationbasedperturbationapproach[ 2 ]aimsatpreservingthecovariancematrix for multiple columns by partitioning the original data into k -record groups and regenerating a set of k records to approximately preserve the distribution and covariance. This approach will notsignificantlysacrificetheaccuracyofdataminingresultsobtainedfromtheperturbeddata. However, since the difference between the regenerated records and their nearest neighbor in original data are very small, the original data records can be estimated from the perturbed data with high confidence [ 6 ].

There have been active researches on defining the right privacy measure. However, the problemhowtoquantifyandevaluatethetradeoffsbetweenmodelaccuracyandprivacyisstill open [ 8 , 9 ]. Paper [ 3 ] suggests to measure privacy using Shannon X  X  information theory. The average amount of information in the non-randomized attribute X depends on its distribution and is measured by its differential entropy. The average amount of information that remains in X after the randomized attribute Z is disclosed can be measured by the conditional differential entropy. The average information loss for X that occurs by disclosing Z can be measured in terms of the difference between the two entropies. The notion of privacy breaches captures rare disclosures. The problem with the definition of privacy breaches from [ 10 ]isthatwe have to specify which properties are privacy-sensitive, whose probabilities must be kept below breach level. In this paper, we use F-norm to quantify the relative amount of noise added to actual data. 7 Conclusion and future work Spectral filtering based techniques have recently been investigated as a major means of point-wise data reconstruction [ 15 , 17 , 18 ]. It was empirically shown that those techniques may be exploited by attackers to breach the privacy protection offered by the additive randomization based privacy preserving data mining methods. This paper presented a theoretical study on evaluating privacy breaches when the spectral filtering techniques are applied. We gave an explicit upper bound of the reconstruction error. This upper bound may be exploited by attackers to determine how close their estimates are to the original data using the spectral filtering techniques. We also derived an explicit lower bound of the reconstruction error. This lower bound can help users determine how much and what kind of noise should be added when a tolerated privacy breach threshold is given. We empirically evaluated the trade-off between privacy preservation and utility loss using one artificial data set and one real data set with three types of additive noise. Our findings showed that the i.i.d. additive noise can be mostly filtered out by the spectral filtering based techniques when strong correlations exist in the original data. As a result, individual privacy may be compromised. Another of the universal information measure although it can better preserve individual privacy. In the future we will explore how other types of additive noise(e.g., generated from distributions like uniform, Poisson, etc.) affect the performance of spectral filtering based techniques. We are also interested in exploring the relationship between the accuracy of data mining results and the universal information loss.
 Appendix: Proof Lemma 1 Let A  X  R n  X  n be a symmetric positive definite matrix, and let  X  1  X   X  2  X   X  X  X  e ] , Y =[ e k + 1  X  X  X  e n ] so that the matrix [ XY ] X  R n  X  n is orthogonal and unitary. Given a perturbation E, let  X  A = A + E, =|| E || F &gt; 1 / 2 , and define  X  =  X  k  X   X  k + 1 .
If  X &gt; 2  X  X = X + Y P form an orthonormal basis for the subspace spanned by the first k eigenvectors of  X  A,  X  e 1 ,  X  e 2 ,...,  X  e k .
 Proof Since A is a symmetric positive definite matrix, we can apply spectral decomposition on A: where L 1 = diag ( X  1 ,..., X  k ) ,and L 2 = diag ( X  k + 1 ,..., X  n ) . Also, let From Theorem V.2.8 of [ 21 ], there exists a matrix P satisfying Since [X Y] is unitary, =|| E || F , it holds true that have Hence, so that the columns of  X  X = ( X + YP ) form an orthonormal basis for the subspace spanned by three eigenvectors of  X  A . The representation of  X  A with respect to  X  X is The eigenvalues associated with these k eigenvectors are the eigenvalues of  X  L 1 ,andthe eigenvalues associate with the rest of  X  A  X  X  eigenvectors are the eigenvalues of all (strictly) greater than the eigenvalues of  X  L 2 .
 Since  X &gt; 2 Similarly, we can derive Then we have By the same argument, we also have Since the Forbenius norm upper bounds the Spectral norm, this also implies The spectral variation of  X  L 1 with respect to L 1 is The spectral variation of  X  L 2 with respect to L 2 is From Corollary IV.3.4 of [ 21 ]: The above conditions ensure that those eigenvalues of  X  L 1 lie in the interval [  X  k  X   X  2 ] , and those of  X  L 2 lie in the interval [  X  n  X  so we have
Let M = P T P ,then || M || F  X || P || 2 F  X  2 2  X   X  2 &lt; 1, where  X   X  =  X   X 
Accordingto[ 21 , pp. 232], we can derive: Proof of Result 3 In the spectral filtering method, when we select the first k components, the error matrix can be expressed as
Similarly, when we select the first k + 1 components, the error matrix becomes ThelasttwopartsinEq. 16 aretheprojectionsofnoiseanddataonthe ( k + 1 ) theigenvector. When the magnitude of the added noise is small compared with that of the original data, we have  X  e i  X  e i . The strength of the data projection can be approximated as
For i.i.d noise, the effect of the projection on any vector should be the same. Thus,
Hence, we include the i th component only when  X  i  X   X  V . The benefit due to inclusion of the i th eigen component is greater than the loss due to the noise projected along the i th eigen component. Since  X   X  i =  X  i +  X  V  X  2  X  V , hence References Author Biographies
