 Abstract This paper describes the acquisition of a new database of dysarthric speech in terms of aligned acoustics and articulatory data. This database currently includes data from seven individuals with speech impediments caused by cerebral palsy or amyotrophic lateral sclerosis and age-and gender-matched control subjects. Each of the individuals with speech impediments are given standardized assess-ments of speech-motor function by a speech-language pathologist. Acoustic data is obtained by one head-mounted and one directional microphone. Articulatory data is obtained by electromagnetic articulography, which allows the measurement of the tongue and other articulators during speech, and by 3D reconstruction from bin-ocular video sequences. The stimuli are obtained from a variety of sources including the TIMIT database, lists of identified phonetic contrasts, and assessments of speech intelligibility. This paper also includes some analysis as to how dysarthric speech differs from non-dysarthric speech according to features such as length of pho-nemes, and pronunciation errors.
 Keywords Speech Articulation Dysarthria 1 Introduction This paper describes the collection of a new English speech database of aligned acoustics and measured 2D and 3D articulatory features from speakers with either cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS), which are two of the most prevalent causes of speech disability (Kent and Rosen 2004 ). This database, called TORGO, is the result of a collaboration between the departments of Computer Science and Speech-Language Pathology at the University of Toronto and the Holland-Bloorview Kids Rehab hospital in Toronto.

Both CP and ALS are examples of a set of speech impairments, collectively called dysarthria, which are caused by disruptions in the neuro-motor interface. These disruptions do not affect the comprehension and cognitive aspects of the production of natural language, but distort motor commands to the articulators of speech, resulting in atypical and relatively unintelligible speech in most cases (Kent 2000 ). This unintelligibility adversely affects the use of traditional automatic speech recognition (ASR) software to the point where severely dysarthric subjects may have a word-error rate of 97.5% on modern systems against 15.5% for the general population (Rudzicz 2007 ). The inability of modern ASR to effectively understand dysarthric speech is a major problem, since the more general physical disabilities often associated with the causative neuro-motor disruptions can make other forms of computer input, such as keyboards or touch screens, especially difficult (Hosom et al. 2003 ). Since speakers with dysarthria differ from the general population in the manner of their articulation, measuring that articulation empirically is essential.
The TORGO database is primarily a resource for developing ASR models more suited to the needs of people with atypical speech production, although it is equally useful to the more general ASR community. A primary reason for collecting detailed physiological information is to be able to explicitly learn  X  X idden X  articulatory parameters automatically in computer speech models via statistical pattern recognition. Recent research has shown that modelling conditional relationships between articulation and acoustics in Bayesian networks can reduce error by about 28% (Markov et al. 2006 ; Rudzicz 2009 ) relative to acoustic-only models for regular speakers. Research in this area has been relatively preliminary, however there has been a marked increase in the use of articulatory models in speech recognition recently (Livescu et al. 2007 ).

This database is also useful in the clinical domain, especially by linguists and pathologists interested in studying atypical speech production. The stimuli uttered by the participants in this study have been carefully chosen to emphasize articulatory contrasts, which in turn can be compared against thorough assessments of speech-motor function, as described below. 1.1 Existing databases To date, no database combining the acoustics and endogenous articulation (e.g., tongue movement) of dysarthric speech is publicly available. Data collection with dysarthric speakers has usually involved fewer than 5 participants (Hasegawa-Johnson et al. 2006 ), frequently producing only about 25 utterances each (Jayaram and Abdelhamied 1995 ). One exception is the A. I. duPont Institute X  X  Nemours database, which is composed of 11 male speakers with varying degrees of dysarthria, each of whom speaks 74 nonsense sentences and two sensible paragraphs (Menendez-Pidal et al. 1996 ). Each nonsense sentence has the form The N 0 is Ving the N 1 , where N 0 and N 1 are unique monosyllabic nouns and V is a monosyllabic verb. The target words, N 0 , V , and N 1 , were randomly selected without replacement in order to provide closed-set phonetic contrasts (e.g., place, manner, voicing). Additionally, one non-dysarthric speaker repeated each sentence in the database. This database costs $ 100USD to license. In our work on building ASR systems for people with dysarthria, we found that the phonemic distribution in the Nemours database was relatively sparse (Rudzicz 2007 , 2009 ). Naturally, the absense of articulatory measurements in this database prohibits its use in the construction of more complex models in speech recognition, and in the study of the underlying effects of dysarthria.

Public databases of endogenous articulation exist, but only for non-dysarthric speakers. The University of Edinburgh X  X  free MOCHA database consists of 460 sentences derived from the TIMIT database (Zue et al. 1989 ) and consists of articulatory measurements from electromagnetic articulography (EMA) (500 Hz sample rate), laryngography (16 kHz sample rate) and electropalatography (EPG, 200 Hz sample rate) (Wrench 1999 ). Each of the 460 phrases in this database were uttered by both a male and a female British speaker without dysarthria. The EMA system measured the 2D midsagittal co-ordinates of 8 points of interest in the vocal tract, namely the upper lip, lower lip, upper incisor, lower incisor, tongue tip, tongue blade (1 cm from the tongue tip), tongue dorsum (1 cm from the tongue blade), and velum.

Recently, Yunusova et al. ( 2008 ) have collected X-ray microbeam data with 7 individuals with Parkinson X  X  disease and 8 with ALS. This data includes point-data in similar positions to the MOCHA database and generally follows the protocol and methodology of the Wisconsin X-ray microbeam database for non-dysarthric speakers (Westbury 1994 ). This database only includes 10 stimuli per speaker, however, which is not enough to train ASR systems. This database may be freely procured from the authors for academic use. The University of Illinois also provides data from 10 dysarthric individuals with cerebral palsy (Kim et al. 2008 ), although this data does not include measurements of the tongue, for instance.

The following sections describe our study population, their speech-motor assessment, and the data collection process. 2 Data collection Data collection began in 2008 through collaboration between the departments of Computer Science and Speech-Language Pathology at the University of Toronto, Holland-Bloorview Kids Rehab hospital in Toronto, and the Ontario Federation for Cerebral Palsy. The following section describes various aspects of the data collection process.
 2.1 Subjects Seven dysarthric subjects (4 male, 3 female) have so far been assessed in this study, covering a wide range of intelligibility. Dysarthric subjects were recruited by a speech-language pathologist at the Bloorview Research Institute in Toronto. The subjects were between the ages of 16 and 50 years old and have dysarthria resulting from cerebral palsy (e.g., spastic, athetoid, or ataxic). In addition, one subject with a confirmed diagnosis of amyotrophic lateral sclerosis (ALS) was recruited. These individuals were matched according to age and gender with non-dysarthric subjects from the general population. Having an equal number of dysarthric and control speakers is useful for comparing acoustic and articulatory differences, and for analyzing these relationships mathematically and functionally (Hosom et al. 2003 ; Kain et al. 2007 ). Data has since been collected from two additional subjects with cerebral palsy, although that data has not yet been analyzed.

Each subject began the data collection process with a short questionnaire that covers general demographic data and health-related questions that can impact speech and language function including various types of motor problems, both gross (e.g., standing, balancing) and fine (e.g., writing, swallowing). All participants were required to have a negative history of severe hearing or visual problems and of substance abuse, and to be able to read at a 6th grade elementary level. This was further quantified by requiring that their cognitive function lie above or at level VIII (i.e., Purposeful-Appropriate) on the Rancho scale (Herndon 1997 ), which is determined during a pre-visit questionnaire. 2.2 Assessment The motor functions of each experimental subject were assessed according to the standardized Frenchay Dysarthria Assessment (FDA; Enderby 1983 ) by a speech-language pathologist. This assessment is designed to categorize and diagnose individuals with dysarthria while being easily applicable to therapy, sensitive to changes in speech, simple and quick to administer, and easily communicable within professional teams. There exist other assessment measures of oral motor ability, such as the Assessment of Intelligibility of Dysarthric Speech (AIDS; Yorkston and Beukelman 1981 ), which quantifies the intelligibility of single words, sentences, and speaking rates of adults and adolescents with dysarthria. However, these tend to focus only on speech production, whereas the FDA also includes analysis of the movement of the articulators in non-linguistic contexts.

The Frenchay assessment measures 28 relevant perceptual dimensions of speech laryngeal, tongue, and intelligibility as described in Table 1 . Influencing factors such as rate and sensation are also recorded. To measure most of these dimensions, the administering clinician either engages the subject in communication or has the subject perform a simple task (e.g., drinking from a cup of water) while observing their oral movements. The subject X  X  oral behaviour is rated on a 9-point scale and plotted with a simple bar graph. The assessment provides characterizations of behaviours across this 9-point scale. For example, for the cough reflex dimension, a subject would receive a grade of  X  X  X (8) for no difficulty,  X  X  X (6) for occasional choking,  X  X  X (4) if the patient requires particular care in breathing,  X  X  X (2) if the patient chokes frequently on food or drink, and  X  X  X (0) if they are unable to have a cough reflex. The resulting graph provides a high-level overview to the clinician to quickly identify problematic aspects of speech or non-speech (e.g., swallowing).
The mildly dysarthric speakers were able to participate in all tasks required of them for the assessment. The more severely dysarthric speakers also engaged in all tasks but levels of fatigue and poor breath control inhibited them from completing some of these tasks. Assessment data of this type is useful in analyzing how modifications to ASR software affects achievable accuracy across the spectrum of intelligibility levels. For example, alterations to the process by which vowels are categorized by the machine may have greater impact for those individuals with more atypical tongue movement, as opposed to pronounced velum differences. Table 1 shows the mean ( l ) and standard deviation ( r ) of our participants, split by gender, across each of the 28 dimensions of the Frenchay assessment. 2.3 Speech stimuli All subjects read English text from a 19-inch LCD screen placed 60 cm in front of them. One subject experienced some visual exhaustion near the end of one session, and therefore repeated a small section of verbal stimuli spoken by an experimenter rather than read these stimuli. No discernible effect of this approach was measured. The stimuli were presented to the participants in randomized order from within fixed-sized collections of stimuli in order to avoid priming or dependency effects. Dividing the stimuli into collections in this manner guaranteed overlap between subjects who speak at vastly different rates, which is the case when dealing with severely dysarthric speakers, especially when the time allowable for each session is limited. There is no dependency relation between the sessions and the presented stimuli. The collected speech data covers a wide range of articulatory contrasts, is phonetically balanced, and simulates simple command vocabularies typical of assistive ASR technology. The following types of stimuli are included across all collections of data:
Non-words These are used to control for the baseline abilities of the dysarthric speakers, especially to gauge their articulatory control in the presence of plosives and prosody. Speakers are asked to perform the following:  X  5 X 10 repetitions of /iy-p-ah/ , /ah-p-iy/ , and /p-ah-t-ah-k-ah/ , respectively. These  X  High-pitch and low-pitch vowels maintained over 5 s (e.g.,  X  X  X ay  X  X ee X  in a high
Short words These are useful for studying speech acoustics without the need for word-boundary detection. These stimuli include formant transitions between conso-nants and vowels, the formant frequencies of vowels, and acoustic energy during plosive phonemes, as explored by Roy et al. ( 2001 ). This category includes the following:  X  Repetitions of the English digits 1 X 10, yes , no , up , down , left , right , forward ,  X  50 words from the the word intelligibility section of the Frenchay Dysarthria  X  360 words from the word intelligibility section of the Yorkston-Beukelman  X  The 10 most common words in the British National Corpus (Clear 1993 ).  X  All phonetically contrasting pairs of words from Kent et al. ( 1989 ). These are
Restricted sentences In order to utilize lexical, syntactic, and semantic processing in ASR, full and syntactically correct sentences are recorded. These include the following:  X  Preselected phoneme-rich sentences such as  X  X  The quick brown fox jumps over  X  The Grandfather passage from the Nemours database (Menendez-Pidal et al.  X  162 sentences from the sentence intelligibility section of the Yorkston- X  The 460 TIMIT-derived sentences used as prompts in the MOCHA database
Unrestricted sentences Since a long-term goal is to develop applications capable of accepting unrestricted and novel sentences, we elicited natural descriptive text by asking participants to spontaneously describe 30 images of interesting situations taken randomly from among the cards in the Webber Photo Cards: Story Starters collection (Webber 2005 ). These are similar in nature to images used in other standardized tests of linguistic proficiency (Campbell et al. 2001 ). This data complements restricted sentences in that they more accurately represent naturally spoken speech, including disfluencies and syntactic variation. 2.4 Instrumentation In each of three sessions, subjects are prepared for either of two instrumental studies. The first involves the use of EMA and the other involves video recordings of facial markers using specialized software to extract their positions over time. For EMA, the preparation takes approximately 30 min in which sensors are placed on the relevant locations of the speech articulators as described below in Sect. 2.4.1 .In the video-based setup, preparation takes about 20 min and involves the placement of phosphorescent markers on relevant landmark positions of the face, as described in Sect. 2.4.2 . The actual data collection process takes no more than 1 h thereafter in either the EMA or video configurations. Of the three recording sessions, two are within the EMA environment since we are interested in the motion parameters of the tongue, which are unavailable in the video setup. We perform three sessions for each participant in order to check the reliability and variability of our data over time. Moreover, the literature suggests that EMA can provide a reliable estimate of speaker variability of speech parameters over time (van Lieshout et al. 1997 ). 2.4.1 Electromagnetic articulograph kinematics The collection of movement data and time-aligned acoustic data is carried out using the three-dimensional AG500 electro-magnetic articulograph (EMA) system (Carstens Medizinelektronik GmbH, Lenglern, Germany) with fully-automated calibration. The 3D-EMA system is considered state-of-the-art technology for studying speech movements and its principles have been elaborated elsewhere (Hoole et al. 2003 ; van Lieshout et al. 2008 ; Yunusova et al. 2009 ; Zierdt et al. 2000 ). This system allows for 3D recordings of articulatory movements inside and outside the vocal tract, thus providing a detailed window on the nature and direction of speech related activity.

In the AG500 system, six transmitters attached to a clear cube-shaped acrylic plastic structure (dimensions L 58.4 9 W 53.3 9 H 49.5 cm) generate alter-nating electromagnetic fields as shown in Fig. 1 a. Each transmitter coil has a characteristic oscillating frequency ranging from 7.5 to 13.75 kHz (Yunusova et al. 2009 ). When sensors (also called transducers ) are brought into the field, induction generates a weak current oscillating with the same frequencies. The energy in each frequency of the induced complex signal depends on the distance of the sensor from the transmitters and its orientation. The spatial position of the sensor coil in the field is then determined by identifying the strength of the contribution of each transmitter coil via a process of demodulation of the complex signal induced in the sensor (Yunusova et al. 2009 ). The induced voltage values in the sensors are compared to expected values based on a known field model (Zierdt et al. 1999 ) and the difference is expressed as root-mean-square (RMS) error. The system translates these voltages into 3D coordinates of sensor positions over time. As will be discussed later, the RMS error is used to position the subject within the recording field and in part to measure the recording accuracy of the system.

As recommended by the manufacturer, the AG500 system is calibrated prior to each session subsequent to a minimum of a 3 h warm-up time. It is reported that, at or close to the cube X  X  centre, positional errors are significantly smaller (Yunusova et al. 2009 ) compared to the peripheral regions of the recording field within the cube. For our system, the stable volume around the center was roughly 0.008 m 3 (approximately the size of a basketball). Thus, care was taken to ensure that all participants were as close to the cube centre as possible, as shown in Fig. 1 a. The subject positioning within the cube was aided visually by the Cs5view real-time position display program (Carstens Medizinelektronik GmbH, Lenglern, Germany). This allowed the experimenter to continuously monitor the subject X  X  position within the cube (repositioning the subject if required) and thereby maintain low RMS error values 1 to ensure good tracking of the sensor coils.

Sensor coils were attached to three points on the surface of the tongue, namely tongue tip (TT X 1 cm behind the anatomical tongue tip), the tongue middle (TM X  3 cm behind the tongue tip coil), and tongue back (approximately 2 cm behind the tongue middle coil). A sensor for tracking jaw movements (JA) is attached to a custom mould made from polymer thermoplastic that fits the surface of the lower incisors and which is necessary for a more accurate and reproducible recording. Four additional coils are placed on the upper and lower lips (UL and LL) and the left and right corners of the mouth (LM and RM). The placement of some of these coils is shown in Fig. 1 b. Further coils are placed on the subject X  X  forehead, nose bridge, and behind each ear above the mastoid bone for reference purposes and to record head motion. Except for the left and right mouth corners, all sensors that measure the vocal tract lie generally on the midsagittal plane on which most of the relevant motion of speech takes place. Sensors are attached by thin and light-weight cables to recording equipment but do not impede free motion of the head within the EMA cube. Many cerebrally palsied individuals require metal wheelchairs for transpor-tation, but these individuals were easily moved to a wooden chair that does not interfere with the electromagnetic field for the purposes of recording. 2.4.2 Video-based articulatory kinematics Although EMA provides detailed recordings of the tongue, which is not normally visible, typical use of speech recognition software will not likely involve such measurements. Therefore, we implement a second recording environment whose purpose is to derive more varied surface-level facial information using digital cameras. Here, recorded positions are meant to mimic the type of information that can be extracted from webcam-based face-recognition software.

Here, two digital video cameras are placed equidistant from the subject, at approximately 45 degree angles to their midsagittal plane, to the front-left and front-right of the subject. Video is captured at 60 frames per second and audio at 16,000 Hz on both cameras. This audio is used for synchronizing the frames from both cameras and for separate acoustic measurements.

Two 250 W black lights are used to illuminate small (2 mm radii) glow-in-the-dark markers placed on the surface of the subject X  X  face at selected points around the lips and over the orbicularis oris, depressor anguli oris, and depressor labii inferioris muscles as in previous studies on speech production (Craig et al. 2007 ) and as shown in Fig. 2 .

Facial markers are tracked by specialized vision software based on strong contrasts between the reflection of the markers and the relatively darker background. These positions are converted into 3-dimensional co-ordinates using pairs of aligned video images and an estimated inter-camera calibration (Tsai 1987 ). Calibration between cameras is performed by first filming a reference object with a known geometry, namely a cube with 30 cm sides. 2.4.3 Acoustics and microphones All acoustic data is recorded simultaneously through two microphones. The first is an Acoustic Magic Voice Tracker array microphone with 8 recording elements generally arranged horizontally along a span of 45.7 cm. The device uses amplitude information at each of these microphones to pinpoint the physical location of the speaker within its 60-degree range and to reduce acoustic noise by spatial filtering and typical amplitude filtering in firmware. This microphone records audio at 44.1 kHz and is placed facing the participant at a distance of 61 cm. The second microphone is a head-mounted electret microphone which records audio at 16 kHz. The electromagnetic field produced by this microphone does not demonstrably affect the field of the EMA system, and so it can be worn during all recordings.
Signals from the two microphones are temporally aligned using simple cross-correlation. Namely, given the two discrete signals f and g , we compute the complex conjugate of the first, giving signal f consisting of real and phase values, and compute the cross-correlation by where N is the length of the longer of the two sequences. The maximum value of this cross-correlation signal is the time delay between the jointly stationary signals, which is the speech signal recorded by both microphones. An example of this alignment is shown in Fig. 3 .

Finally, acoustic noise reduction is performed using the minimum mean squared error estimate of the spectral amplitude (Ephraim and Malah 1985 ; Martin 2001 ). Furthermore, the use of multiple microphones permits the use of various noise-reduction algorithms not suitable for single sources (Aarabi and Shi 2004 ; Shi et al. 2007 ). 3 Data post-processing The AG500 EMA system has an expected error specification of up to 0.5 mm in each dimension ( X , Y , and Z ) and an angular error ( h ) of less than half of a degree. However, in reality it is possible that accuracy may vary slightly across different AG500 systems due to set-up and environmental conditions such as ambient room temperature, type of sensor coils used, and existing electromagnetic fields in the room. These conditions may also vary across time (Kaburagi et al. 2005 ; Yunusova et al. 2009 ). To estimate more realistic values, we carried out a series of static and dynamic accuracy measurements for the AG500 system. For static measurement, 3-dimensional Euclidean distances between pairs of sensor coils were calculated. The sensors used here were those located on relatively rigid surfaces, namely the forehead, nose bridge, and behind the ear on the skin covering the right and left mastoid bone. Under ideal conditions, the distance between the pairs of sensors should remain constant throughout all trials for a given session. In other words, smaller average standard deviations for the 3D Euclidean distances between pairs of reference coils would imply lower static system noise or relative error. Similar methods have been applied to a camera-based marker tracking system (Craig et al. 2007 ) and in other 3D EMA systems (Hoole and Zierdt 2010 ; Yunusova et al. 2009 ). This Euclidean RMS method provides a real and accurate measure of intrinsic system noise and relative error for each recording session. The average value was 0.2 mm across all pairs. These numbers may be taken roughly as the lower limit of the system X  X  resolution (Kroos 2008 ).

Recent studies have indicated that position errors in dynamic measurements, as opposed to static measurements, may be larger in magnitude and may vary across the three spatial dimensions (Kroos 2008 ; Yunusova et al. 2009 ). We therefore ran a set of dynamic accuracy measurements for all coils using a specific tool recommended by the manufacturer. This allows us to estimate dynamic spatial errors as a function of sensor orientation. This accuracy checking tool is a mechanical device that is rigidly fixed in the centre of the cube X  X  recording field and allows user defined manipulations of sets of coils in different orientations and directions. The device is constructed such that sets of coils placed on it can only travel a fixed distance (70 mm) in a particular direction. For the current study, we six times in a row in each dimension ( X , Y , and Z ). A custom Matlab algorithm calculated the maximum 3D Euclidean displacement between points in that trial, as well as the average 3D Euclidean displacement. The algorithm automatically finds the coils that are being moved and the dimensions in which they are moving using maximum variance. Ideally, the maximum and average 3D Euclidean displacement values should be as close to 70 mm as possible. The amount of deviation from 70 mm provides an estimate of direction specific spatial accuracy of the system. We calculated the accuracy averaged across all 12 sensor coils per dimension. This was in the range of 0.54 X 0.60 mm in the Z (up/down) dimension, 0.34 X 0.59 mm in the X (front/back) dimension, and 0.84 X 1.07 mm in the Y (left/right) dimension. 3.1 Data normalization Position normalizations and corrections for head movements were carried out using custom-made NormPos software from the manufacturer of the AG500. The NormPos program does a sample-by-sample head normalization by rotating and shifting the coordinate system such that all reference sensors remain in the same 3D location across all samples and trials. Computationally, this is carried out using algorithms similar to 3D pose estimation methods (Kroos 2008 ). Such algorithms calculate transformation parameters that can transform head position of a given sample to an experimenter chosen arbitrary reference position (that defines the orientation of the head and the origin of the coordinate system). The transformation parameters are derived by minimizing the sum of the squared distances between the reference sensor coils in the reference position and the actual position in other trials using linear least squares approaches such as (Kroos 2008 ). The NormPos program stores these transformational parameters as a normalization pattern file. This normalization pattern file is then used to rotate and translate all other (non-reference) sensor coils positions in the remaining trails of the experiment to yield articulation trajectories that are corrected for head movements and with a fixed head-orientation that is identical across trials (and across subjects).

Since the NormPos program uses a normalization pattern file that is based on a single trial, the quality of the head movement correction for the entire experiment depends on the quality of the data from the reference sensor coils in that trial. At times, the quality of data may not be equally good in all reference coils (as in the case of coil detachment and/or position tracking errors). For this reason, researchers have recommended the use of more than two reference sensor coils, 2 typically four, to allow for redundancy in the available reference sensor coils (Hoole and Zierdt 2010 ). For the present study, the two noise measures that were previously discussed were used to decide which two or three reference sensor coils (of the four available) were suitable to create the normalization pattern file (Hoole and Zierdt 2010 ). Generally, the nose bridge and the two sensor coils behind the ears had the least amount of noise and were chosen to create the normalization pattern. 4 Aspects of dysarthric speech in TORGO There are a number of features which differentiate dysarthric and non-dysarthric speech in our recorded data. Table 2 shows the proportion of phonemes that were mispronounced according to manner of articulation for dysarthric speech. Plosives are mispronounced most often, with substitution errors exclusively caused by errant voicing (e.g. /d/ for /t/ ). By comparison, only 5% of corresponding plosives in total are mispronounced in non-dysarthric speech. Furthermore, the prevalence of deleted affricates and plosives in word-final positions, almost all of which are alveolar, does not occur in the corresponding non-dysarthric speech data.

Figures 4 and 5 show the durations of various steady-state phonemes (i.e., vowels and consonants, respectively) averaged across the dysarthric and control groups of TORGO. All vowels produced by dysarthric speakers are significantly slower than their non-dysarthric counterparts at the 95% confidence interval and can be up to twice as long, on average. We note that the divergence of the nasal consonants are most severe, which may be indicative of poor control of the velum, but the degree of this divergence does not significantly outweigh those among the other consonants. 5 Ongoing work All data in this paper, including noise-reduced audio and articulatory data, can be obtained by contacting the authors at the University of Toronto. This data is currently being used as input to automatic statistical pattern recognition software that identify relationships between the source features of speech and their intended linguistic meanings. Features such as frequency, energy, and pitch are automatically extracted from the raw audio signal using standard feature extraction techniques (e.g., Mel-frequency cepstral coefficients). These sorts of features are typical input to speech recognition programs that identify spoken phonemes through various multivariate Bayesian methods or by probabilistic regression. Articulatory models learned from kinematic data have been embedded directly within standard recognition systems, which has been shown to improve overall accuracy within the Hidden Markov Model (Markov et al. 2006 ; Rudzicz 2009 ).

The data described in this paper reveals that a lack of articulatory control can often lead to observable acoustic consequences. For example, our dysarthric data contain considerable involuntary types of speech and non-speech noise such as velopharyngeal or glottal noise (often associated with respiration), noisy swallowing problems, hesitation (e.g., false starts), and repetition. We intend to work towards methods of explicitly identifying regions of non-speech noise in our speech recognition systems for dysarthric speakers. Since real-world applications of such technology are not likely to have access to measurements of the vocal tract, we are currently developing methods that estimate the configuration of the vocal tract given only acoustic data (Rudzicz 2010 ). Similar approaches have been shown to accurately estimate such positions within an average error of less than 1 mm (Richmond et al. 2003 ; Toda et al. 2008 ).

Data collection with individuals with dysarthria is ongoing according to the protocols described in this paper. All data described in this journal will be made available online in early 2011. The University of Toronto will not charge any fee for access to this data. Audio for each utterance is encoded in individual wave files encoded in the linear PCM format at 16 and 44.1 kHz. Both raw and normalized articulatory data are provided in EMA-format files at 200 Hz. Open-source programs will be provided to access the EMA file format; no proprietary EMA system or analysis software will be required to access this data. This database will occupy approximately 45 GB of disk space.
 Appendix: Articulatory contrasts See Table 3 . References
