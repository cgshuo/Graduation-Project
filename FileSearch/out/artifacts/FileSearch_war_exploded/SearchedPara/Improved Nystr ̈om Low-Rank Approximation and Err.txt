 Ivor W. Tsang IVOR @ CSE . UST . HK James T. Kwok JAMESK @ CSE . UST . HK Kernel methods play a central role in machine learning and have demonstrated huge success in modelling real-world data with highly complex, nonlinear structures. Examples include the support vector machine, kernel Fisher discrimi -nant analysis and kernel principal component analysis. The key element of kernel methods is to map the data into a kernel-induced Hilbert space  X  ( ) where dot product be-tween points can be computed equivalently through the ker-nel evaluation h  X  ( x ple points, this necessitates the calculation of an n  X  n metric, positive (semi-)definite kernel matrix. The result ant complexities in terms of both space (quadratic) and time (usually cubic) can be quite demanding for large problems, posing a big challenge on practical applications. A useful way to alleviate the memory and computational burdens of kernel methods is to utilize the rapid decay-ing spectra of the kernel matrices (Williams &amp; Seeger, 2000) and perform low-rank approximation in the form of K = GG  X  , where G  X  R n  X  m with m  X  n . However, the optimal (eigenvalue) decomposition takes O ( n 3 ) time and efficient alternatives have to be sought. In the following, we give a brief review on efficient techniques for low-rank decompositions of symmetric, positive (semi-)definite ker -nel matrices.
 Greedy approaches have been applied in several fast al-gorithms for approximating the kernel matrix. In (Smola &amp; Sch  X  olkopf, 2000), the kernel matrix K is approximated by the subspace spanned by a subset of its columns. The basis vectors are chosen incrementally to minimize an up-per bound of the approximation error. The algorithm takes O ( m 2 nl ) time using a probabilistic heuristic, where l random subset size. In (Ouimet &amp; Bengio, 2005), a greedy sampling scheme is proposed based on how well a sample point can be represented by a (constrained) linear combi-nation of the current subspace basis in the feature space. Their algorithm scales as O ( m 2 n ) . Another well-known greedy approach for low-rank approximation of positive semi-definite matrices is the incomplete Cholesky decom-position (Fine &amp; Scheinberg, 2001; Bach &amp; Jordan, 2005; decomposition that skip pivots below a certain threshold, and factorizes the kernel matrix K as K = GG  X  where Another class of low-rank approximation algorithms stem from the Nystr  X  om method. The Nystr  X  om method was orig-inally designed to solve integral equations (Baker, 1977). Given a kernel matrix K , the Nystr  X  om method can be deemed as choosing a subset of m columns (hence rows) trix by K  X  EW  X  1 E  X  , where W is the intersection of the selected rows and columns of K . The most popular sampling scheme for Nystr  X  om method is random sampling, which leads to fast versions of kernel machines (Williams &amp; Seeger, 2001; Lawrence &amp; Herbrich, 2003) and spectral clustering (Fowlkes et al., 2004). In (Platt, 2005), severa l variants of multidimensional scaling are all shown to be re-lated to the Nystr  X  om approximation.
 There are also a large body of randomized algorithms for low-rank decomposition of arbitrary matrices (Frieze et al ., 1998; Achlioptas &amp; McSherry, 2001; Drineas et al., 2003), where the goal is to design column/row sampling proba-bilities that achieve provable probabilistic bounds. Thes e algorithms are designed for a more general purpose and will not be the focus of this paper. However, we note that one of these randomized algorithms has been recently re-vised for efficient low-rank approximation of the symmet-ric Gram matrix (Drineas &amp; Mahoney, 2005). Therefore we will use it as a representative of randomized algorithms in our empirical evaluations. The basic idea of (Drineas &amp; Mahoney, 2005) is to sample the columns of the kernel ma-trix based on a pre-computed distribution using the norms of the columns. The reconstruction of the kernel matrix is also normalized by the sampling distribution.
 In terms of efficiency, greedy approaches usually take O ( m 2 n ) time for sampling, while the random scheme only needs O ( n ) and is much more efficient. Probabilistic ap-proaches, or randomized algorithms in general, are usually more expensive in that the sampling distributions have to be computed based on the original matrix, which require at least O ( n 2 ) . In terms of memory, note that the matrices (
E and W ) needed in the Nystr  X  om method with random sampling can be simply computed on demand. This greatly reduces the memory requirement for very large-scale prob-lems. In contrast, the intermediate matrices for greedy ap-proaches have to be incrementally updated and stored. Although the Nystr  X  om method possesses desirable scaling properties and has been applied with success in various ma-chine learning problems, analysis on its key step of choos-ing the landmark set is relatively limited. In (Drineas &amp; Mahoney, 2005), a probabilistic error bound is provided on the Nystr  X  om low-rank approximation. However, the error bound only applies to the specially designed sam-pling scheme, which needs to compute the norms of all the rows/columns of the kernel matrix and is hence quite expensive. In (Zhang &amp; Kwok, 2006), a block quantiza-tion scheme is proposed for fast spectral embedding. The kernel eigen-system is approximated by first computing a block-wise constant kernel matrix and then extrapolat-ing its eigenvectors through the weighted Nystr  X  om exten-sion. However, the error analysis is only on the block-quantization step, and how the Nystr  X  om method affects the approximation quality in general remains unclear. Thus, the motivation of this paper is to provide a more concrete analysis on how the sampling scheme (or the choice of the landmark points) in general influences the Nystr  X  om low-rank approximation, and to improve the sampling strategy while still preserving its computational efficiency. Our key finding is that the Nystr  X  om low-rank approxima-tion depends crucially on the quantization error induced by encoding the sample set with the landmark points. This suggests that, instead of applying the greedy or probabilis -tic sampling, the landmark points can be simply chosen as the k -means cluster centers, which finds a local minimum of the quantization error. To the best of our knowledge, the k -means has not been applied in the Nystr  X  om low-rank approximation. The complexity of k -means is only linear in the sample size and dimension and, as our analysis ex-pected, it demonstrates very encouraging performance that is consistently better than all known variants of Nystr  X  om. We also compare it with the greedy approach of incomplete Cholesky decomposition and again obtain positive results. The rest of the paper is organized as follows. In Section 2, Section 3, we present an error analysis on how the Nystr  X  om low-rank approximation is affected by the chosen landmark points, and propose the k -means algorithm for the sam-pling step. In Section 4, we compare our approach with a number of state-of-the-art low-rank decomposition tech-niques (including both greedy and probabilistic sampling approaches). The last section gives concluding remarks. The Nystr  X  om method is originated from the numerical treatment of integral equations of the form where p ( ) is the probability density function, k is a posi-tive definite kernel function, and  X   X  ,  X  2 , . . . are the eigenvalues and eigenfunctions of the in-tegral equation, respectively. Given a set of i.i.d. sample s { x 1 , x 2 , . . . , x q } proximate the integral in (1) by the empirical average: Choosing x in (2) from { x K ij = k ( x i , x j ) matrix. The eigenfunctions  X  Seeger, 2001): This means, the Nystr  X  om method using different subset sizes q  X  X  are all approximations to  X  ing a small q can also be deemed as approximating the Nystr  X  om method using a large q . Suppose the sample set X = { x i } n i =1 , with the corresponding n  X  n kernel matrix K . Then the Nystr  X  om method that randomly chooses a sub-set Z = { z the eigen-system of the full kernel matrix K  X  by (Williams &amp; Seeger, 2001) R R m  X  m where W ij = k ( z i , z j ) . Using the approximations in (4), K can be reconstructed as K  X  Equation (5) is the basis for Nystr  X  om low-rank approx-imation of the kernel matrix (Williams &amp; Seeger, 2001; Fowlkes et al., 2004). In this section we analyze how the Nystr  X  om approximation error depends on the choice of landmark points. We first provide an important observation (Section 3.1), and then derive the error bound in more general settings based on a  X  X lustered X  data model (Section 3.2-3.4). The error bound gives important insights on the design of efficient sam-pling schemes for accurate low-rank approximation (Sec-tion 3.5). 3.1. Observation Proposition 1. Given the data set X = { x landmark point set Z = { z struction of the kernel entry K ( x exist two landmark points such that z Proof. Let K and the landmark points Z . Then the Nystr  X  om reconstruc-tion of the kernel entry will be K mark set Z . Let W ( k ) be the k th row of W , then we have K and x W Proposition 1 indicates that the landmark points should be chosen to overlap sufficiently with the original data. How-ever, it is often impossible to use a small landmark set to represent every sample point accurately. 3.2. Approximation Error of Sub-Kernel Matrix In this section we apply a  X  X lustered X  data model to analyze the quality of Nystr  X  om low rank approximation. Here, the data clusters can be naturally obtained by assigning each sample to the closest landmark point. As will be seen, this model allows us to derive an explicit error bound for the Nystr  X  om approximation.
 Again, suppose that the landmark set is Z = { z the whole sample set X is partitioned into m disjoint clus-ters S ple x the approximation error in (5): where k k First, we consider the simpler notion of partial approxima-tion error defined as follows.
 Definition 1. Suppose each cluster has T samples 1 . Re-peat the following sampling process T times: at each time t , pick one sample from each cluster, and denote the set of samples chosen at time t as X ...  X  X  I ingly decomposed into T 2 blocks, each of size m  X  m . Let K on ( X the kernel matrix defined on Z . The partial approximation error is the difference between K proximation under the Frobenius norm We assume the kernel k satisfies the following property: ( k ( a, b )  X  k ( c, d )) 2  X  C k X k a  X  c k 2 + k b  X  d k where C k X . The validity of this assumption on a number of com-monly used kernels will be proved in Section 3.4. Proposition 2. For kernel k satisfying property (8), the partial approximation error E where e sample in X Proof. We will first define the following matrices and then show that they have bounded Frobenius norms. Without loss of generality, we specify the indices as fol-lows: K k ( x I k ( z p , z q ) . With property (8), we have k A I = 2 mC k X e I where e For matrix B k B I and similarly for matrix C Note that the partial approximation error E re-written as follows using (11). Using the bounds on k A with the definition in (11), we have Proposition 2 3.3. Approximation Error of Complete Kernel Matrix With the estimated partial approximation error, we can now obtain a bound on the complete error for Nystr  X  om approx-imation (6). The basic idea is to sum up the partial errors E Proposition 3. The error of the Nystr  X  om approximation (6) is bounded by where T = max total quantization error of coding each sample x the closest landmark point z Proof. Here we sum up the terms in (9) separately. X  X  q 2 mC k where e = P T as defined in proposition 3. Similarly, the second term (and the third term) in (9) can be summarized as
X The last term in (9) can be summarized as X By combining all these terms, we arrive at Proposition 3. 3.4. C k In this section, we show that many commonly used ker-nel functions satisfy the property in (8). Consider the sta-tionary kernel k ( x, y ) =  X  x  X  y sian kernel  X  (  X  ) = exp(  X   X  2 ) , Laplacian kernel  X  (  X  ) = exp(  X   X  ) , and inverse distance kernel  X  (  X  ) = (  X  +  X  )  X  By using the mean value theorem and triangular inequality, we have, for any a, b, c, d  X  R d , Let v k v 1 k + k v 2 k ( k a  X  b k  X  k c  X  d k ) 2  X  ( k a  X  c k + k b  X  d k ) 2 So C k bounded ( C k mial kernels of the form k ( x, y ) = ( h x, y i +  X  ) d , ( k ( a, b )  X  k ( c, d )) 2 = ( a  X  b +  X  ) d  X  ( c  X  d +  X  ) = ( p  X  (  X  )( a  X  b  X  c  X  d )) 2 = ( p  X  (  X  ) (( a  X  c )  X  [2 p  X  (  X  )] 2 k ( a  X  c )  X  b k 2 + k ( b  X  d )  X  c k  X  [2 p  X  (  X  ) R ] 2 k a  X  c k 2 + k b  X  d k 2 , where R is the larger one of the two quantities: the max-imum pairwise distance between samples, and maximum distance between samples and the origin point; and p ( z ) = z 3.5. Sampling Procedure The error bound in Proposition 3 provides important in-sights on how to choose the landmark points in the Nystr  X  om method. As can be seen, consistently, that for a number of commonly used kernels, the most important factor that in-fluences the approximation quality is e , the error of quan-tizing each of the samples in X with the closest landmark in Z . If this quantization error is zero, the Nystr  X  om low-rank approximation of the kernel matrix will also be exact. This agrees well with the ideal case discussed in Section 3.1. Motivated by this observation and the fact that k -means clustering can find a local minimum of the quantization er-ror (Gersho &amp; Gray, 1992), we propose to use the centers obtained from the k -means as the landmark points. Here, k is the desired number of landmark points in Z . The larger the k , the more accurate the approximation though at the cost of higher computations. Despite its simplicity, the means procedure can greatly improve the approximation quality compared to other sampling schemes, as will be demonstrated empirically in Section 4. Recent advances in speeding up the k -means algorithms (Elkan, 2003; Ka-nungo et al., 2001) also make this k -means-based sampling strategy particularly suitable for large-scale problems. This section presents empirical evaluations of the various low-rank approximation schemes. First, we discuss how the low rank approximation fits into different applications . One is to solve linear systems of the form ( K +  X I ) x = a where K is the kernel matrix,  X   X  0 is a regularization parameter and I is the n  X  n identity matrix. Given the low-rank approximation K  X  GG  X  , the following holds (Williams &amp; Seeger, 2001) by the Woodbury formula which only needs O ( m 2 n ) time and O ( mn ) memory. Therefore, it can be used in speeding up the Gaussian pro-cesses (Williams &amp; Seeger, 2001) and least-squares SVM (LS-SVM) (Suykens &amp; Vandewalle, 1999).
 The second application is to reconstruct the eigen-system of a matrix approximated by its low-rank decomposition. Proposition 4. Given the low-rank approximation K  X  GG  X  , where G  X  R n  X  m and m  X  n , the top m eigenvec-tors U of K can be obtained as U  X  GV  X   X  1 / 2 in O ( m 2 position of the m  X  m matrix S = G  X  G = V  X  V  X  . Proof can be found in (Fowlkes et al., 2004). Therefore low-rank approximation is useful for algorithms that rely on eigenvectors of the kernel matrix, such as kernel PCA Niyogi, 2002) and normalized cut.
 Note that the Nystr  X  om method, when designed originally to solve integral equations, did not provide orthogonal ap-proximations to the kernel eigenfunctions. Thanks to the matrix completion view (5) (Fowlkes et al., 2004; Williams &amp; Seeger, 2001), the Nystr  X  om method can be utilized for obtaining orthogonal eigenvectors (Proposition 4), thoug h the time complexity increases from the simple Nystr  X  om ex-tension (4) of O ( mn ) to O ( m 2 n ) . In the experiments we focus on the orthogonalized eigenvector approximation. We compare altogether five low-rank approximation algo-rithms, including: 1. incomplete Cholesky decomposition (ICD) 2 ; 2. Nystr  X  om method (with random sampling); 3. the method in (Drineas &amp; Mahoney, 2005); 4. our method (for simplicity, the maximum number of k -means iterations is restricted to 10 ); 5. SVD. Note that SVD (or eigenvalue decomposition in our context) provides the best low-rank approximation in terms of both the Frobenius norm and spectral norm (Golub &amp; Van Loan, 1996). The complexi-ties of basis selection (i.e., choosing E and W in Nystr  X  om, or sampling the columns in (Drineas &amp; Mahoney, 2005) and ICD) in the different algorithms are listed in Table 1. Evaluations are performed in the contexts of kernel matrix approximation (Section 4.1), kernel PCA (Section 4.2), and LS-SVM classification (Section 4.3). We use core(TM)-dual PC with 2.13GHz CPU and the codes are in matlab. 4.1. Approximating the Kernel Matrix We first examine the performance of the low-rank approx-imation schemes by measuring their approximation errors (in terms of the Frobenius norm) on the kernel matrix. We choose a number of benchmark data sets from the LIB-proximation error bound in Proposition 3 applies to most kernel functions (Section 3.4), and preliminary experimen -tal results with these kernels have shown the superiority of our sampling scheme compared with other low-rank ap-proximation methods. However, due to lack of space, we will only report results for the Gaussian kernel K ( x, y ) = exp(  X  X  x  X  y k 2 / X  ) . Here,  X  is chosen as the average squared distance between data points and the mean of each data set. We gradually increase the subset size m from 1% to 10% of the data size. To reduce statistical variability, re-sults of methods 2, 3, and 4 are based on averages over 20 repetitions.
 The approximation errors are plotted in Figure 1. As can be seen, our algorithm is only inferior to SVD on most data sets. Moreover, though the method in (Drineas &amp; Mahoney, 2005) involves a more complicated probabilistic sampling scheme, its performance is only comparable or sometimes even worse than the Nystr  X  om method with simple random sampling. Similar observations have also been reported in the context of SVD (Drineas et al., 2003). ICD seems to be inferior on several data sets. However, for data sets whose kernel spectra decay rapidly to zero 4 (such as the segment , svmguide1a and satimage ), ICD can also quickly attain performance comparable to others.
 We also examine empirically the relationship between and e under different sampling schemes. Figure 2 reports the results on the german data, where m = 100 and each sampling scheme is repeated 100 times. As can be seen, there is a strong, positive correlation between E and e . This is observed on most data and agrees with our error analysis. 4.2. Kernel PCA In kernel PCA, the key step is to obtain eigenvectors of the centered kernel matrix HKH , where H = I  X  1 R 2005), with the low-rank decomposition K  X  GG  X  , the centered kernel matrix can be written as ( HG )( HG )  X  or to the mean of rows in G . Hence the top m eigenvectors can be obtained in O ( m 2 n ) time according to Proposition 4. We evaluate the low rank approximation schemes by the embedding onto the top 3 principal directions. We align the approximate embeddings (  X  U ) with the standard KPCA embedding ( U ) through a linear transform, and report the minimum misalignment error: min The parameter setting is the same as in Section 4.1, ex-cept that we fix m = 0 . 05 n for all the low-rank decom-position algorithms. Again, results of methods 2, 3, 4 in Table 3 are averaged over 20 repetitions. As we can seen, our algorithm is the best on most data sets, next comes the standard Nystr  X  om and the method by (Drineas &amp; Mahoney, 2005). The time consumptions of all low-rank approxima-tion schemes are significantly lower than SVD. 4.3. Least Squares SVM Given the kernel matrix K , the training labels y  X  SVM classifier f ( x ) = P n b = y  X  M  X  1 1 /y  X  M  X  1 y , and  X  = M  X  1 (1  X  by ) , where 1 is a vector of all ones, and M = Y ( K + I/C ) Y and Y = diag ( y ) . Note that M  X  1 = Y ( K + I/C )  X  1 Y can be computed efficiently using (13).
 We evaluate different low-rank approximation schemes in LS-SVM, using some difficult pairs of the USPS digits 5 . We use Gaussian kernel exp(  X  X  x  X  y k 2 / X  ) and C = 0 . 5 Table 4 reports the classification performance of the stan-dard LS-SVM, and those with different low-rank approxi-mation schemes, at m = 0 . 05 n and 0 . 1 n . Again, methods 2, 3, 4 are repeated 20 times. For m = 0 . 05 n , our approach is significantly better than methods 1,2,3 with a confidence level that is at least 99 . 5% . For m = 0 . 1 n , ours is also better with a confidence level that is at least 97 . 5% on the first 7 pairs. For the last 4 pairs, the differences between our approach and methods 1,2,3 are not statistically signif -icant. Note, however, that the testing errors obtained by th e various approximation algorithms on these 4 pairs are all close to those of the exact LS-SVM, i.e., all approximation algorithms have reached their possibly best performance. The Nystr  X  om method is a useful technique for low-rank ap-proximation. However, analysis on its key step of choos-ing the landmark points and especially that in terms of approximation quality is still limited. In this paper, we draw an intuitive but important connection between the Nystr  X  om approximation quality and the encoding capaci-ties of landmark points. Our analysis suggests the k-means as a natural sampling scheme. Despite its simplicity, the k -means-based sampling gives encouraging performance when empirically compared with state-of-the-art low-rank approximation techniques. One future direction is to uti-lize label/side information for task-specific decompositi on, where one excellent example is (Bach &amp; Jordan, 2005) in the context of incomplete Cholesky decomposition. This research has been partially supported by the Research Grants Council of the Hong Kong Special Administrative Region under grant 614907.
 Achlioptas, D., &amp; McSherry, F. (2001). Fast computation of low rank matrix approximations. Proceedings of the 23th Annual ACM Symposium on Theory of Computing (pp. 611  X  618).
 Bach, F., &amp; Jordan, M. (2002). Kernel independent com-ponent analysis. Journal of Machine Learning Research , 3 , 1 X 48.
 Bach, F., &amp; Jordan, M. (2005). Predictive low-rank decom-position for kernel methods. Proceedings of the 22th
International Conference on Machine Learning (pp. 33  X  40).
 Baker, C. (1977). The numerical treatment of integral equations . Oxford: Clarendon Press.
 Belkin, M., &amp; Niyogi, P. (2002). Laplacian eigenmaps and spectral techniques for embedding and clustering. Ad-vances in Neural Information Processing Systems 14 . Drineas, P., Drinea, E., &amp; Huggins, P. (2003). An experi-mental evaluation of a Monte-Carlo algorithm for singu-lar value decomposition. Proceedings of 8th Panhellenic Conference on Informatics (pp. 279 X 296).
 Drineas, P., &amp; Mahoney, M. W. (2005). On the Nystr  X  om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Re-search , 6 , 2153 X 2175.
 Elkan, E. (2003). Using the triangular inequality to ac-celerate k -means. Proceedings of the 21th International Conference on Machine Learning (pp. 147  X  153).
 Fine, S., &amp; Scheinberg, K. (2001). Efficient SVM training using low-rank kernel representations. Journal of Ma-chine Learning Research , 2 , 243  X  264.
 Fowlkes, C., Belongie, S., Chung, F., &amp; Malik, J. (2004). Spectral grouping using the Nystr  X  om method. IEEE
Transactions on Pattern Analysis and Machine Intelli-gence , 26 , 214 X 225.
 Frieze, A., Kannan, R., &amp; Vempala, S. (1998). Fast Monte-Carlo algorithms for finding low-rank approximations.
Proceedings of the 39th Annual Symposium on Founda-tions of Computer Science (pp. 370  X  378).
 Gersho, A., &amp; Gray, R. (1992). Vector quantization and signal compression . Boston: Kluwer Academic Press. Golub, G., &amp; Van Loan, C. (1996). Matrix computations . Johns Hopkins University Press. 3rd edition.
 Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko,
C. D., Silverman, R., &amp; Wu, A. Y. (2001). An efficient k -means clustering algorithm: analysis and implemen-tation. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence , 24 , 881  X  892.
 Lawrence, N. Seeger, M., &amp; Herbrich, R. (2003). Fast sparse Gaussian process methods: The informative vec-tor machine. Advances in Neural Information Processing Systems. (pp. 625 X 632). MIT Press.
 Ouimet, M., &amp; Bengio, Y. (2005). Greedy spectral embed-ding. Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics (pp. 253 X 260). Platt, J. C. (2005). Fastmap, MetricMap, and Landmark
MDS are all Nystr  X  om algorithms. Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics (pp. 261 X 268).
 ear component analysis as a kernel eigenvalue problem. Neural Computation , 10 , 1299 X 1319.
 trix approximation for machine learning. Proceedings of the 17th International Conference on Machine Learning (pp. 911  X  918).
 Suykens, J., &amp; Vandewalle, J. (1999). Least squares sup-port vector machine classifiers. Neural Processing Let-ters , 9 , 293 X 300.
 Williams, C., &amp; Seeger, M. (2000). The effect of the input density distribution on kernel-based classifiers. Proceed-ings of the 17th International Conference on Machine Learning (pp. 1159 X 1166).
 method to speed up kernel machines. Advances in Neu-ral Information Processing Systems 13 (pp. 682  X  688). Zhang, K., &amp; Kwok, J. (2006). Block-quantized kernel matrix for fast spectral embedding. Proceedings of the 23rd international conference on Machine learning (pp.
