 HUIJING ZHAO and JINSHI CUI , Peking University XIAOWEI SHAO and RYOSUKE SHIBASAKI , The University of Tokyo A robust and efficient multitarget tracking system has become an urgent need in var-ious application domains, such as surveillance, intelligent transportation, and pedes-trian flow analysis. Compared to a traditional camera-based tracking system, as a new kind of measurement instrument, the laser range scanner (LIDAR) has received increasing attention for solving tracking problems in recent years. In a laser-based tracking system (as shown in Figure 1), the targets are represented by several points; hence, the tracking becomes much easier, and it is simple to obtain significantly better performance in terms of both accuracy and the time-cost factor when the targets are far apart. The system [Zhao and Shibasaki 2005; Song et al. 2010a] has been success-fully applied to the JR subway station of Tokyo for the analysis of pedestrian flow, and obtained an overall accuracy of 83%.

However, the drawback of a laser-based tracking system is inherent and obvious: it lacks visual information, and it is consequently difficult to obtain a set of features that uniquely distinguish one object from another. Hence, when the targets are in close proximity or frequently interact with each other, robust tracking becomes especially challenging. Moreover, when the well-known  X  X erge/split X  situation occurs (as shown in Figure 2), maintaining the correct tracking seems to be impossible. It is easy to consider the combining of lasers and vision into one framework to solve these prob-lems. Therefore, the core concerns of this research are: (1) How to make the laser and vision fully display their respective advantages in one framework to solve the difficult problems encountered in multitarget tracking? (2) How to develop a tracking system that can obtain a good trade-off between tracking accuracy and the time-cost factor?
In this article, we integrate lasers, vision, tracking, and online learning such that they complement each other in one framework to deal with various tracking problems. The key idea of this work is depicted in Figures 3 and 4. When the targets do not interact with each other, the laser scanner can efficiently track them, and it is easy to extract visual information from the camera data. Due to the reliability of these tracking results, they are used as positive or negative samples to train some classifiers for the situation of  X  X ossible interacting targets X . When the targets are in close proximity, the learned classifiers and visual information will in turn assist in tracking.
This mode of cooperation between lasers, vision, tracking, and learning offers sev-eral advantages: (1) Lasers and vision can fully display their respective advantages (the speed of a laser and the rich information of a camera) in this system. (2) Be-cause the  X  X ossible interacting targets X  are depicted by a discriminative model with an online supervised learning process, this model can consider information from the  X  X onfusing targets X  and sufficiently exploit the targets X  history. Through these discrim-inative models, we can easily deal with some challenging tracking situations. (3) This  X  X racking-learning adaptive loop X  ensures that the entire process can be completely online and automatic. Multiple Target Tracking (MTT) has been studied extensively, and an in-depth review of tracking literature can be found in a recent survey by Yilmaz et al. [2006]. Typi-cally, multitarget tracking can be solved through data association [Bar-Shalom and Fortmann 1998; Bar-Shalom and Li 1995]. The MultiHypothesis Tracker (MHT) [Read 1979; Blackman and Popoli 1999; Arras et al. 2008], which attempts to keep track of all possible association hypotheses over time, can be seen as the most successful algorithm from a data-oriented view. But the MHT algorithm is computationally exponential, both in memory and time, which makes it difficult to apply. The Nearest-Neighbor Standard Filter (NNSF) [Bar-Shalom and Fortmann 1998] associates each target with the closest measurement in the target state space, and employs the particle filter [Doucet et al. 2000] or Kalman filter [Broida and Chellappa 1986; Fod et al. 2002] to complete its tracking. However, this simple procedure prunes away many feasible hypotheses and cannot solve  X  X abeling problems X  when the targets are crowded. In this respect, a wider approach to multitarget tracking is achieved by exploiting a joint state space representation that concatenates all of the targets X  states together [Okuma et al. 2004; Vermaak et al. 2003; Zhao and Nevatia 2004] or infers this joint data association problem by characterizing all of the possible associations between the targets and observations. Examples of this approach include the Joint Probabilistic Data Association Filter (JPDAF) [Bar-Shalom and Fortmann 1998; Rasmussen and Hager 2001; Gennari and Hager 2004], Monte-Carlo-technique-based JPDA algorithms (MC-JPDAF) [Vermaak et al. 2005; Schulz et al. 2003], and Markov Chain Monte Carlo Data Association (MCMC-DA) [Oh et al. 2004; Khan et al. 2006; Yu et al. 2007]. In addition, researchers have also proposed to utilize Random Finite Set (RFS) approach [Goodman et al. 1997; Mahler 2007b] to deal with multitarget tracking and sensor fusion problem. This methodology offers an elegant Bayesian formulation of multitarget filtering, which has resulted in the development of the Probability Hy-pothesis Density (PHD) filter [Mahler 2003] and the Cardinalized PHD (CPHD) filter [Mahler 2007a]. More recently, some efficient implementations of these filters have been proposed and received increasing attention, such as Gaussian Mixture Probability Hypothesis Density filter (GMPHD) [Vo and Ma 2006], Gaussian Mixture Cardinalized Probability Hypothesis Density filter (GM-CPHD) [Ulmke et al. 2010], Multi-Bernoulli filter [Vo and Ma 2009] and Sequential Monte Carlo (SMC) methods [Vo et al. 2005]. Moreover, researchers also propose some global optimization strategies [Jiang et al. 2007; Leibe et al. 2008; Zhang et al. 2008; Shitrit et al. 2011] to reduce complexity and optimize data association algorithms. Recently, the  X  X ow-frequency X  terms of a Fourier decomposition were used to represent distributions over data association [Huang et al. 2009]. This method can maintain and update the permutation distribution directly in the Fourier domain, allowing for polynomial-time band-limited approximations.
Additionally, researchers have proposed the use of multiple parallel filters to track multiple targets [Cai et al. 2006; Shao et al. 2007]; that is, one filter per target where each has its own small state space. In spite of this, when interactions occur among tar-gets, this method encounters difficulty in maintaining the correct tracking. Therefore, modeling the interactions among targets becomes an incredibly important issue. Khan et al. [2005] used a Markov Random Field (MRF) motion prior to model the interac-tions among targets. Qu et al. [2005] proposed a magnetic-inertia potential model to handle the  X  X erge error X  problem. Yu and Wu [2005] allowed for collaboration among filters by modeling the targets X  joint motion prior to using a Markov random network to solve the  X  X erge error X  problem. Lanz and Manduchi [2005] proposed a hybrid joint-separable model to deal with the interactions among targets. Sullivan and Carlsson [2006] tracked isolated and  X  X erging targets X  separately, and then connected their trajectories by a clustering procedure. Nillius et al. [2006] employed a track graph to describe when targets are isolated and how they interact. They utilized a Bayesian network to associate the identities of the isolated tracks by exploiting the graph.
Meanwhile, researchers found that it was somewhat difficult to obtain robust tracking results with a single sensor. Therefore, several tracking systems based on the combination of a laser and a camera have been proposed [Arras et al. 2000; Chakravarty and Jarvis 2006; Scheutz et al. 2004; Blanco et al. 2003; Schulz 2006; Bellotto and Hu 2006; Cielniak et al. 2007]. The system of of Arras et al. [2000] integrates 360 degree laser and monocular vision to perform localization in a mobile robot, whereas that of Chakravarty and Jarvis [2006] utilizes the panoramic vision and a laser finder to track people in an indoor environment. Scheutz et al. [2004] utilized a combination of leg detection, based on distance information from a laser range sensor, and a visual face detector to track people using a mobile robot. However, all of the aforesaid systems can only detect and track one or a few persons, which is not sufficient in a real surveillance and monitoring environment. The systems of Cielniak et al. [2007] and Blanco et al. [2003] use classifiers to assist in tracking, but their classifiers are pretrained detectors that cannot be updated during the tracking, that is, a completely offline system of learning. In addition, the work described in Oliveira et al. [2010], Spinello and Sieg-wart [2008], Song et al. [2008d], and Cui et al. [2008] can be utilized to detect or track more targets in the outdoor environment, but once some challenging situations (such as  X  X erge/split X ) occur, these systems have difficulty maintaining the correct tracking.
More recently, there has been a trend toward introducing online learning techniques into target tracking. Representative publications include Avidan [2007], Li et al. [2007], Lu and Hager [2006], Grabner and Bischof [2006], Kuo et al. [2010], and Song et al. [2008c; 2010b]. Avidan [2007] and Grabner and Bischof [2006] formulated the track-ing task as a classification problem, and continuously update the current boosting classifier that represents the object in order to optimally discriminate it from the cur-rent background. Lu and Hager [2006] utilize random image patches to represent the foreground of an image, and then extract these from the background via an online updating process. Li et al. [2007] propose a particle-filter-based tracker with different online learning observations. Song et al. [2008c; 2010b], and Kuo et al. [2010] utilize some online trained classifiers to describe the appearance model of targets, and try to deal with interaction and occlusion among targets.

Therefore, in this article, we propose a novel, online tracking system that integrates these promising technologies. Our system has the following key features that make it advantageous over previous ones: (1) It can be applied in wide and open areas, and can easily deal with interactions and merge/split problems among targets. (2) It enables laser and vision to fully display their respective advantages in the overall system, is faster than traditional vision-based tracking systems, and more robust than traditional laser-based ones. (3) The proposed system performs in a fully online and automatic way. It can incrementally learn and update targets X  appearance model, and there is no need for manual labeling. We begin by introducing notations to formulize our problem. At time t , pedestrian k is represented by x k , t = [ p L k , t , p R k , t , T positions of the person X  X  two feet, which is a two-dimensional vector p and S k , t are the walking period and the walking stride of a person, respectively. a a t are the accelerations of her feet. Based on the laser or visual measurements z need to estimate its state as The posterior probability p ( x k , t | z t ) can be computed by a Bayesian recursion as where  X  is the normalization constant, p ( z t | x k , t ) is the similarity between the tar-get X  X  state and measurements (observation model), and p ( x probability (dynamic model). In this research, we utilize the  X  X wo feet walking X  model [Shao et al. 2007] as a dynamic model to predict pedestrians X  position, and employ the particle filter technique [Doucet et al. 2000] to compute Eqs. (1) and (2). When the pedestrians are in the far distance (noncorrelated targets), the observation model is mainly based on the laser measurements. When the pedestrians are in close proximity or interact with each other (interacting targets), the observation model is based on the classifier-enhanced visual observations.
 Therefore, the overall tracking system is designed as illustrated in Figure 5 and Figure 6, and mainly consists of four components: a sensor fusion module, a tracking situation switch module, a noncorrelated target tracking module (learning by tracking), and an interacting target tracking module (tracking by learning). In the sensor fusion part (as shown in Figure 5(a)), a number of laser scanners and one video camera are exploited. Each of these is controlled using a client computer. All client computers are connected through a network to a server computer, which synchronizes and integrates the data from all client computers. For data synchronization, each laser scan and video stream is stamped with a time log at the moment it is captured, or starts to be captured, using the client computer X  X  local clock, which is corrected periodically according to that of the server computer. Data measured by different client computers that is stamped with the same time log is aligned to make up an integrated frame. In addition, an extrinsic calibration is conducted by several control points in a box (as shown in Figure 5(b)). Based on the extrinsic calibration, we can easily compute the center position of the bounding box in image coordinates through the  X  X wo feet X  position. For more details about this part, please refer to our previous work [Cui et al. 2008].
The main components that will be described in this article are the tracking situation switch, the noncorrelated target tracking (learning by tracking), and the interacting target tracking (tracking by learning). In the following sections, we provide details about how the lasers, vision, tracking, and learning complement each other in one framework. We examine different situations that are encountered in tracking, such as noncorre-lated target tracking, interacting target (either in close proximity or merging with each other) tracking, splitting of  X  X erged targets X , and the appearance or disappearance of targets. We aim to detect these conditions and make the system switch automatically between tracking and learning. In this research, we employ a detection-driven strategy to perform this task.

In each frame, we must obtain a laser-based detection of the targets to aid in determining the tracking situation. Each detection or target is modeled as Seg { P = ( x , y ) are the moving points that are obtained by background subtraction. We use the mean-shift algorithm, as described in Comaniciu and Meer [1999], to cluster these moving points into a  X  X wo feet segment X  (as shown in Figure 7(a)).

We define four conditions for each target: a noncorrelated target condition, a cor-related target condition, a merge/split condition, and an appearance/disappearance of targets condition. The correlated targets and merge/split targets can be seen as in-teracting targets. A statistical distance function is employed to aid in detecting these conditions (the ellipse shown in Figure 7(b) and (c)): the threshold, and  X  x and  X  y are the covariance. We set the threshold G is the bandwidth of mean-shift clustering. Hence, we can utilize this function to search for the possible detections of targets: for a  X  X wo feet segment X  Seg with its cluster center ( x versa.

Meanwhile, for each target, we assign a variable c  X  X  0 , state, which is defined as Similarly, for each detection, we assign an association state r An example is shown in Figure 7(b) and (c). In Figure 7(b), the target utilizes Eq. (3) to find two detections, hence its association state c = 2. Meanwhile, the two detections are only associated with this target, and so their association state r in Figure 7(c), the target only finds one detection ( c = this detection is r = 1.

Therefore, these tracking situations can be categorized into the following four differ-ent states.

Noncorrelated targets. Given target k in frame t ,if c k , has r j , t = 1, then this is a noncorrelated target (as shown in Figure 7(c)).
Correlated targets. Given target k in frame t ,if c k , t we believe that there is more than one detection having an influence on this target. Hence, we treat it as a correlated target (as shown in Figure 7(b)).

Merge/split condition. Given detection j in frame t ,if r targets have c k , t = 1, we believe that these targets are merging. Hence, we initialize a new state space for this  X  X erged target X  (as shown in Figure 8(b)). Moreover, if this merged target has c k , t = 2 and all of its associated detections have r that this  X  X erged target X  should split (as shown in Figure 8(c)). Note that when a nonmerged target splits, we do not consider this to be a merge/split condition (e.g., where several people appear in the frame together before later splitting).
Appearance/disappearance of targets. Given a target that is on the edge of the coor-dinate plane, if c k , t = 0 we assume that this target should disappear. We save the state of this target and stop tracking it. Similarly, for a detection that is on the edge of the coordinate plane, if r k , t = 0 this becomes an appearing target. Hence, we should assign a new state space for this target and start to track it.

Although this is a simple strategy, it is effective for detecting most of the conditions described before, and can automatically switch between tracking and learning. Actually, when the targets are far apart (noncorrelated), tracking becomes relatively easy as it can be solved through multiple laser-based independent trackers. Moreover, laser scanners are insensitive to weather conditions, and the data is easy to process; therefore, this situation can provide robust and efficient tracking. Once we obtain the tracking results, visual information for this target can be extracted from the camera and used as samples to train a classifier. The positive samples should be from this target, and the negative samples should come from the  X  X ossible confusing targets X . Consequently, the classifier of each target is a discriminative model that not only depicts the appearance of the target, but also considers information from other targets. An example is shown in Figure 9. In this section, we provide further details about this learning by tracking. Multiple independent particle-filter-based trackers are employed to perform laser-based tracking for noncorrelated targets. For each target, we utilize the independent particle filter to compute Eqs. (1) and (2), and the data association is through the near-est neighbor (NN) association. A  X  X wo feet walking model X  [Shao et al. 2007] is utilized as the proposal distribution. The observation is a point set in this segment expressed point sets and the predicted walking style of pedestrians.

For target k , the similarity between the positions of the human feet and their nearest laser points cluster j can be described as find the nearest laser points cluster, we use Euclidean distance here to measure the distance between the barycenter of each cluster and the predicted position of the feet.
On the other hand, Shao et al. [2007] have proved that the distance between two feet varies in a cosine pattern which is simply the cosine function where A stands for the maximum acceleration, and T is the walking period, S is the stride, and  X  is the walking phase. Hence, the walking pattern similarity can be described as where S d is the distance between the  X  X wo-feet X  position, which can be automatically computed by laser measurements.

We assume that these similarities are independent of each other in a short interval, an assumption that has been demonstrated to be workable in our previous research [Shao et al. 2007; Cui et al. 2008]. Hence, the likelihood function to be updated in the filter is
Hence, after the reweighting and resampling in the particle filter process, we obtain the new position of each pedestrian. Once we obtain the tracking results for each target (the position of her two feet), they can be projected onto the image coordinates, and the bounding box (position and scale) of the people being tracked can be automatically computed through the calibrated parameters (for details about extrinsic calibration, please refer to Cui et al. [2008]). A set of random image patches [Lu and Hager 2006] are then spatially sampled within the bounding box of the target. These random image patches all have the same size, and the sampling number is dependent on the scale of the bounding box. We utilize these random image patches as samples for the online supervised learning. In this case, each target is represented by a  X  X ag of patches X  model.

Extracting some distinguishable features from the image patches is relatively im-portant for the learning process. There are a large number of derived features that can be employed to represent the appearance of an image patch, such as color information (RGB intensity vector), texture information (Haralick features), and edge information (edge orientation histogram). As we utilize image patches as samples, the feature vector should contain local information. Hence, we employ the RGB intensity tation Histogram (EOH) [Levi and Weiss 2004] to extract features from image patches (as shown in Figure 9(b)). We adapt a d-dimensional feature vector to represent each image patch. Therefore, these feature vectors can be utilized as samples for learning or testing. The  X  X ossible interacting targets X  each require strong classifiers to be trained, repre-senting the appearance of these targets. In this research, we choose to use Boosting as the learning technique, and employ Classification and Regression Trees [Breiman et al. 1984] as weak classifiers. The basic idea of Boosting is to create an accurate and strong classifier by combining a set of weak classifiers. This is a popular supervised learning method in the area of machine learning. Let each image patch be represented as a d -dimensional feature vector. For target k in frame t , { their labels, where s  X  d and l  X  X  X  1 , + 1 } . The positive samples are the image patches from the bounding box of target k , whereas the negative samples are the image patches from some  X  X ossible confusing targets X . In this work, the L closest targets to person k are seen as the  X  X ossible confusing ones X . Because the learning is performed while the targets are in the far distance and are not interacting with each other, the laser-based trackers are relatively robust and can easily avoid the  X  X rift problem X . Moreover, the system can also ensure that the training patches do not become contaminated with the incorrectly labeled examples. An example is shown in Figure 9: in frame 8954, per-sons A and B are walking towards each other, and should become  X  X ossible interacting targets X . Hence, classifiers A and B are trained for them. Obviously, for classifier A, the positive training image patches are from person A, and the negative ones are from person B, and vice versa for classifier B. We can see that the training image patches can be correctly labeled by the system itself without any human intervention.
As time passes, new image patches from targets become available. In order to make the classifier increasingly stronger and reflect changes in the object X  X  appearance, the strong classifier should be updated synchronously. Therefore, we employ a strategy, motivated by Avidan [2007], to deal with these problems: once the new training samples are available, we keep the K best weak classifiers, and add some new weak classifiers to replace the  X  X ot-working X  ones. In this case, the learned classifiers not only reflect variations in the targets X  appearance model, but also maintain historical information about targets and become increasingly stronger. The overall learning algorithm is shown in Figure 10. When the targets are in close proximity or interact with each other, it is difficult for the laser-based independent tracker to maintain correct tracking. Specifically, when the  X  X erge/split X  condition occurs, associating the identities of the targets becomes a somewhat challenging problem. In this case, visual information and the learned clas-sifiers should help the system to deal with these problems. In this section, we provide details about how these classifiers assist in tracking to deal with such challenging situations. As explained in Section 4, once a target has more than one detections, and is not a  X  X erged target X , we conclude that there are some correlated targets. When this condition occurs, a set of random image patches are sampled within the interacting region of the image, and the feature vectors of these image patches are imputed to the classifiers of the respective interacting targets. The output from these classifiers are scores. Hence, we obtain the score map of these interacting targets effortlessly (as shown in Figure 11(c)).

Next, we utilize the score map as an observation model and employ the particle filter to estimate people X  X  positions. Hence, the observation model to be updated is Function d (  X  ) converts the  X  X wo feet position X  and the persons X  state into the center position of the bounding box in image coordinates, which are dependent on the extrinsic calibration parameters. Hence, d ( x k , t ) is the center position of candidate target k to be estimated, d i k , t is the center position of image patch i ,and depends on the size of the image patch. For each target, the observation is further peaked around its real region, and the particles sampled in this region will have higher weights. As a result, the particles are much more focused around the true target state after each level X  X  reweighting and resampling. Finally, we utilize the weighted average values of these particles as the estimated position of the person, and this is usually close to the real center position of the person X  X  bounding box. Subsequently, we can easily obtain the new position of these interacting targets and avoid the  X  X rift problem X . An overview of the process is shown in Figure 11. Sometimes, it is difficult to obtain separate detections with the laser-based clustering algorithm. Moreover, the targets in an image may occlude each other completely. Hence, the solution described earlier is not available. Once we detect that such a situation has occurred (as discussed in Section 4), we deal with it as a  X  X erge/split X  condition.
If some targets are merging together, we initialize the state of the  X  X erging targets X  and track them as one target. If we detect that this  X  X erging target X  has split and become an interacting or noncorrelated condition, we utilize the classifiers of these targets to identify them (as shown in Figure 12). Hence, we can link the trajectories of these targets without difficulty.

With the help of the classifiers and visual information, our method is able to deal with various complex tracking situations. In addition, the tracking and learning com-plements the laser and vision in the proposed method, consequently becoming an adaptive loop that ensures the entire process can be completely online and automatic. We evaluated our tracking system in a real scene at a library. In order to deal with occluded targets, we utilized two single-row laser scanners. In addition, one camera was set on the third floor of the building. The test lasted for 30 min (about 45000 frames). When the distance between targets was less than 1.5 m, we started to train the classifiers, and once the distance between them was less than 0.3 m, we considered it to be a challenging situation. In this section, we will present our tracking results and perform some quantitative evaluations. Figure 13 shows an example of the tracking results. The first row contains our track-ing results, the second contains the results with the laser-based tracker, and the third shows the results with the visual tracker. The laser-based tracker results are based on the particle filter described in Section 5. For the visual results, we utilized PD-PF [Song et al. 2008b] to perform the visual tracking. We found that once the interactions occurred, laser-based independent trackers had frequent problems with false labeling. Similarly, the vision-based trackers had difficulties dealing with the  X  X erge/split X  prob-lem. Please note the key targets A, B, C, D, and E. Targets A and C are merging in frame 17707, and targets A and E are merging in frame 17789; some 42 frames later, only our tracking system has maintained their correct trajectories.

We selected 6000 continuous frames in which interactions frequently took place, and performed a statistical survey of how many challenging situations (such as correlated targets or  X  X erge/split X  conditions) we could deal with. Once one of these conditions occurred with no failure in tracking, a successful completion was counted. The details of this survey are shown in Table I. From this table, we can see that our tracking system can deal with most correlated targets or merge/split conditions, which are difficult for the traditional laser-based or visual-based trackers. The tracking situation switch module is a key component in the overall system, and can greatly influence its performance. Hence, we conducted a quantitative evaluation of its accuracy. We performed a statistical survey of 5000 continuous frames, and the details are shown in Figure 14. The first row (ground truth) shows the number of correlated targets, merge/split targets, and appearing/disappearing targets in the 5000 frames, which were obtained in a semi-automatic way (system detection labeling). In this experiment, we invited two people to perform the manual labeling, one from an academic background and the other with no academic background in this area. The second row of Figure 14 shows the number of detections and false alarms in these frames. From this figure, we can see that this module achieves a high accuracy and can easily detect various tracking situations, which ensures the overall system automatically switches between tracking and learning. The overall detection accuracy of this module is shown in Table II. Online learning is a key technique in the proposed system, as it can help the overall system to deal with some challenging situations. Hence, we conducted a quantita-tive comparison to show the importance of the online learning to the overall system. The quantitative comparison was conducted between two methods: our system and laser-based PF trackers (without online learning and as described in Section 5). We performed the comparison on a continuous 5000 frames, and the ground truth was obtained in a semi-automatic way (trackers + manual labeling). In this experiment, we also invited the same two people to perform the manual labeling. By comparing the tracking results with the ground truth, it is easy for us to recognize different failed tracking situations, including missed targets, false locations, and identity switches. For more details about these, please refer to our previous work [Song et al. 2008c]. The de-tails of this comparison are illustrated in Figure 15. From this figure, we can see that, with the help of online learning, our system could obtain more robust tracking results than laser-based PF trackers when interactions occurred (as shown in frames 1386 and 3383). The overall success rate and failure tracking situations of these methods is shown in Table III, and the online learning is found to provide a 15% performance improvement over the PF trackers without it. We evaluated the performance of our tracking system from two aspects: tracking accu-racy and time-cost (CPU computational time), and conducted a quantitative comparison among three systems: laser-based only [Song et al. 2008a], camera-based only [Song et al. 2008c], and the system proposed in this article. All of these systems have online learning components. We performed a statistical survey of 5000 continuous frames, the details of which are shown in Figure 16. Please note that the ground truth was obtained by a semi-automatic method (trackers + manual labeling). By comparing the tracking results with the ground truth, it was easy for us to recognize different failed tracking situations, including missed targets, false locations, and identity switches. In addition, the time-cost factor was normalized to the range 0-1. Moreover, once we start to train the classifiers, the interactions should also be counted in this experiment.
From Figure 16, we can see that the time-cost factor of our system was the same as that of the laser-based one (without visual processing) when there was no interaction, and much more efficient than the vision-based system. When interactions occurred, although our system needed some extra computation (for the visual processing), it could deal with most interactions, and performed better than the laser-based system. In addition, the time-cost factor of our system was still less than that of the vision-based one, because only a limited number of targets required visual processing. The overall tracking accuracy and time-cost factor are shown in Table IV. In conditions of an average 10.18 targets per frame and 2.17 interactions per frame, our system obtained the highest tracking accuracy and was much faster than the vision-based system. Indeed, as the number of tracking targets and interactions increases, our system exhibits a much better performance than the other two systems. The proposed system can maintain correct tracking in most situations. However, some tracking failures occasionally took place when some other kinds of targets appeared, such as bicycles and cars. Because the  X  X wo feet walking X  model cannot apply to these targets, the tracking of these targets usually failed, significantly decreasing the per-formance of the proposed system. Some examples are shown in Figure 17 (first row). In order to deal with these problems, we add a classification module to our system. Some geometric and motion features are extracted from the laser data (for instance, people X  X  data is usually like a circle, and a bicycle is like a line), and the offline trained AdaBoost classifier is utilized to perform the classification. Some results are shown in Figure 17 (second row).

After the classification, we utilize different motion models for different targets (the  X  X wo feet walking X  model for people, and the constant velocity model for bicycles). With the help of the classification module, the tracking failures caused by multiple object types are minimized, and the performance of the proposed system is significantly improved. Some results are shown in Figure 17 (third row). In this article, we presented a novel multitarget tracking system using lasers and vision in a wide and open area. By combining the lasers and vision with tracking and learning, the proposed system easily dealt with various challenging tracking situations in the tracking. In addition, experimental results showed that our system fully incorporated the respective advantages of the two sensors and obtained a superior performance in both tracking accuracy and time-cost factor.

Sometimes, the performance of the proposed system was degraded by multiple kinds of objects (such as pedestrians, bicycles, and cars). By adding the classification module, this problem was easily solved. Meanwhile, the proposed system will encounter some problems while some persons who have the similar appearances are walking together, and this is the potential limitation of our system. At present, the EOH feature can in part deal with this problem. However, if the appearances of some interacting persons are almost same, our system will also encounter some uncertainty in labeling. In the future, some more powerful visual feature should be explored to deal with this problem. On the other hand, some semantic scene knowledge learning module should be com-bined into our system also, which can further improve the tracking and classification results.

