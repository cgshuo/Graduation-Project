 Support Vector Machine (SVM) is a well-known kernel-based method that is successfully applied in many applications such as text categorization, information retrieval, DNA detection, pr otein structure detectio n, intrusion detection and image/speech recognition. SVM classifier requires a kernel mapping function to transform original data in a linear space to a non-linear high-dimensional space by means of dot product of kernel function on a pair of data points, known as kernel trick. However, a common problem in SVM-based classification is how to select a suitable kernel function and its hy perparameters (kernel parameters and SVM tradeoff parameter) that leads to satisfactory performance.

As an early work in kernel selection p roblem, Chapelle and Vapnik[1] pre-sented a gradient descent algorithm, that guides searching process to an optimal solution by adjusting kernel parameters under consideration of their gradients in order to improve time complexity of searching and also still maintain clas-sification accuracy. Later, Keerthi [2] pointed out an analytical exploration on several different kernel functions with implementation of radius-margin bound using the gradient descent approach.

As a more complicated approach, Howley and Madden proposed a new ap-proach to use a so-called genetic kernel , which is represented by a tree with a feature vector located at each leaf nod e. Recently, there have been a series of works on a more flexible kernel so-called multiple kernel learning (MKL). Orig-inally Lanckriet et al. [3] proposed the framework of a linear combination of multiple kernels that resulted in a quadratically-constrained quadratic program (QCQP).Later,Sonnenbergetal.[4]r eformulated the MKL problem as a semi-infinite linear program (SILP) and applied it on large scale data. These MKL approaches were designed to find a set of weights in a fixed form of a multi-RBF kernel using numerical analysis. As an alternative, an evolutional method on multi-RBF kernels was proposed by Phienthrakul and Kijsirikul [5]. In their ex-periments, evolutional strategy was shown to be effective in tuning parameters of a multi-RBF kernel when the number of RBFs increases.

More recently, Sullivan and Luke X  X  work[6] applied genetic programming to search an optimal function in a more general form, where operators and kernels can be arbitrary under the Mercer X  X  condition. Later, Methasate and Theeramunkong [7,8] combined genetic programming and gradient search method to find an optimal kernel function. The concept of a so-called kernel tree was introduced to systematically represent a kernel in the form of a tree under the Mercer X  X  conditions. A hybrid of genetic programming and evolutional strategy was presented in [9] to find an optimal for both kernel structure and kernel parameters.

Towards this issue, we propose an adaptive control mechanism into the evolu-tional method to find an optimal kernel more efficiently. The concept of a family of identical-structured kernel trees is in troduced to explore structure space (a global space) using genetic programming and investigate parameter space (a local subspace) on a specific tree using evolutional strategy.

In the rest of this paper, section 2 pres ents the kernel tree and kernel tree family. In section 4, the strategy for sea rching an optimal tree is described. In section 4, experimental results from UC I datasets and text classification are performed. Finally, the conclusion is made in section 5. In this section, the definitions of a kern el tree [8] and a tree family are given. A kernel tree enables the representation of a multi-kernel function in the form of a tree structure. In this work, we introduce the concept of a kernel tree family as a group of structurally identical tr ees (or isostructural trees). Any two trees are considered as structurally identical when they have the same tree structure with the same node type for each node position but may not occupy the same parameter values for any corresponding edge and any corresponding leaf node. Moreover, we also proposed a mechanism to control the population generation in generic programming, based on this kernel tree family. In the rest, the formal definition of kernel trees is given and then a family of kernel tree is explained. 2.1 Definition of Kernel Trees A kernel function K ( x i ,x j ) have been used to implement a kernel trick which is a method to exploit a linear classifier to solve a non-linear problem by mapping the original non-linear observations into a higher-dimensional space by a dot product between two points x i ,x j ,as K ( x i ,y j )=  X  ( x i )  X  ( y j ) Definition 1 (A kernel tree). A kernel tree T (  X  ) encodes a kernel function K (  X  ) by a tree structure T =( N I ,N L ,E ) , where an intermediate node n i  X  N I represents an operator, and a leaf node n l  X  N L specifies a single kernel function and an edge e  X  E expresses a link between node n i and node n j with a coefficient. In other words, a kernel tree is characterized by the way to assign an operator, a coefficient and a kernel function at each intermediate node, each edge and each leaf node, respectively, in a kernel tree. In this work, an operator in consideration is one of two types: addition and multiplication, a coefficient is a real number, a kernel function is one of the three types: linear, RBF, polynomial function and branching factor is set to be two. Figure 2 displays a kernel tree T ( x i ,x j )ofa kernel function with four sub-kernels. 2.2 A Family of Kernel Trees In this section, first we introduce a norm alized structure of a kernel tree and isostructural kernel trees. Thereafter, we describe a family of kernel trees based on these two concepts.
 Definition 2 (Normalized Structure of a Kernel Tree). The normalized structure of a kernel tree is a tree the branches of which are reordered according to their weights calculated from the substructures under these branches, in order to perserve structural regularity. The structural regularity includes the order among leaf node types, the order among intermediate node types, and the order between leaf node types and intermediate node types.
 Technically, since a branch in any tree will have a unique corresponding node (the node under the branch), a weight given to a branch implies the weight given to its corresponding node. In this work, the weight of a node n , denoted by V ( n ), is calculated with the following formula. This formula enables the preservation of structural regularity.
 where B is the maximum branching factor in the tree, M is the maximum number of possible types of a node in the tree, d is the depth level of the node n ,and N c is the set of the child nodes of the node n , W ( n ) is an index value (between 1and M )representingthetypeofthenode n .

Figure 2 illustrates a kernel tree, its normalized structure and the weight of each node in the tree. In this figure, B is set to 2 (i.e, a binary tree), M is fixed to 5 (pointing to the three leaf node types; linear, RBF, polynomial functions, and two intermediate node types; addition and multiplication operators). More-over, the index value ( W ( n )) equals to 1, 2 and 3 when the node n expresses a linear, RBF and polynomial function, respectively. It is 4 and 5 when the node represents an addition operator and a multiplication operator, respectively. Definition 3 (Isostructural Kernel Trees). Two kernel trees, T 1 and T 2 , are isostructural if and only if the normalized structure of T 1 has the identical structure with the normalized form of T 2 .
 It is noted that two isostructural kernel trees will have an identical structure but may have different parameter values at the edges and leaf nodes.
 Definition 4 (A Family of Kernel Trees). A family of kernel trees is defined as a set of isostructural kernel trees. In the task of searching the optimal kernel trees, there are two scales of search that are structural search (global search) and parameter search (local search). For structural search, the main target is to find a good kernel tree structure by evolving with Genetic Programming. On the other hand, the parameter search is done by evolving the parameter vector from the same structure tree.
For a task of global search with GP, crossover and mutation are standard op-erators for producing a new individual in evolution framework. In GP, crossover operator selects two of parents and ra ndomly switch part of trees between the two selected. Alternatively, mutation o perator selects a parent then replace a new structure part to a node randomly. In applied both of these operators, the structure of trees are evolved to find a more efficient tree structure.
Contrast with the structural search, the parameters search focus on tuning the parameters on each kind of selected tr ee structure. Over th e iteration, each family is become more efficient. In this st ep, the evolution is done from a family mutation operator.
 Definition 5 (Mean and Variance of Family). Suppose that the kernel tree T i,f j ,that is a member of tree family f j , has a parameter vector v T i,f j . The mean ,  X  f j ,andvariance,  X  2 f j , of each family is given by where n is the number of kernel trees in the family.
 Definition 6 (Family Mutation). Family mutation is a process for generating kernel tree from a group of kernel in a same family. The parameter vector of a new kernel tree is given by where D is a random vector that all member is less than 1.
 The proposed system integrated a family concept into genetic programming ap-proach. For each generation, the new structure tree can be produced, only a good structure tree will survive. In the s ame time, the parameter of each family also evolved to find a best set of parameter in each family. The family-based genetic programming is shown in figure 3.

In controlling these two scales of searching methodology, this paper proposes a trade-off strategy to balance the number of structure search and parameter search. The method begins with high number of structure search ratio, then the ratio of the parameter search is increased followed by the number of generation. Both structure and parameter search are applied in each iteration parallely with different ration. With this method, the new structure is discovered in the same time with improvement of the found tree.
 To evaluate the proposed method, two e xperiments have been performed. The first experiment compares the classific ation performance of family-based GP(fGP) with other three methods, grid search(GR), gradient search(GD) and the cascaded of genetic programming and gradient method(gGP) that published in [8]. This experiment evaluated from thirteen binary classes UCI datasets. and Text Classification datasets. In the seco nd experiment, the method is evaluated with text classification dataset consisted of WebKB, 20NewsGroup and Thai Text datasets. 4.1 Evaluation on UCI Datasets This experiment compares the performan ce of four kernel selection methods that are grid search, gradient search, cascaded of genetic programming and gradient method and the proposed method. The setting of the experiments is the same as in [8]. The grid and gradient search are searching on a single-RBF kernel function, while the others are multi-kernel function.All results are done with ten-fold cross-validation.

For benchmarking, the proposed method is explored using the thirteen binary-class UCI repository datasets including Banana, Breast-Cancer, Diabetes, Flare-solar, German, Heart, Image, Ringnorm, Splice, Thyroid, Titanic, Twonorm and Waveform . The data are normalized by zero-mean and unit standard deviation.
In benchmarking with UCI, the average accuracies of ten-fold cross-validation is illustrated in Table 1. In this experiment, the paired t-test is performed for a statistical evaluation the proposed method to the other methods. From the result, the proposed method yields a be tter performance significantly than GR and GD. Moreover when compare with gGP , fGP still gives a better result from ten datasets out of thirteen datasets. 4.2 Evaluation on Text Classification Datasets In an evaluation with text classification, there are four text datasets are applied. The first two datasets are the well-known English text categorization datasets, WebKB[10] and 20-NewsGroup[11]. The third dataset is a Thai medical text data that contains five topics as follows disease, organ, drug, herb and hospital ad-dress. For the forth dataset, Thai News i s collected from three newspaper in five topics that are crime, economic, foreig n, politic and sport. The number of docu-ments in 20NewsGroup, WebKB, Thai Medical and Thai News are 19997, 8282, 3599 and 1492 respectively. Specific process for Thai text, the word segmentation process is done by using CTTeX program. Unlike binary-class datasets, the per-formance of the proposed method on these multi-class datas ets is performed by the one-against-one technique. For evaluate with multi-class text classification dataset, the average accuracy of five-fold cross-validation also yields a better result four from five datasets compared with other method.
 This paper introduces family-based genetic programming and a strategy for trading-off between structural search and parameter search. In each of gener-ation, the structure of the population are analyzed then grouping the individual that contain the same structure to be a family. Then, the new generation individ-uals are constructed with structure crossover, structure mutation and parameter mutation operators. The proposed method a better result compared to other search method both in benchmark datasets and text categorization datasets. This work has been supported by Golden Jubilee Ph.D. Scholarship under Thai-land Research Fund (TRF) with the contract No. PHD/0040/2547 and TRF research grant No. BRG50800013.

