 In MDS, target documents mostly give explicit time information as additional year, date, and time, and most documents for the input of MDS contain a year and a date. In the typical three stage of summarization, they are used to order sentences in a realization (generation) phase, and are used as a weighing scheme in the content identification (sentence extraction) phase. However, we use time information to separate documents in order to obtain the main contents according to the time information, we identify the time-slot that has the most documents. Although a set of documents is constructed not considering which time slot is more important than the other, we assume that a time slot having more documents have a high probability for containing important content like term frequency in information retrieval. 
Separating documents by time information, we also have the benefit of detecting redundant content in documents. As the input of MDS is a set of topically related documents, similar content is inevitable. To find similar content, we check the similarity between each sentence that is extracted. In time dependent documents such as a newspaper article, they are published according to events that occur in sequence, we assume that similar contents are detected in documents that were published at an adjacent time. We thus do not need to compare all extracted sentences, but only detect redundant content among documents published at an adjacent time. Most of techniques for extracting sentences are not new, but we add time features to improve extracting sentences. 
In this paper, we propose a method of extracting sentences and detecting redundant sentences, using time features based on the above two assumptions. In experiments, we attempt to find how separating documents and giving weights to sentences by time features affect sentence extraction in time-dependent multiple documents. To identify important content in multiple documents, Mani and Bloedern built activation networks of related lexical items to extract text spans from a document set [7], and used similarities and differences among related news articles for multi-document summarization [8]. SUMMONS [15] is a knowledge-based MDS system that extracts information, using specialized domain specific knowledge sources, and generates summaries from the templates. Centroid-based summarization [16] uses as input the centroids of the clusters, to identify which sentences are central to the topic of the cluster, rather than individual articles. Maximal Marginal Relevance-Multi-Document [3] is a query-based method to minimize redundancy and maximize both relevance and diversity. The Columbia university system [2, 11] identifies similar sections and produces natural language summary using machine learning and statistical techniques. NeATS [5] is an extracted-based system using term clustering, to improve topic coverage and readability. It uses time features for ordering sentences and resolving time ambiguity, not for assigning weights to sentences. DEMS [21] relies on concepts instead of individual nouns and verbs. Like SDS using rhetorical structure theory (RST), Radev [17] introduced CST, a theory of cross-documents structure, which tried to find a relation between documents, such as subsumption, update, and elaboration, to cross-document conceptual links in MDS [18, 19]. However, Radev does not formalize these relationships [10]. 
Time-based summarization techniques, evolving summaries of new documents related to an ongoing event were proposed by Radev [20]. Mani and Wilson [9] have focused on how to extract temporal ex pressions from a text, looking for and normalizing references to dates and times. Allan [1] produced a summary that events within a topic over time with our method. However, using time features, we not only classify documents according to time, but also identify important content from a document set. To find important sentences that can be the candidate of a final summary, four graduate students selected sentences from multiple documents. Each student read all included in a summary. The five sets of topically-related documents consisted of newspaper articles from the 1998 edition of Hankook Ilbo (Korean newspaper). To obtain reasonable results, we counted the number of selected sentences from each result, and determined the final sentences using the frequency of sentences are result; otherwise, nine sentences were selected. The distribution of human-selected sentences is shown in Table 1. 
We found that extracted sentences for a final summary will be located along a time line, and a time slot that containing more documents has a high probability to including important sentences. Four document sets (A,B,C,E) among five sets have important sentences in recent time slots. Our preliminary experiment has confirmed Goldstein [3] who gave a weight to recent documents. We also found that the first time slot contained important sentences in four sets (A,B,D,E). The number of important sentences in the first time slot was different according to sets. We assumed that the difference depended on the characteristic of each document set, and assigned little weight to the first time slot. For summarizing time-dependent documents, we used these assumptions to design our method. 4.1 System Overview documents. We extracted sentences from each document using the method of SDS. Next, among extracted sentences, we detect redundant sentences, and build the term term cluster, we assign a mark to sentences, and choose a sentence with highest score as a topic sentence in a time slot. A topic sentence receives more marks as a representative of a time slot. From each time slot, we obtain a term cluster, and construct the global term cluster. Using these global terms, we give a mark to all sentences again. Finally, we rearrange all sentences ordered by their scores, and extract sentences. In the following sections, we will explain each phase of the system illustrated in Figure 1. 4.2 Grouping Documents by Time Features The time information given by news articles is usually a combination of date and the documents by their publication date. 4.3 Extracting Sentences To extract important sentences in a single document, we used traditional methods: sentence position, length, stigma word, and keywords in a headline. As we dealt with newspaper articles, we mainly relied on sentence position. The score of the i -th sentence S i is calculated as follows: 
In our system, we selected 50% of the sentences that had a higher score than others in each document, using the combination of each scoring function. Selected sentences are used for the input of the next phase. The following subsections describe each weighing scheme. Sentence Position. Sentence position has been used to find important content since located at the beginning of the document obtained a higher score. Sentence Length. We gave a length penalty Pen(Si) to a sentence Si which was too humans in newspaper articles is usually between 10 and 30 eojeols 1 [22]. The length sentences of which length is shorter than 50 syllables or longer than 180 syllables. Stigma Terms. When some sentences contain quotation marks, they can be redundant content in a summary [5]. Therefore we reduce the score of sentences by 0.4 when sentences include quotation marks. Lead Words. A headline is the most simple and concise method of delivering information about news articles. The basic idea is that a sentence that contains words in a headline will deliver important content about the article. Also, the main contents headline and lead sentences to identify important sentences in a document. 4.4 Building Term Cluster of a Time Slot and Removing Redundant Sentences score from each document. Identifying terms from these sentences, we calculated the frequency of the terms, and constructed a term cluster with terms with a higher identify a main sentence which could represent important content in that time slot. Removing Redundant Sentences. After extracting the important sentences in multiple documents, we grouped them by their similarity. We added a module that sentence-extraction. 
Thus, we checked redundant content on the sentence-level, and estimated the redundancy between sentences. The redundancy value is used to construct a cluster of semantically redundant sentences. Our system basically calculated the Dice coefficient as a similarity measure based on the number of words. We developed an improved term-weighing method that assigns weights to words, using syntactic information [6]. 
When measuring redundancy between sentences, we did not rely on term frequency (TF), and inverse document frequency (IDF), because they cannot distinguish words that are more syntactically important from others. When we compared two sentences, we expected that syntactically important words would obtain a higher score than others. Basically, main clauses will deliver more important information than sub clauses. In addition, we believe that subjects, objects, and verbs are syntactically important, compared to others in a sentence. Therefore, we gave locates. 
When comparing words, we used not only the surface form of a word, but also the concept code of it. In topically related document sets, sentences that contain the same contents can be represented with the combination of different words, and the surface form of a word in news article can be varied. We assumed that verbs have a high probability to be represented in different way. Thus we used the concept code only for Kadokawa thesaurus, which has a 4-level hierarchy of 1,110 semantic classes [14]. Concept nodes in level L1, L2 and L3 are further divided into 10 subclasses and nodes in level L4 have 3-digit code between 000 and 999. Formally, the redundancy between two sentences S1 and S2 is calculated as follows: 
We used a single-link clustering algorithm. Sentence pairs having similarity value redundant cluster. The threshold value T r is set to 0.5. After clustering, each cluster is expected to have several redundant sentences. From these clusters we chose only one sentence that had the highest score. 
We compared our redundancy-measuring (System1) to a method which does not from four topically-related newspaper articles (the 1998 edition of Hankook Ilbo). The average number of sentences in test sets is 2.9. Thus, if there were two or more redundant sentences in both the test sets and the sets generated by systems, we consider that the systems correctly detected redundant sets. The threshold value T r is set 0.5. Table 3 shows that syntactic informa tion improves detecting redundant sentences. Thus, we used a Korean dependency parser to obtain syntactic information. The cluster of redundant sentences was also used as a feature to give a weight to sentences. If several documents contain the same information, the repeated information can be the topic among documents and deliver important content [20]. Thus, we add it to a scoring formula in this phase. Find a Topic Sentence in a Time Slot. Using the term cluster, the number of redundant sentences, and a previous score of sentences, we assigned a score to sentences, and regarded a sentence with the highest score among all extracted sentences to be a topic sentence in a time slot. A topic sentence receives more marks verb, or terms are identified as an unknown word, we give more marks to them. A sentence S is calculated as follows: 4.5 Building Global Term Cluster and Scoring Sentences To extract sentences from multiple documents, we constructed a global term cluster slot, we identified terms and select terms that had a higher frequency than the higher score. According to the results of the preliminary experiment, we gave a high probability to a time slot that contained relatively more documents than others, and to indicated a specific time slot. Although we do not know what happens at this reference time slot, we assume that some event related to a topic occurred at that time. If the time slot is referenced by a sentence that is in a different time slot, we assigned weights to the referenced time slot. Also, we considered a sentence X  X  score produced in the previous phase, and the syntactic information of terms mapped with global terms. A sentence S is finally calculated as follows: We used 29 document sets which are concerned with a certain topic. These document sets were used for the formal run evaluation of NTCIR TSC3 (NII-NACSIS Test Collection for IR Systems, Text Summarization Challenge 3). In NTCIR TSC3, documents consisted of newspaper articles from the 1998, 1999 edition of the Mainichi and Yomiuri (Japanese newspaper). NTCIR also provided a scoring tool to evaluate precision and coverage for sentence extraction. We used this tool for evaluating our experiments. Because the document sets were written in Japanese, we extractor. Using a Japanese-to-Korean machine translator, we translated all the document sets. The documents for experiments no doubt contained translation errors. However, there are no available Korean documents sets and tool for evaluating multi-document summarization systems. Thus, it was necessary to use translated documents in this experiment. We briefly describe the evaluation method from Hirao and Okumura X  X  overview paper of NTCIR TSC3 [4]. 6.1 Precision Precision is the ratio of how many sentences in the system output are included in the set of the corresponding sentences. It is defined by the following equation: Where h is the least number of sentences needed to produce the abstract, and m is the number of correct sentences in the system output. 6.2 Coverage Coverage is an evaluation metric for measuring how close the system output is to the abstract taking into account the redundancy found in the set of sentences in the output. case, we have m sets of corresponding sentences. Here, A i , j indicates a set of elements each of which corresponds to the sentence number in the original documents, denoted between sentences in original documents and their abstract as in Table 4. For define the evaluation score e ( i ) for the i -th sentence in the abstract: Function e returns one(1) when any Ai,j is outputted completely. Otherwise, it and the number of sentences in the abstract n, Coverage is defined as follows: Our experiments focused on how separating documents and assigning weights to sentences with the time feature which affects sentence extraction in MDS. To identify the effect of time feature, we implemented two systems. The first system (System1) used the time feature to separate documents and to give a mark to sentences. The other system (System2) did not use the time feature. According to our proposed method, we implemented System1 to detect the redundancy only in the same time slot. As the time feature, we use the date information (yyyymmdd). We attempted to use the referenced time feature in sentences, but we did not yet implement a detecting module for the time feature in our system. For System2, we extracted sentences from checked redundancy between all sentence pairs, and rearranged them by their score. For 29 document sets, we generated two types of sentence extraction: short and long. The number of extracted sentences is already defined for each document set. We calculated precision and coverage for the results of two systems. The evaluation of the two systems is shown in Table 5. 
System1 using the time feature had high precision values in both short and long type extraction. However, coverage values are slightly lower than System2 in both feature to improve sentence extraction from time-dependent multiple documents. Coverage is an evaluation metric for removing redundant contents. Although the result of coverage depends on the algorithm of measuring redundancy between sentences and several parameters, redundant contents were found in different time slots in our results. We assumed that  X  X  time slot having more documents might have assumption in weighing schemes, we didn X  X  consider it for producing a final summary. It possibly resulted in lower coverage in System1. 
Figure 2 and 3 show that the effectiveness of time features strongly depends on the characteristic of document sets. Some docum ent sets (topic id: 310, 440, 550) consisted of time-independent documents, and have a tendency not to follow time-dependent events. Therefore, these document sets revealed high precision in System2. However, topic ID 440 showed high precision in Long type. Although the document set of a topic id 410 consisted of time-dependent documents, System2 obtained higher precision in a short type extraction. For increasing the performance of our system, we needed to analyze document sets. We also compared our results to other systems that participated in NTCIR TSC3. The results are shown in Table 6. Their results were produced from Japanese documents. Although we used translated documents, our method performed well. We propose a method of extracting sentences and detecting redundant sentences, using time features. To improve extracting sentences in MDS, especially in time-dependent documents, the time feature can be used effectively. Using the time feature, separating documents and assigning weight to sentences can improve the output of a system that is based on a sentence-extraction method. Our system needs to refine several additional parameters for incr eased efficiency, and therefore further experimentation is necessary. 
In the future, we will try to scrutinize document sets to increase the performance of our system, and discover how to apply the time feature to documents that have time-independent tendency. Also we will apply category information [12, 13] for distinguishing between time-dependent documents and time-independent ones. We believe that this will greatly enhance our research. Acknowledgements. This work was supported by the Korea Science and Engineering Foundation, through the Advanced Information Technology Research Center (AITrc) and the BK 21 project. 
