 Paul Ruvolo pruvolo@cs.brynmawr.edu Eric Eaton eeaton@cs.brynmawr.edu Versatile learning systems must be capable of effi-ciently and continually acquiring knowledge over a se-ries of prediction tasks. In such a lifelong learning setting, the agent receives tasks sequentially. At any time, the agent may be asked to solve a problem from any previous task, and so must maximize its perfor-mance across all learned tasks at each step. When the solutions to these tasks are related through some underlying structure, the agent may share knowledge between tasks to improve learning performance, as ex-plored in both transfer and multi-task learning. Despite this commonality, current algorithms for transfer and multi-task learning are insufficient for life-long learning. Transfer learning focuses on efficiently modeling a new target task by leveraging solutions to previously learned source tasks, without considering potential improvements to the source task models. In contrast, multi-task learning (MTL) focuses on max-imizing performance across all tasks through shared knowledge, at potentially high computational cost. Lifelong learning includes elements of both paradigms, focusing on efficiently learning each consecutive task by building upon previous knowledge while optimiz-ing performance across all tasks. In particular, lifelong learning incorporates the notion of reverse transfer , in which learning subsequent tasks can improve the per-formance of previously learned task models. Lifelong learning could also be considered as online MTL. In this paper, we develop an Efficient Lifelong Learn-ing Algorithm (ELLA) that incorporates aspects of both transfer and multi-task learning. ELLA learns and maintains a library of latent model components as a shared basis for all task models, supporting soft task grouping and overlap (Kumar &amp; Daum  X e III, 2012). As each new task arrives, ELLA transfers knowledge through the shared basis to learn the new model, and refines the basis with knowledge from the new task. By refining the basis over time, newly acquired knowl-edge is integrated into existing basis vectors, thereby improving previously learned task models. This pro-cess is computationally efficient, and we provide robust theoretical guarantees on ELLA X  X  performance and convergence. We evaluate ELLA on three challeng-ing multi-task data sets: land mine detection, facial expression recognition, and student exam score pre-diction. Our results show that ELLA achieves nearly identical performance to batch MTL with three orders of magnitude (over 1,000x) speedup in learning time. We also compare ELLA to a current method for online MTL (Saha et al., 2011), and find that ELLA has both lower computational cost and higher performance. Early work on lifelong learning focused on shar-ing distance metrics using task clustering (Thrun &amp; O X  X ullivan, 1996), and transferring invariances in neu-ral networks (Thrun, 1996). Lifelong learning has also been explored for reinforcement learning (Ring, 1997; Sutton et al., 2007) and learning by reading (Carlson et al., 2010). In contrast, ELLA is a general algorithm that supports different base learners to learn continu-ally, framed in the context of current MTL methods. Recently, MTL research has considered the use of a shared basis for all task models to improve learn-ing over a set of tasks. Several formulations of this idea have been proposed, including a probabilistic framework (Zhang et al., 2008) and a non-parametric Bayesian method that automatically selects the num-ber of bases (Rai &amp; Daum  X e III, 2010). These meth-ods assume that each model is represented as a pa-rameter vector that is a linear combination of these bases. By using a common basis, these approaches share information between learning tasks and account for task relatedness as the models are learned in tan-dem with the basis. The GO-MTL algorithm (Kumar &amp; Daum  X e III, 2012) also uses a sparsely shared basis for multi-task learning, with the advantage that it au-tomatically learns (potentially) overlapping groups of tasks to maximize knowledge transfer. We employ this rich model of underlying task structure as the starting point for developing ELLA.
 Few papers have focused on the development of very computationally efficient methods for MTL. Simm et al. (2011) present a model for learning multiple tasks that is efficient in the case when the number of tasks is very large. However, their approach suffers from signif-icant drawbacks in comparison with ELLA: (1) their approach is not an online algorithm, limiting its use in the lifelong learning setting, and (2) their underly-ing model of shared task structure is significantly less flexible than our model. Another approach, OMTL by Saha et al. (2011), is designed to provide efficient performance when instances and new tasks arrive in-crementally. However, OMTL is only applicable to classification tasks (not regression) and relies on per-ceptron learning, which we found to perform poorly in comparison to other base learners (see Section 4). We begin by describing the lifelong learning problem and why lifelong learning algorithms must be able to efficiently learn new tasks and incorporate new train-ing data from previous tasks. Then, we introduce ELLA, which efficiently handles both of these opera-tions through a two-stage optimization procedure. We show that ELLA encompasses the problem of online dictionary learning for sparse coding as a special case, and finish by proving robust convergence guarantees. This paper uses the following conventions: matrices are denoted by bold uppercase letters, vectors are de-noted by bold lowercase letters, scalars are denoted by normal lowercase letters, and sets are denoted using script typeface (e.g., A ). Parenthetical superscripts denote quantities related to a particular task (e.g., ma-trix A ( t ) and vector v ( t ) are related to task t ). 3.1. The Lifelong Learning Problem A lifelong learning agent (Figure 1) faces a series of su-pervised learning tasks Z (1) , Z (2) ,..., Z ( T max ) , where space X ( t )  X  R d to a set of labels Y ( t ) (typically Y { X  1 , +1 } for classification tasks and Y ( t ) = R for re-gression tasks). Each task t has n t training instances X given by  X  f ( t ) . We assume that a priori the learner does not know the total number of tasks T max , the distribution of these tasks, or their order.
 Each time step, the agent receives a batch of labeled training data for some task t , either a new task or a previously learned task. Let T denote the number of tasks encountered so far. After receiving each batch of data, the agent may be asked to make predictions on instances of any previous task. Its goal is to construct such that: (1) each f ( t ) will approximate  X  f ( t ) to en-able the accurate prediction of labels for new instances, (2) each f ( t ) can be rapidly updated as the agent en-counters additional training data for known tasks, and (3) new f ( t )  X  X  can be added efficiently as the agent en-counters new tasks. We assume that the total num-bers of tasks T max and data instances P T max t =1 n t will be large, and so a lifelong learning algorithm must have a computational complexity to update the models that scales favorably with both quantities.
 3.2. Task Structure Model for ELLA ELLA takes a parametric approach to lifelong learning in which the prediction function f ( t ) ( x ) = f ( x ;  X  for each task t is governed by the task-specific param-eter vector  X  ( t )  X  R d . To model the relationships be-tween tasks, we assume that the parameter vectors can be represented using a linear combination of shared la-tent model components from a knowledge repository. Many recent MTL methods employ this same tech-nique of using a shared basis as a means to transfer knowledge between learning problems (see Section 2). Our model of latent task structure is based on the GO-MTL model proposed by Kumar &amp; Daum  X e III (2012). ELLA maintains a library of k latent model compo-nents L  X  R d  X  k shared between tasks. Each task pa-rameter vector  X  ( t ) can be represented as a linear com-bination of the columns of L according to the weight vector s ( t )  X  R k (i.e.,  X  ( t ) = Ls ( t ) ). We encourage the s ( t )  X  X  to be sparse (i.e., use few latent components) in order to ensure that each learned model component captures a maximal reusable chunk of knowledge. Given the labeled training data for each task, we opti-mize the models to minimize the predictive loss over all tasks while encouraging the models to share structure. This problem is realized by the objective function: e
T ( L ) = task t , L is a known loss function, and the L 1 norm of s ( t ) is used as a convex approximation to the true vector sparsity. This is similar to the model used in GO-MTL, with the modification that we average the model losses on the training data across tasks (giving rise to the 1 T term). This modification is crucial for obtaining the convergence guarantees in Section 3.6. Since Equation 1 is not jointly convex in L and the s ( t )  X  X , our goal will be to develop a procedure to ar-rive at a local optimum of the objective function. A common approach for computing a local optimum for objective functions of this type is to alternately per-form two convex optimization steps: one in which L is optimized while holding the s ( t )  X  X  fixed, and another in which the s ( t )  X  X  are optimized while holding L fixed. These two steps are then repeated until convergence (this is the approach employed for model optimization in GO-MTL). Next, we discuss two reasons why this approach is inefficient and thus inapplicable to lifelong learning with many tasks and data instances.
 The first inefficiency arises due to the explicit de-pendence of Equation 1 on all of the previous training data (through the inner summation). We remove this inefficiency by approximating Equa-tion 1 using the second-order Taylor expansion of  X   X  ( t ) is an optimal predictor learned on only the train-ing data for task t ). Plugging the second-order Taylor expansion into Equation 1 yields: g T ( L ) = where and k v k 2 A = v &gt; Av . In Equation 2, we have sup-pressed the constant term of the Taylor expansion (since it does not affect the minimum) and there is no linear term (since by construction  X  ( t ) is a minimizer). Crucially, we have removed the dependence of the op-timization on the number of data instances n 1 ...n T in each task. The approximation is exact in an im-portant special case: when the model is linear and the loss function is squared loss (see Section 3.4.1). The second inefficiency of Equation 1 is that in order to evaluate a single candidate L , an optimization prob-lem must be solved to recompute the value of each of the s ( t )  X  X  (which will become increasingly expen-sive as the number of tasks learned T increases). To overcome this problem, we modify the formulation in Equation 2 to remove the minimization over s ( t ) . We accomplish this by computing each of the s ( t )  X  X  when the training data for task t is last encountered, and not updating them when training on other tasks. At first glance this might seem to prevent the ability for previously learned tasks to benefit from training on later tasks (which we call reverse transfer ); however, these tasks can benefit by subsequent modifications to L . Later in Section 3.6, we show that this choice to update s ( t ) only when we encounter training data for the respective task does not significantly affect the quality of model fit to the data as the number of tasks grows large. Using the previously computed values of s ( t ) gives rise to the following optimization procedure (where we use the notation L m to refer to the value of the latent components at the start of the m th itera-tion, and t is assumed to correspond to the particular Algorithm 1 ELLA ( k,d, X , X  ) while isMoreTrainingDataAvailable() do end while task for which we just received training data): where ` ( L , s ,  X  , D ) =  X  k s k 1 + k  X   X  Ls k 2 D . (6) Next, we present the specific steps needed to perform the updates in the preceding equations. 3.3. Model Update for ELLA Suppose that at the m th iteration we receive training data for task t . We must perform two steps to update our model: compute s ( t ) and update L . In order to compute s ( t ) , we first compute an optimal model  X  ( t ) using only the data from task t . The details of this step will depend on the form of the model and loss function under consideration, and thus here we treat it as a black box. If the training data for a particular task arrive interleaved with other tasks and not in a single batch, it may be important to use an online single-task learning algorithm to achieve maximum scalability. Once  X  ( t ) has been computed, we next compute D ( t ) (which is model-dependent) and re-initialize (either randomly or to one of the  X  ( t )  X  X ) any columns of L that are all-zero (which will occur if a particular latent com-ponent is currently unused). We then compute s ( t ) us-ing the current basis L m by solving an L 1 -regularized regression problem X  X n instance of the Lasso.
 To update L , we null the gradient of Equation 5 and solve for L . This procedure yields the updated column-wise vectorization of L as A  X  1 b , where: To avoid having to sum over all tasks to compute A and b at each step, we construct A incrementally as new tasks arrive (see Algorithm 1 for details). Computational Complexity : Each update begins by running a single-task learner to compute  X  ( t ) and D ( t ) we assume that this step has complexity O (  X  ( d,n t )). Next, to update s ( t ) requires solving an instance of the Lasso, which has complexity O ( nd min( n,d )), where d is the dimensionality and n is the number of data in-stances. Equation 3 can be seen as an instance of the Lasso in k dimensions with d data instances, for a to-tal complexity of O ( dk 2 ). However, to formulate the Lasso problem requires computing the eigendecompo-sition of D ( t ) , which takes O ( d 3 ), and multiplying the matrix square root of D ( t ) by L , which takes O ( kd 2 Therefore, updating s ( t ) takes time O ( d 3 + kd 2 + dk A straightforward algorithm for updating L involves inverting a ( d  X  k )-by-( d  X  k ) matrix, which has com-plexity O ( d 3 k 3 ). However, we can exploit the fact that the updates to A are low-rank to derive a more effi-cient algorithm with complexity O ( d 3 k 2 ) based on a recursive method (Yu, 1991) for updating the eigende-composition of A (see Online Appendix). Therefore, using this more advanced approach, the overall com-plexity of each ELLA update is O ( k 2 d 3 +  X  ( d,n t )). 3.4. Base Learning Algorithms Next, we show how two popular single-task learning algorithms can be used as the base learner for ELLA. 3.4.1. Linear Regression squared-loss function. To apply ELLA, we compute the optimal single-task model  X  ( t ) , which is available able in closed form as D ( t ) = 1 2 n  X  ( t ) and D ( t ) , we simply follow Algorithm 1 to fill in the model-independent details.
 3.4.2. Logistic Regression In this setting y ( t )  X  { X  1 , +1 } n t , f ( x ;  X  ) = ply ELLA, we first use a single-task learner for logistic regression (of which there are many free and robust implementations) to compute the value of  X  ( t ) . D ( t ) then given as: Given these formulas for  X  ( t ) and D ( t ) , we follow Al-gorithm 1 to fill in the model-independent details. 3.5. Connection to Dictionary Learning for ELLA is closely connected to the problem of learning a dictionary online for sparse coding a set of input vec-tors. In fact, this problem is a special case of ELLA in which the  X  ( t )  X  X  are given as input instead of learned from training data and the D ( t )  X  X  are equal to the iden-tity matrix. These simplifications yield the following objective function: Equation 9 is identical to the equation used for effi-cient online dictionary learning by Mairal et al. (2009) with the one difference that we use a soft constraint on the magnitude of the entries of L ( L 2 regularization), whereas Mairal et al. use a hard length constraint on each column of L . 3.6. Convergence Guarantees Here, we provide proof sketches for three results; com-plete proofs are available in the Online Appendix. For simplicity of exposition, our analysis is performed in the setting where ELLA receives training data for a new task at each iteration. Therefore, the number of tasks learned T is always equal to the iteration num-ber m . Extending our analysis to the more general case outlined in Algorithm 1 is straightforward. Our convergence proof is closely modeled on the analysis by Mairal et al. (2009).
 We define the expected cost of a particular L as where we use a subscript on the expectation operator to denote which part of the expression is a random variable. The expected cost represents how well a par-ticular set of latent components can be used to repre-sent a randomly selected task given that the knowledge repository L is not modified.
 We show three results on the convergence of ELLA, given respectively as Propositions 1 X 3: 1. The latent model component matrix, L T , becomes 2. The value of the surrogate cost function,  X  g T ( L T 3. L T converges asymptotically to a stationary point These results are based on the following assumptions:
A. The tuples D ( t ) ,  X  ( t ) are drawn i.i.d. from a dis-
B. For all L , D ( t ) , and  X  ( t ) , the smallest eigenvalue Proposition 1: L T +1  X  L T = O 1 T .
 Proof sketch: First, we show that the L 2 regular-ization term bounds the maximum magnitude of each entry of L , and that the L 1 regularization term bounds the maximum magnitude of each entry of s ( t ) . Next, we show that  X  g T  X   X  g T  X  1 is Lipschitz with constant O 1 T . This result and the facts that L T  X  1 minimizes  X  g
T  X  1 and the L 2 regularization term ensures that the minimum eigenvalue of the Hessian of  X  g T  X  1 is lower bounded by 2  X  allow us to complete the proof. Before stating our next proposition, we define: and introduce the following lemma: Lemma 1:
A. min s ` ( L , s ,  X  ( t ) , D ( t ) ) is continuously differ-
B. g is continuously differentiable with  X  g ( L ) =
C.  X  L g ( L ) is Lipschitz on the space of latent model Proof sketch: Part (A) can be easily shown using the fact that  X  is continuous and by applying a corollary of Theorem 4.1 as stated by Bonnans &amp; Shapiro (1998) (originally shown by Danskin (1967)). Part (B) fol-lows directly since the tuple D ( t ) ,  X  ( t ) is drawn from a distribution with compact support. Part (C) of the lemma crucially relies on Assumption (B), which en-sures that the optimal sparse coding solution is unique. This fact, in combination with some properties that the optimal sparse coding solution must satisfy, allows us to prove that  X  is Lipschitz, which implies that  X  L g is Lipschitz due to the form of the gradient established in Parts (A) and (B).
 Proposition 2: A.  X  g T ( L T ) converges a.s.
 B.  X  g T ( L T )  X  g T ( L T ) converges a.s. to 0. C.  X  g T ( L T )  X  g ( L T ) converges a.s. to 0. D. g ( L T ) converges a.s.
 Proof sketch: First, we show that the sum of the pos-itive variations of the stochastic process u T =  X  g T ( L are bounded by invoking a corollary of the Donsker theorem ((Van der Vaart, 2000) Chapter 19.2, lemma 19.36, ex. 19.7). Given this result, we apply a theorem from (Fisk, 1965) to show that u t is a quasi-martingale that converges almost surely. The fact that u t is a quasi-martingale along with a simple theorem of posi-tive sequences allows us to prove part (B) of the propo-sition. The final two parts (C &amp; D) can be shown due to the equivalence of g and g T as T  X  X  X  .
 Proposition 3: The distance between L T and the set of g  X  X  stationary points converges a.s. to 0 as T  X  X  X  . Proof sketch: We employ the fact that both the sur-rogate  X  g T and the expected cost g each have gradi-ents that are Lipschitz with constant independent of T . This fact, in combination with the fact that  X  g T and g converge a.s. as T  X  X  X  , completes the proof. We evaluate ELLA against three other approaches: (1) GO-MTL (Kumar &amp; Daum  X e III, 2012), a batch MTL algorithm, (2) a perceptron-based approach to online multi-task learning (OMTL) (Saha et al., 2011), and (3) independent single-task learning (STL). GO-MTL provides a reasonable upper-bound on the ac-curacy of the models learned by ELLA (since it is a batch algorithm that optimizes all task models simul-taneously). We are chiefly interested in understanding the tradeoff in accuracy between models learned with ELLA and GO-MTL, and the computational cost of learning these models. The comparison to OMTL al-lows us to understand how the performance of ELLA compares with another approach designed to learn ef-ficiently in the lifelong learning setting. 4.1. Data Sets We tested each algorithm on four multi-task data sets: (1) synthetic regression tasks, (2) land mine detection from radar images, (3) identification of three different facial movements from photographs of a subject, and (4) predicting student exam scores. Data sets (2) and (4) are benchmark data sets for MTL. We introduce data set (3) as an MTL problem for the first time. Synthetic Regression Tasks We created a set of T max = 100 random tasks with d = 13 features and n t = 100 instances per task. The task parameter vectors  X  ( t ) were generated as a linear combination of k = 6 randomly generated latent components in R 12 . The vectors s ( t ) had a sparsity level of 0.5 (i.e., half the latent components were used to construct each  X  ( t ) ). The training data X ( t ) was generated from a standard normal distribution. The training labels were given independent univariate Gaussian noise. A bias term was added as the 13th feature prior to learning. Land Mine Detection In the land mine data set (Xue et al., 2007), the goal is to detect whether or not a land mine is present in an area based on radar images. The input features are automatically extracted from radar data and consist of four-moment based features, three correlation-based features, one energy-ratio feature, one spatial variance feature, and a bias term; see (Xue et al., 2007) for more details. The data set consists of a total of 14,820 data instances di-vided into 29 different geographical regions. We treat each geographical region as a different task.
 Facial Expression Recognition This data set is from a recent facial expression recognition challenge (Valstar et al., 2011). The goal is to detect the pres-ence or absence of three different facial action units (#5: upper lid raiser, #10: upper lip raiser, and #12: lip corner pull) from an image of a subject X  X  face. We chose this combination of action units to be a chal-lenge, since two of the action units involve the lower face, suggesting a high potential for transfer, while the other is an upper face action unit, suggesting a low potential for transfer. Each task involves recognizing one of the three action units for one of seven subjects, yielding a total of 21 tasks, each with 450 X 999 images. To represent the images, we utilized a Gabor pyramid with a frequency bandwidth of 0.7 octaves, orienta-tion bandwidth of 120 degrees, four orientations, 576 locations, and two spatial scales, yielding a total of 2,880 Gabor features for each image. We reduced the raw Gabor outputs to 100 dimensions using PCA, and added a bias term to produce the input features. London School Data The London Schools data set consists of examination scores from 15,362 students in 139 schools from the Inner London Education Author-ity. We treat the data from each school as a separate task. The goal is to predict the examination score of each student. We use the same feature encoding as used by Kumar &amp; Daum  X e III (2012), where four school-specific categorical variables along with three student-specific categorical variables are encoded as a collection of binary features. In addition, we use the examination year and a bias term as additional fea-tures, giving each data instance d = 27 features. 4.2. Evaluation Procedure Each data set was evaluated using 10 randomly gen-erated 50/50 splits of the data between a training and hold-out test set. The particular splits were stan-dardized across algorithms. For the online algorithms (ELLA and OMTL), we evaluated 100 randomized presentation orders of the tasks.
 The parameter values of k and  X  for ELLA and GO-MTL were selected independently for each algorithm and data set using a gridsearch over values of k from 1 to either 10 or 1 4 T max (whichever was smaller) and values of  X  from the set { e  X  5 ,e  X  2 ,e 1 ,e 4 } . For ELLA, we selected the parameter values based on the train-ing data alone. For a given training/test split, we further subdivided each training set into 10 random 50/50 sub-training/sub-validation sets and then chose parameter values that maximized the average perfor-mance on each of these 10 sub-validation sets. For GO-MTL, OMTL, and STL, the particular parameter values were selected to maximize test performance on the hold-out data averaged across all 10 random splits. Note that this procedure of choosing parameter values to maximize test performance provides ELLA with a disadvantage relative to the other algorithms. The two parameters (burn-in time and learning rate) for OMTL were optimized using a grid-search. The burn-in time was optimized over the set { 50 , 100 , 150 ,..., 400 } and the learning rate was op-timized over the set { e  X  30 ,e  X  29 ,...,e 0 } . We report results using the LogDet update rule for OMTL, and found that the results did not vary greatly when other rules were employed; see (Saha et al., 2011) for more details on OMTL. For STL, the ridge term for either logistic or linear regression was selected by performing a gridsearch over the set { e  X  5 ,e  X  4 ,...,e 5 } . Each task was presented sequentially to ELLA and OMTL, following the lifelong learning framework (Sec-tion 3.1). ELLA learned each new task from a sin-gle batch of data that contained all training instances of that task. For OMTL, which learns one instance at a time, we performed five passes over the training data for each task. We also tried using more than five passes, but the OMTL model accuracy did not increase further. GO-MTL was considered to have converged when either its objective function value decreased by less than 10  X  3 or 2,000 iterations were executed. We measured predictive performance on the classifi-cation problems using the area under the ROC curve (AUC). This particular performance metric was cho-sen since both classification data sets had highly bi-ased class distributions, and therefore other metrics like misclassification rate would only be informative for specific applications with well-specified tradeoffs between true and false positives. For regression prob-lems, the performance was evaluated using the neg-ative root mean-squared error (-rMSE) metric (with -rMSE, higher numbers indicate better performance). Recall that OMTL does not support regression, and so we do not evaluate it on the regression tasks. The computational cost of each algorithm was mea-sured using wall-clock time on a Mac Pro computer with 8GB RAM and two 6-core 2.67GHz Intel Xeon processors. We report the running time for the batch GO-MTL algorithm, and the speedup that ELLA, STL, and OMTL obtain relative to the batch algo-rithm both for learning all tasks and for learning each consecutive new task. We optimized the implementa-tions of all algorithms to ensure a fair comparison. 4.3. Results For classification problems, ELLA achieves nearly identical performance to GO-MTL (Table 1) while reg-istering speedups of at least 1,350 times for learning all tasks and 38,400 times for learning each new task (Table 2). In addition, OMTL, which is specifically designed for learning efficiently online, achieved sig-nificantly worse accuracy on land mine detection and moderately worse accuracy on facial expression recog-nition. While OMTL did run much faster than GO-MTL, its speed did not match ELLA X  X . STL was the fastest approach (ELLA is necessarily slower than STL since STL is used as a subroutine inside ELLA), but had lower accuracy than ELLA.
 We find similar results for the regression problems, with ELLA achieving nearly identical accuracy to GO-MTL (within 1 . 1% for real data and 2 . 3% for syn-thetic data; see Table 1), while achieving dramatically shorter learning times when learning all tasks (mini-mum speedup of 2,721 times) and each new task (min-imum speedup of 378,219 times). STL was again the fastest of all approaches, but had lower accuracy. Recall that ELLA does not re-optimize the value of s ( t ) unless it receives new training data for task t . There-fore, in each experiment, the value of s ( t ) is set when the training data for that task is presented and never readjusted. Although the values of s ( t ) are not up-dated, it is still possible that previously learned task models can benefit from training on subsequent tasks through modifications to L .
 To assess whether this phenomenon of reverse transfer occurred, we computed the change in accuracy from when a task was first learned until after all tasks had been learned. A positive change in accuracy for a task indicates that reverse transfer did occur. Figure 2 shows this change in accuracy as a function of posi-tion in the task sequence, revealing that reverse trans-fer occurred reliably in all data sets and that reverse transfer is most beneficial for tasks that were learned early (when the total amount of training data seen was low). Most importantly, these results show that, with few exceptions, subsequent learning did not reduce the performance of models that were learned early. We have presented an efficient algorithm for lifelong learning (ELLA) that provides nearly identical accu-racy to batch MTL, while requiring three orders of magnitude less runtime. Also, ELLA is more flexible, faster, and achieves better accuracy than a competing method for online MTL. We have shown that ELLA works well on synthetic data as well as three multi-task problems. Additionally, we discussed ELLA X  X  connec-tions to online dictionary learning for sparse coding, Delta Accuracy and presented theoretical guarantees that illuminate the reasons for ELLA X  X  strong performance. Our fu-ture work will include extending ELLA to settings be-sides linear and logistic models and automatically ad-justing the basis size k as it learns more tasks. This research was supported by ONR grant N00014-11-1-0139. We thank Terran Lane, Diane Oyen, and the anonymous reviewers for their helpful feedback. Bonnans, J.F. and Shapiro, A. Optimization problems with perturbations: A guided tour. SIAM review , 40 (2):228 X 264, 1998.
 Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hr-uschka Jr., E.R., and Mitchell., T.M. Toward an architecture for never-ending language learning. In
Proceedings of the 24th Conference on Artificial In-telligence . AAAI Press, 2010.
 Danskin, J.M. The theory of max-min and its appli-cation to weapons allocation problems , volume 5 of Econometrics and Operations Research. Springer-Verlag, 1967.
 Fisk, D.L. Quasi-martingales. Transactions of the
American Mathematical Society , 120(3):369 X 389, 1965.
 Kumar, A. and Daum  X e III, H. Learning task group-ing and overlap in multi-task learning. In Proceed-ings of the 29th International Conference on Ma-chine Learning , pp. 1383 X 1390. Omnipress, 2012. Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online dictionary learning for sparse coding. In Proceedings of the 26th Annual International Conference on Ma-chine Learning , pp. 689 X 696. ACM, 2009.
 Rai, P. and Daum  X e III, H. Infinite predictor subspace models for multitask learning. In Proceedings of the 13th International Conference on Artificial Intelli-gence and Statistics , pp. 613 X 620, 2010.
 Ring, M.B. CHILD: A first step towards continual learning. Machine Learning , 28(1):77 X 104, 1997. Saha, A., Rai, P., Daum  X e III, H., and Venkatasubra-manian, S. Online learning of multiple tasks and their relationships. In Proceedings of the 14th In-ternational Conference on Artificial Intelligence and Statistics , pp. 643 X 651, 2011.
 Simm, J., Sugiyama, M., and Kato, T. Computation-ally efficient multi-task learning with least-squares probabilistic classifiers. IPSJ Transactions on Com-puter Vision and Applications , 3:1 X 8, 2011.
 Sutton, R., Koop, A., and Silver, D. On the role of tracking in stationary environments. In Proceedings of the 24th International Conference on Machine Learning , pp. 871 X 878, Corvallis, OR, 2007. ACM. Thrun, S. Explanation-Based Neural Network Learn-ing: A Lifelong Learning Approach . Kluwer Aca-demic Publishers, Boston, MA, 1996.
 Thrun, S. and O X  X ullivan, J. Discovering structure in multiple learning tasks: the TC algorithm. In Proceedings of the 13th International Conference on
Machine Learning , pp. 489 X 497. Morgan Kaufmann, 1996.
 Valstar, M.F., Jiang, B., M  X ehu, M., Pantic, M., and
Scherer, K. The first facial expression recognition and analysis challenge. In Proceedings of the IEEE
International Conference on Automatic Face &amp; Ges-ture Recognition , pp. 921 X 926. IEEE, 2011.
 Van der Vaart, A.W. Asymptotic statistics , volume 3. Cambridge University Press, 2000.
 Xue, Y., Liao, X., Carin, L., and Krishnapuram, B.
Multi-task learning for classification with Dirichlet process priors. Journal of Machine Learning Re-search , 8:35 X 63, 2007.
 Yu, K.B. Recursive updating the eigenvalue decom-position of a covariance matrix. IEEE Transactions on Signal Processing , 39(5):1136 X 1145, 1991.
 Zhang, J., Ghahramani, Z., and Yang, Y. Flexible latent variable models for multi-task learning. Ma-
