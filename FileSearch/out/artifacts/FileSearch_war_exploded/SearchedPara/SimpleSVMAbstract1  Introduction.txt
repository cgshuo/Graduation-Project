 S.V.N. Vishwanathan vishy@axiom.anu.edu.au Alexander J. Smola Alex.Smola@anu.edu.au M. Narasimha Murty mnm@csa.iisc.ernet.in Support Vector Machines (SVM) have gained promi-nence in the field of machine learning and pattern clas-sification. In this paper we propose a fast iterative active set method for training a SVM. 1.1. Notation and Background In the following we denote by { ( x 1 , y 1 ) , . . . , ( x X  X { X  1 } the set of labeled training samples, where x i are drawn from some domain X and y i  X  { X  1 } , denotes the class labels +1 and  X  1 respectively. Fur-thermore, let n be the total number of points and let n + and n  X  denote the number of points in class +1 and  X  1 respectively. Denote by k : X  X  X  X  R a Mercer kernel and by  X  : X  X  F the corresponding feature map, that is  X   X ( x ) ,  X ( x 0 )  X  = k ( x, x 0 ). It is well known that the optimal separating hyper-plane between the sets with y i = 1 and y i =  X  1 is spanned by a linear combination of points in feature space (Sch  X olkopf &amp; Smola, 2002). Consequently the classification rule can be expressed in terms of dot products in feature space and we have where  X  j  X  0 is the coefficient associated with a sup-port vector x j and b is an offset.
 In the case of a hard-margin SVM, all SVs satisfy y f ( x i ) = 1 and for all other points we have y i f ( x i 1. Furthermore (to account for the constant offset b ) we have the condition P i y i  X  i = 0. This means that if we knew all SVs beforehand, we could simply find the solution of the associated quadratic program by a simple matrix inversion.
 To cope with errors, a soft margin loss function was in-troduced by Bennett and Mangasarian (1993), leading to various quadratic optimization problems depend-ing on the type of error penalty used. For quadratic penalty it is well known (Cortes &amp; Vapnik, 1995) that such loss functions give rise to a modified hard-margin SV problem, 1 where the kernel k ( x, x 0 ) is replaced by k ( x, x 0 ) +  X  X  x,x 0 for some  X  &gt; 0. Again, if we knew which vectors become SVs beforehand, we could find the solution by a simple matrix inversion.
 Finally, for the linear soft margin formulation, matters are somewhat more complicated since SVs could be margin SVs or margin errors. Here, if we knew the sets of margin errors, margin SVs and their complement, the points classified correctly with a minimum margin, beforehand, we could solve the optimization problem by solving a linear system.
 Such a strategy would be particularly useful if the number of unconstrained SVs was comparatively small with respect to the overall size of the dataset, since in such cases the search for the unconstrained SVs is rather swift. 1.2. The Basic Idea We propose an active set method inspired by the chunking strategies used in the early days of SVs at AT&amp;T: given an optimal solution on a subset, add only one point to the set of SVs at a time and compute the exact solution.
 If we had to re-compute the solution from scratch ev-ery time a new point is added, this would be an ex-tremely wasteful procedure. Instead, as we will see below, it is possible to perform such computations at O ( m 2 ) cost, where m is the number of current SVs and obtain the exact solution on the new subset of points. As one would expect, this modification works well whenever the number of SVs is small relatively to the size of the dataset, that is, for  X  X lean X  datasets. In many cases the kernel matrix may be rank degen-erate or may be approximated by a low rank matrix (Sch  X olkopf &amp; Smola, 2002). In case the kernel matrix is rank degenerate of rank l , updates can be performed at O ( ml ) cost using a novel factorization method of Smola and Vishwanathan (2003), thereby further re-ducing the computational burden (see Vishwanathan (2002) for technical details).
 To cope with  X  X irty X  datasets or the linear soft margin SVM another modification is needed: whenever a point is classified as a margin error (i.e., whenever it hits the upper boundary), it is removed from the current working set and the value of its Lagrange multiplier  X  i is frozen until the point is re-visited.
 While there is no guarantee that one sweep through the data set will lead to a full solution of the opti-mization problem (and it almost never will, since some points may be left out which will become SVs at a later stage and a similar cautionary note applies to margin errors), we empirically observe that a small number of passes through the entire dataset (less than Algorithm 1 Hard-Margin SimpleSVM input Dataset Z
Initialize: Find a close pair from opposing classes A  X  X  i + , i  X  }
Compute f and  X  for A while there are x v with y v f ( x v ) &lt; 1 do end while
Output: A , {  X  i for i  X  A } 10) is sufficient for the algorithm to converge. Algo-rithm 1 gives a high-level description of SimpleSVM in the hard-margin case. 1.3. Outline of the Paper In the following section we describe the active set method underlying SimpleSVM. This section also in-cludes a proof that all algorithms of the SimpleSVM family have linear convergence rate and converge in fi-nite time (this, of course, does not preclude each step from taking up to quadratic time in size of the current active set). Subsequently, Section 3 discusses imple-mentation details and initialization strategies. Exper-imental evidence of the performance of SimpleSVM is given in Section 4 and we compare it to other state-of-the art optimization methods. We conclude with a discussion in Section 5.
 Due to space limitations, most technical details con-cerning the factorization of matrices and rank-one modifications is relegated to Vishwanathan (2002). The latter plus initial experimental code will be made freely available at http://www.axiom.anu.edu.au/ ~vishy . 1.4. Related Work Cauwenberghs and Poggio (2001) proposed an incre-mental SVM algorithm for the hard-margin case where at each step only one point is added to the training set and one re-computes the exact SV solution of the whole dataset seen so far . Although similar in spirit to SimpleSVM this algorithm suffers from a serious drawback: While adding one point at a time converges in a fi-nite number of steps (clearly after n additions we have seen the whole training set), the condition to remain optimal at every step means that at every step the algorithm has to test and potentially train on all the observations seen so far. Such a requirement is clearly expensive. The authors suggest various online variants to alleviate the problem (e.g., by introducing tolerance terms into the margin definition), however the main problem remains.
 The way to overcome the above limitations is to drop the requirement of optimality with respect to all the data seen so far and only require that the new solution strictly decrease the value of the dual objective func-tion (in SV terms the margin of separation) and be op-timal with respect to a subset of variables. The advan-tage of doing this is twofold: firstly, we can deal with a larger family of optimization problems and SVM for-mulations. Secondly, we can maintain and update a numerically stable LDL &gt; factorization of a subset of the kernel matrix.
 Similar steps are proposed in DirectSVM (Roobaert, 2000), which starts off with a pair of points in the candidate SV set. It works on the conjecture that the point which incurs the maximum error (i.e., minimal y f ( x i )) during each iteration is a SV. This is a heuris-tic and as such it may fail. In DirectSVM X  X  case this invokes a random restart of the algorithm, as it has no provisions to backtrack and remove points from the kernel expansion. Instead of describing the technical details of Sim-pleSVM in terms of SVs, it is advantageous to con-sider general constrained optimization problems and describe the SimpleSVM family in terms of an active set method. This alleviates the need of an immediate  X  X eometric X  interpretation of every optimization step. Assume a constrained optimization problem with a small-to-moderate number of equality constraints and a large number of (easy-to-check) box constraints. This situation occurs in all currently known SV set-tings, be it classification, regression or novelty detec-tion, independent of the parameterization (  X  vs. C ), independent of the number of parametric basis func-tions (bias, semiparametric estimation, etc.), and in-dependent of the loss function used (as long as it is piecewise constant, linear, quadratic, or infinite). See Sch  X olkopf and Smola (2002) for explicit formulations. Consider where H  X  R m  X  m is a positive semidefinite matrix,  X , c  X  R m , A  X  R d  X  m , and b  X  R d . In general, interior point codes are among the most efficient ones to solve this type of problems.
 However, if the problem or its solution has special structure, faster solutions can be obtained. This is quite often the case in SVMs, since only a small, and, in the large sample size limit, negligible, fraction of Support Vectors actually ends up lying on the margin for linear soft-margin loss (Sch  X olkopf &amp; Smola, 2002). As a consequence, out of the large number of Lagrange multipliers, almost all end up hitting either the lower constraint  X  i = 0 or upper constraint  X  i = C , thereby rendering the optimization problem much simpler than a direct solution suggests. 2.1. The Algorithm Denote by S 0 := { i |  X  i = 0 } , S C := { i |  X  i = C } , and S work := { i |  X  i  X  (0 , C ) } such that S 0  X  S work  X  S [1 : m ]. In analogy to that denote by  X  0 ,  X  C ,  X  work corresponding sets of variables. Furthermore denote by O work the optimization problem (2), constrained to the  X  work , while keeping  X  0 ,  X  C fixed: minimize subject to 0  X   X  i  X  C and A w  X  work = b  X  A C  X  C Here H ww is the (quadratic) submatrix of H given by the entries determined by S work only, H wC is the (rectangular) submatrix of H formed by selecting rows from S work and columns from S C , and finally c w , A w and A C are the submatrices of c and A arising by selecting columns via S work and S C respectively. Finally assume that we have an initial  X  for which  X  work solves O work and satisfies the constraints im-posed by (2). The algorithm works as follows: 1. At each step add one variable from S C or S 0 2. Check whether the resulting solution is optimal This simple algorithm has several desirable properties: it is relatively simple to implement (see next section) and it enjoys attractive convergence properties: Proposition 1 The active set algorithm described above converges in a finite number of steps to exact solution of (2). The convergence is linear for positive definite H .
 Proof Observe that the dual objective function must strictly improve at every step. This is so since at every step we proceed from a sub-optimal solution (we add in a variable at a time which does not satisfy the Kuhn-Tucker conditions yet) to an optimal solution in these variables.
 Next note that the value after each step of the opti-mization only depends on the choice of sets S 0 , S work , and S C . Since there exists only a finite number of them and the algorithm cannot cycle (since we make steady progress at each step), we must reach the opti-mal partition in finite time.
 Finally, to show linear convergence, note that we are performing updates which are strictly better than coordinate descent at every step (in coordinate descent we only optimize over one variable at a time, whereas in our case we optimize over S work which includes a new variable at every step). Coordinate descent, however, has linear convergence for strictly convex functions (Fletcher, 1989). 2.2. Applying the Optimality Conditions It follows from Farkas X  Lemma (Fletcher, 1989) that for the optimization problem the optimal solution in  X  can be found by requir-ing that the gradient outside the constraints vanishes. This means that we are solving the linear system If we add one variable to (4), the overall matrix only changes by one row/column, making it the perfect can-didate for a rank-one update (the cost is quadratic rather than cubic for a full matrix inversion), which can be done very efficiently .
 Note, however, that we have to solve the box-constrained optimization problem (2) instead of (4). So unless the unconstrained solution ends up lying in-side the box we can use the unconstrained solution only as an indication on how to proceed in the op-timization. Moving towards the unconstrained part along the boundary guarantees that we progress in the objective function. Once we hit the boundary this will remove one variable from the set of free variables 2 Finally, observe that by moving on the line between the old set of variables satisfying the equality con-straints and the new solution (possibly violating the box constraints) we will always strictly satisfy the equality constraint A X  = b . Hence we will remain strictly feasible at every step. 2.3. Selecting new Variables The issue of selecting a new variable, say i , to be added to the working set S work is relatively straightforward: for the convergence of our algorithm we need to ensure that each variable is visited in a sweep through the dataset. However, it only pays to choose it if we can make progress in this variable. For this purpose we check the gradient after correction by the constraints imposed by (5).
 Note that the set of variables we started with, was optimal in the free variables, hence it solves the un-modified system (5). Furthermore  X  is always strictly feasible in the constraints. Given the particular form of the RHS of (5), which is determined by (3), only the i -th row of the system (5) may not be satisfied after enlarging it by one row and column and updating its RHS. This means that we need to check or, in short g i (  X ,  X  ) := [ H X  ] i + [ A &gt;  X  ] i + c Optimization theory tells us that if g i (  X ,  X  ) &gt; 0, we can make progress by shrinking  X  i , and likewise, we can decrease the objective function by increasing  X  i if g i (  X ,  X  ) &lt; 0. This means that only if  X  i = 0 and g &lt; 0 or alternatively  X  i = C and g i &gt; 0 we need to consider this point for optimization. 2.4. SVM Interpretation We now return to the SV formulation for some more geometric intuition.
 In our case H is the kernel matrix, possibly adorned by the labels, that is H ij = y i y j k ( x i , x j ). The vector c has entries all  X  1 (for C -classification) and c = 0 for  X  -classification. Moreover, A = ( y 1 , . . . , y n ) &gt; sification and larger constraint matrices are found for  X  -SVM and semiparametric estimation. In the hard margin case, C =  X  , that is, we ignore upper con-straints.
 Having a set of Lagrange multipliers satisfying the equality constraint A X  = b means, in SV language, that the free variables (constant offset, margin, or semiparametric terms) of the optimization problem are optimally determined. In particular, their values are given by  X  from (5). A detailed description on free variables and dual constraints can be found in Sch  X olkopf and Smola (2002).
 Adding a variable to the set of free variables means that we want to find a (partial) solution for which the contributions of the other points to the weight vec-tor w are kept fixed. For hard margin or quadratic soft-margin SVM this simply means that we ignore points which currently are not SVs. In the case of a linear soft-margin formulation it means that we also ignore points which have turned into margin errors, while keeping their contribution to the weight vector w in (1) fixed.
 In SV terms the selection criterion (6) g i (  X ,  X  ) 6 = 0 means that we add only points which are erroneously flagged as margin errors (  X  i = C but y i f ( x i ) &gt; 1) or those which turn out to be margin errors but with van-ishing Lagrange multiplier (  X  i = 0). The connection to (6) stems from the fact that In this sense it is quite obvious why Algorithm 1 is a special instance of the optimization steps discussed above. However, it would be much more difficult to describe the updates in SV language than it is in a purely algebraic description. This section contains details on initialization strate-gies plus cases where we have rank degenerate kernel matrices. 3.1. Initialization Since we want to find the optimal separating hyper-plane of the overall dataset Z , a good starting point is the pair of observations ( x + , x  X  ) from opposing sets X + , X  X  closest to each other (Roobaert, 2000). This means that we already start with a relatively small upper bound on the margin.
 Brute force search for this pair costs O ( n 2 ) kernel eval-uations, which is clearly not acceptable for the search of a good starting point. Instead one may use one of the following two operations: an iterative scheme which will find the closest pair in log-linear time, or a randomized method, which will find a pair almost as good as the best pair in constant time.
 Algorithm 2 Closest Pair Input: Sets X + , X  X 
Initialize: Draw x + , x  X  at random from X + , X  X  . repeat Output: x + , x  X  The Best Pair: Algorithm 2 finds the best pair by A Randomized Method: denote by  X  := d ( x + , x  X  ) Once a good pair ( x + , x  X  ) has been found, we use the latter as S work (for C -SVM the vector  X  = 0 is feasible) and begin with the optimization.
 Whenever we have upper box constraints  X  i  X  C and somewhat more complex equality constraints (e.g., in  X  -SVM), that is, cases where a single pair of points cannot influence the outcome by too much, a simple random initialization has proven to yield good results. After all, the algorithm finds the sets S 0 , S C and S work relatively quickly. 3.2. Rank-Degenerate Kernels Regardless of the type of matrix factorization we use to compute the SV solutions, we still encounter the prob-lem that the memory requirements scale as O ( m 2 ) and the overall computation is of the order of O ( m 3 + mn ) for the whole algorithm (recall that m is the total num-ber of SVs). This may be much better than other methods (see Section 4 for details), yet we would like to take further advantage of kernels which are rank degenerate, that is, if k ( x, x 0 ) can be approximated on the training set X by z ( x ) z ( x 0 ) &gt; where z ( x )  X  with l m (in the following we assume that this ap-proximation is exact). See (Sch  X olkopf &amp; Smola, 2002; Fine &amp; Scheinberg, 2000) for details how such an ap-proximation can be obtained efficiently. This means that the kernel matrix to be used in the quadratic soft margin algorithm can be written as where Z ij := z j ( x i ) and Z  X  R n  X  l . Extending the work of Fine and Scheinberg (2000) recently an al-gorithm was proposed by Smola and Vishwanathan (2003) which allows one to find an LDL &gt; factoriza-tion of K in O ( nl 2 ) time and which can be updated efficiently in O ( nl ) time. This technique of using a low rank matrix decomposition is faster than using the full matrix inversion. Technical details about the factor-ization can be found in Vishwanathan (2002). Alternatively, a Sherman-Morrison-Woodbury formu-lation could be used, albeit at the expense of much reduced numerical stability. Since the main goal of the paper is to give an al-gorithmic improvement over existing SVM training algorithms, we will not report generalization perfor-mance figures here. 3 Instead, we will compare our method with the performance of the NPA algorithm for the quadratic soft-margin formulation (Keerthi et al., 1999) and the popular SMO algorithm for the linear soft-margin formulation (Platt, 1999), as both are comparable in speed to other methods such as SVMLight (Joachims, 1999).
 Following (Keerthi et al., 1999), we compare the num-ber of kernel evaluations performed by a support vec-tor algorithm as an effective measure of its speed. The latter is relevant in particular if the kernel evaluations are expensive (this happens to be the case with most custom-tailored kernels). Other measures are fraught with difficulty, since comparing different implementa-tions, compilers, platforms, operating systems, etc., causes a large amount of variation even between iden-tical algorithms. 4.1. Experimental Setup and Datasets We uniformly used a value of 1 e  X  5 for the error bound i.e. we stop the algorithm when  X  i max( y i f ( x i )  X  1 , 0)+ we stop if the contribution of every point to the KKT gap (Sch  X olkopf &amp; Smola, 2002) is less than 10  X  5 . This is a much more stringent requirement than what can typically be satisfied with other optimization codes in practice, the exception being interior point methods. The NPA results are those reported by Keerthi et al. (1999). For the sake of comparability we used the same kernel, namely a Gaussian RBF kernel, for all the ex-periments (including the SMO and the NPA), i.e. The datasets chosen for our experiments are described in Table 1. The Spiral dataset was proposed by Alexis Wieland of MITRE Corporation and it is available from the CMU Artificial Intelligence repository. Both WSPBC and the Adult datasets are available from the UCI Machine Learning repository (Blake &amp; Merz, 1998). We used the same values of  X  2 as in Keerthi et al. (1999) and Platt (1999) to allow for a fair com-parison. Note that in general lower values of the regu-larization parameter imply larger number of Support Vectors and vice versa. We compare the scaling be-haviour of various algorithms with respect to the num-ber of Support Vectors by varying the regularization parameter. Experimental results can be found in Fig-ures 2, 3 and 4.
 4.2. Discussion of the Results As can be seen SimpleSVM outperforms the NPA con-siderably on all five datasets. For instance, on the Spiral dataset the SimpleSVM is an order of mag-nitude faster than the NPA (for C 0 &gt; 5). On the Adult-4 dataset for some values of the regularization constant the SimpleSVM algorithm is nearly 50 times faster than the NPA. SimpleSVM also outperforms the SMO when the number of margin SV X  X  is reasonably small. For the extreme case when the number of mar-gin SV X  X  is a significant fraction of the dataset SMO tends to require fewer number of kernel computations. This is exactly what one expects from an algorithm which is geared towards the situation where there are only small numbers of margin SVs.
 Furthermore, unlike NPA and SMO, SimpleSVM X  X  runtime behaviour, given by the number of kernel eval-uations, does not critically depend on the value of the regularization constant. It strictly outperforms NPA, in most cases by more than one order of magnitude. Given a Support Vector set, the solution obtained by SimpleSVM is exact within machine precision, whereas algorithms such as NPA and SMO will only yield ap-proximate expansions for the same Support Vector set. The good performance is due to the fact that often, e.g., in the quadratic soft-margin case, we observed that, we do not require to cycle through the dataset many times (at most 2 -3), indicating that a  X  X rong X  support vector is rarely picked up or removed from the active set. We presented a new SV training algorithm that is effi-cient, intuitive, fast and numerically stable. It signifi-cantly outperforms other iterative algorithms like the NPA and SMO in terms of the number of kernel com-putations. Moreover, it does away with the problem of overall optimality on all previously seen data that was one of the major drawbacks of Incremental SVM, as proposed by Cauwenberghs and Poggio (2001). It should be noted that SimpleSVM performs particu-larly well whenever the datasets are relatively  X  X lean X , that is, whenever the number of SVs is rather small. On noisy data, on the other hand, methods such as SMO may be preferable to our algorithm. This is mainly due to the fact that we need to store the LDL &gt; factorization of H ww in main memory (256 MB of main memory suffice to store a matrix corresponding to as many as 10 , 000 SVs). Storage therefore becomes a se-rious limitation of SimpleSVM when applied to generic dense matrices on large noisy datasets. One possibility to address this problem is to use low-rank approxima-tion methods which make the problem amenable to the low-rank factorizations described in Section 3.2. Due to the LDL &gt; factorization used in finding the SV solution our algorithm is numerically more stable than using a direct matrix inverse. This helps us deal with round off errors that can plague other algorithms. We suspect that similar modifications could be success-fully applied to other algorithms as well.
 At present, our algorithm does not use any kind of ker-nel cache to reuse kernel computations (which would further reduce the number of kernel evaluations re-quired). The design of an efficient caching scheme to scale up the behaviour of our algorithm is currently being investigated.
 It can be observed that the addition of a vector to the support vector set is entirely reversible. Using this property and following the derivation in Cauwenberghs and Poggio (2001) we can calculate leave one out er-rors.
 Acknowledgements S.V.N. Vishwanathan was supported by a Infosys Fel-lowship. Alexander J. Smola was supported by a grant of the Australian Research Council. We thank Prof. Sathiya Keerthi for useful comments and discus-sion.
 Bennett, K. P., &amp; Mangasarian, O. L. (1993). Multi-category separation via linear programming. Opti-mization Methods and Software , 3 , 27 X 39.
 Blake, C. L., &amp; Merz, C. J. (1998). UCI repository of machine learning databases.
 Cauwenberghs, G., &amp; Poggio, T. (2001). Incremental and decremental support vector machine learning.
Advances in Neural Information Processing Systems 13 (pp. 409 X 415). MIT Press.
 Cortes, C., &amp; Vapnik, V. (1995). Support vector net-works. Machine Learning , 20 , 273 X 297.
 Fine, S., &amp; Scheinberg, K. (2000). Efficient SVM train-ing using low-rank kernel representation (Technical Report). IBM Watson Research Center, New York. Fletcher, R. (1989). Practical methods of optimization . New York: John Wiley and Sons.
 Joachims, T. (1999). Making large-scale SVM learn-ing practical. Advances in Kernel Methods X  X upport Vector Learning (pp. 169 X 184). Cambridge, MA: MIT Press.
 Keerthi, S. S., Shevade, S. K., Bhattacharyya, C., &amp;
Murthy, K. R. K. (1999). A fast iterative nearest point algorithm for support vector machine classifier design (Technical Report Technical Report TR-ISL-99-03). Indian Institute of Science, Bangalore. Platt, J. (1999). Fast training of support vector ma-chines using sequential minimal optimization. Ad-vances in Kernel Methods X  X upport Vector Learning (pp. 185 X 208). Cambridge, MA: MIT Press.
 Roobaert, D. (2000). DirectSVM: A simple support vector machine perceptron. Neural Networks for Signal Processing X X  X roceedings of the 2000 IEEE Workshop (pp. 356 X 365). New York: IEEE.
 Sch  X olkopf, B., &amp; Smola, A. J. (2002). Learning with kernels . MIT Press.
 Smola, A. J., &amp; Vishwanathan, S. V. N. (2003).
Cholesky factorization for rank-k modifications of diagonal matrices. SIAM Journal of Matrix Analy-sis . in preparation.
 Vishwanathan, S. V. N. (2002). Kernel methods: Fast algorithms and real life applications . Doctoral dis-sertation, Indian Institute of Science, Bangalore, In-
