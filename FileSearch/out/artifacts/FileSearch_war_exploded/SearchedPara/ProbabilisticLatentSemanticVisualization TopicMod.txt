 We prop ose a visualization metho d based on a topic mo del for discrete data suc h as documen ts. Unlik e con ventional visualizatio n metho ds based on pairwise distances suc h as multi-dimensional scaling, we consider a mapping from the visualizatio n space into the space of documen ts as a gener-ativ e pro cess of documen ts. In the mo del, both documen ts and topics are assumed to have laten t coordinates in a two-or three-dimensional Euclidean space, or visualization space. The topic prop ortions of a documen t are determined by the distances between the documen t and the topics in the vi-sualizati on space, and eac h word is dra wn from one of the topics according to its topic prop ortions. A visualization, i.e. laten t coordinates of documen ts, can be obtained by tting the mo del to a given set of documen ts using the EM algo rithm, resulting in documen ts with simila r topics being embedded close together. We demonstrate the e ectiv eness of the prop osed mo del by visualizing documen t and movie data sets, and quan titativ ely compare it with con ventional visualizatio n metho ds.
 H.2.8 [ Database Manageme nt ]: Database Applications| Data Mini ng ; I.2.6 [ Arti cial Intelli gence ]: Learning; I.5.1 [ Pattern Recognition ]: Mo del| Statistic al Algorithms Visualizatio n, Topic mo del, Probabilisti c laten t seman tic analysis
Recen tly there has been great interest in topic mo dels for analyzing documen ts and other discrete data. A topic mo del is a hierarc hical probabilistic mo del, in whic h a documen t is mo deled as a mixture of topics, where a topic is mo deled as a probabilit y distribution over words. Probabilistic Laten t Seman tic Analysis (PLSA) [10] and Laten t Diric hlet Allo ca-tion (LD A) [4] are represen tativ e topic mo dels, and they are used for a wide variet y of applications suc h as information retriev al, text clustering and collab orativ e ltering.
In this pap er, we prop ose a nonlinear visualizatio n metho d based on a topic mo del, whic h we call Probabili stic Latent Semantic Visualization (PLSV). Visualization is a useful tool for understandin g complex and high dimensional data, and it enables us to bro wse intuitiv ely through huge amoun ts of data. A num ber of documen t visualization metho ds have been prop osed [8, 22], and the imp ortance of visualizing doc-umen ts is increasing since documen ts suc h as web pages, blogs, e-mails, paten ts and scien ti c articles are being accu-mulated rapidly .

In PLSV, we consider a mapping from the visualization space into the space of documen ts as a generativ e pro cess of documen ts. Both documen ts and topics are assumed to have laten t coordinates in a two-or three-dimensional Eu-clidean space, or visualization space. The topic prop ortions of a documen t are determined by the Euclidean distances be-tween the documen t coordinate and the topic coordinates. If a documen t is located near a topic, the probabilit y that the documen t has the topic becomes high. Eac h word in a documen t is dra wn from one of the topics according to its topic prop ortions, as in other topic mo dels. The parameters in PLSV including laten t coordinates of documen ts can be estimated by tting the mo del to the given set of documen ts using the EM algori thm.

A num ber of visualization metho ds have been prop osed, suc h as Multi-Dimensional Scaling (MDS) [20], Isomap [19] and Locally Linear Em bedding (LLE) [17] . However, most of these metho ds tak e no accoun t of the laten t structure in the given data suc h as topics in the case of documen t data. For example, MDS embeds samples so that pairwise distances in the visualizatio n space accurately re ect pair-wise distances in the origi nal space, and it does not consider topics explicitly . On the other hand, because PLSV consid-ers topics, documen ts with simila r seman tics are embedded close together even if they do not share any words.
PLSA or LD A can extract a low-dimensional represen ta-tion of a documen t as topic prop ortions. However, they are not appropriate for visualization since they cannot express more than three topics in the two-dimensional visualization space, and the represen tation is an embedding in the simplex space but not in the Euclidean space. In con trast, PLSV can express any num ber of topics even with a two-dimensional represen tation, and it embeds documen ts in the Euclidean space, whic h is a metric space that most closely corresp onds to our intuitiv e understanding of space. The topic prop or-tions estimated by PLSA or LD A can be embedded in the Euclidean space by Parametric Em bedding (PE) [11], whic h can emplo y a set of topic prop ortions as input. However, the topic prop ortions may not be suitable when they are embedded in the visualization space since these topics are estimated in a di eren t space from the visualization space. Moreo ver, errors accum ulated in the topic estimation pro-cess with PLSA or LD A cannot be corrected in the embed-ding pro cess with PE since they are mo dularized, and this metho d may result in poor visualization. On the other hand, since PLSV sim ultaneously estimates topics and visualizes documen ts in one probabilistic framew ork, topics are esti-mated so as to be optimal when documen ts are embedded in the two-or three-dimensional visualization space. The remainder of this pap er is organized as follows. In Section 2, we form ulate PLSV, and describ e its parame-ter estimatio n pro cedures. In Section 3, we brie y review related work. In Section 4, we demonstrate the e ectiv e-ness of PLSV by visualizing documen t and movie data sets, and quan titativ ely compare it with con ventional visualiza-tion metho ds. Finally , we presen t concluding remarks and a discussion of future work in Section 5.
In the following discussion, we assume that the given data are documen ts. However, PLSV is applicable to other dis-crete data, suc h as purc hase logs in collab orati ve ltering and gene sequences in bioinformatics.

Suppose that we have a set of N documen ts C = f w n g N n =1 Eac h documen t is represen ted by a sequence of M n words de-noted by w n = ( w n 1 ; ; w nM n ), where w nm 2 f 1 ; ; W g is the m th word in the sequence of the n th documen t, M is the num ber of words in the n th documen t, and W is the vocabulary size.

PLSV is a probabilistic topic mo del for nd ing the embed-ding of documen ts with coordinates X = f x n g N n =1 , where x n = ( x n 1 ; ; x nD ) is a coordinate of the n th documen t in the visualization space, and D is its dimensionalit y, usually D = 2 or 3. We assume that there are Z topics indexed by f z g Z z =1 , and eac h topic has its asso ciated coordinate ( z 1 ; ; zD ) in the visualization space. The topic prop or-tion of a documen t is determined by its Euclidean distances from topics in the visualizatio n space as follo ws: where P ( z j x n ; ) is the z th topic prop ortion of the n th doc-umen t, P Z z =1 P ( z j x n ; ) = 1, = f z g Z z =1 is the set of topic coordinates, and k k represen ts the Euclidean norm in the visualization space. If we assume this topic prop or-tion, when the Euclidean distance between coordinates of documen t x n and topic z is small , the topic prop ortion P ( z j x n ; ) becomes high. Since documen ts located close to-gether in the visualizatio n space are similar distances from topic coordinates, they have simila r topic prop ortions. Figure 1: Graphical mo del represen tation of PLSV.
PLSV assumes the follo wing generativ e pro cess for a set of documen ts C : 1. For eac h topic z = 1 ; ; Z : 2. For eac h documen t n = 1 ; ; N : Here = f z g Z z =1 is a set of word probabilities, z = f zw g W w =1 , P w zw = 1, and zw = P ( w j z; ) is the proba-bilit y that the w th word occurs given the z th topic. As in LD A, eac h word w nm is sampled from a topic-sp eci c multi-nomial distribution, where the multinomial parameters z are generated by a Diric hlet distribution that is conjugate to multinomial. The coordinates x n and z are assumed to be generated by zero-mean spherical Gaussian distributions for stabilizi ng the visualization. Giv en x n , and , the probabilit y of w n is given as foll ows: P ( w n j x n ; ; ) = Figure 1 sho ws a graphical mo del represen tation of PLSV, where shaded and unshaded nodes indicate observ ed and laten t variables, resp ectiv ely. Since eac h word in a documen t can be sampled from di eren t topics, PLSV can capture multiple topics as in PLSA and LD A.
We estimate the parameters in PLSV based on maxim um a posteriori (MAP) estimatio n. The unk nown parameters are a set of documen t coordinates X and a set of topic co-ordinates as well as a set of word probabilities . We represen t all the unk nown parameters by = f X ; ; g . The num ber of topics Z is assumed to be kno wn and xed.
The log likeliho od of parameters given a set of docu-men ts C is as foll ows: L ( j C ) = Follo wing the generativ e pro cess describ ed in the previous subsection, we use a Diric hlet prior for word probabilit y and a Gaussian prior with a zero mean and a spherical co-variance for the coordinates of topic z and documen t x n where , and are hyper-parameters. We used = 0 : 01, = 0 : 1 N and = 0 : 1 Z in all the exp erimen ts describ ed in Section 4.

We estimate parameters by maximizing the posterior p ( j C ) using the EM algori thm [5]. The conditional ex-pectation of the complete-data log likeliho od with priors is represen ted as follows: = + where ^ represen ts the curren t estimate, and P ( z j n; m ; represen ts the class posterior probabilit y of the n th doc-umen t and the m th word given the curren t estimate. In E-step, we compute the class posterior probabilit y with the Bayes rule: where P ( z j ^ x n ; ^ ) is calculated by (1). In M-step, we obtain the next estimate of word probabilit y ^ zw by maximizing Q ( j ^ ) w.r.t. zw sub ject to P W w =1 zw = 1: ^ where I ( ) represen ts the indicator function, i.e. I ( A ) = 1 if A is true and 0 otherwise. The next estimates of documen t coordinate x n and topic coordinate z cannot be solv ed in a closed form. Therefore, we estimate them by maximiz-ing Q ( j ^ ) using a gradien t-based numerical optimization metho d suc h as the quasi-Newton metho d [15] . The gradi-ents of Q ( j ^ ) w.r.t. x n and z are resp ectiv ely: @Q @ x n @Q By iterating the E-step and the M-step until con vergence, we obtain a local optim um solution for . We can embed a new documen t with low computational cost by xing topic coordinates ^ and word probabilities ^ to the estimated values.
PLSV is based on Probabilistic Laten t Seman tic Analy-sis (PLSA) [10]. An essen tial di erence between PLSV and PLSA is that a set of topic prop ortions are deriv ed from co-ordinates of documen ts X and topics in PLSV, whereas they are directly estimated as = f n g N n =1 in PLSA, where n = f nz g Z z =1 , and nz = P ( z j d n ; ) is the z th topic pro-portion of the n th documen t. Therefore although PLSV can express any num ber of topics in D-d imensional space, PLSA can express only D +1 topics in D -dimensional space. The num ber of parameters for topic prop ortions in PLSV is ( N + Z ) D , and it is much smaller than that in PLSA N ( Z 1) since usually D Z N . Therefo re, PLSV can prev ent over tting compared with PLSA.

Under PLSA, the probabilit y of w n given d n , and is as foll ows: P ( w n j d n ; ; ) = The unkno wn parameters in PLSA are a set of topic pro-portions and a set of word probabilities . They can be estimated by maximizing the follo wing likeliho od with the EM algor ithm: L ( j C ) = In E-step, the class posterior probabilit y given the curren t estimate can be computed as foll ows:
P ( z j d n ; w nm ; ^ ) = P ( z j d n ; In M-step, the next estimate of topic prop ortion ^ nz is given by: and the next estimate of word probabilit y ^ zw is given by: ^ We can use Diric hlet priors for the topic prop ortions and word probabilities as in PLSV.
Parametric Em bedding (PE) [11] is a nonlinear visualiza-tion metho d, whic h tak es a set of discrete probabilit y distri-butions as its input. The topic prop ortions estimated using PLSA ^ with any num ber of topics can be embedded in a D -dimensional Euclidean space by PE. PE embeds sam-ples in a low-dimensional Euclidean space so as to preserv e the input probabilities by minimizing the following sum of Kullbac k-Leibler div ergences: where P ( z j x n ; ) is the probabilit y that the z th topic is cho-sen given a coordinate x n , and it is de ned in the same equa-tion as PLSV as in (1). The unk nown parameters, a set of coordinates of documen ts X and topics , can be obtained with a gradien t-based numerical optimization metho d. The gradien ts of E ( X ; ) w.r.t. x n and z are resp ectiv ely: We can use spherical Gaussian priors with zero means for the coordinates as in PLSV.

The parameter estimatio n pro cedures in PLSV describ ed in Section 2.2 sho w that the calculation of coordinates in PE is included in the M-step in PLSA. These coordinates are then mapp ed to the topic prop ortions of documen ts and used in the E-step.
There are a few visualizatio n metho ds based on genera-tive mo dels. The examples include Generativ e Topographic Mapping (GTM) [1] and the visualizatio n metho d prop osed in [13]. However, these metho ds are not topic mo dels, in whic h eac h word is assumed to be dra wn from one of the top-ics according to the topic prop ortions. A topic mo del with laten t coordinates based on GTM is prop osed in [12]. How-ever, it is mainly used for predicting sequences, and not for visualizatio n. The correlated topic mo del is a topic mo del that can mo del correlati ons among topics [3]. PLSV can also mo del the topic correlati ons because topics embedded close together in the visualization space are likely to occur together. PLSV is an unsup erv ised visualization metho d, where topics are unobserv able variables, and it is di eren t from sup ervised visualization metho ds, suc h as Fisher linear discriminan t analysis [7]. For evaluation, we compared PLSV with MDS, Isomap, PLSA and PLSA+PE by the visualization in the two-dimen-sional space D = 2.

MDS is a linear dimensionalit y reduction metho d, whic h embeds samples so as to minimize the discrepancy between pairwis e distances in the visualizatio n space and those in the original space. We used word coun t vectors as input v the coun t of the w th word in the n th documen t. We nor-malized the vector so that the L-2 norm becomes one.
Isomap is a nonlinear dimensionalit y reduction metho d, in whic h a graph is rst constructed by connecting h -nearest neigh bors, and then samples are embedded so as to pre-serv e the shortest path distances in the graph by MDS. We used the cosine simila rity between word coun t vectors v for nd ing neigh bors, whic h is widely used for the similar-ity measuremen t between documen ts. We set the num ber of neigh bors at h = 5.

In PLSA, we visualized documen ts using the pro cedures describ ed in Section 3.1. In order to represen t topic pro-portions in a two-dimensional space, we con verted the topic prop ortions estimated by PLSA with Z = 3 to a coordinate in the two-dimensional simplex.

PLSA+PE embeds documen ts using PE according to the topic prop ortions that are estimated by PLSA as describ ed in Section 3.2. We used the foll owing three data sets in the evaluations: NIPS, 20News and Eac hMo vie.

NIPS data consist of pap ers from the NIPS conference from 2001 to 2003 1 . There were 593 documen ts, and the vocabulary size was 14,03 6. Eac h documen t is lab eled with 13 researc h areas, suc h as Neuroscience and Applications. 20News data consist of documen ts in the 20 Newsgroups corpus [14]. The corpus con tains about 20,00 0 articles cat-egorized into 20 discussion groups. We omitted stop-w ords and words that occurred few er than 50 times, and also omit-ted documen ts with few er than 50 words. The vocabulary size was 6,754. We sampled 50 documen ts from eac h of 20 classes , for a total of 1000 documen ts.

Eac hMo vie data consist of movie ratings, whic h are stan-dard benc hmark data for collab ora tive ltering. We re-garded movies and users as documen ts and words, resp ec-tively, where a movie is represen ted by a sequence of rated users. Eac h movie is lab eled with 10 genres, for instance Ac-tion, Comedy and Romance. We omitted users and movies with few er than 50 ratings and movies lab eled with more than one genre. The num ber of movies was 764, and the num ber of users was 7,180 .
We evaluated the visualization results quan titativ ely from the lab el prediction accuracy with the k -nearest neigh bor ( k -NN) metho d in the visualization space. Eac h sample in all three data sets is lab eled with researc h area, discus-sion group or genre. Note that we did not use the lab el informatio n for visualizatio n in any of the metho ds, namely , we performed visualization with fully unsup ervised settings. The accuracy generally becomes high when samples with the same lab els are located close together and samples with di eren t lab els are located far away from eac h other in the visualizatio n space.

The k -NN metho d predicts a sample lab el from the most dominan t lab el among the k nearest samples, where we used the Euclidean distance for nding neigh bors. The accuracy is computed by acc ( k ) = 1 N P N n =1 I y n = ^ y k ( x where y n is the lab el of the n th documen t, and ^ y k ( x the predicted lab el with the k -NN metho d for a sample with coordinate x n .
The accuracies on NIPS, 20News and Eac hMo vie data sets are sho wn in Figure 2 when documen ts are embedded in a 1 Available at http://a i.stanford.edu/~ga l/ two-dimensional space by PLSV, MDS, Isomap, PLSA and PLSA+PE. We set the num ber of topics at Z = 50 for PLSV and PLSA+PE. The num ber of topics for PLSA is automat-icall y determined as Z = 3 since D = 2. In 20News, we created 30 evaluation sets by random sampling, where eac h set consists 1000 documen ts, and evaluated by the average accuracy over the 30 sets. In NIPS and Eac hMo vie, the accu-racies of PLSV, PLSA+PE and PLSA are averaged over 30 visualizatio ns for one data set with di eren t initial param-eters. Only the standard deviations for PLSV are sho wn. In all three data sets, the highest accuracies are achiev ed by PLSV. This result implies that PLSV can appropriately embed documen ts in the two-dimensional Euclidean space while keeping the essen tial characteristics of the documen ts. The accuracies achiev ed by PLSA+PE are lower than those achiev ed by PLSV since it embeds documen ts through two mo dularized pro cesses, where the objectiv e functions are dif-feren t from eac h other. The accuracies achiev ed by PLSA are low since it has only three topics, and it may be inade-quate to measure simil arities among topic prop ortions based on the Euclidean distance.

We also evaluated PLSV and PLSA+PE with di eren t num bers of topics Z = 5 ; 10 ; ; 50. The accuracies with the one-nearest neigh bor metho d are sho wn in Figure 3. With a small num ber of topics, the accuracies achiev ed by PLSV are not very high, and they are comparable to those achiev ed by PLSA+PE. However, as the num ber of topics in-creases , PLSV outp erforms PLSA+PE. This result indicates that the topic prop ortions and the word probabilities over t the high dimensional PLSA parameter space in PLSA+PE, and they may not be appropriately represen ted in the two-dimensional visualization space.

Figures 4, 5 and 6 sho w visualization results obtained by PLSV ( Z = 50), MDS, Isomap, PLSA ( Z = 3) and PLSA+PE ( Z = 50) on NIPS, 20News and Eac hMo vie data sets, resp ectiv ely. Here eac h point represen ts a documen t, and the shap e and color represen ts the lab el. In the PLSV visualizatio ns, documen ts with the same lab el are likely to be clustered together. On the other hand, with MDS and Isomap, documen ts with di eren t lab els are mixed, and thus the accuracy of their visualization is low. In PLSA, man y documen ts are located at the corner, and the laten t topic structure of the given data is not fully expressed in this di-mensionalit y. In PLSA+PE, documen ts are sligh tly more mixed than those in PLSV as sho wn quan titativ ely by the accuracy .

Figure 7 sho ws PLSV visualizatio n results for NIPS data with di eren t num bers of topics Z = 10 ; 20 ; 30 and 40. Al-though documen ts with di eren t lab els are mixed when the num ber of topics is small, the visualization with Z = 40 sho ws similar qualit y to that with Z = 50.

We analyzed the PLSV visualizati on in detail. Figure 8 sho ws the visualization result for 20News data obtained by PLSV with Z = 50. Here eac h blac k circle represen ts a topic coordinate z , and eac h blac k represen ts a mean coordinate of documen ts for eac h lab el, whic h is calculated documen ts lab eled with y . The num ber near the circle corre-sponds to the topic index in the table at the bottom, where the ten most probable words for eac h topic are sho wn. Docu-men ts with the same lab el are clustered together, and closely related lab els are located nearb y, suc h as rec.sp ort.basebal l and rec.sp ort.ho ckey, rec.autos and rec.motorcycles, comp.-sys.mac.hardw are and comp.sys.ibm.p c.hardw are, and soc.-religio n.christian and talk.religio n.misc. In a topic located near a lab el mean, represen tativ e words for the lab el occur with high probabilit y. For example, probable words in top-ics near rec.sp ort ( z = 1 and z = 2) are 'team', 'pla yers' and 'game' , those near com.graphics ( z = 5) are 'graphics', 'points' and 'lines', and those near sci.crypt ( z = 8) are 'clipp er', 'encryption' and 'public'.

Figure 9 sho ws the visualization result for certain movie titles from Eac hMo vie data obtained by PLSV with Z = 50. Mo vies in the same genre are likely to be located close together. For example, classi c movies are located in the bottom righ t, and foreign movies are located at the top. Class ic movies are tigh tly clustered because there may be a num ber of people who see only classic movies.
 The computational time of PLSV on a PC with 3.2GHz Xeon CPU and 2GB memory were 117, 20, and 256 min utes for NIPS, 20News, and Eac hMo vie data sets, resp ectiv ely.
In this pap er, we prop osed a visualizatio n metho d based on a topic mo del, Probabilisti c Laten t Seman tic Visualiza-tion (PLSV), for discrete data suc h as documen ts. We have con rmed exp erimen tally that PLSV can visualize docu-men ts with the laten t topic structure. The results encourage us to believ e that our data visualization approac h based on PLSV is promising and will become a useful tool for visual-izing documen ts.
 Since PLSV is a probabilistic topic mo del, we can extend PLSV easily based on other researc h on topic mo dels. Topic mo dels have been prop osed that mo del not only documen ts but also other information, for example images [2], time [21], authors [16] and citations [6]. We can also visualize this informatio n with documen ts using the framew ork prop osed in this pap er. We assumed that the num ber of topics was kno wn. We can automaticall y infer the num ber of topics by extending PLSV to a nonparametric Bayesian mo del suc h as the Diric hlet pro cess mixture mo del [18]. In addition, we can achiev e more robust estimation using the Bayesian approac h, instead of MAP estimatio n, as in LD A [9]. neigh bors k . of topics Z for PLSV and PLSA+PE.

