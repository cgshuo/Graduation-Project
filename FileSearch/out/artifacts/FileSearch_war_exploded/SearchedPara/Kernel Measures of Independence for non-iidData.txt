 Statistical dependence measures have been proposed as a unifying framework to address many ma-chine learning problems. For instance, clustering can be viewed as a problem where one strives to maximize the dependence between the observations and a discrete set of labels [14]. Conversely, if which maximize the dependence between labels and features [15]. Similarly in supervised dimen-sionality reduction [13], one looks for a low dimensional embedding which retains additional side information such as class labels. Likewise, blind source separation (BSS) tries to unmix independent sources, which requires a contrast function quantifying the dependence of the unmixed signals. [11]. This problem can be averted by using the Hilbert Schmidt Independence Criterion (HSIC). The latter enjoys concentration of measure properties and it can be computed efficiently on any domain where a Reproducing Kernel Hilbert Space (RKHS) can be defined.
 property that many problems do not share ( e.g. , BSS on audio data). For instance many random variables have a pronounced temporal or spatial structure. A simple motivating example is given in y is determined by an XOR operation via y t = x t  X  x t  X  1 . Algorithms which treat the observation In view of its importance, temporal correlation has been exploited in the independence test for blind mixtures, and [18] exploited multiple time-lagged second-order correlations to decorrelate over time. These methods work well in practice. But they are rather ad hoc and appear very different from standard criteria. In this paper, we propose a framework which extends HSIC to structured non-iid data. Our new approach is built upon the connection between exponential family models and Figure 1: From left to right: (a) Graphical model representing the XOR sequence, (b) a graphical model representing iid observations, (c) a graphical model for first order sequential data, and (d) a graphical model for dependency on a two dimensional mesh. the marginal polytope in an RKHS. This is doubly attractive since distributions can be uniquely identified by the expectation operator in the RKHS and moreover, for distributions with conditional independence properties the expectation operator decomposes according to the clique structure of the underlying undirected graphical model [2]. Denote by X and Y domains from which we will be drawing observations Z := domains X and Y are fully general and we will discuss a number of different structural assumptions on them in Section 3 which allow us to recover existing and propose new measures of dependence. For instance x and y may represent sequences or a mesh for which we wish to establish dependence. To assess whether x and y are independent we briefly review the notion of Hilbert Space embeddings of distributions [6]. Subsequently we discuss properties of the expectation operator in the case of conditionally independent random variables which will lead to a template for a dependence measure. Hilbert Space Embedding of Distribution Let H be a RKHS on Z with kernel v : Z X Z 7 X  R . Moreover, let P be the space of all distributions over Z , and let p  X  X  . The expectation operator in H and its corresponding empirical average can be defined as in [6] The map  X  : P 7 X  X  characterizes a distribution by an element in the RKHS. The following theorem shows that the map is injective [16] for a large class of kernels such as Gaussian and Laplacian RBF. Theorem 1 If E z  X  p [ v ( z,z )] &lt;  X  and H is dense in the space of bounded continuous functions C 0 ( Z ) in the L  X  norm then the map  X  is injective. 2.1 Exponential Families We are interested in the properties of  X  [ p ] in the case where p satisfies the conditional indepen-sufficient statistics decompose along the maximal cliques of the conditional independence graph. More formally, denote by C the set of maximal cliques of the graph G and let z c be the restriction of z  X  X  to the variables on clique c  X  X  . Moreover, let v c be universal kernels in the sense of [17] acting on the restrictions of Z on clique c  X  X  . In this case, [2] showed that can be used to describe all probability distributions with the above mentioned conditional indepen-expectations of the sufficient statistics yield injections, we have the following result: Corollary 2 On the class of probability distributions satisfying conditional independence properties according to a graph G with maximal clique set C and with full support on their domain, the operator is injective if the kernels v c are all universal. The same decomposition holds for the empirical counterpart  X  [ Z ] .
 The condition of full support arises from the conditions of the Hammersley-Clifford Theorem [4, 8]: without it, not all conditionally independent random variables can be represented as the product of potential functions. Corollary 2 implies that we will be able to perform all subsequent operations on structured domains simply by dealing with mean operators on the corresponding maximal cliques. 2.2 Hilbert Schmidt Independence Criterion Theorem 1 implies that we can quantify the difference between two distributions p and q by simply we can quantify the strength of dependence between random variables x and y by simply measuring the square distance between the RKHS embeddings of the joint distribution p ( x,y ) and the product of the marginals p ( x )  X  p ( y ) via Moreover, Corollary 2 implies that for an exponential family consistent with the conditional inde-pendence graph G we may decompose I ( x,y ) further into where bracketed random variables in the subscripts are drawn from their joint distributions and un-bracketed ones are from their respective marginals, e.g. , E ( x the challenge is to find good empirical estimates of (6). In its simplest form we may replace each of the expectations by sums over samples, that is, by replacing To illustrate the versatility of our approach we apply our model to a number of graphical models ranging from independent random variables to meshes proceeding according to the following recipe: 3.1 Independent and Identically Distributed Data kernel on the cliques to be v The representation for v t implies that we are taking an outer product between the Hilbert Spaces on x identically distributed, all that is left is to use (8) to obtain an empirical estimate via (7). k  X  c [ p c ( x c ,y c )]  X   X  c [ p c ( x c ) p c ( y c )] k rion and we obtain the biased estimate  X  3.2 Sequence Data A more interesting application beyond iid data is sequences with a Markovian dependence as de-erally, for longer range dependency of order  X   X  N , the maximal cliques will involve the random variables ( x t ,...,x t +  X  ,y t ,...,y t +  X  ) =: ( x t, X  ,y t, X  ) .
 We assume homogeneity and stationarity of the random variables: that is, all cliques share the same sufficient statistics (feature map) and their expected value is identical. In this case the kernel can be used to measure discrepancy between the random variables. Stationarity means that  X  the difference for a single clique.
 L in (9). This works well in experiments. In order to obtain an unbiased estimate we need some more work. Recall the unbiased estimate of I ( x,y ) is a fourth order U-statistic [6]. Theorem 3 An unbiased empirical estimator for k  X  [ p ( x,y )]  X   X  [ p ( x ) p ( y )] k 2 is where the sum is over all terms such that i,j,q,r are mutually different, and h ( x 1 ,y 1 ,...,x 4 ,y 4 ) := and the latter sum denotes all ordered quadruples ( t,u,v,w ) drawn from (1 , 2 , 3 , 4) . The theorem implies that in expectation h takes on the value of the dependence measure. To estab-lish that this also holds for dependent random variables we use a result from [1] which establishes convergence for stationary mixing sequences under mild regularity conditions, namely whenever the kernel of the U-statistic h is bounded and the process generating the observations is absolutely regular. See also [5, Section 4].
 Theorem 4 Whenever I ( x,y ) &gt; 0 , that is, whenever the random variables are dependent, the estimate  X  I ( x,y ) is asymptotically normal with where the variance is given by This follows from [5, Theorem 7], again under mild regularity conditions (note that [5] state their tedious but does not require additional techniques and is therefore omitted. 3.3 TD-SEP as a special case So far we did not discuss the freedom of choosing different kernels. In general, an RBF kernel will lead to an effective criterion for measuring the dependence between random variables, especially in obtain computational savings.
 framework. In [18], for two centered scalar time series x and y , the contrast function is chosen as Furthermore, we use a joint kernel of the form which leads to the estimator of structured HSIC: I ( x,y ) = 1 T (tr HKHL + tr HKHL  X  ) . Here L  X  (up to a multiplicative constant).
 Further generalization can incorporate several time lagged cross-covariances into the contrast func-kernel we are able to obtain better contrast functions, as we will show in our experiments. 3.4 Grid Structured Data Structured HSIC can go beyond sequence data and be applied to more general dependence structures such as 2-D grids for images. Figure 1d shows the corresponding graphical model. Here each node of the graphical model is indexed by two subscripts, i for row and j for column. In the simplest case, the maximal cliques are also be decomposed into the product of k and l , then a biased estimate of the independence measure can be again formulated as tr HKHL up to a multiplicative constant. The statistical analysis of U-statistics for stationary Markov random fields is highly nontrivial. We are not aware of results equivalent to those discussed in Section 3.2. Having a dependence measure for structured spaces is useful for a range of applications. Analogous to iid HSIC, structured HSIC can be applied to non-iid data in applications such as independent component analysis [12], independence test [6], feature selection [15], clustering [14], and dimen-sionality reduction [13]. The fact that structured HSIC can take into account the interdependency between observations provides us with a principled generalization of these algorithms to, e.g. , time series analysis. In this paper, we will focus on two examples: independent component analysis, where we wish to minimize the dependence, and time series segmentation, where we wish to max-imize the dependence instead. Two simple illustrative experiments on independence test for XOR binary sequence and Gaussian process can be found in the longer version of this paper. 4.1 Independent Component Analysis In independent component analysis (ICA), we observe a time series of vectors u that corresponds to a linear mixture u = As of n mutually independent sources s (each entry in the source vector here is a random process, and depends on its past values; examples include music and EEG time series). Based on the series of observations t , we wish to recover the sources using only the independence assumption on s . Note that sources can only be recovered up to scaling and permutation. The core of ICA is a contrast function that measures the independence of the estimated sources. An ICA algorithm searches over the space of mixing matrix A such that this contrast function is minimized. Thus, we propose to use structured HSIC as the contrast function for ICA. By incorporating time of time series. In this respect, we generalize the TD-SEP algorithm [18], which implements this idea using a linear kernel on the signal. Thus, we address the question of whether correlations between higher order moments, as encoded using non-linear kernels, can improve the performance of TD-SEP on real data.
 Table 1: Median performance of ICA on music using HSIC, TDSEP, and structured HSIC. In the top row, the number n of sources and m of samples are given. In the second row, the number of time lags  X  used by TDSEP and structured HSIC are given: thus the observation vectors x,x t  X  1 ,...,x t  X   X  were compared. The remaining rows contain the median Amari divergence (multiplied by 100) for the three methods tested. The original HSIC method does not take into account time dependence are chosen at random without replacement.
 Data Following the settings of [7, Section 5.5], we unmixed various musical sources, combined using a randomly generated orthogonal matrix A (since optimization over the orthogonal part of a general mixing matrix is the more difficult step in ICA). We considered mixtures of two to four sources, drawn at random without replacement from 17 possibilities. We used the sum of pairwise dependencies as the overall contrast function when more than two sources were present. Methods We compared structured HSIC to TD-SEP and iid HSIC. While iid HSIC does not take the temporal dependence in the signal into account, it has been shown to perform very well for  X  = 3 for both structured and iid HSIC. For both structured and iid HSIC, we used gradient descent over the orthogonal group with a Golden search, and low rank Cholesky decompositions of the Gram matrices to reduce computational cost, as in [3].
 Results We chose the Amari divergence as the index for comparing performance of the various ICA methods. This is a divergence measure between the estimated and true unmixing matrices, indicates better performance. Results are shown in Table 1. Overall, contrast functions that take time delayed information into account perform best, although the best time lag is different when the number of sources varies. 4.2 Time Series Clustering and Segmentation We can also extend clustering to time series and sequences using structured HSIC. This is carried out in a similar way to the iid case. One can formulate clustering as generating the labels y from a finite discrete set, such that their dependence on x is maximized [14]: Here K and L are the kernel matrices for x and the generated y respectively. More specifically, y  X  R with bounded norm k y k 2 and setting L st := y s y t , we obtain Principal Component Analysis. This reasoning for iid data carries over to sequences by introducing additional dependence structure sequences make the optimization in (15) intractable. However, for a class of kernels l an efficient decomposition can be found by applying a reverse convolution on k : assume that l is given by where M  X  R (  X  +1)  X  (  X  +1) with M 0 , and  X  l is a base kernel between individual time points. A summation over M to HKH , i.e. , This means that we may apply the matrix M to HKH and thereby we are able to decouple the dependency within y . Denote the convolution by  X  K = [ HKH ] ?M . Consequently using  X  K we can directly apply (15) to times series and sequence data. In practice, approximate algorithms such as incomplete Cholesky decomposition are needed to efficiently compute  X  K .
 Datasets We study two datasets in this experiment. The first dataset is collected by the Australian Institute of Sports (AIS) from a 3-channel orientation sensor attached to a swimmer. The three time series we used in our experiment have the following configurations: T = 23000 time steps with 4 laps; T = 47000 time steps with 16 laps; and T = 67000 time steps with 20 laps. The task is to this problem as a segmentation problem. Since the dataset contains 4 different style of swimming, The second dataset is a brain-computer interface data (data IVb of Berlin BCI group 1 ). It contains EEG signals collected when a subject was performing three types of cued imagination. Furthermore, the relaxation period between two imagination is also recorded in the EEG. Including the relaxation period, the dataset consists of T = 10000 time points with 16 different segments. The task is to automatically detect the start and end of an imagination. We used 4 clusters for this problem. Methods We compared three algorithms: structured HSIC for clustering, spectral clustering [10], discrete label sequence to be generated. The kernel l on y took the form of equation (16), with M  X  R As a baseline, we used a spectral clustering with the same kernel k on x , and a first order HMM with 6 hidden states and diagonal Gaussian observation model 2 .
 Further details regarding preprocessing of the above two datasets (which is common to all algorithms subsequently compared), parameters of algorithms and protocols of experiments, are available in the longer version of this paper.
 Results To evaluate the segmentation quality, the boundaries found by various methods were com-pared to the ground truth. First, each detected boundary was matched to a true boundary, and then the discrepancy between them was counted into the error. The overall error was this sum divided by the number of boundaries. Figure 2d gives an example on how to compute this error.
 According to Table 2, in all of the four time series we studied, segmentation using structured HSIC leads to lower error compared with spectral clustering and HMM. For instance, structured HSIC reduces nearly 1 / 3 of the segmentation error in the BCI dataset. To provide a visual feel of the improvement, we plot the true boundaries together with the segmentation results in Figure 2a, 2b,2c. Clearly, segment boundaries produced by structured HSIC fit better with the ground truth. In this paper, we extended the Hilbert Schmidt Independence Criterion from iid data to structured and non-iid data. Our approach is based on RKHS embeddings of distributions, and utilizes the effi-cient factorizations provided by the exponential family associated with undirected graphical models. Encouraging experimental results were demonstrated on independence test, ICA, and segmentation for time series. Further work will be done in the direction of applying structured HSIC to PCA and feature selection on structured data.
 HMM. (d) An example for counting the segmentation error. Red line denotes the ground truth and for segment R 2 to R 0 2 is c + d . The overall error in this example is then ( a + b + c + d ) / 4 .
