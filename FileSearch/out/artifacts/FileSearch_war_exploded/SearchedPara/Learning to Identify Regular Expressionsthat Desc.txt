 Paul Prasse prasse@cs.uni-potsdam.de Christoph Sawade sawade@cs.uni-potsdam.de Niels Landwehr landwehr@cs.uni-potsdam.de Tobias Scheffer scheffer@cs.uni-potsdam.de Popular spam dissemination tools allow users to imple-ment mailing campaigns by specifying simple gram-mars that serve as message templates. A grammar is disseminated to nodes of a bot net, the nodes cre-ate messages by instantiating the grammar at random. Email service providers can easily sample elements of new mailing campaigns by collecting messages in spam traps or by tapping into known bot nets. When mes-sages from multiple campaigns are collected in a joint spam trap, clustering tools can separate the campaigns reliably (Haider &amp; Scheffer, 2009). However, prob-abilistic cluster descriptions that use a bag-of-words representation incur the risk of false positives, and it is difficult for a human to decide whether they in fact characterize the correct set of messages.
 Regular expressions are a standard tool for specify-ing simple grammars. Widely available tools match strings against regular expressions efficiently and can be used conveniently from scripting languages. A reg-ular expression can be translated into a finite state machine that accepts the language and has an execu-tion time linear in the length of the input string. A specific, comprehensible regular expression which cov-ers the observed instances and has been written by an expert postmaster can be used to blacklist the bulk of emails of that campaign at virtually no risk of covering any other messages.
 Language identification has a rich history in the al-gorithmic learning theory community (see Section 6). Our problem setting differs from the problem of lan-guage identification in the learner X  X  exact goal, and in the available training data. Batches of strings and cor-responding regular expressions are observable in the training data. The learner X  X  goal is to produce a pre-dictive model that maps batches of strings to regu-lar expressions that resemble as closely as possible the regular expressions which the postmaster would have written and feels confident to blacklist (see Figure 1). The rest of this paper is structured as follows. Sec-tion 2 reviews regular expressions before Section 3 states the problem setting. Section 4 introduces the feature representation and derives the decoder and the optimization problem. In Section 5, we discuss our findings from a case study with an email service. Sec-tion 6 discusses related work; Section 7 concludes. Syntactically, a regular expression y  X  Y  X  is either a character from an alphabet  X , or it is an expres-sion in which an operator is applied to one or several argument expressions. Basic operators are the con-catenation ( e.g.,  X  abc  X ), disjunction ( e.g.,  X  a | b  X ), and the Kleene star ( X   X   X ), written in postfix notation, that accepts any number of repetitions of its preceding ar-gument expression. Parentheses define the syntactic structure of the expression. Several shorthands im-prove the readability of regular expressions and can be defined in terms of the basic operators. For in-stance, the any character symbol ( X . X ) abbreviates the disjunction of all characters in  X , square brackets ac-cept the disjunction of all characters ( e.g.,  X  [abc]  X ) or ranges ( e.g.,  X  [a-z0-9]  X ) that are included. The postfix operator  X  +  X  accepts an arbitrary, positive number of reiterations of the preceding expression, while  X  { l,u }  X  accepts between l and u reiterations, where l  X  u . We include a set of popular macros X  X or instance  X  \ d  X  for any digit . A formal definition of the set of regular expressions can be found in the online appendix. The syntactic structure of a regular expres-sion is represented by its syntax tree T y syn = ( V appendix assigns one such tree to each regular ex-pression. A node v  X  V y syn of this tree is tagged by labeling function  X  y syn : V y syn  X  Y  X  with a subex-pression  X  y syn ( v ) = y j . Edges ( v,v 0 )  X  E y syn that node v 0 represents an argument expression of v . Relation  X  y syn  X  V y syn  X  V y syn defines an ordering on the nodes and identifies the root node.
 A regular expression y defines a regular language L ( y ). Given the regular expression, a deterministic finite state machine can decide whether a string x is in L ( y ) in time linear in | x | (Dub  X e &amp; Feeley, 2000). The trace of verification is typically represented as a parse the string x can be derived from the regular expres-sion y . At least one parse tree exists if and only if the string is an element of the language L ( y ); in this case, y is said to generate x . Nodes v  X  V y syn of the syntax tree generate the nodes of the parse tree v 0  X  V y ,x par node of the syntax tree may spawn none (alternatives which are not used to generate a string), one, or several ( X  X oopy X  syntactic elements such as  X   X   X  or  X  +  X ) nodes in the parse tree. In analogy to the syntax trees, the la-beling function  X  y ,x par : V y ,x par  X  X   X  assigns a subexpres-sion to each node, and the relation  X  y ,x par  X  V y ,x par defines the ordering of sibling nodes. The set of all parse trees for a regular expression y and a string x is denoted by T y ,x par . A formal definition can be found in the online appendix.
 Leaf nodes of a parse tree T y ,x par are labeled with ele-ments of  X   X  X  } , where denotes the empty symbol; reading them from left to right gives the generated string x . Non-terminal nodes correspond to subex-pressions y j of y which generate substrings of x . To compare different regular expressions with respect to a given string x , we define the set T y ,x par nodes which are visited on the path from the root to the the i -th character of x in the parse tree T y ,x par Figure 2 shows an example of a syntax tree T y syn and a parse tree T y ,x par for the regular expression y = [ b0 -9 ] { 2 } c ( aa | b )  X  and the string x = 1bc . Finally, we introduce the concept of a matching list. When a regular expression y generates a set x of strings, and v  X  V y syn is an arbitrary node of the syn-tax tree of y , then the matching list M y , x ( v ) charac-terizes which substrings of the strings in x are gen-erated by the node v of the syntax tree. A node v of the syntax tree generates a substring x 0 of x  X  x , if v generates a node v 0 in the parse tree T y ,x par and there is a path from v 0 in that parse tree to every character in the substring x 0 . In the above example, for the set of strings x = { 12c , b4ca } , the matching list for node v 1 that represents subexpression [ b0 -9 ] { 2 } is M pendix introduces matching lists formally. Having established the syntax and semantics of regular expressions, we now turn towards the problem setting. An unknown distribution p ( x , y ) generates regular ex-pressions y  X  Y  X  and batches x of strings x  X  x that are elements of the language L ( y ). In our motivating application, the strings x are emails sampled from a bot net, and the y are regular expressions which an expert postmaster believes to identify the campaign template, and feels confident to blacklist.
 A w -parameterized predictive model f w : x 7 X   X  y ac-cepts a batch of strings and conjectures a regular ex-captures the deviation of the conjecture  X  y from y for batch x . In our application, postmasters will not use an expression to blacklist the campaign unless they consider it to be comprehensibly and neatly written, and believe it to accurately identify the campaign. Loss function  X ( y ,  X  y , x ) compares each of the accept-ing parse trees in T y ,x par , for each string x  X  x , with the most similar tree in T  X  y ,x par ; if no such parse tree exists, the summand is defined as 1 | x | (Equation 1). Similarly to a loss function for hierarchical classification (Cesa-Bianchi et al., 2006), the difference of two parse trees for string x is quantified by a comparison of the paths that lead to the characters of the string; paths are compared by means of the intersection of their nodes (Equation 2). By its definition, this loss function is bounded between zero and one; it attains zero if and only if the expressions y and  X  y are equal.  X ( y ,  X  y , x ) = with  X  tree ( y ,  X  y ,x ) (2) = 1  X  We will also explore the zero-one loss,  X  0 / 1 ( y ,  X  y , x ) = J y 6 =  X  y boolean argument. The zero-one loss serves as an al-ternative, conceptually simpler reference model. Our goal is to find the model f w with minimal risk Training data D = { ( x i , y i ) } m i =1 consists of pairs of batches x i and generating regular expressions y i drawn according to p ( x , y ).
 Since the true distribution p ( x , y ) is unknown, the risk R [ f w ] cannot be calculated. We state the learn-ing problem as the problem of minimizing the regular-ized empirical counterpart of the risk over the param-eters w and the regularizer  X ( w ): We model f w as a linear discriminant function w T  X ( x , y ) for a joint feature representation of the in-put x and output y (Tsochantaridis et al., 2005): 4.1. Joint Feature Representation The joint feature representation  X ( y , x ) captures structural properties of an expression y and joint prop-erties of input batch x and regular expression y . Structural properties of a regular expression y are cap-tured by features that indicate a specific nesting of regular expression operators X  X or instance, whether a concatenation occurs within a disjunction. More for-mally, we first define a binary vector encoding the top-level operator used in the regular ex-pression y . In Equation 6, y 1 ,..., y k  X  X   X  are regular expressions, l,u  X  N , and { r 1 ,...,r l } is a set of ranges and popular macros; for our application, we use the two nodes v 0 and v 00 in the syntax tree of y that are connected by an edge X  X ndicating that y 00 =  X  y syn ( v 00 is an argument subexpression of y 0 =  X  y syn ( v 0 ) X  X he tensor product  X ( y 0 )  X   X ( y 00 ) defines a binary vector that encodes the specific nesting of operators at node v . Feature vector  X ( x , y ) will aggregate these vectors over all pairs of adjacent nodes in the syntax tree of y . Joint properties of an input batch x and a regular ex-pression y are encoded as follows. Recall that for any node v 0 in the syntax tree, M y , x ( v 0 ) denotes the set of substrings in x that are generated by the subexpres-sion y 0 =  X  y syn ( v 0 ) that v 0 is labeled with. We define property may be accounted for; for our application, we include the average string length, the inclusion of the empty string, the proportion of capital letters, and many other attributes. The full list of attributes used in our experiments is included in the online appendix. A joint encoding of properties of the subexpression y 0 and the set of substrings generated by y 0 is given by the tensor product  X ( M y , x ( v 0 ))  X   X ( y 0 ). The joint feature vector  X ( x , y ) is obtained by ag-gregating operator-nesting information over all edges in the syntax tree, and joint properties of subexpres-sions y 0 and the set of substrings they generate over all nodes in the syntax tree:
 X ( x , y ) (7) 4.2. Decoding At application time, the highest-scoring regular ex-pression f w ( x ) = arg max y  X  X  identified. This maximization is over the infinite space of all regular expressions Y  X  . To alleviate the in-tractability of this problem, we approximate this maxi-mum by the maximum over a constrained, finite search space which can be found efficiently.
 The constrained search space initially contains an alignment of all strings in x . An alignment is a regular expression that contains only constants X  X hich have to occur in all strings of the batch X  X nd the wildcard symbol  X ( .  X  ) X . The initial alignment a x of x can be thought of as the most-general bound of this space. Definition 1 (Alignment) . The set of alignments A x of a batch of strings x contains all concatenations in which strings from  X  + and the wildcard symbol  X  ( .  X  )  X  alternate, and that generate all elements of x . An alignment is maximal if no other alignment in A x contains more constant symbols. A maximal align-ment of two strings can be determined efficiently us-ing Hirschberg X  X  algorithm (Hirschberg, 1975) which is an instance of dynamic programming. By contrast, finding the maximal alignment of a set of strings is NP-hard (Wang &amp; Jiang, 1994); known algorithms are exponential in the number | x | of strings in x . Progres-sive alignment heuristics find an alignment of a set of strings by incrementally aligning pairs of strings. Given an alignment a x = a 0 ( .  X  ) a 1 ... ( .  X  ) a n strings in x , the constrained search space contains all specializations of a x in which the j -th wildcard symbol is replaced by any element of a set  X  Y
D . The sets The algorithm starts with Y D which we define to be the set of all subexpressions that occur anywhere in the training data D . From this set, it takes a subset such that each regular expression in  X  Y x ,D generates all strings in x , and adds a number of syntactic variants and subexpressions in which constants have been re-placed to match the elements of M j , where M j is the matching list of the node which belongs to the j -th wildcard symbol. Each of the lines 7, 9, 10, 11, and 12 of Algorithm 1 adds at most one element to  X  Y M j D  X  hence, the search space of possible substitutions for each of the n wildcard symbols is linear in the number of subexpressions that occur in the training sample. We now turn towards the problem of determining the highest-scoring regular expression f w ( x ). Maximiza-tion over all regular expressions is approximated by maximization over the space defined by Equation 8: We will now argue that this maximization problem can be decomposed into independent maximization prob-lems for each of the y j that replaces the j -th wildcard in the alignment a x due to the simple syntactic struc-ture of the alignment and the definition of  X .
 Feature vector  X ( x , y ) decomposes linearly into a sum over the nodes and a sum over pairs of adjacent nodes (see Equation 7). The syntax tree of an instantia-tion y = a 0 y 1 a 1 ... y n a n of the alignment a x consists of a root node labeled as an alternating concatenation of constant strings a j and subexpressions y j (see Fig-ure 3). This root node is connected to a layer on which constant strings a j = a j, 1 ...a j, | a alternate (blue area in Figure 3). However, the terms in Equation 10 that correspond to the root node y and Algorithm 1 Constructing the decoding space Input: Subexpressions Y D and alignment a x = 1: let T a x syn be the syntax tree of the alignment and 2: for j = 1 ...n do 4: Initialize  X  Y M j D to { y  X  X  D | M j  X  L ( y ) } 5: let x 1 ,...,x m be the elements of M j ; add 6: let u be the length of the longest string and l 7: if [  X  y 1 ... y k ]  X   X  Y M j D , where  X   X   X   X  and 8: for all [ y ]  X   X  Y M j D do 9: add [ y ]  X  and [ y ] { l,u } to  X  Y M j D . 10: if l = u , then add [ y ] { l } to  X  Y M j D . 11: if u  X  1, then add [ y ]? to  X  Y M j D . 12: if l &gt; 0, then add [ y ] + to  X  Y M j D . 13: end for 14: end for Return:  X  Y M 1 D ,...,  X  Y M n D . the a j are constant for all values of the y j (red area in Figure 3). Since no edges connect multiple wild-cards, the feature representation of these subtrees can be decomposed into n independent summands as in Equation 11.
  X ( x ,a 0 y 1 a 1 ... y n a n ) (10) = + = + Since the top-level operator of an alignment is a con-catenation for any y  X   X  Y x ,D , we can write  X ( y ) as a a constant  X   X  , defined as the output feature vector (Equation 6) of a concatenation.
 Thus, the maximization over all y = a 0 y 1 a 1 ... y n a can be decomposed into n maximization problems over y  X  j = arg max which can be solved in O ( n  X |Y D | ). 4.3. Optimization Problem We will now address the process of minimizing the reg-ularized empirical risk  X  R , defined in Equation 4, for the ` 2 regularizer  X ( w ) = 1 2 C || w || 2 . Loss function  X , defined in Equation 1, is not convex. To obtain a con-vex optimization problem, we upper-bound the loss by its hinged version, following the margin-rescaling ap-proach (Tsochantaridis et al., 2005):  X  = max The maximum in Equation 12 is over all y  X  X   X  \{ y i } . When the risk is rephrased as a constrained optimiza-tion problem, the maximum produces one constraint per element of y  X  Y  X  \{ y i } . However, since the de-coder searches only the set  X  Y x enforce the constraints on this subset.
 When the loss is replaced by its upper bound X  X he slack variable  X   X  X nd for  X ( w ) = 1 2 C || w || 2 , the min-imization of the regularized empirical risk (Equation 4) is reduced to Optimization Problem 1.
 Optimization Problem 1. Over parameters w , find w  X  = arg min  X  i,  X   X  y  X   X  Y x  X  i :  X  i  X  0 . (15) This optimization problem is convex, since the ob-jective (Equation 13) is convex and the constraints (Equation 14 and 15) are affine in w . Hence, the so-lution is unique and can be found efficiently by cut-ting plane methods as Pegasos (Shalev-Shwartz et al., 2011) or SVM struct (Tsochantaridis et al., 2005). Algorithm 2 Most strongly violated constraint Input: batch x , model f w , correct output y . 1: Infer alignment a x = a 0 ( .  X  ) a 1 ... ( .  X  ) a n for x . 2: Let T a x syn be the syntax tree of a x and let v 1 ,...,v 3: for all j = 1 ...n do 4: Let M j = M a x , x ( v j ) and calculate the  X  Y M j D 5:  X  y 6: end for 7: Let  X  y abbreviate a 0  X  y 1 a 1 ...  X  y n a n 8: if  X  y = y then 9: Assign a value of  X  y 0 j  X   X  Y M j D to one of the 10: end if Return:  X  y During the optimization procedure, the regular expres-sion that incurs the highest slack  X  i for a given x i , has to be identified repeatedly. Algorithm 1 con-structs the constrained search space  X  Y x x  X  L ( y ) for each x  X  x i and y  X   X  Y x  X  X therwise X -case in Equation 1 never applies within our search space. Without this case, Equations 1 and 2 decompose linearly over the nodes of the parse tree, and therefore the wildcards. Hence,  X  y can be identified by maximizing over the variables  X  y j independently in Step 5 of Algorithm 2. Algorithm 2 finds the constraint that is violated most strongly within the constrained search space in O ( n  X |Y D | ). This ensures a polyno-mial execution time of the optimization algorithm. We refer to this learning procedure as REx-SVM . We investigate whether postmasters accept the output of REx-SVM to blacklist mailing campaigns during regular operations of a commercial email service. We also evaluate how accurately REx-SVM and reference methods identify the extensions of mailing campaigns. 5.1. Evaluation by Postmasters REx-SVM is trained on the ESP data set that contains 158 batches with a total of 12,763 emails and corre-sponding regular expressions, collected from the email service provider. The model is deployed; the user in-terface presents newly detected batches of spam emails together with the regular expression conjectured by REx-SVM to a postmaster during regular operations of the service. The postmaster is charged with black-listing the campaigns by suitable regular expressions. Over the study, the postmasters created 188 regular expressions. Of these, they created 169 expressions (89%) by copying a substring of the automatically gen-erated expression. We observe that postmasters prefer to describe only a part of the message which they feel is characteristic for the campaign whereas REx-SVM describes the entirety of the messages. In 12 cases, the postmasters edited the string, and in 7 cases they wrote an expression from scratch.
 To illustrate different cases, Figure 4 compares ex-cerpts of expressions created by REx-and REx 0 / 1 -SVM (a variant of REx-SVM that uses the zero-one loss instead of  X  defined in Equation 1) to expressions of a postmaster. The first example shows a perfect agreement between REx-SVM and postmaster. In the second example, the expressions are close but distinct. In the third example, the SVMs produce expressions that generate an overly general set of URLs and lead to false positives ( X  \ e  X  stands for characters that can occur in a URL). In all three cases, REx-SVM is more similar to the postmaster than REx 0 / 1 .
 The top right diagram of Figure 5 shows the average loss  X  of REx-and REx 0 / 1 -SVM , measured by cross validation with one batch held out. While postmas-ters show the tendency to write expressions that only characterize about 10% of the message, the REx-SVM variants describe the entirety of the message. This leads to relatively high values of the loss function. 5.2. Spam Filtering Performance We evaluate the ability of REx-SVM and baselines to identify the exact extension of email campaigns. We use the alignment of the strings in x as a baseline. In addition, ReLIE (Li et al., 2008) searches for a regular expression that matches the emails in the input batch and does not match any of the additional negative ex-amples by applying a set of transformation rules; we use the alignment of the input batch as starting point. ReLIE receives an additional 10,000 emails that are not part of any batch as negative data. An additional content -based filter employed by the provider has been trained on several million spam and non-spam emails. In order to be able to measure false-positive rates (the rate at which emails that are not part of a campaign are erroneously included), we combine the ESP data set with an additional 135,000 non-spam emails, also from the provider. Additionally, we use a public data set that consists of 100 batches of emails extracted from the Bruce Guenther archive 1 , containing a total of 63,512 emails. To measure false-positive rates, we combine this collection of spam batches with 17,419 emails from the Enron corpus 2 of non-spam emails and 76,466 non-spam emails of the TREC corpus 3 . The public data set is available to researchers.
 In an outer loop of leave-one-out cross validation, one batch is held back to evaluate the true-positive rate (the proportion of the campaign that is correctly rec-ognized). In an inner loop of 10-fold cross validation, regularization parameter C is tuned.
 Figure 5 shows the true and false positive rates for all methods and both data sets. The horizontal axis displays the number of emails in the input batch x . Error bars indicate the standard error. The alignment exhibits the highest true-positive rate and a high false-positive rate because it is the most-general bound of the decoder X  X  search space. ReLIE needs only very few or zero replacement steps until no negative exam-ples are covered. Consequently, it has similarly high true-and false-positive rates. REx-SVM attains a slightly lower true positive rate, and a substantially lower false-positive rate. The false-positive rates of REx and REx 0 / 1 lie more than an order of magni-tude below the rate of the commercial content -based spam filter employed by the email service provider. The zero-one loss leads to comparable false-positive but lower true-positive rates, rendering the loss func-tion of Equation 1 preferable to the zero-one loss. The execution time to learn a model (bottom right) is consistent with prior findings of between linear and quadratic for the SVM optimization process. Gold (1967) shows that it is impossible to exactly identify any regular language from finitely many pos-itive examples. Our notion of minimizing an ex-pected difference between conjecture and target lan-guage over a distribution of input strings reflects a more statistically-inspired notion of learning. Also, in our problem setting the learner has access to pairs of sets of strings and corresponding regular expressions. Most work of identification of regular languages fo-cuses on learning automata (Denis, 2001; Clark &amp; Thollard, 2004). While these problems are identical in theory, transforming generated automata into reg-ular expressions can lead to lengthy terms that do not lend themselves to human comprehension (Fernau, 2009). Some work focuses on restricted classes, such as expressions in which each symbol occurs at most k times (Bex et al., 2008), disjunction-free expres-sions (Br  X azma, 1993), and disjunctions of left-aligned disjunction-free expressions (Fernau, 2009).
 Xie et al. (2008) use regular expressions to detect URLs in spam batches and develop a spam filter with low false positive rate. The ReLIE -algorithm (Li et al., 2008) (used as a reference method in our experiments) learns regular expressions from positive and negative examples given an initial expression by applying a set of transformation rules as long as this improves the separation of positive and negative examples. Complementing the language-identification paradigm, we pose the problem of learning to map a set of strings to a target regular expression. Training data consists of batches of strings and corresponding expressions. We phrase this problem as a learning problem with structured output spaces and engineer an appropri-ately loss function. We derive the resulting optimiza-tion problem, and devise a decoder that searches a space of specializations of a maximal alignment. From our case study we conclude that REx-SVM gives a high true positive rate at a false positive rate that is more than an order of magnitude lower than that of a commercial content -based filter. The system is being used by a commercial email service provider and com-plements content -based and IP-address based filtering. This work was funded by a grant from STRATO AG.
