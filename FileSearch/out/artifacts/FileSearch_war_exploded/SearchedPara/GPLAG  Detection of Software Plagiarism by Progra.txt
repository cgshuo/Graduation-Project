 Along with the blossom of open source projects comes the convenience for software plagiarism. A company, if less self-disciplined, may be tempted to plagiarize some open source projects for its own products. Although current pla-giarism detection tools appear sufficient for academic use, they are nevertheless short for fighting against serious pla-giarists. For example, disguises like statement reordering and code insertion can effectively confuse these tools. In this paper, we develop a new plagiarism detection tool, called GPlag , which detects plagiarism by mining program depen-dence graphs (PDGs). A PDG is a graphic representation of the data and control dependencies within a procedure. Be-cause PDGs are nearly invariant during plagiarism, GPlag is more effective than state-of-the-art tools for plagiarism de-tection. In order to make GPlag scalable to large programs, a statistical lossy filter is proposed to prune the plagiarism search space. Experiment study shows that GPlag is both effective and efficient: It detects plagiarism that easily slips over existing tools, and it usually takes a few seconds to find (simulated) plagiarism in programs having thousands of lines of code.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms, Experimentation  X 
This work was supported in part by the U.S. National Sci-ence Foundation NSF ITR-03-25603 and IIS-03-08215/05-13678. Any opinions, findings, and conclusions or recom-mendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Program dependence graph, Graph mining, Software plagia-rism detection
Along with the blossom of open source projects comes the convenience for software plagiarism. Suppose a com-pany needs to implement a large software product, which, if done from the scratch, could be time-consuming. The developers, if less self-disciplined, may be tempted to find a counterpart in some open source projects, rip off the in-terface components, like I/O and Graphical User Interfaces (GUIs), and finally fit the essential components ( i.e. ,core parts) into their own project with serious disguises .Be-cause only core parts are plagiarized, which accounts for a small portion of the whole project, and heavy disguises are applied, the plagiarism, which we call core-part plagiarism , is hard to notice. In this paper, we study how to detect core-part plagiarism both accurately and efficiently. In the above example, the open source project from which code is copied is the original program , and the company X  X  project is called a plagiarism suspect .

A quality plagiarism detector has strong impact to law-suit prosecution. It reveals where are the plagiarized parts and what plagiarism operations are applied, which may oth-erwise be hard for human beings to identify due to the large code size and tricky disguises.

Although current plagiarism detection tools appear suffi-cient for academic use, like finding copied programs in pro-gramming classes, they are nevertheless short for fighting against serious plagiarists. These tools are mainly based on program token strings, which, as will be explained in Sec-tion 3.2, are fragile to some disguises that can be done auto-matically. For example, disguises like statement reordering, replacing a while loop with a for loop, and code insertion can effectively confuse these tools. Therefore, more robust detection algorithms are well needed.

From a knowledge discovery point of view, the detection of core-part plagiarism is actually an interesting data min-ing problem. In the first place, plagiarism detection is in essence to find from source code interesting patterns that uncover disguised code changes. These patterns should be an intrinsic representation of programs such that they are hardly overhauled in plagiarism. In the second place, be-cause the plagiarized core parts only account for a small portion of the entire program, finding these real plagiarized parts is like anomaly detection. For example, the detec-tion is expected to be low at false positive rate. Finally, for practical concern, the detection algorithm should scale to large programs in both accuracy and efficiency. These three factors altogether make core-part plagiarism detection a challenging, as well as interesting, data mining problem.
We examined those disguises that are effective in confus-ing current plagiarism detection tools, and found that they are nearly futile to the program dependence graph (PDG): PDGs almost stay the same even when the source code is significantly altered. A PDG is a graph representation of the source code of a procedure, where statements are rep-resented by vertices, and data and control dependencies be-tween statements by edges (details in Section 2). Intuitively, PDGs encode the program logic, and in turn reflect develop-ers X  thinking when code is written. Code changes regardless of dependencies are prone to errors, and a plagiarist who wants to alter PDGs through code changes should under-stand the program first. Thus, a plagiarist can freely modify the code, but as long as the program correctness is preserved, the dependence graph is hardly overhauled. Although an ex-traordinarily creative and diligent plagiarist may correctly overhaul the PDGs, the cost is likely higher than rewriting her own code, which contradicts with the incentive of plagia-rism. After all, plagiarism aims at code reuse with disguises, which requires much less effort than writing one X  X  own.
We develop a PDG-based plagiarism detection algorithm, which exploits the invariance property of PDGs. Suppose the original program P and the plagiarism suspect P each have n and m procedures, then the two programs are repre-sented by two PDG sets G and G , respectively, and |G| = n and |G | = m . Then the problem of plagiarism detection boils down to two sub-problems: First, given g  X  X  and g  X  X  , how can we decide whether g is a plagiarized PDG of g ? Second, how to efficiently locate real plagiarized PDG pairs, while in principle n  X  m pairs are to be checked?
We approach the first problem through relaxed subgraph isomorphism testing: Whenever g is  X  -isomorphic to g , ( g, g ) is regarded as a plagiarized PDG pair, where  X  is the relaxation parameter. Although subgraph isomorphism testing is in general NP-complete, it is totally tractable in this application. In the first place, PDGs cannot be arbitrar-ily large as procedures are designed to be of reasonable size for developers to manage. Secondly, PDGs are not general graphs, and their peculiarity, like varieties of vertex types, makes backtrack-based isomorphism algorithm efficient. Fi-nally, different from conventional isomorphism testing, we are satisfied as long as one, rather than all , isomorphism between g and g is found. These three factors make iso-morphism testing efficient, although it appears formidable at the first glance.

As to the second problem, we notice that only a small portionoftheentire n  X  m PDGs pairs need isomorphism testing. Most PDGs pairs can be excluded from detailed isomorphism testing because they are dissimilar even with a high-level examination. Therefore, we design a lossy filter to prune these dissimilar PDG pairs. Different from con-ventional similarity measurement that usually is based on a certain distance metric, this filter follows a similar reasoning to hypothesis testing: A PDG pair ( g, g ) is preserved until enough evidence is collected against the similarity between g and g . In comparison with distance-based methods, this approach avoids the difficulty of proper parameter setting, and it also provides a statistical estimation of the false nega-tive rate. In experiments, the lossy filter, collaborating with another lossless filter, usually prunes about nine tenths of the original search space.

Based on the above design, we implemented a PDG-based plagiarism detection tool, called GPlag . Experimental re-sults indicate that GPlag is both effective and efficient: It accurately catches plagiarism that slips over current state-of-the-art detection tools, and it takes a few seconds to find (simulated) core-part plagiarism in programs having thou-sands of lines of code.

In summary, we makes the following contributions in this study: 1. We design and implement a PDG-based plagiarism detec-tion tool, called GPlag , which conducts plagiarism anal-ysis on the program dependence graphs. Because PDGs are robust to the disguises that confuse current state-of-the-art tools, GPlag is more effective, and hence more suitable for industrial use. 2. In order to make GPlag scalable to large programs, and suitable for core-part plagiarism detection, we design a statistical lossy filter, which, when collaborating with a lossless filter, significantly prunes the search space. This makes GPlag efficient for large programs. 3. Finally, this study introduces PDGs, a new kind of graphs, to the data mining community. Graphs have been used for data modeling in many domains. Here PDGs are in-troduced as a graphic modeling for program source code.
This paper exemplifies that proper mining of PDGs can lead to more effective plagiarism detection algorithm. The rest of the paper is organized as follows. Section 2 intro-duces the program dependence graph and related graph ter-minologies. We review previous plagiarism detection tech-niques in Section 3. The details of PDG-based plagiarism detection and its implementation are discussed in Sections 4 and 5. The experimental evaluations are presented in Sec-tion 6. Section 7 discusses the related work and the potential implications of the GPlag approach to software industry. Finally, Section 8 concludes this study.
A program dependence graph (PDG) is a graph representa-tion of the source code of a procedure [4]. Basic statements, like variable declarations, assignments, and procedure calls, are represented by program vertices in PDGs. Each vertex has one and only one type, and several important types are listed in Table 1, which also illustrates how source code is decomposed and mapped to program vertices. The data and control dependencies between statements are represented by edges between program vertices in PDGs.

Definition 1 (Control Dependency Edge). There is a control dependency edge from a  X  X ontrol X  vertex to a second program vertex if the truth of the condition controls whether the second vertex will be executed.

Definition 2 (Data Dependency Edge). There is a data dependency edge from program vertex v 1 to v 2 if there is some variable var such that:  X  v 1 may be assigned to var , either directly or indirectly through pointers.  X  v 2 may use the value in var , either directly or indirectly through pointers.
  X 
There is an execution path in the program from the code corresponding to v 1 to the code corresponding to v 2 along which there is no assignment to var .

Definition 3 (Program Dependence Graph). The program dependence graph G for a procedure P is a 4-tuple element G =( V, E,  X ,  X  ) ,where  X 
V is the set of program vertices in P  X 
E  X  V  X  V is the set of dependency edges, and | G | = | V  X   X  : V  X  S is a function assigning types to program ver-tices,  X   X  : E  X  T is a function assigning dependency types, either data or control, to edges.
 Therefore, a program dependence graph is a directed, la-belled graph, which represents the data and control depen-dencies within one procedure. It depicts how the data flows between statements, and how statements control or are con-trolled by other statements.

Figure 1 provides an example to illustrate program depen-dence graph. Figure 1(a) depicts the PDG of the procedure sum whose code is on the right in Figure 1(b). Data and control dependencies are plotted in solid and dashed lines respectively. Specifically, the text inside each vertex gives its vertex id, vertex type, and corresponding source code. The edges are explained by Definitions 1 and 2.

Because we will use graph isomorphism to detect plagia-rism, related terminologies are defined below.

Definition 4 (Graph Isomorphism). A bijective func-tion f : V  X  V is a graph isomorphism from a graph G =( V, E,  X ,  X  ) to a graph G =( V ,E , X  , X  ) if  X   X  ( v )=  X  ( f ( v )) ,  X  X  X  e =( v 1 ,v 2 )  X  E,  X  e =( f ( v 1 ) ,f ( v 2 ))  X  E such that  X  ( e )=  X  ( e ) ,  X  X  X  e =( v 1 ,v 2 )  X  E ,  X  e =( f  X  1 ( v 1 ) ,f  X  1 ( v 2 that  X  ( e )=  X  ( e )
Definition 5 (Subgraph Isomorphism). An injective function f : V  X  V is a subgraph isomorphism from G to G if there exists a subgraph S  X  G such that f is a graph isomorphism from G to S .

Definition 6 (  X  -Isomorphic). A graph G is  X  -isomorphic to G if there exists a subgraph S  X  G such that S is subgraph isomorphic to G ,and | S | X   X  | G | ,  X   X  (0 , 1] .
This section reviews existing plagiarism detection algo-rithms. We first illustrate common plagiarism disguises through an example in Section 3.1, and then analyze the shortcom-ings of existing techniques for plagiarism detection in Section 3.2.
Figure 2 shows an example of plagiarism. The left proce-dure make blank is the original code, and is excerpted from aprogram join .This join program joins lines of two files on a common field, and it is shipped with all Linux and Unix distributions. It has 667 lines of C code, excluding blanks and comments, and the procedure make blank is one of the entire 17 procedures. On the right is a plagiarized version of the procedure, prepared by the authors. It exemplifies typ-ical disguises that are commonly employed in plagiarism. From trivial to complicated, they are 1. Format alteration (FA) : Insert and remove blanks and/or comments. 2. Identifier Renaming (IR) : Identifier names can be consistently changed without violating program correctness. For example, in Figure 2, the procedure name make blank is changed to fill content , variable blank is changed to fill , buf to store , etc. Identifier renaming can confuse hu-man beings, but is almost futile to detection tools. 3. Statement reordering (SR) : Some statements can be reordered without causing program errors. For example, the three declarations at lines 4 to 6 in the original code are re-ordered and scattered in the plagiarized code. Moreover, the statements on lines 7 and 11 are also reordered, as indicated by the dotted lines. In contrast, the statements from lines 8 to 10 cannot be reordered due to their sequential depen-dencies. 4. Control Replacement (CR) :A for loop can be equiv-alently replaced by a while loop, or by an infinite while loop with a break statement, and vice versa. An if(a) { A } else block can be replaced by if(!a) { B } else { A } for the same logic. In Figure 2, the for loop from lines 12 to 14 in the original code is replaced by a while loop on the right. 5. Code Insertion (CI) : Immaterial code can be inserted to disguise plagiarism, provided the inserted code does not interfere with the original program logic. In the plagiarized code, a for loop is inserted at lines 14 and 15.
We now review existing techniques for plagiarism detec-tion and examine how each of them is robust to the above five kinds of disguises. Roughly, these techniques fall into the following three categories. 1. String-based : Each statement is treated as a string, and a program is represented as a sequence of strings. Two pro-grams are compared to find sequences of same strings [1]. Because blanks and comments are discarded, this kind of algorithms are robust to format alteration, but fragile to identifier renaming. 2. AST-based : A program is first parsed into an abstract syntax tree (AST) with variable names and literal values discarded. Then duplicate subtrees are searched between two programs, and code corresponding to duplicate sub-trees are labelled as plagiarism [2, 11]. Because this ap-proach disregards the information about variables (in order to make codes differing on variables names appear the same on ASTs), it ignores data flows, and is in consequence frag-ile to statement reordering. In addition, it is also fragile to control replacement. 3. Token-based : In this approach, program symbols, like identifiers and keywords, are first tokenized. A program is then represented as a token sequence, and duplicate to-ken subsequences are searched for plagiarism between two programs [9, 17, 18]. Because variables of the same type are mapped into the same token, this approach is robust to iden-tifier renaming. However, since this approach relies on se-quential analysis, it is generally fragile to statement reorder-ing, and code insertion: A reordered or inserted statement can break a token sequence which may otherwise be regarded as duplicate to another sequence. This fragility can be par-tially remedied through fingerprinting [18], but it does not fundamentally save token-based algorithms. Finally, token-based methods are also fragile to control replacement be-cause for and while loops render different token sequences: Not only are the keywords changed, but the code for itera-tion indexing and condition checking is also moved around.
Two representatives of token-based algorithms, Moss [18] and JPlag [17], are the two most commonly used tools for plagiarism detection in practice. They prove effective in detecting plagiarism in programming classes. However, since they are token-based, experienced plagiarist can con-fuse them through disguises, such as statement reordering and code insertion. Therefore, an approach that is robust (at least) to the five kinds of disguises are expected, and PDG-based algorithms turn out to be a suitable choice. In summary, the robustness of algorithms based on different representation of programs is compared in Table 2.
In this section, we discuss PDG-based plagiarism detec-tion. We first formulate the problem in Section 4.1, where we decompose the problem of plagiarism detection into two sub-problems. We then address the two sub-problems in Sections 4.2 and 4.3, respectively. Finally, Section 4.4 dis-cusses the computation feasibility.
Given an original program P , and a plagiarism suspect P , plagiarism detection tries to search for duplicate structures between P and P in order to prove or disprove the existence of plagiarism. By representing a program as a set of PDGs, the search for duplicates are performed on PDGs.

Suppose P and P each have n and m procedures, then they are represented by two PDG sets G and G respectively, and |G| = n and |G | = m . Then the problem of plagiarism detection consists of the following two sub-problems: 1 . Given g  X  X  and g  X  X  , how to judge whether the corre-sponding procedure of g is plagiarized from that of g ?Ifit is, ( g, g ) is called a plagiarized pair ,ora match for short. 2 . Given G and G , how to locate most, if not all, plagiarism pairs both accurately and efficiently?
Since we are in particular interested in detecting core-part plagiarism, where only a small number of procedures in P are plagiarized from P if plagiarism does exist, the solution to the second problem is critical for the algorithm to be used in practice. We address these two problems in the following two subsections respectively. A detailed examination of the five kinds of disguises in Section 3.1 reveals that while the disguises can significantly alter the source code and the induced token strings, they only insubstantially affect the PDGs. Especially, these dis-guises will result in a PDG to which the original PDG is subgraph isomorphic. We hence have the following claim for plagiarism detection.

Claim 1. Restricted to the five kinds of disguises, if g ( g G ) is subgraph isomorphic to g ( g  X  X  ) , the corresponding procedure of g is regarded plagiarized from that of g .
This claim is validated by examining how each of the five kinds of disguises affects the dependence graph. For easy understanding, the PDGs for the original and the plagiarized code in Figure 2 are plotted in Figure 3. Immediately, one can figure out that the left PDG is subgraph isomorphic to the right one. We check how the applied disguises reflect on the PDGs. For clarity in what follows, we use g and g to denote the PDGs before and after a certain kind of disguises.  X 
Format alteration and identifer renaming do not alter the PDG because neither of them affects the depen-dencies, so g is identical to g . For example, although more than twenty format alterations and renamings are applied in Figure 2, the PDG of the original code is pre-served.  X  Statement reordering also leaves the PDG untouched.
Two or more statements can be reordered only when they are not bounded by dependencies. Otherwise, reordering could break dependencies, and in consequence cause pro-gram errors. As one can see, the statement reordering in
Figure 2 is invisible in Figure 3.  X  Control replacement generally leaves g identical to g .
However, when a while or a for loop is replaced by an infinite loop with a break statement, a new program ver-texoftype X  X ump X  X saddedto g . But since the new vertex does not break any existing dependencies, g is still subgraph isomorphic to g . For example, the two squares in solid lines denote the original for and the plagiarized while loop. As one can see, the PDG structure is un-changed.  X 
Code insertion introduces new program vertices and/or dependencies into g , depending on what the inserted code does. For program correctness, the inserted code is not supposed to interfere with existing dependencies. There-fore, even though g can be significantly larger than g (due to code insertion), g is still subgraph isomorphic to g . For example, the four vertices inside the dotted square in Figure 3(b) correspond to the inserted for loop in Fig-ure 2.
 Therefore, the five kinds of disguises do not hamper the essential part of the original PDG, although they can sig-nificantly alter the code appearance and the induced token strings. Since Figure 3(a) is subgraph isomorphic to Figure 3(b), the code in Figure 2 is regarded as plagiarism according to
Claim 1. In fact, the subgraph isomorphism just uncovers what disguises are applied, for example, the variable count is renamed as num ,andthe for loop is replaced by a while loop, etc. In comparison, we feed the two segments of code into both Moss and JPlag , and neither of them recognizes the similarity. This indicates that PDG-based analysis can detect tricky plagiarism that confuses the state-of-the-art tools.

In this way, restricted to the five kinds of disguises, plagia-rism detection can be accomplished through checking sub-graph isomorphism. However, requiring full subgraph iso-morphism may restrict the detection power in practice be-cause trickier disguises beyond the five kinds do exist, and some of them, even though very trivial, can easily confuse PDG-based detection when full subgraph isomorphism is re-quired. For example, suppose there are two integers, i and j , which serve as the iteration index in two independent loops, then one can be removed and replaced by the other. Reflected on the PDG, the variable removal merges two ver-tices, and in consequence, the original PDG is no longer subgraph isomorphic to the plagiarized one. Therefore, for robustness to unseen and unanticipated attacks, we relax Claim 1 into the following.

Claim 2. If g ( g  X  X  ) is  X  -isomorphic ( 0 &lt; X   X  1 )to g ( g  X  X  ) , the corresponding procedure of g is regarded plagiarized from that of g ,where  X  is the mature rate for plagiarism detection.

The mature rate  X  is set based on one X  X  belief in what pro-portion of a PDG will stay untouched in plagiarism. We set it 0.9 in experiments because overhauling (without errors) 10% of a PDG of reasonable size is almost equivalent to rewriting the code. Nevertheless, one needs to understand the code before breaking dependencies.
In order to find plagiarized PDG pairs, n  X  m pair-wise (relaxed) subgraph isomorphism testings are needed in prin-ciple. However, since most pairs can be excluded through a rough examination, the following two subsections discuss how the search space can be pruned.
First, PDGs smaller than an interesting size K are ex-cluded from both G and G . For plagiarism detection, we only need to locate PDG pairs of non-trivial sizes, which, if found, can provide enough evidence for proving plagia-rism. Second, based on the definition of  X  -isomorphism, a PDG pair ( g, g ), g  X  X  and g  X  X  , can be excluded if | g | &lt; X  | g | . These two forms of pruning are lossless in the sense that no PDG pairs worthy of isomorphism testing are falsely excluded. Even through the above two-stage pruning, for any g  X  G , there are still multiple g  X  X  ,towhich g should be checked for (relaxed) subgraph isomorphism. However, since matched PDGs tend to look similar, pairs of dissimilar PDGs can be excluded. This filter is lossy in that some interesting PDG pairs may be falsely excluded.

The similarity measurement must be light-weighted for it to be cost-effective; otherwise, direct isomorphism testing may be more efficient. We therefore take the vertex his-togram as a summarized representation of each PDG. Specif-ically, the PDG g is represented by h ( g )=( n 1 ,n 2 ,  X  X  X  where n i is the frequency of the i th kind of vertices, and the PDG g  X  X  is similarly presented by h ( g )=( m 1 ,m 2 ,  X  X  X  We measure the similarity between g and g in terms of their vertex histograms. A hypothesis-testing based approach is developed in the following, whose advantages over conven-tional distance-based measurement are discussed in Section 4.3.3.

The major idea is that we first estimate a k -dimensional multinomial distribution P g (  X  1 , X  2 ,  X  X  X  , X  k )from h ( g ), and then consider whether h ( g ) is likely to be an observation from P g .Ifitis,( g, g ) should be checked; otherwise, it is excluded. This judgement is based on a log likelihood ratio test, as outlined below.

The multinomial distribution P g (  X  )for g is estimated with where n = k i =1 n i ,and  X  is a smoothing parameter, and is commonly set as 0.05. We now examine how likely h ( g )is an observation from P g (  X  ). For clarity in what follows, we use X =( m 1 ,m 2 ,  X  X  X  ,m k )todenote h ( g ).
 We formulate a hypothesis testing problem: Then the generalized likelihood ratio with the observation X is Because under H 0 , the model for X is fixed, then The nominator, on the other hand, achieves its maximum when  X  assumes the maximum likelihood estimate, namely, Therefore, the likelihood ratio is Let T ( X )=2log(  X  ( X )), where T ( X ) is the test statistic for G-Test, and asymptoti-cally conforms to  X  2 k  X  1 under H 0 . A rigid derivation for the asymptotical approximation can be found in [19].

Given a significance level  X  , we then check whether T ( X ) &gt;  X   X  1 (  X  ), where  X  2 k  X  1 (  X  ) is the upper  X   X  100 percentile of the  X  2 k  X  1 distribution. If it is, H 0 is rejected, which means that g is dissimilar to g under the significance level  X  .In consequence, the PDG pair ( g, g )isexcludedfromisomor-phism testing. The underlying rationale is that ( g, g )byde-fault should be checked unless sufficient evidence is collected against their similarity, and hence against their chance to be an isomorphic pair. In this sense, this hypothesis testing-based filter is conservative, having a low false negative rate.
In fact, the false negative rate can be estimated in the framework of hypothesis testing. According to the interpre-tation of the Type I error,  X   X  100% of all PDG pairs worthy of isomorphism checking are falsely excluded on average . For plagiarism detection, false negatives are not a serious problem because as long as some (but not necessarily all) nontrivial plagiarism pairs are found, it is sufficient to sup-port convictions of plagiarism. On the other hand, people who are not comfortable with false negatives can neverthe-less lower down the significance level  X  .When  X  =0,there are no false negatives, but no further pruning either. The value of the lossy filter is its ability to prune spurious PDG pairs that would otherwise waste much time in isomorphism checking. Therefore, this is a tradeoff between efficiency and false negatives, and efficiency is usually preferred for plagiarism detection.
There are other alternatives to the above G-test based filtering. For example, one can adopt a distance-based ap-proach: a PDG pair ( g, g ) is excluded if the distance be-tween h ( g )and h ( g ) is larger than a preset threshold  X  . But in general, no guidance is available for a proper set-ting of  X  . Moreover, a proper setting of  X  for a particular pair of programs does not generalize to other programs. In comparison, for the hypothesis testing-based filter, the only parameter  X  is application independent, and can be set in a meaningful way: it balances the pruning power and the false negative rate.

Secondly, people may wonder why the popular Pearson X  X   X  2 test [13] is not used, given hypothesis testing-based prun-ing is better than distance-based ones. The reason is that Pearson X  X   X  2 test is appropriate only when no frequencies in the vertex histogram is near zero because its validity relies on an approximation from a multinomial to a multivariate normal distribution. In our case, since certain m i  X  X  can be or close to 0, Pearson X  X   X  2 test is thus inappropriate. In comparison, G-test is much more robust than Pearson X  X   X  2 test [13], and is hence chosen here.

Finally, people may want to fingerprint PDGs with fea-tures other than vertex frequencies. We choose vertex his-togram just because it is cheap to collect, and performs well. People can index PDGs with structural features, like paths or frequent subgraphs, but this would be computationally expensive, and hence not cost-effective.
Because our PDG-based plagiarism detection involves sub-graph isomorphism testing, we discuss the computation fea-sibility in this subsection.

Although subgraph isomorphism is NP-complete in gen-eral [5], research in the past three decades has shown that some algorithms are reasonably fast on average and become computationally intractable only in a few cases [6] [7]. For example, algorithms based on backtracking and look-ahead, e.g. , Ullmann X  X  algorithm [20] and VF [8] are comfortable with graphs of hundreds or thousands of vertices.
Besides the general tractability, the peculiarity of PDGs and the needs for plagiarism detection also lower down the computation workload. In the first place, PDGs cannot be arbitrarily large as procedures are designed to be of reason-able size for developers to manage. Secondly, PDGs are not general graphs, and their peculiarity, like varieties of vertex types, and incompatibility between different types, makes backtrack-based isomorphism algorithm efficient. Lastly, but not the least, for plagiarism detection, the first iso-morphism between g and g suffices, while the conventional isomorphism testing finds all isomorphism functions. These three factors make the isomorphism testing on PDGs tractable, and efficient in practice.

Finally, the lossless and lossy filters can effectively toss away spurious PDG pairs from detailed isomorphism testing. In consequence, only a small portion of PDG pairs are really checked. Therefore, our PDG-based plagiarism detection is computationally efficient, although it appears formidable at the first glance. Algorithm 1 GPlag ( P , P , K ,  X  ,  X  ) Input: P : The original program Output: F : PDG pairs regarded to involve plagiarism 1:
G =ThesetofPDGsfrom P 2:
G =ThesetofPDGsfrom P 3:
G K = { g | g  X  X  and | g | &gt;K } 4: G 5: for each g  X  X  K 6: let G K,g = { g | g  X  X  K , | g | X   X  | g | , ( g, g ) passes filter 7: for each g  X  X  K,g 8: if g is  X  -isomorphic to g 9: F = F X  ( g, g ) 10: return F ;
Algorithm 1 outlines the work-flow of GPlag ,aPDG-based plagiarism detection tool. It takes as input an original program P and a plagiarism suspect P , and outputs a set of PDG pairs that are regarded as involving plagiarism. In the end, people need to examine these returned PDG pairs, confirming plagiarism and/or eliminating false positives.
At lines 1 and 2, PDGs of the two programs are col-lected. We wrote a Scheme program to derive and simplify the PDGs from CodeSurfer 1 via its provided APIs. Espe-cially, control dependencies are excluded from consideration in the current implementation for efficiency concerns. Then at lines 3 and 4, PDGs smaller than K are excluded. Fi-nally, from lines 5 to 10, GPlag searches for plagiarism PDG pairs. For each g that belongs to the original pro-gram, line 6 obtains all g  X  X  that survive both the lossless and the lossy filters. And line 8 performs the  X  -isomorphism testing. The process from lines 5 to 10 was implemented in C++ based on the VFLib 2 . By default, K =10,  X  =0 . 9, and  X  =0 . 05 unless otherwise stated. The time reported in the following experiments is the time cost from lines 5 to 10, and is in seconds.
In this section, we evaluate the effectiveness and efficiency of
GPlag through experiments. Section 6.1 describes the experiment design and setup, and the following subsections discuss the experimental results in detail.
We chose four subject programs for experiments, whose characteristics are listed in Table 3. The number of lines of http://www.grammatech.com/ http://amalfi.dis.unina.it/graph/ code (LOC) is measured with the tool sloccount 3 ,which excludes both blanks and comments. The third and fourth columns list how many procedures each program has, and how many of them are left with K = 10. Finally, the fifth column concisely describes the subject programs.

The join program is mainly used for effectiveness evalu-ation. We spent two hours plagiarizing it such that both Moss and JPlag were confused. In comparison, we show that GPlag successfully detects the plagiarism (details in Section 6.2). Section 6.3 then focuses on efficiency study, where the three large programs, bc , less ,and tar are used. It examines the pruning power of the lossless and the lossy filters, and their implications to the ultimate time cost. Fi-nally, in Section 6.4, we simulate six core-part plagiarism cases with the four subject programs, and evaluate GPlag  X  X  performance in detecting core-part plagiarism. All experi-ments were carried out on a Pentium 4 PC with 1GB phys-ical memory, running Fedora Core 2. The compiler is gcc-3.3.3 with no optimizations.
We compare GPlag with Moss and JPlag for effective-ness evaluation. We plagiarize the program join according to the following recipe. Because all the three tools are ro-bust to format alteration and identifier renaming, these two kinds of disguises were skipped.
 Plagiarism Recipe: 1. Whenever m (usually 2 to 4) consecutive statements are not bounded by dependencies, reorder them. 2. Replace a while loop with an equivalent for loop, and vice versa. Occasionally, a for loop is replaced by an infinite while loop with a break statement. 3. Replace if(a) { A } with if(!(!a)) { A } ,and if(a) { A with if(!a) { B } else { A } ; recurse if nested if block is en-countered. Finally, apply DeMorgan X  X  Rule if the boolean expression a is complex. 4. Run both Moss and JPlag . For any places that they are not confused, insert a statement or a label. Because inserted code breaks the recognized token sequence, code insertion is always effective in confusing token-based al-gorithms. 5. Finally, run test scripts, and ensure that correctness is preserved during plagiarism.

Although the above recipe suffices to confuse both Moss and JPlag , in order to test GPlag  X  X robustnesstotricky attacks, we tried to eliminate redundant code, but finally failed to find any redundant code. In general, we expect that few redundancies exist in mature programs, like join
The above plagiarism took us about two hours, which sug-gested that nontrivial work is needed to confuse token-based detections manually . However, we notice that the above pla-giarism is mechanical to apply, and with some efforts, the plagiarism can be (at least partially) automated. In con-sequence, confusing token-based algorithms is not laborious any more. This possibility underlines the need for new de-tection tools that are more robust than token-based ones. Finally, we note that although it sounds irrational to spend two hours plagiarizing a program of 667 LOC, writing a sim-ilar program as mature as join will take even much longer. http://www.dwheeler.com/sloccount/
There are totally 17 procedures in the program. The pro-cedures main and usage are excluded because interface pro-cedures like them are always ripped off and rewritten in  X  X rofessional X  plagiarism. Among the rest 15 procedures, 9 procedures are filtered out due to their small sizes. Finally, six procedures are left in both the original and the plagia-rized versions. Table 4 lists the six procedures, together with what disguises are applied to each of them.

Although these disguises succeeded in confusing both Moss and JPlag , the plagiarism was detected by GPlag in less than 0.1 second. Specifically, it finds six isomorphic pairs, each of which corresponds to one plagiarized procedure. This indicates that plagiarism that slips over token-based check-ing can be easily detected by PDG-based algorithm, which reaffirms the comparison in Table 2.
In this subsection, we evaluate the efficiency of GPlag with the three large programs, bc , less ,and tar . Specifically, we take an exact copy of the original program as a pla-giarized version. Because PDGs are insensitive to identifier renaming, statement reordering and control replacement, an exact copy is equivalent to a program intensively plagiarized with the aforementioned three kinds of disguises, as far as GPlag is concerned. We first examine the pruning power of the lossless and lossy filters, and then study the implication of pruning to the ultimate time cost.
Figure 4 plots the pruning effect of the lossless and the lossy filters for different programs, when  X  varies from 0 to 0.1. The y -axis is the pruned ratio, i.e. , what percentage of PDG pairs are excluded from G K  X G K . The horizontal dotted lines show the pruning effect of the lossless filter on the three programs. Roughly, about one half PDG pairs are pruned with the lossless filter only. The solid lines plot the pruning effects when the lossy filter is also employed. The curve rocketing from  X  =0to  X  =0 . 01 indicates that a great proportion of PDG pairs that survive the lossless filter are in fact quite dissimilar, and do not need isomorphism testing. When  X  gets larger, more PDG pairs are pruned, but at a mild rate.

We now examine how the pruning affects the ultimate time cost. Table 5 lists the efficiency comparison on the three subject programs when 1) no filter, 2) only lossless, and 3) both lossless and lossy filters are applied. For each of the three kinds of filter application, we record how many PDG pairs are actually tested, the time cost, and the num-ber of found matches. The time is capped in the sense that a timeout of 100 seconds is set for every isomorphism testing. Timeout is necessary because subgraph isomorphism testing can still take hours (or even longer) to finish for some cases. A PDG pair whose isomorphism testing fails to terminate within 100 seconds is called a time hog , and is regarded un-matched.

We now examine Table 5 in detail. The first interesting point it suggests is that the lossless filter alone does not save much time, although it cuts off about one half of the PDG pairs (Figure 4). Specifically, the saving is only a few sec-onds. The explanation is that for PDG pairs pruned by the lossless filter, the isomorphism testing algorithm also rec-ognizes the impossibility of isomorphism within a few steps of search. On the other hand, when the lossy filter is ap-plied, the time cost significantly shrinks. With a time cap set as 100 seconds, and most isomorphism testings finish within tens or hundreds of milliseconds, we know that the time saving mainly comes from the avoidance of time hogs, rather than from the pure reduction of PDG pairs. There-fore, the lossy filter also helps circumvent time hogs while tossing away spurious PDG pairs.
In this section, we simulate cases of core-part plagiarism with the four subject programs, and examine the effective-ness and efficiency of GPlag in detecting core-part plagia-rism. Specifically, we treat the three large programs as car-rier programs, and embed the original and the plagiarized versions of the six procedures (listed in Table 4) into two different carrier programs. GPlag is expected to find the six plagiarized procedures both accurately and efficiently.
Table 6 presents the experiment results for the six car-rier combinations, with and without the lossy filter. The six matches are all detected in the six carrier program com-binations. Clearly, with the lossy filter, much fewer false positives are alarmed. As to the time cost, similar to the result in Table 5, the lossy filter significantly reduces the time cost for these simulated core-part plagiarisms. Again, this time reduction is due to the avoidance of most time hogs. Therefore, the lossy filter is critical for GPlag to de-tect core-part plagiarism in large programs: Not only can it reduce the false positive rate, but it also makes time cost acceptable.
In this section, we discuss the related work and potential industry implications of our work.
This study is closely related to the previous work on pla-giarism detection. In Section 3.2, we provided an overview of existing techniques based on different representations of programs [1,2,9,11,17,18]. We show that GPlag is more effective than these methods, both conceptually and experi-mentally. The program dependence graph, first proposed by Ferrante et al. [4], has previously been used in the identifi-cation of duplicated code for the purpose of software main-tenance [10, 12]. In this study, we propose GPlag as a PDG-based algorithm for plagiarism detection. Moreover, for both effectiveness and efficiency, a statistical lossy fil-ter is developed, which has not been seen in previous stud-ies [10, 12]. However, on the other hand, since GPlag in-volves graph analysis, it is nevertheless less efficient than those based on sequence analysis. But this difference has only little impact in practice, because GPlag usually ter-minates within seconds even when subject programs are of thousands of lines of code. There are studies on detection of other kinds of plagiarism, such as plagiarized research pa-pers, homework answers, and Web pages [3]. The nature of such plagiarism is rather different from that of software plagiarism which often requires more sophisticated analysis.
From the data analysis point of view, GPlag is related to graph mining. Graphs have been adopted for data modeling in many domains, and many graph mining algorithms are developed. However, these algorithms cannot be used for plagiarism detection because they search for potential can-didates almost everywhere in a  X  X andidate-generation-and-check X  approach. As an example, given two identical PDGs with 30 vertices and 43 edges, CloseGraph [21] fails to ter-minate in two days . This paper proposes an isomorphism-based approach, which proves both effective and efficient. Finally, this study falls into an emerging application do-main in data mining: data mining for software engineer-ing. Previous research indicates that proper mining of soft-ware data can produce useful results for software engineers. Livshits et al. apply frequent itemset mining algorithms to software revision history, which uncovers programming rules that developers are expected to conform to [16]. Liu et al. show that mining program control flow graphs can help de-velopers find logic errors [14,15]. These cases well exemplify the promise and usefulness of data mining in software engi-neering. This study provides yet another such example.
Software plagiarism has been an important issue in soft-ware industry for intellectual property and software license protection, especially for open source projects. Thus it is important to develop robust and effective approaches to software plagiarism detection. Our study shows that pre-vious approaches that rely on string matching, parse-tree construction, and tokenization cannot handle sophisticated control flow alternation and code insertion, and thus cannot be effective at fighting against  X  X rofessional X  plagiarists.
This study proposes a rather different approach, GPlag , which bases the analysis on program dependence graphs. The study demonstrates its effectiveness at the detection of sophisticated plagiarism in comparison with the current state-of-the-art tools. Moreover, our experiments show that this approach is also scalable to large programs. Even in practice when software of millions of lines of code is encoun-tered, GPlag may be still applicable because one compo-nent only needs to be compared with its counterpart. Fi-nally, GPlag can be easily extended to other programming languages. What one needs for a new language is merely a parsing frontend, from which PDGs can be derived. Cur-rently, the frontends for C, C++ and Java are available and ready to use. To this extent, GPlag is not only a program for idea demonstration but also a practical tool.
One may wonder whether GPlag can be really used for lawsuit conviction. In the experiments, we have witnessed low false positive rate. In general, chances are slim that one PDG is isomorphic to another by chance. Even for the same task, two developers will likely come up with different imple-mentations, and different PDGs in consequence. Therefore, if
GPlag judges one case as plagiarism, the chance could be high. However, human participation is nevertheless needed for result verification and final judgement. This paper proposes a new plagiarism detection algorithm, GPlag , which detects program plagiarism based on the analysis of program dependence graphs. Experiments have well demonstrated its effectiveness over existing tools, and its applicability in practice. [1] B. S. Baker. On finding duplication and near [2] I. D. Baxter, A. Yahin, L. Moura, M. Sant X  X nna, and [3] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, [4] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The [5] M. Garey and D. Johnson. Computers and [6] C. Hoffman. Group-theoretic Algorithms and Graph [7] J. E. Hopcroft and J. K. Wong. Linear time algorithm [8] J.E.HopcroftandJ.K.Wong.Performance [9] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder: a [10] R. Komondoor and S. Horwitz. Using slicing to [11] K. Kontogiannis, M. Galler, and R. DeMori. Detecting [12] J. Krinke. Identifying similar code with program [13] E. Lehmann. Testing Statistical Hypotheses . Springer [14] C. Liu, X. Yan, and J. Han. Mining control flow [15] C. Liu, X. Yan, H. Yu, J. Han, and P. S. Yu. Mining [16] V. B. Livshits and T. Zimmermann. Dynamine: [17] L. Prechelt, G. Malpohl, and M. Philippsen. Finding [18] S. Schleimer, D. S. Wilkerson, and A. Aiken. [19] R. R. Sokal and F. J. Rohlf. Biometry: the principles [20] J. R. Ullmann. An algorithm for subgraph [21] X. Yan and J. Han. CloseGraph: mining closed
