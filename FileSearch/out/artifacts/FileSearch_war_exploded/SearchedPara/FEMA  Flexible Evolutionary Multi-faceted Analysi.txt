 Behavioral pattern discovery is increasingly being studied to un-derstand human behavior and the discovered patterns can be used in many real world applications such as web search, recommender system and advertisement targeting. Traditional methods usually consider the behaviors as simple user and item connections, or rep-resent them with a static model. In real world, however, human behaviors are actually complex and dynamic: they include corre-lations between user and multiple types of objects and also con-tinuously evolve along time. These characteristics cause severe data sparsity and computational complexity problem, which pose great challenge to human behavioral analysis and prediction. In this paper, we propose a Flexible Evolutionary Multi-faceted Analysis (FEMA) framework for both behavior prediction and pattern min-ing. FEMA utilizes a flexible and dynamic factorization scheme for analyzing human behavioral data sequences, which can incor-porate various knowledge embedded in different object domains to alleviate the sparsity problem. We give approximation algorithms for efficiency, where the bound of approximation loss is theoret-ically proved. We extensively evaluate the proposed method in two real datasets. For the prediction of human behaviors, the pro-posed FEMA significantly outperforms other state-of-the-art base-line methods by 17.4%. Moreover, FEMA is able to discover quite a number of interesting multi-faceted temporal patterns on human behaviors with good interpretability. More importantly, it can re-duce the run time from hours to minutes, which is significant for industry to serve real-time applications.
 I.5.3 [ Computing Methodologies ]: Pattern Recognition -Cluster-ing; J.4 [ Computer Applications ]: Social and Behavioral Sciences Behavior Modeling; Behavioral Pattern; Evolutionary Analysis; Ten-sor Factorization; Flexible Regularizers
Scientists study human behavior from a variety of cultural, polit-ical, and psychological perspectives, looking for consistent patterns of individual and social behavior and for scientific explanations on those patterns. It is well accepted that human behavior is the prod-uct of a multitude of interrelated factors. The factors such as phys-ical environment, social interaction, and social identity, affect how the behavior takes place with our personalities and interests. As an example, if a researcher changes his affiliation, he will start to collaborate with new friends, join in new projects and eventually study new topics. Given the complexity of multi-faceted factors influencing human behaviors, it is difficult to concisely summarize what they are and how they interact. Moreover, psychological stud-ies [21] demonstrate that human behaviors naturally evolve with the changing of both endogenous factors (e.g., personality) and ex-ogenous factors (e.g, environment), resulting in different dynamic (temporal) behavioral patterns over time. For example, in early 1990s, many researchers focused on database systems and query processing. In late 1990s, with various data collective methods emerging and scales of unlabeled data increasing, they turned to work on clustering and pattern mining problems. In 2000s, people started to focus on social networks and communities since Face-book and Twitter become popular. Consequently, the patterns of human behaviors differ from place to place, era to era and across environments. The complexity and dynamic characteristics pose great challenges to understanding and predicting human behaviors. However, there is a lack of research to support behavioral modeling with both multi-faceted and temporal information.

Traditional methods of data analysis have long been used to dis-cover patterns of human behaviors. Sun et al. [27] perform 3-mode analysis on the click-through data with user, query and web page. Chen et al. [2] models tagging behavior with the decomposition of ternary relationships of user, tag and item. However, their static views on human behavior are not able to learn from temporal in-formation, or capture the dynamic characteristic. Radinsky et al. [22] use several time-series models for representing and predicting web search behavior and content change. Xiang et al. [33] use ses-sion nodes to capture short-term interests of paper-tagging behavior through session-item connections. However, their representations cannot learn from multi-faceted information, or fully describe the complex characteristic of human behavior. Hence, temporal multi-faceted behavioral patterns are rarely investigated, and how to ac-curately predict these behaviors still remains as an open problem.
There are two key challenges to learn human behavioral patterns from the multi-faceted and temporal information. To address these challenges, in this paper, we propose a Flexible Evolutionary Multi-faceted Analysis (FEMA) method based on a dynamic scheme of tensor factorization for temporal multi-faceted behavior prediction and pattern mining, where flexible regulariz-ers are imposed to alleviate the problems brought by high sparsity. In order to fast decompose high-order tensor sequences, we give approximation algorithms to factorize the new tensor with sparse increments, where the bound of approximation loss is theoretically proved. We evaluate FEMA on two real datasets: publication data from Microsoft Academic Search database and tweet data from Tencent Weibo, a Twitter style website in China. The proposed method achieves 30.8% higher accuracy when it uses multi-faceted factors and 17.4% higher accuracy when it uses flexible regulariz-ers. Moreover, it can reduce the run time from hours to minutes, which is of significant interest to serve real-time applications.
Fig. 1 is a showcase of temporal patterns of academic research behavior discovered by FEMA. It can be seen that the evolutions in Professor Jiawei Han X  X  group from database systems, data clus-tering to social/information networks can be effectively and effi-ciently discovered by FEMA. And the pattern clearly shows the co-evolution and interplay between affiliations and his co-authorships, which gives us comprehensive understanding on the temporal and multi-faceted characteristics of behavioral patterns.

The main contributions of this paper are: (1) Enlightened by the psychological studies on human behav-iors, we move one step forward to incorporate temporal dimension into multi-faceted analysis for temporal multi-faceted behavior pre-diction and pattern mining, which is of paramount importance for various applications, such as web search and recommendation. (2) We propose the FEMA framework based on tensor factoriza-tion to predict temporal multi-faceted behaviors. The model incor-porates flexible regularizers to alleviate the sparsity problem. We design approximation algorithms to fast conduct evolutionary anal-ysis of human behaviors. (3) Our algorithm is efficient and has a theoretical guarantee: it runs in near-quadratic time compared to the near-cubic time of the existing algorithms (see Section 4). (4) We conduct extensive experiments to predict human behav-iors in academic research and social networks. The results show that the proposed FEMA can outperforms other methods on both datasets. More importantly, we demonstrate that the approximation algorithms lead to significant time reduction and the loss is small.
We have the usual organization: Survey, problem definition, pro-posed method, experiments and conclusions.
There is a significant body on research related to our problem, which we categorize into three groups: behavior modeling, behav-ioral dynamics, and tensor factorization.

Behavior modeling: Matrix factorization has long been used for modeling and predicting human behavior when it includes two types of objects, such as user-item adoption [18] and drug-target interaction [35]. When the number of types is more than two, there has been a great deal of interest in using high-order tensors to model behaviors, for example, web search [27], image and book tagging [29, 23], and recommender systems [11, 12, 2, 10, 20]. These works summarize a static view of the behavioral pattern, but they cannot capture its temporal characteristics.

Behavioral dynamics: There have been attempts to use tem-poral information to understand past users X  behaviors in order to predict future ones in different applications such as recommender systems [6, 3], research themes [32], semantic graphs [28], and on-line media topics [19, 5, 34]. Xiang et al. [33] divide user interests into long-term and short-term and make use of the difference, using a time factor. In contrast to this approach, we consider the group-level dynamics instead of an individual user behavior. Radinsky et al. [22] develop a learning algorithm capable of selecting an ap-propriate model depending on the time. However, how to appropri-ately use the time information to discover the underlying dynamics of human behavior still remains an important research challenge.
Matrix/tensor factorization: There has been active research on matrix factorization [30, 31], tensor analysis [4, 26, 13], tensor de-compositions [15, 16, 9] and scalable methods [17, 1]. Here we focus on how to efficiently process the increments in tensor de-composition by matrix and tensor perturbation theory [25].
In this section, we first give the modeling of two different types of human behavior including academic research and mentioning to someone in tweets. Then we give a general definition of our problem or the task of our method.

Let the bibliographic dataset be an example of our problem: we focus on finding temporal patterns of academic research behavior. Let the dataset be a list of tuples ( a,f,k,t ) denoting that an author a in an affiliation f (university, research center, etc.) publishes about a keyword k at time t ( t = 1 ,...,T ). We model the data the number of authors, n ( f ) is the number of affiliations, and n is the number of keywords. X t ( a,f,k ) has a value of the number of existing tuples ( a,f,k,t 0 ) ( t 0  X  t ). Our goal is to factorize the tensor sequence where )
Note that the key to solving the sparsity problem in tensor de-compositions is to learn the flexible regularizers such as the au-thors X  co-authorship, the affiliations X  geographical distance and the keywords X  semantic information. The regularizers can be encoded represents the similarity between the i -th and j -th entities (authors, affiliations, keywords): the similarity can be how many papers the authors collaborate or how close the affiliations locate.
The problem is now how to compute the factorizations for the core tensor sequence and projection matrices, given the tensor se-quence and constraints. Note that the scale of the tensors are large but the changes are very small. We denote by  X  X t the increment at time t , which is very sparse: for any 1  X  t &lt; T ,  X  X X t + 1  X  X  t . The problem can be summarized into two steps:
Let the tweet dataset be another example of our problem and thus we come to find temporal patterns of the mention in tweets. Let the dataset be a list of tuples ( s,d,w,t ) denoting that a Twit-ter user s ( X  X ource X ) uses the  X  X username X  format to mention a user d ( X  X arget X , or  X  X estination X ) in the body of a tweet which in-cludes a word w at time t ( t = 1 ,...,T ), so that the user d will see the tweet in his/her  X  X entions X  tab. Similarly to the modeling of academic research behavior, we model the data as a 3-order ten-sources, n ( d ) is the number of targets, and n ( w ) is the number of words. X t ( s,d,w ) is the number of tuples ( s,d,w,t 0 ) ( t Our goal is to factorize the tensor sequence R is the target users X  projection matrix, and W t  X  R n ( w ) words X  projection matrix.

Here, to solve the sparsity problem, the flexible regularizers such as the users X  social relations (e.g., the number of common friends), and the words X  semantic information, can be encoded as Laplacian matrices L ( s ) , L ( d ) , L ( w ) . Similarly, the problem can be summa-rized into two steps:
Our problem is quite different from previous research. First, we incorporate multi-faceted information and constraints into a unified framework. Second, we conduct evolutionary analysis to efficiently deal with sparse increments, which is in contrast with the majority of existing works that decompose a single tensor. We extend the formulation from 3 to M dimensions and give a general definition. Definition 1 (Flexible Evolutionary Multi-faceted Analysis (FEMA)) (1) Initialization: R (2) Evolutionary analysis: At time t ( 1  X  t &lt; T ), given the tensor X t  X  R n (1) increment  X  X t , the old projection matrices A ( m ) t | constraints L ( m ) | M m =1 , find the new projection matrices A and the new core tensor Y t + 1 .
In this section, we provide approximation algorithms for the two steps in Flexible Evolutionary Multi-faceted Analysis (FEMA). We also give a discussion on the computational efficiency and ap-proximation quality.

Here we present how we incorporate the multi-faceted informa-tion and constraints into the tensor decomposition. We denote by  X  ( m ) the weight of the mode-m Laplacian matrix L ( m ) . The co-variance matrix of the m -th mode at time t = 1 is tensor X 1 . The projection matrices A ( m ) 1 | M m =1 can be computed by diagonalization: they are the top r ( m ) eigenvectors of the covari-ance matrix C ( m ) 1 | M m =1 . The pseudocode is listed in Algorithm 1. Algorithm 1 Initialization in FEMA for m = 1 ,...,M do end for
Next we introduce an efficient technique based on tensor per-turbation to adjust the projection matrices according to changes of m matricizing of the tensor X t . We define the covariance matrix C t = X eigenvalue-eigenvector pair of the matrix C ( m ) t . The vector a is exactly the i -th column of the projection matrix A ( m ) To simplify the denotions, we omit  X  t  X  in the terms and equations when it is unnecessary. Thus we can obtain Now the key questions are how to compute changes to the eigen-value  X   X  ( m ) i and eigenvector  X  a ( m ) i , respectively. Expanding Eq.6, we obtain In this paper, we concentrate on first-order approximation, i.e., we assume all high order perturbation terms (such as X ( m ) , in the above equation) are neglectable. By further using the fact that ( X X Now multiplying both sides of Eq.8 with a ( m ) i &gt; and because of the Since the eigenvectors are orthogonal to each other, we assume that the change of the eigenvector  X  a ( m ) i is in the subspace spanned by those original eigenvectors, i.e., where {  X  ij } are small constants to be determined. Bringing Eq.10 into Eq.8, we obtain which is equivalent to Multiplying a ( m ) k &gt; ( k 6 = i ) on both sides of the above equation, we get Therefore, To get  X  ii , we use the fact that Discarding the high order term, and bringing in Eq.10, we get  X  ii = 0 . Therefore, Note that the constraints L ( m ) do not appear in the eigenvalue and eigenvector updating functions Eq.9 and Eq.15. Note that the con-straints have to be learnt only once . Algorithm 2 Evolutionary Analysis in FEMA for m = 1 ,...,M do end for
Here we analyze the computational complexity of Algorithm 2 before the computation of the core tensor. For the m -th mode, we define D ( m ) as the number of features of each point on the m -th di-mension. Since the tensors are usually extremely sparse, we know D zero entries in the tensors. In order to compute the increment on the eigenvalue and eigenvector using Eq.9 and Eq.15 for the m -th mode, we need to compute v ( m ) i , which requires O ( n time. As  X  X ( m ) is very sparse,  X  X ( m ) v ( m ) i only requires con-stant time O ( D ( m ) ) . Therefore, for computing  X   X  ( m ) values and eigenvectors for T times requires O ( T P M m =1 1) D ( m ) ) time. In comparison, if we redo the eigenvalue decompo-sition on X t + 1 , it costs O ( T P M m =1 ( D ( m ) ( n time, which is much higher.

We now present two theorems that bound the magnitude of  X   X  and  X  a ( m ) i . Both theorems confirm our intuition that the mag- X  X ( m ) . Also since the higher order terms are ignored in the ap-proximation, FEMA algorithms only works when those terms are relatively small.
 Theorem 1 The magnitude of the variation on the eigenvalue, i.e., where  X  max
P ROOF . According to Eq.9, we have By Cauchy-Schwarz inequality, where in the first step we use the symmetry of X ( m )  X  X  X  X ( m ) X ( m ) &gt; and in the second step we use the fact that k a 1 . By the definition of matrix 2-norm, we have that Therefore Theorem 2 The magnitude of the variation on the eigenvector, i.e., where  X  max
P ROOF . From Eq.15, we have that
In this section, we evaluate the effectiveness, efficiency and ro-bustness of our proposed FEMA for the tasks of behavior predic-tion. We also provide interesting discovery of the temporal behav-ioral patterns for strong additional evidence of the effectiveness.
We use the following two real-world datasets in our experiments: Statistics MAS Statistics W EIBO Author 7,777 Source user 6,200 Affiliation 651 Target user 1,813 Keyword 4,566 Word in tweet 6,435 Time 32 years Time 43 days Co-authorship 98,671 Social relation 465,438 Number of tuples 171,519 Number of tuples 519,624
Tab. 1 summarizes the characteristics of these academic and tweet datasets. We use the two datasets to perform on two different be-havior prediction tasks. Both the tasks are set up to continuously predict the future behaviors using new-arriving data. Fig. 2 shows how we use the data to set up the experiments. The two datasets were split into three parts: training for initialization, training for evolutionary analysis and testing. We use the earliest 30% for initialization, and then let the behavioral data come 5% by 5% for evolutionary analysis, and use the next 20% for testing. In other words, we will test the performance for T = 10 times that the percents of the training parts are the first  X  t = 35% , 40% to 80% , for t = 1 , 2 ,...,T .
 Figure 2: Experimental settings: we use the first 30% data for initialization, and set up 10 times of predictions that each time we train 5% more data, predict and test with the next 20%. We evaluate the effectiveness and efficiency of the proposed method. To evaluate the effectiveness, we implement 3 versions of FEMA: To compare with the state-of-the-art methods, we implement the following popular methods: To evaluate the approximation quality and efficiency, we also im-plement an offline learning version of FEMA: We implement our framework in MATLAB and perform the exper-iments on a single machine with Intel Xeon CPU at 2.40GHz and 32GB RAM, running Windows Server 2008.

By default, the parameters are r ( i ) = 50 and  X  ( i ) = 0 . 3 , for i = 1 , 2 , 3 . The discussion for the performances of different parameter settings is given later in Section 5.6.
For the first task, complex behavior prediction, we use the stan-dard evaluation metrics Mean Absolute Error ( MAE ) and Root Mean Square Error ( RMSE ) [3] defined as: where D denotes the testing set; r u,v,w is the predicted probability of the behavior that author u publishes keyword v in affiliation w or user u mentions user v in tweets of word w ; and  X  r u,v,w frequency of the behaviors in the testing set and 0 if not. Small MAE and RMSE will be a better model.

Also we use two frequently used metrics, Precision and Recall [7], to evaluate the quality of ranking for prediction values. Let T ( u,v,w ) be the set of behaviors in the testing set and let P ( u,v,w ) be the set of the predicted behaviors. Precision considers the pos-itively predicted entries within all the predictions, and Recall con-siders the positively predicted entries within all the positive ones in the testing set, so that we can plot the Precision -Recall curves by changing the lower limit of the predicted values for P ( u,v,w ) : High Precision and Recall will be a better model.

When make prediction on the mentioned (target) users in the tweet data, we generate a list of N ( N = 5 ) target users named R u,w for each source user u to mention in his/her tweets of a given word w . If the target user v appears in the list, we call it a hit. The Hit Ratio [14] is calculated in the following way: where I (  X  ) is an indicator function, R u,w is a set of top-N men-tioned users to user u in tweet of word w , and v is the hold-off user in the testing set that u posts a tweet with  X  X v X . A high Hit Ratio will be a better model.

For the second task, simplex behavior prediction, we sum the en-tries of the tensor along with the dimension of affiliation or word in tweet w as the result of prediction. Then we define similar defini-tions of MAE , RMSE , Precision , Recall , and Hit Ratio , using ( u,v ) as subscripts instead of ( u,v,w ) and ( u ) instead of ( u,w ) .
In this section, we conduct three different experiments to demon-strate the effectiveness and efficiency of the model settings of our FEMA. First, we present the usefulness of leveraging multi-faceted information with 2W prediction tasks on learning behavioral pat-terns on the academic research data MAS and tweet data W EIBO Second, we present the usefulness of leveraging flexible regulariza-tions with 3W prediction tasks. And finally, we show the effective-ness and efficiency of our evolutionary analysis.
In this subsection, we show the results of 2W prediction: pre-dicting author-keyword behaviors on MAS, and source-target be-haviors on W EIBO . We compare our FEMA with EMA and EA, while EA uses matrix instead of high-order tensor to formulate the human behaviors. In other words, EA does not learn from the in-formation of affiliation and word in tweets. Fig. 3 shows MAE and RMSE of the above methods on the 10 experiments varying the percent of training data  X  t from 35% to 80% by 5%: Fig. 3a and 3b plot the results of MAS, while Fig. 3c and 3d plot the results of W EIBO . FEMA has the smallest MAE and RMSE, while even EMA is much better than EA. Furthermore, we show the num-bers of MAE and RMSE in Tab. 2, when we set the percents of the training part as  X  t = 80% . FEMA decreases the RMSE of EA by 30.8% on MAS and 30.0% on W EIBO .

Fig. 4 plots the precision-recall curves to test the ranking re-sults of predicted human behaviors. We show that the tensor-based method EMA performs much better than the matrix-based one EA, and the FEMA performs the best on both MAS and W EIBO data. EA uses author-keyword matrix to model the academic research Table 2: The tensor-based methods FEMA and EMA have the smaller MAE and RMSE than the matrix-based method EA on 2W Prediction tasks. FEMA models the behavior as tensors, learns from flexible regularizers, and reaches the smallest er-rors. The model is better if the MAE and RMSE are smaller. (a) Precision-recall on MAS (b) Precision-recall on W EIBO Figure 4: FEMA and EMA that use high-order tensors to model human behavior performs the better than the matrix-based EA on 2W Prediction tasks when t = 10 and  X  t = 80% . The model is better if the precision and recall are higher. behavior, while EMA and FEMA use author-affiliation-keyword tensors to model it, and perform better than EA. The information of affiliation has strong impacts in the keywords: when an author changes his/her affiliation, his/her research topics may change be-cause he/she has got new collaborators and new projects. For ex-ample, in Fig. 1 we know that when Professor Jiawei Han moves from Simon Fraser University to University of Illinois at Urbana-Champaign, his main research topics change from the area of database systems to data mining. The methods of multi-faceted analysis EMA and FEMA learn the affiliation information from the MAS dataset and better predict what topic an author will study.
Similarly, EMA and FEMA use the words in tweets as the third facet to model the mentioning behavior on the microblogging dataset W
EIBO . Weibo users usually mention different accounts in their tweets of different content. For example, sports fans usually men-tion their favorite players when they post messages to send their congratulations, comforts or best wishes; they mention their friends in life when they hear some interesting news like marriage, grad-uation, travelling and shopping discounts. Multi-faceted analysis can better model this kind of user behavior and predict who will be mentioned later for a Weibo user.
As mentioned before, here we predict author-affiliation-keyword behaviors on the academic research dataset MAS, and source (user)-target (user)-word (in tweet) behaviors on the social dataset W We compare our FEMA with EMA and three related methods DTA, HOSVD and CP that do not use flexible regularizers on the decompositions. Similarly with Fig. 3, Fig. 5 shows MAE and RMSE of the methods on the 10 experiments varying the per-cent of training data  X  t from 35% to 80% by 5%: Fig. 5a and 5b plot the results of MAS, while Fig. 5c and 5d plot the results of W
EIBO . With the size of the training data increasing, the models can learn more from it and thus the MAE and RMSE often de-crease by the size. FEMA often reaches the smallest values of MAE and RMSE, which shows that flexible regularizers can alle-viate the sparsity problem and thus can help in the prediction task. Furthermore, we show the numbers of MAE and RMSE in Tab. 3, when we set up the experiments with the largest piece of training part  X  t = 80% . FEMA decreases the RMSE of the best of the other methods by 17.1% on MAS and 15.4% on W EIBO .
 Table 3: Flexible FEMA has the smallest MAE and RMSE on 3W Prediction tasks when t = 10 and  X  t = 80% . The model is better if the MAE and RMSE are smaller.

Similarly with Fig. 4, we also plot the precision-recall curves to test their abilities of ranking the predicted probabilities of human behaviors. In Fig. 6, we show FEMA performs the best when we operate all the algorithms on both MAS data and W EIBO data. (a) Precision-recall on MAS (b) Precision-recall on W EIBO Figure 6: FEMA that uses flexible regularizers performs the best on 3W Prediction tasks when t = 10 and  X  t = 80% . The model is better if the precision and recall are higher.
FEMA uses the co-authorship information to constrain the pro-jection matrices on the dimension of author . The co-author net-work is a complementary graph to the authors X  affiliation network. It also has strong impacts in determining the topics of authors X  aca-demic research. Though the author-affiliation-keyword tensor is too sparse, learning the co-authorship matrix can better understand and predict the authors X  behaviors.

Similarly, on the social dataset W EIBO , FEMA uses the social network information to constrain the grouping of both the source users and target users . When a source user u looks for an appro-priate target user v from millions of accounts to mention in his/her tweets, u has often already connected to v and followed up v  X  X  mes-sages. Therefore, learning the social information can help predict the users X  behaviors of mentioning in tweet.
Here we first test the run time of FEMA by changing the fol-lowing three factors: (1) the number of objects in each dimension, i.e., the scale of tensors N = n (1) = n (2) = n (3) ; (2) the num-ber of groups in each dimension R = r (1) = r (2) = r (3) number of tensor increments T . For convenience, let the number of objects/groups be the same in all the dimensions. Second, we show that the loss of FEMA from FMA is quite small, while FEMA saves lots of time. (a) Time vs Num. objects N (b) Time vs Num. groups R (c) Time vs Num. increments T (d) The loss is small.
 Figure 7: FEMA saves lots of time and the loss is small: FEMA runs much faster than FMA .

Fig. 7a shows how much time FEMA and FMA cost by chang-ing the number of objects N in each dimension. We random sample N  X  N  X  N tensors from W EIBO data, for N = 100 ,..., 1000 , so that the density of tensors is stable. We set R = 50 and T = 10 as default. The run time of FEMA goes up much slower than that of FMA. When FMA takes 25 hours (more than one day) to process, FEMA needs only 51 minutes (less than one hour). Fig. 7b shows the time cost by changing the number of groups R from 2 to 100 in each dimension. We use the 1000  X  1000  X  1000 sample tensors and let T be 10. Though the run time of FEMA is proportional to R , it is still much smaller than that of FMA. Fig. 7c shows that the time cost is linear to the number of tensor increments T . The evo-lutionary analysis method FEMA updates the projection matrices with sparse increments, saving lots of time on decomposition.
In Fig. 7d, we check the loss of FEMA using 3W prediction tasks on the 1000  X  1000  X  1000 sampled tensors and find that FMA achieves smaller RMSE than FEMA but the loss is quite small. Since high-order terms in Eq. 8 are small, though FEMA omits the terms, the result is close to that of FMA.
In this section, we present interesting discovery from the behav-iors of both academic research and mentions in tweet. In the In-troduction, we have shown the temporal behavioral pattern of the research groups led by Prof. Jiawei Han in Fig. 1.
 Similarly, in Fig. 8, we give a showcase of our discovery from W
EIBO data, where the three layers are source users, target (men-tioned) users and words in tweets. Fig. 8a shows that the left yellow groups in the three layers are fans of 110-metre hurdle, hurdle runners including the Olympics 2004 champion Xiang Liu ( @liuxiang ), and words about the sport of hurdle and the runners. The right blue groups are fans of badminton, Chinese famous bad-minton players and the related words. In November 2011, Xiang X  X  fans mentioned Xiang and his friends like Dongpeng Shi ( @shi-dongpeng ), talking about their sweet memories of welcoming them back to  X  X hanghai X  in 2004. At the same time, the badminton team of China has just finished their matches in Hong Kong. Their most famous player Dan Lin ( @superdan ) and related words like  X  X ham-pion X  get high weights in their corresponding groups.

Fig. 8b shows the temporal behavioral patterns of these two group clusters in December 2011. Xiang Liu reported his sickness, and his fans sent their best wishes by mentioning to him. Dan Lin posted a message, saying it was the first anniversary of marriage with Xingfang Xie ( @xiexingfang ) who is also a badminton player. Therefore, we can see the words  X  X ove X ,  X  X arriage X  get higher weights than  X  X raining X  and  X  X xercise X . Note that first, the weights of @qqsports and @qqothersports increase in the two source user groups. Second, the weights of Dan and Xingfang increase in the two target user groups. We examine the data and find out the reason that @qqsports , @qqothersports and even some of Xiang X  X  fans congratulated to Dan and Xingfang for their good news.
In this section, we discuss how we set the parameters in our experiments: one is the number of groups R and the other is the weight of regularizers  X  .

On both datasets, we change the number of groups in each di-mension R from 2 to 100 and observe that the RMSE decreases and reaches the bottom when R is larger than 30 . In W EIBO spot the RMSE decreases much when R = 23 . As mentioned be-fore, FEMA updates the eigenvectors with the other eigenvectors multiplied by some weights. When R is smaller than 23, if the test-ing entries have objects in the 23-th cluster, the values of objects are 0 in all the eigenvectors. Thus, the entries will be always zero. We let the default number of groups R to be 50 for the trade-off between better accuracy (smaller RMSE) and faster computing. Figure 9: FEMA sets the default number of groups R in each dimension as 50 : RMSE decreases when R changes from 2 to 100 . In W EIBO , the RMSE decreases much when R = 23 . Figure 10: FEMA sets the default weight of regularizers  X  as 0 . 3 : the performance of our FEMA is insensitive to  X  .
Next, we change the weight of the regularizers in our FEMA  X  from 0 to 1 : when  X  = 0 , FEMA is actually EMA that does not use the flexible regularizers; when  X  = 1 , FEMA uses only the regularizers but none information from the tensors. On both MAS and W EIBO datasets, we observe that the RMSE first decreases and reaches the bottom when  X  is from 0 . 1 to 0 . 3 . The RMSE increases fast when  X  is larger than 0 . 3 . Note that our FEMA is insensitive to the weight  X  . For convenience, we set the default value of the weight as 0 . 3 . We demonstrate that it would be a better model to understand human behavior if it learns from both the sparse high-order tensors and dense flexible regularizers.
In this work, we present a novel tensor factorization based frame-work FEMA for temporal multi-faceted behavior prediction and behavioral pattern mining. The model uses flexible regularizers to alleviate the sparsity problem and gives approximation algorithms to fast process the increments with a theoretical guarantee. Ex-tensive experiments performed on real world datasets demonstrate that our framework is effective and efficient in behavior prediction tasks. The fast speed can support real-time applications of behavior prediction and pattern mining. [1] E. Acar, D. M. Dunlavy, T. G. Kolda, and M. M X rup. [2] W. Chen, W. Hsu, and M. L. Lee. Making recommendations [3] W. Chen, W. Hsu, and M. L. Lee. Modeling user X  X  [4] A. Cichocki and R. Zdunek. Regularized alternating least [5] P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. [6] P. Cui, F. Wang, S. Liu, M. Ou, S. Yang, and L. Sun. Who [7] J. Davis and M. Goadrich. The relationship between [8] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [9] D. M. Dunlavy, T. G. Kolda, and E. Acar. Temporal link [10] L. Hu, J. Cao, G. Xu, L. Cao, Z. Gu, and C. Zhu.
 [11] M. Jiang, P. Cui, R. Liu, Q. Yang, F. Wang, W. Zhu, and [12] M. Jiang, P. Cui, F. Wang, Q. Yang, W. Zhu, and S. Yang. [13] U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos. [14] G. Karypis. Evaluation of item-based top-n recommendation [15] T. G. Kolda. Orthogonal tensor decompositions. SIAM [16] T. G. Kolda and B. W. Bader. Tensor decompositions and [17] T. G. Kolda and J. Sun. Scalable tensor decompositions for [18] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [19] L. Liu, J. Tang, J. Han, M. Jiang, and S. Yang. Mining [20] K. Narang, S. Nagar, S. Mehta, L. Subramaniam, and [21] H. Rachlin. The value of temporal patterns in behavior. [22] K. Radinsky, K. M. Svore, S. T. Dumais, M. Shokouhi, [23] S. Rendle and L. Schmidt-Thieme. Pairwise interaction [24] S. B. Roy, M. De Cock, V. Mandava, S. Savanna, [25] G. W. Stewart and J.-g. Sun. Matrix perturbation theory. [26] J. Sun, D. Tao, S. Papadimitriou, P. S. Yu, and C. Faloutsos. [27] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. Cubesvd: a [28] Y. Sun, J. Tang, J. Han, C. Chen, and M. Gupta.
 [29] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. Tag [30] F. Wang, J. Sun, J. Hu, and S. Ebadollahi. imet: interactive [31] F. Wang, H. Tong, and C.-Y. Lin. Towards evolutionary [32] X. Wang, C. Zhai, and D. Roth. Understanding evolution of [33] L. Xiang, Q. Yuan, S. Zhao, L. Chen, X. Zhang, Q. Yang, and [34] Q. Yuan, G. Cong, Z. Ma, A. Sun, and N. M. Thalmann. [35] X. Zheng, H. Ding, H. Mamitsuka, and S. Zhu. Collaborative
