  X  This paper tackles the problem of mining subgoals of a given search goal from data. For example, when a searcher wants to travel to London, she may need to accomplish several subtasks such as  X  X ook flights, X   X  X ook a hotel, X   X  X ind good restaurants X  and  X  X ecide which sightseeing spots to visit. X  As another example, if a searcher wants to lose weight, there may exist several alternative solutions such as  X  X o physical exercise, X   X  X  ake diet pills, X  and  X  X ontrol calo-rie intake. X  In this paper, we refer to such subtasks or solutions as subgoals, and propose t o utilize s ponsored search data for finding subgoals of a given query by means of query clustering. Adver-tisements (ads) reflect advertisers X  tremendous efforts in trying to match a given query with implicit user needs. Moreover, ads are usually associated with a particular action or transaction. We there-fore hypothesized that they are useful for subgoal mining. To our knowledge, our work is the first to use sponsored search data for this purpose. Our experimental results show that sponsored search data is a good resource for obtaining related queries and for identi-fying subgoals via query clustering. In particular, our method that combines ad impressions from sponsored search data and query co-occurrences from session data outperforms a state-of-the-art query clustering method that relies on document clicks rather than ad im-pressions in terms of purity, NMI, Rand Index, F 1 -measure and subgoal recall.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X 
This research was conducted while the first and third authors were interns at Microsoft Research Asia.
 Algorithms, Experimentation User Intent, Sponsored Search, Query Clustering
The information needs of a web search engine user are some-times complex, and may span multiple queries or even multiple search sessions [21, 31]. When the user wants to travel to Lon-don, she may need to accomplish several subtasks such as  X  X ook flights, X   X  X ook a hotel, X   X  X ind good restaurants X  and  X  X ecide which sightseeing spots to visit, X  and issue multiple queries accordingly, possibly over a length of time. As another example, for a user who wants to lose weight, there may exist several alternative solutions such as  X  X o physical exercise, X   X  X  ake diet pills, X  and  X  X ontrol calo-rie intake. X  The user may not even be aware that these different solutions exist, so she will probably have to issue several queries to find out about them. In this paper, we refer to such subtasks or solutions as subgoals . Our precise definitions are as follows: Note that while a subgoal  X  X ook flights X  alone can only partially satisfy a search goal  X  X ravel (to) London, X  a subgoal  X  X o physical exercise X  may wholly satisfy  X  X ose weight. X 
In this paper, we tackle the problem of automatically mining sub-goals of a given search goal from data. To this end, we propose to utilize sponsored search data for finding subgoals of a given query by means of query clustering. Advertisements (ads) reflect adver-tisers X  tremendous efforts in trying to match a given query with im-plicit user needs. Moreover, ads are usually associated with a par-ticular action or transaction. We therefore hypothesized that they are useful for subgoal mining. To our knowledge, our work is the first to use sponsored search data for this purpose.

We further hypothesize that queries that represent a common subgoal are associated with similar ads, and employ a state-of-the-art query clustering algorithm [28] in order to mine subgoals Table 1: Example search goals and their subgoals mined by our proposed method. search goal: lose weight 1. fitness, gyms, health clubs, workout, ... 2. alli, diet pills, best weight loss pills, hcg drops, ... 3. diabetic recipes, diet recipes, healthy recipes, ... 4. denise austin, jillian michaels, kathy smith 5. high protein foods, protein, protein foods 6. calorie counter search goal: relieve stress 1. cheap lexapro, generic lexapro, lexapro side effects, 2. baseball stress balls, stress ball, stress relief toy, stress toys 3. body massage, massage therapist, massage therapy, 4. exercise heal, gaiam, holden, qigong 5. anxiety medications, herbs anxiety 6. zen garden search goal: travel London 1. cheap hotel london, london airport hotels, london hotel 2. airfare london, airline tickets london, cheap airfare london, 3. london 2012, london summer olympics, 4. car rental london, london car rental 5. london travel card, oyster card london from queries in the sponsored search data. Given a query that rep-resents a search goal, our method first collects its related queries from sponsored search data, and then clusters them based on ad im-pressions as well as within-session query co-occurrences. Table 1 shows a few examples of goal-subgoal relationships mined using our proposed method. It can be observed that for the search goal  X  X ose weight, X  query clusters that represent subgoals such as  X  X o physical exercise, X   X  X ake diet pills  X  and  X  X ontrol calorie intake X  are obtained. Also, for the search goal  X  X elieve stress, X  we can observe that query clusters that represent possible alternative solutions such as  X  X ake medicine, X   X  X uy stress relief toys X  and  X  X ave a massage X  are obtained. This hierarchy of search goals have many possible applications, including:
Figure 1 shows some example ads of a commercial search en-gine. In sponsored search, advertisers bid on various bid phrases so that their ads will be displayed in response to certain queries as shown in the figure. Today X  X  major commercial search engines have this kind of mechanism. According to our preliminary study with 10,000 head queries obtained over one recent week from a popular search engine, 68.3% of them had some ads displayed.

Our experimental results show that sponsored search data is a good resource for obtaining related queries and for identifying sub-goals via query clustering. In particular, combining ad impressions with within-session query co-occurrence information outperforms a state-of-the-art query clustering method that uses document clicks rather than ad impressions, in terms of purity, NMI, Rand Index, F -measure and subgoal recall (See Section 4.4).

The rest of this paper is organized as follows. Section 2 dis-cusses previous work related to our study. Section 3 describes our approach to mining subgoals using sponsored search data. Sec-tion 4 describes our experimental setup and Section 5 discusses the results. Finally, Section 6 concludes this paper. Sponsored search has recently been an area of active research. The main research topics of this area have been the improvement of ads retrieval performance [6, 16, 27] and clickthrough rate (CTR) estimation for the retrieved ads [14, 32]. For example, Broder et al. [6] proposed a technique that leverages related organic (i.e. non-sponsored) search results as the source of query expansion to over-come the problem of low precision and recall of ads retrieval.
Some researchers have analyzed user behaviors in the context of sponsored search [7, 10, 17, 19]. In Jansen X  X  experiments with e-commerce queries [17], the participants X  relevance ratings for or-ganic search results and those for ads were practically the same. Moreover, according to a CTR analysis by Danescu-Niculescu-Mizil et al. [10], users prefer ads that are dissimilar to organic re-sults for navigational queries, and those that are similar to organic results for informational queries. The findings shown in both liter-ature support the fact that ads play an important role in satisfying the users X  information need along with organic search results.
Next, we discuss prior art in query clustering, as our approach applies query clustering to sponsored search data in order to mine subgoals.

Query clustering is useful for understanding the underlying user intents and for improving query suggestions, and thus many query clustering techniques have been proposed. Most of existing work have relied on statistics derived from query session and clickthrough data: within-session query co-occurrences [13, 4, 29], similarity of clicked documents [2, 3, 30] and their combination [8, 28]. In this paper, we apply the query clustering algorithm proposed by Sadikov et al. [28] to sponsored search data, as it has been shown to achieve state-of-the-art performances through a large-scale user study. The main objective of this study is to show the usefulness of sponsored search data as a resource for mining subgoals: effective query clustering methods other than that by Sadikov et al. are also applicable to this problem.

We hypothesized that using the session data alone is not suffi-cient for the purpose of mining subgoals, because queries issued for a single search goal may span multiple sessions over a length of time and even span multiple users. Note that sessions are defined based on 10-30 minutes of inactivity in many studies, and also that a single session may contain queries for several different search goals.

In contrast to session and clickthrough data, sponsored search data may have the following advantages for the purpose of sub-goal mining: (a) we may be able to mine goal-subgoal relationships across sessions and across users; (b) since ads reflect the advertis-ers X  tremendous effort in trying to match queries with the under-lying user intents, we can leverage high-quality query-ad relation-ships that go beyond surface-level matching; (c) as ads are designed to make the user perform an action or transaction, they may directly reflect goals and subgoals.
Categorizing queries into predefined classes is an alternative to the aforementioned bottom-up clustering approaches for the pur-pose of understanding user intents. Many researchers [18, 22, 24] have tackled the problem of categorizing queries into navigational , informational and transactional [5]. Moreover, in the context of sponsored search, Dai et al. classified queries into commercial (e.g. buy or sell something) and non-commercial [9]. Guo and Agichtein [15] further refined the commercial category into re-search and purchase . However, these top-down approaches are not appropriate for our purpose, as we need to mine a variety of un-known subgoals for a given search goal.
The work that is most closely related to our present study is that by Jones and Klinkner [21], who introduced the concepts of search mission and search goal . According to their definitions, a search goal is an atomic information need represented by one or more queries, while a search mission is a set of related infor-mation needs represented by one or more search goals. Although our goal-subgoal relationships are also defined hierarchically just like their mission-goal relationships, ours are different from theirs: while their aim is to analyze individual search tasks of a searcher and thus their mission-goal relationships represent the hierarchical needs for the searcher, our aim is t o mine more general hierarchical needs of searchers, rather than limiting ourselves to a session of a single searcher. In addition, although they proposed a model to au-tomatically determine whether a given pair of queries in the same session shares the same search mission or goal, the candidate query pairs are a given. In contrast, given a query, our approach mines its subgoals from related queries in the sponsored search data.
More recently, Aiello et al. have proposed a clustering algorithm that clusters search missions into underlying topics [1]. Their aim is to find broad topical user profiles of search engine users from query logs, rather than finding mission-goal relationships. For example, they aim at finding the general travel intent from related missions such as  X  X ind information on travel to London. X 
In response to a query, commercial search engines often present sponsored search results in addition to organic search results. This happens when the query matches with the advertisers X  bid phrases,
Table 2: Structure of sponsored search data in this study. and the associated ads are ranked according to their bids and esti-mated CTR [10]. The contents of each ad, devised by the advertis-ers, are typically like the ones shown in Figure 1. The sponsored search data used in our work leverages the above mechanism: some examples are shown in Table 2. Each data record is a triple, con-sisting of the query issued by a search engine user, the ad, and its impression count, which represents the number of times the ad was displayed in response to the query.

Figure 2 shows the overview of our subgoal mining method. In this paper, we refer to the query that is input to our method as search query , to distinguish it from its related queries that are to be clustered by our method. Given the search query q 0 , our first step is to collect a set of related queries Q = { q 1 ,...,q given n by utilizing t he ad contents of the aforementioned spon-sored search data. Then, our second step outputs a set of query clusters C = { C 1 ,...,C k } for a given k . This step uses both sponsored search data and session data and applies the clustering algorithm by Sadikov et al. [28].

Sections 3.2 and 3.3 describe the above two steps in detail.
Given the search query q 0 and the required number of related queries n , we first collect a set of related queries Q = using the ad contents of the sponsored search data, as described below.

Let A be the entire set of ads archived in the sponsored search data, and let A q 0 (  X  X  ) be the set of all ads whose ad contents contain q 0 . For example, note that the ad on the left in Figure 1 contains  X  X ose weight X  which could be a search query. For each query q in the sponsored search data such that w imp ( q, a some a  X  X  q 0 ,where w imp ( q, a ) denotes the impression count (See Table 2), we compute its total impression across the relevant ads: impression and take the top n . That is, we obtain n queries related to q 0 , whose associated ads have high impressions.
Having thus obtained the set of related queries Q , we cluster the queries into k clusters, using the algorithm proposed by Sadikov et al. [28]. The original purpose of their clustering algorithm was to identify intents that are common across different query strings, and it relied on two assumptions: (1) If two queries share the same intent, they are associated with the same clicked documents; and (2) If two queries share the same intent, they co-occur within the same session. For the purpose of mining subgoals from sponsored search data, we adapt the above assumptions as follows: Thus, our departures from the original method by Sadikov et al. are: (a) We construct a query-ad graph instead of a query-document graph; and (b) We use ad impressions for computing the query-ad transition probability instead of document clicks .Below,we provide more details of the clustering algorithm.

Given the set of related queries Q for a search query, we con-struct a query-ad graph as follows. First, we obtain all ads from the sponsored search data that match a query from Q : let the set of these ads be A . Then we construct a query-ad graph G =( where V = Q  X  A denotes the set of nodes in G ,and E denotes the set of edges in G .In G , the edge between any two queries exists iff they co-occur in the same session; and the edge between any query-ad pair exists iff there is an impression record for that pair in the sponsored search data.

Once we have constructed the query-ad graph G , we prepare a transition matrix P , which represents the transition probabilities among the nodes in G . P contains three types of transition proba-bilities: query-to-query, query-to-ad, and ad-to-ad. Figure 3 shows the transition model. As this figure shows, the model contains a parameter called , which determines the probability of transition from a query node to ad nodes. query-to-query transition: This probability is determined based on the probability that a pair of queries co-occurs in the same ses-sion. The transition probability from query q i to query q as: where w cooc ( q i ,q j ) denotes the number of sessions that co-occurred, and R ( q i ) is the set of all queries that co-occurred with q i within the same session. In practice, there are transitions from q i to queries that are outside the set of related queries therefore introduce a special node f in G to collectively represent such queries, and we define the transition probability from as follows: That is, all transitions from each query q i to queries outside are aggregated to node f . The node f has only the self-transition p ( f | f ) with probability 1. query-to-ad transition: This probability is defined as: ad-to-ad transition: We defi ne p ( a j | a i ) to be 0 if i = j .
 Figure 3: Example transitions on a query-ad graph. A random walk is applied on it for clustering queries.

After preparing the transition probability matrix P , we perform a random walk on the query-ad graph G .Let P l be the transition probability matrix after an l -step random walk. The row in q can be interpreted as visit probabilitie s over the nodes in an l -step random walk that started at q i .

As we can see from Figure 3, there is no ad-to-query transition, and there are only self-transitions among ads. Hence, for each the visit probabilities over Q approach zero while those over converge as we iterate the random walk process. As Sadikov et al. notes, 3-5 iterations are enough in practice [28]. The transition probability matrix obtained after the convergence is referred to as P .

Finally, we cluster queries in Q using the transition probabilities from P . For each query q i  X  Q , its query vector is represented by a transition probability vector: where p ( a j | q i ) denotes the transition probability from P , which can be interpreted as the probability that ad a j displayed to the searcher who starts with query q i . When two query vectors q i and q j are similar, this implies that similar ads are likely to be displayed in response to two different queries.

Following Sadikov et al. [28], we use a complete-linkage clus-tering method with cosine similarity of the above query vectors to obtain a set of k query clusters C = { C 1 ,...,C k } .
In our experiments, we used three types of logs, namely, spon-sored search data, clickthrough data and session data. As was shown in Table 2, a sponsored search data record is a query-ad-impression triplet. Similarly, a clickthrough data record is a triplet composed of a query, clicked document and its click count. The latter is used for our implementation of the document-click-based method of Sadikov et al. [28], which we use as a baseline in our experiments. As for session data, each record is a triplet composed of a session ID, a query and a timestamp, and a session is defined based on a 30-minute inactivity. All of these logs were sampled from a popular search engine and span exactly the same period from November 2011.

The raw sponsored search data contained some query-ad pairs for which queries and bid phrases were not exact matches (e.g.  X  X ealth X  vs.  X  X ealth clubs X ), thus we filtered out such pairs. This is because the query-ad pairs obtained through non-exact matches de-pend on the particular ads retrieval algorithm of the search engine, and we wanted to obtain results that are search engine independent. The statistics of the data thus obtained are shown in Table 3.
To examine the effectiveness of our method that relies on spon-sored search data, we implemented the following four methods. (a) AdImp: This method clusters queries to mine subgoals as we described in Section 3. Given a search query, it obtains related queries from the sponsored search data, and then clusters them by combining ad impressions from the sponsored search data and query co-occurrences from the session data. (b) AdImp (no cooc): This is the same as AdImp, except that query-to-query transitions derived from the session data are not used for clustering. That is, only query-to-ad transitions derived from the sponsored search data are utilized. (c) DocClick: This is our implementation of the query clustering algorithm proposed by Sadikov et al. [28], which we treat as a base-line. Thus, given a search query, it obtains related queries from the session data, and then clusters them by combining document clicks from the clickthrough data and query co-occurrences from the ses-sion data. While Sadikov et al. originally obtained n most frequent queries that follow q 0 within the same session, we obtained frequent queries that follow or is followed by q 0 within the same session, as the order of issuing queries is not important for our pur-pose. (d) DocClick (no cooc): This is the same as DocClick, except that query-to-query transitions derived from the session data are not used for clustering. That is, only query-to-document transitions derived from the clickthrough data are utilized.

Following Sadikov et al. , we set the number of related queries to n =80 and the transition probability from query to ad or document to =0 . 6 . Also, to construct the query-ad (or query-document) graph G for a given search query, we used only the top 15 most frequently displayed ads (or clicked documents) for each related query.
In order to evaluate the effectiveness of our subgoal mining meth-ods based on query clustering, we created our own test collections as described below.
Our first step was to select search queries that are to be used as input to our subgoal mining method. Thus, the input queries needed to be reasonably complex and to represent a search goal that may be associated with multiple subgoa ls. To select such input queries, we chose five domains (Business, Health, Recreation, Society and Sports) from the taxonomy used in the search engine, and extracted queries with high impressions from each domain 1 . From the high impression queries, we selected queries that contains at least one verb, or a noun that is derivationally related to a verb according to WordNet 2 . This is because we want to handle queries such as  X  X eight loss X  as well as explicitly verb-oriented queries like  X  X ose weight. X  We thus obtained 892 candidate queries.

Three assessors independently annotated the above candidates to select input queries that are appropriate for subgoal mining. For each query, each assessor first judged whether it represents a search goal (i.e. a particular action that needs to be accomplished) either explicitly or implicitly; if it was judged as a search goal query, she wrote down up to two example subgoals in a verb plus noun phrase format (e.g.  X  X o physical exercise X  for a candidate query  X  X ose weight X ).

Through the annotation task, 59 queries (6.6%) were annotated with two subgoals by all three assessors, and 349 queries (39.1%) were annotated with two subgoals by at least one assessor. Ta-ble 4 shows the inter-assessor agreement statistics on the number of annotated subgoals, where  X #&lt;number&gt; X  means the number of subgoals annotated by each assessor. The Fleiss X  kappa [12] for this data set is 0.343, which is a moderate agreement. This is not altogether surprising, because given the same search queries, some people can think of good subgoals (i.e. subtasks or solutions), while others cannot. We are tackling the problem of subgoal mining pre-cisely because we want to help the user by presenting possible sub-goals that she may not be aware of.

From the annotated queries, we first selected those that were an-notated with at least three unique subgoals, regardless of which as-sessor contributed them. Then, to increase the number of queries, we added some annotated queries for which at least one assessor identified two subgoals. Finally, we removed some queries from the set in order to avoid including very similar search goals. Through this selection process, we obtained a total of 125 search queries for our test collections, 25 queries for each of the five domains. Table 5 shows some example search queries. Note that although we selected these queries from the sponsored search data, these are also part of the session data, as these data sets were obtained from thesameperiod.
In our present study, we view the problem of subgoal mining as a query clustering task. Thus, given a set of queries that are related to the search query, the problem is to cluster them appropriately, so that each query cluster represents an appropriate subgoal of the original search query. In order to evaluate this task, we need to build some ground truth data. We thus hired the same three asses-sors to manually cluster related queries and construct ground truth subgoals.
Donato et al. reported that information needs in domains such as travel, health and education tend to be complex [11].
WordNet, http://wordnet.princeton.edu/
For each search query, we prepared a set of related queries by pooling the related queries obtained by AdImp and DocClick. Re-call that the related queries of AdImp come from the sponsored search data, while those of DocClick come from the session data. As we want to compare the two approaches fairly, we included the related queries from both sides in order to manually identify possi-ble subgoals.

We developed a simple GUI tool to facilitate the manual query clustering process, so that assessors could form clusters by drag-ging and dropping queries on the screen. Because not all of the pooled related queries represent a subgoal of the given search query, we prepared two special clusters called Not Relevant and Not Sub-goal : the former was for queries that were topically non-relevant to the search goals, and the latter was for queries that are topically relevant but do not represent a subgoal. For example, for search query  X  X ose weight, X  a related query  X  X ose weight fast X  should be put into the Not Subgoal cluster, as the latter is a specialization of the original query and does not represent a subgoal.

Using the GUI tool, the assessors manually clustered the dis-played queries, and were also asked to provide a subgoal label in a  X  X erb plus noun phrase X  format. They were told that the target number of clusters was around 10, but were allowed to form fewer or more clusters if necessary. The tool also had a feature for assist-ing the assessor if she was unfamiliar with the related queries being displayed: by a right click on a related query, a web search result was shown to the user in a separate browser window.

Manual clustering is a tedious process: each assessor typically spent 30-60 minutes to complete the ground truth construction for one search query. In order to save the assessment cost while trying to maintain a reliable experimental environment, one assessor was assigned to 100 search queries, and the other two assessors were assigned to independently handle the remaining 25 search queries. Thus, we obtained two separate search query sets: the first one is relatively large but its ground truth data is constructed by only one assessor; the second one is relatively small but it has two sets of ground truth data.
As we mentioned above, we formed two separate sets of queries, one containing 100 and the other containing 25, and assigned one assessor to the former and two asse ssors to the latter. As a result of manual clustering, we found that two search queries from each query set did not contain enough relevant related queries, and there-fore removed them. Thus, we obtained three subgoal mining test collections in the end:  X  X ollection A X  containing 98 search queries, as well as  X  X ollection B-1 X  and  X  X ollection B-2 X  sharing the same 23 search queries but annotated independently by two assessors. Using these three different test collections enables us to focus on general trends of the experimental outcome.

Table 6 shows the statistics of each test collection. The  X  X oth X  columns show statistics on the pooled related queries;  X  X ds Only X  columns show statistics on the queries obtained from the spon-sored search data (by the AdImp method); and  X  X ession Only X  columns shows those on the queries obtained from the session data (by the DocClick method). The  X  X elated query overlap per query X  shows the average of the Jaccard coefficient between the two sets of queries obtained from the sponsored search data and the session data. The  X  X ubgoal overlap per query X  shows the Jaccard coeffi-cient between two subgoal sets that contain the queries obtained from the sponsored search data and those that contain the queries from the session data. In total, we obtained 1,375 ground truth subgoals (i.e. clusters) with 20,880 clustered related queries.
The  X #Not Relevant X  row of Table 6 indicates that many queries from the session data (obtained by the DocClick method) were clas-sified as Not Relevant, i.e., off-topic. This suggests that sponsored search data may be a better resource than session data for obtaining topically related queries. On the other hand, the  X #Not Subgoals X  row shows that both sponsored search data and session data yield some related queries that are topically relevant but do not repre-sent a subtopic of the input search query: recall the aforementioned  X  X ose weight fast X  example. In addition, through the analysis of  X #Not Subgoals X  queries obtained from the sponsored search data, we found some queries may be interpreted as supergoals of the given search query. For example, for the search query  X  X ickbox-ing, X  related queries  X  X ose weight X  and  X  X elf defense X  were ob-tained, due to the advertisers X  effort in promoting kick-boxing lessons. That is, it is the search query  X  X ickboxing X  that can be interpreted as a subgoal of  X  X ose weight X  or  X  X elf defense. X 
The  X  X elated query overlap per query X  row shows that the over-lap between the queries obtained based on the ad impressions in the sponsored search data and those obtained based on query co-occurrences in the session data is small. In contrast, the  X  X ubgoal overlap per query X  row shows that the overlap of identified sub-goals between these two data sources is dramatically higher. This suggests that we may be able to obtain searchers X  intents (often ex-plicitly represented in session data) by leveraging the advertisers X  efforts embedded in sponsored search data, even though the two data sources contain seemingly different queries.

As Collections B-1 and B-2 share the same input query set, we can measure inter-assessor agreement for the manually constructed ground truth clusters. For this, we compute the well-known measure, which measures how any given pair of items to be clus-tered is correctly grouped. Let TP denote the set of query pairs from the same ground truth cluster that were correctly grouped to-gether, FN denote the set for those that were incorrectly separated, and FP denote the set of query pairs from two different ground truth clusters but were incorrectly grouped together. Then F 1 from B-1 are treated as the ground truth, the F 1 of the subgoals from B-2 is 0.55. Conversely, when the subgoals from B-2 are treated as the ground truth, the F 1 of the subgoals from B-1 is 0.69. Thus the assessor agreement between B-1 and B-2 is reasonably high. In addition to F 1 , we also computed purity, Normalized Mutual Information (NMI) and Rand Index (RI) [25] to evaluate the qual-ity of query clusters that represent subgoals. Purity, which we use as our primary metric along with F 1 , measures the homogeneity of each cluster. Given n queries, a set of k clusters C = { C obtained by clustering the queries, and a set of l ground truth clus-ters that represent subgoals S = { S 1 ,...S l } , purity is given by: 1 n X  X  X ominant X  subgoal in C i .

By definition, high purity can easily be achieved by choosing a high k (if every cluster contains exactly one query, then the purity is 1), while high F 1 can be achieved if k is close to the number of ground truth clusters. We thus plot F 1 against purity by varying k , to explore methods that achieve overall high accuracy and high within-cluster homogeneity.

In addition to the above well-known metrics, we also compute subgoal recall of a given cluster set C as:
In contrast to purity, this metric can penalize the case where mul-tiple clusters correspond to the same dominant subgoal.
Figure 4 shows the F 1 / purity graphs for AdImp, AdImp (no cooc), DocClick and DocClick (no cooc), obtained by varying the number of clusters k from 1 to 40 . Here, all of the related queries that were pooled (See Table 6) were included for evaluation. As for the Not Relevant and Not Subgoal queries in the ground truth data, each of them were treated as an independent cluster on its own when computing the metrics, as we are not interested in mak-ing the system form clusters out of these queries. From the figure, it can be observed that AdImp and AdImp (no cooc) achieve higher purity and F 1 compared to DocClick and DocClick (no cooc) for all three test collections. While our method uses an existing query clustering algorithm, it is clear from the results that our novel use of the ads data for the purpose of subgoal mining is highly ef-Figure 5: Query clusters produced by AdImp and DocClick, for the top 20 related queries of the search query  X  X edding. X  fective. Moreover, it can be observed that utilizing the query co-occurrences from the session data for clustering is effective in both AdImp and DocClick.
 As an example, Figure 5 contrasts some clusters obtained by AdImp and those obtained by DocClick for the search query  X  X ed-ding. X  For each method, 80 related queries were clustered with the target number of clusters k =20 : only the top 20 high impres-sion queries for AdImp and top 20 high co-occurrence queries for DocClick are shown in the figure. First, it can be observed that DocClick tends to produce more queries that (arguably) do not di-rectly represent subgoals ( X  X old, X   X  X oney, X  etc.) when compared to AdImp. Second, the clusters obtained by DocClick appear some-what less homogeneous: for example,  X  X im kardashian wedding X  ( X  X im kardashian X  is the name of a celebrity) and  X  X edding ring X  are in the same cluster. This happened because the query  X  X im kar-dashian wedding X  shared the news article about her wedding with the query  X  X edding rings X  in the clickthrough data. The query clus-ters for AdImp look somewhat more organized, thanks to the im-pressions of ads from various wedding services.
While the results in Figure 4 appear to suggest that AdImp is much more effective than DocClick, it should be noted that there are at least two factors that may have contributed to the difference. The first is the quality of the related queries to be clustered: re-call that while AdImp obtains related queries from the sponsored search data, DocClick (i.e. method by Sadikov et al. ) obtains re-lated queries from the session data, and the latter contains a lot of Not Relevant queries, as we have shown in Table 6. The second is the evidence we use for clustering: AdImp uses the ad impres-sions from the sponsored search data (with query co-occurrences from the session data), while DocClick uses the document clicks from the clickthrough data. Figure 4 does not show which of these factors are contributing by how much. Table 7: Comparison of methods with different numbers of clusters k on Collection A, when only relevant related queries are clustered. Results that improved significantly (paired test) from DocClick are marked with  X * X  ( p&lt; 0 . 05 ) and  X ** X  ( p&lt; 0 . 01 ).
 k =5 k =10
To separate the above two factors, Figure 6 shows the F 1 graphs for the four methods when only relevant related queries are clustered ,for k between 1 and 20 . For each search query, let and rel session be the number of relevant queries (i.e. queries clas-sified as neither Not Relevant nor Not Subgoal) obtained from the sponsored search data and the session data, respectively. We take min( rel ad , rel session ) relevant queries from both data sets, so that the contribution to the pool is equal in size for every search query. It can be observed that, even if we remove the effect of noise in the related queries to be clustered, AdImp generally achieves both high F 1 and high purity values.

Table 7 shows several metrics in the same  X  X elevant queries only X  setting for Collection A, when the number of required clusters is 5 and 10 . Significance differences with DocClick according to the paired t -test are indicated by asterisks, and the highest metric value among the four methods are indicated in bold. It can be ob-served, for example, that AdImp is significantly more effective than DocClick in terms of all metrics (purity, NMI, RI, F 1 and subgoal recall) when we require 10 clusters. Although not shown in this pa-per due to lack of space, the results for the other two test collections are generally similar.

These results suggest that the clustering step of AdImp, which relies on ad impressions, has advantages over that of DocClick, which relies on document clicks.
The previous experiments showed that AdImp significantly out-performs DocClick, even if we remove the effect of noise in the re-lated queries obtained from DocClick. Recall that while DocClick obtains related queries from the session data and then clusters them primarily based on document clicks from the clickthrough data, AdImp obtains related queries from the sponsored search data and then clusters them primarily based on ad impressions from the same sponsored search data . Thus, it is possible that the reason why AdImp works so well is that queries from sponsored search data can be clustered accurately by leveraging the ad impressions from the same data . In order to verify this hypothesis, we conducted two additional experiments: the first clustered only relevant queries that were obtained from the sponsored search data; the second clustered only relevant queries that were obtained from the session data.
Figure 7 shows the F 1 / purity graphs for these two additional experiments on Collection A, for k between 1 and 20 . First, Fig-ure 7(a) shows that, for clustering related queries obtained from the sponsored search data, AdImp is clearly more effective than DocClick. Moreover, as the difference between AdImp and AdImp (no cooc) and that between DocClick and DocClick (no cooc) show, using the query co-occurrences from the session data at the cluster-ing stage helps. On the other hand, Figure 7(b) shows that, AdImp is less effective than DocClick for clustering related queries ob-tained from the session data. This does not contradict with the above hypothesis (queries from sponsored search data can be clus-tered accurately by leveraging the ad impressions from the same data). The main reason why AdImp is less effective for queries from the session data is probably due to low coverage of spon-sored search data: some queries have few or no associated ads. However, from the Figure 7 (b), while DocClick does not seem to benefit much from the use of query co-occurrences (compare with DocClick (no cooc)), the same statistics boost the performance of AdImp. This implies that session data can compensate for the low coverage of sponsored search data.
As we explained in Section 3.3, we calculated the query-to-ad transition probability based on ad impressions . One alternative way of calculating it is the use of ad clicks , which means the number of times ads were clicked by the sear ch engine users. To compare the effect of ad impressions with ad clicks, we implemented two addi-tional methods, namely,  X  X dClick X  and  X  X dClick (no cooc), X  both of which cluster the related queries as AdClick and AdClick (no cooc) do, except that both methods use the top 15 most clicked ads for constructing a query-ad graph, and calculate query-to-ad transi-tion probability based on the number of times the ads were clicked. Figure 8 shows the results of these four methods, by varying the number of clusters k from 1 to 20 in the same  X  X elevant query X  setting in Section 5.2 for Collection A. From the figure, we can see that AdImp consistently outperforms AdClick, and AdImp (no cooc) also outperforms AdClick (no cooc) in terms of purity and F . This result is probably due to the sparseness of ad clicks. It is well known that CTR of ads are much less than those of docu-ments (i.e. organic search results). Thus, it is hard to obtain reliable number of clicks for clustering related queries from the sponsored search data. Note, however, that even AdClick generally achieves both higher F 1 and purity than DocClick shown in Figure 6(a). Figure 7: Comparison of ad impressions and document clicks under the same set of relevant queries on Collection A, for between 1 and 20 . Figure 8: Comparison with ad impressions and clicks. The re-sults were produced by AdImp on Collection A, for k between 1 and 20 , when only relevant related queries are clustered.
Our experiments showed that while AdImp generally outper-forms DocClick, it is less effective for clustering related queries from the session data (See Section 5.3). As we discussed with Ta-ble 6, while the overlap of subgoals between queries from the spon-sored search data and those from the session data is high, there are also subgoals that were obtained from only one of the data sources. On average across our three collections, 1.16 and 2.18 subgoals per search query were obtained from pure session data and from pure sponsored search data, respectively. In this subsection, we discuss some actual subgoals for these different cases.
 Table 8 provides some examples from our three test collections. Table 8(a) shows subgoals that were formed only from the session data queries. As queries such as  X  X auses of back pain X  and  X  X oving to do list X  did not trigger any ads, the subgoals learn about cause and make a TODO list could not be obtained from the sponsored search data. Even if we increase the amount of sponsored search data, it is unlikely that our method will identify such subgoals as they are not strongly related to advertisement.

Table 8(b) shows a few example search queries for which both sponsored search data and session data were successful. Thus, both ad impressions and document clicks can successfully group queries like  X  X oving truck X  and  X  X ruck rental X  together.
 Table 8: Example of subgoals obtained: Each subgoal is a clus-ter of related queries with a manually assigned subgoal label.
Finally, Table 8(c) shows a few subgoals that were obtained only from the sponsored search data. Note that a search engine user who is looking for ways to  X  X elieve stress X  or to  X  X uit smoking X  may not even be aware that solutions such as visit a zen garden or have acupuncture treatment exist. Thus, it is the  X  X isdom of advertisers X  that helps our method to propose such solutions to the searcher. It would be very difficult to find these  X  X nexpected X  solutions in the session data: if the searchers are unaware of these solutions, they are highly unlikely to issue queries about them.
In this study, we defined the problem of mining subgoals of a given search goal from data by means of query clustering, and pro-posed to utiliz e sponsored search data fo r this purpose. Our method (AdImp) first obtains related queries from the sponsored search data, and then clusters them based on ad impressions from the same data as well as query co-occurrences from session data. This was compared with a similar state-of-the-art method [28] (DocClick) that first obtains related queries from the session data, and then clusters them based on document clicks from clickthrough data as well as query co-occurrences from session data. Our experimental results using three in-house test collections showed that (1) related queries obtained from sponsored search data are more relevant than those obtained from session data; and (2) AdImp significantly out-performs DocClick in terms of purity, NMI, Rand Index, F subgoal recall.

There are several limitations to the present study. First, this study did not address the problem of generating a label for each query cluster (e.g. [26]). While our current representation of a subgoal is in the form of a cluster of several queries, it would be useful for the searcher if we could also provide an explicit label for each sub-goal. We plan to tackle this problem by leveraging the ad contents: for example, the phrase  X  X et some exercise X  shown in Figure 1 is probably a good candidate as a subgoal label given the query  X  X ose weight. X  Next, although we mentioned in Section 1 that around 68% of our head queries had some ads displayed with the organic search results, the applicability of our method is probably much lower than this number suggests. As we described in Section 4.3.1, only 39.1% of our candidate verb-oriented queries were annotated with multiple subgoals by at least one assessor. On a similar note, Jones and Klinkner [21], who defined the hierarchy of search mis-sions and goals (See Section 2.4), also reported that only 20% of user queries from their Yahoo! query log were associated with hi-erarchically organized needs. Moreover, our method does not work if the query is not included in the sponsored search data, which is generally smaller compared to clickthrough and session data (See Table 3). However, although the fraction of queries that our method can handle may not be large, we argue that these queries represent a very important query segment, for which current search engines require breakthroughs.

As future work, we would like to refine our subgoal mining al-gorithm. As this study focused on verifying the usefulness of spon-sored search data as a resource, we applied an existing query clus-tering algorithm. Thus, we did not have any explicit mechanisms, for example, for classifying subgoals of a given search query, su-pergoals of the query and queries that share the same goal with the query. Moreover, as we mentioned in Section 1, while some types of subgoal wholly accomplish the original search goal (e.g.  X  X o physical exercise X  may be a good solution to  X  X ose weight X ), oth-ers only partially accomplish the original search goal (e.g.  X  X ook flights X  may be one step towards accomplishing the  X  X ravel Lon-don X  search goal, but other subtasks such as  X  X ook a hotel X  are also required). Furthermore, there may be temporal dependencies among these subtasks/subgoals: some subgoals need to be satisfied before others. Mining these different types of subgoal would be useful for improving search effectiveness and experience. [1] L. M. Aiello, D. Donato, U. Ozertem, and F. Menczer. [2] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query [3] D. Beeferman and A. Berger. Agglomerative clustering of a [4] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and [5] A. Broder. A taxonomy of web search. ACM SIGIR Forum , [6] A. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich, [7] G. Buscher, S. T. Dumais, and E. Cutrell. The good, the bad, [8] H.Cao,D.Jiang,J.Pei,Q.He,Z.Liao,E.Chen,andH.Li.
 [9] H.K.Dai,L.Zhao,Z.Nie,J.-R.Wen,L.Wang,andY.Li.
 [10] C. Danescu-Niculescu-Mizil, A. Z. Broder, E. Gabrilovich, [11] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you want to [12] J. Fleiss and J. Cohen. The equivalence of weighted kappa [13] B. M. Fonseca, P. Golgher, B. P X ssas, B. Ribeiro-Neto, and [14] T. Graepel, J. Candela, T . Borchert, and R. Herbrich. [15] Q. Guo and E. Agichtein. Ready to buy or just browsing?: [16] D. Hillard, S. Schroedl, E. Manavoglu, H. Raghavan, and [17] B. J. Jansen. The comparative effectiveness of sponsored and [18] B. J. Jansen, D. Booth, and A. Spink. Determining the [19] B. J. Jansen and M. Resnick. Examining searcher [20] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [21] R. Jones and K. L. Klinkner. Beyond the session timeout: [22] I. Kang and G. Kim. Query type classification for web [23] E. Kanoulas, B. Carterette, P. D. Clough, and M. Sanderson. [24] Y. Liu, X. Ni, J.-T. Sun, and Z. Chen. Unsupervised [25] C. D. Manning, P. Raghavan, and H. Sch X tze. Introduction to [26] M. P. Kato, T. Sakai, and K. Tanaka. Structured query [27] F. Radlinski, A. Broder, P. Ciccolo, E. Gabrilovich, [28] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy.
 [29] X. Wang, D. Chakrabarti, and K. Punera. Mining broad [30] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user queries [31] R. White and R. Roth. Exploratory search: Beyond the [32] W. Xu, E. Manavoglu, and E. Cantu-Paz. Temporal click
