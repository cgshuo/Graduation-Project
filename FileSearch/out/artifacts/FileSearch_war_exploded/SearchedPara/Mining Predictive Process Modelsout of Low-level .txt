 Process Mining techniques aim at extracting useful information from historical process logs, possibly in the form of descriptive or predictive process models, which can support the analysis, design, and improvement of business processes. An emerging research stream [1,9,4] concerns the induction of models for pre-dicting a given performance measure for new cases at run time.

Originally focused on workflow system s, Process Mining research has been moving towards less structured processe s, possibly featuring a wide variety of behaviors and many low-level tasks. This calls for enhancing classical approaches with the capability to capture diverse execution scenarios (a.k.a.  X  X rocess vari-ants X ), and to map log events to high-level activity concepts [3], in order to prevent the construction of useless models giving a cumbersome and undergen-eralized view of process behavior.

The need of providing expressive process views is also witnessed by the prolif-eration of works on activity abstraction [12,11,7] and on log clustering [14,9,4], as well as by recent efforts to model different process variants and their link to environmental factors [13]. Some works also tried to combine trace clustering and activity abstraction, in order to build expressive process models, with different process variants represented via high-level tasks (or sub-processes, at different levels of aggregation/abstraction) [10,8,6].

Unfortunately, most of these methods assume that the log events are mapped to well defined process tasks, which is not true in many real-life collaborative work environments (such as issue tracking systems, or transaction systems), where lots of data are yet stored (in the form of event/case attributes) that might well help model/pre dict system behaviour.
 Example 1. As an example scenario, consider the simplified version of the ITIL X  X   X  X ncident Management X  process (inspired to [3]) at the top of Figure 1. Assume that none of the high-level process tasks appear in the execution traces (like those at the bottom of the figure), where each even t just stores low-level information as atriple op,org,role . Specifically, attribute op , which encodes rather generically the operation performed, can take one of the following values: A (allocating the incident to someone), C (classifying the incident), D (describing the incident or its solution), CI (associating the incident with a configuration item) and CL (closing the incident). Attribute org tells which organizational entity the executor belongs to: 1L (early intervention) and 2L (second level team). Finally, the stored executor role can be either O ( operator )or T (technician). In the log of lowly structured processes, like those above, none of the event attributes fully capture the semantics of all performed actions. In such a case, abstracting each event as a fixed combination of multiple attributes is likely to yield overfitting models, seeing as a very high number of distinct state/activity representations may be produced, most of which just cover a bunch of log events. Conversely, simple event abstraction strategies (consisting in replacing each log event with the associated task or executor or them both) tend to be adopted in the area of predictive process mining, where the combination of model induction and automated activity abstraction has not been investigated so far.
 Contribution To overcome the above limitations, we state the prediction of pro-cess performances as the search for an e nhanced kind of performance model, consisting of three components: (a) an event classification function, for abstract-ing each low-level event into an event cl ass, regarded as a distinct activity type (rather than as a subprocess or macro-activity, as in [12,11,7]); (b) a trace clas-sification function, for discriminating among different process variants, based on case data; and (c) a collection of state-aware pr edictors, each associated with one process variant. The event classification function is meant to partition all (low-level) log events into clusters, like those in the middle layer of Figure 1, each associated with a classification rule (over event attributes). Analogous rules (over case attributes) can be found to partition a ll process traces into execution classes. Such a model supports the forecast of the analyzed performance measure on an ongoing process case  X  via three logical steps: (i) each  X   X  X  event is abstracted by the event classification function; (ii) the obtained abstract trace is assigned to a process variant through the trace classification function and (iii) the predictor of the selected variant is eventually used to predict the performance of  X  .
A discovery algorithm is presented in the paper for inducing both classification functions by way of a co-clustering scheme (extending the logics-based framework of predictive clustering [5]), prior to building a local predictor for each trace cluster. Besides enjoying compelling prediction accuracy (w.r.t. current methods, combined with usual log abstractions), in real-life application cases the approach managed to recognize relevant activity patterns at the right abstraction level. Moreover, the descriptive nature of the discovered classification rules (expressed in terms of event/trace data) can help comprehend process behaviors, and how its performances depend on both cont ext factors and activity patterns. Organization. The rest of the paper is structured as follows. After introducing some preliminary concepts in Section 2, we present, in Section 3, our solution approach and a prototype system implementing it. An empirical analysis on two real-life case studies is then disc ussed in Section 4, before drawing some concluding remarks and future work directions. Let us denote by E and T the universes of all possible events and (both fully un-folded and partial) traces, respectively, for the process under analysis  X  as usual, we assume that a trace is recorded for each process instance (a.k.a  X  X ase X ), en-coding the sequence of events happened during its enactment. For our purposes, an event e  X  E is regarded as a tuple storing a case identifier and a timestamp, denoted by case ( e )and time ( e ), as well as a vector prop ( e ) of data properties, in some given space of event attributes. F or example, in struct ured process man-agement settings (like those handled by WfMSs), any event keeps information on both the task performed and the executo r. However, as discussed above, this does not happen in many flexible BPM env ironments (e.g., issue/project man-agement systems), where the tasks are not precisely conceptualized, or they just represent generic operations (e.g., update a document or exchange a message).
For each trace  X   X  X  ,let  X  [ i ]bethe i -th event of  X  , and let  X  ( i ]  X  X  be the prefix trace consisting of its first i events, for i =1.. len (  X  ), where len (  X  ) is the number of events stored in  X  . Moreover, let prop (  X  ) be a vector of data properties associated with any trace  X   X  X  , possibly including  X  X nvironmental X  variables characterizing the state of the BPM system (as proposed in [9]).
A log L (over T ) is a finite subset of T , while the prefix set of L , denoted by P ( L ), is the set of all prefix traces that can be extracted from L . Finally, events ( L ) indicates the set of all events stored in (some trace of) L .
Let us denote by  X  : T X  R the (unknown) performan ce measure (targeted by our predictive analysis) that virtually assigns a performance value to any (possibly partial) trace  X  for the sake of concreteness, and w.l.o.g.,  X  is assumed to range over real numbers. Two exampl es of such a measure are the remaining processing time and steps of a trace, i.e. the time and steps, respectively, needed to complete the respective process enactment.

A (predictive) Process Performance Model (PPM ) is a model estimating the performance value of any process instance, based only on its current trace. Such a model can be viewed as a function  X   X  : T X  R that approximates  X  all over the trace universe  X  including the prefix traces of unfinished enactments. Dis-covering such a model is an induction problem, where a given log L is used as a training set, and the  X  is known for each (sub-)trace  X   X  X  ( L ).

In order to focus on relevant facets of events, current approaches rely all on some abstraction functions such as those defined below (similarly to [1,9]). Definition 1 (Event/Trace Abstraction). Let T be a trace universe, and E be its associated event universe. Let  X  E = {  X  e 1 ,...,  X  e k } be a given set of ab-stract event representations. An event abstraction function E : E  X   X  E is a function mapping each event e  X  E to E ( e )  X   X  E , based on e  X  X  properties. Moreover, the trace abstraction function abs E : T X  N n is defined as fol-lows: abs E (  X  )= count ( X  e 1 , X  ) ,...,count ( X  e k , X  ) ,where count ( X  e i , X  )= |{ i  X  { 1 ,...,len (  X  ) }|E (  X  [ i ]) =  X  e i }| , for any trace  X   X  X  .
 For example, let  X  1 be the complete trace associated with the first case (Case 1) in the log of Figure 1, and let E op be an event abstraction function that replaces each event with its first field op . Then, by applying function abs E op to the (prefix) traces  X  1 (1],  X  1 (2],  X  1 (3] and  X  1 (4], four tuples are obtained, which encode the multi-sets [ A ], [ A , C ], [ A , C , CI ], and [ A 2 , C , CI ], respectively.
More expressive trace abstraction schemes can be defined, as in [1], which take account for the ordering of events, and po ssibly discard less recent ones (accord-ing to a horizon threshold). However, such an issue is not considered in this work, which mainly aims at studying how a given (even simple) trace abstrac-tion scheme can be enhanced through ad-hoc event/trace clustering methods.
Based on such trace abstractions, an annotated finite state machine ( X  X FSM X ) can be derived as in [1], where each node corresponds to one abstract trace representation (produced by abs E ) and stores an estimate for the target measure, while each transition is labelled with an event abstraction (produced by E ). Alternatively, classic regression methods can be used to extract a PPM from a propositional encoding of the log, like that in Definition 1.

Basic PPM learning methods were recentl y hybridized with Predictive Clus-tering techniques [5], which partition a data set into clusters, while assuming that all data instances own two kinds of features: target attributes (to be pre-dicted), and descriptive attributes (used to define logical splits). Specifically, such a clustering procedure was combined with the AFSM method [9] and standard regression methods [4], with context data used as descriptive trace attributes. Clearly, the effectiveness of current pe rformance mining approaches strongly de-pends on the capability of the event abstraction function E to focus on those properties of log events that are really connected with the behavior (and per-formances) of the process at hand. Unfortunately, the common solution of ab-stracting each event into a task X  X  and/or an executor X  X  label does not fit the case of logs storing fine grain records (corresponding to low-level and generic operations), where none of the event pro perties is suitable for making an effec-tive event abstraction. For instance, in the case of Example 1, considering just the kind of operation performed leads to excessive information loss. Conversely, defining abstract activities as the mere combination of multiple properties (e.g., regarding each distinct triple in Example 1 as an activity) may yield a cumber-some and ineffective representation of p rocess states (as proven empirically in our experimentation). On the other ha nd, many real-life tracing systems keep lots of context information for each p rocess case, which may be used to build precise and articulated performance prediction models like those in [9].
In order to fully exploit the variety of (events X  and cases X ) data stored in a process log, we try to build an expressive performance model for the process, hinging on two interrelated classification models: one allowing to grasp the right level of abstraction over log events, and the other encoding the business rules that determine each variant. A precise definition of such a model is given below. Definition 2 ( CCPM ). Let L be a log, over an event (resp., trace) universe E (resp., T ). Then, a Co-Clustering Performance Model ( CCPM )for L is a triple of the form M = C E , C T , X  ,where: (i) C E : E  X  N is a partitioning function over E ; (ii) C T : T X  N is a partitioning function over T ;and (iii)  X  =  X  1 ,..., X  q is a list of PPMs, all using C E as event abstraction function, such that q is the number of clusters produced by C T ,and  X  i is the model of the i -th cluster, for i  X  X  1 ,...,q } . The overall prediction function encoded by M (denoted by the same symbol M , for shortness) is: M (  X  )=  X  j (  X  ), where j = C T (  X  ). Conceptually, a forecast fo r any new process instance  X  can be made with the help of such a model, in three steps: (i) an abstract representation of  X  is obtained in the form of a vector (as specified in Definition 1) that summarizes both its context data and structure, with each ev ent abstracted into an event class via C ; (ii)  X  is assigned to a trace cluster (repr esenting a particular execution scenario for the process) via function C T ; (iii) the predictor of the chosen cluster is used to make a forecast for  X  , by providing it with abs C E (  X  ). The functions C E and attributes), are hence exploited to abstract raw log events into high-level classes, and to discriminate among different process variants, respectively. 3.1 Solution Algorithm In principle, one might seek an optimal CCPM for a given log L by trying to minimize some suitable loss measure (comparing the actual performance of each trace with the corresponding estimate). By contrast, in order to avoid prohibitive computation times across such a large search space, we rephrase the discovery problem into two simpler ones: (1) find a locally optimal pair of classification functions C E and C T ,and (2) derive a collection of cluster-wise PPM predictors. Our solution approach is summarized in Figure 2 as an algorithm, named CCD ( X  X o-Clustering based Discovery X ). Since the quality of the trace clustering model C T strongly depends on the chosen abstraction function C E ,andviceversa, we regard the first subproblem as a co-clustering one, were an optimal partition must be found for both traces and events. This problem is approached via an iterative alternate-optimization scheme, where, at each iteration k ,updatedver-sions of the two partitioning functions are computed, denoted by C ( k ) E and C ( k ) T , until no satisfactory loss reduction (w.r.t. the previous iteration) is achieved. Notice that, for efficiency reasons, any loss Err ( k ) is measured by accounting for the distribution of performances in each  X  X o-cluster X  as follows: Loss ( C T , C E ,L )=
Each model C ( k ) E is induced, via function minePCM , from an  X  X vent-oriented X  view ( e-view ) EV of the input log, which encodes both event data and a series of performance measurements, computed on current trace clusters. The discov-ered event clustering C ( k ) E is then used, as a novel event abstraction method, to provide minePCM with a  X  X race-oriented X  view ( t-view ) TV of the log, in order to eventually induce an updated trace partitioning C ( k ) T . In this way, any novel trace clustering takes advantage of th e most recent definition of event classes, and vice versa, according to a reinforcem ent learning scheme. Both kinds of views are formally defined below.
 Definition 3 (T-View). Let L be a log, and C E be a partitioning function defined over events ( L ). Then, a t-view for L w.r.t. C E , denoted by V T ( L, C E ), is a relation containing, for each trace  X   X  X  ( L ), a tuple z  X  = prop (  X  )  X  abs C E (  X  )  X   X  (  X  ) ,where  X  stands for tuple concatenation. For any such tuple z  X  , prop (  X  ) and abs C E (  X  ) are considered as descriptive features, while  X  (  X  ) is the associated (unidimensional) target.
 Definition 4 (E-View). Let L be a log, C T be a (trace) partitioning func-tion over P ( L ), and {  X  t 1 ,...,  X  t q } be the clusters which C T ranges over (with  X  t = {  X   X  X  ( L ) |C T (  X  )= i } for i  X  X  1 ...q } ). Then, an e-view for L w.r.t. C
T , denoted by V E ( L, C T ), is a relation consisting of a tuple z e = prop ( e )  X  concatenation, and, for any i  X  X  1 ,...,q } ,itis: val ( e,  X  t i )= For any tuple z e , all the fields in prop ( e ) are regarded as descriptive attributes, and val ( e,  X  t 1 ) ,...,val ( e,  X  t q ) as its associated (multidimensional) target. Provided with such propositional views, function minePCM is meant to induce a logics-based partitioning function, by applying some predictive clustering proce-dure to the given (target and descriptive) data. Details on how this function was implemented in our prototype system can be found in the following subsection.
Once an (locally) optimal pair of event and trace clusterings has been found, each cluster predictor  X  i ,for  X  X  1 ,...,n } , is eventually computed by providing function minePPM with all the traces assigned to the cluster, and with the event abstraction function C E . To this end, the function converts  X  t i in its t-view w.r.t. C , prior to applying some suitable regression method to it.
Notice that the auxiliary parameters of algorithm CCD are meant to give the analyst some control on both computation times ( maxIter ,  X  )andthecom-plexity of the discovered model ( maxCl E , maxCl T ). However, whatever setting is used, the algorithm is guaranteed to terminate, since the loss measure must decrease at each iteration (  X &gt; 0), and the number of event/trace classification functions is finite. Notably, in a wide series of tests, the computation naturally finished in a few steps (less than maxIter ), by just fixing  X  =0. 3.2 Implementation Issues and Prototype System Functions minePPM and minePCM The current implementation of minePPM fol-lows the method proposed in [5] for inducing a PCT (Predictive Clustering Tree), a logics-based predictive clustering model where the cluster assignment function is encoded in terms of decision rules (over descriptive attributes). Basically, such a model is built via a top-down partitioning scheme, where the log is split re-cursively, while selecting each time a descriptive attribute that locally minimizes the (weighted) average of the variances of the newly generated clusters  X  i.e. the 2-norm distances between the centroi d of each new cluster and all instances in it. To curb the growth of the tree, an F-test based stopping criterion is used, possibly combined with a user-given upper bound to the total number clusters.
The current implementation of function minePCM just relies applying one of the two following standard regression methods to a propositional log view like that in Definition 1): the regression-tree induction algorithm RepTree and the k -NN procedure IB-k , both available in the popular Weka library [15]. System Prototype. The approach has been integrated into a prototype system, featuring the conceptual architecture in Figure 3, whose two lower layers leverage some core functionalities of the Process Mining framework ProM [2].

Basically, the bottommost layer is responsible for storing both historical pro-cess logs, and the different kinds of view s extracted from them, as well as the different kinds of models composing each discovered CCPM models. All data min-ing and transformation mechanisms used in our approach are implemented in the Knowledge Discovery layer, which supports the discovery of a new CCPM ,in a interactive and iterative manner, based on the computation scheme of algo-rithm CCD . In particular, the Predictive Clustering and PPM Learning modules implement the functions minePCM and minePPM functions, respectively.

All models discovered out of a process log (i.e., traces X  and events X  clustering models, and the PPM models of each trace cluster) are made available to the Model Evaluation and Reuse Layer , which, in particular, provides the user with an easily-readable report, including the e rror metrics considered in our tests. The OLAP Gateway module is meant to reorganize historical log data into different aggregated views, in order to possibly support OLAP-like analyses.

Thanks to its predictive nature, each discovered CCPM can be used to configure a forecasting service for the process it was discovered for, to estimate (at run time and step-by-step) the performance outcome of any new instance of the process. Besides pure performance prediction, the Advanced Monitoring module supports the anticipated notification of Service Level Agreement (SLA) violations, when-ever a process instance is estimated to fail a given quality requirement, previously established for one of the performanc e measures associated with the process. In order to assess the validity our approach, we conducted a series of tests on the logs of two real process management systems: the operational system of a maritime hub, and bug-tracking system. For readability purposes, Table 1 summarizes some features of both scen arios, indicated hereinafter as harbor and bug , respectively. Notice that a few trace attributes (e.g., comments and votes , in the Bug scenario) are not really known at the very beginning of a case. Clearly, such properties can be used to dynamica lly (re-)assign a process case to a trace cluster only if they have already taken a value. Anyway, as any trace clustering returned by our approach is ensured to cover the entire universe of traces at any step of its unfolding, each process instance falls into one of the trace clusters.
Three variants of the CCD algorithm were studied in our tests, which differ in the implementation of function minePPM : CCD-RT , using the regression-tree induction algorithm RepTree [15]; CCD-IBK , based on the IB-k procedure imple-mented in Weka [15]; and CCD-AVG , where each cluster predictor just returns the average performance in the cluster  X  the last method just serves as a baseline and quantifies co-clustering loss. In all c ases, a fixed setting was used for the aux-iliary parameters: maxIter =20,  X  =0,  X  =1%, and maxCl E = maxCl T =50. Notice that we bounded the number of event/traces to have handier process models and speed up the computation, at the cost of low precision loss (at least for CCD-RT and CCD-IBK ), with respect the default setting maxCl E = maxCl T =  X  .
For the sake of comparison, besides using the two base regressors mentioned above (denoted by RT and IBK ) as baselines, we tested the FSM-based method in [1] (here named AFSM ), the CATP algorithm of [9] (which reuses AFSM ), and two variants of the approach in [4], denoted by as AATP-IBK and AATP-RT ,using IBK and RepTree , respectively, as base learners . We remark that all competitors lack automated mechanisms for abstracting log events (into activity/action enti-ties), and hence need the application of some a-priori event abstraction function. Conversely, all of the tested methods but AFSM can take advantage of case data.
Three standard error metrics have been us ed to evaluate prediction accuracy: root mean squared error (rmse) , mean absolute error (mae) ,and mean absolute percentage error (mape) . For the sake of significance, all the error results reported next were computed via 10 fold cross-validation and averaged over 10 trials. Moreover, a statistical test was applie d to check whether the methods performed really different. Specifically, for each error metrics, we used a paired two-tail Student X  X  t -test to compare the outcomes of each method with those of the most precise one (i.e. the one achieving the low est average error on that metric), at two different confidence levels: 95% and 99%. We then considered a method as almost equivalent to (resp., substantially worse than) the best performer if it did not differ at the 95% level (resp., it did differ at the 99% level) from the latter. 4.1 Tests on the Harbor Scenario This scenario pertains the handling of containers in a maritime terminal, where a series of logistic activities are performed and traced for each container passing through the harbor. As mentioned in Example 1, each log event stores, by way of event attributes, different aspects of t he logistics (move) actions performed on a container, including the followings: (i) the source and destination position it was moved between, in terms of yard X  X  blocks ( block from and block to , respectively) and areas ( area from and area to , respectively) (ii) the kind of operation performed ( movType ), ranging over MOV e, DR ive to B ring, DR rive to G et, LOAD , DIS charge, SH u F fle, OUT ; (iii) the type of instrument used ( vehicleType ), ranging from cranes to straddle-carriers and multi-trailers.
Trace attributes convey instead differen t properties of the handled container, ranging from the previous and next ports ( prevCall and nextCall ), and their associated countries ( prevCountry and nextCountry ), to several physical fea-tures (e.g., size and height ). Like in [9], for each container, we also considered, as a sort of environmental variables, the hour (resp., week-day, month) when it arrived, and the total number of containers ( X  X orkload X ) in the port at that time. The list of all events X  and traces X  attributes can be found in Table 1.
While our approach doesn X  X  need any preliminary event abstraction/labelling, and it can deal with raw (multi-dimensional) event tuples, an event abstraction criterion must be defined prior to applying any other method. Two different so-lutions were used to this purpose in our tests: (EA1) abstracting each container-handling event with just the associated move type (namely, MOV , DRB ,etc.);and (EA2) using the combination of the former five event attributes in Table 1. Prediction Accuracy Results. Table 2 reports the (average and standard devi-ation for the) errors made by our methods and the competitors/baseline ones, when trying to predict the remaining processing time over a sample of 5336 con-tainers, all exchanged with ports of the Mediterranean sea in the first third of 2006. Clearly, when faced with the challenge of dealing with complex events, ac-cording to setting S2 , all competitors exhibit a neat worsening of results, w.r.t. the case where they were just made focus on the kinds of moves performed (setting S1 ). In fact, we verified empirically that this is the best possible single-attribute event abstraction for the given log  X  i.e. worse results are obtained by previous methods when abstracting the events via any other single attribute. Moreover, all base learners IBK , RT and AFSM seem to improve when embed-ded in a trace clustering scheme (see AAPT-IB , AAPT-RT and CATP , respectively). However, the best achievements are clearly obtained by our methods CCD-IBK and CCD-RT . Besides confirming the ability of our approach to find an effective abstraction over raw events, these results show its superiority to the two-phase (i.e. event abstraction, followed by model induction) strategy commonly used in the field of process mining, often relying on manually defined activities. Qualitative Results. Table 3 summarizes some of the classification (i.e. partition-ing) rules appearing in both clustering functions discovered with our approach (precisely, with CCD-RT ) on the harbor scenario. Notice that these rules are quite easy to interpret and validate, and provide the analyst with a useful description of process behavior (besides supporting a ccurate predictions). In particular, the event clusters in the table confirm that p erformance-relevant activity patterns cannot be captured by just one of the event properties, nor by a fixed combi-nation of them. Interestingly, indeed, wh ile some event clusters just correspond to a subset of destination areas, some oth ers also depend on the source area, or even further on the kind of move performed. On the other hand, the descrip-tions of trace clusters let us reckon the pr esence of different execution scenarios, linked to both context factors (e.g., the country of the previous/next port, or the line/service planned to bring the container) and to some of the discovered event clusters (hence playing as high-level activity patterns). 4.2 Tests on the Bug Scenario As a second testing scenario , we analyzed the Eclipse project X  X  bug repository, developed with the Bugzilla bug-tracking system (see http://www.bugzilla.org ).
Essentially, each bug in the repository i s associated with several fields (here regarded as trace attributes), which keep information, e.g., on: who reported the bug ( reporter ); who it has been allocated to ( assignee ); the affected software module ( component , product , version , hardware ); its severity and priority levels; its status and resolution . the number of comments written about the bug ( comments ). Almost all these fields can be updated as long as a bug evolves. In particular, the status of a bug can take one of the following values: unconfirmed , new , assigned , resolved , verified , reopened ,and closed . The resolution of a resolved bug can be: fixed , duplicate , works-for-me , in-valid ,or won X  X -fix .

As to log events, the history of a bug is kept in Bugzilla as by way of update records, possibly grouped in  X  X ug activities X , each of which gathers all changes made within a single access session. In o rder to let our propositional mining methods capture  X  X imultaneous X  updates, we encoded each bug activity a into an event having as many attributes as the number of (modifiable) bug fields, such that each attribute stores either (i) the new value assigned to the corresponding field, if it was really modified in a ,or (ii) null ,otherwise.

In order to provide the competitors with abstracted events, we tried different combinations of bug fields as possible activity labels, and empirically found that the best solution for them consists in only focusing on the changes made to the status (and to the resolution field, if modified  X  X ontemporaneously X ), or to the assignee . In the former case, the activity label just encoded the new value assigned, without keeping any information about the person to whom a bug was (re-)assigned. The resulting abstract events look like the following activity labels: status :=new, status :=resolved + resolution :=fixed, status :=verified, etc.,  X  assignee (simply indicating a generic change to the assignee field). Prediction results. The tests were performed on a subset of the bugs created from January 1st, 2012 to April 1st, 2013, such that they were fixed at least once, but not opened and closed in the same day. Moreover, we filtered out all events (i.e. bug activities) that did not refer any of the fields status , resolution , and assignee . The resulting log consists of 2283 traces, with lengths (i.e. nr. of events) ranging from 2 to 25. Prediction errors on the bug scenario are re-ported in Table 4. Despite the fact that it was not provided with any suggestion on how events should be abstracted, our approach reached excellent prediction results (except when using the na  X   X ve regressor CCD-AVG ), neatly better than all competitors, with the exception of AATP and, partially, of IBK .
 Qualitative results. The models in Table 5 confirm that our approach really managed to automatically extract a suitable abstract representation for the given log events, which looks indeed very similar to the one defined for optimally applying the competitors, Indeed, the status and resolution attributes have been fully exploited for discriminating among event classes. Trace clusters seem to depend mainly on the number of comments associated with bugs, and on the component and/or product affected  X  as well as on some of the discovered event classes, here playing as high-level performance-relevant activity patterns. The method proposed in this paper enhances current process mining approaches for the analysis of business process perfo rmances in different respects. First of all, it removes the common assumption that all traced event logs refer explicitly (or can be easily mapped to) well define d process tasks, and allows to automat-ically replace the formers with high-level activity types, capturing performance behaviors at the right level of abstraction.

Empirical findings from two real application scenarios proved that the ap-proach can achieve compelling prediction accuracy with respect to state-of-the-art process-mining methods, even when th ese latter are provided with a manual definition of process activities, carefully specified by an expert. We believe that prediction accuracy could be improved f urther by resorting to more powerful regression methods for inducing cluster-wise PPMs, in place of the straightfor-ward ones used in our current implementa tion. Moreover, the descriptive power of logical event/trace partitioning rules, beside allowing for a quick validation and evaluation of the discovered models, can really help the analyst better com-prehend the behavior of the process, and the way its performances depend on both context factors and activity patterns.

As to efficiency, the approach seems to w ork well in practice. Indeed, in the two scenarios discussed above, at most 6 co -clustering iterations were needed to find a solution (with  X  =1% and maxCl E = maxCl T =50), and our approach only took 3.6 times longer than the quickest among the competitors  X  excluding IBK , which performs no real learnin gtask.Thisratioonlybecame5 . 4 when no finite upper bound was set for the numbers of clusters.

As future work, we plan to extend the ex pressive power of our event/trace classification models, and to integrate advanced regression methods for learning cluster predictors, as well as to implement our discovery approach as a ProM [2] X  X  plugin, and to refine the OLAP-oriented capabilities of our prototype system. Acknowledgments. The work was partially supported by the Italian Ministry of Education, Universities and Research (MIUR), under project FRAME .

