
Nonnegative matrix factorization (NMF) is a versatile model for data clustering. In this paper, we propose sev-eral NMF inspired algorithms to solve different data min-ing problems. They include (1) multi-way normalized cut spectral clustering, (2) graph matching of both undirected and directed graphs, and (3) maximal clique finding on both graphs and bipartite graphs. Key features of these algo-rithms are (a) they are extremely simple to implement; and (b) they are provably convergent. We conduct experiments to demonstrate the effectiveness of these new algorithms. We also derive a new spectral bound for the size of maximal edge bicliques as a byproduct of our approach. keywords: Nonnegative matrix factorization, clustering, graph matching, clique finding
A large number of data mining tasks can be formu-lated as optimization problems. Examples include K-means clustering, support vector machines (SVM), linear discrimi-nant analysis (LDA), and semi-supervised clustering. Non-negative matrix factorization (NMF) has been shown to be able to solve several data mining problems including classification and clustering. In this paper, we show that NMF provides an optimization framework which has a much broader applicability and can solve a variety of data mining problems.

NMF has been a significant s uccess story in the machine learning literature. Originally proposed as a method for find-ing matrix factors with parts-of-whole interpretations [17], NMF has been shown to be useful in a variety of applied settings, including environmetrics [26], chemometrics [34], pattern recognition [20], multimedia data analysis [5], text mining [35, 27], and DNA gene expression analysis [3]. Algorithmic extensions of NMF have been developed to accommodate a variety of objective functions [6, 10, 21] and a variety of data analysis problems, including classifi-cation [29] and collaborative filtering [31]. A number of studies have focused on further developing computational methodologies for NMF [15, 1, 22, 36, 23]. Researchers have also begun to explore some of the relationships be-tween matrix factorizations and clustering [11, 9, 10], open-ing up a variety of additional potential applications for NMF techniques. It has been shown that NMF with the sum of squared error cost function is equivalent to a relaxed K-means clustering [37, 8], the most widely used unsupervised learning algorithm. In addition, NMF with the I-divergence cost function is equivalent to probabilistic latent semantic indexing, another unsupervised learning method popularly used in text analysis.

In this paper, we show that NMF provides a nice frame-work for solving many data mining optimization problems. In particular, we broaden the scope of application to include several different data mining problems. We provide NMF-inspired solutions to  X  Multi-way normalized cut spectral clustering,  X  Graph matching,  X  Maximal clique and biclique
We also show that new analytical insights flow from this approach. In particular, in the maximal biclique problem our approach allows us to derive a new spectral bound for the size of the maximal edge clique.
Normalized Cuts [30] is a NP-hard optimization prob-lem. We focus on the multi-way version of Normalized Cuts, which can be formulated as the minimization of the follow-ing objective function: where w ij are the entries of an affinity matrix W , { C i disjoint subsets of the vertices, d i = j w ij ,  X  ( C k )= { 0 , 1 } n be an indicator vector for cluster C k .Let D = diag( d 1 ,  X  X  X  ,d n ) .Wehave Let H =( h 1 / || D 1 / 2 h 1 || ,  X  X  X  , h K / || D 1 / 2 h lem becomes [13] If we ignore the nonnegative constraints, and keep the or-thogonality intact, the solution for H is given by the gener-alized eigenvectors of D  X  W . However, the mixed signs of the eigenvector solutions make the cluster assignment diffi-cult. Thus the nonnegativity constraints is the key.
Lee and Seung [19] showed that the NMF problem could be solved by a multiplicative update algorithm. In this sec-tion we show that a similar approach can be adopted for Nor-malized Cuts. We propose the following multiplicative up-date algorithm for solving Eq. (2): In the following two subsections we show that this update yields a correct solution at convergence and we show that the algorithm is guaranteed to converge. 2.1.1 Correctness Theorem 1 Fixed points of Eq. (3) satisfy the KKT condi-tion.
 Proof . We begin with the Lagrangian where the Lagrange multiplier  X  enforces the orthogonality condition H T DH = I . The nonnegativity constraint is en-forced using the KKT complementary slackness condition Summing over j , we obtain ( H T WH ) ii =( H T DH X  ) ii =  X  ii . This gives the diagonal elements of  X  .Tofindthe off-diagonal elements of  X  , we temporally ignore the non-negativity requirement. This gives ( WH  X  DH X  ) ij =0 . Left-multiplying by H i j and summing over j , we obtain ( H T WH ) i i =  X  i i for the off-diagonal elements of  X  . Combining these two results yields Clearly, a fixed point of the update rule Eq. (3) satisfies ( WH  X  DH X  ) ij H 2 ij =0 , which is identical to Eq. (5). This is so because if H ij =0 ,wehave H 2 ij =0 , and vice versa.  X  2.1.2 Convergence Theorem 2 Under the update rule of Eq. (3), the La-grangian function L of Eq. (4) increases monotonically (it is nondecreasing).
 The proof is given in the Appendix.
An approximate solution to the normalized cut problem can be obtained by K-means clu stering in an eigenspace embedding [25]. We use such a clustering to initialize H. Specifically, we (1) compute the K principal eigenvectors of D  X  1 / 2 WD  X  1 / 2 , (2) normalize each embedded point to the unit sphere, and (3) perform K-means on the normalized points. This gives H 0 . We then initialize the update rule Eq. (3) with H 0 +0 . 2 and iterate until convergence.
We report results from running the NMF algorithm on the datasets wine and soybean from the UCI data reposi-tory. Figure 1(a) and Figure 1(b ) plot the clustering accuracy
Figure 1. Objective function (dashed curve) and clustering accura cy (solid curve) as a function of iterations on wine and soybean datasets. On wine dataset, objective function is scaled by a factor of 0.33 to be comparable to clustering accuracy values. On soybean dataset, objective function is scaled by a fac-tor of 0.22. across iterations of the algorithm; we also plot the evolution of the objective function L . Note that the algorithm yields a significant improvement relative to its initialization from the spectral clustering algorithm of [25]. Note also that (as expected from Theorem 2), the objective function increases monotonically.
Graph matching plays an important role in many data mining applications such as pattern recognition, informa-tion retrieval, and frequent pattern mining to determine cor-respondences between the components (vertices and edges) of two attributed structures [16]. Given two graphs with ad-jacency matrices A and B , the graph matching problem is to permute the nodes of A such that ij [( P T AP ) ij  X  B ij ||
P T AP  X  B || 2 is minimized, where P is a permutation ma-trix. Since || P T AP  X  B || 2 = || A || + || B || X  2 Tr P the problem becomes The constraints P T P = PP T = I and P  X  0 together ensure that each row has one nonzero elements, and so does each column. We first consider undirected graphs, i.e., A = A
T ,B = B T .
In this section we show that the following multiplicative update algorithm:
P is correct and converges to a locally optimal solution. This algorithm has O ( n 3 ) complexity. At convergence, the ele-ments of the solution P  X  are not generally { 0 , 1 } -valued and rounding is necessary. We propose to carry out this rounding via the Hungarian algorithm for the bipartite graph matching. This algorithm, which can be formulated as P =argmax P Tr ( PP  X  ) , also has complexity O ( n 3 ) . 3.1.1 Correctness Theorem 3 Fixed points of Eq. (8) satisfy the KKT condi-tion.
 Proof . The Lagrangian function is The KKT complementary condition for the nonnegativity constraint is Summing over j , we obtain ( P T AP B ) ii =( P T P X  ) ii  X  ii . This gives the diagonal elements of  X  .Tofindthe off-diagonal elements of  X  , we temporally ignore the non-negativity requirement and setting  X  X / X  X  =0 , we ob-tain ( P T AP B ) ij =  X  ij for the off-diagonal elements of  X  . Combining these two results together, we have  X  = P T AP B. However, since P T P  X  I is symmetric, Thus only the symmetric part of  X  contributes to L , i.e.,  X  should also be symmetric. Thus we set Clearly, the fixed points of the update rule Eq. (8) satisfy ( AP B  X  P X  ) ij P 2 ij =0 , which is identical to Eq. (10). 3.1.2 Convergence Theorem 4 Under the update rule of Eq. (8), the La-grangian function L ( P ) of Eq. (9) increases monotonically (nondecreasing).
 The proof is similar to that of Theorem 1 and is omitted. 3.1.3 Initialization Let J = P T AP B T . It can be shown that J has an up-per bound [33] and we can solve the optimization problem based on this upper bound to initialize our algorithm. Let the eigen-decompositions of A, B be A = U  X  U T and B = V  X  V T ,where  X  = diag(  X  1 ,  X  X  X  , X  n ) contains the eigen-values of A in descending order, and  X  = diag(  X  1 ,  X  X  X  , X  contains the eigenvalues of B in descending order. We have J = Tr P T ( U  X  U T ) P ( V  X  V T )= Tr  X ( U T PV ) X ( U T Thus an upper bound of J is attained by relaxing P from a permutation matrix to an orthonormal matrix: P = UV T . However, the solution is not unique, because in the d ecomposition A = U  X  U T , the signs of the eigenvectors are not unique. That is, A = U  X  U T holds for any combination of signs of all eigenvectors: U = (  X  u 1 ,  X  X  X  ,  X  u n ) .

To resolve this non-uniqueness, we adopt Umeyama X  X  ap-proach [32]. Let  X  U be the matrix containing the absolute values of U : (  X  U ) ij = | U ij | , and similarly for  X  pute It is easy to see that 0  X  ( P 0 ) ij  X  1 , because each row of and  X  V is a nonnegative vector with length 1. We use P 0 the initial value for our iterative updating algorithm.
Graph matching for directed graphs is harder than for undirected graphs and there has been little research on this topic. It is thus of some interest that our approach to graph matching generalizes to directed graphs in a straightforward way. In this case A = A T ,B = B T . The updating rule that we propose is where  X  = We can readily establish corr ectness and convergence for this update rule using arguments similar to those for the undirected case.

The initialization of the algorithm is somewhat more del-icate than for the undirected case. Indeed, for directed graphs, no upper bound is known to exist. Following [32], we base our initialization on the following two hermitian matrices:  X  A = where i = max P || P T  X  AP  X  B || 2 using the same argument as in Sec-tion 4.1.3. Let the eigendecompositions be  X  A = U  X  U T B = V  X  V T . Note that U, V are complex valued matri-ces. Let U contains the magnitudes of each element of U U ij = | U ij | , and similarly for V .Then P 0 = U V the initial value of P .When A = A T ,B = B T , this algo-rithm reduces to that for undirected graphs.
Following standard procedures in the literature on match-ing algorithms, we test our algorithm by considering ma-trices A ij = 100 r ij where r ij is a uniform random num-ber in [0 , 1] and B ij =( P T t AP t ) ij (1 + r ij ) ,where P is a permutation and sets the noise level. Note that the globally optimal permutation P satisfies || P T AP  X  B || 2  X  || t AP t  X  B || 2 . Because it is difficult to compute the true global solution in experiments, we consider a solution to be optimal if it satisfies this inequality.

The state-of-art practical algorithm for directed graph-matching is Umeyama X  X  algorithm [32]. This algorithm computes P =argmax P Tr PP 0 using the Hungarian al-gorithm. We compare to Umeyama X  X  algorithm. We first present an example with N =6 and =0 . 4 . The input weighted graphs (matrices) are where for ease of inspection, we permute A back to best match B using the solution P . P 0 and P  X  at convergence are shown below: where the underline indicates that the corresponding el-ement is rounded to one and the remaining elements are rounded to zero by the Hungarian algorithm. For this ex-ample, our algorithm correctly computes the permutation : ||
P T AP  X  B || =62 . 42 . Umeyama X  X  algorithm failed to re-cover the correct permutation: || P T 0 AP 0  X  B || =97 . 96 , but did improve the matching (without the permutation, || A  X  B || = 161 . 98 ).

We run Umeyama and our algorithm on graphs with sizes up to 50 at noise levels up to =0 . 4 . The results are shown in Table 2, which presents the success rate of correctly com-puting the global permutation, averaging over 100 runs for N =10 , 50 runs for N =50 and 20 runs for N =50 . P 0
Table 1. Success rate for different sizes ( N ) and noise levels ( ). P 0 :using P 0 and the Hun-garian algorithm. NMF: using NMF and the Hungarian algorithm.

Table 2. Success rate at N =50 . P 0 + refine: using P 0 and refinement. NMF+refine: using
NMF and refinement. gives the results for Umeyama algorithm and NMF for our algorithm. Our algorithm does significantly better than the Umeyama algorithm at higher noise level.

We can improve on the basic algorithm by adding an re-finement algorithm, which exchanges two or three nodes at a time in a greedy fashion to optimize the matching. Table 2 also presents results for this refinement. We see that refine-ment can significantly improve both of the basic algorithms.
We base our approach to computing maximal cliques on a theorem due to [24] that relates maximal cliques to the optimization of a quadratic function. Let A be the adjacency matrix of an undirected graph with weights in { 0 , 1 } .The computation of maximal cliques can be formulated as the solution to the following optimization problem: where A ii =0 .
 Theorem 5 (Motzkin and Straus) Let G be an unweighted graph and let x  X  the optimal solution for the problem of to nonzero elements. If nonzero elements have the same val-characteristic vector of a subset C ), C is a maximal clique in G .

An algorithm for computing solutions to the maximal clique problem has been presented by [28] and [12]. The algorithm has a multiplicative form: and has been shown to be effective in practice [28, 12].
This approach is interes ting not only because it antici-pates our work in using a multiplicative update to solve a combinatorial optimization p roblem, but also because the al-gorithm provides a very clear link between sparsity and the L -norm constraint.
 In this section, we use our NMF-based approach to: (1) Prove the convergence of the update algorithm of Eq. (17); (2) generalize the L 1 -norm constraint to L p -norm constraint and derive an algorithm to compute it; (3) generalize this methodology to bipartite graphs.
We generalize the maximal clique problem to the follow-ing optimization problem: where  X   X  [1 , 2] is a parameter. We show the following: (1) The maximal clique is obtained when  X  =1+ , 0 &lt; A ii =1 enables us to generalize this approach to bipartite graphs. (2) A convergentalgorithm can be obtained for  X   X  [1 , 2] . When  X  =1 , this algorithm reduces to the extant algorithm of Eq. (17). (3) As  X   X  1 + , the sparsity of the solution increases steadily, reflecting the close relation between L 1 constraints and sparsity. At  X  =2 , the solution is given by the principal eigenvector of A .

Let  X  A denote the adjacency matrix of the graph with  X  A ii =1 .andlet A be the adjacency matrix of the graph with A ii =0 . We can prove the following Generalized Motzkin-Straus Theorem: Theorem 6 Using  X  A as the adjacency matrix, and setting  X  =1+ , 0 &lt; 1 .Let C = { i | x i &gt; 0 } be the sub-set corresponding to nonzero elements. If nonzero elements have same values, x i =1 / | C | for  X  i  X  CC is a maximal clique in G .
 Proof sketch . Since the nonzero elements of x have constant values, x  X  must have the form x  X  = first. The objective becomes J =( x ) T  X  A x = | C | 2  X  2 / X  Because 2  X  2 / X  &gt; 0 , max J is equivalent to max | C |
Note that if we use  X  =1 ,then J is equal to one inde-pendent of | C | ; i.e., we are not guaranteed to compute the maximal clique. Theorem 6 can be readily generalized to bipartite graphs. 4.1.1 An algorithm for the generalized Motzkin-When  X  A is positive definite, there is a unique global solution to the quadratic optimization problem. When  X  A is positive semidefinite, there are several solutions with the same opti-mal value of the objective function. When  X  A is indefinite, there are many locally optimal solutions. In our case, since A ii =0 , A is always indefinite. Thus working with the L -norm version (Theorem 6) has some advantages.

To find the local maxima and thus the maximal cliques, we use an algorithm that iteratively updates a current solu-tion vector x ( t ) as follows: The iteration starts with an initial guess x (0) and is repeated until convergence.
 We analyze the basic properties of this algorithm. Feasibility . First, we show that from any initial x (0) ,the iteration will lead to a feasible solution: Optimality . Second, we show that the update rule satisfies the first-order KKT optimality condition. We form the La-grangian function where  X  is the Lagrangian multiplier for enforcing the L p constraint. This leads to the following KKT condition: Summing over index i , we obtain the value for the La-grangian multiplier  X  : Clearly the updating rule satisfies the KKT condition Eq. (36) at convergence. Sub-stituting the value of  X  from Eq. (22), this yields the update rule Eq. (19).
 Convergence . Theorem 7 Under the update rule in Eq. (23), the La-grangian function L of Eq. (20) is monotonically increasing (nondecreasing), Since J is bounded from above, the convergence of this al-gorithm is thus established.
 Upper bound .

From the proof of Theorem 6, we can provide a new derivation of a known upper bound on the size of the maxi-mal clique [cf. 2]:
Let G ( B ) be a bipartite graph with a set R of r -nodes and a set C of c -nodes. Let B be the adjacency matrix of G ( B ) . B is a rectangular matrix of n = | R | rows ( r -nodes) and m = | C | columns ( c -nodes). A biclique is a subset ( R 1 ,C 1 ) ,where R 1  X  R and C 1  X  C , such that every r-node in R 1 is connected to every c-node in C 1 .There are two types of maximal bicliques: (a) maximal edge bi-cliques where the number of edges, | R 1 | X | C 1 | , is maximal and (b) maximal node biclique s where the number of nodes, |
R 1 | + | C 1 | , is maximal. Typically, maximal edge biclique selects the largest block area in the adjacency matrix and is the interesting biclique. Maximum-node biclique is typ-ically a narrow/skinny block in the adjacency matrix and is not very useful.

A maximal biclique is computed via the solution to the following optimization problem: where the region F  X  x is defined as F  X  x : m j =1 x  X  i 0 . The region F  X  y can be defined similarly.

Let ( x  X  , y  X  ) be an optimal solution, let R 1 = { i | 0 } be the subset of nonzero elements in x  X  ,andlet C 1 = { j | y  X  j &gt; 0 } be the subset of nonzero elements in y  X  can derive a Generalized Motzkin-Straus Theorem for bipar-tite graphs: Theorem 8 Let  X  =1+ , 0 &lt; 1 . For an optimal solution ( x  X  , y  X  ) , if nonzero elements of x  X  have the same values, and if nonzero elements of y  X  have the same values, then ( R 1 ,C 1 ) is a maximal edge biclique in B . The objec-tive function has the optimal value J =( | R 1 || C 1 | )
We provide an iterative algorithm to compute the maxi-mal edge biclique. We prove its feasibility, correctness, and rithm updates x and y using: When B is symmetric, this algorithm reduces to Eq. (19). The feasibility, optimality, and convergence of the algorithm can be established in similar manner as in the case of maxi-mal clique.
 Upper bound on the size of biclique .

We can establish a new theoretical bound on the size of biclique as a consequence of Theorem 8. In particular, we have: | which yields the following upper bound:
We test the ability of the algorithm to detect maximal bi-cliques. We embed a known biclique into the standard ran-dom graphs (two nodes are joined with an edge with fixed probability p =0 . 3 ). We vary the size of the embedded cliques, while fixing the rectangular matrix to be 500  X  1000 . We set  X  =  X  =1 . 05 . We tested on 20 bipartite graphs with randomly generated adjacency matrices. The embed-ded maximal bicliques are readily detected in all cases.
In this section, we apply our algorithm to discover bi-cliques in document collections. A document collection can be represented as a binary document-termmatrix where each entry is 1 or 0 denoting whether the correspondingdocument and term co-occur or not. The document-term matrix can be expressed as a unweighted bipartite graph with a set of document nodes and a set of term nodes. If a document con-tains a term, an edge exists to connect them. Hence bicliques in binary document-term matrices are subsets of documents with tightly coupled terms. These bicliques represent spe-cific topics and are explicitly supported/described by both representative subgroups of documents and representative subgroups of words. 4.4.1 Experiment Setup Since each biclique is compos ed of representative docu-ments and representative, purity measure can then be used to measure the extent to which each biclique contained doc-uments from primarily one class [38].
 where S i is a particular biclique or cluster with n i docu-ments, n j i is the number of documents of the j -th class that were assigned to the i -th biclique or cluster, K is the number of bicliques or clusters and n is the total number of extracted documents. In general, the larger the values of purity, the better the documents can describe the biclique.

We use five real world datasets described in Table 3 in our experiments. CSTR is a dataset of the abstracts of techni-cal reports (TRs) published in the Department of Computer Science at a research university. WebKB4 and WebKB7 datasets are webpages gathered from university computer science departments. Reuters is a subset of Reuters-21578 Text Categorization Test collection that includes the 10 most frequent categories. WebACE is the dataset generated from WebACE project [14].

It is important to note that cliques and bicliques discov-ered using our algorithm are not always 100% complete cliques  X  many computed cliques have missing edges. In general, the missing edges are less than 30% of the total pos-sible edges in the computed psuedo cliques.

The clustering procedure is the following. We compute the bicliques one at a time. After a biclique is computed, the edges among the nodes are removed. The algorithm is then repeat once more to find the next biclique. This is repeated until the computed cliques is less than minimum size, which we set to 5. 4.4.2 Results We compare our biclique finding algorithm with the tra-ditional K-Means clustering, an d several co-clustering al-gorithms including information theoretic co-clustering al-gorithm (ITCC) [7], Euclidean co-clustering algorithm (ECCC), and minimum squared residue co-clustering algo-rithm (MRCC) [4] on five real world datasets described in Table 3. The experimental results are shown in Figure 2. the computed bicliques are generally tight.
 The experiments confirm this property of our algorithm. It showes that documents in each biclique have higher pu-rity. In other words, our algorithm is able to extract more meaningful bicliques from document collections.

Recent progresses have shown nonnegative matrix fac-torization (NMF) provides a new versatile model for data clustering. In this paper, we propose several NMF inspired algorithms to solve different data mining problems including multi-way normalized cut spectral clustering, graph match-ing of both undirected and directed graphs, and maximum clique finding on both graphs and bipartite graphs. These NMF inspired algorithms are extremely simple to implement and their convergence can be rigorously proven. We conduct experiments to demonstrate the effectiveness of these new algorithms. This work highlights that techniques developed in machine learning could have much broader applicability.
C. Ding is partially supported by NSF grant DMS-0844497. T. Li is partially supported by NSF grants IIS-0546280 and DMS-0844513.
We use the auxiliary function approach [18]. An auxiliary function G ( H,  X  H ) of function L ( H ) satisfies We define Then by construction, we have This proves that L ( H ( t ) ) is monotonically increasing.
The key steps in the remainder of the proof are: (1) Find an appropriate auxiliary function; (2) Find the global max-ima of the auxiliary function. It is important that the max-ima in Eq. (30) are the global maxima, otherwise the first inequality of Eq. (31) does not hold.
 We can show that
G ( H,  X  H )= is an auxiliary function of L ( H ) of Eq. (4) (the constant term Tr  X  is ignored). Using the inequality z  X  1+log z and lower bound of the first term in Eq. (4).

We note a generic inequality where A&gt; 0 ,B &gt; 0 ,S &gt; 0 ,S &gt; 0 , with A and B sym-metric. Using this, we can see the second term in Eq. (32) is a lower bound of the second term in Eq. (4).

According to Eq. (30), we need to find the global maxima of G ( H,  X  H ) for H . The gradient is The second derivative is Thus G ( H,  X  H ) is a concave function in H and has a unique global maximum. This global maximum can be obtained by setting the first derivative to zero, which gives According to Eq. (30), H ( t +1) = H and H ( t ) =  X  H . Thus we obtain the update rule in Eq. (3).  X  [1] M. Berry, M. Browne, A. Langville, P. Pauca, and [2] I. Bomze, M. Budinich, P. Pardalos, and M. Pelillo. [3] J.-P. Brunet, P. Tamayo, T.R. Golub, and J.P. Mesirov. [4] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum [5] M. Cooper and J. Foote. Summarizing video us-[6] I. Dhillon and S. Sra. Generalized nonnegative ma-[7] I. Dhillon, S. Mallela, and D. S. Modha. Information-[8] C. Ding and X. He. K-means clustering and princi-[9] C. Ding, X. He, and H.D. Simon. On the equivalence [10] C. Ding, T. Li, and W. Peng. Nonnegative matrix fac-[11] E. Gaussier and C. Goutte . Relation between plsa and [12] L. Gibbons, D. Hearn, P. Pardalos, and M. Ra-[13] M. Gu, H. Zha, C. Ding, X. He, and H. Simon. Spec-[14] E-H. Han, D. Boley, M. Gini, R. Gross, K. Hastings, [15] P. O. Hoyer. Non-negative matrix factorization with [16] Marek Karpinski and Wojciech Rytter. Fast parallel [17] D.D. Lee and H. S. Seung. Learning the parts of ob-[18] D.D. Lee and H. S. Seung. Algorithms for non-[19] D.D. Lee and H.S. Seung. Algorithms for non-negative [20] S.Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning [21] T. Li. A general model for clustering binary data In [22] T. Li and S. Ma. IFD: Iterative feature and data cluster-[23] T. Li, S. Ma and M. Ogihara. Document clustering via [24] T.S. Motzkin and E.G. Straus. Maxima for graphs and [25] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clus-[26] P. Paatero and U. Tapper. Positive matrix factorization: [27] V. P. Pauca, F. Shahnaz, M.W. Berry, and R.J. Plem-[28] M. Pelillo. Relaxation labeling networks for the maxi-[29] F. Sha, L. K. Saul, and D. D. Lee. Multiplicative up-[30] J. Shi and J. Malik. Normalized cuts and image seg-[31] N. Srebro, J. Rennie, and T. Jaakkola. Maximum mar-[32] S. Umeyama. An eigendecomposition approach to [33] J. von Neumann. Some matrix inequalities and [34] Y.-L. Xie, P.K. Hopke, and P. Paatero. Positive ma-[35] W. Xu, X. Liu, and Y. Gong. Document clustering [36] D. Zeimpekis and E. Gallopoulos. CLSI: A flexible [37] H. Zha, C. Ding, M. Gu, X. He, and H.D. Simon. Spec-[38] Y. Zhao and G. Karypis. Empirical and theoretical
