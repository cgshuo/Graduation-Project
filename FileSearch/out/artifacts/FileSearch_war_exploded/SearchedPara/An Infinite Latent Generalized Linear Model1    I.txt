 Dirichlet process mixture (DPM) models introduced by [2] is a promising tool to in-of Gaussian process models [4], Dirichlet process mixtures of MultiNomial Logit mod-els (DPMNL) [5], Dirichlet process mixtu res of generalized linear models (DPGLM) These models can be considered as special cas es of mixture-of-experts models, since users, the number of components can be learned automatically from data. However, high, they will suffer from the curse of dimensionality problem.
 dimensional observed data from low-dimen sional latent spaces. Most latent variable models [8], such as Probabilistic Principa l Component Analysis (PPCA), Factor Anal-ysis (FA) and Independent Component Analysis (ICA) are unsupervised, they don X  X  lem, [3] proposes Supervised Probabilistic PCA (SPPCA) model, and demonstrates that the latent variables learned from SPPCA are more discriminative than PPCA. [9] ex-tends SPPCA by using a weighted objective lik elihood function to adjust the relative importance weight on predicting covariates and associated response variables. Inspired by unsupervised and supervised latent variable models, we try to extend DPGLM model [6] with them, which can tr ain DPGLM model in low-dimensional GLM). In ILGLM, we assume latent variable z n is generated from a low-dimensional DPM model in latent space, and the corresponding observed feature x n and class la-ILGLM, we will jointly learn the latent variable model and multiple local generalized linear model under the framework of Dirichlet process mixture. On one hand, ILGLM complexity; on the other hand, it avoid the curse of dimensionality problem.
More interestingly, ILGLM can be extended to semi-supervised setting, training the model using both labeled and unlabeled da ta. However, ILGLM is a general model cation models. We realize it based on Factor Analysis and MultiNomial Logit model, which results in the Infinite Latent MultiNomial Logit (ILMNL) model as an exam-ple of ILGLM. An approximate inference algorithm based on Gibbs sampling is also proposed. We compare the performance of ILMNL with SVM, MNL and DPMNL. Ex-perimental results on several real-world datasets demonstrate the good performance of our proposed model in dealing with high-dimensional data classification problems. DPGLM in Section 2, we introduce ILGLM model in Section 3, then realize it with datasets. Finally, we conclude this paper in Section 6. 2.1 Dirichlet Process Mixtures Dirichlet process mixture can be understood as a special mixture model with the num-{ x where f ( x |  X  k ) denotes the density function for component k with parameter  X  k . If we further assume that all parameters  X  1: K be drawn from base distribution G 0 . nent k, the mixture model can be expressed hierarchically as follows: 2.2 DPGLM DPGLM builds on Dirichlet process mixtures and generalized linear models(GLM). It cess mixtures, DPGLM can automatically learn the number of components according if it learns more than one component. However, DPGLM may suffer from curse of di-as follows: for component k , and the form of GLM y depends on problems. For example, Gaussian for regression or logistic for classification. Now we turn to our Infinite Latent Generalized Linear Model (ILGLM), an extension of DPGLM in latent space that tries to avoid the curse of dimensionality problem. 3.1 Notation R (  X  dimension number and component number respectively. 3.2 Graphical Model In ILGLM, we assume covariate x n and response variable y n are independent conditioned on low-dimensional latent variable z n . (This is different from DPGLM, variable z n is generated from a Dirichlet process mixture of exponential-family dis-p that sample ( z n ,x n ,y n ) belongs to.

With above assumptions, our DP mixtures model is trained in low-dimensional latent up the training but also makes the model more robust. The latent variable model used shared by all mixture latent variable models if ILMNL has more than one component.
The generative process of ILGLM can be summarized as follows, with the number of components K  X  X  X  : The graphical model of ILGLM is demonstrated in Figure 1, and its joint probability distribution is: 3.3 Semi-supervised Extension Many datasets may be short of labeled data, and unlabeled data can be used to enhance R [ v , ..., v N 1 ] .
 Both H and U are assumed to be generated exactly the same as Z and X in ILGLM. After incorporating unlabeled data, joint probability distribution becomes: 3.4 Predictive Distribution According to Bayes X  theorem, predi ctive distribution for new data x is:
Unfortunately, it is infeasible to precisely evaluate p ( y | x, X , Y , U ) since Nevertheless, approximation inference based on variational method or MCMC [1] is possible, and we will demonstrate it via Gibbs sampling in next section. As stated in last section, ILGLM can be applied to both regression and classification with suitable GLM. Besides, the mapping from l atent variables to covariates could be but not limit to any unsupervised latent variable models we have mentioned. The ex-it ILMNL short for Infinite Latent MultiNomial Logit. 4.1 ILMNL Formulation a multivariate Gaussian with parameter  X  c n ,z =(  X  c n , X  c n ) :
The distribution of x n conditioned on z n and c n parameterized by  X  x =( A,  X  ) : Note that  X  x is shared by all components with A as a D  X  d loading matrix, and  X  as diagonal noise precision matrix. It may seem a bit different from ILGLM, but we could actually consider it a special case with  X  k,x being the same for all k. eled by multinomial logit:
The simplified(o mitting some hyper-parameters) graphical model in semi-supervised setting is shown in Figure 2. We summarize the generative process as follows: 1. Generate d random samples {  X  A 1 , ...,  X  A 2. Generate loading matrix A column-wise with each column A : ,col  X  N (0 ,  X  3. Generate diagonal noise precision matrix  X  with each diagonal element  X  i,i from Gamma (  X  2 , X  2 ) ; 4. Generate K mixing proportion {  X  1 , ..,  X  K } with  X  k  X  X  ir (  X  K , ...,  X  K ) ; 5. Generate d random samples {  X  W 1 , ...,  X  W 6. Generate d random samples {  X  b 1 , ...,  X  b components row-wise, with each row W : ,row  X  N 0 , X  W row  X  1 I M and b : ,row  X  components from their conjugate prior Gaussian-Wishart distribution as (  X  k , X  k )  X  N  X  k |  X  0 , (  X  0  X  k )  X  1 W (  X  k | T 0 , X  0 ) ; and indicator vector V for unlabeled data in the same way; 10. Generate latent covariates matrix Z for labeled data with z n  X  N  X  c n , X   X  1 c and latent covariates matrix H for unlabeled data in the same way; 11. Generate covariates matrix X for labeled data with x n  X  N Az n , X   X  1 ,and covariates matrix U for unlabeled data in the same way; 12. Generate response variables vector Y for labeled data with y n  X  4.2 Inference algorithm of ILMNL is described in Algorithm 1 .
 Sample Indicators. Let X =[ X ; U ] , C =[ C ; V ] ,and N = N + N 1 ,and  X  k = {  X  The likelihood function F (  X  c data, For unlabeled data,
Using Gibbs sampling with auxiliary parameters(refer to Neal X  X  algorithm 8), each some j = n such that c n = c j ,for k  X  &lt;i  X  h, sample component parameters  X  i = {  X  value for c i from { 1 , ..., h } using the following probabilities: izing constant.  X  Remove those  X  Sample Latent Variables. For unlabeled data, we sample latent variables one by one from its posterior probability distribution where  X  =  X  + A T  X A we use SIR with proposal and importance weight to sample their latent variables.
 Sample Component Parameters. We update parameters  X  k,x = {  X  k , X  k } and  X  k,y = {
W k ,b k } component-wise.  X  k,x is sampled from Gaussian-Wishart  X  The posterior of  X  k,y also has no suitable conjugate prior, and we sample it using Hybrid Monte Carlo(HMC) as [5] Sample Loading Matrix. We sample loading matrix element-wise from its posterior: Sample Diagonal Precision Matrix. We assume the diagonal precision matrix  X  is Sample Hyperparameters. Hyperparameters H  X  ,suchas  X  for D ir ,  X  A i for loading matrix A are sampled with their corresponding conjugate priors.
 model. After a number of iterations or convergence conditions meet, we begin to es-predictive probability given current samples  X  s is: And the final predictive probability approximated by a finite sum is as follows: where b 0 is the normalizing constant and S is the number of samples used to estimate p ( y = m | x ) Algorithm 1. ILMNL 5.1 Dataset Description The real world datasets we used come from various fields, and their dimensions range from tens to hundreds of thousands. Sonar, GasSensor and PEMS-SF are available from the UCI Machine Learning Repository[11]. ORLRAWS, CLL-SUB and GLA-BRA The extracted object bank features of UIUC-Event is available from Computer Vision according to the proportion of 1:2. 5.2 Experiment Setup vised and semi-supervised scenarios, we implement two versions of ILMNL, they are sILMNL (Supervised ILMNL) and ssILMNL (Semi-Supervised ILMNL) . As multi-nomial logit(MNL) is the linear model used in ILMNL, we will compare ILMNL with MNL and its Dirichlet process mixture extending -DPMNL. To make the comparison more comprehensive, we consider projecting data into a low-dimension latent space us-ing either factor analysis(FA) or its supervised form(sFA) for both MNL and DPMNL. We use MNL-f-d and MNL-sf-d to denote MNL with data projected into d-dimension latent space using FA and sFA respectively. For example, MNL-f-5 stands for MNL tested on data projected into 5-dimension latent space using FA. And MNL-full stands for DPMNL are just alike. We also compare with SVM using both linear(L-SVM) and RBF kernel(R-SVM). When the number of classes is larger than 2, L-SVM and R-SVM use the one-vs-rest scheme. Source codes of MNL and DPMNL are available from au-thor X  X  homepage 4 [5] and for SVM we use LIBSVM 5 . FA can be found in Statistics toolbox, and we implement sFA according to [3].

Parameters affect the performance of ILMN L mainly include latent space dimension d , and parameters for HMC, i.e. esp and L .Wefix L = 200 and tune esp to achieve an acceptance rate of between 60% and 95% for each dataset. For d , we try 5, 10, 20 and 30 for each dataset. The maximum iterations iterMax is set to 1000, and the number of and S are also used for MNL and DPMNL, and we tune esp in the same way as ILMNL. Parameters for both FA and sFA are tuned according to [3]. Parameters for L-SVM and R-SVM are tuned under the guidance of LIB SVM. We repeat each experiment 30 times independently, and the results are illustrated in Table 1. 5.3 Accuracy Analysis From Table 1, we can find out that, in most cases, ILMNL outperforms the competitive models with higher accuracy and smaller var iance, and the advantage is more obvious in case of high-dimensional datasets, such as ORLRAWS, GLA-BRA and PEMS-FS. For example, according to dataset PEMS-FS with 138672 features, ssILMNL performs the highest average accuracy of 0.882 when the latent dimension is set to 30, and the average accuracy of L-SVM, R-SVM, MNL-full and DPMNL-full are 0.734, 0.587, algorithm, SVM performs well with high accu racy and small variance, but its perfor-mance is still lower than ILMNL most of time, because ILMNL perform classification in the low-dimensional latent space. When d ealing with low-dimensional data such as Sonar, MNL-full and DPMNL-full perform well. However, when the dimension is very high, their performance get worse.
 We also observe that, sILMNL and ssILMNL perform better than MNL/DPMNL-FA/sFA with the same latent dimension. As we know, MNL/DPMNL-FA/sFA performs dimension reduction and classification sep arately. The superior performance of ILMNL tive than common factor analysis. However, DPMNL could not work well with sFA in most cases, especially when DPMNL learns more than one component. The reason while DPMNL expects a mixture of linear functions.

Most of time, the Semi-Supervised version of ILMNL (ssILMNL) performs better than Supervised ILMNL (sILMNL) with hi gher accuracy. For example, according to dataset Lung-Cancer, ssILMNL get the highest accuracy of 0.994, and sILMNL get the accuracy of 0.954 when the latent dimension number is 30. With other latent dimension number such as 5, 10 and 20, ssILMNL also performs obvious accuracy improvement compared to sILMNL according to dataset Lung-Cancer. This means unlabeled data information can be used effectively in ssI LMNL to improve the accuracy of the ILMNL. ible nonparametric Bayesian framework jointly learning latent variable and Dirichlet mixture of generalized linear model. This model can handle the high-dimension prob-cause ILGLM is a general model framework, then we realize it based on Factor Analysis and MultiNomial Logit model, which results in the Infinite Latent MultiNomial Logit (ILMNL) model as an example of ILGLM. An approximate posterior inference algo-rithm for ILMNL based on Gibbs sampling is also developed. Experiment results on several real-world datasets show that the proposed model is promising to solve high-dimensional data classifica tion problems in latent space.
 Acknowledgements. This work is supported by the Fundamental Research Funds for the Central Universities under Project No. 12lgpy40 and Guangdong Natural Science Foundation under Project No. S2012010010390.

