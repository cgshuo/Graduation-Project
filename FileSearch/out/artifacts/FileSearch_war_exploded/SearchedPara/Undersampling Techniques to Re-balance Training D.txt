 Rankin g a set of doc u ments based on their relevance with respect to a g iven q u ery is a central problem of information retrieval. Traditionally people have been u sin g ( u ns u -pervised) scorin g f u nctions like BM25, Lan gu a g e Models, etc. to accomplish this task. In recent years researchers have started to u se s u pervised learnin g framework to learn a rankin g f u nction where a trainin g example is a q u ery-doc u ment pair, the correspondin g label is the relevance j u d g ement, and the feat u res are meas u rements of vario u sbase rankers (e g . cosine similarity u sin g tf-idf doc u ment vectors). The task of learnin g a rankin g f u nction from this trainin g set is called learnin g to rank (LtR). LtR al g orithms have been st u died extensively over the past few years [14].

Most of the existin g work in LtR has aimed at developin g better al g orithms for learn-in g a rankin g f u nction g iven a trainin g set, while relatively little research has been de-voted to o u t-of-t h e-box en g ineerin g topics s u ch as how to improve the q u ality of the trainin g data, or how to tackle the scalability iss u eforlar g e scale LtR.

The task of developin g an LtR-based IR system can be viewed as a two sta g e process [5], [16]. The first sta g e involves the followin g steps: 1. To p k retrie va l. An i n iti a lretrie va l a ppro a c h is u sed to retrieve k doc u ments from 2. H uman l a belli ng an dfe a t u re extr a ctio n . Relevance j u d g ements for the selected k The followin g steps are then performed in the second sta g e: 3. Le a r n i ng . An LtR al g orithm is u sedtolearnarankin g f u nction f ( x ) from the train-Once the system has been trained, the followin g steps are performed d u rin g real time eval u ation: 1. To p k Retrie va l. For a q u ery g iven by the u ser, the same top-k retrieval is u sed. 2. Fe a t u re Extr a ctio n . Feat u res are extracted for the retrieved doc u ments with respect 3. Applic a tio n of t h eLe a r n ed Model. A relevance score for each doc u ment is g ener-Fi g . 1 depicts the complete scenario.

Some st u dies have been performed on the first sta g e which will be disc u ssedinthe next section. That is, how to develop a better initial retrieval approach. B u t we are inter-ested in yet another aspect which lies somewhere between the first and second sta g es. One of the characteristics of LtR trainin g data is, after applyin g the initial approach to retrieve the top k doc u ments from the entire doc u ment collection, the trainin g data is u s u ally hi g hly imbalanced in the sense that there are very few relevant doc u ments for a q u ery as compared to the n u mber of irrelevant ones. This aspect is shown in Fi g . 2 for some datasets of the most prominent p u blicly available benchmark LtR dataset collection 1 [19].
Why may imbalanced trainin g data be a problem for LtR? Commercial IR systems have to deal with millions or even billions of doc u ments. So a representative trainin g set (fo u nd by the initial retrieval approach mentioned in the step 1 of the first sta g emen-tioned above) is typically very lar g e. It is comp u tationally challen g in g for a learnin g al g orithm to learn a rankin g f u nction from a bi g trainin g set. So we want to keep the trainin g set small, b u t we cannot red u ce the size of the trainin g data blindly beca u se (1) from the machine learnin g literat u re we know that s u fficient amo u nt of trainin g data is needed to learn a g ood hypothesis, and (2) from the information retrieval literat u re we know that it is the relevant doc u ments which contain most of the information in the sense that there are limited n u mber of relevant doc u ments whereas the n u mber of irrel-evant doc u ments for a partic u lar q u ery is very lar g e. So int u itively, we sho u ld retain all the relevant doc u ments beca u se otherwise we co u ld miss important information (abo u t the relevant doc u ments). Hence we pose the followin g q u estion: are all the irrelevant doc u ments which are c u rrently bein gu sed necessary for learnin g a g ood rankin g f u nc-tion? That is, is a smaller s u bset of irrelevant doc u ments s u fficient for learnin g a g ood rankin g f u nction which can distin gu ish between relevant and irrelevant doc u ments? If we find that we can u seam u ch smaller s u bset from these lar g en u mber of irrelevant doc u ments witho u t compromisin g the acc u racy of the learnin g al g orithm, the trainin g time complexity in lar g e scale LtR problem (which is an important iss u e [4], [13, Ch. 7], [14, Ch. 20]) will be si g nificantly red u ced. Aslam et a l. [1] investi g ate different methods for top-k retrieval from a lar g ecorp u s. That is, they st u died techniq u es for g eneratin g a g ood trainin g set from the ori g inal lar g e doc u ment collection. To complement their work, o u r work is in yet another area: all of their methods have the characteristics that the selected doc u ment collection (to be u sed as trainin g set) is u s u ally still hi g hly imbalanced. We, therefore, want to explore whether or not the techniq u es to deal with imbalanced nat u re of trainin g data can have any positive effect.

McDonald et a l. [16] foc u s on the properties of a g ood trainin g set thro ug hextensive empirical st u dy on some lar g e datasets. They consider the size of the sample to be u sed in the top k retrieval sta g e in their experiment (i.e., different val u es of k ), and empirically search for the optimal val u es of k for different tasks and datasets. They u se a sin g le base ranker as the initial retriev al method, and experimented with some val u es of k whichareselectedbasedonprevio u sst u dies and r u les of th u mb. Their concl u sions incl u de: retrieval performance in g eneral increases with increasin g size of trainin g sample (i.e., the val u es of k ) u p to a certain point, after which it sat u rates. It can be noted here that if o u r experiments can establish that the trainin g set size can be si g nificantly red u ced by discardin g many of the irrelevant doc u ments fo u nd by top k retrieval, then the need for research on which is the optimal val u efor k becomes less si g nificant beca u se we can u se lar g eeno ug h k , and after that we can red u ce the trainin g set size by discardin g many of the irrelevant doc u ments.

Dan g et a l. [5] develop an improved initial retrieval method in the sense that it re-trieves more relevant doc u ments than existin g methods like mere BM25. Their method u ses some advanced feat u res like proximity based feat u res [2]. Note that the g oal of depth-k poolin g approach for initial retrieval is also the same as the g oal of their paper, i.e., to increase the n u mber of relevant doc u ments.

We note that altho ug h one of the g oals of the methods u sed in top k retrieval sta g e is to retrieve as many relevant doc u ments as possible, it t u rns o u t that the disparity between the n u mber of relevant and irrelevant doc u ments is still hi g h[3].

Lon g et a l. [15] propose an Active Learnin g framework which selects the example which minimizes expected DCG loss over a trainin g set. The main motivation of active learnin g is to red u ce the lar g e cost associated with man u ally labellin g doc u ments. Some other works s u ch as Donmez et a l. [6], Y u [21] also try to find the examples which, if added to a trainin g set, increase the q u ality of the learned rankin g f u nction. This cate g ory of works which try to improve the q u ality of the trainin g set while keepin g its size small do not differentiate between relevant and irrelevant doc u ments, whereas o u r work is specifically concerned with the importance of irrelevant doc u ments. That is, the trainin g set fo u nd after applyin g their methods may still be hi g hly imbalanced, and th u s eli g ible for applyin g the techniq u es proposed in o u r work.

From the disc u ssion above, we see that altho ug h there are vario u sst u dies on how to improve the q u ality of the trainin g data while limitin g its size, none of them is primarily concerned with the irrelevant doc u ments, in other words, none of them is concerned with the imbalanced nat u re of the trainin g data. Since o u r re-balancin g techniq u es can be applied after applyin g those methods (and before the learnin g phase), this work is distinctive from and complementary to the existin g works. O u r g oal is to red u ce the n u mber of irrelevant doc u ments which comprise the vast ma-jority of the trainin g set, and then to examine the effect of learnin g from the red u ced trainin g set on eval u ation metrics while testin g on a separate (non-red u ced) test set. In order to achieve this g oal we can u tilize the concept of u ndersamplin g techniq u es for imbalanced data from the machine learnin g literat u re [10]. Undersamplin g techniq u es aim at makin g the trainin g set more balanced in terms of the n u mber of instances from each class. In this paper we report findin g s from two approaches: (1) random u ndersam-plin g , and (2) simple deterministic u ndersamplin g . The application of more advanced techniq u es s u ch as Synthetic Minority Oversamplin g [9] is left to f u t u re work.
Since the LtR data are divided by q u eries, we perform u ndersamplin g at the q u ery level. That is, for each q u ery, we retain all the relevant doc u ments, and select a s u bset of irrelevant doc u ments accordin g to some criteria (either randomly or based on a par-tic u lar feat u re val u e) for incl u sion in the trainin g set. We then eval u ate performance on a (non-red u ced) test set to compare pe rformance at different amo u nts of trainin g data.
The workin g strate g y of random u ndersamplin g is simple  X  we randomly select a s u bset of irrelevant doc u ments. For deterministic u ndersmaplin g ,o u r g oal is to retain the most i n for ma ti v e irrelevant instances (i.e., q u ery-doc u ment pairs) so that the learn-in g al g orithm can effectively learn to distin gu ish between relevant and irrelevant doc u -ments with fewer irrelevant doc u ments in the trainin g set. One approach to identifyin g informative irrelevant doc u ments is to u se an informative feat u re from the trainin g set itself, s u ch as the BM25 score. We ass u me that the lower the BM25 score of an irrel-evant doc u ment with respect to a partic u lar q u ery,themorehi g hly informative it is as an irrelevant doc u ment. The reason is, most of the relevant/irrelevant doc u ments have hi g her/lower BM25 scores respectively. So the irrelevant doc u ments with lower BM25 scores sho u ld represent the  X  X r u e characteristics X  of the  X  X rrelevant nat u re X  of a doc u -ment. However, there do exist some irrelevant doc u ments with hi g hBM25scoresand also some relevant doc u ments with low BM25 scores (simply beca u se BM25 is not a panacea to the rankin g problem). 4.1 Experimental Setup As rank learners we u sed two al g orithms: (1) an u pdated version of the RankSVM al g o-rithm 2 [12], which is a pop u lar pairwise LtR al g orithm that formalizes LtR as a problem of binary classification on instance pairs, and solves the problem u sin g S u pport Vector Machines, and (2) LambdaMart al g orithm [20] which is a listwise LtR al g orithm u s-in g Gradient Boostin g framework [7]. We u se three widely u sed eval u ation metrics, namely, NDCG (normalized disco u nted c u m u lative g ain), precision, and MA P (mean avera g e precision) 3 .We u se nine different datasets 4 from a variety of search-related tasks: domain-specific search (Ohs u med), topic distillation (TD2003 and TD2004), homepa g e findin g (H P 2003 and H P 2004), named pa g e findin g (N P 2003 and N P 2004), and g eneral web search (MQ2007 and MQ2008). We make u se of the eval u ation scripts provided by the Letor website 5 . At first we explain the res u lts fo u nd u sin g RankSVM al g orithm. Later we talk abo u t the LambdaMart. 4.2 Random Undersampling The res u lts of random u ndersamplin g techniq u eareshowninFi g . 3. For each dataset, we plot several performance metrics, namely, NDCG@10, precision@10, MA P ,and avera g e of NDCGs (and precisions) from k=1 u pto10,a g ainst the percenta g eofthe ori g inal trainin g set u sed. Each plot starts with a trainin g set which incl u des all the relevant doc u ments and one irrelevant doc u ment per q u ery. S u bseq u ent trainin g sets (whose percenta g es are indicated alon g the x-axis) contain increasin g n u mbers of irrel-evant doc u ments. To prevent random fl u ct u ations d u e to samplin g effects, we perform 10 independent r u ns of each experiment, and report the avera g eres u lt.
In Fi g . 3 we observe that initially (with very few irrelevant doc u ments) performance is poor, as expected. As the n u mber of irrelevant doc u ment increases so does the per-formance. In g eneral, after the initial increase, relatively minor improvements in per-formance are observe d with increases in the n u mber of irrelevant doc u ment (note the restricted interval on the y-axis).

It appears that performance profiles are task or corp u s dependent since similar pat-terns are observed for MQ2007 and MQ2008, for TD2003 and TD2004, for N P 2003 and N P 2004 etc. A close inspection of the g raphs also reveals that for most of the datasets even better performance can sometimes be achieved u sin g as u bset of the train-in g data rather than the entire dataset. 4.3 Subset Selection Based On Feature Values The second approach performs a deterministic selection of the irrelevant doc u ment s u b-sets. It works as follows: for every q u ery, we, as before, add all relevant doc u ments to the trainin g set. We then sort the irrelevant doc u ments in ascendin g order based on a partic u lar feat u re val u e. We then take only the first irrelevant doc u ment (with the minim u m feat u re val u e) and add it to the trainin g set for that q u ery. For s u bseq u ent iter-ations, additional irrelevant doc u ments are added in order of the feat u re val u e. Here we also perform the same proced u re b u t with descendin g order of feat u re val u es in order to validate the motivation behind u sin g the standard (ascendin g ) one.

Fi g s. 4 and 5 show performance plots across the nine datasets for the feat u re-based orderin g in desce n di ng and a sce n di ng order respectively. For the datasets TD, H P ,N P and OHSUMED, BM25 is u sed to sort the doc u ments. The dataset provider does not disclose the feat u res of MQ2007 and MQ2008, so we simply select the first feat u re for sortin g .

We conject u re that the hi g her the BM25 score (or any other partic u larly informa-tive feat u re) of an irrelevant doc u ment, the less information this doc u ment is likely to contain abo u t the  X  X rrelevance nat u re X  of the doc u ments.

For desce n di ng (Fi g . 4) approach, we see the c u rves initially have very low perfor-mance val u es, beca u se the first irrelevant doc u ments bein g added are those that have hi g h BM25 scores and are th u s the least informative as far as the learnin g al g orithm is concerned. The c u rves of all nine datasets then start to rise q u ickly as the trainin g set size increases, beca u se more informative irrelevant doc u ments (with lower BM25 scores) are startin g to be incl u ded in the trainin g set, and th u s it becomes easier for the learnin g al g orithm to learn patterns of the irrelevant doc u ments thereby yieldin g better performance on test set. After incl u din g eno ug h 6 irrelevant doc u ments, the per-formance appears to sat u rate, s u ch that incl u din g additional irrelevant doc u ments after that point does not improve performance si g nificantly. Finally, one exceptional pattern is witnessed in Ohs u med where the performance first g oes down.

We g ainevenf u rther improvement in terms of trainin g time if we u se a sce n di ng order of sortin g as shown in Fi g .5.Usin g the same ar gu ment mentioned above, we expect a sharp rise (may not be noticeable in the plots u nless we ma g nify the y-axis), and then a ro ug hly flat c u rve. The reason is, in the early trainin g sets, only the irrel-evant doc u ments with very low BM25 scores will be incl u ded which are mostly very distinctive irrelevant doc u ment (beca u se BM25 is a g ood base ranker), so addin g hi g hly informative irrelevant doc u ments helps the learnin g al g orithm to learn (and thereby to distin gu ish) the patterns of relevant and irrelevant ones thereby yieldin g better perfor-mance in test time. After that, when we contin u etoincl u de comparatively lar g er BM25 scores, the performance sho u ld still contin u e to increase sli g htly beca u se even tho ug h BM25 is a g ood ranker, it does not win all the time. So there do exist some irrelevant doc u ments which have hi g hBM25scores.Incl u din g these doc u ments in the trainin g set helps the learnin g al g orithm to learn these patterns in addition to the normal patterns (i.e., low BM25 scores) of the irrelevant doc u ments. The Fi g .5showsthato u ranalysis holds for most of the datasets with an exception in MQ2007.

Th u s the findin g s of these two experiments show that indeed the ascendin g approach has some advanta g e over descendin g one in terms of minim u m percenta g e of trainin g set req u ired to achieve g ood performance. 4.4 Direct Comparison between Subsampling Approaches Now we compare performance in terms of MA P and NDCG@10 of the different ap-proaches to s u bsamplin g in Fi g s. 6 and 7 respectively across the nine datasets. As shown previo u sly, the sortin g approach in descendin g order performs poorly in comparison with the other approaches. Then comes the sortin g approach in ascendin g order. The random u ndersamplin g wins almost consistently over the others. We concl u de that in the trainin g set the importance of irrelevant doc u ments with low BM25 scores is ( g en-erally) more than those with hi g hBM25scores,b u t that low BM25 scores alone cannot yield the best performance, rather some hi g h BM25 scores are also necessary d u ethe fact that some irrelevant doc u ment at test time do indeed also have hi g hBM25scores. So a next nat u ral approach wo u ld be to systematically incl u de some low BM25 and a few hi g h BM25 irrelevant doc u ments after sortin g the scores in order to achieve even better performance than random appro ach. We expect better performance beca u se (1) incl u din g only hi g hBM25scoresinthetrainin g set does not do well (cf. initial sta g eof the plots of descendin g sortin g approach), (2) even tho ug h random approach incl u des some of these less i n for ma ti v e (i.e., havin g hi g h BM25 scores) irrelevant doc u ments drawn from a u niform distrib u tion in the trainin g set, their performance is the best, (3) low BM25 scores are g ood (cf. initial sta g es of the plots of acendin g sortin g approach). So why not systematically incl u din g more low than hi g h in the trainin g set? We leave this to f u t u re work.

In Table 1 we analyze the minim u m percenta g e of trainin g set needed to achieve 90%, 95% and 98% performance (in terms of Avera g e NDCG u p to rank 10) of ori g inal trainin g set. A q u estion may arise here that why we do not u se standard statistical si g -nificant tests? The answer is, statistical si g nificant tests are meant to u se to reject a n u ll hypothesis, they cannot be u sed to prove the n u ll hypothesis. So it will not be appro-priate if we, in the context of o u r experiments, say that si n ce w ec ann ot reject t h e nu ll we simply do not have eno ug h evidence to reject the the n u ll hypothesis.

We see from Table 1 that the percenta g e of the ori g inal trainin g set needed to have similar performance to that of the baseline (100% of the trainin g data) varies from task to task. As was seen also in Fi g . 6, this table confirms that the random u ndersamplin g strate g y is the most reliable, followed by the ascendin g orderin g approach.

We note that o u rres u lts are not directly comparable with that of existin g methods (as explained in the Related Work Section) since we investi g ate s u b-samplin g of only irrel-evant doc u ments in post-labellin g phase whereas existin g works foc u s on (1) samplin g both relevant and irrelevant doc u ments, and (2) before labellin g . 4.5 Training Time We h av e s o f a r d i s c u ssed the importance of red u cin g the trainin g set size in order to decrease the time req u ired to learn a rankin g f u nction, b u t have not th u s far reported on s u ch time improvements. In Table 2 we show the time taken to train the models for the TD, N P and H P datasets (where the positive effect of red u ction in trainin g set size is apparent). We see consistent speed-u ps in the ran g e of 3 to 5 times faster learnin g as ares u lt of red u ctions in the n u mber of irrelevant doc u ments present in the trainin g set, indicatin g the efficacy of o u r approach. Fi g . 8 shows that the trainin g time increases linearly with increasin g trainin g set size. The minor fl u ct u ations are possibly beca u se we ran these experiments in a desktop comp u ter where we also r u n other experiments sim u ltaneo u sly. 4.6 Using LambdaMart Algorithm The above res u ltsandanalysesarebasedontheres u lts fo u nd by r u nnin g RankSVM al-g orithm. To be certain abo u t the findin g s, we r u n another al g orithm called LambdaMart [20] which is a state-of-the-art listwise al g orithm. We u sedanopenso u rce implemen-tation 7 of it mentioned in [8]. P lots of three datasets are shown in Fi g . 9. Since the plots are similar to the ones fo u nd from RankSVM, we do not repeat the res u lt analysis. Trainin g time of the LtR al g orithms in lar g e scale LtR task is considered to be an im-portant iss u e [4], [13, Ch. 7], [14, Ch. 20]. Undersamplin g techniq u es decreases the trainin g set size thereby decreasin g trainin g time. Also, smaller trainin g set req u ires smaller feat u re comp u tation time, and some feat u res are comp u tationally costly to cal-c u late [4], [16,17].

O u r work is distinct from and complementary to the existin g works on u sin g a better initial retrieval approach (top k retrieval as mentioned in Section 2) and on selectin g informative samples in the sense that o u r work is applicable a fter u sin g these methods. The existin g works foc u sontheq u ality of the trainin g set, whereas we primarily foc u s on the trainin g set size in the context of necessity of incl u din g lar g en u mber of irrelevant doc u ments. Moreover, o u r methods can also be extended to foc u sontheq u ality of the trainin g set beca u se we can think of u sin g more advanced methods to select an effective s u bset of irrelevant doc u ments to incl u de in the trainin g set.

Based on o u r experiments, the followin g concl u sions can be drawn.  X  O u r findin g ss ugg est that in many cases m u ch less trainin g data (in some cases as  X  Research on determinin g the most effective val u eof k for initial top-k retrieval  X  Applyin g samplin g techniq u es take only linear time in terms of the size of the  X  We have shown that u sin g simple methods for u ndersamplin g , we can achieve sim- X  The improvement in terms of trainin g time may not look very promisin g .B u twe  X  O u r approach is applicable a fter the relevance j u d g ement of the doc u ments are In this work we have shown that the LtR al g orithms can u se a m u ch smaller and more balanced trainin g set than the existin g practice  X  balanced in the sense that we can retain only a fraction of the available irrelevant doc u ments (which comprise the vast majority) from the ori g inal trainin g set prod u ced by an initial retrieval approach. In doin g so, we can red u ce the trainin g time for the LtR al g orithms which is an important factor for lar g e scale learnin g to rank task.

There is scope to work with advanced u ndersamplin g techniq u es so as to achieve better performance with smaller trainin g set. Also, it may be worth to investi g ate why thereismorefl u ct u ation in the performances of some datasets than others. That is, to find o u tsome elite g ro u p of irrelevant doc u ments (if any) in reasonable comp u tational time which, if incl u ded in the trainin g set, ca u ses the performance to be better.
