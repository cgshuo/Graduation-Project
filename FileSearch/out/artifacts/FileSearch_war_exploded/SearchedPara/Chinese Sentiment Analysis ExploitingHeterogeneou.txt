 Sentiment analysis has receiving extensive attentions in recent years [1, 2, 3, 4]. A number of work has been proposed. Earlier work extracts discrete features such as word unigrams, bigrams and trigrams [5, 6], and then feed them into a classifier such as max-entropy (ME), support vector machine (SVM) to achieve sentiment classification [11], which exploit word embeddings as basic inputs, and then construct a deep neural network such as a convolution neural network to extract features automatically.
 sumption is generally accepted in most cases such as English, which have explicit separators between words. However, it is an exception for the Chinese language. In order to align with these work, we usually have a prerequisitive step, word segmentation, to convert original character-based Chinese sentences into word-based. The segmentation is conducted by a well-established segmentor, such as Stanford NLP Tools [12], LTP [13] and THULAC [14].
 tain segmentor, since it complies with a fixed segmentation style of Chinese, and actually there is no uniform standards for Chinese word segmentation. For ex-ample, in the well-known open competition of SIGHAN 2005 [15], there are four different segmentation standards for Chinese word segmentation. There comes a question that which standard is more suitable for Chinese sentiment analysis? However little work has focused on this question.
 tions for Chinese sentiment analysis to our knowledge. We choose three popular segmentation standards, including CTB, PKU and MSR. Figure 1 shows an ex-ample sentence with the three heterogeneous segmentations. As shown in the figure, we find that CTB differs with PKU in the person name, and MSR tends to get longer words. Actually, there are many other differences between the three segmentations, which can be referred to their guidelines for details 1 . ploiting discrete models and the other exploiting neural models. Further, we integrate three heterogeneous word segmentations, to observe whether better performances can be achieved. We conduct experiments on a manually-collected corpus. Experimental results demonstrate that three different standards can lead to different performances. In addition, we find the integrated models can bring better performances for Chinese sentiment analysis under both discrete and neu-ral settings. We discuss these results to show possible reasons of our observation. There has been a lot of work for sentiment analysis [16, 6, 17, 2, 18, 19]. These work largely can be categorized into two types, namely discrete models and neural models. Discrete models explore manually-crafted features, which are de-signed by composing atomic features from internal information such as words in sentences and from external information such as sentiment lexicons manually. Feature engineering is crucial for these models including feature selection. sive feature engineering work. These models require certain word embeddings to convert word sequences into vector sequences, and then build a neural net-work structure to abstract features from vector sequences. Typical work of word embeddings aimed for sentiment analysis includes Duyu Tang et al [11], Yafeng Ren et al [20], and typical neural network structures includes convolution neural works [21], recurrent neural networks [22] and recursive neural network [23]. related work. Most work focuses their interests on building better corpus or fea-tures. For example, Xiaojun Wan proposed a bilingual method [24]. These work generally ignore the influence of word segmentations by exploiting segmentors such as Stanford NLP tools [12], LTP [13] and THULAC [14]. Another line of previous studies that exploits lower-level information to improve the main clas-sification performance [25, 26]. There exists work that directly using Chinese characters as basic units, which is out of our concern [27]. where  X  denotes a negative microblogs, and + denotes a positive microblogs. In this section, we introduce two discrete models based on single segmentation and heterogeneous segmentations, respectively. 3.1 The Baseline Model In this section, we introduce the baseline discrete model for Chinese sentiment analysis, with a single word sequence as input for a weibo sentence. The single word sequence can be CTB, PKU or MSR styles. We can use this baseline model to study the influences of the three different segmentation standards, and to see which one is more suitable for Chinese sentiment analysis.
 microblog, after word segmentation, we obtain an word sequence by w 1 w 2  X  X  X  w n . We then extract discrete features at each position i (1  X  i  X  n ) according to specified feature templates. In this work, we exploit three kinds of features, including unigram, bigram and trigram. At position i , the unigram feature is w i , we get the feature vector f i = { w i ,w i  X  1  X  w i ,w i  X  1  X  w i  X  w i +1 } . that the entire sentence is represented, which we call this process as (sum) pool-ing, in order to have a better understanding in contrast with neural models which we will introduce the next section. The process can be formulates as follows: output scores: where the matrix W o is a parameter of the discrete model, and o is the out-put two-dimensional vector, with one denoting the positive score and the other microblog as positive, otherwise negative. 3.2 The Proposed model based on multiple segmentations In order to support Chinese sentiment analysis with multiple segmentations, we extend the baseline discrete model. The adaption is very easy by simply adding the feature vectors extracted from multiple segmentations. The framework of our proposed model based on multiple segmentations is shown in Figure 3. sequences based on the CTB, PKU and MSR segmentation styles: w ctb 1 w ctb 2 w ctb m , w obtaining the feature vectors ( f ctb , f pku and f msr ) of the three word sequences, respectively. Last, we get the final feature vector f by: And the f is fed into a linear classifier (the same as Formula 2) to compute the sentence X  X  sentiment polarity scores. In this section, we introduce our neural models for Chinese sentiment analysis. In contrast with discrete models, neural models exploit dense real-valued word embeddings as inputs, and abstract features from the input vectors by neural layers. Neural models have two main advantages: (1) First as low-dimensional real-valued features are used, these models avoid feature sparsity problem; (2) Second feature combinations are conducted by neural layers automatically, thus manually-crafted features are no longer required, saving the work of feature engineering. In the following, we first present the baseline neural model with only a single word sequence for each sentence. Then we extend this baseline to adapt multiple segmentation inputs.
 [28] which uses dense vectors to represent features. The dimension of dense vectors is much lower than one-hot vectors. Both of them use neural network to extract dense real-valued features h from microblogs. 4.1 The Baseline Neural Model First, we introduce the baseline neural model of Chinese sentiment analysis. Figure 4 shows the framework. For a given sentence with the word sequence w w 2  X  X  X  w n , we first obtain word embeddings of w i from a lookup matrix E , resulting in e 1 e 2  X  X  X  e n . Then we consider a window size of three to extend the representation at each position i (1  X  i  X  n ), obtaining a sequence of x 1 x 2  X  X  X  x n which is in spirit similar to the trigrams in the baseline discrete model. Formally, x i = e i  X  1  X  e i  X  e i +1 , which is a concatenation of embeddings of previous word, current word and next word.
 to model a microblog, which is able to capture semantic and syntactic informa-tion of the input sentence automatically, and meanwhile avoids the gradient van-ishing and exploding problem during training. The bi-directional LSTM-RNN LSTM-RNN has been widely used in a number of NLP tasks to achieve similar goals[30, 31, 32]. Taking the left-to-right LSTM-RNN as an example, the hidden sequence ( h l 1 h l 2  X  X  X  h l n ) is computed as following: where W l , U l , V l , b l are all the model parameters, and denotes the element-wise Hadamard product. In the equations, h l i is computed by the previous hidden ce is used to deliver the long-term information, which is controlled by three gates, namely input gate ig , output gate og and forget gate fg , respectively. the reverse order, and the corresponding model parameters are W r , U r , V r , b r . After hidden sequences of both directions are computed, we concatenate them at each position one by one, receiving the final hidden vectors h 1 h 2  X  X  X  h n ( h i = h tence features, which project the variable length hidden sequence into a fixed-dimensional vector h . The three pooling functions are namely max,min,avg (av-erage). Concretely, given a hidden sequence ( h 1 h 2  X  X  X  h n ), pooling is executed by and 0 otherwise; for min pooling,  X  min i,j equals 1 only when i = arg min s ( h s,j ), and 0 otherwise, where s  X  [1 ,n ]; for avg pooling, a avg i = 1 n . Correspondingly, we can obtain three sentence vectors h max , h min , h avg by the three pooling meth-ods, respectively. Finally, we use a non-linear feed-forward layer to get the final, the output can be computed by: where the W s , W o and b s are the model parameters, and the  X  denotes the concatenation. 4.2 The Proposed Neural Model with multiple segmentation We follow the method of extending the baseline discrete model into the pro-posed discrete model with multiple segmentations to adapt the baseline neural model being able to receive multiple segmentations. Figure 5 shows the overall framework of our proposed neural model.
 respond to the three segmentation styles, respectively. Each component uses a bi-directional LSTM-RNN to model the input sentences. Similarly, three compo-nents are used to extract dense real-valued features from word sequence of dif-ferent segmentation styles, respectively. Then we project the hidden sequences into a fix-dimensional vectors with the pooling functions to obtain the repre-sentation vectors h ctb ,h pku ,h msr , respectively. Finally, we exploit a non-linear feed-forward layer to combine multiple segmentation sentence vectors for further classification, which is in spirit similar to the baseline model, and the correspond-ing model parameters are W s , W o , b s . The output nodes are computed by: The cross-entropy loss is exploited as our training objective function for both discrete and neural models under supervised learning. Our goal is to minimize l -regularization term, where  X  is the set of model parameters, and the probability of the oracle output y i is denoted by p y i , which is computed using softmax over the output vector o for both discrete and neural models. Online AdaGrad [33] is used to minimize the objective function for models.
 sampling in (-0.01, 0.01), the look-up table E for word embedding are also a model parameter. The values of E are assigned by pre-training on a large-scale segmented corpus. In this work, we use word2vec 2 to pre-train word embeddings on a collected weibo corpus, with the word sequences obtained by our segmentors automatically. 6.1 Experimental Settings We use the data of NLP&amp;CC2014 Share Task 1, which is collected for emotion analysis in weibo texts. The original task aims to determine the emotion of a weibo text and to classify the weibo texts by its emotion category, including anger, disgust, rear, happiness, like, sadness and surprise. In our work, we exploit the data for Chinese sentiment analysis by labeling the weibo texts with emotions of anger, disgust sadness or fear being negative and being positive with emotions of happiness or like. We have a pre-process step to normalize nickname, URL, emoji and hashtag in weibo texts, and further, we use ZPar [34] to segment weibo texts into word sequences with different segmentation styles. The models of ZPar are trained using CTB, PKU and MSR corpora by ourselves. Table 1 shows the corpus statistics. in our discrete and neural models, which are tuned according to the developmen-tal performances. The values of parameters are shown in Table 2, where both of discrete and neural models use the same regularization parameter  X  and initial updating value  X  . The other parameters in neural models, including the dimen-sion size h lstm  X  rnn of LSTM-RNN layers, dimension size of word embeddings e word and the probability of dropout p drop are shown is Table 2 as well. 6.2 Results We study the influences of heterogeneous segmentations on sentiment analysis, in details, by comparing all possible combinations of these segmentations. The results are shown in Figure 6. We can see that different segmentation styles lead to different accuracies in baseline models. Integrating any two styles of segmentation can obtain higher results than the baseline models, and further our final models which integrate three heterogeneous segmentations achieve the best performance.
 mances with both discrete and neural settings, respectively. For the discrete setting, the best model with single-segmentation input can achieve an accuracy of 81.28%, while it is 84.66% under the neural setting. After three heterogeneous segmentations are combined as input, the discrete model obtains an accuracy of 83.50%, which is 2.22% higher than the single-segmentation input. The neural model with heterogeneous segmentations can also outperform the single segmen-tation input baseline, achieving an accuracy of 86.06%, gaining an improvement of 1.4%. Overall, the results show that neural models can achieve better perfor-mances than the discrete models with similar inputs. 6.3 Model Analysis exploiting multi-segmentations. As for the discrete setting, the number of high-frequency features is greatly boosted by multi-segmentations as input. As known, high-frequency features are highly useful for discrete models, since low-frequency features suffer the sparsity problem. Thus it is reasonable that our final discrete model with the heterogeneous segmentations achieves the best performance. sources, which are learnt from unlabeled texts. For the models using single-segmentation as input, only one kinds of embeddings can be used. While for the multiple segmentation input model, we can utilize all three different embeddings, thus our results are consist with the common intuition.
 corpora, the output word sequences inevitably have incorrect segmentations. When these results are fed into the final-classifiers, it can lead to error propaga-tion for final sentiment analysis. While for our proposed models with multiple segmentations as inputs, the problem can be alleviated as segmentations of dif-ferent styles have different error distributions. We investigated the influences of heterogeneous word segmentations for Chinese sentiment analysis, which has been generally ignored in previous work. First, we showed that the segmentation style does affect the sentiment classification performances. A different segmentation style can lead to a gap of 2% of the final performances. Second, we combined the multiple segmentations of different styles under both discrete and neural setting, finding better accuracies can be achieved by such an integration. We thank the anonymous reviewers for their constructive comments, which helped to improve the paper. This study was supported by Natural Science Foundation of Heilongjiang Province under Grant No.F2016036, National Nat-ural Science Foundation of China under Grant No.61170148, and the Returned Scholar Foundation of Heilongjiang Province, respectively.

