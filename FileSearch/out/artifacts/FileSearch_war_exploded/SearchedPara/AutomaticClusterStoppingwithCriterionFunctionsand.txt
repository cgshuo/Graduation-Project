 Word sense and name discrimination are problems in unsupervised learning that seek to cluster the oc-currences of a word (or name) found in multiple con-texts based on their underlying meaning (or iden-tity). The assumption is made that each disco vered cluster will represent a dif ferent sense of a word, or the underlying identity of a person or organization that has an ambiguous name.

Existing approaches to this problem usually re-quire that the number of clusters to be disco vered ( k ) be specified ahead of time. Ho we ver, in most re-alistic settings, the value of k is unkno wn to the user . Here we describe various cluster stopping measur es that are now implemented in SenseClusters (Puran-dare and Pedersen, 2004) that will group N conte xts flations where some number of names of persons, places, or organizations are replaced with a single name to create pseudo or false ambiguities. For ex-ample, in this paper we refer to an example where we have replaced all mentions of Sonia Gandhi and Leonid Kuchma with a single ambiguous name.

Clustering methods are typically either partitional or agglomerati ve. The main dif ference is that ag-glomerati ve methods start with 1 or N clusters and then iterati vely arri ve at a pre X  X pecified number ( k ) of clusters, while partitional methods start by ran-domly dividing the conte xts into k clusters and then iterati vely rearranging the members of the k clusters until the selected criterion function is maximized. In this work we have used K-means clustering, which is a partitional method, and the H 2 criterion func-tion, which is the ratio of within X  X luster similarity (
I 2 ) to between X  X luster similarity ( E 1 ). In word sense or name discrimination, the num-ber of conte xts ( N ) to cluster is usually very lar ge, and considering all possible values of k from 1 ...N would be inef ficient. As the value of k increases, the criterion function will reach a plateau, indicat-ing that dividing the conte xts into more and more clusters does not impro ve the quality of the solution. Thus, we identify an upper bound to k that we refer to as deltaK by finding the point at which the cri-terion function only changes to a small degree as k increases.

According to the H 2 criterion function, the higher its ratio of within X  X luster similarity to between X  cluster similarity , the better the clustering. A lar ge value indicates that the clusters have high internal similarity , and are clearly separated from each other . Intuiti vely then, one solution to selecting k might be to examine the trend of H 2 scores, and look for the smallest k that results in a nearly maximum H 2 value.

Ho we ver, a graph of H 2 values for a clustering of the 2 sense name conflation Sonia Gandhi and Leonid Kuchma as sho wn in Figure 1 (top) reveals the dif ficulties of such an approach. There is a grad-ual curv e in this graph and there is no obvious knee point (i.e., sharp increase) that indicates the appro-priate value of k . 2.1 PK1 The P K 1 measure is based on (Mojena, 1977), which finds clustering solutions for all values of k from 1 ..N , and then determines the mean and stan-dard deviation of the criterion function. Then, a score is computed for each value of k by subtracting the mean from the criterion function, and dividing by the standard deviation. We adapt this technique by using the H 2 criterion function, and limit k from 1 ...del taK : P K 1( k ) = To select a value of k , a threshold must be set. Then, as soon as P K 1( k ) exceeds this threshold, k-1 is selected as the appropriate number of clus-ters. Mojena suggests values of 2.75 to 3.50, but also states the y would need to be adjusted for dif ferent data sets. We have arri ved at an empirically deter -mined value of -0.70, which coincides with the point in the standard normal distrib ution where 75% of the probability mass is associated with values greater than this.

We observ e that the distrib ution of P K 1 scores tends to change with dif ferent data sets, making it hard to apply a single threshold. The graph of the P K 1 scores sho wn in Figure 1 illustrates the dif fi-culty : the slope of these scores is nearly linear , and as such any threshold is a some what arbitrary cutof f. 2.2 PK2 P K 2 is similar to (Hartig an, 1975), in that both tak e the ratio of a criterion function at k and k-1 , in order to assess the relati ve impro vement when increasing the number of clusters.

When this ratio approaches 1, the clustering has reached a plateau, and increasing k will have no benefit. If P K 2 is greater than 1, then we should increase k . We compute the standard deviation of P K 2 and use that to establish a boundary as to what it means to be  X  X lose enough X  to 1 to consider that we have reached a plateau. Thus, P K 2 will select k 2.4 The Gap Statistic SenseClusters includes an adaptation of the Gap Statistic (Tibshirani et al., 2001). It is distinct from the measures PK1, PK2, and PK3 since it does not attempt to directly find a knee point in the graph of a criterion function. Rather , it creates a sample of reference data that represents the observ ed data as if it had no meaningful clusters in it and was sim-ply made up of noise. The criterion function of the reference data is then compared to that of the ob-serv ed data, in order to identify the value of k in the observ ed data that is least lik e noise, and therefore represents the best clustering of the data.

To do this, it generates a null reference distri-bution by sampling from a distrib ution where the mar ginal totals are fix ed to the observ ed mar ginal values. Then some number of replicates of the ref-erence distrib ution are created by sampling from it with replacement, and each of these replicates is clustered just lik e the observ ed data (for successi ve values of k using a given criterion function).
The criterion function scores for the observ ed and reference data are compared, and the point at which the distance between them is greatest is tak en to pro-vide the appropriate value of k . An example of this is seen in Figure 2. The reference distrib ution repre-sents the noise in the observ ed data, so the value of k where the distance between the reference and ob-serv ed data is greatest represents the most effecti ve clustering of the data.

Our adaption of the Gap Statistic allo ws us to use any clustering criterion function to mak e the comparison of the observ ed and reference data, whereas the original formulation is based on using the within X  X luster dispersion. This research is supported by a National Science Foundation Faculty Early CAREER De velopment Award (#0092784).

