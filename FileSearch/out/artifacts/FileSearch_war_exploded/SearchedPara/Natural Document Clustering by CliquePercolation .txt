 Document clustering is an important technique that facilitates the navigation, search and analysis of information in large unstructured text collections. It uses an unsupervised process to identify inherent groupings of similar documents as a set of clusters such that the intra-cluster similarity is maximized and the inter-cluster similarity is minimized.

Generally, clustering has three fundamental issues to solve: a data presenta-tion model, a data similarity measure, and a clustering algorithm that builds the clusters using the data model and the similarity measure. Most existing cluster-ing methods are based on vector space model [1,17] and represent document as a feature vector of unique content-bearing words that occur in the document sets, which is also known as  X  X ag-of-words X  model. Document similarity is calculated using one of the mathematical association measures, such as Euclidean distance, Cosine, Overlap, or Dice coefficients, etc., formulated with the feature vectors. Many clustering models and algorithms h ave been proposed. From different per-spectives, they can be categorized into agglomerative or divisive, hard or fuzzy, deterministic or stochastic [11].

Most existing clustering algorithms optimize criterion functions with respect to the similarity measure in use over all the documents assigned to each poten-tial partition of the collection [11,22]. They always impose some explicit and/or implicit constraints with respect to the number, size, shape and/or disjoint char-acteristics of target clusters. For example, partitional algorithms like k -means assumes the cluster number k and does not allow one document belonging to multiple groups. Although fuzzy clustering, such as fuzzy C -means/medoids al-gorithm [3,13], does support overlapping clusters by the membership function and fuzzifier parameter, they are still confined by cluster number and can find only spherical shape clusters (due to the assumption like k -means that each cluster can be described by a spherica l Gaussian). Some algorithms, e.g. EM (Expectation-Maximization) clustering , are model-based, assuming Naive Bayes or Gaussian Mixture model [2,14]. They strongly presume certain probabilistic distributions of clustered documents and try to find the model that maximizes the likelihood of the data. When data cannot fit the hypothetical distribution, poor cluster quality can result. k -way clustering or bisection algorithms [22] adapt all kinds of criterion functions, but require to specify cluster number and force clusters to be equally sized. Recently, spectral clustering [8,9] based on graph partitioning has emerged as one of the most effective clustering tools, whose criterion functions are based on max-flow/min-cut theorem [5]. However, they prohibit overlapping clusters which ought to be important for document clustering.

In this paper, we define natural document clustering as a problem of finding unknown number of overlapping document groups with varied sizes and arbi-trary data distributions. We try to obtain the clustering results with these free characteristics by removing as many external restrictions as possible and leav-ing things to the inherent grouping nature among documents. For this purpose, we propose a document clustering technique using a novel graph-theoretic algo-rithm, named Clique Percolation Clustering (CPC). The general idea is to iden-tify adjacent maximal complete subgraphs (maximal cliques) in the document similarity graph using a threshold clique. Certain adjacent maximal cliques are then merged to form one of document clusters that can be fully explored by the threshold clique. Although CPC does introduce an explicit parameter k ,which is the size of threshold clique, our algorithm can automatically settle the critical point, at which the natural clustering of the collection can be achieved. We show that CPC can outperform some representative algorithms with experiments on benchmark data.

The rest of this paper is organized as follows: Section 2 gives related work; Sec-tion 3 describes the proposed CPC method and its algorithmic implementation; Section 4 presents experiments and results; Finally, we conclude this paper. 2.1 Graph-Based Document Representation Two types of graph-based representations have been proposed for modeling doc-uments in the context of clustering. One is the graph obtained by computing the pairwise similarities between the documents [9], and the other is obtained by viewing the documents and the terms as a bipartite graph (co-clustering) [8]. Our work use the first model.

In general, suppose V = { d 1 ,d 2 ,...,d | V | } is a collection of documents. We represent the collection by an undirect graph G =( V, E ), where V is the vertex set and E is the edge set such that each edge { i, j } is a set of two adjacent vertices d i ,d j in V . The adjacent matrix M of the graph is defined by where each entry M ij is the edge weight, and w ij is the value of similarity metric (in what follows we assume Cosine coefficient) between d i and d j . The graph can also be unweighted where an edge exists indicating that the distance of its two vertices is smaller than some threshold, in which case M ij is binary.
A clique in G is a subset S  X  V of vertices, such that { i, j } X  E for all distinct { d complete subgraph of G . A clique is said to be maximal if its vertices are not a subset of the vertices of a larger clique. The maximal cliques are considered the strictest definition of a cluster [16]. In graph theory, enumerating all max-imal cliques (equivalently, all maximal independent sets or all minimal vertex covers) is a fundamental combinatorial optimization problem and its worst-case complexity is believed NP-hard [4,21]. 2.2 Graph-Theoretic Clustering Traditional hierarchical agglomerative clustering (HAC) are intrinsically graph-based. HAC treats each data point as a singleton cluster and then successively merges pairs of clusters until all clusters have been merged into a single cluster that contains all documents. Single-link, complete-link and average-link are the most popular HAC algorithms.

In single-link algorithm [19], the similarity between clusters is measured by their most similar members (minimum dissimilarity). Generally, agglomerative process are rather computationally intensive because the minimum of inter-cluster distances must be found at each merging step. For single-link clustering, an efficient implementation of Minimum Spanning Tree (MST) algorithms of a weighted graph is often involved. Therefore, single-link produces clusters that are subgraphs of the MST of the data and are also connected components. It is capable of discovering clusters of varying shapes, but often suffers from the so-called chaining effect. Complete-link [12] measures the similarity between two clusters by their least similar members (maximum dissimilarity). From graph-theoretic perspective, complete-link clusters are non-overlapping cliques and are related to the node colorability of graphs. Complete-link is not vulnerable to chaining effect, but generates excessive compact clusters and is thus very sensi-tive to outliers. Average-link clustering [6] is a compromise between single-link and complete-link: the similarity between one cluster and another is the averaged similarity from any member of one cluster to any member of the other cluster; it is less susceptible to outliers and elongated chains. 3.1 Preliminaries Suppose | V | number of documents are given in a measure space with a similarity metric w ij . We define a binary relation  X  t between documents on G = { V, E } with respect to parameter t : i  X  t j := w ij  X  t , which is self-reflexive, symmetric and non-transitive. There is an edge { i, j } X  E connecting vertices d i and d j whenever i  X  t j with respect to threshold t . Fig. 1 illustrates that given a matrix reflecting the distances between 7 documents and the t value, a series of graphs for the relation i  X  t j are produced with different connectivity densities. Clearly, if each maximal clique were considered as a cohesive form of cluster, we could discover different number of clusters from these graphs, where t =0 . 5, 2 . 5and 3 . 5 results in 7, 5 and 3 number of clusters, respectively. Different from HAC clusters, they are planar and overlapping rather than hierarchical and disjoint. They also display interesting properties of natural clusters except for excessive intra-cluster cohesiveness like complete-link clusters.

The series of graphs parameterized by t above can be seen as random graphs with constant set of vertices and a chan ging set of edges generated with some probability p , the probability that two vertices can be connected by an edge. Intuitively, tuning the value of t is somehow equivalent to adding or removing some edges according to p in a monotonic manner. In order for an appropriate t , we first try to determine p c , the critical value of p , and then derive t given p c by making use of their interdependency relationship. The critical value p c is defined as the probability, under which a giant k -clique percolation cluster will emerge in the graph, and is known as the percolation threshold for a random network [7]. At this threshold, the percolation transition takes place (see Section 3.2). For clustering, the assumption behind is that no cluster can be excessively larger than others by commanding p&lt;p c . 3.2 k -Clique Percolation Concepts. The concept of k -clique percolation for random networks was re-cently studied in biological physics in [7]. Its successful applications for uncover-ing community structure of co-authorship networks, protein networks and word association graphs can be f ound in [15]. Here we briefly describe some related notions.
 Definition 1. k -clique is a complete subgraphs of k vertices.
 Definition 2. k -clique adjacency: Two k -cliques are adjacent if they share k  X  1 vertices, i.e., if they differ only in a single vertex.
 Definition 3. k -clique percolation cluster is a maximal k -clique-connected sub-graph, i.e., it is the union of all k -cliques that are k -clique adjacent. Definition 4. k -clique adjacency graph is a compressed transformation of the original graph, where the vertices denote the k -cliques of the original graph and there is an edge between two vertices if the corresponding k -cliques are adjacent. Moving a particle from one vertex of a k -clique adjacency graph to another along an edge is equivalent to rolling a k -clique template (threshold clique) from one k -clique of the original graph to an adjacent one. A k -clique template can be thought of as an object that is isomorphic to a complete graph of k vertices. It can be placed onto any k -clique of the original graph, and rolled to an adjacent k -clique by relocating one of its vertices and keeping its other k  X  1 vertices fixed. Thus, the k -clique percolation clusters of a graph are all those subgraphs that can be fully explored by rolling a k -clique template in them [7]. Fig. 2 illustrates the effects of one-step rolling of a k -clique template (for k =2 , 3 , 4when t =3 . 5) that produce different topologies of clusters. Note that a k -clique percolation cluster is equivalent with all maximal cliques adjacent by at least k  X  1 vertices. Thus, compared to the strict maximal clique clusters aforementioned (see Section 3.1), the cohesiveness of a k -clique percolation cluster can be adjusted by the k value. In addition, such clusters are connected components on a k -clique adjacency graph that can be discovered with efficient algorithms. The goal of CPC is to find all k -clique percolation clusters.
 Percolation Threshold p c . How to estimate the threshold probability p c of k -clique percolation with respect to k ? Under such p c (critical point), a giant k -clique percolation cluster that is excessively larger than other clusters will take place [10,7]. Intuitively, the greater the value of p ( p&gt;p c ), the more likely the giant cluster appears, and the bigger its size is (which includes most graph nodes), as if using a k -clique can percolate the entire graph.
Consider the heuristic condition of template rolling at the percolation thresh-old: after rolling a k -clique template from a k -clique to an adjacent one by re-locating one of its vertices, the expectation of the number of adjacent k -cliques, where the template can roll further by relocating another of its vertices, be equal to 1. The intuition behind is that a larger expectation value would allow an infi-nite series of bifurcations for the rolling, ensuring that a giant cluster is present in the graph. The expectation value can be estimated as ( k  X  1)( | V | X  k ) p k  X  1 c =1, where ( k  X  1) is the number of template vertices that can be selected for the next relocation, ( | V | X  k ) is the number of potential destinations for this relocation, out of which only the fraction p k  X  1 is acceptable, because each of the new k  X  1 edges must exist in order to reach a new k -clique after relocation. Therefore, we get the percolation threshold function p c ( k ) with respect to k and | V | : For k = 2 in particular, p c (2) = 1 / ( | V | X  2) gives the percolation threshold of 2-clique connectedness (edge connectedness) of the graph, i.e., most graph nodes can be fully explored by a traversal along the edges.
 Generation of Random Graph. By no means, an appropriate graph for clustering can be obtained without prior information regarding the global or local statistics of node connectivity in terms of certain degree distribution. In order to generate such a graph, one commonly specifies a series of hard threshold values of edge weight t , and then determines a good value t c by trial and error. However, its time cost is generally very expensive due to the complexity of graph-theoretic approaches. Thus, t c is usually hard to achieve. The concept of clique percolation provides a fundamental probabilistic formalism for determining the critical point, with which we can estimate t c more directly without prior knowledge on statistics of graph and save the time cost of trial and error.

We examine the co-relation between p and t .Given p c ,weestimatethe bound(s) of t c so that the graph with the equivalent connectivity as that at the percolation threshold can be generated. Because p -t are monotone, an ap-propriate graph for clustering could be obtained using t slightly less than t c . This is to prohibit the emergence of a giant cluster at the critical point. For simplification, we derive the upper bound of t c by an approximation: where w max and w min are the maximum and minimum values of document similarity in the collection, respectively. 3.3 Algorithmic Implementation The clustering process is turned out to be a problem of finding all maximal cliques and then merging those with k  X  1 common nodes into clusters. The proposed CPC method includes 5 major steps: 1. Preprocessing: Eliminate words in the stop list, use Porter X  X  stemmer as 2. Given k as parameter, compute p c ( k ) using (2), and compute t c ( k )using(3). 3. For each entry in matrix A ,if w ij &lt;t c ( k ), then reassign w ij =1,otherwise 4. Enumerate all maximal cliques in G using Algorithm 1. 5. Create a M  X  M adjacent matrix B (where M is the number of maximal Enumerating Maximal Cliques. Algorithms for finding maximal cliques (step 4 above) was studied in [4,21] and achieved processing time bounded by O ( v 2 )and O ( nm X  ) 1 , respectively. Their algorithms are distinctive because they can be applied to a graph of comparatively large size. We implement an efficient counterpart of the algorithm using back-tracking method (see Algorithm 1). A maximal click is output at each end of back-track. Thus the running time is O ( v ). Because v may be exponential with the growth of the number of vertices in worst case, our method is not a polynomial time algorithm either. Algorithm 1. Enumerate All Maximal Cliques Finding k -Clique Percolation Clusters. When all the maximal cliques are enumerated, a clique-clique adjacent mat rix is prepared. It is symmetric where each row (column) represents a maximal clique and matrix values are equal to the number of common vertices between the two cliques (the diagonal entries are the clique sizes). Note that the intersection of two cliques is always a clique with at least k  X  1 (common) nodes. The k -clique percolation clusters are the one-to-one correspondents to the connected components in the clique-clique adjacency graph, which can be obtained using Algorithm 2 (step 5 above). The algorithm first creates a clique-clique adjacent matrix B , in which every off-diagonal entry smaller than k  X  1 and every diagonal element smaller than k are erased (line 2 X 12), and then carrying out a depth-first-search (DFS) to find all the connected components. The resulted connected components are used as indices of maximal cliques for outputting k -clique percolation clusters. 4.1 Data Sets We conduct the performance evaluations based on Reuters-21578 2 corpus, which is popular for document clustering purpose. It contains 21,578 documents that Algorithm 2. Find All k -Clique Percolation Clusters are manually grouped into 135 classes. The number of documents for different clusters is very unbalanced, ranging from 1 to 3,945. Many documents have multiple category labels, and documents in each cluster have a broad scope of contents. In our experiments, we remove the clusters with less than 5 documents. We then extract 9,459 documents with unique class labels to form one of our data sets TS1, and rest of 11,084 documents with multiple class labels form TS2. At last, we result in 51 classes in TS1 and 73 classes in TS2. Table 1 shows the statistics of the original Reuters corpus (ORG) and the two resulted data sets. 4.2 Evaluation Metrics We adopt two quality metrics widely used for document clustering [20], i.e., F-measure and Entropy. The F-measure of a class i is defined as F ( i )= 2 PR P + R . The precision and recall of a cluster j with respect to a class i are defined as: P = P recision ( i, j )= N ij N of members of class i in cluster j , N j is the size of cluster j ,and N i is the size of class i . The overall F-measure of the clustering result is the weighted average of F ( i ): where | i | is the number of documents in class i .
Entropy provides a measure of homogeneity of a cluster. The higher the ho-mogeneity of a cluster, the lower the entropy, and vice versa. For every cluster j in the clustering result, we compute p ij , the probability that a member of cluster j belongs to class i . The entropy of each cluster j is calculated using E tropy for a set of clusters is calculated as the sum of entropies of each cluster weighted by the size of that cluster: where N j is the size of cluster j , m is the number of clusters, and N is the size of document collection. 4.3 Performance Evaluation Experiment 1. Table 2 shows the performance of CPC given the different size of threshold clique. Obviously CPC produces more clusters than the number of categories in the benchmark. This is because Reuters corpus are manually classified according to a set of pre-defined keywords (one for each class). Thus the schema for the categorization is rather unifarious. One document with less discriminative features may belong to more groups and the grouping criterion could be more diverse. CPC is less limited by external constraints, which favors multifarious categorization schemes, and thus has more clusters.

The results on TS2 are better than on the other two data sets in terms of both F-measure and Entropy. Because TS2 contains documents all belonging to multiple classes, we think the better results on it can be attributed to CPC favoring overlapping clusters. We originally expected that the results on ORG would be far and few between the performances on TS1 and TS2, but the worst results of the three are observed on it. One possible reason is that we had pruned the classes with less than 5 documents for producing TS1 and TS2, where fewer small clusters are remained. This may indicate that CPC is disadvantageous in identifying excessively small clusters. We can also observe that the algorithm gives the best results when k = 4. Note that when k = 2, the performance is significantly poorer than other choices of k . This is because at k =2,the procedure of CPC algorithm is degenerated to find connected components, which is regarded as the most relaxed criterion for clustering.
 Experiment 2. In this experiment, we compare CPC with the other three repre-sentative clustering algorithms, k -means, single-link and complete-link. Because it is impossible to command CPC to produce exact number of clusters with the benchmark, we use k = 4 for the threshold clique, which is the optimal solution based on Table 2 and also brings about the closest number of clusters to the benchmark. To make fair comparisons under this condition, we examine every one of k -means, single-link and complete-link twice: one with the same number of clusters as the benchmark, and the other with the same number of clusters as CPC. The results are denoted by KM-b, KM-c, SL-b, SL-c, CL-b and CL-c (suffixes b and c represent benchmark and CPC, respectively). Furthermore, because k -means is well-known to be sensitive to local optima, we repeat the algorithm 50 times with different initializations (initial centroids) and choose the best outcomes achieved. In order to align with the clustering results of CPC and the benchmark, we stop the HAC process of single-link and complete-link when the specified number of clusters are left.

Table 3 shows that CPC outperforms other algorithms on all three test sets irrespective of the cluster number used. When compared with KM-b, SL-b and CL-b, CPC can only produce proximate number of clusters, but performs better on both measurements. This indicates that CPC clustering, although multifar-ious, is still more accurate than other clusterings with exact the same number of clusters as benchmark. When using the same number of clusters as CPC, the results of KM-c, SL-c and CL-c are even worse in some extent. In addition, CPC performs better on TS2 than TS1, and all the remaining algorithms demonstrate an opposite outcome, i.e., the results on TS1 are relatively better than TS2. This testifies the advantages of our method over the partitional algorithms that can only produce disjoint clusters. k -means performs the worst among the three. Its poor performance on TS2 is very obvious because k -means can only produce spherical partitional clus-ters of the corpus. Single-link results in clusters that are connected components and complete-link clusterings are non-overlapping cliques. It is reasonable for complete-link performing better than single-link. The superiority of CPC stems from several main reasons: First, CPC aims to form natural clusters that should be by all means overlapping for Reuters corpus; Second, the cohesiveness of CPC clusters is moderate in comparison with relaxed single-link and restricted complete-link; Third, because CPC does not assume each cluster described by any distribution, it tends to be more flexible and natural than model-based approaches like spherical k -means. We present a novel clustering algorithm CPC by applying clique percolation technique introduced from the area of biological physics. A more generalized framework related to it is the so-called  X  X mall-world network X  describing many kinds of community structures in nature and society, which is extensively studied in random networks [10]. This is the first time for the clique percolation being applied in document clustering. The preliminary results demonstrate it is feasible and promising for document clustering. We are confident that CPC is interesting and worth of further studies. There are still many issues left to be studied more deeply. One is the determination of threshold values of t c according to the per-colation threshold probability p c . So far, the mathematical relationship between them is not exact and clear-cut. To generate an appropriate random graph from p , an alternative is to make use of the degree distribution of graph vertices. For each vertex, some nearest neighbors asso ciated with its degree distribution are considered to produce the connectivity instead of depending on the harsh cut by t c (derived from p c ). This will lead to the further exploration on techniques to analyze complex networks. Furthermore, due to the NP-hardness of maxi-mal clique enumeration algorithms, the CPC method is time-consuming. More efficient maximal cliques enumeration algorithm is required. In the future, we will also compare CPC to more advanced cl ustering algorithms, such as spectral clustering [8,9] and Information Bottleneck method [18].

