 parameterized by w and a loss function  X  ( , ) , the goal is to minimize E its empirical average on a training sample { ( x stochastic optimization problem: conveniently solved even with standard off-the-shelf opti mization packages. and thus may require a large number of iterations.
 popular  X  stochastic composite optimization, and obtained the conve rgence rate of any iterative algorithm solving stochastic (general) conv ex composite optimization. either because of the strong convexity of  X  (e.g., log loss, square loss) or  X  (e.g.,  X  iterations performed by the algorithm has to be fixed in advan ce. problems, which are the best regret bounds currently known f or these problems. First, we recapitulate a few notions in convex analysis. parameter L if, for any x and y , for any x,y and subgradient g ( x )  X   X  X  ( x ) .
 Lemma 2. [14] Let  X  ( x ) be -strongly convex and x  X  = arg min objective function smooth (  X  ( x ) = 0 ), lower bounds the smallest eigenvalue of its Hessian. Recall that in smooth optimization, the gradient update x can be seen as proximal regularization of the linearized f at the current iterate x words, x component, we have the following more general notion. convex and non-smooth, is called the generalized gradient update, and  X  = 1 is also a common construct used in recent stochastic subgrad ient methods [3, 17]. Let G ( x unbiased estimator of the gradient  X  f ( x ) , i.e., E updating of three sequences { x and x quences {  X  of the generalized gradient update y minimization problems.
 Algorithm 1 SAGE (Stochastic Accelerated GradiEnt).

Input: Sequences { L
Initialize: y for t = 0 to N do end for Output y 3.1 Convergence Analysis Define  X  following, we will show that the value of  X  ( y any x . Let  X  the following lemma.
 Lemma 3. For t  X  0 ,  X  ( x ) is quadratically bounded from below as Proposition 1. Assume that for each t  X  0 , k  X  Proof. Define V z applying Lemmas 2 and 3, we obtain that for any x , Then,  X  ( y where the non-positive term  X  hand, by applying Lemma 3 with x = y where the non-positive term  X  multiplying (9) by  X   X  ( y t )  X   X  ( x )  X  (1  X   X  t )[  X  ( y t  X  1 )  X   X  ( x )]  X  where A = h  X  and B . First, by using the update rule of x On the other hand, B can be bounded as where the second equality is due to the update rule of x Schwartz inequality and the boundedness of  X  where the last step is due to the fact that  X  ax 2 + bx  X  b 2 obtain (8). that the triplet ( x also random. Clearly, z where the first equality uses E Corollary 1.
 So far, the choice of L show that with a good choice of L Theorem 1. Assume that E [ k x  X   X  z where b &gt; 0 is a constant. Then the expected error of Algorithm 1 can be bo unded as If  X  were known, we can set b to the optimal choice of setting of  X  Set where  X  bounded as In comparison, FOLOS only converges as O (log( N ) /N ) for strongly convex objectives. 3.2 Remarks down to O (1 /  X  N ).
 estimating the subgradient from a data subset.
 Unlike the AC-SA algorithm in [13], the settings of L  X  y vectors is unlikely to produce a sparse vector. 3.3 Efficient Computation of y The computational efficiency of Algorithm 1 hinges on the effi cient computation of y y ers, including the  X  details. algorithm, shown in Algorithm 2, is similar to the stochasti c version in Algorithm 1. Algorithm 2 SAGE-based Online Learning Algorithm.

Inputs: Sequences { L
Initialize: z loop end loop Lemma 3. Moreover, let  X  Lemma 4. For t &gt; 1 ,  X  k X  f t ( x ) +  X  g ( x ) k  X   X  Q . Then for Algorithm 2, Proof Sketch. Define  X  Similar to the analysis in obtaining (9), we can obtain  X  On the other hand, h  X  t ,x t  X  z t i  X  on using the convexity of k k 2 . Using (20), the inequality (19) becomes On the other hand, by the convexity of  X  Moreover, by using the update rule of x On using (23), it follows from (22) that  X  Inequality (18) then follows immediately by adding this to ( 21). Theorem 3. Assume that = 0 , and k x  X   X  z where a  X  (0 , 1) is a constant. Then the regret of Algorithm 2 can be bounded as Theorem 4. Assume that &gt; 0 , and k x  X   X  z In particular, with a = 1 RCV1 from [19]. We choose the square loss for  X  ( , ) and the  X  thresholding in this case. Moreover, we do not use strong con vexity and so = 0 . We compare the proposed SAGE algorithm (with L of to of data access operations).
 other algorithms (Figures 1(d) and 1(h)). extended to online learning, obtaining the best regret boun ds currently known. Administrative Region under grant 615209. [4] S. Shalev-Shwartz and A. Tewari. Stochastic methods for  X 
