 A significant number of applications on graph require the key relations among a group of query nodes. Given a rela-tional graph such as social network or biochemical interac-tion, an informative subgraph is urgent, which can best ex-plain the relationships among a group of given query nodes. Based on Particle Swarm Optimization (PSO), a new frame-work of SISP (Searching the Informative Subgraph based on PSO) is proposed. SISP contains three key stages. In the initialization stage, a random spreading method is pro-posed, which can effectively guarantee the connectivity of the nodes in each particle; In the calculating stage of fit-ness, a fitness function is designed by incorporating a sign function with the goodness score; In the update stage, the intersection-based particle extension method and rule-based particle compression method are proposed. To evaluate the qualities of returned subgraphs, the appropriate calculat-ing of goodness score is studied. Considering the impor-tance and relevance of a node together, we present the PNR method, which makes the definition of informativeness more reliable and the returned subgraph more satisfying. At last, we present experiments on a real dataset and a synthetic dataset separately. The experimental results confirm that the proposed methods achieve increased accuracy and are efficient for any query set.
 H.3.3 [ INFORMATION SEARCH AND RETRIEVAL ]: Search process Algorithms, Experimentation, Performance Relational graph, informative subgraph, SISP, Preference NodeRank
Due to the rapid development of information science, many real problems are expressed as relational graph models for the reason of further processing. In a graph, one node al-ways stands for a special object and the edges depict some kinds of relationships between these objects, such as the in-teraction in protein networks, the bandwidth or delay time in computer networks and the acquaintance in friend net-works. The problem that the paper is trying to solve focuses on a new application. For a given query set, we try to find a subgraph which can best explain the relationships among the n query nodes.

Generally speaking, the graphs mentioned above have a large scale. Though the large-scale graph can cover vast information, it is quite difficult to be used under some spe-cific conditions. As a concrete example of entity relational graph, Figure 1 exhibits a co-authorship network of scien-tists. The challenge is how to meet the query requirement about the relations between  X  X OLME,P X ,  X  X NEPPEN K X  and  X  X EONG H X . Although all the three scientists are in-cluded in the initial large graph, we still cannot get a clear answer depicting the key relationships among them because of the disturbance of many remote or uncorrelated nodes. In such a condition, a small and compact subgraph composed of a few key paths will be more satisfying than the whole graph.

Suppose the expected scale of the answer is predefined, using the proposed method, we can clearly know what are the most informative paths connecting the three persons from Figure 2, such as the close co-authorship with  X  X OS-VALL,M X , X  X RUSINA,A X , X  X IM,B X , X  X OON,C X , and X  X IN-NHAGEN, P X .
 Formally, the problem of interest is as follows:
Given: an undirected graph G ; a query Q = { q i } ( i = 1 ,...n ), n = | Q | ; and an integer budget b .

Find: a suitably connected subgraph H that (1)contains all the n query nodes; (2)contains at most b other vertices; and (3)maximizes a goodness function g ( H ); In the problem above, b expresses the largest scale that H can have; and the g function determines the quality of specific H . For different applications, g ( H ) can be defined as multiple different forms. For example, Faloutsos and Mc-Curley [1] use the current delivered by H to find an opti-mal subgraph with two query nodes; Hanghang Tong and Faloutsos [2] define g ( H ) as the sum of relevance score for each node of H to find the Center-Piece subgraphs.
By Theorem 1 given in the following, we can see that the proposed problem is NP-complete. So the heuristic method and approximate method are adopted separately to find the target H .

Theorem 1. It is NP-complete to find the target infor-mative subgraph H .

Proof. Specially, we set n to2and b to the number of nodes in G . Then it is obvious that our problem is reduced to computing the maximum weight path between two query vertices. It is well-known that it is NP-complete to deter-mine maximum weight path between two vertices in a graph [24], and thus our problem is NP-complete.

In any case, according to the problem definition, to get a subgraph like Figure 2, two important issues should be considered. One is how to quickly find the target H and the other is how to design the goodness function to let H make sense.

For the first issue, the paper gives two solutions: sampling method and PSO-based SISP. The former, a simple way, can quickly return an approximate subgraph. The latter is a new framework which contains three key stages: particle initial-ization, fitness calculation and particle update. For each stage, some appropriate algorithms are proposed. First, we use a random spreading method that can effectively guar-antee the connectivity of each particle; Second, by a sign function, we design an appropriate fitness function which can well compute the fitness score of a particle with spe-cific size; Third, based on the precondition of connectivity, we propose the intersection-based particle update strategy which makes each particle move towards a better fitness. Additionally, some effective rules are made to quickly com-press the large particles.

For the second issue, considering the importance and rele-vance of a node together, this paper proposes the Preference NodeRank(PNR) method to evaluate the goodness of each node in the large graph and then gives an appropriate de-scription of g ( H ). The experimental results show that the returned subgraph is satisfying.
 The contributions of this work are as follows.
 1. Based on the key idea of PSO, we propose a new frame-2. We use the sampling method to quickly find the target 3. By combining the importance and relevance together, 4. We do experiments on two different datasets. The ex-
The rest of the paper is organized as follows: in Section 2, we review some related works; Section 3 introduces the over-all framework of SISP and then gives the detail discussions for each stage of SISP. In Section 4, the sampling method is proposed. In Section 5, we give the detail description of the goodness measurement. We present several important experimental results in Section 6 and conclude the paper in Section 7.
Along with the frequent application of complex networks, there are increasing search interests in large graph mining such as the frequent substructure discovery [4], graph sum-mary [5], and dense graph mining [6] and so on. The com-munity detection [7] is a similar problem. It aims to find groups of nodes within which connections are dense, but be-tween which connections are sparser. However, as pointed out in[2] , we cannot directly apply community detection to find the target informative subgraph especially when the source queries are remotely related or lie in different com-munities.

As far as we know, there are few works devoted to the research. [1] is an early research for the problem. It exploits a current-flow algorithm to extract an important subgraph for two query nodes. The authors interpret the graph as an electric network and make the subgraph which delivers maximum current between the query nodes as the answer. In [14], the authors extend the current-flow method to multi-relational graphs. However, the number of query nodes in both their papers is restricted to two.
In [2], the authors propose the CEPS method, which can handle more than two query nodes. The method first chooses the central-piece nodes closely connected with most of the query nodes. Then based on the center pieces, key paths are added into the subgraph until the terminal condition is satisfied. To deal with different types of query scenarios, the paper introduces and handles the KsoftAND queries, which include AND and OR queriesasspecialcases. In [15], a steiner tree which connects all the query nodes is viewed as a part of the target subgraph. Then by a prod-uct of the two-step random walk probabilities, some extra good nodes are gradually added until the subgraph reaches the expected scale. Both the methods do some hypothesizes that some nodes are forced to become the members of tar-get subgraph, such as the node of central-piece in [2] and the nodes in steiner tree in [15]. Although these hypothe-sizes simplify the proposed problem, the searching space of solutions is also shrunken, which will decrease the precision of the final returned subgraph.

Considering the shortcomings of methods above, we pro-pose the sampling method and the PSO-based SISP frame-work for searching the informative subgraph. PSO [19] is a computational method that optimizes a problem by iter-atively trying to improve a candidate solution with regard to a given measure of quality. PSO has an adequate biol-ogy background [20], with easy understanding and realiza-tion. Due to the excellent global search capability on the nonlinear and multi-modal problems, PSO is widely used in scientific research and engineering practice [21, 22]. As far as we know, PSO has not been used in the area of subgraph searching.

The problem of finding important nodes and entity rank-ing [8, 9] in a large graph are also related with our work. In [10], the authors propose a simple way to measure the graph entropy and a systematic way to find important nodes based on their effect on graph entropy. The approach of [11] views an undirected graph as a Euclidean space and depicts the connection of two nodes using commute time distance. Be-sides the works above, there are still many methods which calculate the importance of each node based on the connec-tion structures. Typically, PageRank [3] , Sim-Rank [12], and HITs [13] are popular choices. By combining the impor-tance and relevance of a node together, we further propose the Preference NodeRank method which makes the returned subgraph more meaningful.
According to the definition of target subgraph, we can reconsider the problem as finding a optimal solution of H which maximizes g ( H ) and meets the two constraints: (1) H is connected; and (2) H contains all query nodes and at most b other nodes.
 Based on the basic idea of PSO, the new framework of SISP is proposed in this section. SISP contains three key stages and the particles are the basic operating units. In the background of graph, each particle is viewed as a subset of G and each dimension of the particle stands for a specific node. We will discuss how to select the best nodes in detail in Section 3.2.

SISP starts from initializing m particles, X 1 , X 2 , ... , X Then, by a specific fitness function, we can find the best par-ticle X gbest which has the maximum fitness score. At last, we iteratively update the m particles. Each loop contains two operations. First, every particle is updated by the guid-ance of X gbest . And second the fitness of each new particle is re-computed. When the loop is over, the final X gbest will be the optimal solution. The overview of SISP is shown as algorithm 1.
 Algorithm 1 The SISP framework for searching informa-tive subgraph Input: the initial graph G , the query Q , the budget b Output: the target subgraph H 1: Initialize m particles X 1 , X 2 , ... , X m , and each particle 2: for each X i do 3: Compute f ( X i ); \\ f ( X i ) is the fitness value of X 4: end for 5: X gbest =argmax f ( X i );. 6: while (!(terminal conditions)) do 7: for each X i do 8: X i = Update ( X i , X gbest ); 9: Compute f ( X i ); 10: end for 11: X gbest =argmax f ( X i ); 12: end while 13: return X gbest ;
Generally speaking, the loop can be terminated either when the number of loop exceeds certain given threshold or when X gbest has changed little within multiple loops. In practice, the two conditions are used simultaneously to bet-ter control the loop.

PSO is a kind of evolutionary algorithm, so the returned subgraph may not be the optimal one. In fact, a sub-prime solution is also acceptable to users. For example, if the min-imum amount of nodes connected with all the query nodes is still bigger than b , according to the problem definition the optimal solution does not exist.

The following sections will give detail discussions for ini-tialization, fitness calculation and update strategy separately. In SISP, the first step is to initialize a group of particles. Essentially the initialization is a process of finding some ran-dom solutions of the proposed problem. For different prob-lems, the particles can be initialized by different rules. In the paper, each particle is viewed as a subset of G .Fol-lowing the constraints of connectivity and scale, the most direct way is to initialize each particle as a group of nodes which are connected with Q and its size is not more than b . By this way, the generated particles meet with all the constraints. However, this initialization costs a lot of time. And the greater disadvantage is that the activity space of each particle is confined to a limit small range, which may make the searching process stay locally optimum.

Therefore, we loosen the constraints and make each par-ticle fit only one condition. The simple way is to select randomly b nodes from G as a particle. This kind of ini-tialization is fast. However, many isolated nodes will be generated, to make difficult construction of fitness function and update of particles. So taking no account of the di-mension, we initialize each particle as a group of connected nodes. For an arbitrary particle X i , we first add a random query node of Q into X i , and continue to add the neighbors of X i into X i until all the nodes in X i have been connected with all the query nodes. Figure 3 shows a sample initializa-tion. Starting from a random query node, such as q 1 (Figure 3(a)), the algorithm finds all the neighbor nodes of q 1 and adds a random node into X i ,suchas X i = { q 1 ,2 } in Figure 3(b); Based on the current X i , we repeat the finding and adding until X i is connected with all the n query nodes. In Figure 3(c), the final form of X i is Xi = { q 1 ,2,3,10,11,6, q
The only drawback of random spreading is that the scale of each initial particle is not controlled. Namely a particle can be initialized with any dimension. Actually, this draw-back will not cause tremendous effects. For the particles beyond the scale, their dimensions can be adjusted gradu-ally in the update stage, which will be discussed in detail in Section 3.4.
 In the paper, the vector is used to restore each particle. The pseudo code of initialization is shown as algorithm 2. Algorithm 2 Initialization of particles Input: the initial graph G , the query vector Q and par-Output: X = { X 1 , X 2 , ... , X m } 1: for int i =1 to i = m do 2: Initialize two vectors as X i = null, neighbor i = null; 3: Add a random node of Q into X i ; 4: while (!( X i contains all the query nodes in Q )) do 5: neighbor i = getNeighbor ( X i ); //get the neighbor 6: Node tempn = getRandomNode ( neighbor i ); 7: if (! ( X i contains tempn )) then 8: Add tempn into X i ; 9: end if 10: end while 11: end for 12: return X 1 , X 2 ,. . . , X m
Given all that, the initial particles have the following char-acter.

Character 1. Every initial particle is a connected sub-graph and contains all the n query nodes.

To speed up algorithm 2, a boolean array is created for each particle. In the array, the subscript is used as the iden-tifier of each node. Each element of the array is initialized as false, and when a node is added into the particle, the corresponding position of the array is set to true. By doing this, it is directly judged whether a node has been used or not, and thus the scanning count of X i decreases.
To get X gbext in every loop, a proper fitness function is needed. According to the definition of target informative subgraph, the fitness of a particle X i should be considered from the connectivity with Q , the dimension and the good-ness of X i together.

Throughout the whole framework of SISP, there are two places where the constituent of a particle is changed, the initialization and the update. According to Character 1, we have known that the nodes in initial particles are connected with Q . And faithfully speaking, the update strategy de-scribed in Section 3.4 can still keep the connectivity. So in the whole searching process we are always concentrating on the particles which are actually connected subgraphs. Given all that, the factor of connectivity can be ignored when cal-culating the fitness for each particle.

For a given query vector Q , suppose the normalized good-ness score of g ( j, Q )foreach j  X  H has been pre-calculated, we use formula 1 to calculate the fitness of each particle. where g ( X i ) is the goodness of X i , which is calculated as formula 2; sign ( X i ) is a sign function, which is calculated as formula 3. where d ( X i ) is the dimension of X i .

The random spreading-based initialization makes all the query nodes members of each particle. So the difference between the d ( X i )and( b + n ) will determine whether a particle meets the expected scale or not. The fitness function of formula 1 will generate different ranges for the two kinds of particles. For example, Figure 4 shows a statistical curve distribution for b =9and n =3.

By Figure 4, the designed fitness function has the follow-ing characters.

Character 2. 0.5 &lt; f( X i )  X  1,foreach d ( X i )  X  ( b + n ) ; 0 &gt; f( X j ). Figure 4: Overall fitness distribution related with particle dimension
Proof. 0 &lt;g ( X i )  X  1, 0 &lt;g ( X i )/2  X  0.5. If d ( X then 0 &lt;sign () g ( X i )/2  X  0.5, 0.5 &lt; f( X i )  X  n ), then -0.5  X  sign () g ( X i )/2 &lt; 0, 0  X  f ( X i ) &lt; 0.5. n ).

Character 2 ensures that the X gbest will be generated from the particles whose dimensions are close to b + n .
In each loop, according to the formula 1, we can get a specific X gbest . This section proposes an effective update strategy to make each particle move towards X gbest in next loop.

Different from the basic PSO, the nodes in a particle are co-dependent in our problem. Namely the value of each di-mension is greatly related with other dimensions. So we cannot update the value of each dimension one by one. A better update strategy is to consider each particle as an en-semble unit.

On the whole, the update should follow the following two principles. First, every X i should be move towards X gbest to improve the similarity of contents between them; Second, the scale of X i is extended or compressed to fit the dimension
Given a specific X gbest , for a random particle X i ,the intersection-based extension way is adopted to guarantee the content similarity which is related with the number of common nodes between them. Figure 5 shows the exten-sion process of a particle with 9 dimensions. For the given X (Figure 5(a)) and X gbest (Figure 5(b)), we first get the intersection set Iset = { q 1 ,4,10, q 2 } (Figure 5(c)). Then we find the neighbor set of Iset from G , and add a random node into X i , such as the node labeled as 22 in Figure 5(d). In each update, the number of nodes which can be added into X i can be freely set. Generally speaking, the users will not expect a large returned subgraph, so a slow expansion is effective. In the paper, we add only one node into X i each update. The extension makes X i more similar to X best to some degree.

Meanwhile, the dimension of X i is expected to approxi-mate to X gbest as possible. For the scale constraint, there are two situations. If the dimension of X i is smaller than b + n , X i can be extended as Figure 5, and if bigger some nodes in X i should be removed to reduce the scale. Figure 5: Particle extension based on intersection
The principle of removal is that the rest nodes in X i are still connected. It is necessary to judge whether the connec-tivity of X i is broken when its node is removed. However, when the given X i is relatively large, the calculating of con-nectivity is time-consuming. So some simple rules are set to control the removing.

For a random node j in particle X i ,itcanberemovedif (1)the degree of j is 1, or (2)the degree of j is 2 and the two adjacent nodes of j also have an edge.

Take the particle of Figure 5(d) as an example, the nodes 2 and 14 can be removed by the first rule; the node of 3 can be removed by the second rule. Because the rules are only aimed at the nodes whose degrees are 1 and part of nodes whose degrees are 2, we cannot identify all the nodes which can be removed. However, the compression is a cy-cling program. When some nodes are removed, some other satisfying the rules will appear. So in practical, these sim-ple rules can achieve good performance. Noticeability the compression of particle begin with the node with the lowest goodness score, which can effectively guarantee the quality of the compressed particles.

Combined with extension and compression, the pseudo code of update is shown as algorithm 3. The compress () is a compression function whose pseudo code is shown as algorithm 4.
 Algorithm 3 The update for particle X i Output: X i 1: Compute the intersection set between X i and X gbest as 2: Nv = getNeighbor ( Iset ); 3: extendnode = getRadomNode ( Nv ); 4: X i = X i .add( extendnode ); 5: if ( d ( X i ) &gt; ( b + n )) then 6: X i = compress ( X i ) 7: end if 8: return X i
According to the initialization, all the query nodes are put into the particles. So the intersection set Iset in algorithm 3 always is non-empty (In the worst case, the intersection set still contains all the n query nodes.). Importantly, the Algorithm 4 Algorithm for compression () Input: X i Output: X i 1: for int i =1 to i =( d ( X i )  X  ( b + n )) do 2: Select the node with minimum goodness as 3: if ( judgeDelByRules ( currentnode )) then 4: X i . remove ( currentnode ) 5: end if 6: end for 7: return X i non-empty character ensures the possibility of extension of X Through the analysis above we get the third character.
Character 3. For any given X i , the nodes in X i are still connected even when X i is updated.

Character 3 guarantees the constraint of connectivity, which makes the definition of fitness function shown as formula 1 sense.
The most direct way of finding H is to traverse all the G  X  X  possible subsets (the query nodes are not included) whose sizes are not more than b . For each such subset, by judging the connectivity with Q and calculating the goodness score of g ( H ), we can find the final target subgraph. Undoubt-edly, this searching way can fully guarantee the precision of returned H . However, when the graph is large, the searching space is so huge that we cannot suffer it. So as an approxi-mation, the sampling method is used. Each time, we sample a random subgraph H i of G which meets the following two constrains: (1)the nodes in H i are connected with Q; and (2)the size of H i is not bigger than b .Forthe N sampled subgraphs, the one with maximal goodness will be finally returned. The pseudo code is shown as algorithm 5. Algorithm 5 The sampling method for searching informa-tive subgraph Input: the initial graph G , the query Q , the budget b Output: the target subgraph H 1: Initialize H = null, maxscore =0; 2: for i =1to i =N do 3: H i = getRandomH ( G, Q ); \\ get a random H which 4: if (thenodenumberof H i is not bigger than b ) then 5: if g ( H i ) &gt; maxscore then 6: maxscore = g ( H ); 7: H = H i ; 8: end if 9: end if 10: end for 11: return H ;
When the sampling times N is big enough, [23] gives the accuracy assurance of the the returned subgraph.

Theorem 2. For arbitrary  X  (0 &lt; X &lt; 1) and  X  (  X &gt; 0), 1- X  .

In Theorem 2, E (  X  ( H )) stands for the expected value of maximum goodness score.

In application, we can adjust the parameter of  X  and  X  to balance the quality and time cost.
A natural way to measure the goodness of H is to measure the goodness of the nodes it contains. Similar to [2], the goodness of H is defined as formula 4. where g ( j, Q ) is the goodness score of a given node j with regard to the query set Q .

According to formula 4, the most informative subgraph, which maximizes g ( H ), is chosen by formula 5.

So the key problem of getting H  X  is how to calculate g ( j, Q ). Let g ( j, q i ) be the goodness score of a given node j with regard to q i , then the paper calculates g ( j, Q )asthe sum of g ( j, q i )foreach q i  X  Q . Now, the problem is how to calculate g ( j, q i ). In this section, the effective Preference NodeRank(PNR) is proposed to calculate such goodness for each ( j, q i ) pair. PNR considers g ( j, q i ) from two factors. First, the importance of each node in a large graph is differ-ent and the importance of j can greatly affect g ( j, q i ond, g ( j, q i ) can also be affected by the relevance between j and q i .

The link structures are useful resources in evaluating the importance of each node in a large graph. So utilizing the links in the given graph G , we adopt the PageRank [3] to cal-culate the importance of each node. The concrete is shown as formula 6.
 where NR ( j ) is the importance of node j , B ( j )isnodeset of the outlinks of j , N ( u )isthesetofnodeswhichnode u links to, and  X  is a damping factor set between 0 and 1. In the undirected graph, the outlinks are equal to inlinks. The importance by formula 6 is query-independent, which cannot well reflect the goodness of j with regard to specific q . So PNR further incorporates the relevance between j and q i into NR ( j ).

The path information is used to evaluate the relevance of a node. We assume that sufficient short paths between j and q i can express a good relevance of j with regard to q Formally, the relevance is calculated as formula 7. where P ( j, q i ) is the set of paths connecting j and q a damping parameter for adjusting the influence of each path on r ( j, q i ). In fact, a long path contributes little to r ( j, q Therefore, to reduce the computation cost we abandon the paths whose lengths are bigger than k . According to the real scale of graph, k can be assigned to different values. In the paper, we set k as 4. Incorporating r ( j, q i )into NR ( j ), we get the final calculation of g ( j, q i ) as formula 8. g ( j, q i )= NR ( j ) q =(1  X   X  ) P ( j ) q +  X  where P ( j ) q and P ( j | u ) are similar to [26] and defined as formula 9 and formula 10.

Now we explain how the two formulas of 9 and 10 affect the node ranking. Actually, formula 6 can be considered as a random walk [16, 17]. Starting from a specific node j ,the surfer can walk along a path or jump to a new node. P ( j ) the probability that the surfer jumps to j . So by formula 9, the surfer tends to jump to the nodes which are relevant to q . Similar, P ( j | u ) is the probability that the surfer transits to node j giventhatitisonnode u . So by formula 10, the surfer tends to follow the nodes relevant to q i .
PNR is an iterative algorithm. Suppose the initial rank value of each node is 1/ | G | . We can get the final goodness score for each node by enough iterations. To reduce the on-line response time, we store the ranking lists for as many nodes as possible. Then when a new query is coming, we can get the score of g ( j, Q ) by directly adding each g ( j, q Additionally, all the scores are normalized for operation con-venience. In this section, we demonstrate some experimental results. The experiments are designed to answer the following ques-tions. (1) Does the proposed goodness criterion based on PNR make sense? (2) Do the methods of SISP and sampling capture the optimal target subgraph? (3) How do the parameters balance the quality and re-sponse time? (1)Datasets. We use two datasets to evaluate the pro-posed method. Firstly, we use the netscience dataset to test the quality of the returned subgraph. The network is compiled from the bibliographies of several review articles. Here each author is denoted as a node in G and the edge is added when at least one co-authored paper exists between the corresponding two authors. On the whole, the network has 1,589 nodes and 2,742 edges.

Besides the netscience dataset, we also use a synthetic data set to test the scalability. To make the synthetic data more realistic, we generate a scale-free graph by the BA[25] model. Totally, the synthetic dataset contains 10k nodes and 35k edges. (2)Source Queries. To test the proposed algorithm, we select several persons from different communities to compose the source-query repository. From the real dataset, we select 50 famous persons who come from different areas such as network, physics, intelligence and informatics. The query size is restricted to 2 to 10. Namely a query can be composed of 2 to 10 arbitrary persons from the query repository. (3) Baseline method. This paper compares the experimen-tal results with the MING proposed by Gjergji Kasneci[15] in CIKM 2009. The authors first construct a steiner-tree that closely interconnects all nodes in Q and label the nodes in steiner tree as  X + X . Then for each node in the candidate graph, the authors classify it as  X + X  or  X - X  by a product of the two-step random walk probabilities. At last, within a limited size, the subgraph with maximal probability of  X + X  is returned. (4) Software architecture. Our system was implemented in the Java programming language, and it runs on a Pentium class machine with a 2.0GHz processor. User can submit a group of queries, and then the system will initialize a spec-ified number of particles. By multiple update of particles, the system will finally return a connected subgraph to user. The Java library of GraphStream[18] is used to deal with all the basic operations about the graph. Figure 6: The informative subgraph for Q = {  X  X HERAULAZ,G X ,  X  X AUTRAIS,J X  } .

Using the real dataset, we exhibit some running cases to prove the effectiveness of the proposed method. By three methods, Figure 6 shows the different connection subgraphs with budget 4 for the query nodes  X  X HERAULAZ,G X  and  X  X AUTRAIS,J X . Both the two persons are working in the fields of swarm intelligence in natural and artificial systems and self-organization in biological systems.

We can see that the returned subgraphs are composed of different persons. Figure 6(a) shows close connections among 6 persons by MING. However, the connections pre-sented by Figure 6(b) and 6(c) are obviously stronger than Figure 6(a). In fact, all the five persons X  X UHL,J X , X  X OLE,R X ,  X  X UNTZ,P X ,  X  X ALVERDE,S X  and  X  DENEUBOURG,J X  in Figure 6(b) and 6(c), contribute to investigation in relative industry with self-organization and there are much coopera-tion in paper among them. So as we expected, the returned subgraphs by proposed methods are satisfactory.

Now consider a more complicated query requirement. We want to know the key relationships between the 4 differ-ent persons,  X  X ARK,Y X ,  X  X OH,K X ,  X  X AWRENCE,S X  and  X  X OPCROFT,J X . By resumes, we can see that the 4 per-sons come from largely disjoint communities. And without the returned subgraph, it would be difficult to find the key paths between them.

Running the three algorithms MING, sampling and SISP, we get the results shown as Figure 7(a), Figure 7(b) and Figure 7(c) separately. Compared with Figure 7(a), the subgraphs by Sampling and SISP are more compact. For example, In Figure 7(b) and 7(c) the person  X  X H,E X  has close relations with  X  X OH,K X , they cooperate in many pa-pers about structure and evolution of online social relation-ships; in Figure 7(c), the person  X  X ALLAWAY,D X  has close relations with  X  X OPCROFT,J X  and  X  X EWMAN,M X , they have cooperated in many fields such as the network evolu-tion, data mining and so on.
To further test the performance of the two proposed meth-ods, we use the evaluation criterion of NRatio [2] to exhibit the quality of the retrieved subgraph. NRation expresses how many important/good nodes are captured by g ( H )and it is calculated by formula 11.

To speed up the response time, we do all the following experiments on a candidate subgraph C of G . C contains many key connections among the query nodes. Expanding the query nodes by heuristics, we adopt the similar method of [1] to realize the candidate generation. Then the extracted candidate subgraph is treated as the full graph for the algo-rithms of sampling, SISP and MING.

For different b and n ,wecompare NRatio with the base-line method. The default values of  X  ,  X  and particle number m are to 0.1, 0.1 and 60. Utilizing the netscience dataset, we show the the mean NRatio vs. b as Figure 8(a) and the mean NRatio vs. n as Figure 8(b).

In both cases, the proposed methods capture most of im-portant nodes by any number of budget b or n . For example, when b is 40, SISP captures 95% important nodes on aver-age; when n is 4, SISP captures 85% important nodes on average. From Figure 8(a) we can see that the subgraph captures higher NRatio when the budget b increases. That is because a bigger b makes more nodes with high good-ness scores be added into H . So the ratio of good nodes in H is also increasing; From Figure 8(b) we can see that the subgraph with more source queries captures higher NRatio than those with less ones. The explanation is that the good-ness score of each node becomes more skewed by increasing the number of source queries. So the extracted subgraph contains more important nodes with high goodness scores.
Besides NRatio , we also report the response time of MING and SISP on the dataset with different scales. We do this experiment on the synthetic dataset. For 5 different scales, the average response time of SISP and MING are reported as Figure 9(a). When b is 30, we also record the response time for the queries with different sizes. The experimental results are shown as Figure 9(b). From the both figures, we can see that the proposed SISP has obviously lower re-sponse time. Additionally, in SISP, the size of a query has not a great effect on the response time, which is better than MING.

For SISP, we study the impact of m on NRatio and re-sponse time in the synthetic data. From Figure 10(a) we can see that NRatio grows rapidly when m increases from 20 to 60 and converges for bigger m . It is because that more particles can greatly extend the searching space of the op-timal solution. So the number of captured good nodes is increasing. When the solution space has been covered by multiple particles, the continuous growth of m cannot make more good nodes involved. So the curve is convergent when m is greater than 60.

To find out the time cost of each step for different m ,we report the response time by 3 curves shown as Figure 10(b). The amount of time spent on each stage is increasing along with the increase of m . We can see that the main time consumption is in the stage of initialization. Given all that, for a large graph with 10k nodes, the informative subgraph can be returned in 5 seconds when m is less than 60.
In the sampling method, the quality of returned subgraph depends on the sampling times. According to Theorem 2, the smaller  X  is, the better quality is, but the longer response time becomes. The balance between quality and sampling times for different node scales is shown as Figure 11(a). We Figure 11: the balance between sampling times and quality can see that NRation grows along with the increase of sam-pling times for each dataset. The bigger the dataset, the more the sampling times are needed to get a high NRation .
When b is 30, the balance between quality and sampling times for different query size is shown as Figure 11(b). We can see that H captures higher NRatio when the query size n is increasing. This can be explained similar to figure 8(b). Bigger query size makes H involve more important nodes, so the corresponding convergence velocity is faster than that of smaller one.
We have proposed the sampling and SISP for solving the problem of searching the informative subgraph. Every key stage in SISP is discussed and some efficient algorithms are proposed. In initialization, we use a random spread-ing method, which can effectively guarantee the connectiv-ity of each particle; In fitness calculation, the designed fit-ness function, the goodness and dimension considered, can well reflect the quality of each particle.; In update stage, we propose the intersection-based particle extension and rule-based particle compression method. To make the returned subgraph satisfactory, we further present the PNR method which considers the importance and relevance of a node to-gether
The experimental results prove that the proposed methods achieve increased accuracy and are efficient for any query set.

For the future work, we will continue our research by op-timizing the initialization to decrease time cost.We will also use the SISP framework to solve other problems related with graph searching such as community detection, frequent sub-graph discovery and so on. This research is supported by the National Basic Research Program of China (2011CB302206), National Natural Sci-ence Foundation of China (61025007, 60933001, 61073063) and Fundamental Research Funds for the Central Universi-ties (Grant No. N090304007). [1] C. Faloutsos, K. S. McCurley, and A. Tomkins. Fast [2] H. Tong, C. Faloutsos. Center-piece subgraphs: [3] L. Page, S. Brin, R. Motwani, T. Winograd. The [4] D. J. Cook, L. B. Holder. Substructure discovery using [5] S. Navlakha, R. Rastogi, N. Shrivastava. Graph [6] X. L. Li, S. H. Tan, C. S. Foo, S. K. Ng. Interaction [7] N. Du, B. Wu, X. Pei, B. Wang, L.Xu. Community [8] A. M. Vercoustre, J. A. Thom, J. Pehcevski. Entity [9] J. P. Jiang, W. Lu, X.Q. Rong, Y. Y. Gao. Adapting [10] J. Shetty, J. Adibi. Discovering important nodes [11] S. Vast ,P. Dupont ,Y. Deville, P. S. Barbe. [12] G. Jeh, J. Widom. Simrank: A measure of structural [13] J. M. Kleinberg. Authoritative sources in a [14] C. Ramakrishnan, W. Milnor, M. Perry, and A. Sheth. [15] G. Kasneci, S. Elbassuoni, G. Weikum. MING:Ming [16] H. Tong, C. Faloutsos, J. Pan. Fast Random Walk [17] A. Agarwal, S. Chakrabarti. Learning random walks [18] http://graphstream.sourceforge.net/ [19] J. Kennedy, R. Eberhart. Particle swarm optimization. [20] S. Garnier, J. Gautrais, G. Theraulaz. The biological [21] R. Eberhart, Y. Shi. Particle swarm optimization: [22] K. Parsopoulos, M. Vrahatis. Recent approaches to [23] D. Angluin ,L.G. Valiznt. Fast probabilistic algorithms [24] M. R. Garey, D. S. Johnson. Computers and [25] A. Barablcsi, R.Albert. Statistical mechanics of [26] M. Richardson, P. Domingos. The intelligent
