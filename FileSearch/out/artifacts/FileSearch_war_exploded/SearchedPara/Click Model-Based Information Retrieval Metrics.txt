 In recent years many models have been proposed that are aimed at predicting clicks of web search users. In addition, some information retrieval evaluation metrics have been built on top of a user model. In this paper we bring these two directions together and propose a common approach to con-verting any click model into an evaluation metric. We then put the resulting model-based metrics as well as traditional metrics (like DCG or Precision) into a common evaluation framework and compare them along a number of dimensions.
One of the dimensions we are particularly interested in is the agreement between o  X  ine and online experimental out-comes. It is widely believed, especially in an industrial set-ting, that online A/B-testing and interleaving experiments are generally better at capturing system quality than o  X  -line measurements. We show that o  X  ine metrics that are based on click models are more strongly correlated with on-line experimental outcomes than traditional o  X  ine metrics, especially in situations when we have incomplete relevance judgements.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Human Factors, Verification Click models, evaluation, information retrieval measures, user behavior
There are currently two orthogonal approaches to evalu-ating the quality of ranking systems. The first approach is usually called the Cranfield approach [17] and is done o  X  -line . It uses a fixed set of queries and documents judged by trained people (assessors). Ranking systems are then evalu-ated by comparing how good their ranked lists are X  X mong other things, a system is expected to place relevant docu-ments higher than irrelevant ones.

Another approach described by Kohavi et al. [28] makes use of real online users by assigning some portion of the users to test groups (also called flights ). The simplest vari-ant, called A/B-testing , randomly assigns some users to the  X  X ontrol X  group (these users are presented with the exist-ing ranking results) and the  X  X reatment X  group (these users are presented with the results of an experimental ranking system). Ranking systems are then compared by analysing the clicks of the users in the  X  X ontrol X  against those in the  X  X reatment X  group. In the interleaving method by Joachims [27] users are presented with a combined list made out of two rankings. Then the system that receives more clicks is assumed to be better.

One of the main advantages of online evaluation schemes is that they are user-based and, as a result, often assumed to give us more realistic insights into the real system qual-ity. Interleaving experiments are now widely being used by large commercial search engines like Bing and Yahoo! [11, 31] as well as studied in academia [22, 32]. However, they are harder to reproduce than o  X  ine measurements, whereas in the traditional Cranfield approach one can re-use the same set of judged documents to evaluate any ranking. This makes the use of o  X  ine editor-based evaluation meth-ods unavoidable during the early development phase of rank-ing algorithms. One should take care, however, that the re-sulting editor-based measurements agree with the outcomes of online experiments X  X nline comparison is often used as the final validation step before releasing a new version of a ranking algorithm.

In order to bring the two evaluation approaches closer to each other, we propose a method for building an o  X  ine information retrieval (IR) metric from a user click model . Click models, probabilistic models of the behavior of web search users, have been studied extensively by the IR com-munity during the last five years. The main purpose of pre-dicting clicks, as seen in previous works, is: (1) modeling user behavior when real users are not available (see, e.g., [23]); (2) improving ranking using relevance inferred from clicks (e.g., [10]). We hypothesize that click models can also be turned into o  X  ine metrics and the resulting click model-based metrics should be closely tied to the user and hence should better correlate with online measurements than tra-ditional o  X  ine metrics. In addition, there is a growing trend to ground o  X  ine metrics in a user model and that is exactly what click modeling does X  X rying to propose a better user model. So, the question is why not use better user models, based on click behavior, as the basis for o  X  ine metrics?
We put our proposal for transforming click models into metrics to the test through a set of thorough comparisons with online measurements. Our comparison includes an analysis of correlations with the outcomes of interleaving experiments, an analysis of correlations with absolute on-line metrics, an analysis of correlations between traditional o  X  ine metrics and our new click model-based metrics, as well as an analysis of the discriminative power of the var-ious metrics. One dimension to which we devote special attention in our comparison framework concerns unjudged documents. As was shown by Buckley and Voorhees [5], having partially-judged result pages in the evaluation pool may result in biased measurements. We examine how di  X  er-ent o  X  ine metrics handle this problem. We also show that in situations when we cannot a  X  ord to use only fully-judged data, we can still make good use of the available data by making adjustments, by either a technique called condensa-tion [34] or a new threshold method that we propose.
The main research questions that we address in this work are:  X  How do click model-based IR metrics compare to the tra- X  How well do di  X  erent o  X  ine IR metrics agree with online  X  How well do di  X  erent o  X  ine metrics perform in the pres- X  How can we modify o  X  ine metrics to enhance agreement Our main contributions in this paper are a method for con-verting click models into click model-based o  X  ine metrics. Secondly, we present a thorough analysis and comparison of specific click model-based metrics with online measurements and traditional o  X  ine metrics.

The rest of the paper is organized as follows. Section 2 presents related work. Section 3 shows how to transform a click model into a model-based o  X  ine metric. In Section 4 we examine click model-based and traditional o  X  ine metrics and report on their performance. We finish with a conclusion and discussion in Section 5.
Determining and comparing the quality of information re-trieval systems has always been an important task in IR, both in academic and industrial research. In recent years, competition between large commercial search systems has reached the point where even a small improvement can be of great importance. As a result, a broad range of metrics to assess system performance have been proposed: Discounted Cumulative Gain (DCG) by J  X  arvelin and Kek  X  al  X  ainen [26], Expected Reciprocal Rank (ERR) by Chapelle et al. [10], Expected Browsing Utility (EBU) by Yilmaz et al. [39], to name just a few. They have also been assessed from a variety of angles (see, e.g., [6, 10, 33]).
 Some IR metrics have an underlying user model (e.g., ERR, EBU) or they can be viewed as such (see [6]). How-ever, there is still a big gap between user models and metrics. For example, some of the widely used click models, such as the User Browsing Model (UBM) by Dupret and Piwowarski [19] and the Dependent Click Model (DCM) by Guo et al. [21], have so far not been used to develop an o  X  ine metric. Moreover, since the introduction of these early click mod-els, many more click models have been developed, not only as improvements to previous models [20, 30], but also to address specific modeling issues, such as click models for di-versified search [12, 14], the use of mouse movements along with clicks [24], or to model sessions [41]. We believe that all these models can be converted to evaluation measures.
The creation of traditional test collections against reduced costs has received considerable attention. Carterette and Al-lan [7] and Sanderson and Joho [35] discuss approaches to building test sets for evaluation at low cost. Azzopardi et al. [3], Berendsen et al. [4] go a step further and describe meth-ods for automatically generating test collections and train-ing material for learning-based rankers, respectively. Other works have extensively examined one particular limitation of traditional test collections: the completeness of judgments [5, 34]. Buckley and Voorhees [5] introduce a new metric called bpref to use in a setup where we have missing rel-evance judgements. Sakai [34] propose an alternative so-lution that does not require a new metric. We will also consider this problem when analysing metrics. Apart from evaluation metrics, there are other interesting problems aris-ing when dealing with large query sets; these are addressed by the TREC Million Query Track [2] and further studied by Carterette et al. [8].

Another important group of extremely related studies con-cerns user-based experiments. Introduced by Joachims [27], the interleaving method is now widely used. Since its intro-duction, several modifications to the original method have been proposed, notably Team-Draft Interleaving [32] and Probabilistic Interleaving [22]. A thorough overview of in-terleaving methods can be found in [11]. Radlinski and Craswell [31] analyze and compare the sensitivity of both interleaving and traditional o  X  ine IR metrics against each other. They find that the outcomes of interleaving experi-ments generally agree quite well with o  X  ine metrics while data can be collected at a much lower cost. Below, we apply the same type of analysis to evaluate click model-based met-rics and to compare them against traditional IR metrics. Ali and Chang [1] show that per-query correlation between o  X  -line side-by-side comparisons and online interleaving exper-iments is low even when query filtering is applied. This find-ing suggests that aggregating results from multiple queries as was done in [31] is less noisy than computing correlations on a per query basis. Yue et al. [40] propose ways to in-crease the signal of an interleaving experiment; inspired by this idea we propose to tune o  X  ine metrics through two tech-niques referred to as condensation and thresholding below to enhance the agreement with interleaving (see Section 4.1).
From an initial focus on precision as a metric, the area of web search evaluation has evolved considerably. An early lesson is that we need to apply some sort of discount to the documents that appear lower in the ranking. One of the first metrics to operationalize this idea was Discounted Cumulative Gain (DCG) [26]. This metric is still widely used in the IR community. However, it has some drawbacks. One is that its discount function is not motivated by a user model. Another important issue with this metric is that it is a static metric, i.e., its discount values are fixed numbers. As shown in [39], a dynamic metric that dynamically assigns di  X  erent discount values according to the relevance of the documents appearing higher in the ranking, more accurately represents real user behavior.

In this paper we introduce the notion of click model-based metrics. The main constituent of such a model-based metric is a click model  X  X  probabilistic model aimed at predicting user clicks. Apart from click events ( C k ), a click model usually has hidden variables corresponding to events such as  X  X he user examined the snippet of the k -th document X  ( E k ). These hidden variables are often used to gain deeper insights into users X  behavior. For example, Chapelle and Zhang [9] used a click model (DBN) to predict relevance and train a ranking function and in [19] the parameters of the click model were analysed to explain how previous user clicks influence future clicks. All click models that we study in this paper assume that users click a document only after examining the document X  X  snippet, i.e., P p C k  X  1 | E k 0 q X  0.

Following Carterette [6], we distinguish between utility-based metrics and e  X  ort-based metrics. These give rise to two ways of mapping a click model to a click model-based o  X  ine metric. First, a utility-based metric uses a click model only to predict the click probability P p C k  X  1 q for the k -th document in the ranking. This probability is then used to calculate the metric value as the expected utility : where R k is the relevance of the k -th document. It is com-mon to use four or five relevance grades, from Irrelevant to Highly Relevant that are further mapped to numeric values. For example, the TREC 2011 Web Track [16] uses four levels of relevance: from 0 for Irrelevant documents to 3 for Highly Relevant documents.

Second, an e  X  ort-based metric requires a click model to have a notion of X  X ser satisfaction X ( S k ). A click model must have hidden variables S k such that P p S k  X  1 | C k  X  0 (the user can only be satisfied by the documents she clicked) and P p E j  X  1 | S k  X  1 q X  0 for j  X  k (after being satisfied the user stops examining documents). Having this, we can define a metric to be an expected value of some e  X  ort func-tion 1 at the stopping position: rrMetric  X  where s k  X  P p S k  X  1 | C k  X  1 q is a satisfaction probability .
A click model is usually trained using a click log. As a re-sult we get values of the model parameters that can further be used to calculate the probability of clicks or satisfaction events to use in Equations 1 and 2. Some of the parameters are just constants, some depend on the position(s) in the ranking and some depend on the document and/or query. Parameters of the last type are the hardest ones to be used in a metric, as we want our metric to work even for pre-viously unseen documents. But, fortunately, parameters of this type can usually be approximated from the document X  X  Following [6] we use reciprocal rank 1 k as an e  X  ort function. While we are not doing it here, it would be interesting to evaluate metrics with di  X  erent e  X  ort functions. Table 1: Click model-based metrics and their underlying models. Previously proposed models/metrics are followed by the reference.
 Underlying click model Utility-based E  X  ort-based DBN [9] uSDBN [10] ERR [10] DBN [9] EBU [39] rrDBN DCM [21] uDCM rrDCM
UBM [19] uUBM  X  relevance. In fact, when training a model we assume that these parameters only depend on the document relevance and not on the document itself. We will demonstrate this procedure for the attractiveness parameters in DBN, DCM, UBM and for the satisfaction parameters in DBN.

If a model meets the requirements listed above, it can be transformed into a click model-based metric. There is no step-by-step algorithm for such a transformation but only general guidelines. In the following sections we demonstrate the idea, using well-known click models as an example. We want to stress, however, that our framework is general enough to be applied to other click models, including those that use additional sources of information, such as recently studied session-based click models [41] or click models for vertical search [12, 14].

In Table 1 we classify previously studied metrics (ERR by Chapelle et al. [10], EBU by Yilmaz et al. [39]) and pro-pose several new click model-based metrics: rrDBN, uDCM, rrDCM, uUBM. The left most column lists click models, the center and right most column denote derived o  X  ine metrics, utility-based and e  X  ort-based, respectively. As a recipe for naming a metric, we use the name of the underlying model and prefix it with the type metric that we are defining: u-for utility-based and rr-for reciprocal rank e  X  ort-based metrics.
In this section we show how two previously proposed met-rics, ERR and EBU, can be viewed as click model-based metrics. Despite the fact that they are di  X  erent and were not in fact proposed as derivatives of a click model, they can both be viewed as metrics based on special cases of the Dynamic Bayesian Network click model (DBN) by Chapelle et al. [10]. In this model, the user examines document cap-tions one by one and may be attracted by document u with probability a u . If the user is attracted by the document, she clicks it and becomes satisfied with probability s u . If she is not satisfied by the document she proceeds to the next document with probability and stops otherwise.

The Expected Reciprocal Rank (ERR) metric uses a sim-plified version of the DBN model [9] (we will refer to this model as SDBN) in which, as an additional constraint, all at-tractiveness probabilities are set to 1 ( a u k  X  P p C k 1 q X  1) and therefore all documents are clicked. This leads to s u k  X  r u k , i.e., the satisfaction probability is equal to the probability of the document being relevant to the query. By making this assumption we obtain the probability of clicking the k -th document and the probability of satisfaction where r i is the probability of relevance of the i -th document and is the continuation probability. The probability of being relevant is usually viewed as a mapping R  X  r from the relevance grades to the segment r 0 , 1 s . In the original ERR paper [10] the authors use a mapping motivated by relevance grade ( R max  X  3 in the case of TREC 2011 Web Track), but one may also fit this mapping from a click log. Using probabilities from (3) and (4), we end up with the ERR and uSDBN metrics (cf. Equations (2), (1)): In the original version of the ERR metric, the continuation probability of the DBN model was set to 1. 2 Conversely, for uSDBN [10, Section 7.2], we set ( X  X ne minus the aban-donment probability X ) to 0 . 9, as suggested in [9].
The Expected Browsing Utility (EBU) metric by Yilmaz et al. [39] is also based on a variation of the DBN model. Unlike the original DBN model, their modification allows for di  X  erent continuation probabilities in di  X  erent situations ( p lead to greater flexibility in setting up the metric, they also represent a di cult choice for a practitioner to make. They were all set to 1 in the original paper [39] and here we do the same. By doing so we reduce the EBU model to DBN [9] with continuation probability  X  1. One notable di  X  erence between the ERR and EBU metrics is that EBU does not set the attractiveness probabilities to 1. Instead, the attrac-tiveness probabilities and satisfaction probabilities are both estimated from a click log using the assumptions that they are determined by the document relevance: where C k is the random variable corresponding to a click on the k -th document, L k is the random variable corresponding to leaving the result page after clicking the k -th document and R u k is the relevance of the k -th document u k .
In this section we propose new o  X  ine metrics by introduc-ing an e  X  ort-based variant of the EBU metric and also by converting the two popular click models, UBM and DCM, into click model-based metrics. By doing so we show that our framework of click model-based metrics is not only a way of viewing previously studied metrics, but also a way of deriving new metrics in a principled way.

The rrDBN metric uses essentially the same user model as the EBU metric. In fact, the parameters for EBU and
In the original paper [10, Section 3] an alternative interpre-tation of the ERR metric as a metric based on the Cascade Model by Craswell et al. [18] was also proposed. rrDBN are the same. The only di  X  erence is that rrDBN is calculated using Equation (2) instead of (1).

Next, the uDCM and rrDCM metrics can be derived from the Dependent Click Model (DCM, [21]) in a way similar to how EBU and rrDBN are derived from DBN. The only di  X  erence between DCM and DBN is that the satisfaction probability P p S k  X  1 q depends not on the document itself but on its position k in a ranked list. Thus, the DCM model can be described with the following equations: As was shown by Turpin et al. [37], the attractiveness of a document X  X  snippet can be approximated as a function of its relevance grade. A mapping from grades to attractiveness probabilities can be inferred from a click log using the click model (DCM in this case). 3 For this purpose we impose the constraint that documents with the same relevance have the same attractiveness, i.e., the attractiveness of a document is a function of its relevance grade: a u  X  a p R u q .
Finally, using the click model and Equations (1), (2), we can define uDCM and rrDCM metrics as follows:
Chen et al. [12] report that the User Browsing Model (UBM) [19] performs better than DBN in terms of click pre-diction perplexity. We have also evaluated this model using a Yandex click log. We used a sample of clicks collected in November 2012. We then removed pages without clicks and split the remaining data into training and test set. In to-tal we had 1,191,963 training and 1,292,993 test pages. To compare the models we used the perplexity gain [12, Section 5.2] which is a standard way of comparing perplexity values (see, e.g., [12, 20, 41]). On our data UBM outperforms DBN by 16% which is considered a big di  X  erence.

This finding motivates the idea of deriving an o  X  ine met-ric from UBM. In the UBM model the click probability is governed by the attraction bias and the examination bias:
P p C  X  1 | u, q, r, d q X  P p A  X  1 | u, q q P p E  X  1 | d, r where C stands for click, A for attraction, E for examina-tion; u is the document URL, q is the user query, r is the doc-ument rank (position), and d  X  r  X  max t j  X  r | C j  X  1 distance to the previous click. 4 For convenience, we write a r instead of a uq and r p j q instead of rd , where d  X  r
Like for the EBU/rrDBN and uDCM/rrDCM metrics, we assume that the attractiveness probability a is a function of
The source code for probabilistic inference is freely avail-able at https://github.com/varepsilon/clickmodels .
As in [19] we use a virtual zero position (which is always clicked) to simplify our equations. the relevance of the document: a uq  X  a p R uq q . The exam-ination probabilities r p j q can be precomputed from click logs during the same model training process. One impor-tant di  X  erence from the previously studied models is that the UBM model relies on previous clicks and these are not available o  X  ine. To deal with this problem we factorize the probability P p C r  X  1 q over the position of previous clicks j : P p
C r  X  1 q X  By applying Bayes rule we get
P p C r  X  1 q X  Finally, the click probability is given by a recursive formula: P p C 0  X  1 q X  1 P p
C r  X  1 q X  where a r  X  a p R r q , and a p  X q and r p  X q are known functions estimated from clicks. It is important to note, that un-like Dupret and Piwowarski [19], we used all queries, not only queries with high clickthrough rate. So our resulting function is di  X  erent from that analysed by Dupret and Pi-wowarski, and might be interesting on its own. For example, p j q is much less than 1 for j  X  0 which corresponds to the fact that most of the users click on only one document.
Given the click probability we can define the metric: The UBM click model does not have a notion of user satis-faction and hence we do not introduce an  X  X rUBM X  metric.
In this section we analyze the click model-based metrics previously listed, both old and new, along a number of di-mensions. We compare click model-based metrics to tradi-tional o  X  ine metrics. As traditional metrics we consider precision, with two possible binarizations of four scale judg-ments ( Precision treats the highest three relevance grades (3, 2, 1) as  X  X elevant, X  while Precision2 only treats the high-est two relevance grades (3, 2) as X  X elevant X ) as well as DCG. As was shown by Chapelle et al. [10], the NDCG metric is always worse at capturing user satisfaction than DCG. We decided not to include this metric and thus to overcome po-tential issues with corpus-dependent NDCG normalization.
We start by determining correlations of various o  X  ine metrics to the outcomes of interleaving experiments in a way proposed by Radlinski and Craswell [31]. These correlations are then used to compare o  X  ine metrics to each other. The metric that shows the best correlation with interleaving out-comes is assumed to better represent real user behavior. We then move to more traditional comparison techniques, such as metric-to-metric correlations and discriminative power.
As was shown by Radlinski et al. [32], absolute click met-rics are often unable to determine di  X  erences in IR systems. Moreover, they are always di cult to interpret and may even be misleading, because we cannot know for sure how these metrics are related to user satisfaction.

Fortunately, there is another approach, the pairwise or in-terleaved comparison techniques mentioned earlier [27, 32]. Following this approach, we compare two ranking systems by presenting a user with an interleaved result page, con-taining documents from both result lists. The winner is then determined from user clicks. We assess an o  X  ine IR metric m in terms of its agreement with the interleaving outcomes. Specifically, we use the Team-Draft Interleaving (TDI) method by Radlinski et al. [32]. In this method each document in the interleaved page is assigned to exactly one of the two ranking systems ( X  X he teams X ). We then say that a system wins a comparison if the documents it contributes to the combined list receive more clicks. The system that wins most of the comparisons is assumed to be better. For the current experiment we used a click log of the Yandex search engine collected in October X  X ecember 2012. During this period we focus on five revisions of the core ranking functions ( A , B , C , D , E ), with each revision be-ing compared to the previous one using TDI, that was run for 5-10 days. For each of our ten experiments we had at least 200,000 impressions as in the work by Radlinski and Craswell [31]. Some ranking function revisions influence more than one market (country), so in total we have 10 pairs of algorithms to compare: 1 AB , 2 AB , 1 BC , 1 CD ,
CD , 3 CD , 4 CD , 1 DE , 2 DE , 3 DE .Foreachal-gorithm pair we recorded the interleaving signal value, i.e., the deviation from 50% of the number of cases where the newer system was preferred. For instance, if in the experi-ment labeled i XY system Y was preferred over system X in 51% of all cases, we say that the interleaving signal for the experiment is 1%.

Having interleaving signals, we want to compare them to the signals obtained by the o  X  ine IR measures, i.e., the average di  X  erence of the metric values. Unlike in the tra-ditional Cranfield approach we use queries and documents from the query log. When computing an o  X  ine metric signal for a particular experiment i XY , we extract queries issued by the users assigned to the experimental flight. For these queries we also extract the document lists that would have been produced by each of the systems X and Y if they had not been interleaved. By using click-log-based queries when comparing the signal of an o  X  ine metric to the interleav-ing signal we eliminate the e  X  ect induced by the choice of a query set that one needs to compile for a Cranfield-style evaluation. Here it also allows us to perform experiments with historical revisions of a ranking algorithm that is no longer running. Although this approach has some advan-tages for our research problem, it has some disadvantages for everyday usage. One notable drawback is that we use only part of the judgements available because not all the queries that we have judgements for were submitted by the users of the experimental flight. For each experiment and each metric we keep only the queries that have at least one document judged. Depending on the experiment we have from 178 to 5,815 queries per experiment (median 573). As shown in [31], it is usually su cient to have approximately 100 queries to identify the better system in an o  X  ine com-parison.

The amount of data available to the search engine is usu-ally much larger than a human can handle. Even more im-portant is the fact that the web corpus is constantly chang-ing, so we cannot maintain complete judgements even for a limited set of queries. That is why it seems natural that some documents returned by the system do not have rele-vance judgements. In order to analyse the tradeo  X  between adding noise from unjudged documents and reducing the noise by allowing more queries we introduce a parameter #unjudged . We discard queries for which the number of un-judged documents in the top 10 is bigger than this value for either of the two systems taking part in a TDI experiment. Below, we vary this bound and see how it influences the correlation between o  X  ine metrics and interleaving.
For each o  X  ine metric m and each value of #unjudged from 1 to 9 we compute the weighted Pearson correlation (similar to [10]) between the metric signal and the interleav-ing signal. As a weight we use the number of queries partic-ipating in the calculation of the metric signal (this number is di  X  erent for each experiment). The results are presented in Figure 1. We can see that the e  X  ort-based metrics rrDBN and rrDCM are better at dealing with unjudged documents and are remarkably di  X  erent from their utility-based counter-parts; we will confirm this di  X  erence in Section 4.3. Another interesting observation can be made about the Precision and Precision2 metrics. Their behavior di  X  ers and, moreover, Precision has a negative correlation and this is not the case for Precision2. This seems to be due to the fact that un-judged documents are treated in the same way as the lowest relevance grade 0, whereas in fact they have a higher chance to belong to one of the top relevance grades: 92% of the documents in the top 10 have a relevance grade higher than 0, while only 23% have a relevance grade higher than 1.
As we can see from Figure 1, when we increase #unjud-ged , the maximum number of unjudged documents, to 4 or higher, the correlation drops for all of the metrics stud-ied. This means that adding queries with highly incomplete judgements adds noise to the metric signals. The problem of unjudged documents has previously been studied by Sakai [34], and his proposed solution is to exclude unjudged doc-uments from the ranked list and condense the remaining documents. Despite its heuristic nature, this idea actually leads to an increase in correlation for most of the metrics as shown in Figure 2. The exceptions from this rule are rrDBN and rrDCM that supposedly su  X  er most from the incorrect e  X  ort function values. For example, if we miss a judgement for the first document than for the second document we ap-ply a 1 1 discount instead of 1 2 (see Equation (2)). Even when we apply condensation, we still have a decrease in correlation values for high values of #unjudged . One way of dealing with this problem is to choose an optimal value of #unjudged and use it to get high correlations with in-terleaving outcomes. We propose a di  X  erent way of dealing with this noisy data. Comparing systems A and B , we dis-card all queries with di  X  erences in metric values less than a Figure 1: Pearson correlation between o  X  ine metrics and interleaving signal . Unjudged documents were treated as irrelevant.
 Figure 2: Pearson correlation between o  X  ine metrics and interleaving signal . Unjudged documents were skipped (ranked lists were condensed).
 Figure 3: Pearson correlation between o  X  ine metrics with thresholds and interleaving signal . Unjudged documents were treated as irrelevant. Figure 4: Pearson correlation between o  X  ine metrics with thresholds and interleaving signal . Unjudged documents were skipped (ranked lists were condensed). threshold m for each metric m : where Q m  X  X  q P Q || m p B, q q  X  m p A, q q|  X  m u . This means that we use only some portion of the queries we have (up to 20%), but these are queries that strongly distinguish between systems. The idea is that by choosing an appropri-ate threshold m we can tune a ranking system to produce the best correlation with interleaving outcomes. In order to test the idea we split our data (ten TDI experiments) into train and test set: we use the train set to choose the best threshold and the test set to compute the correlation scores.
While it would be natural to do a time-based train/test split, it appeared to be impractical with the data we have. Firstly, it was impossible to get training and test sets of rea-sonable sizes (either the training or the test set would consist of only 3 experiments which might give too noisy correlation values). Secondly, there are only few possible time-based splits so we are not able to assess statistical significance of the results. Instead, we used all possible 5/5 splits for our experiments, i.e., we take a subset of five experiments as a training set and the remaining five experiments as a test set. In total we had C 5 10  X  252 splits and corresponding cor-relation values. The correlation values were then averaged and error bars were computed using the bootstrap test at 95% confidence level and 1000 samples. Results are shown in Figures 3 and 4.

We can see in Figures 3 and 4 that the confidence intervals are quite narrow and most of the click model-based met-rics continue to show high correlation scores when the value #unjudged is large. If we look at one of the best performing metrics, uUBM, we can see that thresholded variants are a bit worse for #unjudged lower than 5, while for #unjudged equal to 5 and higher the thresholded variants start domi-nating, reaching the highest point for #unjudged  X  9(see Figure 5).

In order to test significance of the di  X  erences in correlation values we used the 5/5 split procedure described above. Un-like what we did for thresholded and thresholded condensed , for the simple and condensed variants we only use the test Figure 5: Pearson correlation between uUBM (in di  X  erent variants) and interleaving signal . set to determine the correlation and just ignored the train-ing set as there is nothing we need to tune. The correla-tion values are then averaged and confidence intervals are computed using the bootstrap method with 1000 samples and 95% confidence level. Three highest correlation scores were shown by thresholded condensed variant of uUBM met-ric (for di  X  erent values of #unjudged ), while the correla-tion score for thresholded condensed uUBM ( #unjudged =9) is significantly higher than any other variant ( simple , con-densed , thresholded ) of any metric. From Figures 1 X 4 we can also conclude that click model-based metrics in general show higher correlation values with the outcomes of interleaving experiments than traditional o  X  ine metrics, especially when we have many incomplete judgements ( #unjudged  X  5), which confirms the hypothesis formulated in the Introduc-tion: click model-based metrics are better correlated with online measurements than traditional metrics. Another in-teresting observation is that for the simple and condensed variants there exist optimal values of the #unjudged param-eter (3 and 5 respectively in our case). Conversely, for the thresholded and thresholded condensed variants it is more important to pick an appropriate metric and then use any value of #unjudged higher than 5.
Following the original work on ERR by Chapelle et al. [10] we also compared o  X  ine IR metrics by looking at their correlation with absolute click metrics. In our experiments we used the following metrics:  X  MaxRR, MinRR, MeanRR  X  maximal, minimal, mean  X  UCTR  X  binary value representing click (the opposite of  X  PLC  X  number of clicks divided by the position of the We did not include the Search Success (SS) metric consid-ered by Chapelle et al. [10] as it uses relevances not only clicks. We also confirmed the findings of [10] that QCTR (clicks per session) has negative or close to zero correlation with all the editorial metrics and skipped it as well. A configuration is a tuple that consists of a query and ten URLs of the top ranked documents presented to a user. For each configuration in our dataset we computed the values of absolute online and o  X  ine metrics. The vectors of these metric values are then used to compute Pearson correla-tion (unweighted). For our dataset we used clicks collected during a three-month period in 2012. Because we used a long period and hence had a su cient amount of data, we were able to collect 12,155 configurations (corresponding to 411 unique queries) where all ten documents have relevance judgements.

The results are summarized in Table 2. A similar com-parison was previously done by Chapelle et al. [10] for ERR and traditional o  X  ine metrics. The numbers they obtained are similar to ours. From the table we conclude that click model-based metrics show relatively high correlation scores while traditional o  X  ine metrics like DCG or Precision gen-erally have lower correlations, which agrees with the results of the previous section. Using the bootstrap test (95% sig-nificance level, 1000 bootstrap samples) we confirmed that all the click model-based metrics show significantly higher correlation with all the online metrics than any of the tra-ditional o  X  ine metrics.

As to the online metrics, we can see that the reciprocal rank family (MaxRR, MinRR, MeanRR) appears to be bet-ter correlated with the e  X  ort-based metrics (ERR, rrDBN, rrDCM), because the e  X  ort function used by these metrics is the reciprocal rank 1 k (see Equation 2). The same holds for PLC as it uses reciprocal rank of the lowest click that could be viewed as  X  X atisfaction position X  used by an e  X  ort-based metric. The di  X  erences between ERR and uSDBN, rrDBN and EBU, rrDCM and uDCM are statistically sig-nificant (using the same bootstrap test). Conversely, for the UCTR metric all the utility-based metrics show significantly higher correlation than corresponding e  X  ort-based metrics.
We also compared newly introduced click model-based metrics with older metrics: ERR (e  X  ort-based) and EBU (utility-based). The result of the comparison is marked as superscripts in the Table 2: the first superscript corre-sponds to ERR, the second one corresponds to EBU. The first (second)  X  means that the metric is statistically sig-nificantly higher than ERR (EBU),  X   X  significantly lower,  X   X   X   X  no statistical di  X  erence can be found (95% signifi-Table 2: Pearson correlation between o  X  ine and absolute online metrics. Superscripts represent statistically signifi-cant di  X  erence from ERR and EBU. Precision  X  0.117  X  0.163  X  0.155 0.042  X  0.027 Precision2 0.026 0.093 0.075 0.092 0.094 DCG 0.178 0.243 0.237 0.163 0.245 ERR 0.378 0.471 0.469 0.199 0.399
EBU 0.374 0.467 0.464 0.198 0.397 rrDBN 0.384  X  X  0.475  X  X  0.473  X  X  0.194  X  X  0.399  X   X  rrDCM 0.387  X  X  0.478  X  X  0.476  X  X  0.194  X  X  0.400  X   X  uSDBN 0.322  X  X  0.412  X  X  0.407  X  X  0.206  X  X  0.370  X  X  uDCM 0.374  X  X  0.466  X  X  0.463  X  X  0.198  X   X  0.396  X  X  cance level, bootstrap test). As we see, in most cases our new click metrics appear to be significantly better than the previously known ERR and EBU metrics, expect for UCTR measure, which does not account for clicks (rather for their absence) and hence obviously lacks the source of correlation with click-model based metrics. According to other metrics, rrDBN and rrDCM are better than ERR in 3 of 4 cases and better than EBU in all 4 cases, while uUBM is better than EBU in 4 of 4 cases.

In general, all the absolute click metrics are poorly corre-lated with o  X  ine metrics X  X he correlation values are much lower than correlation with interleaving outcomes. As was shown by Radlinski et al. [32], absolute click metrics are worse at capturing user satisfaction than interleaving. That is why we propose to use the results of Section 4.1 as the main way to compare o  X  ine metrics with user behavior.
In order to compare o  X  ine metrics to each other in terms of ranking IR systems we used data from the TREC 2011 Web Track [16]. Participants of the TREC competition were o  X  ered a set of queries ( X  X opics X  in TREC parlance) and a set of documents for each query to rank. Each document was judged using a 4-grade scale. 5 For each metric we can build a list of system runs 6 ordered by the metric value aver-aged over queries. We then compute Kendall tau correlation scores between these ordered lists; they are summarized in Table 3. As was shown by Voorhees [38], metrics with cor-relation scores around 0.9 can be treated as very similar because this is the level of correlation one achieves when using the same metric but di  X  erent judges. This level of correlation to distinguish equivalent metrics was also used in subsequent papers, for example [5, 7, 35, 37].
In Table 3 such metric pairs are marked in boldface. We see that all click model-based metrics are highly correlated within their group, utility-based or e  X  ort-based , while corre-lations of the two metrics based on the same model (uSDBN and ERR, EBU and rrDBN, uDCM and rrDCM) are lower.
Another measure frequently used for comparing metrics is the discriminative power by Sakai [33]. This measure is a bit controversial, because high values of discriminative power do not imply a good metric. Nevertheless, extremely low values of discriminative power can serve as an indica-tion of a metric X  X  poor ability to distinguish di  X  erent rank-ings. As was shown in previous work (e.g., [15, 36]) dis-criminative power is highly consistent with respect to sta-tistical test choice. Given this fact we focus on a bootstrap test as it makes fewer assumptions about the underlying distribution. Results based on the same TREC 2011 Web Track data as used in the previous section are summarized in Table 4. As expected, highly correlated metric pairs (e.g., (rrDBN, rrDCM) and (EBU, uDCM)) have similar discrim-inative power.

Another observation to be made is that the e  X  ort-based metrics ERR, rrDBN and rrDCM have a lower discrimina-tive power than the utility-based metrics uSDBN, EBU and
Initially, a 5-grade scale was listed on a TREC 2011 de-scription page, but in the end a 4-grade scale was used for evaluation. As in the trec_eval evaluation tool we do not distinguish between Irrelevant and Spam documents.
In total we have 62 runs submitted by 16 teams. Table 4: Discriminative power of di  X  erent metrics according to the bootstrap test (confidence level 95%).
 uDCM, respectively. This is probably due to the fact that  X  X osition discount X  for the e  X  ort-based metrics goes to zero faster than for the utility-based metrics and hence they are less sensitive to changes in the bottom of the ranked list.
In this paper we proposed a framework of click model-based metrics to build an o  X  ine evaluation measure on top of any click model. Answering the research questions outlined in the introduction we can say that  X  Click model-based metrics generally di  X  er from tradi- X  All click model-based metrics generally show high agree- X  Unjudged documents may decrease correlation with in- X  Condensation and thresholding of o  X  ine metrics are ef-One natural extension of our framework of click model-based metrics can be adding more signals from the assessors. For example, we can ask assessors to judge not only documents, but their snippets as well (a practice already in place at commercial search engines). By using this we can drop the assumption that snippet attractiveness is a function of doc-ument relevance as was assumed by the click model-based metrics. While attractiveness is highly correlated with doc-ument relevance [37], it is essential to use real attractiveness judgements when we need to evaluate a snippet algorithm, not only ranking. It might be interesting to incorporate attractiveness judgements into metrics and re-evaluate our click model-based metrics using proposed modifications.
Another interesting direction is the area of good abandon-ments . Li et al. [29] report that some snippets might be good enough to answer the user query directly on a search engine result page. As was shown in [13], one can ask human judges to indicate whether a snippet contains an answer to the user query (fully or partially). That task appeared to be relatively easy for assessors. Given such judgements, one can modify any evaluation metric by adding additional gain from the snippets that contain an answer to the user X  X  infor-mation need. To convert this into a metric, we assign some gain to the documents that were clicked ( C k  X  1) and some gain to the documents that were only examined, but did not attract the user ( E i  X  1, A k  X  0).

Adapting click models for the unjudged/unknown docu-ments is also an interesting direction. For example, we could modify a click model by adding probability of a document being skipped because it is unjudged. This question requires further investigation and we leave it as future work.
In our work we argued that o  X  ine metrics should be better correlated with interleaving outcomes. However, we might want to have a metric that correlates with user satisfaction. Some steps towards this problem have been taken in early work by Hu  X  man and Hochster [25] where user studies were performed to analyse the meaning of editorial relevance for real users. It would be interesting to perform a study of this type to compare o  X  ine metrics.
 We would like to thank Katja Hofmann, Maria-Hendrike Peetz and our anonymous reviewers for reading the paper and making useful comments and suggestions.
 This research was partially supported by the European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreements nr 258191 (PROMISE Net-work of Excellence) and 288024 (LiMoSINe project), the Netherlands Organisation for Scientific Research (NWO) un-der project nrs 640.004.802, 727.011.005, 612.001.116, HOR-11-10, the Center for Creation, Content and Technology (CCCT), the BILAND project funded by the CLARIN-nl program, the Dutch national program COMMIT, the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), and the Netherlands eScience Center under project number 027.012.105.
 [1] K. Ali and C. Chang. On the relationship between [2] J. Allan, B. Carterette, J. Aslam, and V. Pavlu. [3] L. Azzopardi, M. de Rijke, and K. Balog. Building [4] R. Berendsen, E. Tsagkias, M. de Rijke, and E. Meij. [5] C. Buckley and E. M. Voorhees. Retrieval evaluation [6] B. Carterette. System e  X  ectiveness, user models, and [7] B. Carterette and J. Allan. Incremental test [8] B. Carterette, V. Pavlu, and E. Kanoulas. If I had a [9] O. Chapelle and Y. Zhang. A dynamic bayesian [10] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. [11] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. [12] D. Chen, W. Chen, and H. Wang. Beyond ten blue [13] A. Chuklin and P. Serdyukov. Good abandonments in [14] A. Chuklin, P. Serdyukov, and M. de Rijke. Using [15] C. L. A. Clarke, N. Craswell, and I. Soboro  X  . A [16] C. L. A. Clarke, N. Craswell, I. Soboro  X  , and E. M. [17] C. W. Cleverdon, J. Mills, and M. Keen. Factors [18] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [19] G. Dupret and B. Piwowarski. A user browsing model [20] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, [21] F. Guo, C. Liu, and Y. Wang. E cient multiple-click [22] K. Hofmann, S. Whiteson, and M. de Rijke. A [23] K. Hofmann, A. Schuth, S. Whiteson, and M. de Rijke. [24] J. Huang, R. W. White, G. Buscher, and K. Wang. [25] S. B. Hu  X  man and M. Hochster. How well does result [26] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [27] T. Joachims. Optimizing search engines using [28] R. Kohavi, R. Longbotham, D. Sommerfield, and [29] J. Li, S. Hu  X  man, and A. Tokuda. Good abandonment [30] C. Liu, F. Guo, and C. Faloutsos. BBM. In KDD . [31] F. Radlinski and N. Craswell. Comparing the [32] F. Radlinski, M. Kurup, and T. Joachims. How does [33] T. Sakai. Evaluating evaluation metrics based on the [34] T. Sakai. Alternatives to Bpref. In SIGIR . ACM, 2007. [35] M. Sanderson and H. Joho. Forming test collections [36] M. D. Smucker and C. L. A. Clarke. Time-Based [37] A. Turpin, F. Scholer, K. Jarvelin, M. Wu, and J. S. [38] E. M. Voorhees. Variations in relevance judgments [39] E. Yilmaz, M. Shokouhi, N. Craswell, and [40] Y. Yue, Y. Gao, O. Chapelle, Y. Zhang, and [41] Y. Zhang, W. Chen, D. Wang, and Q. Yang.

