 Amac  X  Herdag  X  delen Abstract Social media is a natural laboratory for linguistic and sociological purposes. In micro-blogging platforms such as Twitter, people share hundreds of millions of short messages about their lives and experiences on a daily basis. These messages, coupled with metadata about their authors, provide an opportunity to understand a wide variety of phenomena ranging from political polarization to geographic and demographic lexical variation. Lack of publicly available micro-blogging datasets has been a hindrance to replicable research. In this paper, I introduce Rovereto Twitter n-gram corpus, a publicly available n-gram dataset of Twitter messages, which contains gender-of-the-author and time-of-posting tags associated with the n-grams. I compare this dataset to a more traditional web-based corpus and present a case study which shows the potential of combining an n-gram corpus with demographic metadata.
 Keywords Twitter n-gram corpus Social media Demographics Metadata Gender Time 1 Introduction Social media and networking platforms have become pervasive in our daily lives (Madden and Zickuhr 2011 ). As a result, a large collection of social media posts became publicly available. Massive, user-generated data present new research opportunities in social science and computational linguistics (Lazer et al. 2009 ; Thelwall et al. 2010 ; Argamon et al. 2007 ). The availability of user-generated data also presents an abundant opportunity to tap personal and informal aspects of communication and daily routines of people, which would otherwise be difficult X  X f not impossible X  X o observe in more controlled settings. In this paper, I present Rovereto Twitter n-gram corpus (RTC), 1 an n-gram dataset of almost 75 million short, personal, social media posts in English, along with information on the gender of the authors of the posts and the time of the posting. 2 Twitter, 3 a micro-blogging service, allows its users to post short messages ( tweets ). Everyday hundreds of millions of tweets are shared publicly. Twitter-based data have already been used to study a variety of social and computational problems such as commonsense data collection and gender modeling (Herdag  X  delen and Baroni 2011 ), news propagation analysis (Lerman and Ghosh 2010 ), fast earthquake detection (Sakaki et al. 2010 ), influenza outbreaks (Culotta 2010 ), daily mood changes (Golder and Macy 2011 ), community detection and political polarization (Conover et al. 2011 ), and prediction of election results (Tumasjan et al. 2010 ). Twitter corpora have also been used in linguistic studies such as demographic and geographic lexical variation (O X  X onnor et al. 2010 ), linguistic change (Cunha et al. 2011 ), adjective disambiguation in Chinese (Pak and Paroubek 2010 ), and identifying sarcasm and verbal irony in short texts (Gonza  X  lez-Iba  X  n  X  ez et al. 2011 ; Davidov et al. 2010 ).
The content of tweets may include, but is not limited to, comments on recent events, what the users have been doing, or small talk X  X n other words, anything that people may find worthy to share with their friends and other people. Earlier studies showed that more than half of the tweets are about  X  X  X elf at present X  X , anecdotal, or personal (Naaman et al. 2010 ; Kelly 2009 ). In addition, the temporal references in tweets are primarily about the immediate present or near future (Ritter et al. 2011 ). These findings suggest that people use Twitter in an embedded manner in their daily lives. This aspect of Twitter-based corpora makes them suitable and particularly interesting data sources for investigating the daily routines and lives of people.
Twitter data is publicly available via an application programming interface (API). 4 However, it has not been possible to construct and share a publicly available dataset of tweets because of legal limitations on the redistribution of raw tweets. The lack of a commonly available dataset is an impediment to replicability since researchers cannot evaluate their algorithms and methods on a shared dataset. Moreover, it results in duplication of effort since almost every researcher who wishes to utilize Twitter data is faced with the burden of collecting a new dataset.
This study describes an extensive, publicly available dataset of tweets. Put briefly, I built and shared a Twitter-based dataset using n-grams, thereby overcoming the limitations on the redistribution of raw tweets. This is a similar strategy to that employed in Google Books N-Gram Dataset which used n-grams because of the limitations imposed by copyright issues (Michel et al. 2011 ). Further, using n-grams gives an additional advantage for providing aggregate statistics about the demographics of the users who used the n-grams in their tweets, without creating privacy issues. The combination of demographic data and the time of posting with the actual content of tweets presents previously unavailable oppor-tunities for research in computational linguistics, sociolinguistics, and related areas. Here, I focus on gender and the posting time of the tweets. The gender label allows us to compare the differences in the content or linguistic patterns between the tweets of the two genders. The time label enables us to identify which topics the users are more interested in at different times of the day or week, and study how the engagement pattern with social media changes throughout time. Together the two labels provide a rich metadata layer on the n-gram corpus. The corpus is distributed under a Creative Commons Attribution-NonCommercial-ShareAlike license. 5
The rest of this paper is structured as follows. In Sect. 2 , I discuss previous work done in building web-based or user-generated corpora. In Sect. 3 , I explain the data collection procedure and technical details such as tokenization, language identifica-tion, gender guessing and the extraction of the local time of posting. In Sect. 4 ,an evaluation of language identification and gender guessing heuristics is given. In Sect. 5 , I provide a descriptive analysis of the newly constructed corpus and compare it to a web-based corpus. In Sect. 6 , I show how a corpus with demographic metadata can be used to extract stereotypical gender expectations from Twitter. Finally, I conclude with a general discussion and recommendations for future directions in Sect. 7 . 2 Relevant work Some of the user-generated content sources that researchers have employed in corpus-based studies are Usenet discussions (Hoffmann 2007 ; Shaoul and Westbury 2011 ), e-mails (Klimt and Yang 2004 ), and chat logs (Shaikh et al. 2010 ). With the rise of social media, micro-blogging platforms such as Twitter also attracted attention as a content source.
 One of the largest Twitter-based corpora released to the public was the Edinburgh Twitter Corpus (Petrovic  X  et al. 2010 ) which consisted of approximately 100 million tweets. Unfortunately, it had to be retracted following a request from Twitter (Osborne 2010 ). Another example is the 427-million-tweet dataset released by Yang and Leskovec ( 2011 ) which was also taken down 6 (Yang and Leskovec 2011 ). In both cases, the datasets consisted of full texts of tweets which is a violation of API Terms of Service of Twitter. 7 During their brief period of availability to the public, these datasets enjoyed considerable interest among the research community (Naveed et al. 2011 ; Herdag  X  delen and Baroni 2011 ; Bifet and Frank 2010 ; Zhao et al. 2011 ; Liu et al. 2011 ).

Tweets 2011 is a dataset of 16 million tweet identifiers (but not content or metadata) released as part of the TREC 2011 Microblog track. 8 The goal here is to agree on a shared set of tweets on which researchers may work, without actually releasing the content, but instead letting researchers download the tweets from the Twitter API 9 themselves. Unfortunately, with regular API rates, a user can only download up to 350 tweets per hour or has to resort to screen scraping X  X oth of which result in very long fetching times. The latter option has the additional, unfortunate, drawback of being illegal in most cases.

Another source of personal textual data is weblogs (blogs). Although blogs are not as deeply embedded in the daily routine of people as micro-blogging platforms are, they nonetheless provide a rich dataset for opinion modeling. In 2009, Yano et al. ( 2009 ) released a dataset which consisted of several million posts and associated comments collected from five political blogs between 2007 and 2008. As part of the ICWSM 2011 Data Challenge, a dataset consisting of over 133 million blog posts and 231 million social media posts collected between January 13th and February 14th, 2011 was released to the public (Burton et al. 2011 ).

The Blog Authorship Corpus (Schler et al. 2006 ; Argamon et al. 2007 ) is a dataset of 681,288 blog posts written by 19,320 writers. Each writer is tagged with his or her age and gender. This produces a gender-and age-tagged corpus of over 140 million words. Another blog-based dataset focusing on a different domain is Puschmann X  X  Corporate Blogging Corpus which is  X  X  X  collection of blog entries that was assembled between September 2006 and September 2007 X  X  from company blogs (Puschmann 2010 ).

In this paper, I will use as a reference corpus, ukWaC, a billion-word English corpus built by crawling web pages in the .uk domain, to show the differences between a user-generated corpus and  X  X  X raditional X  X  web-based corpora (Baroni et al. 2009 ; Ferraresi et al. 2008 ). Interested readers can refer to Baroni et al. ( 2009 ) and Hundt et al. ( 2007 ) for further examples and discussion about using the web as a corpus.

On a final note, the RTC is an n-gram dataset and it shares some limitations in common with other n-gram datasets such as Mark Davies X  n-gram lists 10 which are based on the Corpus of Contemporary American English (Davies 2009 ), Google X  X  Web 1T 5-gram Corpus (Evert 2010 ; Brants and Franz 2006 ), Google Books N-Gram Dataset (Michel et al. 2011 ), and Microsoft X  X  Web n-gram corpus (Wang et al. 2010 ). In particular, the context of the lexical items is limited to 6-grams in the dependencies or entire contents of the tweets. 3 Construction of the corpus In this section, I will explain the steps taken in the construction of the RTC. These are  X  data collection,  X  language identification and filtering,  X  spam and bot removal,  X  tokenization,  X  extracting gender and timezone,  X  aggregation and distribution of the dataset.

Data collection Twitter Streaming API 11 provides a continuous stream of roughly 1 % of all public tweets. Among the data fields that were received, the important ones for our purposes were tweet content (up to 140 characters), tweet posting time in coordinated universal time (UTC), author X  X  name, author X  X  screen name and user id, and author X  X  local timezone. The fetching process was seven months in duration from December 5, 2010 to June 25, 2011, and resulted in 239 million tweets (Fig. 1 ).

Language identification Twitter is a multi-lingual environment with a significant number of non-English tweets (Hong et al. 2011 ). We want an English-only corpus; however, filtering out non-English tweets is not a trivial task because of the short text length and the informal nature of tweets. Abbreviations, typos and acronyms are commonly observed in Twitter. Therefore, language models based on more traditional texts are inadequate. Here, I used a combination of an off-the-shelf language model and bootstrapping techniques. First, a language model trained on more traditional text was used to create a large Twitter-based training set, and then, a custom-built language model was trained on this training set to identify the language of the tweets. Details are below.
 To create a possibly noisy, but certainly a large training set of English and non-English text based on tweets, I first used the language detection library (which I refer to as LDL) of Nakatani Shuyo, 12 which contains character n-gram models of 53 languages trained on Wikipedia abstracts. The content of each tweet was first lowercased and then passed through a whitespace tokenizer. A token was removed from the content if it satisfied at least one of the following conditions:  X  is longer than 50 characters,  X  is a hashtag or a user mention (starts with  X  X # X  X  or  X  X  X  X  X  respectively),  X  is a URL (starts with  X  X  X ttp:// X  X  or  X  X  X ttps:// X  X ),  X  contains more than four consecutive repetitions of the same character,  X  is the special token  X  X  X t X  X  which denotes a retweet .

If the remaining content was longer than 100 characters (remember that a tweet can have at most 140-characters) then its language was identified by LDL (based on the reduced content). This procedure increases the chances that only long tweets with meaningful content are labeled with a language. I used 15 languages 13 and used the  X  X  X ther X  X  label for all other languages as a catch-all category. This step resulted in approximately 13 million English and 12 million non-English tweets.

In the second stage of the bootstrapping, I used LingPipe (Alias-i 2008 ), a natural language processing library, to train character n-gram models for different languages with Witten-Bell smoothing (Witten and Bell 1991 ) (with n B 4). Details of the implementation can be found on LingPipe X  X  Web page 14 and in Carpenter ( 2005 ). I used all of the tweets that were language-labeled in the previous step with the exception of English, Indonesian, Spanish and Portuguese tweets for which due to memory limitations, the training sets were limited to the first 1,000,000 characters for each language. Note that during the training phase, the entire content of the tweets were used X  X ot just the reduced contents utilized in the first step. This way, stylistic and non-standard usages specific to each language were also employed in the training procedure. After the second step of the bootstrapping procedure, I evaluated all 239 million tweets by the trained models and labeled each tweet with the most-likely language according to the n-gram models. This last step resulted in 87 million English tweets. Note that even though the language models were trained on lowercased text to reduce noise, the tweets were kept in their original case.

Spam and bot removal The corpus should consist of tweets written by real people, as opposed to spam and automatically-generated messages. To achieve this, spam and bot accounts must be identified and their tweets must be removed from the dataset. In Twitter, spam is a serious concern and spam filtering is not a trivial task (Yardi et al. 2010 ; Benevenuto et al. 2010 ; Kyumin et al. 2010 ). Twitter itself is continuously fighting against spam 15 and removes accounts which post unsolicited URLs or engage in spamming activity. In order to remove spam from the corpus, I opted to make use of Twitter X  X  efforts: three and a half months after the fetching process was completed, I re-queried the Twitter API to see which accounts in the dataset were still active. I removed all accounts which had been deleted in the meantime. However, a trade-off that needs to be addressed here is that between precision and recall. Discarding all deleted accounts can possibly result in the removal of valid tweets (i.e., because some accounts will have been deleted for reasons other than spam). Given the virtually unlimited supply of tweets X  X he real limitation is not the supply of tweets, but our ability to handle them X  X e opted for a technique that favors precision over recall.

The second issue is bots X  X witter accounts managed by software systems that post tweets automatically and regularly. They do not necessarily engage in spamming or malicious activities (e.g., a bot may simply tweet humorous texts taken from an outside source several times a day, or the weather condition every hour), but they may still distort corpus content because of the sheer number of the tweets posted. Identification of bots in Twitter is also an active area of research (Chu et al. 2010 ). Here, I used a tweet-count heuristic to identify accounts producing a number of tweets not reasonably compatible with the on-line activity of an average (human) user. As already explained, the fetching process continued for approximately 200 days and consisted of a 1 % sample of all tweets. A user who tweeted 100 times a day, for every day during the 200-day sampling period, would have posted 20,000 tweets during the period, and the expected number of tweets we would see in our sample would be 200. Thus, I set the threshold to 200, and all accounts with more than 200 tweets in the entire sample were removed from the dataset.

Spam and bot removal heuristics filtered 1,797,699 accounts. After filtering, I was left with 74,579,965 English tweets posted by 11,434,388 unique users which constitute the basis of rest of the analysis in this study.

Tokenization Tweets contain non-standard lexical entities that we want to protect in the tokenization process, such as hashtags (i.e., keywords prefixed by  X # X , acting as user-provided tags for the tweet) and user name mentions (e.g., @username ). In order to preserve the original content as much as possible, I employed a minimalistic tokenizer which detects and preserves URLs, abbreviations, emails, HTML entities (e.g., &amp;quot;), hashtags, user name mentions and emoticons (e.g.,  X  X :-) X  X ,  X  X :[ X  X ). All other punctuation marks are considered single tokens.
An example content and its tokenization is given below: original RT @justinbieber: and that X  X  for those that dont know ... they X  X e great records like this. # GREATMUSIC : http://www.youtube.com/watch?v=cF tokenized RT @justinbieber: and that X  X  for those that dont know ... they X  ve great records like this. #GREATMUSIC: http://www.youtube.com/watch?v=cF
Extracting gender and timezone We want to tag each n-gram with the gender of the author who wrote the tweet containing that n-gram as well as the time of posting. In this section, I will explain the algorithms used to extract this information. The statistics of the extracted data are given in the next section. Gender and local timezone were extracted based on the profile information of a user which might have changed during the fetching period. I re-computed these tags for each tweet separately. Thus, if a user changed his timezone during the fetching period, the dataset captures this change. The user-based statistics are based on the first computed tag for the user.

Twitter does not collect or disclose the gender of its users. However, user-provided first names can be used to guess the gender of users X  X t least, for the ones frequencies for both genders, released by U.S. Census Bureau and U.S. Social Security Administration. 16 If a name is frequently associated with males (females) all users having this first name are classified as male (female). The scripts used are available publicly. 17
The posting time of tweets are given in UTC which needs to be converted to the local time relative to the user. 65 % of the users (accounting for 78 % of all English tweets) identified a timezone in their profile settings, and the local time of the tweets were computed by correcting the time in UTC. This process may introduce some noise for reasons such as inaccurate timezones provided by users or by user mobility. Nevertheless, previous studies suggest that user-provided timezones are accurate enough to compute local posting times in more detailed analyses (Golder and Macy 2011 ).

By using the local time of tweets, I computed two tags: the day of the week (Monday, ... , Sunday), and hour of the day (0, ... , 23) for given a tweet in the posting user X  X  local timezone. The n-grams occurring in the tweet are tagged with the corresponding day and hour labels. The n-grams of the users without a timezone are labeled with unknown-timezone .

Aggregation and distribution format Two kinds of statistics are reported for the n-grams X  X requency and user count. Frequency is the total number of times the n-gram is mentioned in the corpus; user count is the total number of users who mentioned the n-gram at least once. Each tokenized tweet in the dataset was assigned to a slot which corresponds to a tuple consisting of gender, day of the week, and hour of the day, depending on the gender of the user and the local time of posting. Separately for each slot, the number of times each n-gram was observed and the number of unique users who mentioned the n-gram were calculated. In order to keep the corpus size at manageable levels, n-grams observed &lt;3 times in the entire dataset were removed from the dataset.

There are 7 days a week, 24 h a day, two genders, and two statistics (frequency and user count). Adding an unknown tag (which is used when the timezone or gender is not specified), this translates into (7 ? 1) * (24 ? 1) * (2 ? 1) * 2 = 1,200 slots. There is a small redundancy in this representation, which I decided to keep for consistency, that is, if the day of the week is not known X  X ecause the corresponding users do not have a timezone specified in their profiles X  X hen the hour of the day is also not known.

For practical purposes, the corpus is released as six separate files, one for each n value of the n-grams (from 1 to 6). Each n-gram is represented on a separate line. Each line is a tab-separated list of columns where the first column is the n-gram (with space between the tokens). The remaining 1,200 columns on each line are the frequency and user-count statistics of the n-gram for every combination of gender, day-of-week, and hour-of-day slots.

A truncated example is given in Table 1 . In this table, each column corresponds to a tab-separated field in the corpus files. The slots iterate over frequency, user count, gender, hour, and day in the given order with the  X  X  X nknown X  X  tags placed at the end of the iteration for each variable. Details are given in the corpus homepage. 18 4 Evaluation of meta-data generation heuristics In this section, I evaluate the language identification and gender labeling heuristics and report their accuracy.

Language identification In order to evaluate the performance of language identification, I manually labeled 250 randomly chosen tweets as English or non-English (without knowing their assigned labels). Multi-lingual or ambiguous tweets were labeled as non-English. The precision and recall values of the bootstrapped language identifier are given in Table 2 along with the performance measures of LDL as baseline. Bootstrapping improved both the recall (by 8.74 % points) and the precision (by 3.22 % points). The two classifiers disagreed on 18 tweets, of which 15 were correctly labeled by the bootstrapped classifier ( p = 0.008 for a two-tailed binomial test with equal proportions).

Gender guessing It is possible to evaluate the method X  X  performance on a sample of people with known genders. Unfortunately, we lack a gold standard dataset for Twitter users. However, Freebase, 19 a large, collaborative and structured database X  based on data collected from various sources on the web and contributed by Freebase X  X  users X  X ontains demographics (e.g., full names, birthdays, and genders) of more than two million people. In order to create a gold standard, I sampled all U.S.-born people whose genders were specified in the database and who were born after January 1, 1900. This resulted in a set of 91,520 people. The first-name based heuristic could not provide a guess for the gender of 9,762 people (10.70 %). Among the remaining people, the accuracy was 97.85 %, with approximately symmetric error rates for either gender: 2.94 % of the females and 2.35 % of the males were misclassified.
 This sample of users is not a perfect substitute for a gold standard collected from Twitter. Fortunately, previous studies report additional sanity checks for name-based heuristics, which suggest the method works quite reasonably (Mislove et al. 2011 ; Herdag  X  delen and Baroni 2011 ), and the above results provide further assurance that name-based heuristics have a very high accuracy with reasonable coverage. 5 Analysis and comparison In this section, I will first report descriptive statistics of the RTC broken down by gender and time, and provide a qualitative and quantitative comparison with the web-based corpus ukWaC. 5.1 Analysis Gender The gender guessing heuristic, when applied to the 11,434,388 users represented in RTC, labels 28.79 % of the users as female and 26.78 % as male (for the remaining 44.43 % no gender is guessed and they are labeled as unknown ). The tweets of these users are labeled with the gender of their authors, accordingly. The details of the gender tagging are given in Table 3 .

Time In Fig. 2 , we can see the hourly change in Twitter over the course of week X  X ggregated over all weeks in the dataset X  X n terms of total count of unigrams written per hour. There is a clear daily pattern: In the first four working days (from Monday to Thursday) the activity is at its lowest around 04:00 in the morning. Then, we see an uninterrupted increase in the number of tokens until 20:00 in the evenings. Fridays start similar to other working days but Friday evenings diverge from the regular pattern. We see that the Friday evening peak is much less pronounced. Saturdays are marked with generally reduced activity compared to all other days. On Sundays, the regular pattern resumes with an almost identical shape to those of the working days (Table 4 ).

We can also cross-tabulate the gender and time tags to compare the relative frequency of male and female Twitter activity for different times during the week. In Fig. 3 , we can see the fluctuations in female and male Twitter activity, in terms of percentage points deviated from the expected baseline based on the total number of tokens written by females and males (which is 56 and 44 %, respectively). In the figure, this baseline is represented as the 0 % line. For example, right after 04:00 o X  X lock on Sunday mornings, the number of tokens written by males deviates from the baseline by more than 5 % points, which means the percentage of tokens written by males is approximately 49 % at that time. The deviations may seem small, but there is a remarkably consistent, bi-modal, daily pattern for all days of the week: increased male activity in the early morning and evening, and increased female activity during midnight and midday. In addition to this pattern, during the weekends, the overall male activity increases substantially.

Finally, illustrative examples of 2 X 6 g are provided in Table 5 . I provide common n-grams which start with the word  X  X  X y X  X .
 5.2 Comparison with ukWaC A comparison of the RTC with a web-based corpus like ukWaC will help to better understand the particular nature of Twitter-based content. In this section, I will treat ukWaC as a baseline corpus and report the differences of RTC from this baseline. ukWaC is a web-based corpus that contains approximately two billion tokens extracted from web pages in the uk domain. It is comparable to RTC in size. Its web-based nature makes ukWaC an ideal candidate as a baseline corpus because, just like social media, the web provides an almost unlimited amount of textual data and it is important to understand the differences between the web and the social media as two sources for language-related data.

As a first step, I computed a contingency table for the unigrams and bigrams. To simplify matters, I lowercased the n-grams, aggregated the frequencies of duplicates (occurring because of lowercasing), and limited the analyses to n-grams containing only letters from the English alphabet, with additional space for bigrams. Note that in the absence of the part of speech tags of the tokens in RTC, I focused on word forms and not the types. The numbers reported below should be interpreted as a proxy for the lexical diversity in the two corpora.

In Table 6 , we see a 3 by 3 contingency table according to the unigrams X  frequencies. The thresholds 5 and 20 are picked arbitrarily, with 20 being used in Baroni et al. ( 2009 ) and 5 being a cut-off to mitigate the noise (e.g., typos, words in foreign languages, etc). Table 7 provides similar statistics for bigrams. Due to memory limitations, I only considered bigrams occurring more than five times in at least one corpus.

We see that the overlap between the two corpora is rather limited. Of the 288,812 unigrams which occur at least 20 times in either corpora, only 94,959 (33 %) pass the same threshold in both. Similarly, of 4,331,342 bigrams which occur at least 20 times in RTC or ukWaC, only 612,852 (14 %) pass the same threshold in both corpora.

More than 1.2 million unigrams are observed 5 times or less in both corpora. A majority of these 1.2 million unigrams (1,000,442) are observed only in RTC whereas 188,095 are observed only in ukWaC. The asymmetry in the proportion of very rare unigrams suggests that RTC might have a higher noise ratio and/or idiosyncratic terminology that is not observed in web-based text. This rather high noise should be neither surprising nor alarming. The informality of social media results in many slang, intentional or non-intentional typos, and other kinds of noise. The colloquial nature of the corpus is one of its defining aspects.

In Table 8 , we see the most frequent 20 unigrams in RTC and ukWaC, which are not observed in the other corpus. The Twitter-specific unigrams are mostly online services or Twitter-related terms (retweet, twitpic, tweeting, tweeps, tweeted, unfollow, twitcon, tumblr), artists or artist-related slang (belieber, minaj), or interjections/acronyms that are frequently found in the so called textspeak or other short messaging conventions (lmaoo, ctfu, bouta, shawty, yhu, lmfaoo, lmfaooo, smdh, hahaa). The ukWaC-specific terms include acronyms (mostly governmental departments or organizations in the U.K.), and legal terms such as appellants, rateable, pursuance, and senatus.

In Fig. 4 , the relative frequency of personal pronouns is visualized. Note that the counting is done over surface forms without any part-of-speech (POS) tagging. The first-person pronouns  X  X  X  X  X ,  X  X  X e X  X  and  X  X  X y X  X  are clearly over-represented in RTC X  approximately 90 % of all occurrences of these three pronouns are observed in RTC as opposed to ukWaC. This reflects the first-person narrative style of the tweets.
I also computed the relative frequency of unigrams in RTC and ukWaC, and identified the unigrams that are used disproportionately more often in either of the corpora. Only very frequent unigrams were considered in this analysis (with at least 10,000 occurrences). In Table 9 , we see the most-overrepresented 10 nouns and 10 verbs in RTC and ukWaC. The ranking is done according to the ratio of the number of occurrences in RTC or ukWaC to the number of all occurrences. Note that RTC does not include the part-of-speech tags of the unigrams. The separation of nouns and verbs are done for illustrative purposes only, and unigrams are grouped according to their most frequently assigned POS tags in ukWaC.

Among the most over-represented in RTC are  X  X  X on X  X  and  X  X  X ont X  X . These are presumably misspellings of the auxiliary verb  X  X  X on X  X  X  X , and I confirmed this by spot checks. In RTC, we see nouns and verbs that express emotions and tastes such as love, hate, smile, excited, and hurt. In addition, temporal references such as  X  X  X onight X  X  and  X  X  X omorrow X  X  reflect the immediate nature of Twitter posts. It is harder to categorize the over-represented unigrams in ukWaC. Baroni et al. ( 2009 ) X  X  argument that the presence of entities responsible for publishing content on the Web, such as universities, non-governmental organizations, or governmental departments may bias ukWaC X  X  content seems applicable in this case. Two striking unigrams that are over-represented in RTC are  X  X  X ctober X  X  and  X  X  X ovember X  X  X  X wo months that fell outside of the fetching period of the RTC corpus (which ran from December 2010 to June 2011). Obviously, the under-representation  X  X  X ctober X  X  and  X  X  X ovember X  X  in RTC is an artifact of the time period picked for fetching the data, not a general pattern in social-media. This suggests that one must be careful about the temporal idiosyncrasies that may be present in RTC and similar corpora. 6 Case study: stereotypical gender expectations In this section, I describe a previous study which used the Edinburgh Twitter Corpus X  X  Twitter-based corpus with a comparable size to our new corpus, but not available anymore X  X o predict the stereotypical gender expectations of actions, and to replicate the findings of the original study with the new RTC.

Stereotypical gender expectations play an important role in our daily and social lives whether we agree with or are simply aware of them. In Herdag  X  delen and Baroni ( 2011 ), we argued that in artificial intelligence (AI) applications, such knowledge should be explicitly marked and handled accordingly. We stated  X  X  X hether it X  X  right or wrong, an AI should know that (we expect that) women like shopping and men like football . X  X  (emphasis in original). If stereotypical knowledge is made explicit in the knowledge repositories which AI systems rely on, AI developers will have the opportunity to disregard such knowledge, or make use of them in order to make AI systems relate better to human beings.

In Herdag  X  delen and Baroni ( 2011 ), we focused on the daily activities of people and proposed to use corpus analysis to predict stereotypical expectations about these activities. We used two corpora, the Edinburgh Twitter Corpus and ukWaC, to compute the gender bias of verb phrases. In the Twitter-based corpus, we guessed the gender of each user by using the same tools I used in this study, and computed the number of times each phrase was mentioned by a male or by a female user. In ukWaC, there is no metadata about gender. Instead a pronoun-based heuristic was employed, which looked at the first pronoun or proper name preceding a verb phrase and used its gender to count the male and female occurrences of a verb phrase.
In the Twitter-based gender bias computation, the assumption is that if a certain action is mentioned more often by females it is a more salient action for females (and vice versa for males). The ukWaC-based gender bias quantifies which gender is more frequently reported to perform a given action (e.g.,  X  X  X he became pregnant X  X  would count as a female occurrence). In other words, the RTC provides a measure of  X  X  X ho says what X  X  whereas (web-based) ukWaC provides a measure of  X  X  X ho is reported to be doing what X  X . Below I briefly describe the dataset and methodology in the original study and compare the results with those obtained on RTC with the same methods. Details pertaining to the original study can be found in the original paper (Herdag  X  delen and Baroni 2011 ).
 A publicly available commonsense knowledge repository, OMCS (Speer 2007 ; Havasi et al. 2009 ), was used to create a random sample of verb phrases (e.g., become nurse, enjoy power, impress people, feel cold, etc.). The sampled phrases were rated by human judges on a 5-point scale (i.e., typically/slightly feminine/ masculine or neutral). Each phrase was rated by at least five judges. The average of the scores of a given phrase was used as its gender score . The sampling and annotation process resulted in 441 actions with assigned gender scores. For example, among the phrases the judges deemed the most feminine were  X  X  X ecome nurse X  X ,  X  X  X ake doll X  X , and  X  X  X reshen up X  X . The most  X  X  X asculine X  X  phrases were  X  X  X ant woman X  X ,  X  X  X atch football X  X , and  X  X  X ee pretty girl X  X . This dataset has been made public. 20
The computation of the gender bias of a given phrase starts with determining the number of its male and female occurrences (for Twitter-based scores this is the frequency of the phrase in male and female users X  tweets; for ukWaC-based scores this is the number of times the phrase is preceded by a male or female pronoun or proper name). While looking for occurrences, both the target verb phrases and the corpora are lemmatized, and at most one intermittent token is allowed between two consecutive lemmas of the phrase (e.g.,  X  X  X ecome a nurse X  X  is counted as an occurrence of the phrase  X  X  X ecome nurse X  X ). Let f and m denote the number of female and male occurrences of a verb phrase, respectively. If we assume the phrase is not biased towards either gender, the ratio of male (and female) occurrences should follow a binomial distribution with n = f ? m and p = M /( M ? F ) where F and M are the total female and male occurrences of all verb phrases (i.e., F = M = inherent gender bias. In Herdag  X  delen and Baroni ( 2011 ), we used the standard score (by normal approximation) of male occurrence ratio as the gender score: ( m -pn )/ r where r  X   X  X  X asculine X  X  phrases and negative scores to  X  X  X eminine X  X  phrases. A score of 0 corresponds to the gender-neutral case.

I passed all of the n-grams in RTC through TreeTagger for lemmatization 21 (Schmid 1995 ). The corpus already contains the number of male and female occurrences of n-grams. I used the same phrase detection heuristics to count the male and female occurrences of the verb phrases occurring in the n-grams. To ensure that an occurrence is not double counted because of overlapping n-grams, I applied an additional constraint that a phrase must fully span the n-gram to be counted as an occurrence (i.e., the n-gram starts and ends with the first and last lemmas of the phrase, respectively). Once the female and male occurrence counts were computed, the rest of the gender score computation was carried out as in the original study, described above.

In the original study, Spearman correlations between the computed gender scores and human-based gold standard are reported along with the accuracy of the gender scores in predicting the polarity of the gender bias (whether it is masculine or feminine). In Table 10 , the Spearman correlations reported in the original study and the correlations obtained with the new RTC are given. In  X  X  X atching X  X  cases the Spearman correlation is computed for the verb phrases for which both ukWaC and the Twitter-based corpora have the same gender polarity. The results based on the Edinburgh Twitter Corpus X  X hich contains the full texts of the tweets X  X nd RTC are virtually indistinguishable, and the coverage of RTC is almost as high as the former. Note that combining ukWaC-based scores and Twitter-based scores substantially increases the correlation, in both cases X  X ith a penalty of coverage. In Table 11 , area under the ROC curve (AUC) and accuracy values are reported. In this evaluation, neutral verb phrases (those with a gender bias of 0) are left out. The results based on the binary classification of the directionality of the gender bias are consistent with the correlation-based results. Again, results based on RTC are almost the same with the original results and the coverage is scarcely affected because of the n-gram representation. 7 Conclusions and future work In this study, a large, publicly available dataset of English tweets with gender-of-the-author and time-of-posting tags is introduced. The methods employed for spam removal, gender guessing, local time extraction, and language detection are preliminary analysis of gender-and time-based activity in Twitter reveal interesting patterns that deserve to be studied in detail. Previous gender-based research based on Twitter text is replicated with new data and almost identical results.
 The corpus contains only n-gram statistics as opposed to the full text of tweets. redistributing Twitter-based data. The corpus lacks lemmatization and POS tags. Lemmatization and POS tagging are challenging tasks for Twitter-based text because of the informal and noisy nature of the data. However, there are promising attempts to train Twitter-specific POS taggers (Gimpel et al. 2011 ), and in the future, I plan to evaluate and apply a suitable part-of-speech tagging method on the corpus. Releasing a multi-lingual version of the corpus is also in future plans.
I believe the RTC will be useful for researchers, freeing them from the burden of having to curate a new dataset from scratch, and will also be useful for the research community in general, providing a shared dataset on which different methods and techniques can be compared to each other.
 References
