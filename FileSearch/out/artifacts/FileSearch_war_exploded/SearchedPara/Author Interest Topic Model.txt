 This paper presents a hierarchical topic model that simul-taneously captures topics and author X  X  interests. Our pro-posed model, the Author Interest Topic model (AIT), intro-duces a latent variable with a probability distribution ove r topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model. H.3.1 [ Content Analysis and Indexing ]: Algorithms, experimentation Topic Modeling, Latent Variable Modeling
Attention is being focused on how to model users X  inter-ests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similar-ity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associ-ated with this author. Author-Persona-Topic(APT) [2] in-troduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author X  X  documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona
This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such a s AT and APT. AIT allows a number of possible latent vari-ables to be associated with author X  X  interest, while previo us models limit this number. Therefore, AIT can describe a wider variety of authors X  interests than other models, whic h reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilitie s to previously unseen documents.
This section details our model. Table 1 shows the nota-tions used in this paper. Figure 1 shows graphical models to Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a condi-tional dependency between variables and the plates indicate a repeated sampling with the iteration num-ber shown. This figure shows that each author pro-duces words from a set of topics that are preferred by the author in (a), persona associated with the au-thor in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author. describe the generative process. For modeling each author X  s interest, our proposal, AIT, incorporates document class c it provides an indicator variable that describes which mix-ture of topics each document d takes, into d . Accordingly, AIT represents documents of similar topics as the same doc-ument class in the same way that topic models represent co-occurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author X  X  interest as a mixture of topic variab les  X  (AT) or  X  P a (APT) in each document layer, AIT represents each author X  X  interest as a mixture of document classes  X  in each author layer. Although both  X  a (AT) and  X  P a (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parame -ters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors X  interests.
AIT employs Gibbs sampling to perform inference approx-imation. In the Gibbs sampling procedure, we need to cal-SYMBOL DESCRIPTION A number of authors J number of document classes T number of topics D number of documents V number of unique words A d authors associated with document d D a number of documents written by author a
N d number of word tokens in document d a i author associated with i th token in document d p d persona associated with document d c d document class associated with document d z di topic associated with the i th token in document w di i th token in document d  X  a multinomial distribution of document classes  X  j multinomial distribution of topics specific to in- X  t multinomial distribution of words specific to culate the conditional distributions. The predictive dist ri-bution of adding interest class c d in documents written by author a to topic c d = j is given by P ( j | c \ d ,a, z , X , X  )  X  where n aj \ d represents the number of documents assigned to j in all documents written by author a , except d , and n represents the total number of tokens assigned to topic t in the documents associated with document class j , except token di .

The predictive distribution of adding word w di in docu-ment d written by a to topic z d = t is given by
P ( t | j, z \ di , w , X , X  )  X  where n tw \ di represents the total number of tokens as-signed to word w in topic t , except token di , and n jt \ di represents the total number of tokens assigned to topic t in all tokens assigned to j , except token di .
We focus here on the extraction of interests from given documents, and demonstrate AIT X  X  performance as a gener-ative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that ap-peared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison. In our evaluation, the smoothing parameters  X  ,  X  and Table 2: Perplexity of AT, APT and AIT: This dif-ference between AIT and APT is significant accord-ing to one-tailed t-test with the number of samples G = 100. For fair comparison, the number of topic variables T was fixed at 200, the number of docu-ment classes J was fixed at 40(AIT). Results that differ significantly by t-test p &lt; 0 . 01 , p &lt; 0 . 05 from APT are marked with  X ** X ,  X * X  respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.
 Iteration 2000 4000 6000 8000 10000 Avg  X  were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on ma-chines with Dual Core 2.66 GHz Xeon processors.

To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated pa-rameters and compared the resulting values.

Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is al-gebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather tha n grouping documents by a given author or persona under a few topic distributions. This implies that clustered docu-ments contain less noise than otherwise. If the number of document classes is overly restricted, the difference betwe en the observed data and the data generated by the model un-der test increases, which raises the perplexity.
Our proposed model, AIT, supports the expression of top-ics in text documents and can identify the interests of au-thors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of in -terests and topics. [1] D. J. D. Aldous. Exchangeability and related topics , [2] D. Mimno and A. McCallum. Expertise modeling for [3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L.
