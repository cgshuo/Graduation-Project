 Learning from imbalanced multilabel data is a challenging task. It has attracted considerable attention recently. In this paper we propose a MultiLabel Best First Over-sampling (ML-BFO) to improve the performance of multilabel classification algorithms, based on imbalance minimizati on and Wilson X  X  ENN rule. Our experimental results show that ML-BFO not only duplicates fewer samples but also reduces the imbalance level much more than two state-of-the-art multilabel sampling methods, i.e., an over-sampling method LP-ROS and an under-sampling method MLeNN. Besides, ML-BFO significantly improves the performance of multilabel classification algorithms, and performs much better than LP-ROS and MLeNN. I.2.6 [ Artificial Intelligence ]: Learning -Induction Resampling, Heuristic, Multilabel learning. Multilabel imbalance is a typical problem of multilabel classification. It occurs when there are many more examples containing one label than thos e containing other labels in multilabel classification. Under imbalance circumstances, most multilabel classification algorithms fail to learn a proper model to represent the distributive characteristics of a multilabel dataset. Their prediction accuracies across multiple labels are relatively low. A few existing methods deal with this problem through adapting multilabel classification algorithms. Charte et al. [1] introduced two resampling methods LP-ROS and LP-RUS. LP-ROS is a random over-sampling method, whic h generates new samples in which minority labels appear . LP-RUS is a random under-sampling method, which deletes some samples associated to majority labels. They further developed a heuristic multilabel under-sampling method MLeNN [2 ], based on the well-known Wilson X  X  Edited Nearest Neighbor (denoted as ENN) Rule [3]. The main idea behind is to select the most proper samples associated with majority labels to remove. Their experimental results show that MLeNN performs significantly better than LP-RUS. According to our knowledge, LP-ROS is the only one state-of-the-art over-sampling algorithm for multilabel classification. It randomly selects samples for duplication. Although LP-ROS works well in many cases, it fails to satisfy two key desiderata as follows:  X  Consistently reducing the level of imbalance As we know, LP-ROS is a ra ndom over-sampling method. It could not guarantee that the imbalance level of a preprocessed multilabel dataset decreases. Charte et al. [1] mentioned how to measure the imbalance level in a multilabel dataset, but it didn X  X  study the change of the imbalance level after a multilabel dataset is preprocessed by LP-ROS. We would like to know to what extent a preprocessing method reduces the imbalance level of a multilabel dataset.  X  Bias Correction Based on the experimental results in [1], we found that LP-ROS does not improve accuracy, precision, or recall in a consistent manner. Intuitively, we think the overfitting of over-sampling could be one of potential reasons. We would like to minimize the number of samples duplicated in over-sampling to avoid overfitting, so that the prediction performance of a multilabel classification model built from the preprocessed multilabel dataset increases while the imbalance level of a multilabel dataset preprocessed decreases. Therefore, we propose another multilabel over-sampling method MultiLabel Best First Over-sampling (ML-BFO), which heuristically rather than randomly selects samples for duplication. Its heuristic is founded on imba lance minimization and Wilson X  X  Edited Nearest Neighbor Rule [3], relying on MeanIR measure [1] to assess the imbalance level of a multilabel dataset, as well as on a distance metric between the sets of labels (labelsets) appearing in each pair of instances. Learning from imbalanced data has been thoroughly studied in the context of traditional binary cla ssification. The measurement of the imbalance level in a dataset is obtained as the ratio of the number of samples belonging to a majority class and the number of sample belonging to a minor ity class, known as imbalance ratio. The higher the imbalance ratio, the greater the imbalance level is. However, traditional under-samp ling and over-sampling methods for single-label imbalance classifica tion cannot be directly used in multilabel classification, as they are designed to work with only one output class label. Besides, most prep rocessing methods were developed for binary classifica tion, assuming only one minority label and one majority label. Thus, an approach to preprocess multilabel datasets, which have a se t of labels as their outputs and several of them could be consid ered minority/majority labels, is needed. Recently, a triad of measures of assessing the imbalance level of multilabel datasets were proposed in [1], along with two sampling algorithms, an over-sampling (LP-ROS) and an under-sampling (LP-RUS). LP-ROS clones random samples with minority labelsets until the size of the rebalanced dataset is 25% larger than its original size of a multilabel dataset. Charte et al. [1] concluded that LP-ROS obtains be tter experimental results than LP-RUS does, under different pe rformance evalua tion measures, such as accuracy, precision and recall. They also claimed that LP-ROS is able to improve classification results when it is applied to multilabel datasets with a very large level of imbalance. Our proposed method ML-BFO is closely related to LP-ROS. However, it introduces a complete new approach, best-first replication, to over-sample heuristically selected samples with minority labelsets. An instance in a multilabel dataset may have a group of relevant that the imbalance level of a multilabel dataset monotonically decreases, since it is highly possible that over-sampling increases the occurrence of both the minority and majority labels simultaneously. Meanwhile, the instances that only contain minority labels are scarce. Over-sampling these scarce  X  X oisy X  instances would impact subconcepts learned from the preprocessed data. As a result, c hoosing the most proper instances to duplicate is of critical importan ce. ML-BFO takes three steps to gradually handle its selection. Fi rst, ML-BFO ranks the instances that act as candidates using an instance tree. Second, it finds the proper instance according to the measure MeanIR. Finally, it adopts ENN to probe whether ca ndidates are outliers using a distance metric, which measures the difference between any pair of labelsets. ML-BFO intends to replicate inst ances with minority labels. The principle behind ML-BFO is: if a label appears less frequently, its related instances should have the higher probability to be cloned. Note that ML-BFO tries to create a higher balanced superset from a multilabel dataset. As we know, any labelset of the multilabel set Y of a multilabel dataset could contain both minority and majority labels. If there are more minority labels and fewer majority labels appearing in th is labelset, ML-BFO preferably selects instances belonging to this subset to clone. In order to find these instances, we construct a si mple rank tree with three layers: a root, a label node layer, and a l eaf node layer. Its root layer has one root node, which contains all instances of a multilabel dataset D. Its label node layer contains |Y| label nodes, where each label node corresponds to a label in Y. The |Y| label nodes are ranked from right to left according to the frequency of each label in D. That is, the most frequent label locates on the rightmost. The details of ranking these label nodes will be discussed in the next subsection. Its leaf node layer c ontains many leaf nodes, where each leaf node represents an inst ance whose labelset contains the label represented in its parent label node. Notice that an instance may appear several times in the leaf node layer of the instance tree, since the labelset of the instance is composed of one or many labels. Candidate selection is an impor tant component of our ML-BFO. In order to choose proper instances to be candidates for replication, a method to tell that which labels are minor is needed. To complete this task successfully, ML-BFO relies on two measures (IRL bl and MeanIR) proposed in [1]. IRL bl is a measure to assess the imbalance level of each label in Y. The IRL bl value of the most frequent label is set as 1. Then, the IRL bl values of other labels should be greater than 1. In general, the higher the IRL bl value of a label, the greater the imbalance level imbalance level of an entire multila bel dataset. MeanIR is such a measure which measures the average of the IRL bl values of all labels of a multilabel dataset. ML-BFO ranks the label nodes in the instance tree according to their corresponding IRL bl s. The label node with the largest IRL bl locates at the leftmost, under which the leaf nodes (child instances) are preferably selected . That is, these instances have more odds to be proliferated. They are the candidates. ML-BFO takes two steps to evaluate th ese candidates one by one to determine which is proper to be cl oned. The first step of ML-BFO is to evaluate whether a candidate instance can reduce the MeanIR of the current superset of the multilabel dataset D. Note that the superset of a multilabel dataset is an enlarged multilabel dataset with some already duplicated instances. Only the candidates that can reduce the MeanIR of the current superset are further evaluated in the second step of ML-BFO for duplication. The second evaluation step of ML-B FO is a heuristic evaluation. It is based on Wilson X  X  ENN rule . As we know, the ENN rule has been extensively used in traditional classification. It is often used to remove an example whose class differs from the class of at least two of its three nearest ne ighbors. ML-BFO uses ENN in its second evaluation step to exclude suspicious candidates to duplicate, so that it avoids noise proliferation. Specifically, ML-BFO proposes a simple similar ity measure to evaluate the similarity of a candidate to each of its neighbors. This simple similarity is proposed based on our investigation. According to our study, we found that only a small proportion of instances share more than one label with their nearest neighbors in a multilabel dataset. Thus, we define a loose criterion to evaluate the similarity between a candidate and its neighbors. If the labelset of the candidate and the labelset of one of its neighbors contain one common label, the ca ndidate is considered to be similar to this neighbor. For a ca ndidate, if there are more than half neighbors similar to it, this candidate will be duplicated. After the candidates under the first label node are used up, and the the beginning of this preprocessing method ML-BFO, ML-BFO moves to the next label node. The instances under the next label node become candidates. ML-BFO evaluates each candidate as before and duplicates proper candidates. ML-BFO keeps evaluating each instance under each label node until either the size of the superset reaches or the candidates under the most frequent label node in the instance tree are evaluated. Note that ML-BFO iteratively calculates the MeanIR value of the superset when a candidate is duplicated. Thus, each duplication can reduce the MeanIR value. The pse udo code of our over-sampling algorithm ML-BFO is shown in Algorithm 1. sampleToDuplicate Output: Resampled dataset R Construct an instance tree from D while N &lt; sampleToDuplicate do candidates = instances whose labels et contains with the largest unvisited IRLbl foreach candidate in candidates label if newMeanIR &gt; currentMeanIR end for R = R  X  bestCandiate 
N = N + 1 end while We conduct experiments on four datasets from MULAN repository, the characteristics of which are shown in Table 2. Table 1. The characteristics of the four multilabel datasets. 
Dataset #Ex. #Atts. #La bels MaxIR MeanIR emotions 593 72 6 1.78 1.48 As we know, the main purpose of preprocessing is to improve the model performance. We investigated the performance of our ML-BFO, comparing with LP-ROS and MLeNN, under the three different multilabel classification algorithms, Binary Relevance (BR) [4], Calibrated Label Ranking (CLR) [5], and Random K-labelsets (RAKEL) [6], respectively. In our ML-BFO, the number of duplications is set as 25% of its original size. Our experimental results obtained through a 5-fold cross-validation are shown in Figures 1-6 respectively. Figures 1-3 show the average accuracy of each approach under each multilabel classification algorithm respectively. Figures 4-6 show the average F-measure value of each approach under each multilabel classification algorithm respectively. These figures show that ML-BFO significantly improves the performance of all three multilabel classification algorithms on all four datasets, in terms of accuracy and F-measure. The performance of LP-ROS is consistent under different multilabel classification algorithms, but varies with the datasets. The performance of MLeNN depends on the underline multilabel classification algorithms and also varies with the datasets. On the dataset emotions, MLeNN improves the performance of Binary Relevance. However, it doesn X  X  improve the performance of both Calibrated Label Ranking and Random K-labelsets. It downgrades the performance of Calibrated Label Ranking. On the dataset Corel5k, MLeNN consistently performs better than LP-ROS, improving the performance of all three multilabel classification algorithms. On the other two datasets, MLeNN performs similarly to LP -ROS. Figures 1-6 show that LP-ROS only consistently impr oves the performance of all the three multilabel classification algorithms on the dataset emotions, but not on the other three datasets. It even downgrades the performance of all the three multilabel classification algorithms on the two datasets enron and yeast. Among the three sampling methods, it is obvious that our ML-BFO consistently performs the best, under different multilabel cl assification algorithms and on different datasets, in terms of prediction accuracy and F-measure. 
Algorithm 1. Multilabel Best First Over-sampling. In this paper, we proposed a novel over-sampling method ML-BFO for imbalanced multilabel classification. This method heuristically selects examples to clone to reduce the imbalance level of a multilabel dataset. To avoid the proliferation of potential noises, it further utilizes Wilson X  X  ENN rule to refine its selection. Our experi mental results showed that our method ML-BFO not only duplicates fewer examples for a multilabel dataset, but also reduces its imbalance level much more, comparing with the state-of-the-art multilabel over-sampling method LP-ROS. Furthermore, in terms of prediction accuracy and F-measure, ML-BFO not only performs much better than the state-of-the-art over-sampling method LP-ROS, but also performs much better than the state-of-the-art under-sampli ng method MLeNN on all four datasets under all three multilabel classification algorithms. This research was partially supported by the Natural Science Foundation of China under grant No. 61170020, 61402311 and 61440053, the Jiangsu Province Colle ges and Universities Natural Science Research Project under grant No. 13KJB520021, the U.S. National Science Foundation (IIS-1115417), and the Jiangsu Province Postgraduate Cultiva tion and Innovation Project under grant No. ZY32001814. [1] Charte, F., Rivera, A.J., Jesus, M.J.D., and Herrera, F. 2013. [2] Charte, F., Rivera, A.J., Jesus, M.J.D. and Herrera, F. 2014. [3] Wilson, D.L. 1972. Asympto tic Properties of Nearest [4] Godbole, S. and Sarawagi, S. 2004. Discriminative Methods [5] F X rnkranz, J., H X llermeier, E., Loza Menc X a, E. and Brinker, [6] Tsoumakas, G. and Vlahavas, I. 2007. Random k-labelsets: 
Figure 4. The F-measure value s on the four datasets 
