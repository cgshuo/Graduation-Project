 In this paper we adopt the general framework for summarization put forward by Sp  X  arck-Jones (1999)  X  which views summarization as a three-fold process: interpretation, transformation and generation  X  and attempt to provide a clean in-stantiation for each processing phase, with a par-ticular emphasis on the last, summary-generation phase often omitted or over-simplified in the main-stream work on summarization.

The advantages of looking at the summarization problem in terms of distinct processing phases are numerous. It not only serves as a common ground for comparing different systems and understand-ing better the underlying logic and assumptions, but it also provides a neat framework for devel-oping systems based on clean and extendable de-signs. For instance, Gong and Liu (2002) pro-posed a method based on Latent Semantic Anal-ysis (LSA) and later J. Steinberger et al. (2007) showed that solely by enhancing the first source interpretation phase, one is already able to pro-duce better summaries.

There has been limited work on the last sum-mary generation phase due to the fact that it is unarguably a very challenging problem. The vast amount of approaches assume simple sentence se-lection, a type of extractive summarization, where often the summary representation and the end summary are, indeed, conflated.

The main focus of this work is, thus, to in-vestigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This decision is also mo-tivated by empirical evidence from TAC 2009 data (see table 1) showing that human summaries con-tain on average more and shorter sentences than the system summaries. The intuition behind this is that, by containing more sentences, a summary is able to capture more of the important content from the source.

Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data 1 yield ROUGE scores (Lin and Hovy, 2003) com-parable to the participating systems in the Sum-marization task at TAC 2009. Taking into account that our approach is completely unsupervised and language-independent, we find our preliminary re-sults encouraging.

The remainder of the paper is organised as fol-lows: in the next section we briefly survey the related work, in  X  3 we describe our approach to summarization, in  X  4 we explain how we tackle the generation step, in  X  5 we present and discuss our experimental results and towards the end we conclude and give pointers to future work. There is a large body of literature on summariza-tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec et al., 1995). The most closely related work to the approach presented hereby is work on summariza-tion attempting to go beyond simple sentence ex-traction and to a lesser degree work on sentence compression. We survey below work along these lines.

Although our approach is related to sentence compression (Knight and Marcu, 2002; Clarke and Lapata, 2008), it is subtly different. Firstly, we reduce the number of terms to be used in the sum-mary at a global level, not at a local per-sentence level. Secondly, we directly exploit the resulting structures from the SVD making the last genera-tion step fully aware of previous processing stages, as opposed to tackling the problem of sentence compression in isolation.

A similar approach to our sentence reconstruc-tion method has been developed by Quirk et al. (2004) for paraphrase generation. In their work, training and test sets contain sentence pairs that are composed of two different proper English sen-tences and a paraphrase of a source sentence is generated by finding the optimal path through a paraphrases lattice.

Finally, it is worth mentioning that we are aware of the  X  X apsule overview X  summaries proposed by Boguraev and Kennedy (1997) which is similar to our TSR (see below), however, as opposed to their emphasis on a suitable browsing interface rather than producing a readable summary, we precisely attempt the latter. We chose the LSA paradigm for summarization, since it provides a clear and direct instantiation of Sp  X  arck-Jones X  three-stage framework.

In LSA-based summarization the interpreta-tion phase takes the form of building a term-by-sentence matrix A = [ A 1 ,A 2 ,...,A n ] , where each column A j = [ a 1 j ,a 2 j ,...,a nj ] T represents the weighted term-frequency vector of sentence j in a given set of documents. We adopt the same weighting scheme as the one described in (Stein-berger et al., 2007), as well as their more general definition of term entailing not only unigrams and bigrams, but also named entities.

The transformation phase is done by applying singular value decomposition (SVD) to the initial term-by-sentence matrix defined as A = U  X  V T .
The generation phase is where our main contri-bution comes in. At this point we depart from stan-dard LSA-based approaches and aim at produc-ing a succinct summary representation comprised only of salient terms  X  Term Summary Represen-tation (TSR). Then this TSR is passed on to an-other module which attempts to produce complete sentences. The module for sentence reconstruc-tion is described in detail in section 4, in what fol-lows we explain the method for producing a TSR. 3.1 Term Summary Representation To explain how a term summary representation (TSR) is produced, we first need to define two con-cepts: salience score of a given term and salience threshold . Salience score for each term in matrix A is given by the magnitude of the corresponding vector in the matrix resulting from the dot product of the matrix of left singular vectors with the diag-onal matrix of singular values. More formally, let T = U  X   X  and then for each term i , the salience score is given by | ~ T i | . Salience threshold is equal to the salience score of the top k th term, when all terms are sorted in descending order on the basis of their salience scores and a cutoff is defined as a percentage (e.g., top 15%). In other words, if the total number of terms is n , then 100  X  k/n must be equal to the percentage cutoff specified.

The generation of a TSR is performed in two steps. First, an initial pool of sentences is selected by using the same technique as in (Steinberger and Je  X  zek, 2009) which exploits the dot product of the diagonal matrix of singular values with the right singular vectors:  X   X  V T . 2 This initial pool of sen-tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are identified in the initial pool of sentences and those terms whose salience score is above the salience threshold are copied across to the TSR. Thus, the TSR is formed by the most (globally) salient terms from each one of the sentences. For example: This section describes a probabilistic approach to the reconstruction problem. We adopt the noisy-channel framework that has been widely used in a number of other NLP applications. Our interpre-tation of the noisy channel consists of looking at a stemmed string without stopwords and imagining that it was originally a long string and that some-one removed or stemmed some text from it. In our framework, reconstruction consists of identifying the original long string.

To model our interpretation of the noisy chan-nel, we make use of one of the most popular classes of SMT systems: the Phrase Based Model (PBM) (Zens et al., 2002; Och and Ney, 2001; Koehn et al., 2003). It is an extension of the noisy-channel model and was introduced by Brown et al. (1994), using phrases rather than words. In PBM, a source sentence f is segmented into a sequence of I phrases f I = [ f 1 ,f 2 ,...f I ] and the same is done for the target sentence e , where the notion of phrase is not related to any grammatical assump-tion; a phrase is an n-gram. The best translation e best of f is obtained by: e best = arg max e p ( e | f ) = arg max e where  X  ( f i | e i ) is the probability of translating a phrase e i into a phrase f i . d ( a i  X  b i  X  1 ) is the distance-based reordering model that drives the system to penalize substantial reorderings of words during translation, while still allowing some flexibility. In the reordering model, a i denotes the start position of the source phrase that was trans-lated into the i th target phrase, and b i  X  1 denotes the end position of the source phrase translated into the ( i  X  1 th ) target phrase. p LM ( e i | e 1 ...e is the language model probability that is based on the Markov chain assumption. It assigns a higher probability to fluent/grammatical sentences.  X   X  ,  X 
LM and  X  d are used to give a different weight to each element (for more details see (Koehn et al., 2003)).

In our reconstruction problem, the difference between the source and target sentences is not in terms of languages, but in terms of forms. In fact, our source sentence f is a stemmed sentence with-out stopwords, while the target sentence e is a complete English sentence.  X  X ranslate X  means to reconstruct the most probable sentence e given f inserting new words and reproducing the inflected surface forms of the source words. 4.1 Training of the model In Statistical Machine Translation, a PBM system is trained using parallel sentences, where each sen-tence in a language is paired with another sentence in a different language and one is the translation of the other.

In the reconstruction problem, we use a set, S 1 of 2,487,414 English sentences extracted from the news. This set is duplicated, S 2 , and for each sen-tence in S 2 , stopwords are removed and the re-maining words are stemmed using Porter X  X  stem-mer (Porter, 1980). Our stopword list contains 488 words. Verbs are not included in this list, because they are relevant for the reconstruction task. To optimize the lambda parameters, we select 2,000 pairs as development set.
An example of training sentence pair is:
In this work we use Moses (Koehn et al., 2007), a complete phrase-based translation toolkit for academic purposes. It provides all the state-of-the-art components needed to create a phrase-based machine translation system. It contains different modules to preprocess data, train the Language Models and the Translation Models. For our experiments we made use of the TAC 2009 data which conveniently contains human-produced summaries against which we could eval-uate the output of our system (NIST, 2009).

To begin our inquiry we carried out a phase of exploratory data analysis, in which we mea-sured the average number of sentences per sum-mary, words per sentence and words per summary in human vs. system summaries in the TAC 2009 data. Additionally, we also measured these statis-tics of summaries produced by our system at five different percentage cutoffs: 100%, 15%, 10%, 5% and 1%. 4 The results from this exploration are summarised in table 1. The most notable thing is that human summaries contain on average more and shorter sentences than the system summaries (see 2nd and 3rd column from left to right). Sec-ondly, we note that as the percentage cutoff de-creases (from 4th column rightwards) the charac-teristics of the summaries produced by our system are increasingly more similar to those of the hu-man summaries. In other words, within the 100-word window imposed by the TAC guidelines, our system is able to fit more (and hence shorter) sen-tences as we decrease the percentage cutoff.
Summarization performance results are shown in table 2. We used the standard ROUGE evalu-ation (Lin and Hovy, 2003) which has been also used for TAC. We include the usual ROUGE met-rics: R 1 is the maximum number of co-occurring unigrams, R 2 is the maximum number of co-occurring bigrams and R SU 4 is the skip bigram measure with the addition of unigrams as counting unit. The last five columns of table 2 (from left to right) correspond to summaries produced by our system at various percentage cutoffs. The 2nd col-umn, LSA extract , corresponds to the performance of our system at producing summaries by sentence extraction only. 5
In the light of the above, the decrease in per-formance from column LSA extract to column  X  X t 100% X  can be regarded as reconstruction error. 6 Then, as we decrease the percentage cutoff (from 4th column rightwards) we are increasingly cover-ing more of the content comprised by the human summaries (as far as the ROUGE metrics are able to gauge this, of course). In other words, the im-provement of content coverage makes up for the reconstruction error, and at 5% cutoff we already obtain ROUGE scores comparable to LSA extract . This suggests that if we improve the quality of our sentence reconstruction we would potentially end up with a better performing system than a typical LSA system based on sentence selection. Hence, we find these results very encouraging.

Finally, we admittedly note that by applying a percentage cutoff on the initial term set and further performing the sentence reconstruction we gain in content coverage, to a certain extent, on the ex-pense of sentence readability. In this paper we proposed a novel approach to summary generation from summary representa-tion based on the LSA summarization framework and on a machine-translation-inspired technique for sentence reconstruction.

Our preliminary results show that our approach is feasible, since it produces summaries which re-semble better human summaries in terms of the av-erage number of sentences per summary and yield ROUGE scores comparable to the participating systems in the Summarization task at TAC 2009. Bearing in mind that our approach is completely unsupervised and language-independent, we find our results promising.

In future work we plan on working towards im-proving the quality of our sentence reconstruction step in order to produce better and more readable sentences.
