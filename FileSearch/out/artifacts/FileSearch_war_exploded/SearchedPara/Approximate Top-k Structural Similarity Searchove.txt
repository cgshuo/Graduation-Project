 Nowadays, Extensible Mark-up Language (XML) is becoming pervasive in more and more applications, such as Digital Library, XML subscribe/publish system, and other XML repositories. The structural similarity between individual XML documents can be used to determine the category of a new document with respect to a collection of pre-categorized documents. After modelling XML documents as rooted ordered labeled trees, simila r XML documents mostly have similar tree structures. Then, this problem can be transformed to computing structural similarity among trees. Unfortunately, comparing XML trees is an expensive operation, so some elegant and effi cient algorithms are needed.

There are some related works on the similarity computation on trees [1, 3, 4, 5, 6, 7]. The common measure they use to d escribe the difference between trees is edit distance. One tree can be change d to another through a sequence of edit operations ( insert , delete or relabel ). Each edit operation is assigned a nonnega-tive cost. The sum of all the costs of operations in the sequence is called the cost of the whole edit sequence. For a given transformation, there might exist infinite number of sequences for conducting such a transformation; therefore, the edit distance between two trees is defined as the cost of the edit sequence with the minimum cost.

The algorithm in [1] is so far the best one for computing tree edit distance, which is called ZS algorithm in the thereafter in this paper. Because their method should match nearly each pair of nodes in two trees, the computation is time-consuming and so not accepta ble in real world applicat ions. Recently, some ap-proximate algorithms for XML documents similarity search has been proposed to make the tree edit distance computation practical and efficient. These works mostly focus on modifying the allowed edit operations or setting restrictions on them. Different from those works, [2] proposes another approach by reducing nesting and repetitions in order to get the summary structure of a tree before applying edit distance algorithm. Generally, the summarized tree has quite less number of nodes than the original tree, so their approach reduce the computation cost dramatically.

On the other hand, most existing work [1, 2, 6] assign the same cost for each unit edit operations on all nodes. However, it can be easily observed that differ-ent nodes in an XML tree may be of totally different significance. For instance, in Fig. 1, node SigmodRecord can be more significant than nodes author in-tuitively, because SigmodRecord represent a larger/highe r category/semantics information than author . As a result, taking the weight characteristics in struc-ture into account will be helpful to reach a better understanding for structural similarity.
 This paper is devoted to address the af orementioned problem in some extent. The contribution of the work reported here could be summarized as follows 1. Different from the existing works, in this paper, we introduce the weight 2. Based on nodes X  weight, an algorithm for obtaining the summary structure 3. We present a new cost model for edit operations on weighted nodes and a
The rest of this paper is organized as fo llows. Section 2 reviews the related work on structural similarity over XM L documents. Section 3 introduces two weight functions on XML trees. Sectio n 4 introduces our improved summary algorithm, in which, weight information are accumulated when deleting some repeated nodes. And the summary distance between two trees is defined in Sect. 4. The edit distance algorithm based on the new cost model is given in Sect. 4. Section 5 is for the experiments and the analysis of the experimental results. Finally, in Sect. 6 some concluding remarks are given. Structural similarity between two XML documents has been studied in the lit-erature [2, 9, 12, 13]. Most of the them are based on the similarity search over trees after modelling XML documents as rooted ordered labeled trees. The edit distance [1] measure is an intuitive way to describe the distance between two trees. However, the comput ation of edit distance between two trees is of quite high complexity. For instance, the time complexity of ZS algorithm in [1] is O ( | M || N | depth ( M ) depth ( N )), and the time complexity of Chawathe X  X  algorithm in [10] is O ( | M || N | ), where M and N are the number of nodes in two trees re-spectively, and depth(M) and depth(N) are the heights of two trees respectively. To improve the efficiency of these algorithms, [2, 8, 9] consider adding restric-tions on the allowed edit operations, which is different from [1] allowing insert and delete operations on anywhere in the trees. [8] proposes a Restricted Top Down Mapping ( RTDM ) in transforming a tree into another, and it only looks for identical subtrees at the same level. T his modification results in better per-formance though the worst time complexity is still O ( | M || N | ). [9] adds both restrictions on insert and delete operations, i.e., they are only allowed on leaf nodes, and introduces new InsertTree and DeleteTree operations. These two re-strictions reduce unnecessary mapping operations between node pairs in [1, 10]; however, the overall complexity is still O ( | M || N | ). [2] adopts another approach, which is based on the structural summary obtained by reducing nesting and repetition nodes. But it loses some structural description information, such as repeated number.

In addition to edit distance, representing XML with vector or feature [11, 12] is also an alternative approach which is originally from data mining or informa-tion retrieval field. It can run fast but achieve approximate results for real edit distances, so such an approach can only be used in filter-and-refine applications.
In a word, the previous researches compute the structural similarity of XML documents either using tree edit distance or using simplified representation, such as structural summary or feature vector. However, computing tree edit distance with ZS approach is a time-consuming task, so it is not practical for similar-ity search over large XML data repositories. On the other hand, simplified ap-proaches ignore some information of XML structure, which might be important for searching. The work reported here is to design some new techniques based on tree edit distance to calculate the st ructural similarity of XML documents. In this section, we give some formal definitions about XML document and edit distance on XML trees. And then, we introduce two weight functions on XML document tree, the first one is linear to lev el and the second one is exponential to the level. These weight functions will be used for distance computing in Sect. 4. 3.1 Problem Statement We represent XML document as a rooted ordered labeled tree, as in Fig. 1, which is an XML tree for the SIGMOD Record dataset [14]. Each node in the tree stands for an element in XML document. With this tree model, the key problem of answering Top-k queries in XML repositories could be transformed to the similarity computation over trees.

The measure with Edit Distance [1] is one of the best approaches to describe distance between two trees in the lit erature. Our approach is based on edit distance as well. We introduce edit operations at first. Generally, there are three kinds of edit operations: insert , delete and relabel .(1) Insert means to insert a node, (2) delete means to delete a node while remaining its subtrees, (3) relabel means to change a node X  X  label. Given two trees, we can transform one into another by performing a sequence of such edit operations over the original tree. Such a sequence can be represented by a g raphical mapping between two trees. For example, the mapping in Fig. 2 shows the way to transform the left tree into the right one. Dotted lines map the nodes in the left tree to the nodes in the right one. Two nodes connected by such a dotted line are checked to see if they have the same labels. If not, a relabel operation is done. For those nodes in the left (right) tree which aren X  X  reached by the dotted lines, some delete ( insert ) operations are needed.

Since one node can be mapped to any node in another tree or not, there are a large number of possible mappings between two trees. With a cost model illustrating costs for edit operations, each mapping is associated with an overall cost of transforming one tree into another. Thus, the edit distance problem is actually equal to finding a mapping between trees with the minimal cost. But such a computing process is time consuming, and the previous works [1, 10] address this problem using dynamic programming algorithms. We will introduce a new approach for computing edit distance over XML documents based on structural summary in Sect . 4. Before introducing the new cost model and weight definition, two weight functions should be introduced at first. 3.2 Linear Weight Function Consider a rooted labeled tree T .Let depth ( T )(  X  1) denote its depth and level ( n )denotethelevelofnode n ( level ( root ) = 0). The linear weight function weight ( n ) could be defined for a node n :
This formula builds a relation between the weight of node and its level. The weights of all nodes fall in the range of (0, 1]. The weight of the root node is 1 and weights for other nodes are all smaller than 1 but bigger than 0. This weight is easy to calculate with linear time and we can get it by traversing the tree starting from the root and to all nodes in the tree. This weight function will be used for distance computation in Sect. 4. Figure 3 is an example about XML tree and the linear weight for nodes in each level. 3.3 Exponential Weight Function The second weight function is similar to [13]. Assuming taht the depth of a tree T is d , i.e. depth ( T )= d . We assign weight of  X  d to the root node.  X  is a parameter which can be adjusted to reflect the importance of level information in the tree. For children of the root node, that is, the nodes on level 1, we assign weight of  X  d  X  1 to them. According to this rule, the nodes at level i get weight of  X  d  X  i , and the resulting weights of all nodes fall in the interval of [  X ,  X  d ]. We define the exponential weight function as follows: It can be seen that this weight function tr eat the nodes on different levels very differently. Given the tree in Fig. 3, and assume  X  = 2, we can get the expo-nential weights of the nodes as in Fig. 4. Both liner and exponential weight functions are easy to compute. The motiv ation of these weights is to reflect the node X  X  significance or relevance from th e semantic viewpoint. For example, edit operations in the higher level of tree should have more influence than the edit operation on the leaf nodes. But the previous works on tree edit distance ignores this difference and treats all edit operations with same/uniform cost. In Sect. 5, we will show the experiments result by using the weight function defined here. In this section, we present the algorithm on structural summary by using the weight defined in last section. We also introduce the new cost model for XML summary distance. 4.1 Structural Summary From the above discussion, it could be seen that the computation of the edit distance of two original trees is extremel y expensive. Therefore, we have to sim-plify the trees before applying the distance algorithm. Note the fact that XML documents have many identical sub-structures, that is, there are many nodes with the same paths from the root to them. Here, we ignore the position infor-mation (i.e.,  X /a[1]/b[1] X  and  X /a[1]/b[2] X  are regarded to be the same paths.), and we can treat these nodes as belonging to an equivalent class and use one node to represent them. An example is given in Fig. 5. The original tree T 1 and its summary structure after simplification are shown there. The change of the weight information of the two trees are listed in the right columns. Taken node A as an example, where A is the fifth child of root node R , there is already a node A with the same path of R/A appearing before it. So, we add its children nodes to node A before deleting it. In Fig. 5, we use the linear weight function. For the second node A to be deleted, we add its weight 2 / 3 to the remaining node A , so the new weight of node A is 2 / 3+2 / 3=4 / 3. With such a simplification, the number of the nodes is reduced from 16 to 11, consequently the computation cost for tree distance calculation is reduced.

We combine the two procedures, one for simplifying the original tree, the other for weight assigning, in our Algorithm 1. When we calculate the summary structure of a Tree T rooted at node n , the weight information for n  X  X  children is initialized at first, then n  X  X  child nodes with the same label are combined. If one child node has sibling nodes occurring after it with the same label, those repeated sibling nodes will be deleted, and their weights are added to it. After this procedure, there are no repeated nodes in the level right below n .Foreach child node of n , this procedure is invoked recursively. 4.2 Summary Distance Based on New Cost Model In this section we introduce a new cost model based on weight and simplified XML tree. Based on ZS tree distance, th e cost model is redefined as follows: 1. insert :  X  (  X  X  X   X  )= weight (  X  ) 2. delete :  X  (  X   X  X  X  )= weight (  X  ) 3. relabel :  X  (  X   X   X  )= where,  X  denotes the cost for each edit operation,  X   X  X  X  means deleting node  X  in the source tree, and  X  X  X   X  means inserting node  X  into the source tree. The explanations for the insert and delete costs are obvious. For the relabel Algorithm 1 getSummary operation, though their labels may be the same, the numbers of different nodes they represent can be different, so the difference of their weights is used to denote the relabel cost. If their labels are different, we assign the sum of their weights to this operation. Based on the ZS edit distance algorithm, we reinvent the definition of edit distance to our n ew summary distance on weighted trees. Definition 1. Given two trees T 1 and T 2 , and assuming that S ( T 1 ) and S ( T 2 ) are the associated summary structures, respectively. Let ED ( T 1 ,T 2 ) denote the edit distance between two trees and weight ( T ) denote the sum of all nodes X  weight in tree T . The weighted summary distance WD of two trees is defined as: To illustrate the computation of edit distance with our new cost model, in Fig. 6, we give another tree T 2 , and its summary structure can be obtained using the algorithm in Sect. 4.1. Weight information is assigned using the linear weight function. Now, we calculate the cost of transforming the summary structure of tree T 1 in Fig. 5 to the summary structure of tree T 2 in Fig. 6. The operation se-( delete ( Q ) , 1 / 3), ( insert ( M ) , 1 / 3) and ( delete ( M ) , 1 / 3).
Suppose ( insert ( A ) , X  ) stands for inserting a node A with cost  X  .Thesum of the costs of the whole sequence is relabel ( B, B )+ relabel ( C, D )+ delete ( I )+ delete ( Q )+ insert ( M )+( delete ( M )=10 / 3. Note that, if we use traditional cost model, edit operations like relabel ( B, B ) should have cost of 0. In our algorithm, we treat this node representing a set of e quivalent nodes. So, we should do a set of difference operations though their labels are the same. From weight ( T 1 )=21 / 3 and weight ( T 2 )=17 / 3, we get the final summary distance between T 1 and T 2 is (10 / 3) / (21 / 3+17 / 3) = 10 / 38. This value describes the difference of these two trees. In fact, since the sum of the two weights is the upper bound of the edit distance between their su mmary structures, the summary distance is always in [0 , 1]. 4.3 Answering Top-k Queries To answer top-k queries, the steps of o ur algorithm are described as follows : 1. First, construct tree models for all XML documents. 2. Get summary structure for each tree using our algorithm getSummary (), 3. Use WD distance formula to calculate di stances between the summary struc-4. Sort the distance array in ascending order and then return the first k (may
Because this procedure is quite simple, we omit the details due to lack of space. The edit distance algorithm of ZS in [1] and our weighted distance algorithm are implemented and tested on synthetic and real data set. All the experiments are conducted on a platform with Pentium IV 3.2 G CPU and 2 GB of RAM. The synthetic data is a set of XML document s generating with DTDs [14, 16] using IBM X  X  XMLGenerator [15]. The real data is gotten from the XML repository [14]. In our experiments, the top-k answers are compared with those gotten using real edit distance. The default value of k is set to 10. To measure the quality of our results, a definition of precision is given. Assuming that we get m answers in ZS algorithm and n answers in the proposed algorithm. If q of our n answers are in the m ones, we use q/n to describe the precision of our result. Because we only focus on how many answers in our k results are correct, we do not use the metric of recal l as in [2], which should be calculated as q/m . 5.1 Time Performance The time performance of our algorithm is tested with the increase of the input tree size on the synthetic dataset gener ated from SigmodRecord.dtd by using XMLGenerator. This dataset has 1000 documents whose average node number is 35 and the maximum is 78. We change input tree X  X  size and observe the time changes in two algorithms. Using CPU time as the standard measure, we get time costs of two algorithms in Fig. 7, where the time includes the costs of constructing trees for all XML documents and the calculation of the distance array as well. We also take the time of distance calculation as the cost of answering top-k query. For each tree size, 10 queries are processed a nd average of their calculating time are used in two algorithms. Figure 7 shows that, as the number of nodes in the input tree increases, the time cost of ZS distance algorithm increases rapidly. However, in our approach, we calculate the distance of summary structures of two trees instead, which usually does not change. Let S ( M ) denote the number of nodes in the summary structure of tree M . Our distance algorithm has time complexity O ( | S ( M ) || S ( N ) | depth ( M ) depth ( N )). The influence brought by input tree X  X  size in our algorithm is very tiny time difference in the stage of getting the summary structure for the input tree; therefore, this can be ignored when comparing with high cost distance computation.
In Fig. 8 we show how the algorithms is performing when the size of dataset is increasing. We get the testing data from three categories, each of which has 1000 documents. We take documents from these three collections to form date sets of size 300, 600, 1500 and 3000. For the top-k queries, in both two algorithms, the time increases in a linear way with respect to the document numbers in the data set. But the time cost of our algorithm is far less than the cost of ZS algorithm. 5.2 Precision Evaluation Extensive experiments are conducted on real datasets to compare the precision of our approach with algorithm in [1]. From the SIGMOD Record data, we get 51 XML documents with root element issue , and from Mondial data we get 231 XML documents rooted at Country . The average node number of the former dataset is 87 and the number of the latter is 57. 10 queries are processed on each dateset. Then, using the definition of our precision, we compare two algorithms in terms of both time and precision.

The results when adopting the linear weight function are shown in Table 1. It can be seen that our approach achieves highe r precise results, and the time costs are far less than that of ZS algorithm. For the SIGMOD Record dataset, the reduction on time can be 98%. The reason is that they have a large number of repeated elements, and the nodes in thei r summary structure are far less than in the original trees. And for Mondial data whose node numbers do not decrease as much as in the SIGMOD Record dataset after transformation, its time reduction is not so high. But, the more close to the original structure after transformation, the more precise results we will get. So the resulting precision on Mondial data is higher than that on SIGMOD Record data.

In above experiments, we assume a unit cost in ZS algorithm. That is, we are comparing our results containing level information with ZS results contain-ing no level information. In next experiments, the weight information is added to the ZS edit operations, and then redo the comparison. Table 2 shows re-sults when comparing our result with weighted ZS algorithm. The results show that ZS algorithm gets a higher precision than with no weight information, and this confirms our observation about that weight information is helpful for struc-ture similarity computation. Furthermore, we test two weight functions on this dataset, and find both of them are helpful and the exponential one could achieve better performance for Sigmod and Mondial dataset.
 Structural Similarity search in XML documents has been gaining more and more attention from the related communities. Previous work was mainly based on tree edit distance and ignored the level info rmation of nodes. From a semantic point of view, we regard nodes in the high levels as more significant ones, and the edit operations for them need more cost s. In this paper, we propose two weight functions to add weight information on nodes. Moreover, we propose a weighted distance between summary st ructures of two trees. Cal culation of this distance is based on our new cost model. When getting the summary structure of a tree, our approach can reduce tree size at the same time when making our distance more closer to original distance. Experimental results show our approach works well in answering top-k queries. For the future work, we are planning to study the structure similarity further and try to apply it in XML clustering and XML retrieval.
 This work is partially supporte d by NSFC under g rant No. 60496325, 60228006 and 60403019.

