 There has been considerable interest in incorporating di-versity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics :diversityrankersscoredocuments against a set of hypothesized subtopics, and diversity rank -ings are evaluated by assigning a value to each ranked docu-ment based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with pro-gressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is cor-rect, then users, when given a choice, should prefer to see adocumentthathasmorevaluetotheevaluation.Wefor-mulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or doc-ument B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judg-ments whatsoever.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval] Keywords: diversity, user study, preference judgments
Research on novelty and diversity aims to improve the ef-fectiveness of search engines by providing results that ser ve arangeofpossibleuserintentsforthegivenquery. These problems have been the subject of much interest in IR and Batch e ff ectiveness evaluation of retrieval systems serves sev-elty and diversity in search results.
 eral important purposes: first, giving developers and re-searchers a measurable objective; second, allowing for fai lure analysis and troubleshooting; and third, trying to estimat e how useful search results will be to users. For the last of these, it is helpful to think of the evaluation measure and relevance judgments as a model of user utility. Measures like precision and recall can be seen as modeling utility in terms of the proportion of retrieved documents that are rel-evant and the proportion of relevant documents retrieved; measures like discounted cumulative gain or expected recip -rocal rank o ff er a more refined model of user utility that incorporates graded judgments and rank-based discounts.
None of these models capture novelty or redundancy in ranked results. All of them will reward a system for retriev-ing the same relevant document 10 times in a row, or 10 relevant documents that are only superficially di ff erent fro m each other; while that may be useful for knowing whether retrieval features are working correctly, it is not likely t o be very useful to a user. Diversity evaluation attempts to model novelty and redundancy in ranked results so as to give amoreprecisemodelofuserutility.

Current diversity evaluation measures in the literature re -quire judgments of relevance to subtopics (also called as-pects, facets, or nuggets) of a topic. For example, judgment s for the query Windows would include binary relevance judg-ments for each document for the subtopics window panes , windows operating system ,etc. Thesesubtopicjudgments are used to determine whether a document is redundant with a previously-ranked document, or whether it contains some new information that a user might find interesting, or whether it is relevant to an alternative intent and perhaps not useful to this user but still useful enough to a di ff erent user. Measures like  X  -nDCG, ERR-IA, subtopic recall, and D-measures [10, 8, 17, 14] all use this same basic model, as-signing more value to a document with more novel subtopics and less value to one with more redundant subtopics.
Like any model, the subtopic model surely has shortcom-ings. Novelty and redundancy are certainly not the only reasons a user might prefer one relevant document over an-other. Apart from a study by Sanderson et al. that showed that user preferences for rankings correlate with  X  -nDCG [15], there has not been much work on validating this model against real user preferences. And if the model does not track user preferences, then it is hard to justify its contin -ued use: it conflates various aspects of e ff ectiveness in such a way that, if used as an objective function, it can be di ffi cult to understand the precise e ff ect of change in the ranker.
Fortunately these measures produce directly-testable hy-potheses about user preferences. In this work we describe a novel user study to test these hypotheses. In Section 2 we start by describing the diversity retrieval problem in more detail and define the model more precisely. In Section 3 we present a user study, including a crowdsourced design; we show that while the model is not perfect, it is certainly not invalid. Section 4 builds on this by presenting a preference -based method for determining a diversity-aware ranking of documents. We conclude in Section 5.
Consider a user that has an unambiguous but broad in-formation need and goes to a search engine to help satisfy it. This user will input a query and then see a ranked list of results, some of which will be relevant and some of which will not. The user will presumably click the relevant result s to view and absorb the information they contain. Ideally, each relevant result would provide some new information that was not provided by previous relevant results; in other words, the relevant results would not be redundant with each other. The idea is that, each time a user clicks on a new rel-evant document, the amount of knowledge a user gains must be maximized by the novel content in the document.
The goal of ranking documents with novelty is to ensure that each relevant document a user sees as they progress down a ranked list provides new, non-redundant informa-tion that will help them satisfy their need. This means that arankingofdocumentscannotbebasedsolelyontheprob-ability of relevance; the novelty of a document depends to no small degree on the documents that have been ranked above it. Similarly, evaluation of these results cannot be based solely on binary or even graded relevance judgments, since these judgments are made to individual documents in-dependently of all the other documents that might have been ranked. Part of studying the task is defining evaluation mea-sures that can model redundancy and novelty.
The novelty task has similarities with some existing tasks such as the diversity task studied as part of the TREC Web track. Diversity aims at retrieving a subset of documents that has the maximum coverage of subtopics with the as-sumption that di ff erent users may be interested in di ff erent subtopics. In novelty ranking, the goal is to provide a set of documents for a single topic from which the user can get as much information as possible for that particular topic. We assume all users are interested in all of the subtopics, like the standard ad hoc assumption that all users are interested in all of the relevant material.
Researchers in the past have identified two types of di-versity: extrinsic and intrinsic [12]. Extrinsic diversit yad-dresses the uncertainty in an ambiguous query where the in-tent is unclear and is best served by a ranking of documents covering several intents. Intrinsic diversity can be descr ibed as diversification that focuses on reducing redundancy and providing novel information for an unambiguous but still underspecified information need. In our work, we focus on intrinsic diversity which we refer to as novelty ranking ,aswe believe it will be easier for assessors to express preferenc es when there is no ambiguity of intent.
Evaluation measures for novelty and diversity must ac-count for both relevance and novelty in the result set. It is important that redundancy caused by documents containing previously retrieved subtopics be penalized and documents containing novel information be rewarded. Most evaluation measures solve this problem by requiring that the subtopics for a query be known and that documents have been judged with respect to subtopics. Subtopic recall. Subtopic recall measures the number of unique subtopics retrieved at a given rank [17]. Given that aquery q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m .
  X  -nDCG scores a result set by rewarding newly found subtopics and penalizing redundant subtopics. In order to calculate  X  -nDCG we must first compute the gain vector [10]. The gain vector is computed by summing over subtopics appearing in the document at rank k : where c j,i is the number of times subtopic j has appeared in documents up to (and including) rank i .Oncethegain vector is computed, a discount is applied at each rank to penalize documents as the rank decreases. The most com-monly used discount function is the log 2 (1 + i ), although other discount functions are possible. The discounted cu-mulative gain is given by  X  -DCG must be normalized to compare the scores against various topics. This is done by finding an  X  X deal X  ranking that maximizes  X  -DCG, which can be done using a greedy algorithm. The ratio of  X  -DCG to that ideal gives  X  -nDCG. Intent-aware family. Agrawal et al. studied the problem of answering ambiguous web queries, which is similar to the subtopic retrieval problem [2]. The focus of their evaluati on measure is to measure the coverage of each intent separately for each query and combine them with a probability distri-bution of the user intents. They call this the intent-aware family of measures. It can be used with most of the tradi-tional measures for evaluations such as precision@ k ,MAP, nDCG, and so on.
 ERR-IA. Expected Reciprocal Rank (ERR) is a measure based on  X  X iminishing returns X  for relevant documents [9]. According to this measure, the contribution of each docu-ment is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents. AweightedaverageoftheERRmeasuresforeachinterpre-tation would give the intent-aware version of ERR [8]. D-Measure. The D and the D# measures described by Sakai et al. [14] aims to combine two properties into a single evaluation measure. The first property is to retrieval docu-ments covering as many intents as possible and the second is to rank documents relevant to more popular intents higher than documents relevant to less popular intents.
All of these measures estimate e ff ectiveness of a system X  X  ranking by iterating over the ranking, rewarding relevant documents containing a unseen subtopic(s) and penalizing relevant documents containing subtopic(s) seen before in t he ranking. They are all based on a few principles in general: 1. A document with more unseen subtopics is worth more 2. A document with both unseen and already-seen subtopics 3. A document with unseen subtopics is worth more than
One of our goals with this work is to test whether these principles hold for real users.
Our analysis was conducted primarily on the Newswire data created by Allan et al. [3] to investigate the relation-ship between system performance and human performance on a subtopic retrieval task. The data consists of 61 topics, each with a short (3-6 word) query, and judgments of rele-vance to documents in a subset of the TDT5 corpus. The Newswire data includes relevance judgments for the top 130 documents retrieved by a query-likelihood language model for the short query for each query. The judgments consists of binary relevance judgments for each document, and for each relevant document, a list of subtopics contained in that doc -ument. This data reflects an intrinsic diversity task and is therefore most appropriate to this work.
In Section 2.3.2, we identified some principles on which the evaluations for diversity are based on. In this section w e tests if these principles hold for real users and further stu dy in detail the role of subtopics in influencing user preferenc e. Although in practice the same evaluation measures are used for both intrinsic and extrinsic diversity, our focus is on i n-trinsic diversity as it is easier for assessors to understan d the concept of relevance when there is no ambiguity of in-tent. We explore the factors that influence user preference for novelty ranking using a preference based framework.
The idea of pairwise preference judgments is relatively new in the IR literature, having been introduced by Rorvig in 1990 [13] but not subject to empirical study until the past several years [6, 5]. Comparison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with bet-ter agreement between assessors (and more consistency with individual assessors) [6]. Also with preferences tassesso rs can make much finer distinctions between documents.
We propose a preference-based framework to study nov-elty consisting of a set up in which three relevant documents that we refer to as a triplet are displayed such that one of them appears at the top and the other two are displayed as a pair below the top document. We will use D T , D L ,and D to denote the top, left, and right documents respectively, a nd atripletas  X  D L ,D R | D T  X  .Anassessorshownsuchatriplet would be asked to choose which of D L or D R they would prefer to see as the second document in a ranking given that D
T is first, or in other words, they would express a prefer-ence for D L or D R conditional on D T .Forthepurposeof this study we will assume we have relevance judgments to atopic,andforeachrelevantdocument,binaryjudgments of relevance to a set of subtopics. Thus we can represent adocumentasthesetofsubtopicsithasbeenjudgedrele-vant to, e.g. D i = { S j ,S k } means document i is relevant to subtopics j and k .Varyingthenumberofsubtopicsintop, left and right documents yields specific hypotheses about preferences for novelty over redundancy.
The triplet framework allows us to collect judgments for novelty based on preferences and also enables us to test var-ious hypotheses. As discussed above, varying the number of subtopics in D T , D L and D R it is possible to enumer-ate various hypotheses concerning the e ff ect of subtopics in adocument. Wedefinetwotypesofhypotheses;onevery specific with respect to subtopic counts and redundancy, and the other more general.

Hypothesis Set 1 : First we propose the simplest possi-ble hypotheses that capture the three principles above. We will denote a preference between two documents using $ , e.g. D L $ D R means document D L is preferred to docu-ment D R .Thenthethreehypothesesstatedformallyare:
H 1 :if  X  D L ,D R | D T  X  =  X  { S 2 } , { S 1 }|{ S 1 }  X  ,then D
H 2 :if  X  D L ,D R | D T  X  =  X  { S 1 ,S 2 } , { S 2 }|{ S 1
H 3 :if  X  D L ,D R | D T  X  =  X  { S 2 ,S 3 } , { S 2 }|{ S 1
Hypothesis Set 2 : Here we define a class of hypotheses in which the number of subtopics contained in each docu-ment in a triplet is categorized by relative quantity. We ide n-tify six variables based on number of subtopics that almost completely describe the novelty and redundancy present in the triplet. The six variable are as follows: 1. Tn -Numberofsubtopicsin D T ; 2. NLn -Numberofsubtopicin D L not present in D T ; 3. NRn -Numberofsubtopicin D R not present in D T ; 4. Sn -Numberofsubtopicssharedbetween D L and D R ; 5. RLn -Numberofsubtopicsin D L and present in D T ; 6. RRn -Numberofsubtopicsin D R and present in D T .
The number of subtopics for each of the six variables are categorized as low or high .Thesixvariablesenableusto test the e ff ect of novelty and redundancy w.r.t the number of subtopics in a triplet. The variables NLn and NRn focus on novelty whereas RLn and RRn focuses on redundancy. For instance, by varying NLn and NRn and holding the other variables constant, it is possible to test the e ff ect of the relative quantity of novel subtopics in a document.
In this section we describe the experimental design used to test the hypotheses defined above. Notice that the defined hypotheses are based on the number of subtopics contained in the documents and they fit into the triplet framework which requires conditional preference judgments. Therefo re, to test our hypotheses, two kinds of judgments were needed: subtopic level judgments and conditional preference judg-ments. The subtopic level judgments were obtained from the data described in Section 2.4. Conditional preference judgments were collected using crowd sourcing as it is a fast , easy and a low cost way of collection user judgments [4].
We used Amazon Mechanical Turk (AMT) [1]; an online labor marketplace to collect user judgments. AMT works as follow: requestor create a group of Human Intelligence Task (HITs) with various constraints and worker from the marketplace works on these task to complete the task. In this work we, use a design similar to the one used by Chan-dar and Cartertte [7]. Designing a user study using AMT involves deciding on the HIT layout and HIT properties. In order to collect user judgments for our hypotheses using AMT, we had to organize the triplets satisfying a given hy-pothesis into HITs. Each HIT consisted of the following (in order of display): a set of instructions about the task, orig i-nal keyword query, topic description, five preference tripl ets, and a comment field allowing worker to provide feedback. A brief description about each element is given below:
Guidelines: Workers were provided with a set of in-structions and guidelines prior to judging. Guidelines spe c-ified that workers should assume that everything they know about the topic is in the top document and are trying to find a document that would be most useful for learning more about the topic. Guidelines did not mention anything about subtopics, or even novelty/redundancy except as examples of properties assessors might take into account in their pre f-erences (along with recency, ease of reading, and relevance ).
Query text and topic description: The query text de-scribed the topic in a few words (we used the topic  X  X itles X  in the traditional TREC jargon) and topic description pro-vided a more verbose and informative description about the topic. Again, there was no mention of explicit subtopics.
Preference triplet: Figure 1 shows an example prefer-ence triplet with the query text and topic description. Each preference triplet consists of three documents, all of which were relevant to the topic and the document were picked ran-domly from the data described in Section 2.4 to meet the constraints of a given hypothesis. One document appeared at the top followed by two documents below it, the triplets were chosen randomly such that the hypothesis constraints were satisfied. A HIT consisted of five preference triplets belonging to the same query shown one below the other.
The triplets were followed by a preference option for the workers to indicate which of the two documents they pre-ferred. The workers were asked to pick the document from the lower two that provided the most new information, as-suming that all the information they know about the topic is in the top document. They could express a preference based on whatever criteria they liked; we listed some exam-ples in the guidelines. Note that we do not show them any subtopics, nor do we ask them to try to determine subtopics and make a preference based on that.

Comments Field was provided at the end, so that the workers could to provide a common feedback for all the five triplets, if they chose to do so.
Workers are paid for each HIT they complete and picking an appropriate amount for each task is always tricky. In our study, workers were paid $0.80 for each completed HIT. Also each HIT had a time limit of three hours before which the HIT had to be completed. While the actual task might not take three hours to complete; the extra time allows them to take breaks if needed, since the workers had to read fifteen documents per HIT. We had five separate workers judge each HIT for the our first set of hypotheses and three separate workers judge each HIT for the our second set of hypotheses .
Triplets were generated by randomly picking the three rel-evant documents for a given query and representing them as subtopic(s). Triplets for the first set of hypotheses in Sec-tion 3.2 were considered such that the constraints are sat-isfied for each hypothesis. For example, for hypothesis H given a query x the triplet would consist of D T containing only the subtopic S 1 and D L containing the subtopics S 1 S .Sixquerieswereusedtotestthefirstsetofhypotheses with four triplets for each query.

The triplets were generated in a similar way for the sec-ond set of hypotheses but the constraints for each hypoth-esis were based on the six variables described in 3.2. For example, a triplet with a variable setting of Tn =High, Sn =High, NLn =Highand NRn =Highwouldcontain5 or more subtopics in the top document D T and 3 or more subtopics in the left and right documents ( D L and D R )such that there are 1 or more subtopics shared between D L and D
R .Thedetailsofthenumberofsubtopicsforeachcate-gories of high and low levels for each variables are provided in the Table 1. Eight queries were used to test the second set of hypotheses with four di ff erent triplets for each query .
There are two major concerns in collecting judgments through crowdsourcing platform such as AMT. One is  X  X o the workers really understand the task? X  and the other is  X  X re they making faithful e ff ort to do the work or clicking randomly? X . We address these concerns using three tech-niques: majority vote, trap questions, and qualifications.
Majority vote: Since novelty judgments to be made by the workers are subjective and it is possible some workers are clicking randomly, having more than one person judge atripletiscommonpracticetoimprovethequalityofjudg-ments. In our study, each HIT was judged by 5 or 3 di ff erent workers (depending on hypothesis set). We look at the in-dividual preferences as well as the majority preference.
Trap questions: Triplets for which answers are obvious were included to assess the validity of the results. We in-cluded two kinds of trap questions:  X  X on-relevant document trap X  and  X  X dentical document trap X . For the former, one of the bottom two documents was not relevant to the topic and should never be preferred. For the latter, the top document and one of the bottom two documents were the same. The workers were expected to pick the non-identical document as it provides novel information. One of the five triplets in aHITwasatrapandthetypewaschosenrandomly.
 Table 1: Number of subtopics corresponding to the high and low categories for each variable in our sec-ond set of hypotheses.

Qualifications: It is possible to qualify workers before they are allowed to work on your HITs in AMT. Worker qualifications can be determined based on historical perfor -mance such as percentage of approved HITs. Also, worker X  X  qualification can be based on a short questionnaire or a test. The two qualifications used in are study are explained below: 1. Approval rate: HITs can be restricted to workers 2. Qualification test: Qualification tests can be used to
Judgments for a total of 60 triplets (out of which 12 triplets were traps) were obtained for the hypothesis set 1. Since we had each triplet assessed by five separate assessors, a total of 300 judgments were collected out of which 60 were traps. We had 39 unique workers (identified by worker ID) on AMT judge these triplets across six topics.

Table 2 shows results for H 1 .Itturnsoutthatthereis no clear preference for either redundant or novel documents weapons for urban fighting 15 5 3 1 Table 2: Results for H 1 :thatnoveltyispreferredto redundancy. The  X  X ll prefs X  columns give the num-ber of preferences for the redundant and the novel document for all assessors. The X  X onsensus X  X olumns take a majority vote for each triplet and report the resulting number of preferences.
 kerry endorsement 9 11 2 2 childhood obesity 4 16 0 4 terrorism indonesia 13 7 4 0 Table 3: Results for H 2 :thatnoveltyandredun-dancy together are preferred to novelty alone. The  X  X ll prefs X  columns give the number of preferences for the redundant+novel document and the novel document for all assessors. The X  X onsensus X  X olumns take a majority vote for each triplet and report the resulting number of preferences. for the four queries. For two of our queries assessors tended to prefer the novel choice; for the other two they tended to prefer the redundant choice. When we use majority vote to determine a consensus for each triplet, we find that the outcomes are exactly equal. Thus while we cannot reject H ,wehavetoadmitthatifitholdsitismuchlessstrong than we expected.

Table 3 shows a clearer (but still not transparent) prefer-ence for H 2 ,noveltyandredundancytogetherovernovelty alone. Over all assessors and all triplets, the preference i s significant by a binomial test (50 successes out of 80 trials; p&lt; 0 . 05). Still, there is one query ( X  X ohn kerry endorse-ment X ) for which the di ff erence is insubstantial, and one tha t has the opposite result ( X  X errorism indonesia X ). The latte r kerry endorsement 9 11 1 3 childhood obesity 3 17 0 4 terrorism indonesia 2 18 0 4 Table 4: Results for H 3 :thattwonovelsubtopics are preferred to one. The  X  X ll prefs X  columns give the number of preferences for the novel+novel doc-ument and the novel document for all assessors. The  X  X onsensus X  columns take a majority vote for each triplet and report the resulting number of prefer-ences. Table 5: Results of preference judgments by the number of new subtopics in D L ,D R over D T (vari-ables NLn,NRn ). Counts are aggregated over all values of Tn,Sn per query. The first column gives preference counts for the document with more new subtopics over the document with fewer when NLn $ NRn .Thesecondcolumnisthebaseline,giving counts for preferences for left over right. case is particularly interesting because it is the opposite of what we would expect after seeing the results in Table 2: given that assessors preferred redundant documents to nove l documents for that query, why would they prefer novel doc-uments to documents with both novelty and redundancy?
Table 4, with results for H 3 ,isthestrongestpositivere-sult: a clear preference for documents with two new subtopic s over documents with just one. In this case both results are significant (58 successes out of 80 trials and p&lt; 0 . 0001 over all triplets and all assessors; 14 successes out of 16 trials and p&lt; 0 . 01 for majority voting). Nevertheless, there are still queries for which the preference is weak.

Based on this, it seems like novelty + novelty &gt; novelty , novelty + redundancy  X  novelty ,butnot novelty  X  redundancy .
There were a total of 640 triplets (out of which 128 triplets were traps) for the second part of our study. Each of these triplets were judged by three separate assessors, thus a tot al of 1920 judgments were made out of which 384 were traps. And for this study we had 38 unique workers (identified by worker ID) on AMT working on our triplets. Some of these workers had worked on the first study as well. Almost 70% of the judgments were completed by 15% of the work-ers and about 93% of the irrelevant traps were passed by the workers. This power law distribution for our task has been observed earlier for other tasks as well [11], we hope to investigate on this issue in the future.
 Triplets were generated by controlling four variable: Tn , Sn , NLn and NRn ,weobtainedsixteenuniquesettingsfor the four variable combination as each of the four variables were categorized into low and high with equal number of triplet in each setting. This allowed us to perform ANOVA such that the number of new subtopics in the left or right document was the primary predictor of preference, with the number of subtopics in the four variables as the secondary predictors. ANOVA indicated that there is a lot of resid-ual variance, suggesting there are various factors influenc ing preferences that we have not included in the model. Table 5 analyzes preferences for more new subtopics in D L or D R over fewer new subtopics (variables NLn and NRn )bytopic. Welookedatfourcases: thefirsttwo( NLn high, NRn low; NLn low, NRn high) can tell us whether users prefer to see more new subtopics over fewer, while the second ( NLn high, NRn high; NLn low, NLn low) along with the first two give us a baseline preference for left over right. While we would expect the baseline preference to be 50% (since which document appears on the left versus right is randomized), there may be other unmodeled factors that cause it to be more or less than 50%, so it is useful to compare to this baseline.

It is clear from this table that users as a group prefer to see more new subtopics, just as we saw in the results for H 3 above. Still, there are individual queries for which that preference is not strong, especially when compared to the baseline (e.g. the  X  X hio highway shooting X  topic), and even when the preference is strong in aggregate there are cases where they do not hold.
 There is some e ff ect due to the number of subtopics in D T ,withpreferencesformorenewsubtopicsstrongerwhen Tn is low. When it is low, the preference for high versus low is 271 to 113 (70%) against a baseline preference for left over right of 347 to 421 (45%) 2 .When Tn is high, the preference for high versus low is 251 to 133 (65%) against a baseline of 396 to 372 (52%). We conjecture that when the top document already has a lot of information about the topic, there is a little less reason to prefer either left or r ight regardless of how many subtopics they contain.

There is not much e ff ect due to the number of shared subtopics between D L and D R .When Sn is low, the prefer-ence for more new subtopics over fewer is 268 to 116 (70%) against a baseline of 370 to 398 (48%); when it is high, the preference for more new is 254 to 130 (66%) against a base-line of 373 to 395 (49%). This may be because fewer shared subtopics makes it easier to express a preference. However, the e ff ect is too small to draw any firm conclusion.
There is interesting interaction between the number of new subtopics and the number of redundant subtopics in D
L and D R .Whenonehasahighnumberofnewsubtopics and the other has a low number of new subtopics, num-ber of redundant subtopics seems to influence the strength of preference for the one with more new subtopics: if there are more redundant subtopics along with the new subtopics, the preference is 118 to 44 (73%), but when there are fewer redundant subtopics with more new subtopics and more re-dundant subtopics with fewer new subtopics, the preference is even at 51 to 51 (50%). This suggests again that users like redundancy, sometimes enough to overcome a lack of nov-elty. However we must note that data here is sparse, also the two variables RRn and RLn were not the ones that we controlled for in our experiment. the right document is just due to random chance.

Topic Agreement No. triplets childhood obesity 0.71 15 weapons for urban fighting 0.92 5 kerry endorsement 0.58 10 libya sanctions 0.62 10 earthquake 0.72 5 terrorism indonesia 0.71 15 Table 6: Interassessor agreement scores for each topic for the first study.

Topic Agreement No. triplets oil producing countries 0.63 80 terry nichols guilt evidence 0.72 80 no child left behind 0.61 80 german headscarf court 0.57 80 medicare drug coverage 0.66 80 earthquakes 0.65 80 european union member 0.59 80 ohio highway shooting 0.59 80 Table 7: Interassessor agreement scores for each topic for the second study.
As described above, each triplet was judged by five dif-ferent workers for the first study (hypotheses set 1) and by three workers for the second study (hypotheses set 2). We calculated an inter-assessor agreement score for each trip let for the first study as follows. The judgments were consid-ered as 10 pairs of answers given for a single triplet, adding 1pointstothescoreifthetwoworkersagreed(complete agreement); and adding nothing if they judged di ff erent doc-uments (no agreement). The perfect agreement would sum up 10 points, so we divided the score obtained by 10 and normalized from 0 (no agreement at all) to 1 (perfect agree-ment). Mean agreement for the first study is given in Table 6 for each query. Overall a high mean agreement of 0.7 was found across all triplets and the scores are close to the agre e-ment observed previously [6]. Since the mean agreement was quite high for the first study, it encouraged us to reduce the number of workers for each triplet and increase the number of queries for the second study. Similar mean agreement can be seen for the second study in Table 7.
The way the hits were displayed may introduce some con-founding e ff ects, possibly causing assessors to choose doc-uments for reasons other than novelty or redundancy. We investigated two such e ff ects: Document length Apreferencetowardsshorterdocuments was observed in general, though the preference gets weaker over the three hypotheses. For H 1 ,assessorspreferredthe shorter document in 79% of triplets. For H 2 ,thatdecreased to 71% of triplets, and for H 3 it dropped steeply to only 44%. However, it is also true that the mean di ff erence in length for the pair of documents they were choosing between was great-est for H 1 triplets and least for H 3 triplets ( H 1 :158 terms, H :126 terms, H 3 :47 terms). Therefore its safe to conclude there seems to be a preference towards shorter documents. Highlighted terms It turns out that assessors tended to prefer the document with fewer highlighted query terms. For H ,assessorspreferredthedocumentwithmorequeryterms only 35% of the time. For H 2 that drops to 13%, and for H it comes back up to 29%. The mean di ff erence in number of query term occurrences is quite low, only on the order of one additional occurrence on average for H 1 and H 3 documents, and only 0.2 additional occurrences for H 2 .Whilethee ff ect is significant, it seems unlikely that assessors can pick up o n such small di ff erences. We think the e ff ect is more likely due to the distribution of subtopics in documents.
While the results suggest that the number of subtopics influences user preferences, it is also clear that from the analysis that other factors are a ff ecting preferences. The results from H 1 and the weaker preference in H 2 were not what we expected. We investigated this more by looking at anumberoftripletsourselvesandidentifyingsomenewhy-potheses about why assessors were making the preferences they were. From looking at triplets for the  X  X arthquakes X  topic, we identified three possible reasons for preferring a document with a redundant subtopic: This suggests to us that there are other factors that a ff ect user preferences, in particular recency, completeness, an d value. It may also suggest that there are implicit subtopics (at finer levels of granularity) that the original assessors did not identify, but that make a di ff erence in preferences. None of this is surprising, but there is currently no evaluation paradigm of note that take all of these factors into account i n aholisticway.Preferencejudgmentscan,andthisanalysis suggests additional hypotheses for testing with preferenc es.
The user study shows that although users tend to prefer documents containing more novel subtopics, it is also evide nt that factors other than subtopics play a vital role. The stud y also shows that the presence of subtopic in a document is taken into account implicitly and preferences are based not only on the number of subtopics but also on several other factors that include subtopic importance, relevance of the subtopic, readability of the document, etc. In this section , we propose an approach that attempts to capture these fac-tors implicitly using a preference based framework to form a full ranking of documents with novelty as an implicit qualit y.
Our approach involves a series of sets of preference com-parisons. Each set is essentially a comparison sort algo-rithm, with the comparison function a simple preference conditional on information contained in top-ranked docu-ments from prior sets of comparisons, generalizing the trip let framework we introduced above.

The first set of preferences is meant to produce a relevance ranking: given a choice between two documents, assessors select the one they prefer, with topical relevance being the primary consideration in the judgment. Once these compar-isons are done for all pairs, it is possible to obtain the best or  X  X ost relevant X  document, i.e. the most preferred document based on the number of times a document was selected.
For the second set of preferences, the assessor needs to consider the novelty of information in the document along with relevance. This leads to exactly the triplet framework we used previously. For this second set, the assessor will se e the top-ranked document from the previous set as D T ,then pick from two documents D L , D R conditional on that.
The sequence continues by adding more documents to the top. For the third set, the comparison involves information in two previously ranked documents along with a pair of documents; for the fourth, it involves information in three previously ranked documents along with a pair. This contin-ues to the final set, in which there are only two documents to compare conditional on n  X  2previoustopdocuments.
When complete, the most preferred document in the first set takes rank 1, the most preferred document in the sec-ond set takes rank 2, and so on. Observe that the first set of judgments correspond to relevance judgments and sets 2 through n  X  1correspondtonovelty.

This method asks for a very large number of preferences: if fully judged, there would be O ( n 2 )preferencesinthefirst set, O (( n  X  1) 2 )inthesecond,andsoon,foratotalof hypothesize that the first two sets of preferences (one for relevance and one for novelty) will provide a near-optimal approximation to the full set and if judgments are transitiv e (that is, if document A is preferred to B and B is preferred to C ,then A should be preferred to C as well), the number of judgements needed can be reduced drastically. We will test both of these hypotheses below.
As described above, we asked assessors to make the first two sets of judgments for each topic. The first set of judg-ments attempts to rank documents by relevance to the topic; intuitively, these judgments could be used to find the most relevant document in the ranked list: that which is pre-ferred to everything else (assuming judgments are transiti ve) is most relevant. The second set of judgments attempts to rank the remaining documents by the degree of novelty they provide given that we know the document that is ranked at position one from the first set.

For this experiment we elected not to use MTurk. We wanted a single assessor to do all the preferences for a singl e topic, first so they would be able to build a familiarity with the topic as they judge, and second so we could assess their self-consistency. Thus we asked students at our institutio n to participate in the study. These students are mostly in computer science, mostly studying NLP and language tech-nologies. Like the workers in the previous section, they wer e not given explicit instruction regarding subtopics; they w ere only asked to express a preference. We had 6 assessors com-plete preferences for at least one topic.

We designed two new web interfaces running on a local server to be used by assessors to collect preferences for bot h relevance and novelty, the first two sets of preferences de-scribed above. Common elements in both interfaces are the original keyword query, topic description, article tex ts (with query keywords highlighted), preference buttons for indicating which of the two documents the assessor prefers, aprogressbarwitharoughestimateofthepercentageof preferences completed, and a comment field allowing them to say why they made their choice (if they wish). Elements specific to each experiment are described in more detail in the respective sections below.

For this study, we asked assessors to judge all pairs of documents in the first two sets. Topics were chosen from the data described in Section 2.4. We wanted to include all known relevant documents for the topic in the preference experiment. Since we were asking assessors for all pairs, we limited our selection to topics with a relatively small number of relevant documents. We then added a randomly-selected set of nonrelevant documents from among the top-ranked documents for the topic. We kept the total number of preferences in an experiment to less than 200.
The first two documents shown to an assessor were chosen randomly from the set of all documents to be ranked. After that, whichever document the assessor preferred remained fixed in the interface; only the other document changed. This way the assessor only had to read one new document after each judgment, just as they would in normal single-document assessing. Furthermore after the first O ( n )judg-ments we know the top-ranked document for the current set, and thus if transitivity holds it follows that we only need a linear number of preferences at each set.
In the first set of judgments, the assessor was shown two documents (news articles) and a statement of an informa-tion need (a topic); the task was to pick the most preferred document using the X  X refer left X  or  X  X refer right X  buttons. A screenshot of the first level judgments is shown in Figure 2
The assessor was provided with a set of instructions and guidelines prior to judging. The guidelines specified that the assessor should assume they know nothing about the topic and are trying to find documents that are topically relevant, that is, that provide some information about it. If a document contains no topical information, the assessor could judge it  X  X ot relevant X ; if they do so, the system will assume they prefer every other document to that one and remove it from this set as well as all subsequent sets so it will not be seen in future comparisons. Assessors could also judge  X  X oth not relevant X  to remove both from the set and see a new pair. These buttons can make the task easier by reducing the total number of preference judgments the assessors need to make.

If both documents were topically relevant, the assessor could express a preference based on whatever criteria they liked. Some suggestions included in the guidelines were: on e document is more focused on the topic than the other; one document has more information about the topic than the other; one document has more detailed information than the other; one document is easier to read than the other. Assessors could exit for a break as long as they liked and return at the point where they stopped. A progress indicator let them know roughly how close they were to the end
For the second set of preferences, the assessor was shown three documents and a statement of an information need (a topic); the task was to pick the most useful document from two of the three to learn more about the topic given what is presented in the third.

The interface for the second level judgment was very sim-ilar to the triplet layout shown is Figure 1. One document Figure 2: Screenshot of the preference collection in-terface for relevance preferences. appeared at the top of the screen; this was the most pre-ferred document as identified by the assessor after the first set of preferences. The assessors were asked to pick a doc-ument from a pair of documents (appearing below the top document) that provided the most novel information given that they know all the information in the top document.
Guidelines specified that the assessor should pretend that the top document is the entirety of what they know about the topic, and their goal is now to find the best document for learning more about the topic. Beyond that, they could express a preference based on whatever criteria they liked, including those listed above.

There are no nonrelevant judgment buttons in this inter-face. Any document that was judged nonrelevant in the first set of preferences will not be seen in this set. Anything that was relevant in the first set is assumed to still be relevant; if a relevant documents provides no new relevant informa-tion, we assume the assessor X  X  preferences will result in th at document being ranked near the bottom of this set.
As described above, we conducted novelty preference judg-ing with five topics from the data described in Section 2.4. On average, 16.8 documents were judged for each topic. A total of 605 pairs were judged for 4 topics by 6 assessors for experiment levels 1 and 2. We compared these judgments to the original subtopic-based judgments in the data. Agreement on Relevance. To assess agreement on the relevance of a document, we assume that any document not explicitly judged not relevant must be relevant. We conside r the original relevance judgments derived from the subtopic judgments as the ground truth and assess the performance of our assessors relative to that. Table 9 shows the confu-sion matrix between assessors making preference judgments and the original assessors making subtopic judgments; broa d agreement on the two classes is 71%. Preference assessors identified 76% of the relevant documents that the original assessors found, and 60% of the documents judged relevant by at least one assessor were judged relevant by both. This is a high level of agreement for IR tasks; compare to the 40% agreement on relevance reported by Voorhees [16]. Table 8: Kendall X  X   X  correlations between rankings from real preference judgments and rankings from simulated preference judgments (for the relevance ranking (level 1) and the novelty ranking (level 2)). Table 9: Confusion matrix for relevance judgments derived from the subtopic judgments in the original Newswire collection and derived from our preference judgments (all queries aggregated).
 Rank Correlation. Another way to compare preference judgments to the original subtopic judgments is by using both to construct a ranking of documents, then comput-ing a rank correlation statistic between the two rankings. The subtopic judgments included in the Newswire data were obtained by assessors explicitly labeling subtopics for ea ch relevant document. We use the subtopic information to sim-ulate preference judgments that might have been obtained via our experiment. For the first set, we always prefer the document with the greatest number of subtopics. (Except in the case of a tie, when we prefer a random document.) For the second set, the top-ranked document from the first set becomes the  X  X op document X , and then for each pair we prefer the document that contains the greatest number of subtopics that are not in that top-ranked document. The fi-nal ranking has the most-preferred document from the first set of preferences at rank 1 followed by the ranking obtained from the second set of preferences.

Kendall X  X   X  rank correlation for each topic for both level 1andlevel2preferencejudgmentsisshowninTable8.
 Kendall X  X   X  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating a random reordering. The values we observe are positive and statistically signifi -cant (except for level 2 judgments for topic foreign students visa restrictions ). Kendall X  X   X  is based on pairwise swaps, and thus can be converted into agreement on pairwise pref-erences by adding 1 and dividing by 2. When doing this we see that agreement is again high for the relevance ranking, and also high for the novelty ranking, well over the 40% ob-served by Voorhees (except for topic foreign students visa restrictions ). We believe this validates our second set of preferences, though certainly the question is not closed.
One issue in using our preference judgments for novelty is that the number of pairwise judgments increases quickly with number of documents. Increase in number of judgments means increase in assessor time, but if the assessors are con -sistent i.e. if their judgments are transitive, then we can Figure 3: S-recall increases as we simulate deeper levels of preference judgments, but the first set of novelty preferences (level 2) gives an increase that nearly exceeds all subsequent levels combined. reduce the number of preferences from O ( n 2 )to O ( n log n ) at each level; furthermore, since we really only need the  X  X est X  document at each level, transitivity would allow us t o reduce the number of preferences to O ( n )ateachlevel.
We performed experiments to check for transitivity in the novelty task by looking at triplets of documents. A triplet of documents  X  i, j, k  X  is transitive if and only if i is preferred to j , j is preferred to k ,and i is preferred to k .Thera-tio of number of triplets found to be transitive to the total number of triplets give a measure of transitivity in the pref -erence judgments. On average transitivity holds for 98% across all queries with each query being transitive 96% of the time. This suggests that the assessors are highly consis -tent in their judgments; thus using a sorting algorithm with minimum information loss could further reduce the number of judgments required. It also suggests that whatever other features of documents (apart from topical relevance and nov -elty) the assessors are using in their decision process, the y are consistent in their use of those features.
In this section we show that the first two sets of pref-erences, i.e. experiments 1 and 2, are approximately su ffi -cient to produce an optimal ranking. We again use prefer-ences simulated from subtopic judgments: a relevance rank-ing is found by always preferring the document with more subtopics ( X  X evel 1 X ); a first approximation to a novelty ran k-ing is found by always preferring the document with the most subtopics that are not in the top document ( X  X evel 2 X ); asecondapproximationbyalwayspreferringthedocument with the most subtopics that are not in the first two docu-ments ( X  X evel 3 X ); and so on up to level 20.

Figure 3 shows the S-recall scores increasing as the number of preference sets increases. Clearly the increase in S-rec all from level 1 to level 2 is the largest, nearly exceeding the to -tal increase obtained from all subsequent levels put togeth er. This suggests that the first approximation novelty ranking is likely to be su ffi cient; this has the benefit of reducing the amount of assessor e ff ort needed to produce the data.
We have taken initial steps into investigating the use of preference judgments for novelty ranking tasks. We have proposed a novel framework for obtaining preference judg-ments for the novelty task and explicated the pros and cons of using preference judgments. Preliminary results for com -paring explicit subtopic labels with preference judgments suggest that preference judgments can give similar infor-mation about both relevance and novelty as the subtopic judgments that are typically used.

Based on this, we proposed a preference-based approach to obtaining a full ranking for relevance, novelty, and all other factors that contribute to user preferences. We showe d that rankings obtained in this way correlate well to ranking s based on subtopic judgments, and since assessors are highly self-consistent, probably capturing a great deal of other i n-formation as well. Of course, if subtopic judgments were replaced with preferences, we would need a new set of eval-uation measures. This clearly is a direction for future work . Acknowledgments: This work was supported in part by the National Science Foundation (NSF) under grant num-ber IIS-1017026. Any opinions, findings and conclusions or recommendations expressed in this material are the authors  X  and do not necessarily reflect those of the sponsor.
