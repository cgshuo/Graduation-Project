 Wei Gao gaow@lamda.nju.edu.cn Rong Jin rongjin@cse.msu.edu Shenghuo Zhu zsh@nec-labs.com NEC Laboratories America, CA, 95014, USA Zhi-Hua Zhou zhouzh@lamda.nju.edu.cn AUC (Area Under ROC curve) ( Metz , 1978 ; Hanley &amp; McNeil , 1983 ) is an important perfor-mance measure that has been widely used in many tasks ( Provost et al. , 1998 ; Cortes &amp; Mohri , 2004 ; Liu et al. , 2009 ; Flach et al. , 2011 ). Many algorithms have been developed to optimize AUC based on sur-rogate losses ( Herschtal &amp; Raskutti , 2004 ; Joachims , 2006 ; Rudin &amp; Schapire , 2009 ; Kotlowski et al. , 2011 ; Zhao et al. , 2011 ).
 In this work, we focus on AUC optimization that re-quires only one pass of training examples. This is par-ticularly important for applications involving big data or streaming data in which a large volume of data come in a short time period, making it infeasible to store the entire data set in memory before an optimization procedure is applied. Although many online learning algorithms have been developed to find the optimal so-lution of some performance measures by only scanning the training data once ( Cesa-Bianchi &amp; Lugosi , 2006 ), few effort addresses one-pass AUC optimization. Unlike the classical classification and regression prob-lems where the loss function can be calculated on a single training example, AUC is measured by the losses defined over pairs of instances from different classes, making it challenging to develop algorithms for one-pass optimization. An online AUC optimization algorithm was proposed very recently by Zhao et al. ( 2011 ). It is based on the idea of reservoir sampling, and achieves a solid regret bound by only storing instances, where T is the number of training examples. Ideally, for one-pass approaches, it is crucial that the storage required by the learning process should be in-dependent from the amount of training data, because it is often quite difficult to expect how many data will be received in those applications.
 In this work, we propose a regression-based algorithm for one-pass AUC optimization in which a square loss is used to measure the ranking error between two in-stances from different classes. The main advantage of using the square loss lies in the fact that it only needs to store the first and second-order statistics for the received training examples. Consequently, the storage requirement is reduced to O ( d 2 ), where d is the dimen-sion of data, independent from the number of training examples. To deal with high-dimensional data, we de-velop a randomized algorithm that approximates the covariance matrix of d  X  d by a low-rank matrix. We show, both theoretically and empirically, the effective-ness of our proposal algorithm by comparing to state-of-the-art algorithms for AUC optimization.
 Section 2 introduces some preliminaries. Sections 3 proposes the OPAUC (One Pass AUC) framework.
 Section 4 provides theoretical analysis. Section 5 presents our experimental results. Section 6 concludes with future work. We denote by X X  R d an instance space and Y = { +1 ,  X  1 } the label set, and let D denote an unknown (underlying) distribution over X X Y . A training sam-ple of n + positive instances and n  X  negative ones
S = { ( x + is drawn identically and independently according to distribution D , where we do not fix n + and n  X  before the training sample is chosen. Let f : X X  R be a real-valued function. Then, the AUC of function f on the sample S is defined as  X  where I [  X  ] is the indicator function which returns 1 if the argument is true and 0 otherwise.
 Direct optimization of AUC often leads to an NP-hard problem as it can be cast into a combinatorial opti-mization problem. In practice, it is approximated by a convex optimization problem that minimizes the fol-lowing objective function
L ( w ) = where  X  is a convex loss function and  X  is the regular-ization parameter that controls the model complexity. Notice that each loss term  X  ( w  X  ( x + i  X  x  X  j )) involves two instances from different classes; therefore, it is dif-ficult to extend online learning algorithms for one-pass AUC optimization without storing all the training in-stances. Zhao et al. ( 2011 ) addressed this challenge by exploiting the reservoir sampling technique. To address the challenge of one-pass AUC optimiza-tion, we propose to use the square loss in Eq. ( 1 ), i.e.,
L ( w ) =  X  The main advantage of using the square loss lies in the fact that it is sufficient to store the first and second-order statistics of training examples for optimization, leading to a memory requirement of O ( d 2 ), which is independent from the number of training examples. Another advantage is that the square loss is consistent with AUC, as will be shown by Theorem 1 (Section 4 ). In contrast, loss functions such as hinge loss are proven to be inconsistent with AUC ( Gao &amp; Zhou , 2012 ). As aforementioned, the classical online setting can-not be applied to one-pass AUC optimization because, even if the optimization problem of Eq. ( 2 ) has a closed form, it requires going through the training examples multiple times. To address this challenge, we modify the overall loss L ( w ) in Eq. ( 2 ) (with a little varia-tion) as a sum of losses for individual training instance  X  L ( w ) = for i.i.d. sequence S t = { ( x 1 , y 1 ) , . . . , ( x t is an unbiased estimation to L ( w ). For simplicity, we denote by X + t and X  X  t the sets of positive and negative instances in sequence S t , respectively, and we further denote by T + t and T  X  t their respective cardinalities. Also, we set L t ( w ) = 0 for T + t T  X  t = 0. If y t = 1, we calculate the gradient as  X  X  + It is easy to observe that c  X  t = S the mean and covariance matrix of negative class, re-spectively; thus, Eq. ( 3 ) can be further simplified as  X  X  Algorithm 1 The OPAUC Algorithm Input : The regularization parameter  X  &gt; 0 and step-Initialization : Set T + 0 = T  X  0 = 0, c + 0 = c  X  0 = 0 , w 0 = 0 and  X  + 0 =  X  1: for t = 1 , 2 , . . . , T do 2: Receive a training example ( x t , y t ) 3: if y t = +1 then 7: Calculate the gradient b g t ( w t  X  1 ) 8: else 12: Calculate the gradient b g t ( w t  X  1 ) 13: end if 15: end for In a similar manner, we calculate the following gradi-ent for y t =  X  1:  X  X  where S + t = =  X  of positive class, respectively.
 The storage cost for keeping the class means ( c + t and c  X  t ) and covariance matrices ( S + t  X  1 and S  X  t O ( d 2 ). Once we get the gradient  X  X  t ( w ), by theory of stochastic gradient descent, the solution can be up-dated by w t +1 = w t  X   X  t  X  X  t ( w t ), where  X  t is the stepsize for the t -th iteration.
 Algorithm 1 highlights the key steps of the proposed d . At each iteration, we set  X  + t = S + t and  X   X  t = S and update  X  + t (Line 6) and  X   X  t (Line 11), respectively, by using the following equations 12 in Algorithm 1 are given by  X  X  t ( w t  X  1 ) that are calculated by Eqs. ( 4 ) and ( 5 ), respectively. Dealing with High-Dimensional Data. One lim-itation of the approach in Algorithm 1 is that the stor-age cost of the two covariance matrices S + t and S  X  t is O ( d 2 ), making it unsuitable for high-dimensional data. We tackle this by developing a randomized algorithm that approximates the covariance matrices by low-rank matrices. We are motivated by the observation that S t and S where I t is an identity matrix of size t  X  t and 1 t is an all-one vector of size t . To approximate S + t and S t , we approximate the identify matrix I t by a ma-trix of rank  X   X  d . To this end, we randomly sam-ple r i  X  R  X  , i = 1 , . . . , t from a Gaussian distribu-tion N (0 , I  X  ), and approximate I t by R t R  X  t , where R that contain the subset of the rows in R t correspond-ing to all the positive and negative instances received before the t -th iteration, respectively. Therefore, the covariance matrices S + t and S  X  t can be approximated, respectively, by X imate covariance matrix b S  X  t , the approximation algo-rithm essentially tries to minimize
L if y t = 1; otherwise,
L Further, we have the following recursive formulas: It is important to notice that we do not need to calcu-late and store the approximate covariance matrices b S + t and b S  X  t explicitly. Instead, we only need to maintain matrices Z + t and Z  X  t in memory. This is because the stochastic gradient b g t ( w ) based on the approximate covariance matrices can be computed directly from Z + t and Z  X  t . More specifically, b g t ( w ) is computed as b g t ( w ) = c  X  t  X  1  X  x t +  X  w + ( x t  X  c  X  t  X  1 )( x for y t = 1; otherwise b g t ( w ) = x t  X  c + t  X  1 +  X  w + ( x t  X  c + t  X  1 )( x We require a memory of O (  X  d ) instead of O ( d 2 ) to cal-culate b g t ( w ) by using the trick A [ A ]  X  w = A ([ A ] where A  X  R d  X  1 or R d  X   X  .
 To implement the approximate approach, we initialize  X  0 =  X  each iteration, we set  X  + t = Z + t and  X   X  t = Z  X  t , and compute the gradient b g t ( w t  X  1 ) of Lines 7 and 12 in Algorithm 1 by Eqs. ( 10 ) and ( 11 ), respectively.  X  + t and  X   X  t are updated by Eqs. ( 8 ) and ( 9 ), respectively. Remark. An alternative approach for the high-dimensional case is through the random projec-tion ( Johnstone , 2006 ; Hsu et al. , 2012 ). Let H  X  R  X   X  be a random Gaussian matrix, where  X   X  d . By performing random projection using H , we compute a low-dimensional representation for each instance x t as b x t = H  X  x t  X  R  X  and will only maintain covariance matrices of size  X   X   X  in memory. Despite that it is com-putationally attractive, this approach performs signif-icantly worse than the randomized low-rank approx-imation algorithm, according to our empirical study. This may owe to the fact that the random projection approach is equivalent to approximating S  X  t = I d S  X  t by HH  X  S  X  t HH  X  , which replaces both the left and right identity matrices of S  X  t with HH  X  . In contrast, our proposed approach only approximates one iden-tity matrix in S  X  t , making it more reliable for tackling high-dimensional data. This section presents our main theoretical results. Due to the page limit, we present the detailed proofs and analysis in a longer version ( Gao et al. , 2013 ). We first prove the consistency of square loss: Theorem 1 For square loss  X  ( t ) = (1  X  t ) 2 , the sur-with AUC.
 Proof Sketch: Let X = { x 1 , x 2 , . . . , x n } with marginal probability p i &gt; 0 and conditional proba-bility  X  i , and we denote by the expected risk
R ( f ) = where  X  ( t ) = (1  X  t ) 2 and C 0 is a constant w.r.t. f . According to ( Gao &amp; Zhou , 2012 ), it suffices to prove that, for every solution f s.t. R ( f ) = inf f  X  R ( f  X  we have f ( x i ) &gt; f ( x j ) if  X  i &gt;  X  j . If
X = { x timal solution f ( x 1 )  X  f ( x 2 ) = sgn(  X  1  X   X  2 ) for  X  and this shows the consistency.
 If
X = { x every i  X  [ n ], then minimizing R  X  ( f ) gives the optimal  X  = 0; this also shows the consistency.
 If
X = { x for some i 0  X  [ n ], then subgradient conditions give  X  for i  X  [ n ]. Solving the above n linear equations yields f ( x i )  X  f ( x j ) = where  X  &gt; 0 is a polynomial in p k and  X  [ k 1 ] +  X  [ k Define w  X  = arg min w rem shows the convergence rate for Algorithm 1 when the full covariance matrices are provided.
 Theorem 2 For  X  x t  X  X  X  1 ( t  X  [ T ]) ,  X  w  X   X  X  X  B and T L  X   X  where  X  = 4 +  X  and  X  t = 1 / (  X  + This theorem presents an O (1 /T ) convergence rate for the OPAUC algorithm if the distribution is separable, i.e., L  X  = 0, and an O (1 / general case. Compared to the online AUC optimiza-tion algorithm ( Zhao et al. , 2011 ), which achieves at most O (1 / rithm clearly reduce the regret. The faster convergence rate of our proposed algorithm owes to the smooth-ness of the square loss, an important property that has been explored by some studies of online learning ( Rakhlin et al. , 2012 ) and generalization error bound analysis ( Srebro et al. , 2010 ).
 Remark : The bound in Theorem 2 does not explic-itly explore the strongly convexity of L t ( w ), which can lead to an O (1 /T ) convergence rate. Instead, we focus on exploiting the smoothness of the loss function, since we did not introduce a bounded domain for w . Due is reasonable to restrict w t by | w t | X  1 / X  , leading to a regret bound of O (ln T / [  X  3 T ]) by applying the stan-dard stochastic gradient descent with  X  t = 1 / [  X t ]. This bound is preferred only when  X  =  X ( T  X  1 / 6 ), a scenario which rarely occurs in empirical study. This problem may also be addressable by exploiting the epoch gra-dient method ( Nocedal &amp; Wright , 1999 ), a subject of future study.
 We now consider the case when covariance matrices are approximated by low-rank matrices. Note that the low-rank approximation is accurate only if the eigen-values of covariance matrices follow a skewed distribu-tion. To capture the skewed eigenvalue distribution, we introduce the concept of effective numerical rank ( Hansen , 1987 ) that generalizes the rank of matrix: De nition 1 For a positive constant  X  &gt; 0 and semi-positive definite matrix M  X  R d  X  d of eigenvalues {  X  i the effective numerical rank w.r.t.  X  is defined to be r ( M,  X  ) = It is evident that the effective numerical rank is upper bounded by the true rank, i.e., r ( M,  X  )  X  rank ( M ). To further see how the concept of effective numerical rank captures the skewed eigenvalue distribution, con-sider a PSD matrix M of full rank with for small k . It is easy to verify that r ( M,  X  )  X  k , i.e., M can be well approximated by a matrix of rank k . Define the effective numerical rank for a set of matrices { der the assumption that the effective numerical rank S t can be well approximated by low-rank matrices), the following theorem gives the convergence rate for |  X  Eqs. ( 6 ) and ( 7 ).
 Theorem 3 Let r = r ( { S  X  t } T t =1 ,  X  ) be the effective numerical rank for the sequence of covariance matrices {
S  X  x with probability at least 1  X   X  ,  X  provided  X   X  32 r X  (log 2 dT / X  ) / X  2 , where  X  = 4+  X  and  X  = 1 / (  X  + For the separable distribution L  X  = 0, we also obtain an O (1 /T ) convergence rate when the covariance ma-trices are approximated by low-rank matrices. Com-pared with Theorem 2 , Theorem 3 introduces an ad-ditional term 2  X L  X  in the bound when using the ap-proximate covariance matrices, and it is noteworthy that the approximation does not significantly increase the bound of Theorem 2 if 2  X T L  X   X  B i.e.,  X   X  B proximate algorithm will achieve similar performance as the one using the full covariance matrices provided  X  =  X ( r X T (log d +log T ) / (  X  +4)). When  X  = O (1 /T ), this requirement is reduced to  X  =  X ( r [log d + log T ]), a logarithmic dependence on dimension d . We evaluate the performance of OPAUC on bench-mark datasets and high-dimensional datasets in Sec-tions 5.1 and 5.2 , respectively. Then, we study the parameter influence in Section 5.3 . 5.1. Comparison on Benchmark Data We conduct our experiments on sixteen benchmark datasets 1 , 2 , 3 as summarized in Table 1 . Some datasets have been used in previous studies on AUC optimiza-tion, whereas the other are large ones requiring one-pass procedure. The features have been scaled to [  X  1 , 1] for all datasets. Multi-class datasets have been transformed into binary ones by randomly partition-ing classes into two groups, where each group contains the same number of classes.
 In addition to state-of-the-art online AUC approaches OAM seq and OAM gra ( Zhao et al. , 2011 ), we also compare with:  X  online Uni-Exp : An online learning algorithm  X  batch Uni-Log : A batch learning algorithm  X  batch SVM-OR : A batch learning algo- X  batch LS-SVM : A batch learning algorithm All experiments are performed with Matlab 7 on a node of computational cluster with 16 CPUs (Intel Xeon Due Core 3.0GHz) running RedHat Linux En-terprise 5 with 48GB main memory. For batch algo-rithms, due to memory limit, 8,000 training examples are randomly chosen if training data size exceeds 8,000, whereas only 2,000 training examples are used for the epsilon dataset because of its high dimension. Five-fold cross-validation is executed on training sets to decide the learning rate  X  t  X  2 [  X  12:10] for online al-gorithms, the regularized parameter  X   X  2 [  X  10:2] for OPAUC and  X   X  2 [  X  10:10] for batch algorithms. For OAM seq and OAM gra , the buffer sizes are fixed to be 100 as recommended in ( Zhao et al. , 2011 ). For uni-variate approaches, the class ratios are chosen as done in ( Kotlowski et al. , 2011 ).
 The performances of the compared methods are eval-uated by five trials of 5-fold cross validation, where the AUC values are obtained by averaging over these 25 runs, as summarized in Table 2 . It is evident that OPAUC is better than the other three online algo-rithms OAM seq , OAM gra and online Uni-Exp, par-ticularly for large datasets. The win/tie/loss counts show that OPAUC is clearly superior to these online algorithms, as it wins for most times and never loses. It is also observable that OPAUC is highly competi-tive to the three batch learning algorithms; this is im-pressive because these batch algorithms require stor-ing the whole training dataset whereas OPAUC does not store training data. Additionally, batch LS-SVM which optimizes the square loss is comparable to the other batch algorithms, verifying our argument that square loss is effective for AUC optimization. We have also compared with SVM-perf ( Joachims , 2005 ), on-line and batch univariate square loss, and our results show that OPAUC is is significantly better than online and batch univariate square loss, and highly competi-tive to SVM-perf. Due to page limit, we present these results in a longer version ( Gao et al. , 2013 ). We also compare the running time of OPAUC and the online algorithms OAM seq , OAM gra and online Uni-Exp, and the average CPU time (in seconds) are shown in Figure 1 . As expected, online Uni-Exp takes the least time cost because it optimizes on single-instance (univariate) loss, whereas the other algorithms work by optimizing pairwise loss. On most datasets, the running time of OPAUC is competitive to OAM seq and OAM gra , except on the mnist and epsilon datasets which have the highest dimension in Table 1 . 5.2. Comparison on High-Dimensional Data Next, we study the performance of using low-rank ma-trices to approximate the full covariance matrices, de-noted by OPAUCr. Six datasets 4 , 5 with nearly or more than 50,000 features are used, as summarized in Table 3 . The news20.binary dataset contains two classes, different from news20 dataset. The original news20 and sector are multi-class datesets; in our ex-periments, we randomly group the multiple classes into two meta-classes each containing the same number of classes, and we also use the sector.lvr dataset which re-gards the largest class as positive whereas the union of other classes as negative. The original ecml2012 and rcv1v2 are multi-label datasets; in our experiments, we only consider the label with the largest population, and remove the features in ecml2012 dataset that take zero values for all instances.
 Besides the online algorithms OAM seq , OAM gra and online Uni-Exp, we also evaluate three variants of OPAUC to study the effectiveness of approximating full covariance matrices with low-rank matrices:  X  OPAUC f : Randomly selects 1 , 000-dim features  X  OPAUC rp : Projects into a 1 , 000-dim feature  X  OPAUC pca : Projects into a 1 , 000-dim feature Similar to Section 5.1, five-fold cross validation is exe-cuted on training sets to decide the learning rate  X  t  X  2  X  12:10] and the regularization parameter  X   X  2 [  X  10:2] Due to memory and computational limit, the buffer sizes are set to 50 for OAM seq and OAM gra , and the rank  X  of OPAUCr is also set to 50. The performances of the compared methods are evaluated by five trials of 5-fold cross validation, where the AUC values are obtained by averaging over these 25 runs.
 The comparison results are summarized in Table 4 and the average running time is shown in Figure 2 . These results clearly show that our approximate OPAUCr approach is superior to the other compared methods. Compared with OAM seq and OAM gra , the running time costs are comparable whereas the performance of OPAUCr is better. Online Uni-Exp is more effi-cient than OPAUCr because it optimizes univariate loss, but the performance of OPAUCr is highly com-petitive or better, except on rcv1v2 , the only dataset with less than 50,000 features. Compared with the three variants, OPAUC f and OPAUC rp are more effi-cient, but with much worse performances. OPAUC pca achieves a better performance on rcv1v2 , but it is worse on datasets with more features; particularly, on the two datasets with the largest number of fea-tures, OPAUC pca cannot return results even after run-ning out 10 6 seconds (almost 11.6 days). Our ap-proximate OPAUCr approach is significantly better than all the other methods (if they return results) on the two datasets with the largest number of features: news.binary with more than 1 million features, and ecml2012 with nearby 100 thousands features. These observations validate the effectiveness of the low-rank approximation used by OPAUCr for handling high-dimensional data. 5.3. Parameter In uence We study the influence of parameters in this section. Figure 3 shows that stepsize  X  t should not be set to values bigger than 1, whereas there is a relatively big range between [2  X  12 , 2  X  4 ] where OPAUC achieves good results. Figures 4 shows that OPAUC is not sen-sitive to the value of regularization parameter  X  given that it is not set with a big value. Figure 5 shows that OPAUCr is not sensitive to the values of rank  X  , and it works well even when  X  = 50; this verifies Theorem 3 that a relatively small  X  value suffices to lead to a good approximation performance. Figure 6 compares stud-ies the influence of the iterations for OPAUC, OAM seq and OAM gra , and it is observable that OPAUC con-vergence faster than the other two algorithms, which verifies our theoretical argument in Section 4 . Due to page limit, we only present the results of two datasets for the study of each parameter, but the trends are similar on other datasets, and more results can be found in our longer version ( Gao et al. , 2013 ). In this paper, we study one-pass AUC optimization that requires going through the training data only once, without storing the entire dataset. Here, a big challenge lies in the fact that AUC is measured by a sum of losses defined over pairs of instances from different classes. We propose the OPAUC approach, which employs a square loss and requires the storing of only the first and second-statistics for the received training examples. A nice property of OPAUC is that its storage requirement is O( d 2 ), where d is the dimen-sion of data, independent from the number of training examples. To handle high-dimensional data, we de-velop an approximate strategy by using low-rank ma-trices. The effectiveness of our proposed approach is verified both theoretically and empirically. In partic-ular, the performance of OPAUC is significantly bet-ter than state-of-the-art online AUC optimization ap-proaches, even highly competitive to batch learning approaches; the approximate OPAUC is significantly better than all compared methods on large datasets with one hundred thousands or even more than one million features. An interesting future issue is to de-velop one-pass AUC optimization approaches not only with a performance comparable to batch approaches, but also with an efficiency comparable to univariate loss optimization approaches.
 Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games . Cambridge University Press, 2006. Cortes, C. and Mohri, M. AUC optimization vs. error rate minimization. In Advances in Neural Informa-tion Processing Systems 16 , pp. 313 X 320. MIT Press, Cambridge, MA, 2004.
 Flach, P. A., Hern  X andez-Orallo, J., and Ramirez, C. F.
A coherent interpretation of AUC as a measure of aggregated classification performance. In Proceed-ings of the 28th International Conference on Ma-chine Learning , pp. 657 X 664, Bellevue, WA, 2011. Gao, W. and Zhou, Z.-H. On the consistency of AUC optimization. CoRR/abstract , 1208.0645, 2012. Gao, W., Jin, R., Zhu, S., and Zhou, Z.-H. One-pass AUC optimization. CoRR/abstract , 1305.1363, 2013.
 Hanley, J. A. and McNeil, B. J. A method of compar-ing the areas under receiver operating characteristic curves derived from the same cases. Radiology , 148 (3):839 X 843, 1983.
 Hansen, P. C. Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion . SIAM, 1987.
 Herschtal, A. and Raskutti, B. Optimising area under the ROC curve using gradient descent. In Proceed-ings of the 21st International Conference on Ma-chine Learning , Alberta, Canada, 2004.
 Hsu, D., Kakade, S., and Zhang, T. Random design analysis of ridge regression. In Proceedings of the 25th Annual Conference on Learning Theory , pp. 9.1 X 9.24, Edinburgh, Scotland, 2012.
 Joachims, T. A support vector method for multivariate performance measures. In Proceedings of the 22nd
International Conference on Machine Learning , pp. 377 X 384, Bonn, Germany, 2005.
 Joachims, T. Training linear svms in linear time.
In Proceedings of the 12th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining , pp. 217 X 226, Philadelphia, PA, 2006. Johnstone, I. High dimensional statistical inference and random matrices. In Proceedings of the Inter-national Congress of Mathematicians , pp. 307 X 333, Madrid, Spain, 2006.
 Kotlowski, W., Dembczynski, K., and H  X ullermeier, E.
Bipartite ranking through minimization of univari-ate loss. In Proceedings of the 28th International Conference on Machine Learning , pp. 1113 X 1120, Bellevue, WA, 2011.
 Liu, X.-Y., Wu, J., and Zhou, Z.-H. Exploratory undersampling for class-imbalance learning. IEEE
Trans. Systems, Man, and Cybernetics -B , 39(2): 539 X 550, 2009.
 Metz, C. E. Basic principles of ROC analysis. Semi-nars in Nuclear Medicine , 8(4):283 X 298, 1978. Nocedal, J. and Wright, S. J. Numerical optimization . Springer, 1999.
 Provost, F. J., Fawcett, T., and Kohavi, R. The case against accuracy estimation for comparing induc-tion algorithms. In Proceedings of the 15th Inter-national Conference on Machine Learning , pp. 445 X  453, Madison, WI, 1998.
 Rakhlin, A., Shamir, O., and Sridharan, K. Mak-ing gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th
International Conference on Machine Learning , pp. 449 X 456, Edinburgh, Scotland, 2012.
 Rudin, C. and Schapire, R. E. Margin-based ranking and an equivalence between AdaBoost and Rank-
Boost. Journal of Machine Learning Research , 10: 2193 X 2232, 2009.
 Srebro, N., Sridharan, K., and Tewari, A. Smooth-ness, low noise and fast rates. In Advances in Neural Information Processing Systems 24 , pp. 2199 X 2207. MIT Press, Cambridge, MA, 2010.
 Zhao, P., Hoi, S., Jin, R., and Yang, T. Online AUC maximization. In Proceedings of the 28th Interna-tional Conference on Machine Learning , pp. 233 X 
