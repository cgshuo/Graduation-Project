 Among the visual features of multimedia content, shape is of particular interest because humans can often recognize objects solely on the basis of shape. Over the past three decades, there has been a great deal of research on shape analysis, focusing mostly on shape indexing, clusteri ng, and classification. In this work, we introduce the new problem of finding shape discords , the most unusual shapes in a collection. We motivate the problem by considering the utility of shape discords in diverse domains including zoology, anthropology, and medicine. While the brute force search algorithm has quadratic time complexity, we avoid this by using locality-sensitive hashing to estimate similarity between shapes which enables us to reorder the search more efficiently. An extensive experimental evaluation demonstrates that our approach can speed up computation by three to four orders of magnitude. Anomaly Detection, Shape Large image databases are used in an increasing number of applications in fields as diverse as entertainment, business, art, engineering, and science [36]. Among the visual features contained in an image (e.g. shape, color, and texture), shape is of particular importance since humans can often recognize objects on the basis of shape alone [42]. Because of this special property, shape analysis has received much research attention in the past three decades. Most research effort in the shape analysis community is fo cused on indexing, clustering, and classification. In this work, we propose the new problem of finding the shape that is least similar to all other shapes in a dataset. We call such shapes discords . Figure 1 gives a visual intuition of a shape discord found in an image dataset of 1,301 marine creatures. Note that while most creatures are represented in the dataset several times, the starfi sh only appears once, and is thus reasonably singled out as the most unusual shape. Note also that any intuitive definition of shape discords will have to allow for invariance to the classic transformations that plague shape similarity measures. For example, the seahorse in the top left corner must be rotated approximately 40 degrees before matching another seahorse , and eliminating itself as a contender for discord. The utility of shape discords is very clear; as shown in Figure 1 they allow a user to find surprising shapes in a massive database. Nevertheless, as we are introducing a new problem, we feel it appropriate to concretely motivate the utility of shape discords with several examples from diverse fields. Medical Data Mining: Drosophila melanogaster is the species of fruit fly that has been most commonly used for genetic experiments in the last century. Drosophila is one of the most studied organisms in biological research, particularly in genetics and developmental biology. Drosophila is small and easy to breed in the laborat ory, and its genome is short and  X  X imple X  (only four pairs of chromosomes). Genetic transformation techniques for this organism have been available since the late eighties. One common type of transformation is mutagenesis, where a specific gene is mutated and the developing organi sm is examined for changes in physiology or behavior. Figure 2 shows a subset of wing images collected for a mutagenesis experiment carried out at Florida State University [41], and the discord discovered by our algorithm. Note that the entire wing image was analyzed up to, but not including, the articulation (1 mm from wing attachment to thorax). While the discord discovered in th is example is likely due to a technicians mishandling of a sample and not due to the phenotypical mutation, the result still hints at the utility of shape discords. Note that a brute force attempt to find this discord would have required 512,880,378 shape comparisons. Anthropological Data Mining: Anthropology offers many interesting challenges for data mining, particularly mining of shapes [7]. Examples of shapes which anthropologists may be interested in mining include petroglyphs, pottery [7], projectile points ( X  X rrowheads X ) [30], and bones [20]. It is difficult to overstate the need for efficient algorithms when working with such datasets. As another example, the number of projectile points in the collection at the authors X  institution exceeds one million objects [31]. We collected more than 16,000 projectile point images for an unrelated project, but can consider this dataset with our discord mining algorithm. As Figure 3 shows, the dataset comes from diverse sources, including photographs, onsite field sketches, and silhouettes. Since we are ignoring all but the shape information, this diversity of data sources makes no difference to the task at hand. While some subjectivity exists, the two discords discovered are arguably the most unusual shapes in the dataset. While the vast majority of projectile points are symmetric, the first discord is an unusual asymme tric point from Texas. The second discord is a typical and common  X  X asal Notched X  point, except this example has its right tang broken off. Discovering such anomalies by eye even in a mere 16,000 images is non-trivial, and motiv ates the need for a scalable algorithm. Discords may have other uses . Shape clustering algorithms often suffer diminished utility due to a handful of outlier shapes in the dataset. We may reasonably expect discords to be among those tricky cases. Finding and removing them can serve as a preprocessing step for shape clustering algorithms. This idea may also be beneficial for image compression with techniques that use global base s, like Principal Component Analysis (PCA), since very unusual images are bound to introduce more bases (or more reconstruction error among the  X  X rue X  bases) [14]. As we have shown, the notion of unusual shapes can be useful in different domains. However, to the best of our knowledge, the problem of finding these shapes has not yet been addressed. In this paper, we introduce a novel definition that defines measurement of difference) to its nearest neighbor. This definition has the advantage that the unusualness of a shape is not tied exclusively to its structure. Instead it depends on the similarity between it and its n earest neighbor, and is therefore context dependent. The definition eliminates the need of an explicit description of usual shap es. In addition, this definition requires zero parameters, which is especially suitable for data mining applications [19]. In pa rticular, as we demonstrate below, we can apply our algor ithm in very diverse domains without having to do any tuning or  X  X weaking X  of any kind. Our definition of shape discord would be of little use to the data mining community without an efficient algorithm to discover it. The brute force al gorithm requires a quadratic  X  X ll-to-all X  comparison, which is simply untenable for large real-world datasets. We introduce an algorithm that uses locality-sensitive hashing to quickly discover likely candidates for discords, and use this informa tion to generate heuristics to reorder the search in a more efficient way. The algorithm is able to avoid many fruitless calculations and can achieve three to four orders of magnitude speedup on real problems. The rest of the paper is organized as follows. In Section 2, we discuss some background material and formally define the problem of shape discord discovery. In Section 3, we introduce a general framework for shape discord searching and provide observations for speeding it up. Section 4 presents an algorithm to enable an efficient search strategy based on locality-sensitive hashing. We perform a comprehensive empirical evaluation in Secti on 5 to demonstrate both the utility of shape discords and the efficiency of our search strategy. Finally Section 6 o ffers some conclusions and directions for future work. This section begins with some necessary background material before introducing the formal definition of shape discords. We then discuss why existing novelty/outlier detection approaches are not suitable for the problem at hand. We consider the shape of an object as a binary image representing the outline of the object [25]. In order to find/index/classify a shape, th e shape must be described or represented in some way. However this is a difficult task as shapes may be corrupted with noise, defects, arbitrary distortions, and occlusions. There are many shape representations and descripti on techniques in the literature. Among them, one-dimensional re presentations have been shown to achieve comparable or superior accuracy in shape matching [20]. Therefore this simple representation has been used by an increasingly large fraction of the literature [8][20][38]. There exist dozens of techniques to convert shapes into one-dimensional representations (also known as pseudo  X  X ime series X ). We refer the interested reader to [25] and [42] for excellent surveys. Note that our approach works for any of these representations. To give the reader a concrete idea, in Figure 4, we show the well-known centroid distance approach to convert a shape into a time se ries [42]. This approach is particularly attractive because it requires zero parameters and, after suitable normalization, it removes the effects of scale and offset. Rotation will be discussed below. Even though this one-dimensional representation is very simple, we claim that it is very effective in preserving features of the original shape. To show this, we conducted a classification experiment on se veral publicly available shape datasets, as shown in Table 1. For each dataset, we converted the shapes to one-dimensional representations and performed one-nearest-neighbor classifica tion using rotation invariant Euclidean distance (explained later). The error rates, as measured by leaving-one-out evaluation, are shown in the second column of Table 1. In the third column, we list the best error rates reported by other wo rks, using more complicated shape representations/ distance measures. These experiments show that the very simple time series representations of shapes and the simple Euclidean distance can be competitive to other more complex representations and distance measures. Therefore in this work, we only consider one-dimensional representation of the shapes. From now on, we will use the words  X  X hape X  and  X  X ime series X  interchangeably. Let us first formally define time series . 
Definition 1. Time Series : A time series n c c C ,..., 1 ordered set of n real-valued variables. In our case the ordering is not temporal but spatial; it is defined by a clockwise sweep of the shape boundary. Recall that our task is to find the most unusual shape within a collection. We formally define the problem below. Definition 2. Shape Discord : Given a collection of shapes 
S , shape D is the discord of S if D has the largest distance to its nearest match. That is,  X  shape C in S , the nearest match M C of C and the nearest match M D of D , Dist ( D , M Dist ( C , M C ). Note that if there are two unus ual shapes in the dataset and they are both unusual in the same way (the  X  X win freak problem X ), we can simply change the definition to  X  X ind the shape that has the maximal distance to its second nearest neighbor X . For simplicity, we will not consider this case in this work. However it is a trivial modification. In many cases, we may be interested in examining the top k discords, which is a simple extension of the previous definition. 
Definition 3. k th Shape Discord : Given a collection of largest distance to its nearest match. A critical component of the previous two definitions is the function to measure the distance between two time series, which we formally define below. 
Definition 4. Distance Function : Dist is a function that has two time series Q and C (both of length n ) as inputs and returns a nonnegative value as the distance from Q to C . 
For subsequent definitions to work, we require that the function be symmetric, that is, Dist ( Q , C ) = Dist ( C , Q ). As a concrete instantiation of a distance function, we define the most common distance measure for time series, Euclidean distance [6][16]. 
Definition 5. Euclidean Distance : Given two time series Q and C of length n , the Euclidean distance between them is defined as If the shapes are rotationally aligned, Euclidean distance will usually reflect the intuitive similarity. However if the shapes are not rotationally aligned, the corresponding time series will also be misaligned. In this case, Euclidean distance can produce extremely poor results. To overcome this problem, we need the distance function to be rotation invariant. To achieve this, we need to hold one shape fixed, rotate the other, and record the minimum distance of all possible rotations. We accomplish this in the time series space by representing all rotations of a shape by a rotation matrix . 
Definition 6. Rotation Matrix : Given a time series C of length n , its possible rotations constitute a rotation matrix C of size n by n where each row of the matrix is simply a time series shifted (rotated) by one time point from its neighbors. For notational convenience, we denote the i th row as C allows us to denote the rotation matrix in the more compact form of C = { C 1 , C 2 ,..., C n }. Note that we do not need to actually build the full matrix if space is premium, however doing this simplifies the notation and allows some optimizations [20]. We can now define the Rotation invariant Euclidean Distance between two time series. Definition 7. Rotation invariant Euclidean Distance : 
Given two time series Q and C of length n , the rotation invariant Euclidean distance between them is defined as ( ) The rotation invariant Euclidean distance provides an intuitive measure of the distance between two shapes, at the expense of length n is O( n 2 ). This long discussion of rotation invariance is motivated by the observation that shapes are often rotated in real world domains. For example, Figure 5 shows two sample wing images from a collection of Drosophila images. Note that the rotation of images can vary even in such a controlled domain. This distance definition can be easily generalized to handle enantiomorphic invariance (mirror image), which may be useful in some domains. For example, when matching faces, the best match may simply be facing the opposite direction. If enantiomorphic invariance is required, we can trivially achieve this by augmenting matrix C to contain C i and reverse ( C 1  X  i  X  n . Before calling the distance function, each time series is normalized to have mean zero and a standard deviation of one, because it is well understood that it is generally meaningless to compare time series with different offsets and scales [16]. However, when dealing with time series that are derived from shapes, there is one important exception: if a shape is almost round, we should not do the normalization. The reason is, for nearly circular shapes, the distance from every point on the profile to the center is almost the same. So the resulting time series will be almost a flat line. However even a perfect circle will not produce a perfectly straight line due to rasterization effects. Normalization will exagge rate slight variations of the flat line, producing apparent features in the time series. We have shown such an example in Figure 6. Therefore, whenever we detect that the sum of the differences between each data point and the mean value of the time series is less than a predefined threshold, we will not do the normalization. Note that this is a very rare case th at only happens in one of the six domains we examined (see Section 5.1.2). At this point, we have shown that shapes can be converted to time series. One might ask whether the existing time series novelty detection methods could solve the problem at hand. We believe the answer is no . We cannot leverage off the existing time series novelty detection techniques because most of them assume that time series subsequences are extracted by sliding a window across a long time series [17][18][34], while we have individual time series here. Another possibility would be to simply project the shape time series into n -dimensional space and use existing outlier detection methods [5][22]. The pr oblem with this approach is that most outlier detection methods require the distance function be a metric. While the Euclidean distance is a metric, the rotation invariant Euclidean distance is not. Even if we could bypass or mitigate this problem, most of the current outlier detection methods degrade to quadratic time complexity for high dimensional data. The above analysis suggests that existing algorithms are of little utility for finding shape di scords. This motivates the introduction of our original algorithm in the next section. Our work is closest in spirit to the unusual time series detection work of [17], from which we take the name  X  X iscord X . However, detecting unusual time series is a considerably easier problem because they do not have to deal with the rotation invariance problem, whic h (as we shall show below) considerably adds to the complexity of the problem. Given a shape dataset S of size m , the brute force algorithm for finding the discord is simple and obvious. We simply take each time series and find its distance to its nearest match. The one that has the greatest such value is the discord. The pseudo code of this simple algorithm is shown in Table 2. While the brute force algorithm is intuitive, we will show a running example to develop some intuition on how to improve this algorithm. Figure 7 shows a  X  X race X  of the brute force algorithm on a simple dataset of six marine creatures. The first discord happens to be the starfi sh (shape 4), whose distance to its nearest match is 26.7 (shown in bold in Figure 7). To find the discord, the brute force algor ithm searches the dataset with nested loops, where the outer l oop searches over the rows for each candidate shape, and the inner loop scans across the columns to identify the candidate X  X  nearest rotation invariant match. The brute force algorithm needs to compute the distance between 6*(6-1)/2 = 15 pairs of shapes (the upper triangle of the distance matrix). The brute force algorithm is easy to implement and produces exact results. However, it has O( m 2 ) time complexity (recall for even moderately large data sets. Note that even though the time complexity is quadratic, the space needed by the brute force algorithm is constant. We show the full distance matrix only for clarity. In actual implementation, we can build and examine only one cell at a time. The astute reader may have already noticed a way to speed up the search. In the inner loop, we do not actually need to find the true nearest neighbor to the current candidate. Once we find any shape that is closer to the current candidate than the best_so_far_dist variable, we can stop the search in that row, safe in the knowledge that the current candidate could not be the shape discord. We call this simple optimization early abandoning . If we apply this optimization to the marine creature dataset, we only need to compute the distance between 12 pairs of shapes. Figure 8 shows a trace of the search process. With early abandoning, we can save some computation. The utility of this optimization depends on the order in which the outer loop considers the candidates for the discord, and the order in which the inner loop visits the other shapes. Note that there is nothing special about the top-to-bottom (outer loop), left-to-right (inner loop) ordering th at we have been using. It is simply the classic default of nested  X  for X  loops. As far as the brute force algorithm is concerned, any permutation of the orders is acceptable. However, the early abandoning algorithm can benefit from certain permutations. For example, imagine that a friendly oracle gives us the best possible orderings as follows. For outer loop, shapes are sorted in descending order of the distance to their nearest neighbor, so that the true discord is the first object examined. For inner loop, shapes are sorted in ascending order of the distance to invocation of the inner loop will run to completion. Thereafter, all subsequent invocations of the inner loop will be abandoned during the very first iteration. Returning to our running example (see Figure 9), we only need to compute the distance between 9 pairs of shapes by s earching in the best orderings. The above example motivates th e introduction of a slightly expanded version of the brute force algorithm. The new algorithm is augmented by the early abandoning technique (line 7 in Table 3) and the additional outer and inner heuristics. The pseudo code is shown in Table 3. Now we have reduced the discord discovery problem to a generic framework where all one needs to do is to specify the heuristics. Our goal then is to find the best possible approximation to the optimal ordering, which is the topic of the next section. When trying to approximate the optimal ordering, we need to keep one thing in mind: we should not attempt to  X  X heat X  the algorithm. For example, we c ould provide very good orderings if we are allowed to compute the distances between each pair of the shapes beforehand! However this is simply hiding the time complexity in a different part of the implementation. Therefore we must insist that the outer heuristic (invoked only once) takes at most O( m ) to calculate and the inner heuristic (invoked m times) takes O(1). Note that this requirement precludes the possibility of using R-trees, K-d trees or other classic indexing algorithms: they require at least O(log( m )) time per lookup, but we can spare only O(1) time. With these time constraints, the problem at hand is much harder. The optimal heuristic requires a perfect ordering of shapes in the inner loop, and any perfect ordering (i.e., sorting) requires at least O( m log m ), but we are only allowed O(1). Furthermore, the only known way to produce the perfect ordering of shapes in the outer loop requires O( m run the entire brute force algorithm), but we are only allowed O( m ) time. The following two obser vations, however, offer us some hope for a fast algorithm. 
Observation 1 : In the outer loop, we do not actually need to find a perfect ordering to achieve dramatic speedup. All we really require is that, among the first few shapes being examined, there is at least one that has a large distance to its nearest neighbor. This will give the best_so_far_dist variable a large value early on, which will make the conditional test on line 7 of Table 3 be true more often, thus allowing more early abandonment of the inner loop. 
Observation 2 : In the inner loop, we also do not actually need to find a perfect ordering to achieve dramatic speedup. 
All we really require is th at, among the first few shapes being examined there is at least one that has a distance to the candidate that is less than the current value of the best_so_far_dist variable. This is a sufficient condition to allow early termination of the inner loop. Based on these two observations, we propose an algorithm that uses locality-sensitive hashing to estimate similarity between pairs of shapes, and then generates heuristics to order the outer and inner loops. The first step of our approximation algorithm is the symbolization of time series. Symbolization serves several important purposes. First, it provides us with a lower dimensional representation that re duces the noise effect in the raw time series while preservi ng its properties. Second, it gives us a string representation that will be used in the subsequent step by the location-sensitive hash function. There are many different symbolic approximations of time series in the literature [2][9][ 11]. In this work, we choose the S ymbolic A ggregate Appro X imation (SAX) representation introduced by Lin, et al. [23], because it allows both dimensionality reduction and lo wer bounding. Below, we give a brief review of the SAX repr esentation. We start with the Piecewise Aggregate Approximation (PAA) [15]. Definition 8. Piecewise Aggregate Approximation (PAA) : lower dimensionality w , the Piecewise Aggregate 
Approximation of time series C is a w-dimensional vector In other words, the time series is divided into segments of equal length and each segment is substituted by its mean value. Having the PAA representation, we can apply a further transformation to obtain a disc rete representation. It is desirable to have a discretiza tion technique that will produce symbols with equiprobability. This is easily achieved since normalized time series have highl y Gaussian distributions [23]. We can simply determine the  X  X reakpoints X  that will produce equal-sized areas under a Gaussian curve. 
Definition 9. Breakpoints : Breakpoints are a sorted list of numbers  X  =  X  1 ,...,  X  | X | -1 , where | X | is the size of the alphabet, such that the area under a N (0,1) Gaussian curve from  X  i to  X  i+ 1 = 1/ | X | (  X  0 and  X  | X | are defined as - X  and  X  , respectively). These breakpoints may be determined by referring to a statistical table. For example, Table 4 gives the breakpoints for values of | X | from 3 to 6. Once the breakpoints have been obtained, we can assign the same letter to all PAA coefficients that belong to the same interval. For example, all PAA co efficients that are below the smallest breakpoint are mapped to the symbol  X  a  X , all coefficients greater than or equal to the smallest breakpoint and less than the second smallest breakpoint are mapped to the symbol  X  b  X , etc. Figure 10 illustrates this idea. Note that in this example, the four symbols,  X  a  X ,  X  b  X ,  X  c  X , and  X  d  X  are equiprobable, as desired. We call the concatenation of symbols that represent a time series a word . 
Definition 10. Word : A time series C of length n can be represented as a word w c c C  X  , ,  X   X  1 K = . Let  X  element of the alphabet, i.e.,  X  1 = a and  X  mapping from a PAA approximation C to a word C obtained as follows: We have now completely defi ned SAX representation. Note that the tendency of time series to have Gaussian distributions [23] is not critical to the correctness of any algorithms that use SAX, including those in this wo rk. A pathological dataset that violates this assumption will only affect the efficiency of the algorithms. As we noted earlier, to give relatively good outer and inner orders, we need to quickly approximate the similarities between all shapes. Estimation of similarity based on sparse sampling of positions from feature vectors has been used in diverse areas for different purposes, including high-dimensional search [28], multimedia indexing [38], and motif discovery [37], etc. Among the rich literature, the locality-sensitive hashing search technique proposed by Indyk and Motwani [12] is perhaps the mo st referenced in this area. Since this technique is a corner stone of our contribution, we give the formal definition of locality-sensitive hashing below. 
Definition 11. Locality-sensitive Hash Function : Consider a string s of length w over an alphabet  X  and k indices i 1 , ... , i k chosen uniformly at random from the set {1, ... , w }. Define the locality-sensitive hash function f :  X  by In other words, the locality-sensitive hash function concatenates characters from, at most, k distinct positions of Clearly, strings similar to each other are more likely to be hashed to the same LSH value. This is the most important property of locality-sensitive hashing, which enables efficient search, indexing, and many other works [12]. Unfortunately, this property does not hold for shapes because of the rotation variance. For example, the two arrowheads in Figure 11 are quite similar but differently aligned. Their time series representations are shifted by some offset and the resulting SAX words are completely different. They will not be hashed to the same LSH value no matter which k positions are chosen. Considering the above problem, we define a rotation invariant locality-sensitive hash function . Definition 12. Rotation invariant Locality-sensitive Hash Function : Consider a string s of length w over an alphabet 
 X  and k indices i 1 , ... , i k chosen uniformly at random from the set {1, ... , w }. Define the rotation invariant locality-sensitive hash function f X  :  X  w  X  (  X  k ) w by where LSHIFTS ( s ) is the set of all possible left shifts of string s . The rotation invariant locality-s ensitive hash function maps a value of one shift of s . By doing this, similar shapes (even with different orientations) are more likely to be mapped together to some LSH value. For example, consider the same two images in Figure 11. Using rotation inva riant hashing, both of them are mapped to the LSH value  X  X a X , as shown in Figure 12. At this point, we are ready to present our algorithm of estimating the simila rity among shapes and approximating the optimal ordering of early abandoning search. Suppose we have a dataset S of m shapes (each has been converted to a time series of length n ), the SAX word size w , and the SAX alphabet  X  . We begin by converting all time series to SAX words and placing them in an array. Note that each row index of the array refers back to the original shapes. Figure 13 gives a visual intuition of this, where both w and | X | are set to 4. Note that in spite of the redundancy of multiple rotations, the size of the array is much smaller than the time series data and inconsequential in size compared to the raw images. Once the array has been constructed, we randomly choose a rotation invariant locality-sensitiv e hash function and use it to hash SAX words into buckets. For example in Figure 13, we choose indices {1, 3}. Therefore the SAX words in the array are hashed into buckets based on the first and third columns of their shifts: first SAX word daca is hashed to buckets dc , aa , and cd ; fourth SAX word adad is hashed to buckets aa and dd ; so on and so forth. If two shapes corresponding to SAX words i and j are hashed to the same bucket, we increase the count of cell( i , j ) in the collision matrix by one, which has been initialized to all zeros. In our example, we increase the count of cell(1, 4) and cell(4, 1). Note that the words corresponding to shapes 2 and 3 are also hashed into a common bucket ba , yet they are not similar to each other (even after considering all rotations). This problem can be solved simply by repeating the hashing process a number of times, each time with a new, randomly chosen hash function. The above process of time seri es extracting, discretizing, hashing, and collision recording is formalized in Table 5. Note accumulated throughout the entire process, while the hash buckets cannot be reused fro m one iteration to another. After repeating the process an appropriate number of times, we examine the collision matrix. If two shapes are similar, we expect the corresponding cell in the collision matrix to have a large value. If two shapes are different, the collision matrix tells us nothing. However, we can infer from the lack of a value in the collision matrix that two shapes are probably different. So we can use collision matrix as a guideline to decide the outer and inner orderings (line 16 in Table 5). 
Outer Heuristic: We scan the collision matrix row by row to find the largest number of collisions each shape has with others. Then we sort the shapes in ascending order using this value. The resulting ordering is given to the outer loop. The intuition behind our outer heur istic is that unusual shapes are very likely to have fewer collisions with others. By considering the candidate shapes in the ascendant order of the number of collisions they have, we have an excellent chance of giving a large value to the best_so_far_dist variable early on, which (as noted in observation 1) will make the conditional test on line 7 of Table 3 be true more often, thus allowing more early abandonment of the inner loop. 
Inner Heuristic: When candidate shape i is considered in the outer loop, the inner loop examines the shapes in the descending order of the number of collisions they have with shape i . The intuition behind our inner heuristic is that shapes which frequently collide with each other are very likely to be highly similar (this fact is at the heart of more than twenty research efforts [3][6][19][21][24][36]). As noted in observation 2, we just need to find one such shape that is similar enough (having a distance to the candidate less than the current value of the best_so_far_dist variable) to terminate the inner loop. There are several minor optimizations we can apply to the heuristic search algorithm. For example, imagine we are considering candidate C i in the outer loop, and as we traverse through the inner loop, we find that time series C enough to it to allow early abandonment. In addition to saving time with the early abandonment, we can also delete C the list of candidates in the outer loop (if it has not already been visited). The key observation is that since we are assuming a symmetric distance measure, if nearness to C disqualifies candidate C i from being the discord, then the same nearness to C i would also disqualify candidate C the discord. Empirically, this simple optimization gives a speedup factor of approximately two. In addition, there are several well-known optimizations to the Euclidean distance [16] and some special optimi zations to rotation invariant Euclidean distance [20] that we can use. Recall that we require the outer heuristic take at most O( m ) time to calculate and the inner heuristic take O(1). We will now show that our optimal order approximation algorithm fulfills these requirements. As explained above, our algorithm has three steps. First, we convert shape time series to SAX words, requiring just one pass through the time series. So th e time for this step is linear in the size of the dataset, m . Secondly, we apply rotation invariance hash functions on SAX words to separate them into buckets. This again only requi res one pass through the SAX words for each iteration. In the third step, we pair the indices in the same buckets and update the collision matrix accordingly. This could have quadratic time complexity in the worst case. For practical datasets, however, the number of pairs is clearly sub-quadratic. More precisely, the time required by this step is O( i|CM| ), where i is the number of iterations and | CM | is the number of non-zero entries in the collision matrix CM . In general, the collision matrix is extremely sparse, and the number of iterations is on the order of 10 to 100. So the time for this step is linear in the size of the dataset. To summarize, the time complexity of the optimal order approximation algorithm is O( m ). For the optimal order approxi mation algorithm, we must choose three parameters: the SAX alphabet size |  X  |, the SAX word size w , and the number of iterations i . Recall that we would like the shape discords hash to some rarely used buckets, while all other shapes hash to popular buckets. If we choose very large values for |  X  | and/or w , almost all shapes will map to unique words; if we choose very small values for |  X  | and/or w , all shapes will map to just a small handful of words. Either of these situati ons is bad for separating shapes into buckets. The good news is that there is little freedom for the |  X  | parameter. Extensive experiment s carried out by the current authors [6][17][18][19] and dozens of other researchers worldwide [3][21][32][36] suggest that a value of either 3 or 4 is best for virtually any task on any dataset. After empirically confirming this on the current problem with experiments on more than 50 datasets, we will simply hardcode |  X  | = 4 for the rest of this work. Having fixed |  X  |, we performed an exhaustive empirical examination of the role of the w parameter. The best value for this parameter depends on the data. In general, relativel y smooth and slowly changing datasets favor a smaller value of w , whereas more complex time series favor a larger value of w . Empirical studies show that the speedup does not critically depend on w parameter. We will simply use w = 20 for the rest of this work. For the setting of the number of iterations i , there are three possibilities. We can do a fixed number of iterations, or we can stop when the user is not willing to wait more time. Since the algorithm keeps the best_so_far_index variable, it always has a candidate discord to show at any point of time after the collision matrix has been built (this actually makes it an anytime algorithm [10]). Or we can stop when the collision matrix  X  X onverges X  (the change of the values of its entries are less than some threshold). Again, for simplicity, we fix the number of iterations to 30 in this work. At the risk of redundancy, we emphasize once again that our algorithm produces exact answers. None of these parameters affects the correctness of the algorithm. Setting these parameters inappropriately will only affect the efficiency of the algorithm. We begin this section by s howing the utility of the shape discords in diverse domains, a nd continue by demonstrating that the algorithm can find discords very efficiently using the approximate optimal heuristic. For all the experiments in this work, we use rotation invariant Euclidean distance to measure the distance between two shapes. To demonstrate the usage of shape discords in real world applications, we conducted disc ord searches on datasets from diverse domains. Butterfly wings are an interesting domain in which to test image mining algorithms. Depending on the area of research, the shape, color, texture or even fractal dimension of the wings may be of interest [4]. Here we restrict our attention to shape. The large size of such collections motivates the use of scalable algorithms. For example, the Morphbank archive [27] currently has approximately 2, 000 butterfly images online, and many lepidopterists (butterfly collectors) have much larger personal collections. Consider the image dataset in Figure 14, which purportedly shows a (subset of) collection of Heliconius erato (Red Passion Flower) Butterflies. We ran our algorithm on the entire collection to find the most unusual butterfly, which happens to be the one shown in the lower right of Figure 14 . We ask entomologist Dr. Agenor Mafra-Neto to explain the result. In fact, the butterfly in question is not an example of Heliconius erato , it is an example of Heliconius melpomene (The Postman). The uncanny resemblance of the two is not a coincidence, but an example of M X llerian mimicry. In brief, it is believed that the two species originally looked different, and (perhaps independently) evolved the de fense mechanism of tasting unpalatable. Being foul tasting is only useful if you advertise the fact, and once one species had evolved the orange/red flashes to communicate this to predators, the other species leveraged off the predators X  avoidance by mimicking their appearance. Figure 15 clearly shows why the one butterfly was singled out from the collection. We ran some tests on images extracted from red blood cell data. Figure 16 shows a subset of an image. The discord discovered is a teardrop shaped cell, or dacrocyte . Such cells are indicative of several blood disorders, including myelofibrosis, metaplasia and anemia. Note that for those cells which are almost round, we did not normalize the time series derived (cf Figure 6). We perform similar experiment s on images taken at even higher resolutions. Figure 17 s hows an image taken with a scanning electron microscope. Th e image shows some spores produced by a rust (fungus) known as Gymnosporangium, which is a parasite of apple and pear trees. Note that one spore has sprouted an  X  X ppendage X  known as a germ tube, and is thus singled out as the discord. Note that the spore above and to the left of the discord is just beginning to start sprouting a germ tube. If it also had a full germ tube, then the discovered discord would not longer be so far away from its nearest neighbour (the  X  twin freak problem  X , discussed in Section 2.1). As noted earlier we could mitigate this problem by a simple change in the definition of discord. We compare the performance of the discord discovery algorithm using our approximate optimal heuristic, the random heuristic (deciding the orders of outer and inner loops randomly), and brute force search . The measuremen t we use is the number of times that the di stance function is called on line 7 in Table 3. A simple analysis of the pseudo code (confirmed with a profiler) tells us that this single line of code accounts for more than 99% of the running time for the algorithm. This metric is implementation-free so it avoids the bias introduced by examining wall clock or CP U time, a problem noticed by many researchers [16][17][40]. For our approximate optimal he uristic, we include a startup cost of O( m ), which is the time complexity required to build the collision matrix (cf. Secti on 4.4). For brute force search, the number of times that th e distance function is called depends only on m and can simply be computed (recall m is the size of the dataset). If we had to actually run the brute force search for all the experime nts in this work, it would take several years. We first tested a homogeneous dataset of 10,000 projectile point images. The tim e series derived are all of length 251. The results are shown in Figure 18. Each time we use a subset of the database (the size varies from 50 to 10,000) and measure the number of distan ce function calls required by each strategy, divided by the number of calls required by brute force. We can see that the cost of building the collision matrix is dwarfed by rotation invariant comparisons. The approximate optimal heuristic is faster even for small dataset of size 50. As we expect, the approximate op timal heuristic performs better as the size of the dataset increases. By the time we have examined the entire database , our approximate optimal heuristic is one order of magnitude faster than the random heuristic and several orders of magnitude faster than brute force. Sometimes techniques that wo rk well for highly homogeneous datasets do not work well for he terogeneous datasets, and vice versa. We consider this possibility by testing on a heterogeneous dataset in Figure 19. The heterogeneous dataset consists of all the data used in the classification experiments (cf. Table 1), plus 1,000 projectile points. In total, it contains 5,844 images and the derived time series are of length 512. In this dataset, it takes our approximate optimal heuristic slightly longer to beat random heuristic. However by the time we have seen 200 objects, we have already broken even and thereafter rapidly race towards beating random heuristic by one order of magnitude and brute force search by several orders of magnitude. As a final sanity check, we also measured the wall clock time of our best implementation of all methods. The results are essentially identical to those shown above, and are omitted in the sake of brevity. In many applications, it can be useful to discover the most unusual shape in a collection of images. In this work, we introduce a novel definition of such shapes: the discord. This definition is particularly attractive to data mining applications because it is parameter-free. In addition, we propose an efficient algorithm to discover the shape discords. The algorithm uses locality-sensitive ha shing to estimate similarity between pairs of shapes and gene rates heuristics to reorder the search in a more efficient way. On real problems, our algorithm is three to four orders of magnitude faster than the brute force algorithm. There are many directions in which this work may be extended. We intend to inves tigate image discords not only using shapes but also textures. In addition, we plan to conduct a field study of shape discord discovery in anthropology and archeology. All images are used with permi ssion, and remain copyright of their respective owners. Thanks to Dr. Stefano Lonardi; Dr. Michail Vlachos; Dr. Agenor Mafra-Neto (entomology); Biosciences Electron Microsc opy Facility, University of British Columbia; Dr. Leslie A. Quintero and Dr. Philip J. Wilke (projectile points); Karolina Maneva-Jakimoska (Morphbank); Jill Brady; Daniel von Dincklage. [1] Adamek, T. and O X  X onnor, N. E. A multiscale representation [2] Andre-Jonsson, H. and Badal, D. Using signature files for [3] Bentley, J. L. and Sedgewick, R. Fast algorithms for sorting [4] Castrejon-Pita, A. A., Sarmiento-Galan, A., Castrejon-Pita, J. [5] Chen, Z., Fu, A., and Tang, J. On complementarity of cluster [6] Chiu, B., Keogh, E., and Lonardi, S. Probabilistic discovery [7] Clark, J. T., Bergstrom, A., Landrum, J. E. III, Larson, F., [8] Davies, E. R. Machine Vision: Theory, Algorithms, [9] Daw, C. S., Finney, C. E. A ., and Tracy, E. R. Symbolic [10] Grass, J. and Zilberstein, S. Anytime algorithm development [11] Huang, Y. and Yu. P. S. Adaptive query processing for time-[12] Indyk, P., Motwani, R., Raghavan, P., and Vempala, S. [13] Jalba, A. C., Wilkinson, M. H. F., Roerdink, J. B. T. M., [14] Jolliffe , I. T. Principle Component Analysis . Springer, 2 [15] Keogh, E., Chakrabati, K., Pazzani, M., and Mehrotra, S. [16] Keogh, E. and Kasetty, S. On the need for time series data [17] Keogh, E., Lin, J., and Fu, A. HOT SAX: Efficiently Finding [18] Keogh, E., Lonardi, S., and Chiu, W. Finding surprising [19] Keogh, E., Lonardi, S., and Ratanamahatana, C. Towards [20] Keogh, E., Wei, L., Xi, X., Lee, S., and Vlachos, M. [21] Kitaguchi, S. Extracting feature based on motif from a [22] Knorr, E., Ng, R., and Tucakov, V. Distance-based outliers: [23] Lin, J., Keogh, E., Lonardi, S., and Chiu, B. A symbolic [24] Lin, J., Keogh, E., Lonardi, S., Lankford, J. P., and Nystrom, [25] Loncarin, S. A Survey of Shape Analysis Techniques. [26] Mollineda, R. A., Vidal, E., and Casacuberta, F. Cyclic [27] Morphbank. http://morphbank2.csit.fsu.edu/ [28] Narayanan, M. and Karp, R.M. Gapped Local Similarity [29] Qamra, A., Meng, Y., and Chang, E. Enhanced Perceptual [30] O X  X rien, M.J., Darwent, J., and Lyman, R.L. Cladistics is [31] Philip, J. W. Personal Communication. 2006. [32] Rombo, S. and Terracina, G. Discovering representative [33] Sadakane, K. Compressed text databases with efficient query [34] Shahabi, C., Tian, X., and Zh ao, W. Tsa-tree: A wavelet-[35] S X derkvist, O. J. O. Computer Vision Classification of [36] Tanaka, Y. and Uehara, K. Motif discovery algorithm from [37] Tompa, M. and Buhler, J. Finding motifs using random [38] Van Otterloo, P. J. A contour-oriented approach to shape [39] Veltkamp, R. and Hagedoorn, M. State of the art in shape [40] Vlachos, M., Vagena, Z., Yu, P. S., and Athitsos, V. [41] Zimmerman, E., Palsson, A., and Gibson, G. Quantitative [42] Zhang, D. and Lu, G. Review of shape representation and 
