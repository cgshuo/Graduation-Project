 The firehose of data generated by users on social network-ing and microblogging sites such as Facebook and Twitter is enormous. Real-time analytics on such data is challenging with most current e ff orts largely focusing on the e ffi cient querying and retrieval of data produced recently. In this paper, we present a dynamic pattern driven approach to summarize data produced by Twitter feeds. We develop a novel approach to maintain an in-memory summary while retaining su ffi cient information to facilitate a range of use r-specific and topic-specific temporal analytics. We empir-ically compare our approach with several state-of-the-art pattern summarization approaches along the axes of storage cost, query accuracy, query flexibility, and e ffi ciency using real data from Twitter. We find that the proposed approach is not only scalable but also outperforms existing approach es by a large margin.
 H.2.8 [ Database Applications ]: Data Mining Data Summarization, Analytics, Twitter
Microblogging, a lightweight and easy form of communi-cation within social networks such as Facebook, Google+ and Twitter, has become ubiquitous in its use with over 4billionmobiledevicesworldwideofwhichover1billion support smart services. An increasing number of organiza-tions and agencies are turning to extract and analyze useful nuggets of information from such services to aid in function s as diverse as emergency response, viral marketing, disease outbreaks, and predicting movie box o ffi ce success. A funda-mental challenge for e ff ective human-computer interaction (querying and analytics) is the scale of the data involved. Twitter for instance has over 200 million users (and grow-ing) and several hundred million tweets per day. Supporting interactive querying and analytics requires novel approac hes for filtering and summarizing such data.

Given the diverse nature of applications, a number of queries may be of interest. Queries such as: What are the currently trending topics?; What did a specific user tweet about today or yesterday? are straightforward to support since one only needs to maintain recent data to answer such queries. However, often times users and organizations, are interested in capturing high level trends about both curren t and past activity  X  particularly highly trending past activ -ity to understand the evolution of user interests and topic trending. For example complex queries of the following form would be of interest: What topics were people talking about in a specific time interval (2 weeks in the past)?; How has a particular topic evolved across multiple time intervals?; How have a user X  X  or a group X  X  tweets, or topics they tweet on changed over time? Answers to such questions may enable organizations to understand questions related to the lineage of topic evolution as well as to better understand user inter -ests, their influence, and possibly build a model of trust for specific users and groups.

Answering such queries in real-time is challenging simply because of the scale of the data that is produced  X  the mem-ory footprint will grow linearly with time and it can easily overwhelm the capacity of even the most powerful computer systems. In this study, we aim to build a summary of mi-croblogging data, focusing on Twitter feeds, that can fit in alimitedmemorybudgetandcanhelptoanswercomplex queries. In our view the desiderata for such a framework in-clude: 1) e ffi cient, incremental summary construction (ide-ally using a single pass and at pace with data influx rate); 2) budgeted memory which grows at most logarithmically with data influx; and 3) support for complex querying and an-alytics with low reconstruction error (particularly on mor e recent data, or on highly trending data). Ideally, we would like to be able to answer queries about the topics in a time interval in the past, and evolutionary events related to spe -cific topics across multiple time intervals.

Anovelframework(Figure1)isproposedtoaddressthis desiderata. The elements of our framework include: a) SPUR, a batch summarization and compression algorithm that relies on a novel notion of pattern utility and ranking which can be incrementally updated; b) D-SPUR, a dynamic variant of SPUR that accordingly merges summaries and maintains pyramidal time frames that grows logarithmicall y while enabling querying at multiple temporal contexts; and c) TED-SPUR, a topic and event based analytics tool to support complex querying on dynamic data.

We compare the e ff ectiveness of various SPUR variants against state-of-the-art pattern summarization algorith ms on a large corpus of Twitter data along the axes of com-pressability, reconstruction error, e ffi ciency and flexibil ity in querying. We find that the SPUR variants are up to two orders of magnitude faster and can produce summaries with much lower reconstruction errors than extant approaches. Furthermore, maintaining temporal information in D-SPUR enables the approximate reconstruction of original data over arbitrary time intervals facilitating novel complex queri es. We further demonstrate the e ffi cacy of TED-SPUR, in an-alyzing temporal topic evolution and capturing real-world behavioral events. Figure 1: Overview of Summarization via Pattern Utility and Ranking (SPUR) Framework
In this section, we introduce the summarization compo-nent of our proposed stream processing framework. Figure 2: Division and compression of message stream
Given the input message stream with proper word stem-ming and stop-word removal performed, we divide it into approximately equal-sized batches, e.g. one hour per batch (the first arrow in Figure 2). To compress each batch of messages into a summary object which can fit in a con-stant memory budget M (the second arrow in Figure 2), we describe our SPUR algorithm in Section 2.1. Then in Section 2.2 we discuss the D-SPUR algorithm which ensures the summary size grows logarithmically with time.
We develop a novel algorithm called SPUR (Summariza-tion via Pattern Utility and Ranking) to summarize a batch of transactions with low compression ratio and high quality in a highly scalable fashion. Our basic idea of compressing abatchoftweetsistoreplaceindividualwordswithfre-quently used phrases that can cover the original content of the tweets. Consider the example in Figure 3 (left), each col -umn represents a word and each row represents a tweet. The original tweets need 24 words to be stored in memory. How-ever, if we use the frequent phrases as patterns to represent the tweets, we can save 10 of 24 words (Figure 3 right). Our approach represents each tweet as a transaction of words and abatchasasetoftransactions. Wecastthechallengeof finding frequent phrases as a frequent itemset mining prob-lem. To compress a batch of tweets into a summary with memory budget M ,weaimtoreducethestoragesizeby covering the transactions with frequent patterns. There ar e three main challenges one need to address: compressability , scalability and quality (of compressed summary). Figure 3: A batch of tweets compressed to a summary
Algorithm 1 provides the pseudo code of SPUR. The algo-rithm receives a batch of transactions B ,amemorybudget M ,asupportthreshold  X  and a false positive rate f as input. It outputs a summary which can fit in the memory budget M with false positive rate lower than f .First,ouralgo-rithm mines frequent itemsets above the support threshold  X  as candidate patterns (line 1). We use LCM [13] for our purposes. Second, we define a function to capture the util-ity of each pattern in terms of compressing the transactions (line 3). Third, we rank all the candidates by their utility values and insert them into a priority queue (lines 2  X  4). Finally, we iteratively select the top ranked pattern to cov er the items in transactions until we reach the memory budget M or the top pattern is not cost e ff ective for compression any more (lines 5  X  13).
 Algorithm 1 SPUR( B , M ,  X  , f )
Acoverageoftransaction T i  X  B using pattern p will re-place the items in T i  X  p with a pointer to p .Twotypesof errors will be introduced: false negative errors are the ite ms in the original data but are not covered by any pattern; false positive errors are items not belonging to a transacti on but are introduced by a pattern coverage, i.e. the items in p \ T i .Falsenegative/positiverateistheratiooffalseneg-ative/positive errors over the total number of items in the transactions. In Algorithm 1, f is a threshold to control false positive rate; and the false negative rate is controll ed by  X  and M together where  X  decides the infrequent items dropped by frequent pattern mining and M controls the se-lection of frequent items in the summary, thus indirectly determines the frequent items that will be dropped. Next, we will first introduce the definition of pattern utility.
Pattern Utility and Ranking: We define the utility of apattern p covering a transaction T i as: where | T i  X  p | captures the storage saved by p ,1isthe storage cost of a pointer to p in the compressed representa-tion of T i and | p \ T i | is the amount of false positive errors. Here | T i  X  p |  X  1recordsthetotalsavingsinstoragespace whereas | p \ T i | penalizes false positive errors 1 .
There is also a cost of space for storing pattern p ,because we need to record the actual items in p .Butthisisnotacost for a coverage with an individual transaction, but the cost for a set of transactions. Given a set of transactions C  X  B , the total utility of covering all transactions in C with p is defined as the sum of the utilities on all transactions in C less the cost of storing pattern p :
The compression utility Utility ( p )ofpattern p is the max-imum value of U ( p, C )amongall C  X  B .The coverage set C ( p )of p is defined as the set of transactions that yields this maximum value. So, Utility ( p )=max
For example, in Figure 3, the best compression using pat-tern p 1 is to cover transactions T 1 , T 2 and T 3 .So C ( p { T 1 ,T 2 ,T 3 } , u ( p 1 ,T i )= | T i  X  p 1 |  X  1  X  | p 1 3and Utility ( p 1 )= P 3 i =1 u ( p 1 ,T i )  X  | p 1 | =5.Ourcompres-sion algorithm needs to find the value of Utility ( p )foreach pattern p and the transactions in C ( p )withtheconstraint that the false positive rate should be below a threshold f .
Let X  X  first consider a simple case where no false positive errors are allowed in a coverage. In this case p can only cover transactions that contain p .Supposepattern p with support  X  ( p )andlength l ( p ), if we use sup ( p )= { T | p  X  T } to represent the set of all transactions that contain p ,then C ( p )= sup ( p ). Therefore, Utility no fp ( p )= X
Next, let X  X  increase the complexity of the pattern coverage problem by allowing false positive errors but no limitation of the false positive rate. Each transaction T i will add u ( p, T to Utility ( p ). So the value of Utility ( p )isatmaximum when all T i  X  X  with u ( p, T i )  X  0arein p  X  X  coverage set C ( p ). We can rewrite the definition of u ( p, T i )as:
Apatterncoveragewillnotintroducefalsenegativeerrors , because the support threshold in frequent pattern mining decides the infrequent items we dropped.
 So the set of transactions that can maximize p  X  X  utility is C ( p )= { Ts.t. | T  X  p |  X  ( l ( p )+1) / 2 } .Itisine ffi cient if we intersect all possible pattern and transaction pairs. Th e following theorem provides a faster way to find C ( p )byonly considering p and its sub-patterns X  supporting transactions.
Theorem 1. Suppose C # ( p )= { S sup ( p i ) | p i  X  pand l ( p i )  X  ( l ( p )+1) / 2 } , where sup ( p i ) represents al l transac-tions containing p i ,then C ( p )= C # ( p ) .

Proof. First show C ( p )  X  C # ( p ).  X  T i  X  C ( p ), | T i  X  p |  X  ( l ( p )+1) / 2. Let p # = T p  X  p and l ( p # )  X  ( l ( p )+1) / 2. p #  X  T i  X  T i  X  sup ( p C ( p ). So C ( p )  X  C # ( p ).

Then show C # ( p )  X  C ( p ).  X  T i  X  C # ( p ),  X  p i s.t. T i  X  sup ( p i ), p i  X  p and | p ( l ( p )+1) / 2. Then p i  X  T i  X  p and | T i  X  p |  X  | p  X  T i  X  C ( p ). So C # ( p )  X  C ( p ).

Theorem 1 shows that the utility value of a pattern p can be maximized by covering all transactions that contain sub-patterns of p with more than half of the length of p .We essentially replace p  X  X  sub-patterns with p in those trans-actions. This process will reduce the storage size by intro-ducing false positive items. However, we cannot control the upper bound of the false positive rate in this strategy. Next , we will discuss how we can guarantee that the false positive rate in the summary is below a threshold f .
 Algorithm 2 Utility( p , f )
Suppose p i is a sub-pattern of p ,ifwereplace p i with p in the transactions that contain p i but not p ,thefalseposi-tive rate is 1  X  l ( p i ) /l ( p ). Therefore, longer sub-patterns will introduce lower false positive rate. To control the false po si-tive rate below a threshold f ,Algorithm2firstsortsthesub-patterns of p from long to short (line 3  X  4) and keeps replac-ing the sub-patterns in this order until the false positive r ate is higher than f (line 5  X  21). With this greedy strategy, Al-gorithm 2 can find the maximum utility of a pattern below a false positive rate threshold. Algorithm 2 also generates t he transactions in the coverage ( p .coverage set) as well as the sub-patterns that are replaced by p ( p .replaced patterns). Note: we define the utility of singleton patterns as 0. In-stead of making a new pattern with only a single item and storing pointers to it, we can directly store the item id in each transaction and it will cost the same amount of memory as the original data.

The SPUR algorithm calls Algorithm 2 to initialize the ranking of pattern utilities. However, the ranking can chan ge dynamically during the compression iterations. Existing ap-proaches either use a static approximation (e.g. Krimp [11] ) or if dynamic, need multiple passes of the data. Hence, they do not fit in the setting of summarizing data streams. Next, we will show how our utility function can be dynamically and e ffi ciently updated without accessing the original data. Compression with Dynamic Ranking Adjustment There are three categories of patterns whose utility values will be a ff ected by a top 1 pattern p : p  X  X  super-patterns, sub-patterns and overlapping patterns (Figure 4). Algorithm 3 elaborates how we penalize the utilities of these three type s of patterns when p is selected. For an a ff ected pattern p first find the transactions in the intersection of p a  X  X  and p  X  X  coverage sets (line 2). The e ff ective coverage area of p be changed by including p in these transactions, because the common items of p and p a are already covered by p .Based on the type of p a ,wecanpenalizeitsutilityvaluebythe total area covered by p already. For a super-pattern p super the area covered by p is l ( p )pertransaction(line4)and l ( p overlap  X  p )foraoverlappingpattern p overlap (line 12). For a sub-pattern p sub ,theareacoveredby p is l ( p sub )per transaction. But since the transactions covered by p are not in p sub  X  X  coverage set any more, the space of the pointers to p sub in those transactions are saved. So each transaction is penalized by l ( p sub )  X  1(line6).Wecanseeouralgorithm only needs to deduct a value of area from a pattern X  X  utility without scanning the items in the transactions.
 Algorithm 3 UpdateRank( Q , p )
There are several implementation and performance related issues worth mentioned: First, our algorithm needs to find the sub-/super-/overlapping-pattern relationships amon gall patterns. However, once these relationships are establish ed, they are reused by the many iterations of the SPUR al-gorithm. Second, the overlapping-patterns with p are found by taking the union of all super-patterns of p  X  X  sub-patterns. Third, we use a max heap to maintain the candidate pattern queue dynamically. We now present D-SPUR, the dynamic version of SPUR. In D-SPUR, we enhance and modify the pyramidal time window suggested by Aggarwal at al. [1] for clustering in data streams. Our enhancements center on the fact that we need to find an e ff ective way to manage the stream of sum-mary objects produced by SPUR while limiting the growth of memory footprint and reconstruction error (especially on recent and trending data). D-SPUR summarizes dynamic message streams by maintaining the pyramidal time win-dow in Figure 5.

The input to the pyramidal time window is a stream of summary objects, each with memory size M .(righthalfof Figure 2). A level of the time window can hold two summary objects. Figure 5 demonstrates how the summary objects are inserted into the time window. Initially, the time win-dow is empty, so Summary 1 and 2 can be directly inserted into Level 1 of the time window (Figure 5(a), (b) and (c)). When Summary 3 is ready, Level 1 of the time window is full (Figure 5(c)). We will merge the summary objects on the filled level, expand the time window with one more level, and insert the merged summary objects into the expanded level. In Figure 5(d), Summary 1 and 2 are merged and moved to Level 2, and Sumary 3 is inserted into Level 1. Similarly, Summary 4 is inserted into Level 1 (Figure 5(e)). When Summary 5 is ready, we first merge Summary 3 and 4intoanewsummaryobjectSummary3-4andinsertit into Level 2 of the time window and place Summary 5 on Level 1 (Figure 5(f)). Note that the product of the merging operation (e.g. Summary 1-2 and Summary 3-4) must also fit in the constant memory budget M .

Using the pyramidal time window, the total memory foot-print will grow logarithmically. More importantly, histor ical data is more compressed than recent data, so the summary is more accurate in recent time intervals. Next, we discuss the key operations of maintaining the time window: merging two summaries and maintaining time information.
Merging Two Summary Objects Given two summary objects S 1 and S 2 ,thecorrespondingpatternsandthetrans-actions represented by the patterns are S 1 =( P 1 ,T 1 )and S 2 =( P 2 ,T 2 ). Both S 1 and S 2 are produced by the SPUR al-gorithm and can fit in a constant memory budget M .When merging S 1 and S 2 in the time window (Figure 5), we want to merge them into a new summary S =( P, T )thatcan also fit in M .Wefirstcombinethepatternsets P 1 and P 2 to a new pattern set P # by removing the duplicate patterns, representing all the transactions in T 1 and T 2 with a unified alphabet P # .Thenwemergethetransactionsets T 1 and T 2 into a new transaction set T # to get a new summary ( P #
However, we cannot guarantee that ( P # ,T # )canfitinthe memory budget M .Morecompressionisneededtomeet the budget. We rely on the output of SPUR to perform the compression. Remember the SPUR algorithm outputs autilityvalueforeachpattern. Thisvaluemeasuresthe compression performance of a pattern. We use a greedy strategy to reduce the memory space of ( P # ,T # ). We sort the pattern utility values from low to high. We start with dropping the low utility patterns and removing them from the transactions they cover. This process will stop when the total size of the patterns and transactions left is below M .Weoutputthefinalmergedsummary S =( P, T )asour result. D-SPUR uses this merging algorithm to maintain the pyramidal time window in Figure 5. After merging two summaries, however, the time information of the messages is lost since we cannot distinguish the messages between two batches. We will address this problem next.

Maintaining Time It is necessary to maintain time in-formation for the transactions in a summary, otherwise the summary cannot e ff ectively answer a query about an arbi-trary time interval. For example, if the state for the sum-mary is as shown in Figure 6, and we want to retrieve mes-sages in batches 3 to 6, the query cannot be answered if no time information is stored with each transaction as the messages in batches 1 to 4 and 5 to 8 are all merged together. When we compress individual batches to summaries with SPUR, the transactions in the summary are represented by the patterns selected by SPUR. It is likely that two simi-lar but non-identical transactions will be represented by t he same set of patterns and they will essentially be identical i n the summary. Instead of storing them separately, we store distinct transactions in the summary and associate a count with each transaction to indicate how many times a trans-action appeared in a batch. Figure 7: An example of the time series of a transaction
When D-SPUR merges summaries of two adjacent batches, the merging operation combines their transaction sets. If two transactions contain the same set of patterns, they must be from two di ff erent batches, because within a batch, we only keep distinct transactions. Instead of summing the count of these two transactions, we could concatenate their counts in time order and form a time series with two points. As D-SPUR combines more summaries, we concatenate more points to each transaction. A time series that spans batches (i.e. the red dashed line in Figure 7) is therefore formed for each transaction, enabling reconstruction of the exact cou nt in any time interval.

However, this time series will grow linearly with time, which will violate the memory budget constraint. We there-fore approximate (compress) the time series using a con-stant number of linear regression lines (blue line in Figure 7). Whenever the number of regression lines exceed a constant k after merging summary objects, D-SPUR will compress the two series for all transactions. We employ methods pro-posed by Palpanas et al. [9] for the purpose of compressing the time series. Their methods can find the optimal approx-imation when concatenating two adjacent time series.
Next, we present the query processing component of our framework, an analytical tool TED-SPUR (T opic and E vent detection with D-SPUR), to support complex queries on dy-namic data. One typical example of analytical query on text data is to discover topics [3, 10, 12]. Under the dynamic set-ting of message streams, it is also important to model the evolutionary behaviors [2] of topics. With the in-memory summary produced by D-SPUR, we can approximately re-construct the original messages to perform those two tasks on arbitrary time interval(s).

We employ an implementation of non-negative matrix fac-torization (NMF) algorithm [12] to find the topics from the tweets within a query time interval, as NMF has been shown to handle sparse input well. The main idea is to factor a d  X  w document-word matrix into two non-negative matrices, the first of which a d  X  k document-topic matrix and the second a k  X  w topic-word matrix.

Another interesting task of analyzing tweets stream is to discover the evolution of the topics. For this task, we define aseriesofeventstocapturethe appearance/disappearance of a topic, the growth/ shrinkage/ continuation of topic pop-ularity and the merging/ split/ transformation of topic con-tent. Given two time intervals I 1 and I 2 ,assume I 2 is after but not necessarily succeeding I 1 .Lettheminedtopicsets from both intervals be T 1 and T 2 respectively. Each topic is further represented as a word distribution z and a sup-port value sup indicating how many times it appears in the time interval. We use asymmetric KL-divergence [5] to cap-ture the di ff erence between two topics with regard to their content. The formalization of topic events is as follows: Appearance: Atopic z appears in T 2 i ff there is no topic z in T 1 such that D KL ( z, z # ) &lt;  X  ,whereparameter  X  mea-sures the closeness of two topics. The intuition here is that if we cannot find a topic z # in T 1 which is close enough to topic z ,wewillconsidertopic z as a novel topic in T 2 . Disappearance: Atopic z disappears in T 1 i ff there is no topic z # in T 2 such that D KL ( z, z # ) &lt;  X  . Growth: For two topics ( z, sup )in T 2 and ( z # ,sup # )in T if D KL ( z, z # )  X   X  ,and sup/sup #  X  1+ $ where 0 &lt;  X  &lt;  X  and $ &gt; 0, then topic z grows from topic z # .Toexplain, we find a topic z in T 2 whose content is similar enough to topic z # in T 1 while it is also more frequent than z # .Here the similarity threshold  X  should be much lower than  X  . Shrinkage: For two topics ( z, sup )in T 2 and ( z # ,sup T ,if D KL ( z, z # )  X   X  ,and sup/sup #  X  1  X  $ where 0 &lt;  X  &lt;  X  and $ &gt; 0, then topic z # shrinks to topic z . Continuation: For two topics ( z, sup )in T 2 and ( z # ,sup # ) in T 1 ,if D KL ( z, z # )  X   X  ,and1  X  $ &lt;sup/sup # &lt; 1+ $ where 0 &lt;  X  &lt;  X  and $ &gt; 0, then there is a continuation from z # to z .Toputitinformally,ifwecanfindtwotopicsin T 1 and T 2 with similar enough contents and their strengths are also similar, then we regard the topic in T 2 as the continuation of the topic in T 1 .
 Merging: Given a topic z in T 2 ,andasubsetoftopics { z # from T 1 such that each z # satisfies  X  &lt;D KL ( z, z # ) &lt;  X  .For any pair of topics z # 1 and z # 2 in { z # } ,lettheweightedsumof then we say z is merged from z # 1 and z # 2 .The merging event deals with topics which can neither be classified as growth/ shrinkage/ continuation from a previous topic, nor anewlyappearedtopic.Theexplanationisthatifthecon-tent of a topic is similar enough to that of the weighted combination of two topics from previous time interval, then we consider the new topic to be merged from the previous two topics. Note that usually the resultant z  X  X  from NMF are divergent from each other, making it unlikely that mul-tiple pairs in { z # } merge into the same z .
 Split: Given a topic z in T 1 ,andasetoftopics { z # } from T ,suchthat z # satisfies  X  &lt;D KL ( z # ,z ) &lt;  X  .Foranypair of topics z # 1 and z # 2 in { z # } with the same definition of Z as above, if D KL ( Z, z )  X   X  ,then z is split into z # 1 and z Transformation: If a topic z in T 2 is not involved in any of the seven aforementioned events, z is said to be trans-formed from topics { z # } in T 1 ,suchthateach z # satisfies  X  &lt;D KL ( z, z # ) &lt;  X  .Theunderlyingintuitionisthatifa topic can find topics in the previous time interval which are partly similar to it and are not involved in any other event, then there is a content transformation to this topic.
By issuing multiple topic modeling queries over subse-quent time windows and applying the above event detection algorithms, one can find evolutionary events over time.
Next, we present results for an extensive set of experi-ments we conducted to evaluate our stream summarization methods and advanced query processing capabilities. We considered several other algorithms as candidates for our baseline. Methods that use a probabilistic models to sum-marize patterns [15] are not e ffi cient enough to handle the data influx rate we would like to handle. We therefore omit comparisons with these methods. We compare SPUR with the following algorithms: a) CDB [16] that uses rectangles to cover a transactional database b) RPMine [17] that tries to cluster the patterns and use the cluster centers to cover the remaining patterns. c) Krimp [11] that generates a stati c ranking of all patterns first and then summarizes the data using the top ranked patterns. Neither CDB nor RPMine can compress streaming data. Therefore, the D-SPUR algo-rithm is compared only with StreamKrimp [14], the stream-ing version of Krimp. We gathered 2100 hours of Twitter message streams from June to September in 2010 2 .Thetotaldatasizeis5.9GB. For our evaluation, we partition the message stream into 1-hour batches. There are 100,000 messages per batch and 8 words per message on average after stop word removal and word stemming.
As provided by Twitter, it is a 15% random sample of all messages.

All experiments were performed on a desktop machine with dual boot Red Hat Linux 6 and Windows 7 operating systems. The machine is equipped with an Intel i7 3.4GHz CPU and 16GB of main memory. Except for Krimp and StreamKrimp for which only Windows binaries are available, all algorithms were executed under Linux. All algorithms were implemented in C++.
In this section we present performance results for com-pressing a batch of messages to a summary object. Note that we only present results for batch summarization and not for the summary merging procedure. The SPUR algorithm is compared with the three baseline algorithms ( CDB, RP-Mine and Krimp ). The windows executable for Krimp and the source code for CDB and RPMine were obtained from the authors X  websites. As the baseline algorithms can-not compress to a target memory budget, we relax the mem-ory budget constraint in our algorithm by summarizing a batch until the utility of the top pattern is negative. We present results for processing the first 100 batches since they are su ffi ciently representative of the entire data. Our comparative study focuses on three aspects: execution time , false positive rate (i.e. compression quality) and compres -sion ratio.
We first compare SPUR with CDB and RPMine as they all support the trade o ff between false positive rate and com-pression ratio. Support level is set to 0 . 01% for all three methods. We set a low support level because we want the summary to cover as many topical words as possible. We set the maximum false positive rate for SPUR and CDB to 0.1. For RPMine, the pattern cluster tightness parameter is also set to 0.1.

Figure 8a presents the running times for the three meth-ods in log scale. It is easy to see that our method is signifi-cantly more e ffi cient  X  it is at least one order of magnitude faster than RPMine, and two orders of magnitude faster than CDB. SPUR is able to process an hours worth of data in less than one minute, lending itself to our requirement of being able to process the stream in real-time.

We next evaluate the algorithms X  false positive rate. Ide-ally, we want to introduce as few false positives as possible when reducing the data size. We therefore measure the ac-tual false positive rate each method exhibits when given the same parameters for support threshold and maximum false positive rate. Figure 8b presents the false positive rate fo r each batch of the stream. Again, the performance of SPUR is solid  X  its false positive rate never exceeds 0.005. In con -trast, CDB constantly su ff ers from a higher false positive rate, which is close to the maximum specified value. We also find that RPMine is susceptible to changes in data size, as suggested by the two spikes at batches 41 and 83. With rel-atively small data size for both batches, false positive rat es reach 0.365 and 0.399 respectively.

As for the compression ratio (the lower the better), our method performs midway between CDB and RPMine with arangeof50%to60%(seeFigure8c).Againnotetherel-atively high compression ratio for RPMine for batches 41 and 83. We conjecture that RPMine tends to cluster single-ton patterns with longer patterns for these cases, leading to less reduction in size and a higher false positive rate. Note that the compression quality for our method is more robust against changes in data size, which is a favorable property.
All algorithms achieve exactly the same false negative rate as this only depends on the support level that is set to be same for all algorithms. For this set of experiments, we compare Krimp with SPUR. We compare KRIMP separately from CDB and RPMine for two reasons. First, it does not allow for false positives dur -ing compression. Second, its running time is very large when support level is as low as 0 . 01%. Therefore, we set the sup-port level to 0 . 03% in the following set of experiments. For our method, we set the maximum false positive rate to 0.0.
The running times and compression ratios for the two al-gorithms are shown in 8d and 8e. Our method is always at least 10 times faster than Krimp, and it also bests Krimp in terms of compression ratio.
In this section, we present the performance of the D-SPUR algorithm on summarizing the Twitter message stream. There is no streaming version for CDB and RPMine, thus we only compared D-SPUR with StreamKrimp at support level = 0 . 03% and false positive rate = 0. For D-SPUR, the mem-ory budget M is set to 1.2 MB as we find it yields low false negative error rate in the SPUR summarization step. We use 5 linear segments to approximate the time series of the transactions. This configuration leads to a compression rat io of approximately 50% for each batch. We evaluate the algo-rithms in three aspects: execution time, memory footprint, and quality of the summary.
 Execution time :Figure8fshowstherunningtimesforD-SPUR and StreamKrimp. The x-axis represents the batch number and the y-axis shows the time di ff erent stream sum-marization algorithms take to summarize the input stream. We can see D-SPUR is much faster than StreamKrimp at the support level of 0 . 03%. Even at a lower support level of 0 . 01%, D-SPUR only takes approximately 40 minutes to summarize 100 hours worth of Twitter messages. Our method can thus process the data faster than it arrives, which is desirable for real-time stream processing. Memory footprint : As for memory consumption, we plot the storage size of the raw stream together with the mem-ory footprint for di ff erent summarization algorithms in Fig -ure 8g. We can see that the memory footprint for D-SPUR grows very slowly  X  asymptotically it grows logarithmicall y in the size of the input. Furthermore, the size of D-SPUR X  X  summary is 8 times smaller than the raw data and 3 times smaller than the summary produced by StreamKrimp. This is because the merging of two summaries in D-SPUR is based on the pyramidal time window. If one does not use the pyra-midal summarization scheme, the memory footprint grows linearly (see the green line in Figure 8g). Also note that StreamKrimp has an even higher memory footprint than our SPUR algorithm. When a new batch of transactions streams in, the StreamKrimp algorithm first tries to com-press the transactions with the codetable used for the pre-vious batches. The old codetable is not always suitable for compressing the new batch and often it is forced to rebuild anewcodetableforthenewbatch. Ine ff ect, it ends up maintaining separate summaries for individual batches. Quality :Weevaluatethesummaryqualitybyquerying the summary with a randomly generated query workload. The query is in the format of a keyword w and a times-tamp t . w and t are sampled uniformly at random from all words above the support threshold and all time stamps. We do not consider infrequent words because they are dropped by the frequent pattern mining algorithm and can never be retrieved by any frequent pattern based summarization al-gorithms. All messages containing keyword w at time t are reconstructed and returned as the query result. We rep-resent the result as a multiset of words, and compute the Jaccard similarity between the query result and the origina l messages that contain w at time t to measure query accu-racy. We expect the query accuracy to be high for both recent data and frequent keywords.

We present the distribution of query accuracy over time and word frequencies. Figure 8i presents the distribution o f query accuracy over time and word frequencies where sys-tem time T is 62. The y-axis represents the query time stamps aligned with the pyramidal time window at T and x-axis represents the frequency of the query keywords. The color represents the accuracy at a specific time and word frequency, the darker the better. We can see on recent data, e.g. T = 61  X  62, the accuracy is high on all words; on historical data, e.g. T = 1  X  32, the accuracy is higher on frequent words than infrequent words; the overall accuracy on the recent data is higher than the historical data; and the accuracy on frequent words is generally higher than in-frequent words.

Figure 8h shows the distribution of query accuracy when we rewind the system time T to 30. We observe the same trend as the accuracy on recent data for frequent words is higher than that on historical data for infrequent words. Compared with the summary at system time 62 (Figure 8i), we can see the overall query accuracy for batch 1 to 30 is higher, except for the infrequent words for batch 1 to 16. At system time 62, our pyramidal time window compresses the historical data (batch 1 to 30) more, so query accuracy on those batches is low. But at system time 30, the summary can keep more information and is more accurate.
In this section, we present qualitative evaluations of our algorithm by presenting experimental results for answerin g topic modeling and event detection queries with our sum-mary on the Twitter data. For these experiments, we apply D-SPUR on subsets of the 2100-hour dataset to answer topic modeling and event detection queries. The data is divided into 6-hour batches with approximately 700K messages per batch. Support threshold is set to 0.01%, false positive rat e Table 1: Event detection from the topics in Table 2b and 2c to 0.1 and M to 20 MB. For the topic modeling algorithm, we use k =50asthenumberoftopics.

Topic modeling: We build a summary of data from June 11th to June 26th. All messages generated between 5pm, June 12th and 5pm, June 13th and match the query  X  X orld cup X  are retrieved from the summary. This time interval is the second day after the World Cup in South Africa be-gan. The top 10 topics returned by NMF are listed in Ta-ble 2a, with each topic represented by the top 5 words of the topic X  X  word distribution. There are several topics detect ed from our summary that are very related to the keywords  X  X orld cup X . For example,  X  X outh africa X  indicates the coun -try where the World Cup was held;  X  X uvuzela X  is a handy horn used by South African fans in the World Cup stadiums, and there was a popular debate about whether  X  X uvuzela X  should be banned, because it made an annoying noise. Note that the query time interval is also one day before the game between England and USA. The topic modeling algorithm detected a topic mentioning this game with the keywords  X  X NG X  and  X  X SA X . The detected topics show that our sum-mary can approximately reconstruct the messages to sup-port topic modeling queries.

Event Detection: Next, we give an example of an event detection query. A summary is built for data from June 26th to July 11th, and the  X  X orld cup X  query is run on two time intervals: 5pm, July 3rd to 5pm, July 4th; 5pm, July 4th to 5pm July 5th. We first perform topic modeling on the re-trieved messages. The resultant topics are listed in Table 2 b and 2c. Note that the first time interval is the day after the quarter-final game between Netherlands and Brazil, and the day before the quarter-final game of Argentina against Ger-many. From Table 2b, we can see some related keywords of these two games, for example, NED , BRA , ARG and GER are abbreviations for the teams, and Maradona is the coach of Argentina, Messi and Klose are the players of Argentina and Germany respectively. In this time interval, we detecte d topics which discuss the NED vs. BRA game on the day be-fore and predict the ARG vs. GER game. Similarly, the second time interval is the day after Germany beat ARG in the quarter-final and before the semifinal began. Table 2c lists the topics in the second time interval. We also detect topics which discuss previous day X  X  ARG vs. GER game, such as the keywords ARG, GER, Messi and Klose. There are also messages which predict that Germany can enter the final because they defeated Argentina, and messages that predict Spain (ESP) can enter the semi-final.

Next, we apply the event detection algorithms introduced in Section 3 on the topics of the two time intervals. We set the parameters in the algorithm to  X  =0 . 5,  X  =0 . 5and $ =0 . 3. The detected events are presented in Table 1. The numbers in the table are topic ids in Table 2b and 2c. Among the events, the newly emerged topics about predicting the semifinal and final games are detected as appear; the topic about the NED vs. BRA game in the day before is catego-rized as disappear; two same topics about live TV coverage from the two time intervals are detected as continue event; the two topics about the ARG vs. GER game in the first time interval are merged to a single topic in the second time interval after Germany defeated Argentina; finally, a threa d of topic about Argentina is transformed to a similar topic in the second time interval. We can see that our event detection algorithm can detect evolutionary events which are aligned with events in the real world. These events can help users understand the dynamics of the topics in Twitter messages.
Since the content generated by social media is available as a stream of timestamped messages, dynamic studies have been conducted to track topics on the stream [7, 6], and find temporal patterns [18]. However, all of these studies assume unlimited amount of memory to store the data. We are interested in summarizing the data stream with a lim-ited memory budget and serving new analytical tasks with the summary. Existing work can also benefit from the sum-mary from approximately reconstructing the original data by querying the summary.

We treat each social media post as a transaction of words and use frequent patterns to summarize. The CDB [16] al-gorithm uses rectangles to cover a transactional database; RPMine [17] first tries to cluster the patterns and use the cluster centers to cover the remaining patterns. Methods that use a probabilistic model of the patterns [15] are slow and not capable of processing streams that arrive at a fast pace. All the above algorithms do not support mining on data streams. Krimp [11] follows the MDL principle to summarize the data with codetables, which is essentially a static ranking of patterns. StreamKrimp [14], the stream-ing version of Krimp, can dynamically adjust the codetable for streaming data. Traditional methods of mining frequent patterns on streams [4, 8] focus on either counting item or pattern frequencies rather than summarizing the data.
We proposed an e ffi cient stream summarization frame-work, which can incrementally build summaries of Twitter message streams with one-pass over the data. We devel-oped a novel algorithm to compress twitter messages with low compression ratio, high quality and fast running time. The memory footprint of our stream summarization algo-rithm grows approximately logarithmically with time. Our summary allows one to issue queries to retrieve messages over arbitrary time intervals. The original messages can be approximately reconstructed to support topic modeling al-gorithms. We also defined a suite of events on topics from two time intervals. An event detection algorithm is propose d to find evolutionary events between two time intervals. Acknowledgments: This material is based upon work sup-ported by the National Science Foundation under Grant No. SoCS-1111118 and IIS-0917070. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
