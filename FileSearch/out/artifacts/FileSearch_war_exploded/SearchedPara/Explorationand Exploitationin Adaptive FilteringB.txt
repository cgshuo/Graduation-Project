 Yi Zhang* yiz@cs.cmu.edu Wei Xu** xw@ccrl.sj.nec.com Jamie Callan* callan@cs.cmu.edu An information  X ltering system monitors a documen t stream to  X nd the documen ts that matc h information needs speci X ed by user pro X les. One of the most dif- X cult task for  X ltering is on-line time-critical  X ltering,
Not Deliv ered R  X  =B R N  X  =B N where the value of a documen t decays rapidly with time. In this case the potentially relev ant documen ts must be deliv ered immediately , thus the system has no time to accum ulate and rank a set of documen ts. Adaptive information  X ltering systems receiv e periodic feedbac k from the user about which deliv ered docu-ments were relev ant, which provides training data for learning. 1 User satisfaction is typically modeled with a linear util-ity function. The function used in recen t TREC Fil-tering track evaluations is shown below (Rob ertson &amp; Hull, 2001).
 U tility = A R  X  R + + A N  X  N + + B R  X  R  X  + B N  X  N  X  (1) This model corresp onds to assigning a positiv e or neg-ative value to each elemen t in the categories of Ta-ble 1, where R  X  ; R + ; N  X  ; and N + corresp ond to the number of documen ts that fall into the corresp onding category , and A R ; A N ; B R and B N corresp ond to the credit/p enalt y for each elemen t in the category . B R and B N are usually set to zero because i) user satis-faction is mostly in X  X enced by what has been seen, and ii) N + and N  X  are usually too big. For example, the TREC-9 Filtering Track Utilit y was T 9 U = 2 R +  X  N + . The traditional metho ds of deciding whether to de-liver a documen t try to maximize the estimated im-mediate utilit y of the decision. That is, a documen t is deliv ered if and only if the expected immediate util-ity of deliv ering it is greater than the expected utilit y of not deliv ering it. The expected immediate utilit y can be calculated from the probabilit y that the docu-ment is relev ant. For example, for the utilit y functions used by the TREC-9, TREC-10 and TREC-11 Filter-ing tracks, participan ts usually set the threshold where P ( relevant j document ) = 1 = 3 because the expected utilit y at that point is 0 (Arampatzis &amp; Hameren, 2001; Zhang &amp; Callan, 2001). In fact, deliv ering a documen t i X  P ( relevant j document ) &gt; = 1 = 3 was writ-ten explicitly in the guidelines of some recen t TREC adaptiv e  X ltering tracks (Rob ertson &amp; Hull, 2001). The deliv ery criterion above tries to optimize the im-mediate satisfaction of the user. It does not consider the possibilit y that the system can impro ve its knowl-edge about the user's information need based on the feedbac k from the user so that it can better serve the user in the future. Especially in the early stage of  X l-tering, when the system's knowledge about the user's information need is very limited, the potential gain from impro ving the user model can be substan tial. The work describ ed in this paper is based on consid-ering the value of longer-term exploration along with the immediate reward of deliv ering a documen t when setting decision boundaries. We prop ose to use utilit y divergence, which is de X ned in Section 2.2, as the mea-sure of the model qualit y. Unlik e measures of model qualit y used in most activ e learning metho ds, util-ity divergence has the advantage of having the same scale as the traditional utilit y model adaptiv e  X ltering systems try to optimize. Thus we can combine the expected immediate utilit y with the expected model qualit y to get a single quan tity that measures the short-term and long-term value of a documen t in the documen t stream. This combined measure is the basis for deciding whether to deliv er the documen t. The following sections describ e our researc h on ex-ploration and exploitation while  X ltering based on Bayesian theory . Section 2 describ es the general framew ork of optimizing the utilit y based on the trade-o X  between exploration and exploitation using Bayesian activ e learning. Sections 3 and 4 describ e our experimen tal metho dology and results. Section 5 discusses related work and Section 6 concludes. In order to maximize the overall utilit y of the system we prop ose to have two modules for the system: The exploitation module and the exploration module. We also prop ose to use utilit y as the measure for both modules: U 1 , the direct utilit y gain of deliv ering a documen t for the exploitation module, and U 2 , the expected utilit y gain of knowing the documen t label for the exploration module. Thus we have uni X ed con-trol of the exploration and exploitation trade-o X , and deliv er a documen t if the combined utilit y is above 0. For simplicit y we use Bayesian logistic regression as our learner, and the objectiv e function to maximize is: 2.1. Exploitation Using Bayesian Inference As mentioned in Section 1, the direct gain/loss in util-ity due to deliv ering a documen t can be estimated based on the probabilit y of relev ance for that docu-ment. Supp ose we have a model parameterized as  X  to estimate the probabilit y of the relev ance of a given doc-umen t. The prior distribution of the model parameters is p (  X  ). After seeing data D = f ( x 1 ; y 1 ) ;  X  X  X  ; ( x the posterior distribution p (  X  j D ) is: where P ( D j  X  ) is the likeliho od of the user feedbac k given deliv ered documen ts and  X  .
 The Bayesian average of the immediate utilit y gain of deliv ering a documen t is: where A y is the utilit y of deliv ering a documen t with the true label y . According to Table 1, A y = A R if the true label is \relev ant", and A y = A N if the true label is \non-relev ant". 2.2. Exploration Using Bayesian Activ e For exploration we need to de X ne the qualit y of the learned model. In the activ e learning framew ork pro-posed by (Tong &amp; Koller, 2000), if we choose to use model ~  X  and the true model is  X  , we incur some loss Loss (  X  jj ~  X  ). Although we do not know the exact value of  X  , p (  X  j D ) represen ts our beliefs about the distribu-tion of  X  given the evidence. Thus the expected loss of using model ~  X  is given by: where p (  X  j D ) is the posterior distribution of di X eren t model parameters. The qualit y of a model after we have seen data set D is: where  X   X  D = arg min ~  X  Loss ( D; ~  X  ).
 Smaller Loss ( D ) means a better model. For activ e learning, Loss (  X ;  X   X  D ) needs to capture the notion of uncertain ty of the model. One commonly chosen metric is Kullbac k-Leibler divergence (KL-div ergence) (Tong &amp; Koller, 2000): KL (  X  jj  X   X  D ) = where p ( x ) is the distribution of input x , which is in-dependen t of  X  and is usually given or learned from unlab elled data.
 The usefulness of knowing the label of a documen t is measured by its potential to lower Loss ( D ). However, in information  X ltering the ultimate goal is to optimize some utilit y function in the long run. It is unnatural to combine KL-div ergence with the expected immediate utilit y credit/loss to get a single quan tity on which the decision of whether to deliv er a documen t can be made. It is also unclear how a smaller KL-div ergence relates to higher utilit y. In stead of using KL-div ergence, we prop ose to use the di X erence between the best possi-ble utilit y and the actual utilit y, which we call utility diver gence , as the function Loss (  X ; ~  X  ) to measure the model qualit y: where U (  X  0 ;  X  00 ) is the expected utilit y if we choose to use model  X  00 and the true model is  X  0 . A well-de X ned U (  X  0 ;  X  00 ) should have the following prop erty which essen tially says that the expected utilit y of an incorrect model cannot exceed the expected utilit y of the correct model. It is worth noting that KL-divergence is a special case of utility diver gence . If we which shows that when using loglik eliho od as utilit y, U D (  X  jj ~  X  ) = KL (  X  jj ~  X  ).
 Using utilit y divergence as the loss function, Loss ( D ) can be rewritten as shown below.
 For information  X ltering, the goal is to maximize the utilit y (Equation 2), so we use the following as utilit y: of x where model  X  00 \thinks" deliv ering x has imme-diate positiv e utilit y.
 The expected reduction of utilit y divergence due to knowing the label of a documen t x is: U 2 ( x j D ) = If there are N futur e (discoun ted) future documen ts, then the expected utilit y of deliv ering documen t x is: We deliv er a documen t x if and only if U ( x j D )  X  0 2.3. Logistic Regression to Determine the Information  X ltering systems usually compute numeric scores that indicate how well each documen t matc hes each pro X le. For example, one approac h used widely in the TREC adaptiv e  X ltering comm unity for learning what the user is interested in is based on incremen-tal Rocchio (Rocchio, 1971). Documen ts with scores above pro X le-sp eci X c dissemination thresholds are de-livered. Since the dissemination decision cannot be delayed, it is necessary to  X nd a dissemination thresh-old for each pro X le. Optimal dissemination thresholds are di X cult to determine a priori unless the relev ance score is a good probabilit y measure, which is not true for many pro X le learning algorithm (e.g., Rocchio, sta-tistical language modeling, and Naiv e Bayes). Learn-ing optimal dissemination thresholds adaptiv ely is a very importan t adaptiv e  X ltering problem (Arampatzis &amp; Hameren, 2001; Robertson &amp; Walker, 2000; Zhang &amp; Callan, 2001). In this section we apply the theory discussed above to the problem of determining the dis-semination threshold. We assume we already have a separate module that can compute the score of doc-umen t. The input to the algorithm is the score x of a documen t; the problem is to determine whether to deliv er the documen t given its score.
 We use logistic regression to model the conditional probabilit y of user feedbac k y given the documen t score x : where the prior distribution p ( w ) equals the Gaussian distribution N ( w ; m 0 ; v 0 ). m 0 is the mean of the Gaus-sian and v  X  1 0 is the covariance of the Gaussian. In information  X ltering the explicit goal is to  X nd a de-cision boundary , in this case the dissemination thresh-old t , to optimize the linear utilit y function. t corre-sponds to the boundary of S (  X  00 ) in Equation 11. Thus the qualit y of the model should be quan ti X ed by the expected utilit y of using t  X   X  that de X nes S (  X  00 ) in Equation 11: where t  X  00 is the threshold model  X  00 \thinks" is the best.

U (  X  0 ;  X  00 )
Loss ( D; ~  X  ) To calculate Loss ( D ), we can  X nd t  X   X  2.4. Computational Issues Computation of U 1 in Equation 5 and Loss ( D ) in Equation 10 involve integration over posterior p (  X  j D ). However, the posterior distribution p ( w j D ) for logis-tic regression is quite complicated and the integration cannot be calculated in closed form. Our strategy is to use a Monte Carlo metho d to get an appro ximate solution. We generate K random samples  X  i using the Metrop olis-Hastings algorithm (Tanner, 1996). Then U 1 and Loss ( D ) can be appro ximated as shown below. FUNCTION : Calculate dissemination threshold INPUT : D = ( x 1 ; x 2 ) ;  X  X  X  ; ( x k ; y k ) OUTPUT : dissemination threshold LOOP binary searc h for x such that : return x FUNCTION : Calculate Loss ( D ) INPUT : D = ( x 1 ; x 2 ) ;  X  X  X  ; ( x k ; y k ) OUTPUT : Loss ( D ) Calculate the MAP estimation  X  MP Calculate the Gaussian appro ximation of P (  X  j D ) Generate K samples using Metrop olis Algorithm Calculate t  X   X  Return Loss ( D ) = Loss ( D; t  X   X  In order to generate random samples from the poste-rior p (  X  j D ) e X cien tly, we apply Laplace's metho d to use a Gaussian distribution N ( w ;  X  MP ; v ) to appro xi-mate the posterior, where  X  MP is the maxim um a pos-teriori estimation of  X  and v is the Hessian matrix of the loglik eliho od log( P ( D j  X  ) p (  X  )) at  X  MP . Then we use this Gaussian appro ximation to generate candi-date values for the Metrop olis-Hastings metho d. We summarize the computational procedure for deter-mining the dissemination threshold in Table 2. A doc-umen t is deliv ered when its score is above the thresh-old. When a documen t is deliv ered, the dissemination threshold is recomputed based on the scores and labels of all of the deliv ered documen ts. The algorithm describ ed in Table 2 was tested exper-imen tally using the metho dology describ ed below. 3.1. Datasets Two text corpora were used in the experimen ts: the OHSUMED dataset used in the TREC-9 Filtering Track, and the Reuters 2001 dataset used in the TREC-10 Filtering Track. As required by the TREC adaptiv e  X ltering track, for each user pro X le, the sys-tem begins with two identi X ed relev ant documen ts and a natural language description of the information need, which is the title and the description  X eld of the corre-sponding topics provided by NIST. The two datasets have rather di X eren t prop erties, as describ ed below. 3.1.1. OHSUMED Data The OHSUMED dataset contains 348,566 medical ab-stracts published from 1987 to 1991 (Hersh et al., 1994). It was used by the TREC-9 Filtering Track (Rob ertson &amp; Hull, 2001). 63 OHSUMED queries were used to simulate user pro X les. The relev ance judg-ments were made by medical librarians and physicians based on the results of interactiv e searc hes. In the TREC-9 Filtering Track it is assumed that the user pro X le descriptions arriv ed at the beginning of 1988, so the 54,709 articles from 1987 can be used to learn word occurrence (e.g., idf) and corpus (e.g., average documen t length) statistics. The average number of relev ant articles per pro X le in the testing data is 51. 3.1.2. Reuters 2001 Data The Reuters 2001 data is a collection of about 810,000 Reuters English News stories from August 20, 1996 to August 19, 1997. It was used by the TREC-10 Filter-ing Tracks (Rob ertson &amp; Soboro X , 2002). In TREC-10, 84 Reuters categories were used to simulate user pro X les. The average number of relev ant articles in the testing data is about 9,795 documen ts per pro X le, which is much larger than in the OHSUMED dataset. However, the  X ltering system only begins with 2 posi-tive and zero negativ e training data, thus this is con-sidered a very di X cult dataset. 3.2. Evaluation Metho dology Utilit y was measured by the macro average of T 9 U = 2  X  R +  X  N + and a normalized version of T 11 SU . 3 The \Normal-Exp onen tial" threshold setting algo-rithm describ ed in (Arampatzis &amp; Hameren, 2001) was used as a baseline. It uses a normal distribu-tion to  X t the relev ant documen ts' scores and an expo-nential distribution to  X t the non-relev ant documen ts. This algorithm was a comp onen t of the most e X ectiv e system tested in the TREC-9  X ltering track (Rob ert-son &amp; Hull, 2001). 4 One problem with using genera-tive models such as the Normal-Exp onen tial model for learning adaptiv e  X ltering thresholds is that although the training data is assumed to be represen tativ e, it is in fact biased because the system only gets rele-vance feedbac k for documen ts it deliv ers (i.e., docu-ments with scores above the dissemination threshold). (Zhang &amp; Callan, 2001) prop osed a Maxim um Likeli-hood Normal-Exp onen tial (ML N-E) algorithm to ex-plicitly comp ensate for this sampling bias. This algo-rithm was used as a second experimen tal baseline. For the Bayesian approac h we also did some experimen ts without activ e learning, which we call \Bayesian Im-mediate Learning". It was used as a third experimen-tal baseline.
 The algorithms cannot learn when the threshold is too high to let any documen ts be deliv ered, so the  X ltering system gradually decreased the threshold in such cases. 3.3. Filtering Environmen t An adaptiv e information  X ltering system developed by us was used for our experimen ts. Documen ts were pro-cessed by remo ving symbols such as punctuation and special characters, excluding stopwords, and stemming terms with the Porter stemmer. Processed documen ts were compared to each pro X le using a varian t of the BM25 tf.idf form ula (Callan, 1996) to measure the sim-ilarit y of the documen t and user pro X le. The Rocchio algorithm was used for updating user pro X les. The Bayesian activ e learner's input was the score that indi-cates the similarit y; its output was a threshold. Docu-ments with similarit y scores above the threshold were deliv ered. Relev ance judgemen ts were provided im-mediately for deliv ered documen ts, which enabled the system to update user pro X les.
 For each topic, the  X ltering system created initial pro- X les using terms from the TREC topic Title and De-scription  X elds. Because the  X rst two relev ant docu-ments were sampled according to P ( x j y = 1) instead of P ( x ), we cannot use it for training the discrimi-nativ e model. A heuristic set the initial threshold to allow the highest-scoring documen ts (top 1%) in the training data to pass. Once the system had at least 1 positiv e and 1 negativ e feedbac k in the testing docu-ment stream, the prop osed algorithm was used to set dissemination thresholds.
 Our algorithm also needs to model P ( x ), the distribu-tion of documen t scores. We used a simple exponen tial model to  X t P ( x ), in our experimen ts. We set the num-where R + is the number of relev ant documen t deliv-ered, N new is the number of expected documen ts in the future, N old is the number of  X ltered documen ts, and Precision 0.463 0.481 0.464 0.496 Docs/Prof 4,527 3,895 2,792 3,380  X  is a constan t that controls the exploration rate.  X  was set arbitrarily to 200 in our experimen ts. This is a conserv ative estimate, and is similar to the discoun ted future rewards used in reinforcemen t learning. Our  X rst experimen t compared the threshold setting algorithms on the TREC-10 Reuters corpus. This is considered a relativ ely di X cult corpus because the ini-tial training data is very limited and not particularly represen tativ e of the variety in the large number of rel-evant documen ts in the test data. Table 3 and Figure 1 summarize the experimen tal results.
 Activ e learning was very e X ectiv e compared to the baseline metho ds. T9U utilit y was higher, T11SU utilit y was slightly higher, Precision and Recall were comparable, and many more documen ts were deliv-ered without hurting utilit y. As expected, the Max-imum Likeliho od Normal-Exp onen tial metho d, which comp ensates for sampling bias, outp erformed the ba-sic Normal-Exp onen tial metho d. Sampling bias is Precision 0.300 0.325 0.256 0.339
Docs/Prof 31 25 46 20 not a problem for discriminativ e models such as the Bayesian activ e and Bayesian immediate metho ds. Bayesian activ e learning outp erformed the other mod-els at all times during the experimen t (Figure 1). When we compared the performance of Bayesian ac-tive and immediate learning on pro X les where activ e learning signi X can tly impro ved performance we found that activ e learning increased both Recall and Preci-sion (e.g., Table 4). This impro vemen t is partly due to the pro X le (term and term weight) learning algorithm, which also bene X ts from the additional training data generated by the activ e learner. Our thresholding al-gorithm did not consider the bene X t of an impro ving pro X le, so it was suboptimal (although e X ectiv e). For simplicit y we have focused only on threshold learning in this paper, however the activ e learning algorithm (Section 2) is not restricted to problems of low di-mensionalit y; a higher dimensionalit y version of the algorithm could also incorp orate pro X le learning. The threshold-setting algorithms were also tested on the TREC-9 dataset, which contains a relativ ely small percen tage of relev ant documen ts (0 : 016%); the test data consists of more than 300,000 documen ts, but only 51 documen ts per pro X le are relev ant, on average. The OHSUMED topic descriptions are well-written, which provides relativ ely accurate initial pro X les. One migh t expect that on this dataset exploitation would be much more importan t than exploration, thus activ e learning migh t be detrimen tal. If the threshold is set too low, the system deliv ers thousands of non-relev ant documen ts, hurting utilit y. In TREC-9 Filtering Track evaluations some participan ts reported negativ e T 9 U utilit y on this dataset (Rob ertson &amp; Hull, 2001). The experimen tal results are summarized in Table 5. As expected, activ e learning did not impro ve utilit y on this dataset. More importan tly, it did not hurt utilit y, either. Consequen tly these results rank in the top 2 when compared with results from the 9 systems that participated in the TREC-9 Adaptiv e Filtering track. 5 Bayesian immediate learning can be viewed as an ac-tive learner that only selects documen ts on the positiv e side of the decision boundary for exploration; Bayesian activ e learning also samples on the negativ e side of the decision boundary . The comparable performance of the Bayesian activ e and Bayesian immediate learners indicates that the activ e learner recognized that the relativ ely good initial pro X les, the limited number of relev ant documen ts in the stream, and the penalt y for deliv ering non-relev ant documen ts collectiv ely made exploring the negativ e side of the decision boundary a poor choice. Activ e learning did not hurt accuracy , even in a test where exploration was a risky choice. There has been much work on activ e learning in the machine learning and IR researc h comm unities (Joac hims, 1998; Lewis &amp; Catlett, 1998; Freund et al., 1997; McCallum &amp; Nigam, 1998). For example, the \Query by Committee" algorithm selects the next query according to the principle of maximal disagree-ment between a committee of classi X ers (Freund et al., 1997). (McCallum &amp; Nigam, 1998) modi X ed the \Query by Committee" algorithm with a Naiv e Bayes model, together with unlab elled data for activ e learn-ing. (Lewis &amp; Catlett, 1998) introduced \uncertain ty sampling" to choose the instance that curren t classi- X ers are most uncertain about. (Tong &amp; Koller, 2000) provided a theoretical framew ork for activ e learning and applied it to Supp ort Vector Machine classi X ers. Unfortunately , most of the prior activ e learning re-searc h focused on interactiv e retriev al tasks, thus it did not address the trade-o X  between exploitation and exploration. It cannot be applied easily to adaptiv e  X l-tering, where the system is evaluated based on utilities such as T 9 U , and the direct cost/rew ard of deliv ering the documen t (exploitation) is as importan t as the im-provemen t in model estimation (exploration). There is also much prior researc h on adaptiv e  X ltering, especially on setting dissemination thresholds. How-ever, as discussed in Section 1, none of the previous researc h addressed the trade-o X  between exploration and exploitation. The common approac h is to consider only exploitation. (Chai et al., 2002) used the infor-mation gain between a documen t and the model to select documen ts, however the task was batch  X ltering instead of adaptiv e  X ltering, and the metho d did not provide a framew ork for combining information gain with the immediate cost/rew ard.
 Prior researc h in related areas in X  X enced our work. The Bayesian framew ork, on which our algorithm is based, was used for activ e learning by (Tong &amp; Koller, 2000) and (McCallum &amp; Nigam, 1998). Our approac h also matc hes the risk minimization model describ ed in (La X ert y &amp; Zhai, 2001). The work describ ed in this paper di X ers from prior work in two major aspects.  X  This researc h uses utility diver gence to measure  X  This researc h considers the direct and indirect To summarize, exploitation and exploration are com-bined in a single, uni X ed framew ork based on utilit y divergence. This paper provides a framew ork, based on Bayesian activ e learning, for modeling the trade-o X  between ex-ploitation and exploration in adaptiv e information  X l-tering. It provides a quan titativ e measure of the im-mediate cost/rew ard and future reward of deliv ering a documen t when the objectiv e is maximizing a user-de X ned utilit y measure. We believe that this is the  X rst work to study the trade-o X  between exploration and exploitation in adaptiv e information  X ltering. The experimen tal results demonstrate that a combi-nation of exploration and exploitation can impro ve re-sults, for example, when the initial pro X les are poor and the documen t stream contains many relev ant doc-umen ts. The results also demonstrate that it can have little e X ect, for example, when the initial pro X les are good and the documen t stream contains few relev ant documen ts. Bayesian activ e learning handles both of these situations well, exploring only when it is useful to do so. When the algorithm does not explore, it is as good as three comp eting metho ds.
 There are several directions for future work. The re-searc h reported here only applied the new framew ork to setting dissemination thresholds, however the basic principle of combining expected immediate utilit y and utilit y divergence gain is also applicable to problems of higher dimensionalit y, for example learning the im-portance of features such as words or phrases. The computational metho ds describ ed in this paper (i.e. appro ximation based on sampling using Metrop olis al-gorithm) are tailored for low dimensionalit y problems. For high dimensionalit y problems, di X eren t metho ds, such as variational metho ds or Gibbs sampling, migh t be necessary in order to get the appro ximation e X -ciently. Our model of P ( x ) and the way we calculate the expected number of future documen ts is also weak; additional researc h in this direction is needed. We thank Tom Mink a for suggestions on this researc h. This work was supp orted in part by NSF grant EIA-9983253. Any opinions,  X ndings, conclusions, or rec-ommendations expressed in this paper are the au-thors', and may not re X  X ct those of the sponsor. Arampatzis, A., &amp; Hameren, A. (2001). The score-distribution threshold optimization for adaptiv e bi-nary classi X cation task. Proceedings of the 24th ACM SIGIR Confer ence .
 Callan, J. (1996). Documen t  X ltering with inference networks. Proc. of the 19th ACM SIGIR Confer ence . Chai, K. M. A., Chieu, H. L., &amp; Ng, H. T. (2002). Bayesian online classi X ers for text classi X cation and  X ltering. Proceedings of 25th Annual International
ACM SIGIR Confer ence on Research and Develop-ment in Information Retrieval . ACM.
 Freund, Y., Seung, H. S., Shamir, E., &amp; Tishby., N. (1997). Selectiv e sampling using the query by com-mittee algorithm. Machine Learning (pp. 133{168). Hersh, W., Buckley, C., J.Leone, T., &amp; Hickam, D. (1994). OHSUMED: An interactiv e retriev al evalu-ation and new large test collection for researc h. Pro-ceedings of the 7th ACM SIGIR Confer ence .
 Joachims, T. (1998). Text categorization with supp ort vector machine. Proceedings of the European Con-ference on Machine Learning . Springer-V erlag. La X ert y, J., &amp; Zhai, C. (2001). Documen t language models, query models, and risk minimization for in-formation retriev al. Proceedings of the 24th ACM SIGIR Confer ence .
 Lewis, D., &amp; Catlett, J. (1998). Heterogeneous uncer-tainty sampling for supervised learning. Proceedings of the Eleventh International Confer ence on Ma-chine Learning . Morgan Kaufmann.
 McCallum, A., &amp; Nigam, K. (1998). Emplo ying EM in pool-based activ e learning for text classi X cation.
Proceeding of the International Confer ence on Ma-chine Learing .
 Robertson, S., &amp; Hull, D. (2001). The TREC-9 Filter-ing track report. The Ninth Text REtrieval Confer-ence (TREC-9) (pp. 25{40). National Institute of
Standards and Technology , special publication 500-249.
 Robertson, S., &amp; Soboro X , I. (2002). The TREC-10 Filtering track  X nal report. Proceeding of the
Tenth Text REtrieval Confer ence (TREC-10) (pp. 26{37). National Institute of Standards and Tech-nology , special publication 500-250.
 Robertson, S., &amp; Walker, S. (2000). Threshold set-ting in adaptiv e  X ltering. Journal of Documentation , 312{331.
 Rocchio, J. J. (1971). Relev ance feedbac k in informa-tion retriev al. The SMAR T Retrieval System{ Ex-periments in Automatic Document Processing (pp. 313{323). Pren tice Hall.
 Tanner, M. A. (1996). Tools for statistic al inference . Springer. 3 edition.
 Tong, S., &amp; Koller, D. (2000). Activ e learning for pa-rameter estimation in bayesian network. Neur al In-formation Processing Systems (NIPS) .
 Zhang, Y., &amp; Callan, J. (2001). Maxim um likeliho od estimation for  X ltering thresholds. Proceedings of
