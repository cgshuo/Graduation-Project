  X  ( x ), w h i ch maps a ny in sta n ce x  X  X (in sta n ce space ) t oon ek nown c l ass l abe l y  X  X  c s i b l et o pr ovi de e no ugh tra inin ge x amp l es .A saresu l t , semi-supervised learning , i. e . learning from labeled and unlabeled examples ,i sstud i ed ,w here only afe wl a -Diff ere n tfr o m learning from positive and negative examples , a no ther spec i a l k in d o f the pr o b l em ,n ame ly, learning from positive and unlabeled examples , ga in s m o re a n dm o re atte n t ion. I tdea l s wi th such a case that no l abe l ed n egat iv e e x amp l es e xi st in the tra inin gset , that i st o sa y, only afe wl abe l ed p o s i t iv e ab o ut n egat iv ec l ass .
 c l asses o fma il s :n e w sa n de n terta in me n t .W eca n bu il dac l ass i fier acc o rd in g a n de n terta in me n tca n be c l ass i fied eas ily a n dc o rrect ly. How e v er ,i fthe n e w p o s i t iv ec l asses .

T h i s paper addresses the pr o b l em o f learning from positive and unlabeled ex-are pr o p o sed in rece n t y ears .T here are ma inly t wo d iff ere n tk in ds as f ollow s :  X  Ano ther k in d o f appr o aches , e . g ., P E BL[ 2 ] a n dR o c -C l u -SVM[3], c on s i der Diff ere n tfr o mf o rmer wo rk , th i spaperpr o p o ses a e n tr o p y-based meth o d -SLE T he c on tr i but ion o fth i spaper i sasf ollow s : 1. W estud y the pr o b l em o fc l ass i f yin gu nl abe l ed data us in gp o s i t iv etra inin g 2 .Ino rder t o d i sc ov er the h i dde nin f o rmat ion in the u nl abe l ed e x amp l es , En-3. A fter c on struct in gthe n e w tra inin gsetb y Entropy , a lo g i st i cregress ion 4 .A ser i es o fe x per i me n ts are c on ducted .W etestthee ff ect iv e n ess on t wo T h i spaper i s o rga ni zed as f ollow s :S ect ion 2prese n ts e xi st in gre l ated research wo rks ;S ect ion 3 in tr o duces the pr o p o sed SLE appr o ach ;Fin a lly, T he e x per i-me n ta l resu l ts are in S ect ion 4 , f ollow ed b y the c on c l us ion in S ect ion 5. T here are f o ur k in ds o f appr o aches f o rth i spr o b l em . 2 .Wi th the u nl abe l ed e x amp l es , c o-tra inin gbased on l abe l ed p o s i t iv e on es 3. Qu i te d iff ere n tfr o mtheab ov e on es ,B-P r [ 7 ] a n d W-SVM[ 8 ] be lon gt o the 4 .L G N[ 4 ]i sbased on the assumpt ion that the i de n t i ca l d i str i but ion o f wo rds Diff ere n tfr o mtheab ov e appr o aches ,w eusethee n tr o p y X  spr o pert y t o re v ea l the a n d n egat iv e in sta n ces . T h i s sect ion in tr o duces the pr o p o sed SLE ( S em i-super vi sed L ear nin gmeth o d us in g E n tr o p y) c l ass i ficat ion meth o dt o s olv e the pr o b l em w here the tra inin g S ect ion 3.1. 3.1 Preliminaries Feature Extraction. Each d o cume n t o b j ect i sm o de l ed as a feature v ect o r wi th hu n dreds o rth o usa n ds o ftermd i me n s ion s .TF-IDF are b o rr ow ed fr o m I R t o measure the w e i gh o feachterm .
 w here t a n d w are wo rd features ; c i i sthe i-th c l ass in tra inin gset ; n t a n d n w are |
D | i sthe n umber o fa ll o b j ects ; | D Over-Sampling. Re -samp lin g i sac o mm only used tech ni que in trad i t ion a l c l as -T here are t wo meth o ds : u n der -samp lin ga n d ov er -samp lin g . Ge n era lly, thr o ugh u n der -samp lin g , the s i ze o fma jo rc l asses decreases t o the sca l e o fm ino r on es ; Opp o s i te ly, ov er -samp lin gmakesthe n umber o fm ino rc l asses  X  e x amp l es appr ox-i mate t o the n umber o fma jo r on es .

B ased on the assumpt ion that m o re e x amp l es i mp li es m o re in f o rmat ion, m o re samp l ed set S o must meet the f ollowin gre l at ion sh i p : S o  X  S .
 Entropy H ( x ) . T he e n tr o p y[9]o f the pr o bab ili t y d i str i but ion P ( x )on the set { x d i in the u nl abe l ed set , a n dthe n the re vi sed f o rmu l a o fe n tr o p yi sdefi n ed as f ollow s : j-th c l ass c j ; | C | i sthe n umber o fk nown o rpredefi n ed c l ass l abe l sappear in g in the tra inin gset .

Fo rtheu nl abe l ed set (l abe l ed as U), the e n tr o p yo fe v er yin sta n ce are ca l cu -l ated , a n dthe in sta n ces a n d amp l es respect iv e ly. 3.2 Problem Description Given that a training set(P) only containing positive examples, from multiple (at least 2) classes, without negative ones, and one unlabeled set(U) containing both positive and negative examples, our task is to construct a classifier( C ), finding all likely negative examples( U n ) hidden in the unlabeled set and it is formulated as follows: input ( P, U ) C  X  output ( U n ) . 3.3 SLE Approach T he deta il ed a l g o r i thm are sh own in Al g o r i thm 1, 2a n d 3.  X  In Al g o r i thm 2 , the n e w tra inin gset ( P + S p ) are used t ol ear n a n e w in g SVM[11], R L G [1 2 ] a n d LMT[13,1 4 ], as the c l ass i fiers in the Algorithm 3 o f SL E appr o ach . In th i s sect ion, w ee v a l uate the perf o rma n ce o f o ur appr o ach .T he e x per i me n ts are c on ducted on In te l 2 .6 G H Z P C wi th 2 G Bo fR AM.

T he o b j ect iv e o fe x per i me n ts are li sted here :  X  Fi rst ly, w estud y the perf o rma n ce o f SL E ov er d iff ere n tdatasets in c l ud in g  X  S ec on d ly, w etestther o bust n ess o f the pr o p o sed meth o df o rthed iff ere n t  X  T h i rd ly, w e v er i f y the i mp o rta n ce o f Entropy t o dea lwi th the e x treme ly Two represe n tat iv erea l be n chmarks are used .T he t wo be n chmark in f o rmat ion i s li sted in T ab l e 1.  X  T he first be n chmark i s2 0N e w sgr o ups 2 ,w here the data c ov ers ma ny d iff ere n t  X  T he sec on dbe n chmark i s U C I rep o s i t o r y 3 .In th i spaper ,w eusethe letter W e i mp l eme n t the pr o p o sed SL Emeth o d in S ect ion 3 a n dthreef o rmer ap -pr o aches :L G N[ 4 ], NB-E [15] a n d on ec l ass -SVM[1]. A sd i scussed in S ect ion 3.3, SL Ec on s i sts o f L-SVM , L-RLG a n d L-LMT respect iv e ly. 1. L-SVM ad o pts the sta n dard SVM[11] as a c l ass i fier in the Al g o r i thm 3o f 3. L-LMT app li es LMT[13,1 4 ] as the bas i cc l ass i fier in the Al g o r i thm 3o f SL E 4.1 Experimental Setting Fo rt wo data c oll ect ion s ,w edefi n ethec l ass i ficat ion tasksasf ollow s .  X  Benchmark 1 . 2 0N e w sgr o ups has appr oxi mate ly 2 0000 d o cume n ts , d i- X  Benchmark 2 .U C IL etter data i sused in the e x per i me n t ,w h i ch c on ta in s 4.2 Experimental Result T he e x per i me n tra n d o m ly ru n ss ix t i mes t o get the a v erage F-sc o re v a l ue as the fi n a l resu l t .In the e x per i me n ts ,  X  i stherat io o ftheu nl abe l ed n egat iv e e x amp l es c o mpared t o the u nl abe l ed set . E . g .  X  =0 . 05 mea n s that the n umber o ftheu nl abe l ed n egat iv ee x amp l es i s only 5% o ftheu nl abe l ed set . Performance for the 2-classes problem T ab l e2rec o rds the F-sc o re v a l ues c o mputed b y1-SVM,L G N, NB-E ,L-SVM, L-R L Ga n d L-LMT on the be n chmark 1, i. e ., 2 0N e w sgr o ups dataset .A ssh own in T ab l e2 , f o reachr ow, L-SVM, L-R L Ga n d L-LMT are better tha no ther c l ass i fier meth o ds ;M ea nw h il e ,NB-E o utperf o rms L G N, but b o th NB-Ea n d L G N are better tha n1-SVM.

F r o mthee x per i me n ta l resu l ts ,w eca n see that :(1)W he n the prec on d i t ion that no tmet ,L G N d o es no tperf o rm w e ll; ( 2 )W he n the n umber a n d pur i t yo ftra inin g (3) Fo rb o th te x tdataa n d no m in a l data ,L-SVM,L-R L Ga n d L-LMT ha v emuch better perf o rma n ce tha no thers appr o aches .

In Fi g . 4 ( C ), there are no ta ny p o s i t iv ea n d n egat iv e wo rds e xi st in g in U C I are n ear ly zer o. C o mpared t o d o cume n tdata ,U C Il etter has only 16 attr i butes , 1-SVM, L-SVM a n d L G N.
 Performance for the 3-classes problem c l asses U C Il etter data .A ssh own in T ab l e 3, L-R L Ga n d L-LMT are better tha no ther c l ass i fier meth o ds ;M ea nw h il e ,F-sc o re v a l ues o f L G N are zer o, but 1-SVM better tha nNB-E .
 wi th in the 2 -c l asses e x per i me n ts .
 Effect for dealing with unbalanced data. Al g o r i thm 1 fi n ds the li ke ly p o s -in the n e x tstep .W e v er i f y th i sp oin t in th i s sect ion.
 Fi g .5(A) rec o rds the resu l ts o fus in g Al g o r i thm 1o r no tus in g i t .T he g o a lo f o ftheu nl abe l ed n egat iv ee x amp l es i s 30% o ftheu nl abe l ed set , the d iff ere n ce t o dea lwi th u n ba l a n ced u nl abe l ed set .Fi g .5(B) sh ow sthes i m il ar resu l ts in 3-c l asses d o cume n te x per i me n ts .

F r o mtheab ov ee x per i me n ts ,w eca n dra w the f ollowin gc on c l us ion. t o the d i str i but ion in the u nl abe l ed set ,o b vio us ly i td o es no t meet the assump -t ion o f L G N, w h i ch i sthema in reas on o f low er F-sc o re o f L G N c l ass i ficat ion appr o ach ; ( 2 )NB-E appr o ach c l ass i fies the u nl abe l ed set base on e n tr o p y ma xi m i zat ion, gr ow sup , the perf o rma n ce appr oxi mates L-R L G . (3) C o mpared t oo ther e xi st in gmeth o ds , the ga in degree o f SL E i smuch l arger f o r2 0n e w sgr o ups tha n f o r l etter data .T here e xi sts on ema in cause : f o r l etter data , the n umber o f attr i butes i s only 16, but 2 0n e w sgr o ups data has hu n dreds o f attr i bute wo rds ,w h i ch v er i f y that much m o re in f o rmat ion will g iv e m o re c on tr i but ion t o the c l ass i ficat ion resu l ts .

In summar y, the pr o p o sed c l ass i ficat ion appr o ach SL E o utperf o rms o ther appr o aches ,in c l ud in g L G N, NB-Ea n d 1-SVM. By ad o pt in gE n tr o p y, ov er -cat ion perf o rma n ce ,w he n the n umber o fp o s i t iv ec l asses in tra inin gset v ar i es . In th i spaper ,w etack l e the pr o b l em o f l ear nin gfr o mp o s i t iv ea n du nl abe l ed e x amp l es a n dprese n ta nov e l appr o ach ca ll ed SLE .Diff ere n tfr o mf o rmer wo rk , e x per i me n ts ,w e v er i f y that the pr o p o sed appr o ach o utperf o rms f o rmer wo rk in the li terature .In the further wo rk ,w e will further stud y the parameter l ear nin g pr o b l em a n ds o me o pt i m i zat ion strateg i es t oi mpr ov e SL E appr o ach . Acknowledgments. T h i s wo rk i s supp o rted b yNSF Cgra n ts (No. 60 77 30 7 5 a n d No. 609 2 500 8 ), N at ion a lHi-T ech 8 63 pr o gram u n der gra n t2 009AA01 Z 1 4 9, 9 7 3 pr o gram (No. 2 010 C B3 28 106), S ha n gha iIn ter n at ion a l C oo perat ion F u n d P r oj ect (P r oj ect No.09530 7 0 84 00) a n d S ha n gha iL ead in g A cadem i c Di sc i p lin e P r oj ect (No. B 4 1 2 ).

