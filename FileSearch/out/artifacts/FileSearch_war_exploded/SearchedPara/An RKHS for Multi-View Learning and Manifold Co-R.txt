 Vikas Sindhwani vsindhw@us.ibm.com David S. Rosenberg drosen@stat.berkeley.edu In semi-supervised learning, we are given a few la-beled examples together with a large collection of un-labeled data from which to estimate an unknown tar-get function. Suppose we have two hypothesis spaces, H 1 and H 2 , each of which contains a predictor that well-approximates the target function. We know that predictors that agree with the target function also agree with each other on unlabeled examples . Thus, any pre-dictor in one hypothesis space that does not have an  X  X greeing predictor X  in the other can be safely elimi-nated from consideration. Due to the resulting reduc-tion in the complexity of the joint learning problem, one can expect improved generalization performance. These conceptual intuitions and their algorithmic in-stantiations together constitute a major line of work in semi-supervised learning. One of the earliest ap-proaches in this area was  X  X o-training X  (Blum &amp; Mitchell, 1998), in which H 1 and H 2 are defined over different representations, or  X  X iews X , of the data, and trained alternately to maximize mutual agree-ment on unlabeled examples. More recently, sev-eral papers have formulated these intuitions as joint complexity regularization, or co-regularization , be-tween H 1 and H 2 which are taken to be Reproducing Kernel Hilbert Spaces (RKHSs) of functions defined on the input space X . Given a few labeled exam-{ x i } i  X  U , co-regularization learns a prediction func-tion, where f 1  X   X  H 1 and f 2  X   X  H 2 are obtained by solving the following optimization problem, In this objective function, the first two terms measure complexity by the RKHS norms k k 2 H in H 1 and H 2 respectively, the third term enforces agreement among predictors on unlabeled examples, and the final term evaluates the empirical loss of the mean function f = ( f 1 + f 2 ) / 2 on the labeled data with respect to a loss function V ( , ). The real-valued parameters  X  1 ,  X  2 , and allow different tradeoffs be-tween the regularization terms. L and U are index sets over labeled and unlabeled examples respectively. Several variants of this formulation have been pro-posed independently and explored in different con-texts: linear logistic regression (Krishnapuram et al., 2005), regularized least squares classification (Sind-hwani et al., 2005b), regression (Brefeld et al., 2006), support vector classification (Farquhar et al., 2005), Bayesian co-training (Yu et al., 2008), and generaliza-tion theory (Rosenberg &amp; Bartlett, 2007). The main theoretical contribution of this paper is the construction of a new  X  X o-regularization RKHS, X  in which standard supervised learning recovers the so-lution to the co-regularization problem of Eqn. 2. Theorem 2.2 presents the RKHS and gives an ex-plicit formula for its reproducing kernel. This  X  X o-regularization kernel X  can be plugged into any stan-dard kernel method giving convenient and immediate access to two-view semi-supervised techniques for a wide variety of learning problems. Utilizing this ker-nel, in Section 3 we give much simpler proofs of the results of (Rosenberg &amp; Bartlett, 2007) concerning bounds on the Rademacher complexity and general-ization performance of co-regularization. As a more algorithmic application, in Section 4 we consider the semi-supervised learning setting where examples live near a low-dimensional manifold embedded in a high dimensional ambient euclidean space. Our approach, manifold co-regularization ( CoMR ), gives major em-pirical improvements over the manifold regularization ( MR ) framework of (Belkin et al., 2006; Sindhwani et al., 2005a).
 The recent work of (Yu et al., 2008) considers a similar reduction. However, this reduction is strictly trans-ductive and does not allow prediction on unseen test examples. By contrast, our formulation is truly semi-supervised and provides a principled out-of-sample ex-tension. We start by reformulating the co-regularization opti-mization problem, given in Eqn. 1 and Eqn. 2, in the following equivalent form where we directly solve for the final prediction function f  X  : f  X  = argmin 2 Consider the sum space of functions,  X  H , given by,  X 
H = H 1  X  H 2 (4) and impose on it a data-dependent norm, The minimization problem in Eqn. 3 can then be posed as standard supervised learning in  X  H as follows, where  X  = 1 2 . Of course, this reformulation is not really useful unless  X  H itself is a valid new RKHS. Let us recall the definition of an RKHS.
 Definition 2.1 (RKHS) . A reproducing kernel Hilbert space (RKHS) is a Hilbert Space F that possesses a reproducing kernel, i.e. , a function k : X  X  X  X  R for which the following hold: (a) k ( x , . )  X  F for all f  X  F , where h , i F denotes inner product in F . In Theorem 2.2, we show that  X  H is indeed an RKHS, and moreover we give an explicit expression for its re-producing kernel. Thus, it follows that although the domain of optimization in Eqn. 6 is nominally a func-tion space, by the Representer Theorem we can express it as a finite-dimensional optimization problem. 2.1. Co-Regularization Kernels Let H 1 , H 2 be RKHSs with kernels given by k 1 , k 2 re-spectively, and let  X  H = H 1  X  H 2 as defined in Eqn. 4. We have the following result.
 Theorem 2.2. There exists an inner product on  X  H for which  X  H is a RKHS with norm defined by Eqn. 5 and reproducing kernel  X  k : X  X  X  X  R given by, where s ( x , z ) is the (scaled) sum of kernels given by, and d x is a vector-valued function that depends on the difference in views measured as, definite matrix given by H = ( I + S )  X  1 . Here, S is the gram matrix of s ( , ) , i.e. , S =  X   X  1 1 K 1 UU +  X   X  1 where K i UU = k i ( U, U ) denotes the Gram matrices of We give a rigorous proof in Appendix A. 2.2. Representer Theorem Theorem 2.2 says that  X  H is a valid RKHS with kernel  X  k . By the Representer Theorem, the solution to Eqn.6 is given by The corresponding components in H 1 , H 2 can also be retrieved as, f 1  X  ( x ) = X Note that H 1 and H 2 are defined on the same do-main X so that taking the mean prediction is meaning-ful. In a two-view problem one may begin by defining H 1 , H 2 on different view spaces X 1 , X 2 respectively. Such a problem can be mapped to our framework by extending H 1 , H 2 to X = X 1  X  X 2 by re-defining f ( x 1 , x 2 ) = f 1 ( x 1 ) , f 1  X  H 1 ; similarly for H we omit these technical details, it is important to note that in such cases, Eqns. 9 and 10 can be reinterpreted as predictors defined on X 1 , X 2 respectively. By eliminating all predictors that do not collectively agree on unlabeled examples, co-regularization intu-itively reduces the complexity of the learning prob-lem. It is reasonable then to expect better test perfor-mance for the same amount of labeled training data. In (Rosenberg &amp; Bartlett, 2007), the size of the co-regularized function class is measured by its empiri-cal Rademacher complexity, and tight upper and lower bounds are given on the Rademacher complexity of the co-regularized hypothesis space. This leads to general-ization bounds in terms of the Rademacher complexity. In this section, we derive these complexity bounds in a few lines using Theorem 2.2 and a well-known result on RKHS balls. Furthermore, we present improved generalization bounds based on the theory of localized Rademacher complexity. 3.1. Rademacher Complexity Bounds Definition 3.1. The empirical Rademacher complex-ity of a function class A = { f : X  X  R} on a sample x , . . . , x  X   X  X is defined as where the expectation is with respect to  X  = {  X  1 , . . . ,  X   X  } , and the  X  i are i.i.d. Rademacher ran-dom variables, that is, P (  X  i = 1) = P (  X  i =  X  1) = 1 Let H be an arbitrary RKHS with kernel k ( , ), and denote the standard RKHS supervised learning objec-Let f  X  = argmin f  X  X  Q ( f ). Then Q ( f  X  )  X  Q (0) = P i  X  L V ( y i , 0) . It follows that k f  X  k if we have some control a priori on Q (0), then we can restrict the search for f  X  to a ball in H of radius r = p Q (0) / X  .
 We now cite a well-known result about the Rademacher complexity of a ball in an RKHS (see e.g. (Boucheron et al., 2005)). Let H r := { f  X  H : || f || H  X  r } denote the ball of radius r in H . Then we have the following: Lemma 3.2. The empirical Rademacher complexity on the sample x 1 , . . . , x  X   X  X for the RKHS ball H r bounded as follows: 1 4  X  where K = k ( x i , x j )  X  For the co-regularization problem described in Eqns. 3 and 6, we have f  X   X   X  H r where r 2 =  X  sup y V (0 , y ), where  X  is number of labeled examples. We now state and prove bounds on the Rademacher complexity of  X  H r . The bounds here are exactly the same as those given in (Rosenberg &amp; Bartlett, 2007). However, while they have a lengthy  X  X are-hands X  ap-proach, here we get the result as a simple corollary of Theorem 2.2 and Lemma 3.2.
 Theorem 3.3. The empirical Rademacher complexity on the labeled sample x 1 , . . . , x  X   X  X for the RKHS ball  X  H r is bounded as follows: where  X  D Proof. Note that  X  K is just the kernel matrix for the co-regularization kernel  X  k ( , ) on the labeled data. Then the bound follows immediately from Lemma 3.2. 3.2. Co-Regularization Reduces Complexity The co-regularization parameter controls the extent to which we enforce agreement between f 1 and f 2 . Let  X  H ( ) denote the co-regularization RKHS for a partic-ular value of . From Theorem 3.3, we see that the Rademacher complexity for a ball of radius r in  X  H ( ) decreases with by an amount determined by where  X  ( , ) is a metric on R | U | defined by  X  ( s , t ) = (  X   X  1 1 s  X   X   X  1 2 t )  X  ( I + S )  X  1 (  X  We see that the complexity reduction,  X ( ), grows with the  X  -distance between the two different repre-sentations of the labeled points. Note that the metric  X  is determined by S , which is the weighted sum of the gram matrices of the two kernels on unlabeled data. 3.3. Generalization Bounds With Theorem 2.2 allowing us to express multi-view co-regularization problems as supervised learning in a data-dependent RKHS, we can now bring a large body of theory to bear on the generalization performance of co-regularization methods. We start by quoting the theorem proved in (Rosenberg &amp; Bartlett, 2007). Next, we state an improved bound based on localized Rademacher complexity. Below, we denote the unit ball in  X  H by  X  H 1 .
 Condition 1. The loss V ( , ) is Lipschitz in its first argument, i.e., there exists a constant A such that Theorem 3.4. Suppose V : Y 2  X  [0 , 1] satisfies Con-dition 1. Then conditioned on the unlabeled data, for any  X   X  (0 , 1) , with probability at least 1  X   X  over the sample of labeled points ( x 1 , y 1 ) , . . . , ( x  X  i.i.d. from P , we have for any predictor f  X   X  H 1 that We need two more conditions for the localized bound: Condition 2. For any probability distribution P , Condition 3. There is a constant B  X  1 such that for every probability distribution P and every f  X   X  H 1 we have, P ( f  X  f  X  ) 2  X  BP ( V [ f ( x ) , y ]  X  V [ f In the following theorem, let P  X  denote the empirical probability measure for the labeled sample of size  X  . Theorem 3.5. [Cor. 6.7 from (Bartlett et al., 2002)] Assume that sup x  X  X  k ( x , x )  X  1 and that V satisfies the 3 conditions above. Let  X  f be any element of  X  H 1 satisfying P  X  V [  X  f ( x ) , y ] = inf f  X   X  H exist a constant c depending only on A and B s.t. with probability at least 1  X  6 e  X   X  ,  X  , . . . ,  X   X  are the eigenvalues of the labeled-data kernel matrix  X  K LL in decreasing order.
 Note that while Theorem 3.4 bounds the gap between expected and empirical performance of an arbitrary f  X   X  H 1 , Theorem 3.5 bounds the gap between the empirical loss minimizer over  X  H 1 and true risk min-imizer in  X  H 1 . Since the localized bound only needs to account for the capacity of the function class in the neighborhood of f  X  , the bounds are potentially tighter. Indeed, while the bound in Theorem 3.4 is in terms of the trace of the kernel matrix, the bound in Theo-rem 3.5 involves the tail sum of kernel eigenvalues. If the eigenvalues decay very quickly, the latter is poten-tially much smaller. Consider the picture shown in Figure 1(a) where there are two classes of data points in the plane ( R 2 ) lying on one of two concentric circles. The large, colored points are labeled while the smaller, black points are unlabeled. The picture immediately suggests two no-tions of distance that are very natural but radically different. For example, the two labeled points are close in the ambient euclidean distance on R 2 , but infinitely apart in terms of intrinsic geodesic distance measured along the circles.
 Suppose for this picture one had access to two kernel functions, k 1 , k 2 that assign high similarity to nearby points according to euclidean and geodesic distance respectively. Because of the difference in ambient and intrinsic representations, by co-regularizing the asso-ciated RKHSs one can hope to get good reductions in complexity, as suggested in section 3.2. In Figure 1, we report the value of complexity reduction (Eqn. 12) for four point clouds generated at increasing levels of noise off the two concentric circles. When noise be-comes large, the ambient and intrinsic notions of dis-tance converge and the amount of complexity reduc-tion decreases.
 The setting where data lies on a low-dimensional submanifold M embedded in a higher dimensional ambient space X , as in the concentric circles case above, has attracted considerable research interest re-cently, almost orthogonal to multi-view efforts. The main assumption underlying manifold-motivated semi-supervised learning algorithms is the following: two points that are close with respect to geodesic distances on M should have similar labels. Such an assump-tion may be enforced by an intrinsic regularizer that emphasizes complexity along the manifold.
 Since M is truly unknown, the intrinsic regularizer is empirically estimated from the point cloud of labeled and unlabeled data. In the graph transduction ap-proach, an nn -nearest neighbor graph G is constructed which serves as an empirical substitute for M . The vertices V of this graph are the set of labeled and un-labeled examples. Let H I denote the space of all func-tions mapping V to R , where the subscript I implies  X  X ntrinsic. X  Any function f  X  H I can be identified with the vector f = [ f ( x i ) , x i  X  V ] T . One can impose a norm k f k 2 I = P ij W ij [ f ( x i )  X  f ( x j )] 2 on H provides a natural measure of smoothness for f with respect to the graph. Here, W denotes the adjacency matrix of the graph. When X is a euclidean space, a j are nearest neighbors and 0 otherwise. In practice, one may use a problem dependent similarity matrix to set these edge weights. This norm can be conveniently written as a quadratic form f T M f , where M is the graph Laplacian matrix defined as M = D  X  W , and D is a diagonal degree matrix with entrees D ii = P j W ij . It turns out that H I with the norm k k I is an RKHS whose reproducing kernel k I : V  X  V  X  R is given by k I ( x i , x j ) = M  X  ij , where M  X  de-notes the pseudo-inverse of the Laplacian. Given H
I with its reproducing kernel, graph transduction solves the standard RKHS regularization problem, f y is the label associated with the node x i . Note that the solution f  X  is only defined over V , the set of la-beled and unlabeled examples. Since graph transduc-tion does not provide a function whose domain is the ambient space X , it is not clear how to make predic-tions on unseen test points x  X  X . Possessing a prin-cipled  X  X ut-of-sample extension X  distinguishes semi-supervised methods from transductive procedures. 4.1. Ambient and Intrinsic Co-Regularization We propose a co-regularization solution for out-of-sample prediction. Conceptually, one may interpret the manifold setting as a multi-view problem where each labeled or unlabeled example appears in two  X  X iews X : (a) an ambient view in X in terms of eu-clidean co-ordinates x and (b) an intrinsic view in G as a vertex index i . Let H A : X  X  X  X  R be an RKHS defined over the ambient domain with an asso-ciated kernel k A : X  X  X  X  R . We can now combine ambient and intrinsic views by co-regularizing H A , H I . This can be done by setting k 1 = k A and k 2 = k I in Eqn. 7 and solving Eqn. 6. The combined prediction function f  X  given by Eqn. 8 is the mean of an ambient component f 1  X  given by Eqn. 9 and an intrinsic compo-nent f 2  X  given by Eqn. 10. Even though f  X  is transduc-tive and only defined on labeled and unlabeled exam-ples, the ambient component f 1  X  can be used for out-of-sample prediction. Due to co-regularization, this ambient component is (a) smooth in H X and (b) in agreement with a smooth function on the data mani-fold. We call this approach manifold co-regularization, and abbreviate it as CoMR . 4.2. Manifold Regularization In the manifold regularization ( MR ) approach of (Belkin et al., 2006; Sindhwani et al., 2005a), the following optimization problem is solved: where f = [ f ( x i ) , i  X  L, U ] T . The solution, f  X  is defined on X , and can therefore be used for out-of-sample prediction.
 In Figure 2, we show a simple two-dimensional dataset where MR provably fails when H A is the space of lin-ear functions on R 2 . The LINES dataset consists of two classes spread along perpendicular lines. In MR the intrinsic regularizer is enforced directly on H A . It can be easily shown that the intrinsic norm of a linear function f ( x ) = w T x along the perpendicular lines is exactly the same as the ambient norm, i.e. , k f k 2 H nores unlabeled data and reduces to supervised train-ing with the regularization parameter  X  1 +  X  2 . The linear function that gives maximally smooth pre-dictions on one line also gives the maximally non-smooth predictions on the other line. One way to remedy this restrictive situation is to introduce slack  X  ( f +  X  ) T M ( f +  X  ) + k  X  k 2 + P i  X  L V ( y i , f ( x Re-parameterizing g = f +  X  , we can re-write the  X  k g k 2 H be viewed as a variant of the co-regularization prob-lem in Eqn. 2 where empirical loss is measured for f alone. Thus, this motivates the view that CoMR adds extra slack variables in the MR objective function to better fit the intrinsic regularizer. Figure 2 shows that CoMR achieves better separation between classes on the LINES dataset.
 4.3. Experiments In this section, we compare MR and CoMR. Similar to our construction of the co-regularization kernel, (Sind-hwani et al., 2005a) provide a data-dependent kernel that reduces manifold regularization to standard su-pervised learning in an associated RKHS. We write the manifold regularization kernel in the following form, where we have,  X  s =  X   X  1 1 k 1 ( x , z ) ,  X  d x =  X   X  1  X  trix of k 1 = k A over labeled and unlabeled examples, the kernel can be easily compared with corresponding quantities in the co-regularization kernel Eqn. 7. In this section we empirically compare this kernel with the co-regularization kernel of Eqn. 7 for exactly the same choice of k 1 , k 2 . Semi-supervised classification experiments were performed on 5 datasets described in table 1.
 The LINES dataset is a variant of the two-dimensional problem shown in Figure 2 where we added random noise around the two perpendicular lines. The G50C , USPST , COIL20 , and PCMAC datasets are well known and have frequently been used for empirical studies in semi-supervised learning literature. They were used for benchmarking manifold regularization in (Sindhwani et al., 2005a) against a number of com-peting methods. g50c is an artificial dataset gener-ated from two unit covariance normal distributions with equal probabilities. The class means are adjusted so that the Bayes error is 5%. COIL20 consists of 32  X  32 gray scale images of 20 objects viewed from varying angles. USPST is taken from the test subset of the USPS dataset of images containing 10 classes of handwritten digits. PCMAC is used to setup bi-nary text categorization problems drawn from the 20-newsgroups dataset.
 For each of the 5 datasets, we constructed random splits into labeled, unlabeled, test and validation sets. The sizes of these sets are given in table 1. For all datasets except LINES, we used Gaussian am-sic graph kernel whose gram matrix is of the form K 2 = ( M p + 10  X  6 I )  X  1 . Here, M is the normalized Graph Laplacian constructed using nn nearest neigh-bors and p is an integer. These parameters are tabu-lated in Table 4 for reproducibility. For more details on these parameters see (Sindhwani et al., 2005a). We chose squared loss for V ( , ). Manifold regulariza-tion with this choice is also referred to as Laplacian RLS and empirically performs as well as Laplacian SVMs. For multi-class problems, we used the one-versus-rest strategy.  X  1 ,  X  2 were varied on a grid of spect to validation error. The chosen parameters are also reported in Table 4. Finally, we evaluated the MR solution and the ambient component of CoMR on an unseen test set. In Tables 2 and 3 we report the mean and standard deviation of error rates on test and unla-beled examples with respect to 10 random splits. We performed a paired t-test at 5% significance level to as-sess the statistical significance of the results. Results shown in bold are statistically significant.
 Our experimental protocol makes MR and CoMR ex-actly comparable. We find that CoMR gives major empirical improvements over MR on all datasets ex-cept G50C where both methods approach the Bayes error rate. In this paper, we have constructed a single, new RKHS in which standard supervised algorithms are immedi-ately turned into multi-view semi-supervised learners. This construction brings about significant theoretical simplifications and algorithmic benefits, which we have demonstrated in the context of generalization analysis and manifold regularization respectively.
 Bartlett, P. L., Bousquet, O., &amp; Mendelson, S. (2002).
Localized rademacher complexities. COLT 02 (pp. 44 X 58). Springer-Verlag.
 Belkin, M., Niyogi, P., &amp; Sindhwani, V. (2006).
Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLR , 7 , 2399 X 2434.
 Bertinet, A., &amp; Thomas-Agnan, C. (2004). Reproduc-ing kernel hilbert spaces in probability and statistics . Kluwer Academic Publishers.
 Blum, A., &amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. COLT .
 Boucheron, S., Bousquet, O., &amp; Lugosi, G. (2005).
Theory of classification: a survey of some recent ad-vances. ESAIM: P&amp;S , 9 , 323 X 375.
 Brefeld, U., G  X artner, T., Scheffer, T., &amp; Wrobel, S. (2006). Efficient co-regularised least squares regres-sion. ICML (pp. 137 X 144).
 Farquhar, J. D. R., Hardoon, D. R., Meng, H., Taylor, J. S., &amp; Szedm  X ak, S. (2005). Two view learning: SVM-2K, theory and practice. NIPS .
 Krishnapuram, B., Williams, D., Xue, Y., &amp;
A. Hartemink, L. C. (2005). On semi-supervised classification. NIPS .
 Rosenberg, D., &amp; Bartlett, P. L. (2007). The
Rademacher complexity of co-regularized kernel classes. AISTATS .
 Sindhwani, V., Niyogi, P., &amp; Belkin, M. (2005a). Be-yond the point cloud: From transductive to semi-supervised learning. ICML .
 Sindhwani, V., Niyogi, P., &amp; Belkin, M. (2005b). A co-regularization approach to semi-supervised learning with multiple views. ICML Workshop on Learning in Multiple Views (pp. 824 X 831).
 Yu, S., Krishnapuram, B., Rosales, R., Steck, H., &amp; Rao, R. (2008). Bayesian co-training. NIPS .
 This theorem generalizes Theorem 5 in (Bertinet &amp; Thomas-Agnan, 2004).
 The Product Hilbert Space We begin by intro-ducing the product space, and an inner product on F defined by, ( f 1 , f 2 ) , ( g 1 , g 2 ) It X  X  straightforward to check that h , i F is a valid inner product. Moreover, we have the following: Lemma A.1. F is a Hilbert space.
 Proof. (Sketch.) We need to show that F is complete. Let ( f 1 n , f 2 n ) be a Cauchy sequence in F . Then f is Cauchy in H 1 and f 2 n is Cauchy in H 2 . By the completeness of H 1 and H 2 , we have f 1 n  X  f 1 in H 1 and f 2 n  X  f 2 in H 2 , for some ( f 1 , f 2 )  X  F . Since H 1 and H 2 are RKHSs, convergence in norm implies pointwise convergence, and thus the co-regularization part of the distance also goes to zero.  X  H is a Hilbert Space Recall the definition of  X  H in Eqn. 4. Define the map u : F  X   X  H by u ( f 1 , f 2 ) = subspace of F , and thus its orthogonal complement N  X  is also a closed subspace. We can consider N  X  as a Hilbert space with the inner product that is the natural restriction of h , i F to N  X  . Define v : N  X   X   X  H as the restriction of u to N  X  . Then v is a bijection, and we define an inner product on  X  H by We conclude that  X  H is a Hilbert space isomorphic to N The Co-Regularization Norm Fix any f  X   X  H , and note that u  X  1 ( f ) = v  X  1 ( f ) + n | n  X  N . Since v  X  1 ( f ) and N are orthogonal, it X  X  clear by the Pythagorean theorem that v  X  1 ( f ) is the element of u  X  1 ( f ) with minimum norm. Thus We see that the inner product on  X  H induces the norm claimed in the theorem statement.
 We next check the two conditions for validity of an RKHS (see Definition 2.1). (a)  X  k ( z,  X  )  X   X  H  X  z  X  X Recall from Eqn. 7 that the co-regularization kernel is defined as Define h 1 ( x ) =  X   X  1 1 k 1 ( x , z )  X   X   X  1 1 k 1 U x h ( x ) =  X   X  1 2 k 2 ( x , z ) +  X   X  1 2 k 2 ( x , U )  X  z h and similarly, h 2  X  H 2 . It X  X  clear that  X  k ( z , ) = h 1 ( ) + h 2 ( ) , and thus  X  k ( z , )  X   X  H . (b) Reproducing Property For convenience, we collect some basic properties of h 1 and h 2 in the fol-lowing lemma: Lemma A.2 (Properties of h 1 and h 2 ) . Writing h ( U ) for the column vector with entries h 1 ( x i )  X  i  X  U , and similarly for other functions on X , we have the following: h 1 ( U )  X  h 2 ( U ) =  X  z (19) Proof. The first two equations follow from the defini-tions of h 1 and h 2 and the reproducing kernel property. The last equation is derived as follows: h 1 ( U )  X  h 2 ( U ) =  X   X  1 1 k 1 ( U, z )  X   X   X  1 1 k 1 where the last line follows from the Sherman-Morrison-Woodbury inversion formula.
 Since  X  k ( z , ) = h 1 ( ) + h 2 ( ), it is clear that ( h v  X  1  X  k ( z , ) + n , for some n  X  N . Since v  X  1 ( f )  X  N we have D f,  X  k ( z , ) E
