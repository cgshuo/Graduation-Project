 Sonia Bergamaschi  X  Riccardo Martoglia  X  Serena Sorrentino Abstract SoftwaredevelopmentisstillconsideredabottleneckforSmallandMediumEnter-prises (SMEs) in the advance of the Information Society. Usually, SMEs store and collect a large number of software textual documentation; these documents might be profitably used to facilitate them in using (and re-using) Software Engineering methods for systematically designing their applications, thus reducing software development cost. Specific and seman-tics textual filtering/search mechanisms, supporting the identification of adequate processes and practices for the enterprise needs, are fundamental in this context. To this aim, we present an automatic document retrieval method based on semantic similarity and Word Sense Dis-ambiguation techniques. The proposal leverages on the strengths of both classic information retrieval and knowledge-based techniques, exploiting syntactical and semantic information provided by general and specific domain knowledge sources. For any SME, it is as easily and generally applicable as are the search techniques offered by common enterprise Con-tent Management Systems. Our method was developed within the FACIT-SME European FP-7 project, whose aim is to facilitate the diffusion of Software Engineering methods and best practices among SMEs. As shown by a detailed experimental evaluation, the achieved effectiveness goes well beyond typical retrieval solutions.
 Keywords Knowledge management  X  Text retrieval  X  Semantic knowledge  X  Semantic similarity  X  Word sense disambiguation  X  Software engineering 1 Introduction and motivations One of the main bottlenecks for the development of the Information Society [ 1 ] has been soft-ware development, as the quality and productivity of work has not been able to keep up with the society software needs [ 10 , 44 ]. According to the analysis performed by the INNOSme project [ 22 ] across several countries, these issues are especially critical for software Small and Medium Enterprises (SMEs): The available resources cannot be devoted to new technol-ogy training as they are absorbed in the activity of software production. Thus, the integration between software and knowledge engineering has become unavoidable in order to reduce time and cost for software development and to increase software quality [ 7 , 20 ].
Usually, SMEs store and collect a large number of software textual documentation: Indeed, text is everywhere and even test cases, and inline comments could be useful knowledge sources [ 25 ]. This textual information might be profitably used to facilitate them in using (and re-using) Software Engineering methods for developing their applications; however, their inadequate information systems often prevent them from doing so [ 15 ]. To this aim, specific document filtering/search mechanisms based on textual similarity techniques are fundamental.

In literature, several methods for document filtering/searching in a software development context have been proposed [ 20 ]; however, a large number of fundamental challenges still need to be faced. Indeed, the great majority of these approaches is based on syntactic informa-tion retrieval techniques [ 18 , 38 ]. Such approaches have found a wide application in the more general purpose content management systems (CMSs), which are commercially available and can be easily adopted and exploited by SMEs [ 47 ].

Standard syntactical search techniques [ 2 ] 1 often suffer of low effectiveness as they are inadequate to capture the similarity between documents and disregard the semantic connec-tions ( synonyms or semantic relations ) of the terms composing them. For instance, let us consider the piece of document  X  ... clients for your activity ...  X  and the fragment of query  X  ... product requirements specified by the customer ...  X  (e.g., from the Methodology sce-nario). In a syntactical search approach, no match would be found between the query and the document, as they have no words in common. On the contrary, by adding semantics, i.e., synonyms and related terms (i.e., broader, narrower or correlated terms), it is possible to discover that  X  X ustomer X  is a synonym of  X  X lient X  and, thus, that this document may be relevant for the query.

Other approaches make also use of semantic methods based on dictionary or thesauri (e.g.,WordNet) 2 by considering synonyms and related terms [ 40 , 43 ]. However, they do not consider the term ambiguity problem which may affect the effectiveness of the method: A term may have more than one possible meaning (e.g.,  X  X lient X  means  X  X omeone who pay for goods or services X  when used in a Business context, while it means  X  X ny computer that is hooked up to a computer network X  when used in a Computer Science context). For instance, let us consider the piece of document  X  Clients and servers exchange messages in a request-response messaging pattern ...  X . As regards the previous query, this document is potentially relevant, since it contains  X  X lient X  which is, in its Business sense, a synonym of  X  X ustomer X . However, in this piece of document,  X  X lient X  is used in a Computer Science context, and thus, it is not applicable to the query.
Finally, other more complex approaches require the knowledge of technical languages (such as SPARQL) to be used [ 19 , 23 , 26 , 42 , 48 ] and thus, they are not suitable for non-skilled users in SMEs.

Startingfromtheseconsiderations,inthispaper,weproposeafullyautomaticandsemantic approach for filtering/searching software documentation, carefully considering the actual user-targets. The proposed solution significantly extends the preliminary work presented in [ 30 ], and building on some of the initial ideas anticipated in the short communication [ 6 ], it aims to: 1. be easily and generally applicable/maintainable by IT SMEs: It allows users to look for 2. effectively and automatically identify the similarities between such queries and a refer-3. be flexible so to become a basis of many high-level functionalities, i.e., filtering software
Our method has been studied and evaluated in the context of the European FP7 3-year project  X  X acilitate IT-providing SMEs by Operation-related Models and Methods X  [ 13 ]and is implemented in the Semantic Helper component of the FACIT-SME solution.

In the rest of the paper, Sect. 2 gives an overview of the overall FACIT-SME scenario and of the proposed semantic helper. In Sects. 3 and 4 , we focus on the analysis and semantic techniques on which it is based. Section 5 analyzes the related work, while the detailed experimental evaluation presented in Sect. 6 shows the achieved effectiveness results, going beyond typical retrieval solutions. Finally, Sect. 7 concludes the work. 2 FACIT-SME solution and semantic helper overview The target of the FACIT-SME project is to promote the use of SE methods within IT SMEs, for designing and developing their applications integrated with the business processes in a more systematic way. Another goal is to provide efficient and affordable certification of these processes according to internationally accepted standards and to securely share best practices, tools and experiences with development partners and customers. The achieve-ments FACIT-SME are the following: a novel Open Reference Model (ORM) [ 34 ]forICT SMEs serving as an underlying knowledge Backbone; on top of the ORM, a customizable Open Source Enactment System (OSES) [ 35 ] as IT support for the project-specific appli-cation of the ORM. More specifically, the ORM stores existing reference knowledge for software-developing SMEs, including different engineering methods, tools, quality model requirements, and enterprise model fragments of IT SMEs in a computer-processable form. On top of the ORM repository, specific search mechanisms, representing a key part of the OSES, support the identification of adequate processes and data structures for a specific enterprise. The inputs of the search mechanism are the company and project information and/or the existing methodology descriptions. Then, through a filtering phase, the organi-zation receives a set of suggestions in the form of the most relevant/useful elements and models in the ORM. Besides five R&amp;D partners providing the required competencies, the project consortium also includes five SMEs operating in the IT domain which will evaluate the results in daily-life applications.

The Semantic Helper is the FACIT-SME software component which implements our method. The goal of the Semantic Helper is to filter and search the relevant information available in the ORM, in two possible scenarios:  X   X  X rom scratch X  scenario assisted filtering/selection of ORM elements given specific enter-prise objectives, e.g., to give pointers to useful information for helping the company in achieving its certification objectives;  X   X  X rom methodology X  scenario assisted suggestion proposal for a given enterprise method-ology,e.g.,tohelpidentifyingrelevantinformationorgapsbetweenthegivenmethodology and the ORM methodologies.

To this end, a representation of the key parts of the ORM in a semantic and machine-processable way is needed. The semantic helper supports two main processes to deal with textual information (see Fig. 1 ): 1. Semantic Glossary Creation this is an off-line process where the Semantic Helper auto-2. Document Selection it is an online process where user queries are processed and, con-The keyword extraction and enhancement phase, involved in both processes, is detailed in Sect. 3.1 ; in Sect. 3.2 , the structure of the semantic glossary produced in the off-line process is described, while the semantic similarity computation phase, involved in the online process, is detailed in Sect. 3.3 . 3 Semantic helper techniques 3.1 Keyword extraction and enhancement Our goal for keyword extraction and enhancement has been to design and develop an effective and easy-to-apply technique for automatically analyzing text and extracting terms, together with their associated semantics and statistics. In particular, we wanted to devise a flexible technique to be exploited both for  X  X ff-line X  analysis (thus working on the textual descriptions already available in the ORM) and for  X  X n-line X  querying operations, i.e., applied on the fly to the submitted query documents. The keyword extraction and enhancement phase is composed by the following steps: 1. Tokenization terms are identified and punctuation is removed; 2. Stemming the tokens are X  X ormalized X  and X  X temmed X , i.e., terms are reduced to their base 3. POS (Part of Speech) Tagging the tokens are X  X agged X  with Part of Speech tags (i.e., 4. Composite term identification possible composite terms (such as X  X roduct action plan X  or 5. Filtering and enhancement by exploiting external knowledge sources, the most relevant 6. Term statistics and weight computation weights are computed for each term, reflecting
Even if our techniques are able to extract terms belonging to different parts of speech, we limit the extraction to nouns, as most of the semantics of a sentence is usually carried by noun terms [ 33 ]. 3.2 The semantic glossary By applying batch keyword extraction and enhancement to the documents currently available intheORM,weachievedafirstsignificantresultintheFACIT-SMEproject,i.e.,theautomatic generation of the Semantic Glossary . This first draft can be automatically updated/enriched whenever new content is added to the ORM, while more fine-grained user interventions for adding/modifying/eliminating information are also possible.
The Semantic Glossary stores all the terms in all the documents with their statistics ( global view ) and the terms occurrences in each document with their statistics ( per-document view ). The glossary global view is an alphabetical sort of all the extracted terms, in a tabular form. Table 1 shows an excerpt of it. The format is:
Note that, with  X  X ossible X  synonyms and definitions, we mean the collection of synonyms and definitions available in the knowledge sources for the different meanings associated with the terms. The glossary per-document view is a list of all the term occurrences in the documents, sorted by the document ID, together with their statistics (see an excerpt in Table 2 ). For each term in each document, the view contains:
As we will see in the next section, the content of the glossary allows the similarity func-tions of the Semantic Helper to draw useful knowledge from both the semantic and the text retrieval research areas. Moreover, by exploiting the weight TF*IDF 4 [ 39 ] in the similarity computation, common terms, which are probably less meaningful, will give a lower contribute to the final similarity (since they will have a low weight).

The similarity techniques described in the next section are designed to work automatically without further intervention (and, as proved in Sect. 6 , provide encouraging results). Never-theless, the list of synonyms and definitions retrieved from the external knowledge sources can be automatically refined by means of WSD techniques (as we will deeply investigate in Sect. 4 ) in order to maximize retrieval effectiveness. 3.3 Semantic similarity computation As anticipated in the past sections, the need of effectively and efficiently computing simi-larities between documents is crucial in contexts like the one of the projects. With this goal in mind, we aim to define a document similarity formula DSim ( D x , D y ) :foragivena expresses the similarity of the source document w.r.t. the target document. In particular, the computation of DSim between a given D x (e.g., a given quality requirement description) and each possible submitted D y (i.e., each available software methodology description of the ORM) involves the following steps: 1. considering each term in D x and finding the most similar term or terms available in D y 2. inducing a ranking of the available documents (on the basis of DSim ), thus predicting Let us now discuss in detail the proposed formulas for DSim and TSim . The document similarity formula between a given source document D x and a target document D y is shown in Eq. ( 1 ): the similarity is given by the sum (defined in (2)) of all term similarities between each term in D x and each term in D y maximizing the term similarity with the term in D x : where w x i = tf x i  X  idf i and w y the final similarity with a different weight, i.e., more frequent and more significant terms contribute more to the similarity between the two documents. 5 TSim is computed by means of Eq. ( 3 ) which considers synonyms (thus, implicitly equal terms) and semantically related terms :
Note that the case of maximum similarity (i.e., value 1) holds when the two terms are synonyms ( SY N relation). Moreover, when the two terms are in some way related from a semantic point of view (i.e., broader/narrower terms etc.), the formula provides a similarity value of r ,where0 &lt; r &lt; 1 is a user-defined fixed similarity value.

Besides synonym information, we consider two different ways to determine whether two terms are related. Equation ( 5 ) shows a possible way of computing the similarity by exploiting the glosses (definitions) of the terms:
Two terms are in relation REL , thus semantically related, if their gloss similarity, GSim exceeds a given threshold Th . The Literature presents many possible ways of computing similarities between glosses [ 33 ]. We decided to use the extended gloss overlap measure [ 3 ] shown in ( 7 ), being it one of the most popular and effective. It quantifies the similarity between the two glosses by finding overlaps in them (the similarity is the sum of the squares of the overlap lengths). If one or both terms are associated with more than one gloss, the formula returns the similarity between the two closest glosses.

Exploiting the relations between terms coming from the WordNet thesaurus is another possibility to compute semantic relatedness. Indeed, in WordNet terms are associated with one or more different meanings (or senses), and each term is then is connected to other terms meanings by hypernym (i.e.,  X  X s-a X ) relations 6 [ 31 ]. We adopt one of the most widely used methods in knowledge management, relying on the hypernym relations:
In our case, two terms are semantically related if their hypernym similarity HSim exceeds a given threshold Th . In particular, the HSim shown in ( 7 ) derives from the works [ 24 , 27 ] and computes a score which is inversely proportional to the length of the shortest path connecting the (senses of the) two terms. H is a constant representing the maximum depth of the hypernym tree, which for WordNet is defined as 16. On the other hand, the similarity is 0 if the two terms are not connected in the WordNet hypernym structure.

Let us now see how the keyword extraction/enhancement and semantic techniques, we just presented, can be used in the context of a small illustrative example.
 Example 1 Let us suppose that D 1 is a fragment of a document available in the ORM repository: Given the following queries: D 1 has no terms in common with both Q 1and Q 2. However, by using the Semantic Helper techniques, we can easily determine that D 1 might contain information potentially relevant to Q 1, as  X  X ustomer X  is a synonym of  X  X lient X . Further, by analyzing the semantic similarity of the terms in Q 2 w.r.t. those already available in the Semantic Glossary, the term  X  X rgani-zation X  can easily be found as strictly related to  X  X nterprise X  by means of formulas ( 5 )or( 7 ). Therefore, D 1 will also be presented as a possible  X  X uggestion X  for Q 2, even if, typically, with a lower score (related terms usually contribute to weaker scores than synonyms or equal terms). 4 Beyond term ambiguities: sense-aware techniques Human Language is intrinsically ambiguous, and terms may be polysemous , i.e., they may have different senses (or meanings) on the basis of the context where they are used. For example, as we have previously seen in Sect. 1 , the term  X  X lient X  may be employed in a Business context with the meaning of  X  X ustomer X  or in a Computer Science context with the meaning of  X  X omputer X . Therefore, when user queries and/or documents contain poly-semous terms, the similarity techniques described so far may not be sufficient to compute document similarity, and several non-relevant documents might be returned. Let us consider two motivating examples.
 Example 2 Starting from the scenario illustrated in Example 1 , let us suppose that the ORM contains also document D 2: As regards the query Q 1, both documents are potentially relevant, since they both contain the term  X  X lient X  which is, in its Business sense, a synonym of  X  X ustomer X . However, while in D 1,  X  X lient X  is used in its Business sense, in D 2, it is used in a Computer Science context: In this case, only D 1 is pertaining to the user query, while D 2 should be discarded. Example 3 Let us now consider the following additional query: Thisqueryhasnotermsincommonwiththedocuments.However,byanalyzingtheirsemantic by means of formula ( 7 ), as  X  X omputer X  is a hypernym of  X  X lient X  in WordNet. However, this is true only when  X  X lient X  is used in a Computer Science context. As a consequence, only document D 2 is relevant to the query.
 To address term ambiguity problems in the FACIT-SME context, the idea is to exploit Word Sense Disambiguation (WSD), i.e., a Natural Language Processing technique for automati-cally (or semi-automatically) identifying the sense of a term in a context [ 33 ]. Indeed, term senses represent strategic information in order to avoid such  X  X itfalls X  as the ones illustrated in the above examples. Specifically, by using WSD, we can improve the searching/filtering mechanisms in two ways: (1) by excluding documents containing false-synonyms of the query keywords (such as terms  X  X lient X  and  X  X ustomer X  in Example 2 ); (2) by excluding documents containing false-related terms (asdiscussedinExample 3 for document D 1). As further examples of the first aspect, in WordNet both terms  X  X ustomer X  and  X  X ode X  are potential synonyms of  X  X lient X ; however, when  X  X lient X  is used in a Business context, only  X  X ustomer X  is a true-synonym, while  X  X ode X  is a false-synonym. As to the second aspect, the term  X  X lient X  is related to the term  X  X ebsite X  only when it is used in a Computer Science context, while when it is in a Business context,  X  X ebsite X  represents a false-related term.
Starting from these considerations, we decided to enhance the Similarity Techniques employed in the FACIT-SME Semantic Helper, by including the sense information deriving from the application of WSD techniques. In the following sections, we describe the extensions needed to make the Semantic Glossary and the similarity formulas aware of the senses of terms in their context. 7 4.1 Sense-aware extensions to the keyword extraction and enhancement WSD is usually applied on text, and it is performed w.r.t. a knowledge source (e.g., glossaries, thesauri or dictionaries) representing the sense inventory (i.e., all the possible meanings of terms) [ 33 ]. As previously described, the Semantic Helper makes use of two knowledge sources: the IEEE vocabulary and WordNet. The main differences between these two sources are in: 1. Coverage the IEEE vocabulary includes only terms and senses belonging to the Computer 2. Granularity the IEEE vocabulary makes such a fine-grained sense distinction that for
As described in [ 36 ], the performance of WSD strictly depends on the granularity of the sense distinctions, which should be selected on the basis of the application. In a software development context, we do not need a fine sense distinction; instead, errors may typically come out when terms have orthogonal senses (as in the case of  X  X lient X ). Starting from these considerations, we decided that we needed WSD only for terms existing in WordNet. If a term does not exist in WordNet, we check whether it is present in the IEEE vocabulary, and in this case, we associate the first sense proposed to it.
 We used the STRIDER WSD algorithm described in [ 27 , 29 ] to perform WSD; other WSD algorithms might be employed as the ones described in [ 37 ], developed in the context of the MOMIS Data Integration System [ 4 ], and in [ 33 ]. STRIDER is designed to perform effective disambiguation of terms w.r.t. the WordNet thesaurus, also managing structures that go beyond plain text (e.g., nodes in XML trees or RDF graphs). Its outcome is a ranking of the plausible senses for each term, from which the top sense is automatically suggested.
STRIDER returns an annotation, A ( t i ) = s j ,where s j is the top sense of a ranking { s ,..., s k } of plausible senses for each term t i occurring in a given document D .Weapply WSD to all the documents available in the ORM: WSD becomes a new step to be performed during the keyword extraction and enhancement phase (see Sect. 3.1 ). Further, the Semantic Glossary structure needs to be extended for storing the WSD information: We need to asso-ciate each term with the corresponding annotation and the list of documents where the term is used with that sense, thus going toward a sense-aware semantic glossary.

We modified the previous Semantic Glossary structure in the following way: All references to terms (also including synonyms, frequency information, and so on) become references to senses. Table 3 shows an excerpt of the sense-aware Semantic Glossary (global view), as extracted from documents D 1and D 2 of our previous examples. As we can see, the TERM column becomes the ANNOTATION column, containing the annotation information in the form  X  term # senseIndex  X  (e.g.,  X  client #3  X  means that the term  X  X lient X  has been annotated with the third sense proposed by the knowledge source). Moreover, TF, i.e., term frequency, becomes AF (annotation frequency), i.e., the frequency of a specific term annotation.
The other columns do not change their name but have some significant changes in their content: WN and IEEE inform whether the annotation is w.r.t. WN or w.r.t. the IEEE vocab-ulary; SYNS contains the synonym annotations (e.g., for the ANNOTATION  X  client #3  X , it contains  X  guest #4  X  as the fourth meaning of  X  X uest X  is a synonym of the third sense of client); DEFS contains only the definition corresponding to the annotation (e.g., for  X  client #3  X ,  X  X ny computer that is hooked up to a computer network X ); DOC_LIST contains the list of docu-ments in which the term occurs with the same annotation; and finally, IDF and WEIGHT are computed on the basis of the annotation frequency AF. 4.2 Sense-aware semantic similarity We now need to enhance the document similarity techniques in order to fully exploit the new information available in the sense-aware Semantic Glossary. In particular, the formula DSim ( D x , D y ) has to be modified; we define the revised DSim as: where A ( t x i ) is the annotation of the i th term in document x (i.e., the sense associated with sense associated with term t j in document y). SSim is a sense similarity function which computes the similarity between two annotations, i.e., between the senses associated with two terms. It is defined as follows: where A ( t i ) SYN ( t j ) means that the two annotations are the same. The REL relation can be computed, as always, through the functions HSim and GSim ; the only difference in this case is that, instead of considering all the possible senses for a term, we restrict the computation to the senses specified in the annotations. Thus, the sense-aware formulas for GSim and HSim will be: 4.3 Sense-aware techniques in practice When a user submits a query, the Semantic Glossary already contains the term document annotations A ( t y j )  X  D y . Therefore, at run time, we need to apply our WSD algorithm only to the user query X  X  terms.
 Notice that there exist cases where it is not convenient to apply WSD to the query terms. Let us consider, for instance, a possible keyword query  X  X uest address X : In this case, if no other keyword is available, there is not enough context information to determine whether  X  X uest X  is a computer in the network or a visitor, as well as if  X  X ddress X  means a computer address or a street address. As a consequence, in this and in other similar cases, we consider all the possible senses for the query terms, since WSD would not have sufficient information to perform annotations. Further, we still apply formulas ( 8 , 9 , 10 ) by computing SSim for each possible annotation of the query terms (and, then, by considering the maximum value). We conclude the section by providing an example of how the sense-aware similarity techniques actually work on a simple case.
 Example 4 Let us consider query Q 1 and documents D 1and D 2 of our previous examples. As previously seen, for this query, only document D 1 should be returned by the semantic helper, while document D 2 should be discharged. Let us see how this is accomplished by our sense-aware techniques.
 The Semantic Glossary shown in Table 3 is obtained after the application of the Semantic Glossary process described in Sects. 3.1 and 3.2 . The composite term identification step iden-tifies the composite term  X  X usiness enterprise X  and  X  X lient request X . Neither WordNet nor the IEEE vocabulary provides an entry for  X  X lient request X : In these cases, we consider and disambiguate the single terms (i.e.,  X  X lient X  and  X  X equest X ). 8 Then, we automatically disam-biguate the query terms. The returned query annotations are as follows: client #3 , ser v er #3 , and communication #1 . Now, we can compute the document similarity between Q 1, D 1and Q 1, D 2: 9 i.e., the similarity between Q 1and D 2 is null, and only D 1 is selected as relevant for the query. 5 Related work In literature, several approaches applying syntactic information retrieval techniques to spe-cific software engineering tasks have been proposed. For instance, the well-established notions of vector space model, tf-idf weighting and pre-processing techniques for stem-ming and stopword removal are exploited in a number of works (e.g., [ 18 , 38 , 45 ]), as also noted in a recent survey on software maintenance and evolution [ 7 ]. Generally speaking, such methods are focused on very specific tasks or scenarios (and, therefore, specific kinds of input information). Their results confirm the possible applicability of standard information retrieval techniques (such as the ones from which we started to devise our approach) to SE scenarios, even if they are limited by the absence of semantic analysis.
 A  X  X lassic X  information retrieval foundation also characterizes the more general purpose CMSs which are commercially available and can be easily adopted and exploited by SMEs [ 47 ] (a notable example is Alfresco). 10 Similarly to our approach, the automatic nature, general applicability and ease of use of these CMSs make it easy to search for information also for non-skilled users, for instance by allowing them to compose queries in a simple keyword-based way. However, in such CMSs, only syntactic features are exploited, while the semantics of terms is not taken into account, therefore limiting the achievable results (see also Sect. 6 for an experimental comparison with our technique).

Several papers tried to go beyond syntactic retrieval techniques, showing the possible benefits of exploiting semantic knowledge-based methods for specific SE tasks [ 14 , 16 , 40 , sufferfromloweffectiveness,asitwasalsostatedin[ 20 ].Ontheotherhand,knowledge-based approaches can really enhance the effectiveness of the component reuse task by proposing the usage of semantics. In [ 16 ], a software reuse system based on the processing of the natural language descriptions of software artifacts is described. The retrieval mechanism is based on a similarity analysis which exploits synonym, hypernym and hyponym relationships extracted from WordNet. In [ 40 ], the authors propose a toolcalled FindConceptable to expand search queries with synonyms from WordNet. Furthermore, other notable works propose and present methods for retrieving information from specifically structured software documents or artifacts. For instance, in [ 16 , 43 ], the retrieving process is performed by focusing on the comments that can be found all through the software code. The approach [ 14 ] is based on semantic metrics, calculated by first extracting class names and the relevant paragraphs from IEEE-formatted design documents.

Generally speaking, most of the available knowledge-based SE methods cannot be directly compared to ours, since they offer solutions tailored for very specific SE tasks and/or very specifically structured documents, rather than a general method that can be applied on any kind of textual documentation without any preparation or prerequisite. Further, most available approaches limit their semantic features to the use of synonyms and related terms extracted from general lexical resources such as WordNet [ 40 , 43 ]. Instead, as WordNet does not contain several software engineering terms, we chose to also exploit specific domain vocabularies for the software engineering context. Further, we take advantage not only of synonyms and related terms but also of WSD techniques able to capture the meaning of terms in a context.
Finally, some approaches go even beyond the above-mentioned semantic techniques by exploiting formal descriptions and representations of the software information [ 9 , 19 , 23 , 26 , 32 , 42 , 48 ]. For instance [ 32 ], exploits the popular Telos language to represent requirements, design and code; other approaches make use of ontologies to describe the functionality of components [ 19 , 23 , 26 , 42 ]. The use of such knowledge representation formalisms allows convenient and powerful querying, for instance by using SPARQL [ 19 ]. However, this kind of approaches requires expert knowledge management skills in order to create the queries. Moreover, the use of specialized ontologies requires their complete update when new sources (e.g. new documents) are added to the repository, making these solutions hardly suitable or even applicable in the context of small enterprises. The discussion on integrating SE and KE approaches has been, in many cases, too academic, often neglecting the applicability and usability issues, as observed in [ 20 ]. On the contrary, our method allows anyone to create queries by simply using keywords (or already existing pieces of documentation), while offering at the same time the benefits given by advanced semantic document management. 6 Experimental evaluation In this section, we present the results of the evaluation of our tool within the FACIT-SME project. We collected a set of 1,500 documents in the domain of quality requirements, contain-ing approximately 25,000 words in total, taken from project partners and from well-known quality models, such as CMMI [ 8 ] and ISO 9000 [ 12 ]. Starting from this set of documents, we automatically generated a Semantic Glossary and obtained as a result 903 different terms extracted from the documents. Then, we considered and evaluated 100 typical queries which are usually submitted by FACIT-SME partners with reference to this collection; each query is either composed by a short text containing candidate keywords (80 queries), as in the  X  X rom Scratch X  scenario of the project, or by a whole document (20 queries), according to the  X  X rom Methodology X  scenario. In the following, for clarity of presentation and without loss of gen-erality, we present the results we obtained on a reduced sample set of queries which is repre-sentative of the results we obtained on the whole set. In particular, ( Q 1 X  Q 9) are the queries executedforthe X  X romScratch X  X cenario,whilequeries QT 1 X  QT 4aretheexistingenterprise documents selected for the  X  X rom Methodology X  scenario. Table 4 shows the main keywords contained in Q 1 X  Q 9 queries; on the other hand, documents QT 1 X  QT 4 are very large and it is not possible (nor useful to the this analysis) to summarize them to a few keywords. In the majority of the original considered queries, they were mostly constituted by specific domain terms (queries Q 1 X  Q 6 capture this fact), while queries Q 7 X  Q 9 are representative of less com-mon (and ambiguous) requests. Each query has been processed by the proposed techniques so to generate a set of possible  X  X uggestions X , i.e., pointers to the relevant documents in the For evaluating the effectiveness of our approach, we compared the output of the Semantic Helper, for each query, with a  X  X old standard X , i.e., the relevant answers manually selected from the set of documents by experts in the field. Two baselines representing typical syntactic retrieval methods are also considered in order to fully understand the benefits of the semantic features. Finally, Sect. 6.3 concludes the evaluation and is specifically devoted to analyze in detail the effectiveness of the proposed ranking and scoring techniques; therefore, it will focus on the  X  X arger X  QT 1 X  QT 4 queries. 6.1 Effectiveness of all-senses techniques The first analysis we conducted was to assess the quality of the all-sense semantic similarity techniques results (sense-aware extensions will be discussed in the next section) in terms of precision and recall, which are typical evaluation metrics in the information retrieval field 11 [ 2 ]. Table 5 shows the results for Q 1 X  Q 9 (left part of figure). The shown results are obtained with the gloss similarity as similarity function for IEEE terms and the hypernym-based one for WordNet terms. Besides precision and recall, we also report their weighted harmonic mean ( F -measure).

Further, in order to emphasize the contribution and benefits of these techniques w.r.t. the ones typically available in commercial tools used by SMEs, we also present the results concerning two baselines (right part of Table 5 ): (1) the syntactic retrieval method offered by most enterprise CMSs such as Alfresco, where synonyms and related terms are not han-dled and only exact match among terms is allowed (see also Sect. 5 ); (2) another syntac-tic method not exploiting the keyword enhancement phase. The comparison between the achieved F -measures is also graphically shown in Fig. 2 . Let us now analyze the results in detail.

In Table 5 , the precision and recall levels achieved by the semantic similarity techniques on all the queries are shown. The levels on the queries (Q1Q6) are very satisfying (equal or higher to 0.84 and 0.90, respectively). In general, the processing of all queries greatly benefits from the keyword extraction/enhancement phase: In fact, without it, recall levels significantly drop to 0.2 X 0.4 (for instance, different inflections of the same term are not correctly identified). Keyword enhancement can also bring great benefits in precision as, for instance, in Q 1and Q 2: Since they contain, among others, composite expressions, such as  X  X nterface requirement X  ( Q 1) or  X  X onfiguration management system X  ( Q 2), not correctly identifying them leads to a large number of irrelevant retrieved documents (in the second baseline, precision drops to less than 0.01, compared to 1 for the standard results). Queries Q 3 X  Q 6 require synonyms and related terms management in order to provide satisfying answers: for instance, one of the key terms in Q 3 is  X  X upplier X , a concept which is expressed as  X  X endor X  in some of the documents (recall goes from 1 to 0.96 of the first baseline), while Q 4 contains  X  X urpose X  which is mostly expressed as  X  X bjective X  in the collection (recall drops from 1 to less than 0.08). The same holds for the related terms: By applying the gloss similarity formulas exploiting the IEEE definitions, we achieve near-perfect recall levels (as opposed to the less than optimal ones of the first baseline), also maintaining high precision levels for (queries Q 1 X  Q 6). For example, most documents containing  X  X eview X  are also relevant to Q 5, which contains  X  X udit X ; the ones containing  X  X ocument X  are also relevant to Q 6 asking for  X  X ocumentation X , and so on.

Queries Q 7 X  Q 9 obtained less satisfying levels of precision and recall, due to the presence of several common terms that are not present in the specialized IEEE vocabulary and for which only the use of WordNet similarity is allowed. Even if in some cases, as in Q 7, the WordNet-based similarity proves equally useful as the gloss based one, and in other cases (as in Q 8and Q 9), it leads to several false positives. This is mainly due to the non-specialized nature of the WordNet thesaurus, which covers several domains and thus, unlike IEEE, includes highly polysemic terms (e.g.,  X  X rea X  and  X  X ubject X ). In the next section, we will see how the sense-aware techniques can significantly improve the obtained results. 6.2 Impact on effectiveness of sense-aware techniques In this section, we are interested in evaluating the effect of the WSD process on the semantic similarity techniques. To this end, we applied the sense-aware similarity techniques to the queries Q 1 X  Q 9. Figure 3 shows the results of the sense-aware similarity techniques (queries Q 6 X  Q 9) by comparing precision (Fig. 3 a), recall (Fig. 3 b) and F -Measure (Fig. 3 c) to the ones obtained with the all-senses similarity techniques.

For the queries Q 1 X  Q 6, the sense-aware extensions behave in a similar way, with reduced improvements over the results presented in the previous section (differences in terms of 0.04 in precision or less). Thus, for sake of simplicity, we report in Fig. 3 only the results for query Q 6. The motivations are the following. Queries Q 1 X  Q 6 mainly contain specific and technical terms (e. g.,  X  X SO X , i.e., the acronym for  X  X nternational Organization for Standardization X  or  X  X raceability X ) existing only in the IEEE vocabulary and not in WordNet. As a consequence, these terms are typically not affected by disambiguation issues (see Sect. 4 ). Moreover, these queries contain several terms existing in WordNet but with a unique meaning (i.e., monosemic terms, e.g.,  X  X oftware X  or  X  X dentifier X ) or with few strongly related meanings (e.g.,  X  X ethodology X ,  X  X ardware X , or  X  X ecovery X ).

On the contrary, the use of WSD is able to significantly improve the accuracy of the document retrieval process for queries Q 7 X  Q 9(seeFig. 3 ), for which we obtained an average increment in precision of 0.53. The main reason for these results is that these queries are more ambiguous than Q 1 X  Q 6 as they contain general terms, which might be related to several documents. For instance, query Q 9 greatly benefits from WSD, as it contains, among others, the term  X  X rea X : This term may mean a  X  X eographical region X  (as in Q 9) or  X  X  subject of study X . ORM documents are usually not about  X  X rea X  as  X  X eographical region X , while in several ones,  X  X rea X  has the  X  X  subject of study X  sense. Without disambiguation, we obtained several false-positive documents corresponding to  X  X rea X  with the  X  X ubject of study X  sense and other documents containing false-related terms, such as  X  X opic X ,  X  X ssue X  and  X  X ubject X . The same happens for query Q 8 containing, among others, the general and polysemic term  X  X cheme X , which can assume several meanings, such as a  X  X trategy X , a  X  X odge X , a  X  X ystem X  etc.

As regards to query Q 7, by using WSD, we were able to achieve perfect precision. In this case, the improvement w.r.t. the all-sense technique is of 0.16, which is a smaller increment than for Q 8and Q 9: This is due to the fact that, even if the query contains common terms, their meaning is the same as in the majority of their occurrences in the ORM documents. For instance, in Q 7, the term  X  X arket X  is used in the  X  X ommercial activity X  meaning: The ORM document collection includes several documents containing  X  X arket X  with this meaning. On the other hand, as we can note from Fig. 3 b, recall is only slightly affected by WSD: We obtained very similar results to the all-senses ones, with a decrease of 0.093. The reason is that WSD mainly performs a pruning action w.r.t. the document collection: In nearly all cases, the documents containing the query terms with the correct sense are preserved, while the irrelevant ones are correctly pruned out (this effect will be even clearer in the ranking analysis we provide in the next section).

We are now interested in investigating the impact of the WSD process on the single simi-larity techniques composing the method: In particular, we will investigate how WSD affects the use of synonyms and the use of related terms in the document similarity computation. We focus on Q 7 X  Q 9. Table 6 compares the results obtained by the sense-aware (on the left) and the all-senses (on the right) similarity techniques, by using only synonyms (top of figure) and only semantically related terms (bottom). We can note that both synonyms and related terms greatly benefit from the use of WSD. In particular, while recall is again almost unchanged (with an average decrease of 0.09), precision is strongly improved (with an average improve-ment of 0.46). The unique exception is represented by Q 7 for which we obtained a slight improvement in using WSD with related terms. As previously observed, this result is due to the fact that Q 7 represents the case of queries containing ambiguous terms that are present in the document collection with a unique meaning.

In conclusion, by analyzing the overall performances obtained by using the sense-aware similarity techniques, we can observe that, independently from the ambiguity of query terms, we can safely use WSD, as it improves precision (w.r.t. the all-senses techniques) without significantly decreasing recall. Moreover, as we will see in detail in the next section, WSD helps in positioning the most relevant documents among the top documents in the ranking. 6.3 Detailed ranking effectiveness evaluation In this section, we will deepen the effectiveness analysis by considering queries QT 1 X  QT 4, in the form of actual text documents typically used in the FACIT-SME environment, for which to find related documents in the collection. Differently from Q 1 X  Q 9, these queries may contain a large number of terms and produce a very large number of results. For this reason, it is essential to evaluate not only which answers are returned but also their scores and the induced ranking, assessing whether the best suggestions are returned in the top positions and, thus, whether the proposed weighting scheme is effective.

We start this analysis by assessing the impact of the use of weights and synonyms/related terms on the all-senses techniques described in Sect. 3.3 (Fig. 4 ). To this end, we consider queries QT 1and QT 2, whose majority terms are very specific and technical (as in queries Q 1 X  Q 6). In Fig. 4 (left part), we show for QT 1 and all-senses techniques, the precision values obtained at different recall levels, i.e., when a given percentage of relevant documents have been found (top), and the distance from the ideal ranking (bottom). The all-senses technique is compared to two baselines: a non-weighted version of the DSim which does not consider the term weights (i.e., essentially, they are fixed to 1) and the non-semantic CMS-like retrieval method which does not consider synonyms and related terms. Notice that the all-senses technique achieves high precision levels even at high recall levels: for instance, at recall level 0.6, the precision is still 1, while the baselines X  precision levels have already dropped lower than 0.03. This confirms that our techniques are able to identify the most significant terms in the queries, without being misled by non-relevant ones. The optimal ranking distance analysis confirms the goodness of the retrieved results: For each alternative, the curve represents the normalized Spearman footrule distance [ 11 ] between the retrieved and the ideal ranking, i.e., the normalized sum of the absolute values of the difference between the ranks. For QT2, we found an equally good performance with very similar graphs; therefore, we will not show them in detail.

Finally, we were interested in evaluating how the sense-aware techniques contribute to a highly effective ranking. We considered documents QT 3and QT 4, containing a large percentage of common/ambiguous terms, which are well suited to stress the effectiveness of the proposed techniques on difficult requests. The right part of Fig. 4 shows the results we obtained for QT 4( QT 3 showed similar trends): As we can see, the precision is kept high even at high recall levels, and the distance from optimal ranking is kept much lower than for the all-senses techniques. This once again shows the positive impact of the WSD technique, which gives a key contribution in retrieving the most relevant results among the first ones in the ranking. 7 Concluding remarks In this paper, we proposed a fully automatic and semantic approach for filtering/searching software documentation, carefully considering the actual user-targets, IT-SME.

Our approach has been studied and evaluated in the context of the European FP7 3-year project  X  X acilitate IT-providing SMEs by Operation-related Models and Methods (FACIT-SME), X  and is implemented in the Semantic Helper component of the FACIT-SME solution.
The main achievements and effectiveness of the proposed approach, as documented by the experimental section, are the following:  X  Firstly, it is an easily and generally applicable/maintainable tool for IT SMEs to look for information in their large amounts of already available textual documents (i.e., method-ology descriptions, and so on); this is done by specifying simple keyword queries or document queries, without requiring any conversion toward complex structured formats which would be time and cost consuming;  X  Secondly,thetoolisabletoautomaticallyidentifythesimilaritiesbetweensuchqueriesand a reference set of documents. The limitations of standard syntactical techniques (such as the ones usually exploited by enterprise CMSs) are overcome by considering the semantics intrinsically associated with document/query terms and by addressing the problem of term ambiguity through the use of WSD algorithms. To this aim, we exploited different kinds of external knowledge sources (both general and specic domain dictionaries or thesauri);  X  Thirdly, its flexibility enables many high-level functionalities, i.e., filtering software methodologies for software process assessment and improvement, quality requirements for helping in certication process, best practices for facilitating knowledge sharing, and so on;  X  finally, the approach does not have any prerequisite, such as the knowledge of complex formal representation/querying standards or the need of converting/updating the docu-mentation already available in the enterprise. To this end, our proposal leverages on the strengths of both classic information retrieval and knowledge-based techniques, without impairing general applicability and usability.
 Several paths will be contemplated as future work:  X  We will further analyze and refine the similarity techniques, user feedback on the retrieved suggestions, multilanguage information management and querying support;  X  We will investigate how techniques we developed in complementary contexts, such as multi-version semi-structured data management [ 17 ], could help in the exploitation of other non-textual knowledge available in the ORM repository;  X  Leveraging on our previous works on Peer-to-Peer network [ 5 , 28 ], we will extend our approach in this direction. Indeed, large software development projects are complex endeavors that involve numerous participants which can work across several sites and act in various roles; therefore, we will also consider notable past experiences such as [ 21 ], where Peer-to-Peer-based metadata management semantic technologies were proposed as an important enabler to improve information and knowledge sharing in such scenarios. References
