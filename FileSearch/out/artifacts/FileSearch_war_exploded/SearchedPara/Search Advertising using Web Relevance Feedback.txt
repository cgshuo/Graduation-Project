 The business of Web search , a $10 billio n industry, relies heavily on sponsored search , whereas a few carefully-selected paid advertisements are displayed alongside algorithmic search results. A key technical challenge in sponsored search is to select ads that are relevant for the user X  X  query. Identifying relevant ads is challenging because queries are usually very short, and because users, consciously or not, choose terms intended to lead to optimal Web search results and not to optimal ads. Furthermore, the ads themselves are short and usually formulated to capture the reader X  X  attention rather than to facilitate query matching.

Traditionally, matching of ads to queries employed stan-dard information retrieval techniques using the bag of words approach. Here we propose to go beyond the bag of words, and augment both queries and ads with additional knowledge-rich features. We use Web search results initially returned for the query to create a pool of relevant documents. Clas-sifying these documents with respect to an external taxon-omy and identifying salient named entities give rise to two new feature types. Empirical evaluation based on over 9,000 query-ad pairwise judgments confirms that using augmented queries produces highly relevant ads. Our methodology also relaxes the requirement for each ad to explicitly specify the exhaustive list of queries ( X  X id phrases X ) that can trigger it. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval General Terms: Algorithms, economics, performance Keywords: Online advertising, relevance, Web search
The Web has become an integral part of our lives: peo-ple around the world use it for information, entertainment, shopping, communication, and many other activities. Navi-gating the Web without search engines would be impossible and more than 2 billion searches are performed every day [9].
The prevailing business model of Web search relies heavily on sponsored search , whereas a few carefully-selected paid advertisements are displayed alongside algorithmic (or or-ganic ) search results. There is a fine but important line between placing ads reflecting the query intent, and plac-ing unrelated ads: users may find the former beneficial, as an additional source of information or an additional Web navigation facility, while the latter are likely to annoy the searchers and hurt the user experience (we discuss Web ad-vertising in more detail in Section 2).

Identifying relevant ads is far from trivial, mainly be-cause search queries are so short (the average query is only about 2.5 words long), and because users, consciously or not, choose query terms intended to retrieve the best search results rather than the best ads.

In the realm of Web search (and more generally within the field of information retrieval), there have been a number of studies on query augmentation [1, 3, 31, 48, 50], but as far as we know, no studies focused on query expansion for ad search. The latter task is notably more difficult than gen-eral query expansion since ads are typically quite short and are often formulated as abrupt, non-grammatical phrases in-tended to capture reader X  X  attention rather than to facilitate query matching. Consequently, the usefulness of the ad cor-pus itself for query expansion is limited, hence we opted to explore ways of query augmentation using external sources of knowledge, including Web search results and a large tax-onomy of commercial topics.

Given the original query (called the Web query in the se-quel), we first send it to a Web search engine, and then use the returned top-scoring pages to gather additional knowl-edge about the query. We use this knowledge to create an augmented query (called the ad query in the sequel), which is evaluated against the ad corpus to retrieve relevant ads for the original Web query. Of course, short queries are also difficult for Web search; however, modern search engines use a huge amount of additional knowledge such as past query statistics, link analysis, page popularity, anchor text, and click-through data, and thus can return decent results even for very short inputs. Thus, the highest-scoring search re-sults are often quite good, and so we use them for query augmentation within a blind relevance feedback approach.
Such additional knowledge becomes invaluable when pro-cessing malformed queries, such as misspelled ones or queries in which several words are glued together. For example, when the user types a misspelled query  X  X ar insuance X , the search engine automatically corrects the spelling mistake and the search results for the corrected query can also be used for matching ads.

Historically, the mainstream approach to textual docu-ment retrieval has been based on the bag of words paradigm, where both the query and the documents to be retrieved are represented as vectors of word-based features [41], whose val-ues are computed using a variant of the TFIDF weighting scheme [39]. In this work we go beyond the bag of words by using search results returned for the Web query to construct three classes of features that together form the ad query. For the first class of features, we pool together the words (un-igrams) that occur within the result pages, and select the most representative ones to be used in addition to the origi-nal query words. The second class of features is based on our previous work on query classification using Web search re-sults [7]. In that work, we classified the search results with respect to a large external taxonomy of over 6,000 nodes, and then used voting to determine the best classifications for the original query. Here we apply a similar technique to classify the Web query into relevant classes, which then de-fine new features of the ad query. The third class of features is defined by a large lexicon of phrases, built by analyzing the set of all Web pages crawled by the underlying search engine. We identify all the entries of this lexicon that appear in the search results for the Web query, and then retain the most representative ones as additional features.

Ads undergo a similar processing, consisting of word anal-ysis, taxonomy classification, and extraction of lexicon phra-ses. When both queries and ads are represented in this aug-mented space of features, their matching amounts to com-puting conventional similarity metrics such as cosine [53].
The contributions of this study are fourfold:
Using the classification-based and phrase-based features facilitates thematic matching that goes beyond the simple bag of words approach and captures deeper semantic sim-ilarity. Our experimental evaluation confirms that using these additional features greatly improves the accuracy of ad matching, resulting in more relevant ads.
A large part of the Web advertising market consists of tex-tual ads , the ubiquitous short text messages usually marked as  X  X ponsored links X . There are two main channels for dis-tributing such ads. Sponsored search (or paid search adver-tising ) places ads on the result pages of a Web search engine, where ads are selected to be relevant to the search query (see [16] for a brief history of the subject). All major Web search engines (Google, Microsoft, Yahoo!) support sponsored ads and act simultaneously as a Web search engine and an ad search engine. Content match (or contextual advertising ) places ads on third-party Web pages. Today, almost all of the for-profit non-transactional Web sites 1 rely at least to some extent on contextual advertising revenue. Content match supports sites that range from individual bloggers and small niche communities to large publishers such as major newspapers.

In this paper we focus on sponsored search. However, we believe that additional knowledge-based features are also likely to be beneficial for content match, and plan to inves-tigate this direction in future work.

Sponsored search is an interplay of three entities. The advertiser provides the supply of ads. Usually the activ-ity of the advertisers is organized around campaigns ,which are defined by a set of ads with a particular temporal and thematic goal (e.g., sale of digital cameras during the hol-iday season). As in traditional advertising, the goal of the advertisers can be broadly defined as promotion of products or services. The search engine provides  X  X eal estate X  for placing ads (i.e., allocates space on search results pages), and selects ads that are relevant to the user X  X  query. Users visit the Web pages and interact with the ads.

The prevalent pricing model for textual ads is that the advertisers pay for every click on the advertisement (pay-per-click or PPC). There are also other models, such as pay-per-impression, where the advertiser pays for the number of exposures of an ad, and pay-per-action, where the advertiser pays only if the ad leads to a sale or similar completed trans-action. In this paper we deal with the PPC model, which is the one most often used in practice.

The amount paid by the advertiser for each sponsored search click is usually determined by an auction process [14]. The advertisers place bids on a search phrase, and their po-sition in the column of ads displayed on the search results page is determined by their bid. Thus, each ad is annotated with one or more bid phrases . In addition to the bid phrase, an ad also contains a title usually displayed in bold font, and a creative , which is the few lines of text, usually shorter than 120 characters, displayed on the page. Naturally, each ad contains a URL to the advertised Web page, called the landing page .

In the model currently used by all the major search en-gines, bid phrases serve a dual purpose: they explicitly spec-ify queries that the ad should be displayed for and simulta-neously put a price tag on a click event. Obviously, these price tags could be different for different queries. For ex-
Non-transactional sites are those that do not sell anything directly. ample, a contractor advertising his services on the Internet might be willing to pay a small amount of money when his ads are clicked from general queries such as  X  X ome remod-eling X , but higher amounts if the ads are clicked from more focused queries such as  X  X ardwood floors X  or  X  X aminate floor-ing X . Most often, ads are shown for queries that are expressly listed among the bid phrases for the ad, thus resulting in an exact match (i.e., identity) between the query and the bid phrase. However, it might be difficult (or even impossible) for the advertiser to list all the relevant queries ahead of time. Therefore, search engines also have the ability to ana-lyze queries and modify them slightly in an attempt to match pre-defined bid phrases. This approach, called broad (or ad-vanced ) match, facilitates more flexible ad matching, but is also more error-prone, and only some advertisers opt for it. Nonetheless, bid phrases remain a mandatory component of the ad definition.

Given a query q , the revenue from a click can be estimated as where k is the number of ads displayed on the page with search results for q and price ( a i ,i ) is the click price of the ad a i at position i . The price in this model depends on the set of ads presented on the results page. Several models have been proposed to determine this price, most of them based on generalizations and variants of second price auctions (for more details, see [14] and references therein). For simplicity, in this paper we ignore the pricing model and concentrate on finding ads that will maximize the first term of the product, that is, we search for Furthermore, we assume that the probability of a click for a given ad and query is determined by the ad X  X  relevance score with respect to the query, thus ignoring the positional effect of the ad placement on the results page. We assume that this is an orthogonal factor to the relevance component, and could be easily incorporated in the model. In this section we present our methodology for using the Web for constructing new features for representing queries and ads. This approach allows us to leverage external knowl-edge available to search engines in order to create more infor-mative features for matching ads to queries. Furthermore, by using features that characterize the entire ad rather than only its bid phrase, we relax the requirement for advertisers to explicitly specify bid phrases.
The input to our system is a search (or  X  X eb X ) query, and the output is a set of ads that are relevant to this query. Processing the input query involves two main phases. In the first phase, we conduct a Web search with the original query, and analyze the top-scoring results obtained for it. We use these search results to augment the Web query and construct an ad query, which is then evaluated against an index of ads. Figure 1 presents a high-level view of the information flow.

We represent ad queries and ads in three distinct feature spaces that are formed using three different kinds of features, namely, unigrams, classes, and phrases. Thus, each object is represented as a feature vector, which is composed of three sub-vectors, each of which is normalized and scored sepa-rately. Let q be a query, then its feature vector is defined as follows: v q = uq 1 ,...,uq | U | ,cq 1 ,...,cq | C | ,pq where U , C and P are the sets of unigrams, classes and phrase features, respectively. Given an ad a and its vector v pute its score for a query using cosine similarity metric: where  X  ,  X  and  X  are the weights reflecting the importance of the different feature classes.

Figure 2 gives an overview of the system architecture. Al-though we currently use three different kinds of features, our modular approach could easily incorporate additional feature types, which could be built using additional knowl-edge sources.
Our primary source of augmenting the Web query and constructing new features is the set of top-scoring search results for the original Web query. We adopt the blind rele-vance feedback approach, and assume that most of the top-scoring results are relevant to the query to some extent. Let R = { r 1 ,...,r | N | } be a set of top search results.
To construct word-level unigram features U we first pool together all the individual words that occur in search re-sults pages. Taking all the words that occur in any of the result pages would necessarily be very noisy, hence we use feature selection to represent the query only with features that are truly characteristic of it. Since we employ the blind relevance approach, we do not have any kind of labeling of search results, and hence the feature selection step should be unsupervised. Therefore, we cannot use inherently su-pervised methods like information gain, and resort to using metrics based on document frequency and TFIDF. It should be noted, however, that studies in (supervised) text catego-rization confirm that feature selection based on document frequency yields results that are on par with those based on information gain [42].

Having selected a desired number of features, we assign their values using the TFIDF scheme [39], where we use logarithmic term frequency and IDF computed over the ad corpus. Precisely, featu re weights are computed as uq i (1 + log ( tf )) log NA NA ( uq i ) ,where tf is the number of occur-rences of uq i in the pooled search results  X  i r i , NA is the to-tal number of ads, and NA ( uq i ) is the number of ads whose text contains the word uq i . Finally, unigram weights un-dergo cosine normalization: uq i = uq i
If a query and an ad are highly related but use different vocabulary, the bag of words matching will be insufficient to capture their relatedness. While this problem is alleviated to some degree by expanding the query with search results, it cannot be completely solved this way. We use text clas-sification with respect to an external taxonomy in order to identify commonalities between related but different vocab-ularies. To achieve this aim, we use a large taxonomy of commercial-intent topics, and build a document classifier that is capable of mapping an input fragment of text into a number of relevant classes. We then use these classes to create new features for queries and ads. Doing so not only allows us to generalize from the level of individual words to higher-level abstractions, but also explicitly benefits from the external knowledge that was used to build this auxiliary classifier.

Our choice of taxonomy was guided by a Web advertis-ing application. Since we want the classes to be useful for matching ads, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both  X  X ore foot X  and  X  X lu X  queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such sit-uations, the taxonomy needs to provide sufficient discrimi-nation between common commercial topics. Therefore, we employed a large taxonomy of approximately 6 , 000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Human editors populated the taxonomy with la-beled bid phrases of actual ads (approx. 150 phrases per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. We used the same taxonomy in our earlier work [7], where it is described in more detail.

Few machine learning algorithms can efficiently handle so many different classes and training examples. Suitable can-didates include the nearest neighbor and the Naive Bayes classifier [13], as well as prototype formation methods such as Rocchio [37] or centroid-based [19] classifiers. We used the latter method to implement our text classifier. For each taxonomy node we concatenated all the phrases associated with this node into a single meta-document. We then com-puted a centroid for each node by summing up the TFIDF values of individual terms, and normalizing by the number of phrases in the class: where c j is the centroid for class C j and p iterates over the phrases in a particular class.

The classification is based on the cosine of the angle be-tween the input document and the centroid meta-documents: where F is the bag of words, and c i and d i represent the weight of the i th feature in the class centroid and the doc-ument, respectively. The scores are normalized by the doc-ument and centroid lengths to make the scores of different documents comparable.

Given the search results produced for the Web query, we classify each result page and then perform voting among them to select several classifications that best characterize the query. As reported in our previous work [7], the accu-racy of this query classification approach is quite high with P recision =0 . 807 and F 1=0 . 893 at 100% recall. Follow-ing [17], we construct new features based on these immediate classifications as well as their ancestors in the taxonomy (the weight of each ancestor feature was decreased with a damp-ing factor of 0.5). The weights of classification features are defined by the confidence scores assigned by the document classifier. The only transformation applied to these scores is cosine normalization.
For phrase extraction, we used a proprietary variant of Al-tavista X  X  Prisma refinement tool [2] developed in-house. This tool includes two components, an online and an offline one. Given a fragment of text, the online component analyzes it to identify named entities and other stable phrases. This component has been integrated into the crawling and index-ing pipeline of the search engine, and is routinely invoked on all the pages included in the search engine index. The offline component collectively analyzes the phrases found in all the crawled pages, and retains the most significant ones based on their statistical properties. These phrases can then be used as a restricted lexicon for indexing any piece of text they occur in. Approximately 10 million phrases (called Prisma terms in the sequel) are selected for the English language.
Given the set of search results, we first identify Prisma terms that occur in them, and then perform feature selec-tion to retain the most characteristic ones. Both feature selection and TFIDF-based feature weighting are performed similarly to the processing of unigrams explained in Sec-tion 3.2.1. Other feature weighting schemes, notably, BM25 [36] have been reported in the literature, and we intend to report their application to sponsored search advertising in our future work.

At the end of the feature construction process we ob-tain an augmented query represented using three kinds of features X  X nigrams, classes, and Prisma terms. In contrast to a few words that comprised the original Web query, these additional features have been constructed by collectively an-alyzing the set of search results produced for the original Web query. The augmented ad query is then evaluated against the ad index to retrieve relevant ads.
The ads are available ahead of time and the ad processing is performed offline over the Hadoop grid-computing infras-tructure ( http://lucene.apache.org/hadoop/ ). We ana-lyze the ad text and construct the same three types of fea-tures that we do for queries, namely, unigrams, classes, and Prisma terms. At this moment, we do not analyze the con-tents of the Web page pointed at by the ad (the landing page), as in our previous work we found that it is often too noisy.

In an online advertising system, the number of ads can easily reach tens and even h undreds of millions. Therefore, to facilitate fast ad search and retrieval we use an inverted index of ads. Finding relevant ads for the query amounts to efficiently evaluating the scores of candidate ads as defined by Equation (1), and then retrieving the desired number of highest-scoring ads.

As opposed to traditional search engines where the queries are short and documents are long, in our case ad queries are composed of Web-based features (as explained in the pre-ceding section), and are fairly long. An ad query has on average 100 X 200 features, more than the number of features constructed for some ads. Therefore, we are not looking for a subsumption of the query vector by the ad vector; instead, we search for ads that are most similar to the query. To ef-ficiently perform the similarity search over the ad space, we have adapted the WAND algorithm [8] to work with longer queries. WAND uses a branch-and-bound approach to re-duce the number of ads considered. For each query feature, one cursor is opened to traverse the posting lists. The cur-sors are moved based on the upper bound of the score of the document that the cursor currently points at. Only docu-ments with upper bounds higher than the minimal score in the current candidate set are considered.
The system described in this paper used our experimental prototype where search result pages are crawled and ana-lyzed at query time. However, in a real-life system, feature extraction can be performed at page indexing time. Since the set of possible search results is final, the search engine can analyze each Web page and classify it at the time the page is added to the search index. At query time, the search engine just needs to pass the precomputed features of top-scoring search results to the advertising subsystem. Thus, at runtime we do not have to pay the penalty of page crawling and feature extraction. This way, the approach presented in this paper can be practically implemented to conform to the ad serving latency requirements (several hundreds of milliseconds).
We implemented our methodology for feature construc-tion using relevance feedback in an ad matching platform named Onyx. In this section we report the results of its experimental evaluation.
We start with describing the implementation details and the datasets we used, and then proceed to presenting the results of empirical evaluation of our methodology.
Given a query, we run it through the Yahoo Web search engine, and keep the top 40 URL results (this number was empirically determined to be optimal for query classification [7]). We crawl the returned search results, tokenize their text, remove stop words, and stem the remaining words with the Porter stemmer [32]. For both unigrams and Prisma terms we selected up to 50 features of each type, while we evaluated feature selection based on document frequency (DF) as well as based on TFIDF weights. To construct clas-sification features, we first obtained top 5 classes for each individual search result, and then performed voting to select 5 best classes for the query. We constructed features based on these 5 classes as well as their ancestors in the hierar-chy. The optimal number of classes per query was obtained through validation on a held-out dataset.

We implemented the following four system settings (where  X ,  X ,  X  are the relative weights for the different feature types; see Equation (1)): Onyx1  X  =1 . 0 , X  =0 . 5 , X  =0 . 5, feature selection = DF Onyx2  X  =0 . 5 , X  =1 . 0 , X  =0 . 5, feature selection = DF Onyx3  X  =0 . 5 , X  =0 . 5 , X  =1 . 0, feature selection = DF Onyx4  X  =1 . 0 , X  =0 . 5 , X  =0 . 5, feature selection = TFIDF
The rationale behind the first three settings is to empha-size different feature types. The choices of  X  ,  X  ,  X  values in the three settings above were designed to provide good sampling of the parameter space. Given the human rel-evance judgments (explained in the following section), we also subsequently tuned the  X  ,  X  and  X  values for optimal performance. The fourth setting was used to evaluate the TFIDF formula as a feature selection metric.

Throughout the paper, most graphs only display the per-formance of the first system setting (Onyx 1), which we ab-breviate as simply  X  X nyx X . The performance of all the four settingsispresentedinFigure4.
We used a set of 700 Web queries, which has been con-structed in the following way. We started with a set of all queries received by the Yahoo Web search engine during the week of July 23 X 29, 2007. We divided the 10 million most frequent queries into deciles by frequency, and ran-domly sampled 50 queries from each decile. We furthermore sampled 200 queries from the distribution tail (beyond the 10 million most frequent ones).

Each query has been matched to up to three ads using each of the above four system settings, resulting in over 9,000 query-ad pairings (some queries could only be matched to fewer than three ads). A team of six analysts, all of whom hold college degrees and have a high command of the English language, provided relevance judgments for each query-ad pair using the following scale: Perfect, Certainly Attractive, Probably Attractive, Somewhat Attractive, Probably Not Attractive, and Certainly Not Attractive.

In order to compute the standard metrics of precision and recall, we converted the above judgments to binary by con-sidering the first four as relevant, and the rest as irrelevant. To compute precision at various levels recall, we ordered all the query-ad pairs by their scores (as assigned by Equa-tion 1), and used a threshold to include progressively larger fractions of ads.

It should be noted that human relevance judgments are quite expensive, especially when thousands of judgments are needed. Therefore, our choices were either to judge several ads for many queries, or numerous ads for just a few queries. We adopted the former approach, as it provides a better as-sessment of our methodology for a realistic sample of queries of very different frequencies.
In order to assess the value of our methodology, we com-pare its results with the baseline that does not use feature construction, and only uses features that are available from the query per se , without query augmentation using search results. Remember that the Onyx approach performs broad match, as it matches the entire ad text to the augmented query representation, rather than merely matching the ad X  X  bid phrases to the original (unaugmented) Web query. Con-sequently, to make the comparison meaningful our baseline system also performs broad match, albeit without query aug-mentation. That is, the baseline system matches the query words to any part of the ad rather than solely to its bid phrases. Restricting the baseline to exact match only would drastically limit its coverage, making the results not compa-rable to those of Onyx.

In Section 4.4 we also compare Onyx performance to that of the log-based query substitution system [22] as another baseline.

Figure 3 shows the standard precision-recall tradeoff curve for Onyx setting 1. In the case of sponsored advertising, this curve is of particular importance for the following reason. Conventional information retrieval systems always produce some results if the queried collection contains documents that match some query words. Even though these docu-ments may be irrelevant to the query, IR systems are nor-mally expected to yield some results. However, in the case of Web search advertising, in some cases it is desirable not to show any ads. In this scenario, if no ads are relevant to the user X  X  information need, then showing irrelevant ads should be avoided as it deteriorates user experience. Therefore, it is essential to ascertain that our method offers sufficiently high precision at low to medium coverage levels.

Indeed, as we see in Figure 3, the precision of our method improves steadily as we reduce the fraction of queries for which ads are to be displayed. Furthermore, it can be read-ily seen that feature construction based on search results improves ad relevance compared to the baseline over the en-tire range of recall values.

In the rest of this paper, we only report Onyx precision at 100% recall (with the exception of Figure 6, see explanation in Section 4.4). However, according to the above analysis, we could always select a lower recall (and correspondingly higher precision) for actual system implementations.
Figure 4 presents the performance of the four different system settings listed in Section 4.1.1. Observe that the performance of all the four system variants is superior to that of the baseline. Interestingly, the performance of Onyx setting 4 is nearly identical to that of Onyx setting 1, imply-ing that the quality of feature selection based on document frequency (DF) and on TFIDF is essentially the same.
We also conducted a search over all  X ,  X , and  X  combina-tions, with the values of each parameter varying from 0.0 to 1.0 in 0.1 increments (1,330 combinations in total, ex-cluding the trivial combination  X  =  X  =  X  =0). Thebest performance was achieved using  X  =1 , X  =0 . 6 , X  =0 . 4, and it was insignificantly different from that achieved by Onyx setting 1 (  X  =1 , X  =0 . 5 , X  =0 . 5). Figure 5: Onyx performance over a range of query frequencies
As explained in Section 4.1.1, we constructed our query set by stratified sampling out of a Yahoo Web search query log collected over one full week. The distribution of query frequencies follows a power law [45] (commonly referred to as Zipf X  X  law), hence it is interesting to observe the relevance of ads that our method provides for queries of different fre-quencies.

Figure 5 shows Onyx and baseline performance for queries in different frequency ranges. In this figure,  X 1 X  designates the first decile (i.e., most frequent queries),  X 10 X  represents the last decile of the first 10 million queries, and  X  X ail X  stands for rarest queries (sampled from the tail of the distribution). Observe that our methodology provides more relevant ads than the baseline for all query frequencies. Furthermore, it should be observed that our method allows to provide relevant ads even for the tail queries.

Interestingly, the relevance of ads for the most frequent queries (decile 1) is notably lower than that of less frequent queries. We believe the reason for that is that a large frac-tion of queries in the first decile are navigational (e.g., X  X bay X ,  X  X outube X  or  X  X otmail X ), that is the user merely wants to get the URL of the corresponding Web site. In such a case, the user is rarely interested even in search results beyond the first one, let alone any ads that might be shown, hence a majority of such ads are co nsidered less relevant.
As an alternative baseline, we also compare our method-ology with log-based query substitution [22] (abbreviated as LBQS in the sequel). LBQS is a method designed to im-prove Web search queries by automatically analyzing query logs, and learning from query transformations manually per-formed by Web search users. Consequently, LBQS can be viewed as a query transformation technique that uses alter-native source of knowledge, namely, search query logs. In the advertising scenario, we use LBQS to transform original search queries into better ones, and then match them to ads.
LBQS generates possible substitutions by first finding all pairs of successive queries issued by the same user in a search engine log, and then analyzes these queries and finds com-mon transformations. Given a new query such as  X  X ew York maps X , the system segments it into phrases using pointwise mutual information. This way, the example query would be segmented as  X (New York) (maps) X . To generate candidate substitutions, LBQS then applies common transformations observed earlier, for instance, transforming  X  X aps X  into  X  X i-rections X , yielding a substitute query  X  X ew York directions X . The score of a substitution is determined by a machine learn-ing classifier trained on a set of features that capture textual similarity as well as the frequency of the transformations ap-plied.
 Figure 6 compares the performance of LBQS and Onyx. For this experiment, we used an existing LBQS implementa-tion that provided substitutions for 24% of the 700 queries in our dataset (for the other queries, its learned model could not apply any known transformation). Consequently, to make the comparison meaningful, Figure 6 also shows the Onyx precision at 24% level of recall.
 As we can see from Figure 6, Onyx outperforms LBQS. However, it is also interesting to compare the performance of the two systems in greater depth by looking at individual queries. Table 1 shows for each of the two systems how many queries in our dataset get relevant ads, irrelevant ads, or are not covered at all. Since Onyx and LBQS use very different sources of knowledge (Web search results and search query logs, respectively), it is intuitive to understand that they perform well on very different query subsets. Furthermore, for as many as 22% of the queries, Onyx provides relevant ads while LBQS provides no ads at all. This observation implies that it is possible to design a fusion approach that provides relevant ads for an even larger fraction of input queries by using the two systems together. 2 We intend to develop such a combined approach in our future work. Irrelevant 4.7% 6.3% 48.9% 59.9% Uncovered 0% 0% 5.2% 5.2%
Table 1 was generated for LBQS coverage of 24% and Onyx coverage of 100%. This is meaningful for our discussion, since for higher coverage levels LBQS precision will neces-sarily drop, thus providing relevant ads for an even smaller fraction of the queries.
There have been several bodies of prior research that are relevant to our study.
Online advertising is an emerging area of research, so the published literature is quite sparse. A recent study [49] confirms the intuition that ads need to be relevant to the user X  X  interest to avoid degrading the user X  X  experience and increase the probability of reaction.

In the content match scenario, Ribeiro-Neto et al. [34] ex-amined a number of strategies for matching pages to ads based on extracted keywords. They used the standard vec-tor space model to represent ads and pages, and proposed anumberofstrategiestoimprovethematchingprocess.
 While both pages and ads are mapped to the same space, there is a discrepancy (called  X  X mpedance mismatch X ) be-tween the vocabulary used in the ads and in the pages. For example, the plain vector space model cannot easily account for synonyms, that is, it cannot easily match pages and ads that describe related topics using different vocabularies. The authors achieved improved matching precision by expanding the page vocabulary with terms from similar pages, which were weighted based on their overall similarity to the origi-nal page. In this paper, we  X  X ridge X  between related words by defining new features based on higher-level concepts from the classification taxonomy.

In their follow-up work [26], the authors proposed a method to learn the impact of individual features by using genetic programming to produce a matching function. The func-tion is represented as a tree composed of arithmetic op-erators and functions as internal nodes, and different nu-merical features of the query and ad terms as leaves. The results show that genetic programming finds matching func-tions that significantly improve the matching compared to the best method (without page-side expansion) reported in [34].

Another approach to contextual advertising is to reduce it to the problem of sponsored search advertising by extract-ing phrases from the page and matching them to the bid phrases of the ads. Yih et al. [52] described a system for phrase extraction that uses a variety of features to deter-mine the importance of page phrases for advertising pur-poses. The system is trained with pages that have been hand-annotated with important phrases. The learning algo-rithm takes into account features based on TFIDF, HTML meta data, and search query logs to detect the most impor-tant phrases. During evaluation, each phrase up to length 5 is considered a potential result and evaluated against the trained classifier.

Langheinrich et al. [27] studied customization techniques for matching ads to users X  short-term interests. To capture short-term interests, the authors used search queries as well as visited URLs, which could then be looked up in Web directories. Jin et al. [21] used Web page classification to determine whether a given Web page does not contain sen-sitive content, so that it is acceptable to display ads on it.
Prior studies on sponsored search mostly experimented with the information explicitly available in the query and the ad. In contrast, in this work we study the importance of constructing new features based on exogenous sources of knowledge, such as Web search results and a large-scale tax-onomy of commercial topics.
Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. This can be done either using electronic dictionaries and thesauri [48], or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on man ually reviewing the returned results [40, 37]. However, the sheer volume of queries nowa-days does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [51, 31, 15, 35]. As an alternative to relevance feedback, other stud-ies performed query augmentation based on the analysis of query logs [10, 22].

More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused que ry interpretation. Indeed, Kowalczyk et al. [25] found that using query classes im-proved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. [4] used semi-supervised learning as well as unlabeled data [5]. Gravano et al. [18] classified queries with respect to geographic locality in order to determine whether their intent is local or global.
The 2005 KDD Cup on Web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [29, 43, 44, 23, 47]. The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier.

Web search results have also been used in a related task of measuring similarity of short segments of text [38, 30]. More generally, the use of search results as a source of additional features, and especially the use of Web-based Prisma terms, is also related to the studies of the Web as a corpus [24]. In our methodology, we use a text classifier that maps Web search results onto a taxonomy of commercial topics, whereas taxonomy nodes define new features for represent-ing queries and ads. This approach is related to transfer learning , where knowledge learned in one domain is trans-ferred to another domain. Transfer learning methods [6, 12, 46, 33] leverage information from different but related learning tasks, so that features constructed while solving one problem can be used for solving another problem.
Only a few recent studies focused on cross-corpora query augmentation. He and Peng [20], and later Diaz and Metzler [11] used several document collections to augment TREC queries. Li et al. [28] used Wikipedia as an external corpus to augment ad-hoc TREC queries. There are two notable differences between these works and our approach presented herein. First, TREC queries are usually much longer than Web queries. Second, our target collection of ads is substan-tially different from TREC documents, since ads are short and are often created with presentation in mind, and are hence particularly difficult for indexing. To this end, in this work we also augment indexed ads with constructed features.
Web search engines are complex systems, built as a re-sult of many years of research and development. Running a search engine requires a sophisticated infrastructure, built and maintained to provide comprehensive and up-to-date answers to users X  queries. In this work we build upon this effort by using a search engine to improve search advertis-ing. The key idea of our approach is the use of Web search results to construct new featu res for the ad query, which is used to select the ads shown alongside search results. We also expand the ad representation using classification and phrase extraction.

The contributions of this paper are fourfold. First, we formulate a methodology for cross-corpora query expansion, whereweuseonecorpus(theWeb)toaugmentqueriesto be evaluated against another corpus (ads). Second, we pro-pose a method for constructing new features based on ex-ternal knowledge, which provides a richer representation of both queries and ads. Next, we relax the requirement that advertisers need to explicitly specify queries that their ads should be shown for. Instead, we use the entire contents of the ad to identify queries for which it should be shown. Using the classification-based and phrase-based features fa-cilitates thematic matching that goes beyond the simple bag of words. The use of search results for ad matching provides an additional benefit that the search results and ads are the-matically matched. Finally, we provide an evaluation of an end-to-end ad selection system based on an inverted index that supports long queries.

In our experimental evaluation we show that using the constructed features allows us to match Web search queries to significantly more relevant ads. We also conducted abla-tion studies to assess the individual utility of each feature class, and showed that while the bag of words is still the most important type of features for ad matching, the phrases and classes also have a significant impact on the ad selection quality. We compared our approach to a query substitu-tion system that uses search logs as an alternative source of knowledge [22], and argued for a possibility of building a superior system by merging the two approaches.

Actual search advertising systems also incorporate past click data into the ad matching process. In this work, we focused solely on textual relevance, but in our future work we plan to combine both relevance features and click-through features. We also plan to evaluate our system in a real-world setting and measure actual click-through rates in addition to collecting human relevance judgments.
 We thank Donald Metzler for fruitful discussions and point-ers to relevant literature, and Bo Pang for comments on an early draft of the paper. We also thank Ann Hsieh and her editorial team for judging the ad relevance. [1] E.Agichtein,S.Lawrence,andL.Gravano.Learning [2] P. Anick. Using terminological feedback for web search [3] L. Ballesteros and B. Croft. Phrasal translation and [4] S. Beitzel, E. Jensen, O. Frieder, D. Grossman, [5] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, [6] P. N. Bennett, S. T. Dumais, and E. Horvitz.
 [7] A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, [8] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and [9] 61 billion searche s conducted worldwide in August. [10] H. Cui, J.-r. Wen, J.-Y. Nie, and W.-Y. Ma.
 [11] F. Diaz and D. Metzler. Improving the estimation of [12] C. Do and A. Ng. Transfer learning for text [13] R. Duda and P. Hart. Pattern Classification and Scene [14] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [15] E. Efthimiadis and P. Biron. UCLA-okapi at TREC-2: [16] D. Fain and J. Pedersen. Sponsored search: A brief [17] E. Gabrilovich and S. Markovitch. Feature generation [18] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein. [19] E.-H. S. Han and G. Karypis. Centroid-based [20] D. He and Y. Peng. Comparing two blind relevance [21] X.Jin,Y.Li,T.Mah,andJ.Tong.Sensitivewebpage [22] R. Jones, B. Rey, O. Madani, and W. Greiner. [23] Z. Kardkovacs, D. Tikk, and Z. Bansaghi. The ferrety [24] A. Kilgariff and g. Grefenstette. Introduction to the [25] P. Kowalczyk, I. Zukerman, and M. Niemann.
 [26] A. Lacerda, M. Cristo, M. A. Goncalves, W. Fan, [27] M. Langheinrich, A. Nakamura, N. Abe, T. Kamba, [28] Y. Li, R. Luk, E. Ho, and F. Chung. Improving weak [29] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: [30] D. Metzler, S. Dumais, and C. Meek. Similarity [31] M. Mitra, A. Singhal, and C. Buckley. Improving [32] M. Porter. An algorithm for suffix stripping. Program , [33] R. Raina, A. Ng, and D. Koller. Constructing [34] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S. [35] S. Robertson, S. Walker, S. Jones, [36] S. E. Robertson, S. Walker, S. Jones, [37] J. J. Rocchio. Relevance feedback in information [38] M. Sahami and T. Heilman. A web-based kernel [39] G. Salton and C. Buckley. Term weighting approaches [40] G. Salton and C. Buckley. Improving retrieval [41] G. Salton and M. McGill. An Introduction to Modern [42] F. Sebastiani. Machine learning in automated text [43] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and [44] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building [45] A. Spink, D. Wolfram, B. Jansen, and T. Saracevic. [46] C. Sutton and A. McCallum. Composition of [47] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, [48] E. M. Voorhees. Query expansion using [49] C. Wang, P. Zhang, R. Choi, and M. D. Eredita. [50] J. Xu and W. B. Croft. Query expansion using local [51] J. Xu and W. B. Croft. Improving the effectiveness of [52] W.-t. Yih, J. Goodman, and V. R. Carvalho. Finding [53] J. Zobel and A. Moffat. Exploring the similarity space.
