 1. Introduction
In the last years, machine learning, data mining and pattern recognition have represented a very important tool for different academic and study areas ( Uraikul et al., 2007 ; Jarmulak et al., 2001 ; Francesca et al., 2006 ; Masnata and Sunseri 1996 ; Vicen et al., 2004 ; Carvalhoa et al., 2006 ; Specht, 1990 ). Even though a lot of practical details for specific use of these methods must still be studied and implemented, the aforementioned techniques are also becoming of fundamental importance for technological and indus-trial applications. One of the industrial areas where the artificial intelligence and pattern recognition methods have shown success when applied are the NDE (non-destructive evaluation) methods ( Jarmulak et al., 2001 ; Francesca et al., 2006 ). Here, Artificial Neural Network (ANN) technology for detecting patterns in high complex data has been widely used and, similarly, ANNs have been successfully incorporated to a number of nondestructive methods such us Ultra Sound ( Masnata and Sunseri, 1996 ; Vicen et al., 2004 ) and Flux Leakage ( Carvalhoa et al., 2006 ) techniques. One the most effective ANNs for classification tasks is the Probabilistic Neural Network (PNN) proposed by Specht (1990 ). The PNN is intrinsically a multiclass classification method, its Bayesian surface for optimal classification being made as complex as necessary; it allows to classify vectors with a high number of PNN has been suggested ( Frean, 1990 ; Tsai, 2000 ). In particular,
Wang et al. (2009) show that, in order to determine the optimal configuration, one of the best choices is to use the wrapper approach for features selection combined with the PNN. classification are the Deep Belief Networks (DBN) ( Hinton et al., 2006 ). The DBN increase significantly the efficiency of the classifica-tion by creating a model of the non linear feature interconnection.
This method could be particularly important for systems which interconnected features such as physical phenomenon. However, the DBN proposes generic algorithm of feature interdependence search which is independent of the nature of the data involved. In this work we do not use the DBN algorithm but another method that search for the features dependen cy considering that they have a local interdependency that could be modeled by the power spec-trum function of small sections. This can be done because we have knowledge of the magnetic signal behavior in this case the Magnetic Barkhausen Noise (MBN) which is the input of our system.
Vector Machine (SVM) with good clas sification rates for a long range of data types ( Begg, 2005 ; Smola and Scholkopf, 2004 ). This method has the advantage of present good results for complex distributed classes with a large number if features. The main disadvantages are the complexity of its implementati on, particularly the optimization of the quadratic form in the dual problem, the analysis of the kernel sometimes becomes obscure and confusing including the search of kernel optimal parameters. Additionally, the method may become very slow when the data base is big. The results of the classification system proposed in this work are compared with PNN, Radial Basics Functions (RBF) and SVM classification outcomes.
 stage where the PNN is optimizes, by means of the feature selection method, and uses the trash-recycling process to suggest which samples should be re-measured in order to eliminate misclassifica-tions. In the online stage input values are classified according to previous data and misclassified samples are separated in order to be analyzed later. This method should not be confused with the wake-sleep algorithm of the DBN ( Hinton et al., 2006 ) which is of different nature. (7). Using the data of the feedback samples the pattern data is 2.2. Configuration of the data set
Signal features that are commonly used in magnetic non-destructive studies are the values of the magnetic flux at some position ( x , y ) or at some time instant t . However, if one starts from the probability principle that all features should be considered as independent events in order to apply postulates of the probability theory, then the mentioned approach presents some limitations. This last assertion is due to the fact that there is actually no independence between neighbor events and succes-sive events in electromagnetic phenomena. In the typical feature independent formulation, the probability of a feature sequence D  X  D 1 , D 2 , ... , D n fg is given by: p  X  D  X  X  where p ( A i ) is the probability of the features i , A i being the value of the magnetic measurement (in our case, MBN envelope) at the time instant t i . However, physical studies have demonstrated that the MBN signal, at some time instant t , is correlated to the MBN signal at the previous time instant t 1. In the case of MBN signals, one possibility is modelling this phenomenon using a physical model ( Taisuke et al., 2010 ) or using a DBN. Never-theless, using the previous knowledge of the MBN phenomenon, another approach can be formulated considering that neighbors features are related through a model M . This method will reduce the number of interactions that should be performed if we applied the several layer structure of the DBN. In this case we already know the way in which features are most probably interconnected is through nearest neighbors due to special type of MBN signals.

The according to the condition probability formula, if there is data D , and a model M that generates or correlated the elements of this data, then: P  X  D , M  X  X  P  X  M = D  X  P  X  D  X  X  2  X  where P  X  D , M  X  X  P  X  D \ M  X  is the join probability of the data, D , and the model, M , P  X  M = D  X  is the probability to see M given D . One way of representing the model is through the Fourier transform which gives the data harmonic distribution. It is clear that P  X  M = D  X  r 1 because there could be several models for the same data depending on how its elements D 1 , D 2 , ::: , D n fg are arranged. Therefore: P  X  D , M  X  r P  X  D  X  X  3  X 
There is information in the D about the data values and additional information in M about data harmonic distribution of the data. The information contained in the join set of the data and the model D \ M will be higher that the information contained only in the D . This can be seen also using the information theory (6). Obtain neighbors features NEIG  X  CBF(Norm(BOUND k )) and (7). if NFS 4 NFS i then go to step 5. (8). Apply a SFBS algorithm with NE(SBF)  X  1: for ( i  X  1; i o  X  (9). The SFFS and the SFBS include a loop to test the smoothing first to the last feature of the original network is performed using blocks of predefined size, the rate of classification of the PNN is tested, using vectors composed only by the selected set of features (the pattern as well as the training vectors). Features that give the highest classification rates are saved into the new feature set and deleted from the original one.
 themselves are used to obtain the new classification rates, step (5).
If classification rate increases, all these new used features are saved and step (5) is repeated until the next neighbor do not increases the classification rates. The directio nforthesearchofnewneighborsis the direction of the maximum r W  X  X  @ W =@ t  X  ,  X  @ W =@ o  X  , that of the set obtained for a different configuration of the pattern and training data. This configuration can be obtained by random permutation of data. Another way to reduce the feature incon-sistency is to use the value W /STD as the target function, where W is the classification rate and STD is the standard deviation of features in the pattern layer. In this case, the optimal features will present less standard deviation as well as higher classification rate. 2.4. Feature probabilistic neural network
Fig. 3 shows a typical PNN structure. The network is composed by: Input layer that does not perform any operation and is formed by the input vectors x ; the Pattern layer that calculates the Euclidean distance between the input vector x and the pattern vector of each class. Output of the Pattern layer is obtained by evaluating this distance as argument for a Radial Basic Function, which is usually a Gaussian function. The Feature Probabilistic Neural Network is designed using the set of features obtained by means of the Feature Selection Algorithm and by the PNN with isotropic covariance matrix S . 2.5. The trash-recycling algorithm
The aim of this algorithm is to determine, which is the sample that must be studied (re-measured in the laboratory) in order to have the highest classification achieved in the next cycle. It is clear that the possible  X  X  X andidate X  X  samples must be misclassified samples. Thus, they must be located in the region of  X  X  X oubt X  X , are given by the following condition: or min P max  X  x miss  X  P j  X  x miss  X  r p d j  X  1 , ... , M , j a max  X  8  X  is the maximum value of P j ( x ), M is the number of classes and p d is Fig. 5 presents the trash-recycling algorithm: This figure shows one misclassified sample point x mis , being P probability of this point belonging to class B , and P d their difference.

The circle ( Fig. 5 ) is the representation of the hyper-sphere of diameter 2 d in which center is the misclassified sample. The higher the number of points inside the sphere, the higher the possibility of this sample becoming the recycling sample. This criterion will be called: (i)  X  X  X entroid point criterion X  X .
Alternatively, we will use two other criteria in order to establish, by comparison among them, the best method. These other criteria, we named (ii)  X  X  X urthest point criterion and (iii)  X  X  X earest point criterion X  X , being:
The Furthest point criterion based on the condition: P d ( X furthest )  X  min( P d ( X mis )).

The Nearest point criterion based on the condition: P d ( X nearest )  X  max( P d ( X mis )).

In other words, in the furthest point criterion the recycled sample is the sample for which the probability of belonging to all the classes is similar. These samples are undoubtedly nearer the center of the class interception. On the other hand, in the nearest point criterion, the recycle sample is the sample having the highest probability of belonging to one of the possible classes, so it must be located in the border of one of the possible classes. 3. Results and discussions 3.1. Dependence of the MBN on carbon content
Fig. 6 shows the dependence of the Vrms of the MBN for different carbon contents and different plastic deformations:
It can be seen, from this figure, that it is not possible to establish a one-to-one relation between Vrms and the parameters related to plastic deformation and carbon content.

Fig. 7 (a) X (c) present the dependence of the MBN envelope for different conditions of carbon content and plastic deformation.
It is remarkable the fact that the dependence of the MBN on plastic deformation increases with carbon content and also that the MBN signal, at negative magnetic fields, is shifted toward lower field values as plastic deformation increases ( Perez-Benitez and Padovese, 2011a , 2011b ). This fact means that, even when it is difficult to find a precise pattern for this behavior, there is physical information that could be used by the system for classification of samples 3.2. Results of feature selection algorithm applied to MBN signals classifications
The objective is to classify MBN signals measured from samples where two parameters (carbon content and plastic deformation) changes simultaneously. Thus, the MBN signal could corresponds to a sample with any of the possible 18 combination of the two parameters. We have to establish which features of MBN signals allow a better classification of these signals. Fig. 8 shows the feature classification rate of a MBN envelope. This figure is obtained after applying the SFFS algorithm to one MBN envelope (step 1 X 3). It is remarkable that features corre-sponding to negative field values present the highest rates of classification. This fact corroborates the physical analysis that we have previously suggested: the reverse domain process contains significant information about the combined influence. Fig. 9 shows a sequence of Feature Selection steps considering the envelope and spectrogram as input data are summarized in Table 1 . The results presented in this table were obtained after the optimization of all the parameters for each one to the methods, PNN, RBF, MSVM and FPNN.
 The CPU training time is higher for SFFS and even higher for FPNN (Feature PNN, PNN with neurons obtained after Feature Selection algorithm) algorithm s than PNN and MSVM CPU training time. However, the resulting topology of PNN requires higher computer memory than the resulti ng topology of SFFS and of FPNN. SFFS consumes less CPU training time than the FPNN algorithm. The MSVM presents high classification rate than PNN and SFFS and even than the FPNN using the MBN enve lope as input, low memory use and average CPU time. However, the FPNN using spectrogram as input presents the highest classification rate of all, up to 95%.
The statistical analysis of the results shows that the fractional uncertainties of the CPU time and the classification rate are very low, less than 4% of the best estimate. In this case the absolute uncertainties were taken as d  X  2 s where s is the standard deviation ( s t for CPU time and s r for rate classification), which means that the uncertainties are reported with a 95% of con-fidence. The comparison between the best classification rate of FPNN (95.2 7 0.6%) and the second best result which corresponds rates vs. the number of recycled samples curves, corresponding to 100 different pattern-test configurations, in order to take into account feature inconsistency. In this figure the best estimates of the classification rates and the uncertainties were represented.
The uncertainties d  X  2 s with 95% confidence are represented as error bars. The standard deviation of classification rates obtained point, nearest point and furthest point criterion, respectively.
This results indicates that centroid point criterion gives best classifications rates and also presents a higher classification consistence than the other methods.
 methods present almost the same increase of classification rate per recycled sample. However, the classification rate values of the proposed method are always higher than those of the other methods. 4. Conclusions classifier, the FPNN. The results show that the SVM presents good classification rate for this data. However, the proposed method
FPNN, present the highest classification rate when the spectro-gram is used as input data. The reason may be related to the fact that SVM is considered a general propose classification and regression method. If the data also contains information in relation between features the FPNN seems to be more effective in considering this additional information for classification than the SVM. Even though, this work is not intended to create a general purpose classification method better than PNN or SVM but the most efficient method for classification of nondestructive data of magnetic nondestructive device according to the present results. It seems that the maximum gradient method known as cluster growing method that search for the best features, increases the changes of success in the Feature Search in signal generated by physical events such as magnetic signals, because in these phenomena the nearest-neighbor events tend to be corre-lated. The system proposed present another contribution: the trash-recycling algorithm. This algorithm allows selecting the best candidate for feedback the system pattern, increasing con-tinuously the performance of the system. The proposed feedback algorithm also showed significant increase in the classification rate when used in combination with the  X  X  X entroid point criterion X  X  for the selection of feedback samples.

