 Samuel J. Gershman sjgershm@princeton.edu Matthew D. Hoffman mdhoffma@cs.princeton.edu David M. Blei blei@cs.princeton.edu Approximate posterior inference X  X stimating the con-ditional distribution of hidden variables given some observations X  X s an important problem in many set-tings. In this paper, we develop a new variational inference algorithm for complex probabilistic mod-els. Compared to traditional variational methods, our method can capture more expressive distributions and be applied to a wider class of models.
 Variational inference methods define some restricted family of distributions over the hidden variables  X  and try to find the member of that family that is closest to the posterior. The family is chosen so that the problem of finding the distribution q that best approximates the posterior becomes a tractable optimization problem. Variational methods are effective and widely used. These methods usually find a unimodal approxima-tion of the posterior, especially when the variational family is the commonly chosen mean-field family (Jor-dan et al., 1999; Beal, 2003). Such an approximation is inadequate when the posterior is multimodal. Further-more, variational inference algorithms are challenging to derive for models that lack conditional distributions in tractable exponential families (i.e., for models with-out conditional conjugacy).
 We develop a variational inference method for continu-ous hidden variables that captures multimodality and can be applied to many non-conjugate models. The variational family is a mixture of Gaussians, where the variational parameters are the locations and vari-ances of each mixture component. This family of dis-tributions resembles classical kernel density estimators from nonparametric statistics (Silverman, 1986). To approximate the variational objective function, we use Taylor series approximations of the log joint distribu-tion and a bound on the entropy. We call this method nonparametric variational inference (NPV). In con-trast to traditional unimodal variational distributions, the multiple components of the mixture can capture different aspects of the posterior.
 While mixture approximations have been studied in the variational inference literature (Bishop et al., 1998; Jaakola &amp; Jordan, 1998), we develop this idea into a more generally applicable framework. (We discuss other approaches to mixture approximations in Sec-tion 4.) NPV is  X  X eneral X  in the sense that it is not tailored to a specific model, only requiring that the first and second derivatives of the log joint probabil-ity log p (  X ,y ) be computable. Thus, it can be used in non-conjugate settings, i.e., where conditionals of the the individual hidden variables cannot be computed, such as in Bayesian models with non-conjugate pri-ors. While previous methods for variational inference in non-conjugate models rely on mathematics tailored to the problem at hand, NPV is easily adapted to many settings.
 In the following sections, we describe the variational objective function using this family and a general-purpose algorithm to approximately optimize it. We illustrate its performance on two models. First, we show that it performs as well in Bayesian logistic re-gression as the method of Jaakkola &amp; Jordan (2000), which is tailored to that specific model. Second, we show that it outperforms several MCMC methods for a non-conjugate matrix factorization model of brain activity data (Gershman et al., 2011). Nonparametric variational inference is a promising strategy for ap-proximating posterior distributions in complex proba-bilistic models. We consider the problem of computing the posterior distribution of hidden variables  X   X  R D given observed data y , This computation is analytically intractable for many models of interest because the denominator is difficult to compute.
 The idea behind variational methods is to approxi-mate p (  X  | y ) with a distribution q (  X  ) that belongs to a constrained family of distributions, indexed by a vari-ational parameter (Jordan et al., 1999; Beal, 2003). The goal is to choose a member of that family that is  X  X losest X  to the posterior. In variational inference, closeness is measured by Kullback-Leibler (KL) diver-gence, Thus, inference becomes an optimization problem: we choose the variational parameter to minimize the KL divergence. The family of distributions is chosen to make this optimization tractable.
 The KL divergence is difficult to optimize because it requires knowing the distribution that we are trying to approximate. In variational inference, we maximize an objective that is equal to the negative KL divergence plus a constant. Recall that KL[ q (  X  ) || p (  X  | y )]  X  0. We define a lower bound on the log marginal likelihood (evidence) log p ( y ) through the relation where is the negative free energy, also known as the evidence lower bound (ELBO). Here H [ q ] is the entropy of q and f (  X  ) = log p ( y, X  ). The ELBO is equal to the negative KL divergence plus the marginal distribution of the ob-servations, which is constant with respect to the family q . It therefore reaches a maximum when p (  X  | y ) = q (  X  ), where the KL is zero. Note that this is only attainable when the target posterior p (  X  | y ) is in the variational family, which it usually is not. Typically, q will be con-strained to a family of simpler distributions, and F [ q ] is optimized to find the distribution in this family that is closest (in KL) to the true posterior.
 The most commonly used variational inference algo-rithm is mean-field variational inference. Mean-field methods find q from the family of factorized poste-riors: q (  X  ) = Q i q i (  X  i ), where it is often convenient to choose q i (  X  i ) to have the same functional form as the conditional distribution p (  X  i |  X   X  i ,y ). When p (  X  chosen to be conjugate to p ( y |  X  ), the calculus of vari-ations leads to closed-form coordinate ascent updates that converge to a local maximum of F [ q ] (Beal, 2003). Despite the computational convenience of the mean-field approximation, it can be overly restrictive if there are strong dependencies between the hidden variables in the posterior distribution. Moreover, the closed-form updates are only available when using conjugate priors; many likelihood models of interest, such as lo-gistic regression and the multilayer perceptron, cannot be paired with conjugate priors, making the applica-tion of mean-field methods more difficult. We now consider a flexible family of variational ap-proximations that admits an efficient inference algo-rithm. Our algorithm is appropriate for models with continuous-valued hidden random variables, and does not require conjugacy between pairs of variables. We choose the distribution q (  X  ) to be a uniformly-weighted Gaussian mixture with isotropic covariances, where  X  n is the mean of the n th Gaussian component and  X  2 n is its variance. We call this a  X  X onparamet-ric X  family: We are making a weak set of assumptions about the shape of the posterior, since the Gaussian mixture family can approximate arbitrarily complex posteriors given a sufficient number of components. Further, this family resembles kernel density estima-tors used in classical nonparametric statistics (Silver-man, 1986), with  X  n playing the role of a kernel center and  X  2 n playing the role of a bandwidth parameter. 3.1. The Evidence Lower Bound If q is in the family defined by Eq. 5, we cannot com-pute the ELBO F [ q ]; in general there is no closed-form expression either for the expectation of a non-linear function under a Gaussian distribution or for the entropy of a mixture of Gaussians. However, we can approximate the ELBO and optimize this approx-imation (see Lawrence, 2000; Honkela et al., 2007, for other approaches to this problem). First, we lower bound the entropy term H [ q ]. Then, we approximate the expected log joint E q [log p ( y, X  )].
 We lower bound the entropy (the first term in Eq. 4) using Jensen X  X  inequality (Huber et al., 2008), Each integral in Eq. 6 is the sum of N convolved Gaus-sians, each component convolved with the n th. We obtain the final bound by using the fact that the con-volution of two Gaussians is another Gaussian, We now turn to the expected log joint f (  X  ), which is the second term in Eq. 4, We approximate each term in this sum with a second-order Taylor series expansion of f (  X  ) around  X  n , where H n =  X  2  X  f (  X  ) is the Hessian matrix of second derivatives. The approximate expectation is This approximation is known as the multivariate delta method for moments (Bickel &amp; Doksum, 2007), and is often used within variational inference schemes for models that cannot exploit conjugacy (e.g., Braun &amp; McAuliffe, 2010).
 Finally, we add the bound in Eq. 7 to the approxima-tion in Eq. 9. This gives the approximate ELBO 1 Intuitively, the likelihood term, f (  X  n ), encourages placing samples in areas of high probability den-sity, while the entropy term, log q n , penalizes  X  X ver-crowded X  locations (i.e., where many samples are near each other). The Hessian term captures the local cur-vature of the posterior, discouraging the algorithm from placing samples in areas with high probability density but low volume (and therefore low mass). We note two attractive properties of the approximate ELBO in Eq. 10. First, we have made no conjugacy assumptions; our only requirement is that the log joint f (  X  ) = log p (  X ,y ) is twice differentiable (or thrice dif-ferentiable if one wishes to use gradient ascent; but see below). Second, although the objective function involves a Hessian term, it only requires the calcula-tion of the diagonal components; the cost of computing the diagonal of the Hessian is comparable to the cost of computing the gradient. 3.2. Optimizing the ELBO Eq. 10 is a tractable approximation of the ELBO in Eq. 4. Our goal is now to maximize Eq. 10 with respect Algorithm 1 Nonparametric variational inference Input: data y , number of components N .

Initialize  X  1: N randomly. repeat until change of L 2 [ q ] is less than 0.0001. to the variational parameters  X  n and  X  n . One option is to use a gradient-based solver. However, there is a serious computational problem with this approach X  computing the gradient of Eq. 10 requires computing a matrix of third derivatives, since we must compute the gradient of the Hessian trace Tr( H n ). This leads to a cost that is quadratic in the number of parameters. To avoid the calculation of third derivatives, we use both first-and second-order approximations of the ELBO. The first-order approximation is This is obtained in the same way as Eq. 10, but us-ing a first-order approximation of f (  X  ), rather than the second-order approximation in Eq. 8. We iterate between optimizing the variances  X  using the second-order approximation in Eq. 10 and optimizing the means  X  using the Eq. 11. Each optimization is done using L-BFGS. We found that it is more efficient to op-timize L 1 [ q ] with respect to one mean at a time, hold-ing the others fixed, and iterating over components. This coordinate ascent procedure converges faster than batch optimization of  X  1: N , but coordinate and batch optimization produce similar results. Our algorithm is summarized in Algorithm 1.
 Both L 1 [ q ] and L 2 [ q ] are approximations of F [ q ]. Split-ting the optimization problem into these two steps al-lows us to avoid the cost of calculating the gradient of  X  2 n 2 Tr( H n ) with respect to the means  X  . In our ex-periments, 3 iterations typically proved sufficient to achieve convergence. Although the first-order approx-imation may appear drastic, it still achieves our main goal: placing kernels in areas of high probability mass. Further simulation work is needed to assess the trade-offs involved in this approximation.
 As an illustration, we constructed a synthetic multi-modal  X  X osterior X  f (  X  ) using a mixture of skewed bi-variate t -distributions. Figure 1 shows f (  X  ) alongside the NPV approximation with several settings of N . With N = 1, the approximation is only able to cap-ture a single mode, but with N = 2 it is able to cap-ture the two modes with high fidelity, though it can-not capture the true covariance structure or the heavy tails. With N = 10, the approximation better cap-tures the skew by placing several low-variance com-ponents along the diagonal. This illustration demon-strates some strengths and weaknesses of the NPV ap-proximation: it can capture multi-modality, but the isotropic covariance of the components makes it diffi-cult to capture skew in the posterior. This problem can be ameliorated by using more components.
 Note that the number of parameters that need to be fit with NPV increases linearly with N (the number of components in the mixture). This may pose challenges for models with a large number of hidden variables. On the other hand, it may only be necessary to use a small number of components (e.g., less than 10) to cap-ture the major aspects of the posterior (as suggested by Figure 1). We note also that the KL divergence between the mixture distribution q and the true pos-terior decreases at best logarithmically in the number of mixture components N , suggesting that there may be diminishing returns to using very large values of N (Jaakola &amp; Jordan, 1998). 3.3. Relationship to other algorithms The NPV objective relates to several other methods. When there is one component N = 1, the entropy term log q 1 does not depend on the mean  X  1 , and when  X  2 1 becomes sufficiently small, the Hessian term of Eq. 10 goes to 0. Consequently, the NPV objective when N = 1 and  X  1  X  0 is L [ q ] = log p ( y, X  ) + const . = log p (  X  =  X  | y ) + const . The maximum of this function is the maximum a pos-teriori (MAP) solution.
 When N = 1 and  X  2 1 is allowed to vary, we obtain a Gaussian approximation centered around the MAP solution. This can be understood as a diagonalized Laplace approximation (MacKay, 1995), i.e., where we ignore correlations between the dimensions of  X  . The Laplace approximation has drawbacks: for example, it is not invariant to reparameterization, it performs badly when the mean and mode of the posterior are far apart, and it cannot capture multiple modes (Beal, 2003).
 When N &gt; 1 and  X  2 n  X  0, we obtain a quasi-Monte Carlo approximation of the posterior, q (  X  ) = located at  X  n . Thus one way to look at the NPV al-gorithm is as a deterministic sampling method. Approximate inference for non-conjugate models is an active area of research. Some authors have used numerical or Monte Carlo methods to approximate intractable integrals. For example, Lawrence et al. (2004) used importance sampling to approximate the expectations required for inference in a Bayesian model of microarray images. Ihler et al. (2009) general-ized particle filtering for approximate inference in fac-tor graphs with continuous variables. Honkela et al. (2007) used numerical quadrature to approximate ex-pectations in a nonlinear factor analysis model. These techniques are useful, but may fail in high dimensions. Several researchers use specialized approximations for certain classes of models, such as those with logistic nonlinearities (e.g., Jaakkola &amp; Jordan, 2000; Khan et al., 2010). In contrast, our goal is to develop an al-gorithm for inference in general non-conjugate models with continuous hidden variables.
 Closely related to our method is the mixture mean-field (MMF) method (Bishop et al., 1998; Jaakola &amp; Jordan, 1998; Lawrence, 2000), which models the pos-terior as a mixture of mean-field approximations. Re-cently, Bouchard &amp; Zoeter (2009) revisited this ap-proach using soft-binning functions. NPV can be viewed as a special case of MMF because each com-ponent factorizes into a collection of one-dimensional Gaussian sub-components (due to the isotropic covari-ances). Our innovation is that we exploited the func-tional form of the Gaussian mixture to derive an effi-cient approximate inference algorithm. NPV requires no user input beyond specifying the joint likelihood function, its gradient, optionally the diagonal of its Hessian, and the number of components. These mod-est requirements give NPV a practical advantage in situations where it is difficult to derive the MMF up-dates. In this section, we apply the NPV algorithm to several probabilistic models and compare its performance to other widely-used methods. 5.1. Logistic regression In this section, we ask whether NPV produces rea-sonable approximations for models where closed-form updates can be applied. We focus on a hierarchical logistic regression model and compare its accuracy to a standard variational treatment (Jaakkola &amp; Jordan, 2000, henceforth  X  X J X ).
 Generative model . The observed data y = { c , X } consist of T binary class labels, c t  X  { X  1 , 1 } , and K covariates for each datapoint, x t  X  R K . The hidden variables  X  = { w , X  } consist of K regression coeffi-cients w k  X  R , and a precision parameter  X   X  R + . We assume the following model (MacKay, 1995): Here a and b are hyperparameters (shape and inverse scale, respectively) that we assume to be fixed. Results . We evaluated NPV and JJ on 13 binary classification data sets compiled by Mika et al. (1999). 2 The number of covariates in these data sets ranges from 2 to 60, and the number of observations ranges from 24 to 7400. We used split-half training/testing. We used the following hyperparameter settings: a = 1, b = 0 . 01, N = 5 (similar results were obtained with N = 10).
 The predictive distribution for NPV was approximated using a Monte Carlo estimate. We drew 1000 samples from the fitted variational mixture of Gaussians and estimated the log-likelihood of the test data as an av-erage of the log-likelihoods under each sample. Figure 2 (top) compares the log-likelihood of the test data under the NPV and JJ approximations. NPV and JJ achieve statistically indistinguishable accuracy. Figure 2 (bottom) shows the same comparison for the ELBO, confirming that NPV closely mimics the JJ approxima-tion. We emphasize that JJ exploits special properties of the generative model (i.e., a clever lower bound on the logistic sigmoid function), whereas NPV only uses the derivatives of the joint distribution.
 We also fit the model using an MCMC algorithm, Hamiltonian (or Hybrid) Monte Carlo algorithm (HMC; Neal, 2011), which takes the same inputs as NPV (the log joint probability and its gradient). HMC uses the gradient of f (  X  ) to efficiently explore the pos-terior, making it one of the most effective samplers for models with continuous variables. With 1000 samples, we found that this algorithm predicts held-out data significantly worse ( p &lt; 0 . 00001, Wilcoxon signed-rank test) compared to NPV and JJ. Presumably the infe-rior performance of HMC could be improved by run-ning the sampler for longer, but this would result in greater computational overhead. 5.2. Topographic latent source analysis We now study our method with a more complicated model, for which standard variational algorithms are inapplicable. We apply the NPV approximation to a nonlinear latent variable model of functional magnetic resonance imaging (fMRI) data. Data from fMRI ex-periments contain measurements of brain activity that are collected while a subject performs a task, such as labeling images. The goal of these experiments is to understand the relationship between cognitive pro-cesses and brain activity. One reason this problem is complicated is that fMRI data is spatial. Brain activ-ity is measured in 3D brain-space (a grid of  X  X oxels X ). Measurements made on nearby voxels are dependent. Gershman et al. (2011) developed a factorization model of spatial patterns in fMRI data, topographic la-tent source analysis (TLSA). TLSA decomposes voxel activations into a set of spatial functions (topographic latent sources). These functions are related to task and cognitive variables (called  X  X ovariates X ) through a weight matrix that is also inferred from the data. We can evaluate the quality of a fitted model by us-ing it to predict held-out brain data, conditional on covariates. Unlike traditional probabilistic matrix fac-torization models, TLSA is not conditionally conju-gate and closed-form mean-field inference is not avail-able. Gershman et al. (2011) approximated the pos-terior with MCMC, but their method was too slow to analyze large data sets.
 Generative model . Each datapoint t in an fMRI ex-periment consists of a vector of V voxel activations, u t  X  R V , and a vector of C covariates, x t  X  R C . The intuition behind TLSA is that the spatial orga-nization of voxel activations arises from a small num-ber of anatomically localized brain regions involved in processing the task. Formally, TLSA decomposes the voxel activations into a covariate-dependent superpo-sition of K latent sources: where tv  X  N (0 , X   X  1 ) is a Gaussian noise term, w ck is a weight that specifies how covariate c influences source k , and g kv is the activation of source k in voxel v . This generative process (illustrated in Figure 3) can be viewed as a probabilistic matrix factorization model where { g k } are basis images that are combined to produce the observed neural activity.
 Each basis image is constructed by evaluating a pa-rameterized spatial basis function at each voxel loca-tion. Following Gershman et al. (2011), we chose this function to be a radial basis function with parameters  X  k = {  X  r k , X  k } : where  X  r k  X  [0 , 1] M is the source center (in normalized coordinates),  X  k  X  R + is a width parameter, and r v  X  [0 , 1] is the location of voxel v . In the notation of Section 2, the observed variables are y = { X , U , R } and the hidden variables are  X  = { W , G } .
 To complete the generative model, we placed the fol-lowing priors on the parameters: w ck  X  X  (0 , X  2 w ) ,  X  r kd  X  Beta(1 , 1) ,  X  k  X  Exp(  X  ) . In all our analyses, we used the following hyperparam-eter settings:  X  = 1 , X  2 w = 5 , X  = 1.
 Results . We fit TLSA to data collected by Mason and Just (unpublished), involving subjects viewing words. Each word was either the name of a type of tool or of a type of building (i.e., there were 2 classes), and the subject X  X  task was to think about the word and its properties. There were a total of 84 trials per subject (see Gershman et al., 2011, for more details). We re-stricted our analysis to a 1,323 voxels (a single slice of the brain activity data) from a single subject. We trained the model on one half and then generated predictions of the neural data for the other half, con-ditioning on the test covariates. For NPV, we approxi-mated the predictive distribution using a Monte Carlo estimate, as described in the previous section. As an il-lustration of the model fits, Figure 4 shows the average data and reconstructions derived from the MAP esti-mate and the NPV approximation. While the MAP estimate captures the global pattern of activity, each component of the NPV approximation captures small idiosyncrasies that may be difficult to extract using a single point estimate. In other words, the NPV ap-proximation captures several local maxima of the pos-terior; we next show that this translates into better predictive accuracy.
 We evaluated the quality of the reconstruction by calculating the mean-squared reconstruction error of held-out neural data, a quantity proportional to the negative log-likelihood of the held-out data. We also fit TLSA using HMC (see above); we collected 5000 sam-ples, keeping the last 200 for the predictive distribu-tion. We repeated this procedure for the Metropolis-Hastings (MH) sampler used in the original TLSA pa-per (Gershman et al., 2011). The results are shown in Figure 5. NPV works well with a varying number of components (though best when N &gt; 3), substantially outperforming the MAP and MCMC estimators.
 We re-emphasize here that TLSA is non-conjugate, and hence MMF cannot be applied without using specially-tailored approximations (Lawrence, 2000). Note that while both MH and HMC are asymptoti-cally guaranteed to perfectly approximate the poste-rior, these algorithms require tuning and are often slow to converge. In our experiments, NPV was about 3 times faster than HMC.
 We developed an approximate inference method for posteriors that do not necessarily enjoy the conjugacy properties that make common variational approxima-tions (e.g., mean-field) possible. Our algorithm is easy to apply to new probabilistic models; all that is re-quired is the likelihood function and its gradient (a requirement shared by many other algorithms, includ-ing MAP estimation and HMC). When applied to a hierarchical logistic regression model, we found that NPV incurs little loss in accuracy compared to a more specialized variational algorithm (Jaakkola &amp; Jordan, 2000). We further showed, using a nonlinear latent variable model of fMRI data, that NPV can find an approximation of the posterior that improves predic-tive performance over MAP estimation and MCMC. NPV has limitations. First, it assumes a simple ap-proximating family. This could be improved by in-troducing a full covariance matrix into the component distributions or by allowing the components to be non-uniformly weighted. Further, NPV only applies to con-tinuous variables. We plan to extend it to models with discrete hidden variables.
 In summary, NPV is a posterior inference algorithm that is a step towards generically applicable variational approximations. The need for such approximations is increasing, as researchers begin to explore more and more complicated probabilistic models to cope with the increasing complexity of large data sets. Our hope is that by employing generic inference algorithms, the hard work of inference can proceed  X  X nvisibly, X  and re-searchers can devote more time to testing and refining the assumptions of their models.

