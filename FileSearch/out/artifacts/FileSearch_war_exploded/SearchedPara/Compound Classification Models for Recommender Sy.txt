
Recommender systems recommend products to cus-tomers based on ratings or past customer behavior. With-out any information about attributes of the products or cus-tomers involved, the problem has been tackled most suc-cessfully by a nearest neighbor method called collaborative filtering in the context, while additional efforts invested in building classification models did not pay off and did not increase the quality. Therefore, classification methods have mainly been used in conjunction with product or customer attributes.

Starting from a view on the plain recommendation task without attributes as a multi-class classification problem, we investigate two particularities, its autocorrelation struc-ture as well as the absence of re-occurring items (repeat buying). We adapt the standard generic reductions 1-vs-rest and 1-vs-1 of multi-class problems to a set of binary clas-sification problems to these particularities and thereby pro-vide a generic compound classifier for recommender sys-tems. We evaluate a particular specialization thereof us-ing linear support vector machines as member classifiers on MovieLens data and show that it outperforms state-of-the-art methods, i.e., item-based collaborative filtering.
Recommender systems are online information systems that recommend products to customers, i.e., perform some sort of automatic selling in e-commerce, or more gener-ally, recommend information items to users, e.g., books to library users, research papers to citeseer users, courses to students etc. Contrary to static lists like best sellers, special offers, editor X  X  choice, etc. recommender systems typically are personalized and targeted at the individual customer. To achieve personalization, they make use of customer profiles consisting of explicit and implicit product ratings. In com-puter science, the term recommender system often is used synonymously with collaborative filtering, in the context of information retrieval they are also known as relevance feed-back.
 A typical recommendation scenario is shown in fig. 1. The system contains a set of users and items, partial rat-ing information by users about items as well as information about attributes of users and items. The task is to predict further ratings of users or recommend new items that will achieve high ratings by users.

This problem has been handled in two different ways in the research literature so far: (i) using heuristic correlation measures and a simple nearest neighbor method called col-laborative filtering, and (ii) using learning methods to train a classification model that predicts further ratings or rated items. Already in an early publication [4], the nearest neigh-bor methods outperformed the classification models on the plain problem without item or user attributes. Until now the reasons for the failure of the classification models are not well understood. One would expect, that the investment in a more complex learning method pays off if applied cor-rectly. Since then, several attempts have been made to put classification methods to work for recommendation tasks, mostly by using either information about item or user at-tributes (sometimes called content-based and demographic filtering), or by building probabilistic models customized for the problem. But many solutions turned out inferior, many solutions never have been compared to collaborative filtering at all or not on the well-known data sets such as MovieLens.

In this paper we will thoroughly re-investigate standard classification models for recommender systems. We will identify two specific properties of the problem that make straight-forward applications of classification models fail: the intrinsic autocorrelation structure and the absence of item re-occurrences (repeat buying). Furthermore we will adapt the well-known multi-class model setup strategies 1-vs-rest and 1-vs-1 to reflect these particularities, providing a generic framework for using any binary classifier for rec-ommendation generation. Finally, we will evaluate a com-pound 1-vs-rest and 1-vs-1 classifier using linear SVMs as member models and show that the 1-vs-1 model outper-upper-right table cointains attributes of items. forms the state-of-the-art methods, i.e., item-based collabo-rative filtering.

We will shortly review related work in the following sec-tion, give a formal outline of the different recommendation tasks in sections 3 and 4, and describe some baseline and state-of-the-art solutions for the problem that do not use learning methods in section 5. In section 6 we describe dif-ferent model setups that turn the recommendation task in a classification problem, i.e., the adaptation of the 1-vs-rest and 1-vs-1 multi-class setups, and in section 7 provide some experimental support for the superiority of our approach. Finally, we conclude with some remarks on open problems.
While having their very early roots in relevance feed-back in information retrieval [17] and adaptive hyperme-dia [19], recommender systems gained momentum once they had been formulated as filtering techniques, generally grouped in three different types: (i) collaborative filtering is basically a nearest-neighbor model based on user X  X tem cor-relations; if correlations are computed between users, it is called user-based , if between items, it is called item-based . (ii) content-based or feature-based recommender systems use similarities between rated items of a single user and items in the repository. User-and item-based collaborative filtering and content-based recommender systems have been introduced in [9, 16], [18] and [2], respectively, and are ex-emplified by the three systems presented there, MovieLens, Ringo, and fab. (iii) Hybrid recommender systems try to combine both approaches [2, 5, 15, 20].

While the so-called content-based and hybrid methods try to integrate classifiers based on item and/or user at-tributes with collaborative filtering techniques, two further approaches have been taken: (iv) recommender systems have been viewed as classification problems [3, 4, 12] and different model classes have been tried, but either classifi-cation models did not improve quality much or they have not been compared to strong collaborative filtering models such as item-based.

Finally, (v) special probabilistic models have been built to catch the particularities of the recommendation task [6, 14, 10, 1]. Again, either attributes are used or results do not improve much on collaborative filtering or results are not compared to state-of-the-art collaborative filtering methods; and anyway, specialized models do not allow for a pluggability of the learning component as our compound model does.

So still, item-based collaborative filtering models claim to provide the best published results on some of the public data sets such as MovieLens [7].
Generally, the data used in recommender systems can be described by (i) a set of modes (users/customers/agents, (ii) the attributes of these modes (demographic informa-(iii) a partial cube of ratings for combinations of instances All real-life systems we are aware of as well as most re-search literature focuses on just two modes, users and items, as the sparsity of the rating matrix already is a problem for two-modal data and would be even more severe if more modes are considered.

Let U and I be sets of uninterpreted elements (e.g., inte-gers), called users and items , respectively. Let S  X  R be the set of possible ratings, e.g., S = { 1 , 2 , 3 , 4 , 5 higher values indicate a stronger liking, and a partial function that associates ratings to user/item pairs, i.e., r is defined only for some, in general not for all pairs ( u, i ) for a user u  X  U and an item i  X  I , as users typically rate only small subsets of items. We denote the set of pairs r is defined for as its domain dom r  X  U  X  I . In data sets, r typically is represented as a list of tuples ( u, i, r ( u, i often a timestamp is available as a fourth field providing in-formation about the actual order in which ratings have been entered by users.

We can distinguish two different tasks that should be ac-complished by recommender systems: (i) predict the ratings , i.e., given the rating matrix r (ii) predict the rating events , i.e., given the rating ma-
In the literature, rating events typically are grouped by user: events, i.e., items, are predicted for each user, as rec-ommender systems typically work in a pull-scenario like an online shop or an information portal where we have no control about which users are there. In push scenarios like mailings grouping by items also would make sense, e.g., if we look for customers to approach about a given product.
At first sight, it might look as if the second problem can be addressed by solutions of the first problem: when we al-ready have a function to predict ratings, we predict events (items for a given user) by decreasing predicted rating. This makes perfect sense in many applications where we want to recommend only items that users will like, not items, we expect them to rate and eventually not like, especially, as in applications the predicted rating is not shown. But ob-viously, having access to both predictions, the rating and the rank of an item for a user, allows us to implement addi-tional services like issuing warnings (high rank, low rating) and stress recommendations for items that are unlikely be found by the user (low rank, high rating). Furthermore, in evaluations in the lab, using rating predictions for rank pre-dictions will only work if there is a very strong correlation between the rating and the item occurrence probability: in published data sets such as MovieLens such a correlation can be observed, but it might be overlaid by other effects, as, e.g., incomplete information (we do not know in advance if we will like a movie) and variety seeking customer behav-ior. Furthermore, the second problem is of importance on its own as in some applications ratings are collected implic-itly by customer behavior, e.g., the web pages viewed or the products put in a market basket or bought, and no explicit ratings are available.

So both problems should be treated on their own. If events should be predicted, it is a good idea to discard the actual rating values, i.e., r becomes constant 1 . Please re-member that r is a partial function and only recorded for observed pairs. In dense matrix representations r often is encoded by two values, 0 and 1 :but 0 does not mean that we observed a counterexample, but just that there is a miss-ing value.

Recommender tasks can be further described by the availability of information on attributes of users and items. As already mentioned, most recent approaches that aimed at using classification models for recommender systems tried to gain advantage from taking attributes into account. But (reliable) attributes for customers and  X  in some domains such as community-driven information portals or product domains with high fluctuations in the assortment and a high heterogeneity of products (like auctions)  X  also for items may be hard to come by. In these cases we will have to resort to methods that work without attributes.
In the following, we will address the most simple rec-ommendation task, as we think that it is at the heart of the problem: (i) we do not consider ratings, but just events, i.e., unary data and prediction of ranks of items, (ii) we do not consider attributes of items or users, but just atomic entities. Data for recommender systems with users and items without attributes simply can be described by a multiset T X  X  ( I ) of subsets of items called transactions in the fol-lowing. A recommender system for users and items without attributes can be described by where ranking ( I ) denotes the set of all bijective functions I  X  X  1 ,..., | I |} that map each item to its unique rank on the recommendation list. For n  X  N let be the set of items recommended at ranks 1 to n.

Such recommender systems are evaluated by a test multi-set of split transactions, T test  X  X  ( I )  X P ( I ) and a number n of relevant rank positions counting as hits by the usual recall measure:
In this evaluation scenario, precision is forced by tak-ing into account only a restricted number n of recommen-dations, so that there is no need to evaluate precision or F1 measures: with fixed n , precision (and thus F1) is just the same as recall up to a multiplicative constant. Alter-natively, evaluations could either (i) take into account the whole ranking, but weight down hits at lower ranks [4] or (ii) expect recommender systems to adapt the number of recommendations given by themselves and measure then full recall, precision, and F1. All three possibilities are not unproblematic as (i) depends on the choice of a decay con-stant and (ii) does not take into account the ranks at all. We choose recall of clipped rankings because it reflects some specifics of recommender web applications where a fixed number of products are shown per page, because it is the simplest one and because it is used frequently in recom-mender systems literature.  X  Please note that for a fair comparison we have to make sure that all recommender sys-tems actually provide at least n recommendations; if not, ranks have to be filled by a fallback system.

In predicting ranks of items we have to take into account an additional property of most recommender systems: typ-ically they are used to call users attentions to new items users do not know yet and have not rated already in the past, which may be just a desired feature of the system or be due to the fact that there is no repeat-buying in domains like books, movies, music etc. in which these systems typically operate, such that recommending already known items does not make much sense economically. There are, of course, other domains, such as food stores and broadcast media like radio and TV, where re-occurring events play a crucial role and thus have to be modeled. We will stick to domains with-out re-occurring events. All recommender systems should take advantage of this information and tweak their recom-mendation list by removing all items that already occurred in the past of a test case.
To assess the quality of recommendations found by our classification method, we compare it to a baseline model and one of the state-of-the-art methods.

We compare user-specific recommender systems to a (al-most) constant recommender system that provides (almost) the same recommendations to all users (sometimes also called  X  X ost popular X ). For this, items are recommended by decreasing total frequency in the training data. Recom-mendations may vary from user to user a little bit, as items already rated are removed from the constant list. The scores of this model will tell us, what we can achieve already with-out personalization and therefore, when we compare to per-sonalized systems, how large the benefit from personaliza-tion actually is.

Furthermore, we compare to a simple, non-learned near-est neighbor classifier, called item-based collaborative fil-tering. Depending on a parameter k  X  N called neighbor-hood size , it proceeds in three simple steps: (i) Compute item correlations C := ( corr ( i, j )) i,j  X  I (ii) Sparsify C by keeping only the highest k entries in (iii) For a test case X  X  X  ( I ) compute item scores via A more detailed description can be found in [7].
Classification models for recommending items get as training data just a multiset T X  X  ( I ) of itemsets contain-ing the items a user has already rated in the past. They have to solve two problems: (i) how to split the training data into cases and (ii) to learn a multi-class classifier.
If timestamps for ratings are available, training transac-tions can be split, s.t. the ordering is taken into account: each transaction ( x 1 ,...,x k ) can be split at k different po-sitions m  X  X  1 ,...,k } and a split at position m in past ( x 1 ,...,x m  X  1 ) and future ( x m ,...,x k ) comprises k  X  m 1 cases (( x 1 ,...,x m  X  1 ) ,x m ) , (( x 1 ,...,x m  X  1 ... , (( x 1 ,...,x m  X  1 ) ,x k ) .

If no timestamp information is available or considered not important  X  what we will do in the following  X , there are 2 k splits in past and future and several cases made from each split.

As there obviously are too many possible splits, we will restrict to the k splits that take out just one item as fu-ture, i.e., (( x 1 ,...,  X  x i ,...,x k ) ,x i ) where  X  x the item is dropped from the list.

Once cases have been built, the problem is converted to a usual multi-class classification problem with one binary predictor variable X i for each item i  X  I and a nominal tar-get variable Y with | I | different values, one for each item. In analogy to time-variant scenarios, where the target vari-able is predicted based on former states of itself, and in ac-cordance with some literature from relational data mining [13] we call such a model an autocorrelation model.
When looking for methods to learn classifications from this data, we can either (i) restrict ourselves to methods that can handle multi-class classification tasks intrinsically (such as decision trees or special versions of SVMs) or (ii) use a generic model setup that allows to use a broader va-riety of learning methods. As the focus of this paper is not on a particular method, but on solving the problem gener-ically, we choose the second possibility that employs stan-dard methods to reduce the multi-class problem to a set of binary problems.

Two such methods are used in the literature (see, e.g., [11]): 1-vs-rest and 1-vs-1 model setups. In a 1-vs-rest model setup we build | I | different classifiers, one for each class. The classifier for class i  X  I is trained with the full training data, where cases that belong to class i are re-labeled as positive (or i ), while cases that belong to any other class are re-labeled as negative (or rest). The com-pound classifier is applied by computing the probability of each class by its member classifiers and then sort the items be decreasing probability.

To adapt a 1-vs-rest model setup to autocorrelation mod-els, one can simplify: (i) work on transactions, not on cases split in advance:
Figure 2. 1-vs-rest model setup adapted to autocorrelation models for i =49 . (ii) As the predictor X i is missing in all cases of classifier Simplification (i) reduces the number of cases from the total number T  X  X  | T | of item occurrences to the number |T| of transactions dramatically.

As binary member classifier any classification method could be chosen, e.g., logistic regression, decision trees, support vector machines, neural networks etc.

Alternatively, in a 1-vs-1 model setup, one builds one classifier for each pair ( i, j ) of competing target classes i, j  X  I . Each member classifier is trained on all cases that belong either to class i or to class j , all other cases are discarded.

Also the general 1-vs-1 model setup has to be adapted to fit autocorrelation models (see fig. 3): (i) we have to remove predictor variables X i and X j for (ii) when we apply a standard 1-vs-1 model setup to split 1-vs-rest compound models contain only | I | member models, while 1-vs-1 compound models contain | I | ( | I | X  1) / 2 different member models. But in usual non-autocorrelation scenarios, both model setups 1-vs-rest and
Figure 3. 1-vs-1 model setups adapted to autocorrelation models for competing classes. 1-vs-1 train on almost the same number of cases in total: let C denote the original number of cases, then 1-vs-rest trains on | I | X  C cases, as each case is used in each classifier ei-ther as positive or negative example, while 1-vs-1 trains on ( | I | X  1)  X  C cases, as each case is used in the classifier for each of the | I | X  1 competing classes. Also, in the literature both model setups are seen as competitive [11].

For the adaptations to autocorrelation models, it is differ-ent. Here, 1-vs-rest compound models are trained on only | I | X | U | many cases, while 1-vs-1 compound models still have to be trained on ( | I | X  1)  X  C cases minus some cases that stem from correlated items. So 1-vs-1 compound mod-els are expected to be much slower for autocorrelation sce-narios.
We build two different classifiers for recommending items without attributes: (i) a 1-vs-rest compound model and (ii) a 1-vs-1 compound model, both making use of a SVM with linear kernel as member model. We used lib-svm [8] as work horse to train the member models. We compared these models with the simple constant baseline as well as with the state-of-the-art method for recommender systems, item-based collaborative filtering [7]. The latter is claimed to give the best published results on our eval-uation dataset without taking into account attributes. Ties of the item-based collaborative filtering method were bro-ken by the ranks of the constant system, ties in the constant system by smallest index. The neighborhood parameter of the item-based collaborative filtering method was chosen to give optimal results on the test data (neighborhood size 20) by means of a grid search over all neighborhood sizes in steps of 5. So the item-based collaborative filtering system used optimal parameters and thus was granted a small ad-vantage: its score is more an upper bound than a fair quality measure, but we expect that a calibration of the size of the neighborhood on the training data gives almost the same re-sults.
 We evaluated on the classical MovieLens 100k dataset. 1 It contains 100,000 ratings on a 5-point scale from 943 users on 1,682 items. The publicly available version is a subsam-ple of a larger data set, where only users with at least 20 ratings have been retained.

As experimental setup we have chosen a leave-one-out split in training and test data (sometimes also called all-but-one) that was done at random. We used the recall measure that takes into account only the n =10 top-ranked items for each user. For leave-one-out splits there is no difference between micro and macro averaged recall values.

In fig. 4 results are shown. The x -axis restricts the set of competing items: competing items are added from left to right by decreasing total frequency; the x -axis shows the cumulative relative frequency of competitive items consid-ered. The right-most values of each curve give the final result: the figures for the constant and the item-based col-laborative filtering system are in line with published results. Additionally, one can see that the constant system achieves its recall almost completely on the 10 most frequent items  X  it still improves a little bit on less frequent items, as items already in the past are removed from the recommendation list, that allows also items below position 10 to be shown. The item-based collaborative filtering system improves con-siderably on less frequent items until the items that make approx. 60% of all item occurrences in the dataset are used, from less frequent items it cannot improve much.

The outcome of the two classification methods is a sur-prise: the 1-vs-rest model completely fails and achieves a score that is way below the constant system. On the other hand, the 1-vs-1 model clearly outperforms the item-based collaborative filtering method. The sharp edge at approx. 60% competitive item occurrences stems from the fact, that we stopped at 240 most-frequent items due to time restrictions (see below about runtime). So actually, we ex-pect the curve to rise even further a little bit.
Figure 4. Recall on 10 top-ranked items for a) constant, b) item-based collaborative fil-tering, c) 1-vs-rest compound classifier us-ing SVMs, and d) 1-vs-1 compound classifier using SVMs.

The failing of the 1-vs-rest model can be explained by the autocorrelation structure and the absence of re-occurring items. 1-vs-rest tries to learn the absolute concept of class i , but all its positive examples cannot be of class i in the evaluation as there is no re-occurrence of items and i has already been rated, and all transactions that eventually can contain item i later on are explicitly used as negative exam-ples. So in a sense it learns just the wrong way. Contrary, 1-vs-1 tries only to learn differences between the concepts of two classes: and as there are many original transactions dropped that contain neither of the two classes considered, these transactions are not labeled as negative examples.
A characteristics of the recommendation task is the very high number of classes: in the example, we have 1682 dif-ferent movies that play the role of a class. Most multi-class problems have only a comparatively small number of classes, such as 10 for classifying hand-written digits or 26 for hand-written letters. For 1682 classes we have to build 1,413,721 pairwise classifiers. As there are 100,000 item occurrences in total, the sum of all training set sizes is ap-prox. 168,200,000 cases. Even if we could build a pair clas-sifier in a second, we would need approx. 16 days to train the system. Therefore, we restricted the pairwise classifier to the 240 most-frequent items, i.e., trained 28,680 pairwise classifiers, each taking approx. 3s on average on a standard Linux box (Intel 2.4 GHz, 1.5 GB RAM), resulting in a total training time of approx. 1 day for the whole system.  X  As we have already seen in section 6, the requirements for the 1-vs-rest classifier are much more modest; it was build in a few hours.
In this paper, we have argued, that the basic recommen-dation problem, i.e., recommending items without having access to any attributes of items or users, can be viewed as a classification task. It features a special structure, where the same variable occurs in both roles, as predictor and target variable, what we called autocorrelation model in analogy with similar phenomena in other contexts (time-variant and relational data).

We showed that the standard methods for reducing a multi-class problem to a set of binary problems cannot be used literally for autocorrelation models, but have to be adapted slightly. This adaptation leads to a completely dif-ferent complexity of these two approaches, contrary to the non-autocorrelation case. Furthermore, we have seen that for scenarios without item re-occurrence (repeat buying), the faster 1-vs-rest classifier completely fails, as it learns to predict past items that cannot occur anymore and treats transactions to which an item eventually is added explicitly as negative examples.

But the 1-vs-1 classifier clearly outperforms the state-of-the-art methods in this domain and for our test data set, item-based collaborative filtering.

Finally, runtime became a real issue as learning the 1-vs-1 classifier takes days, while building the collaborative filtering correlation matrix takes seconds. Further research has to address this problem: beneath obvious possibilities as parallelization that is trivially accomplished for build-ing a set of pairwise classifiers, more sophisticated meth-ods could be thought of, such as, e.g., building hierarchical classifiers only for a restricted set of contrasts that are in-duced by a taxonomy on the items. Once runtime issues are fixed, handling recommendation tasks as classification tasks will allow to use the whole data mining machinery to tackle the problem, especially, integrating attributes might then be done in a less ad-hoc and heuristic manner than before.
