 Social media have given birth to a new fo rm of marketing known as electronic word-of-mouth marketing ( eWOM ) [9]. There is also an increasing trend of users on social media to express their opinions about various companies and their prod-ucts. This phenomenon is particularly evident on Twitter 1 due to its real-time nature, and hence tweets serve as a significant repository for a company to mon-itor its online reputation and thereby t ake the necessary steps to tackle threats to it. Furthermore, such real-time social streams (e.g. Twitter) have motivated a whole new area of research known as Online Reputation Management. A funda-mental task within Online Reputation Management is continuous  X  X onitoring X  of social streams for early identification of topics that may have an impact (ei-ther positive or negative) on the reputat ion of an entity of interest. The RepLab Monitoring Task at CLEF 2012 [1] introduced a task to tackle the  X  X onitoring X  problem in tweets within the context of Online Reputation Management, the main characteristics of which are:  X  clustering tweets based on their top ics: for example, the company Apple  X  ordering tweets by priority to the company: the idea is that tweets critical
In this paper we focus on the task of  X  X onitoring X  tweets for a company X  X  rep-utation, in the context of the RepLab2012, where we are given a set of companies and for each company a set of tweets, which contain different topics pertaining to the company with different levels of priority. Performing such a monitoring of tweets is a significantly challenging task as tweet messages are very short (140 characters) and noisy. We alleviate these problems through the idea of core term expansion in tweets. We perform the two phases of clustering and priority level assessment separately, where the clustering applies an unsupervised technique, while a supervised technique is used for priority level assessment.

The rest of the paper is organized as fo llows. Section 2 pres ents a description of the problem in more detail. Section 3 pr esents an overview of work related to ours along with a description of how we differ from past approaches. Section 4 presents our technique for clustering and a ssigning priority levels to the clusters. Section 5 describes the experiments and finally Section 6 concludes the paper. In this section we briefly define the probl em statement related to this contri-bution. It is important to outline that this contribution aims at presenting a formal method that was evaluated (but never formally presented in a previous publication) within a CLEF campaign. We have a stream of tweets for different companies collected by issuing a query corresponding to the company name. The stream of tweets for the companies were then divided into a training set and a test set. In the training set each stre am of tweets for a company was clustered according to their topics. Furthermore, these clusters were prioritized into five different levels as follows: Alert &gt;average priority &gt;low priority &gt; X  X ther X  cluster &gt; X  X rrelevant X 
The  X  X lert X  category corresponds to th e cluster of tweets that deserves imme-diate attention by the company. Likewise, the tweet clusters with average and low priority deserve attention as well but with relatively less urgency than those with alert priority level. The label  X  X t her X  refers to a cluster of tweets that are about the company but that do not qualify as interesting topics, and that are negligible to the monitoring purposes. Finally,  X  X rrelevant X  labels the cluster of tweets that do not refer to the company.

Our task is to cluster the stream of unseen tweets (test set) of a given company with respect to topics. Fur thermore, we have to assign these clusters a priority level chosen from the above-mentioned five levels. The analysis and organization of tweet messages is a rapidly evolving research area with a significant amount of interest from the academic community [6]. The relative freshness of the area leads to new problems being defined almost every day 2 . This section attempts to present an overview of the huge body of works on tweets X  content mining while at the same time explaining how we differ from existing works. Within the area of mining tweets the proposed works are organized into the following cate gories: a) event detection via tweets, b) sentiment analysis of tweets, and c) su mmarization and detection of topics in tweets.

Given the real-time nature of textual data in Twitter streams, a lot of work has been done to extract real-world events from these streams [3] [8] [12] [15]. Most of these techniques formulate event detection as a classification problem and use various features within tweets s uch as keywords occurring frequently in bursts, context surrounding events, and temporal signals such as tweet volume within a time interval. Some different event detection approaches however utilize word or topic clustering based [5] [17] approaches. Event detection approaches cannot be directly applied to the task of monitoring a company X  X  reputation as it is often the case that something that is beneficial or detrimental to the reputation of a company is not a major event in most of the cases.

Sentiment analysis of Twitter messages has gained significant attention over the past few years [4] [10] [11] and consists in classifying tweets with respect to their polarities (positive, negative) or to their subjectivities. The sentiment anal-ysis approaches for Twitter utilize a machine learning framework largely relying on use of emoticons and hashtags as indications of polarity and sentiment. De-spite the overlaps between the company reputation monitoring task and tweets X  sentiment analysis task there are some differences between the two which make traditional sentiment analysis techniques unsuitable for the task at hand. We explain with the help of example tweet s in Table 1; it can be seen that the ex-ample tweets for the three companies Marriott, Lufthansa and Apple contain no obvious expression of sentiment (subjectivity) and yet they fall into the  X  X lert X  priority level according to labels assigned by reputation management experts.
Topical analysis of tweet messages has also gained increasing attention over the past few years. The works in this direction involve use of latent dirichlet allocation ( X  X DA X ) to form topical repres entations of tweet content [7] [13] [18]. One limitation of such topical based representations is however their application to authors of tweets whereby a single author X  X  tweets are aggregated to help build a topic profile for that particular Twitter user. We argue that such approaches are not well-suited to the task under consideration on account of the fact that tweets expressing opinions about companies X  reputation is independent of the users expressing it and often come from a diverse number of sources. The proposed method solely involves processing the tweets X  contents, i.e., it does not use any external knowledge resource such as Wikipedia or the content of any Web page. Before applying our method we expand the shortened URL mentioned inside a tweet into a full URL in order to avoid redundant link forwarders to the same URL. In the following subsections we present the strategy we used for the monitoring tweets X  task. 4.1 Pre-processing In this section we describe two necessary pre-processing steps. We first perform both of them so that the outputs from these steps can be used in the main algorithm that we explain later. 4.1.1 Tweet Core Terms Extraction In the first step we extract core terms (i.e., important terms) from each tweet in the training and test set so as to be able to identify a topic. To achieve this goal, we filter out the trivial components from each tweet X  X  content such as mentions, RT, MT and URL string. Then, we apply POS tagging to identify the terms having a label  X  X N X ,  X  X NS X ,  X  X NP X ,  X  X NPS X ,  X  X J X  or  X  X D X  as a core term. For the purpose of POS tagging, we use the Stanford POS tagger [16] which achieves an accuracy of 80% on tweets [14] and such an accuracy serves well for the task at hand. 4.1.2 Training Priority Scores for Core Terms In this step each core term extracted f rom the training set is associated with multiple weights, which represent the strength of association of the core term with each priority level. The applied algorithm is shown in Procedure 1. We employ the training data, where each cluster of tweets is labelled with a priority level. First we associate each tweet in a cluster with the label of that cluster i.e., we borrow the label from the tweet X  X  cluster as shown in Step 1 of Procedure 1 (here, a label implies a priority level out of the five priority levels discussed in Section 2). Then we count the number of occurrences of each core term in tweets belonging to a priority level (label) as Steps 2-7 show. The intuition is that if a core term is frequently appearing in tweets that belong mostly to one unique priority level, then that core term is very likely to belong to this level. Procedure 1. Assigning priority scores to core terms 4.2 Main Algorithm Once the preliminary steps have been completed, the algorithm described in this section is applied to cluster the stream of tweets (test set) with respect to their topics, and to assign them a priority level. The algorithm iteratively learns two threshold values i.e., the content threshold and the specificity threshold ( content threshold and specificity threshold in Procedure 2 and 3 respectively) from a list of threshold values provided to it as explained in the following subsections. Clustering. We cluster the tweets according to t he similarity of their contents, to the specificity of core terms in tw eets, and to common URL mentions in tweets. We explain these three clustering phases as follows: 1. An initialization phase of clusters is applied according to tweets X  content 2. The second phase aims to cluster tweet s according to the specificity of core 3. In the final step of clustering according to URL similarity we exploit the Procedure 2. Cluster initialization Predicting Priority Levels. In this step, we assign a priority level to each cluster. To this aim, we first estimate a pri ority level for each tweet in the corpus, and then by using the assignment of priority level to the tweets we decide a priority for each cluster. The process is explained here below.
 Estimation of Priority Level for Each Tweet: First, we generate five ag-gregations across each priority level for a tweet. Then, the highest aggregation corresponding to a priority level become s the priority level for that tweet. Each aggregation is computed by aggregating each core term X  X  priority score (as esti-mated in Section 4.1.2) corresponding to the priority level for that tweet. Procedure 3. Cluster expansion via specificity of core terms in clusters Procedure 4. findExpandSimilarityScore() Estimation of Priority Level for Each Cluster: Since each cluster is com-posed of tweets, the assigned priority level for a tweet is counted as a vote for a cluster X  X  priority level, and the priority level that gets the maximum number of votes (for a cluster) becomes the priority level for that cluster.
 Global Error Estimates and Optimization. This step enables the algorithm to learn optimized threshold values. To this aim, we estimate the global error as follows. We first estimate the number of errors per cluster by counting the num-ber of inconsistencies (i.e., non-uniformity) among the priority levels assigned to the tweets of a cluster. Here, inconsisten cies refers to mismatch among priority levels for each tweet in the cluster; note that the priority levels assigned to each tweet come from the priority assigned to the individual core terms as explained previously. We finally aggregate these errors estimates across each cluster to de-fine a global error estimate. The threshold values across which the global error estimation is minimum are declared to be optimized threshold values. Note that the threshold values are optimized again for each company in the test dataset. The output corresponding to the optimized threshold values is reported as the final output of the algorithm.
 5.1 Data Set We performed our experiments by using the data set provided by the Monitoring task of RepLab2012 [1]. In this data set 37 companies were provided, out of which six were in the training set, while the remaining 31 were in the test set. For each company a few hundred tweets were provided with the language of the tweets being English and Spanish; furthermore, most of the tweets are in Spanish and we translate a tweet that is not written in English using the Bing Translation API 4 . 5.2 Evaluation Measures The measures used for the purposes of evaluation are Reliability and Sensitivity, whicharedescribedindetailin[2].
In essence, these measures consider two types of binary relationships between pairs of items: relatedness  X  two items b elong to the same cluster  X  and priority  X  one item has more priority than the other. Reliability is defined as preci-sion of binary relationships predicted by the system with respect to those that derive from the gold standard. Sensitivity is similarly defined as the recall of relationships. When only clustering relationships are considered, Reliability and Sensitivity are equivalent to BCubed Precision and Recall [2]. 5.3 Results and Discussion Table 2 presents a snapshot of the official results for the Monitoring task of RepLab, where CIRG_IRDISCO is the name of our team. It shows that our algorithm performed competitively, and is the second from the top. In addition, our algorithm shows the best BCubed precision for the clustering of tweets com-pared to other algorithms. Our algorithm did not perform well for the priority level assignment to clusters on account of extremely scarce training data 5 .Table 3 shows the details of individual companies for which our proposed technique beats the systems submitted by other teams. The top three companies in the table perform well because o f the presence of similar companies in the training set: Telefonica and Yahoo! have tweets similar in nature to Apple, while Ferrari has tweets similar in nature to Armani 6 . Given these encouraging results, we believe the algorithm to be quite promising for the task of  X  X eal-time monitoring X  in social streams with the availability of even a modest amount of training data.
Another interesting observation we mad e with respect to per entity (company) evaluation results is that our algorithm suffers due to translation issues 7 .This is particularly obvious for entities with high percentage of English tweets e.g., our algorithm performed best for the entity RL2012E11 (Bing) which had 98% of tweets in English and RL2012E28 (Chevrolet) which had 96.75% of tweets in English (shown as bottom two rows of Table 3). We proposed an algorithm for clustering tweets and for assigning them a priority level for companies. Our algorithm did not make use of any external knowledge resource and did not require prior information about the company. Even under these constraints our algorithm showed competitive performance. Our approach for monitoring  X  X eal-time social streams X  may open a promising new dimension as witnessed by the evaluation results.

