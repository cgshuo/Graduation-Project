 Lemmatisation and part-of-speech (POS) tagging are necessary steps in automatic processing of lan-guage corpora. This annotation is a prerequisite for developing systems for more sophisticated au-tomatic processing such as information retrieval, as well as for using language corpora in linguistic re-search and in the humanities. Lemmatisation is es-pecially important for processing morphologically rich languages, where the number of different word forms is too large to be included in the part-of-speech tag set. The work on morphologically rich languages suggests that using comprehensive mor-phological dictionaries is necessary for achieving good results (Haji X c, 2000; Erjavec and D X zeroski, 2004). However, such dictionaries are constructed manually and they cannot be expected to be devel-oped quickly for many languages.

In this paper, we present a new general approach to the task of lemmatisation which can be used to overcome the shortage of comprehensive dictionar-ies for languages for which they have not been devel-oped. Our approach is based on redefining the task of lemmatisation as a category tagging task. Formu-lating lemmatisation as a tagging task allows the use of advanced tagging techniques, and the efficient in-tegration of contextual information. We show that this approach gives the highest accuracy known on eight European languages having different morpho-logical complexity, including agglutinative (Hungar-ian, Estonian) and fusional (Slavic) languages. Lemmatisation is the task of grouping together word forms that belong to the same inflectional morpho-logical paradigm and assigning to each paradigm its corresponding canonical form called lemma. For ex-ample, English word forms go, goes, going, went, gone constitute a single morphological paradigm which is assigned the lemma go . Automatic lemma-tisation requires defining a model that can determine the lemma for a given word form. Approaching it directly as a tagging task by considering the lemma itself as the tag to be assigned is clearly unfeasible: 1) the size of the tag set would be proportional to the vocabulary size, and 2) such a model would overfit the training corpus missing important morphologi-cal generalisations required to predict the lemma of unseen words (e.g. the fact that the transformation from going to go is governed by a general rule that applies to most English verbs).

Our method assigns to each word a label encod-ing the transformation required to obtain the lemma string from the given word string. The generic trans-formation from a word to a lemma is done in four steps: 1) remove a suffix of length N new lemma suffix, L N  X   X  h N s , L s , N p , L p i defines the word-to-lemma transformation. Each tuple is represented with a label that lists the 4 parameters. For example, the transformation of the word going into its lemma is encoded by the label h 3 ,  X  , 0 ,  X  X  . This label can be observed on a specific lemma-word pair in the train-ing set but it generalizes well to the unseen words that are formed regularly by adding the suffix -ing . The same label applies to any other transformation which requires only removing the last 3 characters of the word string.

Suffix transformations are more frequent than pre-fix transformations (Jongejan and Dalianis, 2009). In some languages, such as English, it is sufficient to define only suffix transformations. In this case, all the labels will have N ever, languages richer in morphology often require encoding prefix transformations too. For example, in assigning the lemma to the negated verb forms in Czech the negation prefix needs to be removed. In this case, the label h 1 , t, 2 ,  X  X  maps the word nev  X  ed  X  el to the lemma v  X  ed  X  et . The same label generalises to other (word, lemma) pairs: ( nedok  X  azal , dok  X  azat ), ( neexistoval , existovat ), ( nepamatoval , pamatovat ). 1
The set of labels for a specific language is induced from a training set of pairs (word, lemma). For each pair, we first find the Longest Common Substring (LCS) (Gusfield, 1997). Then we set the value of N cede the start of LCS and N acters in the word that follow the end of LCS. The value of L lemma and the value of L ing LCS in the lemma. In the case of the example precede the LCS in the word and 1 follows it. There are no characters preceding the start of the LCS in the lemma and  X  t  X  follows it. The generated label is added to the set of labels. We apply the presented technique to induce the la-bel set from annotated running text. This approach results in a set of labels whose size convergences quickly with the increase of training pairs.
Figure 1 shows the growth of the label set size with the number of tokens seen in the training set for three representative languages. This behavior is ex-pected on the basis of the known interaction between the frequency and the regularity of word forms that is shared by all languages: infrequent words tend to be formed according to a regular pattern, while ir-regular word forms tend to occur in frequent words. The described procedure leverages this fact to in-duce a label set that covers most of the word occur-rences in a text: a specialized label is learnt for fre-quent irregular words, while a generic label is learnt to handle words that follow a regular pattern.
We observe that the non-complete convergence of the label set size is, to a large extent, due to the pres-ence of noise in the corpus (annotation errors, ty-pos or inconsistency). We test the robustness of our method by deciding not to filter out the noise gener-ated labels in the experimental evaluation. We also observe that encoding the prefix transformation in the label is fundamental for handling the size of the label sets in the languages that frequently use lemma prefixes. For example, the label set generated for Czech doubles in size if only the suffix transforma-tion is encoded in the label. Finally, we observe that the size of the set of induced labels depends on the morphological complexity of languages, as shown in Figure 1. The English set is smaller than the Slovene and Serbian sets. The advantage of structuring the lemmatisation task as a tagging task is that it allows us to apply success-ful tagging techniques and use the context informa-tion in assigning transformation labels to the words in a text. For the experimental evaluations we use the Bidirectional Tagger with Guided Learning pre-sented in Shen et al. (2007). We chose this model since it has been shown to be easily adaptable for solving a wide set of tagging and chunking tasks ob-taining state-of-the-art performances with short ex-ecution time (Gesmundo, 2011). Furthermore, this model has consistently shown good generalisation behaviour reaching significantly higher accuracy in tagging unknown words than other systems.

We train and test the tagger on manually anno-tated G. Orwell X  X   X 1984 X  and its translations to seven European languages (see Table 2, column 1), in-cluded in the Multext-East corpora (Erjavec, 2010). The words in the corpus are annotated with both lemmas and detailed morphosyntactic descriptions including the POS labels. The corpus contains 6737 sentences (approximatively 110 k tokens) for each language. We use 90% of the sentences for training and 10% for testing.

We compare lemmatisation performance in differ-ent settings. Each setting is defined by the set of fea-tures that are used for training and prediction. Table 1 reports the four feature sets used. Table 2 reports the accuracy scores achieved in each setting. We es-tablish the Base Line (BL) setting and performance in the first experiment. This setting involves only features of the current word, [ w form, suffixes and prefixes and features that flag the presence of special characters (digits, hyphen, caps). The BL accuracy is reported in the second column of Table 2).

In the second experiment, the BL feature set is expanded with features of the surrounding words ([ w ([ lem the second experiment are reported in the third col-umn of Table 2. The consistent improvements over the BL scores for all the languages, varying from the lowest relative error reduction (RER) for Czech (5.8%) to the highest for Romanian (31.6%), con-firm the significance of the context information. In the third experiment, we use a feature set in which the BL set is expanded with the predicted POS tag of the current word, [ pos in the third experiment (Table 2, column 4) shows consistent improvement over the BL (the best RER is 34.2% for Romanian). Furthermore, we observe that the accuracy scores in the third experiment are close to those in the second experiment. This allows us to state that it is possible to design high quality lemmatisation systems which are independent of the POS tagging. Instead of using the POS information, which is currently standard practice for lemmatisa-tion, the task can be performed in a context-wise set-ting using only the information about surrounding words and lemmas.

In the fourth experiment we use a feature set con-sisting of contextual features of words, predicted lemmas and predicted POS tags. This setting com-bines the use of the context with the use of the pre-dicted POS tags. The scores obtained in the fourth experiment are considerably higher than those in the previous experiments (Table 2, column 5). The RER computed against the BL varies between 28.1% for Hungarian and 66.7% for English. For this set-ting, we also report accuracies on unseen words only (UWA, column 6 in Table 2) to show the generalisa-tion capacities of the lemmatizer. The UWA scores 85% or higher for all the languages except Estonian (78.5%).

The results of the fourth experiment show that in-teresting improvements in the performance are ob-tained by combining the POS and context informa-tion. This option has not been explored before. Current systems typically use only the information on the POS of the target word together with lem-matisation rules acquired separately from a dictio-nary, which roughly corresponds to the setting of our third experiment. The improvement in the fourth experiment compared to the third experiment (RER varying between 12.5% for Czech and 50% for En-glish) shows the advantage of our context-sensitive approach over the currently used techniques.
All the scores reported in Table 2 represent per-formance with raw text as input. It is important to stress that the results are achieved using a general tagging system trained only a small manually an-notated corpus, with no language specific external sources of data such as independent morphological dictionaries, which have been considered necessary for efficient processing of morphologically rich lan-guages. Jur X si X c et al. (2010) propose a general multilingual lemmatisation tool, LemGen, which is tested on the same corpora that we used in our evaluation. LemGen learns word transformations in the form of ripple-down rules . Disambiguition between multi-ple possible lemmas for a word form is based on the gold-standard morphosyntactic label of the word. Our system outperforms LemGen on all the lan-guages. We measure a Relative Error Reduction varying between 81% for Serbian and 86% for En-glish. It is worth noting that we do not use manually constructed dictionaries for training, while Jur X si X c et al. (2010) use additional dictionaries for languages for which they are available.

Chrupa X a (2006) proposes a system which, like our system, learns the lemmatisation rules from a corpus, without external dictionaries. The mappings between word forms and lemmas are encoded by means of the shortest edit script . The sets of edit instructions are considered as class labels. They are learnt using a SVM classifier and the word context features. The most important limitation of this ap-proach is that it cannot deal with both suffixes and prefixes at the same time, which is crucial for effi-cient processing of morphologically rich languages. Our approach enables encoding transformations on both sides of words. Furthermore, we propose a more straightforward and a more compact way of encoding the lemmatisation rules.

The majority of other methods are concentrated on lemmatising out-of-lexicon words. Toutanova and Cherry (2009) propose a joint model for as-signing the set of possible lemmas and POS tags to out-of-lexicon words which is language indepen-dent. The lemmatizer component is a discrimina-tive character transducer that uses a set of within-word features to learn the transformations from in-put data consisting of a lexicon with full morpho-logical paradigms and unlabelled texts. They show that the joint model outperforms the pipeline model where the POS tag is used as input to the lemmati-sation component. We have shown that redefining the task of lemma-tisation as a category tagging task and using an ef-ficient tagger to perform it results in a performance that is at the state-of-the-art level. The adaptive gen-eral classification model used in our approach makes use of different sources of information that can be found in a small annotated corpus, with no need for comprehensive, manually constructed morphologi-cal dictionaries. For this reason, it can be expected to be easily portable across languages enabling good quality processing of languages with complex mor-phology and scarce resources. The work described in this paper was partially funded by the Swiss National Science Foundation grants CRSI22 127510 (COMTIS) and 122643.
