 In this paper we present a policy -based error anal y-sis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression ( Knight and Marcu, 2000; Turner and Charniak, 2005; McDonald, 2006; Clark and L a-pata 2006 ). 
Specifically , in typical statistical compression approaches , a simplifying assumption is made that compression is accomplished strictly b y means of wor d deletion. Furthermore, each sequence of co n-tiguous words that are dropped from a source se n-tence is considered independently of other sequences of words dropped from other portions of the sentence , so that the features that predict whether deleting a sequence of words is preferred or not is based solely on local considerations . This simplistic approach allows all possible derivations to be modeled and decoded efficiently within the search space, using a dynamic programming alg o-rithm. 
In theory, it should be possible to learn how to generate effective compressions using a corpus of source -target sentence pairs, given enough exa m-ples and sufficiently expressive features. Ho w-ever, our analysis casts doubt that this framework with its str ong assumptions of locality is suff i-ciently powerful to learn the types of example compressions frequently found in corpora of h u-man generated gold standard compressions regar d-less of how expressive the features are.

Work in sentence compression has be en som e-what hampered by the tremendous cost involved in producing a gold standard corpus. Because of this tremendous cost, the same gold standard corpora are used in many d ifferent published stud ies almost as a black box. This is done with little scrutin y of the limitations on the learnability of the desired target systems . These limitations result from i n-consistencies due to the subtleties in the process by which humans generate the gold standard compre s-sions from the source sentences , and from the stron g locality assumptions inherent in the fram e-works.

Typically, the humans who have participated in the construction of these corpora have been i n-structed to preserve grammaticality and to produce compressions by deletion. Human ratings of the gold stand ard compressions by separate judges confirm that the human developers have literally followed the instructions, and have produced co m-pressions that are themselves largely grammatical. Nevertheless, what we demonstrate with our error analysis is that they have used meaning preserving transformation that did n't consistently preserve the grammatical relations from the source sentence while transform ing source sentences into target sentences . This places limitations on how well the preferred patterns of compr ession can be learned using the current paradigm and existing corpora .
In the remainder of the paper, we discuss rel e-vant work in sentence compression . We then i n-troduce our policy -based error analysis technique. Next we discuss the error analysis it self and the conclusions we draw from it. Finally, we conclude with future directions for broader application of this error analysis technique. Knight and Marcu (2000) present two approaches to the sentence compression problem -one using a n oisy channel model and the other using a dec i-sion -based model. Subsequent work (McDonald, 2006) has demonstrated an advantage for a soft constraint approach , where a discriminative model learns to make local decisions about dropping a sequence of words fro m the source sentence in o r-der to produce the target compression . Features in this system are defined over pairs of words in the source sentence, with the idea that the pair of words would appear adjacent in the resulting co m-pression, with all intervening words dropped. Thus, the features represent this transformation, and the feature weights are meant to indicate whether the transformation is associated with good compressions or not. 
We use McDonald  X  X  ( 2006 ) proposed model as a foundation for our wo rk because its soft constraint approach allows for natural integration of a variety of classes of features , even overlapping features . In our prior work we have explored the potential for improving the performance of a compression system by including addi tional, more sophisticated syntactically motivated features than those i n-cluded in previously published models. In this p a-per, we evaluate the gold standard corpus itself using similar syntactic grammar policies. I n the domain of Sentence Compression, the co r-pus consists of source sentence s each paired with a gold standard compressed sentence. Most of t he above related work has been evaluated using the following 2 corpora, namely the Ziff -Davis (ZD) set ( Knight and Marcu, 2002 ) co nsisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) ( Clarke and Lapata, 2006 ) originally consist ing of 1 619 sentences , of which we used 1070 as the training set in our development work as well as in the error analysis below . Hence, we use these two popular corpora to present our work . W e hypothesiz e certain grammar policies that i n-tuitively should be followed while deriving the target -compressed sentence from the source se n-tence if the mapping between source and target sentences is p roduced via grammatical transform a-tions . The basic idea behind these policies grows out of the same ideas motivating the syntactic fe a-tures used in McDonald ( 2006 ) . These poli cies , extracted using the MST (McDonald, 2005) d e-pendency parse structure of the source sentence, are as follows: These grammar policies make predictions about where , in the phrase stru cture , constituents are likely to be dropped or retained in the compre s-sion . Thus, these policies have similar motivation to the syntactic features in the McDonald (2006) model . However, there is a fundamental difference in the way these po licies are computed. In the McDonald (2006) model , the features are co m-puted lo cally over adjacent words y i -1 &amp; y i compression and the words dropped from the original sentence between that word range y i -1 y . I n cases where the syntactic structure of the i n-volved words extends beyond this range , t he e x-tracted features are not able to cap ture all of the relevant syntactic dependencies . On the other hand, in our analysis the policies are computed globally over the complete sentence without specify ing any range of words. As an illustrative example, l et us consider the following sen tence from the CL Co r-pus (bold represents dropped words) : 1. The 1 leaflet 2 given 3 to 4 Labour 5 activists
According to Policy 2, since the verb 'mentions' is retained , the subject of the verb  X  X he leaflet X  should also be retained. In the McDonald ( 2006 ) model , by looking at the local range y i -1 = 5 and y = 7 for the verb 'mentions', we will not be able to compute whether the subject(1,2) wa s retained in the compression or not. So t his policy can be ca p-tured only if the global context is taken into a c-count while evaluating the verb 'mentions'.

Now we evaluate each sentence in the corpus to determine whether a particular policy was applic a-ble and if applicable then whether it was violated. Table 1 shows the summary of the evaluation of all the sentence s in the two corpora . Column 2 in the table shows the percentage of sentences in the ZD Corpus where the respective policies were applic a-ble. And column 3 shows the percentage of se n-tences where the respective policies were violated , whenever applicable. Columns 4 and 5 show r e-spective percentages for the CL corpus. In this section we discuss the results from evalua t-ing the 8 grammar pol icies discussed in Section 3 over the ZD and CL corpora , as discussed above . 
The policies were evaluated with respect to whether they applied in a sentence, i.e., whether the premise of the  X  X f ... then X  rule is true in the sentence, and whether the policy was broken when applied, i.e., if the premise is true but the cons e-quent is false. The striking finding is that for every one of the policies discussed in the previous se c-tion, they are violated for at least 10% of the se n-tences where they applied , and s ometimes as much as 72%. For most policies, the proportion of se n-tences where the policy is violated when applied is a minority of cases. Thus, based on this, we can expect that grammar oriented features motivated by these policies and derived from a syn tac tic analysis of the source and/ or target sentences in the gold standard could be used to improve the pe r-formance of compression systems that don X  X  make use of syntactic information to that extent. Ho w-ever, the noticeable proportion of violations with r espect to some of the policies indicate that there i s a limited extent to which these type s of feature s can contribute to wards improved performance.
One observation we make from Table 1 is that while the proportion of sentences where the pol i-cies (Column s 2 and 4) apply as well as the propo r-tion of sentences where the policies are broken when applied (Columns 3 and 5 ) are highly corr e-lated between the two corpora. Nevertheless, the distributions are not identical. Thus, again, while we predict that using t his style of dependency sy n-tax features might improve performance of co m-pression systems within a single corpus, we would not expect trained models that rely on these synta c-tic dependency features to generalize in an ideal way between corpora .
 Policy1 100% 34% 100% 14% Policy2 66% 18% 84% 18% Policy3 50% 10% 61% 24% Policy4 59% 59% 46% 72% Policy5 62% 17% 77% 27% Policy6 65% 22% 79% 29% Policy 7 57% 25% 58% 40% Policy8 55% 16% 58% 36% Beyond the above evaluation illustrating the extent to which g rammar inspired policies are violated in human generated gold standard corpora, interesting insights into challenges that must be addressed in order to improve performance can be obtained by taking a close look at typical examples from the CL corpus where the policies are broken in the gold standard corpora (bold represents dropped words) . In Sentence 1, retaining the dependent Noun stru c-ture of the dropped Preposition on in the PP vi o-late s Policy 7. Such a NP to Infinitive Phrase transformation changes the syntactic structure of the sentence. Sentence 2 also breaks several pol i-cies , na mely Policies 1, 4 and 7. The syntactic root has is dropped. Also the main verb has used is dropped while retaining th e Subject Annely . In Sentence 3, b reaking Policies 1, 2 and 4, the h u-man annotator s replaced the pronoun it with the noun Labor , the subject of a dropped verb  X  X as said X . Such anaphora resolution cannot be done without relevant context, which is not availa ble in strictly local paradigms of s entence c ompression. In Sentence 4, policies 3. 5 and 8 are violated. T ransformations like s ubstituting Nicholas X  X  me m-ory by the metonym Nicholas and popular move by popular n eed to be identified and analyzed. Such varie d tran s formations , made in the syntactic stru c-ture of the sentences by human an notators , are counter -intuitive, making them hard to be captured in the linear models learned in association with the syntactic features in current compression systems. In this paper we have introduced a policy -based error analysis technique that was used to invest i-gate the potential impact and limitations of adding a particular style of dependency parse features to typical statistical compressi on systems . We have argued that the reason for the limitation arises from the strong assumption of the local nature of the decisi ons that are made in obtaining the system -generated compression from a source sentence. 
O ther related technologies such as statistical machine translation and statistical paraphrase are based on similar paradigms with similar assum p-tions of the local nature of decisions that are made in the search for an acceptable deri vation. We co n-jecture both that it is likely that the same issues related to the construction of the gold stan dard corpora likely apply and that a similar policy -based error analysis approach could be used in order to assess the extent to which this is true and identify possible directions for improving perfo rmance . In our ongoing work , we plan to conduct a similar error analysis for these problems in order to eval u-ate the generality of the findings reported here. This work was funded in part by the Office of N a-val Research grant number N000 14510043 .
 Kevin Knight and Daniel Marcu. 2000 . Statistics -Based
