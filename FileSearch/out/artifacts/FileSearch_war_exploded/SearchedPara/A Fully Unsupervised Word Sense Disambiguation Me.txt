 In many natural languages, a word can represent multiple meanings/senses, and such a word is called a homograph. Word sense disambiguation(WSD) is the process of determining which sense of a ho-mograph is used in a given context. WSD is a long-standing problem in Computational Linguis-tics, and has significant impact in many real-world applications including machine translation, informa-tion extraction, and information retrieval. Gener-ally, WSD methods use the context of a word for its sense disambiguation, and the context informa-tion can come from either annotated/unannotated text or other knowledge resources, such as Word-Net (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihal-cea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V  X  eronis, 1998) many different WSD approaches were de-scribed. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006),  X  Dictionary and knowledge based methods.  X  Supervised methods. Supervised methods  X  Semi-supervised methods. To overcome the  X  Unsupervised methods. These methods acquire
Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in (Yarowsky, 1995). However, such approaches suffer a signifi-cant performance drop in practice when domain or vocabulary is not limited. Such a  X  X liff-style X  per-formance collapse is called brittleness, which is due to insufficient knowledge and shared by many tech-niques in Artificial Intelligence. The main challenge of a WSD system is how to overcome the knowl-edge acquisition bottleneck and efficiently collect the huge amount of context knowledge. More pre-cisely, a practical WSD need figure out how to create and maintain a comprehensive, dynamic, and up-to-date context knowledge base in a highly automatic manner. The context knowledge required in WSD has the following properties: 1. The context knowledge need cover a large 2. Natural language is not a static phenomenon.
Taking into consideration the large amount and dynamic nature of context knowledge, we only have limited options when choosing knowledge sources for WSD. WSD is often an unconscious process to human beings. With a dictionary and sample sen-tences/phrases an average educated person can cor-rectly disambiguate most polysemous words. In-spired by human WSD process, we choose an elec-tronic dictionary and unannotated text samples of word instances as context knowledge sources for our WSD system. Both sources can be automat-ically accessed, provide an excellent coverage of word meanings and usage, and are actively updated to reflect the current state of languages. In this pa-per we present a fully unsupervised WSD system, which only requires WordNet sense inventory and unannotated text. In the rest of this paper, section 2 describes how to acquire and represent the con-text knowledge for WSD. We present our WSD al-gorithm in section 3. Our WSD system is evaluated with SemEval-2007 Task 7 (Coarse-grained English All-words Task) data set, and the experiment results are discussed in section 4. We conclude in section 5. Figure 1 shows an overview of our context knowl-edge acquisition process, and collected knowledge is saved in a local knowledge base. Here are some details about each step. 2.1 Corpus building through Web search The goal of this step is to collect as many as possi-ble valid sample sentences containing the instances of to-be-disambiguated words. Preferably these in-stances are also diverse and cover many senses of a word. We have considered two possible text sources, 1. Electronic text collection, e.g., Gutenberg 2. Web documents. Billions of documents exist
To start the acquisition process, words that need to be disambiguated are compiled and saved in a text file. Each single word is submitted to a Web search engine as a query. Several search engines provide API X  X  for research communities to auto-matically retrieve large number of Web pages. In our experiments we used both Google and Yahoo! API X  X  to retrieve up to 1,000 Web pages for each to-be-disambiguated word. Collected Web pages are cleaned first, e.g., control characters and HTML tags are removed. Then sentences are segmented simply based on punctuation (e.g., ?, !, .). Sentences that contain the instances of a specific word are extracted and saved into a local repository. 2.2 Parsing Sentences organized according to each word are sent to a dependency parser, Minipar. Dependency parsers have been widely used in Computational Linguistics and natural language processing. An evaluation with the SUSANNE corpus shows that Minipar achieves 89% precision with respect to de-pendency relations (Lin, 1998). After parsing sen-tences are converted to parsing trees and saved in files. Neither our simple sentence segmentation ap-proach nor Minipar parsing is 100% accurate, so a small number of invalid dependency relations may exist in parsing trees. The impact of these erroneous relations will be minimized in our WSD algorithm. Comparing with tagging or chunking, parsing is rel-atively expensive and time-consuming. However, in our method parsing is not performed in real time when we disambiguate words. Instead, sentences are parsed only once to extract dependency relations, then these relations are merged and saved in a local knowledge base for the following disambiguation. Hence, parsing will not affect the speed of disam-biguation at all. 2.3 Merging dependency relations After parsing, dependency relations from different sentences are merged and saved in a context knowl-edge base. The merging process is straightforward. A dependency relation includes one head word/node and one dependent word/node. Nodes from different dependency relations are merged into one as long as they represent the same word. An example is shown in Figure 2, which merges the following two sen-tences:  X  X omputer programmers write software. X   X  X any companies hire computer programmers. X 
In a dependency relation  X  word 1  X  word 2  X , word 1 is the head word, and word 2 is the depen-dent word. After merging dependency relations, we will obtain a weighted directed graph with a word as a node, a dependency relation as an edge, and the number of occurrences of dependency relation as weight of an edge. This weight indicates the strength of semantic relevancy of head word and dependent word. This graph will be used in the following WSD process as our context knowledge base. As a fully automatic knowledge acquisition process, it is in-evitable to include erroneous dependency relations in the knowledge base. However, since in a large text collection valid dependency relations tend to repeat far more times than invalid ones, these erroneous edges only have minimal impact on the disambigua-tion quality as shown in our evaluation results. Our WSD approach is based on the following in-sight:
If a word is semantically coherent with its context, then at least one sense of this word is semantically coherent with its context.

Assume that the text to be disambiguated is se-mantically valid, if we replace a word with its glosses one by one, the correct sense should be the one that will maximize the semantic coherence within this word X  X  context. Based on this idea we set up our WSD procedure as shown in Figure 3. First both the original sentence that contains the to-be-disambiguated word and the glosses of to-be-disambiguated word are parsed. Then the parsing tree generated from each gloss is matched with the parsing tree of original sentence one by one. The gloss most semantically coherent with the original sentence will be chosen as the correct sense. How to measure the semantic coherence is critical. Our idea is based on the following hypotheses (assume word 1 is the to-be-disambiguated word):  X  In a sentence if word 1 is dependent on word 2 ,  X  In a sentence if a set of words DEP 1 are de-
For example, we try to disambiguate  X  X ompany X  in  X  X  large company hires many computer program-mers X , after parsing we obtain the dependency rela-tions  X  X ire  X  company X  and  X  X ompany  X  large X . The correct sense for the word  X  X ompany X  should be  X  X n institution created to conduct business X . If in the context knowledge base there exist the depen-dency relations  X  X ire  X  institution X  or  X  X nstitution  X  large X , then we believe that the gloss  X  X n institu-tion created to conduct business X  is semantically co-herent with its context -the original sentence. The gloss with the highest semantic coherence will be chosen as the correct sense. Obviously, the size of context knowledge base has a positive impact on the disambiguation quality, which is also verified in our experiments (see Section 4.2). Figure 4 shows our detailed WSD algorithm. Semantic coherence score is generated by the function TreeMatching , and we adopt a sentence as the context of a word.
We illustrate our WSD algorithm through an ex-ample. Assume we try to disambiguate  X  X ompany X  in the sentence  X  X  large software company hires many computer programmers X .  X  X ompany X  has 9 senses as a noun in WordNet 2.1. Let X  X  pick the fol-lowing two glosses to go through our WSD process.  X  an institution created to conduct business  X  small military unit
First we parse the original sentence and two glosses, and get three weighted parsing trees as shown in Figure 5. All weights are assigned to nodes/words in these parsing trees. In the parsing tree of the original sentence the weight of a node is reciprocal of the distance between this node and to-be-disambiguated node  X  X ompany X  (line 12 in Fig-ure 4). In the parsing tree of a gloss the weight of a node is reciprocal of the level of this node in the parsing tree (line 16 in Figure 4). Assume that our context knowledge base contains relevant depen-dency relations shown in Figure 6.
 Input: Glosses from WordNet; S : the sentence to be disambiguated; G : the knowledge base generated in Section 2; 1. Input a sentence S , W = { w | w  X  X  part of speech 2. Parse S with a dependency parser, generate 3. For each w  X  W { 4. Input all w  X  X  glosses from WordNet; 5. For each gloss w i { 6. Parse w i , get a parsing tree T wi ; 7. score = TreeMatching( T S , T wi ); 8. If the highest score is larger than a preset 9. Otherwise, choose the first sense. 10. } TreeMatching( T 11. For each node n Si  X  T S { 12. Assign weight w Si = 1 13. } 14. For each node n wi  X  T wi { 15. Load its dependent words D wi from G ; 16. Assign weight w wi = 1 17. For each n Sj { 18. If n Sj  X  D wi 19. calculate connection strength s ji 20. score = score + w Si  X  w wi  X  s ji ; 21. } 22. } 23. Return score;
The weights in the context knowledge base are as-signed to dependency relation edges. These weights are normalized to [0, 1] based on the number of de-pendency relation instances obtained in the acquisi-tion and merging process. A large number of occur-rences will be normalized to a high value (close to 1), and a small number of occurrences will be nor-malized to a low value (close to 0).

Now we load the dependent words of each word in gloss 1 from the knowledge base (line 14, 15 in Figure 4), and we get { small, large } for  X  X nstitu-tion X  and { large, software } for  X  X usiness X . In the dependent words of  X  X ompany X ,  X  X arge X  belongs to the dependent word sets of  X  X nstitution X  and  X  X usi-ness X , and  X  X oftware X  belongs to the dependent word set of  X  X usiness X , so the coherence score of gloss 1 is calculated as (line 19, 20 in Figure 4): 1 . 0  X  1 . 0  X  0 . 7 + 1 . 0  X  0 . 25  X  0 . 8 + 1 . 0  X  0 = 1.125
We go through the same process with the second gloss  X  X mall military unit X .  X  X arge X  is the only de-pendent word of  X  X ompany X  appearing in the depen-dent word set of  X  X nit X  in gloss 2, so the coherence score of gloss 2 in the current context is: 1 . 0  X  1 . 0  X  0 . 8 = 0.8
After comparing the coherence scores of two glosses, we choose sense 1 of  X  X ompany X  as the cor-rect sense (line 9 in Figure 4). This example illus-trates that a strong dependency relation between a head word and a dependent word has a powerful dis-ambiguation capability, and disambiguation quality is also significantly affected by the quality of dictio-nary definitions.

In Figure 4 the TreeMatching function matches the dependent words of to-be-disambiguated word (line 15 in Figure 4), and we call this matching strat-egy as dependency matching. This strategy will not work if a to-be-disambiguated word has no depen-dent words at all, for example, when the word  X  X om-pany X  in  X  X ompanies hire computer programmers X  has no dependent words. In this case, we developed the second matching strategy, which is to match the head words that the to-be-disambiguated word is de-pendent on, such as matching  X  X ire X  (the head word of  X  X ompany X ) in Figure 5(a). Using the dependency relation  X  X ire  X  company X , we can correctly choose sense 1 since there is no such relation as  X  X ire  X  unit X  in the knowledge base. This strategy is also helpful when disambiguating adjectives and adverbs since they usually only depend on other words, and rarely any other words are dependent on them. The third matching strategy is to consider synonyms as a match besides the exact matching words. Synonyms can be obtained through the synsets in WordNet. For example, when we disambiguate  X  X ompany X  in  X  X ig companies hire many computer programmers X ,  X  X ig X  can be considered as a match for  X  X arge X . We call this matching strategy as synonym matching. The three matching strategies can be combined and applied together, and in Section 4.1 we show the experiment results of 5 different matching strategy combinations. We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al., 2007). The task organiz-ers provide a coarse-grained sense inventory cre-ated with SSI algorithm (Navigli and Velardi, 2005), training data, and test data. Since our method does not need any training or special tuning, neither coarse-grained sense inventory nor training data was used. The test data includes: a news article about  X  X omeless X  (including totally 951 words, 368 words are annotated and need to be disambiguated), a re-view of the book  X  X eeding Frenzy X  (including to-tally 987 words, 379 words are annotated and need to be disambiguated), an article about some trav-eling experience in France (including totally 1311 words, 500 words are annotated and need to be dis-ambiguated), computer programming(including to-tally 1326 words, 677 words are annotated and need to be disambiguated), and a biography of the painter Masaccio (including totally 802 words, 345 words are annotated and need to be disambiguated). Two authors of (Navigli et al., 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%. This inter-annotator agreement is usually considered an upper-bound for WSD systems.

We followed the WSD process described in Sec-tion 2 and 3 using the WordNet 2.1 sense repository that is adopted by SemEval-2007 Task 07. All exper-iments were performed on a Pentium 2.33GHz dual core PC with 3GB memory. Among the 2269 to-be-disambiguated words in the five test documents, 1112 words are unique and submitted to Google API as queries. The retrieved Web pages were cleaned, and 1945189 relevant sentences were ex-tracted. On average 1749 sentences were obtained for each word. The Web page retrieval step took 3 days, and the cleaning step took 2 days. Parsing was very time-consuming and took 11 days. The merg-ing step took 3 days. Disambiguation of 2269 words in the 5 test articles took 4 hours. All these steps can be parallelized and run on multiple computers, and the whole process will be shortened accordingly. The overall disambiguation results are shown in Table 1. For comparison we also listed the re-sults of the top three systems and three unsuper-vised systems participating in SemEval-2007 Task 07. All of the top three systems (UoR-SSI, NUS-PT, NUS-ML) are supervised systems, which used annotated resources (e.g., SemCor, Defense Science Organization Corpus) during the training phase. Our fully unsupervised WSD system significantly out-performs the three unsupervised systems (SUSSZ-FR, SUSSX-C-WD, SUSSX-CR) and achieves per-formance approaching the top-performing super-vised WSD systems. 4.1 Impact of different matching strategies to To test the effectiveness of different matching strate-gies discussed in Section 3, we performed some ad-ditional experiments. Table 2 shows the disambigua-tion results by each individual document with the following 5 matching strategies: 1. Dependency matching only. 2. Dependency and backward matching. 3. Dependency and synonym backward matching. 4. Dependency and synonym dependency match-5. Dependency, backward, synonym backward,
As expected combination of more matching strategies results in higher disambiguation quality. By analyzing the scoring details, we verified that backward matching is especially useful to disam-biguate adjectives and adverbs. Adjectives and ad-verbs are often dependent words, so dependency matching itself rarely finds any matched words. Since synonyms are semantically equivalent, it is reasonable that synonym matching can also improve disambiguation performance. 4.2 Impact of knowledge base size to To test the impact of knowledge base size to dis-ambiguation quality we randomly selected 1339264 sentences (about two thirds of all sentences) from our text collection and built a smaller knowledge base. Table 3 shows the experiment results. Overall disambiguation quality has dropped slightly, which
Matching d001 d002 d003 d004 d005 Overall strategy P R P R P R P R P R P R shows a positive correlation between the amount of context knowledge and disambiguation quality. It is reasonable to assume that our disambiguation per-formance can be improved further by collecting and incorporating more context knowledge.
 Broad coverage and disambiguation quality are crit-ical for WSD techniques to be adopted in prac-tice. This paper proposed a fully unsupervised WSD method. We have evaluated our approach with SemEval-2007 Task 7 (Coarse-grained English All-words Task) data set, and we achieved F-scores ap-proaching the top performing supervised WSD sys-tems. By using widely available unannotated text and a fully unsupervised disambiguation approach, our method may provide a viable solution to the problem of WSD. The future work includes: 1. Continue to build the knowledge base, enlarge 2. WSD is often an unconscious process for hu-3. Test our WSD system on fine-grained SemEval This work is partially funded by NSF grant 0737408 and Scholar Academy at the University of Houston Downtown. This paper contains proprietary infor-mation protected under a pending U.S. patent.
