 David Silver silver@cs.ualberta.ca Richard S. Sutton sutton@cs.ualberta.ca Martin M  X uller mmueller@cs.ualberta.ca Reinforcement learning can be subdivided into two fundamental problems: learning and planning . Infor-mally, the goal of learning is for an agent to improve its policy from its interactions with the environment. The goal of planning is for an agent to improve its policy without further interaction with its environment. The agent can deliberate, reason, ponder, think or search, so as to find the best behaviour in the available com-putation time. Sample-based methods can be applied to both problems. During learning, the agent samples experience from the real world: it executes an action at each time-step and observes its consequences. During planning, the agent samples experience from a model of the world: it simulates an action at each computa-tional step and observes its consequences. We propose that an agent can both learn and plan effectively using sample-based reinforcement learning algorithms. We use the game of 9  X  9 Go as an example of a large-scale, high-performance application in which learning and planning both play significant roles.
 In the domain of Computer Go, the most success-ful learning methods have used sample-based rein-forcement learning to extract domain knowledge from games of self-play (Schraudolph et al., 1994; Dahl, 1999; Enzenberger, 2003; Silver et al., 2007). The value of a position is approximated by a multi-layer perceptron, or a linear combination of binary features, that form a compact representation of the state space. Temporal difference learning is used to update the value function, slowly accumulating knowledge from the complete history of experience.
 The most successful planning methods use sample-based search to identify the best move in the current position. 9  X  9 Go programs based on the UCT algo-rithm (Kocsis &amp; Szepesvari, 2006) have now achieved master level (Gelly &amp; Silver, 2007; Coulom, 2007). The UCT algorithm begins each new move with no domain knowledge, but rapidly learns the values of positions in a temporary search tree. Each state in the tree is explicitly represented, and the value of each state is learned by Monte-Carlo simulation, from games of self-play that start from the current position. In this paper we develop an architecture, Dyna-2, that combines these two approaches. Like the Dyna archi-tecture (Sutton, 1990), the agent updates a value func-tion both from real experience, and from simulated ex-perience that is sampled using a model of the world. The new idea is to maintain two separate memories: a permanent learning memory that is updated from real experience; and a transient planning memory that is updated from simulated experience. Both memories use linear function approximation to form a compact representation of the state space, and both memories are updated by temporal-difference learning. We consider sequential decision-making processes, in which at each time-step t the agent receives a state s t , executes an action a t according to its current policy  X  ( s, a ), and then receives a scalar reward r t +1 . 2.1. Sample-Based Learning Most efficient reinforcement learning methods use a value function as an intermediate step for computing a policy. In episodic tasks the action-value function Q  X  ( s, a ) is the expected total reward from state s after taking action a and then following policy  X  . In large domains, it is not possible or practical to learn a value for each individual state. In this case, it is necessary to approximate the value function using fea-tures  X  ( s, a ) and parameters  X  . A simple and success-ful approach (Sutton, 1996) is to use a linear function approximation Q ( s, a ) =  X  ( s, a ) T  X  . We note that ta-ble lookup is a special case of linear function approx-imation, using binary features  X  ( s, a ) = e ( s, a ), where e ( s, a ) is a unit vector with a one in the single compo-nent corresponding to ( s, a ) and zeros elsewhere. The TD(  X  ) algorithm (Sutton, 1988) estimates the value of the current state from the value of subse-quent states. The  X  parameter determines the tem-poral span over which values are updated. At one ex-treme, TD(0) bootstraps the value of a state from its immediate successor. At the other extreme, TD(1) updates the value of a state from the final return; it is equivalent to Monte-Carlo evaluation (Sutton &amp; Barto, 1998). TD(  X  ) can be incrementally computed by maintaining a vector of eligibility traces z t . The Sarsa algorithm (Rummery &amp; Niranjan, 1994) combines temporal difference evaluation with policy improvement. An action-value function is estimated by the TD(  X  ) algorithm, and the policy is improved by selecting actions according to an -greedy policy. The action-value function is updated from each tuple ( s, a, r, s 0 , a 0 ) of experience, using the TD(  X  ) update rule, 2.2. Sample-Based Search Sample-based planning applies sample-based reinforce-ment learning methods to simulated experience. This requires a sample model of the world: a state tran-sition generator A t ( s, a )  X  S  X  A 7 X  S and reward generator B t ( s, a )  X  S  X  A 7 X  R . The effectiveness of sample-based planning depends on the accuracy of the model (Paduraru, 2007). In sample-based search , experience is simulated from the real state s , so as to identify the best action from this state.
 Monte-Carlo simulation is a simple but effective method for sample-based search. Multiple episodes are simulated, starting from the real state s , and fol-lowing a random policy. The action-values Q ( s, a ) are estimated by the empirical average of the returns of all episodes in which action a was taken from the real state s . After simulation is complete, the agent selects the greedy action argmax a Q ( s, a ), and proceeds to the next real state.
 Monte-Carlo tree search constructs a search tree con-taining all state X  X ction pairs that have been visited by the agent. Each simulation consists of two dis-tinct phases: greedy action selection while within the tree, and then random action selection until termi-nation. If a simulated state s is fully represented in the search tree, i.e. all actions from s have already been tried, then the agent selects the greedy action argmax a Q ( s, a ). Otherwise, the agent selects actions at random. After each simulation, the action-values Q ( s, a ) of all states and actions experienced in the episode are updated to the empirical average return following each state X  X ction pair. In practice, only one new state X  X ction pair is added per episode, resulting in a tree-like expansion.
 The UCT algorithm (Kocsis &amp; Szepesvari, 2006) im-proves the greedy action selection in Monte-Carlo tree search. Each state of the search tree is treated as a multi-armed bandit, and actions are chosen using the UCB algorithm for balancing exploration and exploita-tion (Auer et al., 2002). 2.3. Dyna The Dyna architecture (Sutton, 1990) combines sample-based learning with sample-based planning. The agent learns a model of the world from real ex-perience, and updates its action-value function from both real and simulated experience. Before each real action is selected, the agent performs some sample-based planning. For example, the Dyna-Q algorithm remembers all previous states, actions and transitions. During planning, experience is simulated by sampling states, actions and transitions from the empirical dis-tribution. A Q-learning update is applied to update the action-value function after each sampled transi-tion, and after each real transition. 2.4. Tracking Traditional learning methods focus on finding a single best solution to the learning problem. In reinforcement learning one may seek an algorithm that converges on the optimal value function (or optimal policy). How-ever, in large domains the agent may not have suffi-cient resources to perfectly represent the optimal value function. In this case we can actually achieve better performance by tracking the current situation rather than converging on the best overall parameters. The agent can specialise its value function to its current re-gion of the state space, and update its representation as it moves through the state space. The potential for specialisation means that tracking methods may out-perform converging methods, even in stationary do-mains (Sutton et al., 2007). We define a memory to be the set of features and corresponding parameters used by an agent to esti-mate the value function. In our architecture, the agent maintains two distinct memories: a permanent mem-ory (  X ,  X  ) updated during sample-based learning, and a transient memory (  X   X ,  X   X  ) updated during sample-based search. The value function is a linear combination of the transient and permanent memories, such that the transient memory tracks a local correction to the per-manent memory, where Q ( s, a ) is a permanent value function, and  X  Q ( s, a ) is a combined value function.
 We refer to the distribution of states and actions en-countered during real experience as the learning distri-bution , and the distribution encountered during simu-lated experience as the search distribution . The per-manent memory is updated from the learning distribu-tion and converges on the best overall representation of the value function, based on the agent X  X  past ex-perience. The transient memory is updated from the search distribution and tracks the local nuances of the value function, based on the agent X  X  expected future experience. Algorithm 1 Episodic Dyna-2 1: procedure Learn 2: Initialise A, B . Transition and reward models 3:  X   X  0 . Clear permanent memory 4: loop 5: s  X  s 0 . Start new episode 6:  X   X   X  0 . Clear transient memory 7: z  X  0 . Clear eligibility trace 8: Search ( s ) 9: a  X   X  ( s ;  X  Q ) . e.g. -greedy 10: while s is not terminal do 11: Execute a , observe reward r , state s 0 12: ( A, B )  X  UpdateModel ( s, a, r, s 0 ) 13: Search ( s 0 ) 14: a 0  X   X  ( s 0 ;  X  Q ) 15:  X   X  r + Q ( s 0 , a 0 )  X  Q ( s, a ) . TD-error 16:  X   X   X  +  X  ( s, a )  X z . Update weights 17: z  X   X z +  X  . Update eligibility trace 18: s  X  s 0 , a  X  a 0 19: end while 20: end loop 21: end procedure 22: procedure Search ( s ) 23: while time available do 24:  X  z  X  0 . Clear eligibility trace 25: a  X   X   X  ( s ;  X  Q ) . e.g. -greedy 26: while s is not terminal do 27: s 0  X  A ( s, a ) . Sample transition 28: r  X  B ( s, a ) . Sample reward 29: a 0  X   X   X  ( s 0 ;  X  Q ) 30:  X   X   X  r +  X  Q ( s 0 , a 0 )  X   X  Q ( s, a ) . TD-error 31:  X   X   X   X   X  +  X   X  ( s, a )  X   X   X  z . Update weights 32:  X  z  X   X   X   X  z +  X   X  . Update eligibility trace 33: s  X  s 0 , a  X  a 0 34: end while 35: end while 36: end procedure The Dyna-2 architecture can be summarised as Dyna with Sarsa updates, permanent and transient mem-ories, and linear function approximation (see Algo-rithm 1). The agent updates its permanent memory from real experience. Before selecting a real action, the agent executes a sample-based search from the current state. The search procedure simulates com-plete episodes from the current state, sampled from the model, until no more computation time is avail-able. The transient memory is updated during these simulations to learn a local correction to the perma-nent memory; it is cleared at the beginning of each real episode.
 A particular instance of Dyna-2 must specify learn-ing parameters: a policy  X  to select real actions; a set of features  X  for the permanent memory; a tempo-ral difference parameter  X  ; and a learning rate  X  ( s, a ). Similarly, it must specify the equivalent search param-eters: a policy  X   X  to select actions during simulation; a set of features  X   X  for the transient memory; a temporal difference parameter  X   X  ; and a learning rate  X   X  ( s, a ). The Dyna-2 architecture subsumes a large family of learning and search algorithms. If there is no transient memory,  X   X  =  X  , then the search procedure has no effect and may be skipped. This results in the linear Sarsa algorithm.
 If there is no permanent memory,  X  =  X  , then Dyna-2 reduces to a sample-based search algorithm. For exam-ple, Monte-Carlo tree search is achieved by choosing icy that is greedy within the tree, and then uniform random until termination; and selecting learning pa-counts the number of times that action a has been selected in state s . The UCT algorithm replaces the greedy phase of the simulation policy with the UCB rule for action selection.
 Finally, we note that real experience may be accumu-lated offline prior to execution. Dyna-2 may be exe-cuted on any suitable training environment (e.g. a he-licopter simulator) before it is applied to real data (e.g. a real helicopter). The permanent memory is updated offline, but the transient memory is updated online. Dyna-2 provides a principled mechanism for combin-ing offline and online knowledge(Gelly &amp; Silver, 2007); the permanent memory provides prior knowledge and a baseline for fast learning. Our examples of Dyna-2 in Computer Go operate in this manner. In domains with spatial coherence, binary features can be constructed to exploit spatial structure at multiple levels (Sutton, 1996). The game of Go exhibits strong spatial coherence: expert players describe positions us-ing a broad vocabulary of shapes (Figure 1a). A simple way to encode basic shape knowledge is through a large set of local shape features which match a particular configuration within a small region of the board (Silver be the vector of local shape features for m  X  m square regions, for all possible configurations and square lo-cations. For example, Figure 1a shows several local shape features of size 3  X  3. Combining local shape fea-tures of different sizes builds a representation spanning many levels of generality: we define the multi-level fea-In 9  X  9 Go there are nearly a million  X  square (1 , 3) fea-tures, about 200 of which are non-zero at any given time.
 Local shape features can be used as a permanent mem-ory, to represent general domain knowledge. For exam-ple, local shape features can be learned offline, using temporal difference learning and training by self-play (Silver et al., 2007; Gelly &amp; Silver, 2007). However, local shape features can also be used as a transient memory 2 , by learning online from simulations from the current state. The representational power of lo-cal shape features is significantly increased when they can track the short-term circumstances (Sutton et al., 2007). A local shape may be bad in general, but good in the current situation (Figure 1b). By training from simulated experience, starting from the current state, we can focus learning on what works well now . We apply the Dyna-2 algorithm to 9  X  9 Computer Go using local shape features  X  ( s, a ) =  X   X  ( s, a ) = lowing action a in state s (Sutton &amp; Barto, 1998). We use a self-play model, an -greedy policy, and default Dyna-2 algorithm slightly to utilise the logistic func-tion and to minimise a cross-entropy loss function, by replacing the value function approximation in (4) and (5), where  X  ( x ) = 1 1+ e In addition we ignore local shape features consisting of entirely empty intersections; we clear the eligibility trace for exploratory actions; and we use the default policy described in (Gelly et al., 2006) after the first D = 10 moves of each simulation. We refer to the com-plete algorithm as Dyna-2-Shape , and implement this algorithm in our program RLGO , which executes al-most 2000 complete episodes of simulation per second on a 3 GHz processor.
 For comparison, we implemented the UCT algorithm, based on the description in (Gelly et al., 2006). We use an identical default policy to the Dyna-2-Shape algo-rithm, to select moves when outside of the search tree, and a first play urgency of 1. We evaluate both pro-grams by running matches against GnuGo, a standard benchmark program for Computer Go.
 We compare the performance of local shape features in the permanent memory alone; in the transient memory alone; and in both the permanent and transient mem-ories. We also compare the performance of local shape features of different sizes (see Figure 3). Using only the transient memory, Dyna-2-Shape outperformed UCT by a small margin. Using Dyna-2-Shape with both permanent and transient memories provided the best results, and outperformed UCT by a significant mar-gin.
 Local shape features would normally be considered naive in the domain of Go: the majority of shapes and tactics described in Go textbooks span considerably larger regions of the board than 3  X  3 squares. Indeed, when used only in the permanent memory, the local shape features win just 5% of games against GnuGo. However, when used in the transient memory, even the  X  square (1 , 2) features achieve performance compa-rable to UCT. Unlike UCT, the transient memory can generalise in terms of local responses: for example, it quickly learns the importantance of black connecting when white threatens to cut (Figures 1c and 1d). We also study the effect of the temporal difference parameter  X   X  in the search procedure (see Figure 2). We see that bootstrapping (  X   X  &lt; 1) provides signifi-cant benefits. Previous work in sample-based search has largely been restricted to Monte-Carlo methods (Tesauro &amp; Galperin, 1996; Kocsis &amp; Szepesvari, 2006; Gelly et al., 2006; Gelly &amp; Silver, 2007; Coulom, 2007). Our results suggest that generalising these approaches to temporal difference learning methods may provide significant benefits when value function approximation is used.
 In games such as Chess, Checkers and Othello, master level play has been achieved by combining a heuristic evaluation function with  X  - X  search. The heuristic is typically approximated by a linear combination of bi-nary features, and can be learned offline by temporal-difference learning and self-play (Baxter et al., 1998; Schaeffer et al., 2001; Buro, 1999). Similarly, in the permanent memory of our architecture, the value func-tion is approximated by a linear combination of binary features, learned offline by temporal-difference learn-ing and self-play (Silver et al., 2007). Thus it is natu-ral to compare Dyna-2 with approaches based on  X  - X  search.
 Dyna-2 combines a permanent memory with a tran-sient memory, using sample-based search. In contrast, the classical approach uses the permanent memory Q ( s, a ) as an evaluation function for  X  - X  search. A hybrid approach is also possible, in which the com-bined value function  X  Q ( s, a ) is used as an evaluation function for  X  - X  search, including both permanent and transient memories. This can be viewed as searching with a dynamic evaluation function that evolves ac-cording to the current context. We compare all three approaches in Figure 4.
 Dyna-2 outperformed classical search by a wide mar-gin. In the game of Go, the consequences of a par-ticular move (for example, playing good shape as in Figure 1a) may not become apparent for tens or even hundreds of moves. In a full-width search these conse-quences remain beyond the horizon, and will only be recognised if represented by the evaluation function. In contrast, sample-based search only uses the perma-nent memory as an initial guide, and learns to identify the consequences of particular patterns in the current situation. The hybrid approach successfully combines this knowledge with the precise lookahead provided by full-width search.
 Using the hybrid approach, our program RLGO estab-lished an Elo rating of 2130 on the Computer Go On-line Server, more than any handcrafted or traditional search program. The Computer Go program MoGo uses the heuris-tic UCT algorithm (Gelly &amp; Silver, 2007) to achieve dan -level performance. This algorithm can be viewed as an instance of Dyna-2 with local shape features in the permanent memory, and table lookup in the transient memory. It uses a step-size of  X   X  ( s, a ) = nent memory is specified by n prior in terms of equiva-lent experience , i.e. the worth of the permanent mem-ory, measured in episodes of simulated experience. In addition, MoGo uses the Rapid Action Value Esti-mate (RAVE) algorithm in its transient memory (Gelly &amp; Silver, 2007). This algorithm can also be viewed as a special case of the Dyna-2 architecture, but using features of the full history h t and not just the current state s t and action a t .
 We define a history to be a sequence of states and actions h t = s 1 a 1 ...s t a t , including the current action a . An individual RAVE feature  X  RAV E sa ( h ) is a binary feature of the history h that matches a particular state s and action a . The binary feature is on iff s occurs in the history and a matches the current action a t ,  X  Thus the RAVE algorithm provides a simple abstrac-tion over classes of related histories. The implemen-tation of RAVE used in MoGo makes two additional simplifications. First, MoGo estimates a value for each RAVE feature independently of any other RAVE fea-tures, set to the average outcome of all simulations in which the RAVE feature  X  RAV E sa is active. Second, for action selection, MoGo only evaluates the single RAVE and candidate action a t . This somewhat reduces the generalisation power of RAVE, but allows for a partic-ularly efficient update procedure. Reinforcement learning is often considered a slow pro-cedure. Outstanding examples of success have, in the past, learned a value function from months of offline computation. However, this does not need to be the case. Many reinforcement learning methods are fast, incremental, and scalable. When such a reinforcement learning algorithm is applied to simulated experience, using a transient memory, it becomes a high perfor-mance search algorithm. This search procedure can be made more efficient by generalising across states; and it can be combined with long-term learning, using a permanent memory.
 Monte-Carlo tree search algorithms, such as UCT, have recently received much attention. However, this is just one example of a sample-based search algorithm. There is a spectrum of algorithms that vary from table-lookup to function approximation; from Monte-Carlo learning to bootstrapping; and from permanent to transient memories. Function approximation provides rapid generalisation in large domains; bootstrapping is advantageous in the presence of function approx-imation; and permanent and transient memories al-low general knowledge about the past to be combined with specific knowledge about the expected future. By varying these dimensions, we have achieved a signifi-cant improvement over the UCT algorithm.
 In 9  X  9 Go, programs based on extensions to the UCT algorithm have achieved dan -level performance. Our program RLGO, based on the Dyna-2 architecture, is the strongest program not based on UCT, and suggests that the full spectrum of sample-based search meth-ods merits further investigation. For larger domains, such as 19  X  19 Go, generalising across states becomes increasingly important. Combining state abstraction with sample-based search is perhaps the most promis-ing avenue for achieving human-level performance in this challenging domain.
 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002).
Finite-time analysis of the multi-armed bandit prob-lem. Machine Learning , 47 , 235 X 256.
 Baxter, J., Tridgell, A., &amp; Weaver, L. (1998). Exper-iments in parameter learning using temporal differ-ences. International Computer Chess Association Journal , 21 , 84 X 99.
 Buro, M. (1999). From simple features to sophisticated evaluation functions. First International Conference on Computers and Games (pp. 126 X 145).
 Coulom, R. (2007). Computing Elo ratings of move patterns in the game of Go. Computer Games Work-shop .
 Dahl, F. (1999). Honte, a Go-playing program using neural nets. Machines that learn to play games (pp. 205 X 223). Nova Science.
 Enzenberger, M. (2003). Evaluation in Go by a neural network using soft segmentation. 10th Advances in Computer Games Conference (pp. 97 X 108).
 Gelly, S., &amp; Silver, D. (2007). Combining online and offline learning in UCT. 17th International Confer-ence on Machine Learning (pp. 273 X 280).
 Gelly, S., Wang, Y., Munos, R., &amp; Teytaud, O. (2006). Modification of UCT with patterns in Monte-Carlo Go (Technical Report 6062). INRIA.
 Kocsis, L., &amp; Szepesvari, C. (2006). Bandit based
Monte-Carlo planning. 15th European Conference on Machine Learning (pp. 282 X 293).
 Paduraru, C. (2007). Planning with approximate and learned MDP models. Master X  X  thesis, University of Alberta.
 Rummery, G., &amp; Niranjan, M. (1994). On-line Q-learning using connectionist systems (Technical Re-port CUED/F-INFENG/TR 166). Cambridge Uni-versity Engineering Department.
 Schaeffer, J., Hlynka, M., &amp; Jussila, V. (2001). Tempo-ral difference learning applied to a high-performance game-playing program. Proceedings of the Seven-teenth International Joint Conference on Artificial Intelligence (pp. 529 X 534).
 Schraudolph, N., Dayan, P., &amp; Sejnowski, T. (1994).
Temporal difference learning of position evaluation in the game of Go. Advances in Neural Information Processing 6 (pp. 817 X 824).
 Silver, D., Sutton, R., &amp; M  X uller, M. (2007). Reinforce-ment learning of local shape in the game of Go. 20th
International Joint Conference on Artificial Intelli-gence (pp. 1053 X 1058).
 Sutton, R. (1988). Learning to predict by the method of temporal differences. Machine Learning , 3 , 9 X 44. Sutton, R. (1990). Integrated architectures for learn-ing, planning, and reacting based on approximating dynamic programming. 7th International Confer-ence on Machine Learning (pp. 216 X 224).
 Sutton, R. (1996). Generalization in reinforcement learning: Successful examples using sparse coarse coding. Advances in Neural Information Processing Systems 8 (pp. 1038 X 1044).
 Sutton, R., &amp; Barto, A. (1998). Reinforcement learn-ing: an introduction . MIT Press.
 Sutton, R., Koop, A., &amp; Silver, D. (2007). On the role of tracking in stationary environments. 17th
International Conference on Machine Learning (pp. 871 X 878).
 Tesauro, G., &amp; Galperin, G. (1996). On-line policy improvement using Monte-Carlo search. Advances
