 REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human cor-pora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effec-tiveness by measuring task success in virtual in-teractive environments (Byron et al., 2009; Gar-gett et al., 2010; Janarthanam et al., 2012). Vir-tual environments however eliminate real-world uncertainty, such object recognition errors or clut-tered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014).
Related work has focused on computer gener-ated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in  X  X omplex photographs of real-world cluttered scenes X . They report that REs are heavily influenced by the object type. Here, we are inter-ested in studying REs for visual objects in urban scenes. As the success of a RE is heavily depen-dent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE.

Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released)  X   X  X eferring Expression Anchored Language X  (Sec-tion 2), and compares the findings to those re-ported in virtual worlds (Gargett et al., 2010). We then provide a detailed analysis of how syntactic and semantic features contribute to the success of REs (Sections 4.1, 4.2, 4.3), accounting for unob-servable latent variables, such as the complexity of the visual scene (as described in Section 3). Fi-nally, we summarise our work and discuss the im-plications of our work for NLG systems (Section 5). The dataset and models will be released. The REAL corpus contains a collection of images of real-world urban scenes (Fig. 1) together with verbal descriptions of target objects (see Fig. 2)
Figure 1: Original picture. Figure 2: Target object in yellow generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions (Fig. 3). The data was collected through a web-based inter-face. The images were taken in Edinburgh (Scot-land, UK), very early one summer morning. This was necessary to reduce the occlusion of city ob-jects from buses and crowds, and to minimise lighting and weather variations between images. 2.1 Experimental Setup There were 190 participants recruited (age be-tween 16 to 71). Each participant was presented with an urban image (Fig. 1), where the target ob-ject was outlined by a yellow box (Fig. 2), and was asked to describe the target using free text. After completing a (self-specified) number of tasks, par-ticipants were then asked to validate descriptions provided by other participants by clicking on the object using previously unseen images (Fig. 3).
Overall, 868 descriptions across 32 images were collected, averaging around 27 descriptions per image. The balance of generation and validations was adjusted to ensure that all descriptions were identified by at least 3 other participants, generat-ing 2618 image tag verifications. All cases were manually checked to determine if the  X  X orrect X  (green) or  X  X ncorrect X  (red) target had been identi-fied Fig. 3. Overall, 76.2% of human descriptions provided were successfully identified. For the ex-periments reported in following sections, we sum-marised answers categorised as  X  X ncorrect X ,  X  X mbi-tious X  and  X  X ot found X  as unsuccessful . 2.2 Comparison to GIVE-2 Corpus We now compare this data with human data gen-erated for the GIVE-2 challenge (Gargett et al., 2010). In GIVE-2, the target objects have dis-tinct attributes, such as colour and position. For instance, an effective RE in GIVE-2 could be  X  the third button from the second row  X . In real-world situations though, object properties are less well defined, making a finite set of pre-defined quali-ties unfeasible. Consider, for instance, the build-ing highlighted in Figure 2, for which the follow-ing descriptions were collected:
It is evident that the REAL users refer to a va-riety of object qualities. We observe that all par-ticipants refer to the colour of the building ( white, black and white, greyish-whitish ) and some men-tion location ( by the river, at the water side ).
Experimental Factors influencing Task Per-formance: In REAL, task success is defined as the ability to correctly identify an object, whereas in GIVE-2, task success refers to the successful completion of the navigation task. In contrast to GIVE-2, not all REAL participants were able to correctly identify the referred objects (76.2% task Table 2: Descriptive statistics for GIVE-2 and REAL success). We assume that this is because GIVE-2 was an interactive setup, where the participants were able to engage in a clarification dialogue. Gender: In REAL, gender was not a significant factor with respect to task success (Mann-Whitney U test, p =0 . 2 ). Length of REs (no. words): In REAL, females tend to provide lengthier REs than males, however the difference is not statistically significant (Mann-Whitney U test, p =0 . 58 ). In GIVE-2, only German females produced signifi-cantly longer descriptions than their male counter-parts. Relation between length (no. words) and task success: The REAL data shows a positive relationship between length and success rate, i.e. for a one word increase in length, the odds of cor-rect object identification is significantly increased ( p&lt; 0 . 05 , Logit), i.e. longer and more complex sentences lead to more successful REs. We assume that the complexity of the urban scene represented in the image is hidden due to the lack of semantic annotations. Our dataset does not in-clude any quantifiable image descriptions, such as computer vision output as in (Mitchell et al., 2012) or manual annotations as in (Yatskar et al., 2014). In addition, the same RE might not always re-sult in successful identification of an object due to scene complexity. In order to marginalise the effect of the scene complexity, we exploit the mul-tiple available data points per image. This allows us to estimate the average success rate of each re-ferring expression SR RE (the proportion of suc-cessful validations) and the average success rate of each image SR i (the proportion of the correctly identified objects in the image). We use SR i to marginalise over the (hidden) image complexity, where we assume that some pictures are inher-ently more complex than others and thus achieve lower success rates. Similar normalisation meth-ods are used for user ratings to account for the fact that some users are more  X  X olerant X  and in general give higher ratings (Jin and Si, 2004). We employ Gaussian normalisation (Resnick et al., 1994) to normalise image success rates by considering the following factors: 1. Shift of average success rate per image: some images are inherently easier than others and gain higher success rates, independently of the REs used. This factor can be accounted by subtract-ing average success rates of all images from the average rating of a specific image x . 2. Different ratings: there are 27 REs per image on average, some of which are harder to understand than others, thus they gain lower success rates. To account for this, the success rates of each image are divided by the overall SR variance.

The normalised image success rate ( NSR i ) per image x is defined by the following equation: Using the ( NSR i ), we now investigate the REs in terms of their linguistic properties, includ-ing automatically annotated syntactic features and manually annotated semantic features. Unlike previous work, we use both successful and unsuccessful REs in order to build a model that is able to predict the success or the failure of a RE. 4.1 Syntactic Analysis of REG Success We use the Stanford CoreNLP tool (Manning et al., 2014) to syntactically annotate the REs and we investigate which linguistic features contribute to the RE success in relation to the image complexity. Note that these analyses are based on normalised values, as discussed in Section 3).
 Predicting RE Success Rate ( SR RE ): Initially, we compare successful and unsuccessful REs by taking the upper and lower quartiles and extract-ing their syntactic features., i.e. the top and bottom 25% of REs with respect to their average success rate, and group them into two groups. We then extract syntactic features of these two groups and compare their frequencies (occurrence per RE), means, and standard deviations (Table 3), and compare them using a t-test ( p&lt; 0 . 05 ). The dif-ference between successful and unsuccessful ex-Table 3: Statistics regarding the linguistic fea-tures in successful vs unsuccessful referring ex-pressions. ( * denotes significant difference at p&lt; 0 . 05 ). pressions lies in the use of NP (Noun phrases), NNP (Proper noun, singular), NN (Noun, singu-lar or mass), JJ (Adjective) and VBN (Verb, past participle) (Table 3). Successful REs include more NPs, including NNPs and NNs, which indicates that more than one reference is used to describe and distinguish a target object. This could mean that distractors are explicitly mentioned and elim-inated or that the object of interest has a complex appearance, as opposed to simply structured ob-jects, such as buttons, in GIVE-2. For example, the following description refers to a complex ob-ject:
In addition, successful REs contain significantly which indicates that the object was further de-scribed and distinguished using its attributes, as for instance the following description:
The main difference between successful and un-successful REs is the amount of detail provided to describe and distinguish the target object. This is also in-line with our previous results that success is positively correlated to the number of words used (Section 2.2) and it might explain why hu-mans overspecify.

To further verify this hypothesis, we build a pre-dictive model of average success rate, using multi-ple step-wise linear regression with syntactic fea-tures as predictors. We find a significant ( p&lt; 0 . 05 ) positive relationship between success rate and NP, PP (Prepositional phrase), ADVP (Adver-bial phrase), CD (Cardinal number), and length (Table 4). NPs are used to distinguish and de-scribe the target object. ADVPs and PPs serve a similar function to adjectives in this case, i.e. to describe further attributes, especially spatial ones, like  X  X he one near the river X ,  X  X ext to the yellow building X . Cardinal numbers are used to refer to complex structured features of the target object, e.g. two-story building or two large double doors . Predicting Image Success Rate ( NSR image ): We repeat a similar analysis for estimating how syntactic features relate to image success rate, i.e. how the image complexity, as estimated from the success rate of an image, influences how humans describe the target object, i.e. how human gener-ated descriptions change with respect to the im-age complexity as estimated from the (normalised) success rate of an image. We find that humans use significantly more PPs and number of words ( p&lt; 0 . 05 ) when describing complex images.
In sum, syntactical features, which further de-scribe and distinguish the target object (such as NPs, ADJ, and ADVPs and PPs) indicate success-ful REs. However, they cannot fully answer the question of  X  X hat makes a RE successful X , there-fore we enrich our feature set using manually an-notated semantic features. 4.2 Semantic Analysis of REG We extract semantic features by annotating spa-tial frames of reference as described in (Gargett et al., 2010). We annotate a sample of the corpus (100 instances), which allows us to perform a di-rect comparison between the two corpora.
 Comparison to GIVE-2 Corpus: We observe that in the REAL corpus, the taxonomic property, the relative property and the macro-level landmark Table 5: Frequency of semantic frames in REAL vs. GIVE-2 ( * denotes significant differences at p&lt; 0 . 05 ,  X  2 test). intrinsic property of the object in question are used significantly more often than in the GIVE-2 corpus
In contrast, in GIVE-2 the absolute property of the object, such as the colour, and references to distractors are used significantly more often than in REAL. These results reflect the fact that scenes in REAL were more complex, and as such, rel-ative properties to other objects and landmarks were used more often. In GIVE-2, target objects were mostly buttons, therefore, absolute descrip-tions (  X  X he blue button X  ) or referring to an intrin-sic distractor (  X  X he red button next to the green X  ) are more frequent. In addition, real-world en-vironments are dynamic. Humans choose to re-fer to immovable objects ( macro-level landmarks ) more often than in closed-world environments. In GIVE-2, immovable objects are limited to walls, ceilings or floors, whereas in REAL there is a wide range of immovable objects /landmarks that a user can refer to, e.g. another building, rivers, parks, shops, etc. Landmark descriptions will play an im-portant role in future navigation systems (Kandan-gath and Tu, 2015).
 Predicting RE Success Rate ( SR RE ): Next, we analyse which spatial frames significantly con-tribute to task success, using multiple step-wise linear regression .We find that taxonomic and ab-solute properties significantly ( p&lt; 0 . 05 ) con-tribute to the success of a referring expression (Ta-ble 4). Semantic features explain more of the vari-ance observed in SR RE , than syntactic features. 4.3 Joint Model of REG Success Both syntactic and semantic features contribute to the success of a RE. Therefore, we construct a joint model for predicting SR RE using step-wise linear regression over the joint feature space. We find that both syntactic and semantic features sig-nificantly ( p&lt; 0 . 05 ) contribute to the success of a RE, see Table 4. This model explains almost half of the variation observed in SR RE ( R 2 = . 407 ). Clarke et al. (2013) reports an influence of visual salience on REG, therefore, in future, we will in-vestigate the influence of visual features. From the results presented, the following conclu-sions can be drawn for real-world NLG systems. Firstly, semantic features have a bigger impact on the success rate of REs than syntactic features, i.e. content selection is more important than sur-face realisation for REG. Secondly, semantic fea-tures such as taxonomic and absolute properties can significantly contribute to RE success. Tax-onomic properties refer to the type of target ob-ject, and in general depend on the local knowledge of the information giver. Similarly, the success of the RE will depend on the expertise of the infor-mation follower. As such, modelling the user X  X  level of knowledge (Janarthenam et al., 2011) and stylistic differences (Di Fabbrizio et al., 2008) is crucial. Absolute properties refer to object at-tributes, such as colour. Attribute selection for REG has attracted a considerable amount of at-tention, therefore it would be interesting to inves-tigate how these automatic attribute selection al-gorithms perform in real-world, interactive envi-ronments. Finally, the more complex scenes seem to justify longer and more complex descriptions. As such, there is an underlying trade-off which needs to be optimised, e.g. following the gener-ation framework described in (Rieser et al., 2014).
In future, we will compare existing REG algo-rithms on our dataset, in a similar experiment to Mitchell et al. (2013b). Then, we will extend ex-isting algorithms to take into account other prop-erties such as material (e.g.  X  X ooden X ), compo-nents of the referred object (e.g.  X  X alconies X ) etc. Finally, we will incorparate such an algorithm in interactive settings to investigate the influence of user dialogue behaviour and the influence of visual features, such as salience (Clarke et al., 2013), in order to improve the fit of our predictive model. This research received funding from the EPSRC GUI project Generation for Uncertain Informa-tion (EP/L026775/1). The data has been col-lected through the European Community X  X  Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement no. 216594 (SPACEBOOK project).

