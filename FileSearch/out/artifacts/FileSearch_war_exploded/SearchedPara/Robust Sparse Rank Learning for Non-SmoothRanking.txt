 Recently increasing attention has been focused on directly optimizing ranking measures and inducing sparsity in learn-ing models. However, few attempts have been made to re-late them together in approaching the problem of learning to rank. In this paper, we consider the sparse algorithms to directly optimize the Normalized Discounted Cumulative Gain (NDCG) which is a widely-used ranking measure. We begin by establishing a reduction framework under which we reduce ranking, as measured by NDCG, to the importance weighted pairwise classification. Furthermore, we provide a sound theoretical guarantee for this reduction, bounding the realized NDCG regret in terms of a properly weighted pairwise classification regret, which implies that good per-formance can be robustly transferred from pairwise classi-fication to ranking. Based on the converted pairwise loss function, it is conceivable to take into account sparsity in ranking models and to come up with a gradient possessing certain performance guarantee. For the sake of achieving sparsity, a novel algorithm named RSRank has also been devised, which performs L 1 regularization using truncated gradient descent. Finally, experimental results on bench-mark collection confirm the significant advantage of RSRank in comparison with several baseline methods.
 H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Theory, Algorithms, Experimentation Information Retrieval, Learn to Rank, RSRank, truncated gradient
Rank learning, which aims to automatically learn a rank-ing function according to documents X  relevance to a given query, plays a pivotal role in document retrieval. In order to credit ranking functions for their ability to retrieve highly relevant documents, evaluation measures from the viewpoint of information retrieval (IR) are developed, such as Winner Takes All (WTA)[17], Mean Reciprocal Rank (MRR)[17], Mean Average Precision (MAP)[2], and Normalized Dis-counted Cumulative Gain (NDCG)[11].

These widely-used IR measures characterize themselves by permutational structure dependent, which makes the task of ranking present a challenging research topic in supervised learning. Because permutational structure is only avail-able via ordering, and ordering is intrinsically discontinuous. Thus, any evaluation measure used to examine the goodness of a ranking function up to a given sort is non-smooth. It seems difficult to learn an efficient ranking function through directly optimizing evaluation measures, which really makes sense in practice.

Accordingly, great efforts have been made to address the issue of direct optimization of ranking measures whose pur-pose is mainly to follow IR measures to push the more rel-evant document closer to the top of the ranked list. The algorithms, which have been recently developed and vali-dated, can be divided into two categories. The methods in the first major category attempt to formulate an explicit smooth objective function which are then approached by various optimization strategies[8, 14, 16, 18]. The second category of methods, however, works with implicit objective functions which faciliate us to specify some rules about how to change rank orders for a given sorted instances, repre-sented by LambdaRank[6] which is also our concern.
LambdaRank which characterizes itself by an implicit loss function and an explicitly defined gradient provides several edges in working with loss functions that are either flat or non-differentiable, such as easy to implement and good per-formance in practice. However, there still remain some issues in need of further investigation.

First, there isn X  X  sufficient theoretical justification on the correlation between NDCG and the implicit loss function which is constructed by reducing ranking into importance weighted pairwise classification, thus lacking in a general guideline for generating gradients with desirable properties.
Second, few attempts have been made to touch on the is-sue of sparsity in ranking models, which means that some elements in model parameter vector are exactly zero. Along with the number of available ranking features mounts up, ranking models become more complicated and worse inter-pretable, which inspires a ranking model that not only fits the data well but can also achieve sparsity.

To tackle the problems above, this paper attempts to in-troduce reduction analysis and sparsification to approach rank learning problems under NDCG criteria.

We first validate the approach that reduces ranking, as measured by NDCG, to the importance weighted pairwise classification. In terms of regret analysis, we demonstrate that for arbitrary probability distributions over instances, the importance weighted pairwise classifier with regret r points to an NDCG regret of at most r . This implies that ranking algorithms based on such a reduction are robust in a sense that a small pairwise classification regret cannot lead to a large NDCG regret. In other words, pairwise clas-sification algorithm can be robustly translated into NDCG ranking algorithm, whereupon a general framework for gen-erating gradients with desirable properties can be derived.
When sparsity is concerned, it is natural to fall back on 1  X  norm regularization which enjoys sound theoretical ex-planation[22, 23]. To the gradient approximation oriented problem, however, simply adding L 1 regularization to the gradient cannot lead to sparsity[13]. Therefore, we further devise a novel ranking algorithm, referred to as RSRank, which performs L 1 regularization using truncated gradient descent for the sake of achieving sparsity. The empirical results indicate that even with sparse models, the ranking performance is still comparable to that of the standard gra-dient descent ranking algorithm.

Finally, we evaluate the proposed method on LETOR 3 . 0 benchmark collections[1]. Experimental results manifest that RSRank not only achieves good sparsity in practice, but also exhibits a high level of performance in comparison with several proposed baseline algorithms.
 The remainder of the paper is organized as follows. In Section 2 the related work is presented. Section 3 intro-duces some basic definitions and provides theoretical anal-ysis on the concerned reduction model. Section 4 discusses the design of RSRank algorithm, and empirical results are reported in Section 5. Section 6 concludes the paper and discusses the future work.
Direct optimization of IR measures has become one de-sirable direction for dealing with rank learning problems in recent years. The approaches that have been brought for-ward to handle this problem fall into two categories. For the approaches in the first category, [8, 9, 14] are under the large-margin structured learning framework and constructs an explicit objective function with constraints, [16, 10, 15] bypass an explicit sort and creates a smoothed objective with approximate ranking positions, and [18, 19] build an optimization objective after performing a sort and designs parameter update rules.

Instead of defining an explicit objective function, the sec-ond category of approaches resorts to a more straightforward way and simply focuses on a desirable change of each doc-ument X  X  score for a given sorted list. In other words, the gradients are only required to be specified with respect to an implicit objective function. This category, characterized by LambdaRank[6], not only benefit from easiness to imple-ment, but also from significant efficiency in applications of commercial interest. Given a pair of documents with differ-ent relevance degrees, LambdaRank specifies the gradient with respect to an implicit objective function, for exam-ple the RankNet cost scaled by the NDCG gain found by swapping the two documents in question. When applied to commercial search engines, this method performs well and enjoys significant benefits on training speed and accuracy. To improve the efficiency of LambdaRank in high dimen-sional search space, Yisong Yue etc. [20] use Simultaneous Perturbation Stochastic Approximation (SPSA) as the gra-dient approximation method and also examine the empirical optimality of LambdaRank.

This paper attempts to conduct a study on the property when reducing ranking, as measured by NDCG, to impor-tance weighted pairwise classification. In reality, analogous researches have been engaged in on reduction between classi-fication tasks[4, 5] and reduction from ranking, as measured by the Area Under the Receiver Operating Characteristic Curve (AUC), to binary classification[3]. For the proposed reduction transformations, regret analysis receives intensive investigations. Regardless of the number of samples needed to achieve a certain level of performance, regret analysis fo-cuses on relative error guarantees between general problems arising in practice and basic problems that are better under-stood, and apply when there are arbitrary high-order depen-dencies between samples. The derived bound have validated these reduction models which are robust in a sense that a small regret on the subproblem implies a small regret on the original problem. In addition, there are convincing empir-ical evidences that these reductions work well in practice, which offers further support to this style of analysis[12].
The problem of learning to rank can be described as be-low. Based on a given query, there is a corpus of documents with labels specifying the degree of relevance. The input to the ranker consists of a sequence of feature vectors con-structed from query-document pairs, and then the ranking function assigns a score to each item in the set, followed by producing a sorted list of items in descending order of scores. The quality of the sorted list is then evaluated by ranking measures used in the information retrieval community.
In order to evaluate the quality of the retrieved docu-ments, several ranking measures from the viewpoint of IR are developed, such as WTA, MRR, MAP and NDCG. WTA pays attention to whether the top ranked document is rel-evant. MRR is calculated according to the position of the top ranking relevant document. In the sight of MAP, the higher relevant documents are ranked, the better retrieved result gets, regardless of the fact that documents are not of equal relevance. NDCG, however, gives a high evaluation to the sorted list where highly relevant documents are ranked in front. More accurately, the NDCG value of a sorted list at position k is calculated as bellow: where Z k is the normalization constant so that the perfect ranking is evaluated as 1. r ( j ) is the label of the j th doc-ument in the sorted list. Let a ( j ) = 2 r ( j )  X  1 and b ( j ) = 1 /log 2 (1 + j ), equation (1) can be simplified as:
Apparently, the former three ranking measures are based on binary relevance judgments, while NDCG, based on graded relevance judgments, is more suitable for the user require-ments and is chosen as the ranking measure in this paper.
We begin by presenting the basic notation descriptions to put instances to be ordered, where each instance x i  X  R d is a feature vector, and S = { ( x 1 , y 1 ) , ( x 2 , y 2 denote the set of input instances and the corresponding rel-evance labels, with each label y i  X  { r 1 , r 2 , . . . , r total order relation exists, i.e. r l &gt; r l  X  1 &gt;  X  X  X  &gt; r X ordered pairs of different instances in X ( n ) , and the cor-responding set with relevance labels is written as S (2) = { (( x i , x j ) , 1 ( y i , y j )) | ( x i , y i ) , ( x 1 (  X  ) is 1 when the argument is true and 0 otherwise. Let f : R d  X  R denote the ranking function, and f ( x i ) indicate the score assigned to instance x i by the ranking function f . Let  X  denote a permutation of [1 , 2 , . . . , n ], and  X  the ranking position of instance x i .

We consider the problem of ranking in a reduction man-ner, referred to as NDCG reduction in this paper. The NDCG reduction reduces ranking, as measured by NDCG, to importance weighted pairwise classification. We first give definitions of the problems involved.

Definition 1. An importance weighted pairwise classifica-tion problem is defined by a distribution P over the set S . The goal is to find a pairwise classifier c : X (2)  X  { 0 , 1 } which minimizes the expected importance weighted w ( y i , y j , c ) denotes the importance weight for the ordered pair ( x i , x j ), and c ( x i , x j ) = 1 if c prefers x otherwise.

Definition 2. A NDCG ranking problem is defined by a distribution P over the set S . The goal is to find a ranker h : S  X   X  which maximizes the expected NDCG gain, E S X  P Z n  X  P j =1 a ( j )  X  b ( j ), where j denotes the index of input instances, a ( j ) = 2 y j  X  1 and b ( j ) = 1 /log 2 (1 +  X  j ).
Note that the number of relevance levels is usually deter-mined in advance, while there is no need to fix the size of input instance sequence in our analysis. In addition, the query variables which are not explicitly delivered, may still have underlying effect on the joint distribution P .
The procedure of NDCG reduction is composed of two components. During the training phase, NDCG-Train (Pro-cedure 1), takes as input a set of instances to be ordered in S . On the basis of a ranking function, any linear order-ing can be expressed as a collection of preference relations of type S (2) on this issue. The induced learning problem when encoded with the information about the loss of differ-ent pairs is to predict, given an arbitrary pair of instances, whether one instance should be ranked above or below the other. Thus, the induced distribution over S (2) can be de-fined by drawing an importance weighted pair from S which
Procedure 1 NDCG-Train (labeled set S , importan-ce weighted pairwise learning algorithm A )
Initialize a pairwise classifier c 0 . for t = 1 , 2 , . . . , T : end for Return c T .

Procedure 2 NDCG-Test (unlabeled set U , pairwise classifier c T )
For x i  X  X  , let
Sort U in ascending order of pos ( x i ). is randomly drawn from an underlying distribution P . We can represent the induced distribution as NDCG-Train( P ).
In fact, given a pairwise classifier, the importance weight is specified as where a ( i ) = 2 y i  X  1 , b ( i ) = 1 /log 2 (1 + pos ( x [ |{ x j : c ( x i , x j ) = 1 , x j  X  X ( n ) }| + 1] and Z ization factor.

During the test phase, NDCG-Test (Procedure 2), applies the pairwise classifier c T constructed in Procedure 1 to the unordered set U . The ranking position of one instance can be obtained by accumulating the times that each of the other instances in the set would beat it under the pairwise contest.
Before presenting the theorem in point, we will give the definition of the regret at first. Regret is the difference be-tween the incurred loss(gain) and the lowest(highest) achiev-able loss(gain) on the problem, which is commonly used to analyze the prediction accuracy of algorithms involving on-line learning [13] and reduction algorithm [3]. Unlike the sample complexity analysis, regret analysis for a reduction algorithm does not depend on any assumptions about the way that samples are generated, but focuses on the rela-tive error guarantee between the subproblem and the orig-inal problem. When the error transformation is bounded independently of the number of instances, the reduction al-gorithm can be regarded as robust [3]. Regret analysis is applicable, especially when there are arbitrary high-order dependencies between instances, such as ranking to be dis-cussed below.

The loss of the importance weighted pairwise classifier c on a set S = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x
Given any distribution P on S , the expected loss of the importance weighted pairwise classifier c is given by
Thus, the regret of c on P is defined as
The importance weighted classifier with minimum expected loss is referred to as the  X  X ayes optimal pairwise classifier X  in this paper. Similarly, taking S as input, the gain of the ranker h measured by NDCG is defined as
The notations of the right-hand in equation (6) are defined as before. Given any distribution P on S , the expected gain of the ranker h measured by NDCG is given by
Thus, the regret of h on P is defined as
The ranker with maximum NDCG expected gain is de-scribed as the  X  X ayes optimal ranker X  in this paper. As a pairwise classifier acts as the ranker, we are interested in whether a small importance weighted pairwise regret leads to a small NDCG regret.

Theorem 1. For all pairwise learning algorithms A and importance weighted datasets S 0 , let c = A ( S 0 ) . Then for all distributions P , r ( NDCG  X  Test (  X  , c ) , P ) &lt; r ( c, NDCG  X  Train ( P )) . The proof of the theorem is given in the appendix.
This theorem states that the NDCG regret is bounded by the importance weighted pairwise regret, which implies that good performance on the induced importance weighted learning problem can be robustly transferred to that on the ranking problem measured by NDCG.

With the error transformation guarantee, our goal in solv-ing ranking problem is to find a predictor f ( x ) minimizing the expected loss below.

Given a set of training examples S 1 , S 2 , . . . , S m , indepen-dently drawn from P , we generally consider the minimiza-tion of f with respect to the following empirical loss Observe that without the importance weight terms, various developed pairwise classification methods can serve the pur-pose with sound theoretical basis.

In order to obtain the values of importance weights, the ordering information is encoded, which challenges the com-mon convex optimization strategies. We approach this issue by simply specifying the sub-gradients of the loss which com-bines importance weights with the pairwise loss functions of general interest after the sort, and then updating.
As can be seen above, the NDCG reduction gives us a simple and quite general way to cope with rank learning problems evaluated from the user point of view. To learn an effective ranking function, in this section, we follow such a reduction manner and further devise a novel ranking algo-rithm referred to as RSRank.
Reducing ranking to importance weighted pairwise clas-sification, each ordered pair is assigned with an importance value which measures how important it might be in com-parison with the other ones generated from a permutation. Hence, given a set of instances with cardinality n , written as S , we measure the loss as follows, where w ( y i , y j , f (  X  )) is the importance weight as defined in equation (3) except for a tiny difference in notations. w ( y i , y j , f (  X  )) = Z n  X  (2 y i  X  2 y j )  X  ( 1 Note that Z n is defined as before and a permutation  X  can be obtained in descending order of f (  X  ).

We then follow the idea of classification by substituting the indicator function 1 (  X  ) with a well-behaved loss func-tion. Intuitively, for the ordered pair  X  ( x i , x j ) , 1 ( y the ranking function f should receive no punishment when matching the given preference well. Otherwise, there is no need to exert too much punishment for the rare cases when f is quite inconsistent with the preference. To achieve these ends, we take the modified Huber loss function[21] as our choice, assuming that y i &gt; y j , v = f ( x i )  X  f ( x
By replacing the indicator function in equation (12) with the above modified Huber loss, we get the loss function
In this paper, we assume that f is a linear combination of model parameters, i.e., f ( x i ) =  X   X , x i  X  , where  X   X  R model parameter vector, and  X  X  ,  X  X  denotes the inner product.
Statistically, the solution of  X  tends to overfit the train-ing examples without a regularization condition which im-poses constraints on the parameter space. One important constraint is sparsity with desirable properties in practice. 1-norm regularization which often leads to sparse solutions is applied in our model. Given a set of training examples S , S 2 , . . . , S m , the objective function is formulated as where g is a non-negative regularization parameter making a tradeoff between the loss on the training set and the penalty on model parameters.
In order to solve (14), the sub-gradients of the loss, ex-pressed as  X  L (  X  ), are calculated after each sort, yet sim-ply adding L 1 regularization to the gradients cannot lead to sparsity. Therefore, we further devise the following ranking algorithm, referred to as RSRank, which performs L 1 reg-ularization using truncated gradient descent for the sake of achieving sparsity.
 Algorithm 1 RSRank Algorithm
Inputs:
Initialize model parameter vector  X  = [  X  1 , . . . ,  X  d for t = 1 , 2 , . . . end for
The main idea that motivates RSRank algorithm is de-rived from the work [13] in which truncated gradient is ini-tially proposed to induce sparsity for online learning algo-rithms. When applied into our algorithm focusing on a de-sirable gradient for a given sorted list, it is also effective. We will demonstrate its validity in the following experiments. In this section, we evaluate the proposed algorithm on OHSUMED and TD2003 data in Letor3.0 benchmark collec-tion released lately[1]. The goals of our experiments mainly include two aspects where one is to compare our algorithm without sparsification to the existing baseline results and the other is to examine the efficiency of our algorithm with sparsity option.
The OHSUMED dataset collected from medical publica-tions contains 106 queries and 16,140 query-document pairs in total. For each query-document pair, there are 45 rank-ing features extracted and a 3-level relevance judgement provided, i.e. definitely relevant, possibly relevant or not relevant. In the TD2003 dataset constructed from web en-try pages in topic distillation tasks, altogether 50 queries and 49,171 query-document pairs are included. Each query-document pair comprises 64 ranking features and a binary relevance judgement, i.e. relevant or not relevant.
For each dataset, we measured the performance of our al-gorithm averaged over five folds off-the-shelf, each of which consists of training, validation, and test set. The validation set was used to identify the best set of parameters, including the initial model parameter vector  X  , the regularization pa-rameter g , and the number of iterations t , which were then verified on the test set. We fixed the learning rate  X  = 0 . 001 and the truncation step interval K = 1 in these experiments.
Several common baseline algorithms were chosen as com-parisons to our algorithm without sparsification, including LambdaRank[6], AdaRank.NDCG[18] and ListNet[7]. [6] and [18] allow direct optimization of NDCG criteria, while [7] attempts to optimizes a listwise loss function. In addi-tion, the ranking functions that these baseline methods tar-get at are of linear relationship with the model parameters, the same as our method.
In the first set of experiments, we are interested in how our ranking algorithm performs, which is based on a robust reduction mechanism in theory, and no sparsification is con-sidered, i.e. the parameter g is set to zero. The experimental results on OHSUMED and TD2003 dataset are illustrated in Figure 1-2.
Figure 1: NDCG at top 10 on OHSUMED dataset Figure 2: NDCG at top 10 on TD2003 dataset
The above two figures show that RSRank consistently outperforms the reference baseline algorithms in terms of NDCG at top 10. For OHSUMED dataset, the improvement of RSRank over LambdaRank is also statistically significant for NDCG@3 to NDCG@10 with a p-value of less than 0.032 in t-test, which indicates that the excellent effectiveness of our ranking approach following the NDCG reduction prac-tice. However, for TD2003 dataset, RSRank behaves a bit differently. By comparison with LambdaRank, the perfor-mance of RSRank is not superior statistically in terms of t-test, in spite of rising around 6 percent on NDCG@1. It is yet noteworthy that our algorithm still keep obvious advan-tages over AdaRank.NDCG and ListNet.
The next set of experiments is designed to demonstrate the behavior of RSRank when working with sparse rank-ing models. First and foremost, we are concerned about how much degree of sparsity can be achieved without af-fecting ranking performance significantly. To this end, we performed RSRank from the same initial model parameters in one fold simply changing the setting of regularization pa-rameter. Table 1 gives the results on the fraction of features left after sparsification. It is interesting to observe that the number of features is reduced by a fraction of more than 60% on average, while the accuracy loss in NDCG at top 10 decreases by no more than 3% on average, as shown in Figure 3.
 Table 1: Fraction of features left after sparsification for OHSUMED and TD2003 datasets when NDCG at top 10 is changed by less than 3% on average.
 fraction of features left 0.36888 0.34062 Figure 3: The ratio of the NDCG at top 10 when sparsification is used over the case when no sparsifi-cation is used on OHSUMED and TD2003 datasets.

Specifically, for OHSUMED dataset, the NDCG metrics on the test set drop at most 3 . 5% with more than 63% of features removed, indicating that only a fraction of features are enough for ranking at a slight cost of prediction accuracy loss. Especially, the NDCG value with sparsification hits its peak value of 0 . 5605 at the first rank position in comparison with 0.5613 without sparsity option, which is still compa-rable to the reference baseline algorithms. Meanwhile, for TD2003 dataset with more ranking features added, a qual-itatively similar observation can be derived. Besides, we found that the NDCG criteria at the second rank position was actually improved from 0.39 to 0.40 due to removal of some redundant features. Better results can be expected if more extensive parameters are tried in cross validation.
The other aspect we are concerned about is the influence of regularization parameter g when performing truncated gradient. Given a regularization parameter g , the initial weight parameter vector that performed best on the valida-tion set was evaluated on the test set. Figure 4 and 5 plot the curves on NDCG criteria at the tenth rank position and the number of features selected. Based on the experimental results, we can make the following observations. Figure 4: The trajectories of training NDCG@10 and number of nonzero coefficients versus regular-ization parameter g on OHSUMED training dataset Figure 5: The trajectories of test NDCG@10 and number of nonzero coefficients versus regularization parameter g on OHSUMED dataset
First, more large regularization parameter g is, more de-gree of sparsity we can achieve. When g is set to zero, the linear composition of all the ranking features is used for ranking prediction. When g is set to 250, only 3 features or so exercise the power of ranking prediction.

Second, as the regularization parameter g increases, the variations of the evaluation measures in terms of NDCG@10 hold within a range of less than 0 . 75% for training set and less than 2% for test set. In other words, better spar-sity leads to a approximately equal level of performance maintained on OHSUMED dataset, which shed light on the strength of our ranking algorithm in representing sparse models for ranking.
In this paper, we introduced a robust and sparse mech-anism to deal with the rank learning problem under the NDCG criteria. After describing an efficient reduction of ranking to importance weighted pairwise classification, we bound the NDCG performance in terms of the induced pair-wise classification performance. With the aim of achieving sparsity in ranking models, we then constructed the objec-tive function composed of the converted pairwise loss func-tion and an additional 1-norm regularization penalty. We also developed a novel ranking algorithm via truncated gra-dient, called RSRank. Empirical studies on benchmark col-lection justified the effectiveness of the proposed algorithm.
In future work, we will continue to study the reduction-based model for ranking measured by other non-smooth ranking measures besides NDCG, also try other sparse learn-ing methods in rank learning problems, and pursue further theoretical investigation on the ability of algorithms to pro-duce sparse ranking models.
This work was supported partially by 973 Program (No. 2004CB318103), NSFC Projects (No. 60835002 and 60621001) and by Microsoft Research Asia Funding. [1] M. R. Asia. Letor3.0: benchmark datasets for learning [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] M.-F. Balcan, N. Bansal, A. Beygelzimer, [4] A. Beygelzimer, V. Dani, T. Hayes, and J. Langford. [5] A. Beygelzimer, V. Dani, T. Hayes, J. Langford, and [6] C. Burges, R. Ragno, and Q. Le. Learning to rank [7] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. [8] S. Chakrabarti, R. Khanna, U. Sawant, and [9] O. Chapelle, Q. Le, and A. Smola. Large margin [10] J. Guiver and E. Snelson. Learning to rank with [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] J. Langford and A. Beygelzimer. Sensitive error [13] J. Langford, L. Li, and T. Zhang. Sparse online [14] Q. Le and A. Smola. Direct optimization of ranking [15] T. Qin, T.-Y. Liu, and H. Li. A general approximation [16] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [17] E. Voorhees. Overview of the trec 2002 question [18] J. Xu and H. Li. Adarank: a boosting algorithm for [19] J.-Y. Yeh, J.-Y. Lin, H.-R. Ke, and W.-P. Yang. [20] Y. Yue and C. Burges. On using simultaneous [21] T. Zhang. Statistical behavior and consistency of [22] T. Zhang. Some sharp performance bounds for least [23] P. Zhao and B. Yu. On model selection consistency of Proof of Theorem 1
Given an unlabeled test set U , the joint distribution P induces a conditional distribution P ( y 1 , y 2 , . . . , y the set of relevance label sequences { r 1 , r 2 , . . . , r as D . In the following, we identify U with { 1 , 2 , . . . , n } . Due to the existence of a ranking function, any pairwise classifier can equally induce a permutation. The following lemma is the essential step in our analysis.

Lemma 1. For all conditional distributions D , suppose that the pairwise classifier c induces an imperfect permu-tation  X  , while the pairwise classifier c 0 induces a perfect permutation  X  0 , then the following relationship holds, Note that there is generally more than one perfect permu-tation, and we make no differential treatment of notations for simplicity.

Proof. For any imperfect permutation  X  with inversion pairs of k  X  N , there exists at least one inversion pair(  X  through swapping which the number of inversion pairs de-creases only by one. Thus,  X  can be transposed into a perfect permutation  X  0 via k such operations denoted as  X  .
This allows us to decompose the left hand of the above inequality in the following way. Let  X  (0) =  X  , and  X  (1)  X  G (  X  (1) , D )  X  G (  X  (0) , D ) For permutation  X  (1) , suppose that swapping inversion pair (  X 
G (  X  (2) , D )  X  G (  X  (1) , D ) Similarly, the rest may be deduced by analogy.
G (  X  0 , D )  X  G (  X  ( k  X  1) , D ) Whence, telescoping the sum of the above equations, we obtain
G (  X  0 , D )  X  G (  X , D )
As to the right hand of the inequality, it can also be ex-pressed in terms of inversion pairs.

L ( c, NDCG  X  Train ( D ))  X  L ( c 0 , NDCG  X  Train ( D )) Observe that although the both sides of the inequality share the same inversion pairs ( i 0 , j 0 ) , ( i 1 , j 1 ) , . . . , ( i position deviation between elements in each inversion is dif-ferent. It is easy to see that the following inequalities hold,  X  The first inequality follows from the monotone of the func-tion 1 /log 2 (1 + x )  X  1 /log 2 (2 + x ). Summing over m = 1 , 2 , . . . , k  X  1, we get the lemma.
 Proof. (of theorem 1) Note that we represent NDCG-Train(D) as N-T(D) in short. In terms of the definition of regret with respect to the ranker h and the pairwise classifier c aforementioned, we have r ( h, D ) = max The fist inequality follows by applying Lemma 1 to obtain the desired bound.
