 LingoTurk is a crowdsourced experiment manage-ment system aimed at the use case where the ex-periment user interface must be highly customized. Common web browsers now permit the design of experimental user interfaces with highly sophisti-cated presentations, allowing crowdsourcing envi-ronments to be used as laboratories for psycholin-guistic experimentation with paradigms that in the recent past could only be run  X  X n-lab X .

Crowdsourcing in language science was origi-nally popularized among researchers for the collec-tion of labelled training data. It has recently gained popularity as a platform for collecting experimen-tal data for cognitive modeling (e.g., Gibson et al., 2013; Kush et al., 2015). Experimenters trade di-rect control over subject demographics and envi-ronment for faster, cheaper experiment completion with many more subjects. Crowdsourcing can pro-vide experimenters with a way to access populations which are not locally available (e.g., native speak-ers of a non-local language). Commercial platforms provide micropayment architectures to provide re-wards to users. They also manage abuse, track user reliability, track (usually-pseudonymized) identities, and recruit participants.

We designed LingoTurk to handle the condition where the actual experiment must be hosted outside of the  X  X efault X  systems provided by crowdsourc-ing platforms. This is motivated by cases in which there is functionality not directly supported by the crowdsourcing platform but can be managed exter-nally, such as the separation of experimental condi-tions or the storage of specialized data types. Inso-far as common crowdsourcing platforms support ex-ternal interfaces, LingoTurk provides an easily de-ployed server-side solution to external experiment management. As its administration functions are also web-based, LingoTurk allows for the steering and management of crowdsourcing experiments to be performed without strong technical skills, such as student assistants in non-technical majors.
The source code is made available at https://github.com/FlorianPusse/ Lingoturk . 1.1 Crowdsourcing and language science Crowdsourcing has been a trend in language re-search for the better part of a decade and has prin-cipally been focused on the collection of annotated training data for supervised machine learning in fields like machine translation (Zaidan and Callison-Burch, 2011) and opinion mining (Sayeed et al., 2012). In these areas, the psyche of the annotators is not the principal object of interest.

This has consequences of the relationship of the  X  X equester X  X  X mazon X  X  term for the task designer X  to the crowdsourcing worker. When psycholinguis-tic experiments are crowdsourced, the objects of in-terest are no longer directly the  X  X nnotations X  them-selves, but rather what they reveal about the humans who made them. This means that the relationship of the requester to the workers is quite different from what it is in annotation efforts: qualification for the task is replaced by qualification for the study, and annotator reliability is augmented by the need for experimental control.

Psycholinguistic experiments attempt to confirm a hypothesis about the way in which linguistic stim-uli are evaluated by the human mind. Experimen-tal items are therefore often separated by condition Normally each subject should only see one item in each condition.

Furthermore,  X  X earning effects X  are often a risk in psycholinguistic experiments. Subjects can get used to the experimental paradigm, and as time goes on, their responses can be said to become less and less the spontaneous reaction of linguistic cognition.
Consequently, fine-grained control over condition presentation and worker exclusion are desiderata of a crowdsourcing platform for psycholinguistic ex-perimentation. LingoTurk is designed to address this need via the self-hosting of experiments while of-fering integration with existing crowdsourcing plat-forms. 1.2 Crowdsourcing platforms Amazon Mechanical Turk (MTurk) is the earliest, most widely-used crowdsourcing platform. MTurk provides a set of standard task designs for crowd-sourcing as well as the option to create a custom task. Custom tasks can be hosted on an external server, if they are served to Amazon via the MTurk API.

However, MTurk lacks the architecture for exper-imental exclusion of workers (subjects) by condi-tion. Nevertheless, its API provides the information to construct one server-side, if experimenters host the task on their own server. The MTurk API also permits the experiment interface to appear as a pane inside the MTurk interface, allowing subjects to ex-perience the task seamlessly.

Prolific Academic (PA) is an alternative platform, useful for needs currently unmet by Amazon, partic-ularly a non-US-centric clientele. PA does not pro-vide an API that allows for full external-question in-tegration, but it allows for worker redirection that permits similar server-side participant tracking. 1.3 Comparison to alternative systems There are other server-side experiment publishing platforms aimed at psychological and psycholin-guistic research: for example, Ibex (Drummond, 2013) and PsiTurk (McDonnell et al., 2012). These platforms have functions that overlap with Lingo-Turk, but do not cover LingoTurk X  X  full design goals. LingoTurk provides an administration GUI front end for experimenters (figure 1), so that the day-to-day management does not have to be per-formed via the command line by technically-skilled researchers. LingoTurk is also integrated with the Play Framework, which is intended to accelerate the development of complex, highly scalable web applications using a well-engineered Model-View-Controller (MVC) paradigm in Java and Scala. The MVC paradigm facilitates the development not only of the subject-facing experiment UI, but also user-friendly experiment item entry and testing views for stimulus preparers. The MVC paradigm combined with database in-tegration makes LingoTurk a platform for reliably engineered experimental interfaces that can han-dle complex data structures as well as easy object-oriented extensibility. LingoTurk is intended for a self-hosting use case; once a web server has been set up, the Play Framework enables LingoTurk to be a turn-key solution for experiment administration.
Figure 2 shows the design of LingoTurk in terms of its overall workflow. LingoTurk manages com-munication with the crowdsourcing platform as well as governs interactions with a participant database which keeps track of experimental conditions and exclusions and an item database that keeps track of stimuli and responses. LingoTurk selects the pages to be served to the crowdsourcing platform based on information provided by the platform. On MTurk, that means that a worker who is ineligible to see an experimental condition will be presented with a page that informs of them of this and asks them to return the task.

Creating a LingoTurk experiment based on an ex-isting interface type (section 3) is done from the graphical administrative console, which is itself a web site. The experimenter instantiates an exper-iment type and fills the stimuli into web forms that are designed to handle experimental conditions. The experimenter also uses the administrative interface to provide credentials for the crowdsourcing plat-form as well as to preview and publish the experi-ment and retrieve the results. Excluded worker IDs (such as those who participated in previous runs of the experiment) can also be uploaded to Lin-goTurk this way. New experiment designs can be developed and added to the interface using Play Framework-based HTML and Scala templates; com-mon Javascript libraries are provided by default, and an API is provided for server communication.
LingoTurk also allows for the creation of quality control questions that can be used to exclude poorly-performing workers after a threshold of wrong an-swers is reached. To use this feature, the experi-menter must include stimuli with correct or expected responses. LingoTurk has been used for experiments performed by researchers at Saarland University. Here, we discuss a couple of examples of interfaces that are included with LingoTurk. We provide other paradigms in the package.
 Drag-and-drop discourse connectives Demberg et al. (2015) present the problem of discourse rela-tion prediction: specifically, how do speakers inter-pret an implicit gap between propositions? They in-vestigated this question through an experiment that allowed subjects to fill in the gap between two sen-tences, implicitly connected in the Penn Discourse Treebank, with an explicit discourse connective. For this purpose, they used a drag-and-drop paradigm, wherein subjects selected connectives from a set of labelled tiles and dragged them into a target zone (figure 3). They divided this task into phases in or-der to narrow down the discourse type, with Lingo-Turk presenting a subsidiary tableau of connective phrases depending on the result of the first tableau. The selection of connective tableaux is controlled via the item entry interface on the administrative side of LingoTurk. Data analysis for this task is still on-going.

The advantage of a drag-and-drop paradigm is that it requires the subject to make an explicit choice but also to use a little bit of effort in doing so. This reduces the bias that might be introduced by the least-effort of choosing the first or the last item (Say-eed et al., 2011).
 Script alignment by connector drawing Wan-zare et al. (2016) use the LingoTurk system to present a task involving the alignment of collected narratives to prepared scripts (e.g., for baking a cake) for an on-going project that is investigating the psycholinguistic aspects of script knowledge as well as developing script corpora. In this paradigm, steps of the narrative (an account of an action col-lected from subjects during previous research) are presented as tiles in a column on the left side of the window, while steps of the standardized script are presented on the right side. Subjects draw connec-tions between narrative steps and standardized script steps. For our demonstration at the conference, we will bring a computer and present a running instance of the LingoTurk system. We will proceed through the construction of example experiments which will be pushed through to the MTurk Sandbox (MTurk X  X  testing server). We will make use of our built-in ex-perimental paradigms to demonstrate the versatility and convenience of LingoTurk. We will also demon-strate the underlying practical details of developing new experimental paradigms and integrating them into LingoTurk. There are considerable opportunities to expand the system. One possible direction is integration with other platforms that have been gradually emerging, such as ClickWorker. We are exploring the possibil-ity of integration with the CrowdFlower platform; an important challenge in this case is the integration with CrowdFlower X  X  built-in quality control system. Another direction is increasing the customisability of experiment designs by including a graphical web design tool, reducing the need to interact directly with the Play Framework when developing new ex-perimental paradigms. Demberg, V., Sayeed, A., and Pusse, F. (2015). Dis-course annotation via mechanical turk. In First
Action Conference of the TextLink COST Initia-tive , Louvain-la-Neuve, Belgium.
 Drummond, A. (2013). Ibex farm. Online server: http://spellout.net/ibexfarm .
 Gibson, E., Bergen, L., and Piantadosi, S. T. (2013).
Rational integration of noisy evidence and prior semantic expectations in sentence interpretation.
Proceedings of the National Academy of Sciences , 110(20):8051 X 8056.
 Kush, D., Lidz, J., and Phillips, C. (2015). Relation-sensitive retrieval: Evidence from bound variable pronouns. Journal of Memory and Language , 82:18  X  40.
 McDonnell, J., Martin, J., Markant, D., Coenen, A.,
Rich, A., and Gureckis, T. (2012). psiturk (ver-sion 1.02)[software]. new york, ny: New york university.
 Sayeed, A. B., Boyd-Graber, J., Rusk, B., and Wein-berg, A. (2012). Grammatical structures for word-level sentiment detection. In Proceedings of the 2012 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies , NAACL HLT  X 12, pages 667 X 676, Stroudsburg, PA, USA. Association for Computational Linguistics.
 Sayeed, A. B., Rusk, B., Petrov, M., Nguyen, H. C., Meyer, T. J., and Weinberg, A. (2011).
Crowdsourcing syntactic relatedness judgements for opinion mining in the study of information technology adoption. In Proceedings of the 5th ACL-HLT Workshop on Language Technology for
Cultural Heritage, Social Sciences, and Human-ities , LaTeCH  X 11, Stroudsburg, PA, USA. Asso-ciation for Computational Linguistics.
 Wanzare, L., Zarcone, A., Thater, S., and Pinkal,
M. (to appear 2016). A crowdsourced database of event knowledge sequence descriptions for the acquisition of high-quality script knowledge. In Language resources and evaluation conference , Portoro  X  z, Slovenia.
 Zaidan, O. F. and Callison-Burch, C. (2011). Crowd-sourcing translation: Professional quality from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies -Volume 1 , pages 1220 X 1229, Stroudsburg,
PA, USA. Association for Computational Lin-guistics.
