 DBLP and CiteSeer, tagging networks such as Bibsonomy and Delicious, video shar-discovery problems which are useful for fulfilling different suggestion or recommen-sors, finding program committee members for conferences etc. words and co-authors for author A. 
Previously, three major frameworks used to identify the author interest are (1) sty-structure based methods by using keywords as a basis for representation and analysis grouping at group level. GAT which models the author interest and relationships by considering both type of problem and usefulness of our method. We should mention that exploitation of author ledge discovery problems [4]. 
The major contributions of our work described in this paper are the followings: (1) formulization of author interest finding problem from subgroup to group level relationship between perplexity and entropy. sions and section 4 concludes this paper. briefly introduce topic modeling idea followed by author-topic model, inverse-author-topic model and conditionally-independent-author-topic model. 2.1 Topic Modeling w of topic given document for each word with word having generated probability of word of-the-art topic model Latent Dirichlet Allocation is given in Eq. 1. 2.2 Author-Topic Model (AT) AT a randomly chosen author from a subgroup is responsible for generating words of topically related authors but did not consider explicit group information. 2.3 Inverse-Author-Topic Model (IAT) IAT is a two way stochastic process which is based on the idea that a randomly cho-not consider explicit group information. 2.4 Conditionally-Independent-Author-Topic Model (CIAT) CIAT is based on the idea that words and authors are independently generated from a IAT assumes that randomly chosen author or word generates a topic, respectively. On the contrary CIAT assumes that authors and words are independently generated by the Eq. 4. CIAT did not consider explicit group information. 2.5 Group-Author-Topic Modeling (GAT) randomly chosen author from a group is responsible for generating words of a group. ( w author (s) of subgroup d i . 
Subgroup based methods considers that an author is responsible for generating la-given in Eq. 5. The generative process of GAT is as follows: For each author r = 1 ,..., K of group g Choose  X  r from Dirichlet (  X  ) For each topic z = 1 ,..., T Choose  X  z from Dirichlet (  X  ) For each word w = 1 ,..., N g of group g Choose an author r uniformly from all authors a g Choose a topic z from multinomial (  X  r ) conditioned on r Choose a word w from multinomial (  X  z ) conditioned on z and r is given by:  X  instance. words w and new topics z conditioned on w and z . 3.1 Corpus We downloaded five years paper corpus of conferences from DBLP database [6], by total, we extracted 112,317 authors and 90,124 papers. We then processed corpus by 26,078 authors in the corpus. mats. For example, for some very common names there can be multiple authors (e.g. this work we do not focus on name disambiguation problems. 3.2 Parameter Settings 3.3 Performance Measures We used three performance measures for evaluating the performance of methods from ranking accuracy; a performance measure for evaluating recommendation is given in Eq. 11. We employ the top-k recommendations, that is, each ranking algorithm needs original set. 3.4 Baseline Methods for generating words of a document, IAT, which considers that words and authors of a authors of a document, and CIAT, which considers that words and authors of a docu-and authors of a document. 3.5 Results and Discussions vocabulary (semantic, web, ontology, owl, rdf, annotation, semantics, and knowledge) this topic. 
GAT also discovered several other topics such as image retrieval, neural networks, business process modeling, semi-supervised learning and XML databases. In addition, by doing analysis of authors X  home pages and DBLP [10], we have found that all au-related to semantic web topic for authentication. http://www.cs.manchester.ac.uk/~carole/ http://www.cs.manchester.ac.uk/~stevensr/ http://semanticweb.org/wiki/Peter_Haase http://knoesis.wright.edu/amit/ http://www.uni-koblenz.de/~staab/ 3.5.1 Perplexity Based C Perplexity is a standard m e models. It shows generaliz a perplexity corroborate bett e
Fig. 3 on the left side p r of Z . GAT performs better formance difference betwe e under root of perplexity w mance difference between over baseline methods. Fig . for author interest finding t the unseen dataset.
 3.5.2 Entropy Based Co m Fig. 4 provides a quantita t IAT. Fig. 4 (left) shows th e for all topics calculated by u T = 20,40, ...300 proves t h compared to baselines. W e mance of GAT, AT and C I can see a clear performan c group and group structures performance of method [5] .

Fig. 4 (right) shows the all topics calculated by usi n IAT entropy is lower than o see that when number of t CIAT is same but when th e of GAT with baselines is o b 3.5.3 Prediction Accura c We show quantitatively th e words and authors of doc u with AT, CIAT and IAT fo for number of topics varie d GAT, 0.49 for AT, 0.49 fo r better than AT and CIAT which show the better pe r accuracy results for autho r and 0.50 for IAT which s h than CIAT, and 4% better t exploiting subgroup and g r tive power of topic mode l predicting words and autho r This study deals with the grouping existing in co-au t effective. GAT uses bot h baselines for several performance measures from different domains. We can say that words and authors. on the Web by using novel methods. Pakistan. 
