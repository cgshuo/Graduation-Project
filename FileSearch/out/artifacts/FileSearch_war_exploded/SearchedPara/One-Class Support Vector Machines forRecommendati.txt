 Recently, the importance of recommender s ystems has increased rapidly with the growing availability of online information on the Web. Customers visiting the largest e-commerce sites often have difficulty in finding a particular item among the enormous number of products for sale. Many recommender systems [5, 8] have been installed to filter out irrelevant products and locate products that might be of interest to individual customers.

Collaborative filtering is one of the most successful technologies for recom-mendation tasks, in which customer ratings on products or historical records of purchased products are exploited to extract the preferences of individuals. Collaborative filtering calculates sim ilarities between customers based on the customer rating, or the purchased products patterns of each individual. Col-laborative filtering then finds a set of the most similar patterns, and recom-mends products for a particular individual. In the present paper, we provide new approaches for recommendation tasks using kernels defined on a graph that represents the relationships between the products.

Very recently, Fouss et al. [3] introduced a graph kernel, referred to as the commute time kernel and directly applied the kernel-based dissimilarities to the recommendation task. More p recisely, they defined the kernel over a bipartite graph with two sets of nodes corresponding to a set of customers and products. They placed edges between the custome r nodes and the product nodes when the customer has purchased the product. They defined a random walk model over this graph by assigning the transition probabilities over the edges. They showed that the average commute time between the two nodes is given by the kernel and that it can be used as a distance measur e between the corresponding customer and product.

In the present paper, we use the 1-SVM with graph-based kernels to select relevant products for each customer. We introduce new formulations for the 1-SVM that can efficiently manipulate sever al recently developed graph kernels, such as [11, 10, 1, 4]. In addition, we show that a special case of our formula-tion does not require any optimization calculations. More importantly, the new kernel matrix is significantly smaller than that of the method reported in [3], which enables us to apply the present approach to large e-commerce sites with a practical amount of computation.

In Sect. 2, we briefly review the standard formulation of the 1-SVM and its ba-sic settings for recommendation tasks. In Sect. 3, we describe v arious graph ker-nels, and in Sect. 4, we introduce new formulations for the 1-SVM. Experiments using a movie dataset are presented in Sect . 5, and conclusions are presented in Sect. 6. The SVM was originally designed as a method for two-class classification prob-lems. In this section, we will describe a variant of the SVM, called the one-class SVM (1-SVM) [7], which can handle problems that consider a single class of data points.

Suppose that we have a set of N -dimensional data points x j  X  R N ( j = 1 , 2 ,...,l ). Also, assume that we have a function  X  (  X  ): R N  X  X  that maps the data points into a higher-dimensional feature space , denoted by F . Hereinafter, for simplicity, we denote the mapped image  X  ( x j )as  X  j .Let w  X  X  and  X   X  R . Also, the inner product in F is denoted as  X  ,  X  . The purpose of the 1-SVM is to calculate a hyperplane that holds most of the data points in its positive side, i.e., w , X  j  X   X &gt; 0.

Introducing additional variables  X  =(  X  1 , X  2 ,..., X  l ) T , w and  X  are obtained by solving the following quadratic programming problem: where  X   X  (0 , 1] is a predefined positive parameter. Let ( w  X  , X   X  )denoteanop-timal solution of the problem (1). When a data point, the mapped image of which is denoted by  X  , belongs to the negative side of the hyperplane, i.e., w  X  , X  +  X   X  &lt; 0, the pattern can be considered to be different from the given single class of data points.

The objective of the recommendation task is to find products that have not yet been purchased but that would likely be purchased by a specific customer, hereinafter referred to as an active customer. Suppose that we are given a set of that are rated as preferable products, or that have actually been purchased by the distance from the hyperplane calculated as ( w  X  , X  i +  X   X  ) / w  X  , w  X  can be used as a preference score of the product i . Ignoring the constants, one can use the inner product w  X  , X  i as a score to rank the product i for the specific active customer a .

Generating a nonlinear map  X  (  X  ) is quite important in SVM. Usually, this is done implicitly by kernels that are naturally introduced by the following dual formulation of the problem (1).
 be defined using only the values of the inner products, without knowing the to the dual problem. Then, the associated optimal primal solution is given as w
Let K = { K ij } be a symmetric matrix called a kernel matrix, which consists of the inner products  X  i , X  j as the i  X  j element. Any positive semidefinite matrices K can be used as kernel matrices. It has been shown that positive semidefiniteness ensures the existence of the mapped points,  X  i s (see, for exam-ple, [9]). Recently, several studies [11, 10, 1, 4] h ave reported the development of kernels using weighted graphs. In this section, we will review such kernels.
First, let us introduce a weighted graph G ( V, E ) having a set of nodes V and a set of undirected edges E .Thesetofnodes V corresponds to a set of data items such as products in a recommendation task. For each edge ( i, j )  X  E ,a is assigned. We assume that the larger the weight b ij , the greater the similarity between the two nodes. Let M be the number of nodes in V ,andlet B be an M  X  M symmetric matrix with elements b ij for ( i, j )  X  E. Note that if there exists no edge between i and j ,thenweset b ij =0.
 Next, let us introduce the Laplacian matrix L of the graph G ( V, E )as L = D  X  B, where D is a diagonal matrix, the diagonal elements d ii of which are the sum of the i th row of B , i.e., d ii = j b ij . Throughout this paper, we assume that the graph G ( V, E ) is connected.

There are several methods for gen erating kernel matrices based on L .Fouss et al. [3] considered a random walk model on the graph G ,inwhich,foreach considered the ave rage commute time n ( i, j ), which represents the average num-ber of steps that a random walker, starting from node i , will take to enter node j for the first time and then return to node i . They indicated that the average commute time n ( i, j ) can be used as a dissimilarity measure between any two data points corresponding to the nodes of the graph, and that n ( i, j )isgivenas of the Moore-Penrose pseudoinverse of L , which is denoted by L + .Fouss et al. [3] also showed that as long as the graph is connected, the pseudoinverse L + is explicitly given as follows: where e is a vector of all ones. Since L is positive semidefinite [2], so is its pseudoinverse L + , which implies that L + can act as a kernel matrix [3].
Here, L and L + share the common eigenvectors. Let v 1 , v 2 ,..., v M and  X  , X  2 ,..., X  M be the eigenvectors and the corresponding eigenvalues of L ,re-that the pseudoinverse is also given as
Several variants of the above equation have been proposed. Smola &amp; Kondor [10] introduced the following regularized Laplacian kernel matrix Moreover, by introducing the modified Laplacian L  X  =  X D  X  B with a parameter 0  X   X   X  1, Ito et al. [4] defined the modified Laplacian regularized kernel matrix as In particular, when  X  = 0 this kernel matrix is the von Neumann diffusion kernel, which is defined as Next, we will describe recommendation methods based on the 1-SVM using the kernel matrices K described in the previous section. Recall that we are given a set of M products P = { 1 , 2 ,...,M } and a subset P ( a )  X  P , which have been purchased by the active customer a . We assume that P ( a )= { 1 , 2 ,...,l } .In addition, the elements of the kernel matrix K represent the inner products of the feature vectors corresponding to the products.

Let us first rewrite the primal formulation. To this end, introducing M vari-ables  X  =(  X  1 ,  X  X  X  , X  M ) T , let us assume that w  X  X  is given as a linear com-bination of M points as w = M j =1  X  j  X  j satisfying M j =1  X  j =1 . Substituting these equations into the primal problem (1), the following is obtained: Let  X   X  be an optimal solution of this problem, the preference score of the product
Here, generating the kernel matrices gi ven in Sect. 3 requires calculation of the inverse of the matrices as described in (3) and (5) through (7). The inverse operations require a significant computational effort, which prevents us from using these kernel matrices for the recommendation tasks when the number of which causes difficulty in holding the kernel matrices in memory during the time required for solving the problem (8). In the subsequent subsections, however, we will propose new formulations of 1-SVMs which can handle the kernel matrices defined by (3) and (6) efficiently. 4.1 Modified Laplacian Regularized Kernel Suppose that the kernel matrix K is the modified Laplacian regularized kernel matrix given by (6), which includes the regularized Laplacian kernel matrix (5) and the von Neumann diffusion kernel matrix (7) as the special cases.
Let us first introduce a new vector of variables  X  =(  X  1 , X  2 ,..., X  M ) T  X  R M , and define  X   X  K  X  .Notethat  X  j =( K  X  ) j = M i =1  X  i  X  i , X  j holds for each j . It follows that  X  = K  X  1  X  =( I + tL  X  )  X  holds. The equality constraint e  X  =1in(8)canthenbeverifiedtobe( e  X  t (  X   X  1) d ) T  X  =1where d = D e = B e . Furthermore, a straightforward calculation reveals that  X  T K  X  =  X  T ( I + tL  X  )  X  .

Therefore, the problem (8) can be equiva lently formulated with respect to the new variable  X  as follows: Here, it should be emphasized that we can formulate the 1-SVM without the inversion calculations.
 4.2 Commute Time Kernel When we use the commute time kernel matrix L + as K in (8), a simpler formu-lation can also be derived. First, as in the previous section, let us introduce a vector of variables  X  =(  X  1 , X  2 ,..., X  M ) T , and let us define For each j ,if  X  satisfies the constraint e T  X  = 1 of the problem (8), then  X   X  = L  X  ee T M  X   X  e M holds. In addition, we can easily verify that the con-straint e T  X  = 1 in (8) is written as e T  X  =0.Furthermore,  X  T L +  X  =  X  T L  X  holds if  X  satisfies e T  X  =0 . Therefore, the primal pro blem (8) can be equiva-lently formulated as follows:
Let (  X   X  ,  X   X  , X   X  ) be an optimal solution of the problem (11). We have the following lemma.
 Lemma 1. The optimal solution (  X   X  ,  X   X  , X   X  ) satisfies  X   X  j  X   X   X  for all j = 1 ,...,M .

Let I  X  i |  X   X  i =  X   X  . Note that  X   X  i =0forany i  X  I. In addition, for a  X   X  i  X   X   X   X  if i  X  I ,and  X   X  i  X   X   X  i + (  X   X  ,  X   X  , X   X  ), which is a contradiction.
 From Lemma 1, the following corollary can be obtained.
  X  j =  X  Consequently, by substituting  X  j =  X   X   X  j , the problem (11) can be simplified as follows: 4.3 Some Special Cases It has been shown that the 1-SVM formulation given in (1) can be solved an-alytically when  X  =1 . 0. This is also true for our formulation given in (8). We have the following lemma: Proof. Let us assume, to the contrary, that there exists an index k such that  X   X  =(  X   X  1 ,...,  X   X  l )and X   X  as follows:  X   X  proof.
 This lemma also ensures that  X   X  j =  X   X   X  M i =1  X   X  i  X  i , X  j holds for each j = problem (13), the following formulation is obtained: where y =( y 1 ,y 2 ,...,y M ) T is an M -dimensional vector such that y 1 = y 2 =  X  X  X  = y l =1and y l representing the purchased products by the active customer.

The problem (14) can be solved analyti cally. Since the gradient of the objec-tive function W (  X  ) is described as  X  W (  X  )= K  X   X  1 l K y , a stationary point of W (  X  )isgivenas  X  = 1 l y , which happens to satisfy the constraint e T  X  =1. Therefore, the problem (14) is solved.
 To evaluate the performances of the pro posed approaches, numerical experi-ments are conducted using a real-world dataset. We use the MovieLens dataset developed at the University of Minnesota. This dataset contains 1,000,209 rat-ings of approximately 3,900 movies made by 6,040 customers. We use 100,000 randomly selected ratings [6] containing 943 customers and 1682 movies. This set of ratings is divided into five subsets to perform five-fold cross-validation. The divided dataset can be retrieved from http://www.grouplens.org/data/ . Moreover, in order to demonstrate the scalability of the proposed approach, we use the original full dataset, which is also randomly divided into five subsets to perform the cross-validation.

In these experiments, all of the rating v alues are converted into binary values, indicating whether a customer has rated a movie. This conversion has been used in several papers, including [6, 3]. Let M and N be the number of products and customers, respectively. Then th e dataset is represented as an N  X  N binary matrix A ,wherethe i  X  j element A ij = 1 if customer i has watched movie j .
In order to generate the graph-based kernels, we first construct a k -nearest neighbor graph G ( V, E ) where the set of nodes V corresponds to that of the movies. For each node j  X  V ,let A j denote the j th column vector of matrix A . Based on the cosine similarities A movie i is among the k nearest neighbors of movie j ,orwhenmovie j is among those of movie i , we place an edge ( i, j )  X  E and assign a unit weight b ij =1. We report the results obtained by the kernel matrices given in (3) and (5).
For each kernel matrix, we solve the 1-SVM with the parameter  X  =1for generating the preference scores, which can be achieved by solving a system of a ,let y a  X  R M be an M -dimensional binary vector representing the purchased products by active customer a . Then, the preference score of each product i is given as the i th element of the vector L  X  ee T /M matrix (3), or of the vector ( I + tL )  X  1 y a when we use (5).

The cross-validation is conducted using the training and test set splits de-scribed above. We first calculate the score using the training set. Note that, for each active customer, the movies contained in the corresponding test set are not contained in the training set. Then, if the score is ideally correct, these movies have to be ranked higher than any other mo vies not watched in the training set. For comparison, the performance of the proposed method is evaluated in the manner described in [3] usi ng the degree of agreement, which is the proportion of pairs ranked in the correct order wit h respect to the total number of pairs. Therefore, a degree of agreement of 0.5 w ill be generated by the random ranking, whereas a degree of agreement o f 1.0 is the correct ranking.

The average degrees of agreement of the five-fold cross validation are given in Figs. 1 through 3. Figures 1 and 2 show the results for the 100,000 selected ratings, and Fig. 3 shows the results for the full MovieLens dataset with more than one million ratings. Figure 1 shows the results obtained by the kernel matrix by changing the number of neighbors ranging from k =4to k = 100, as well as the parameter t in (5), which ranges from t =2  X  10 to t =2 10 . Note that contour lines that are less than 0.893 are omitted from Fig. 2, and those that are less than 0.911 are omitted from Fig. 3.

For comparison, we also perform the sa me five-fold cross-validation using a previously proposed scoring method [3]. In this case, the average degree of agreement is 0.8780, which is approximately the same as the results of the kernel matrix (3), but is significantly less than that obtained by (5). It should be emphasized that the proposed method offe rs better performance in a wide range of parameter settings (See Fig. 2). Furthermore, the kernel matrix used in [3] is generated from a large graph, the nodes of which corresponds to all of the product and customers. When the full movie dataset is considered, the size of the kernel matrix is approximately 10 , 000  X  10 , 000, which can not be handled due to memory constraints. The present kernel matrix, however, is defined by a graph with nodes corresponding only to the products and does not depend on the number of the customers, which is another advantage of the proposed method. We have introduced a new method for r ecommendation tasks based on the 1-SVM. Using special structures of graph kernels, we show that the 1-SVM can be formulated as rather simple quadratic programming problems. In addition, the formulations can take advantage of the sparsity of the Laplacian matrix. Numerical experiments indicate that th e quality, of our recommendations is high, as is the scalability of the method, which can handle tasks with over one million ratings.
 This study was supported in part by Grants-in-Aid for Scientific Research (16201032 and 16510106) from JSPS.

