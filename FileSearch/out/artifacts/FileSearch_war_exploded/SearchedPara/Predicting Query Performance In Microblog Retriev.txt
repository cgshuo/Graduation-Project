 Query Performance Prediction (QPP) is the estimation of the retrieval success for a query, without explicit knowledge about relevant documents. QPP is especially interesting in the context of Automatic Query Expansion (AQE) based on Pseudo Relevance Feedback (PRF). PRF-based AQE is known to produce unreliable results when the initial set of retrieved documents is poor. Theoretically, a good predic-tor would allow to selectively apply PRF-based AQE when performance of the initial result set is good enough, thus enhancing the overall robustness of the system. QPP would be of great benefit in the context of microblog retrieval, as AQE was the most widely deployed technique for enhanc-ing retrieval performance at TREC. In this work we study the performance of the state of the art predictors under mi-croblog retrieval conditions as well as introducing our own predictors. Our results show how our proposed predictors outperform the baselines significantly.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Ad-hoc Retrieval; Query Performance Prediction; Query Ex-pansion
Most information retrieval systems experience a high vari-ability in retrieval performance across different queries. Whilst many queries are satisfied successfully, the system produces poor results for many others. Since a number of retrieval ap-proaches rely on the initial set of results, it would be highly desirable to predict when retrieval results are not satisfac-tory enough.

This task is known as query performance prediction (QPP), and has been an active and challenging area of research over the last decade. Multiple predictors have been proposed in the literature with varying degrees of success. These pre-dictors fall mainly into two categories: pre-retrieval and post-retrieval predictors. Pre-retrieval predictors are com-puted before retrieving any documents, thus relying solely on query term features. On the other hand, post-retrieval predictors rely on features extracted from the retrieved doc-uments. Post-retrieval predictors mainly estimate how well a query is represented by the retrieved documents.
In this work, we study pre and post retrieval predictors for microblog retrieval tasks. Although much work has been done in predicting the performance of queries over web col-lections, to the best of our knowledge, no work has been done in the context of microblogs. Microblogging platforms such as Twitter have gained momentum over recent years provid-ing a new way of sharing information and broadcasting short messages over a network of users. Microblogs present many differences with respect to web documents both in morphol-ogy and content [9]. Mainly, microblogs constitute a time or-dered stream of very short documents as they are published. Moreover, microblogs contain community defined tags to re-fer to certain topics (hashtags), or people (mentions), which we intent to investigate in our QPP study.

The motivation behind studying QPP for microblogs re-sides in increasing the robustness of existing retrieval ap-proaches. More specifically, QPP can be especially handy for selectively applying pseudo relevance feedback (PRF) based automatic query expansion (AQE) approaches [2]. PRF-based AQE approaches rely on the initially retrieved set of documents. Thus if these documents loosely represent the initial information need, PRF-based approaches most likely result in unexpected behaviour, and unreliable results.
Effective QPP represents an opportunity to estimate the performance of a system for a particular query, based on pre-retrieval and post-retrieval features. In turn, this would allow an IR system to selectively perform AQE, when the circumstances are most propitious, based on estimates given by the predictors.
 Our work is driven by two research questions. (RQ1) To what extent, can we predict the performance of a re-trieval model for microblog corpora?. (RQ2) To what ex-tent, the combination of predictors can improve overall pre-diction performance, in the context of microblogs?.
In this work, we investigate the performance of previ-ously proposed predictors [5], in the context of microblogs. We then show that they fail to perform effectively which prompts the need to develop better predictors. We propose a number of predictors, which take into consideration the characteristics of microblogs. Our evaluation findings show how our predictors outperform those found in the literature. Finally we further improve our performance by learning a prediction model, combining our predictors by means of a support vector machine for regression.
One of the main works in query performance prediction is that by [3]. In their work, they proposed a predictor based on the Kullback-Leiber divergence between the query X  X  and the collection X  X  language models. This predictor attempts to quantify the  X  X larity X  of the query. In other words, the non-ambiguity of the query which in turn should reflect on how well it represents a particular topic. Their evaluation shows good correlation of their predictor with average precision, using Spearman X  X  ranking correlation tests.

Work by [7] suggests other predictors such as the standard deviation of IDF values within the query. They also defined a simplified version of the  X  X larity Score X  proposed by [3] namely Simplified Clarity Score (SCS). Finally, they pro-posed an alternative to SCS called query scope (QS). Their main objective was to investigate pre-retrieval predictors, as post-retrieval predictors are normally computationally more expensive to use.

To predict query difficulty [8] proposed a query coherence score (QC-1) which attempts to quantify how related are the query terms to the retrieved set of documents as well as measuring the differences between the language used in the retrieved set and the query, with respect to the collec-tion. They found that their approach correlates well with average precision, using Spearmans X  X  rank correlation test. Furthermore they also suggested two other versions of this score however their performance was poorer than their sim-pler first version. For their evaluation they used a number of retrieval models, including BM25 and TFIDF to retrieve documents from the TREC Robust track collection.

Work by [10] proposed a series of pre-retrieval perfor-mance predictors. One of the most succesful was SCQ. The aim of SCQ is to compute a similarity score between the queries and the collection. Moreover, they also proposed a variability measure relying on the standard deviations of TFIDF scores for the query terms. Furthermore, they also proposed a predictor using both previous approaches to-gether. Their evaluation showed how the combined predic-tor outperformed all previous approaches. It is important to note that their joint approach is slightly better than their simple SCQ, only when the linear interpolation gives most of the weight to SCQ, albeit being much more complex, thus computationally much more expensive. In this work, we will evaluate the performance of SCQ in our particular context.
A short but comprehensive survey of performance predic-tors was produced by [6]. This study helped on deciding which predictors to include in our study in the context of microblogs, as it showed results for many state of the art predictors across multiple collections.
 Previous evaluations. The  X  X e facto X  evaluation proce-dure in previous work has been the statistical correlations between the predictors and the evaluation metric results for a given system. More often than not, the evaluation metric used was Average Precision (AP). The better the predic-tor estimates the performance of the system in terms of an evaluation metric, the higher the correlation scores.
The most used correlation metrics are Kendall-Tau (K.Tau) and Spearman X  X  (SP.Rho) rank correlation coefficients. The SP.Rho correlation coefficient is a measure of statistical de-pendence between two variables, by which it is estimated how well their relation is represented by a monotonic func-tion (I.e.: grows/decreases always in the same direction). SP.Rho uses Pearson X  X  correlation coefficient in such a way that is much less sensitive to outliers. Kendall-Tau X  X  cor-relation coefficient is slightly different in that, it does not rely on the values of the variables themselves, but it rather measures the similarity in the ordering of the data provided when ranked by each of the variables.
 State of the art prediction. The correlation coefficients obtained for AP in web collections vary wildly. The Kendall-tau coefficients, with respect to AP, for the best performing pre-retrieval predictors range from 0.30 to 0.49 depending on the collection [1]. On the other hand, the Kendall-tau coefficients for post-retrieval predictors are generally higher.
It is important to note the high variability in terms or predicting performance, with respect to the collection. The collections used in the literature include  X  X REC Vol. 4+5 X ;  X  X T10g X  and  X  X OV2 X , where it is often the case for a par-ticular predictor to be the best for a particular collection and the worst for another.
 Selective Query Expansion. One of the main applica-tions of QPP is selective Query Expansion [1]. It refers to selectively applying automatic query expansion (AQE) whenever predicted performance is above a certain thresh-old. This serves as a warranty for PRF-based AQE ap-proaches, as they rely on the top N retrieved documents to perform optimally.
In this section, we describe the state of the art predictors we will be considering in our evaluation, including our pro-posed predictors. Subsequently, we introduce the evaluation approach followed to benchmark and compare their perfor-mances.
 Baseline Predictors: As a starting point we selected those predictors performing best from the survey by [6].
The QueryTermIdf predictor utilizes the IDF values of query terms for making estimations of retrieval perfor-mance. The intuition is that the higher the IDF value the more specific a term is, thus score variations across terms may indicate drifting concepts, negatively affecting the performance. We derive different predictors consider-ing the mean, median, standard deviation (Std), max, min and diff( max  X  min ) IDF scores from each query [6]. More-over, Simplified Clarity Score (SCS) proposed by [7], attempts to model the clarity of a query, i.e. how well it targets a particular topic based on the collections metrics. An homologous predictor to SCS is Query Scope (QS) , which was also proposed by [7].

Another predictor is Similarity of Collection w/ Query (SCQ) which was proposed by [10] to compute the similar-ity between the collection and the query at hand. In a similar context, Term Weight Variability (VAR) was proposed. This predictor measures the variability of weights for a term across the collection. [10] hypothesises that the higher stan-dard deviation for a term, the more discriminative it is.
Moreover, the work by [1] introduced four post retrieval predictors namely NQC , WIG , QF and Clarity . NQC measures the normalized standard deviation of the top scores. The intuition behind this predictor is that relevant docu-ments are assumed to have a much higher score than that of the mean score. Similarly WIG measures the divergence of retrieval of the top-ranked results scores from that of the documents in the corpus. QF and Clarity are predictors that take into account the actual content of the documents. QF measures the divergence between the original top results for the query and the results that would be obtained for a query constructed from the top results. Finally, Clarity measures the KL divergence between a (language) model induced from the result list and the corpus model.
 Proposed Predictors: The first two predictors we defined are to measure the coverage of terms over the documents retrieved. The CoveredQueryTerms (QTCov) predic-tor measures how well the query is being represented by the documents in the result list. For each document, we di-vide the number of query terms found in the document by the total number of query terms, which gives a normalized value between 1 and 0. (1 being a document that completely fits the query). Similarly we defined TopTermsCoverage (TTCov) which measures the coverage of the top N terms in the result list. Intuitively, the more times these terms appear the more likely documents are to revolve around a particular topic.

Another predictor is TimeCohesion (TimeCH) , which taps into the distribution of retrieved tweets over time. We assume that the closer the documents appear with respect to time, the more likely they refer to the same event. To com-pute it, we take the differences between retrieved document timestamps. Differences are taken only between contiguous documents in the rank.
 Also, exploiting microblog specific features we defined the Http predictor. This predictor measures how common is to find a Url in the retrieved set of documents. To this end we count the number of documents with Urls and divide by the total number of documents retrieved. (I.e the rate of Urls). This predictor relies in previous findings which suggest that the presence of Urls indicates the presence of relevant documents [4].

Finally, we defined HashTagCount as the rate of docu-ments with hashtags in the retrieved results set. Hashtags are important in the context of Twitter as they refer to par-ticular topics. Thus the presence of similar hashtags often indicates that users are dealing with the same topic. Datasets. In this evaluation we utilize the Tweet2011, 2012 and 2013 collections with a total of contains 170 topics. The collections have been merged together to produce enough ev-idence for learned predictor models.
 Retrieval Model Used. We utilized the DFRee for pro-ducing the runs, as it provides competitive performance. (P@10 = 0.527 &amp; P@30= 0.409).
 Predictor X  X  Correlation. In the literature, evaluations have mainly taken into account either K.Tau or SP.Rho as correlation measures with respect to average precision (AP). Table 1: Correlations of DFRee runs with AP (** p &lt; 0 . 01 &amp; * p &lt; 0 . 05) In this work, we also pay attention to Pearson X  X  correlation coefficient.

In microblog retrieval, it is most important to optimise performance for the first retrieved documents due to its real-time nature. It has been agreed in the literature that a user will not look further than the first 30 documents, thus AP might not be appropriate for this task. Moreover, to help in selectively applying PRF-based AQE, we focus on the very top retrieved documents, thus we also study prediction in terms of P@10.
In this section we introduce and discuss the results we obtained during the evaluation of the above mentioned pre-dictors. Tables 1 and 2 show the correlation coefficients in terms of K.Tau, SP.Rho and Pearson for a subset of predic-tors. Since it was not possible to show all the predictors in this paper, we have chosen to include only those achieving a Pearson coefficient higher than 0.19. The predictors are pre-fixed with either  X  X re  X  or  X  X ost  X  to indicate whether they are pre-retrieval or post-retrieval predictors. Furthermore, the suffixes: Mean, Median, Std, Max, Min, Lower and Up-per; denote mean, median, Standard Deviation, maximum, minimum, lower percentile and upper percentile, of the pre-dictor values respectively. Moreover, Sum refers to the Sum of all predictor values, whereas Diff is the difference between Max and Min.
 Table 1 shows the correlations coefficients in terms of AP. In the survey done by [6] the maximum correlation achieved using K.Tau ranged from 0.30 to 0.49 depending on the col-lection.
 State of the art predictors such as VAR, SCS, NQC or WIG (Described in Section 3) performed poorly in the con-text of microblogs, as their K.Tau coefficient values ranged between 0 and 0.16, thus are not shown in Tables 1 or 2. This demonstrates how challenging performance prediction is in the context of microblog retrieval, and the need for tai-lored predictors to this new task. On the other hand, the predictors we proposed achieved a much better correlation than that obtained by the state of the art in this context. post TTCov upper is one of such predictors, achieving a K.Tau coefficient of 0.356, being the best correlation with respect to AP. This predictor takes the upper percentile of the rate at which top terms appear in the retrieved set of documents.

These results are very promising, but our main focus is to enable selective PRF-based AQE, thus we present correla-tions in terms of P@10 in Table 2. As it can be observed, amongst the top performing predictors we find those rely-Table 2: Correlations of DFRee runs with P@10 (** p &lt; 0 . 01 &amp; * p &lt; 0 . 05) ing on microblog specific features, namely post TimeCH which measures how close in time are the retrieved tweets and post http measuring the presence of URL X  X  in docu-ments. Additionally, the correlations achieved by these pre-dictors with respect to P@10, are generally higher than that achieved for MAP, with post TTCov Median being the best performing predictor.

An interesting observation regarding post TTCov upper and post TTCov Median is that they may be referring to the same documents, as with AP a larger set of documents is considered compared to P@10.
 Combining Predictors. Since our main objective is pre-dicting performance with AQE in mind, we focus on P@10. To combine the predictors for P@10, we used a Support Vec-tor Machine for regression. To avoid biasing we performed a ten-fold cross-validation. The learned prediction model is defined as follows:
The correlation coefficients obtained for this model, are 0.412 (+12.88%), 0.559(+22.59%), and 0.539 (+22.22%), for K.Tau, SP.Rho and Pearson respectively.

Finally, the predictors proposed in this work outperform those in the literature, within this particular context. How-ever, as in previous attempts, it is uncertain that it will be enough for enabling effective selective AQE. Nonetheless, these predictors may represent an important step towards that much sought after objective, within the context of mi-croblogs.
In this work, we studied the performance of the state of the art predictors in the context of microblogs. The most sought after benefit from predicting query performance is increasing the robustness of PRF-based AQE approaches. To this end we paid special attention to the prediction in terms of the top retrieved documents, specifically Precision@10 (P@10).
Our evaluation suggests that predictors in the literature perform poorly in the context of microblogs, thus we need to come up with predictors that are better fit for purpose. To this end, we defined a number of predictors relying on microblog features and characteristics. We benchmarked their performance and showed that most of them outper-form those in the literature, with TTCov being the most correlated with MAP and P@10.

Finally, we used support vector machines for regression to learn a prediction model based on the best performing pre-dictors. The resulting model further increased performance by a +22% in terms of the Pearson correlation coefficient, and +12.88% for K.Tau.

Future work will put these findings to a practical appli-cation for selective approaches to PRF-AQE, or in the se-lection of a baseline model to optimize a system X  X  overall performance given the conditions of a particular query. Fur-thermore, we will study the performance of other predictors which will consider more microblog specific features.
This research is partially supported by the EU funded project LiMoSINe (288024). [1] D. Carmel and E. Yom-Tov. Estimating the query [2] C. Carpineto and G. Romano. A survey of automatic [3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [4] D. F. Gurini and F. Gasparetti. Trec microblog 2012 [5] C. Hauff. Predicting the effectiveness of queries and [6] C. Hauff, D. Hiemstra, and F. de Jong. A survey of [7] B. He and I. Ounis. Inferring query performance using [8] J. He, M. Larson, and M. De Rijke. Using [9] J. Teevan, D. Ramage, and M. Morris. # [10] Y. Zhao, F. Scholer, and Y. Tsegay. Effective
