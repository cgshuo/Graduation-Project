 of Technology, Tehran, Iran 1. Introduction
Reinforcement Learning (RL) is a method to improve the performance of autonomous agents in interacting with a dynamic environment [25]. In particular, at each step of the interaction, the agent performs an action that changes the environment. The execution value of the action is transmitted to the agent via a scalar reinforcement signal, called reward. The agent X  X  behavior chooses competent actions to increase the sum of the long-run rewards.

A key scaling issue of RL is to make an enormous number of decisions in large domains. Therefore, instead of learning using individual primitive actions, an agent could potentially learn much faster if it could abstract the innumerable micro-decisions to form high level behaviors known as  X  X acro actions X  or  X  X kills X . Skill utilization in reinforcement learning can signi fi cantly improve the agent X  X  learning ef fi ciency to solve dif fi cult problems. The main idea of Hierarchical Reinforcement Learning (HRL) methods (e.g. [3,8,21,26]) is to decompose the learning task and to form hierarchies of reusable skills. This decomposition decreases the size of an agent X  X  state space and consequently expedites the learning process. This issue raises the questio n of how the autonomous agent can develop new skills automatically. A common response is to de fi ne subtasks in the state space context. The learning agent identi fi es important states called  X  X ubgoals X , which are believed to possess some  X  X trategic X  importance and are worthwhile to reach. The agent learns sub-policies to reach subgoals in order to form new skills.
Generally, here are three approaches to identify subgoals automatically. The fi rst one is frequency based which observeseither how often a state has been visited by the agent during successfultrials or how much reward a state has gained [9,17,22]. The motivation in this approach is based on the fact that states frequency based approach is that the agent may need excessive exploration of the environment in order to distinguish between  X  X mportant X  and  X  X egular X  states. The second approach is policy based in which the agent initially learns a task and then analyzes the learned policy for certain structural properties [2, 10]. This approach may not prove useful in domains with delayed reinforcement. Finally, the third approach which we are interested in is graph theoretic where the agent transition history is mapped to a graph, and then the states within strongly connected regions are identi fi ed as subgoals [12,16,18,23, 24]. In this approach subgoal states are identi fi ed using Max Flow-Min Cut algorithm [8], while in [12, 16,24] the state space is partitioned using some graph clustering algorithms. The motivation here is to introduce activities to identify states on the borde r of strongly connected regions of the graph as subgoals and then learn policies to reach these states. The main advantage of graph theoretic approach is that the agent transition graph can be constructed without usin g of reward information signals. Therefore, this approach can be used for domains with delayed reinforcement. On the other hand, these graph based methods suffer from the computational complexity of O ( n 3 ) for partitioning the transitions graph and fi nding subgoal states, where n is the number of nodes in the graph.

Graph centrality measures are used as an effective measure to fi nd and evaluate the subgoals in the graph theoretic approach [23]. There are several centrality measures which numerically quantify the importance of nodes in a graph, includi ng betweenness centrality, closeness centrality, stress centrality, and graph centrality [5]. Among these measures, the node betweenness centrality measure attracted much attention recently [23]. The main distinction of this work is to de fi ne the utility of a given state as a) a subgoal using a graphical representation of the problem and b) a measure of how prominent the node is on certain shortest paths on the graph. More importantly, this work forms a useful guideline for developing techniques that do not rely on explicit or complete representation of the graph. From the computational complexity point of view, the betweenness centrality measure can be computed in O ( nm ) , where n and m are the number of nodes and edges in the graph, respectively. This complexity can be reduced to o ( n 2 ) for sparse graphs. Alternatively, this measure can be computed using approximation methods [7]. As a result, the skill acquisition method based on betweenness centrality measure could be so less computationally demanding that can be applied to large state space problems.

According to the de fi nition and application of skills, the subgoa l states not only frequently contribute to solve several tasks but also lie on rather long and hard to fi nd solutions. Betweenness centrality scores nodes based only on the number of the shortest paths, rather than by considering the length of the shortest paths. As a result, bo ttleneck states as well as their nei ghboring sta tes achieve high score values. Consequently, neighboring states might be considered as candidate subgoals in Betweenness centrality. In this paper, we propose a new graph centrality measure called node connection graph stability (NCGS) that consider the length of paths to score subgoal states. Then we propose a new graph clustering based algorithm for automatic skill development including identifying and evaluating subgoals. After fi nding a set of subgoals, the corresponding initiation sets should be determined. To the best of our knowledge, this determination is performed based on heuristic methods, e.g. choosing the states located on the same successful trajectory with the subgoal [17] or the states in the same cluster as the subgoal [12,16,18,24]. In this paper, a betweenness-based scoring function has been used to couple the initiation set and the corresponding subgoal. Investigating some benchmark problems, we show that the proposed subgoal discovery algorithm out performs all other centrality measures in terms of subgoal identi fi cation and learning performance signi fi cantly, and forms useful skills. To complete the procedure of skill ac quisition, one should d etermine the policy for each subgoal-initia tion pair. To this end, the policy is initiated e ither randomly or with equal proba bilities, and then i s updated based on the empirical experiments. Furthermore, to speed up the training process, one can inject some prior knowledge about the problem to the policy learning procedure. This information will be useful for the agent to select a proper skill in a given state during the fi rst episodes. Moreover, we propose a new method for incorporating prior knowledge to the policy to increase the speed of learning.

The rest of the paper is organized as follows. In Section 2, the basics of the reinforcement learning and its extensions to use options are described. The proposed method is described in Section 3. In Section 4 the benchmark environments are described and the performance of the proposed method in comparison with the benchmark tasks is evaluated in Section 5, and the paper is concluded and discussed in Section 6. 2. Reinforcement learning with option
The interaction of the agent and the environment can be represented using Markov Decision Process (
MDP ) framework. A fi nite MDP is a tuple &lt; S,A,T,R &gt; ,where S is a fi nite set of states, A is a fi nite set of actions, T : S  X  A  X  S  X  [0 1] is a state transition probability function and R : S  X  A  X  X  is a reward function. At each decision stage, the agent observes a state s X S and executes an action a X A with probability T which results in a state transition to s  X S . The agent obtains a scalar reward r  X  X  which is a (possibly stochastic) function of the current state and the action performed by the agent. The agent X  X  goal is to fi nd a map from states to actions, called policy, which maximizes the expected discounted reward over time, E {  X  reward obtained at time t . To represent skills, we use the options framework [26]. A (Markov) option is a temporally-extended action or macro action, speci fi ed by a triple &lt; I, X , X  &gt; where I denotes the option X  X  initiation set, i.e., the set of states in which the option can be invoked;  X  is the option X  X  policy, mapping states belonging to I to a sequence of actions;  X  denotes the option X  X  termination condition, which  X  ( s ) denotes the probability that the option terminates in states. In this paper, Macro-Q-Learning algorithm [26] (the extension of Q-Learning [25] algorithm with options) is used to optimize policies for both primitive actions and options. The Q-function that maps every state-action pair to the expected reward for taking this action at that state is updated as follow: where  X  is the actual duration of the option o t ,  X  is the learning rate. The update rule for a primitive action is similar with  X  = 1. Performing a skill, the agent follows th e policy of the skill until it reaches a termination node. Since the skills have longer execu tion time, the penalty assigned to a skill is higher than primitive actions and consequently developing useless skills leads to very low performance. 3. Proposed method In this section, we fi rst propose a centrality measure for state ranking in Subsection 3.1. Then, in Subsection 3.2, we propose a method for subgoal identi fi cation, automatic skill acquisition, initiation set assignment, and prior knowledge incorporati on. The outlin e of the proposed algorithm is shown in Fig. 1. According to this algorithm, the agent explores repeatedly the environment until stop conditions are met and then the agent X  X  state space history is translated to a graph. Each visited state becomes a node in the graph and each observed t ransition s  X  s ( s, s  X  S ) is translated to an edge ( s, s ) in the graph. The nodes of the obtained graph are scored based on th e proposed graph centrality measure and then the nodes that have scored more than a prede fi ned threshold value are considered as candidate subgoals. In the next step, the redundant subgoals are removed and the most effective subgoals are extracted from the set of candidate subgoals. Then, for each subgoal, the corresponding initiation set is de termined and a skill is developed for each subgoal based on the optio n framework [26]. Then t hese sk ills will be added are derived. In fact, these values re present the priorities o f the options available a t each state. Finally, the agent can in each state s elects either a primitive action or a skill, and it update s the corre sponding policy based on gained rewards.

There are several possible stopping conditions fo r activating the skill acquisition algorithm. For example, the agent can wait until the predetermined number of iterations is exceeded. Another possible stop condition is to wait until no new states are encountered for a prede fi ned time step. If the stop conditions are met, then the skill acquisition procedure will be executed and new skills will be added to the agent X  X  action space. On the other hand, we would like the skill acquisition a lgorithm to be performed incrementally in the learning process. In the experiment reported in this paper, we assumed that the number of iterations is predetermined, so that the s topping condition is simply t o stop when the number of iterations reaches some pred etermined thr eshold. Conditions for activatin g the skill acquisition algorithm should be task independent in general, which calls for further study. In the following, we will discuss the main steps of the algorithm in detail. 3.1. Node connection graph stability ( NCGS )
It is shown that graph centrality measures can be utilized as an effective method not only for fi nding the subgoals but also for evaluating them [23]. One can fi nd different centrality measures in the literature including: betweenness, closeness, degree and eigenvector centrality. In this section, we will brie fl y present the de fi nition of the closeness and betweenness centralities that are more applicable for RL problems and then the proposed centr ality measure will be described.

Closeness centrality of a node u, denoted by CC ( u ) ,isde fi ned as the mean geodesic distance (i.e., the shortest path) between a node v and all other nodes reachable from it [5]. It is de fi ned as: where n is the size of the graph and d ( u, v ) presents the length of the shortest path connecting u and v . Closeness can be interpreted as a measure of how long it takes to reach any vertex in the graph in average.

Betweenness centrality of a node u , denoted by BC ( u ) ,isde fi ned as the frequency that a node lies on a shortest path connecting two distinct nodes [5], i.e., where  X  st ( u ) is the number of shortest paths that pass through node u and  X  st denotes the number of shortest paths between node s and node t .

According to the de fi nition and application of sk ills, three conditions can be de fi ned for characterize the candidate subgoals. The fi rst condition is that the candidate subgoals should be frequently contribute and the third is that the candidate subgoals should hav e some centrality property to decrease the average time of exploration. Moreover, the skill acquisitio n method should not be com putationally demanding such that it is applicable to large state space problems. According to the mentioned conditions, it is clear that the betweenness centrality measure ful fi lls fi rst and third conditions but cannot cover the second condition. In this paper we propose a new node centrality measure called  X  node connection graph stability (NCGS) X  that ful fi lls all previously mentioned requirements. The connection graph stability score was fi rst introduced by Belykh et al. [4] as a score assigned to the  X  X dges X  of the graph to fi nd a suf fi cient condition f or synchronization in a network of coupl ed dynamical systems. As we mentioned, NCGS score is the generalized form of CGS score for nodes. It was proved that a lower bound based on CGS can be obtained for the second smallest eigenvalue of Laplacian matrix of an unweighted graph, i.e. algebraic connectivity [1]. Therefore, this measure is highly correlated to min-cut and connectivity of the graph and is able to determine bottleneck nodes. In this paper, we extend the de fi nition to nodes and present a new node centrality measure in a graph. Node connection graph stability centrality is de fi ned as the sum of the path length of the shortest paths that making use of a node i.e.
 where  X  st ( u ) is the length of the shortest path connecting s and t that path through node u .Itisvery similar to node betweenness centr ality except t hat it uses a weight for each path and therefore can be considered as a weighted node betweeness measure. This new centrality measure satis fi es all three requirements mentioned conditions for detecting of node s related to useful skills. It is worth mentioning that the proposed measure can be calculated with the same computational complexity of betweenness centrality using Brandes algorithm [16] with minor modi fi cations to involve the length of the paths [4]. 3.2. Subgoal identi fi cation
In order to fi nd subgoals, the nodes of corresponding graph a re scored with proposed centrality measure (NCGS) and then the nodes with a NCGS value less than a threshold are eliminated while the rest are considered as candidate subgoals and are preserved for further investigation. This choice is motivated by the fact that it is expected that the subgoals are w ithin the set of high-centrality nodes. We will show further that the proposed method is not sensitive to the threshold value.

In general, the neighboring nodes of the most central nodes have also high centrality values, where using a simple thresholding may results in redundant nearby subgoals. To avoid such redundancy, we propose to create a new, properly weighted graph with the candidate subgoals as nodes. This graph will be decomposed into a set of clusters and one main subgoal is chosen in each cluster. By de fi nition, a cluster is a subset of nodes which are strongly connected, but weakly connected to the other nodes outside. Starting with a complete graph, the edge weights are assigned as follows, where W ij is the weight of the edge connecting nodes i and j , Sj is the NCGS score of j ,and C ij presents the co-betweenness centrality of nodes i and j ,de fi ned as the number of shortest paths that path though node i and j simultaneously [5], i.e.
 where  X  st ( i, j ) denotes the number of shortest paths between s and t that pass i and j simultaneously. Using this weighting scheme, the candidate subgoals that are highly correlated, i.e. lie on many similar shortest paths, become highly coupled, while the rest are rather loosely coupled. This weighted graph is divided into clusters using the Fast Newman clustering which has been proved to have both good performance and low computational complexity i.e. O ( mdlog ( n )), where n and m are the number of nodes and edges, respectively. Moreover, d is the depth of the dendrogram describing the network community structure [6]. Then, the node with the maximum NCGS in each cluster is taken as the fi nal subgoal. This set of subgoals is denoted by G = g subgoals. 3.3. Initiation set assignment
After identifying the effective subgoals, the initiation set assignment phase starts. By de fi nition, the i.e., if there is more than one subgoal extracted in the previous step, the agent should not necessarily bind all identi fi ed subgoals to each state, in order to avoid high complexity and slow convergence in the learning procedure. Therefore, an important step is to de fi ne a suitable subset of subgoals that can be reached in each state. Here, we propose a heuristic approach based on graph theoretic measures previously described in Section 3.2 which is as follows. For each state s , a subgoal g is assigned to the available targets of that state if (a) s and g have high co-betweenness centrality compared to the other subgoals and (b) there is no other higher scored subgoal on the shortest path connecting s and g . Condition (a) considers subgoals that play an im portant role for the connectivity of state s to other nodes. Condition (b) removes subgoals which are redundant for an agent standing being in state s .In fact, if there is such a subgoal g that has a higher co-betweenness score than g and lies on the same shortest path connecting s and g , then s can choose g and then the nodes beyond g will choose g as a potential subgoal with high probability due to the high centrality score of g . It should be mentioned that the calculation of co-betweenness centrality scores for the initiation set assignment doesn X  X  impose any additional complexity because all the co-betweenness values can be calculated simultaneously during the NCGS calculation process. 3.4. Prior knowledge injection
For an agent that has developed some skills, the execution probability of skills is assumed to be equal in the current methods. Here, we propose to use non-equa l probabilitie s for the selection of the developed skills based on prior knowledge obtained from the state graph.

Since the agent must explore its environment through trial-and-error, many methods have been derived to bias this exploration. These methods range from quite simple to exceedingly complex and each one has its advantages and drawbacks. The most common way to explore the environment is  X  -greedy action selection method where the probability of selecting action a given that the agent is in the state s, i.e.  X  p ( a | s ) ,isde fi ned as follows [25]: is the discounted reward if the agent applies action a being in state s.

At the fi rst iterations of learning in complex environments, Q-values are far from the optimal value due to the lack of positive rewards. Therefore, actions are not selected properly in the fi rst iterations.
Alternatively, one can de fi ne a method that incorporates prior knowledge to guide an agent X  X  exploration at the fi rst iterations. We can embed the prior knowledge of the environment into the agent X  X  policy while learning the value function to reduce the amount of exploration in RL.

We present a heuristic for combining trial and error in exploration phase with some prior knowledge which is expressed as the probability T ( a | s ) of selection of action a in state s, to be used in the fi rst iterations of learning. Subsequently, we modify the action selection probabilities as follows: with the environment according to the  X  -greedy method, T ( a | s ) represents the prior and  X  represents the combination rate of the prior knowledge and the recently learned information from the empirical experiments.

It should be mentioned that at the very fi rst episodes, the agent is not able to learn much from interacting with the environment so it is reasonable to give higher preference to the prior knowledge. After several iterations, it is better to rely on the learnt information, which is much more accurate and reward directed, since the agents have learned suf fi cient information from the environment. For this reason, a time dependent rate combination parameter  X  is set as follows: where t is the episode number and  X  is a positive number that adjusts the decreasing rate of  X  ,i.e.the higher  X  , the slower vanishes the in fl uence of the prior knowledge. The prior probability T ( a | s ) can be de fi ned either manually or, as it is shown in the following, it can be extracted automatically from the explored state graph. At the very fi rst steps of the learning, when there is not enough information about the rewarding mechanism of the environment, the agent should be able to explore the environment effectively. Here, by effectively we mean a suitable tradeoff between local and global search. Thus, the skills that help the agent to make this tradeoff shoul d get some priorities at the beginning. To this end, as an intuitive heuristic, we propose that an agent in state s ranks the available skills as follows: where N a is number of primitive actions available in s, O s denotes the set of available skills in the state s,
C sg ( a ) denotes the co-betweenness centrality between state s and the corresponding subgoal state g of skill o , available at state s ,and 0  X  1 is the prior probability of selection between primitive actions and skills at each state. In this paper,  X  =0 . 5 . We will show through simulations that this type of prior knowledge speeds up the average convergence rate of the learning. 3.5. Complexity analysis
The complexity of the proposed algorithm for identifying subgoals consists of computing NCGS value for each node in the state graph and computing clustering algorithm to remove redundant subgoals in the corresponding graph of candidate subgoa ls. The proposed centrality measure ( NCGS ) can be calculated with the same computational complexity of betweenness centrality using Brandes algorithm [16] with minor modi fi cations to involve the length of the paths. Therefore, NCGS can be computed in O ( nm ) time and O ( n + m ) space on unweighted graphs, where n and m are the number nodes and edges in the corresponding graph of explored states, respectively. In this case on weighted graphs the space requirements remains the same but the time requirement increases to O ( nm + n 2 log n ) [5]. Because of the MDP properties and limitation of number of actions, in the most environments the corresponding graphs of explored states are sparse. Therefore the time complexity of computing NCGS centrality scores for unweighted and weighted graphs reduces to O ( n 2 ) and O ( n 2 log n ) respectively.

Moreover Fast Newman algorithm [6,20], which is used to remove redundant candidate subgoals from the their corresponding graph, will be computed in O ( n  X  ( n  X   X  1) of nodes in the corresponding graph. Moreover, d is the depth of the dendrogram describing the network community structure. In this case, due to the elimination procedure, the number of candidate sub-goals is much less than the total number of graph nodes, i.e., n  X  n . Therefore, bringing all together, the proposed algorithm will be computed in O ( nm + n 2 log n ) and it reduces to O ( n 2 ) for unweighted sparse graphs and O ( n 2 log n ) for weighted sparse graphs. 4. Environment and benchmarks
To test our method, we use fi ve benchmark tasks namely  X  X our-room grid world X ,  X  X axi driver X ,  X  X occer simulation X ,  X  X mails network X  and  X  X ower of Hanoi X  worlds which are brie fl y introduced in this section. For training the agent in each environment, the agent is given several tasks and to optimize the policy for each task, the agent may repeat the task for several episodes.
 4.1. Four-room grid world
The four-room grid world shown in Fig. 2(a) was introduced by Sutton [26]. It consists of four rooms connected to each other through four doors. The agent is located at a randomly selected start point and asked to fi nd a randomly selected goal point. The agent has four primitive actions, namely  X  MoveUp  X ,  X  MoveDown  X ,  X  MoveLeft  X  X nd X  MoveRight  X . In the corresponding state t ransition graph, the cells are represented as nodes which are connected to their four neighbors. Then, we randomly select 60 tasks i.e. 60 pairs of randomly chosen &lt; start, goal &gt; . Each task is performed 100 times (episodes). The agent receives a reward of + 1000 at the goal state and a reward  X  1 for all other states. To set the exploitation and exploration trade-off, the agent uses an  X  -greedy policy with  X  =0 . 1 . The learning rate  X  and the discount factor  X  are set to 0.1 and 0.9 respectively. 4.2. Taxi driver world
The taxi driver world has been a popular illust rative problem for RL algorithms since its introduc-tion [8]. This environment, shown in Fig. 2(b), consists of a 5  X  5 grid with 4 special cells (RGBY). A passenger should be picked up from a cell and then dropped off in a destination cell, where the pickup and drop off locations are two randomly chosen cells from the set of RGYB nodes. The corresponding state transition graph consists of two identical grids i.e. one for the case that the taxi is searching for the passenger and one for the case when the taxi has picked up a passenger and is looking for the drop off location, that are connected through the pairs of corresponding RGBY nodes i.e. the nodes where a change of state is possible. In each episode, the location of the taxi is chosen randomly. The taxi must pick up the passengerand deliver him, using the primitive actions  X  MoveUp  X ,  X  MoveDown  X ,  X  MoveLeft  X ,  X  MoveRight  X ,  X  Pickup  X  X nd X  Putdown X . A sequence of 300 tasks each including 60 episodes was con-sidered. The taxi receives a reward of + 20 for successfully delivering the passenger,  X  10 for attempting to pickup or drop off the passenger at incorrect locations and  X  1 for other actions. The other parameters were set the same as in the four-room grid problem. 4.3. Soccer simulation environment
The soccer simulation environment consists of a 6  X  10 grid environment, two goals, a ball and two agents. At each episode, one agent tries to own the ball, carries it to the opponent X  X  goal and scores. The other agent tries to defend and own the ball. Each agent has fi ve primitive actions: MoveLeft , MoveRight , MoveUp , MoveDown and Hold . The timing of the game is discrete and the agents perform their actions one after each other, i.e. in each time step only one agent performs an action. Performing a Move action will change the agent location toward the corresponding direction. Performing the Hold action causes the agent to hold the ball. When the agent holds the ball, performing any move action will change the position of the agent and ball simultaneously.

To score a goal, an agent must hold the ball and carry it to one of two states in front of the opponent X  X  goal and then perform a MoveRight ( MoveLeft ) action if the opponent X  X  goal is in right (left) side of the fi eld. When an agent scores a goal, the ball is given to the opponent and two players are placed at speci fi ed locations in front of their goals. Furthermore, when two agents choose actions which leads to the situation that the agents must be placed in the same location, owning of the ball is determined by the following rules, 1. if the agent which does not hold the ball is going to enter the cell in which the other agent stands, 2. if the agent that holds the ball is going to enter the cell in which the other agent stands, then the The environment will be speci fi ed with three state variables namely, the location of the agents and the owner of the ball which is given the values  X  1 , +1 and zero when the agent, the opponent or none of them is the ball owner, respectively. In the other word, the corresponding graph has totally 60  X  60  X  2=10 800 nodes. The agents play 20 games where game longs 1500 time steps. The agents receive  X  1 reward for each action, +10 for owing the ball,  X  1 0 for missing the ball and +20 for scoring a goal. The agent uses an  X  -greedy policy with  X  =0 . 1 . The learning rate  X  and the discount factor  X  are set to 0 . 1 and 0 . 9 , respectively. 4.4. Email network
The email network is a friend relationship network, shown in, and consists of 1133 students from ten differentcolleagues [11]. The whole datasetconsists of 3332 friend relationship. This network is modeled by an undirected graph where each student represents a node in the graph and each edge represents a friend relationship between two students. Here, we repeat the famous experiment of Milgram [19] on this network. Suppose that one of the students, i.e. A , wants to send a letter to another student that is not his immediate friend, i.e. B . To this end, A delivers the letter randomly to one of his friends, i.e. C , that A believes might be friend of B .If C is a friend of B , he will send the letter to B ;otherwise C will choose one of his own friends that he believes might know B and send the letter to him. This process is repeated until the receiver receives the letter. The goal is to deliver the letter as fast as possible.
To solve this problem, in each step, the agent asks the student to introduce his friends then the agent decides to ask one of the friends to fi nd the receiver of the letter. If he doesn X  X  know the receiver, the he receives 1000 units reward and in the other cases he receives  X  1 unit penalty. This experiment was repeated over 100 different tasks where in each tasks a sender and a receiver of a letter was selected randomly among 1133 different students. In each task the sender sent 50 letters to the receiver, i.e. 50 episodes. The learning rate and discount factor for the reinforcement learning agent is set to 0 . 1 and 0 . 9 respectively.
 4.5. Tower of Hanoi
The Towers of Hanoi puzzle that is shown in Fig. 4 consists of three pegs and a number of disks of different sizes that can slide onto any peg. Each move consists of taking the top disk from one of the pegs and sliding it onto another peg, on top of the other disks that may already be present there. A disk may not be placed on top of a disk that is smaller than itself. Only one disk may be moved at a time. Legal positions are those in which no disk is placed on top of a disk smaller than itself. In this experiment a Tower of Hanoi with fi ve disks is considered and a reinforcement learning agent is applied to move all disks from the fi rst peg to the third peg. In each step the agent tries to move a disk form one peg to another peg. The agent receives + 1000 unit rewards for successfully moving all of the disks to the third peg and in the other cases receive  X  1 unit penalty. The learning rate and discount factor for the reinforcement learning agent is set to 0.1 and 0.9 respectively. 5. Performance and evaluation 5.1. Experiments results
The simulations are performed using standard Q-Learning algorithms [25] and Macro-Q-Learning algorithm [2] for all mentioned environments. We com pared different centrality-based scoring methods, namely Closeness Centrality ( CC ), Betweenness Centrality ( BC )and NCGS methods in our simulations.
Table 1 reports the scores assigned for the four-room grid task. Considering the results illustrated in the table, the proposed graph centrality measure ( NCGS ) assigned high scores to the door points (i.e. nodes labeled 25, 51, 62 and 78) distinctly comparing to two other measures. All doors were detected using the proposed method for a threshold larger than 96% but the other methods were not able to fi nd the doors distinctly and they also assigned high scores to same neighbors (i.e. nodes labeled 24, 26, 43, 54, 70, 77 and 79) and e ither faced fals e acceptance, e.g. for threshol d of 95% CC found 10 additional nodes, or false rejection, e.g. BC discards the main doors for the thresholds larger than 82%.
Table 2 shows the same qualitative results for the Taxi Driver environment. According to the Table 2 results, it is clear that the NCGS and BC assigned higher scores to passenger locations that labeled with R, G, B and Y comparing to their neighbors. But CC assigned the same score to the passenger locations and their neighbors so it was dif fi cult to distinct passenger locations and their neighbors. For example considering location R, CC assigned value 0.7707 and for its neighbors i.e. (2, 5) and (1, 4), assigned values 0.7116 and 0.7457 respectively. This leads to wrong accept these neighboring nodes as candidate subgoals. It worthy to mentioned that BC assigned values 0.0079 and 0.1046 and NCGS assigned 0.0244 and 0.1371 to the mentioned neighbors respectively. In this case when the threshold was set to 0.50, CC, BC and NCGS found 46, 10 additional and 4 nodes, as candidate subgoals, respectively.
 These results show that the NCGS assigns higher scores to the bottleneck nodes comparing to CC and BC centrality measures. Figures 5(a) and 5(b) show another representation for the result of the NCGS centrality scoring of the corresponding graphs of the f our-room grid and taxi driver benchmark problems, respectively. The lighter color of cells corres ponds to the higher centrality scores. As it was expected, the nodes around main subgoals, e.g. neighbors of hallway doors in the four-room grid world, have also high centrality scores.

The next step is the elimination of the redundant subgoals which can be done by thresholding or, as it is proposed, by clustering. By varying the threshold, different numbers of nodes are extracted as subgoals. For example, in the four-room grid world, when the threshold is set to 0.3, there are 23 candidate subgoals extracted as it is marked in Fig. 5(a) by either  X  X  X  or  X  X  X  symbols. In the taxi world, the candidate subgoals, extracted when the threshold was set to 0.4, are presented in Fig. 5(b). It can be seen from Fig. 5 that most of founded candidate subgoals are neighboring states, since they stands on the same paths with the doorways and therefore centrality scor es of these nodes are rather high. Therefore, the thresholding step detects these states as candidate subgoals. If these redundant subgoals are considered for creating skills, the agent is imposed more complexity while it obtains no bene fi ts. Indeed, if we set the threshold to t = 0.96 we would just extract the doorways which are the most effective subgoals in the grid world. No such threshold can be found that leads to extraction of only principal subgoals in the taxi world, since the cells (2, 3) and (4, 3) have higher centrality values comparing to the RGBY cells. As we mentioned, the fi nal subgoals can be identi fi ed using our proposed clustering approach. In the Fig. 5, fi nal subgoals are speci fi ed with symbol  X  X  X  in the corresponding cells. It can be seen that the proposed step is able to identify the fi nal subgoals precisely. Figures 6(a) and 6(b) compares the number of identi fi ed subgoals using thresholding and the proposed approaches when the threshold value slides from zero i.e. detects all nodes as candidate subgoals, to one, i.e. detects no subgoal, for four room grid and taxi driver respectively, i.e. extract all nodes, to one, i.e. extract no node. It is clear that our method has effectively reduced the sensitivity of appropriate threshold selection.
In the skill development step, the agent developed skills based on the identi fi ed subgoals using option framework. To create a new option for each identi fi ed subgoal the option X  X  input set, I , can be initialized using the proposed approach explained in the section 3.3. The option X  X  policy is speci fi ed through an RL process employing action replay [15] and using a pseudo reward function [8].

To show the effectiveness of proposed centrality measur e and clustering approachesin gaining rewards, we repeat the experiments for the case that the agent extracts the subgoals by thresholding on CC , BC and NCGS or by applying the proposed clustering approach on the candidate subgoals. Figures 7(a) and 7(b) show the average obtained reward in four room grid and taxi-worlds respectively when the threshold was set to 0.8 for four room grid worlds and 0.4 for taxi driver world. In each sub-fi gure, the gained reward is shown as a function of episodes for three different con fi gurations, namely,  X   X  without skill  X  where the agent uses standard RL, i.e. just trained based on primitive actions.  X   X  thresholded skill  X  where the agent set a threshold and selected randomly four subgoals among  X   X  clustered thresholded skill  X  where the agent extracts the fi nal subgoals using the proposed approach.
In this experiment,forthe four-room(taxidriver)world, when the subgoals extracted usingthe proposed approach ( NCGS +Clustering), the agent identi fi es 4(3) subgoals and was able to gain higher than 90% of the asymptotic average reward after only 16 (5) episodes while using thresholding of NCGS , BC and CC scores, the agent identi fi es 12(6), 16(10) and 72(152) subgoals respectively. Furthermore, 90% of the asymptotic average reward reached after 23 (25), 28 (31) and 39 (50) episodes respectively. Moreover the agent reached this point af ter 41(152) episodes when usi ng  X  X ithout skill X  approach.

By varying the threshold, different numbers of potential skills are extracted which lead to different performances. Table 3 shows the number of extracted potential skills and the episode number in which, the gained reward reaches 90% of the asymptotic reward for the four room grid world when the threshold is 50%, 80% and 96% respectively. If the threshold is set to 96% then CC fi nds 72 termination points, BC loses the main doors and fi nds 6 potential termination points while NCGS fi nds 4 points and keeps the main doors that increase its performance in average. The sensitivity analysis of the threshold is very important because it shows how much the measure would be robust in larger applications or in presence of noise in real applications. Moreover we report the same qualitative trend in taxi world task. The experiments were rep eated for the soccergrid world and the s ame qualitative resu lts were observed. Figure 8(a) shows the number of scores obtained by the agent for the three mentioned con fi gurations when the threshold is set to 0.5. In this experiment the agent was able to gain 200 goals after 730 time steps when the subgoals were extracted using the proposed approach, while using NCGS, BC and CC the agent gained the same goals after 957, 932 and 1332 time steps respectively and using  X  X ithout skill X  approach the agent gained the same goals after 1484 time steps.

Figure 8(b) shows the same qualitative results for the email network environment. This experiment is simulated in two versions. In the fi rst version of the experiment the learning agent sent the letters friend by friend, i.e. can be considered as primitive actions. In this approach, the agent was able to deliver the developed a set of skills. Here, skill can be de fi ned as a routing plan, i.e. an ordered list of student X  X  names, attached to the letter. Meeting each student, the student can propose an immediate friend or attach a list of names on the letter and ask the agent to follow the list and ask the address of the next person from previous one until it meets the last student in the list. Brie fl y, the agent can accept a list or just go to an immediate friend of t he student. The pr obability of accepting the list, the last student of the list and the proposed list of the students can be learnt by the agent. The agent identi fi ed the set of the last students by applying CC , BC , NCGS and NCGS with clustering approaches. When the threshold is set to 0.5, CC founds 44 students, BC founds 13 student, NCGS found 12 student and only 3 students is found by NCGS with clustering approach. The agent creates a skill for each identi fi ed student to deliver the letter to them through a proper list using option framework. Figure 8(b) compares the performance letter after 125, 104, 101 and 63 time steps using CC , BC , NCGS and NCGS with clustering approaches 49 and 26 time steps respectively.

In order to demonstrate the robustness, we also evaluate the performance of the proposed method in the  X  X ower of Hanoi X  domain as a non-navigational environment. In this experiment, the agent X  X  state transition is mapped into a graph, in which a node and an edge represent a Tower of Hanoi situation and a move to another situation, respectively.

The corresponding interaction graph of the Tower of Hanoi is shown in Fig. 9. The nodes with the lighter color correspond to the higher NCGS scores. Figure 10 shows six Tower of Hanoi situations with the highest NCGS scores. The corresponding nodes of these situations divide the graph into three clusters where the largest disk is always on the sa me peg in each cluster. Sk ills that take the agent to these situations correspond to the setup of the pegs in such a way that the largest disk might be moved to a different peg. The setup for this move involves clearing the stack on top of the largest disk (and clearing another peg entirely.

In this case, the experiments report the same qualitative results. Figure 11(a) shows the number of subgoals identi fi ed by the previously mentioned methods when the threshold value varies from zero to one. As shown, NCGS and BC centrality measures perform better than CC . For example, when the threshold value is set to 0.7, BC and NCGS fi nd 12 and 6 subgoals, respectively, whereas CC fi nds 210 subgoals. However, NCGS and BC identify some redundant subgoals for threshold values set to less than 0.6. For example, when the threshold is set to 0.3, BC fi nds 97 and NCGS fi nds 96 subgoals, whereas the  X  NCGS + clustering X  method identi fi es only 11 subgoals. From these results, we conclude that  X  X CGS+clustering X  is not sensitive to the threshold value and identi fi es real subgoals for different threshold values as compared to NCGS , BC, and CC .

To showthe effectiveness ofthe proposedapproachin terms ofasymptotic reward gaining,an agentwith Then, the experiments are repeated for the case in which the agent uses learnt skills based on identi fi ed subgoals. Figure 11(b) compares the average asymptotic reward obtained for the above-mentioned situations when the threshold value is set to 0.7. These results show that the  X  NCGS +Clustering X  subgoal discovery method outperforms the other methods. As an instance, the agent is able to move all disks to the third peg in the 5 th episode, after 1982, 1310, 1094, and 566 time steps using CC , BC , NCGS, and  X  NCGS +clustering X  approaches, respectively. In the 15 th episode, the agent is able to move all the disks using the above-mentioned approaches after 866, 834, 714, and 459 time steps, respectively. These results are comparable with the case in which the age nt learns without using any skill. In this case, the
Finally, the effectiveness of the proposed procedures for initiation s et assignment and prior knowledge contribution was assessed. In the four-room grid world, let us consider the node labeled 22 in the top left room. As the result of subgoal identi fi cation, there are four skills attached to these subgoals, i.e., four doors. According to our previously me ntioned heuristic for i nitiation set assignment, for this particular developed skills for that particular state. According to Eq. (7), since there are four primitive actions Furthermore, to show the contribution of the prior knowledge, the three-hall world was considered. This domain is shown in Figs 12(a) and 12(b), and consists of a 23  X  17 grid that is divided into three halls, namely top ,l eft, and right . The agent was located randomly in the top hall and a randomly selected target in the left or right hall should be reached by the agent. The wall that separates left and right halls is moveable and one can slide it to change the size of the halls. Figures 12(a) and 12(b) show two versions of the world when the left room is smaller than or is equal to the right hall. Suppose that the agent has learned two sk ills to r each the gateways based on the previ ously mentioned procedure. Now consider two versions in which the skills are chosen with different probabilities determined by Eqs (7) and (8). Figure 12(c) shows the difference between the gained reward of these two versions for the symmetric (gray) and asymmetric (black) three-hall world. It is clear that in the asymmetric case, the prior knowledge helps a lot to gain reward while in the symmetric case, since the co-betweenness scores equal. These experiments show if the corresponding state graph of the environment is heterogeneous, then the assumption of equal initial probability consideration is far from optimal and the prior knowledge mentioned can help signi fi cantly. 5.2. Evaluation with the other methods
This section focuses on evaluating subgoal discovery based skill acquisition al gorithms. Furthermore, in this section, a number of evaluation criteria to classify and evaluate these algorithms have been introduced. However, by using these criteria we are able to evaluate the subgoal discovery part of skill acquisition algorithms. These criteria are described as follows. 5.2.1. Reward dependency
This metric is used to determine whether or not the algorithm needs the reward information of the task to identify subgoals. Some of the proposed subgoal discovery algorithms such as those in [2,9,10,17, 22] use the reward information of the task to identify subgoals. These algorithms suffer from two main drawbacks. The fi rst drawback is that the agent needs exhaustive exploration of the environment to learn the task to identify subgoals. The second one is that the identi fi ed subgoals are dependent on the task at hand and may not be useful for other tasks with different reward information. According to the results in Table 4, the graph based subgoal discovery methods [12,16,18,23,24] will be able to identify subgoals using the agent X  X  corresponding states transition graph without using the reward information. 5.2.2. Time complexity The time complexity determines the number of time steps that the agent needs to identify subgoals. In the frequency based [9,17] and the policy based [2,10] subgoal discovery algorithms, the agent needs to learn the task completely and then identify subgoals. The time complexity of learning the task can be reduced to O ( n 3 ) ,where n is the number of states [14]. On the other hand, the graph based subgoal discovery algorithms [12,16,18,23,24] use the corresponding graph of the agent state space to identify subgoals without needing to learn the task completely. Therefore, the agent X  X  states should be visited by the agent only once to extract the graph of the environment. Considering the results illustrated in Table 4, the proposed method runs in O ( nm ) , where n and m are the number of nodes and edges in the corresponding graph of the explored states, respectively. This complexity will be reduced to O ( n 2 ) for sparse graphs. This result is the same as that in [23] and comparable with the method proposed in [18] with complexity O ( n 3 ) , where n is the number of states, and in [24] with complexity O ( n 3 ) ,where n is the number of states observed in the last episode. 5.2.3. Subgoal ranking
Ranking subgoals means assigning different scores to them based on their importance. If we can rank subgoals, we will be able to select the most important subgoals and ignore other redundant ones. Therefore, the skills will be built based on these subgoals. As regar ds each skill, the policy function is needed to be saved in a Q-Table. So, in the case of limitation of the agent X  X  memory size, it is better to method has an ability to rank them. From Table 4 it i s clear that, among the proposed methods, those based on the graph centrality measure [23] have the a bility to rank subgoals. The empirical experiments show that the proposed graph centrality based subgoa l discovery method outperforms the previous one. 5.2.4. Number of adjustable parameters
Most subgoal discovery algorithms use some adjustable parameters. The values of these parameters should be set up by the designer before running the algorithm. These parameters might be assigned different values for different environments. According to the results illustrated in Table 4, it is clear that the proposed method has a special dominance in the number of adjustable parameters. While other methods, such as L-Cut [24] and Relative Novelty [22], include many manually tuned parameters, the proposed method includes three adjustable parameters, namely, the threshold value t that was shown to be a non-sensitive parameter,  X  that controls vanishing speed of the prior knowledge, and  X  that adjusts the prior probability of selection of skill and pri mitive action in each state. 5.2.5. Experimental environments
The subgoal discovery experimental environments can be divided into navigational and non-navigationaldomains. In the navigational domains, the agenttries to move from a point A to another point B in the environment. Different types of Grid World (G) [2,9,10,12,17,22 X 24], Taxi Driver (T) [22 X 24], Play Room(P) [23], Football (F) and Email (E) domains are examples of this type of environment. On the other hand, in the non-navigational domain, the agent tries to solve a problem instead of physically moving to the goal point. The Tower of Hanoi (H) domain is an example of this type of environment. Table 4 includes the results obtained by applying the proposed method has to both navigational and non-navigational domains. On the other hand, the previous subgoal discovery algorithms, such as [2,9, 10,12,17,22 X 24], have been focused only on the navigational domains. 5.3. A challenging environment In this section, we introduce  X  X ubik X  X  Cube X  (Fig. 13(a)) as a challenging environment benchmark for HRL methods. Rubik X  X  Cube is a 3-D puzzle made up of 3  X  3  X  3 smaller cubes with each of the six faces covered by nine stickers, among six solid colors (trad itionally white, red, blu e, orange, green, and yellow). A pivot mechanism enables each face to turn independently, thus mixing up the colors. The goal is to turn the cube back to the situation where each face shows only one color as shown in Fig. 13(g). Suppose that a reinforcement learning agent is used t o solve the task. Its corresponding state transition graph consists of 43.3 quintillion possible states. Figures 13(b), 13(c), 13(d), 13(e), and 13.f show some bottleneck state con fi gurations of the cube, but they do not include all the possibilities. By identifyi ng and using the cube X  X  subgoal states, the overall task can be decomposed into a set of subtasks. Therefore, our proposed algorithm can be applied to this domain to accelerate the agent X  X  learning performance, but the main problem is the time complexity of NCGS scores calculation steps which needs O ( n 2 ) time steps. In this case, the bottleneck states can be found after 2 . 3  X  10 22 years when a computer with a 2 GHZ CPU is used, whereas in the same situation, the other algorithms with O ( n 3 ) time complexity are able to identify the cube subgoals after 1 . 26  X  10 42 years. As regards memory requirements, there is a need of at least 0 . 373  X  10 9 terabytes to save the cube X  X  state information. Moreover, in each state there are 8 possible actions, so 2 . 984  X  10 9 terabytes is needed to save the Q-Table of the reinforcement learning agent. According to these facts, the approximation (or incremental calculation) of centrality measures to solve such complex tasks is called for in future works. 6. Discussion and conclusion
In this paper, a novel graph theoretic based skill acquisition method was presented. In brief, the main contributions of the proposed met hod are to utilize complex network theory measures for improving the subtotal identi fi cation process, proper initiation set assignmen t, and introducing prior knowledge to the policy of each developedskill in the option framework. In particular, the NCGS centrality was de fi ned and applied for candidate subgoal ranking and extraction. Then, the co-betweenness measure of the extracted candidates was used to create a weighted graph of candidates where a low cost clustering algorithm can be used to extract principal subgoals as the cluster centers. Furthermore, the co-betweenness measure was used for assignment of states to the initiation set of skills and determination of a prior probability for execution of skills in each state. Being applied to fi ve benchmark problems, the proposed method assignment, and policy learning, signi fi cantly. Here, we report that the proposed method is able to create skills incrementally. To do so, some temporary skills will be built based on the explored states and in the next episodes, by exploring more states, some new skills can be identi fi ed and then r edundant skills or weaker ones will be removed. Further investigation on this issue and the utilization of the proposed approach in more challenging environments are under progress.
 References
