
University of Guilan Universit  X  e Aix-Marseille
English: 16 . 21% in the first case and 8 . 83% in the second. 1. Introduction structural descriptions? His answer to this question was the extended domain of locality principle of Tree Adjoining Grammars, which allows us to represent in a single structure (an elementary tree 1 ) a predicate and its arguments. tural description has always been a major issue for syntactic formalisms and parsers.
Giving an overview of the different solutions that have been proposed to answer this question is clearly beyond the scope of this article. We will limit our discussion to the framework of graph-based dependency parsing (Eisner 1996; McDonald and Pereira 2006), in which our study takes place. The basic first-order model makes an extreme choice with respect to the domain of locality by limiting it to one dependency (the score to the target dependency. It has been shown experimentally that second-order models perform better than first-order ones, which tends to prove that second-order factors do capture important syntactic regularities. More recently, Koo and Collins (2010) proposed extending factor size to three dependencies and showed that such models yield better accuracy than second-order models on the same data. However, extending the order considered in a sentence grows exponentially with the order of the model. Using such models in a Dynamic Programming framework, such as Eisner (1996), becomes quickly intractable in terms of processing time and memory requirements. Additionally, by reliably estimating the scores of such factors we are quickly confronted with the problem of data sparseness.
 second-order models. This is why first-and second-order models perform quite well:
They reach a labeled accuracy score of 85 . 36% and 88 . 88% for first-and second-order
The same models trained on the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) reach 88 . 57% and 91 . 76% labeled accuracy score, respectively. The extension of the domain of locality can therefore be limited to some syntactic phenomena. This is the case in Tree Adjoining Grammars for which the extended domain of locality only concerned some aspects of syntax, namely, subcategorization.
 ing . The patching process is responsible for modeling two important aspects of algorithms. The first one is based on Dynamic Programming and the other one uses Integer Linear Programming (ILP). Second, they rely on different data sets. The first uses a standard treebank whereas the second uses much larger unannotated corpora.
 processes. As already mentioned, high-order factors are needed to model some specific syntactic phenomena and we do not want to systematically increase the order of the model in order to take them into account. Using factors of different sizes in a Dynamic
Programming parser is not practical and leads to ad hoc adaptation of the parser. This is the reason why patching is based on ILP, which can accommodate more naturally for constraints defined on structures of different sizes. 56 used to train them. The parser is trained on a standard treebank whereas the patching bilexical dependencies of the Collins parser (Collins 1997) have a very limited impact on the performances of the parser, although they were supposed to play a key role for some important ambiguous syntactic attachments. The reason for this is that treebanks are not large enough to correctly model bilexical dependencies. We show in Sections 7.2 and SFs.
 global syntactic phenomena in a parser. Parse reranking is one of them. The idea is to produce the n -best parses for a sentence and rerank them using higher order features, such as in Collins and Koo (2005) and Charniak and Johnson (2005). This solution has the advantage of drastically limiting the search space of the high-order search algorithm to the k best parses. Although the parsing architecture we propose in this article does use k -best parse lists, our solution allows us to combine dependencies that appear in any of the parses of the list. We show in Section 6.3 that combining dependencies that appear in any parse of a k -best list can yield parses that are more accurate than the best parse of this k -best list. Our method is closer to forest rescoring (Huang and Chiang 2007) or forest reranking (Huang 2008).
 and the global constraints as a single ILP program. This solution is possible because dependency parsing can be framed as an ILP program. Riedel and Clarke (2006) propose a formulation of nonprojective dependency parsing as an ILP program. This program, however, requires an exponential number of variables. Martins, Smith, and
Xing (2009) propose a more concise formulation that only requires a polynomial number of variables and constraints. Martins, Almeida, and Smith (2013) propose extending the model to third-order factors and using dual decomposition. All these approaches have in common their ability to produce nonprojective structures (Lecerf 1961). Our work departs from these approaches in that it uses a dynamic programming parser combined with an ILP solver. Additionally, we do not take nonprojective structures into account. given sentence, a set of optimal SFs and SCs, using ILP. The exact nature of SFs and produced by the two methods using constrained parsing. In Section 5, we justify the use of ILP by showing that the patching process is actually NP-complete. Sections 6, 7, and 8 constitute the experimental part of this work. Section 6 describes the experimental set-up, and Sections 7 and 8, respectively, give results obtained on French and English. Section 9 concludes the article.

Nasr, and Roux (2012) and Mirroshandel, Nasr, and Sagot (2013). In Mirroshandel, Nasr, and Roux (2012), a method for taking into account selectional constraints in a parser is presented, and in Mirroshandel, Nasr, and Sagot (2013) a method of introducing sub-categorization frames in the parser is described. This paper proposes a general frame-work for dealing with these two problems, both separately and jointly, using Integer
Linear Programming, whereas the two previous techniques used ad hoc adaptations and could not treat the two problems jointly. Additionally, we extended the experimen-tal part of our work to English, whereas previous work only concerned French. 2. Parsing
In this section, we briefly present the graph-based approach for projective dependency strained parsing , that forces the parser to include some specific patterns in its output.
Then, a simple confidence measure that indicates how confident the parser is about any subpart of the solution is presented. 2.1 Graph-Based Parsing Graph-based parsing (McDonald, Crammer, and Pereira 2005a; K  X  ubler, McDonald, and
Nivre 2009) defines a framework for parsing that does not make use of a generative grammar. In such a framework, given a sentence S = w 1 ... w the set of all dependency trees of sentence S . Such scores are usually the sum of the scores of subparts of S . The parser returns the tree that maximizes such a score: score of subpart  X  . The scores of the subparts are computed using machine learning algorithms on a treebank, such as McDonald, Crammer, and Pereira (2005b). model is the second-order model , which decomposes a tree into subparts made of two dependencies.
 contains an exponential number of trees. However, it can be solved in polynomial time using dynamic programming (Eisner 2000). 2.2 Constrained Parsing
The graph-based framework allows us to impose structural constraints on the parses that are produced by the parser in a straightforward way, a feature that will prove to be valuable in this work.
 58 and r the dependency label, with 1  X  i , j  X  l . We can define a new scoring function s the following way: sure that dependency d will be part of the solution produced by the parser because the score of all its competing dependencies (i.e., dependencies of the form ( i to  X  X  X  . As a result, the overall score of trees that have such competing dependencies cannot be higher than the trees that have our desired dependency. In other words, the resulting parse tree will contain dependency d . This feature will be used in Section 4 advance.
 cies in the output of the parser. Suppose D is a set of dependencies; we then define s the following way: by Koo and Collins (2010) to ignore dependencies whose probability was below a approach. 2.3 Confidence Measure
The other extension of graph-based parsers that we use in this work is the computation of confidence measures on the output of the parser. The confidence measure used is based on the estimation of posterior probability of a subtree, computed on the k -best list of parses of a sentence. It is very close to confidence measures used for speech recog-nition (Wessel et al. 2001) and differs from confidence measures used for parsing, such measures for complete parses of a sentence.
 parse set. Then C M (  X  ), the confidence measure associated with  X  , is computed as: more detail on the performance of this measure. Other confidence measures could be used, such as the edge expectation described in McDonald and Satta (2007), or inside-outside probabilities. 3. Patching patching sentence S is to detect in S the occurrences of predefined configurations.
In section 3.1 we introduce Lexico-Syntactic Configurations, which correspond to the instances of the patching problem, namely, subcategorization frame selection and se-together. 3.1 Lexico-Syntactic Configuration called the score of the LSC, and T is a dependency tree of arbitrary size. Every node of the dependency tree has five features: a syntactic function, a part of speech, a lemma, a fledged form, and an index, which represents a position in a sentence. The features of the score of i and T ( i ) will represent its tree.
 five features of the dependency tree nodes are represented between square brackets and are separated by colons. as the set of its leaf indices.

LSC on the sentence gives the following ILSC: 60 3.2 Selecting the Optimal Set of ILSCs of elements of L on S . Once the set I has been defined, our aim is to select a subset made of compatible 3 ILSCs such that combinatorial reasons: The power-set of I contains an exponential number of elements. Such a problem will be solved using ILP.
 of set L for a given sentence S must be built. Several methods can be used to compute such a set, as described in Section 6.3.

Section 3.3, ILSCs will represent SFs, and in Section 3.4, they will represent SCs. 3.3 Subcategorization Frames
A subcategorization frame is a special kind of LSC. Its root is a predicate and its leaves are the arguments of the predicate.
 frame; it has both a direct object and an indirect one introduced by the preposition `a as in Jean donne un livre `a Marie. [Jean gives a book to Marie ]. data sets that will be described in Sections 7 and 8. 3.3.1 Selecting the Optimal Set of Subcategorization Frames. Given a sentence S of length
N and the set I of Instantiated Subcategorization Frames (ISFs) over S , the selection constraints that are specific to SF selection. The exact formulation of the ILP program is now described. ambiguous prepositional phrase attachment: the prepositional phrase `a la biblioth`eque [ borrow ] . The set I is composed of the four following ISFs: 62 corresponds to a variable. Black dots (  X  ) correspond to the value 1 and white dots (  X  ) to the value 0. The optimal solution is solution number 7 for which verb rendre selects biblioth`eque is attached to the verb rendre . 3.4 Selectional Constraints
A selectional constraint (SC) is another kind of LSC. Its dependency tree is either made of a single dependency or two dependencies in a (daughter, mother, grandmother) configuration. An SC models the tendency of two words (the root and the leaf) to co-occur in a specific syntactic configuration.  X  1  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 0 2  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 2 3  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 4 4  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 3 5  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 6 6  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 5 7  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 8 8  X   X   X   X   X   X   X   X   X   X   X   X   X   X  0 . 7 a subject and object selectional constraint, respectively. Patterns 3 and 4, respectively, describe selectional constraints on an indirect object introduced by the preposition de and `a : of control and raising verbs, the SC pattern will be defined over the subject and the control/raising verb and not the embedded verb X  X lthough the selectional constraint is between the subject and the embedded verb. In the event of coordination of subject or object, only one of the coordinated elements will be extracted.
 lemma l l to appear together in configuration C . It should be maximal if whenever l curs as the root of configuration C , the leaf position is occupied by l if whenever l l occurs as the leaf of configuration C , the root position is occupied by l
A function that conforms to such a behavior is the following: with lemma l as a root (respectively, leaf) and C ( C , l configuration C with lemma l r as a root and lemma l l as a leaf, simultaneously. 64 always co-occur). It is close to pointwise mutual information (Church and Hanks 1990) but takes its values between 0 and 1. 3.4.1 Selecting the Optimal Set of Selectional Constraints. Given a sentence S of length N optimal set  X  I 0  X  I 0 is framed as the following ILP program. emprunt  X e `a la biblioth`eque. ). A set of four ISCs is defined:
The problem admits twelve solutions, represented in Figure 2. The optimal solution is verb emprunter . 3.5 Combining Subcategorization Frames and Selectional Constraints
SF selection and SC satisfaction can be combined together in a single ILP program that combines the variables and constraints of the two problems and adds a new constraint that takes care of the incompatibilities of SCs and SFs.
 of the optimal set  X  I 00  X  I  X  I 0 is the solution of the following program: 1  X   X   X   X   X   X   X   X  0 . 0 2  X   X   X   X   X   X   X   X  0 . 2 3  X   X   X   X   X   X   X   X  0 . 2 4  X   X   X   X   X   X   X   X  0 . 4 5  X   X   X   X   X   X   X   X  0 . 6 6  X   X   X   X   X   X   X   X  0 . 4 7  X   X   X   X   X   X   X   X  0 . 6 8  X   X   X   X   X   X   X   X  0 . 8 9  X   X   X   X   X   X   X   X  0 . 6 10  X   X   X   X   X   X   X   X  0 . 8 11  X   X   X   X   X   X   X   X  0 . 8 12  X   X   X   X   X   X   X   X  1 . 0 66 be exposed fully here. Its set of variables and constraints is the union of the variables and the constraints of the preceding ones, plus the two following ones:
ISCs 5, 6, and 8. 4. Combining Parsing and Patching
Two processes have been described in Sections 2 and 3. In the first one, parsing is based on dynamic programming and produces a complete parse of a sentence S whereas in the second, patching is based on ILP and produces a partial parse of S : the set of ISCs and ISFs.
 coming from parsing and patching. We have exposed in Section 1 the reasons why we chose to keep these two processes separated and to combine their solutions to produce a single parse. The whole process is composed of three steps: 1. Sentence S is first parsed using a first-order parser and the set of k -best 2. Patching is then performed and computes the optimal set 3. A new scoring function s +  X  of the scoring function s +  X  solution does not perform well.
 dence measure CM , defined in Section 2, is introduced in the objective function of the quantity is added to the scores of SFs and SCs introduced in sections 3.3 and 3.4. where the values of  X  1 and  X  2 are determined experimentally on a development set. In the definition of  X  s SF and  X  s SC , the first component ( s lexical score and the second one ( CM ) as a syntactic score.
 second order parser is modified in order to force the ISFs and ISCs of the parser X  X  solution.
 setting to  X  X  X  the scores of the dependencies that compete with those dependencies that occur in  X  I 00 . At this point, the set  X  I 00 factors, whatever the ILSC they are a member of. The scores of second-order factors are not modified. This solution works because the second-order parser uses both first-and second-order factors. There is no need to modify the scoring function of the second-(or higher) order factors since the first-order factors that they are composed of will be discarded anyway, because of their modified scores.
 wrongly attaches the preposition `a to the noun glace whereas it should be attached to the verb commander . Suppose that, after patching, the set  X  I
SBJ:N OBJ:N AOBJ:N ): the verb commander takes an indirect object introduced by prepo-serveuse as a dependent, are set to  X  X  X  and will be discarded from the parser solution. 5. Complexity of the Patching Problem
The two-stage architecture proposed in this paper X  X arsing using dynamic program-ming; then patching using ILP X  X uffers from a potential efficiency problem because of the complexity of ILP, which is NP-complete (Karp 1972) in the general case. Our system has therefore an exponential worst-case complexity. Two questions then arise: 68
Could patching be solved using a polynomial algorithm? How does a general ILP solver perform, in practice, on our data? patching is actually NP-complete. Building the optimal solution requires exponential time in the worst case. Second, we will show in Section 7, that, due to the size of the instances at hand, the processing time using a general ILP solver is reasonable.
NP-complete (Karp 1972). The representation of a patching problem as a SP problem will be called patching as a Set Packing problem (P-SP). We will then reduce SP to P-SP by showing that any instance of SP can be represented as an instance of P-SP, which will prove the NP-completeness of P-SP and therefore the NP-completeness of patching. of SP, the Maximum Set Packing problem (MSP), looks for the maximum number of pairwise disjoint sets in U .

More precisely, we restrict ourselves to instances such that: reduce any SP problem to a patching problem. Reducing an SP problem to some special general case might be represented as an MSP problem but this is not our goal.
C : a word cannot be the predicate of more than one ISF
C : a word cannot be the argument of more than one ISF
C : for an ISF to be selected, both its predicate and its arguments must be selected of S composed of the indices of the arguments of the ISF. The list of ISF I is therefore optimal solution  X  U that satisfies C 2 and partly C 3 . C of  X 
U are pairwise disjoint. C 3 is partly satisfied because all the arguments of an ISF are selected altogether, but not the predicate.

ISF predicate in the subset corresponding to the ISF. Unfortunately, this solution is not incorrectly prevent a word from acting as a predicate of an ISF and as an argument of another ISF. In order to solve this problem, we will associate two integers to a word of the sentence, one representing the word as a predicate and the other representing it as having word w p as a predicate and words w a clearly verifies our three constraints.
 { 1, ... , N } , with N  X  2 n 4 and the list U such that any element of U corresponds to an
ISF of I , using the method described here. The solution to the MSP on input ( S , U ) is called a P-MSP problem (patching as Maximum Set Packing). Recall that the decision version of the problem is called P-SP (patching as Set Packing).
 of SP to an instance of P-SP. Given the set S = { 1, ... , n } and the list U = ( U of subsets of S , let us build the set S 0 = { 1, ... , n + m } and the list U was a polynomial algorithm to solve patching, there would be a polynomial algorithm therefore NP-complete. 6. Experimental Set-up
We describe in this section and the two following ones the experimental part of this work. This section concerns language-independent aspects of the experimental set-up French and English data.
 offline processes are: 1. Training the parser on a treebank. 2. Extracting SFs and SCs from raw data and computing their scores. The 1. Generating the set I of ISFs and ISCs on a sentence S , using L . We will call 2. Patching sentence S with I , using ILP. The result of this process is the set 3. Parsing S under the constraints defined by  X  I .
 details about the parser that we use; Section 6.2 describes some aspects of the extraction of SCs and SFs from raw data. Section 6.3 focuses on the candidate generation process. 70 described in section 2.2. 6.1 Parser
The parser used for our experiments is the second-order graph-based parser implemen-tation of Bohnet (2010). We have extended the decoding part of the parser in order to implement constrained parsing and k -best parses generation. The first-order extension is based on algorithm 3 of Huang and Chiang (2005), and the second-order extension relies on a non-optimal greedy search algorithm.

Each feature template describes the governor and dependent of a labeled dependency, feature is associated with a score, computed on a treebank. A parse tree score is defined as the sum of the features it contains and the parser looks for the parse tree with the highest score.

Standard Feature Templates are the standard first-order feature templates. They de-
Linear Feature Templates describe the immediate linear neighbors of the governor
Grandchild Feature Templates are the first type of second-order features. They de-
Linear Grandchild Feature Templates describe the immediate linear neighbors of the
Sibling Feature Templates are the second type of second-order feature templates. They
Linear Sibling Feature Templates describe the immediate linear neighbors of the All features are used when the parser is run in second-order mode.
 the third and fourth have been specifically defined to monitor the effect of our model on the performances of the parser:
Labeled Accuracy Score (LAS) of a tree T is the ratio of correct labeled dependencies
Unlabeled Accuracy Score (UAS) of a tree T is the ratio of correct unlabeled dependen-
Subcategorization Frame Accuracy Score (SFAS) of a tree T is the ratio of verbs in T
Selectional Constraint Accuracy Score (SCAS) of a tree T is the ratio of correct occur-6.2 Extracting SFs and SCs from Raw Data
The LSC extraction process takes as input raw data and produces a set L composed of SCs and SFs. The process is straightforward: The corpora are first parsed, then LSC templates are applied on the parses, using unification, in order to produce SCs and SFs along with their number of occurrences. These numbers are used to compute selectional constraints scores ( s SC ), as defined in Section 3.4, and subcategorization frame scores ( s ) as described in Section 3.3.
 tion of research for many years. Giving an exhaustive description of this body of work is clearly beyond the aim of this article. The dominant approach for extracting lexical co-occurrences, as described, for example, in Volk (2001), Nakov and Hearst (2005), Pitler et al. (2010), and Zhou et al. (2011) directly model word co-occurrences on word strings.
Co-occurrences of pairs of words are first collected on raw corpora or n -grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of word co-occurrences is generally very simple: It is either based on the direct adjacency of the words in the string or their co-occurrence in a window of a few words. Bansal but use more sophisticated patterns, based on simple paraphrase rules, for identifying co-occurrences. Our work departs from these approaches by extracting co-occurences in specific lexicosyntactic contexts that correspond to our SC patterns. Our approach allows us to extract more fine-grained phenomena but, being based on automatically parsed data, it is more error-prone.
 for a long time, dating back at least to the work of Brent (1991) and Manning (1993). method we use for extracting SF is not novel with respect to such work. Our aim was not to devise new extraction techniques, but merely to evaluate the resource produced by such techniques for statistical parsing. 6.3 Candidate Generation The candidate generation process has as input a sentence S and a set L of SCs and SFs.
It produces the set I made of instantiations of elements of L on S . This set constitues the input of the ILP solver that will produce the solution 72 process is very important because it defines the search space of the patching process.
If the search space is not wide enough, it might fail to contain the correct solution. If it is too wide, the IL programs generated will be too large to be solved in a reasonable amount of time.

One could compute I on the linear representation of S enriched with part of speech tags and lemmas. Bechet and Nasr (2009), for example, represent LSCs as finite-state proposes some very unlikely instantiations.
 ple, LSCs can be matched on a set T of possible parses of S in order to produce I . The set T itself can be the k -best parses produced by a parser, as in parse reranking (Collins and Koo 2005; Charniak and Johnson 2005). It can also be built using parse correction techniques (Hall and Nov  X  ak 2005; Attardi and Ciaramita 2007; Henestroza and Candito best parses, such as in parse reranking. But this list is merged into a set of dependencies
D k that is the union of all the dependencies appearing in the k -best trees. The set D not a tree because a single word can have several governors. This set can be seen as a superset of the k -best parses. We will call this method sub-parse combination or simply combination when the context is not ambiguous.
 that it defines is therefore larger than the k -best list. The set D pruned forest of Boullier, Nasr, and Sagot (2009). Once the set D are matched on the dependencies of D k using unification.
 the corresponding D k set, we define two measures on the k -best parse list. The first one (Oracle LAS) corresponds to the LAS of the best tree in the k -best list. The second one (Recall LAS) is the part of labeled dependencies of the reference parse that appear in
D . Oracle LAS constitutes an upper bound for reranking and Recall LAS is an upper bound for combination. The values of these two measures are represented in Figure 3 for first-order parsing (left) and second-order parsing (right) measured on the test set of the French Treebank.

Oracle LAS curve. In the case of first-order parsing, the Oracle curve quickly reaches a be combinations of dependencies that appeared in previous parses. The number of new dependencies created tend to decrease quickly with k . For k = 100, the difference between the Oracle and the Recall scores is almost equal to 5 points. This difference clearly demonstrates the advantage of using D k instead of the k -best list. not reach an asymptote, as it was the case for first-order parsing. New dependencies are continually being created as k increases. The reason why we have considered first-order models in this section is that generating a first-order k -best solution can be done very efficiently; second-order k -best generation is more time-consuming. Performing method. We will see in the two following sections that first-order k -best generation gave good results on French but did not perform well on English, where second-order k -best generation had to be used.

Figure 4 answers this question by showing the evolution of Recall SFAS and Recall SCAS almost 94% of the ISCs and 93% of the ISFs that appear in the reference can be recovered.
The results for second-order k -best are, respectively, equal to 97% and 96%. 7. Experiments on French
We describe in this section the experiments performed on French and the results ob-tained. We start, in Section 7.1, with the description of the data used to train the parser 74 an analysis of the errors is presented in Section 7.4. Section 7.5 analyzes the runtime performances. 7.1 Parsing transformed into dependency trees by Candito et al. (2009). The size of the treebank and its decomposition into train, development, and test sets are represented in Table 1. tively, 88 . 88% and 90 . 71%. The SCAS reaches 87 . 81% and the SFAS is 80 . 84%, which These results are for sentences with gold part-of-speech-tags and lemmas. ments, which account for 23 . 67% of the errors. A more detailed description of the errors is given in Table 2. Every line of the table corresponds to one attachment type. The first column describes the nature of the attachment. The second indicates the frequency of this type of dependency in the corpus. The third column displays the accuracy for this type of dependency (the number of dependencies of this type correctly predicted by the parser divided by the total number of dependencies of this type). The fourth column error rate. We have used this error analysis to define the following four SC patterns, which we call SBJ, OBJ, VdeN, and VaN: type of error. One can see that all SC patterns are not described at the same level of detail X  X ome of them specify the label of the dependency, if considered important (to distinguish subject and object, for example), and others specify the lexical nature of the dependent (for prepositions, for example). 7.2 Extracting SFs and SCs
In order to automatically produce the set L of SCs and SFs, we gathered a raw corpus that we have parsed using the parser described in Section 7.1. The corpus is actually a of news reports of the French press agency Agence France Presse ; the second (EST REP) is a collection of newspaper articles from a local French newspaper: l X  X st R  X epublicain .
The third (WIKI) is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in Table 3.

MACAON tool suite (Nasr et al. 2011), then parsed in order to get the most likely parse for every sentence. The SFs and SCs were then extracted from the parses and their scores computed. In the two following sections, we give some statistics on the SFs and SCs produced. 7.2.1 Extracted SF. As described in Section 3.3, an SF frame is a special kind of LSC. Its root is a predicate and its leaves are the arguments of the predicate. Some linguistic constraints are defined over SF. The first one is the category of the root, which must be either a finite tense verb (V), an infinitive form (VINF), a past participle (VPP), or a present participle (VPR). The second is the set of syntactic functions that introduce indirect object introduced by preposition `a (A-OBJ) or de (DE-OBJ).
 of speech. We have factored pronouns, common nouns, and proper nouns into a single category N. We have not gathered verbs of different modes into one category because abstraction is the absence of linear order between the arguments.
 are shown in Table 4, column A 0 . As can be observed, the number of verbal lemmas and 76
SFs are unrealistic. This is because the data from which SFs were extracted are noisy: they consist of automatically produced syntactic trees that contain errors. There are two main sources of errors in the parsed data: the pre-processing chain (tokenization, part-not; and, of course, parsing errors, which tend to create incorrect SFs. In order to fight against noise, we have used a simple thresholding: We only collect SFs that occur more than a threshold i . The results of the thresholding appear in columns A the subscript is the value of the threshold. It is a very crude technique that allows us to eliminate many errors but also rare phenomena. As expected, both the number of verbs and SFs decrease sharply when increasing the value of the threshold. The average number of SFs per verb is 14 . 26 without threshold and reaches 16 . 16 for a threshold corpus: A large part of hapax are words mistakenly tagged as verbs. They disappear with thresholding, and their disappearance tends to increase ambiguity.
 represents the coverage of the SF extracted from FTB TRAIN. These figures constitute an interesting reference point when evaluating the coverage of the SFs extracted from raw data. We have distinguished lexical coverage (ratio of verbal lemmas of FTB DEV present in the resource) and syntactic coverage (ratio of SFs of FTB DEV present in the resource). Both measures were computed on types and tokens. Table 5 shows that lexical automatically built resource is much higher than syntactic coverage of the same resource extracted from FTB TRAIN. This clearly shows that many SFs of FTB DEV were not seen in FTB TRAIN. This observation supports our hypothesis that treebanks are not large enough to accurately model the possible SFs of a verb. Comparing coverage of T and
A x is somehow unfair because, as we already mentioned, the automatic resources are noisy, and have a tendency to overgenerate whereas T contains only correct SFs (up to the corpus annotation errors). Accuracy results of Section 7.3 will give a more balanced view by computing how many errors are actually corrected using the noisy extracted resource. configurations are shown in Table 6 for the three thresholds 0, 5, and 10.
SC coverage is much lower than SF coverage. This is due to the different nature of the two phenomena: SC is a bilexical phenomena whereas SF is a monolexical one. Ideally, a larger corpus should be used to extract SCs. One can also note that the coverage of
SCs extracted from FTB DEV is very low. 7.3 Results defined in Section 6.1. The accuracy scores of the baseline parser are presented in line 1.
Line 2 shows the results obtained when setting parameters  X  case, only combines ILSCs that have a good confidence measure. This experiment is important in order to ensure that s SF and s SC do carry useful information. As one can the accuracy of the parser can be increased just by imposing the final parse sub-parses that have a good confidence measure.
 These values are used for the experiments corresponding to lines 3, 4, and 5. Lines 3 78 and 4 indicate the accuracy for SF selection and SC satisfaction alone, as described in Sections 3.3 and 3.4. Line 5 provides the result when they are combined, as described in Section 3.5.
 methods increase both LAS and UAS with respect to the baseline. The best results are obtained by the combined method. SCAS and SFAS allow us to gain a better under-standing of what is happening. As could be expected, SF selection has a stronger impact on SFAS whereas SC satisfaction has a stronger impact on SCAS. But both methods also have a positive influence on the other measure. This is because SFs and SCs are not independent phenomena and correcting one can also lead to correcting the other. The combined method yields the best results on our four measures. 7.4 Error Analysis
There are three main causes for SC and SF errors: insufficient coverage of L , narrowness of the patching search space I , or inadequacy of the ILP objective function. Given a instantiation of y ). The three error causes are described as follows:
NotInDB y /  X  L . This is the situation where an ILSC cannot be extracted from the k -
NotInKB x /  X  I  X  I 0 . This is the situation where x does not appear in any of the k -best describes the nature of the attachment. The four first rows correspond to specific SC patterns, row 5 is the average of all SCs, and row 6 concerns SF selection. Column 2 decrease of the number of errors and columns 4, 5, and 6 give the distribution of the errors over the three error types defined earlier.
 errors and by 22% the number of SF errors. The difference between these two figures comes from the fact that SFs are more complex phenomena that SC patterns. SFs can be composed of a large number of dependencies and an error in one of them leads to an error in the SF selection.
 the cases, the correct SC is not L ; in 42% of the cases, the correct SC does not appear in that the objective function of the ILP is based on the scores of ILSCs, which are them-selves a combination of a lexical and a syntactic score: s (  X  ) = (1  X   X  ) s The cause of the error could be the lexical score ( s weight (  X  ).
 of the s SF score, which is a simple conditional probability. 80 7.5 Runtime Analysis
Figure 5 displays the runtime of our system with respect to the length of the sentences parsed. Runtime has been decomposed in three parts: first-order 100-best generation, process is not reported because it is negligible compared with the runtime of the other processes.
 (approximately 330 words per second on average), we show that the patching problem is NP-complete and we have no guarantee that the problem will be solved in polynomial time. 8. Experiments on English
A second series of experiments was performed on English. Although the setting used is almost the same as for French, there are some notable differences between the two series of experiments with respect to the data used: 8.1 Parsing
The parser was trained and evaluated on the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). We have used the standard decomposition of the Penn Treebank into training, development, and test sets, with sizes as reported in Table 10. The main better results than the same parser trained on the French Treebank. The LAS is 91 . 76 and UAS is 93 . 75, a relative increase of 3 . 24% and 3 . 35% with respect to French. preposition used. The nine most frequent prepositions occuring in the Penn Treebank A set of 20 SC patterns are defined: { OBJ, SBJ, VofN, VinN, VtoN, VforN, VwithN, VonN, VatN, VfromN, VbyN, NofN, NinN, NtoN, NforN, NwithN, NonN, NatN, NfromN,
NbyN } . SFs are defined as the dependents of a verb that are introduced by one of the nine prepositions, plus the subject and the object.
 errors on prepositional attachments. This situation may be due to the fact that prepo-sitional attachments are more ambiguous in French (4 . 56 prepositions on average per sentence in the FTB compared with 3 . 15 for the PTB).
 reaches 18 . 87%, which is an approximation of the upper limit of the improvement that we can expect from our method. The mean accuracy for these patterns is almost equal to 92%. Two situations should be distinguished, however: the subject and direct object dependencies, on the one hand, and the prepositional attachments, on the other. The 82 prepositional attachments (restricted to the nine most frequent prepositions) represent equal to 13 . 37% and the mean accuracy of these attachments is equal to 82 . 32%, whereas subject and direct object dependencies correspond to 13 . 19% of the dependencies but generate only 5 . 5% of the errors. 8.2 Extracting SFs and SCs
The ISFs and ISCs were extracted from the Google-parsed Gigaword (Napoles, Gormley, and Van Durme 2012b, 2012a) (noted as the GIGA corpus in the remainder of the article), which is a subset of the original English Gigaword, fifth edition (Parker et al. 2011), parsed with the parser described in Huang, Harper, and Petrov (2010). The origin of the corpora as well as their sizes are reported in Table 12. As one can see, the size of the corpus is one order of magnitude larger than the French corpus we gathered to extract ISFs and ISCs X  X ore precisely, it is 31 times larger.
 Table 13. A total of 33 . 6 million occurrences of SC patterns have been extracted.
This difference is mainly due to the definition of English SFs, which take into account more prepositions. The mean number of SFs per verb is 15 . 39 without thresholding and reaches 17 . 73 for a threshold of 10.
 coverage for SCs and SFs extracted from PTB TRAIN.
 more data. The most striking figure of Table 15 is the coverage of the training corpus. only by the larger size of the training corpus for English. It seems to indicate a higher lexicosyntactic variety in the French Treebank than in the Penn Treebank, or a higher variety between the development and training sets of the French Treebank compared with the Penn Treebank.
 pairs [verbal lemma, SF] of PTB DEV present in the GIGA corpus) is 77 . 56% when no of the more extended definition of SF in English. But the more surprising figure is the coverage of the PTB TRAIN, which is 82 . 7% (it was 73 . 54% for French). This difference could be explained, as was the case for SC coverage, by a difference of lexicosyntactic variety between the French Treebank and the Penn Treebank. 8.3 Results
English experiment, as already noted, the accuracy of the second-order parser is higher enough room for improvement over a second-order parser. The situation is detailed in 84
Table 17. The table shows that the Oracle accuracy of 100 best parses for a first-order parser is often below the accuracy of the first-best parse for a second-order parser. The situation is slightly more favorable for k = 200 but is still dominated for some SCs by the first-best second-order accuracy. This is the reason why we decided to use a second-order parser in the candidate generation step. The rest of the process is unchanged. ments is 3 . 15% and the decrease of unlabeled error attachments is 4 . 16%. These results are lower than what we observed for French, where the decrease is, respectively, 5 . 66%
Subcategorization Frame assignments by 8 . 83%. They were equal to 41 . 6% and 22%, respectively, for French. These results are disappointing X  X e expected that the larger size of the corpus used for extracting ISCs and ISFs would yield a larger decrease of SC and SF errors. 8.4 Error Analysis
Table 19 allows us to gain a better understanding of the causes of the errors. The first cause of errors is the ILP objective function. In 53% of the cases for SC errors and 59% of the cases for SF errors, the ILP failed to find the correct SFs or SCs although it is because it has, itself, three causes, as we mentioned in our experiments on French: the
SC or SF scores, the confidence measure, or the weight  X  . The second cause of errors 86 is coverage. Despite the size of the raw corpus used to extract SCs and SFs, the lack of coverage remains an important source of errors. The quality of the search space of the
ILP is the third cause of errors. This observation shows that candidate generation with second-order parsing yields better results than first-order parsing. 9. Conclusions and Discussion We showed in this paper that erroneous Subcategorization Frame assignment and
Selectional Constraint violations can be partially corrected by using weighted lexicosyn-tactic patterns, called patches, representing Subcategorization Frames and Selectional
Constraints that have been extracted from a large raw corpus. The experiments, per-formed on the French Treebank, allowed us to decrease by 41 . 6% Selectional Constraint violations and by 22% erroneous Subcategorization Frame assignments. These figures are lower for English: 16 . 21% in the first case and 8 . 83% in the second. sub-processes. The first one, called patching, identifies, in a sentence, a optimal set of patches that can be of arbitrary size. The second process is based on constrained graph-based parsing; it takes as input an optimal set of patches and builds a complete parse of the sentence that contains these patches.
 and global lexicosyntactic patterns in a parser. It has been applied to the problem of selecting Subcategorization Frames and Selectional Constraints but it could be used to frames or discourse structures. solving an instance of the patching problem could require exponential time. In practice, for our task, the resolution time is reasonable, but it could become an issue when dealing with larger patches. Other approximate methods could be used to perform the patching task, this is the first direction in which we wish to continue this work. process, namely, candidate generation. Patches being lexicosyntactic structures, the gen-eration process needs to have access to the possible syntactic structures of the sentences.
The solution we have proposed uses a first-or second-order k -best list of parses from would be to perform the patch generation directly on the parse forest.

ISFs is an important cause of errors, despite the size of the corpus used to extract ISCs and ISFs. Collecting larger corpora might not be the right solution to increase coverage.
The third direction we would like to explore is generalizing over the events that are observed in the data, using distributional semantics.
 Acknowledgments References 88
