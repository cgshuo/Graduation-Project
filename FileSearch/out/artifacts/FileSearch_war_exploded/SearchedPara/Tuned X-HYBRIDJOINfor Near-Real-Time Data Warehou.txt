 Near real-time data warehousing exploits the concepts of data freshness in tra-ditional static data repositories in order to meet the required decision support capabilities. The tools and techniques for promoting these concepts are rapidly evolving. Most data warehouses have already switched from a full refresh [7] to an incremental refresh policy [4]. Further the batch-oriented, incremental refresh approach is moving towards a continuous, incremental refresh approach.
One important research area in the field of data warehousing is data transfor-mation, since the updates coming from the data sources are not in the format required for the data warehouse. Furthermore, a join operator is required to implement the data transformation.

In traditional data warehousing the update tuples are buffered in memory and joined when resources become available [6] . Whereas, in real-time data warehous-ing these update tuples are joined when they are generated in the data sources. One important factor related to the join is that both inputs of the join come from different sources with different arrival rates. The input from the data sources is in the form of an update stream which is fast, while the access rate of the lookup table is comparatively slow due to disk I/O cost. It creates a bottleneck in pro-cessing of update stream and the research challenge here is to minimize this bottleneck by optimizing the performance of the join operator.
 To overcome these challenges a novel st ream-based join algorithm called X-HYBRIDJOIN (Extended Hybrid Join) [1] was proposed recently by the author. This algorithm not only addressed the issues described above but also was de-signed to take into account typical market characteristics, commonly known as the 80/20 sale Rule [2]. According to this rule 80 percent of sale focus on only 20 percent of the products, i.e., one observes a Zipfian distribution. To achieve this objective, one component of the algorithm called disk-buffer is divided into two equal parts. The contents of one part of the disk buffer (called the non-swappable part) are kept fixed while the contents of the other part (called the swappable part) are exchanged for each iteration of the algorithm. As the non-swappable part of the disk buffer always contains the most frequently used disk pages, most stream tuples can be processed without invoking the disk. Although the author presented an adaptive algorithm to adopt the typical market characteristics, the components of the algorithm are not tuned to make efficient use of available memory resources. Further details about this issue are provided in Section 3.
On the basis of these observations, a revised X-HYBRIDJOIN is proposed with name Tuned X-HYBRIDJOIN. The cost model of existing X-HYBRIDJOIN is revised and the components of the proposed algorithm are tuned based on that cost model. As a result the available memory is distributed among all compo-nents optimally and consequently it improves the performance of the algorithm significantly.

The rest of the paper is structured as follows. The related work to proposed algorithm is presented in Section 2. Sectio n 3 describes problem statement about the current approach. The proposed solution for the stated problem is presented in Section 4. Section 5 presents the tuning of the proposed algorithm based on the revised cost model. The experimen tal study is discussed in Section 6 and finally Section 7 concludes the paper. Some techniques have already been introduced to process the join queries over continuous streaming data [5]. This section presents only those approaches which are directly related to the stated problem domain.

A stream-based algorithm called Mesh Join (MESHJOIN) [8] was designed specifically for joining a continuous stream with disk-based data in an active data warehouse. This is an adaptive approach but there are some research is-sues related to inefficient memory distribution among join components due to unnecessary constraints and an inefficien t strategy for accessing the disk based relation.
R-MESHJOIN (reduced Mesh Join) [9] is a revised version of MESHJOIN that focuses on the optimal distribution of memory among the join components. The R-MESHJOIN algorithm introduces the new strategy for memory distribu-tion among the join components by implementing real constraints. However, the mechanism used for accessing the disk based relation is similar to MESHJOIN.
One approach to improve MESHJOIN has been a partition-based join algo-rithm [10]. It uses a two-level hash tabl e in order to attempt to join stream tuples as soon as they arrive, and uses a partition-bases waiting area for other stream tuples. In this approach the author keeps focus about the analysis of the stream buffer in terms of back log tuples rather than analysing the performance of the algorithm.
 A recent piece of work introduces a novel stream-based join called HYBRID-JOIN (Hybrid Join) [11]. The key objective of this effort is to minimize the disk overhead using an index-based approach and to deal with intermittency in the stream. Although both issues are successfully addressed in this approach, it is not optimal with respect to stream data with Zipfian distribution. This section presents a working overview of X-HYBRIDJOIN along with the research issue. In the field of real-time data warehousing X-HYBRIDJOIN is an adaptive algorithm for joining the bursty data stream with disk-based master data. Although the typical characteristics of market data are considered, optimal settings for the available limited memory resources are not considered. Before describing the problem it is first necessary to explain the major components of X-HYBRIDJOIN and the role of each component. Figure 1 presents an abstract level working overview of X-HYBRIDJOIN where m is the number of partitions in the queue to store stream tuples and n is the number of pages in disk-based master data R .Moreover, R is assumed to be sorted with respect to the access frequency. The stream tuples are stored in the hash table while the join attribute values are stored in the queue. The queue is implemented using a doubly linked-list data structure to allow the random deletion of matching tuples. The disk buffer is used to load the disk pages into memory. To make efficient use of R by minimizing the disk access cost, the disk buffer is divided into two equal parts. One is called the non-swappable part which stores a small but most frequently used portion of R into memory on a permanent basis. The other part of the disk buffer is swappable and for each iteration it loads the disk page p i from R into memory.

Before the join process starts X-HYBRIDJOIN loads the most frequently used page of R into the non-swappable part of the disk buffer. During the join process, for each iteration the algorithm dequeues the oldest join attribute value from the queue and using this value as an index it loads the relevant disk page into the swappable part of the disk buffer. After loading the disk page into memory the algorithm matches each of the disk tuples available in both the swappable and non-swappable parts of the disk buffer with the stream tuples in the hash table. If the tuple match, the algorithm generates the resulting tuple as output and deletes the stream tuple from the hash table along with its join attribute value from the queue. In the next iteration the algorithm again dequeues the oldest element from the queue, loads the relevant disk page into the swappable part of the disk buffer and repeats the procedure.

X-HYBRIDJOIN minimizes the disk acces s cost and improves performance significantly by introducing the non-swappable part of the disk buffer. But in X-HYBRIDJOIN the memory assigned to the swappable part of the disk buffer is equal to the size of disk buffer in HYBRIDJOIN [11] and the same amount of memory is allocated to the non-swappable part of the disk buffer. In the following it will be shown that this is not optimal. The problem considered in this paper is to tune the size of both parts of the disk buffer so that the memory distribution among these two components is optimal. Once these two components acquire optimal settings, based on that memory can be assigned to the rest of join components. As a solution for the above stated problem, a revised version of X-HYBRIDJOIN calledTunedX-HYBRIDJOINispropose d. This section presents the memory architecture and a revised cost model f or the proposed algorithm. Most im-portantly, the tuning for the proposed algorithm is presented while the tuning procedure is based on both a measurement strategy and the cost model. 4.1 Memory Architecture The memory architecture that Tuned X-HYBRIDJOIN uses is shown in Fig-ure 2. From the figure, Tuned X-HYBRIDJOIN includes the same number of components as X-HYBRIDJOIN however, the memory size for each component is different to that in X-HYBRIDJOIN. In Tuned X-HYBRIDJOIN since mem-ory is allocated to the each componen t after executing the tuning module and therefore, each component is assigned an optimal size of memory. Particulary, X-HYBRIDJOIN uses same memory for the both swappable and non-swappable parts of the disk buffer but after tuning it has been explored that the optimal size of memory for the both components is different. The reason for it is presented in Section 5. 4.2 Cost Calculation This section revises the cost formulas d erived in X-HYBRIDJOIN. The reason for revising the cost model is that X-HYBRIDJOIN uses equal memory for both the swappable and non-swappable parts of the disk buffer, and therefore the formulas do not apply for other relative sizes. Following the style of cost modeling used for MESHJOIN, the cost for any algorithm is expressed in terms of memory and processing time. Equation 1 describes the total memory used to implement the algorithm while Equation 2 calculates the processing cost for w tuples. Memory Cost. Since the optimal values for the sizes of both the swappable part and non-swappable part can be different, it is assumed k number of pages for the swappable part and l number of pages for the non-swappable part. Overall the largest portion of the total memory is used for the hash table while a much smaller amount is used for each of the disk buffer and the queue. The memory for each component can be calculated as given below: Memory for the swappable part of disk buffer (bytes)= k  X  v P (where v P is the size of each disk page in bytes).
 Memory for the non-swappable part of disk buffer (bytes)= l  X  v P .
 Memory for the hash table (bytes)=  X  [ M  X  ( k + l ) v P ](where M is the total allocated memory and  X  is memory weight for the hash table).
 Memory for the queue (bytes)= (1  X   X  )[ M  X  ( k + l ) v P ](where(1  X   X  )ismemory weight for the queue).
 The total memory used by the algorithm can be determined by aggregating the above.
 Currently the memory reserved for the s tream buffer is not included because it is small (0.05 MB was sufficient in all experiments presented in this paper). Processing Cost. This section presents the processing cost for the proposed approach. The cost for one iteration of the algorithm is denoted by c loop and express it as the sum of the costs for the individual operations. Therefore the processing cost for each component is first calculated separately.
 Cost to read the non-swappable part of disk buffer (nanoseconds) = c I/O ( l  X  v P ) . Cost to read the swappable part of disk buffer (nanoseconds)= c I/O ( k  X  v P ) . Cost to look-up the non-swappable part of disk buffer in the hash table (nanosec-onds) = d N c H (where d N = l v P v buffer in terms of tuples, v P is size of disk page in bytes, v R is size of disk tuple in bytes, and c H is look-up cost for one disk tuple in the hash table). Cost to look-up the swappable part of disk buffer in the hash table (nanoseconds)= d of tuples).
 Cost to generate the output for w matching tuples (nanoseconds) = w  X  c O (where c O is cost to generate one tuple as an output).
 Cost to delete w tuples from the hash table and the queue (nanoseconds)= w  X  c E (where c E is cost to remove one tuple from the hash table and the queue). Cost to read w tuples from stream S into the stream buffer (nanoseconds)= w  X  c S (where c S is cost to read one stream tuple into the stream buffer). Cost to append w tuples into the hash table and the queue (nanoseconds)= w  X  c A (where c A is cost to append one stream tuple in the hash table and the queue). As the non-swappable part of the disk buffer is read only once before execution starts, it is excluded. The total cost for one loop iteration is: Since in every c loop seconds the algorithm processes w tuples of stream S ,the performance or service rate  X  can be calculated by dividing w by the cost for one loop iteration. The stream-based join operators normally execute within limited memory and therefore tuning of join components is important to make efficient use of the available memory. For each component in isolation, more memory would be bet-ter but assuming a fixed memory allocation there is a trade-off in the distribution of memory. Assigning more memory to one component means less memory for other components. Therefore it needs to find the optimal distribution of memory among all components in order to attain maximum performance. A very impor-tant component is the disk buffer because reading data from disk to memory is expensive.

In the proposed approach tuning is first performed through performance mea-surements by considering a series of values for the sizes of the swappable and non-swappable parts of the disk buffer. Later a mathematical model for tuning is also derived from the cost model. Finally, the tuning results of both approaches are compared to validate the cost model. The details about the experimental setup are presented in Table 1. 5.1 Tuning through Measurements This section presents the tuning of the key components of the algorithm through measurements. In the measurement approach the performance is tested on particular memory settings for swappable and non-swappable parts rather than on every contiguous value.

The measurement approach assumes the size of total memory and the size of R are fixed. The sizes for the swappable and non-swappable parts vary in such a way that for each size of the swappable part the performance is measured against a range of sizes for the non-swappable part. By changing the sizes for both parts of the disk buffer the memory sizes for the hash table and the queue are also affected.

The performance measurements for var ying the sizes of both swappable and non-swappable parts are shown in Figure 3. The figure shows that the perfor-mance increases rapidly by increasing the size for the non-swappable part. After reaching a particular value for the size of non-swappable part the performance starts decreasing. The plausible reason behind this behavior is that in the be-ginning when the size for the non-swappable part increases, the probability of matching stream tuples with disk tuples also increases and that improves the performance. But when the size for the non-swappable part is increased further it does not make a significant difference in stream matching probability. On the other hand, due to higher look-up cost and the fact that less memory is avail-able for the hash table the performance decreases gradually. A similar behavior is seen when the performance against the swappable part is tested. In this case, after attaining the maximum performan ce it decreases rapidly because of an increase in the I/O cost for loading the growing swappable part. From the mea-surements shown in the figure it is possible to approximate the optimal settings for both the swappable and non-swappable parts by finding the maximum on the two-dimensional surface. 5.2 Tuning Based on Cost Model A mathematical model for the tuning is also derived based on the cost model presented in Section 4.2. From Equation 3 it is clear that the service rate depends onthesizeof w and the cost c loop . To determine the optimal settings it is first necessary to calculate the size of w . The main components on which the value of w depends are:-size of the non-swappable part ( d N ), size of the swappable part ( d S ), size of the master data ( R t ), and size of the hash table ( h S ).
Typically the stream of updates can be a pproximated through Zipf X  X  law with a certain exponent value. Therefore, a s ignificant part of the stream is joined with the non-swappable part of the disk buffer. Hence, if the size of the non-swappable part (i.e. d N ) is increased, more stream t uples will match as a result. But the probability of matching does not increase at the same rate as increasing d
N because, according to Zipfian distribution, the matching probability for the second tuple in R is half of that for the first tuple and similarly the matching probability for the third tuple is one third of that for the first tuple and so on [2]. Duetothisproperty,thesizeof R (denoted by R t ) also affects the matching probability. The swappable part of the disk buffer deals with the rest of the master data denoted by R (where R = R t  X  d N ),whichislessfrequentin the stream than that part which exists permanently in memory. The algorithm reads R in partitions, where the size of each partition is equal to the size of the swappable part of the disk buffer d S . In each iteration the algorithm reads one partition of R using an index on join attribute and loads it into memory through a swappable part of the disk buffer. In the next iteration the current partition in memory is replaced by a new partition, and so on. As mentioned earlier, using the Zipfian distribution the matching probability for every next tuple is less than the previous one. Therefore, the total number of matches against each partition is not the same. This is explained further in Figure 4, where n total partitions are considered in R . From the figure it can be seen the matching probability for each disk partition decreases continuou sly as one moves toward the end position in R .

The size of the hash table is another component that affects w . The reason is simple: if there are more stream tuples in memory, the number of matches will be greater and vice versa. Before driving the formula to calculate w it is first necessary to understand the working strategy of Tuned X-HYBRIDJOIN. Consider for a moment that the queue contains stream tuples instead of just join attribute values. Tuned X-HYBRIDJOIN uses two independent inner loops under one outer loop. After the end of the first inner loop, which means after finishing the processing of the non-swappable part, the queue only contains those stream tuples which are related to only the swappable part of R , denoted by R . For the next outer iteration of the algorithm these stream tuples in the queue are considered to be an old part of the queue. In that next outer iteration the algorithm loads some new stream tuples into the queue and these new stream tuples are considered to be a new part of the queue. The reason for dividing the queue into two parts is that the matching probability for both parts of the queue is different. The matching probability for the old part of the queue is denoted by p old and it is only based on the size of the swappable part of R i.e. R .Onthe other hand, the matching probability for the new part of the queue, known as p new , depends on both the non-swappable as well as the swappable parts of R . Therefore, to calculate w it is first needed to calculate both these probabilities.
Therefore, if the stream of updates S obeys Zipf X  X  law, then the matching probability for any swappable partition k with the old part of the queue can be determined mathematically as shown below. Each summation in the above equation generates a harmonic series, which can be summed up using the formula constant whose value is approximately equal to 0.5772156649 and  X  k is another constant which is  X  1 2 k . The value of  X  k approaches 0 as k goes to  X  [3]. In this paper the value of 1 2 k is small and therefore, it is ignored.

If there are n partitions in R , then the average probability of an arbitrary partition of R matching the old part of the queue can be determined using Equation 4. Now the probability of matching is determined for the new part of the queue. Since the new input stream tuple can match either the non-swappable or the swappable part of R , the average matching probability of the new part of the queue with both parts of the disk buffer can be calculated using Equation 5. where p N and p S are the probabilities of matching for a stream tuple with the non-swappable part and the swappable part of the disk buffer respectively. The values of p N and p S can be calculated as below.
 Assume that w are the new stream tuples that the algorithm will load into the queue in the next outer iteration. Therefore, The size of the new part of the queue (tuples)= w The size of the old part of the queue (tuples)=( h S  X  w ) If w are the average number of matches per outer iteration with both the swap-pable and non-swappable parts, then w can be calculated by applying the bino-mial probability distribution on Equations 4 and 5 as given below. After simplification the final formula to calculate w is described in Equation 6. By using the values of w and c loop in Equation 3 the algorithm can be tuned. 5.3 Comparisons of Both Approaches To validate the cost model the tuning results based on the measurement approach are compared with those that are achieved through cost model.
 Swappable Part: This experiment compares the tuning results for the swap-pable part of the disk buffer using both the measurement and cost model ap-proaches. The tuning results of each approach (with 95% confidence interval in case of measurement approach) are shown in Figure 5 (a). From the figure it is evident that at every position the results in both cases are similar, with only 0.5% deviation.
 Non-swappable Part: Similar to before, the tuning results of both approaches for the non-swappable part of the disk buffer are also compared. The results are shown in Figure 5 (b). Again, it can be seen from the figure, the results in both cases are nearly equal with a deviation of only 0.6%. This proves the correctness of the tuning module. To strengthen the arguments an experimental evaluation of proposed Tuned X-HYBRIDJOIN is performed using the synthetic datasets. Normally, in Tuned X-HYBRIDJOIN kinds of algorithms, the total memory and the size of R are the common parameters that vary frequen tly. Therefore, the experiments pre-sented here compare the performance by varying both parameters individually. Performance Comparisons for Different Sizes of R : This experiment com-pares the performance of Tuned X-HYBRIDJOIN with the other related algo-rithms for different sizes of R . Therefore it is assumed that the size of R varies exponentially while the total memory budget remains fixed (50MB) for all values of R . For each value of R the performance is measured separately. The perfor-mance results of this experiment are shown in Figure 6 (a). From the figure it is clear that for all settings of R the Tuned X-HYBRIDJOIN performs significantly better than other approaches.
 Performance Comparisons for Different Memory Budgets: Second ex-periment analyses the performance of Tuned X-HYBRIDJOIN using different memory budgets while the size of R is fixed (2 million tuples). Figure 6 (b) depicts the performance results. From the figure it can be observed that for all memory budgets the Tuned X-HYBRIDJOIN again performs significantly better than all approaches. This improvement increases gradually as the total memory budget increases. This paper investigates a well known stream-based join algorithm called X-HYBRIDJOIN. Main observation about X-HYBRIDJOIN is that the tuning factor is not considered but it is necessary, particularly when limited memory resources are available to execute the join operation. By omitting the tuning fac-tor, the available memory cannot be distributed optimally among the join com-ponents and consequently the algorithm cannot perform optimally. This paper presents a variant version of X-HYBRIDJOIN called Tuned X-HYBRIDJOIN. The cost model presented in X-HYBRIDJOIN is revised and the proposed algo-rithm is tuned based on that revised cost model. To strengthen the arguments a prototype of Tuned X-HYBRIDJOIN is implemented and the performance with existing approaches is compared.

