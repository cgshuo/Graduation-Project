 Queries that users pose to search engines are often ambiguous -either because different users express different query intents with the same query terms or because the query is underspecified and it is unclear which aspect of a particular query the user is interested in. Search result diversification, whose goal is the creation of a search result ranking cove ring a range of query intents or aspects of a single topic respectively, has been shown in recent years to be an effective strategy to satisfy search engine users in those circumstances. Instead of a single query intent or a limited number of aspect s, search result rankings now cover a set of intents and wide variety of aspects. In 2009, with the introduction of the diversity search task at TREC [1], a large increase in research efforts could be observed, e.g. [2 X 5].

Recent research [6], comparing users X  qu ery behaviour on microblogging plat-forms such as Twitter and the Web has shown that Web search queries are on average longer than Twitter queries. This is not surprising, as each Twitter mes-sage (tweet) is limited to 140 characters and a longer query might remove too many potentially relevant tweets from the result set. Considering the success of diversity in Web search, we believe that it is an even more important technology on microblogging platforms due to the shortness of the queries.

However, to our knowledge, no publicly available microblogging data set (i.e. a corpus and a set of topics with subtopic-based relevance judgments) exists as of yet. In order to further the work on diversity in the microblog setting, we created such a corpus 1 and describe it here.

Specifically, in this paper we make the following contributions: (i) we present a methodology for microblog-based corpus creation, (ii) we create such a corpus, and, (iii) conduct an analysis on its validity for diversity experiments. In the second part of the paper we turn to the question of (iv) how to improve search and retrieval in the diversity setting b y evaluating the recently introduced de-duplication approach to microblogging streams [7]. Users of (Web) serach engines typically employ short keyword-based queries to express their information needs. These queries are often underspecified or ambiguous to some extent [8]. Different users who pose exactly the same query may have very different query intents. In order to satisfy a wide range of users, search results diversification was proposed [9].

On the Web, researchers have been studying the diversification problem mostly based on two considerations: novelty an d facet coverage. To increase novelty, maximizing the marginal relevance while adding documents to the search re-sults [10, 11] has been proposed. Later studies have focused on how to maximize the coverage of different facets [2] of a giv en query. Furthermore, there are works that consider a hybrid solution to combine benefits from both novelty-based and coverage-based approaches [12, 3].

In order to evaluate the effectiveness of s earch result diversification, different evaluation measures have been proposed. A number of them [13 X 16] have been employed at the Diversity Task [1] of the Text REtrieval Conference (TREC), which ran between 2009 and 2012.

Given the difference [6] in querying behavior on the Web and microblogging sites, we hypothesize that the diversification problem is more challenging in the latter case due to the reduced length of th e queries. Tao et al. [7] recently pro-posed a framework to detect (near-)dup licate messages on Twitter and explored its performance as a search result diversification tool on microblogging sites [7]. The approach can be categorized as novelt y-based since it exploits the depen-dency between documents in the initial result ranking. The evaluation though was limited due to the lack of an explicit diversity microblogging corpus (i.e. a corpus with topics and subtopics as well as relevance judgments on the subtopic level). In this paper, we now tackle this very issue. We describe our methodology for the creation of a Twitter-based diversity corpus and investigate its properties. Finally, we also employ Tao et al. X  X  framework [7] and explore its effectiveness on this newly developed data set. We collected tweets from the public Twitter stream between February 1, 2013 and March 31, 2013 -the dates were chosen to coincide with the time interval of the TREC Microblog 2013 track 2 .

After the crawl, in order to create topics, one of this paper X  X  authors ( Anno-tator 2 ) consulted Wikipedia X  X  Current Events Portal 3 for the months February and March 2013 and selected fifty news events. We hypothesized that only topics with enough importance and more than local interests are mentioned here and thus, it is likely that our Twitter strea m does contain some tweets which are per-tinent to these topics. Another advantage of this approach is that we were able to also investigate the importance of time -we picked topics which are evenly distributed across the two-month time span.

Having defined the documents and topics, two decisions need to be made: (i) how to derive the subtopics for each topic, and, (ii) how to create a pool of documents to judge for each topic (and corresponding set of subtopics). Previous benchmarks have developed different approaches for (i): e.g. to derive subtopics post-hoc, i.e. after the pool of documents to judge has been created or to rely on external sources such as query logs to determine the different interpretations and/or aspects of a topic. With respect to (ii), the setup followed by virtually all benchmarks is to create a pool of documents to judge based on the top retrieved documents by the benchmark participants, the idea being that a large set of diverse retrieval systems will retrieve a diverse set of documents for judging.
Since in our work we do not have access to a wide variety of retrieval sys-tems to create the pool, we had to opt for a different approach: one of this paper X  X  authors ( Annotator 1 ) manually created complex Indri 4 queries for each topic topics. We consider this approach a valid alternative to the pool-based approach, as in this way we still retrieve a set of diverse documents. A num-ber of examples are shown in Table 1. The Indri query language allows us to define, among others, synonymous terms within &lt;..&gt; as well as exact phrase matches with #1( ... ) .The # combine operator joins the different concepts iden-tified for retrieval purposes. Since we do not employ stemming or stopwording in our retrieval system, many of the synonyms are spelling variations of a partic-ular concept. The queries were create d with background knowledge, i.e. where necessary, Annotator 1 looked up information about the event to determine a set of diverse terms. The created Indri q ueries are then deployed with the query likelihood retrieval model. Returned are the top 10 , 000 documents (tweets) per query. In a post-processing step we filter out duplicates (tweets that are similar with cosine similarity &gt; 0 . 9 to a tweet higher in the ranking) and then present the top 500 remaining tweets for judging to Annotator 1 and Annotator 2 .After the manual annotation process, the dup licates are injected into the relevance judgments again with the same relevance score and subtopic assignment as the original tweet.

The annotators split the 50 topics among them and manually determined for each of the 500 tweets whether or not they belong to a particular subtopic (and which one). Thus, we did not attempt to identify subtopics beforehand, we created subtopics based on the top retrie ved tweets. Tweets which were relevant to the overall topic, but did not discuss one or more subtopics were considered non-relevant. For example, for the topic Hillary Clinton steps down as United States Secretary of State we determined the first tweet to be relevant for subtopic what may be next for Clinton , while the second tweet is non-relevant as it only discusses the general topic, but no particular subtopic:
Thus, during the annotation process, we focused on the content of the tweet itself, we did not take externally linked Web pages in the relevance decision into account -we believe that this makes our corpus valuable over a longer period of time, as the content behind URLs may change frequently. This decision is in contrast to the TREC 2011 Microblog track, where URLs in tweets were one of the most important indicators for a tweet X  X  relevance [17].

We note, that defining such subtopics is a subjective process -different an-notators are likely to der ive different subtopics for the same topic. However, this is a problem which is inherent to all diversity corpora which were derived by human annotators. In order to show the annotator influence, in the exper-imental section, we not only report the results across all topics, but also on a per-annotator basis.

At the end of the annotation process, we had to drop three topics, as we were not able to identify a sufficient number of subtopics for them. An example of a dropped topic is 2012-13 UEFA Champions League , which mostly resulted in tweets mentioning game dates but little e lse. Thus, overall, we have 47 topics with assigned subtopics that we can use for our diversity retrieval experiments. In this section, we perform a first analysis of the 47 topics and their respective subtopics. Where applicable, we show the overall statistics across all topics, as well as across the topic partitions according to the two annotators.
 The Topics and Subtopics. In Table 2, we list the basic statistics over the number of subtopics identified, while Figure 1 shows concretely for each topic the number of subtopics. On average, we find 9 subtopics per topic. The large standard deviation indicates a strong variation b etween topics with respect to the number of subtopics (also evident in Figure 1). On a per annotator basis we also observe a difference in terms of created subtopics: Annotator 1 has a considerably higher standard deviation than Annotator 2 . This result confirms our earlier statement -subtopic annotation is a very subjective task.

The topics yielding the fewest and most subtopics, respectively, are as follows:  X  Kim Jong-Un orders preparation for strategic rocket strikes on the US main- X  Syrian civil war (21 subtopics)  X  2013 North Korean nuclear test (21 subtopics).

The annotators spent on average 6.6 s econds on each tweet in the annotation process and thus the total annotation effort amounted to 38 hours of annotations.
Apart from a very small number of tw eets, each relevant tweet was assigned to exactly one subtopic -this is not surprising, considering the small size of the documents. The Relevance Judgments. In Figure 2 we present the distribution of relevant and non-relevant documents among the 500 tweets the annotators judged per topic 5 . Twenty-five of the topics have less than 100 relevant documents, while six topics resulted in more than 350 rele vant documents. When considering the documents on the annotator-level, we se e a clear difference between the annota-tors: Annotator 1 judged on average 96 documents as relevant to a topic (and thus 404 documents as non-relevant), while Annotator 2 judged on average 181 documents as relevant.
We also investigated the temporal distribution of the relevant tweets. In Fig-ure 3 we plot for each topic the number of days that have passed between the first and the last relevant tweet in our data set. Since our data set spans a two-month period, we note that a number of topics are active the entire time (e.g. the topics Northern Mali conflict and Syrian civil war ) while others are active froughly 24 hours (e.g. the topics BBC Twitter account hacked and Eiffel Tower, evacuated due to bomb threat ). We thus have a number of short-term topics and a number of long-term topics in our data set. Inn contrast to the TREC Mi-croblog track 2011/12, we do not assign a particular querytime to each topic (therefore we implicitly assume that we query the data set one day after the last day of crawling). We do not consider this a limitation, as a considerable number of topics are covered across weeks. Diversity Difficulty. Lastly, we consider the extent to which the search results can actually be diversified. Diversification does not only depend on the ambi-guity or the underspecification of the query, it is also limited by the amount of diverse content available in the corpus . Golbus et al. [18] recently investigated this issue and proposed the diversity difficulty measure ( dd ) which is a function of two factors: the amount of diversity that a retrieval system can achieve at best and the ease with which a retrieval system can return a diversified result list. Intuitively, a topic has little inherent diversity if the maximum amount of diversity a retrieval system can achieve is small. A topic is considered  X  X ome-what more diverse X  by Golbus et al. in the case where a diverse result list can be achieved but it is difficult for the system to create one. A topic has a large amount of diversity if a retrieval system not tuned for diversity is able to return a diverse result list. These intuitions are formalized in a diversity formula with dd  X  [0 , 1] . A large score ( dd &gt; 0 . 9 ) indicates a diverse query, while a small score ( dd &lt; 0 . 5 ) either indicates a topic with few subtopics or a fair number of subtopics which are unlikely to be discovered by an untuned retrieval system. In Table 3 we present the diversity difficulty average and standard deviation our topics achieve -they are very similar for both annotators and also in line with the diversity difficulty scores of the TREC 2010 Web diversity track [18]. We thus conclude, that in terms of diversity our topic set presents a well constructed data source for diversity experiments.

Finally, we observe that the diversity difficulty score of long-term topics ,that is topics whose first and last relevant tweet cover at least a 50 day timespan, is higher ( dd long-term =0 . 73 ), than the diversity difficulty score of short-term topics (the remaining topics) where dd short-term =0 . 70 . Having analyzed our corpus, we will now exp lore the diversification effectiveness of the recently proposed de-duplication framework for microblogs [7] on this data set. 5.1 Duplicate Detection Strategies on Twitter In [7] it was found that about 20% of search results returned by a standard adhoc search system contain duplicate information. This finding motivated the development of a de-duplication approach which detects duplicates by employ-ing (i) Sy ntactical features, (ii) Se mantic features, and (iii) Co ntextual features in a machine learning framework 6 . By combining these feature sets in different ways, the framework supports mixed strategies named after the prefixes of the feature sets used: Sy , SySe , SyCo and SySeCo . Not surprisingly, the evalua-tion showed that the highest effectivenes s was achieved when all features were combined.

Given an initial ranking of documents (tweets), each document starting at rank two is compared to all higher ranked documents. The duplicate detection framework is run for each document pair a nd if a duplicate is detected, the lower ranked document is filtered out from the result ranking. 5.2 Diversity Evaluation Measures As researchers have been studying the diversification problem intensively on the Web, a number of measures have been prop osed over the years to evaluate the success of IR systems in achieving diversi ty in search results. We evaluate our de-duplication experiments according to the following measures:  X  -(n)DCG [14]. This measure was adopted as the official diversity evaluation Precision-IA [13]. We evaluate the ratio of relevant documents for different Subtopic-Recall [20]. We report the subtopic recall (in short S-Recall )to Redundancy The measure shows the ratio of repeated subtopics among all 5.3 Analysis of De-duplication Strategies We evaluate the different de-duplication s trategies from two perspectives: (i) we compare their effectiveness on all 47 topics, and, (ii) we make side-by-side comparisons between two topic splits, a ccording to the annotator and the tem-poral persistence. This enables us to investigate the annotator influence and the difference in diversity between long-term and short-term topics.
 Apart from the de-duplication strategies we also employ three baselines: the Automatic run is a standard query likelihood based retrieval run (language modeling with Dirichlet smoothing,  X  = 1000 ) as implemented in the Lemur Toolkit for IR. The run Filtered Auto builds on the automatic run by greedily filtering out duplicates by comparing each document in the result list with all documents ranked above it -if it has a cosine similarity above 0 . 9 with any of the higher ranked documents, it is removed from the list. The de-duplication strategies also built on top of the Automatic Run by filtering out documents (though in a more advanced manner). All these runs take as input the adhoc queries (i.e. very short keyword queries) as defined in Table 1.

The only exception to this rule is the Manual run which is actually the run we derived from the manually created complex Indri queries that we used for annotation purposes with cosine-based filtering as defined above.
 Overall comparison. In Table 4 the results for the different strategies averaged over all 47 topics are shown. Underlined is the best performing run for each evaluation measure; statistically significant improvements over the Filtered Auto baseline are marked with  X  (paired t-test, two-sided,  X  =0 . 05 ). The Manual Run -as expected -in general yields the best res ults which are statistically significant in all measures at level @20 .

We find that the de-duplication strategies Sy and SyCo in general outperform the baselines Automatic Run and Filtered Auto , though the improvements are not statistically significant. Not surpisingly, as the de-duplication strategies take Automatic Run as input, Preicision-IA degrades, especially for Precision-IA@20. On the other hand, in terms of lack of redundancy, the de-duplication strategies perform best. De-duplication strategies that exploit semantic features ( SySe and SySeCo ) show a degraded effectiveness, whic h is in stark contrast to the results reported in [7]. We speculate that the main reason for this observation is the recency of our corpus. Semantic feature s are derived from named entities (NE) recognized in the top-ranked tweets and queries. Since in [7] a corpus (documents and topics) from 2011 was used, it is likely that many more NEs were recognized (i.e. those NEs have entered the Linked Open Data cloud) than for our very recent topics. As a concr ete example, the topic Syrian civil war retrieves tweets which contain person names and locations important to the conflict, but they have not been added to standard semantics extraction services such as DBpedia Spolight 7 . Influence of Annotator Subjectivity and Temporal Persistence. In Table 5, the results are shown when splitting the topic set according to the annotators. Here we find that although the absolute scores of the different evaluation measures for Annotator 1 and Annotator 2 are quite different, the general trend is the same for both. The absolute  X  -nDCG scores of the various de-duplication strategies are higher for Annotator 2 than for Annotator 1 , which can be explained by the fact that Annotator 2 , on average, judged more documents to be relevant for a topic than Annotator 1 . The opposite observation holds for the Manual Run , which can be explained by the inability of cosine filtering to reduce redundancy. Given that there are more relevant documents for Annotator 2  X  X  topics, naturally the redundancy problem is more challenging than for Annotator 1  X  X  topics.
Finally, Table 6 shows the results when comparing short-term and long-term queries. For long-term topics, the de-duplication strategies consistently outper-form the baselines, while the same cannot be said about the short-term topics. We hypothesize that short-term topics do not yield a large variation in vocabu-lary (often a published news report is rep eated in only slightly different terms) so that features which go beyond simple term matching do not yield significant benefits. Long-term topics on the other hand develop a richer vocabulary dur-ing the discourse (or the course of the event) and thus more complex syntactic features can actually help.
 In this paper, we presented our efforts to create a microblog-based corpus for search result diversification experiments. A comprehensive analysis of the corpus showed its suitability for this purpose. The analysis of the annototators X  influence on subtopic creation and relevance judgme nts revealed considerable subjectivity in the annotation process. At the same time though, the de-duplication retrieval experiments showed that the observed trends with respect to the different eval-uation measures were largely independent of the specific annotator.

The performance of the de-duplication s trategies and their comparison to the results reported in [7] indicate the importance of the feature suitability for the topic type (long-ter m vs. short-term topics and topic recency).

In future work we plan to further analyze the impact of the different strate-gies and the annotator subjectivity. We will also implement and evaluate the de-duplication strategy with diversification approaches which have been shown to perform well in the Web search setting, e.g. [4, 5]. Furthermore, we will in-vestigate the potential sources (influences and/or motivations) for the observed annotator d ifferences.
