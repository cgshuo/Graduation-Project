 when the dataset is large and cannot fit into the main mamory. Consequently, time with less memory.
 Binary hashing is an approximate method for NNS which meets the time and as an with same binary code a bucket . The NNs of a novel query are approximately original datum, hashing enables NNS of a large amount of data on memory. codes. This approach is called learning to hash .
 into R r by dimensionality reduction, and then binarized to SH are also designed to convert neighboring samples to similar binary codes. contains a large number of samples. To verify this statement, we show a toy example of one dimensional artificial data illustrated in Fig. 1 (a). dinate of data, and y axis is each element of data embedded into divide any buckets, which causes each bucket to be large. samples in a same bucket. Therefore, large buckets degrade the precision of NNS. For instance, in the above toy example, the third bit does not improve may potentially have the same inefficiency as SH.
 In this paper, we propose Locally Optimized Hashing (LOH), formulated as an optimization problem based on that of SH with stricter constraints which matrix [ 12 ] or Anchor Graph as same as Anchor Graph Hashing [ 6 ]. hash learning methods and explain the formulation of SH and its extensions. experimental results. In Sect. 5 , we make conclusion. Hashing methods proposed in early stage of study such as Locality Sensitive on the given data. These randomized methods are asymptotically guaranteed to be precise, which though require very long binary codes to be usable. Binary codes optimized to data can be compact and accurate. Therefore, hash of such methods convert neighboring samples in the dataset to similar binary learn hash functions using pairwise  X  X ear X  and  X  X ar X  labels. Spectral Hashing [ 11 ] (SH) is one of the hash learning methods. Here we describe its formulation and extensions related to our methods. Let be a similarity matrix of n data points x (1) ,...,x ( n ) to y to1and  X  1, respectively. We denote entire embedding of n ( y ,...,y The binary codes of SH are obtained by solving the problem min to penalize a pair of neighboring samples whose corresponding codes are far apart, which makes the embedding smooth. Balance constraints imize the information of each bit and off-diagonal elements of The constraints Y Y = nI also contain normalization constraints without which the optimum degenerates to 0. The global optimum of ( 1 )is Y = L whose corresponding eigenvalues are 0 =  X  0  X   X  1  X   X  X  X   X   X  angle whose axes are aligned to the principal components of the data, which called Self-Taught Hashing (STH) to obtain hash functions, where Vector Machines are learned to obtain r hash functions using each bit of as a supervisor. We use this approach in our method.
 matrix can be efficiently computed by Lanczos method with implicit restart anchor similarity matrix Z  X  R n  X  m is computed as
Z where each row of Z is normalized to be a stochastic vector. much smaller than m . The similarity matrix is formulated as  X  = diag( Z 1 ) is the column-wise normalizer of Z . The rank of which enables us to efficiently compute its eigenvectors. lem. We also introduce an algorithm to solve our problem. 3. 1 Buckets Partitioning Constraints We start by analyzing a simple toy example of eight samples indexed by 1 with three-bit codes. Suppose the first two bits are already fixed as Eight samples are currently divided into four buckets, { 1 { 7 , 8 } enough to divide these buckets; e.g. the third bit (1 , 1 (1 ,  X  1 strain y 3 to explicitly divide each bucket. It means y 3 vectors of { y 1 ,y 2 } . We define bucket matrix of { y 1 written as length r and data size n . Suppose y 1 ,...,y r  X  1 are fixed, and let B the constraints B y r =0forcethe r -th bit to partition current buckets. 3.2 Combinatorial Orthogonality Constraints be seen as an extension of the balance and orghotonality constraints ( 1 ). Here we start by above toy example again. The whole constraints are 1 1 y = y 1 y 2 = B y 3 = 0. Note that the subspace spanned by { y } equals to the image of B , where is the elementwise product operator. Therefore, the constraints can be rewritten as where P ( S ) is the power set of a set S and [ r ]= { 1 ,...,r } constraints are now expressed in symmetric form of y 1 ,y that y Theorem 1 . Suppose r  X  2 , 1 y 1 =0 and y 1 ,...,y r  X  X  1 are orthogonal to bucket vectors of { y 1 ,...,y j  X  1 } for all Constraints, COC ). Proof. We prove it by induction. The case of r = 2 is trivial. Suppose Buckets partitioning constraints of y 2 ,...,y r  X  1 and COC of { y denote the corresponding bucket vector by 1 S . Then, 1 S combination of elementwise products of nonempty subsets of We can see it by observing that the bucket S is a subset of the bucket { y 1 are equivalent to COC. Note that the balance constraints 1 orthogonality constraints y j y k = 0 are special cases of COC with respectively. COC thus completely contains these constraints used in SH. Here L is either the graph Laplacian D  X  A or its normalized version relaxation and binarize the solution as same as SH. 3.3 Sequential Buckets Partitioning Algorithm indicates highly nonconvex manifold in the space of Y where at most the following problem for each y = y k ,k  X  X  1 ,...,r } . where B is the bucket matrix of { y 1 ,...,y k  X  1 } . In the case The problem ( 6 ) is transformed into an eigenproblem as follows. Let the largest eigenvalue of L and let  X  A =  X I  X  L . Note that Since y Ly =  X  y 2  X  y  X  Ay =  X n  X  y  X  Ay , minimizing y Ly maximizing y  X  Ay . One can transform it using the constraints Note that I  X  BB is the projection matrix onto Ker B =(Im B B to the following problem. Here the buckets partitioning constraints are suppressed, since them cannot be optimal due to the projection matrix I  X  BB function. The global optimum of ( 7 ) is the first eigenvector of ( BB ). It is efficiently computed by Lanczos method. Alternatively, when one uses the low rank similarity matrix A = Z X   X  1 Z as same as AGH, the optimal solution y is also calculated using the top eigenvector v as y = Here we used the fact that I  X  BB is idempotent, i.e. ( I  X  BB since it is a projection matrix as we noted before. 3.4 Slow Partitioning for Singletons Problem erate subsequent bits of singletons to zero; if a singleton k -bit code is generated, then the corresponding constraint of the next bit is = 0. Once there appear too many singletons, the search precision can-not improve with longer codes anymore. We therefore introduce a method which the buckets. This idea induces us to an intermediate method of SH and strict ( Such vectors for each step are the optimum of the following problem. algorithm for the optimization problem with a subset of COC which is still more redundant and globally better codes than strict partitioning, while the codes are still locally optimized compared to those of the existing methods. 3.5 Computational Cost LOH requires computation of graph Laplacian L with its top eigenvalue times computation of eigenvectors of ( I  X  BB )  X  A ( I  X  BB ( where the cost is reduced to linear time. Hence, LOH runs in linear time of costs O ( n 2 ) time, which is dominant in the whole procedure. We conducted experiments on image and document datasets. In this section, we mental settings, and show the results. 4. 1 Objective of Experiments We design LOH by focusing on dividing buckets to be smaller than those of respectively. We compare LOH with STH (Self-Taught Hashing), AGH (Anchor extensions to SH. 4.2 Experimental Settings We used two similarity graphs in our method, undirected Anchor Graph. We call these variants LOH and ALOH (Anchor LOH), respec-tively. In our method and STH, we used Support Vector Machines with linear mentation of linear kernel SVM. The raw samples x (1) ,...,x training vectors for SVM in LOH and STH, while we used Z in ALOH. In other words, we used sample-anchor similarities as a feature vector in ALOH. The experiment is conducted as follows for each method and dataset. We Note that these binary codes may be different from binarized We use two evaluation measures: mean number of samples in each bucket hamming distance of binary codes as the search precision. 4.3 Datasets and Detailed Settings We conducted experiments on two datasets, MNIST and RCV 1 , which are MNIST 1 is a dataset consisting of 70,000 handwritten digit images. Each pixels, each of which is normalized into the interval [0 , methods. In LOH and STH, we use the weighted k -NN graph of similarity matrix. In ALOH and AGHs, we use m = 500 anchors chosen by 10 iterations of k -means, and s = 5 as the sparsity parameter of Anchor Graph. RCV 1 is Reuters newswire dataset [ 4 ]. Each document is represented by used as training data and 15,564 samples as queries. We use the heat kernel methods. Hellinger distance, defined by ALOH and AGHs, we use m = 800 anchors chosen by 10 iterations of and s = 10 as the sparsity parameter of Anchor Graph. 4.4 Experimental Results q ALOH of q = 1 and STH achieve small buckets in short code lengths. Note that small buckets are meaningful only when the precision is high, which we become singletons at r = 16 within the algorithm. Since y the resulting mean bucket size is not 1. On the other hand, ALOH of slowly improves the bucket size, which is same or better than that of AGHs. codes of high precision. In particular, its precision at r that of AGH at r = 44. On the other hand, ALOH of q = 16 slowly improves the precision, which overtakes that of q = 1 at the code length the samples of MNIST are dense vectors, LOH and STH fails to correctly learn This result is similar to that on MNIST . Here LOH of q = 16 produces small buckets with short codes. The NNS precision is shown at Fig. 3 (b). LOH and ALOH of Finally, we show the NNS precision of various redundancy parameter shown, since they are superior to LOH on the dataset. On the other hand, we precision with smaller q is a bit better at small r than that of larger We showed that existing hash learning methods have a problem that they pro-small buckets, which are transformed into symmetric form named combinatorial orthogonality constraints (COC). The optimization problem with COC is solved each of resulting buckets may contain only one sample, which causes the sub-slow partitioning, which we saw as an intermediate method between COC and that our Locally Optimized Hashing method and its variant using Anchor Graph ods on both image and document datasets. We would like to find more detailed embedding in the future.

