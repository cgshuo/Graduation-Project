 Sunil Aryal 1( Using the bag-of-words [ 1 ] vector representation, a document d n documents ( i =1 , 2 ,  X  X  X  ,n ) is represented by a r -dimensional vector (where r is the number of terms in the dictionary), i.e., d i = d i 1 entry d ij represents the occurrence frequency of term t j used cosine distance [ 1 ] estimates the dissimilarity of two vectors using their geometric positions only, it is important to adjust their positions in the space according to the importance of their terms. Two types of term weighting factors are used in the literature to estimate the importance of term t d ( w ij )[ 2 ]. First, the term frequency (tf) based factor of t estimated in different ways: (a) Binary ( bin tf ): tf ij otherwise; (b) Raw term frequency ( Raw tf ): tf ij = d ij weighting factor of a term t j ( idf j ) is estimated using the number of documents in the given collection having term t j ( n j )as idf j = log represented as having component w ij = tf ij  X  idf j . The dissimilarity between document vectors d 1 and d 2 using the cosine distance is estimated as follows: the traditional 2 -norm in text retrieval. The only difference between the cosine distance and 2 -norm is that it uses the length normalized vector which is referred as cosine normalization in the literature.
 lowing three monotonic assumptions [ 3 ]: (i) Multiple appearances of a term in a document are no less important than single appearance (the tf assumption). (ii) Rare terms are no less important than frequent terms (the idf assumption). (iii) For the same quantity of term matching, long documents are no more important than short documents (the cosine normalization assumption).
 biases toward smaller documents, documents with infrequent terms and doc-uments with multiple occurrences of terms which can be disadvantageous in some cases [ 4 ]. The cosine distance with term weighted document vectors may perform well in some data sets or domains where the above assumptions hold but it may perform worse in other data sets where the assumptions do not hold (see the experimental results in Sect. 3 ).
 the documents d 1 -d 7 using the cosine distance with raw tf -idf term weighting is provided in the fourth column. Even though d 4 and d 5 rences of the common term t 2 with d q , d 4 is considered to be more similar to d than d 5 for no particular reason because of the idf assumption ( d more due to the mismatch in infrequent term t 3 ). Similarly, d be more similar to d q than d 1 because of the cosine normalization assumption ( d is shorter than d 1 ). Even though d 6 has the same occurrences of the common term t 2 with d q , d 7 is considered to be more similar to d tf assumption ( d 7 has more occurrences of t 2 ).
 Furthermore, in order to use the tf based weights such as raw tf and log tf , the frequency of each term in each document is required. However, in some appli-cation domains such as legal and medical, it may not be possible to have the exact term frequencies due to privacy issue because it is possible to infer infor-mation in the document from its term frequencies [ 5 ]. Hence, in some domains, only binary representation of documents is available rather than their raw term frequencies.
 In this paper, we investigate a dissimilarity measure that does not require an adjustment of bag-of-words vectors and demonstrate that the recently pro-posed data-dependent dissimilarity measure called m p -dissimilarity [ 6 ]isone such measure. It uses a similar statistic as used in idf j measure of dissimilarity directly rather than for vector adjustment in the space. Our empirical evaluation shows that m p -dissimilarity with the simplest binary representation performs either better than or competitively to the cosine dis-tance with different term weighting schemes in document retrieval tasks. Its performance is more consistent across different data sets than that of the cosine distance with any term weighting scheme. In order to measure dissimilarity between two r -dimensional data points x and y , rather than just relying on the positions of x and y in the space, m [ 6 ](werefertoitas m p hereafter) considers the probability data mass in the dissimilarity as follows [ 6 ]: where | R j ( x, y ) | is the number of data points falling in R number of data points; and p&gt; 0 is a parameter.
 Simply by replacing the geometric distance in each dimension by the prob-ability mass in the range, m p has been shown to provide more reliable nearest neighbours than p -norm in high dimensional spaces [ 6 ]. But, it is very expensive to compute as it requires a range search to determine how many instances fall in each R j ( x, y ). Using a binary search tree, one-dimensional range search can be done in O (log n ) resulting in the run-time complexity of O ( r log n )tomeasure dissimilarity of a pair of vectors.
 In a document collection, only a few terms in the dictionary appear in each document. Many terms do not appear in either of the two documents given for dissimilarity measurement. Since the absence of a term in both the documents does not provide any information about (dis)similarity of documents, those terms should be ignored. Hence, we make a simple modification in the formulation of m p showninEq. 2 by considering only those terms that occur in either of the two documents as follows: where | T 1 , 2 | = | T 1  X  T 2 | ( T i is the set of terms that appear in d ization term employed to account for different numbers of terms used for any two documents.
 tor d i has only two values { 1,0 } indicating whether the term t uments in the collection ( n ) and the number of documents where t as follows: not measure dissimilarity of d 1 and d 2 w.r.t a term which does not appear in both d 1 and d 2 . n j can be precomputed for all t j as a pre-processing; thus, |
R m p -dissimilarity of a pair of documents using Eq. 3 which is equivalent to that of the cosine distance. The pre-processing to compute n j for all t time and O ( r ) space complexities. Note that the same complexities are required in computing idf factor.
 space because it is not using the absolute positions of two vectors in the dissimi-larity measure. It estimates dissimilarity w.r.t each term t the documents based on the distribution of documents (i.e., high dissimilarity if t j appears in many documents, and low dissimilarity if it appears only in a few documents) and assigns maximal dissimilarity of 1 w.r.t terms that appear in only one of them. Even though a similar statistic as in idf based weighting is used in the case of matching term, it is not used to transform vectors in the space but it is used as a measure of (dis)similarity between two documents w.r.t. t j directly. In the example shown in Table 1 , m p m p ( d q ,d 1 )= m p ( d q ,d 2 )and m p ( d q ,d 6 )= m p In this section, we present the empirical results of m p (using the binary represen-tation) and the cosine distance with the six different term weighting schemes as discussed in Sect. 1 ( bin tf , raw tf and log tf with and without idf ) in document retrieval tasks. Since we want to capture the contrast between two documents with low dissimilarity in a few common terms and maximal dissimilarity w.r.t many terms that appear in either one of them, p&lt; 1 is preferred to amplify the effect of low dissimilarities in the average. Hence, we used p =0 . 1for m m ) in our experiments 1 .
 We used 4 different data sets from 4 benchmark document collections that are used in the text mining literature. The data characteristics are provided in Table 2 .NG20 2 is the widely used 20 Newsgroup data set and R52 (See footnote 2) is a subset of the another widely used Reuters document collection [ 7 ]. Ohscal is a data set from Ohsumed patients X  medical information collection and Wap (See footnote 3) is a collection of web pages from Yahoo [ 8 ].
 Given a query document d q , documents in a data set were ranked in ascend-ing order of their distance/dissimilarity to d q ; and the first k documents were presented as the relevant documents. For performance evaluation, a document was considered to be relevant to d q if they have the same category label. A good retrieval system returns relevant documents at the top. Hence, the precision in the top 10 (P@10) retrieved documents was used as the performance measure. The same process was repeated for every document in a data set as a query and the rest of the documents were ranked. The average P@10 over n (the num-ber of documents in a collection) queries of m 0 . 1 and cosine with six different term weighting schemes are provided in Table 3 . Note that all the differences are statistically significant as they are averaged over n ( standard error is negligible (up to two decimal places) in each case. The performance of m 0 . 1 was more consistent than the cosine distance with any term weighting scheme across four data sets (see the average result in the last column in Table 3 ). It was among the top three performers in each data set (and has the best performance in Wap and Ohscal, the second best in R52 and the third best in NG20) whereas none of the term weighting schemes were among the top three performers in all data sets. in good performance as it produced poor results in R52 and Ohscal with any of the three tf representations. Even though log tf produced the best result in R52 and Ohscal, it did not produce the top three results in the other two data sets. Similarly, log tf -idf produced the best result in NG20 and the third best in Wap but did not produce the top three results in Ohscal and R52. Cosine with bin tf -idf was among the top three performers in two data sets whereas bin tf and raw tf in one data set and raw tf -idf did not produce the top three results in any data set. Since the cosine distance measures (dis)similarity solely based on the positions of two vectors, it is important to adjust the positions of document vectors in the space w.r.t the importance of the terms in those documents. In the literature, many term weighting schemes are proposed using the tf and idf factors based on some assumptions. Though these methods perform well in some data sets when the assumptions hold, they could perform poorly when the assumptions do not hold.
 improve the performance of the cosine distance, this paper opens a different avenue for research by investigating an alternative dissimilarity measure that does not require the adjustment of document vectors using a term weight-ing scheme. We show that a  X  X ata-dependent X  dissimilarity measure called m a pair of documents based on the distribution of documents in each term. with the binary bag-of-words representation produces either better or competi-tive results in comparison to the cosine distance with the state-of-the-art term weighting schemes.
