 Kernel Principal Component Analysis (KPCA) is a key ma-chine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimen-sional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communica-tion efficient fashion while maintaining provable and strong guarantees in solution quality?
In this paper, we give an affirmative answer to the ques-tion by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algo-rithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algo-rithm can take as input an arbitrary configuration of dis-tributed datasets, and compute a set of global kernel princi-pal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing k principal compo-nents with relative error over s workers has communica-tion cost  X  O ( s X k/ + sk 2 / 3 ) words, where  X  is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets. The experimental results showed that the algo-rithm produces a high quality kernel PCA solution while using significantly less communication than alternative ap-proaches.
 Kernel method; Principal Component Analysis; distributed computing The authors are listed in alphabetical order.

Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from complex datasets, such as image, text, healthcare and biological data [27, 26, 28]. The original kernel PCA algo-rithm is designed for a batch setting, where all data points need to fit into a single machine. However, nowadays large volumes of data are being collected increasingly in a dis-tributed fashion, which poses new challenges for running ker-nel PCA. For instance, a large network of distributed sensors can collect temperature readings from geographically distant locations; a system of distributed data centers in an Internet company can process user queries from different countries; a fraud detection system in a bank needs to perform credit checks on people opening accounts from different branches; and a network of electronic healthcare systems can store patient records from different hospitals. It is very costly in terms of network bandwidth and transmission delays to com-municate all of the data collected in a distributed fashion to a single data center, and then run kernel PCA on the cen-tral node. In other words, communication now becomes the bottleneck to the nonlinear feature extraction pipeline. How can we leverage the aggregated computing power in a large distributed system? Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality?
While recent work shows how to do linear PCA in a com-munication efficient and distributed fashion [8], the kernel setting is significantly more challenging. The main problem with previous work is that it achieves communication pro-portional to the dimension of the data points, which if im-plemented straightforwardly in the kernel setting would give communication proportional to the dimension of the feature space which can be very large or even infinite. Kernel PCA uses the kernel trick to avoid going to the potentially infinite dimensional kernel feature space explicitly, so intermediate results are often represented by a function ( e.g. , a weighted combination) of the feature mapping of some data points. Communicating such intermediate results requires commu-nicating all the data points they depend on. To lower the communication, the intermediate results should only depend on a small number of data points. A distributed algorithm then needs to be carefully designed to meet this constraint.
In this paper, we propose a communication efficient al-gorithm for distributed KPCA in a master-worker setting where the dataset is arbitrarily partitioned and each portion sits in one worker, and the workers can communicate only through the master. Our key idea is to design a communica-tion efficient way of generating a small representative subset of the data, and then performing kernel PCA based on this subset. We show that the algorithm can compute a rank-k subspace in the kernel feature space using just a represen-tative subset of size O ( k/ ) built in a distributed fashion. For polynomial kernels, it achieves a (1 + ) relative-error approximation to the best rank-k subspace, and for shift-invariant kernels (such as the Gaussian kernel), it achieves (1 + )-approximation with an additive error term that can be made arbitrarily small. In both cases, the total com-munication for a system of s workers is  X  O ( s X k/ + sk 2 words, where  X  is the average number of nonzero entries in each data point, and is always bounded by the dimension of the data d and independent of the dimension of the kernel feature space. This for constant nearly matches the lower bound  X ( sdk ) for linear PCA [8]. As far as we know, this is the first algorithm that can achieve provable approximation with such communication bounds.

As a subroutine of our algorithm, we have also developed an algorithm for the distributed Column Subset Selection (CSS) problem, which can select a set of O ( k/ ) points whose span contains (1 + )-approximation, with communication O ( s X k/ + sk 2 ). This is the first algorithm that addresses the problem for kernels, and it nearly matches the commu-nication lower bound  X ( s X k/ ) for this problem in the linear case [10]. The column subset selection problem has various applications in big data scenarios, so this result could be of independent interest.

Furthermore, our algorithm also leads to some other dis-tributed kernel algorithms: the data can then be projected onto the subspace found and processed by downstream ap-plications. For example, an immediate application is for distributed spectral clustering, that first computes KPCA to rank-k/ and then does k -means on the data projected on the subspace found by KPCA ( e.g. , [17]). This can be done by combining our algorithm with any efficient distributed k -means algorithms ( e.g. , [6]).

We evaluate our algorithm on datasets with millions of data points and hundreds of thousands of dimensions where non-distributed algorithms such as batch KPCA are imprac-tical to run. Furthermore, comparing to other distributed algorithms, our algorithm requires less communication and fewer representation data points to achieve the same approx-imation error.
There has been a surge of recent work on distributed ma-chine learning, e.g. , [5, 30, 20, 6]. In this setting, the data sets are typically large, and small error rate is required. This is because if only a coarse error is needed then there is no need to use large-scale data sets; a small subset of the data will be sufficient. Furthermore, one prefers relative error rates instead of additive error rates, since the latter is worse and harder to interpret without knowing the optimum. Our algorithm can achieve small relative error with limited com-munication.

Since there exist communication efficient distributed lin-ear PCA algorithms [6, 20], it is tempting to adopt the ran-dom feature approach for distributed kernel PCA: first con-struct m random features and then solve PCA in the primal form, i.e. , apply distributed linear PCA on the random fea-tures. However, the communication of this method is too high. One needs m =  X  O ( d/ 2 ) random features to preserve the kernel values up to additive error , leading to a com-munication of O ( skm/ ) = O ( skd/ 3 ). Another drawback of using random features is that it only produces a solu-tion in the space spanned by the random features, but not a solution in the feature space of the kernel.

The Nystr  X  om method is another popular tool for large-scale kernel methods: sample a subset of data points uni-formly at random, and use them to construct an approxi-mation of the original kernel matrix. However, it also suf-fers from high communication cost, since one needs O (1 / sampled points to achieve additive error in the Frobenius norm of the kernel matrix [21]. A closely related method is incomplete Cholesky decomposition [3], where a few pivots are greedily chosen to approximate the kernel matrix. It is unclear how to design a communication efficient distributed version since it requires as many rounds of communication as the number of pivots, which is costly.

Leverage score sampling is a related technique for low-rank approximation [29]. A prior work of Boutsidis et al. [8] gives the first distributed protocol for column subset se-lection. [11] gives a distributed PCA algorithm with optimal communication cost, but only for linear PCA. In compari-son, our work is the first communication efficient distributed algorithm for low rank approximation in the kernel space.
For any vector v , let k v k denote its Euclidean norm. For any matrix M  X  R d  X  n , let M i : denote its i -th row and M : j its j -th column. Let k M k F denote its Frobenius norm, and k M k 2 denote its spectral norm. Let its rank be r  X  min { n,d } , and denote its SVD as M = U  X  V &gt; where U  X  R d  X  r ,  X   X  R r  X  r , and V  X  R n  X  r . Let [ M ] k denote its best rank-k approximation. Finally, denote its number of non-zero entries as nnz( M ).

In the distributed setting, there are s workers that are connected to a master processor. Worker i has a local data concatenation of the local data ( n = P s i =1 n i ).
Kernels and Random Features. For a kernel  X  ( x,x 0 ), let H denote its feature space, i.e., there exists a feature mapping  X  (  X  )  X  H such that  X  ( x,x 0 ) =  X   X  ( x ) , X  ( x  X  ( A )  X  H n denote the matrix obtained by applying  X  on each column of A and concatenating the results. Through-out the paper, we regard any M  X  H n as a matrix whose columns are elements in H and define matrix operations ac-cordingly. For example, for any M  X  H n and N  X  H m , k M k 2 H = tr M &gt; M . When there is no ambiguity, we omit the subscript H .

The random feature approach is a recent technique to scale up kernel methods. Many kernels can be approx-imated by 1 m P m i =1  X   X  i ( x )  X   X  i ( y ) where  X  i sampled. These include Gaussian RBF kernels and other shift-invariant kernels, inner product kernels, etc ([24, 15]). For example, Gaussian RBF kernels,  X  ( x,y ) = exp(  X  X  x  X  y k 2 / 2  X  2 ), can be approximated by 1 where z  X ,b ( x ) = distribution with density proportional to exp(  X   X  2 k  X  k and b i is uniform over [0 , 2  X  ].

In this paper, we provide guarantees for shift-invariant kernels using Fourier random features (the extension to other kernels/random features is straightforward). We assume the kernel satisfies some regularization conditions: it is defined over bounded compact domain in R d , with  X  (0)  X  1 and bounded  X  2 k (0) [24]. Such conditions are standard in prac-tice, and thus we assume them throughout the paper.
Kernel PCA. An element u  X  H is an eigenfunction of  X  ( A )  X  ( A ) &gt; with the corresponding eigenvalue  X  if k u k = 1 and  X  ( A )  X  ( A ) &gt; u =  X u . Given eigenfunctions { u  X  ( A )  X  ( A ) &gt; and eigenvectors { v i } of  X  ( A ) &gt; the singular decomposition U  X  k V &gt; + U  X   X   X  V &gt;  X  V are the lists of top k eigenfunctions/vectors,  X  k is a diag-onal matrix with the corresponding singular values, U  X  , V are the lists of the rest of the eigenfunctions/vectors, and  X   X  is a diagonal matrix with the rest of the singular values. Kernel PCA aims to identify the top k subspace U , since the best rank-k approximation [  X  ( A )] k = U  X  k V &gt; = UU Typically, the goal is to find a good approximation to this subspace. Formally,
Definition 1. A subspace L  X  X  k is a rank-k (1 + ,  X ) -approximation for kernel PCA on A if L &gt; L = I k and Kernel PCA leads to immediate solutions for some other nonlinear component analysis (e.g., kernel CCA), and pro-vides needed subroutines for tasks like spectral clustering.
Subspace Embeddings. Subspace embeddings are a useful technique that can improve the computational and space costs by embedding data into lower dimension while preserving interesting properties. They have been exten-sively studied in recent years [25, 1, 13]. The recent fast sparse subspace embeddings [13] and its optimizations [22, 23] are particularly suitable for large-scale sparse datasets, since their running time is linear in the number of non-zero entries in the data matrix. They also preserve the sparsity of the input data. Formally,
Definition 2. An -subspace embedding of M  X  R m  X  n is a matrix S  X  R t  X  m such that for any x , Subspace embeddings can also be done on the right hand side, i.e. , S  X  R n  X  t and x &gt; MS = (1  X  ) x &gt; M .
Mx is in the column space of M and SMx is its em-bedding, so the definition means that the norm of any vec-tor in the column space of M is approximately preserved. This then provides a way to do dimensional reduction for problems depending on inner products of vectors. Our al-gorithm repeatedly makes use of subspace embeddings. In particular, the embedding we use is the concatenation of the following known sketching matrices: CountSketch and i.i.d. Gaussians (or the concatenation of CountSketch , fast Hadamard and i.i.d. Gaussians). The details can be found in [29]; we only need the following fact.

Lemma 1. For M  X  R d  X  n , there exist sketching matri-ces S  X  R t  X  d with t = O ( n/ 2 ) that are -subspace embed-dings. Furthermore, SM can be successfully computed in time  X  O (nnz( M )) with probability at least 1  X   X  . The work of [2] shows that a fast computational approach, TensorSketch , is indeed a subspace embedding for the polynomial kernel. However, there are no previously known subspace embeddings for other kernels. We develop efficient and provable embeddings for a large family of kernels includ-ing Gaussian kernel and other shift invariant kernels. These embeddings will be a key tool used by our algorithm.
In view of the limitations of the related work, we instead take a different approach, which first selects a small subset of points whose span contains an approximation with rela-tive error rate , and then find a low rank approximation in their span. It is important to keep the size of the subset small and also guarantee that their span contains a good approximation (this is also called kernel column subset se-lection). A well known technique is to sample according to the statistical leverage scores.

Challenges. However, this immediately raises the fol-lowing technical challenges.

I. Computing the statistical leverage scores is prohibitively expensive. Na  X   X vely computing them requires communicating all data points. There exist non-trivial fast algorithms [18], but they are designed for the non-distributed setting. Using them in the distributed setting leads to communication lin-ear in the number of data points, or linear in the number of random features if one uses random features and computes the leverage scores for them.
 Our key idea is that it is sufficient to compute the (gener-alized) leverage scores of the data points, i.e., the leverage scores of another matrix whose row space approximates that of the original data matrix. So the problem is reduced to designing kernel subspace embeddings that can approximate the row space of the data.

II. Even given the embedded data, it is unclear how to compute its leverage scores in a communication efficient way. Although the dimension of the embedded data is small, ex-isting algorithms will lead to communication linear in the number of data points, which is impractical.

III. Simply sampling according to the generalized leverage scores does not give the desired results: a good approxima-tion can only be obtained using a much larger rank, specifi-cally, O ( k/ ).

IV. After selecting the small subset of points, we need to design a distributed algorithm to compute a good low rank approximation in their span.

Algorithm. We have designed a distributed kernel PCA algorithm that computes an approximated solution with rel-ative error rate using low communication. The algorithm operates in following key steps, each of which addresses one of the challenges mentioned above (See Figure 1):
I. Kernel Subspace Embeddings. To approximate the sub-space of the original data matrix, we propose subspace em-beddings for a large family of kernels. For polynomial ker-nels we improve the prior work by reducing the embedding dimension and thus lowering the communication. Further-more, we propose new subspace embeddings for kernels with random feature expansions, allowing PCA for these kernels to be computed in a communication efficient manner. See Section 5.1 for the details.

II. Distributed Leverage Scores. To compute the leverage scores, sampling with constant approximations is sufficient. We can thus drastically reduce the number of data points: first do another (non-kernel) subspace embeddings on the embedded data, and then send the result to the master for computing the scores. See Figure 1(a) for an illustration and Section 5.2 for the details.

III. Sampling Representative Points. We take a two-step approach as leverage scores alone is not good enough : first sample according to generalized leverage scores, and then sample additional points according to their distances to the span of the points sampled in the first step. The first step gains some coarse information about the data, and the sec-ond step use it to get the desired samples. The two steps are illustrated in Figure 1(b) and 1(c), respectively, while the details are in Section 5.3.

IV. Computing an Approximation. After projecting the data to the span of the representative points, we sketch the projections by (non-kernel) subspace embeddings. We then send the compressed projections to the master and compute the solution there. See Figure 1(d) for an illustration and Section 5.4 for the details.

Main Theoretical Results. Given as input the local datasets, the rank k and error parameters ,  X , our algo-rithm outputs a (1 + ,  X )-approximation to the optimum with large probability. Formally, we have the following the-orem.

Theorem 1. Algorithm 4 produces a subspace L for ker-nel PCA on A that with probability  X  0 . 99 satisfies: 1. L is a rank-k (1 + , 0) -approximation when applied to 2. L is a rank-k (1 + ,  X ) -approximation when applied to The total communication is  X  O ( s X k + sk 2 3 ) words, where  X  is the average number of nonzero entries in one data point. The constant success probability can be boosted up to any high probability 1  X   X  by repetition, which adds only an extra O (log 1  X  ) term to communication and computation.
The output subspace L is represented by  X  O ( k/ ) sampled points Y from A ( i.e. , L =  X  ( Y ) C for some coefficient ma-trix C ), so L can be easily communicated and the projec-tion of any point on L can be easily computed by the kernel trick. The communication has linear dependence on the di-mension and the number of workers, and has no dependence on the number of data points, which is crucial for big data scenarios. Moreover, it does not depend on  X  (but the com-putation does), so the additive error can be made arbitrarily small with more computation.

The theorem also holds for other properly regularized ker-nels with random feature expansions (see [24, 15] for more such kernels); the extension of our proof is straightforward.
We also make the following contributions: ( i ) Subspace embedding techniques for many kernels. ( ii ) Distributed al-gorithm for computing generalized leverage scores with low communication. ( iii ) Distributed algorithm for kernel col-umn subset selection.
Our algorithm first computes the (generalized) leverage scores that measure the non-uniform structure, then samples a subset of points whose span contains a good approximated solution, and finally finds such a solution in the span.
Leverage scores are critical for importance sampling in many fast randomized algorithms. The leverage scores are defined as follows.

Definition 3. For E  X  R t  X  n with SVD E = U  X  V &gt; , the leverage score ` j for its j -th column is ` j = k V j : k Their importance is reflected in the following fact: suppose E has rank at most k , and suppose P is a subset of O ( k log k columns obtained by repeatedly sampled from the columns of E according to their leverage scores, then the span of P contains an (1 + , 0)-approximation subspace for E with probability  X  0 . 99 (see, e.g. , [19]). Here, sampling one col-umn according to the leverage scores ` j means to define sam-pling probabilities p j such that p j  X  ` j 4 P then pick one column where the j -th column is picked with probability p j . Note that setting p j = ` j P cient, but a constant variance of p j is allowed at the expense of an extra constant factor in the sample size. This means that it is sufficient to compute constant approximations  X  for ` j , and then sample according to p j =  X  ` j P
However, even computing constant approximations of the leverage scores are non-trivial: na  X   X ve approaches require SVD, which is expensive. Actually, SVD is more expensive than the task of PCA itself. Even ignoring computation cost, na  X   X ve SVD is prohibitive in the distributed setting due to its high communication cost. Fortunately, it turns out that the leverage scores are an over kill for our purpose; it suffices to compute the generalized leverage scores, i.e., the leverage scores of a proxy matrix.

Definition 4. If E has rank q and can approximate the row space of M up to (1 + ,  X ) , i.e. , there exists X with then the leverage scores of E are called the generalized lever-age scores of M with respect to rank q .

This generalizes the definition in [18] by allowing the rank of E to be larger than k and allowing additive error  X , which are important for our application. The generalized leverage scores can act as the leverage scores for our purpose in the following sense.

Lemma 2. Let P be O ( q log q 2 ) columns sampled from M according to their generalized leverage scores w.r.t. rank q . Then with probability  X  0 . 99 , the span of P has a rank-s (1 + 2 , 2 X ) -approximation subspace for M .

Proof. It follows from combining Theorem 5 in [19] and the definition of the generalized leverage scores.
Computing the generalized scores with respect to rank q could be much more efficient, since the intrinsic dimension now becomes q , which can be much smaller than the ambient dimension (the number of points or the dimension of the feature space). However, as noted in the overview, there are still a few technical challenges. Our final algorithm consists of four key steps, each of which addresses one of the above challenges. They are elaborated in the following four subsections respectively, and the final subsection presents the overall algorithm.
Recall that a subspace embedding S for a matrix M is such that k SMx k  X  k Mx k , i.e. , the norm of any vector in the column space of M is approximately preserved. Subspace embeddings can also be generalized for the feature mapping of kernels, simply by setting M =  X  ( A ), S a linear mapping from H7 X  R t and using the corresponding inner product. If the data after the kernel subspace embedding is sufficient for solving the problem under consideration, then only S X  ( A ) in much lower dimension is needed. This is especially interest-ing for distributed kernel methods, since directly using the feature mapping or the kernel trick in this setting will lead to high communication cost, while the data after embedding can be smaller and lead to lower communication cost.
A sufficient condition for solving many problems (in par-ticular, kernel PCA) is to preserve the low rank structure of the data. More precisely, the row space of S X  ( A ) is a good approximation to that of  X  ( A ), where the error is compa-rably to the best rank k approximation error. Then S X  ( A ) can be used to compute the generalized leverage scores for  X  ( A ), which can then be utilized to compute kernel PCA as mentioned above.

More precisely, we would like S X  ( A ) to approximate the row space of  X  ( A ) up to (1 + ,  X ), as required in the defi-nition of the generalized leverage scores. We give such em-beddings a particular name.

Definition 5. S is called a (1 + ,  X ) -good subspace em-bedding for  X  ( A )  X  X  n , if there exists X such that k X ( S X  ( A ))  X   X  ( A ) k 2  X  (1 + ) k  X  ( A )  X  [  X  ( A )]
We now identify the sufficient conditions for (1 + ,  X )-good subspace embeddings, which can then be used in con-structing such embeddings for various kernels.

Lemma 3. S is a (1 + ,  X ) -good subspace embedding for  X  ( A )  X  X  n if it satisfies the following.

P1 (Subspace Embedding): For any orthonormal V  X  H k
P2 (Approximate Product): for any M  X  X  n ,N  X  X  k ,
Polynomial Kernels. For polynomial kernels, there ex-ists an efficient algorithm TensorSketch to compute the embedding [2]. However, the embedding dimension has a Algorithm 1 Distributed Leverage Scores: {  X  ` j } = disLS ( E i 1: Each worker i : do 1 4 -subspace embedding E i T i  X  2: Master: QR-factorize E 1 T 1 ,...,E s T s &gt; = UZ ; 3: Each worker i : compute  X  ` i j = ( Z &gt; )  X  1 E i quadratic dependence on the rank k , which will increase the communication. Fortunately, subspace embedding can be concatenated, so we can further apply another known sub-space embedding such as one of those in Lemma 1 which, though not fast for feature mapping, is fast for the already embedded data and has lower dimension. In this way, we can enjoy the benefits of both approaches.

The guarantee of TensorSketch in [2] and the property of the subspace embeddings in Lemma 1 can be combined to verify P1 and P2 . So we have
Lemma 4. For polynomial kernels  X  ( x,y ) = (  X  x,y  X  ) q exists a (1 + , 0) -good subspace embedding matrix S : R d R t with t = O ( k/ ) .

Kernels with Random Feature Expansions. Poly-nomial kernels have finite dimensional feature mappings, for which the sketching seems natural. It turns out that it is possible to extend subspace embeddings to kernels with infi-nite dimensional feature mappings. More precisely, we pro-pose subspace embeddings for kernels with random feature  X  (  X  ). Therefore, one can approximate the kernel by using m features z  X  ( x ) on randomly sampled  X  . Such random feature expansion can be exploited for subspace embeddings: view the expansion as the  X  X ew X  data points and apply a sketch-ing matrix on top of it. Compared to polynomial kernels, the finite random feature expansion leads to an additional additive error term. Our analysis shows that bounding the additive error term only requires sufficiently large sampled size m , which affects the computation but does not affect the final embedding dimension and thus the communication. In summary, the embedding is S X  ( x ) = TR (  X  ( x )), where R (  X  ( x ))  X  R m is m random features for x and T  X  R t  X  m an embedding as in Lemma 1. The properties P1 and P2 can be verified by combining Lemma 1 and the guarantees of random features.

Lemma 5. For a continuous shift-invariant kernels  X  ( x,y ) =  X  ( x  X  y ) with regularization, there exists a (1 + ,  X ) -good subspace embedding S : H7 X  R t with t = O ( k/ ) .
Given the matrix E obtained from kernel subspace em-bedding, we would like to compute the leverage scores of E . First note that this cannot be done simply in a local man-ner: the leverage score of a column in E i is different from the leverage score of the same column in E . Furthermore, though data in E have low dimension, communicating all points in E to the master is still impractical, since it leads to communication linear in the total number of points.
Fortunately, we only need to compute constant approxi-mations of the scores, which allows us to use subspace em-bedding on E to greatly reduce the number of data points. Algorithm 2 Sampling Representative Points: Y = RepSample ( A i s i =1 , {  X  ` i j } ,k, ) 1: Workers: sample O ( k log k ) points according to {  X  2: Master: send all the sampled points P to the workers; 3: Workers: sample O ( k/ ) points  X  Y according to the 4: Master: send Y =  X  Y  X  P to all the workers. In particular, we apply a 1 4 -subspace embedding T i one of those in Lemma 1) on each local data set E then send them to the master. Let ET denote all the em-bedded data, and do QR factorization ( ET ) &gt; = UZ . Now, the rows of U &gt; = Z &gt;  X  1 ET are a set of basis for ET . Then, think of U &gt; T  X  = Z &gt;  X  1 E as the basis for E , so it suffices to compute the norms of the columns in Z &gt;  X  1
The details are described in Algorithm 1 and Figure 1(a) shows an illustration. The algorithm is guaranteed to output constant approximations of the leverage scores of E . Lemma 6. Let ` i j be the true leverage scores of E . Then Algorithm 1 outputs  X  ` i j = (1  X  1 / 2) ` i j .
Proof. The algorithm can be viewed as applying an em-bedding T = diag T 1 ,...,T s on E to approximate the scores while saving the costs. Each T i is an 1 4 -subspace em-bedding matrix, then for any x , So T is also 1 4 -subspace embedding. Such a scheme of using embedding for approximating the scores has been analyzed (Lemma 6 in [18]), and the lemma follows.

We note that though a constant approximation is suffi-cient for our purpose, but the algorithm can output (1  X  ) ` i j by doing an 2 -subspace embedding (instead of which can be useful for other applications.
Sampling directly to the leverage scores can produce a set of points P such that the span of  X  ( P ) contains a (1 + ,  X )-approximation to  X  ( A ). However, the rank of that approxi-mation can be as high as O ( k/ ), since its rank is the same as that of the embedded data (see Lemma 2), which will be O ( k/ ) to achieve error. To get a rank-k approximation and also enjoy the advantage of leverage scores, we propose to combine leverage score sampling and the adaptive sam-pling algorithm in [16, 9].

The details are presented in Algorithm 2. We first sample a set P of O ( k log k ) points according to the leverage scores, so that the span of  X  ( P ) contains a (2 ,  X )-approximation. Then we use the adaptive sampling method: sample O ( k/ ) points according to the square distances from the points to their projections on P and then add them to P to get the Algorithm 3 Computing an Approximation: L = disLR ( A i s i =1 , Y , k , ,  X ) 1: Each worker i : compute the basis Q for  X  ( Y ) and  X  2: Master: concatenate  X  T =  X  1 T 1 ,...,  X  s T s and send 3: Each worker i : set L = QW . desire set Y of representative points. Figure 1(b) and 1(c) demonstrate the two steps of the algorithm.
 Adaptive sampling has the following guarantee:
Lemma 7. Suppose there is a (2 ,  X ) -approximation for  X  ( A ) in the span of  X  ( P ) . Then with probability  X  0 . 99 , the span of  X  ( Y ) has a rank-k (1 + ,  X ) -approximation.
Therefore, we solves the column subset selection problem for kernels in the distributed setting, with O ( k log k + k/ ) se-lected columns and with a communication of only O ( s X k/ + sk 2 ). This also provides the foundation for kernel PCA task.
To compute a good approximation in the span of  X  ( Y ), the na  X   X ve approach is to project the data to the span and compute SVD there. However, the communication will be linear in the number of data points. Subspace embedding can be used to sketch the projected data, so that the number of data points is greatly reduced.

Algorithm 4 describes the details and Figure 1(d) shows an illustration. To compute the best rank-k approximation for the projected data  X , we do a subspace embedding on the right hand side, i.e. , compute  X  T =  X  1 T 1 ,...,  X  Then the algorithm computes the best rank-k approximation W for  X  T , which is then a good approximation for  X  and thus  X  ( A ). It then returns L , the representation of W in the coordinate system of  X  ( A ). The output L is guaranteed to be a good approximation.

Lemma 8. If there is a rank-k (1 + ,  X ) -approximation subspace in the span of  X  ( Y ) , then
LL &gt;  X  ( A )  X   X  ( A ) 2  X  (1+ ) 2  X  ( A )  X  [  X  ( A )]
Proof Sketch. For our choice of w , T i is an -subspace embedding matrix for  X  i . Then their concatenation B is an -subspace embedding for  X , the concatenation of  X  i . Then we can apply the idea implicit in [20].
 By Pythagorean Theorem, the error can be factorized into
Note that  X  = Q &gt;  X  ( A ), and W is the best rank-k sub-space for its embedding  X  T . By property of T (Theorem 7 in [20]), it is also a good approximation for  X . So T 1  X  [ Q &gt;  X  ( A )] k  X  Q &gt;  X  ( A ) 2 = Q [ Q &gt; .
 Algorithm 4 Distributed Kernel PCA: L = disKPCA ( A i s i =1 ,k,,  X ) 1: Each worker i : do a (1 / 4 ,  X )-good subspace embedding 2: Compute the leverage scores: 3: Sample points: Y = RepSample ( A i s i =1 , {  X  ` i j 4: Output L = disLR ( A i s i =1 ,Y,k,,  X ).
 Table 1: Dataset specification: d is the original feature dimen-Combining this with T 2, and applying Pythagorean Theo-rem again, we know that the error is roughly
Now, by assumption, there is a rank-k (1+ ,  X )-approximation subspace X in the span of  X  ( Y ). Since [ Q &gt;  X  ( A )] rank-k approximation to Q &gt;  X  ( A ), = Q [ Q &gt;  X  ( A )] k  X  QQ &gt;  X  ( A ) 2 + QQ &gt;  X  ( A )  X   X  ( A )  X  X  X  QQ &gt;  X  ( A ) 2 + QQ &gt;  X  ( A )  X   X  ( A ) 2 = k X  X   X  ( A ) k 2 .
 The lemma then follows.
Now, putting things together, we obtain our final algo-rithm for distributed kernel PCA (Algorithm 4). Our main result, Theorem 1, follows by combining all the lemmas in the previous subsections (with properly adjusted and  X ).
We use ten datasets to evaluate our algorithm. They con-tain both sparse and dense data and come from a variety of different domains, such as text, images, high energy physics and biology. We use two smaller ones to benchmark against the single-machine batch KPCA algorithm while the rest are large-scale datasets with up to tens of millions of data points and hundreds of thousands dimensions. Refer to Table 1 for detailed specifications. (c) error on har (c) error on har
Each dataset is partitioned on different workers according to the power law distribution with exponent 2 to simulate the distribution of the data over large networks [14]. De-pending on the size of the dataset, the number of workers used ranges from 5 to 200 (see Table 1 for details).
Since our key contribution is sampling a small set of data points intelligently, the natural alternative is uniformly sam-pling. We compare with two variants of uniform sampling algorithms: 1) uniformly sampling representative points and use Algorithm 3 to get KPCA solution (denoted as uni-form+disLR); 2) uniformly sampling data points and apply batch KPCA (denoted as uniform+batch KPCA).

For both algorithms, we compare the tradeoff of low rank approximation error and communication cost. Particularly, we compare the communication needed to achieve the same error. Each method is run 5 times and the mean and the standard deviation are reported.

For polynomial kernel, the degree is q = 4 and for Gaus-sian RBF kernel, the kernel bandwidth  X  is set to 0 . 2 of the median pairwise distance among a subset of 20000 randomly chosen data points (a.k.a, the  X  X edian trick X ). For Gaussian random feature expansion, we use 2000 random features.
In all experiments, we set the number of principle compo-nents k = 10, which is the same number for k -means. The algorithm specific parameters are set as follows: 1) The sub-space embedding dimension for the feature expansion t is 50; 2) The subspace embedding dimension for the data points p is 250; 3) We vary the number of adaptively sampled points |  X 
Y | from 50 to 400 to simulate different communication cost; 4) The subspace embedding dimension w is set to equal | Y | .
We compare to the  X  X round-truth X  solutions produced by batch KPCA on two small datasets where it is feasible. The experiment results for the polynomial kernel and the Gaus-sian RBF kernel are presented in Figures 2 and 3, respec-tively. In both cases, the approximation error of disKPCA decreases as more communication is allowed. It can nearly match the optimum low-rank approximation error with much fewer data points. In addition, it is much faster: we gain a speed up of 10  X  by using five workers.
In these experiments, we compare the tradeoff between communication cost and approximation accuracy on large-scale datasets. The alternative, uniform + batch KPCA, is stopped short in many experiments due to its excessive computation cost for larger number of sampled data points.
Figure 4 demonstrates the performance on polynomial kernels on four large datasets. On all four datasets, our algorithm outperforms the alternatives by significant mar-gins. Especially on bow, which is a sparse dataset, the usage of kernel embeddings takes advantage of the sparsity struc-ture and leads to much smaller error. On other datasets, uniform + disLR cannot match the error achieved by our algorithm even when using much more communication.
Figure 5 shows the performance on Gaussian kernels. On mnist8m, the error for uniform + batch KPCA is so large (almost twice of the errors in the figure) that it is not shown. On other datasets, disKPCA achieves significant smaller er-ror. For example, on higgs dataset, to achieve the same error, uniform + disLR requires more than 5 times com-munication. Since it does not have the communication of computing leverage scores, this means that it needs to sam-ple much more points to get similar performance. Therefore, our algorithm is very efficient in communication.
Besides polynomial and Gaussian kernels, we have also conducted experiments using arc-cos kernel [12]. The arc-cosine kernels have random feature bases similar to the Rec-tified Linear Units (ReLU) used in deep learning. We use degree n = 2 and Figure 6 shows the results. Our algorithm consistently achieves better tradeoff between communication and approximation and the benefit is especially more pro-nounced on sparser dataset such as 20news. In Figure 7, we present the scaling results for disKPCA. In these experiments, we vary the number of workers and record the corresponding computation time (communication time excluded). On both datasets, the runtime decreases as we use more workers, and it eventually plateaus. Our algo-rithm gains about 2  X  speedup by using 4  X  more workers. Note that our algorithm is designed to strike a good balance between communication and approximation. Even though computation complexity is not our first priority, the experi-ments show disKPCA still enjoys favorable scaling property.
We have also experimented a form of spectral clustering (KPCA followed by k -means clustering). We project the data onto the top k principle components and then apply a distributed k -means clustering algorithm [6]. The evaluation criterion is the k -means objective, i.e. , average distances to the corresponding centers, in the feature space.

Figure 8(a) presents results for polynomial kernels on the 20news and susy and Figure 8(b) presents results for Gaus-sian kernels on ctslice and yearpredmsd. Our algorithm com-pares favorably with the other methods and achieves a better tradeoff of communication and error. This means that al-though the other methods require similar communication, they need to sample more data points to achieve the same loss, demonstrating the effectiveness of our algorithm.
This paper proposes a communication efficient distributed algorithm for kernel Principal Component Analysis with the-oretical guarantees. It computes a relative-error approxi-mation compared to the best rank-k subspace, using com-munication that nearly matches that of the state-of-the-art algorithms for distributed linear PCA. This is the first dis-tributed algorithm that can achieve such provable approx-imation and communication bounds. The experimental re-sults show that it can achieve better performance than the baseline using the same communication budget. M.-F. B. and Y. L. were supported in part by NSF grants CCF-0953192 and CCF-1101283, ONR N00014-09-1-0751, AFOSR grant FA9550-09-1-0538, a Microsoft Faculty Fel-lowship, and a Google Research Award. Y. L. was also sup-ported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329. L. S. and B. X. were sup-ported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, and NSF CA-REER IIS-1350983. D.W. acknowledges the support from XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323. [1] N. Ailon and B. Chazelle. The fast [2] H. Avron, H. Nguyen, and D. Woodruff. Subspace [3] F. Bach and M. Jordan. Predictive low-rank [4] K. Bache and M. Lichman. UCI machine learning [5] M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. [6] M.-F. Balcan, V. Kanchanapally, Y. Liang, and [7] P. Baldi, P. Sadowski, and D. Whiteson. Searching for [8] C. Boutsidis, M. Sviridenko, and D. P. Woodruff. [9] C. Boutsidis and D. P. Woodruff. Optimal cur matrix [10] C. Boutsidis and D. P. Woodruff.
 [11] C. Boutsidis, D. P. Woodruff, and P. Zhong.
 [12] Y. Cho and L. K. Saul. Kernel methods for deep [13] K. L. Clarkson and D. P. Woodruff. Low rank [14] A. Clauset, C. R. Shalizi, and M. E. Newman. [15] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. [16] A. Deshpande and S. Vempala. Adaptive sampling [17] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel kmeans, [18] P. Drineas, M. Magdon-Ismail, M. Mahoney, and [19] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. [20] R. Kannan, S. Vempala, and D. Woodruff. Principal [21] S. Kumar, M. Mohri, and A. Talwalkar. Sampling [22] X. Meng and M. W. Mahoney. Low-distortion [23] J. Nelson and H. L. Nguy X en. Osnap: Faster numerical [24] A. Rahimi and B. Recht. Random features for [25] T. Sarl  X os. Improved approximation algorithms for [26] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [27] B. Sch  X  olkopf, A. J. Smola, and K.-R. M  X  uller. Kernel [28] B. Sch  X  olkopf, K. Tsuda, and J.-P. Vert. Kernel [29] D. P. Woodruff. Sketching as a tool for numerical [30] Y. Zhang, M. J. Wainwright, and J. C. Duchi.
