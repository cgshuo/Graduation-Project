 The I-SPY meta-search engine uses a technique called col-laborative Web search to leverage the past search behaviour (queries and selections) of a community of users in order to promote search results that are relevant to the commu-nity. In this paper we describe recent studies to clarify the benefits of this approach in situations when the behaviour of users cannot be relied upon in terms of their ability to consistently select relevant results during search sessions. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  relevance feedback, retrieval models Experimentation, Measurement, Human Factors Collaborative Web search, personalisation, relevance, noise
Collaborative Web search (CWS) [4] is a form of meta-search that manipulates the results of underlying Web search engines, such as Google and HotBot, in response to the learned preferences of a given community of users. A com-munity shares similar information needs, and may be de-fined implicitly, for example an ad-hoc group of searchers using a search box located on a particular themed Web site, or explicitly where a particular information need is defined for a community. Within a community, results that have been preferred in the past for similar queries are actively promoted in a new result-list. To do this CWS maintains a data structure called the hit matrix , H to represent the search behaviour of a given community of users. Each time a community member selects a result p j in response to some query q i the entry in cell H ij is incremented. In turn, the rel-evance of a page p j to q i is estimated as the relative number
This material is based on works supported by Science Foun-dation Ireland under Grant No. 03/IN.3/I361.

The TREC Terabyte collection consists of 25 million Web pages (approx. 426GB) from the .gov domain. The TREC Terabyte track of 2004 includes 50 topics as target search topics. Each includes a short text description and during the evaluation, competing search engines are compared by their ability to retrieve documents relevant to these topics.
Normally in CWS the hit-matrix is trained by the searches of a given community of users, but in our TREC experiment we were interested in whether CWS could be used to imple-ment a relevance model that was based on link connectivity and anchor text; each document d i was represented by a new document d i , made up of the anchor text entries for all links to d i within the collection. The CWS hit-matrix was then trained using a version of the F  X   X sr  X  eal benchmark search engine[2] that relied on an anchor text index pro-duced from the d i  X  X . A set of training queries was generated by extracting subsets of terms from the TREC topic descrip-tions and narratives; for each topic we generated 250 queries with between 2 and 8 terms each. Each of these queries was submitted to the F  X   X sr  X  eal engine and the hit-matrix was up-dated with the top 20 results. During testing, the TREC test queries were submitted to a version of CWS that used the above hit-matrix and the F  X   X sr  X  eal benchmark search en-gine using a standard document index, so that documents that tended to match on anchor text terms were promoted within the final result-list.

The results were mixed, with our TREC Terabyte track run ranking only 56th out of 71 submitted runs [2, 3]. It quickly became apparent that our approach to training was unlikely to produce high-quality (relevant) result promo-tions. Specifically, naively updating the hit-matrix with the top 20 results led to a significant degree of noise (non-relevant results) being added to the hit-matrix. And, of course, this noise was being expressed during testing through the promoted results.
After the TREC 2004 Terabyte track, relevance results were made available to participants to help with the evalu-ation of new search techniques. These provide ground-truth relevance assessments for the topics and allow for a more detailed and principled evaluation of the factors that con-tribute to the success of CWS, especially in relation to the presence of selection noise in the hit-matrix data.
To do this we configured our CWS engine to work with the F  X   X sr  X  eal benchmark search engine and the same training queries were used during hit-matrix training; we also set a query similarity threshold of &gt; 0 so that query reuse is triggered once the queries share at least one common term. This query similarity method is evaluated with respect to CWS in [1]. However, this time we use the TREC relevance judgments to simulate the selections of a live-user under dif-ferent noise conditions. We control two basic parameters: k refers to the number of selections made by this user during a search sessions; and n refers to the percentage of these selections that are noisy (non-relevant). So, for example, k =10and n =0 . 4 indicates that during training 10 re-sults were selected per search but that only 60% of these results were actually relevant according to TREC relevance assessments. We trained different hit-matrices for a range of different combinations of k and n , and for each we cal-
