 Matrix factorization is one of the most powerful techniques in collaborative filtering, which models the (user, item) in-teractions behind historical explicit or implicit feedbacks. However, plain matrix factorization may not be able to un-cover the structure correlations among users and items well such as communities and taxonomies. As a response, we design a novel algorithm, i.e., hierarchical group matrix fac-torization (HGMF), in order to explore and model the struc-ture correlations among users and items in a principled way. Specifically, we first define four types of correlations, includ-ing (user, item), (user, item group), (user group, item) and (user group, item group); we then extend plain matrix fac-torization with a hierarchical group structure; finally, we de-sign a novel clustering algorithm to mine the hidden struc-ture correlations. In the experiments, we study the effec-tiveness of our HGMF for both rating prediction and item recommendation, and find that it is better than some state-of-the-art methods on several real-world data sets. H.3.3 [ Information Search and Retrieval ]: Information Filtering Algorithm, Experimentation, Performance Collaborative Filtering; Hierarchical Structure; User Group; Item Group
Matrix factorization (MF) is one of the most powerful methods in collaborative filtering (CF) [5, 15, 24], and has achieved great successes in various open competitions and real industry applications [6, 20]. The main idea of matrix factorization is that a rating matrix can be represented by a product of two low-rank matrices. However, MF alone may not be able to uncover the structure correlations among users and items well. Various auxiliary information is thus leveraged to overcome this limitation [9,26]. One objective of using auxiliary information is to detect the local corre-lations among users and/or items, which may help capture more information of users X  potential preferences.
In practice, we find that the structure correlations in many applications are multi-level. This reflects the characteris-tics from individuals to communities, and iteratively forms four types of correlations, including (user, item), (user, item group), (user group, item) and (user group, item group). We illustrate these four types of correlations in Figure 1. Figure 1: Illustration of four types of correlations a mong users and items.

In order to exploit some of these correlations, some meth-ods have been proposed to incorporate the group informa-tion into matrix factorization. The authors in [11] find that items can be treated as different groups and then construct the (user, item group) by using auxiliary information such as tags and temporal information. [10] mainly focuses on (user group, item) correlations from social networks or rat-ing similarities. Both of them show that incorporating group structure into MF can result in better recommendation per-formance.

However, the aforementioned methods only focus on one particular type of correlations and ignore the others. For ex-ample, they do not consider the correlations of (user group, item group), which represents high-level structure correla-tions. Instead of considering one certain type of correlations in a specific situation, in this paper, we combine all those four types of correlations into a single unified algorithm called hierarchical group matrix factorization (HGMF). HGMF extends the plain matrix factorization to a hierarchical one, w hich contains multi-level latent factors. For situations with-out hierarchical structure, we design a greedy-based cluster-ing algorithm to learn the group information. Note that the user group in [14] is different since it is randomly generated on-the-fly of the learning process instead of learned and fixed before matrix factorization.

The rest of paper is organized as follows. In Section 2, we discuss some related works on exploiting correlations among users and items. In Section 3, we describe our HGMF algo-rithm for explicit feedbacks and implicit feedbacks, respec-tively. In Section 4, we design a greedy-based clustering al-gorithm to construct the hierarchical structure correlations. In section 5, we conduct extensive experiments to study the effectiveness of our HGMF for both rating prediction and item recommendation. Finally, we conclude this paper with some future directions in section 6.
In collaborative filtering, many methods have been pro-posed to integrate some of the four types of correlations, i.e., (user, item), (user, item group), (user group, item) and (user group, item group), into plain matrix factorization.
For the correlations of (user, item), some MF models [5, 17] are proposed to use latent factors of users and items to represent their relationships. It assumes that the preference of a user i to an item j can be represented by a product of their latent factors h U i , V j i , and hence the rating matrix can be approximated as a product of two low-rank latent matrices, i.e., R = h U, V i . Then, the optimization problem will be where I is an indicator matrix for non-zero elements in R , kk
F is a Frobenius norm, and  X  is Hadamard product for element to element multiplication. Typically, a batch gra-dient descent (BGD) or stochastic gradient descent (SGD) algorithm is used to learn latent factors U and V .
To dig the potential inner correlations of (user, item group), [2,22,23] integrate LDA into MF, which tries to use a class of items with the same topic instead of individual items to improve the recommendation accuracy. [11,12] try to utilize the auxiliary information such as tags, categories to establish the inner connections between users and item groups, and also find that items can be divided into hierarchical groups. Usually, a plain matrix factorization is applied to learn the latent factors of users and item groups by combining item group information into items X  latent factors.

To exploit the correlations of (user group, item), [10] tries to leverage social information to establish the local similari-ties of users and enable a group of users to help an individual.
Some clustering algorithms are also applied in recommen-dation [4,18,25]. They usually first cluster users and items into groups based on some similarity measurements, and then use KNN algorithm or plain MF to predict the rat-ing scores based on the generated clusters. What we should note here is that they still focus on some partial preference of users and fail to establish the structure relationships among all those four types of correlations.

Recently, a hierarchical MF model is proposed to capture the multi-level information. For predicting plant traits, the authors in [19] propose a hierarchical probabilistic matrix factorization (HPMF) with multi-level plants information. It is similar to building correlations among users, items and user groups in collaborative filtering. The authors in [27] also propose a hierarchical matrix factorization, which di-vides the rating matrix level-by-level by considering the lo-cal context such as mood of users, and then applies MF to each sub-matrix. In each sub-matrix, they still focus on the correlations of (user, item) instead of structure correlations involving user group or item groups.

The aforementioned methods have empirically been shown helpful for improving recommendation performance. How-ever, all of them only focus on some types of correlations and fail to establish the inner structure connections among all the four types of correlations and systematically study their performance. In this paper, we propose a novel and generic algorithm which takes all those four types of correla-tions into account. For the data sets without group informa-tion, we further design a greedy-based clustering algorithm to construct the hierarchical structure correlations.
Our first task is to represent the four types of correlations, i.e., (user, item), (user, item group), (user group, item) and (user group, item group). Inspired by [19], we find that the correlations can be represented by constructing new matri-ces based on the source rating matrix and group informa-tion. For example, in Figure 2, there are 7 users and 5 items. The shaded circles represent groups, e.g., users 1 2 (1) and 3 (1) belong to group 1 (2) which further belongs to group 2 (3) . Here  X (  X  ) X  denotes the group level at  X  . R the source rating matrix which represents the correlations of (user, item). We denote the matrix that generated by (user, item group) as Q (1) , and the one generated by (user group, item) as P (1) . For example, in Figure 2, the rating ma-trix P (1) is a rating matrix of user groups { 1 (2) , 2 (2) { 1 (2) , 2 (2) } . Furthermore, (user group, item group) can be obtained by P (1) or Q (1) , and the generated matrix is defined as R (2) , which is at a higher level as compared with R (1) example, R (2) in Figure 2 is a rating matrix of user groups { 1 lowing the process, we can transform the hierarchical group information into hierarchical matrices.

Formally, we denote the matrix at level  X  as R (  X  ) , which includes the rating values of user groups { u (  X  ) i } N groups { v (  X  ) i } M  X  i =1 with the size of N  X   X  M  X  , where { x ber and M  X  is the column number of the  X  th rating matrix. The parent group of u i in R (  X  ) is defined as P U (  X  ) ple, 2 (3) is the parent group of 1 (2) , which is the parent group 1 child group of user u i at level  X  . For example, in Figure 2, parent group P V (  X  ) j and item child group CV (  X  ) j are defined in similar way. We define the number of user group levels as 0 4 averaging the scores of users in each user group. R (2) is the rating matrix at the second level and can be constructed by Q (1) or P (1) .
 L
U and the number of item group levels as L V . In Figure 2, both user group level number and item group level number are 3, i.e., L U = L V = 3. The matrix P (  X  ) is the rating ma-{ v { v Given the matrix R (  X  ) and group information, P (  X  ) , Q R (  X  +1) can be constructed in many ways. For example, we can construct the group preference as the average preference of its members, i.e., P (  X  ) ij = 1 R
One benefit of representing the four types of correlations by different levels of matrices is that the matrices can be combined into a generative hierarchical model which can effectively reflect the inner connections between local simi-larity and global tendency. In order to combine (user, item) and (user group, item), we propose the model User Group MF (UGMF), and to combine (user, item) and (use, item group), we propose Item Group MF (IGMF). The models are illustrated in Figure 3.

For UGMF, in order to establish the connections among users, items and user groups, we assume that the latent fac-tors of users (user groups) are sampled from their parent V (1) is shared by R (1) and P (  X  ) . Then the generative pro-cess is as follows: 1. For each 1  X   X   X  L U , generate U (  X  )  X  X  U (  X  +1) ,  X  2. Generate V (1)  X  X  0 ,  X  2 V I . 3. For each non-missing entry ( i, j ) in P (  X  ) at each level where  X  is the standard deviation and U ( L U +1) = 0. Then the posterior probability over { U (  X  ) } L U  X  =1 and V L
U is: where  X  is an indicator function, i.e.,  X  R (  X  ) ij = 1 if R
Similarly, for IGMF, we suppose that the latent factors of items (item groups) are sampled from their parent groups, i.e., V (  X  )  X  N ( V (  X  +1) |  X  2 V I ) and the latent factor U shared by R (1) and Q (  X  ) . Then the posterior probability
Both UGMF and IGMF are more comprehensive than MF s ince they not only learn the correlations of (user, item) but also (user, item group) or (user group, item), and the assumption that the latent factors are sampled from par-ent groups rather than be generated randomly is also more reasonable. Comparing with HPMF, UGMF is of rich in-teractions between different levels. It uses a single latent factor V (1) to represent the latent factors of items at all lev-els, and hence can more effectively share information among each level. Our experiments will show that both UGMF and IGMF are more effective than MF and HPMF.

However, UGMF and IGMF are not our ultimate goal since they still have some limitations. For example, UGMF cannot get benefit from hierarchical group information of items and IGMF cannot get benefit from hierarchical group information of users. Furthermore, they ignore the correla-tions of (user group, item group), which is useful to reflect the one-to-one preference at a higher level. So a more elabo-rate model should be designed to overcome these shortcom-ings.
HGMF is proposed by combining all the four types of cor-relations into a single hierarchical model. For HGMF, in order to predict the unobserved ratings at different levels, L latent factors of users { U (  X  ) } L  X  =1 and items { V combined to represent both user groups and item groups. HGMF is shown in Figure 3, which is also a generative model. To learn the latent factors, we assume that the rating from a higher level to a lower level. The generative process is as follows: 1. For each 1  X   X   X  L , generate U (  X  )  X  X  U (  X  +1) ,  X  2. For each 1  X   X   X  L , generate V (  X  )  X  X  V (  X  +1) ,  X  3. For each non-missing entry ( i, j ) in R (  X  ) at each level where L = min( L u , L v ) and U ( L +1) = V ( L +1) = 0. Then the posterior probability over { U (  X  ) } L  X  =1 and { V HGMF-L is:  X  where  X  = {  X  2 R ,  X  2 P ,  X  2 Q ,  X  2 U ,  X  2 V } .
The benefit of HGMF is that it can give a balance of all the four types of correlations instead of a particular one or F igure 3: Graphical models of MF, IGMF, UGMF and HGMF. two. Another benefit of HGMF is that it can be extended to a hierarchical structure which is more comprehensive and reasonable than HPMF, UGMF and IGMF.
Since the learning algorithms of HGMF, UGMF and IGMF are similar, for simplicity, we only focus on the learning pro-cess of HGMF in this paper. The maximum likelihood esti-mation of HGMF-L can be written as: L =  X  X  X  1  X  1  X  1  X  1  X  1
Note that maximizing L is not a convex problem. We can use a batch gradient ascent (BGA) algorithm to up-V U i , and  X  u P  X  v Q (  X  ) j ,  X  1 ,  X  2 can be obtained in the similar way. R U
Instead of directly applying BGA, following stochastic block co-ordinate descent [1], we update U and V at each level sequentially in order to give a faster convergence. Fur-sequentially updated. Therefore, we use the following up-date sequence to update one level of HGMF at  X  . The general learning procedure of HGMF-L for rating pre-diction is shown in Algorithm 1. The time complexity of HGMF-L depends on the number of level linearly. In prac-tice, with a larger value of L , the time cost does not increase much, and may even decrease in some experiments with the help of group information.
In previous sections, we have discussed the learning al-gorithm of HGMF for rating prediction. However, we find that implicit feedbacks are usually easier to get and item rec-ommendation may be more useful than rating prediction in some situations. Hence, for a comprehensive comparison of our HGMF with MF, we also study HGMF on collaborative filtering with implicit feedbacks. The implicit feedbacks only contain two types of information, i.e., the observed user-item pairs and non-observed pairs and hence the rating prediction is meaningless for these binary matrices. In [16], the authors propose a matrix factorization method called BPR-MF, in which a pair-wise ranking assumption is adopted. They as-Algorithm 1 T he learning algorithm of HGMF.

Input:
Output: 1: for  X  = 1 , 2 , ..., L  X  1 do 4 : end for 5: for  X  = 1 , 2 , ..., L do 7 : end for 8: for t = 1 , 2 , ..., T do 9: for  X  = L, L  X  1 , ..., 1 do 12: if  X  6 = 1 then 17: end if 18: end for 19: end for 21: Output latent factors and predicted matrix. sume that observed user-item pairs are more preferred than n on-observed user-item pairs. We find that BPR can be integrated into HGMF seamlessly and show the learning al-gorithm of BPR-HGMF in Algorithm 2. Note that | X | is the number of observations of the matrix X . The main differ-ence between BPR-MF and BPR-HGMF is that we apply BPR to all group matrices with the SGD algorithm while BPR-MF only applies BPR to the first level matrix, i.e., R Algorithm 2 T he learning algorithm of BPR in HGMF-L . 1: f or  X  = L, L  X  1 , ..., 1 do 2: Train BPR in R (  X  ) with 10 | R (  X  ) | iterations. 3: if  X  6 = 1 then 4: Train BPR in P (  X   X  1) with 10 | P (  X   X  1) | iterations. 5: Train BPR in Q (  X   X  1) with 10 | Q (  X   X  1) | iterations. 6: end if 7: end for
W e have proposed a novel and generic algorithm for both rating prediction and item recommendation given the hier-archical group information. However, in some applications, Figure 4: The generated user groups on Douban M usic with explicit feedbacks by two different vari-ants of the clustering algorithm, where the threshold is set as threshold ( i ) = c = 0 . 1 for the first variant and variant. In the left figure, many users are gathered into a few big groups, while in the right figure, the groups are of similar size. it may be difficult to obtain such structure information. In order to address this problem, we design a greedy-based clus-tering algorithm for learning the structure from a raw rating matrix.

Due to the sparsity of users X  feedbacks, it is usually diffi-cult to design a proper clustering algorithm that can effec-tively put the related users into the same group. Because users with few ratings may be isolated due to the small sim-ilarity values with others. One intuition is that the users with more ratings will be in bigger groups and have more friends because those users are active and have a wide range of interests and their behaviors are thus shared with more people. Another intuition is that all the group sizes should be roughly equal in order to help users with few ratings. We adopt these two assumptions and design a simple but effec-tive clustering algorithm with a controlled threshold param-eter.

We first rank users w.r.t. their rating frequency, i.e., the first user has most ratings and the last user has fewest rat-ings. Then, from the first user to the last user, we find some of their most similar neighbors whom are not selected into any group, and then put them into a new group. Note that the most similar neighbors are defined by a similarity thresh-old for user i , i.e., threshold ( i ). We apply Cosine similarity for explicit feedbacks and Jaccard Index 1 for implicit feed-backs. The details of our algorithm is shown in Algorithm 3. We find that the similarity values based on ratings of users obey long-tail distribution. If we fix the similarity thresh-old as a constant value, i.e., threshold ( i ) = c for all users, the first user will have many friends while the last one will sometimes have no friends. It is actually the first intuition we mentioned above. If we set threshold ( i ) = 1 a ( b +l og( i )) find that with some reasonable values of a and b , the sizes of all groups are roughly equal. We denote the first variant as c 1 and the second variant as c 2. We show the generated clusters on Douban Music data with explicit feedbacks in Figure 4. h ttp://en.wikipedia.org/wiki/Jaccard index Algorithm 3 A greedy-based clustering algorithm.

Input: Output: 1: for  X  = 1 , 2 , ..., L do 2: Compute activeness (behavior frequency) of each user. 3: Sort users w.r.t. activeness in reverse order. 4: Compute user-user similarity matrix S . 5: Initialization: P U (  X  ) = 0, CU (  X  +1) = 0, x = 0, F = 6: for i = 1 , ..., N  X  do 7: if u i /  X  F then 8: F u  X  { u i } X  X  u j | S u i ,u j &gt; threshold ( i ) , u 9: x  X  x + 1. 10: CU (  X  +1) x  X  Index set of users in F u . 12: F  X  F  X  F u . 13: end if 14: end for 15: N  X  +1  X  x . 16: for i = 1 , ..., N  X  +1 do 18: end for 19: Construct Q (  X  ) , R (  X  +1) in a similar way. 20: end for Table 1: Statistics of the real-world data sets used i n the experiments.

Four real-world data sets of explicit feedbacks are used in our rating prediction experiments and three real-world data sets of implicit feedbacks are used in our item recommen-dation experiments. The information of users and items as well as the sparsity of the corresponding data sets are shown in Table 1.

The four data sets of explicit feedbacks contain 1  X  5 rating scores. The first data set is MovieLens 1M (ML1M) 2 which contains 6040 users and 3952 movies. Following [17], we split the data set into two parts, 90% is used for training and 10% is used for test. The Book, Movie and Music data h ttp://grouplens.org/datasets/movielens/ sets are crawled from Douban 3 . We use 70% of the data for training, 10% for validation and 20% for test.

The three data sets of implicit feedbacks are also crawled from Douban. They are sampled to guarantee that each user has at least 10 actions and each item is observed by at least 20 users. The data sets have a lot of implicit feedbacks such as  X  X ead a book but not give a comment X ,  X  X ish to read a book but not read X , etc. All the observed (user, item) pairs are recorded as 1s and others are recorded as 0s. In each item recommendation experiment, we use 50% of the data for training, 10% for validation and 40% for test.
We use several evaluation metrics to study the recommen-dation performance, including RMSE (Root Mean Square Error) for rating prediction, and MRR (Mean Reciprocal Rank), Precision, Recall, F1 and NDCG (Normalized Dis-counted Cumulative Gain) for item recommendation [13,21]. RMSE is defined as where e R u,i is the predicted rating value of u to i and D is the test data.
 MRR is defined as where U test are users in test data and RR u is the recipro-cal rank of user u , i.e., RR u = 1 /min i  X  I test predicted ranking list for user u .

We define LI  X  u as the top-k recommended item list for user u and LI u the item list of user u in test data. P recision and Recall u @ k are defined as P recision u @ k = | LI u | definitions for precision and recall are as follows, The F 1 score is defined as NDCG is defined as where N DCG u @ k = 1 Y DCG u @ k score for user u , and t i is 1 if the item at i is hit and 0 otherwise.
For rating prediction using explicit feedbacks, we study the following six methods. h ttp://www.douban.com/
For item recommendation using implicit feedbacks, we study the following three methods.
We first describe the parameter settings for rating predic-tion using explicit feedbacks. For KNN, we search the neigh-borhood size from { 5 , 10 , 15 , ..., 110 } . For HPMF, UGMF, IGMF and HGMF, we fix the number of levels as L = 2 for fair comparison, and thus have HPMF-2, UGMF-2, IGMF-2 and HGMF-2. We also study the performance of HGMF with L = 3, i.e., HGMF-3. For PMF, HPMF-2, UGMF-2, IGMF-2, HGMF-2 and HGMF-3, we search the regulariza-when d = 10. We also study the recommendation perfor-mance when d = 20. For all factorization methods, we fix the learning rate as  X  = 0 . 0005. For the first variant of the clus-tering algorithm, we set the threshold as threshold ( i ) = 0 . 1, and for the second variant of the clustering algorithm, we use of HGMF-2 and HGMF-3 are shown in Table 2.

For experiments of item recommendation using implicit feedbacks, we fix the dimension of latent factors as d = 20 and the learning rate as  X  = 0 . 1. For all algorithms, we and fix the size the recommendation list as k = 10. We search the outer iteration number from T  X  { 1 , 2 , 3 , ..., 40 } using the validation data. For the first variant of the clus-tering algorithm, we set the threshold as threshold ( i ) = 0 . 1, and for the second variant of the clustering algorithm, we use of HGMF-2 are shown in Table 3.
M Table 3: The number of groups of implicit feedbacks. The recommendation performance on RMSE are shown in Table 4. From Table 4, we have the following observations:
In a summary, we can see that a complete combination of four correlations as constructed by our clustering algorithm is very effective.

We also study the time cost of those methods. For a fair comparison, we implement all algorithms in MATLAB. All the experiments are conducted on a computer with Intel(R) Xeon(R) E5620 @ 2.40GHz CPU and 24GB RAM. The re-sults are shown in Figure 5.4.1. We can see In a summary, both HGMF-2 and HGMF-3 are efficient and comparable as compared with the PMF.
We study the item recommendation performance using implicit feedbacks by integrating BPR into the basic MF and HGMF. Because both HGMF-2 and HGMF-3 give bet-ter results in rating prediction using explicit feedbacks, we only adopt HGMF-2 and the second variant of the cluster-ing algorithm for simplicity, i.e., HGMF-2 c 2 . We show the results in Table 5. From Table 5, we can see, We also study the the performance of BPR-MF and BPR-HGMF-2 c 2 with different recommendation size, i.e., different values of k in top-k recommendation. We show the results on Douban Book in Figure 5.4.1. From Figure 5.4.1, we can see that as the recommendation size increases, F1 and Recall scores also increase while Precision and NDCG decrease; and for different values of k , our BPR-HGMF-2 c 2 is always better than BPR-MF.
In this paper, we propose a novel and generic algorithm, i.e., hierarchical group matrix factorization (HGMF), for mod-eling structure correlations among users and items. Specif-ically, we integrate four types of correlations into plain ma-trix factorization in a principled way, including (user, item), (user, item group), (user group, item) and (user group, item group). Furthermore, we design a novel clustering algo-rithm in order to construct the hierarchical structure cor-relations. Experimental results on several real-world data sets for rating prediction and item recommendation show that our HGMF performs better than some state-of-the-art methods.
 For future works, we are interested in generalizing our HGMF in two aspects, including (i) collectively mining com-plex structure correlations from heterogeneous domains [3], and (ii) incorporating auxiliary data such as social networks and mobile context [7,8].
This research is supported by the National Natural Sci-ence Foundation of China (NSFC) No. 61272303, National Basic Research Program of China (973 Plan) No. 2010CB327903 and Natual Science Foundation of SZU No. 201436. [ 1] Dimitri P Bertsekas. Nonlinear programming. 1999. [2] David M Blei and Jon D McAuliffe. Supervised topic [3] Bin Cao, Nathan Nan Liu, and Qiang Yang. Transfer [4] Thomas George and Srujana Merugu. A scalable [5] Yehuda Koren. Factorization meets the neighborhood: [6] Yehuda Koren, Robert Bell, and Chris Volinsky. [7] Nathan N. Liu, Luheng He, and Min Zhao. Social [8] Qi Liu, Haiping Ma, Enhong Chen, and Hui Xiong. A [9] Hao Ma, Irwin King, and Michael R. Lyu. Learning to [10] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin [11] Ali Mashhoori and Sattar Hashemi. Incorporating [12] Aditya Krishna Menon, Krishna-Prasad Chitrapura, [13] Weike Pan and Li Chen. Cofiset: Collaborative [14] Weike Pan and Li Chen. Gbpr: Group preference [15] Steffen Rendle. Factorization machines with libfm. [16] Steffen Rendle, Christoph Freudenthaler, Zeno [17] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic [18] Badrul M Sarwar, George Karypis, Joseph Konstan, [19] Hanhuai Shan, Jens Kattge, Peter Reich, Arindam [20] Amit Sharma and Baoshi Yan. Pairwise learning in [21] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, [22] Chong Wang and David M Blei. Collaborative topic [23] Quan Wang, Zheng Cao, Jun Xu, and Hang Li. Group [24] Markus Weimer, Alexandros Karatzoglou, and Alex [25] Bin Xu, Jiajun Bu, Chun Chen, and Deng Cai. An [26] Yi Zhen, Wu-Jun Li, and Dit-Yan Yeung. Tagicofi: [27] Erheng Zhong, Wei Fan, and Qiang Yang. Contextual
