 Hua-Yan Wang wanghy@cis.pku.edu.cn Qiang Yang qyang@cse.ust.hk Hong Qin qin@cs.sunysb.edu Hongbin Zha zha@cis.pku.edu.cn Compositional data (positive constant-sum real vec-tors) are frequently encountered in various scientific disciplines and industrial applications. They quantita-tively describe the parts that comprise the entire en-tity. In geology, scientists investigate relative propor-tion of different minerals in rocks. In microeconomics, household expenditure in different commodity/service groups is recorded as relative proportion. In informa-tion retrieval, documents are usually represented as relative frequencies of words in a prescribed vocabu-lary. Generally, compositional data are natural repre-sentations when the variables (features) are essentially probabilities of complementary and mutually exclusive events. The variables (features) in compositional data are referred to as components in this paper. Feature extraction is often applied in machine learning when the datasets are large and complex. The same is needed for compositional data. The need for feature extraction arises from four aspects. First, prediction performance in classification and regression can ben-efit from a lower dimensional representation with de-correlated components to avoid the curse of dimension-ality. Second, feature extraction may improve over-all domain understanding, e.g., we could expect the learned components to represent latent independent sources from which the data are generated. Third, the computational expense of subsequent data processing can be reduced with a lower dimensionality. Finally, reducing data to two or three dimensions facilitates visualization and further analysis by domain experts. However, traditional feature extraction techniques are not suitable for compositional data due to several reasons. First, the traditional measurement of  X  X or-PCA only captures a linear relationship between two random variables. In contrast, the  X  X urved X  nature (Aitchison, 1983) of compositional data and the  X  X pu-rious correlation X  (Pearson, 1896) induced by the constant-sum constraint make it problematic to inter-pret correlation as merely a linear relationship. We thus need a new concept of  X  X orrelation X  for compo-sitional data. Second, the positive and constant-sum constraints for compositional data are not considered in most dimensionality reduction techniques, and sim-ply modifying them to accommodate these constraints may induce biases.
 PCA is one of the most widely used techniques for feature extraction. Given a target dimension k , PCA identifies an orthogonal projection to a k dimensional subspace that maximizes the estimated Gaussian vari-ance of the projected data. Moreover, the covariance matrix is diagonalized such that the variables are de-correlated. Our approach adapts this framework for compositional data. In particular, we first identify a family of projections that preserve a simplex con-straint as substitutes for the orthogonal projections in PCA. Then, we find an optimal projection that mini-mizes the  X  X irichlet correlation X  among the projected components , as a substitute for maximizing the esti-mated Gaussian variance in PCA. The Dirichlet cor-relation among the components is defined as the esti-mated Dirichlet precision on projected data. The com-ponents are better de-correlated and separated with a smaller Dirichlet correlation. The notion of Dirichlet correlation extends the traditional  X  X inear X  interpreta-tion of correlation connoted in the covariance structure of multivariate Gaussian and PCA. Because of our ap-proach X  X  affinity to the Dirichlet distribution, we call it Dirichlet component analysis (DCA) .
 Although the Dirichlet distribution is a natural para-metric family on the simplex, its role in modeling compositional data is not well studied. As pointed out in (Aitchison, 1982), the  X  X ltimate independence X  property of the Dirichlet family prevents us from di-rectly applying it to model compositional data. Con-sequently, the use of Dirichlet family in compositional data analysis has been superseded by the log-ratio framework (eliminating the constraints by a transfor-mation to R N ) originated from (Aitchison, 1982). For example, the centered log-ratio is defined as dividing all components by their geometric mean and then ap-plying the log function. Although this framework has been very successful, it has certain problems. The log-ratio well captures variability in the central area of the simplex, but encounters singularity in periph-eral areas. For example, in sparse compositional data (e.g., term frequencies in documents with thousands of terms) the log-ratio is not well defined as most de-nominators would be zero.
 In this paper, we make three main contributions. 1) We identify a rich family of dimensionality reduction transformations for compositional data, as an alterna-tive to existing compositional operators such as sub-composition , amalgamation , and partition (Aitchison, 1982). 2) We exploit the Dirichlet family for composi-tional data analysis to capture data variability beyond traditional concepts of statistical correlation. 3) We show that the entire framework of DCA is effective and conceptually succinct, and validate its effectiveness on two synthetic datasets and two real-world datasets. 2.1. The Projection Family Compositional data are positive constant-sum vectors. Without loss of generality, we assume all components to sum to one: x = ( x 1 , x 2 ,  X  X  X  , x N ) T , x i  X  0 for all i, All points satisfying these constraints constitute the ( N  X  1)-simplex, denoted as S N . As low dimensional examples, S 3 is a triangle and S 4 is a tetrahedron. Given a target dimension K ( K  X  N ), our first aim in dimensionality reduction is to identify a family of projections from S N to S K .
 Proposition 1 For linear projections y is in S K for all x in S N if and only if 1) r ij  X  0 for all i, j . 2) The proof is quite straightforward and we omit it here for brevity.
 Such projections could be viewed as rearranging mass from the N original components to the K new compo-nents , while the law of conservation of mass is satisfied. Hence we refer to such linear transforms from S N to S K as rearrangements .
 Unfortunately, we could have degenerate rearrange-ments when some rows of R are close to zero and as a result, the corresponding new component is al-most ignored in the rearranging process. Without a priori knowledge we should treat the K new compo-nents equally, which gives rise to the family of balanced rearrangements : Definition 1 (Balanced Rearrangement) A lin-ear projection R x = y is a balanced rearrange-ment , if R = ( r ij ) K  X  N satisfies: 1) r ij  X  0 for all i, j . 2) 3) The balanced is described by the following proposition, which gives rise to a univariate (symmetric) Dirichlet family, as we will discuss in Section 2.2.
 Proposition 2 If R K  X  N is a balanced rearrangement matrix, x is a random vector in S N satisfying E ( x i ) = N for all i , then y = R x is a random vector in S K and E ( y i ) = 1 K for all i .
 The proof is straightforward given the linearity of the expectation operator.
 The space of balanced rearrangement projections from S N to S K is a NK  X  N  X  K + 1 dimensional vector space, which is closed with respect to the operator of weighted average. This property is useful in developing the optimization algorithm in Section 3: Proposition 3 If R 1 and R 2 are balanced rearrange-ment matrices,  X  and  X  are positive real numbers, then (  X  R 1 +  X  R 2 ) / (  X  +  X  ) is a balanced rearrangement ma-trix.
 This is easy to validate from the definition of balanced rearrangement.
 A noticeable property of balanced rearrangements is the  X  X hrinking effects X  stated as follows: Proposition 4 Let min( x ) be the minimum compo-nent of x . R K  X  N is a balanced rearrangement matrix with K  X  N , then min( Rx )  X  min( x ) for all x in S N . The proof is obvious as long as we notice that each component of Rx is N/K times a weighted average of the components of x , where equality holds only in some trivial cases. For example, R is the identify matrix or x = (1 /N, 1 /N,  X  X  X  , 1 /N ).
 Intuitively, Proposition 4 states that the balanced rear-rangements always make data points  X  X hrink X  toward the central area of the simplex, which is undesirable this problem, we induce the regularization operator for compositional data. As shown in Figure 1, we impose on the data points a parallel move along the direction x 1 = x 2 =  X  X  X  = x N , and then project the data points back to the simplex by radial projection: Definition 2 (Regularization) Given a com-positional dataset X = { x 1 , x 2 ,  X  X  X  , x M } , a regularization on the dataset is de-noted as: e X = { f x 1 , f x 2 ,  X  X  X  , g x M } , where e i = 1 , 2 ,  X  X  X  , M , and the regularization factor  X  = min(min( x 1 ) , min( x 2 ) ,  X  X  X  , min( x M )) . The regularization operator can be viewed as a  X  X cal-ing X , which preserves Euclidean geometrical properties such as distance (allowing a constant scaling factor) and angle. Intuitively it  X  X xpands X  the data points and compensates for the  X  X hrinking effect X  of balanced rearrangements. Its usefulness will be illustrated in a toy example in Section 2.3.1. 2.2. Dirichlet Correlation The Dirichlet distribution (3) is conjugate prior of the multinomial, which is quite natural for compositional data arisen from independent components .
 Parameters  X  = (  X  1 ,  X  2 ,  X  X  X  ,  X  N ) could be summarized by the Dirichlet precision mean (  X  1 ,  X  2 ,  X  X  X  ,  X  N ) / actually encodes the expectation of each component : Without domain knowledge, we assume the compo-nents in original data to be equally important. Ac-cording to Proposition 2, the feature extraction pro-cess should not  X  X refer X  any new component . We therefore adopt a uniform Dirichlet mean: The traditional concept of  X  X orrelation X  (Pearson product-moment correlation coefficient) encodes linear relationships between components (variables). With strong linear relationships, some components are re-dundant and the total amount of information declines. In information theory, the amount of information is measured by  X  X ncertainty X  of a distribution. The Gaussian distribution with larger variances is more  X  X ncertain X , thus is preferred in PCA. For the Dirich-let distribution (5), a smaller  X  0 indicates higher  X  X n-certainty X  (amount of information) and less  X  X orrela-tion X  among the components (see Figure 2), which co-incides with the traditional statistical interpretation of  X  X orrelation X . Hence we define correlation for compo-sitional data in terms of  X  0 : Definition 3 (Dirichlet Correlation) Given i.i.d. compositional data set X = { x 1 , x 2 ,  X  X  X  , x M } arisen from (5), the Dirichlet correlation among the components with respect to X is defined as the maximum likelihood estimation of  X  0 .
 Note that  X  0 is the overall (not pairwise)  X  X orrelation X  among all components. The intuitive interpretation of the Dirichlet correlation is shown in Figure 2: 1) when  X  0 &gt; 1, the distribu-tion is bump-shaped, where the components are highly correlated and are likely to mix together in samples; 2) when  X  0 = 1, the distribution is uniform, and any proportion of mixture is equally preferred; 3) when  X  0 &lt; 1, the distribution is valley-shaped with peaks at simplex vertices, and the components are better de-correlated such that the components present them-selves as more purified elements in the data samples. With the specially designed transform family and cor-relation measure for compositional data, we define Dirichlet component analysis (DCA) as follows: Definition 4 (Dirichlet Component Analysis) Given i.i.d. compositional data set X = { x 1 , x 2 ,  X  X  X  , x M } with N components, and the target dimension K , Dirichlet component analy-sis (DCA) applies a balanced rearrangement  X  R K  X  N and a regularization on X to minimize the Dirichlet correlation among the resulted K components: where ^ R ( X ) denotes that we first apply balanced re-arrangement R to X , and then apply a regularization according to Definition 2. The i.i.d. assumption is for factorization of the joint likelihood. The optimization problem will be discussed in Section 3. 2.3. Illustrative Examples 2.3.1. Example 1: Composition of Rocks in In this example, suppose that some rock samples are collected in a geological study in an attempt to ana-lyze their composition. Original representation of each rock sample is a point in S 3 (see Figure 3 left) indicat-ing relative proportion of 3 minerals. The data points demonstrate three peaks that correspond to three sub-stances that have fixed compositions in terms of the minerals. These peaks are formed because the forma-tion of different substances depends on certain geolog-ical factors that vary from site to site. Hence a par-ticular substance tends to dominate rock samples col-lected from some particular site. The substances had been decomposed by the chemical tests on the rocks, so that we only observe proportions of minerals. Given the target dimension of three, DCA obtains a new representation of the rock samples (see Fig-ure 3 right). The learned new components correspond to three underlying substances in the rock samples. Three peaks are found near the vertices of the sim-plex, which indicates that the new components are  X  X e-correlated X  in the sense that the samples tend to be explained by individual components instead of linear combinations of multiple components . This effect of de-correlation could be interpreted analogous to PCA. In PCA, we diagonalize the covariance matrix in order that variance in data is separately  X  X xplained X  by in-dividual variables rather than linear combinations of multiple variables. In our representation, we reveal more information about the rocks X  substances because the individual components are easier to explain in fur-ther statistical analysis. In contrast, we note that PCA cannot be used to solve this rock analysis problem be-cause by its nature this problem cannot be resolved through an orthogonal transformation. 2.3.2. Example 2: Term Frequencies in We consider a simplified bag-of-words model for docu-ment retrieval, where relative frequency values of four terms are counted in a set of documents. Each docu-ment is represented as a point in S 4 (see Figure 4 left). Predictably, many documents would mention both  X  X conomy X  and  X  X arket X  a lot, and many documents would mention both  X  X errain X  and  X  X eography X  a lot, which gives rise to two ridge-shaped modes, corre-sponding to two underlying classes in these documents (one concerns economical issues, and the other dis-cusses geological issues). Reducing the dimensionality is very likely to boost the prediction performance in classification tasks because it helps avoid overfitting (the curse of dimensionality), especially in more so-phisticated high-dimensional document datasets. Given the target dimension of two, DCA identifies two latent components (see Figure 4 right). The projection actually merges two pairs of semantically close compo-nents , and the resulting representation best preserves the information that distinguishes the two classes. Note that as an unsupervised approach, DCA can-not see any class label X  X ll it does is minimizing the Dirichlet correlation. Although applying PCA to this toy case may have similar effects, our approach greatly outperforms PCA in higher dimensional cases, be-cause it is specially designed for compositional data (as shown on a real-world dataset in Section 4.2). The optimization problem of DCA as defined in (6) lacks an explicit analytical loss function. Moreover, the regularization operator adds to the difficulty in identifying gradients or judging convexity in the pa-rameter space.
 Maximum likelihood estimation of Dirichlet precision can be carried out efficiently (Minka, 2003). The solu-tion space is closed with respect to weighted average (Proposition 3), which motivates us to use the genetic algorithm (Goldberg, 1988), in which the weighted av-erage serves as the crossover operator 3 . Although genetic algorithm is generally inefficient, it is still tractable with additional acceleration tricks. Never-theless, genetic algorithm is just one of many choices in the optimization of DCA.
 The algorithm is formalized in Algorithm 1, where  X  BR  X  is abbreviation for balanced rearrangement ma-trix;  X  DC  X  is abbreviation for Dirichlet correlation;  X  X AX X  is the maximum number of iterations allowed;  X  X IZE X  is the size of population. The fitness score is Algorithm 1 Genetic Algorithm for DCA Input: dataset X  X  S N , target dimension K
Initialize population of BR , denoted as P 0 . for iter = 0 to MAX  X  1 do end for computed as: where  X  X edian DC  X  is the median Dirichlet correla-tion in current population. This is a key trick to ac-celerate the algorithm, because it prunes half of the population by assigning zero fitness scores. The prun-ing is based on the intuitive observation that: 1) the regularization factor is a continuous function of the BR matrix given the dataset X ; 2) the Dirichlet precision is a continuous function of the regularization factor and BR matrix. Hence the target function is approx-imately continuous and smooth in the solution space. Retaining 50% good candidate solutions in each gen-eration is sufficient. Since the total diversity of the population diminishes, the population size could be reduced accordingly in each iteration. 4.1. The Llobregat River Basin We investigate the hydrogeochemistry dataset from DCA. This dataset had been studied in (Tolosana-Delgado, 2005) using factor analysis under the log-ratio framework, with which they obtained inter-pretable latent factors. Applying our approach on this dataset yields even more interesting results. The dataset consists of 485 samples, each being a 14 di-mensional compositional vector representing the con-HCO  X  3 , etc.) in the water samples. These samples are collected monthly over a certain period of time from 31 sites in the Llobregat River Basin. We classify the sites into two categories: upstream and downstream , separated by the red line (see figure 5). The 485 water samples are also classified into two categories accord-ing to the site from which they had been collected. DCA is applied on this dataset with a target dimen-sion of three to facilitate visualization. Visualization of high-dimensional data is crucial in disciplines such as geology, chemistry, etc., because it facilitates fur-ther analysis by domain experts.
 Interestingly, although there is no location informa-tion in this dataset (locations are known from labels unseen for DCA), the two categories are well sepa-rated in the latent representation (see Figure 6). This underlying pattern is attributable to various geolog-ical and anthropogenic factors thoroughly described in (Tolosana-Delgado, 2005), which we omit here for brevity. These new patterns that are discovered by DCA was not reported in (Tolosana-Delgado, 2005), a fact highlighting the power of DCA in knowledge discovery.
 4.2. Twenty newsgroups dataset fication task of the  X  X lt X  class (798 documents) versus the  X  X isc X  class (965 documents). We show that our approach avoids overfitting and improves the predic-tion accuracy when we train the classifiers with a very small number of training examples, in which case the problem of overfitting could be the severest. In the preprocessing step, the  X  X top words X  and scarce words with less than 10 total occurrences are removed. Thus the dataset we used consists of 1763 documents, where each document is represented by a 2711 dimensional sparse vector of relative word frequency values, which satisfy the constant-sum constraint.
 The dimensionality is reduced to K with DCA, PCA, and LDA (latent Dirichlet allocation) (Blei, 2003), re-spectively. We then used a linear SVM to classify these low dimensional representations as well as the original high dimensional data for comparison. Per-formance results on the test test dataset are plotted with a varying number of training samples (see Fig-ure 7) for target dimensions of K = 10, 20, and 50. Different choices of training data may affect the pre-diction performance, especially in our case where the size of training set is very small. So the performance results in our experiments are averaged over 500 differ-ent random choices of the training set. The advantage of DCA in improving prediction is clear when com-paring to other techniques with the same target di-mensionality, especially with very small training sets. Although the highly specialized technique for bag-of-words data (LDA) beats our approach in some cases, these cases are extreme (the target dimension is 10 and the number of training samples exceeds 30). The results also justify the applicability of DCA on sparse compositional data, for which the traditional log-ratio framework (Aitchison, 1982) is not applicable. To make the optimization step tractable for high-dimensional data, we employed several variations in implementation. In order to reduce the solution space, we used a restricted family of transformations rather than general rearrangements. The restricted family is  X  X malgamations X  (binary rearrangement matrices) introduced in (Aitchison, 1982), and the  X  X alanced X  requirement is not imposed. This is inspired from the toy example in Section 2.3, for which the optimal re-arrangement matrix is actually an amalgamation. Feature extraction for de-correlating and reducing variables date back to K. Pearson X  X  original idea (Pear-son, 1901) on PCA. There have been a large body of research papers in the statistics and machine learn-ing literature that address this issue, including ICA (Hyv  X arinen, 2001), kernel PCA (Sch  X olkopf, 1998), etc. Directed and undirected graphical models (Blei, 2003; Welling, 2004) have also been exploited to handle this problem, where they treat the target variables as latent nodes in the graph. Besides, the manifold assumption motivates a family of non-linear methods (Tenenbaum, 2000; Roweis, 2000), in which they use coordinates on the manifold to encode original high dimensional data. Statistical analysis of compositional data has received a lot of concern since J. Aitchison X  X  seminal work (Aitchison, 1982). The author proposed to trans-form from S N to R N +1 by log-ratio functions, and transplanted PCA to S N under the log-ratio frame-work (Aitchison, 1983). Our approach is an alterna-tive PCA-like technique on S N , which focuses on dif-ferent statistical properties (Dirichlet correlation) of data. Moreover, the log-ratio is not well-defined for sparse compositional data. In contrast, our approach do not have this problem. Algebraic-geometric struc-tures (Pawlowsky-Glahn, 2001) on the simplex had been investigated, which facilitate analysis of relation-ship among compositional data points. Unsupervised metric learning for compositional data had been ad-dressed in the machine learning literature (Lebanon, 2003; Wang, H.-Y., 2007). A major unresolved issue in the DCA framework is the theoretical implication of the regularization operator (see Figure 1), which is not compatible with the pop-ular log-ratio framework, because it does not preserve the ratio between different components . Nevertheless, the regularization operator preserves Euclidean geo-metrical properties such as distance (allowing a con-stant scaling factor) and angle. Although these prop-erties are not emphasized in the log-ratio framework, they are nonetheless meaningful as long as classifica-tion or regression tasks are concerned.
 This work was supported in part by NKBRPC No. 2004CB318000, NHTRDP 863 Grant No. 2006AA01Z302, and No. 2007AA01Z336. Qiang Yang thanks Hong Kong CERG grants 621307 and and CAG grant HKBU1/05C.
 Aitchison, J. (1982).  X  X he statistical analysis of com-positional data X , Journal of the Royal Statistical
Society. Series B (Methodological) , Vol.44, No.2, pp.139-177 Aitchison, J. (1983).  X  X rincipal component analysis for compositional data X , Biometrika , Vol.70, No.1, pp.57-65 Blei, D.M., Ng, A.Y., Jordan, M.I. (2003).  X  X atent Dirichlet allocation X , Journal of Machine Learning Research , 3, pp.993-1022 Goldberg, D.E. and Holland, J.H. (1988)  X  X enetic Al-gorithms and Machine Learning X , Machine Learning Volume 3, Numbers 2-3.
 Hyv  X arinen, A., Karhunen, J., Oja, E. (2001).  X  Inde-pendent Component Analysis  X , John Wiley &amp; Sons. Lebanon, G. (2003).  X  X earning Riemannian metrics X , In Proceedings of the 19th UAI Minka, T.P. (2003)  X  X stimating a Dirichlet distribu-tion X , Technical Report, Microsoft Research Pawlowsky-Glahn, V. and Egozcue, J.J. (2001).  X  X e-ometric approach to statistical analysis on the simplex X , Stochastic Environ. Res. Risk Assess. (SERRA) , v.15, no.5, pp.384-398.
 Pearson, K. (1896).  X  X n a form of spurious correla-tion which may arise when indices are used in the measurements of organs X , Proceedings of the Royal Society of London , 60, pp.489-502 Pearson, K. (1901).  X  X n lines and planes of closest fit to systems of points in space X , Philosophical Maga-zine , 2, (6), pp.559-572 Roweis, S. and Saul, L. (2000).  X  X onlinear Dimension-ality Reduction by Locally Linear Embedding X , Sci-ence , 290, pp.2323-2326 Sch  X olkopf, B., Smola, A., M  X uller, K.R. (1998).  X  X on-linear component analysis as a kernel eigenvalue problem X , Neural Computation , Vol 10, p.1299-1319, Tenenbaum, J., Silva, V., Langford, J. (2000).  X  X  global geometric framework for nonlinear dimen-sionality reduction X , Science , 290, pp.2319-2323 Tolosana-Delgado, R., Otero, N., Pawlowsky-
Glahn, V. and Soler, A. (2005).  X  X atent Composi-tional Factors in the Llobregat River Basin (Spain) Hydrogeochemistry X , Mathematical Geology , Vol.37, No.7, pp.681-702 Wang, H.-Y., Zha, H., Qin, H. (2007).  X  X irichlet ag-gregation: unsupervised learning towards an opti-mal metric for proportional data X , In Proceedings of the 24th ICML Welling, M., Rosen-Zvi, M., Hinton, G. (2004).  X  X x-ponential family harmoniums with an application to
