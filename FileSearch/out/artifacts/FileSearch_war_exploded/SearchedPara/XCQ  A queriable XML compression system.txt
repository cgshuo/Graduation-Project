 REGULAR PAPER Wilfred Ng  X  Wai-Yeung Lam  X  Peter T. Wood  X  Mark Levene Abstract XML has already become the de facto standard for specifying and ex-changing data on the Web. However, XML is by nature verbose and thus XML documents are usually large in size, a factor that hinders its practical usage, since it substantially increases the costs of storing, processing, and exchanging data. In order to tackle this problem, many XML-specific compression systems, such as XMill, XGrind, XMLPPM, and Millau, have recently been proposed. However, these systems usually suffer from the following two inadequacies: They either sacrifice performance in terms of compression ratio and execution time in order to support a limited range of queries, or perform full decompression prior to pro-cessing queries over compressed documents.
 provided by a Document Type Definition (DTD) associated with an XML doc-ument. We show that a DTD is able to facilitate better compression as well as generate more usable compressed data to support querying. We present the ar-chitecture of the XCQ, which is a compression and querying tool for handling XML data. XCQ is based on a novel technique we have developed called DTD Tree and SAX Event Stream Parsing (DSP). The documents compressed by XCQ are stored in Partitioned Path-Based Grouping (PPG) data streams, which are equipped with a Block Statistics Signature (BSS) indexing scheme. The indexed PPG data streams support the processing of XML queries that involve selection and aggregation, without the need for full decompression. In order to study the compression performance of XCQ, we carry out comprehensive experiments over a set of XML benchmark datasets.
 Keywords XML  X  Document type definitions  X  Compression algorithms  X  Query processing  X  Performance 1 Introduction The Extensible Markup Language (XML) [ 6 ] is proposed under the auspices of the World Wide Web Consortium (W3C) as a standardized data format designed for specifying and exchanging data on the Web. With the proliferation of mobile devices, such as pocket PCs, sensor networks, and mobile phones, as means of communication in recent years, it is reasonable to expect that in the foreseeable future a massive amount of XML data will be generated and exchanged between applications in order to perform dynamic computations over the Web.
 a pressing issue from a design perspective. However, in practice XML documents are usually extremely large in size, due to the fact that they often contain much re-dundant data, such as repeated tags (for example, see the DBLP documents [ 30 ]). As a result, an XML-ized document is usually much larger than one conveying the same information but adopting a standard document format. For example, an XML-ized Weblog document in [ 31 ] is roughly three times the size of the original file.
 flation problem seriously hinders the future use of XML in exchanging, parsing, and querying data, due to the fact that the data size grows much faster than the communication bandwidth. On the one hand, we enjoy the flexibility of XML, since the markup facilities of XML are intuitive for people and better able to fa-cilitate web data exchange. On the other hand, we have to pay the extra cost of consuming more storage space and computational resources to store and process XML data.
 make effective use of the information provided by a Document Type Definition (DTD) associated with an XML document [ 2 , 7 , 11 , 31 , 35 , 45 ], or do not support the querying of compressed documents directly [ 11 , 29 , 31 , 42 ].
 ment and help to produce more usable compressed data. For example, XMill [ 31 ] needs to perform a full decompression prior to processing queries over compressed documents, resulting in a heavy burden on system resources such as CPU pro-cessing time and memory consumption. At the other extreme, some technologies can avoid (full) XML data decompression but unfortunately only at the expense of compression performance. For example, XGrind [ 45 ] adopts a homomorphic transformation strategy to transform XML data into a specialized compressed for-mat and support direct querying on compressed data but at the expense of the compression ratio; thus, the inflation problem is not satisfactorily resolved. pression and Querying System (XCQ), which attempts to balance the objectives of tackling the inflation problem and supporting querying on compressed data without the burden of performing a full decompression. We develop the XCQ pro-totype and study the feasibility of using XCQ in practice. We evaluate the perfor-mance of XCQ in compression, and demonstrate that a competitive compression ratio, which is comparable to that of XMill [ 31 ], is achieved by XCQ at the ex-pense of compression time.
 form to a DTD by making use of the DTD information to aid the compression pro-cess. We achieve this using our DTD Tree and SAX Event Stream Parsing (DSP) technique. In addition, we propose and analyze two simple but effective techniques for handling queries over compressed XML data. First, we use a novel Partitioned Path-Based Grouping (PPG) strategy for storing path-based compressed XML data in a number of streams of blocks. Second, we impose a minimal indexing scheme, called a Block Statistics Signature (BSS), on the compressed data blocks. We show that these techniques are not only efficient enough to support selection and aggregation queries over compressed XML data via partial decompression, but they also require a low computation and storage space overhead.
 structure stream, derived from a given XML document and its DTD, is stored and compressed separately from the data streams. XMill [ 31 ] also divides an XML document into a number of separate containers, one for the structure and one for each attribute and element name used in the document. The PPG scheme differs from this in at least two ways: Firstly, the structure stream is encoded using infor-mation from the DTD; secondly, the data streams are based on paths in the DTD (as shown in Fig. 1 ) rather than simply names.
 as a DTD tree such as that shown in Fig. 2 ,where name is an attribute node ; author , title , year , publisher ,and num copy are element nodes ;and paper , course note ,and book are empty elements. (A full description of DTD trees is given in Sect. 3 .) The values in a data stream all have the same path back to the root of the DTD tree (or root of the XML document), as suggested in Figs. 1 and 2 .
 pre-defined block size , which helps to increase the overall compression ratio (cf. [ 26 , 31 , 36 , 37 ]). A data block in a PPG data stream is able to be compressed or decompressed as an individual unit. This partitioning strategy allows us to access the data in a compressed document by decompressing only those data blocks that contain the data elements relevant to the input query, which we call the strategy of partial decompression .
 records in the stream d 0 are packed in the first data block, while the next batch of n records are packed in the second block. As such, each data block in the data streams contains a certain number of elements in the order listed in their corre-sponding data stream.
 low-level compressor gzip [ 18 ]. Intuitively, a smaller block size (i.e., using a finer partitioning in a PPG data stream) improves query performance, since a more precise portion of the compressed document can be identified for decompression in order to evaluate a query. However, there is a trade-off in that a finer block partitioning degrades the compression ratio, since fewer redundancies in the data streams can be eliminated by gzip if each block is compressed as a finer individual block unit.
 streams when processing queries on XCQ compressed data. A BSS stream is suit-able for block-oriented compressed data, which includes parameters such as min and max generated for each data block. This signature summarizes the content in a compressed data block, which supports more effective  X  X itting X  of the target blocks in answering queries. The storage space overhead required by the BSS in-dexing scheme is low. We do not need to generate a bit pattern for each record in a data block as some compressors do. With respect to the computation overhead, the operations of generating and scanning a signature can be carried out in O ( n ) time. In the case of signature generation, n is the number of elements in a PPG data stream. In the case of signature scanning, n is the number of compressed data blocks in a PPG data stream.
 architecture of XCQ, which supports querying compressed documents. In Sect. 3 , we present in detail the DSP technique, outline its working principles, and discuss the parsing algorithm that realizes the technique. In Sect. 4 , we discuss the PPG strategy and the BSS indexing scheme used in XCQ. In Sect. 5 , we present the ex-perimental results of compressing real-world XML documents using XCQ, which compare with a range of state-of-the-art compressors. In Sect. 6 , we review related work and recent developments in XML compression. Finally, in Sect. 7 ,wegive our concluding remarks and discuss future research work pertaining to XCQ. 2 XCQ architecture In this section, we present the architecture of XCQ and discuss how the XCQ sys-tem supports querying over partially decompressed documents. The architecture of XCQ comprises the Compression Engine and the Querying Engine . This pro-totype is developed using C++, the SAX XML parser of [ 12 ], and the gzip library of [ 19 ].
 the following components: The DTD parser ,the DTD tree-building module ,the SAX parser ,the DSP module ,the partition and indexing module ,andthe compres-sion module .
 2.1 DTD parser and DTD tree-building modules The DTD parser module parses the input DTD document and analyses its content. The result is utilized by the DTD tree-building module to construct a DTD tree. Elements that are used in multiple content models in the DTD are represented by separate nodes in the tree. The tree-building module assumes that the DTD is non-recursive. 2.2 SAX parser This module uses the SAX parser in [ 12 ] in order to generate a SAX event stream [ 34 ] that corresponds to the given XML document. 2.3 DSP module This module implements the DSP algorithm, which we discuss in Sect. 3 . It takes as input the DTD tree and the SAX event stream created by the DTD tree-building module and the SAX Parser module, respectively. It outputs the corresponding structure stream and set of data streams. 2.4 Partition and indexing (BSS) module This module first partitions the incoming data streams, generated by the DSP mod-ule, into their corresponding sets of blocks. Each block contains a certain number of data elements belonging to its corresponding PPG data stream. The module then generates a BSS index for each data block in a PPG data stream. 2.5 Compression module The structure stream generated by the DSP module and the indexed PPG data streams generated by the partition and indexing module are compressed individu-ally and then packed and merged into a single file. The compression module also manages the data buffer in order to minimize disk access frequency. This module is built on top of the gzip compression libraries [ 19 ]. We could have used the bzip2 library of [ 40 ] as an alternative in this module. However, we found that, in general, bzip2 substantially increases both the compression and query response times. titioned Path-Based Data Grouping by only partially decompressing them. The underlying idea used in the engine is that it only decompresses portions of the compressed document that are relevant to the query evaluation. Figure 4 shows the architecture of the Querying Engine, which comprises the query parser ,the query processor ,andthe storage manager . We now briefly explain the functional-ity of each of these components below. 2.6 Query parser The query parser converts a query formulated in XPath [ 13 ] into an internal form used in XCQ. A set of tokens are used to describe the content, such as the pred-icates and the relevant elements, of the input query. The tokenized query is then fed into the query processor.
 2.7 Query processor The query processor is used to generate a set of access commands basedonthe input tokenized query. These access commands are used to instruct the storage manager to access the required portions of the compressed file. The query proces-sor then evaluates the input query based on the results returned from the storage manager. The result generated by the query processor is passed back to the storage manager, which outputs the result as an XML document. 2.8 BSS manager The BSS manager is responsible for checking the data block signatures (i.e., BSS indexes). When the engine is initialized, the BSS manager loads the BSS indexing information from the header of the input compressed document into main memory. It subsequently helps the storage manager to determine whether a compressed data block contains the required data elements based on its BSS index. 2.9 Storage manager The storage manager is responsible for instructing the operating system to access the compressed files. It also provides buffer management to minimize disk I/O overhead.
 XCQ above, we should mention that any XML document compressed with XCQ can be faithfully recovered by decompression (except possibly for those whites-pace characters that are not significant). Since both the structure stream and the data streams are compressed using gzip which is lossless, they can be recovered. The structure of the document can be reconstructed from the structure stream, as shown, for example, in [ 29 ]. Finally, since the data streams in XCQ are written out in document order, it is straightforward to reconstruct the original XML document from the structure, the data streams, and the DTD. 3 DTD tree and SAX event stream parsing In this section, we first give an overview of the DTD Tree and SAX Event Stream Parsing (DSP) technique and highlight those of its features that are desirable for XML compression. We then present the DSP algorithm and illustrate the idea with a detailed example. 3.1 Overview of DSP In DSP, we use a SAX event stream [ 34 ] and a DTD tree data structure together to model a given XML document. The generation of a SAX event stream is carried out by the XML parser in [ 12 ], whereas the creation of a DTD tree is carried out by the DTD tree-building module. We now illustrate our basic ideas about building a DTD tree.
 tion. A DTD tree for the document is built as follows. Each element that has a unary operator ( X ? X ,  X   X   X , or  X  +  X ) applied to it, such as  X  X ntry X  and  X  X ublisher X , is transformed into a corresponding operator node, with the element operand as part of the node, as shown in Fig. 2 . This is a shorthand representation for an operator node with a single child. Sequences of elements (separated by the  X , X  operator) are also represented implicitly by the ordering of child elements in the tree (as in Fig. 2 ) unless nodes corresponding to the  X , X  operator are required because of the complexity of the content model defined in the DTD.
 book) X , are transformed into a choice node with the corresponding elements as children. If the operands of  X  |  X  operator are expressions that are more complicated than a single element name, then more elaborate subtrees are built. An element having attributes, such as  X  X uthor X , is transformed into a tree node with the at-tributes associated with the node. PCDATA nodes are attached to those elements that are defined as  X #PCDATA X  in the DTD, such as  X  X itle X . The generated DTD tree is as shown in Fig. 2 .
 plements DSP as shown in Fig. 6 . The functions of this module are (1) to extract the structural information [ 29 , 42 ] from the input XML document that cannot be inferred from the DTD during the parsing process, and (2) to group data elements in the document based on their corresponding tree paths in the DTD tree. By structural information we mean the information necessary to reconstruct the tree structure of the XML document. By data elements we mean the attributes and PC-DATA within the document. The module parses the DTD tree by using a special traversal sequence, which depends on the SAX event stream in order to explore the required information. The output of this module is a stream of structural infor-mation, which we call the structure stream , and streams of XML data, which we call the data streams .
 detailed as follows. 1. Less memory is required . Since an XML document is converted into a SAX 2. Partial decompression is supported . Data values with related semantics that 3. No user expertise is required . The streams output from the DSP module can 3.2 DSP algorithm The DSP algorithm is implemented in the DSP module. It is used for realizing a Pseudo-Depth-First (PDF) traversal strategy, which explores the required informa-tion from the DTD tree and SAX event streams, in order to generate the structure and data streams.
 when traversing the DTD tree. In particular, the traversal path is determined on the fly based on the input SAX event stream. Using PDF traversal, the DSP module traverses the DTD tree in a depth-first traversal manner with respect to the input SAX event stream until an operator node or a choice node is encountered. It then determines the subsequent traversal path based on what node in the DTD tree the next SAX event matches.
 DTD tree be denoted by v . 1. If v is a PCDATA node, the DSP module first computes the path from the root 2. If v is an element node, the module process the attributes returned by the cur-3. If v is labeled with  X , X  the children of v are processed in depth-first order. 4. If v is labeled with  X ? X  then if the current SAX event matches a descendant of 5. If v is labeled with  X   X   X  X r X  +  X  then if the current SAX event matches a de-6. If v is labeled with  X  |  X  (a choice node) then the current SAX event must match 3.3 Example execution of the DSP algorithm We now illustrate further the underlying ideas of the DSP algorithm by using the example XML document of Fig. 7 , which conforms to the DTD given in Fig. 5 . When the document is parsed, the stream of SAX events shown in Fig. 8 is gen-erated. The DTD tree and the SAX event stream are then processed by the DSP module. In Fig. 9 we show the DSP process, which starts from the DTD tree X  X  root node (i.e., the  X  X ibrary X  node).
 To k e n 0 in Fig. 8 ), matches the current DTD tree node (an element node), the module traverses to the subtree of the  X  X ibrary X  node in a depth-first manner using path P 1 in Fig. 9 . Hence, the  X  X ntry  X   X  node becomes the current DTD tree node. The module then processes the second SAX event token in the SAX event stream. put, the value of which depends on whether the current SAX event token matches the current DTD tree node. As the second SAX event token is an  X  X ntry X  start-element event (i.e., a match), the module outputs a 1-bit and then traverses the path P 2 as shown in Fig. 9 .
 element event (i.e., To k e n 2 in Fig. 8 ). Since the occurrence of this element is required by the DTD, nothing is output to the structure stream. However, since  X  X uthor X  possesses a  X  X ame X  attribute, the attribute value, which comes with brary/entry/author/@name X . The module then receives an  X  X uthor X  end-element event (i.e., To k e n 3 in Fig. 8 ) and traverses to the next child node of the  X  X ntry  X   X  node (i.e., the  X  X itle X  node).
  X  X itle X  start-element event (i.e., To k e n 4 in Fig. 8 ), matches the current DTD tree node, it traverses to its subtree and reaches the PCDATA node using path P 4 . The module then expects a PCDATA event. When this event occurs with value  X  X omp123: Operating Systems -Introduction X  (i.e., To k e n 5 in Fig. 8 ), the value is output to the data stream whose path is  X /library/entry/title/text() X . The module then receives a  X  X itle X  end-element event (i.e., To k e n 6 in Fig. 8 )andtraversesto the next child node of the  X  X ntry* X  node (i.e., the  X  X ear X  node) using the path P 5 .  X  X itle X  node. The module then reaches the  X  X ublisher? X  node and waits for the next SAX event to occur. As the  X  X ublisher X  node is labeled with an optional operator  X ? X , a bit is output to the structure stream. Since the next incoming SAX event is not a  X  X ublisher X  start event but rather a  X  X ourse note X  start event (i.e., To k e n 1 0 in Fig. 8 ), the module outputs a 0-bit. The module then traverses along path P 8 to the next child of the  X  X ntry* X  node, which is the choice ( X  |  X ) node. node matches the current SAX event. In this example, the module finds it is the second child, which has an index 1, so it outputs a byte with value 1 to the structure stream. The module then traverses path P 9 ,followedbypath P 10 when it receives a  X  X ourse note X  end element event. The  X  X um copy X  node is then processed in a similar manner to the  X  X itle X  node.
 node using path P 12 and waits for the next SAX event to occur. Since the next SAX event token is another  X  X ntry X  start-element event, the process repeats in the manner described above. The PDF traversal continues until all the tokens in the SAX event stream are processed. After the DSP process is finished, a set of output streams that correspond to the structural information and the path-based grouped data values (PCDATA and attribute values) of the input XML document will have been generated. 4 Partitioned path-based grouping In this section, we discuss the PPG strategy, which is adopted to support partial decompression of a compressed document during query evaluation. We impose a minimal indexing scheme over compressed XCQ documents in order to facilitate better query processing. Finally, we present a cost analysis of PPG when evaluat-ing selection and aggregation queries. 4.1 PPG data streams and BSS indexing As we have discussed in Sect. 3 , the DSP module outputs the data elements of an XML document to their corresponding data streams based on their tree paths in the DTD tree . Within the partitioned path-based data grouping, XCQ also partitions the data streams into their corresponding sets of blocks, as shown in Fig. 1 . Each of these data blocks can be compressed or decompressed as an individual unit. This partitioning strategy helps the underlying generic text compressor in XCQ to explore and eliminate redundancies in the input data when these streams are compressed individually, thus increasing the overall compression ratio [ 31 , 42 ]. data streams, the minimum unit of data access in XCQ is a single compressed data block. Unfortunately, accessing a compressed data block can still be costly. This is because when a data block is accessed, XCQ needs to load the required data block from disk into main memory and then to decompress it, which involves the following three major costs: The disk seek time during block searching, the data transfer time during block fetching, and the processor time used during block decompressing and scanning.
 the input query, we impose a minimal indexing scheme over a PPG data stream called Block Statistic Signature indexing. BSS indexing over PPG data streams is minimal in the sense that the scheme requires a very small amount of space and time resources in XCQ. This indexing scheme is a simplified version of the signature file indexing approaches [ 15 , 17 , 32 ]. Like Projection Signature Index-ing in [ 15 ], BSS indexing is desirable for indexing block-oriented compressed data. We restrict our discussion of the BSS indexes that are created for those data streams that comprise only numerical data. Assume that there are n blocks. The Block Statistic Signature index of the i th block is given by B i = s i , b i where b i represents the set of data items in the compressed block and the BSS index count are the usual operations. We define the value range for a BSS indexed block B , denoted as l i ,by min ( b i ), max ( b i ) . The same principle can be applied to alphabetical data with some modification of the BSS index values.
 erating a PPG data stream for numerical data values, a statistical signature is gen-erated for each compressed data block. The signature summarizes the data values inside the block. When a query is evaluated, the compressed data blocks in relevant data streams are accessed by XCQ. If BSS indexes are built on the data streams, a filtering process is carried out by XCQ as follows. Before a data block is fetched from the disk, XCQ consults the corresponding BSS index and ignores those data blocks that do not contain the required record(s). To do this, XCQ checks the BSS signature of the data block and decides whether the value range of that block over-laps with the value range specified in the query. If the two ranges overlap, which means that the data block may contain the required record(s), then the data block is fetched and decompressed for evaluation. If the two ranges do not overlap, the block does not contain the required record(s), in which case the data block is not fetched. 4.2 Query processing in XCQ In this section, we do not intend to present a detailed evaluation of XCQ queries in the scope of this paper, since the mechanism of processing XCQ queries and the optimization issues involved need the full space of another paper. Thus, we now only highlight the principle that PPG data streams help to process some XPath query fragments over a compressed XML document that conforms to the DTD in Fig. 5 .
 ranged as shown in Fig. 11 . Let us consider the path query fragment This query fragment selects those entry elements that have both an author with name  X  X ess X  and a publisher whose value is  X  X lear Ltd. X . The evaluation of the query depends on both the document structure and the data values. The evaluation of the two predicates in Q 1 involves data streams d 0 and d 3 .As d 0 and d 3 con-tain string values, neither has a BSS index associated with it. We first explain how XCQ evaluates the first predicate  X  X uthor/@name =  X  X ess X  X , in Q 1 . Since there is no BSS index on d 0 , XCQ needs to decompress the whole data stream and to test each record in the stream against the value  X  X ess X . Assuming there are two records, r 1 and r 2 in d 0 , satisfying the first predicate, XCQ then needs to find the corresponding  X  X ublisher X  records to evaluate the second predicate. 1 To fi n d t h e corresponding record indexes, XCQ parses the structure stream against the DTD tree and calculates the record indexes in data streams d 1 ,..., d 4 that correspond to the matched record indexes in d 0 . Assume that record s 1 in the first block and record s 2 in the second block of data stream d 3 are the publisher records corre-sponding to the name records that satisfy the first predicate. 2 XCQ decompresses only the first and second blocks of d 3 and retrieves the two matched records to evaluate the second predicate. Now assume that only record s 1 satisfies the second predicate in Q 1 . In order to construct the result, XCQ then decompresses the cor-responding blocks in data streams d 1 , d 2 ,and d 4 . The blocks needed are calculated from the matching record indexes that were found during structure stream parsing. Assuming that the required records are each in the first block of the corresponding data stream, XCQ needs to decompress only the shaded blocks in Fig. 11 when processing Q 1 .
 improve query performance, since a more precise portion of the compressed doc-ument is decompressed during query evaluation. However, there is a trade-off in that finer block partitioning degrades the compression ratio, since fewer redundan-cies in the data streams can be eliminated by a text compressor. In addition, we see that if the selectivity of the input query increases, the number of data blocks required to be decompressed during the query evaluation will also increase. tion like Q 2 =  X  X ount(//entry) X  and Q 3 =  X  X um(//num copy[text() &gt; 10]) X . The former counts the number of entry elements in the compressed XML document and the latter sums the values of all those num copy elements in the compressed XML document that have a value greater than 10. In Q 2 , only the number of entry elements in the XML document is needed to generate the answer. Thus, this type of query can be answered without decompressing the data streams. XCQ only needs to parse the structure stream against the DTD tree in Fig. 2 once. It then counts and returns the number of  X  X ntry  X   X  node occurrences that are assigned bit value 1. More complex structural queries can be processed by XCQ using a simi-lar procedure. In Q 3 , only one data stream is involved and the result of the query is an aggregate value. In this case, XCQ just needs to find those data values in the data stream d 4 that are greater than 10 and then sum these values. The BSS index constructed for d 4 can be used to filter out those blocks that contain only values less than or equal to 10, allowing XCQ to decompress only a subset of the blocks of d 4 in order to answer the query. 5 XCQ compression performance In this section, we present the experimental results of evaluating the performance of XCQ compression. We study the scalability of XCQ for different sizes of XML documents, and examine critically the impacts of varying PPG block sizes and of imposing BSS indexing on XCQ compression. 5.1 Experimental design and setup We compare the performance of XCQ with that of the following three compres-sors: (1) gzip , which is a widely used generic text compressor, (2) XMill ,which is a well-known XML-conscious compressor, and (3) XGrind , which is a well-known XML-conscious compressor that supports querying of compressed XML data.
 figuration:  X  PIII machine with a clock rate of 600 MHz.  X  192 MB RAM of main memory.  X  20 GB hard disk (Ultra DMA/66, 4200 rpm, 512 KB cache, 12 ms seek time). During the experiments, the number of processes running on the machine was minimized in order to reduce unrelated influences. The time taken to compress and decompress the documents is obtained by running the corresponding processes repeatedly five times and taking the average of the last three runs. The main reason for doing this is to reduce the disk I/O influences on the results by loading the whole document into the physical memory if possible (the same technique is also used in [ 31 ]).
 commonly used in XML research (see the experiments in [ 11 , 31 , 45 ]): We b l o g , SwissProt , DBLP , TPC-H , XMark ,and Shakespeare [ 1 , 5 , 30 , 43 , 46 , 47 ]. We now briefly introduce each dataset. 1. We b l o g is constructed from the Apache webserver log [ 1 ]. The original docu-2. Swissprot is constructed from the documents in the SwissProt database [ 43 ] 3. DBLP is a collection of the XML documents freely available in the DBLP 4. TPC-H is an XML representation of the TPC-H benchmark database, which 5. XMark is an XML document that models an auction website. It is generated 6. Shakespeare is a collection of the plays of William Shakespeare in XML [ 5 ]. documents have a very regular structure, whereas the last one is regarded as document-centric as the XML documents have a less regular structure. It is worth mentioning that the XML-ized weblog dataset (XSize) is about 1.6 times bigger than its non-XML-ized data counterpart (Size), as shown in Fig. 12 . This is due to the fact that we need to insert control information, such as element tags, into the documents. 5.2 Notion of compression ratio There are two different expressions that are commonly used to define the Com-pression Ratio ( CR ) of a compressed XML document (see the different definitions used in [ 11 , 31 , 35 , 45 ]): The first compression ratio, denoted CR 1 , expresses the number of bits required to represent a byte . Using CR 1 a better performing compressor achieves a rela-tively lower value. On the other hand, the second compression ratio, denoted CR 2 , expresses the fraction of the input document eliminated . Using CR 2 , a better per-forming compressor achieves a relatively higher value.
 ratios achieved by gzip and XMill in Table 1 . CR 2 shows that the fraction of an in-put document eliminated by gzip is only a few percent smaller than that of XMill. This means that the performance of gzip and XMill based on CR 2 appear to be similar. However, the actual size of a document compressed by XMill is generally much smaller than that of the document compressed by gzip. For example, for the Weblog document the size after compression by XMill is about 60% of the size after compression by gzip. This is also true for the SwissProt, TPC-H, and XMark documents. On the other hand, as we can see in Table 1 , the difference is better reflected by the ratio CR 1 . For example, we can see from Table 1 that there is an 11-fold difference between the CR 1 values for the Weblog (0.177 bits/bytes) and Shakespeare (2.016 bits/bytes) datasets using XMill, while the difference between the corresponding CR 2 readings (i.e., 97.8 and 74.8%) is only 23%. In addition, the notion behind CR 1 (i.e., the number of bits required to represent a byte) gives us an intuition related to the amount of information in the dataset, a commonly used notion in information theory [ 41 ]. Thus, we henceforth choose to adopt CR 1 as the metric to measure compression performance. 5.3 Compression performance of XCQ We now present an empirical study of XCQ performance with respect to com-pression ratio, compression time, and decompression time. All the numerical data used to construct the graphs can be found in the tables listed in [ 49 ]. 5.3.1 Compression ratio Figure 13 shows the compression ratios that are achieved on the above-mentioned six datasets expressed in CR 1 (bits/byte). Notably, both XMill and XCQ con-sistently achieve a better compression ratio than gzip. The compression ratio achieved is relatively high for data-centric documents (i.e., Weblog, SwissProt, DBLP, TPC-H, and XMark) and relatively low for document-centric documents (i.e., Shakespeare). This can be explained by the fact that the Shakespeare docu-ment does not have a regular structure, and therefore XMill and XCQ cannot take much advantage of the document structure during compression.
 much worse than that achieved by the other three compressors. This is due to the fact that XGrind independently compresses the data values inside an XML docu-ment, which is one of the requirements of its homomorphic transformation [ 45 ]. Thus, XGrind cannot take full advantage of eliminating the redundancies among data values within a document. We now show that, in a statistical sense, XCQ achieves a significantly better compression ratio than XMill. The evidence for this is obtained from performing formal hypothesis testing for two sample means (cf. Chapter 8 in [ 38 ]) on a set of 30 different XML datasets as follows. an XML document. (the first is the null hypothesis and the second is the alternative hypothesis): good compression ratio (i.e., there is no statistical difference), and H A represents the fact that XCQ achieves a better compression, which involves a one-sided test on the positive region of the distribution curve. From the results given in Table 10 in [ 49 ], we find that the sample mean ( x )and variance (  X  2 ) are 0 . 034 and 0 . 000622, respectively. It should be pointed out that the Central Limit Theorem (see Chapter 6.4 in [ 38 ]) allows us to assume that the sampling distribution will be approximately normal, even though our data may not be distributed normally in the parent populations. We now use the z -test to reject H 0 , which is a stan-dard statistical technique. Using the values of x and  X  2 above, we compute that z -v alue = 7 . 476. If we set the significance level of  X  = 0 . 01 (note that this is stricter than the acceptable level of  X  = 0 . 05), the critical value = 2.33. As the z -v alue &gt; 2 . 33, the null hypothesis H tribution curve, which means that H A is supported. In other words, XCQ achieves a better compression ratio than XMill with a confidence level of 99%. This indi-cates the effectiveness of our compression approach. With the knowledge of the DTD, XCQ does not need to encode as much structural information as XMill does in the compressed documents. 5.3.2 Compression time Figure 14 shows the compression time (expressed in seconds) required by the compressors to compress the XML documents. It is clear that gzip outperforms the other compressors in this experiment. XMill had a slightly longer compres-sion time than gzip, and XCQ in turn had a slightly longer compression time than XMill. The time overhead can be explained by the fact that both XMill and XCQ introduce a pre-compression phase for re-structuring the XML documents to help the main compression process. In the pre-compression phase, XCQ generates pre-cise PPG data streams by recursively traversing the DTD tree. In contrast, XMill adopts by default an approximation match on a reversed DataGuide to determine which container a data value belongs to. This grouping by enclosing tag heuristic runs faster than the grouping method used in XCQ and thus XMill runs slightly faster than XCQ. It should be noted, however, that the data grouping result gener-ated by XMill may not be as precise as our PPG data streams. This complicates the search for related data values of an XML fragment in the separated data containers in a compressed file. In addition, the compression buffer window size in XMill is set at 8 MB, which is optimized solely for better compression [ 31 ]. Such a large chunk of compressed data is costly in full or partial decompression. On the other hand, the compression time required by XGrind is generally much longer than that required by gzip, XMill, and XCQ. XGrind uses Huffman coding and thus needs an extra parse of the input XML document to collect statistics for a better compression ratio, resulting in almost double the compression time required in a generic compressor [ 45 ]. 5.3.3 Decompression time Figure 15 shows the decompression time (expressed in seconds) required by the decompressors. One observation from Fig. 15 is that, in general, gzip outperforms the other compressors in decompression and XMill runs faster than XCQ. Another observation is that XGrind requires a much longer decompression time than the other five decompressors. We also note that XMill decompresses Weblog docu-ments slightly faster than gzip, which conforms to the results reported in [ 31 ]. original positions in the structure after decompressing the data containers (or data streams) may explain longer decompression times compared to gzip. However, when the XMill-compressed file size is much smaller than the gzip-compressed achieves a decompression time that is shorter than that of gzip, mainly due to the much smaller disk read overhead. 5.3.4 Scalability of XCQ compression We now study the scalability of XCQ with respect to the other compressors. As we have observed that the compressors behave in a similar way for different document types, we choose to use We b l o g documents of different sizes, presented in Fig. 12 , as the dataset in this experiment.
 pressed in MB) obtained by different compressors. All four compressors scale roughly linearly with respect to the input document size, which is consistent with the findings shown in Fig. 13 . XCQ and XMill produce compressed documents of very similar sizes, while the poor performance of XGrind (consistently large gradient) is expected according to Fig. 13 .
 time (expressed in seconds), presented on a logarithmic scale. Clearly, gzip out-performs the other compressors consistently regardless of the document size. In particular, both XMill and XCQ have a longer compression time than gzip for all documents, since they introduce a pre-compression phase. XMill takes about 1.6 times longer than gzip to complete the compression process, a finding consistent with the results given in [ 31 ], while XCQ, in turn, takes about 1.6 times longer than XMill. XGrind takes considerably longer than XCQ.
 in seconds), presented on a logarithmic scale. It can be seen that XMill completes the decompression process either more quickly than or in roughly the same time as gzip. This is consistent with the results in [ 31 ]. However, on the other benchmark XML documents we used, such as DBLP, Shakespeare, SwissProt, and TPC-H, XMill requires a slightly longer decompression time than gzip (cf. Fig. 14 ). XCQ takes 1.3 s longer than XMill to decompress a 32 MB Weblog XML document. However, it should be noted that XCQ is able to process queries by only partially decompressing the document, implying that the decompression overhead will be much lower. 5.3.5 Summary and discussion To summarize, we find that both XMill and XCQ achieve better compression ra-tios than gzip at the expense of compression and decompression time. XCQ needs more time than XMill to generate a PPG data stream in an XML document when the document is compressed. This enables XCQ to achieve a slightly better com-pression ratio than XMill. On the other hand, the compression performance of XGrind is consistently worse than those of XMill and XCQ. This supports the findings reported in [ 45 ]. 5.4 Block partitioning and BSS indexing In this section, we study the impact of varying the block size and imposing BSS indexing on data streams in XCQ. We only present the effect on the 89 MB Weblog dataset, since other datasets exhibit similar behavior. 5.4.1 Effect of block partitioning We now present the results related to the choice of block size when partitioning PPG data streams.
 ent block sizes. For ease of reference, we superimpose a dotted line on the figure to indicate the compression ratio achieved by XCQ when no partitioning is made. It can be seen from the figure that the compression ratio degrades when a smaller block size (i.e., a finer partitioning) is used. The degradation in the compression ratio is due to the fact that fewer redundancies in the data streams can be elimi-nated by a text compressor if each block is compressed as a finer individual unit. When the block size is increased to around 5000 records per block, the compres-sion ratio achieved is comparable to that achieved when no partitioning is made; the difference is less than 4%. Figure 17b and c show that both the compression and decompression times are also degraded when a finer partitioning is used. The degradation in the compression and decompression times is due to the fact that, if we set a smaller block size in XCQ, the number of compression and decompres-sion operations is increased. Consequently, the total overhead is increased, since each block is compressed and packed as an individual unit. 5.4.2 Effect of BSS indexing We now study the impact of BSS indexing on XCQ compression. In the following, we compare the performance of two configurations of XCQ: The first is the default configuration with BSS indexing, while the second is XCQ with BSS indexing turned off . and decompression time, respectively, between the two configurations of XCQ. As can been seen, only small overheads are added to the compression (roughly 5%) and decompression (roughly 1 X 2%) times when BSS indexing is adopted in XCQ. There is virtually no difference in the compression ratio between the two XCQ versions (i.e., with and without BSS indexing). These results agree with our expectations, since the BSS indexing scheme is designed to be minimal for block-oriented compressed data. 5.4.3 Discussion We find that a very fine partitioning (smaller than 3000 records per block) on PPG data streams imposes overheads in compression ratio, compression time, and de-compression time. However, when using a finer data stream partitioning, XCQ can utilize the advantage of decompressing a more precise portion of the compressed document when answering queries, as was discussed in Sect. 4.2 . We need small blocks in a PPG data stream in order to have efficient XCQ query processing. On the other hand, the overhead of BSS indexing is minimal, and is virtually indepen-dent on the block size in our study.
 6 Related work Because there are usually substantial redundancies embedded in an XML doc-ument structure, information theory states that we should be able to achieve significant compression of XML data. However, such embedded redundancies are not trivial to discover and are largely ignored by conventional textual compres-sion such as gzip [ 18 ]orbzip2[ 40 ]. Thus, many XML-conscious compression technologies have been proposed and developed in recent years.
 of DTDs. They are Differential DTD Tree (DDT) compression in Millau [ 42 ]and the Structure Compression Algorithm (SCA) proposed in [ 29 ]. The DDT and SCA approaches adopt a similar compression strategy that encodes only the information that cannot be inferred from a given DTD. (A similar approach to encoding a document with respect to a DTD was used for a different purpose in [ 20 ].) The limitation of these approaches is that, when parsing an XML document in order to create a corresponding tree structure, a large amount of memory is required to store the generated DOM tree. The vigorous use of virtual memory leads, in practice, to frequent thrashing of disk I/O, which degrades the efficiency of the compression process.
 ogy. 3 It achieves a good compression ratio but the compressed data needs to go through a full decompression in order to evaluate queries. XMill has a pre-compression phase introduced prior to the main compression process. The pre-compression phase is designed to perform the following two main tasks: First, to separate the document structural information from the data, and second, to group data items with related semantics in the same  X  X ontainer X . The structural infor-mation includes element tag names and attribute names. The data items include PCDATA and attribute values.
 tionmatchingonthe reversed DataGuide [ 23 , 31 ] to determine which containers data values belong to. In its default setting, data items with the same tag or attribute name are grouped in the same data container. Each container is then compressed individually in the main compression phase by using an ordinary text compressor such as gzip, whose output is then concatenated as a single file. In addition, path expressions can be specified as command line arguments to instruct the XMill compressor how to group data items. Specific semantic compressors can also be employed in order to pre-compress the corresponding data containers before they are compressed by a text compressor. This further helps to achieve a better com-pression ratio. However, user expertise and manual effort are needed to intervene in the compression process.
 erarchical Modeling (MHM) in [ 11 ]. This offers better compression ratios than XMill at the expense of compression speed. Like XMill, the compressed docu-ments need to be decompressed before queries can be evaluated on them. recent XML compression technologies provide direct access to compressed doc-uments. XGrind [ 45 ] is the first known queriable XML compressor. XGrind adopts a homomorphic transformation strategy to transform an XML document into a specialized compressed format that preserves the syntactic and seman-tics information of the original document. All the tag and attribute names in the compressed document are tokenized using a dictionary encoding approach, and enumeration-type attribute values are binary encoded. PCDATA and general attribute values are compressed individually by using non-adaptive context-free Huffman encoding [ 24 ].
 mation of the input document, all operations that can be executed over the original document, such as querying, are preserved. These operations can be executed us-ing existing techniques and tools with some modifications. However, it should be noted that the advantage of avoiding decompression 4 when querying is obtained at the expense of compression ratio. For instance, XGrind compresses an 89 MB Weblog XML document into a 38 MB compressed document, while XMill is able to compress the same document to only 2.3 MB.
 to be evaluated directly on compressed XML documents. The technique adopted in [ 7 ] compresses the skeleton of a given XML document (essentially its structure) by using a technique based on the idea of sharing common subtrees, thereby trans-forming the skeleton into a directed acyclic graph (DAG). This DAG can be further compressed by replacing any consecutive sequence of out-edges to the same ver-tex by a single edge labeled with the appropriate cardinality. The focus of Bune-man et al. X  X  skeleton framework is different from our approach, in that skeleton compression aims at reducing the size of the document structure, rather than the textual data items in the document, and the framework does not use knowledge of a DTD to perform structure compression. In [ 8 ], the skeleton and its corresponding data storage (or data vectors) are used together to support processing a fragment of XQuery. We can view the skeleton component as a  X  X seudo-DTD X , since, roughly speaking, it presents a compact structure of a given XML document, while a data vector in [ 8 ] is essentially a non-partitioned data stream.
 document well, the overall compression ratio (including textual data) achieved by this framework, as mentioned in [ 7 ], is worse than that of XMill. However, using the proposed compression technique, the authors formally study the evaluation of expressions in Core XPath [ 7 ] and XQuery [ 8 ]. The essence of evaluating such queries is done by manipulating the compressed skeleton instance with only par-tial decompression. This technique allows the navigational aspect of query eval-uation, which is responsible for a large portion of the query processing time, to be carried out in main memory. Such techniques are complementary to our work. In principle, we could extend our work to output a query result in a compressed format (i.e., outputting the related fragment of the structure stream and the data according to their corresponding data streams).
 query processing time (two to three times faster) than that of XGrind, according to the experimental results reported in [ 35 ]. However, as also mentioned in [ 35 ], the compression ratio of XPRESS is in fact worse than that of XMill. In addition, compression time is almost twice that of XMill, since XPRESS requires parsing the input XML document twice in the compression process.
 that XQuery language can be fully supported. Like XGrind and XPRESS, XQueC is able to compress an XML document as well as to avoid full decompression during query evaluation. However, the approach differs from that used by XGrind and XPRESS in that XQueC separates the XML structure from the XML data items and uses a variety of auxiliary structures, such as DataGuides [ 23 ], structure trees, and other indexes, in order to support efficient evaluation of XQuery [ 4 ]. The individually compressed data items are organized into containers, and they can be efficiently accessed by pointers from the auxiliary data structures. How-ever, it seems that the fine-grained compression is very likely to result in a worse compression ratio than that of XMill. Moreover, the auxiliary data structures, to-gether with the pointers to the individually compressed data items, would incur a huge space overhead. 7 Conclusions and future work We have presented the development of XCQ, a prototype system designed to sup-port querying over compressed XML documents. Overall, we showed that by ex-ploiting the information present in a DTD, XCQ is able to achieve better com-pression and to support evaluation of a set of fundamental XPath queries. Our development is based on the following series of novel techniques.  X  We proposed DTD Tree and SAX Event Stream Parsing (DSP), which enables  X  We proposed the Partitioned Path-Based Data Grouping of data streams as an  X  We proposed a simple and minimal indexing scheme for PPG data streams XCQ can achieve good compression and can compress XML-ized documents con-sistently better than the generic text compressor gzip. It also achieves a slightly better compression ratio, at the expense of a greater compression time, than state-of-the-art systems such as XMill, which is optimized only for compression ratio. Based on the study conducted in Sect. 5.3 , we found that XCQ achieves a better compression ratio than gzip at the expense of the compression and decompression times. Comparing it to another well-known unqueriable compressor, XMill, XCQ performs slightly better in terms of the compression ratio but worse in compres-sion and decompression time. XCQ was found to be scalable for a wide range of XML benchmark documents, as listed in Sect. 5.1 . We also found that XCQ per-forms consistently better than another well-known queriable XML compressor, XGrind, in compression performance. Admittedly, the main drawback of DSP is that the compression and decompression times for processing an XML document as a whole are relatively longer than those of the generic compressor gzip. The underlying reason for this is that XCQ needs to generate PPG data streams cor-responding to the compressed XML document. However, we argue that the time overhead is worthwhile, since the generated PPG data streams are able to sup-port queries over compressed documents in an efficient manner. In practice, this time overhead is a once-off consumption, since the generated data streams can be buffered when XCQ is used in the context of an XML application.
 querying engine that is able to support more sophisticated XPath and XQuery queries. Currently, we are also developing a cost model that is able to account for how the response time is affected by various parameters involved in the com-pression strategy, such as the block size in a data stream, the number of clusters, the cost of scanning indexes, the cost of decompressing a block, query selectiv-ity, data distribution, and workload statistics. The cost model can be incorporated into the XCQ engine for optimizing query evaluation. An orthogonal but promis-ing direction related to query optimization is to employ a caching technique in the engine to handle the PPG data blocks of a compressed document. An efficient caching scheme for fetching and updating compressed data blocks would help XCQ to minimize overheads, such as I/O costs, during the compression, querying and updating of XML documents.
 compressed XML documents. However, an append operation could be supported by XCQ in a straightforward manner. In order to append an XML fragment to the compressed document, XCQ would first extract the structural information and data information from the fragment and then append the extracted information to the structure stream and the corresponding data streams. Using this approach, only the structure stream and the last block of each updated data stream would have to be re-compressed. However, for general update operations, we still need to devise efficient techniques to deal with the deletion and modification of fragments of compressed XML data.
 would be straightforward to modify it to handle recursive DTDs. In the first place, the DTD parser would build a DTD graph rather than a tree. When parsing a document against the DTD graph, the graph would still be traversed in depth-first order. However, there would now be the possibility that a node in the graph could be visited multiple times while parsing a single path in the document, although the number of times would be bounded by the maximum depth of any node in the document. In order to determine the correct data stream on which to output a text node, the system could consult the stack maintained by the depth-first search of the DTD graph. This technique effectively implements a deterministic PDA as shown to be necessary and sufficient for the single-pass validation of XML documents by Segoufin and Vianu in [ 39 ].
 References
