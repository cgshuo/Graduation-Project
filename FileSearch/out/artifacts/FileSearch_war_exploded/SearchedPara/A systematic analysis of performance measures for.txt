 1. Motivation
MachineLearning(ML)divides classificationonto binary,multi-class,multi-labelled, andhierarchicaltasks. In thiswork we presentasystematicanalysisoftwentyfourperformancemeasuresusedintheseclassificationsubfields.Wefocusonhowwell classes are identified without reference to computation cost or time. We consider a set of changes in a confusion matrix that correspond to specific characteristics of data. We then analyze the type of changes that do not change a measure X  X  value and taxonomywithrespecttoallrelevantlabeldistributionchangesinaclassificationproblem.Wesupplementtheformalanalysis by examples of applications where invariance properties of measures lead to a more reliable evaluationof classifiers;examples are taken from text classification. Note, that we focus on recent ML developments; more details on ML measures can be found, for example, in Sokolova, Japkowicz, and Szpakowicz (2006) which looks into relations between the measures and assessment of medical trials. To the best of our knowledge, our current study is the first reviews of ML measures which comprehensively 2007).Thisstudyexpandstheresultstwo-fold,withdiscussionofnewinvariantproperties,insomecases,addingmonotonicity properties, and consideration of multi-class, multi-labelled, and hierarchical measures.

Empirical evaluation remains the most used approach for the algorithm assessment, although ML algorithms can be eval-uated through empirical assessment or theory or both, e.g., derived generalized bounds and empirical results ( Marchand &amp;
Shawe-Taylor, 2002 ). Evaluation techniques based on multiple experiments are considered in Dietterich (1998) , one of the most cited work on empirical evaluation of ML algorithms. An extensive critique of ML evaluation practice can be found in
Salzberg (1999) . The author analyzes the currently used methods and their statistical validity. The paper distinguishes two goals of evaluation: a comparison of algorithms, and the feasibility of algorithms on a specific domain. Demsar (2006) sur-veys how classifiers are compared over multiple data sets. Empirical comparison is most often done by applying algorithms on various data sets and then evaluating the performance of the classifiers that the algorithms have produced; accuracy being the most often used measure. In all these assessment approaches, the algorithm and the output classifiers take the central stage.

We take an alternative route looking how characteristics affect the objectivity of measures. Our formal discussion of ML performance measures complements popular statistical and empirical comparisons such as the ones presented in Goutte and
Gaussier (2005) . We show that, in some learning settings, the correct identification of positive examples may be important whereas in others, the correct identification of negative examples or disagreement between data and classifier labels may be more significant. Thus, standard performance measures should be re-evaluated with respect to those scenarios. Previously,
ML studies of performance measures have primarily focused on binary classification. For a complete review, we add multi-community did not consider measures X  invariance when new ones were introduced (Bengio, Mari X thoz, &amp; Keller, 2005;
Huang &amp; Ling, 2007 ) or suggested for adoption from other disciplines (Sokolova et al., 2006 ). 2. Overview of classification tasks
Supervised ML allows access to the data labels during the algorithm X  X  training and testing stages. Consider categorical labels when data entries x 1 ; ... ; x n have to be assigned into predefined classes C of the following tasks:
Binary : the input is to be classified into one, and only one, of two non-overlapping classes  X  C the most popular classification task. Assigned categories can be objective, independent of manual evaluation (e.g, repub-lican or democrat in the votes data of the UCI repository (Asuncion &amp; Newman, 2007 )) or subjective, dependent on man-ual evaluation (e.g., positive or negative reviews in Amazon.com (Blitzer et al., 2007 )). Classes can be well-defined (e.g., the votes labels), ambiguous (e.g., the review opinion labels), or both (e.g., medical vs. other texts in the Newsgroups collection 1 ).

Multi-class : the input is to be classified into one, and only one, of l non-overlapping classes. Multiclass problems include ing the original 135 categories in the benchmark Reuters collection, neutral ( Wilson, Wiebe, &amp; Hwa, 2006 ). As for the binary case, multi-class categorization can be objective or subjective, well-defined or ambiguous.

Multi-labelled : the input is to be classified into several of l non-overlapping C tions of yeast genes ( Mewes, Albermann, Heumann, Lieb, &amp; Pfeiffer, 1997 ), identifying scenes from image data (Li, Zhang, &amp; Zhu, 2006 ) or text-database alignment and word alignment in machine translation (Snyder &amp; Barzilay, 2007 ). In text mining of medical information, multi-label classification methods are often evaluated on OHSUMED, a collection of medical references ( Hersh, Buckley, Leone, &amp; Hickam, 1997 ). When the learning task is document topic classification, multi-labelling is often referred as multi-topic classification such as for clinical texts that are assigned multiple disease codes from ICD-9-CM ( Sasaki, Rea, &amp; Ananiadou, 2007 ). Binary, multi-class, and multi-labelled problems form flat classification ( Yang, 1999 ), in which categories are isolated and their relations are not considered important. The next, hierarchical, problem addresses relations among categories and includes their structure into learning targets.
Hierarchical : the input is to be classified into one, and only one, C superclasses. The hierarchy is defined and cannot be changed during classification. Text classification and bioinformatics supply many examples, e.g., protein function prediction (Eisner, Poulin, Szafron, Lu, &amp; Greiner, 2005 ). Hierarchical classification can be transformed into flat classification. For example, the Reuters collection classification can be multi-class ( Bobicev &amp; Sokolova, 2008 ), multi-labelled (Tikk &amp; Bir X , 2003 ), and hierarchical (Sun, Lim, &amp; Ng, 2003 ). A frequent appearance of language and text problems among the listed above examples can be explained by a special role
Natural Language Processing (NLP) holds in ML applications. The richness of language characteristics and the fast-increasing volume of readily available digital texts make texts not only a nearly inexhaustible research area, but also one of the most nent place among ML applications to NLP problems. It is dedicated to finding texts according to a given criteria (Sebastiani, 2002) and it includes the classification of documents (research papers, technical reports, magazine articles, etc.). For topic classification (e.g., identification of documents about a given city or documents about bands and artists, etc.) documents of relevant documents being the more important task, the focus in this case is on true positive classification. First compre-hensive books on Machine Learning were published in late 1990 X  X  (Langley, 1996; Mitchell, 1997 ). As a discipline, ML bor-1983). In some ways, text classification borrows from Information Extraction (IE) which preluded the use of Machine
Learning in automated text processing and understanding, e.g., the automated analysis and generation of synonymous texts (Boyer &amp; Lapalme, 1985). IE and IR metrics in the evaluation of ML algorithms are an example of such borrowing. The these measures neglect the correct classification of negative examples, they instead reflect the importance of retrieval of positive examples in text/document classification:
Precision : the number of correctly classified positive examples divided by the number of examples labeled by the system as positive Recall : the number of correctly classified positive examples divided by the number of positive examples in the data Fscore : a combination of the above.

In recent years, the NLP and ML communities have turned their attention to the study of opinions, subjective statements, and sentiments. The corresponding empirical problems are represented by the classification of political debates, web post-phone conversations and political debates, electronic negotiation transcripts, etc. Chart-boards, blogs and movie reviews are discussions, records of phone conversation and electronic negotiation transcripts are used in studies of individual behavior. depends on the problem statement. Transcripts of the US Congress debates are used in the social network analysis, a new area of Artificial Intelligence research. Here a common task is to define important influence factors and predict the future &amp; Lee, 2006 ).

These sources represent records of human communication that convey meanings sent by a speaker and received by a hearer. These meanings can be complex and subtly expressed and constituted from both what is said and what is implied.
So far, there is no common consensus on the choice of measures used to evaluate the performance of classifiers in opinion, subjectivity and sentiment analysis. Additional performance measures other than the above are Accuracy used in Pang et al. (2002) and Thomas et al. (2006), or the correspondence between Precision and Recall in Gamon, Aue, Corston-Oliver, and
Ringger (2005) . When going from document classification to the classification of human communication, it is important to know how different performance measures relate to each other in order to help resolve disagreements among perfor-mance evaluations. This phenomenon happens quite often in experimental studies. 3. Performance measures for classification
The correctness of a classification can be evaluated by computing the number of correctly recognized class examples (true positives), the number of correctly recognized examples that do not belong to the class (true negatives), and exam-ples that either were incorrectly assigned to the class (false positives) or that were not recognized as class examples (false negatives). These four counts constitute a confusion matrix shown in Table 1 for the case of the binary classification .

Table 2 presents the most often used measures for binary classification based on the values of the confusion matrix. AUC measures. However, we present Fscore  X  X  properties because of its extensive use in text classification.
Table 3 presents the measures for multi-class classification . For an individual class C tp ; fn i ; tn i ; fp i : Accuracy i ; Precision i ; Recall i assessed in two ways: a measure is the average of the same measures calculated for C aging shown with l indices). Macro-averaging treats all classes equally while micro-averaging favors bigger classes. As there AUC in the list of multi-classification measures.

The quality of multi-topic classification (Table 4 ) is assessed through either partial or complete class label matching rank. We do not include such measures as One-error which counts how many times the top-ranked label was not a member
Section 4, we show that these two measures are not interchangeable with respect to confusion matrix transformations; thus, they may not be equally applicable to similar settings.

For hierarchical classification measures (Table 5 ), we consider measures that incorporate the problem X  X  hierarchy. These measures either evaluate descendant or ancestor performance (Kiritchenko, Matwin, Nock, &amp; Famili, 2006 ). We omit distance-and semantics-based measures suggested for hierarchical classification (Blockeel, Bruynooghe, Dzeroski, Ramon, &amp;
Struyf, 2002; Sun et al., 2003 ). These measures extend flat, non-hierarchical, measures by estimating differences and simi-larities between classes. However, in these measures, acceptable differences and similarities are often specified by users applies to depth-dependent measures, which relate classes by imposing vertical distances ( Blockeel et al., 2002 ).
Data Mining has successfully exploited the invariant properties of interestingness measures for comparison of association rent study, we consider new invariant properties and expand discussed measures by including multi-class, multi-topic and hierarchical classification measures. Although the latter three types of classification are quite popular, their measures have not been studied to the same extent as for binary classification measures. 4. Invariance properties of measures
We focus on the ability of a measure to preserve its value under a change in the confusion matrix. A measure is invariant if its value does not change when a confusion matrix changes, i.e. invariance indicates that the measure does not detect the change in the confusion matrix. This inability can be beneficial or adverse, depending on the goals.
 Let X  X  consider a case when invariance to the change of tn is beneficial. Text classification extensively uses Precision and tion, a large number of unrelated documents constitute a negative class without having a single unifying characteristic. The independently of performance on the irrelevant documents. Precision and Recall do not depend on tn , but only on the correct spective on a classifier X  X  performance for document classification.

On contrast, the same invariance for the tn change can be an adversary. Consider the classification of human communi-cation where negative classes are also important. In those problems, classes often have distinct features (male or female) for which both positive and negative classes are well-defined. The retrieval of a positive class, the discrimination between clas-ses or the balance between retrieval from both classes are problem-dependent tasks. Thus, an appropriate evaluation mea-sure should take into account the classification of negative examples and reflect the changes in tn when the other matrix elements stay the same.

We now examine eight invariance properties  X  I 1 6 k 6 8  X  with respect to changes of elements in a confusion matrix. All the eight changes are results of elementary operations on matrices: addition, scalar multiplication, interchange of rows or col-uniform increase in the number of examples. Henceforth, I discuss binary classification because other evaluation measures are derived from the binary confusion matrix and its perfor-mance measures. In several parts of the discussion, we refer to data quality. By this we understand how well examples rep-resent the underlying notion (especially, ease of understanding and interpretability), how accurate is the data, including its
Our claim is that the following invariance properties affect the applicability and trustworthiness of a measure. 4.1.  X  I 1  X  Exchange of positives and negatives
This shows measure invariance with respect to the distribution of classification results because it does not distin-guish tp from tn and fn from fp and may not recognize the asymmetry of classification results. Thus it may not be trust-worthy when classifiers are compared on data sets with different and/or unbalanced class distributions. For example, invariant measures may be more appropriate for assessing the classification of consumer reviews than for document classification. 4.2.  X  I 2  X  Change of true negative counts fn ]) = f ([ tp ; fp ; tn 0 ; fn ]).
This measure does not recognize the specifying ability of classifiers. Such evaluation may be more applicable to domains sures are suitable for the evaluation of document classification. If the measure is non-invariant, then it acknowledges the a well-defined, unimodal, negative class. Non-invariant measures are preferable for evaluating communications in which there are criteria for both positive and negative results. 4.3.  X  I 3  X  Change of true positive counts fn ]) = f ([ tp 0 ; fp ; tn ; fn ]).
This measure does not recognize a classifier X  X  sensitivity. Such evaluation can be complementary to other measures, but can hardly stay on its own. It may be reliable for comparison in domains with a well-defined, unimodal, negative class. As opposed to I 2 , these invariant measures are not suitable for the evaluation of document classification. Non-invariant mea-sures can be used as stand alone for evaluating classification with a strong positive class. 4.4.  X  I 4  X  Change of false negative counts fn ]) = f ([ tp ; fp ; tn ; fn 0 ])
Invariance indicates measure stability under disagreement between the data and the negative labels assigned by a clas-
Hurst (2004) argue that humans can agree on only 74% of labels for negative opinion), an invariant measure may give mis-leading results. For non-invariant measures, its value X  X  monotonicity is important. If the classifier evaluation improves when fn increases, the measure may favor a classifier prone to false negatives. The use of invariant and non-invariant measures should be decided based on problem and data characteristics. 4.5.  X  I 5  X  Change of false positive counts fn ]) = f ([ tp ; fp 0 ; tn ; fn ]).
An invariant measure may provide reliable results when some of positive data labels are counter-intuitive. This can hap-pen when the positive examples have outliers that cannot be explained by the mainstream data. We call such outliers counterexamples .

A non-invariant measure may not be suitable for data with many counterexamples. If the classifier evaluation improves when fp increases, the measure may favor a classifier prone to false positives. This is especially important for problems involving subjective labelling. Some data entries may not have consistent labels because of the difficulty of imposing rigorous labelling rules. This can occur in the classification of records of long-term communications in which the data may contain a substantial number of counterexamples. 4.6.  X  I 6  X  Uniform change of positives and negatives
A measure f ([ tp ; fp ; tn ; fn ]) is invariant under a uniform change of positives and negatives if f  X  X  tp ; fp ; tn ; fn  X   X  f  X  X  k 1 tp ; k 1 fp ; k 1 tn ; k 1 fn  X  ; k 1  X  1.
An invariant measure is stable with respect to the uniform increase of data size, i.e., scalar multiplication of the confusion matrix. If we expect that for different data sizes the same proportion of examples will exhibit positive and negative charac-teristics, then the invariant measure may be a better choice for the evaluation of classifiers.

When a measure is non-invariant, then its applicability may depend on data sizes. The non-invariant measures may be more reliable if we do not know how representative the data sample is in terms of the proportion of positive/negative examples. 4.7.  X  I 7  X  Change of positive and negative columns
A measure f ([ tp ; fp ; tn ; fn ]) is invariant under columns X  change if f  X  X  tp ; fp ; tn ; fn  X  X  f  X  X  k
Suppose that different data sizes have the same proportion of positive and negative examples. This change in the con-fusion matrix is caused by changes in the proportion of positive and negative labels issued by an algorithm, i.e., the col-umns are multiplied by different scalars. This may happen when the quality of additional data substantially differs from the initial data sample (e.g., the information inflow can add more noise). However, an invariant measure does not show the performance change. Thus, it requires support of other measures to assess a classifier X  X  performance on different classes.

A non-invariant measure reflects on the performance of a classifier on different classes. It is more appropriate if we can expect a change in the algorithm X  X  performance across classes. 4.8.  X  I 8  X  Change of positive and negative rows
A measure f ([ tp ; fp ; tn ; fn ]) is invariant under rows X  change if f  X  X  tp ; fp ; tn ; fn  X  X  f  X  X  k
We again expect that different data sizes have the same proportion of positive and negative examples. Then the change in the confusion matrix corresponds to changes of an algorithm X  X  performance within a positive (negative) class, i.e., the rows measure may be a better choice for the evaluation of classifiers.

When a measure is non-invariant, its applicability may depend on the quality of data classes. The non-invariant measures may be more reliable if we do not know how representative the data sample is in terms of the quality of positive and neg-ative classes, which might be the case in web-posted consumer reviews.

For multi-class classification, we consider transformations of the confusion matrix for each class C sures retain their invariance properties regardless of micro -or macro-averaging .
 For multi-topic classification, Exact Match Ratio and Accuracy have different invariant properties. Thus, referring to Exact Match Ratio as Accuracy may be misleading.

Measures used in hierarchical classification have a somewhat limited reliability because they evaluate the performance of a classifier either on subclasses or on superclasses, but not on both. Thus, invariance properties should be assessed with re-spect to the classification of subclasses  X  for Precision
Table 6 displays the invariance properties of the measures described in Tables 2 X 5 . By assessing the invariant properties of commonly used measures, we show that Precision ; Precision characteristics. Thus, we group them as Precision G for general. Similarly, we group Recall ; Recall
Recall " as Recall G ; Fscore ; Fscore l ; Fscore M ; Fscore Rate , essentially 1-Average Accuracy ,as Accuracy G .

As a result, we further consider only those performance measures that vary in their invariance properties. Table 7 lists the measures and their properties. Our next step is to associate the invariant properties with particular settings. 5. Analysis of invariant properties
To identify similarities among the measures, we compare them according to their invariance and non-invariance proper-ties shown in Table 7 . First, we present measure outliers whose properties remarkably differ them from others. Two mea-sures hold unique invariant properties: Precision G is the only measure invariant under vertical scaling  X  I
Match Ratio is the only measure non-invariant under uniform scaling  X  I sitive to all the changes in the confusion matrix except for uniform scaling.
 Next we generalize on the properties:
The invariance I 1 has been much discussed in the Machine Learning community, albeit from a negative point of view ( Jap-The invariance I 2 is a well-known property of Precision , Recall , and Fscore and less known for Labelling Fscore and Hamming
The invariance I 3 so far eludes thorough studies. Measures are expected to be non-invariant under the change of tp . The
The invariance I 4 under change in fn indicates that Precision , Specificity , and Exact Match Ratio may be more reliable when
The invariance I 5 under fp change indicates that Recall and Exact Match Ratio may provide reasonably conservative esti-
The invariance I 6 under uniform scaling holds for all the measures except Exact Match Ratio . The nine invariant measures The invariance I 7 under the scalar column change holds only for Precision . This supports a common practice of combining
Invariance with respect to the matrix transformations is especially important because it connects evaluation measures to particular learning settings. We now summarize the applicability of these measures to two subfields of text classification: document classification and classification of human communications. One might be tempted to apply Fscore measures on any text classification evaluation. However, various classification problems exhibit different characteristics which may require different evaluation measures. Based on our analysis, we propose the following.

Since document classification data is often highly imbalanced, relevant documents constitute a small well-defined posi-tive class, but the rest is a heterogeneous negative class built from non-relevant documents as  X  X  X verything non-positive X .
Presence of a negative class that complements the positive class favors the use of the Fscore measures. In many such prob-lems, examples of the positive class remain the same and the class keeps its modality, whereas examples of the negative class change. Since the Fscore measures X  invariance under the change of correctly classified negative examples  X  I drastic changes, they will be less sensitive to changes in the negative class.

Classification of human communications is most often represented by sentiment classification applied to collections of free form texts containing product evaluations. The number and ratio of positive and negative examples depends on the pop-is non-invariant under the change of correctly classified negative examples  X  I tive class better than Fscore measures.

For other types of classification of communications in social activities, other measure combinations might also be suit-able. Political debates and electronic negotiations are examples of such communications. Their data can exhibit a unimodal negative class and a large number of counterexamples. In political debates, counterexamples are records that praise the discussed matter, but vote against it at the end, either because of a hidden motive or randomness of behavior ( Sokolova examples  X  I 5  X  , may be used for a reliable evaluation of classifiers. 6. Conclusion and future work
In this study, we have analyzed twenty four performance measures used in the complete spectrum of Machine Learning well-known measures have been studied. In all the cases, we have shown that the evaluation of classification results can depend on the invariance properties of the measures. A few cases required that we additionally considered monotonicity of the measure. These properties have allowed us to make fine distinctions in the relations between the measures. One way to insure a reliable evaluation is to employ a measure corresponding to the expected quality of the data, e.g., represen-characteristics, we constructed the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem.

We supplemented the formal discussion by analyzing the applicability of performance measures on different subfields of text classification. We have shown that the classification of human communications differs from document classification, and thus that these two types of text classification may require different performance measures.
 Our study has dealt with measures used in text classification but it could be extended to other language applications of
Machine Learning. The next step would be to study measures used in Machine Translation. This will considerably expand the measure list. Applicability of the measures to traditional Natural Language Processing tasks, e.g., word sense disambiguation, sifier are independent. Person authentication problems, in which the appropriate measures are a false acceptance rate and a false rejection rate (Bengio et al., 2005 ), is another example of possible applications.
 Acknowledgments
This work has been funded by the Natural Sciences and Engineering Research Council of Canada and the Ontario Centres of Excellence. We thank Elliott Macklovitch for fruitful suggestions on an early draft. We thank anonymous reviewers for helpful comments.
 References
