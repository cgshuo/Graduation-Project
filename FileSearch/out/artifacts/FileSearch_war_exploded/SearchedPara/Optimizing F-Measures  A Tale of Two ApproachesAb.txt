 Nan Ye yenan@comp.nus.edu.sg Kian Ming A. Chai ckianmin@dso.org.sg DSO National Laboratories, Singapore 118230 Wee Sun Lee leews@comp.nus.edu.sg Hai Leong Chieu chaileon@dso.org.sg DSO National Laboratories, Singapore 118230 F-measures (van Rijsbergen, 1974) or F-scores have been commonly used in tasks in which it is important to retrieve elements belonging to a particular class cor-rectly without including too many elements of other classes. F-measures are usually preferred to accura-cies as standard performance measures in information retrieval (Manning et al., 2008), particularly, when rel-evant items are rare. They are also popular in informa-tion extraction tasks such as named entity recognition (Tjong Kim Sang &amp; De Meulder, 2003) where most of the elements do not belong to a named class.
 Various methods have been proposed for optimizing F-measures. They fall into two paradigms. The empiri-cal utility maximization (EUM) approach learns a clas-sifier having optimal F-measure on the training data. Optimizing the F-measure directly is often difficult as the F-measure is non-convex. Thus approximation methods are often used instead. Joachims (2005) gave an efficient algorithm for maximizing a convex lower bound of F-measures for support vector machines, and showed it worked well on text classification. Jan-sche (2005) gave an efficient algorithm to maximize a non-convex approximation to F-measures using lo-gistic regression models, and showed it works well on a text summarization problem. A simpler method is to optimize the F-measure in two stages: First learn a score function using standard methods such as logistic regression or support vector machines, then select a threshold for the score function to maximize the em-pirical F-measure. Though simple, this method has been found to be effective and is commonly applied, for example, in text categorization (Yang, 2001). The decision-theoretic approach (DTA), advocated by Lewis (1995), estimates a probability model first, and then computes the optimal predictions (in the sense of having highest expected F-measure) according to the model. This method has not been commonly applied for F-measures, possibly due to the high computa-tional complexity of existing algorithms for the predic-tion step. Assuming the independence of labels, Lewis showed that, in the optimal prediction, the probabili-ties of being positive for irrelevant items are not more than those for relevant items. He also gave a bound for expected F-measures, which can be computed in O ( n ) time, but can be very loose. Based on Lewis X  X  characterization, Chai (2005) gave an O ( n 3 ) time al-gorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai X  X  work, Jansche (2007) solved the same problem in O ( n 4 ) time. For the gen-eral case when the labels are not necessarily indepen-dent, Dembczynski et al. (2011) gave an O ( n 3 ) time algorithm given n 2 +1 parameters of the label distribu-tion, but the parameters can be expensive to compute. They also showed that the independence assumption can lead to bad performance in the worst case, but on the practical datasets used in their experiments, methods assuming the independence assumption are at least as good as those not assuming independence. We have only discussed works on binary classification. There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum  X e et al., 2009) and mul-tilabel tasks (Fan &amp; Lin, 2007; Zhang et al., 2010; Pet-terson &amp; Caetano, 2010).
 Optimality in EUM and DTA are different. EUM considers only instance classifiers (functions mapping instances to labels), and roughly speaking, an opti-mal classifier is an instance classifier having highest F-measure on a very large test set among all instance classifiers. On the other hand, DTA considers set clas-sifiers (functions mapping sets of instances to sets of labels), and an optimal classifier in DTA is a set clas-sifier having maximum expected F-measure among all set classifiers. Optimality in these two approaches are also achieved differently using different learning objec-tives. Unless otherwise stated, optimal classifiers refer to EUM-optimal classifiers, and optimal predictions refer to predictions by DTA-optimal classifiers. In this paper, we study the relative effectiveness of the two approaches, and develop theories and algorithms for this purpose. We focus on binary classification, as-suming the data is independently and identically dis-tributed (i.i.d.). The contributions of this paper are as follows. In Section 2, we establish a consistency result for empirical maximization of F-measures, together with bounds on the rate of convergence. This provides some insights into the factors affecting the convergence rate in EUM. In particular, our bounds suggest that rare classes require more data for performance guar-antee, which is consistent with our intuition. We then show that thresholding the true conditional distribu-tion on a large i.i.d. test set can perform as well as the best instance classifier, justifying the popular hy-brid approach of learning a conditional distribution followed by learning a threshold. We also show that an EUM-optimal classifier and a DTA-optimal clas-sifier are asymptotically equivalent if the probability measure for any set of instances with the same condi-tional probability of being relevant is negligible. In Section 3, we give a new O ( n 2 ) time algorithm for computing optimal predictions, assuming indepen-dence of labels. Our algorithm can compute opti-mal predictions on tens of thousand instances within seconds, significantly faster than previous algorithms which require hours or more. 1 In Section 4, we compare EUM and DTA on synthetic and real datasets. Our theoretical results are useful in explaining the experimental results. Empirically, EUM seems more robust against model misspecifica-tion, but given a good model, DTA seems better for handling rare classes on small datasets and a common scenario of domain adaptation. Let X and Y denote the input and output random variables. We assume there is a fixed but unknown distribution P ( X,Y ) that generates i.i.d. ( X,Y ) pairs during training and testing. We use X and Y to de-note their domains as well. In this paper, Y = { 0 , 1 } , with 0 for the negative or irrelevant class and 1 for the positive or relevant class. I(  X  ) is the indicator function. Let D n = { ( x 1 ,y 1 ) ,..., ( x n ,y n ) } be a set of n (pos-sibly non-i.i.d.) examples, and let x and y denote ( x 1 ,...,x n ) and ( y 1 ,...,y n ) respectively. If the pre-dicted labels are s = ( s 1 ,...,s n ), then precision p ( s , y ) is the number of true positives over the number of predicted positives, and recall r ( s , y ) is the number of true positives over the number of positives. F  X  measure (van Rijsbergen, 1974) F  X  ( s , y ) is a weighted harmonic mean of precision and recall. Formally, p ( s , y ) = P i s i y i / P i s i and r ( s , y ) = P i Thus, F  X  = (1 +  X  2 ) / (  X  2 /r + p ). In addition, F 0 precision and F  X  is the recall. F 1 is most frequently used in practice. Henceforth, we assume  X   X  (0 ,  X  ). 2.1. Uniform Convergence and Consistency for Consider an arbitrary classifier  X  : X 7 X  Y . Let F  X ,n (  X  ) denote the F  X  score of  X  on D n . Let p ij,n (  X  ) be the empirical probability that a class i instance is observed and predicted as class j by  X  ; that is, p F  X ,n (  X  ) = Let p ij (  X  ) = E(I( Y = i  X   X  ( X ) = j )), that is, the proba-bility that a class i instance is predicted as class j by  X  . Under the i.i.d. assumption, for large i.i.d. sample, the law of large numbers implies that p ij,n (  X  ) X  X  converge to p ij (  X  ) X  X . Thus F  X ,n (  X  ) is expected to converge to where  X  Y denotes P ( Y ). Hence we can define this to be the F  X  -measure of the classifier  X  . The above heuristic argument is formalized below. We often omit  X  from the notations whenever there is no ambiguity. All proofs are in the supplement (See foonote 1). Lemma 1. For any &gt; 0 , lim ) = 1 .
 By using a concentration inequality, such as the Ho-effding X  X  inequality, in place of the law of large num-bers, we can obtain a bound on the convergence rate. Lemma 2. Let r ( n, X  ) = q 1 2 n ln 6  X  . When r ( n, X  ) &lt; 2(1+  X  2 ) , then with probability at least 1  X   X  , | F  X ,n F We now show that training to maximize the empirical F  X  is consistent, using VC-dimension (Vapnik, 1995) to quantify the complexity of the classifier class. Theorem 3. Let  X   X  X 7 X  Y , d = V C ( X ) ,  X  F The above bound indicates that for smaller  X  1 and  X  , more samples are probably required for convergence to between F  X ,n (  X  ) and F  X  (  X  ) is at most 6(1+  X  2 ) 2.2. Optimality of Thresholding in EUM We now consider a common EUM approach: learn-ing a score function and then using a fixed threshold on the score function. This threshold is obtained by optimizing the F-measure on the training data. Assume we know the true conditional distribution P ( Y | X ). Consider the class T of probability thresh-2 T  X  X  0 has VC dimension 1, so empirical maximiza-tion of F-measure for this class is consistent. Although T  X  X  0 does not contain all possible classifiers on X , an optimal classifier can be found in this class. Let t Theorem 4. For any classifier  X  , F  X  (  X  )  X  F  X  ( t  X  ) . Thresholding is often applied on a score func-tion f : X 7 X  R , rather than on the true condi-tional distribution. For example, output of a sup-port vector machine is commonly thresholded. Let f tion f is called an optimal score function if there is a  X  tion for a score function to be optimal. A score func-tion f is rank-preserving if it satisfies f ( x 1 ) &gt; f ( x condition relates rank-preservation to optimality: Theorem 5. A rank-preserving function is an optimal score function.
 By Theorem 5, we can sidestep learning the true dis-tribution and instead try to learn a function which is likely to be rank-preserving. An optimal score func-tion may not be rank-preserving. For example, we can swap the scores of x  X  X  above the optimal threshold. 2.3. An Asymptotic Equivalence Result We now investigate the connections between EUM-optimal classifiers and DTA-optimal classifiers when the true distribution P ( X,Y ) is known. By definition, a DTA-optimal classifier is expected to be better than an EUM-optimal classifier if tested on many i.i.d. test sets. We shall give an asymptotic equivalence result for EUM-optimal classifiers and DTA-optimal classifiers on large i.i.d test sets. In light of Theorem 4, we only need to consider an optimal probability-thresholding classifier as a representative EUM-optimal classifier. In the following, let x = ( x 1 ,...,x n )  X  X n be an i.i.d. sequence of observations. For any classifier  X  , let taken under the conditional distribution P ( y | x ). The following theorem says that for an arbitrary classifier  X  , when n is large enough, then for any x , the expected F-measure of  X  ( x ) is close to F  X  (  X  ).
 Theorem 6. For any classifier  X  , any , X  &gt; 0 , there exists N  X ,, X  such that for all n &gt; N  X ,, X  , with proba-Such approximation holds uniformly for the class T . 3 Lemma 7. For any , X  &gt; 0 , there exists N  X ,, X  such that for all n &gt; N  X ,, X  , with probability at least 1  X   X  , for all  X   X  [0 , 1] , | E[ F  X  (I  X  ( x ) , y )]  X  F  X  (I The above uniform approximation result leads to the following asymptotic equivalence result.
 Theorem 8. Let s  X  ( x ) = max s E[ F  X  ( s , y )] , with s satisfying { P (1 | x i ) | s i = 1 } X  X  P (1 | x i ) | s i Let t  X  = arg max t  X  X  F  X  ( t ) . Then for any , X  &gt; 0 , (a) There exists N  X ,, X  such that for all n &gt; N  X ,, X  E( F  X  ( s  X  ( x ) , y )) &lt; E[ F  X  ( t  X  ( x ) , y )] + . (b) There exists N  X ,, X  such that for all n &gt; N  X ,, X  with probability at least 1  X   X  , | F  X  ( t  X  ( x ) , y )  X  F ( s  X  ( x ) , y )) | &lt; .
 Part (a) says that the t  X  ( x ) and s  X  ( x ) have almost the same expected F  X  , and Part (b) says that for a large i.i.d. test set ( x , y ), t  X  and s  X  have almost identical F The constraint on s ensures that instances with the same probability of being positive are placed in the same class. In general, optimal predictions may not satisfy this constraint (Lewis, 1995). However, if the underlying distribution satisfies that P ( P (1 | X ) =  X  ) = 0 for any  X  , then the above result is essentially this: given P , an optimal prediction and the prediction using the optimal threshold are asymptotically equiv-alent. This is demonstrated empirically in Section 4. We first discuss approximations to EUM, then discuss DTA and present a new efficient prediction algorithm. 3.1. Approximations to the EUM Approach Exact empirical optimization of F-measures for a para-metric family is difficult due to its complex piecewise linear nature, and typically only approximations of the F-measures are maximized. We discuss three methods. In view of the optimality of probability threshold-ing classifiers, it is natural to first learn an estimate p ( Y | X ) for P ( Y | X ), and then learn an optimal thresh-old  X  . If p ( Y | X ) is chosen from a parametric family using the maximum likelihood (ML) principle, then under very general conditions, the learned distribution follows an asymptotically normal convergence to the model with smallest KL-divergence to the true distri-bution (White, 1982). Thus when the model family is well-specified, the resulting classifier is asymptotically optimal. We call this the ML  X  approximation . Strictly, this is a combination of the conditional probability es-timation and F-measure optimization of the threshold, and the convergence rate in Theorem 3 does not apply. Jansche (2005) learned a logistic regression model p ( Y | X, X  ) by maximizing the empirical F  X  in eq. 1, but with each binary decision s i replaced by the predic-tive probabilities p i = p (1 | x i , X  ). The eventual classi-whether this method is consistent or whether it follows any asymptotic convergence. There is also no apparent reason to use 0.5 as the threshold, so we shall optimize the threshold on the training data in addition to esti-mating  X  . We call this the F  X  approximation . We considered learning a rule h ( x ) = I( p (1 | x, X  ) &gt;  X  ) directly, where  X  ,  X  are parameters, by approximating large  X  . However, this seemed to overfit easily, and it We will not consider it further. 3.2. Maximizing Expected F-measure Given a utility function U ( s , y ), the decision-theoretic In general, the true distribution P is not known and is estimated. The approach that involves first esti-mating true distributions using maximum likelihood (ML) and then making decision-theoretic optimal pre-dictions will be called the ML E approach . We discuss algorithm for computing the optimal predictions. First, the asymptotic convergence of ML (White, 1982) when estimating with sufficient training examples in a well-specified family. In practice, we will not know whether the model family is well-specified. Neverthe-can yield results indistinguishable from the optimal if the model family is misspecified but contains a reason-able approximation to the true distribution.
 Second, for arbitrary utility function U , computing the expectation can be computationally difficult. But for the case when the utility function is an F-measure, and Algorithm 1 Compute f  X  ;1 ,...,f  X  ; n , where  X  2 = q/r 1: For 0  X  i  X  n , set C [ i ] as the coefficient of x i in 2: For 1  X  i  X  ( q + r ) n , S [ i ]  X  q/i ; 3: for k = n to 1 do 5: Divide C by p k x + (1  X  p k ); 6: for i = 1 to ( q + r )( k  X  1) do 7: S [ i ]  X  (1  X  p k ) S [ i ] + p k S [ i + q ]; 8: end for 9: end for designed by exploiting the following characterization of an optimal prediction. Let p i = P (1 | x i ). Theorem 9. (Probability Ranking Principle for F-measure, Lewis 1995) Suppose s  X  = max s E( F  X  ( s , y )) . Then min { p i | s  X  i = 1 } X  max { p i | s  X  i = 0 } . Thus the decision-theoretic optimal prediction con-tains the top k instances that are most likely to be pos-itive for some k  X  X  0 ,...,n } . This reduces the number of candidate predictions from 2 n to n + 1. We shall use this result to give an efficient algorithm for computing the optimal predictions. 3.2.1. A Quadratic Time Algorithm We give an O ( n 3 ) time algorithm for computing the optimal predictions, then improve it to O ( n 2 ) when  X  2 is rational, which is often the case.
 Let F  X  ; k ( y ) be the F  X  -measure when the first k instances are predicted as positive, then we have F  X  ; k ( y ) = (1 +  X  2 ) P f satisfying S 1: k = k 1 and S k +1: n = k 2 , their F  X   X  X  are happens is P ( S 1: k = k 1 ) P ( S k +1: n = k 2 ), thus One can show that P ( S 1: k = i ) and P ( S k +1: n = i ) are the coefficients of x i in Q k j =1 [ p j x + (1  X  p j Q j = k +1 [ p j x +(1  X  p j )] respectively. Thus, each f  X  ; k be computed in O ( n 2 ) time using O ( n ) space. Hence computing all f  X  ; k  X  X  takes O ( n 3 ) time and O ( n ) space. For rational  X  2 , we can improve the computation to O ( n 2 ) time and O ( n ) space. The key is to note that f where s ( k, X  ) = P n  X  k k rational  X  , the s values required for the f  X  ; k  X  X  are shared. To compute s , use s ( n, X  ) = 1 / X  , and which follows from P ( S k : n = i ) = p k P ( S k +1: n = i  X  1) + (1  X  p k ) P ( S k +1: n = i ).
 The pseudo-code is given in Algorithm 1, with q/r as the reduced fraction of  X  2 . Correctness can be seen by observing that at line 3, S [ i ] = s ( k,i/q ), and C [ k P ( S 1: k = k 1 ). In practice, polynomial division can be numerically unstable, and it is preferred to precompute all the C [ i ] X  X  using O ( n 2 ) time and space first. We empirically demonstrate that EUM can be more ro-bust against model misspecification, but DTA can be better for rare classes on small datasets and a common scenario of domain-adaptation. We use a synthetic dataset, the Reuters-21578 dataset, and four multil-abel classification datasets. 4.1. Mixtures of Gaussians We consider a mixture of Gaussians on D dimensions: P ( X,Y ) =  X  Y N ( X ;  X  Y ,  X  Y ), with  X  1 =  X  0 = I  X  1 = ( S + O ) 1 / where S and O are non-negative constants. Thus S is the distance between the centers. We shall vary S , O , D ,  X  1 and the number of training examples N tr All instances are i.i.d. The optimal F 1 achievable by a classifier  X  can be computed (see eq. 2), and it depends only on S and  X  1 . N tr determines how close the estimated distribution is to the optimal model; and the number of test examples, N ts , affects the gap in the performance between the thresholding method and the expectation method (Theorem 8).
 We train logistic regression (LR) models using three different attribute vector representations: R 0 consists of the coordinates only, R 1 is R 0 with an additional dummy attribute fixed at 1, and R 2 is R 1 with ad-ditional all degree two monomials of the coordinates. LR with R 2 includes the true distribution. The meth-where last two methods use the true model P ( X,Y ) for thresholding and expectation.
 The first column in Table 1 lists the parameter set-tings. For the row headed by Default , we use D = 10, S = 4, O = 0, N tr = 1000, N ts = 3000, and  X  1 = 0 . 5. This dataset is low dimensional, almost noiseless, bal-anced and has sufficiently many train and test in-
R stances. 4 Each of the remaining rows uses the same set of parameters, except for the one parameter indi-cated on the first column. LR with R 0 or R 1 contains a good approximation to the true distribution for all settings except  X  1 = 0 . 05 and O = 50. For  X  1 = 0 . 05, the class is imbalanced and such imbalance cannot be modelled without the dummy attribute. Thus R 0 will not give a good model, but R 1 will. For O = 50, the centers are far from the origin, and this makes both R 0 and R 1 inadequate for density estimation.
 In Table 1, the F 1 results for Truth E , Truth  X  and The-ory (the theoretical optimal F 1 ) are similar. These are expected according to Theorem 8. Most other scores these scores are expected due to the presence of a good approximation to the true distribution in the model family, and the asymptotic convergence property of theoretical convergence to an optimal classifier, the re-sults suggest that such convergence may hold.
 The scores obtained using R 2 are generally lower than scores obtained using R 0 and R 1 under the settings Default , S = 0 . 4, D = 100, and N tr = 100, though R 2 gives a well-specified model class while R 1 and R 0 do not. Thus, a well-specified model class is not necessar-ily better. This is because a misspecified model class with a small VC dimension can converge to the opti-mal model within the class using fewer samples than a well-specified model class with a higher VC dimension. To choose a class of the right complexity, one may fol-low the structural risk minimization principle (Vapnik, 1995). This requires bounds like those in Lemma 2 and Theorem 3. However, the given bounds cannot be used because they only apply for large samples.
 The gaps between R 2 scores and the optimal score for Default is significantly smaller than the gaps for S = 0 . 4, D = 100, N tr = 100, and  X  1 = 0 . 05. This suggests that higher noise level, higher model class complexity, smaller training size, and smaller positive ratio make it harder to learn a good classifier. Note that Theorem 3 already suggests that in EUM, smaller positive ratio can make learning more difficult.
 For the setting  X  1 = 0 . 05, using R 0 , ML E performs formance is expected due to poor quality of the learned by Theorem 5: the thresholding method can remain optimal when the score function is rank-preserving but not close to the true probability distribution. For the setting O = 50, both ML E and ML  X  perform poorly R by model misspecification, it is still relatively robust. In addition, for  X  1 = 0 . 05 and O = 50, F  X  has much ML  X  . This suggests that if the model class is severely misspecified, then EUM can be more robust than DTA . with N ts = 100 (Theorem 8 only holds for large test cantly better than ML  X  when  X  1 is small. To illus-trate, Table 2 gives the results when the same setting as  X  1 = 0 . 05 in Table 1 is used to generate the data. can be better than ML  X  and F  X  on rare classes . 4.1.1. Effect of Model Quality We also perform experiments to study the effect of incorrect probability models on ML E . We use the De-fault setting in the previous section, with  X  1 = 0 . 5 and S = 4 changed to S = 2, as the true distribution, to generate a set of 3000 i.i.d. test instances. We make op-timal predictions using an assumed distribution which is the same as the true one except that we vary  X  For each  X  1 , we compute the F 1 and the Kullback-0 0 . 2 0 . 4 0 . 6 0 . 8 1 KL 1  X  F 1 Leibler-divergence (KL) from the true to the assumed distribution on the test set. These are plotted in Fig-ure 1(b), where 1  X  F 1 is plotted instead of F 1 . Fig-ures 1(a) and 1(c) plot for similar experiments, but using 0.1 and 0.9 as the true  X  1 instead. Our choice of S = 2 instead of S = 4 for the true distribution has made the difference between the true and assumed dis-tributions more pronounced in the plots. Comparing the curves for KL and 1  X  F 1 within each figure, we see that the F-measure of DTA is roughly positively cor-related with the model quality. The plot for 1  X  F 1 in Figure 1(a) exhibits higher curvature around the true  X  1 than those in the other two figures. This suggests that if the true distribution has a small positive ratio, the performance is more sensitive to model quality. 4.1.2. Domain Adaptation In domain adaptation, the test distribution differs from the training one. One common scenario is when P ( X ) changes but P ( Y | X ) does not. Using the mix-ture of Gaussians with D = 10, S = 4, O = 0 and  X  1 = 0 . 5, we generate 5000 i.i.d. training instances, and 5000 test instances with P ( Y | X ) &lt; 0 . 5. The F are 21%, 38%, 11% and 36% respectively. Similar re-sults are obtained under similar settings. Under such conditions, DTA is more robust than EUM. 4.2. Text Classification We evaluate on the Reuters-21578 dataset 5 using the ModApte partition, which has 9603 training docu-ments and 3299 test documents. We train two mod-els: the standard multinomial na  X  X ve Bayes (NB) model and a LR model, using word occurrence counts and a dummy attribute fixed at one. Both models are reg-ularized. For NB, we use the Laplace corrector with one count for class and word counts. For LR, we use the Gaussian norm on the parameters. We use only those topics with at least C positive instances in both the train and test sets, and we vary C . Table 3 reports macro-F 1 scores (the F 1 averaged over topics), where ML . 5 uses 0.5 to threshold the probabilities, 100 73.7 73.5 73.7 75.5 75.9 76.5 75.8 In Table 3, although NB generally does not provide cents better for rare classes. Chai (2005) used Gaus-sian process and obtained similar conclusion. 4.3. Multilabel Datasets We evaluate on four standard multilabel classification datasets. 6 We train regularized LR, with the regu-larization parameter for each class selected using two fold cross validation. Macro-F 1 scores are shown in Table 4. The bracketed scores are obtained by choos-ing the regularization parameter giving a model with minimum empirical KL divergence on the test data. Each bracketed score is higher than its non-bracketed counterpart, thus models closer to the true one per-smaller C , suggesting ML E is better for rare classes. We gave theoretical justifications and connections for optimizing F-measures using EUM and DTA. We em-pirically demonstrated that EUM seems more robust against model misspecification, while given a good model, DTA seems better for handling rare classes and a common domain adaptation scenario.
 A few important questions are unanswered yet: ex-istence of interesting classifiers for which EUM can be done exactly, quantifying the effect of inaccurate models on optimal predictions, identifying conditions under which one method is preferable to another, and practical methods for selecting the best method on a dataset. Results in this paper only hold for large data sets, and it is important to consider the case for small number of instances. Experiments with and analyses of other methods may yield additional insights as well. This work is supported by DSO grant DSOL11102.
