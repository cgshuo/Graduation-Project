 Recen t pap ers have claimed that the result of K-means clus-tering for time series subsequences (STS clustering) is inde-pendent of the time series that created it. Our pap er revisits this claim. In particular, we consider the follo wing question: Given sever al time series sequenc es and a set of STS clus-ter centr oids from one of them (gener ated by the K-me ans algorithm), is it possible to reliably determine which of the sequenc es produc ed these cluster centr oids? While recen t results suggest that the answ er should be NO , we answ er this question in the armative .

We presen t cluster shap e distanc e , an alternate distance measure for time series subsequence clusters, based on clus-ter shap es . Giv en a set of clusters, its shap e is the sorted list of the pairwise Euclidean distances between their cen troids. We then presen t two algorithms based on this distance mea-sure, whic h matc h a set of STS cluster cen troids with the time series that pro duced it. While the rst algorithm cre-ates smaller \ ngerprin ts" for the sequences, the second is more accurate. In our exp erimen ts with a dataset of 10 sequences, it pro duced a correct matc h 100% of the time.
Furthermore, we o er an analysis that explains why our cluster shap e distance pro vides a reliable way to matc h STS clusters to the original sequences, whereas cluster set dis-tance fails to do so. Our work establishes for the rst time a strong relation between the result of K-means STS clus-tering and the time series sequence that created it, despite earlier predictions that this is not possible.
 H.3.3 [ Information Searc h and Retriev al ]: clustering, retriev al mo dels Algorithms, Exp erimen tation, Measuremen t cluster shap e distance, K-means subsequence clustering, mean-ingless
It has been claimed in recen t literature [18] (preliminary version [19]) that the result of K-means clustering for time series subsequences (STS clustering) is indep endent of the time series that created it: Speci cally , it was sho wn that when measuring the distance between sets of cluster cen troids ( cluster set distanc e ), this distance is on average no smaller for cluster sets from the same time series, obtained by using di eren t random seeds in the clustering algorithm, than for cluster sets from di eren t time series.

The claim of meaninglessness for STS subsequence clus-tering \has cast a shado w over the emerging discipline of time series data mining" [28]. This work has also generated a urry of follo w-up researc h activit y, including the curren t pap er.

Our pap er focuses on the issue of dependenc e between the output of STS clustering (i.e., the set of clusters) and its input (i.e. a time series sequence). Speci cally , we revisit the follo wing question, whic h was believ ed to be solv ed in the negativ e by [18]:
To accomplish this task, we rst de ne an alternate dis-tance measure for time series subsequence clusters, cluster shap e distanc e . Giv en a cluster set, its shap e consists of the pairwise Euclidean distances between the cluster cen troids in the set; cluster shap e distance is the Euclidean distance between the shap es of two cluster sets. Our distance mea-sure can be con trasted with the older measure of cluster set distanc e . Unlik e the latter, our measure is invarian t under translations and rotations of the cluster sets.
When using the new distance measure, we nd that the input and output of subsequence clustering are no longer in-dep enden t. In fact, we pro vide two algorithms for matc hing an STS cluster set with the original time series that gener-ated it. These algorithms tak e as input a dataset consisting of multiple time series sequences, and a set of queries , where eac h query is a set of STS cluster cen troids that are to be matc hed to one of the sequences in the dataset.

Giv en a dataset D = f S 1 ; S 2 ; :::S n g and a query Q (i.e. the cen troids of an STS cluster set generated by some dataset from D ), our algorithms return some index j into D ; this means that S j is believ ed to be the time series that gener-ated Q (i.e. its matc h). If this is correct, then we say that our algorithm has pro duced a correct matc h.
 Both matc hing algorithms con tain two phases, where phase I preprocesses the time series sequences in our dataset, and phase II uses the results of phase I to match a query with some sequence in the dataset. The separation of matc hing algorithms into two phases allo ws them to be easily adapted to a datab ase setting , where the dataset of multiple time se-ries sequences is given ahead of time, and the queries are supplied in an ad-ho c fashion one at a time. This setting allo ws prepro cessing to be done o -line, and ensures a very quic k answ er for eac h query .

In phase I, both algorithms compute a list of cluster shap es for eac h sequence, by performing multiple clustering runs with di eren t initial seeds. The rst algorithm outputs the aver age of these cluster shap es, whic h we call a cluster con-stel lation , whereas the second algorithm outputs the whole list . While the rst algorithm creates smaller \ ngerprin ts" for the sequences, the second is more accurate. In our exp er-imen ts with a dataset of 10 sequences, it pro duced a correct matc h 100% of the time!
These results establish a strong dep endence between the result of K-means STS clustering and the time series se-quence that created it. As far as we kno w, ours is the only work that achiev es this while using the same K-means STS clustering algorithm as in [18], thereb y dispro ving their claim. Furthermore, we are able to distinguish between man y sequences at a time, whereas other work, suc h as [18, 8, 27], only distinguished between two sequences at a time.
In addition, we o er an analysis that explains why our cluster shap e distance pro vides a reliable way to matc h STS clusters to the original sequences, whereas cluster set dis-tance fails to do so.
 Outline. We start by pro viding some bac kground in Sec-tion 2. We describ e our algorithm for STS clustering, whic h is the same as that in [18]. In Section 3 we de ne the two notions of distance between cluster sets, cluster set distanc e and cluster shap e distanc e . The latter is a new approac h to measuring distances between sets of cluster cen troids. In Section 4, we pro vide an algorithm for matc hing an STS cluster set with the original time series, whic h is based on cluster shap e distance. In Section 5, we presen t accuracy tests for our matc hing algorithm, and discuss their results. In Section 6, we consider the notion of sequenc e smo othness , and observ e its correlation with certain asp ects of the se-quence's cluster shap es, suc h as their num ber. In Section 7, we explain why the shap e of the cluster cen troids dep ends on the sequence, whereas the actual position of the cluster cen troids does not; we also explain the correlation between sequence smo othness and the num ber of shap es. We con-clude with Section 8.
In this section, we pro vide the bac kground information about the K-means subsequence clustering algorithm.
Our algorithm for subsequence clustering is the same as that in [18]; we describ e and discuss it in this section.
The input to the STS clustering algorithm consists of a time series sequence of length m , a windo w size w , and the num ber of clusters K . We slice the original time series into m w + 1 subsequences, eac h of length w and we put these into a subsequence matrix M [ m w + 1][ w ]. We then nor-malize eac h subsequence, using the standard technique that was rst introduced to time-series similarit y querying in [13]. Normalization is performed by subtracting the subsequence average from eac h value in the subsequence, and dividing it by the standard deviation. As a result, eac h subsequence has an average of 0 and a standard deviation of 1. We then cluster the normalized subsequences from M using the K-means clustering algorithm introduced in [20], as follo ws: STS Clustering Algorithm. 1. Randomly choose K subsequences indices in the range 2. For eac h subsequence in M , calculate its Euclidean 3. After eac h subsequence has been assigned to a cluster, 4. Rep eat steps 2-3 until the cluster cen troids remain un-We refer to one run of this algorithm as a clustering run . Giv en a data sequence S , the num ber of clusters K , and the windo w length w , a clustering run returns some set of cluster cen troids for S .
We now give a persp ectiv e on K-means clustering, whic h is a subroutine of K-means STS clustering. It is a technique for disco vering groups of similar patterns in a large collection of unlab eled vectors. It was used by one of the authors as early as 1964 for nding similar Han (Chinese) characters for the rst level of a two-lev el classi er [4], and later for sorting unlab eled prin ted glyphs into alphab etic classes in a cryptographic approac h to optical character recognition [5, 6, 24], for feature extraction [23], and for unsup ervised crop classi cation by remote sensing [25].

The algorithm was popularized as a general metho d for \exploratory" multiv ariate analysis of unlab eled data [2, 20]. A variation that addressed some of the shortcomings of the elemen tary algorithm by splitting and merging classes was called Isodata [3]. In the comm unications comm unit y, it-erativ e minimization of the sum-of-squared-error criterion became kno wn as Vector Quantization [12, 29]. Examples of easy-and dicult-to-cluster pattern con gurations were presen ted in [22] and in [15].
Among the rst attempts at evaluating its e ectiv eness was [9]. Variations of the metho d with resp ect to initial-ization, cost function, splitting and merging clusters, and distance metrics, are describ ed in [16, 29, 10]. Curren t researc h focuses on com bining multiple cluster con gura-tions obtained by di eren t algorithms, i.e., clustering en-sembles [30].

It is easy to see that both the sample re-assignmen t step and the recomputation of the cluster cen troids decrease the sum of the distances of the samples to their cluster cen troids. Therefore starting with any set of seed points (initial cluster cen troids), K-means alw ays con verges to a (local) minim um. Di eren t seeds may lead to di eren t nal cluster con gura-tions with di eren t sums of squared error. Because the sum of the within-cluster and between-cluster scatter is a con-stan t, the algorithm sim ultaneously maximizes the latter.
The nal num ber of clusters is not necessarily equal to the num ber of initial seeds: some clusters may become un-populated, but neither step forms new clusters. Finally , and for our analysis most imp ortan tly, multiple global minima, eac h reac hable by a di eren t initialization, may exist.
In STS analysis, the K-means algorithm is used to nd the averages of overlapping subsequences. The K cluster cen troids are represen tativ e of the di eren t subsequences, and may therefore be considered as a condensed represen-tation of the entire sequence (much like the eigen vectors in Karh unen-Lo eve Transform or Principal Comp onen t Anal-ysis). Di eren t sequences typically yield di eren t cen troids. It app ears, however, that with di eren t initializations, even the same sequence may lead to di eren t cen troids.
Our abilit y to correctly matc h K-means STS clusters to the sequences that pro duced them hinges on the issue of cluster distance. While it has been sho wn imp ossible to accomplish this matc hing by measuring the Euclidean dis-tances between sets of cluster cen troids, we avoid this prob-lem by de ning an alternate distance measure for sets of cen troids. This section discusses both distance measures.
In [18], it was sho wn that the cluster set distanc e between two sets of cluster cen troids from the same time series, ob-tained by using di eren t random seeds in the clustering al-gorithm, is on average no smaller than that between two sets of cluster cen troids from di eren t time series.

Definition 1. (Cluster Set Distance) Given two sets of K cluster centr oids , X = ( x 1 ; x 2 ; : : : ; x K ) and Y , their cluster set distance is the sum of minimal Euclide an dis-tanc es: wher e dist is the Euclide an distanc e, and closest ( x i notes the cluster center in Y with the smal lest Euclide an distanc e to x i (nearest neighb or).

Figure 1 illustrates the de nition. In this case, the cluster set distance between X and Y is the sum of AD + BD + CD ; the cluster distance between Y and X is DB + EC + F B . Note that this distance measure is not comm utativ e.
We have implemen ted subsequence clustering and cluster set distance, and con rmed the results observ ed in [18], us-ing both Matlab and Java. We agree that the cluster set distance between the two sets of clusters for the same time series, obtained by using di eren t random seeds in the clus-tering algorithm, is no smaller than for two sets of clusters from di eren t time series. In addition, we have con rmed another observ ation in [18], that the plots of the cluster cen-troids resem ble sine waves, and that the mean of the cluster cen troids, when weighed by cluster size, is a line or very close to it.

While it is clear that the cluster set distanc e measure does not pro duce meaningful results, we will presen t an alter-nate measure for comparing time series subsequence clusters. This new measure, called cluster shap e distanc e will allo w us to create algorithms that de nitiv ely establish a dep endence between the input of STS clustering and its output. We dis-cuss it next.
In this section, we introduce the notion of cluster shap es , that will later be used for an alternate distance measure be-tween sets of cluster cen troids. They are de ned as follo ws:
Definition 2. (Cluster Shap e) Given a set S of K clusters with centr oids C 1 ; : : : ; C k , the shap e of S is the sorte d sequenc e of all the pairwise Euclide an distanc es be-tween the cluster centr oids in S; it contains K ( K 1) = 2 numb ers.

To illustrate this de nition, imagine that after clustering a set of time-series subsequences using the algorithm in Sec-tion 2.1, we have obtained K cluster cen troids of length w . We then calculate the Euclidean distances for all pairs of these cen troids; there are K ( K 1) = 2 suc h pairs. For ex-ample, when K = 3, we obtain 3 distances D 1 ; 2 ; D 1 ; 3 where D i;j represen ts the Euclidean distance between clus-ter cen troids i and j .

Figure 2 illustrates the results for 9 di eren t clustering runs for the ocean series from [17]. 1 It sho ws the distances between cluster cen troids , as well as their sum and average. These results pro duced by clustering runs with K = 3 and w = 8 ; 16 ; 32; three di eren t runs were performed for eac h com bination of K and w .

It is easy to see that in Figure 2, when we consider dif-feren t runs with the same K and w , the sum and average are very close regardless of the initial choice of cluster cen-troids. While their distances (columns 4-6) are not necessar-ily close, the insigh t here is that the order of the distances is
All data sequences used in this pap er were of length 1000. 1 8 226 549 82 2.7771 4.4644 2.4942 3.2452 2 8 902 7 171 4.4855 2.7416 2.9164 3.3812 3 8 525 751 820 4.4928 2.5958 2.8388 3.3091 4 16 226 549 82 5.1477 6.5741 3.1317 4.9512 5 16 902 7 171 6.6168 3.6607 4.8998 5.0591 6 16 525 751 820 6.5801 3.2478 5.0325 4.9535 7 32 226 549 82 8.3883 9.2677 3.7964 7.1508 8 32 902 7 171 6.9156 9.4574 5.9889 7.4539 9 32 525 751 820 9.2988 4.9502 7.4518 7.2336
Figure 2: di eren t clustering runs for ocean series arbitrary . By sorting the set of distances D i;j for eac h run, the resulting lists of num bers 1 ; 2 ; 3 are also similar for di eren t initial seeds, as can be seen in Figure 3. This list of num bers is the shap e of the cluster set.
In this section, we introduce a new approac h to measur-ing distances between sets of cluster cen troids, called cluster shap e distanc e . With this new approac h, given n series from di eren t sources and m sets of cluster cen troids, these sets can be mapp ed bac k to their original series with high accu-racy .

Cluster shap e distanc e is just the distance between the shap es of two sets of cluster cen troids:
Definition 3. (Cluster Shap e Distance) Given two sets of clusters, X and Y , with cluster shap es f 1 ; 2 is the Euclide an distanc e betwe en their shap es: Note that the storage requiremen ts are much smaller for cluster shap e distance than for cluster set distance. In order to compute the cluster set distance between a given set of cluster cen troids S and another one that we will be receiving in the future (our query ), one needs to store S in its entiret y; for K cen troids of length w , this means K w values. On the other hand, to compute the cluster shap e distance between S and a future set, one only needs to store the shap e of S , whic h means K ( K 1) = 2 values. Since K is typically much smaller than 2 w , the amoun t of storage is greatly reduced.
Cluster constel lations are simply the result of averaging together man y cluster shap es for the same series:
Definition 4. (Cluster Constellation) A cluster con-stel lation is the aver age of cluster shap es resulting from mul-
Figure 4: cluster constellations for K = 3 ; w = 8
Figure 5: cluster constellations for K = 3 ; w = 16 tiple clustering runs over the same series with the same w and K . Cluster constel lations are denote d by ( 1 ; 2 ;
Example 1. Let A , B , C , and D be sets of cluster cen-troids produc ed by four clustering runs over the same time series, with K = 3 . Let a 1 ; a 2 ; a 3 be the shap e of A , wher e a is the shortest length betwe en any two cluster centr oids in A , a 2 is the second shortest, and a 3 is the longest. Similarly, we compute b 1 ; b 2 ; b 3 , c 1 ; c 2 ; c 3 , and d 1 ; d B , C , and D respectively. Then, ( 1 ; 2 ; 3 ) is the cluster constel lation for ( A; B; C; D ) , wher e i = avg ( a i ; b
As can be seen in Figure 3, for a given series with any given com bination of K and w , these shap es for various clustering runs tend to be very similar to eac h other. While all these shap es have K = 3, this is not necessary . These shap es remained similar when we varied K and w , or when we tried other series.

On the other hand, shap es for clustering runs from di er-ent time series, tend to be very di eren t. This is illustrated in Figures 4, 5, and 6. Figure 4 sho ws cluster constellations for ve sequences from [17], with K = 3 and w = 8. Fig-ure 5 is for the same sequences, with K = 3 and w = 16. Figure 6 is with K = 4 and w = 8. Eac h of these cluster constellations were computed by performing 100 clustering runs and averaging the resulting shap es together.
On the basis of these empirical observ ations, one can con-jecture that cluster shap e distance is a more appropriate distance for STS clusters than cluster set distance. Our conjecture will be con rmed in the next section by sho wing that cluster shap e distance can serv e as a basis for an algo-rithm that matc hes STS cluster sets bac k to their original time series. ocean 2.4787 2.5844 2.9337 2.9778 3.2012 4.7337 pac ket 2.2235 2.2518 2.3438 2.5314 2.5647 2.6129 soil 2.0568 2.0874 2.1464 2.1803 2.2105 2.2533 tide 2.4059 2.4585 3.3746 3.4247 4.0473 4.1776
Figure 6: cluster constellations for K = 4 ; w = 8
We now pro vide an algorithm for matc hing an STS clus-ter set with the original time series, whic h uses the distances between cluster shap es (and constellations), as opp osed to cluster set distances. There are two variations of this algo-rithm, whic h we treat as separate algorithms.

The input to both algorithms consists of: Both algorithms consist of two phases, where phase I pre-processes all the time series sequences in the input dataset, and phase II uses the results of phase I to matc h eac h query with some sequence in the dataset.
 STS Cluster Matc hing Algorithm I
I-a For eac h sequence in the dataset, perform Q di eren t I-b Store the resulting N constellations in a master table II-a For eac h query , compute its shap e, and nd the Eu-II-b The sequence with the smallest distance is the answ er In phase I, both algorithms compute a num ber of cluster shap es for eac h sequence, by performing multiple cluster-ing runs, and create a master table M . However, the rst algorithm stores in M , for eac h input sequence, the aver-age of these cluster shap es (a cluster constellation), whereas the second algorithm stores all these shap es in an individual table .
 STS Cluster Matc hing Algorithm II
I-a For eac h sequence in the dataset, perform Q di eren t
I-b Store the resulting N individual tables in a master II-a For eac h query , compute its shap e, and nd the Eu-II-b The sequence with the smallest distance is the answ er Figure 7 sho ws a set of 10 cluster shap es ( 1 ; 2 ; 3 these shap es came from a di eren t clustering run, two runs for eac h of the ve series listed in Figure 4. The information about the sequence that pro duced eac h shap e was then hid-den, and the cluster matc hing algorithm was used to matc h these shap es (queries) with the original sequences (dataset). Figure 7: sample shap es and their assignmen t for K = 3 ; w = 8 In this case, the assignmen ts are all correct, and app ear in the righ tmost column of Figure 7.

Note that both matc hing algorithms can be easily adapted to a datab ase setting , where the dataset of multiple time se-ries sequences is given ahead of time, and the queries con-sisting of STS cluster set cen troids are supplied in an ad-ho c fashion one at a time. This setting allo ws prepro cessing to be done o -line, and ensures a very quic k answ er for eac h query .
We now presen t our accuracy tests for the two matc hing algorithms de ned above. These tests used a dataset of ten sequences from [17], listed in Figure 8. While suc h a dataset seems small, it must be remem bered that earlier work has predicted that reliable matc hing would be imp ossible even in the case of only two sequences! Ten sequences is therefore more than sucien t for our purp oses.
 Accuracy test for STS cluster matc hing 1. We varied K and w , so K took values between 3 and 2. There were 10 queries for eac h invocation of the algo-3. Q , the num ber of clustering runs per sequence (see step 4. Eac h invocation of the algorithm pro duced 10 poten-5. We then computed the percen tage of correct matc hes
The results of our test for Algorithm I app ear in Fig-ure 8. Overall there is an accuracy average of over 90%. This means that, given a set of cluster cen troids, Algorithm I will correctly iden tify whic h series pro duced it in over 90% of the cases.
The outcome for Algorithm II is more dramatic. All the scores are 100% ! These results sho w that it is possible to tell whic h STS cluster set comes from whic h input sequence, by using the cluster shap e distance as a similarit y measure. If we are able to iden tify the original series that pro duced the cluster cen troids, this in turn means that the output STS clustering does dep end on input, despite earlier claims to the con trary .
In this section, we consider the notion of smo othness for data sequences, and its e ect on the num ber of unique shap es for the sequences, as well as on those shap es themselv es. We observ e that there is a strong correlation between them.
First, we de ne the notion of sequence smo othness . If we look at the DFT of a sequence, the energy for a large class of signals ( color ed noises ) is concen trated in the rst few coecien ts. These signals have a skewed energy (or power) spectrum, that drops as O ( f b ), where f is the fre-quency . [11]
Definition 5. (Smo othness Coecien t) Given a time series sequenc e , its smo othness coecient b is measur ed as follows: j DFT m ( ) j = O ( f b ) , wher e DFT m ( ) represents the rst m coecients of the Discr ete Fourier Transform of for some smal l value of m . [11] From the de nition, it follo ws that the smaller the exp onen t of f , the few er coecien ts of the DFT are needed to store most of the amplitude of the time series. The actual value strongly dep ends on the nature of the data sequences. For example, for b = 2, we have brown noise , or a random walk, whic h mo dels stock movemen ts and exc hange rates; its en-ergy spectrum follo ws O ( f 2 ). Informally , the larger the b , the smo other the time series.
 For our set of 10 sequences, the values of b are sho wn in Figure 9. These were computed with the value of 8 for m ; since the DFT coecien ts are complex, this represen ts 16 real num bers.

In the next section, we sho w a correlation between the b value of a sequences and some prop erties of its shap e. For that purp ose, we classify the sequences in Figure 9 into two groups: less smo oth (sequences 1-6) and mor e smo oth (sequences 7-10).
Figure 9: The smo othness of the data sequences
Figure 10: unique shap es for ocean ( K = 3 ; w = 16)
When examining the individual tables in Algorithm II, we often nd that after man y clustering runs on any one series, only a few unique shap es are returned; that is, all other shap es are copies of one of these (up to some precision factor).

For instance, after 100 clustering runs for the ocean se-ries, with K = 3 and w = 16, only 9 unique shap es (up to precision of 4 decimal digits) were found; Figure 10 sho ws these shap es. Note that all three shap es for the ocean series from Figure 3 with w = 16 have exact matc hes in Figure 10, namely shap es 1, 8, and 5.

However, other series pro duced more unique shap es, suc h as 82 for the burstin sequence ( K = 3 ; w = 16). Are there any di erences between these sequences that can accoun t for this disparit y? The explanation seems to be in their smo othness coecien t b .

In general, we have observ ed a strong correlation between sequence smo othness and the num ber of unique shap es for it. Figure 11: num ber of unique shap es for the two groups (precision 4) Figure 12: num ber of unique shap es for the two groups (precision 3) Figure 11 sho ws the maxim um num ber of unique shap es for the sequences in the two groups from Figure 9, for various com binations of K and w . It is clear that the more smo oth sequences have few er unique shap es than the less smo oth ones.

While there are more shap es for the less smo oth sequences, they tend to be \closer" to eac h other. That is, when we use precision 3 rather than 4, the num ber of less smo oth shap es decreases signi can tly whereas the num ber of the more smo oth shap es does not (Figure 12).

Furthermore, there is a strong correlation between se-quence smo othness and the shap es themselv es. Speci cally , we consider the shap e skew , de ned as follo ws:
Definition 6. (Shap e Skew) Given a shap e ( 1 ; 2 ; : : : ) , its skew is the standar d deviation of i 's, divide d by their av-erage: skew ( 1 ; 2 ; : : : ) = stddev ( 1 ; 2 ; : : : ) / avg ( For equilateral triangles, skew is 0; the further a triangle is from an equilateral, the greater is its skew.

Figure 13 sho ws the average skew of the sequences in the two groups from Figure 9. It is clear that the shap es for the more smo oth sequences are more skewed than for the less smo oth ones.
We will now explain why the shap e, or structur e of the con guration of STS cluster cen troids dep ends on the se-quence (Section 5), whereas the actual position of the clus-ter cen troids does not. We also explain why the num ber of shap es dep ends on the smo othness coecien t (Section 6). In order to explain the exp erimen tal results presen ted in Section 5, we rst reform ulate the usual K-means objectiv e function or optimization criterion, i.e., the Sum-of-Squar ed-Err ors (SSE), in terms of the scalar pro ducts (pairwise dis-tances) between pattern vectors.

Let N d -dimensional row vectors, lab eled x 1 ; : : : ; x be clustered into K clusters with cen troids 1 ; : : : ; Let I ( k ) be the index of the cluster to whic h x i was assigned, and n ( k ) be the num ber of patterns in cluster k . Then, the follo wing is the form ula for SSE: = min Therefore the con ventional objectiv e function SSE is equiv a-lent to maximizing the sum of the intra-cluster scalar pro d-ucts where We conclude that if all the vectors have the same length, then two cluster con gurations with iden tical intra-cluster pairwise scalar pro ducts x i x 0 j (or alternately iden tical pair-wise distances ( x i x j )( x i x j ) 0 , must yield the same value of SSE. However, the cluster cen troids of the two con gura-tions will, in general, be di eren t.
Figure 14 sho ws a set of points in 2-D vector space that exhibit multiple global maxima of the SSE. When clustered with the K-means algorithm with K = 3 these samples will be group ed either as indicated in Figure 15 or as in Fig-ure 16. The two con gurations have the same SSE (the sum of the distances from the patterns to their cluster cen-troids), and the same shap e (here, congruen t triangles), but eviden tly not the same position; that is, the cluster cen troids cannot be mapp ed into one another by ren um bering.
The existence of multiple equiv alen t con gurations for the points in Figure 14 is due to symmetries in the con gura-tions of the subsequence vectors. We will sho w that appro x-imate symmetries inevitably arise in STS clustering because man y pairs of shifted subsequences have almost the same Euclidean distance between them. This results in multiple cluster con gurations with nearly iden tical SSEs. Figure 15: Cluster con guration, with cen troids sho wn with crosses, found by K-means algorithm with K = 3 . Figure 16: Cen troids found with di eren t initial cluster cen troids. The two cluster con gurations have the same shap e and the same SSE, but the cen troids cannot be mapp ed into one another one another simply by ren um bering them.
We will rst sho w that perio dic signals exhibit man y pairs of equidistan t shifted subsequences and, consequen tly, mul-tiple global minima of the SSE. Overlapp ed subsequences of a perio dic sequence can be con venien tly represen ted as cyclic shifts of a single subsequence (pro vided that the win-dow length is equal to the perio d (or to a multiple thereof ).
Consider a sequence S consisting of man y rep etitions of a single subsequence s of length w , i.e., S = s; s; s; s; s; : : : . With in nite rep etition, all of the shifted subsequences of length w of s can be obtained from the circulant matrix M . For example, if w = 5, s = 2 ; 3 ; 8 ; 4 ; 7, and S = 2 ; 3 ; 8 ; 4 ; 7 ; 2 ; 3 ; 8 ; 4 ; 7 ; 2 ; 3 ; 8 ; 4 ; 7 ; : : : then
Because all of the cyclic-shifted subsequences con tain the same values, their lengths are also the same. Now consider two pairs of rows that corresp ond to the same shift, for in-stance rows 1 &amp; 4, and rows 2 &amp; 5. The scalar pro duct of both pairs will be the same (113 in this case). Therefore the Euclidean distance between row vectors 1 &amp; 4, and be-tween rows 2 &amp; 5, is also the same. The dot pro duct and the distance between the vectors corresp onding to rows 3 &amp; 1 is also the same (because 6 mo d 5 = 1): they are shift equivalent to row vectors 1 &amp; 4.

In fact, there can be at most w= 2 distinct distance values (aside from 0) for w even and ( w 1) = 2 distinct values for w odd. (The factor of 2 arises because a shift forw ard or bac kward is the same.) Therefore the scalar pro ducts among the (5 x 4)/2 = 10 pairs of vectors created by cyclic shifts of 2, 3, 8, 4, 7, will have only two distinct values: 104 and 113. So the ve row vectors lie in 5-D space at the vertices of a polyhedron with only two distinct edge lengths (i.e., an isosc eles simplex ).

If we self-concatenate the above subsequence man y times, di eren t random initializations will form three clusters pop-ulated either by rows 1 &amp; 4; 3 &amp; 5; and 2, or by rows 1 &amp; 3; 2 &amp; 5; and 4. The SSE for the two con gurations is the same: 58.0. So are their shap es, or lengths of the sides of the two triangles formed by the two sets of cluster cen-troids. However, the coordinates of the cen troids of the two con gurations are di eren t.

The shap e of the triangles dep ends on the two distinct val-ues of pairwise distances in 5-D space. We saw that for 2, 3, 8, 4, 7, all the scalar pro ducts between vectors were either 104 or 113. For a di eren t sequence, 1, 2, 4, 8, 16, they are 124 or 186. The ratios of the sides of the triangles of the op-timal cluster con gurations (whic h form isosceles triangles) are 0.86 and 0.97: therefore their shap es are di eren t.
We are not clustering perio dic sequences, but the num ber of dissimilar distance values between shifted subsequences in man y real problems is also lower than what migh t be exp ected. Consider the arbitrary sequence sho wn in Figure 17. Examine two pairs of subsequences, with windo w length w = 8, beginning at positions 1 &amp; 3, and 2 &amp; 4 resp ectiv ely. If the windo w were shifted cyclically , then the two pairwise distances would be exactly equal. Even as it is, however, we see that sev en pairs of comp onen t values (sho wn in italics) are iden tical. If we shifted the second sequence one more position, then six of the eigh t pairs of dot-pro duct comp onen ts would still be the same. We would therefore exp ect that the corresp onding distances between the shifted subsequences would also be very similar.
This multiplicit y of almost iden tical distances gives rise to alternativ e cluster con gurations with almost iden tical SSEs. With di eren t random initializations, K-means lands on one of these local extrema. The average value of eac h dis-tinct group of distances dep ends, of course, on the underly-ing sequence, and therefore so will the shap e of the resulting cluster con guration. (Note that Figure 14 had man y pairs of patterns with almost iden tical distances.) As in the case of cyclic shifting, the cen troids of the di eren t con gurations cannot be mapp ed onto one another by ren um bering.
S: 2 3 8 4 7 1 6 5 9 4 1 3 8 (1) 2 3 8 4 7 1 6 5 (3) 8 4 7 1 6 5 9 4 (2) 3 8 4 7 1 6 5 9 (4) 4 7 1 6 5 9 4 1 Figure 17: Similar distance comp onen ts in shifted subsequences.

If the same subsequence (a pattern ) occurs sev eral times in the sequence, then the num ber of iden tical pairwise distance values will increase corresp ondingly . Strict perio dicit y is the extreme case. Smo oth sequences have more similar shift-equiv alen t pairs of subsequences, and therefore give rise to more congruen t and less skewed cluster con gurations. We note, however, that exact equiv alences arise only with peri-odic sequences, and only if the windo w length is equal to an integer multiple of the perio d.
In STS, the K-means sum-of-squared-error optimization criterion is a function of the scalar pro duct of pairs of sub-sequence. Pairs of subsequences with the same displace-men t ( shift-e quivalent pairs ) are almost equidistan t (exact equalit y holds only for strictly perio dic sequences). The multiplicit y of similar pairwise distance values gives rise to appro ximately equal extrema of the optimization criterion (the SSE) for sev eral cluster con gurations, any of whic h may be reac hed with some random initialization. The cen-troids of these con gurations have the same structure, but the clusters con tain di eren t vectors, hence their cen troids cannot be mapp ed into one another by ren um bering. The distinctiv e shap e of the con gurations dep ends on the dis-tances between tigh t groups of shift-equiv alen t subsequences (windo ws). The cluster con gurations of smo oth, rep etitiv e sequences displa y the most symmetry .
The recen t claim of meaninglessness for STS subsequence clustering [18] (preliminary version [19]) \has cast a shado w over the emerging discipline of time series data mining" [28]. It has also generated a urry of follo w-up researc h activit y. As a result of this claim, man y have moved away from STS subsequence clustering, suc h as [26, 1, 21]. Others tried to nd \meaningful" results by using alternate clustering meth-ods, suc h as density-b ased clustering [8] and self-or ganizing maps [27], or by limiting themselv es to cyclic data [7], or by suggesting the use of medoids rather than means for clus-tering [14].

As far as we kno w, ours is the only work that succeeds to demonstrate a strong dep endence of output on input for the K-means STS clustering algorithm. This is despite the fact that our matc hing criteria are tougher than elsewhere, suc h as [18, 8, 27] { we are not restricted to just two sequences. We therefore hop e that this work will help to lift some of the shado w that has been cast over time-series data mining.
Our researc h raises sev eral questions, suc h as: 1. Wh y does cluster shap e distance pro vide a reliable way 2. How does sequence shap e dep end on various prop erties 3. Giv en some value of Q (num ber of shap es), what is the We have pro vided the answ er to Question 1, and partially to Question 2, in Section 7. Question 3 remains open; we now discuss it further.

We found that Q = 100 was sucien t for 85% accuracy for algorithm I and 100% accuracy of Algorithm II. However, this clearly dep ends on the nature of the sequences. Would Q = 25 suce? Will there be cases where Q = 100 is not enough? How can we tell, for a given dataset and a given degree of accuracy , what the appropriate value of Q should be?
There is a direct connection between the num ber of unique shap es and the value of Q . Clearly , if there are at most j unique shap es for any sequence in the input dataset, and we only store those, then Q = j suces to achiev e 100% accuracy with Algorithm II. We therefore hop e that our re-searc h into shap es, and speci cally the two correlations we have iden ti ed in Section 6, will pro ve a good starting point for answ ering Question 3.

In general, relating the error rate to the size of the train-ing or reference set has been the objectiv e of long-standing researc h in pattern recognition. Only asymptotic results have been obtained, the best kno wn of whic h is the bound of the Nearest Neigh bor classi er at twice the Bayes Error Rate [10]. There are also no general results on reac hing local vs. global optima with di eren t initializations in gra-dien t descen t algorithms. We therefore exp ect that strong assumptions about the nature of the sequences in the dataset will be necessary to pro vide a full answ er to Question 3.
Another question that arises naturally is whether the sen-sitivit y of our clustering algorithm to di eren t initializations is simply due to the fact that it nds local, rather than global, minima. We believ e that this is not the primary cause. We chose to use the clustering algorithm from [18] so as to duplicate their setting. However, we have exp eri-men ted with other K-means STS clustering algorithms, that attempted to get closer to the global minim um, and the re-sults where the same. We also saw no impro vemen t in our matc hing algorithm when we used K-medoids in place of K-means.
George Nagy gratefully ackno wledges the supp ort of NSF gran t #0414854. [1] A. J. Bagnall, G. Janak ec, B. de la Iglesia, and [2] G. Ball. Data analysis in the social sciences: What [3] G. H. Ball and D. J. Hall. A clustering technique for [4] R. Casey and G. Nagy . Recognition of prin ted chinese [5] R. Casey and G. Nagy . Autonomous reading mac hine. [6] R. Casey and G. Nagy . Adv ances in pattern [7] J. Chen. Making subsequence time series clustering [8] A. Den ton. Densit y-based clustering of time series [9] R. Dub es and A. Jain. Validit y studies in clustering [10] R. Duda, P. Hart, and D. Stork. Pattern [11] C. Faloutsos. Searching Multime dia Datab ases by [12] A. Gersho and R. Gra y. Vector quan tization and [13] D. Goldin and P. Kanellakis. On similarit y queries for [14] T. Ide. Wh y does subsequence time-series clustering [15] A. Jain. Cluster Analysis (ch.2) in Handb ook of [16] A. Jain and D. R. Algorithms for Clustering Data . [17] E. Keogh and T. Folias. Data mining archiv e [18] E. Keogh and J. Lin. Clustering of time series [19] E. Keogh, J. Lin, and W. Trupp el. Clustering of time [20] J. MacQueen. Some metho ds for classi cation and [21] M. Mahoney and P. Chan. Learning rules for time [22] G. Nagy . State of the art in pattern recognition. In [23] G. Nagy . Feature extraction on binary patterns. IEEE [24] G. Nagy , S. Seth, and K. Einspahr. Deco ding [25] G. Nagy and J. Tolaba. Nonsup ervised crop [26] P. Rodrigues, J. Gama, and P. Pedroso. Hierarc hical [27] G. Simon, J. Lee, and M. Verleysen. On the need of [28] Z. Struzik. Time series rule disco very: Tough, not [29] S. Theo doridis and K. Koutroum bas. Pattern [30] A. Topchy, A. Jain, and W. Punc h. Clustering
