 1. Introduction
The works presented in this article deals with color classifica-tion on wooden boards in an industrial environment. Color recognition is an important step for matching wood pieces which affects many activities of the timber industry (veneer, paneling, manufacture of squares, etc.). The wood pieces have to be assembled according to their perceived color. The aim is mainly to provide a wood piece that seems to be homogeneous and massive. This kind of problems belongs to the aesthetic aspect of wood characterization. The aspect control issue can be divided into two categories: detection, localization and identification of singularities and classification and identification of color and/or grain (textured aspect of wood).

In the context of wood classification (color, texture and singularities), different methods can be found in the literature.
For the defect recognition, there are essentially  X  X  X ompilation X  X  methods which  X  X  X ompile X  X  an important training data set to obtain the output classes X  model: Neural Networks (NN) ( Cho et al., 1991; Schmoldt et al., 1996; Kauppinen et al., 1999 ) and Genetic
Algorithms (GA) ( Estevez et al., 2003 ). For color or texture identification, the used methods are principally based on Neural
Networks (NN) ( Bai and Wang, 2007; P  X  olzleitner and Schwing-shakl, 2004; Wang and Bai, 2007 ) but also on k -Nearest Neighbor algorithm ( k -NN) ( Kline et al., 1999; Maenpaa et al., 2003 ). Distance Minimization algorithms ( Lu, 1997; Daul et al., 2000;
Hanbury, 2002; Srikanteswara et al., 1997 ) or Genetic Algorithms (Sathyanath and Sahin, 2001 ) are used too.

The aim of the global vision system is to identify the wood color in a continuous mode during the production. Such a system involves lots of constraints.

Firstly, the classification method must be able to work with small training data sets. Indeed, some classes which are rare in nature are defined with few samples. Moreover, providing a training data set requires the industrialist to label each sample.
This task is really painful and highly time-consuming. To adapt the system to the changes in the system and/or products, they should be renewed if necessary.

Then, colors which must be identified are subjective. Because of the impact of the wooden fiber to the perceived color of the wood, human operators can have different perceptions of the wood color. The output classes are gradual and non-disjointed too. For example, there are no strict bounds between a  X  X  X ed X  X  wood and a  X  X  X ight red X  X  wood.

Finally, there are other specific constraints to respect such as the real-time aspect of the production system or how easy the method is to set up.

So, the used classification method must take into account these constraints and present a low complexity of the recognition model.
The existing methods do not answer exactly to these specific constraints. Indeed, Neural Networks are popular machine learning algorithms that remain to be widely used for wood recognition problems. Their main advantages are their ease of use and their good behavior in term of classification rates. But their main drawback is that they need lots of training samples ( Tou et al., 2009 ). Other compilation methods such as Genetic
Algorithm have the same drawback. The other most popular method in wood color recognition is k -Nearest Neighbor. The k -NN is easily implemented as it does not require a training process. It is useful especially when there is a small dataset available. However, the major drawback of the Nearest Neighbor algorithms is that the computing time will increase according to the k number of neighbors used ( Tou et al., 2009 ). Moreover, the classification is very dependent on the choice of these neighbors. So, the setting must be done by an expert in image processing. We propose to use some tools provided by the Fuzzy Sets
Theory ( Zadeh, 1965 ), which seems to be well adapted for taking into account the above detailed constraints. Fuzzy Logic is able to provide non-separated output classes. While in the past fuzzy rule-based systems have been mainly applied to control pro-blems, they have also been used recently in pattern classification tasks ( Roubos et al., 2003; Nakashima et al., 2007 ) and they proved their ability to work with few learning data sets ( Wang fuzzy linguistic rules seems to be more appropriate. Another advantage is the interpretability of such Rule Systems ( Roubos et al., 2003; Nauck and Kruse, 1998, 1999 ). On the other hand, for instance, Neural Network are used as  X  X  X lack box X  X  and there is no explicit link between the vocabularies used to define the output classes and the image features characterizing the color, it is very hard to interpret the decision taken by the classification method.
The aim of this study is to obtain a vision sensor which delivers an answer in the  X  X  X ood X  X  vocabulary. This problem is often referred to as the  X  X  X emantic gap X  X , defined as  X  X  X he lack of coincidence between the information that one can extract from the visual data and the interpretation that the same data have for a user in a given situation X  X  ( Smeulders et al., 2000 ) Indeed, the wood expert will use words like  X  X  X ed X  X  or  X  X  X rown X  X , whereas the vision expert will define colors with numerical values like {90, 35, 12} in RGB space for instance.

Moreover, the setting parameters must be comprehensible for non-specialists too. In this way, it is judicious to be interested in the classification methods based on fuzzy set theory, because it allows the integration of the information expressed under field like the timber industry, the experts make decisions whatever the work conditions. Thus, the vision system, which must carry out the colorimetric control of the wooden boards, can be hatched as a decision system reproducing the human expert reasoning. D X  X cquila recommends using inference engines which allows not only work from a representation space with n dimensions, but also takes into account all forms of uncertainty and imprecision ( D X  X cquila et al., 2002 ).

This paper is organized as follows: Section 2 introduces the problems of the wood color recognition and the vision system used to do that. Section 3 details the proposed Fuzzy Rule Classifier by explaining the different steps of the method and its settings are explained in Section 4. Finally, Section 5 presents several comparisons with other classifiers and analyses the experimental results obtained from several University of Califor-nia Irvine learning databases ( Blake and Merz, 1998 ) and the industrial database corresponding to the application described in Section 2. 2. Industrial vision process
For about fifty years, the timber industry has been placed on a competitive market. In the order to get away from the industries which provide  X  X  X ottom-of-the-range X  X  products, some companies have given each other consequent means, like the use of vision systems in order to control and to enhance the production performances. As mentioned in the introduction, the context of this study is the classification of wooden colors. 2.1. Acquisition step
This recognition is carried out in real time on the industrial production line. These lines may reach speeds up to 400 m of board length per minute. After the color identification step, done by the vision system, color information is sent to an optimization step. Then each board is sent to a sorting line or to a cutting line. The cutting line aims to split the boards into uniformly colored piece of wood. The sorting line aims to group pieces of wood into specific classes, whose number and definition are given by the final customer. The boundary classes are very subjective in both cases.

The originality of the process concerns the color sorting which is only realized on the wooden board edges (board thickness). Indeed, the machining of handrails requires a uniform color in a are glued by their face. So, the final product makes illusion of a product carved in an uncut wood piece.

The entire vision system is detailed in Fig. 2 . The acquired images are processed to obtain color descriptors (see Section 2.2). These features are then used by the classification stage (see Section 3) to provide the color label of the wooden board or a part of it if the color changes along the board.

Fig. 3 shows an image obtained with the acquisition system which is made up of one type of linear sensors: CCD color cameras. This CCD sensor provides the red, green and blue components of the signal. The signals are sampled at the rate of 1500 lines per second along the y -axis ( Fig. 3 ). Each line is composed of 900 pixels ( x -axis in Fig. 3 ). In the industrial case presented, the wooden boards are around 3 m tall. With a longitudinal resolution equal to 1.5 mm per pixel, the images are made up of around 2000 lines. Thus, working in real-time, the data processing must be carried out under time constraints of around 1.5 s.
In this study, we work essentially with red oak because this wood species represents the most disadvantageous case. Indeed, for a particular wood hue, it is very hard to define with precision the customer color classes. The color variation can be very gradual (light red, medium red and dark red). Fig. 3 illustrates this gradual variability of the color. 2.2. Characterization of the color
Two aspects are essential to characterizing color: the reference color space and the characteristic vector.

One of the most common color spaces denoted RGB, organizes the color information of an image into its red, green, and blue components. However, the International Commission on Illumi-nation (CIE) does not recommend its use because the color components are not independent of one another ( International
Commission on Illumination, 1986 ). Other popular color spaces include the Lab and HSV (hue, saturation, value (intensity)) spaces. Many studies on color space selection have been conducted elsewhere, i.e. Burd and Dorey (1984) and Leon et al. (2006) . After conducting several internal tests on various sets of wood samples, we decided to work in the Lab space because it provides the best color discrimination in term of recognition rates. We have also made this choice in relation to an objective criterion funded on D cielab recommended distance ( Schmitt, 2007 ).
This could perhaps be explained because this colorimetric reference space represents colors in the same order than humans do and the color class definitions are given by customers.
In the same way, it is necessary to characterize a color with a set of parameters which are extracted from the image. This set, called  X  X  X haracteristic vector X  X  characterizes color in a simpler way.
To answer to the real-time constraints imposed by the industrial production system, the characteristic vector must be easy to calculate. In order to choose the best attributes to define the color, an extraction of 1D histogram features (statistical moments, entropy and homogeneity) from the color images is made. Then, a feature selection method, based on an adaptive algorithm of parameter space scaling using the Support Vector Machine (SVM) techniques ( Grandvalet and Canu, 2002 ), is applied. The suit-ability is determined from the calculation of the mean inertia of each feature according to a data set. The higher this value is, the more relevant the parameter is to distinguish the different classes.
Fig. 4 illustrates the mean inertia of each parameter, calculated on the industrial training data set (see Section 5.2 for details). This result shows that five features give a mean inertia higher than others.

The characteristic vector choice has been checked by taking into account the finality of the global system. Thus, the characteristic vector which provides the best recognition rate has been chosen ( Schmitt, 2007 ):
V  X  X  m L ; m a ; m b ; hom L ; hom h  X  0 2 4 6 8 10 12 14 16
Mean inertia of the parameter where m x is the average of the component x , hom x is the homogeneity of the component x: hom x  X  P h i  X  x  X  = i with h the histogram of x .

The characteristic vector is composed with the components L , a and b from the Lab color reference space and the component h (hue) calculated from the components a and b with the following equation: h  X  arctan  X  b = a  X  3. The Fuzzy Reasoning Classifier
The different specifications allow the choice of a classification method based on the use of a linguistic model. The system integrating linguistic rule models are techniques often used in the computer vision field ( Keller et al., 1996 ). These methods are comprised of three parts: a set of  X  X  X F y THEN y  X  X  rules, a database, and an inference engine which allows to interpret the different rules and to provide the final classification result.
The Fuzzy Reasoning Classifier ( Schmitt et al., 2006 ) is based on a fuzzy linguistic rule mechanism. It is well adapted to the presented industrial application. Indeed, it presents a very good and efficient generalization from a few sample sets and is able to provide gradual membership for output classes. Its satisfactory behavior will be shown in Section 5.2 by several comparisons with other classifiers such as k -Nearest Neighbor ( k -NN), Neural Networks (NN) or Support Vector Machine (SVM).

This implemented algorithm, for the fuzzy recognition method, is a supervised learning mechanism divided into two stages (Fig. 5 ). The Training stage is done off-line in order to set the recognition system and Generalization stage is used to classify color board on-line. The training part is composed of three steps: the Input Fuzzification of the characteristic vector, the Fuzzy Rule generation from training data set and the Rule Adjustment which is the iterative part of the algorithm.
 These three steps are done off-line at the setting of the system.
Then, these settings are used on-line. The membership functions defined in the fuzzification step are applied in the next step on the characteristic vector of  X  X  X nknown X  X  samples The rule set which is automatically obtained from training without human checking, is used by the generalization step and the output class is determined by the rule of maximal answer. Fig. 5 represents the different steps of the fuzzy recognition method. It should be noted, that there is no defuzzification step in the proposed method because the exposed problem is quite different from the fuzzy control application. Symbolical outputs are needed, not numerical ones.
Each output could be understood as a fuzzy singleton whose membership degree can be interpreted as the possibility of the sample to belong the considered class ( Dubois and Prade, 1997 ).
This output formalism contributes to reducing the difference existing between both industrial wood and image processing vocabularies ( Smeulders et al., 2000 ). The chosen formalism also improves the interpretability of the system. 3.1. Fuzzification of the characteristic vector
The fuzzification step aims to translate numerical variables into linguistic variables. A linguistic variable ( Zimmermann, 2001 ) is defined by a triple value ( V , X , T v ) where
V is a variable (luminance, hue, etc.) defined on a set of reference X ; X is the universe of discourse (field of variation of V ) and
T v is the vocabulary chosen to describe in a symbolic way the values of V (weak, high, dark, light, etc.).

The set T v  X  { A 1 , A 2 y }, finite or infinite, contains normalized fuzzy subsets of X (also called Terms) which are usable to characterize V . Each fuzzy subset A i is defined by the membership
This fuzzification step defines the decomposition number of the considered variable to provide the fuzzy rule premises.
For example the membership function for variable L , called  X  X  X uminance X  X , is initialized with respect to the data analysis of the training sample set. The symbolic vocabulary then associated with the variable L is T L  X  {Weak, Medium, High}. So, the linguistic variable  X  X  X uminance X  X  is split into three Terms (or fuzzy sets) and this variable is characterized by a vector composed of three Fig. 6 .

To summarize this step, one characteristic is represented by distributed terms in its definition field, called the universe of discourse, according to its useful and variable parts. The choice of the term number to be used to qualify a linguistic variable is one of the difficulties of this step. The others are to choose the shape of the membership function used to define these terms and to locate them on the universe of discourse. However, the industrial user, who is not an expert in pattern recognition, often chooses a regular distribution of terms, generally having more terms than are needed. Whenever the number of terms increases, so does the number of rules and, thus, the overall complexity of the entire system. Indeed, the rule number is given by Numbers of Rules  X  with N the number of feature and Card ( T v ) the number of terms for variable V .

The choice of the fuzzification best adapted to the application problem is presented in Section 4. 3.2. Fuzzy rule generation
This second step allows the defining of  X  X  X F y THEN y  X  X  fuzzy rules.

Training samples Fuzzification Characteristic 
If two linguistic variables are considered for input ( V 1 (Zadeh, 1975):
IF V is A i AND V 2 is A j THEN Z is C k  X  2  X  where V 1 , V 2 are the input linguistic variables defined on X (intensity and surface for example). Z 3 is the output linguistic variable defined on Y (color name : X  X  X ight Red X  X  for instance). A
A
T v (chosen vocabulary:  X  X  X eak X  X  and  X  X  X igh X  X  for example). C the class of k th color.
 Each rule describes the perceived defect, related to the system.
Such rules can be classified into two categories: conjunctive rules and implicative rules. These two categories are regrouped, respectively. On the one hand, there are the possibility rules and the anti-gradual rules and, on the other hand, the certitude rules and the gradual rules ( Dubois and Prade, 1992 ). The conjunctive rules are derived from the data analysis field where reasoning mechanisms are led by the data whereas implicative rules are most utilized in the cognitive sciences field where reasoning is led by knowledge ( Dubois and Prade, 1996 ).
For this application, conjunctive reasoning mechanisms have been logically selected. Each rule is activated in parallel and a disjunction operator combines the intermediate results. This inference mechanism gives an interpretation and semantics, which differ from mechanisms using implications ( Dubois and
Prade, 1996). In particular, it assures the consistency of the rule base ( Dubois et al., 1997). If no information is processed, that is, the input space is not covered by the rule set; the output gives an  X  X  X nknown defect X  X  class. The two main models using these rules are the Larsen X  X  model and the Mamdani X  X  model ( Mendel, 1995).
The Sugeno X  X  model ( Sugeno, 1985 ) is not suitable in this case because the aim is not to achieve numerical output values.
They are many techniques to generate automatically linguistic rules. Alcala et al. counts some methods ( Alcala et al., 2007 ). The genetic algorithms are often used for this step ( Cordon et al., 2001, 1999; Ishibuchi and Yamamoto, 2003 ) or the Decision Tree
Method ( Marsala, 2000; Michie et al., 1994 ). Nevertheless, these techniques generally allow the fuzzification step and the rule generation step to be carried in the same way. Other methods are used too: the Wang and Mendel algorithm ( Wang and Mendel, 1992), an algorithm based on a weight average of the trained outputs ( Alcala et al., 2007 ), a mixed genetic algorithm based on a combination of the Wang and Mendel algorithm and an algorithm of fuzzy splitting of the discourse universes ( Alcala et al., 2007 ).
Table 1 summarizes a comparison of these different methods applied on the industrial training database (see Section 5.2).
The results obtained with the algorithms presented by Alcala et al. have two problems. The algorithm based on a weight average of the trained outputs is well adapted when the output classes are very distinct. The mixed genetic algorithm needs a consequent set of learning data to provide a good model and in our industrial context such a large set cannot be easily obtained.
So, the chosen classifier provides interesting results. It is based on Ishibuchi X  X  algorithm which provides an automatic rule generation step ( Ishibuchi et al., 1992 ) detailed below. Moreover, its inference mechanism follows the Larsen X  X  model, which is better than the Mamdani X  X  model, because the Product is more adapted than the Minimum for the manipulation of several variable input space. The iterative version of the Ishibushi X  X  algorithm ( Nozaki et al., 1997; Ishibuchi and Nakashima, 1999 )is used here. It allows the adjustment of the input space splitting by supporting the rule of having the maximum response. The entire algorithm is given in Appendix.
 In this algorithm the  X  X  X ND X  X  of Eq. (2) corresponds to the
Cartesian product between V 1 and V 2 linguistic variables ( Mendel, 1995). This operation is done with a T-Norm. A product is used in the algorithm
T  X  x , x 2  X  X  m A  X  x 1  X  U m B  X  x 2  X  X  3  X  The IF y THEN implication is done through the Generalized
Modus Ponens mechanism based on the use of Maximum/Product composition law. This inference follows Larsen X  X  model ( Mendel, 1995), which uses a pseudo-implication operator represented by the product:
IF_THEN  X  V 1 , V 2 , Z  X  X  T  X  T  X  x 1 , x 2  X  , y  X  X  4  X  b  X  X  m A  X  x 1  X  U m B  X  x 2  X  U m Z  X  y  X  X  5  X  Eq. (4) corresponds to the expression Eq. (5) in the algorithm.
Then, each rule gives a partial conclusion. b Ck is aggregated to the others according to a fuzzy operator of disjunction. The disjunc-tion operator is represented by the maximum operator according to Zadeh X  X  case: b  X  max f b C 1 , b C 2 , ... , b CM g X  6  X  where b CX corresponds to the maximum membership degree given by the rule defined on A i A j .

Finally, CF ij confident coefficient is calculated for all the M classes as follows:
CF  X  X  b CX b  X  =
CF ij coefficient is associated to each rule R ij and is automatically adjusted in an iterative step. 3.3. Rule adjustment The adjustment represents the iterative part of the algorithm.
The following mechanism allows for adjusting the splitting of representative space according to the achieved results ( Nozaki et al., 1997):
From the training patterns, the algorithm generates the first model.

If the classification rate is below a e threshold, defined by the user, the iterative part is performed to adjust this rate. In fact, fuzzy rules are generated again by both injecting the training patterns and considering the new response of each rule and adjusting the CF ij confident coefficient with the following equations.

When x is properly classified by the R ij rule, the adjustment of the CF ij confident coefficient is done by
CF  X  CF ij  X  Z 1  X  1 CF ij  X  X  8  X 
On the opposite side, when x is poorly classified by the R the adjustment of the CF confident coefficient is done by
CF  X  CF ij  X  Z 2 CF ij  X  X  9  X 
The rule set is automatically generated from learning data, without human checking. The CF ij confident coefficient can be considered as a truth degree of a rule. An example of a generated whose first line could be interpreted as:  X  X  IF Luminance L is Weak AND Chrominance a is Low AND Chrominance b is Low
AND y THEN Output Class is Dark Brown with CF confident coefficient equal to 0.851.

The algorithm proposes an additional refining step. This step allows the improvement of the membership degree of the maximum membership class by modifying the slope of its membership function. This way is not studied here due to the gradual answer which needs to be kept because this vagueness improves the generalization capability of the classifier.
In the same way, recent works extend the algorithm by allowing the incorporation of weighted training samples ( Naka-shima et al., 2007 ). In our case, the enhancement cannot be used because no information about training samples can be obtained. 3.4.  X  X  X nknown X  X  sample classification
There are two steps to classify an  X  X  X nknown X  X  sample from the rule set previously obtained: (1) to compute a CT for each T class ( T  X  1, y , M ): (2) to allocate sample x to the X class such as
Thus, each  X  X  X nknown X  X  wooden color sample will be classified with a membership degree a CT to a symbolic color class T . This membership degree can be interpreted as a certainty degree corresponding to the possibility of the sample x to belong to the class T according to Dubois and Prade (1997) . 4. Fuzzy Rule Classifier settings The main settings of the Fuzzy Rule Classifier deals with the
Fuzzification step. It has a great influence on the classification result. In this step, the number of Fuzzification Terms, the shape of the membership functions defining them and their position on the universe of discourse. In this section we present two ways in order to fuzzify the input variable before using by the rule set. Thus, we present a regularly distributed Fuzzification and an original automatic one. All the tests, presented in this section, are done with the industrial generalization data set (see Section 5.2). 4.1. Equally distributed fuzzification
This is the simplest fuzzification way which consists in splitting the variable universe of discourse into regular parts, depending on the term number.

The term number is empirically chosen from tests on input data in relation to the application field. Generally, this number is odd and small in order to limit the rule number (see Eq. (1)). The shape of the membership functions, describing terms have also to be fixed. Table 3 presents the results obtained from an equal-distributed fuzzification with the FRC. The different parameters of the characteristic vector are fuzzified with the same number of terms and with the same shape of membership function. These tests have been done with Matlab Fuzzy Toolbox where the membership functions have been represented with mathematical expression.

For each shape of membership function, the best classification results are obtained by splitting the variables in seven terms. Decreasing rates appear when the splitting is more important. In this case, it is too precise to be representative of the classes to be identified. In relation with these tests, triangular/trapezoidal membership function shape is chosen because they provide the highest recognition rate (84.37%). It should be noted that for the best rate, the rule number is equal to 16 807 (see Eq. (1)).
In Section 5, the presented results are obtained with this kind of curves. In Section 5.1, the number of terms is fixed to 7. But the regular distributed fuzzification does not fit with the industrial case because it generates too many rules and low recognition rates.

Another way is to manually adapt the number and the position of these membership functions. 4.2. Adapted fuzzification
The adapted fuzzification needs expert knowledge. In this case, the choice of the term number and their position are determined from human interpretation of the data or from knowledge about the industrial context. We tried to model expert knowledge used to set up the Fuzzification step ( Bombardier et al., 2007 ). By possible to obtain an adapted Fuzzification which as described in Table 4 for the used characteristic vector.

With this fuzzification, the recognition rate reaches 83.25% on the generalization industrial database. By comparing this rate with the ones obtained with an equal-distributed fuzzification, a 1%-decreasing of the recognition rates is showed. Nevertheless, this fuzzification allows the reduction of the number of rules which is equal to 48 according to Eq (1). The main advantage is that such a rule set becomes interpretable and adding or modifying a rule manually will be easier.

However, this method has the drawback of needing an expert to set up the FRC. So in the following section we propose an automatic method to fix the number and the position of the membership function of each variable. 4.3. Automatic fuzzification
The automatic fuzzification is directly linked to the analysis of the learning data set. In this case, only the numerical data are necessary in order to evaluate the better splitting of the discourse universe for each parameters of the characteristic vector. The advantage of such a fuzzification resides in the simplicity of the use. Indeed, the users of the classification systems, in an industrial context, are not experts, and thus prefer to quickly configure the recognition module. That is why they use often an equal-distributed fuzzification even if it is not adapted to the problem.
There are many techniques for carrying out the splitting of the discourse universes. They are usually based on clustering methods ( Kempowsky et al., 2006; De Carvalho, 2007 )oron genetic algorithms ( Ishibuchi et al., 1994; Oh et al., 2003; Cordon et al., 2004 ). Nevertheless, considering the genetic algorithm, it is very hard to obtain the right results in relation to the data sets used in our study. Indeed, like the neural networks, these techniques are known to be used with lots of learning points.
Moreover, if the partition of the input variable space does not fit with the real data, the terms and the number of terms will be inappropriate.

That X  X  why a fuzzification method based on the study of the output class typicality scores is used. Typicality measure T ( V )is calculated with the following equation from extern dissimilarity and intern likeness according to output classes ( Forest et al., 2006):
T  X  x  X  X 
R  X  x  X  X 
D  X  x  X  X  where x u a is the value of parameter a for sample x , x f parameter a for sample f belonging to the same class than x , x the value of feature a for sample e not belonging to the same class which belongs to the same class than sample x , m is the number of samples which does not belong to the same class than sample x .
From Typicality measure T ( V ), correlation (Corr) and cross-correlation (Xcorr) coefficients are calculated for each output classes. Then, from the ratio Corr/Xcorr, which characterizes the inter-classes similarity, the number of terms is determined. These terms are represented by triangular/trapezoidal membership functions which gives the best results (see Section 4.1). Their positions are obtained by computing the mean value of the samples belonging to the considered output classes ( Schmitt et al., 2007). Fig. 7 provides an example of automatic Fuzzification obtained with the proposed method and Table 5 gives the term number achieved for the entire characteristic vector of the application. This fuzzification has been obtained from the industrial training data set.

The main interest takes place in the automatic adaptation of the fuzzification step which makes the tuning of the system easier. Table 6 illustrates the results obtained on the industrial generalization data set with the FRC and the above-quoted automatic fuzzification methods. It also gives the number of generated rules according to the retained term number.
From all these tests concerning the fuzzification methods, two enhancements appear: one about the recognition rates and one about the rule number. Firstly, by comparing the Automatic
Fuzzification to the others, a 2%-improvement of the recognition rates is observed. Finally, we notice a decreasing of the rule number for the automatic fuzzification in comparison with an equal-distributed Fuzzification. However, the number of generated rules with an automatic fuzzification is higher than the number obtained in the Adapted case. Thus the rule base is harder to interpret, but it corresponds best to the expectations of the customers in terms of good classification and simplicity of F.R.C. use. 5. Experimental results
In this section, some tests are proposed with the aim of comparing or situating the efficiency of the proposed Fuzzy Rule
Classifier. These comparisons are done in relation with usual methods used in pattern recognition or classification problems. So we applied the following classifier on some of the University of
California  X  Irvine learning databases and on the industrial data set: Bayesian classifier (Bayes), Decision Tree Method (DTM: C4.5 algorithm) ( Michie et al., 1994 ), k -Nearest Neighbor algorithm et al., 1985), Neural Networks (NN) ( Pham and Sagiroglu, 2001 ), Genetic Algorithms (GA) ( Sathyanath and Sahin, 2001 ) and
Support Vector Machine (SVM) ( Hao et al., 2007 ) which are the newer trends in machine learning algorithm which has been popular in many pattern recognition problems in recent years (Chen et al., 2006 ).
 Some results concerning the application of this classifier on UCI database are given. The aim of testing UCI databases is to situate the efficiency of FRC in relation to other reference classifiers ( Blake and Merz, 1998 ).

Then, the results about color classification on wooden boards will be shown. They attempt to show the good capability of the proposed classifier for generalization from few training samples. 5.1. UCI benchmark experimentations
Before giving the classification results concerning the indus-trial context, it is important to compare the proposed F.RC. Method on the University of California  X  Irvine learning databases.
The comparison is made with usual methods whose setting are similar to those used in Section 5.2 except for k -NN, Fk -NN and FRC. For k -NN and F -k -NN methods, k is chosen equal to 3 and for FRC setting, fuzzication is equally distributed in 2 or 3 terms.
Table 7 gives the recognition rates obtained with several UCI databases:
UCI Iris benchmark : It is composed of 150 samples divided into 3 classes. For each sample, 4 different features are calculated.
UCI Liver benchmark : It is composed of 345 samples divided into 2 classes. For each sample, 6 different features are calculated.

UCI Diabetes benchmark : It is composed of 768 samples divided into 2 classes. For each sample, 8 different features are calculated.

UCI Glass benchmark : It is composed of 214 samples divided into 6 classes. For each sample, 9 different features are calculated.

For the Iris and Glass benchmark, the obtained results show very similar recognition rates for all classifiers except for Decision
Tree Method. These tests demonstrate the good behavior of the proposed FRC method. It also gives the best results for two other
UCI databases. Comparative results on these UCI databases with other classifiers are shown in Atif Tahir et al. (2007) .
In addition to the classification rates, the Cohen kappa coefficient is interesting to evaluate the performance of the FRC. This coefficient is used to evaluate the quality of the classifiers (Garcia et al., 2009 ). The coefficient kappa is computed with the confusion matrix obtained in a classification step ^ k  X  N where N is the total number of samples; X ii is the value in the row i and the column I; r is the number of output classes; X i + sample number of the row i ; X + i is the sample number of the column i .

This coefficient ranges from 1 to 1. The higher the result is, the better the classification is. In the UCI presented problems, the obtained results are summarized in Table 8 .

According to the scale defined by Landis and Koch (1977) , the results presented in Table 6 confirm that the FRC is comparable to the other classifiers and even gives the best results in three cases. Nevertheless, such academic databases cannot really prove the generalization capacities of the proposed FRC method. That is why an evaluation of the generalization capability of the FRC is developed in the next section. 5.2. Industrial experimentations
We use an industrial data set composed of 943 samples distributed in 6 classes. This data set is composed with 84  X  X  X ark Brown X  X  samples, 176  X  X  X rown X  X , 259  X  X  X ight Brown X  X , 54  X  X  X ark Red X  X , 197  X  X  X ed X  X  and 173  X  X  X ight Red X  X . To compare the different classifiers, some samples were randomly generated with a Gaussian white noise. Thus, for these tests, a data set composed of 5000 samples per class for the training step and 5000 samples per class for the generalization step (Test Extended Database) was used. Fig. 8 represents the evolution of the recognition rates during the generalization step functions of the number of learning samples per output class.

These results correspond to an average of the results obtained after some repetitions (100 random runs of the learning samples).
Fig. 8 shows the evolution of the generalization recognition rates in relation to the training sample number. An asymptote appears for each classifier: 71.5% for the Bayesian classifier; 76% for the k -Nearest Neighbor algorithm ( k  X  5); 80.2% for the Fuzzy k -Nearest Neighbor algorithm ( k  X  5); 83.4% for the Neural Networks (5 input neurons, 3 hidden layers of 20 neurons, 6 output neurons, 1500 iterations,
Levenberg X  X arquardt Algorithm); 84.4% for the Fuzzy Rule Classifier ( Z 1  X  0.4, Z 2  X  0.004, 500 iterations, e  X  92%); 85.4% for a Genetic Algorithm (reproduction rate  X  0.3, muta-tion probability  X  0.1, 1000 iterations) and 85.5% for the Support Vector Machine (Gaussian RBF Kernel, standard deviation  X  0.1, C  X  50, step of 5, tole  X  rance 10
The FRC is one of the most effective classifiers. However, a difference exists concerning the convergence speed towards these maxima. Usually, the neural networks, the genetic algorithms and the SVM use lots of samples in the training step in order to give good results. For equivalent performances, these algorithms need more 100 as much (NN and GA) and 25 as much (SVM) of points so that the recognition rates are comparable with that obtained with the FRC. Thus this aspect is very important, because it is not easy to have lots of training samples in an industrial environment.
Moreover, in our applicative case, it is possible that some output classes are defined with few samples (about 10 or 20 samples). Thus, the methods which need lots of samples must be avoided.
The efficiency of the FRC method in generalization step has been shown and is strengthened in the industrial context. Fig. 9 illustrates the comparative results obtained with a  X  X  X on-extended X  X  training step. Indeed, in these cases, the training step was done with a dataset made of 25 randomly chosen samples of each class.

These results show the same tendency. The lowest rates are obtained with two compilation methods which are effective with lots of training samples (NN and GA). The statistical methods, parametric or not, provide acceptable results. But FRC gives the best results (85.65%). This behavior is really interesting in our industrial case because of the low number of training samples needed. Finally, it should be noted that this data set is given directly by the industrialist. This is no filtering and it is possible that there are many aberrant samples. It therefore shows a certain robustness and reliability of the proposed classification method.
This method fits in well the time constraint of the system because it needs less than 1.30 10 4 s to recognize one sample (R.O.I.). It has been implemented in C/C++ with Microsoft XP and Intel core duo 3.00 GHz CPU.

As UCI problems, the Kappa coefficient calculation allows to position our method. Table 9 presents these results which confirm the efficiency of the FRC classifier. 6. Conclusions
In this paper, an original approach for a multi-class classifica-tion has been presented. Firstly, the efficiency of the FRC method has been presented through several comparisons on academic and industrial databases. Then, the interest of exploiting the fuzzy output of the FRC classifier has been shown in an industrial application case.

The proposed method uses Fuzzy Rules learned from small data sets. UCI benchmark results demonstrate that our method is Recognition rate 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 Recognition rate fkNN (k=5) kNN (k=5) Bayes comparable to other existing techniques like SVM, Genetic
Algorithm, Neural Networks, etc. The application of the method to an industrial problem (identification of wood board colors) has showed the efficiency, the robustness and the generalization capabilities of our proposition. In applicative cases, the setting of the method is very important to provide the best results, because the users are not specialists of the fuzzy classifica-tion methods. The use of an automatic fuzzification method allows the improvement of the results in comparison with an equal-distributed fuzzification which is always employed by the users.

However, the global decision must take into account the decisions provided by two color sensors (right and left edges). The proposed Fuzzy Rules Classifier has been integrated in two fuzzy sensors. The results provided by these sensors are merged with a fuzzy operator detailed in Perez Oramas (2000) . The final recognition rates show an improvement of about 10% in comparison with a simple symbolic merging (according to the nominal single decision) ( Bombardier et al., 2009 ).
One of the advantages of using fuzzy rules remains in the possibility of interpretation of the recognition model, but actually the size of the rule set does not allow it. So, the future work aims to reduce the complexity of the system in terms of model interpretability. Indeed, even if the results are interesting for an industrial production system, it should be important to be able to check the linguistic model from the expert knowledge (Bombardier et al., 2007). Another way of investigation is to reduce the size of the characteristic vector by selecting suitable features. With this aim in view, an original method has been proposed ( Schmitt et al., 2008 ). It associates the FRC classifier with a suitable feature selection method, based on the Choquet integral.
 Acknowledgment
The authors wish to thank their industrial partner who has provided the data sets and the knowledge concerning the framework.
 Appendix Outline of the Ishibushi rule generation algorithm.

A fuzzy rule R ij IJ for a two-dimensional classification problem can be written as follows:
Rule R ij IJ :If x 1 p is A i I and x 2 p is A j J then ( x
C with CF  X  CF ij IJ , i  X  1, 2, y , I ; I  X  1, 2, y , I max 2, y , J max , where C ij IJ is one of the M classes and CF ij the fuzzy rule R ij IJ . (1) Calculate b CT for T  X  1, 2, y , M (2) Find Class X ( CX ) such that (3) CF ij IJ is determined as References
