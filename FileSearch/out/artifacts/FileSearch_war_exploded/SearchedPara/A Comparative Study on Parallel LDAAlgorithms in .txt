 Latent Dirichlet allocation (LDA) [ 1 ], as a generative probabilistic topic model, represents each document by a mixture of latent topic distributions over the vocabulary words. To get the best topic labeling configuration over observed words, LDA maximizes the joint probability of latent topics and observed words by inference algorithms, which calculate the posterior distribution of topics con-ditioned on words. In the past decade, LDA inference algorithms can be broadly categorized into two types. The first is based on the idea of stochastic gradi-ent descent (SGD) [ 2 ], which uses the topic posterior distribution of each word to update the LDA parameters sequentially. Typical SGD-based inference algo-rithms include Gibbs sampling (GS) [ 3  X  5 ], zero-order approximation of collapsed variational Bayes (CVB0) [ 6 , 7 ], and asynchronous belief propagation (BP) [ 8 ]. The second is based on the idea of coordinate descent (CD), which fixes the LDA parameters to infer the posterior of topics over all words, and then fixes the inferred posterior to estimate LDA parameters iteratively. Typical CD-based inference algorithms include maximum a posterior (MAP), variational Bayes (VB) [ 1 , 9 ], and synchronous BP [ 8 ].
 In the big data era, scaling out batch or online LDA inference algorithms on a cluster of computers is a necessary step to web-scale industrial applica-of different parallel LDA algorithms in real-world industrial applications. There-fore, within the unified MapReduce framework, we compare three representative parallel LDA algorithms (PVB [ 1 ], PGS [ 3 ]andPBP[ 8 ]) in terms of complex-ity, accuracy and practical performance in search engine and online advertising system of Tencent, one of the biggest Internet companies in China. Through experimental results, we obtain the following observations:  X  PGS is more scalable than PVB and PBP in the MapReduce framework because of its low memory usage in storing LDA parameters and fast speed by efficient sampling techniques. It achieves the best performance in two industrial applications such as search engine and online advertising system.  X  PBP has the best predictive performance in terms of held-out log-likelihood.
However, it consumes more memory space than PGS, and has difficulty in scaling out to the larger number of topics. Moreover, its performance in two real-world applications is worse than PGS, which indicates that the best topic modeling performance does not mean the best application performance.  X  PVB has the slowest speed and the lowest held-out log-likelihood. Its appli-cation performance is also the worst among three algorithms.
 MapReduce is a simple framework for dealing with large-scale data. Tradi-tionally, the intercommunication of machines using MapReduce is based on the shared disk files like Hadoop distributed file (HDF) system for iterative algorithm due to repeatedly load from and write to disk. However, the advantage of MapReduce is redundancy and fault tolerance mechanism. If the training time per iteration is long and errors occur during one iteration, the fault tolerance is useful to recover model parameters from the last successful iteration. Therefore, MapReduce has been widely used in industry and can be viewed as a fair framework for this comparative study. Prior works have imple-mented PGS [ 17 ]andPVB[ 15 ] within the MapReduce framework. In PGS, each mapper processes the subset of documents and the reducer collects the output of all mappers and synchronizes the model parameters. Mahout MapReduce version of collapsed variational Bayes (CVB) algorithm [ 6 ], which has a close relation to BP [ 8 ] and MAP inference. Mr.LDA [ 15 ] develops PVB based on the MapReduce framework. Although parallel LDA algorithms have been implemented within other framework [ 13 , 14 , 18 ], we focus on MapReduce for a fair comparison of three representative parallel LDA algorithms. representative batch LDA inference algorithms like VB [ 1 ], GS [ 3 ] and BP [ 8 ]. We show that they infer different posterior probability of LDA with different time and space complexities. Section 3 introduces our multi-channel MapReduce framework for a fair comparison among PVB, PGS and PBP. Section 4 describes their topic modeling results, and compares the performance of topic features learned by three inference algorithms in search engine and online advertising system of Tencent. We conclude this paper in Section 5 . LDA allocates a set of thematic topic labels, z = { z k w,d ments in the document-word co-occurrence matrix x Table 1 summarizes the important notations in this paper. The nonzero ele-ment x w,d = 0 denotes the number of word counts at the index word token x w,d,i = { 0 , 1 } ,x w,d = i x w,d,i , there is a topic label k =1 z k w,d,i =1 , 1  X  i  X  x w,d . The full joint probability of LDA is p multinomial parameters for document-topic and topic-word distributions, satis-fying k  X  d ( k )=1and w  X  w ( k ) = 1. Both multinomial matrices are generated by two Dirichlet distributions with hyperparameters  X  and element x w,d = 0, there is a normalized vector k  X  w,d ( responsibility that the topic k takes for word index { w, d } sage vector in [ 8 ]). For simplicity, we consider the smoothed LDA with fixed symmetric hyperparameters [ 3 ].
 the full joint probability [ 19 ]. We choose three representative LDA algorithms
VB [ 1 ] p (  X  , z | x ,  X  , X , X  ) 2  X  K  X  NNZ  X  digamma 2
GS [ 5 ] p ( z | x , X , X  )  X  1  X  K  X  ntokens  X  2  X  K  X  W +
BP [ 8 ] p (  X  ,  X  | x , X , X  ) 2  X  K  X  NNZ 2  X  K  X  ( D + W for comparison because they infer slightly different posterior probability of LDA. Also, other inference algorithms have close relation to these three representative algorithms. We shall see that these three representative algorithms all originate from the expectation-maximization (EM) framework for maximum-likelihood estimation.
 The first inference algorithm is VB [ 1 ] that infers the posterior, However, computing this posterior is intractable because the denominator contains intractable integration,  X  , z p ( x , z ,  X  ,  X  |  X ,  X  approximate variational posterior based on the variational EM algorithm:  X  Variational E-step:  X  Variational M-step: In variational E-step, we update  X  w,d ( k )and  X   X  d ( k makes the variational posterior approximate the true posterior by minimizing the Kullback-Leibler (KL) divergence between them. However, the In addition, the calculation of exponential digamma function exp[ putationally complicated. As shown in Table 2 , the time complexity of VB for one iteration is O (2  X  K  X  NNZ  X  digamma ), where digamma time for exponential digamma function. For each non-zero element, we need iterations for variational E-step and K iterations for normalizing space complexity is O (2  X  K  X  ( D + W )) for two multinomial parameters and temporary storage for variational M-step.
 The second is the GS [ 3 ] algorithm that infers the posterior, ence called Markov chain Monte Carlo (MCMC) EM is used as follows:  X  MCMC E-step:  X  MCMC M-step: In the MCMC E-step, GS infers the posterior  X  w,d,i ( k )= for each word token, and randomly samples a new topic label this posterior. Here, we use the notation  X   X  ( k )= w  X   X  sponding matrices {  X   X ,  X   X  } . In the MCMC M-step, GS updates immediately by the new topic label of each word token. In this sense, GS can be viewed as a kind of SGD algorithm that learns parameters by processing data point sequen-tially. Normalizing {  X   X ,  X   X  } yields the multinomial parameters the time complexity of GS for one iteration is O (  X  1  X  K  X  ntokens The reason is that we require K iterations in MCMC E-step and less for normalizing  X  w,d,i ( k ). According to sparseness of techniques [ 5 ] can make  X  1 even smaller. Practically, when  X  recover  X   X  K  X  D . So, the space complexity is O (  X  2  X  K  X  W  X   X  K  X  W can be compressed due to sparseness [ 5 ]. Practically, when than 1000,  X  2  X  0 . 8. Note that all parameters in GS are stored in integer type, saving lots of memory space than double type used by both VB and BP. we integrate out the labeling configuration z in full joint probability, and use the standard EM algorithm to optimize this objective ( 10 ):  X  E-step:  X  M-step: In the E-step, BP infers the responsibility  X  w,d ( k ) conditioned on parameters {  X   X ,  X   X  } . In the M-step, BP updates parameters {  X   X ,  X   X  } responsibility  X  w,d ( k ). Here, we use the notation denominator in ( 11 ). Normalizing {  X   X ,  X   X  } yields the multinomial parameters Unlike VB, BP can touch the true posterior distribution p E-step for maximization. Besides BP [ 8 , 20 ], CVB0 [ 7 ] and MAP are all like EM algorithms. When compared with VB, the time complexity of BP for one iter-ation is O (2  X  K  X  NNZ ) without calculating exponential digamma functions. The space complexity of BP is the same as VB with O (2  X  K  X  of storing {  X   X ,  X   X  } as well as the temporary variables in the M-step. Currently, both Hadoop 3 , Spark [ 21 ] are distributed computation systems. Ha-doop stores and processes data in hard disks by Hadoop distributed file system (HDFS), while Spark processes data in memory. Spark integrates the MapRe-duce of Hadoop, primarily targeted at speeding up batch analysis jobs, iterative machine learning jobs, interactive query and graph processing. Due to the mem-ory limitation of our cluster, in this paper we focus on MapReduce in Hadoop to process big data with fault toleration, though the training time will be longer than that with Spark. Note that these parallel LDA algorithms can be easily deployed in the MapReduce framework of Spark.
 In real-world applications, the vocabulary size W is often larger than 10 Taking polysemy and synonyms under consideration, a rough estimate of the number of topics is close to the same magnitude of vocabulary words, i.e., 10 . If we store the full topic-word matrix  X   X  K  X  W in memory, the entire space needs to be larger than 10GBytes, which is impractical with general parallel framework like PLDA [ 17 ] and Mr.LDA [ 15 ], where each machine in the cluster has only 2GBytes memory. Also, an industrial computer cluster often processes multiple tasks, and the available memory space for parallel LDA algorithms is significantly less than 2GB for each machine. Therefore, we propose a new multi-channel MapReduce framework to reduce the memory demand of each task. 3.1 Overview To reduce the memory demand, we partition the matrix  X   X  K  X  W shards  X   X  m,n w ( k ) with 1 /M size of  X   X  K  X  W . To reduce the memory demand for  X   X 
K  X  D , we partition it into N column shards  X   X  the meanwhile, we partition the input document-word matrix dimensions, where the D and W denote the number of documents and the size of vocabulary, respectively. Fig. 1 shows the partition of input document-word matrix into small shards. As with [ 22 ], we use the random shuffling method in both document and vocabulary dimensions for data shard load balancing, i.e., making the number of words almost equal in each data shard. In MapReduce, each mapper uses one core of CPU in a machine, and takes one small data shard and two small parameter shards in memory for computation. Therefore, each mapper only consumes 1 / ( N  X  M ) data size and 1 /M size when compared with the original batch algorithms on a single machine. We set the values of M and N for fitting the topic-word distribution and document-topic distribution in the memory. For example, when M = 10, we need only 1GB memory of each machine to hold a small shard of 10GB LDA  X   X  blocks by corresponding parameter shards using equations ( 2 ), ( 6 ) and ( 11 ). After we obtain the updated local parameters {  X   X  m,n d ( ple channels of reducers to aggregate them into the updated global parameters called synchronization. Each channel contains multiple reducers for the same kind of tasks. For example, to get the document-topic distribution of one docu-ment  X   X   X  ,n d ( k ), we aggregate the distribution  X   X  m,n document index d using N reducers illustrated as the Doc-Reducer channel in Fig. 1 . To get the topic-word distribution of one topic  X   X  m,  X  the distribution  X   X  m,n w ( k )of N shards with the same word index ers illustrated as the Wordstats-Reducer channel in Fig. 1 . Similarly, we use the Globalstats reducer to aggregate  X   X  m,  X  w ( k )into  X   X  we use the Likelihood-Reducer to check the convergence of training process. The held-out log-likelihood is defined as follows [ 1 , 7 , 8 ]: PVB 2  X  K  X  NNZ  X  digamma/ ( M  X  N ) 2  X  K  X  ( D/N + W/M ) PGS  X  1  X  K  X  ntokens/ ( M  X  N )  X  2  X  K  X  W/M + ntokens/N
PBP 2  X  K  X  NNZ/ ( M  X  N ) 2  X  K  X  ( D/N + W/M ) where x held-out w,d denotes the held-out test data. If the data is changed to the training log-likelihood can be used to check the convergence condition. When the absolute difference ratio of training log-likelihood between two successive iterations is smaller than a threshold, i.e., 0 . 1%, we terminate the training pro-cess. In total, there are M + N + 2 reducers for aggregation as shown in the left panel of Fig. 1 . The updated model parameters are copied back to each mapper for the next training iteration. Similar to Table 2 , Table 3 shows the time and space complexities of PVB, PGS and PBP without considering the synchronization and I/O costs. We see that both time and space complexities have been significantly reduced. In practice, M and N can be set according to each machine X  X  memory constraint. 3.2 Mapper and Reducer Algorithms In the MapReduce framework, all data are organized in key-value pairs. Fig. 2 shows the mapper algorithm, which performs the local E-step and M-step based on local parameters. In the first iteration t = 1, each mapper randomly initialize the local parameters in line 2. Then, all mappers run E-step/M-step in parallel, and calculate local log-likelihood (lines 8  X  10). In the successive iterations, all mappers read the aggregated parameters from HDFS (lines 4 and 5), and continues to do E-step and M-step in parallel. Finally, all mappers output the updated local parameters and log-likelihood to multi-channel reducers (line 12). Fig. 3 shows the multi-channel reducer algorithm, which aggregates the local parameters and log-likelihood by sum operations.
 solutions [ 15 , 17 ] lies in the parallel reducers in Fig. 3 . In practice, mappers can speed up parallel LDA algorithms linearly while reducers introduce addi-tional synchronization time when compared with the batch algorithms on a sin-gle machine. By splitting the document-word matrix into M  X  N not only save the memory consumption for model parameters, but also imple-ment parallel reducers for a further speedup. In this way, the synchronization time can be reduced linearly with the number of reducers. Future work includes the multi-channel MapReduce in Spark, which can save the I/O cost of each mapper and reducer iteration in parallel LDA algorithms. We compare PVB, PGS and PBP within the unified multi-channel MapRe-duce framework in three tasks: topic modeling accuracy measured by held-out log-likelihood, search matching measured by mean average precision (MAP), and online advertising system by area under curve (AUC) of click-through-rate (CTR) prediction. We compose the training corpus of search queries received in recent months from Tencent (called SOSO data set). The SOSO corpus contains one billion search queries with 4 . 5 word tokens per query on average and takes 17 . 2GB storage space. The vocabulary size of SOSO is around 2 the subset of SOSO data set in our comparative study. The subset contains about 10 million queries and takes 216MB storage space. Each query is a document in the LDA model. We set M = N = 10 to partition the data into 100 data blocks, and set the same Dirichlet hyper-parameters  X  =5 /K and  X  LDA algorithms. We carried out experiments on a cluster of 257 machines with XFS of Tencent, where each machine has 16 cores. 4.1 Topic Modeling Performance We evaluate the topic modeling performance by two measures: 1) Scalability: the ability to handle a growing number of topics; 2) Accuracy: the held-out log-likelihood achieved by increasing number of iterations (We randomly select 10 shows the training time per iteration with the increasing number of topics { 10 , 10 3 , 10 4 , 5  X  10 4 } . The training time per iteration of PVB/PBP is too long to be shown when K = { 10 4 , 5  X  10 4 } . Clearly, PGS is much more scalable to the larger number of topics than PVB and PBP (less per-iteration training time). There are three reasons why PGS can scale out to more number of topics. First, PGS uses integer type as well as sparse storage for model parameters [ 5 ], so that its memory requirement is insensitive to the number of topics as shown in Table 3 . Using the same amount of memory, PGS can process the larger number of topics. Second, PGS uses the efficient sampling technique [ 5 ], whose time complexity is insensitive to the number of topics. Third, the reducers of PGS consume significantly less cost in synchronizing model parameters in the sparse format and integer type. Since the processed queries are very short (less than 5 words), it is memory-efficient to store the labeling configuration z rather than the document-topic distribution  X   X  d ( k ). As a comparison, PVB and PBP have to sore parameters in floating type and full/dense format, leading to more synchronization time, memory consumption and I/O costs. PBP is faster than PVB due to the lack of complicated digamma function in Table 3 . The results are consistent with our complexity analysis in Table 3 .
 Fig. 4 B shows the held-out log-likelihood as a function of the number of training iterations (fixing K = 2000). We see that PBP reaches the highest value using around 20 iterations, PGS converges at the middle level value in around 100 iterations, while PVB converges at the lowest level value in around 10 iterations. Taking per-iteration training time into account, it takes PGS 1 hours, PBP 16 hours and PVB 33 hours for convergence. The reasons why PBP reaches the highest held-out log-likelihood are as follows. First, PBP X  X  objective Second, PBP X  X  E-step can touch the lower-bound of the true posterior without approximation. Through Fig. 4 , we may make a conclusion that PVB is neither scalable nor accurate in topic modeling when compared with PGS and PBP. Although PGS is more scalable than PBP, its topic modeling accuracy measured by held-out log-likelihood is lower than that of PBP.
 Fig. 5 to compare the top 5 words in 5 topics learned by PGS, PBP and PVB. We select the top 5 words by ranking  X   X  w ( k ) (in descending order) in terms of w in each topic k . For a better illustration, we translate these Chinese words into English, while  X  X q X  represents an instant message software from Tencent. We see that the topic topic words generated by PBP have highest probability and they are much more correlative (subjective judgement). The top topic words produced by PGS are partly the same as those by PBP, but the word probability is smaller and less concentrated than those by PBP. PVB generates almost the same words in these 5 topics and these words have almost the same probabilities, which are small and dispersive. These duplicated topics are unfavorable in many applications such as search engine and online advertising, because given one query it is hard to decide which topic this query belongs to. 4.2 Industrial Applications We compare the topic features produced by PVB, PGS and PBP. The topic feature is the likelihood p ( w | d ) of a vocabulary word The
W -length vector p ( w | d ) is compatible with the standard word vector space model. Ranking p ( w | d ) in descending order yields top likely topic features of the query. In the following experiments, we fix K = 2000 to obtain the topic features. The difference between word vectors and topic features is that word vectors are sparse while topic features are compact in latent semantic space. In real-world applications, we prefer compact features because they can be used by different classifiers. 4.3 Search Engine Search engines use the well-known vector space model in information retrieval and compute cosine similarity between queries and documents in their vector representations. For information retrieval, our test-bed is a real search engine in Tencent, www.soso.com , which ranks the fourth largest in China market. The test set comes from human-labeled data used for relevance evaluation of the search engine. This set consists of 922 common queries and 58 document pairs with human labeled relevance rate. Each query-document pair was rated by three human editors and the average rate was taken. We use cosine similarity to measure the similarity between one query and one document both represented by word vectors. Top similar documents are retrieved to be relevant to the given query. We use the similarity in word vector space to do information retrieval as the baseline in Table 4 . We compute MAP using TREC evaluation tool [ 24 ]. The higher MAP means the better retrieval performance. We use topic features ( 15 ) generated by PVB, PGS and PBP to replace the word vector in the retrieval system.
 Table 4 shows the MAP values of all algorithms in information retrieval. The MAP values of PBP and PGS are higher than that of baseline (PBP 4% improvement and PGS 32 . 4% improvement), while the MAP of PVB is signifi-cantly lower. This result implies that the topic features learned by PVB cannot provide differentiable semantics in information retrieval. Indeed, these topic fea-tures significantly reduce the relevance between queries and retrieved documents when compared with the baseline. From the top words shown in Fig. 5 ,wecan see that PVB find many duplicated topics that are semantically ambiguous. Although PBP can recover topic features with the highest likelihood in Fig. 4 B, it has a smaller MAP value than PGS in Table 4 . Analyzing the topic fea-tures by PBP and PGS, we find that PBP cannot differentiate similar queries in latent topic space. For example, two queries such as  X  X ashion clothes X  and  X  X ew clothes X  will have the similar topic feature vectors, where the top likely 40 topic features are almost the same. On the contrary, PGS, due to its random sampling process, can better understand the subtle semantic difference between these two queries, where the top 5 likely topic features of two queries are different but useful. The experimental results confirm that PGS is more suitable to learn the latent semantic information for short documents like queries. One major reason is that PGS infers the posterior p ( z | x , X , X  ) to find the best labeling configu-ration z  X  over words, which provides more explainable topic features for each word. As a comparison, PBP integrates out z during inference and cannot find the best topic labeling configuration. 4.4 Online Advertising System Online advertising has been a fundamental financial support of the many free Internet services. Most contemporary online advertising systems follow the Gen-eralized Second Price auction model, which requires that the system is able to predict the CTR of an ad. One of the key questions is the availability of suitable input features that allow accurate CTR prediction for a given ad impression. These features can be grouped into three categories: ad features including bid phrases, ad title, landing page, and a hierarchy of advertiser account, campaign, ad group and ad creative. User features include recent search queries, and user behavior data. Context features include display location, geographic location, content of page under browsing, and time. Most of these features are text data in word vector space. We learn an L 1 -regularized log-linear model as the base-line, which uses a set of text and other features such as ad title, content of page under browsing, content of landing page, ad group id, demographic informa-tion of users, categories of ad group and categories of the page under browsing. Besides the traditional ad features mentioned above, we add their topic fea-tures ( 15 ) by PVB, PGS and PBP as input to L 1 -regularized log-linear model. the Task 2 in KDD Cup 2012, in performance measure Area Under ROC Curve (AUC). The higher AUC value the better prediction performance. The AUC value of the baseline model is 0 . 74393. Fig. 6 shows the relative improvement of AUC over the baseline model. PGS gains the highest AUC improvement (around 1 . 5% improvement). The AUC improvements of PVB and PBP are slight when compared with the baseline model. Analyzing the weights of the added topic features, we find that the weights of topic features by PVB and PBP are almost zeros, implying that these topic features do not influence the prediction result. These results are consistent with those in search engine. In this study, we build a novel multi-channel MapReduce framework to compare fairly three representative parallel LDA algorithms. Although PBP achieves the best topic modeling performance, it performs worse than PGS in two industrial applications like search engine and online advertising system. PVB does not work well in both topic modeling accuracy and industrial applications. We think that the major reason may lie in their different inference objectives. In our future work, we will study their performance on more applications, and improve the parallel architecture for online LDA algorithms to handle streaming data.
