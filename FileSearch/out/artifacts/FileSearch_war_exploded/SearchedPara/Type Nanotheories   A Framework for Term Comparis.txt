 We present in this paper Type Nanotheories (TN), a framework for representing the knowledge necessary for performing simi-larity comparisons between pairs of terms of the same type. TN itself uses another methodology, namely Support Outcomes, which is also introduced. Many IR and NLP applications use redundancy as a factor to incr ease confidence, and TN-based comparisons can determine redundanc y better than simple string comparisons. Results include a showing of a 14% increase in Confidence-Weighted Score for an end-to-end QA system and an up to 68% improvement over baseline in an answer-key equivalencing experiment. I.2.7 [ Artificial Intelligence ]: Natural Language processing  X  text analysis.
 Design, Experimentation. Term Comparison, Question Answering, Type Systems. Named-entities, broadly construe d, are the basic currency of many applications in the field of Knowledge Management. On the structured side, they are often the keys and values of data-base lookups, while on the unstructu red side they play a variety of roles. In IR they form the bulk of important keyword query terms; in Question-Answering they are the topics of questions and very often the sought answers; they play key roles in Sum-marization and Rhetorical Structure Analysis. In all these cases it is useful to know when two specific instances refer to the same quantity. The knowledge a nd methods to do this have not as far as we know until know been organized in a concise, pow-erful and easy to extend framework. Knowledge management applicat ions in general and IR and NLP applications in particular often require comparing two terms found in text. Strict equa lity may sometimes be the neces-sary condition, but approximate matching can often be suffi-cient, and sometimes even pref erred. The method usually em-ployed in applications to perform such comparisons is string or substring matching, typically afte r case normalization and either lemmatization or stemming; occasionally after spell-checking too. When the same term is found to occur multiple times in a proc-essing context, we have an occurrence of redundancy , which is sometimes desirable and sometimes not. Some situations in NLP applications where term comparison is necessary include:  X  Identifying repeated answers in Question-Answering sys- X  Identifying repeated terms in cluster labeling programs (here  X  Forming co-reference chains; part of this problem concerns In general, one can view the most desirable result of a compari-son between two terms not as a bi nary yes/no, but a measure of the degree of match, typically given in the range of 0-1, plus a descriptor of the kind of matc h encountered. The strength of this match can be used to inform whatever decision-making apparatus acts upon the comparison result. For example, in a Question-Answering context, the degree of match can be trans-lated to degree of mutual support, as we shall see later. The kind of match can be important to applications which are con-sidering using one term to represen t two or more terms that were positively compared; for numeric terms, arithmetic closeness might be important, but for other others subsumption or other relationships might be preferable to use  X  we will be more con-crete about this later. In IR, term matching is typically done in the core of the search engine (after pre-processing), a nd results in a term-by-term bi-nary decision. The term compar ison processing described in this paper is not recommended for imp lementation there, for per-formance reasons. However, th e normalization functions that we will describe can be pa rt of the pre-processing. Now, simple string-oriented normalization and comparison op-erations do not account for th e systematic and idiosyncratic variations of expression commonl y used in language, and thus often miss equivalency and near-e quivalency. One particular difficulty that automatic system s face is that although the sys-tematic variation is very widespread, the specific nature of the variation depends on the type of the term under consideration. To take a simple example, consider the simple substring test. The relationship between  X  X alifornia X  and  X  X orthern Califor-nia X  is quite different from that between  X  X atman X  and  X  X atman Returns X  or between  X  X odium X  a nd  X  X odium chloride X , and so the consequent decisions of whet her to treat the pairs the same way and/or whether they should mutually support each other must be made differently. To address this problem, we w ill introduce the notions of Sup-port Outcomes and Type Nanotheories 1 . Support Outcomes (SO) are a codification of th e different ways (e.g. EQUAL, CLOSE, SUBSUMES etc.) two te rms may be judged to be the same or similar. Type Nanoth eories (TN) is a framework for performing term comparisons based on the semantic types of the terms. While applicable in both NLP and IR contexts, the effect of TN is more readily measurable in the former. To apply TN to an IR system, one would not wish perf orm the implementation within the core search engine for performance reasons, but to revisit the degree of match on terms within the leading documents re-trieved. Any benefit observed w ould be a conflation of TN itself and named-entity recognition and other abilities. Thus to isolate the effects of TN we have chosen to evaluate it within a Ques-tion-Answering context. This paper describes the TN an d SO frameworks, and one par-ticular implementation that has been shown to be effective with our type system. Two evaluations are presented: on our end-to-end QA system we show a 14% increase in Confidence-Weighted Score and 25% increase in Ranking Ability, and in an answer-key equivalencing experi ment we show increases of 52%-68% absolute over baseline. In a QA system, finding multiple occurrences of the same an-swer in a set of candidate answers is an indication that the an-swer X  X  confidence should be gi ven a boost. These duplicates can arise from searching multiple corpora, using multiple agents or multiple algorithms, or simply from analyzing multiple documents/passages returned by s earch (see, e.g [3, 12]). [7] performs an extensive analysis of the contribution of different factors to redundancy-based QA techniques, using ablation stud-ies. However, none of these investigate term-matching beyond simple string comparisons. Also in QA, similarity between relation-paths between two terms is explored in the soft pa ttern-matching of [4], but despite intensive efforts to match variant relationship chains, no flexibil-ity in term matching is handled there either. In our earlier work in [11], we discuss the need for methods to establish term equivalence for Question Inversion. In this context, a question is inverted by removing a term and then asking the inverted question about a candidate answer, hoping to get back the origi-nal term. We point out that different lexical forms or specifici-ties of the same original concept can get returned, thus necessi-tating type-based equivalency checking, but beyond describing the problem, no solution is offered. Investigations of models of stat istical strengths of association based on co-occurrence in large corpora are found in [2]; Latent Semantic Indexing [5] and Latent Relational [14] also measure relatedness due to co-occurrence. [13] uses a combination of 1 The name is a play on the Microtheories of Cyc [6], which are apparently 1000 times as big. occurrence counts and position in a taxonomy to determine simi-larity. None of these statistical approaches can be explicit about the kind of similarity that exists between terms. The closest work found to ours is that of Moriceau [10]. She examined sets of answers to numerical questions to determine the manner of variation (time, pla ce or other restriction) between correct answers. However, the focus of her work was on gener-ating from the set the best (most accurate and representative) single answer, and the discussion concentrates on analysis of the contexts of the answer words, not the words themselves. In doing failure analysis of our QA system, we discovered that often the candidate answer li st contained mutually-supporting instances of the same concept w ith different le xicalizations. Although the system rewarded redundancy by boosting the score, in many cases these instances were not detected as the same or similar, and thus none of the correct answers ended up with the highest score. This is particularly damaging in an evaluation context where the question is judged right or wrong based solely on the top answer. Had equivalency across these non string-matching terms been recognized, a simple reinforce-ment scheme could have boosted one of the correct instances into first place. In this section we will present some motivating examples. We observed that for the most part, fairly simple techniques could be used to successfully compare two terms; the exact techniques, however, whether they were in the form of manipu-lation or table lookup or something else, were very dependent on the semantic types of the terms. We developed two notions to address this need, and they are the subjects of subsequent sec-tions:  X  Support Outcomes : A set of discrete outcomes that compari- X  Type Nanotheories : A hierarchy of type-based normalizers Table 1 illustrates some TREC questions where two or more non-top-ranked candidate answer terms were mutually support-ing (through some kind of support outcome , to be described); in each case the first listed term was later promoted to first place when supported. We will now discuss the kinds of normalization and comparison that would be necessary to get the terms in the right-hand col-umn to match. The following list numbers correspond to rows in Table 1. Space does not perm it a discussion of all types and all kinds of simila rity relationships. 1a/b. LENGTH/MEASUREMENT:  X 120 feet X  and  X 120-foot X  are equal to each other;  X 120ft 4i n X  is clearly close to them. Simple transformations can handle the first two, but a notion of compound measurements and how to normalize them is needed for the third one. There is also a need to handle distances using different units and different systems of measurement (e.g.  X 42.195 kilometers X ,  X 26.2-mile X ) , using tables of conversion factors. This discussion generalizes to all units of measurement, although each one has its own te rminology, conversion factors and idiosyncrasies (such as LENGTH syntax:  X 5-foot-8 X  2 ). 2. COLLEGE: A common strate gy used to abbreviate U.S. state college names is to use  X  X  X  followed by the abbreviation for the state. 3. LOCATION(a):  X  X dense, Denm ark X  is located inside  X  X en-mark X . When the  X  X ontained-place comma containing-place X  syntax is used, the relationship can be determined with no exter-2 An answer to  X  X ow tall is Tom Cruise? X  nal knowledge, but otherwise table lookup is required (e.g. if the second term were merely  X  X dense X ). 4. LOCATION(b): Suffixing a location with a geophysical term ( X  X eninsula X ,  X  X sland X , et c.) usually doesn X  X  change the designated location. Generally speaking, preceding a location with  X  X outhern X ,  X  X orthern X ,  X  X entral X  etc. indicates a part of the same location. This is not always so with  X  X orth X ,  X  X outh X  etc.  X   X  X estern Virginia X  is not the same as  X  X est Virginia X . There are other subtleties: prefixing a city name with  X  X ld X  typically indicates the old/ancient quarter, contained within the city itself, but  X  X ew X  followed by a city, c ounty or country usually denotes an entirely different entity. 5. DATE:  X 1939 X  is within the range  X 1930-1940 X , so a sub-sumption (in the sense of part-of) is detected. In addition, syn-tax such as  X 1930s X  and  X 30s X  n eeds to be handled, as well as dates mentioning months and days , seasons or centuries. Gener-ally, sensible heuristic size ratio limits can be used for subsump-tion, so a year might be decree d to subsume a month, a decade might subsume a year, or a century a decade, but a century should probably not subsume a year 3 . 6. POPULATION: Since popul ations change over time, and also are subject to different cal culation/estimation methods, it is common to find many different renditions of purportedly the same value in a corpus of any size. Therefore, approximate equality should be considered to be quite a positive result. The purpose of the framework being presented here is to make explicit and organize the knowledge necessary to first normalize and then compare terms in a type specific way, so that mutual support of the kind shown above may occur. Support Outcomes form an open-ended set of outcome labels that are deemed useful for the user X  X  application. The ones pre-sented here have been used successfully in a QA system. While the list is technically open-ended, it is expected to be complete enough as-is for many other applications too, although in Sec-tion 7.2 we talk about one other outcome one might want to add. The outcomes described here are shown later in Table 2. Com-parisons take place on terms that have been normalized. The outcome of EQUAL occurs when two terms are the exact same string. EQUIVALENT is us ed for numerical values that are very close, such as might be due to rounding (e.g.  X 100,123,456 X  and  X 100 million), COMPARABLE is used in a similar manner for non-numerics ( X  X exas Ranger X  and  X  X exas Rangers X ). CLOSE is used when two numeric terms are within a small margin  X  the closeness threshold  X  of each other; AROUND is similar but with a larger threshold. OVERLAP is used for numeric ranges that overlap (e.g.  X 30 to 40 Fahrenheit X  and  X  X etween 35 and 42 degrees Fahrenheit X ). SUBSUMES is used when one term is more specific than the other, for example,  X  X witzerland  X  and  X  X urich, Switzerland X . It is also used for numeric ranges when one includes the other or for numeric ranges and point-val ues, when the point value is 3 Well of course technically it does, but we mean that because the two are magnitudes apart, one should not support the other. within the range. SUBSUMED is the inverse of SUBSUMES. The label SUBSUMES is more broadly understood than onto-logical subsumption, since it is also used to indicate part-of . RESTATEMENT indicates that one of the terms is a restatement quently in text a term is followed, often in parentheses, by a different representation of the sa me concept or value, for cases where the reader might be unfamiliar with one or the other forms. Examples include, in either order:  X  Imperial and metric measurements  X  Monetary amounts in foreig n and domestic currencies  X  Foreign words and their translations  X  Abbreviations/acronyms and their expansions Not just for QA, but in the event that any of these situations occur for terms that are the subject of search, algorithms such as tf*idf that reward high term fre quencies should not count re-statements as being multiple occu rrences, so it is important that this condition be detected, an d concrete support withheld. Finally, the outcome NONE is ge nerated when n one of the oth-ers is found to apply. In the rest of this paper, the phrase  X  X ositive outcome X  refers to any Support Outcome other th an NONE or RESTATEMENT. Type Nanotheories (TN) are embodi ments of the essential lexi-cal characteristics of semantic types  X  i.e. how terms are ex-pressed in text. Each Nanotheory is a collection of structures, properties and methods in an or ganizational framework; Java programmers can think of them as classes. Each TN has the following properties:  X  An associated set of zero or more type-system types.  X  A location in a TN inheritance hierarchy.  X  A method M TN, i for evaluating each of the Support Out- X  A type-specific normalization method Z TN .  X  A set of other methods to compute various properties, either  X  A structure S TN for representing analyzed terms.  X  A method to decompose a term into its logically constituent The TN hierarchy will generally resemble the type ontology, but will differ from it in places. The reason for this is that the type ontology is a hierarchical representation of what entities are , while the TN hierarchy represents how entities are written . The TN hierarchy emerges from a consideration of written rep-resentations of entities of given types. For example, our seman-tic types 4 Quotation and Composition (titled work) are totally unrelated, but since the recognizer for Composition assumes such entities are all written insi de quote marks, the lexical prop-erties of the two types are very much related, and in fact the COMPOSITION TN is a child of the QUOTATION TN. Every TN node has or inherits a comparator method to evaluate whether any of the Support Outcomes holds for a pair of terms of that type. The top of the hierarchy is a TN (called TOP) where all comparator methods evaluate to false, except for the EQUAL method which test s for string equality. It is not necessary to directly associate every type with a TN, since inheritance in the type ontology can be used. For example, the type hierarchy may have the type Nation with AsianNation , EuropeanNation , etc. as subtypes, and SoutheastEuropeanNa-similar manner, only one TN is needed for Nation and its chil-dren. Finding an appropriate TN for a given entity type is now simply a matter of climbing up th e type hierarchy to the first found type that maps to a TN. Table 3 lists the TNs we have developed to handle our training data, along with their parents in the TN hierarchy and types that are associated directly with them. Every type that we use in our QA work is so associated or has an ancestor that is so associ-ated. Normalization. Each TN has a method to perform type-specific normalization, which precedes the comparators. The POPULA-TION normalizer will strip off term prefixes like  X  X  population of X  and suffixes like  X  X itizens X ,  X  X esidents X  etc. The FACILITY normalizer will invert expressi ons like  X  X useum of X X  to  X  X  Museum X . The kind of knowledge required for these operations is similar to that needed by the Named Entity Recognition that identified the terms in the first place  X  we will discuss this later. Canonical Forms. The TNs have more than comparator meth-ods. Each TN has a frame-and-slot structure S TN which is used to hold deconstructed entities. Thus, S DATE will have slots for day, month, year, (both points and ranges), season, era and  X  X ther X . Any date entity processed by the system is analyzed and one or more appropriate slots are filled. Comparisons are then performed easily by reference to these slots. The merge method C TN will take two structures and create a merged form, if possible. Thus, suppose the S PERSON has slots for title, first name/initial, middle name/initial, last name, and we have populated structures for  X  X ohn A. Smith X  and  X  X r. J. Smith X . Merging them will give  X  X r. John A. Smith X . By iterating through a set of potentially compatible terms canonical forms for equivalence cla sses in the set are generated. 4 Type names are written here in italics , to distinguish them from TNs, often similarly named. 5 One will not want to do this across an entire corpus, since the a-priori probability is too low that such similar-looking terms refer to the same entities. However, within a document or across a hit-list in response to a query or question, these prob-abilities are much higher. Other properties of TNs. TNs have collecti ons of utility meth-ods; for example, whether RE STATEMENT is a possible out-come for it, whether commas are important in its strings or should be normalized away. TN s also contain type-specific vocabulary lists for use by the comparators and normalizers. In order to make use of the support outcomes that the TN com-parators produce, two more conc epts need to be introduced:  X  Support Factors . Numeric amounts associated with Sup- X  Support Functions . Procedures for adjusting the scores of Support Factors. The Support Outcome framework does not dictate how these outcomes are to be used in the application. Instead, we suggest numerical values be associated with them, to control how much effect a positive outcome will have. In our Question Answering application, terms in the candidate answer list are compared with each other in order to boost the score of repeated answers, modulo comparison outcome. We associate each outcome with a numeric Support Factor, be-Clearly, the extreme values are for EQUAL and NONE. Fol-lowing earlier arguments, RESTA TEMENT has factor 0. We chose to make EQUIVALENT a nd COMPARABLE the same as EQUAL since we did not have enough training data to make what we thought would be very fi ne distinctions. The remaining values were discovered by hi ll-climbing the search space. We made the closeness threshold 5% for most numerical types, but 10% for POPULATION. The threshold for AROUND was twice that. Support Functions. A support function is a procedure for boosting a candidate answer X  X  score based on mutual support from other candidates. This s ection describes a support function that we found useful. with score s 2 the outcome has support factor G 12 , the new score for T 1 is computed by the support function f : interpretation for combining probabilities. The function f has the nice commutative property that if T 1 is supported by two other terms T 2 and T 3 , with scores s 2 and s 3 , and support factors G 12 and G 13 , then so it doesn X  X  matter in which order the supports are applied. This generalizes for arbitrar y numbers of supporting terms. For both the evaluations reporte d below, we trained on data from TREC2001 and 2002 (500 questions from each), and tested on the 413 factoid questions from TREC2003 6 . The first evalua-tion shows the impact of using our methodology on end-to-end QA system performance. While showing a considerable boost, the benefits were limited to those situations where two or more mutually-supporting correct answers were lower than first place in the result list. The second evaluation was designed to exhibit the potential benefits of TNs more broadly, and does so by rec-onciling the frequently-occurring different forms of an answer in TREC-QA answer keys. For this evaluation we ran our QA system with and without the new TN Framework. We report three figures: the number of 1 -place answers, the Confidence-Weighted Score (CWS), and the Ranking Ability. The CW S was introduced in TREC2002 as a way of rewarding systems who  X  X new what they knew X  [15]. Briefly, an entire result answer set is reordered in decreasing order of confidence. The eval uation is calculated by equation EQ1, which weights answers earlier in the list more than those 6 Data from past TREC evaluations is available from the TREC web site at http://tr ec.nist.gov/data/qamain.html The CWS score is of particular relevance to us, since an already correct answer in the baseline condition will not increase the #correct score by merely having its score boosted. But if such boosting makes the system more confident in correct answers than incorrect ones, this is a be nefit to the end-user, which is reflected in the CWS score. Indeed, we see in Table 4 a 14% increase in CWS. The Ranking Ability [1] is The expCWS (expected CWS ) is what one would get with no ability to assign confidence scores, and is equal to %correct. The maxCWS value for n correct out of N is given by ([1]): Note that Ranking Ability is on a scale of -1 to 1. In TREC2002 only three participants had an RA of over 0.6 7 . We see that our TN methodology can increase the Ranking Ability by 25% in magnitude. For this evaluation we used the answer keys made available by NIST and/or NIST participants. The answer keys contained patterns to match all correct answers extracted from text, as determined by the NIST assessors . Many questions either had multiple patterns or patterns that expanded (see below) into multiple instances. We considered these to be mostly different representations of the same entity, and so provided good test data for our normalizers and comparators. Preparing the Data. The TREC answer-key files are in a stan-dard format, so that common evaluation scripts can be used by the community. Each line in an answer key file consists of the question number label, followed by a space and a regular ex-pression in Perl. Several lines can be used with the same ques-tion label in order to indicate a disjunction of alternative an-swers. We made literal answer strings from patterns. Some patterns  X  e.g.  X  X nsect(ivorou)?s? X   X  had b een written too generally; only the clearly intended expansions were generated. Finally, as mentioned earlier, some questions had sets of differ-ent correct answers, not just different forms of the same answer. For example, question 1898  X  X hat city is Disneyland in? X  had answers  X  X aris,  X  X naheim X  and  X  X okyo X . Because it was not reasonable to expect these answ ers to have a positive compari-7 While a system X  X  base accuracy may vary from test set to test set, we have informally observed that Ranking Ability is more stable  X  the %correct of our system over 3 TREC question sets varied by a over a factor of 2, but the RA varied by less than 10%; consequently comparing our RA on TREC2003 data with community TREC2002 results is not unreasonable. son outcome, we manually changed the labels in the answer key file to 1898a, 1898b and 1898c so they would not be compared, and made similar changes to other cases of clearly different answers. [When we left these la bels untouched, which we did as an experiment to verify that our comparators were not producing false positives, no wrong match (out of 96 possible) was gener-ated.] Table 5 shows the result of the evaluation on every answer pair with the same label. Column 1 is the name of the type, column 2 shows how many positive comparison outcomes were found out of the total possible in column 3, and column 4 is the ratio of these numbers. Column 5 is the baseline ratio, which is the result of running our original normalization procedure. The baseline did lemmatization, nu mber normalization and some basic person and place normalization. LARGEWHOLENUMBER 66 66 1.0 0.05 MUSICALINSTRUMENT 1 1 1.0 0.0 The micro-average went from 0.19 to 0.71; the macro-average went from .09 to 0.77. There were a total of 961 trials. The cases where the baseline was beaten are shown in bold . The TN is invoked after our standard lemmatization and normalization, which as can be seen from the ba seline numbers fares very well with person names, and moderate ly with some other quantities like times and places, but is otherwise quite limited. We currently have 36 comparators in the TN hierarchy, covering 124 types in our type-system. The TREC2003 answer set con-sisted of instances of 59 of our ty pes; only 33 of them (the rows in Table 5) had variations. There are often no definitive arguments regarding how the com-parison of two terms should tu rn out. Should  X  X bout 750 X  ex-actly match  X 763.035 X , si nce both authors might have the same number in mind, but one is offering a round number? Indeed one could argue for SUBSUMES, too. Our system takes the direct path of stripping qualifie rs expressing approximation, so the outcome in our case is CLOS E. We believe such fine dis-tinctions rarely make any di fference, for our purposes. Many questions are asked abou t how people die; sometimes variant correct answers describe different degrees of detail - X  X uicide X  vs.  X  X tepping into an English river to her death X  different degrees of immediacy - X  X rug abuse X  vs.  X  X eart at-tack X  9 . It is in principle possible to build a comparator that un-derstands such relationships, or at least the simpler ones be-tween, say,  X  X ssassinated X ,  X  X hot X  and  X  X urdered X . We did not, and the consequent scores for corresponding types probably suffered. As just stated, we currently make no attempt to identify syno-nyms, although the TN framework makes it easy to plug in such a component. For simple terms, WordNet [8] is the obvious choice, but it is less directly useful for some compound terms than single words. For example, consider the provided answers to  X  X ow did George Washington die? X  , namely WordNet gloss for quinsy 10 . This may be achievable with the lexical chaining and Logic Prover of [9], but this may be too heavyweight for some applications. Statistical approaches as mentioned in Section 2 can be used to establish an unnamed connectio n between two terms  X   X  X unger X  and  X  X ood X , for example. This would suggest adding a new outcome UNNAMED, adding a comparator method for it to all TNs that might benefit from it (e.g. DISEASE, or NOUN-PHRASE), and learning the corresponding Support Factor. 8 Virginia Woolf 9 Jerry Garcia 10 The gloss reads:  X  X eritonsillar abscess --(a painful pus-filled inflammation of the tonsils and surrounding tissues; usu-ally a complication of tonsillitis) X  While there is no direct link between TNs and Named-Entity Recognizers, it is clear that the former will do better if it shared the same understanding of a type-sys tem type as the latter  X  viz. the discussion of Quotation and Composition earlier. Further-more, benefit can be gained from sharing vocabulary lists. The exact relationship between these tw o is still emer ging, and will be the subject of further work. As an example of the overlap, in order for  X  X tlanta X  and  X  X al-cons X  to be considered the same in answer to a Sportsteam ques-tion, a list of professional team s including  X  X tlanta Falcons X  must be used by the comparator, presumably the same list that the named-entity recognizer used. However, the degree of so-phistication required is not the same in both cases. Now consider the recognizer for type River . One can identify three subclasses of river, relati ve to how they are most often written. The identifier  X  X iver X  is either an optional prefix, an optional suffix, or a required suffix, thus: A named-entity recognizer will have very high precision if it has river lists, in order to correctly recognize the presence and span of river mentions in text of th e form  X  X he X Y Z ... X . However, if the RIVER TN is presented with terms that have already been so recognized, then for normalization all it need do is strip out the  X  X he X  and  X  X iver X . In this case, no such lists are needed. Building the TN hierarchy requires manual effort  X  in our case 2-3 person-months to generate th e benefits shown in our evalua-tions. About half of the time wa s spent in designing the frame-work, and the other half in populating it. The latter was charac-terized by capturing and encoding the information in our rule-based named-entity recognizer (N ER), along with access to its authority lists. Especially as the nodes in the TN hierarchy re-flect the types in our type-system, subject to the aggregation described in Section 5, the implemented TN framework is just as general-purpose and as scalable as the type-system and NER themselves. These two are both general-purpose and intended for open-domain applications, a lthough in English only. Given that the TN substrate now exists , re-implementing for a different language would be primarily a matter of capturing the corre-sponding NER rules, along with identification of any idiosyn-cratic inter-type relationshi ps (cf. the discussion of Quotation and Composition ). By introducing a component to perform generalized comparison of terms, significant performance increases can be realized in applications that can take advantage of such a framework. To do this on a reasonable scale requ ires a large amount of linguis-tic knowledge, in particular how entities of different semantic types are expressed in variant forms in written language. A framework called Type Nanotheor ies was introduced for arrang-ing, storing and executing this knowledge, along with a Support Outcome methodology for communicating outcomes of com-parisons. Such knowledge is typically alr eady present in any rule-based named-entity recognizer. The work described in this paper in-volved recoding of these rules, but it is ultimately desirable to allow both components to share a common representation or library. The Type Nanotheory framework is a hierarchical arrangement of type-based aggregations of knowledge of how instances of the type are typically expressed. This knowledge is in the form of comparators and other functions, along with methods of de-composing instances into their component parts and generating canonical forms. Two evaluations were performed; the first on the overall per-formance of a QA system showing a 14% increase in Confi-dence-Weighted Score and a 25% increase in Ranking Ability; the second on sets of equivalent answers in an answer key showed a 52-68% increase over the baseline in ability to identify similar answers. [1] Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and D. Fer-[2] Church, K.W. and Hanks, P. "Word Association Norms, [3] Clarke, C., Cormack, G., Kisman, D. and Lynam, T. [4] Cui, H., Kan, M-Y. and Chua , T-S. Unsupervised Learning [5] Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.Landauer, [6] Lenat, D. 1995. "Cyc: A Large-Scale Investment in [7] Lin, J. An Exploration of th e Principles Underlying Redun-[8] Miller, G.  X  X ordNet: A Lexical Database for English X , [9] Moldovan, D.I. and Rus, V.  X  X ogic Form Transformation [10] Moriceau, V.  X  X umerical Data Integration for Cooperative [11] Prager, J.M., Duboue, P. and Chu-Carroll, J. "Improving [12] Prager, J.M., Chu-Carroll, J. and Czuba, K. "Question An-[13] Resnik, P. Using information content to evaluate semantic [14] Turney, P. Measuring Semantic Similarity by Latent Rela-[15] Voorhees, E.  X  X verview of the TREC 2002 Question An-
