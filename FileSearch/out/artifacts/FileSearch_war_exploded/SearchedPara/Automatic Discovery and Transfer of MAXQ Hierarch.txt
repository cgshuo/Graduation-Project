 Neville Mehta mehtane@eecs.oregonstate.edu Soumya Ray sray@eecs.oregonstate.edu Prasad Tadepalli tadepall@eecs.oregonstate.edu Thomas Dietterich tgd@eecs.oregonstate.edu Oregon State University, Corvallis OR 97331, USA Scaling up reinforcement learning (RL) to large do-mains requires leveraging the structure in these do-mains. Hierarchical reinforcement learning (HRL) pro-vides mechanisms through which domain structure can be exploited to constrain the value function and pol-icy space of the learner, and hence speed up learning (Sutton et al., 1999; Dietterich, 2000; Andre &amp; Rus-sell, 2002). In the MAXQ framework, a task hierarchy is defined (along with relevant state variables) for rep-resenting the value function of the overall task. This allows for decomposed subtask-specific value functions that are easier to learn than the global value function. Automated discovery of such task hierarchies is com-pelling for at least two reasons. First, it avoids the sig-nificant human effort in engineering the task-subtask structural decomposition, along with the associated state abstractions and subtask goals. Second, if the same hierarchy is useful in multiple domains, it leads to significant transfer of learned structural knowledge from one domain to the other. The cost of learning can be amortized over several domains. Several researchers have focused on the problem of automatically induc-ing temporally extended actions and task hierarchies (Thrun &amp; Schwartz, 1995; McGovern &amp; Barto, 2001; Menache et al., 2001; Pickett &amp; Barto, 2002; Hengst, 2002; S  X im  X sek &amp; Barto, 2004; Jonsson &amp; Barto, 2006). In this paper, we focus on the asymmetric knowledge transfer setting where we are given access to solved source RL problems. The objective is to derive use-ful biases from these solutions that could speed up learning in target problems. We present and evalu-ate our approach, HI-MAT, for learning MAXQ hier-archies from a solved RL problem. HI-MAT applies dynamic Bayesian network (DBN) models to a single successful trajectory from the source problem to con-struct a causally annotated trajectory (CAT). Guided by the causal and temporal associations between ac-tions in the CAT, HI-MAT recursively parses it and defines MAXQ subtasks based on each discovered par-tition of the CAT.
 We analyze our approach both theoretically and em-pirically. Our theoretical results show that, under appropriate conditions, the task hierarchies induced by HI-MAT are consistent with the observed trajec-tory, and possess compact value-function tables that are safe with respect to state abstraction. Empiri-cally, we show that (1) using a successful trajectory can result in more compact task decompositions than when using only DBNs, (2) our induced hierarchies are comparable to manually-engineered hierarchies on target RL tasks, and MAXQ-learning converges signif-icantly faster than flat Q-learning on those tasks, and (3) transferring hierarchical structure from a source task can speed up learning in target RL tasks where transferring value functions cannot. We briefly review the MAXQ framework (Dietterich, 2000). This framework facilitates learning separate value functions for subtasks which can be composed to compute the value function for the overall semi-Markov Decision Process (SMDP) with state space S and action space A . The task hierarchy H is repre-sented as a directed acyclic graph called the task graph , and reflects the task-subtask relationships. Leaf nodes are the primitive subtasks corresponding to A . Each composite subtask T i defines an SMDP with param-eters  X  X i , S i , G i , C i  X  , where X i is the set of relevant state variables, S i  X  S is the set of admissible states, G i is the termination/goal predicate, and C i is the set of child tasks of T i . T 0 represents the root task. T i can be invoked in any state s  X  S i , it terminates when s  X  G i , and ( s, a ) is called an exit if Pr( s 0 | s, a ) &gt; 0. The set S i is defined using a projection function that maps a world state to an abstract state defined by a subset of the state variables. A safe abstraction function only merges world states that have identical values. The local policy for a subtask T i is a map-ping  X  i : S i 7 X  C i . A hierarchical policy  X  for the overall task is an assignment of a local policy to each T . A hierarchically optimal policy for a given MAXQ graph is a hierarchical policy that has the best pos-sible expected total reward. A hierarchical policy is recursively optimal if the local policy for each subtask is optimal given that all its child tasks are in turn re-cursively optimal.
 HEXQ (Hengst, 2002) and VISA (Jonsson &amp; Barto, 2006) are two existing approaches to learning task hi-erarchies. These methods define subtasks based on the changing values of state variables. HEXQ employs a heuristic that orders state variables based on the fre-quencies of change in their values to induce an exit-option hierarchy. The most frequently-changing vari-able is associated with the lowest-level subtask, and the least frequently-changing variable with the root. VISA uses DBNs to analyze the influence of state vari-ables on one another. The variables are partitioned such that there is an acyclic influence relationship between the variables in different clusters (strongly-connected components). Here, state variables that in-fluence others are associated with lower-level subtasks. VISA provides a more principled rationale for HEXQ X  X  heuristic  X  a variable used to satisfy a precondition for setting another variable through an action typically changes more frequently than the other variable. A key difference between VISA and HI-MAT is the use of a successful trajectory in addition to the DBNs. In Section 5.1, we provide empirical evidence that this allows HI-MAT to learn hierarchies that are exponen-tially more compact than those of VISA.
 The algorithm developed by Marthi et al. (2007) takes a search-based approach to generating hierarchies. Flat Q-value functions are learned for the source do-main, and are used to sample trajectories. A greedy top-down search is conducted for the best-scoring hi-erarchy that fits the trajectories. The set of relevant state variables for each task is determined through sta-tistical tests on the Q values of different states with differing values of the variables. In contrast to this approach, HI-MAT relies less on direct search through the hierarchy space, and more on the causal analysis of a trajectory based on DBN models. In this work, we consider MDPs where the agent is solving a known conjunctive goal. This is a subset of the class of stochastic shortest-path MDPs. In such MDPs, there is a goal state (or a set of goal states), and the optimal policy for the agent is to reach such a state as quickly as possible. We assume that we are given factored DBN models for the source MDP where the conditional probability distributions are represented as trees (CPTs). Further, we are given a successful trajectory that reaches the goal in the source MDP. With this in hand, our objective is to automatically induce a MAXQ hierarchy that can suitably constrain the policy space when solving a related target prob-lem, and therefore achieve faster convergence in the target problem. This is achieved via recursive parti-tioning of the given trajectory into subtasks using a top-down parse guided by backward chaining from the goal. We use the DBNs along with the trajectory to define the termination predicate, the set of subtasks, and the relevant abstraction for each MAXQ subtask. We use the Taxi domain (Dietterich, 2000) to illustrate our procedure. Here, a taxi has to transport a passen-ger from a source location to a destination location within a 5  X  5 grid-world. The pass.dest variable is restricted to one of four special locations on the grid denoted by R, G, B, Y ; the pass.loc could be set to R, G, B, Y or in-taxi ; taxi.loc could be one of the 25 cells. The goal of pass.loc = pass.dest is achieved by taking the passenger to its intended destination. Be-sides the four navigation actions, a successful Pickup changes pass.loc to in-taxi , and a successful Putdown changes pass.loc from in-taxi to the value of pass.dest . 3.1. Definitions and Notation We say that a variable v is relevant to an action a if the reward and transition dynamics for a either check or change v ; it is irrelevant otherwise. The set of trajectory-relevant (t-relevant) variables of a , a subset of the relevant variables, are the variables that were actually checked or changed when a was executed in the trajectory. A causal edge a v  X  X  X  b connects a to another action b ( b following a in the trajectory) iff v is t-relevant to both a and b , and irrelevant to all actions in between. A sink edge, a v  X  X  X  End connects a with a dummy End action iff v is relevant to a and irrele-vant to all actions before the final goal state; this holds analogously for a source edge Start v  X  X  X  a . A causally annotated trajectory (CAT) is the original trajectory annotated with all the causal, source, and sink edges. Moreover, the CAT is preprocessed to remove any cy-cles present in the original trajectory (failed actions, such as an unsuccessful Pickup , introduce cycles of unit length). A sample CAT for Taxi is shown in Figure 1. Given a v  X  X  X  b , the phrase  X  X iteral on a causal edge X  refers to a formula of the form v = V where V is the value taken by v in the state before b is exe-cuted. We define DBN-closure( v ) as the set of vari-ables that influence v recursively as follows. From the action DBNs, add all variables that appear in internal nodes in the CPTs for the dynamics of v . Next, for each added variable u , union DBN-closure( u ) with this set, repeating until no new variables are added. Similarly, the set DBN-closure( reward ) contains all variables that influence the reward function of the MDP. The set DBN-closure( f luent ) is the union of the DBN-closure s of all variables in the fluent. For example, DBN-closure( goal ) is the set of all variables that influence the goal fluent. The CAT ignores all variables v /  X  DBN-closure( goal ) , namely, those vari-ables that never affect the goal conjunction. 3.2. The HI-MAT Algorithm Given a CAT and the MDP X  X  goal predicate (or re-cursively, the current subtask X  X  goal predicate), the main loop of the hierarchy induction procedure is il-lustrated in Algorithm 1. The algorithm first checks if two stopping criteria are satisfied (lines 2 &amp; 4): ei-ther the trajectory contains only a single primitive action, or it consists of actions whose relevances are identical. (In the latter case, any further partition-ing would yield subtasks with the same abstraction as the parent.) Otherwise, it first initializes the set of  X  X nsolved X  goals to the set of literals in the goal con-junction (line 9). It then selects any unsolved goal u , and finds the corresponding subtask (line 12). Algo-rithm 2 returns indices i, j marking the boundaries of the subtask in the CAT. If this CAT segment is non-trivial (neither just the initial state nor the whole tra-jectory), it is stored (line 17), and the literals on causal edges that enter it (from earlier in the trajectory) are added to the unsolved goals (line 18). This ensures that the algorithm parses the entire trajectory barring redundant actions. If the trajectory segment is equal to the entire trajectory, this implies that the trajectory achieves only the literal u after the ultimate action. In this case, the trajectory is split into two segments: one segment contains the prefix of the ultimate action a n with the preconditions of a n forming the goal literals for this segment (line 14); the other segment contains only the ultimate action a n (line 15). CAT scanning is repeated until all subgoal literals are accounted for. The only way trajectory segments can overlap is if they have identical boundaries, and the ultimate ac-tion achieves the literals of all these segments. In this case, the segments are merged (line 23). Merging re-places the duplicative segments with one that is as-signed a conjunction of the subgoal literals. The HI-MAT algorithm partitions the CAT into unique segments, each achieving a single literal or a conjunction of literals due to merging. It is called re-cursively on each element of the partition (line 27). It can be proved that the set of subtasks output by the algorithm is independent of the order in which the literal u is picked (line 11). 3.2.1. Subtask Detection Given a literal, a subtask is determined by finding the set of temporally contiguous actions that are closed with respect to the causal edges in the CAT such that the final action achieves the literal. The idea is to group all actions that contribute to achieving the spe-cific literal being considered. This procedure is shown in Algorithm 2.
 Algorithm 1 HI-MAT Algorithm 2 CAT-Scan As before, when considering causal edges in line 3, we can ignore all causal edges that are labeled with vari-ables not in the DBN-closure of any variable in the current unsolved goal list. Because of the way we con-struct the CAT, we can show that this procedure will always stop before adding an action which has a rel-evant variable that is not relevant to the last action in the partition. Note that the temporal contiguity of the actions we assign to a subtask is required by the MAXQ-style execution of a policy. A hierarchical MAXQ policy cannot interrupt an unterminated sub-task, start executing a sibling subtask, and then return to executing the interrupted subtask. 3.2.2. Termination Predicate After finding the partition that constitutes a subtask, we assign a set of child tasks and a termination pred-icate to it. To assign the termination condition to a subtask, we consider the relational test(s) t u in the action and reward DBNs involving the variable u on the causal edge leaving the subtask (line 27 of Algo-rithm 1). When a subtask X  X  relational termination condition involves other variables not already in the abstraction, these variables are added to the state ab-straction (line 30), effectively creating a parameterized subtask. For example, consider the navigation subtask that terminates when taxi.loc = pass.dest in the Taxi domain. The abstraction for this subtask already in-volves taxi.loc . However, pass.dest in the relational test implies that pass.dest behaves like a parameter for this subtask. 3.2.3. Action Generalization To determine if the set of primitive actions available to any subtask should be expanded, we follow a bottom-up procedure (not shown in Algorithm 1). We start with subtasks that have only primitive actions as chil-dren. We create a merged DBN structure for such a subtask T using the incorporated primitive actions. The merged DBN represents possible variable effects after any sequence of these primitive actions. Next, for each primitive action that we did not see in this trajectory, we consider the subgraph of its DBN that only involves the variables relevant to T . If this is a subgraph of the merged DBN of T , we add this ac-tion to the set of actions available to T . The ratio-nale here is that the added action has similar effects to the actions we observed in the trajectory, and it does not increase the set of relevant variables for T . For example, if the navigation actions used on the ob-served trajectory consisted only of North and East ac-tions, this procedure would also add South and West to the available actions for this subtask. When con-sidering subtasks that have non-primitive children, we only consider adding actions that have not been added to any of the non-primitive children.
 Given the termination predicate and the generalized set of actions, the set of relevant variables for a sub-task is the union of the set of relevant variables of the merged DBN (described above) and the variables ap-pearing in the termination predicate (line 30). Com-puting the relevant variables is similar to explanation-based reinforcement learning (Tadepalli &amp; Dietterich, 1997) except that here we care only about the set of relevant variables and not their values. Moreover, the relevant variables are computed over a set rather than a sequence of actions. In this section, we establish certain theoretical prop-erties of the hierarchies induced by the HI-MAT al-gorithm. We consider a factored SMDP state-space S = D x of variable x i . We assume that our DBN models have the following property.
 Definition 1 A DBN model is maximally sparse if for any y  X  Y where Y is the set of parents of some node x (which represents either a state variable or the reward node), and Y 0 = Y  X  { y } ,  X  y 1 , y 2  X  D y Pr( x | Y 0 , y = y 1 ) 6 = Pr( x | Y 0 Maximal sparseness implies that the parents of a vari-able have non-trivial influences on it; no parent can be removed without affecting the next-state distribution. A task hierarchy H =  X  V, E  X  , is a directed acyclic graph, where V is a set of task nodes, and E rep-resents the task-subtask edges of the graph. Each task node T i  X  V is defined as in Section 2.
 A trajectory-task pair  X   X  , T i  X  , where  X  =  X  s 1 , a 1 , . . . , s n , a n , s n +1  X  and T i =  X  X i , S is consistent with H if T i  X  V , and { s 1 , . . . , s n If T i is a primitive subtask then n = 1, and C i = a 1 . If T i is not primitive then { s 1 , . . . , s n }  X  G i =  X  , s n +1  X  G i , and there exist trajectory-task pairs  X   X  j , T j  X  consistent with H where  X  is a concatenation of  X  1 , . . . ,  X  p and T 1 , . . . , T p  X  C i . A trajectory  X  is consistent with a hierarchy H if  X   X  , T 0  X  is consistent with H .
 Definition 2 A trajectory  X  s 1 , a 1 , . . . , s n , a n non-redundant if no subsequence of the action sequence in the trajectory, a 1 , . . . , a n , can be removed such that the remaining sequence still achieves the goal starting from s 1 .
 Theorem 1 If a trajectory  X  is non-redundant then HI-MAT produces a task hierarchy H such that  X  is consistent with H .
 Proof sketch: Let  X  =  X  s 1 , a 1 , . . . , s n , a n , s the trajectory. The algorithm extracts the conjunction of literals that are true in s n +1 (and not before), and assigns it to the goal, G i . Such literals must exist since, otherwise, some suffix of the trajectory can be removed while the rest still achieves the goal, violating the property of non-redundancy. Since the set S i is set to all states that do not satisfy G i , the condition that all states s 1 , . . . , s n are in S i is satisfied. Whenever the trajectory is partitioned into a sequence of sub-trajectories, each sub-trajectory is associated with a conjunction of goal literals achieved by that sub-trajectory. Hence, the above argument applies re-cursively to each such sub-trajectory.
 Definition 3 A hierarchy H is safe with respect to the DBN models M if for any trajectory-task pair  X   X  , T i  X  consistent with H , where T i =  X  X i , S i , G i , C i  X  , the to-tal expected reward during the trajectory is only a func-tion of the values of x  X  X i in the starting state of  X  . The above definition says that the state variables in each task are sufficient to capture the value of any trajectory consistent with the sub-hierarchy rooted at that task node.
 Theorem 2 If the procedure HI-MAT produces a task hierarchy H from  X  and the DBN models M then H is safe with respect to M . Further, if the DBN models are maximally sparse, for any hierarchy H 0 which is consistent with  X  and safe with respect to M , and T i =  X  X i , S i , G i , C i  X  in H , there exists T 0 i =  X  X in H 0 such that X i  X  X 0 i .
 Proof sketch: By the construction procedure, in any segment of trajectory  X  composed of primitive actions under a subtask T i , all primitive actions check or set only the variables in X i . Thus, changing any other variables in the initial state s of  X  yielding s 0 does not change the effects of these actions according to the DBN models. Similarly, all immediate rewards in the trajectory are also functions of the variables in X i . Hence, the total accumulated reward and the probability of the trajectory only depend on X i , and the hierarchy produced is safe with respect to M . Suppose that H 0 is a consistent hierarchy which is safe with respect to M . Let a i be the last action in the trajectory  X  i corresponding to the subtask T i in H . By consistency, there must be some task T 0 i in H 0 that matches up with a i . Recall that X i includes only those variables checked and set by a i to achieve the goal G i . We claim that the abstraction variables X 0 i of T 0 i must include X i . If this is not the case then, by maximal sparseness, there is a variable y in X i  X  X 0 i and some values y 1 and y 2 such that the probabilities of the next state or reward are different based on whether y = y 1 or y = y 2 . Hence, H 0 would not be safe, leading to a contradiction.
 Corollary 1 If the DBN models are maximally sparse then the maximum size of the value function table for any task in the hierarchy produced by HI-MAT is the smallest over all safe hierarchies which are consistent with the trajectory.
 Proof: Direct consequence of part 2 of the previous theorem.
 The significance of the above corollary lies in the fact that the size of the value-function table is exponential in the number of variables n i = | X i | in the abstraction of task T i . If all features are binary and there are t tasks then the total number of values for the value-a tree with the primitive actions at the leaves, the number of subtasks is bounded by 2 l where l is the length of the trajectory. Hence, we can claim that the number of parameters needed to fully specify the value-function tables in our hierarchy is at most O ( l ) times that of the best possible.
 Our analysis does not address state abstractions aris-ing from the so-called funnel property of subtasks where many starting states result in a few terminal states. Funnel abstractions permit the parent task to ignore variables that, while relevant inside the child task, do not affect the terminal state. Nevertheless, our analysis captures some of the key properties of our algorithm including consistency with the trajec-tory, safety, minimality, and sheds some light on its effectiveness. We test three hypotheses. First, we expect that em-ploying a successful trajectory along with the action models will allow the HI-MAT algorithm to induce task hierarchies that are much more compact than (or at least as compact as) just using the action models. Second, in a transfer setting, we expect that the hier-archies induced by HI-MAT will speed up convergence to the optimal policy in related target problems. Fi-nally, we expect that the HI-MAT hierarchies will be applicable to and speed up learning in RL problems which are different enough from the source problems such that value functions either do not transfer or lead to poor transfer. 5.1. Contribution of the Trajectory To highlight our first hypothesis, a modified Bitflip do-main (Diuk et al., 2006) is designed as follows. The state is represented by n bits, b 0 b 1 . . . b n  X  1 . There are n actions denoted by Flip ( i ). Flip ( i ) toggles b i if both b i  X  1 is set and the parity across bits b 0 , . . . , b i  X  1 when i is even (odd otherwise); if not, it resets the bits b , . . . , b i . All bits are reset at the initial state, and the goal is to set all bits.
 We ran both VISA and HI-MAT in this domain with n = 7, and compared the induced hierarchies (Fig-ure 2). We observe that VISA constructs an ex-ponentially sized hierarchy even with subtask merg-ing activated within VISA. There are two reasons for this. First, VISA relies on the full action set to con-struct its causal graph, and does not take advantage of any context-specific independence among its variables that may arise when the agent acts according to cer-tain policies. Specifically, for this domain, the causal graph constructed from DBN analysis has only two strongly connected components (SCCs): one partition has { b 0 , . . . , b n  X  2 } , and the other has { b n  X  1 SCC cannot be further decomposed using only infor-mation from the DBNs. Second, VISA creates exit op-tions for all strongly connected components that tran-sitively influence the reward function, whereas only a few of these may actually be necessary to solve the problem. Specifically, for this problem, VISA creates an exit condition for any instantiation that satisfies tial number of subtasks shown in Figure 2(a). The successful trajectory provided to HI-MAT achieves the goal by setting the bits going from left to right, and re-sults in the hierarchy in Figure 2(b). The performance results are shown in Figure 3. VISA X  X  hierarchy con-verges even slower than the basic Q learner because the root has O (2 n ) children as opposed to O ( n ). This domain has been engineered to highlight the case when access to a successful trajectory allows for sig-nificantly more compact hierarchies than without. We expect that access to a solved instance will usually im-prove the compactness of the resulting hierarchy. 5.2. Transfer of the Task Hierarchy To test our remaining hypotheses, we apply the trans-fer setting to two domains: Taxi and the real-time strategy game Wargus . The Taxi domain has been de-scribed in Section 3. The source and target problems in Taxi differ only in the wall configurations; the pas-senger sources and destinations are the same. This is engineered to allow value-function transfer to occur. For Wargus , we consider the resource collection prob-lem. Here, the agent has units called peasants that can harvest gold and wood from goldmines and forests respectively, and deposit them at a townhall . The goal is to reach a predetermined quota of gold and wood . Since the HI-MAT approach does not currently gener-alize to termination conditions involving numeric pred-icates, the state representation of the domain replaces the actual quota variables with Boolean variables that are set when the requisite quotas of gold and wood are met. We consider target problems whose specifica-tions are scaled up from that of the source problems, including the number of peasants , goldmines , forests , and the size of the map. In this domain, coordina-tion does not affect the policy significantly. Thus, in the target maps, we learn a hierarchical policy for the peasants using a shared hierarchy structure without coordination (Mehta &amp; Tadepalli, 2005). In each case, we report the total reward received as a function of the number of episodes, averaged over multiple trials. We compare three basic approaches: (1) non-hierarchical Q-learning ( Q ), (2) MAXQ-learning ap-plied to a hierarchy manually engineered for each do-main ( Manual ), and (3) MAXQ-learning applied to the HI-MAT hierarchy induced for each domain ( HI-MAT ). The HI-MAT algorithm first solves the source prob-lem using flat Q-learning, and generates a successful trajectory from it. In Taxi , we also show the perfor-mance of initializing the value-function tables with val-ues learned from the source problem  X  these curves are suffixed with the phrase  X  with value  X . In Wargus , we include the performance of VISA. The results of these experiments are shown in Figures 4 and 5.
 Although the target problems in Taxi allow value-function transfer to occur, the target problems are still different enough that the agent has to  X  X nlearn X  the old policy. This leads to negative transfer evidenced in the fact that transferring value functions leads to worse rates of convergence to the optimal policy than transferring just the hierarchy structure with uninitial-ized policies. This indicates that transferring struc-tural knowledge via the task-subtask decomposition can be superior to transferring value functions espe-cially when the target problem differs significantly in terms of its optimal policy. In Wargus , the difference between the source and target problems renders direct value-function transfer impossible even though the hi-erarchy structure still transfers.
 In Taxi , we observe that MAXQ-learning on HI-MAT X  X  hierarchy converges to the optimal policy at a rate comparable to that of the manually-engineered hierar-chy. However, in Wargus , HI-MAT X  X  hierarchy is faster to converge than the manually-engineered one because, by analyzing the solved source problem, it is able to find stricter termination conditions for each subtask. Consequently, reducing the policy space in the target problem leads to a greater speed-up in learning than reducing the number of value parameters via subtask sharing as in the manually-engineered hierarchy. The improved rate of convergence is in spite of the fact that HI-MAT does not currently merge subtly different in-stantiations of the same subtask so there is room for further improvement. VISA X  X  performance suffers ini-tially due to a large branching factor at the root option (which directly includes all the navigation actions). We have presented an approach to automatically in-ducing MAXQ hierarchies from solved RL problems. Given DBN models and an observed successful tra-jectory, our method analyzes the causal and temporal relationships between actions, and partitions the tra-jectory recursively into subtasks as long as the state abstraction improves. We show that the learned hier-archies are consistent, safe, and have compact value-function tables. Our empirical results indicate that using the observed trajectory can allow us to learn more compact hierarchies. Further, in a transfer set-ting, our hierarchies perform comparably to manually-engineered hierarchies, and improve the rate of conver-gence where direct policy transfer does not help. We are currently working on extending the approach to handle disjunctive goals. Further, in order to en-sure hierarchical optimality, we may need to deal with non-zero pseudo-rewards. In related work, we are also investigating methods that learn compact DBN mod-els of the MDP. Finally, an important challenge for the future is to investigate the transfer scenario where the induced hierarchy may need to be altered structurally in order to apply effectively to the target problem. We thank Mike Wynkoop and the anonymous review-ers for their input. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency under DARPA grant FA8750-05-2-0249.
 Andre, D., &amp; Russell, S. (2002). State Abstraction for Programmable Reinforcement Learning Agents. AAAI (pp. 119 X 125).
 S  X im  X sek,  X  O., &amp; Barto, A. (2004). Using Relative Nov-elty to Identify Useful Temporal Abstractions in Re-inforcement Learning. ICML (pp. 751 X 758).
 Dietterich, T. (2000). Hierarchical Reinforcement
Learning with the MAXQ Value Function Decom-position. Journal of Artificial Intelligence Research , 13 , 227 X 303.
 Diuk, C., Littman, M., &amp; Strehl, A. (2006). A Hierar-chical Approach to Efficient Reinforcement Learning in Deterministic Domains. AAMAS (pp. 313 X 319). Hengst, B. (2002). Discovering Hierarchy in Reinforce-ment Learning with HEXQ. ICML (pp. 243 X 250). Jonsson, A., &amp; Barto, A. (2006). Causal Graph Based
Decomposition of Factored MDPs. Journal of Ma-chine Learning Research , 7 , 2259 X 2301.
 Marthi, B., Kaelbling, L., &amp; Lozano-Perez, T. (2007). Learning Hierarchical Structure In Policies. NIPS Hierarchical Organization of Behavior Workshop . McGovern, A., &amp; Barto, A. (2001). Automatic Dis-covery of Subgoals in Reinforcement Learning using Diverse Density. ICML (pp. 361 X 368).
 Mehta, N., &amp; Tadepalli, P. (2005). Multi-Agent Shared
Hierarchy Reinforcement Learning. ICML Rich Rep-resentations in Reinforcement Learning Workshop . Menache, I., Mannor, S., &amp; Shimkin, N. (2001). Q-Cut -Dynamic Discovery of Sub-Goals in Reinforcement Learning. ECML (pp. 295 X 306).
 Pickett, M., &amp; Barto, A. (2002). PolicyBlocks: An Al-gorithm for Creating Useful Macro-Actions in Rein-forcement Learning. ICML (pp. 506 X 513).
 Sutton, R., Precup, D., &amp; Singh, S. (1999). Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. Artificial Intelligence , 112 , 181 X 211.
 Tadepalli, P., &amp; Dietterich, T. (1997). Hierarchical
Explanation-Based Reinforcement Learning. ICML (pp. 358 X 366).
 Thrun, S., &amp; Schwartz, A. (1995). Finding Structure
