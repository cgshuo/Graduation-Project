 The abundance of Linked Data brings many challenges to the integration, query-ing and maintenance of RDF datasets [1]. One of the challenges is to know about the ontology to which the RDF data conforms, especially when the data schema is absent. Although there are many ontologies built manually, there are also many RDF datasets published without any prescribed schema to adhere. The generation of ontologies from formal and semi-formal data is still a problem de-spite it has been studied for several years within the Semantic Web community.
The more RDF datasets that are available without existing schema, the more important that the ontology mining problem becomes. For ontology mining, it means all those activities that allow to discover hidden knowledge from ontolog-ical knowledge bases [2]. There are two purposes to the discovered knowledge. For the dataset that does not come with any existing schema, the discovered knowledge can be considered as the customized ontology for the dataset. For the dataset that does conform to existing schema, the discovered knowledge can be used to enrich the existing schema and to detect the degree to which the dataset conforms to the existing schema.

In this paper, We propose a parallel approach to generate ontologies from large RDF datasets based on statistical data analysis. Firstly, we propose an non-parallel approach which implements SPARQL queries to obtain ontologies and describe them by EL profile of OWL 2 Web Ontology Language. Secondly, to deal with large RDF datasets such as DBpedia, we study the problem of par-allel ontology mining. We divide and allocate the large data to parallel hardware, generate the OWL 2 EL ontology based on statistical measures on data. Finally, we evaluate our approach on two kinds of DBpedia datasets (Mapping-based Dataset with ontology and Raw Infobox Dataset without ontology). The eval-uations tested on DBpedia with ontology show that the mined knowledge is a supplement to the existing ontology. Meanwhile, the overlap between the mined knowledge and the existing ontology shows the degree to which the dataset conforms to the existing ontology. The evaluations tested on DBpedia without ontology show that the mined knowledge can be considered as a customized on-tology for the dataset. Furthermore, the evaluations on scalability confirm that our parallel approach can deal with large Linked Data datasets.

The rest of the paper is organized as fo llows. Section 2 introduces the related work. Section 3 introduces the SPARQL-based statistical measure approach for ontology mining. Section 4 presents the steps to parallel ontology mining from large RDF datasets. Section 5 details the experimental results of our approach. Section 6 concludes the study. There are some research work in the field of ontology generation from formal and semi-formal data by using Machine Learn ing techniques. Several methods have been proposed for constructing ontology classes by means of Machine Learning techniques from positive and negative examples. Research work [3] and [4] are tailored for small and medium size knowledge bases, while they cannot be directly applied to large knowledge bases due to their dependency on reasoning methods. [5] presents an approach for leveraging Machine Learning algorithms for learning of ontology class descriptions in large knowledge bases.

Other research work focus on learning ontologies from text documents by the use of association rules. Research wor k [6] uses association rules to discover causal relations in RDF-based medical data. [7] considers containment relation-ships between sets of class instantiations for producing alignments among several Linked Data repositories, including DBpedia. While their approach could as well be used to suggest refinements for a single ontology, they currently only acquire mappings which express subsumption or equivalence between so-called restric-tion classes.

Statistical Schema Induction (SSI) [8] is the most similar work to our ap-proach. SSI can generate ontologies from RDF datasets based on association rule mining. SSI acquires the terminology by posing SPARQL queries to the repositorys endpoint. The result of this step is a set of relational database ta-bles containing the URIs of all those RDF resources corresponding to classes and properties. Then, the transaction tables are constructed to mine the dataset for the various kinds of OWL axioms. Although the experiments on DBpedia datasets show that the time needed to finally compute the association rules is less than 5 seconds for the largest transac tion table. The process to construct the association tables is very t ime-consuming and space-consuming. [9] is a following work to mine RDF data for various types of property axioms.

Compared with SSI and [9], our approach is distinct in three aspects. Firstly, we propose a non-parallel approach which avoids the constructing of transaction tables. Since the process to construct tr ansaction tables is time-consuming and space-consuming, our approach is more efficient than SSI. Secondly, we propose an approach which supports the concurrent ontology mining on parallel hard-ware. This parallel approach improves the efficiency further. Thirdly, we test our non-parallel and parallel approach on both the RDF data with ontology and the RDF data without ontology. The evaluations show that our approach is effective and efficient. 3.1 EL Profile of OWL 2 OWL 2 EL, which is based on the description logic EL + +, is particularly useful in applications employing ontologies that contain very large numbers of properties and/or classes. This profile captures the expressive power used by many such ontologies and is a subset of OWL 2 for which the basic reasoning problems can be performed in time that is polynomial with respect to the size of the ontology.

In EL + +, concepts are inductively defined from a set N C of concept (or class) names, a set N R of role (or property) names, and a set N I of individual names. We use C and D to refer to concepts, r to refer to a role name, a and b to refer to individual names. The semantics of EL + + concepts is defined in terms of an interpretation I =(  X  I ,  X  I ). The domain  X  I is a non-empty set of individuals and the interpretation function  X  I maps each concept name A  X  N C toasubset A I of  X  I , each role name r  X  N R to a binary relation r I on  X  I , and each individual name a  X  N I to an individual a I  X   X  I . The syntax and semantics of EL + + are listed in Table 1.

With the axioms listed in Table 1, EL + + generalizes several means of ex-pressivity such as role equivalences can be expressed as r " s , s " r ; transitive roles can be expressed as r  X  r " r ; the bottom concept in combination with general concept inclusions (GCIs) can be used to express disjointness of complex concept descriptions. 3.2 Ontology Mining In this subsection, we show the approach to obtain the OWL 2 EL axioms based on statistical measures on data. Firstly, we use a set of indicators, their related interpretations in this paper are described in Table 2 1 .

Notice that, there can be more than one classes in indicator Res ( { C i } ). For example, Res ( { C i ,C j } ) denotes the set of resources typed both as class C i and C . Also, there can be more than one properties in indicator ResPair ( { r m } ). Table 3 shows how we get the axioms with the statistical measures on data. For example, | SubjectRes ( C, r ) | / | SubjectRes ( r ) | = 1 means that every instance that has property r is also typed as class C , then we can induce that the domain of property r is a subclass of class C . According to the descriptions of indicators, the numerical values in Table 3 can be obtained by SPARQL querying. For instance, a SPARQL query like  X  SELECT count ( distinct ? s ) WHERE { ? s get the cardinality of set SubjectRes ( C i ,r,C j ). After querying the classes and properties in a dataset, many SPARQL queries are issued to get the numerical values, then the OWL 2 EL axioms listed in Table 3 are generated based on statistical measures. We find the approach proposed in Section 3 is time-consuming when applied to large scale dataset. It is natural to consider the idea of concurrent ontology mining on parallel hardware. In this section, we introduce how to divide the dataset into blocks and perform parallel ontology mining.

When observing the axiom types in Table 3, we find that the indicator de-scriptions to compute the first six axioms include at most one property. It is reasonable to consider dividing the data by property. If we divide the triples with same property into one block and add instance type triples to every block, the statistical data computed in blocks will be same to the results computed in the whole data. However, the indicator descriptions to compute the last two axiom types involve more than one properties. It may lead to wrong statisti-cal data if we divide the dataset simply by property. Suppose that r m " r n , if we divide triples with property r m and r n into different blocks, the compu-tation of | ResPair ( { r m ,r n } ) | in one block may be wrong, this axiom can not be mined consequently. So, we must make sure that those properties might have subproperty relationship are divided into one block. Similarly, considering axiom r or might compose property chains into one block. In OWL 2 EL, the property inclusion axiom with property chains is limited to only object property. Hence, we deal with data property and object property separately.

For data property, those properties might have subproperty relationship must be ensured in one block. We go through the RDF data and find out subprop-erty candidates. The subproperty candidate denotes a property pair which has common instance pair. Such as property pair r i and r j where sr i o and sr j o . Considering the property as node, the subproperty candidate as an undirected edge, the definition of G dp is as follows.
 Definition 1. G dp =( V,E, X  ) is an undirected graph. Each node in V rep-resents a data property, ( v i ,v j )  X  E iff property pair ( v i ,v j ) is subproperty candidate.  X  is a function from V to positive integer,  X  ( v i )= | ResPair ( { v i } ) | denotes the instantiation number of property v i .
 The connected subgraphs of G dp help us to divide data property into blocks. The nodes in a connected subgraph denotes the properties should be divided into one block, the sum of these nodes X  weight denotes the corresponding triple number. While the experiment shows th at there may has one connected subgraph contains most of the triples. For instance, one connected subgraph of DBpedia Raw Infobox Dataset 3.9 contains more than 75% triples. It leads to the problem of unbalance when allocating data to parallel processing unit.

To break the connectivity of graph G dp and get more subgraphs, we try to cut edges in graph G dp . If property node v i and v j in G dp are proved that they can not have subproperty relationship, edge ( v i ,v j ) can be removed. We use SPARQL queries to verify whether two properties have subproperty relationship or not. Since verifying all subproperty candidates is infeasible, we focus on verifying the subproperty candidates which could help the increase of the connected subgraphs number or the decrease of triple number in one block. To increase the number of connected subgraphs, we verify all the cut edges in graph G dp firstly. Then, we verify the adjacent edges of the node (or top-k nodes) with largest degree. To decrease the triple number in one block, we verify the adjacent edges of the node (or top-k nodes) with the largest weight. Finally, the verified edges are removed from graph G dp , the BFS algorithm runs to get all connected subgraphs. Based on these property blocks, we divide the RDF data into blocks and allocate them to parallel processing unit.

For object property, those properties (or property chains) might have sub-property relationship must be ensured in one block. The method to deal with object property is similar to the one for data property. We test our approach on two kinds of datasets (the datasets with ontology and the datasets without ontology). We evaluate the effectivity and efficiency of our approach. Section 5.1 describes the two kinds of datasets in the experi-ments. Section 5.2 gives the evaluations on those datasets. We use Jena toolkit (jena.sourceforge.net) to manage RDF data for SSI and our approach. The ex-periments were developed within the Ec lipse environment and on two 64bit quad Core ThinkStations with 3.10 MHz and 16 GB of RAM (of which 14 GB was assigned to the JVM). 5.1 Datasets We choose the real world RDF data DBpedia from Linked Data to evaluate our approach. DBpedia contains extracted data from Wikipedia, we concentrate on the infobox subset. The DBpedia project has different datasets including Mapping-based Datasets and Raw Infobox Dataset.
The Mapping-based Datasets are extracted based on hand-generated map-pings of Wikipedia infoboxes/templates to a DBpedia ontology. The instance data within the Mapping-based Dataset is much cleaner and better structured. For comparison, we consider three new est versions of DBpedia Mapping-based Datasets, named DBpedia Dataset 3.7, DBpedia Dataset 3.8 and DBpedia Dataset 3.9.

The Raw Infobox Dataset extracts all properties from all infoboxes and tem-plates within all Wikipedia articles. P roperty names in Raw Infobox Dataset are not cleaned or merged. There is no consistent ontology for Raw Infobox Dataset. We consider the Raw Infobox Dataset of DBpedia 3.9 as test datasets without ontology.

Table 4 shows the statistics about the different versions of DBpedia: its name, the number of classes, data properties and object properties, the number of triples it contains. Notice that, the statistics are collected from the DBpedia data not from the DBpedia ontology. For comparison, we list the corresponding numbers obtained from DBpedia ontology in brackets except the Raw Infobox Dataset 3.9. Because Raw Infobox Dataset do not have consistent ontology.
For the Mapping-based Datasets, most o f the classes and properties collected from datasets are declared in the ontology except those with other namespaces such as property foaf : familyName 2 . For the Raw Infobox Dataset, the num-bers of properties are much larger than the numbers in Mapping-based Datasets. The Raw Infobox Dataset is relatively noisy, for example there are many mis-spellings like the property http : //dbpedia.org/property/deathplce which sup-posed to be http : //dbpedia.org/property/deathplace . 5.2 Effectivity Evaluation Results We apply our approach to different versions of DBpedia datasets. There are some interesting mining results, for instance, we generate the axioms that prop-erty dbpedia : lowestMountain is a subproperty of dbpedia : lowestPlace and the class which exits a property dbpedia : editor to class dbpedia : Writer is asubclassof dbpedia : Work from DBpedia dataset 3.7. We get the axiom that property chain dbpedia : wineProduced to dbpedia : country is a sub-property of dbpedia : location from DBpedia dataset 3.8. We also obtain the axiom that the domain of property dbpedia : designCompany is a subclass of schema : CreativeWork from DBpedia dataset 3.9 3 . However, these axioms are not declared in the corresponding DBpedia ontology.

Table 5 lists the axioms numbers obtained from different versions of DBpedia datasets. We find that the three versions of Mapping-based Datasets have similar mining results. While there are much more axioms about  X  r.C j " C i , dom ( r ) " C , ran ( r ) " C and r m " r n for Raw Infobox Dataset 3.9. The reason is that the property numbers in Raw Infobox Dataset are larger than the numbers in Mapping-based Dataset. We also find that the numbers of axiom C i " X  r.C j are both zero for all the datasets. It means that we do not find that all instances of class C i have the property r and the value instance is typed as C j . To show the effectivity of our approach, we compare our mining results from Mapping-based Datasets with the axioms declared in DBpedia ontology in Fig-ure 1. The numbers from 1 to 8 in x-coordinate denotes the axiom type from C distribution. For every axiom type, suppose the axiom number generated by our approach is n mined , the axiom number declared in DBpedia ontology is n onto ,the common axiom number both generated by our approach and declared in DBpe-dia ontology is n com . The yellow bar denotes the percentage n onto n the blue bar denotes the percentage n com n
Figure 1 shows that the results are similar when applying our approach to three versions of DBpedia datasets. For the axioms C i  X  C j " C k ,  X  r.C j " C i and r m  X  r n " r l , the percentage of the axioms mined by our approach is 100% because there are no such statements in DBpedia ontology.

For the axiom C i " C j , the axioms mined by our approach not only cover most of the axioms declared in the ontology but also contain many subclass axioms which are not declared in the ontology.

For the axiom dom ( r ) " C , the percentage of the common axioms and the percentage of the axioms mined by our approach are relatively small. There are many domain statements declared in DBpedia ontology are not generated by our approach. One of the reasons is that there are many properties with domain statement in ontology are not used in the dataset. For instance, there are 1 , 406 data properties declared in DBpedia ontology 3.9 while only 688 data properties are used in the dataset. The other reason is that one property can be applied to different classes in the Mapping-based Dataset. For instance, the property dbpedia : volume canbeappliedtoboththeclass dbpedia : lake and the class dbpedia : planet . This affects the mining results of the domain axiom type.
For the axiom ran ( r ) " C , most of the axioms mined by our approach are declared in ontology, there are still many range axioms declared in ontology but not mined by our approach. One of the reason is also because there are many properties with range statement in ontology are not used in the dataset. The other reason is that some instances as the value of a property do not have a class type.

In summary, we apply our approach to both datasets with ontology and dataset without ontology to test the effectivity. The evaluations on the datasets with ontology show that our approach can be applied to obtain axioms effectively. The comparison between the mined axioms and the axioms from ontology shows that our approach can generate not only many axioms declared in the ontology but also many axioms which are not declared in the ontology. These axioms can be considered as a supplement to the ontology. Moreover, the overlap between mined axioms and ontology axioms can be used to show the degree to which the dataset conforms to the ontology. The more overlap means the better that the dataset conforms to the ontology. The evaluations on dataset without ontology show that our approach can be applied to obtain axioms, these axioms can be considered as a customized ontology for the dataset. 5.3 Efficiency Evaluation Results To show the efficiency of our approach, we conduct a performance evaluation. We compare SSI with our non-parallel approach and our parallel approach. We test these approaches on four different datasets, the time taken by different approaches are measured.

The premise for SSI is to construct the transaction tables which are used to mine for various kinds of OWL axioms. So, the time taken by SSI is the sum of the transaction tables constructing time and the association rules mining time. The running time of our non-parallel approach is the time to apply our approach to the whole dataset. The running time of our parallel approach is the time to apply our approach to the separated datasets on 8 parallel computing units. Figure 2(a) shows the comparison results on DBpedia 3.7, 3.8, 3.9 and Raw Infobox 3.9. Notice that, the y-coordinate is logarithmic coordinates. The yellow column denotes the running time of SSI, the blue bar denotes the running time of our non-parallel approach, the cyan bar denotes the running time of our parallel approach.

We find that the running time of SSI is longer than our approach even the non-parallel approach. That is because SSI needs to construct 6 transaction databases in order to obtain the axioms. Although the time needed to obtain axioms is less than few seconds, the premise is that it takes a lot of time to construct the transaction tables. And there also needs a large space to store the transaction databases. Take the DBpedia dataset 3.7 for example, the space needed is about 56GB. Since our approach only query the dataset for statistical data, there is no need to construct the transaction databases. Even the non-parallel approach is faster than SSI. The parallel approach separates the dataset on 8 parallel computing units, the running time is much faster than SSI and the non-parallel approach. It confirms the scalability of our approach to other large Linked Data repositories.

Notice that, the running time for Raw Infobox dataset 3.9 is much longer than other datasets. The reason is that Raw Infobox dataset 3.9 has much more properties than other datasets. We query the dataset to obtain the statistical data by SPARQL queries. The number of these SPARQL queries is decided by the number of the classes and properties . Since there are much more properties in Raw Infobox dataset 3.9 than other datasets, we need much more SPARQL queries to obtain the statistical data. It leads to the longer running time on Raw Infobox dataset 3.9 than other datasets.

To test our parallel approach, the running time on different numbers of com-puting units are depicted in Figure 2(b). The black line with square shows the running time tested on DBpedia Dataset 3.7. The blue line with circle shows the running time tested on DBpedia Dataset 3.8. The cyan line with upper tri-angle shows the running time tested on DBpedia Dataset 3.9. The yellow line with lower triangle shows the running time tested on Raw Infobox Dataset 3.9. Notice that the y-coordinate is logarithmic coordinates. The tests show that the running time of our parallel approach is approximately halved when the number of computing units is doubled for up to 8 compute units.

In summary, the efficiency test shows that both our non-parallel approach and parallel approach are faster than SSI. And, the scalability of the parallel approach confirms that it can deal with the large Linked Data datasets. In this paper, we proposed an approach for parallel mining of OWL 2 EL ontol-ogy from Linked Data. Firstly, a non-parallel approach is introduced to obtain the OWL 2 EL axioms based on statistical data computation. Then, a parallel approach is proposed to divide the dataset into blocks and perform ontology mining on parallel hardware. Experimental results, using the real-life Linked Data DBpedia, support the comparison between SSI, our non-parallel approach and parallel approach. The evaluations tested on two kinds of DBpedia datasets (Mapping-based Dataset with ontology and Raw Infobox Dataset without on-tology) show the effectivity and efficiency of our approach.

The effectivity evaluation results show that our approach is effective regardless applying to the dataset with ontology or the dataset without ontology. There are two advantages when applying our approach to the dataset with ontology. One is that our approach can generate not only many axioms declared in the ontology but also many axioms which are not declared in the ontology. These axioms can be considered as a supplement to the ontology. The other is that the overlap between mined axioms and ontology axioms can be used to show the degree to which the dataset conforms to the ontology. The more overlap means the better that dataset conforms to the ontology. When applying our approach to the dataset without ontology, the most important advantage is to obtain axioms. These axioms can be considered as a c ustomized ontology for the dataset.
The efficiency evaluation results show that both our non-parallel approach and parallel approach are faster than SSI. Moreover, the tests show that the running time of the parallel approach is approximately halved when the number of computing cores is doubled. The scalability of the parallel approach confirms that it can deal with the large Linked Data datasets.
 Acknowledgments. The work is supported by the National Natural Science Foundation of China under grant No. 61170165 and No. 61003055.

