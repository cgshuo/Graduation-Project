 1. Introduction
Authentication (for access to secure physical or digital locations) very popular in recent times ( Jain and Prabhakar, 2004 ). There are two major reasons behind this growing popularity of biometric recognition (authentication and/or identi fi cation)inbothresearch and commercial use. The biometric recognition systems are much less susceptible to impersonation as the biometric traits of a human being are unique to that person. Furthermore, the problems of remembering or securely storing/t ransmitting the authentication code do not arise in case of biometric recognition.

In particular, there have been intensive research activities in face recognition as a viable means of achieving biometric recogni-tion ( Jafri and Arabnia, 2009 ). This is because the discriminating signi fi cance of facial features is more intuitive to understand and also facial images are more convenient to acquire. A facial image acquisition system can comprise only an ordinary digital camera where as a more sophisticated, and hence more costly, set up is required to acquire, e.g., an iris dataset.

Face recognition, whose roots belong to the highly related image processing, machine learning and pattern recognition ( Zhao and Poggio, 1993 ), depending on the philosophy of the method of feature extraction used. The fi rst approach utilizes one or more physical characteristics or geometrical features present in each face to discriminate between faces of different persons, like distance between the eyes, length of the nose, shape of eye and nose, contour of lips, etc. This approach may also seek to utilize differences in pose ( Blanz and Vetter, 2003 ). The second approach broadly utilizes, in different ways, the inherent statistical properties in the pixel intensities. Thus the second approach generally takes help of a transform which accepts the intensities of the pixels of the image as input and produces the discriminating features as output. Our proposed method in this work broadly falls into the second category.
Several transformation methods have so far been utilized to solve such face recognition problems e.g. feature extraction using Discrete Cosine Transform (DCT) or Discrete Wavelet Transform (DWT) ( Ramadan and Abdel-Kader, 2009 ). Another possible transform based method for these application domains is the utilization of Log Polar
Transform (LPT), a hybrid version of which has been employed in ( Abdel-Kader et al., 2008 ). Moreover, a successful method of employ-ing transformation can be that of multiplying or convolving the original image matrix with a transformation matrix or kernel as used in the Volterrafaces method in ( Kumar et al., 2009 ). Some other popular approaches belonging to the second category that have so far been implemented with varied degree of accuracy include Eigenfaces KRR ( An et al., 2007 ), MLASSO ( Pham and Venkatesh, 2008 ), Spectral 2007 ), KLPPSI ( An et al., 2008 ), TANMM ( Wang and Zhang, 2008 ),
Smooth LDA ( Cai et al., 2007 ), CTA ( Fu and Huang, 2008 )etc.The work in ( Kumar et al., 2009 ) has presented, in a comprehensive manner, the relative successes (in terms of recognition rates) of many of these methods in performing face recognition, for several bench-mark image databases, that have been extensively used for face recognition purposes over the years.

The feature extraction process in a supervised learning based method usually generates a large set of features leading to a considerable computation cost during classi fi cation. Feature selec-tion/reduction generates a reduced set from the original set of features and is thus hoped to decrease the computational cost of classi fi cation, the processing stage that follows the stages of feature extraction and feature selection. The reduced set of features may even have higher discriminatory power as a higher number of features do not necessarily result in a better recognition rate ( Tu et al., 2007 ). Classically Principal Component Analysis (PCA) and Independent Component Analysis (ICA) ( Cataltepe et al., 2007 ) have been popular in performing such feature selection purposes. However, in recent times, it has also been demonstrated that non-gradient based stochastic optimization algorithms can be suitably utilized as alternatives to solve these feature selection problems with encouraging results by minimizing a relevant cost function, like those reported employing particle swarm optimiza-tion (PSO) ( Ramadan and Abdel-Kader, 2009 ) and ant colony optimization ( Kanan et al., 2007 ) based methods. It should be mentioned here that the features in the reduced set may be of two types: (i) they may be generated as a function of the original feature set in which case the reduced feature set is not a subset of the original feature set ( Cataltepe et al., 2007 ) or (ii) they may be selected from the original feature set in which case the reduced set is a subset of the original set ( Kanan et al., 2007; Ramadan and Abdel-Kader, 2009 ).

The present work shows how a metaheuristic optimization algorithm, called gravitational search algorithm (GSA), based suitable feature selection methodology can be developed for face recognition problems. The proposed feature selection methodol-ogy is used to suitably reduce the dimensionality of the features extracted from each face. In this work, features are extracted from each face utilizing three contemporary, popular, transform based face representation methods called local binary patterns (LBP) ( Ojala et al., 1996 ), modi fi ed census transform (MCT) ( Frobaand and Ernst, 2004 ), and local gradient patterns (LGP) ( Jun and Kim, 2012 ). The LBP method was originally proposed mainly for the texture classi fi cation purpose and its strength lies in its perfor-mance invariance to global intensity variations. MCT was later proposed as an enlarged variation of LBP where MCT utilizes nine bits to represent a pixel in an image, as opposed to eight bit based representation of a pixel in LBP. However, both LBP and MCT are sensitive to local intensity variations that become prominent along edge components and LGP was recently proposed to overcome these dif fi culties ( Jun and Kim, 2012 ). While LBP, MCT, and LGP based feature extraction methods are already justi fi ed to provide suf fi cient discriminating power, it is sincerely felt that hybridiza-tion of a suitable feature selection methodology with each of these powerful algorithms may further improve the face recognition or authentication performance, with reduced computational burden.
The gravitational search algorithm (GSA), proposed in ( Rashedi et al., 2009 ), is a comparatively recent but highly popular meta-heuristic optimization algorithm that is based on Newton's Laws of Gravity and Motion. In our work the feature selection problem is con fi gured as a binary decision making problem. Keeping that factor in mind a binary version of GSA is developed in this work, inspired by similar earlier works carried out in binarization of particle swarm optimization (PSO) ( Kennedy and Eberhart, 1997; Khanesar et al., 2007; Jun and Chang, 2009 ). The present work also proposes an improved version of the binary GSA that introduces a novel dynamic adaptation of weight features and this algorithm is named as Binary Adaptive Weight GSA (BAW-GSA) strategy. Then, in our work, both binary version of traditional GSA and also the proposed BAW-GSA are separately utilized to select a reduced set of features from the features extracted separately utilizing LBP, MCT, and LGP. Finally backpropagation neural network (BPNN) employing Levenberg  X  Marquardt learning algorithm is employed as the input. Each such hybrid combination of feature extraction-feature selection methodology is employed for fi ve well known, benchmark face databases, namely, Yale A, Yale extended B, ORL, LFW and AR face databases and these extensively detail perfor-mance comparisons are tabulated and compared with several competing, contemporary methods popularly employed for similar face detection purposes. These performance evaluations conclu-sively justify the implementation of our proposed feature selection scheme i.e. BAW-GSA and prove its superiority in providing over-all, enhanced face recognition performance.

The rest of the paper is organized as follows. In Section 2 we present an overview of the feature extraction schemes utilized in this work. In Section 3 , a binary version of the traditional GSA algorithm has been introduced to make the method suitable for our desired feature selection. Our proposed BAW-GSA algorithm is also introduced and described in detail in this section. A brief description of the classi fi cation strategy employed is given in Section 4 . Section 5 contains the detailed experimental results and comparisons with competing methods. The paper is con-cluded in Section 6 . 2. Feature extraction schemes
As mentioned before, we consider three well known, contem-porary methods of face representation, called LBP, MCT, and LGP, for the purpose of feature extraction. These methods are described in a nutshell now. 2.1. Local binary pattern (LBP)
LBP proposed by Ojala et al. (1996) is an effective feature extraction tool which captures the local intensity variation trend of an image and has good discrimination characteristics.
Let i c be the intensity of an image I at pixel ( x c , y ... ,7) be the intensity of a pixel in the 3 3 neighborhood of ( x excluding the center pixel i c .

Then the LBP value for the pixel ( x c , y c ) is given by Jun and Kim (2012) LBP  X  x c ; y c  X  X   X  7 where s  X  x  X  X 
For easy understanding, the computation of LBP is compactly provided in Fig. 1 with the help of an illustrative example. 2.2. Modi fi ed census transform (MCT)
Modi fi ed census transform (MCT) proposed by Froba and Ernst (2004) is a variation of LBP where the measure incorporates the mean intensity of the neighborhood. Moreover this is an extended version of the Census Transform taking all the nine bits of a 3 3 neighborhood including the center pixel. Thus the neighboring intensities in MCT, unlike LBP, vary over n  X  0, ... ,8. Let the mean intensity be denoted as i c . Hence it can be computed as i  X  1 9
Then the MCT value for the pixel ( x c , y c ) is given by the following equation:
MCT  X  x c ; y c  X  X   X  8 n  X  0 s  X  i n i c  X  2 n  X  4  X  where s  X  x  X  X 
Again, for easy understanding, the computation of MCT is compactly provided in Fig. 2 with the help of the same illustrative example, considering an identical 3 3 neighborhood. 2.3. Local gradient pattern (LGP)
LGP is an improvement over the traditional MCT and LBP, proposed by Jun and Kim (2012) , which emphasizes the local variation in the neighborhood by incorporating the intensity gradient pro fi le of the neighborhood in the measure.
Like in the case of LBP, let i c be the intensity of an image I at 3 3 neighborhood of ( x c , y c ) excluding the center pixel i
Then we de fi ne the gradient of the neighboring pixel n as follows: g  X j i n i c j X  6  X 
Next, the average gradient g of the neighboring n pixels ( n  X  0,1, ... ,7) is computed. This average gradient is given by the following equation: g  X  1 8
Then the LGP value for the pixel ( x c ,y c ) is given by the following equation:
LGP  X  x c ; y c  X  X   X  where s  X  x  X  X 
Fig. 3 shows the result of LGP for the same illustrative example considered in Figs. 1 and 2 . 3. Feature selection schemes employing modi fi ed GSA
The next stage involves selecting a subset of features from the original set. The present work proposes to solve this problem using a metaheuristic optimization algorithm which minimizes a suitable cost function. The cost function used here is the ratio between the intra class Euclidean distance of features and the inter class Euclidean distance of features. As mentioned before, the gravitational search algorithm has been used in this work to solve this optimization problem. However, since GSA was originally introduced in the continuous domain, it has been suitably mod-i fi ed for our binary decision making problem of selecting or not selecting each extracted feature. For this purpose a binary version of the traditional GSA is proposed in this work, following similar existing works devoted to the development of binary PSO ( Ramadan and Abdel-Kader, 2009; Kennedy and Eberhart, 1997;
Khanesar et al., 2007; Jun and Chang, 2009 ). In Section 3.3 the novel variant of GSA proposed in this work, named as BAW-GSA, is described in detail. The proposed variation introduces a dynamic adaptive inertia weight in the traditional GSA algorithm to achieve a more intelligent iterative update strategy. 3.1. Gravitational search algorithm (GSA) Let us consider an isolated universe of p particles obeying
Newton's Laws of motion and gravity. Let the position of the i th by
X  X  t  X  X f x 1 i  X  t  X  ; x 2 i  X  t  X  ; ............ ; x n i  X  t  X g X  10  X 
The gravitational force experienced by a particle i due to the particle j at iteration t in the dimension d is given by Newton's law of gravity as follows ( Rashedi et al., 2009 ):
F  X  t  X  X  G  X  t  X  M i  X  t  X  M j  X  t  X  R i ,  X  is a small positive constant (  X  4 0), R distance between two agents i and j . Here it is assumed that, for each particle i , active gravitational mass ( M ai )  X  passive gravita-tional mass ( M pi )  X  inertial mass ( M ii )  X  M i .

R  X  t  X  X  J X i  X  t  X  ; X j  X  t  X  J  X  12  X 
G  X  t  X  X  G  X  t 0  X  exp  X  t = T  X  13  X  t is the initial iteration, T is the total number of iterations and the exponential decay constant. Hence G ( t ) gradually decreases over time, as it is desired that the changes should be lesser and lesser as the system converges toward a solution ( Rashedi et al., 2009; Mansouri et al., 1999 ). The total gravitational force on a particle i in the d th dimension is ( Rashedi et al., 2009 )
F  X  t  X  X   X  p where rand j is an uniformly distributed random number in the interval [0,1] and is responsible for the stochastic nature of the algorithm. Next, by Newton's law of motion, the acceleration of the agent i at time t in dimension d is calculated as follows: a  X  t  X  X  F
The velocity, position and mass update equations are ( Rashedi et al., 2009 ): v  X  t  X  1  X  X  rand i v d i  X  t  X  X  a d i  X  t  X  X  16  X  x  X  t  X  1  X  X  x d i  X  t  X  X  v d i  X  t  X  1  X  X  17  X  m  X  t  X  1  X  X  fit i  X  t  X  worst  X  t  X  best  X  t  X  worst  X  t  X 
M  X  t  X  1  X  X  m i  X  t  X  1  X   X  p
So the best ( t ) and the worst ( t ), the best and worst among the p particles at iteration t respectively, are given as follow: best  X  t  X  X  min worst  X  t  X  X  max
After the stopping criterion is met the position of the agent with the largest mass gives the solution of the search. The larger the mass of an agent, more slowly will it change its position and this is what is desired as it is nearer to the solution. Also it can be noted here that unlike the classical PSO, the basic GSA is a memory less algorithm since unlike PSO there is no need to keep record of the best position of each particle and also the globally best position. 3.2. Binary GSA (BGSA)
Let us assume that for each image, n features were extracted in the fi rst stage and let the n dimensions of the position vector of each particle represent the selection variables corresponding to those features. Hence to take the decision of selecting or not selecting a feature based on the value of the corresponding selection variable, a proper binarization of the GSA algorithm is necessary.

So after calculating the velocity vectors v i ( t  X  1) using Eq. (16) , the position vectors are binarized as follows: x  X  t  X  1  X  X 
Here rand is an uniformly distributed random number in the interval [0,1]. At the end of each iteration, the position vector for each particle i is carefully scanned for each of its dimension. Then, features, if the corresponding dimension of its position vector possesses a value of 1 or rejected, if the corresponding value is 0. Hence the number of features selected for each particle i corre-sponds to the number of entries of 1 in the corresponding position vector.

Obviously at the start of the algorithm the position vectors of the particles need to be randomly initialized with values in {0, 1}.
Suppose at iteration t , for the particle i , out of the original n features, m features have been selected.

Fitness or goodness function of the particle i at iteration t is de fi ned as the ratio of the intra class distance or within class distance ( Dintra i t ) to the interclass distance or between class distance ( Dinter i t ). fit where Dintra i t is the sum of the Euclidean distances between the selected feature sets (for the i th particle at iteration t ) of the images belonging to same class.
 Dintra t i  X   X  F t . Similarly Dinter i t is the sum of the Euclidean distances between images belonging to different classes.
 Dinter t i  X   X  3.3. BAW-GSA
Let us propose the binary adaptive weight GSA in this subsec-tion, in detail now.

From Eq. (16) , the velocity update equation for particle i in dimension d is given as follows: v  X  t  X  1  X  X  rand i v d i  X  t  X  X  a d i  X  t  X  X  26  X 
Now we introduce an adaptive dynamic weight w i t in the velocity update equation which facilitates the convergence of the algorithm.

Thus the velocity update equation is modi fi ed as follows: v  X  t  X  1  X  X  w t i v d i  X  t  X  X  a d i  X  t  X  X  27  X  where w  X  min  X  fit t i
Here fit t is the average fi tness of all particles at iteration t such that fit t  X  1
The rationale behind introducing the adaptive dynamic weight w in this form is as follows. We know from previous equations that the acceleration is minimum for the best particle (with maximum mass) and maximum for the worst particle (with minimum mass). Thus the position (which is the output variable) of the best particle, being nearest to the solution, undergoes the least amount of change in each iteration. Hence there is a speci theoretical insight which guides the update equations which may sometimes be compromised by the random term ( rand i )in Eq. (26) .

Furthermore instead of providing the worst particle with the maximum change in velocity (and hence position), a more intuitive option would be to design the weight of the velocity v ( t ) in such a way as to boost the change in velocity in particles of comparatively higher fi tness than the worst so as to accelerate convergence. This is achieved by designing the adaptive weight w as in Eq. (26) such that it is higher for the particles with nearer to the average value and lower for the particles with far away from the average value.

This modi fi cation in conjunction with the discrimination by the acceleration term is introduced in a bid to achieve several important objectives. Firstly, the minimum change applicable for the best particle is preserved in the acceleration term by removing the effect of the random term in the velocity update equation.
Secondly, instead of only preferring maximum change in velocity of the worst particle, the change in velocity of particles with better fi tness is deliberately boosted for better convergence (through introduction of the adaptive weight term). The Algorithm 1 describes the implementation of the proposed BAW-GSA. Fig. 4 presents the same algorithm in fl ow chart form.
 Algorithm 1. BAW-GSA BEGIN
Create p particles and make randomized initialization of their n dimensional positions X in {0, 1}.
 Initialize iteration number t  X  1 REPEAT : FOR i  X  1to p where Dintra t i  X   X  and Dinter t i  X   X  END FOR
Calculate global best fi tness: best  X  t  X  X  min
Calculate global worst fi tness: worst  X  t  X  X  max FOR i  X  1to p Calculate gravitational constant: G  X  t  X  X  G  X  t 0  X  exp  X  t = T ; t 0  X  1 FOR j  X  1to p , j a i Calculate distance between two particles:
R  X  t  X  X  J X i  X  t  X  ; X j  X  t  X  J Calculate force between two particles: FOR d  X  1to n
F ij  X  t  X  X  G  X  t  X  END FOR END FOR
Calculate average fi tness of all particles: fit t  X  1 p  X  p FOR d  X  1to n
Calculate total force on i th particle: F d i  X  t  X  X   X  p j  X  1 ; j a i
Calculate velocity: v d i  X  t  X  1  X  X  w t i v d i  X  t  X  X  a d
Calculate position: x d i  X  t  X  1  X  X  END FOR END FOR UNTIL termination criterion is satis fi ed
END 4. Neural network based classi fi cation
Once the fi nal feature set is selected, the training and testing is performed using neural network based classi fi ers. Back propaga-tion neural networks (BPNN), employing multi-layered feedfor-ward architecture, still remain one the most popular variants of supervised neural networks which utilize the traditional method of gradient descent and other fi rst order and second order optimization algorithms to suitably determine the weights and biases. Some of these very popular learning algorithms include
Levenberg  X  Marquardt (LM) learning based BPNN, resilient back-propagation, conjugate gradient-Fletcher Powell based BPNN algo-rithm etc. In this work, we have utilized a three-layered architec-ture with 20 hidden layer neurons and the LM-learning based
BPNN is employed for training exemplars from the training set. 5. Experiments and results
The binary version of the traditional GSA for feature selection proposed in Section 3.2 and the binary adaptive weight based variation (BAW-GSA) proposed in Section 3.3 have been separately evaluated for each separate set of extracted features employing
LBP, LGP and MCT based feature extraction algorithms for fi popular standard, benchmark databases, namely, Yale A, Yale B
Extended ( Yale Univ., 2002; Yale Univ., 2001; Georghiades et al., 2001 ), ORL ( ORL database, 2002; Samaria and Harter, 1994 ), LFW ( Huang et al., 2007; Wolf et al., 2011 ) and AR ( Martinez and
Benavente, 1998 ) face databases. The results obtained are com-pared with several competing, state-of-the-art algorithms in existence for solving identical face recognition problems.
The Yale A database published by Yale University has 11 images each from 15 individuals with lighting, occlusion and facial emotion variations. Fig. 5 shows 10sample images for one person in the Yale A database. The Yale B Extended database published by
Yale University has 64 images each from 32 individuals with similar variations of emotion, lighting etc. Fig. 6 shows 10 sample images for one person in the Yale B Extended database. The ORL database published by Cambridge University has 10 images each from 40 individuals. Here there are also variations of occlusion and facial expressions along with changes in scale and tilting. Fig. 7 shows the 10 different images for one person in the ORL database.
The LFW (Labeled Faces in the Wild) database ( Huang et al., 2007 ), ( Wolf et al., 2011 ) contains images of 5749 different subjects in unconstrained condition, i.e., the images were taken in natural conditions without posing in an image acquisition session and hence the name of the database. Due to this special characteristic of the database, the images are mainly of notable personalities whose images are more frequently available publicly and also consequently these images have a high variation of pose, orientation, illumination, expression, accessories and background, thus making this a challenging database. Fig. 8 shows ten different images for one person in the LFW database. The LFW-a is an aligned version of the LFW database and it has been used in the present work. For comparison with the results available in Zhu et al. (2012) , the experimental conditions have been kept the same, which are as follows. A set of 158 subjects of no less than 10 sample images each are taken from LFW-a. The images are cropped to 121 121 and then resized to 32 32. The training set size is varied from 2 to 5 and the training images are randomly chosen for each person. Another 2 images are randomly chosen to constitute the testing set.

The AR database developed in the Ohio State University contains over 4000 images of 126 subjects. Fig. 9 shows 7 different images of one person in the AR database. For comparison with the results available in Martinez and Benavente (1998) , the experi-mental conditions have been kept the same, which are as follows.
A subset of 50 male and 50 female subjects were chosen with 7 images each. The images are resized to 32 32. The training set size is varied from 2 to 5 and the training images are randomly chosen for each person from session 1. Another 3 images are randomly chosen from session 2 to constitute the testing set.
Extensive performance evaluation has been carried out for images from all fi ve face databases and the experimental results obtained for Yale A, Yale B Extended, ORL, LFW and AR databases are presented in Tables 1  X  5 correspondingly. For each database, the size of the training dataset is varied in conformation with the similar variations utilized to report other competing algorithms in existing literatures, in a bid to provide performance comparisons in similar experimental platforms. Several competing algorithms, whose performances, in terms of percentage errors in face recog-nition, are comprehensively reported in ( Kumar et al., 2009 ), are considered for comparing the performances of our proposed algorithms. In these tables, some entries for competing algorithms are kept blank because those performances were not reported in their original works. For each training dataset size considered, the winning algorithm, i.e. the algorithm that showed the lowest percentage error rate, is shown in bold.

A closer inspection of these tabular results will reveal that, like other competing algorithms, each of the six algorithms proposed in this work, namely LBP  X  BGSA, MCT  X  BGSA, LGP  X  BGSA,
LBP  X  BAW-GSA, MCT  X  BAW-GSA, LGP  X  BAW-GSA, produced enhance recognition rates with increase in the size of the training dataset, which is understandable. Overall, the performances of LBP based algorithms were bettered by corresponding MCT based algorithms and the performances of MCT based algorithms were bettered by corresponding LGP based algorithms. Also one can easily conclude that the LGP  X  BAW-GSA algorithm has emerged as the best performing algorithm which produced best perfor-mance in seven situations out of eight possible situations for
Yale A database, in six situations out of eight possible situations for Yale B extended database, in eight situations out of eight possible situations for ORL database, in four out of four possible situations in each of LFW and AR databases. In fact, in Table 1 , the
LGP  X  BAW-GSA scheme could produce at least 100% improvement in recognition performance compared to the competing, pre-viously published algorithms considered and this improvement in performance undergoes steep rise with increase in the training dataset size. A similar trend in improvement in system perfor-mance can also be observed in Table 2 . On the other hand, for the
ORL database, the most improved performance, in comparison with the competing, previously published algorithms, is observed for the smallest training dataset size. For LFW and AR databases, considerable improvements are achieved in some training set sizes when BAW-GSA is combined with LGP. This conclusively proves that when our proposed BAW-GSA feature selection algorithm is hybridized with the LGP feature extraction scheme, then the face recognition performance is signi fi cantly enhanced, compared to several similar, contemporary, existing, competing face recogni-tion algorithms.

When the feature selection step is not employed, i.e., only feature extraction using LBP, MCT, or LGP algorithm is employed, then the total number of features selected, in each case, is equal to the size of the input image. Hence, for our systems, considering each image of 32 32 size, the number of features extracted employing each of LBP, MCT, or LGP algorithm only, will be 1024 in each case. However, when a feature selection phase is added, i.e.
BGSA or BAW-GSA is hybridized with each of LBP, MCT, or LGP algorithm, then the average number of features fi nally selected is found to be around 50% of the initial set of features i.e. the average number of features selected is approximately 510. The average number of features selected by each feature selection algorithm is computed as the mean of the results obtained over 10 runs, as each of BGSA or BAW-GSA essentially uses a stochastic optimi-zation algorithm.

In this context, it can also be stated that, in the implementation phase, for an unknown face recognition problem, each of the proposed BGSA and BAW-GSA based algorithms consumes approximately 2 min of average computation time to arrive at a decision. The average time of each algorithm was computed over 10 runs, in MATALAB s environment, in a Windows 7 machine, with 4 GB RAM. Another important factor for employing such a metaheurisitc algorithm for a non-convex problem is the choice of the free parameters for the optimization algorithm. In our case, at fi rst we arrived at a satisfactory set of these free parameters, which is common for both BGSA and BAW-GSA based algorithms, manually, by trial-and-error evaluations. Table 6 gives this set of parameters chosen. Once this initial set of parameters is deter-mined, this set is kept fi xed for all algorithm runs for all case studies under consideration. Hence it can be recommended that other users can utilize this set of parameters for implementing our proposed algorithms for similar face recognition problems and it is expected that they will produce satisfactory results for such problems too.

For further extensive comparisons with respect to different baselines, additional experiments have been carried out on the same fi ve standard databases, replacing parts of the proposed algorithm with different popular, state-of-the-art alternatives. Firstly, using the same extracted feature sets and the same cost function as described by Eq. (23) , we implemented the same face recognition algorithm where we replaced our proposed BAW-GSA technique by the binary versions of two other very popular, modern meta-heuristic optimization techniques, called binary genetic algorithm (BGA) and binary particle swarm optimization (BPSO), to perform the desired feature selection step. Then we perform relative performance comparisons among BGA, BPSO and BAW-GSA techniques for those fi ve datasets, as reported in
Tables 7  X  11 . Secondly, using the same extracted feature sets, we solve the same face recognition problem using two very popular feature selection algorithms, called Fisher linear discriminant ana-lysis (FLDA) and principal component analysis (PCA). The corre-sponding performances achieved, for those fi ve datasets, are also presented in Tables 7  X  11 . These extensive performance compari-sons show that, out of a total of 32 comparative case studies considered, in 24 situations, the (LGP  X  BAW-GSA) algorithm pro-duced the best performance and, in remaining 8 situations, the (LGP  X  BAW-GSA) algorithm produced the second best performance.
Hence, these extensive experimentations clearly demonstrate the overall superiority and effectiveness of our proposed (LGP  X  BAW-
GSA) algorithm, compared to many other popular algorithms. 6. Conclusion
The present work demonstrated how an effective feature selection scheme can be employed for face recognition problems, employing metaheuristic optimization algorithms. We have chosen a recently proposed algorithm in this genre, called GSA, to demonstrate its effectiveness in this regard. At fi rst, the formulation of the of the within class distance to the between class distance. As the traditional GSA cannot be used for solving our speci fi cproblem,a binary version of the traditional GSA is fi rst developed. Then a novel variationofthisBGSAisdevelop ed with dynamic adaptive inertia weight, named BAW-GSA, to further enhance its search capability.
These feature selection schemes are hybridized with local binary pattern (LBP), modi fi ed census transform (MCT), and local gradient pattern (LGP) based well known feature extraction algorithms for face recognition and they were extensively tested for three benchmark face databases. Extensive experimen tations carried ou testablishedour notion that the proposed feature selection methodologies could yield improved face recognition performance compared to several other competing face recognition algorithms, and, among all algorithms considered, the hybrid LGP  X  BAW-GSA methodology produced the most superior performance.

In this context, we would also like to mention that the fi functiongiveninEq. (23) , which computes the ratio of the intra class the intra class distances among samples belonging to the same class should be as small as possible and the interclass distances among samples belonging to different classes should be as large as possible.
Hence, lower this ratio computed in (23) , better is the clustering value that can be achieved for the ratio described by Eq. (23) .
A superior face recognition perform ance, in turn, should be achieved for a low value of this ratio. However, it is very dif fi
Eq. (23) as a direct function of the features under consideration and hence it is dif fi cult to employ a conventional optimization technique to solve this problem. This inspired us to utilize a metaheuristic optimization procedure to solve this optimization problem for feature selection and we chose GSA, one of the modern candidate metaheur-istic optimization techniques, for our further development of feature selection algorithms for the face recognition purpose.
Our present research efforts were focused in solving face recognition problems and hence we chose several popular, bench-mark datasets speci fi cally utilized in solving face recognition problems. However, in near future, we also intend to test the utility of our proposed algorithm for problems other than face recognition problems e.g. for solving other image processing based biometric authentication problems using palm prints, fi ngerprints, hand veins etc. In this context we would also like to mention that the problem under consideration in this work is essentially a face recognition problem. In near future we intend to expand our research focus to face veri fi cation problems too and would like to examine how the proposed algorithms can be adapted for face veri fi cation problems and with what degree of satisfaction. Acknowledgments
This work was supported by University Grants Commission (UGC) India under University with Potential for Excellence (UPE) Phase II Scheme awarded to Jadavpur University, Kolkata, India. References
