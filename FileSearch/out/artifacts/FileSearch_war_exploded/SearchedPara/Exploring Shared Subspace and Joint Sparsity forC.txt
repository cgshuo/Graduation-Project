 Canonical correlation analysis (CCA) has been extensively employed in various real-world applications of multi-label annotation. However, two major challenges are raised by the classical CCA. First, CCA frequently fails to remove noisy and irrelevant features. Second, CCA cannot effec-tively capture correlations between multiple labels, which are especially beneficial for multi-label learning. In this paper, we propose a novel framework that integrates joint sparsity and low-rank shared subspace into the least-squares formulation of CCA. Under this framework, multiple label interactions can be uncovered by the shared structure of the input features and a few highly discriminative features can be decided via structured sparsity inducing norm. Owing to the inclusion of the non-smooth row sparsity, a new efficient iterative algorithm is derived with proved convergence. The empirical studies on several popular web image and movie data collections consistently deliver the effectiveness of our new formulation in comparison with competing algorithms. H.3.3 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation, Performance Canonical correlation, Multi-label, Subspace, Sparsity
Multi-label dimensionality reduction (MLDR), manipu-lating data associated with multiple labels, has gained in-creasing interest in many potential multimedia applications [3]. One of the promising MLDR methods is the canonical cor-relation analysis (CCA) [2, 8] that is widely leveraged to maximally measure the similarity between a pair of data sets, i.e., the high dimensional input feature space and the dimensionality-reduced label space. However, there are sev-eral inherent limitations of the standard CCA. From the per-spective of feature learning [2, 7, 6, 5], there are typically a small number of informative features in the high dimen-sional original features, but CCA cannot yield the attractive sparse representation. On the other hand, CCA does not ef-fectively take into account the intrinsic interactions [1, 4, 5, 10] among multiple pre-given labels that are quite helpful for multi-label annotations.

Inspired by the equivalence relationship [8, 2] between the least-squares and the generalized eigenvalue problem, we propose a new CCA model by simultaneously incorporat-ing the shared common structure and row sparsity-inducing 2 ,p -norm into a unified objective, dubbed shared S ubspace and structural S parsity CCA (SSCCA 2 ,p ). Specifically, we employ a row-wise structured sparsity regularizer, shrinking some rows of projection functions to zeros, to identify the es-sential discriminative features and eliminate the redundant and noisy dimensions for predictive functions; meanwhile, we exploit the shared common structure to encourage the interactions among different labels so as to compensate for the CCA X  X  lack of the label correlation capture in the em-bedded space. Further, owing to the involvement of the non-smooth row-sparsity term in this unified formulation, we derive an iterative alternating learning paradigm with guaranteed convergence. With the learned predictive clas-sifiers, empirical evaluations are conducted on four public Internet data sets with hundreds of thousands of samples to showcase the competitive advantages of our model for effi-cient multi-label annotation.
We formulate the problem of the least-squares CCA un-der a new framework by introducing the structural sparsity-inducing norm and the common subspace shared by dif-ferent labels. We first briefly review the shared structures that greatly assist the multi-label prediction, then present our new model solved by the efficient alternating iterative optimization in this section. Additionally, we use X = { x 1 ,  X  X  X  ,x n } X  R d  X  n denoting the n data points of dimen-sion d and L  X  X  0 , 1 } n  X  c designating the label space such that L j i =1if x i is grouped into j -th label, and 0 otherwise, where c is the number of labels. Without any loss of gener-ality, we consider both the input data space X and the label space L are normalized to have zero mean.
Following the supervised learning framework, we aim to discriminative subspace Q from the input training data X by minimizing the below regu larized empirical risk: where y j i is a well-defined response variable, F (  X  )isapre-scribed loss function over the labeled data, the regularizer G (  X  )measuresthecomplexityof f j ization parameter  X  controls the fitness of predictive func-tions. In order to capture the common subspace [1, 4] shared among different labels, we can define the predictive classifier in which w j  X  R d is the weight vector for the predictor, p j  X  R d denotes the weight vector minimizing the fitting residue w j  X  Qr j , Q  X  R d  X  r is a shared subspace matrix for the c predictors and r j  X  R r is the weight vector for the low-dimensional embedded shared structure Q T x that can better elucidate the label dependency [5, 1, 4]. Note that the shared dimensionality r is much smaller than the input space dimensionality d , i.e., r d .Importantly, the column orthogonality constraint is subtly imposed on the shared projection operator Q , Q T Q = I ,whichcanbe thought of as looking for the principle component of these c predictive functions. To be specific, there are several ma-jor advantages of the column orthogonality constraint: first, to preserve as much discriminative information as possible; second, to avoid the arbitrary rotation and to reduce the burden of computation (see Section 2.2 in detail).
Although the shared structure can properly exploit such label dependency, it fully lacks the characteristic of sparsity for the learned classifiers from the viewpoint of sparse fea-ture learning. The sparseness has shown the prominent per-formance in multimedia understanding [6, 7, 5], especially for the input data with noisy and redundant features. As a result, we investigate the joint structred sparsity-inducing norm  X  2 ,p , designating the 2 ,p -norm of a matrix, i.e., in which p  X  (0 , 2) and W i  X  represents the i th row of the matrix W . Unlike the p regularizer resulting in the entry sparsity, 2 ,p -norm (3) is designed to first compute the norm distance for each decision classifier and then perform -norm over dimensions of c classifiers. Hence the 2 ,p -norm shrinks the rows of predictive functions to zero, leading to the row sparsity, and also 2 ,p -norm can be interpreted as flexibly select those highly discriminative features associated with those nonzero rows in the predictors W =[ w 1 ,  X  X  X  ,w We emphasize that the smaller the value of p , the sparser the predictive classifiers, and so the different degree of sparsity between features can be tuned by the value of p . Typically, p =1or0 . 5 has demonstrated the efficacy in multimedia feature learning [6]. Further, it is worth noting that 2 ,p (1 &gt;p&gt; 0) matrix norm is nonconvex and so it should be regarded as pseudo-norm of a matrix, whereas 2 , 1 -norm is convex. When the value of p increases to 2, the 2 ,p -norm apparently becomes the Frobenius norm of a matrix which does not impose any sparsity in predictors.

We now direct our attention to the design of the empirical loss F (  X  )in (1) on the labeled data. We propose to take ad-vantage of CCA as the loss function that substantially differs from the conventional linear regression for the loss function because CCA can maximally leverage the correlation be-tween the instance space and the label space. Nonetheless, the aforementioned two promising constraints: the shared subspace and row sparsity, cannot be straightforwardly in-troduced into the standard CCA formulation where W X is the projection matrix of CCA with the dimen-sion of d  X  c and I c is a c  X  c identity matrix. Owing to the inherent connection [8] between the generalized eigenvalue problem and the least-squares, the optimization (4) can be derived as a least-squares formulation where W is the regression coefficients with size of d  X  c and the response Y can be computed as follows. Let  X  L = let the SVD of  X  R be  X  R = U  X  V T , and thus the target Y is given by Y =  X  QU .Notethat  X  L is well-defined because it is reasonable to assume that the number of data samples is significantly larger than the number of labels ( n c ). Proceeding as above, we arrive at our proposed objective Q by integrating the shared structure (2) and the structured sparsity-inducing norm (3) into the least-squares CCA (5): min size of r ,andboth  X ,  X  &gt; 0 are the penalty parameters.
As the objective (6) compromises the column orthogonal-ity constraint and the non-smooth regularization term 2 ,p norm, it is generally not easy to be solved and so we derive an efficient iterative algorithm to optimize (6). To begin with, we denote a diagonal matrix
D =diag { p Then the objective (6) can be equivalently rewritten as: where Tr designates the trace operator of a matrix. Zeroing the derivative of (8) with respect to R and ignoring those terms independent of R , we gain Q T ( W  X  QR )=0and then From (9) it follows that the objective in (8) can be rewritten as follows by regrouping the terms dependent on W min Setting the derivative of (10) equal to zero gives the solution for W By removing the terms independent of the W ,wenotethat (10) can be alternatively seen as min By plugging the solution to W (11) back into (12), we arrive at the optimization of Q : On the basis of Woodbury formula, by rearranging the terms dependent on Q we get in which A XX T +  X I +  X D . By substituting (14) back into (13) and dropping the terms independent of Q ,weob-tain the below generarlized eigenvalue problem to solve Q : Hence the solution to Q can be derived analytically via the eigen-decomposition of ( I  X   X A  X  1 )  X  1 ( A  X  1 XY Y T X However, we emphasise that A is also an unknown variable due to the dependence upon in D (7). Consequently, we re-sort to an iterative procedure to tackle our presented model (6) outlined in Algo.1. The computational cost of our model is primarily dominated by the eigen-decompostion for Q and several inverse operations for A and W , leading to an over-all time complexity O ( d 3 ), but all these computations are offline and we may further employ the first order Taylor ex-pansion to approximate ( I  X   X A  X  1 )  X  1 because of the small  X  . Besides, the convergence proof is omitted due to the page limits. However, following the proof in [7], we can prove that the objective value of (1) monotonically decreases w.r.t. each iteration until convergence using Algo. 1.
Three well-noted image data corpora and one movie data collection are used in the experiment to verify the effective-ness of our SSCCA 2 ,p model in comparison with the follow-ing state-of-the-art multi-label learning methods: (1) MLDA [9]: Multi-label linear discriminant analysis is an extension of the classical LDA for dealing with multi-label classifica-tion; (2) MDDM [10]: Multi-label Dimensionality reduction via Dependence Maximization is designed to maximize the dependence between the input space and the label space by virtue of the Hilbert-Schmidt independence criterion; (3) MDSS [4]: Multi-label DR via the Shared Subspace directly combines the common shared structure with the square loss function; (4) CCA [8]: Canonical Correlation Analysis max-imizes the correlations between variables in the embedded space and the corresponding regularized CCA (regCCA) [8] is to avoid singularity and overfitting. As for MLDA, MDDM, CCA and regCCA, after obtaining the low dimen-sional projected space, we apply the ridge regression with the Algorithm 1: S ubspace and structural S parsity CCA
Input :  X  The input data features X and its corresponding label matrix L ;  X  Parameters:  X ,  X  &gt; 0; p = { 0 . 5 , 1 } ;  X  The shared dimensionality: r =2 c/ 3;  X  Convergence tolerance:  X  =1 e  X  6;  X  Random initialization: W (0) . repeat
Output : Projection vectors W  X  R d  X  c . regularization parameter tuned in { 10 i } 3 i =  X  3 rather than the SVM to achieve the fast speed of multi-label annotations in the embedded space. In terms of the performance measures, we adopt the widely used AUC, Macro and Micro F1 [5, 10, 4] whose higher values indicate better performance.
For all the data sets, we can directly access their descrip-tions and features by searching for their titles, so we simply list their important statistics in Table 1. We note that three types of visual descriptors, including 100-D DenseHue, 300-D DenseHueV3H1 and 1000-D DenseSift, are concatenated to represent the images in the MIRFlickr08. Moreover, we remove the samples without any pre-given labels and also discard those labels without any training samples. On top of that, we select the most popular 40 labels among all 374 labels in the Corel5k in order to better illustrate the results of different learning models.

Concerning the NUS-WIDE and IMDB Updated data sets, because of the several hundreds of thousands of data points, we pick 1 out of 10 random partitions as the training data whereas the remaining partition as the testing data. For the Corel5K, 1 out of 3 random partitions is taken as the train-ing set while the rest as the test set. For the MIRFlickr08, we use the original partitioned training data with one half size and the other half as the test data.

We highlight that in the objective (6), there are two pa-rameters  X  and  X  , both of which are tuned in the range { 10 i } { 5  X  10 i } 3 i =  X  3 as well as the parameter p = in the sparsity inducing 2 ,p -norm. In light of the other com-pared algorithms, we also tune their corresponding parame-ters and then the best results are reported.
We offer the following observations in Table 1: (1) Com-pared with CCA based algorithms like CCA and regCCA, our proposed SSCCA 2 ,p significantly outperforms these base-lines over all benchmark data sets, which provide strong sup-port for our interpretation of integrating the shared common subspace and joint sparsity into the standard CCA. (2) In contrast with MLDA, MDDM and MDSS, our SSCCA si-multaneously considers the mechanism of the shared com-mon structure and the advantage of CCA that finds the max-imal similarity between the transformed space and the label space. (3) Although the smaller value p in the 2 ,p -norm implies row sparser predictor, the quality of the sparsity-induced norm SSCCA 2 , 1 / 2 does not often exceed those of SSCCA 2 , 1 , which means that the best results of SSCCA 2 ,p varies according to the unique domain. Figure 1: Perf. variation w.r.t.  X  and fixed  X  =10  X  3 .
In what follows, we meticulously carry out experiments on the NUS-WIDE and MIRFlickr08 to exemplify how the parameters affect the performance. For simplicity, we fix  X  =10  X  3 because in contrast with  X  , the parameter  X  is more sensitivity to the quality of SSCCA 2 ,p , largely depen-dent on the row sparsity for the predictive functions. As seen in Figure 1, it is clear that the DR accuracy decreases when the value of  X  goes up. However, we may generally achieve good quality due to relatively wide ranges of  X  that are smaller than 10  X  1 . Figure 2: Convergence curves of obj.values of (6) via Algo. 1.
Even though the convergence behavior of Algo. 1 can be mathematically proved, in practice how fast our proposed al-gorithm converges is also crucial. As a result, Fig. 2 depicts the convergence curves over NUS-WIDE under the two fixed parameters that generate the best results listed in Table 1. Practically, our proposed alternating scheme can speedily converges within about dozens of iterations shown in Fig. 2.
We have studied the problem of new least-squares CCA oriented feature learning by taking into consideration of the shared common subspace and structured sparsity patterns. Under this scheme, not only can we elucidate the multiple label dependence unveiled by the shared common subspace, but also maximally characterize the similarity between the input feature space and the label space via CCA. Owing to the row sparsity-inducing 2 ,p -norm regularizer, a new alter-nating algorithm is derived to handle the formulated objec-tive. Experimental studies on a collection of four web data sets have shown the effectiveness and efficiency of our new model. In the future, we will perform the diagonalization technique to further expedite the offline computation of the proposed SSCCA 2 ,p .

