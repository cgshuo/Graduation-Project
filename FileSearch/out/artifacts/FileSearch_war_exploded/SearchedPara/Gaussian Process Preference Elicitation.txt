 Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences. A crucial requirement for PE systems is that they should be able to make optimal or near optimal recommen-dations based only on a small number of queries. In order to achieve this, a PE system should (a) maintain a flexible representation of the user X  X  utility function; (b) handle uncertainty in a principled manner; (c) select queries that allow the system to discriminate amongst the highest utility items; and (d) allow for the incorporation of prior knowledge from different sources.
 While previous Bayesian PE approaches have addressed (a), (b) and (c), they appear to ignore an important aspect of (d) concerning generalization from previous users to a new unseen user in order to reduce the elicitation burden on new users. In this paper we propose a Bayesian PE approach to address (a) X (d), including generalization to new users, in an elegant and principled way. Our approach places a (correlated) Gaussian process (GP) prior over the latent utility functions on the joint space of user features ( T , mnemonic for tasks) and item features ( X ). User preferences over items are then seen as drawn from the comparison of these utility function values.
 The main advantages of our GP-based Bayesian PE approach are as follows. First, due to the non-parametric Bayesian nature of GPs, we have a flexible model of the user X  X  utility function that can handle uncertainty and incorporate evidence straightforwardly. Second, by having a GP over the joint T  X X space, we can integrate prior knowledge on user similarity or item similarity, or simply have more general-purpose covariances whose parameterization can be learned from observed pref-erences of previous users (i.e. achieving integration of multi-user information). Finally, our approach draws from concepts in the Gaussian process optimization and decision-making literature [1, 2] to propose a Bayesian decision-theoretic PE approach. Here the required expected value of informa-tion computations can be derived in closed-form to facilitate the selection of informative queries and determine the highest utility item from the available item set as quickly as possible. In this paper we focus on pairwise comparison queries for PE, which are known to have low cognitive load [3, 4]. In particular, we assume a likelihood model of pairwise preferences that factorizes over users and preferences and a GP prior over the latent utility functions correlates users and items. Let x denote a specific item (or product) that is described by a set of features x and t denote a user (mnemonic for task) that can be characterized with features t . For a set of items X = { x 1 ,...,x N } and users T = { t 1 ,...,t M } we are given a set of training preference pairs: the number of preference relations observed for user j .
 The preference elicitation problem is that given a new user, described by a set of features t  X  , we aim to determine (or elicit) what his/her preferences (or favourite items) are by asking a small number would like to obtain the best user preferences with the smallest number of possible queries. The key idea of this paper is that of learning a Gaussian process (GP) model over users X  latent utility functions and use this model in order to drive the elicitation process of a new user. Due to the non-parametric Bayesian nature of the GPs, this allows us to have a powerful model of the user X  X  utility function and to incorporate the evidence (i.e. the responses the user gives to our queries) in a principled manner. Our approach directly exploits: (a) user-relatedness, i.e. that users with similar characteristics may have similar preferences; (b) items X  similarities and (c) the value of information of obtaining a response to a query in order to elicit the preferences of the user. Our likelihood model considers that the users X  preference relationships are conditionally indepen-dent given the latent utility functions. In other words, the probability of a user t preferring item x over item x 0 given their utility functions is: variance of the normally distributed variable  X  that dictates how different the latent functions should be for the corresponding relation to hold. Hence: where  X (  X  ) is the Normal cumulative distribution function (cdf). The conditional data-likelihood is then given by: As mentioned above, we model user (and item) dependencies via the user latent utility functions, which are assumed to be drawn from a GP prior that accounts for user similarity and item similarity directly: on item features x . We will denote the parameters of these covariance functions (so-called hyper-parameters) by  X  t and  X  x . (These types of priors have been considered previously in the regression setting, see e.g. [5].) Additionally, let f be the utility function values for all training users at all training input locations F be the N  X  M matrix for which the j th column corresponds to the latent values for the j th user at all input points such that f = vec F . Hence: where K t is the covariance between all the training users, K x is the covariance between all the training input locations, and  X  denotes the Kronecker product. Note that dependencies between users are not arbitrarily imposed but rather they will be learned from the available data by optimizing the marginal likelihood. (We will describe the details of hyper-parameter learning in section 7.) Given the data in (1) and the prior over the latent utility functions in equation (6), we can obtain the posterior distribution: where we have emphasized the dependency on the hyper-parameters  X  that include  X  t ,  X  x and  X  2 The non-Gaussian nature of the conditional likelihood term (given in equation (5)) makes the above integral analytically intractable and hence we will require approximations. In this paper we will fo-and the evidence, using the Laplace approximation.
 omitting the terms that are independent of f , we focus on the maximization of the following expres-sion: Using Newton X  X  method we obtain the following iterative update: Once we have found the maximum posterior  X  f by using the above iteration we can show that: 5.1 Predictive Distribution In order to set-up our elicitation framework we will also need the predictive distribution for a fixed test user t  X  at an unseen pair x 1  X  , x 2  X  . This is given by: with: where  X  is defined as in equation (7) and: Now we have the main components to set up our preference elicitation framework for a test user characterized by features t  X  . Our main objective is to use the previously seen data (and the corre-sponding learned hyper-parameters) in order to drive the elicitation process and to incorporate the information obtained from the user X  X  responses back into our model in a principled manner. Our main requirement is a function that dictates the value of making a query q ij . In other words, we aim at trading-off the expected actual utility of the items involved in the query and the information these items will provide regarding the user X  X  preferences. This is the exploration-exploitation dilemma, usually seen in optimization and reinforcement learning problems. We can address this issue by computing the expected value of information (EVOI, [2]) of making a query involving items i and j . Before defining the EVOI, we will make use of the concept of expected improvement, a measure that is commonly used in optimization methods based on response surfaces (see e.g. [1]). 6.1 Expected Improvement We have seen in equation (13) that the predictive distribution for the utility function on a test user t on item x follows a Gaussian distribution: (14). Let us assume that, at any point during the elicitation process we have an estimate of the utility of the best item and let us denote it by f best . If we define the predicted improvement at x as I = f ( t  X  , x |D ,  X  )  X  f best then the expected improvement (EI) of recommending item x (for a fixed user t  X  ) instead of recommending the best item x best is given by: and  X  (  X  ) is the Normal probability density function (pdf). Note that, for simplicity in the notation, we have omitted the dependency of EI ( x |D ) on the user X  X  features t  X  . Hence the maximum expected improvement (ME) under the current observed data D is: 6.2 Expected Value of Information Now we can define the expected value of information (EVOI) as the expected gain in improvement that is obtained by adding a query involving a particular pairwise relation. Thus, the expected value of information of obtaining the response for the queries involving items x  X  i , x  X  j with corresponding utility values f  X  = ( f  X  ( t  X  , x i  X  ) ,f  X  ( t  X  , x j  X  )) T is given by: Algorithm 1 Gaussian Process Preference Elicitation Require: hyper-parameters  X  x ,  X  t ,  X   X  { learned from M previous users } and corresponding D repeat until Satisfied where As mentioned above, f best can be thought of as an estimate of the utility of the best item as its true training users and the test user, where D + denotes the data extended by the set of seen relationships is the matrix containing the mean estimates of the latent utility function distribution given by the Laplace approximation in equation (9). Alternatively, we can draw samples from such a distribution and apply the max operator.
 In order to elicit preferences on a new user we simply select a query so that it maximizes the expected value of information EVOI as defined in equation (23). A summary of our approach is presented in algorithm 1. We note that although, in principle, one could also update the hyper-parameters based on the data provided by the new user, we avoid this in order to keep computations manageable at query time. The reasoning being that, implicitly, we have learned the utility functions over all users and we represent the utility of the test user (explicitly) on demand, updating our beliefs to incorporate the information provided by the user X  X  responses. Throughout this paper we have assumed that we have learned a Gaussian process model for the utility functions over users and items based upon previously seen preference relations. We refer to the hyper-parameters of our model as the hyper-parameters  X  t and  X  x of the covariance functions (  X  t and  X  x respectively) and  X   X  = log  X  , where  X  2 is the  X  X oise X  variance.
 Although it is entirely possible to use prior knowledge on what these covariance functions are (or their corresponding parameter settings) for the specific problem under consideration, in many prac-tical applications such prior knowledge is not available and one requires to tune such parameteriza-tion based upon the available data. Fortunately, as in the standard GP regression framework, we can achieve this in a principled way through maximization of the marginal likelihood (or evidence). As in the case of the posterior distribution, the marginal likelihood is analytically intractable and approximations are needed. The Laplace approximation to the marginal log-likelihood is given by:  X  X upport X  the seen relations and hence we should write e.g.  X  f o ,  X  o where the subindex {} o indicates this fact. However, for simplicity, we have omitted this notation.
 Given the equation above, gradient-based optimization can be used for learning the hyper-parameters in our model. As we shall see in the following section, for our experiments we do not have much parameter learning by maximization of the marginal log-likelihood. In this section we describe the dataset used in our experiments, the evaluation setting and the results obtained with our model and other baseline methods. 8.1 The Sushi Dataset We evaluate our approach on the Sushi dataset [6]. Here we present a brief description of this dataset and the pre-processing we have carried out in order to apply our method. The reader is referred to [6] for more details. The Sushi dataset contains full rankings given by 5000 Japanese users over N = 10 different types of sushi. Each sushi is associated with a set of features which include style, major group, minor group, heaviness, consumption frequency, normalized price and sell frequency. The first three features are categorical and therefore we have created the corresponding dummy variables to be used by our method. The resulting features are then represented by a 15-dimensional vector ( x ). Each user is also represented by a set of features wich include gender, age and other features that compile geographical/regional information. As with the item features, we have created dummy variables for those categorical features, which resulted into a 85-dimensional feature vector ( t ) for each user. As pointed out in the documentation of the dataset, Japanese food preferences are strongly correlated with geographical and regional information. Therefore, modeling user similarities may provide useful information during the elicitation process. 8.2 Evaluation Methodology and Experimental Details We evaluate our method via 10-fold cross-validation, where we have sub-sampled the training folds in order to (a) keep the computational burden as low as possible and (b) show that we can learn sensible parameterizations based upon relatively low requirements in terms of the preferences seen on previous users. In particular, we have subsampled 50 training users and selected about 5 training pairwise preferences drawn from each of the N = 10 available items.
 For the GPs we have used the squared exponential (SE) covariance functions with automatic rele-vance determination (ARD) for both  X  t and  X  x and have carried out hyperparameter learning via gradient-based optimization of the marginal likelihood in equation (28). We have initialized the hyper-parameters of the models deterministically, setting the signal variance and the length-scales of the covariance function to the initial values of 1 and the  X  2 parameter to 0 . 01 . ommendation based on the available information. The normalized loss function is defined as: ( u achieved by the recommendation provided by the system. (errors of the mean) error bars. (a) The performance of our model compared to the RVOI method model when the hyper-parameters have been optimized via maximization of the marginal likelihood (GPPE-OPT) compared to the same GP elicitation framework when these hyper-parameters have been set to their default values (GPPE-PRIOR).
 We compare our approach to two baseline methods. One is the restricted value of information al-gorithm [7] and the other one is the best and largest heuristic, which we wil refer to as the RVOI method and the B&amp;L heuristic respectively. The RVOI approach is also a VOI-based method but it does not leverage information from other users and it considers diagonal Gaussians as prior models largest uncertainty. Both baselines have been shown to be competitive methods for preference elic-itation (see [7] for more details). Additionally, we compare our method when the hyper-parameters have been learned on the set of previously seen users with the same GP elicitation approach when the hyper-parameters have been set to the initial values described above. This allows us to show that, indeed, when prior information on user and item similarity is not available, our model does learn sensible settings of the hyper-parameters, which lead to better quality elicitation outcomes. 8.3 Results Figure 1(a) shows the normalized average loss across all 5000 users as a function of the number of queries. As can be seen, on average, all competing methods reduce the expected loss as the number of queries increases. More importantly, our method (GPPE) clearly outperforms the other algorithms even for a small number of queries. This demonstrates that our approach exploits the inter-relations between users and items effectively in order to enhance the elicitation process on a new user. Although it may be surprising that the B&amp;L heuristic outperforms the RVOI method, we point out that the evaluation of these methods presented in [7] did not consider real datasets as we do in our experiments.
 Figure 1(b) shows the normalized average loss across all 5000 users for our method when the hyper-parameters have been set to the initial values described in section 8 (labeled in the figure as GPPE-PRIOR) and when the hyper-parameters have been optimized by maximization of the marginal like-indeed, the GPPE model that learns the hyper-parameters from previous users X  data significantly outperforms the same method when these (hyper-)parameters are not optimized. Preference elicitation (PE) is an important component of recommender systems and market research. Traditional PE frameworks focus on modeling and eliciting a single user X  X  preferences. We can categorize different PE frameworks in terms of query types. In [8], the authors propose to model utilities as random variables, and refines utility uncertainty by using standard gamble queries. The same query type is also used in [9], which differs from [8] in treating PE as a Partially Observable Markov Decision Process (POMDP). However, standard gamble queries are difficult for users to respond to, and naturally lead to noisy responses. Simpler query types have also been used for PE. For example, [7] uses pairwise comparison queries, which are believed to have low cognitive load. Our work also adopts simple pairwise comparison queries, but it differs from [7] in that it makes use of users X  preferences that have been seen before and does not assume additive independent utilities. In the machine learning community preference learning has received substantial interest over the past few years. For example, one the most recent approaches to preference learning is presented in [10], where a multi-task learning approach to the problem of modeling human preferences is adopted by extending the model in [11] to deal with preference data. Their model follows a hierarchical approach based on finite Gaussian processes (GPs), where inter-user similarities are exploited by that they consider the dual representation of the GPs as they do not generalize over user features. Furthermore, they do not address the elicitation problem, which is the main concern of this paper. Extensions of the Gaussian process formalism to model ordinal data and user preferences are given specifications of our model. In other words, unlike the work of [10], their model (as ours) considers multi-task case or generalize across users. More importantly, an elicitation framework for actively querying the user is not presented in such works. [14] proposes an active preference learning method for discrete choice data. Their approach is based on the model in [13]. Unlike our approach they do not leverage information from seen preferences on previous users and hence their active preference learning process on a new user starts from scratch. This leads to the problem of either relying on good prior information on the covariance function or on hyper-parameter updating during the active learning process, which is computationally too discrete choices, their approach completely relies upon the expected improvement (EI) measure. In this paper we have presented a Gaussian process approach to the problem of preference elicitation. One of the crucial characteristics of our method is that it exploits user-similarity via a (correlated) Gaussian process prior over the users X  latent utility functions. These similarities are  X  X earned X  from preferences on previous users. Our method maintains a flexible representation of the user X  X  latent utility function, handles uncertainty in a principled manner and allows the incorporation of prior knowledge from different sources. The required expected value of information computations can be derived in closed-form to facilitate the selection of informative queries and determine the highest utility item from the available item set as quickly as possible.
 We have shown the benefits of our method on a real dataset of 5000 users with preferences over involving a Likert scale [15] where our approach may be effective. The main practical constraint is that in order to carry out the evaluation (but not the application) of our method on real data we require the full set of preferences of the users over a set of items.
 Our main motivation for the Laplace method is its computational efficiency. However, [10] has ence learning problem. We intend to investigate other approximation methods to the posterior and marginal likelihood and their joint application with sparse approximation methods within our frame-work (see e.g. [16]), which will be required if the number of training users is large. Acknowledgments NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. [1] Donald R. Jones. A taxonomy of global optimization methods based on response surfaces. [2] R.A. Howard. Information value theory. IEEE Transactions on Systems Science and Cyber-[3] Urszula Chajewska, Daphne Koller, and Ronald Parr. Making rational decisions using adaptive [4] Vincent Conitzer. Eliciting single-peaked preferences using comparison queries. Journal of [5] Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian [6] Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order re-[7] Shengbo Guo and Scott Sanner. Real-time multiattribute Bayesian preference elicitation with [8] Urszula Chajewska and Daphne Koller. Utilities as random variables: Density estimation [9] Craig Boutilier. A POMDP formulation of preference elicitation problems. In Proceedings [10] Adriana Birlutiu, Perry Groot, and Tom Heskes. Multi-task preference learning with an appli-[11] Kai Yu, Volker Tresp, and Anton Schwaighofer. Learning Gaussian processes from multiple [12] Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. Journal of [13] Wei Chu and Zoubin Ghahramani. Preference learning with Gaussian processes. In Proceed-[14] Brochu Eric, Nando De Freitas, and Abhijeet Ghosh. Active preference learning with discrete [15] Rensis Likert. A technique for the measurement of attitudes. Archives of Psychology , [16] Joaquin Qui  X  nonero Candela and Carl Edward Rasmussen. A unifying view of sparse approx-
