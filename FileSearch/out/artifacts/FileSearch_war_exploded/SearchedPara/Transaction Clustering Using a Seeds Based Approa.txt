 Clustering is the process of finding naturally occurring groups in data. Clustering is one of the most widely studied techniques in the context of data mining and has many ap-plications, including disease classification, image processing, pattern recognition, and document retrieval. Traditional clustering techniques deal with horizontal segmenta-tion of data, whereby clusters are formed from sets of non-overlapping instances. Many efficient algorithms exist for the traditional clustering problem [1,2,3,4]. In contrast, transaction clustering has fundamentally different requirements, and has been gaining increasing attention in recent years. Unlike trad itional clustering, transaction clustering requires that transactions be partitioned across clusters in such a manner that instances within a cluster share a common set of large items, where the concept of large follows the same meaning attributed to frequent items in association rule mining [5]. Thus it is clear that transaction clustering requires a fundamentally different approach from the traditional clustering techniques. Compounding the level of difficulty is the fact transac-tion data is known to have high dimensionality, sparsity, and a potentially large number of outliers [6].

Current research in both data mining and information retrieval suggests that transac-tion clustering functionality needs to ext end well beyond a near-neighbourhood search for similar instances [7,8]. In this paper we propose a new approach to the problem of transaction clustering based on an initial seeding of cluster centroids. Our approach consists of two phases: a seed generation phase followed by a transaction allocation phase.
 In the recent past there has been an increasing lev el of interest in transaction clustering. All such approaches have employed quite different methods when compared to tradi-tional clustering methods. Wang et al. (1999) utilised the concept of large items [5] to cluster transactions. Their approach measures the similarity of a cluster based on the large items in the transaction dataset. Each transaction is either allocated to an existing cluster or assigned to a new cluster based on a cost function. The cost function mea-sures the degree of similarity between a transaction and a cluster based on the number of large and small items shared between that tran saction and the given cluster. To speed-up the method proposed above, Yun et al. (2001) introduced a method called SLR (Small-Large Ratio). Their method essentially uses the measurement of the ratio between small to large items to cluster transactions. Both the large item [7] and SLR [9] method suffer a common drawback. In some cases, they may fail to give a good representation of the clusters. Xu et al. (2003) proposed a method using the concept of a caucus. The basic idea of introducing a caucus to cluster transactions is motivated by the fact that cluster quality is sensitive to the initial choice of cluste r centroids [6]. Funda mentally different from most other clustering algorithms, their approach attempts to group customers with similar behaviour. In their approach they first determine a set of background attributes from the dataset that are significant. A set of caucuses, consisting of different subsets of items is then constructed to identify the initial cluster centroids. The main drawback of this method is that it requires the user to define the initial centroids which is difficult as it requires some form of prior knowledge about the dataset. Overall the clustering is divided into two main phases: seed generation and allocation phases. 3.1 Seed Generation Phase We start by describing a method for finding the optimal number of clusters. Our initial choice of seeds are the large items in the dataset and we thus begin by setting a minimum support threshold,  X  ,where 0 &lt; X &lt; 1 . Any item in the dataset that has support above |
D | X   X  is considered a large item. Let L i denote the set of large items or large itemsets. We now allow the items L i to be extended to itemsets L i +1 in the same way as Apriori generates candidate frequent itemsets. For a large itemset to be considered a cluster seed the frequency of co-occurrence of all pairs of subsets within the seed must occur together with a frequency above a threshold value at a given significance level. This effectively ensures that cluster seeds of size  X  2 have items that co-occur together at a frequency that is statistically significant. In addition, we require that all cluster seeds satisfy an improvement constraint when they are extended. We first define the concept of relative support. Definition 1 (Relative Support). The relative support of an itemset X k of size k is defined to be the ratio of the support of X k to the support of Y k  X  1 which is that ( k  X  1) -sized subset of X k with the maximum support. Thus, Definition 2 (Extension of a Seed). Given two existing seeds, X k  X  1 and Y k  X  1 , X k  X  1 is extended to a new seed X k  X  1  X  Y k  X  1 if and only if: where  X  denotes the chi square correlation coefficient,  X  2 c , the chi square cut-off thresh-old at the c% confidence level and  X  is a user-defined threshold.

The rationale behind extension lies in the fact that the new itemset to be added to the seed has a statistically strong correlation with the existing seed and that the inclusion of the new itemset will improve the relative support of the seed above a user defined minimum threshold. The algorithm for the seed clustering phase is shown below. 3.2 Allocation Phase The seeds produced in the initial phase are considered as the initial centroids for the clusters. In this phase, transactions are assigned to clusters on the basis of similarity to cluster centroids. In order to measure similarity we use a modified version of the Jaccard similarity coefficient [10]. For each transaction, t , we calculate the similarity between t and the existing centroid, c k . The similarity, sim , is between t and the c k is calculated as: and c 1 is calculated as 2 / (5  X  2+1)=0 . 5 . The greater the overlap between t and C k , the greater the value of sim coefficient.

Once all transactions are allocated to cluste rs, further refinement is accomplished by recomputing the centroids which may need to be updated with large items belonging to transactions allocated to a given cluste r but not presently part of its centroid. The updating of centroids will result in the need for reorganisation of the clusters, thus the process of centroid update and cluster reorganisation will need to be repeated in tandem until a suitable point of stabilisation is reached. In order to determine the point at which stabilisation is reached, we use a fitness function adapted from particle swarm optimi-sation approach was proposed to find the optimal clusters. For all cluster { C 1 ,...C k } , the fitness function is calculated as: Typically, we want to maximise the fitness va lue generated. The fitness measure cal-culates the average similarity between every transaction in a cluster to its centroid. We show the algorithm for allocation phase as Algorithm 2 above. In order to evaluate the effectiveness of our seed based approach to transaction clus-tering we conducted an experimental comparison with the large item approach [7]. We used seven different real world datasets taken from the UCI Machine Learning Repos-itory [11]. The first stage of analysis involved an overall comparison of cluster quality. Secondly, we selected one dataset, name ly the Congressional Vote and conducted a more in depth analysis of the properties of the clusters produced by the two approaches. In this section we report on cluster quality, as measured by the Root Mean Square Standard Deviation or RMSSTD index [12]. Table 1 below shows that our Cluster Seed-ing approach outperforms Large Items across all datasets tested. In terms of cluster qual-ity the cluster seeding algorithm consistently returned lower RMSSTD values than its Large Item counterpart. With respect to processing time the Cluster Seeding approach returned run times that were consistently lower, with the difference in timing between the two approaches widening with increasin g database size, as evidenced with the soy-bean and mushroom dataset. Thus we can conclude that the cluster seeding approach scales better with respect to dataset size.
 The Congressional Votes dataset consists of the United States Congressional Voting Records in 1984. Each record represents one Congressman X  X  vote on 16 different issues. In order to make the comparison between the algorithms fair, we ran both algorithms with settings that resulted in the same number of clusters. Both algorithms produced 4 clusters, out of which three had the vast majority of instances labelled as Democrat while the other had a clear majority of instances with the Republican label. However, an investigation of the homogeneity within the clusters revealed significant differences in the formation of the clusters. Table 2 shows these differences.

The Cluster Label column indicates the party label belonging to the majority of in-stances in a given cluster, with the number bes ide it denoting the percentage of instances in that cluster that contain the label. The Coverage column tracks the percentage of in-stances falling into a given cluster. For each cluster we record the support received by each attribute; if this suppor t falls below 70% then we consider the attribute to be non-homogeneous. The most significant differences between the two approaches are apparent when we compare the number of non-homogeneous attributes. It is clear from Table 2 that the Cluster Seeding approach produces clusters with a much higher degree of homogeneity with an average value of 3.75 for the number of non-homogeneous attributes, versus 7.0 for the Large Items approach.

In order to further quantify the differences between the two approaches we focused on the three clusters containing Democrats as the two algorithm performed very sim-ilarly for the Republican cluster. Ideally, a clusterer should show sharp differences in voting patters between the three Democrat clusters. We used the set symmetric operator to assess the difference in voting patterns amongst Democrats. We evaluate C i  X  C j =( C i  X  C j )  X  ( C j  X  C i ) for pairs of values ( i, j ) in the range [ 1 ... 3 ]for each of the two clusters. Table 3 summarises the results. The larger the value of the set symetric operator the larger the contrast or difference between the clusters involved. Table 3 shows that the Cluster Seed algorithm produces a better differentiation between the Democrat clusters with an overall set symmetric cardinality of 25 as opposed to 14 for the Large Item approach.
 In this paper we proposed a new approach to the problem of transaction clustering. Our approach differed from previous work in that we used seeds containing frequent items to guide the allocation of transactions to cl usters. Our seeds were generated in such a fashion to actively promote the presence of frequent items across different clusters. Our experimentation on several real world datasets showed that our approach produced clusters with a much higher degree of homogeneity when compared to the current state-of-the-art algorithm.

