 ORIGINAL PAPER J. Y. Ramel  X  S. Leriche  X  M. L. Demonet  X  S. Busson Abstract In this paper, based on the study of the spec-ificity of historical printed books, we first explain the main error sources in classical methods used for page layout analysis. We show that each method (bottom-up and top-down) provides different types of useful infor-mation that should not be ignored, if we want to obtain both a generic method and good segmentation results. Next, we propose to use a hybrid segmentation algo-rithm that builds two maps: a shape map that focuses on connected components and a background map, which provides information about white areas corresponding to block separations in the page. Using this first seg-mentation, a classification of the extracted blocks can be achieved according to scenarios produced by the user. These scenarios are defined very simply during an interactive stage. The user is able to make processing sequences adapted to the different kinds of images he is likely to meet and according to the user needs. The proposed  X  X ser-driven approach X  is capable of doing segmentation and labelling of the required user high level concepts efficiently and has achieved above 93% accurate results over different data sets tested. User feedbacks and experimental results demonstrate the effectiveness and usability of our framework mainly because the extraction rules can be defined without diffi-culty and parameters are not sensitive to page layout variation.
 Keywords Page layout analysis  X  Segmentation  X  Digital libraries  X  Analysis strategy  X  Zone classification 1 Introduction High-level analyses of document images are mainly based on the output of a page segmentation process. For example, the extracted text regions can be the input for an OCR system to retrieve the ASCII characters printed on the pages. The spatial relationships between seg-mented blocks along with other features can be used in logical page organization analysis to group the extracted components appropriately and recover the correct read-ing order. Many techniques for page segmentation have been proposed in the literature [3,21,14,15,22], but most of them are based on the assumption that an input document image consists of a set of rectangular blocks. Furthermore, the classification step is generally domain specific and uses static rules to automatically deter-mine, for each block, the coherent label selected from a predefined list (title, paragraph, graphic, table, etc.). These limitations appear too restrictive with respect to historical documents and new approaches need to be developed.

In this context, we present a work achieved in collabo-ration with the  X  X entre d X  X tude Superieur de la Renais-sance X  of Tours (CESR/http://www.cesr.univ-tours.fr). The CESR is a training and research centre, which receives students and researchers who wish to work on various domains of the Renaissance using a rich library of historical books. The CESR wants to create a human-istic virtual library; however, until now, only bitmap versions of several books that have been scanned or photographed are accessible. The initial objective of the CESR was to obtain an ASCII version of the text contained in the pages of these historical books. The centre first tried to use the commercial OCR software to index their books, but they quickly realized that, applied to historical documents, this procedure had been an avowed failure. So, the CESR asked our Pattern Recog-nition and Image Analysis research team to help them to define a new system adapted to their needs. They have appreciated our efforts, as our collaboration will lead to a system able to bring a better description and index-ation of the content of their books and would also make the search and the reading of these precious historical books easier.

In this paper, we first study the old book specifici-ties in order to infer some invariant characteristics used during the automatic analysis of documents with com-plex structures. Next, we describe the layout extraction methods applicable to such documents by focusing on their quality and their drawbacks. We also specify the adaptations we need to make in the classical methods in order to analyse complex documents. The second part of the article proposes a new hybrid method for the extraction of the layout of the documents based on the construction of two representations of the contents of the images. A mapping of the shapes and a mapping of the background are computed. By exploiting this infor-mation, our algorithm produces and sends back a list of blocks constituting a first segmentation result.
Then, this initial representation of the image is used during a more sophisticated analysis. Having an aim of genericity, the architecture of the system that we carried out authorizes an interactive installation of scenarios for analysis of the image contents. Scenarios work on the initial representation provided by the first step of the segmentation. According to its needs (localization of the ornamental letters, the notes at margins, titles, etc.) and using user-friendly interfaces, the user (not expert in image processing) builds scenarios allowing to label, merge or remove the blocks contained in the intermediate representation. One can thus locate the desired entities without taking care of the other areas of the image. The elaborated scenarios can then be stored, modified and applied to various sets of images during batch processing. 2 A brief study of the characteristics of historical books and of processing methods 2.1 Characteristics of historical books 2.1.1 Production values Since its creation, the CESR has established a collec-tion of precious historical books currently numbering around 3,000 copies, dating from the middle of the 14th century to the beginning of the 17th century. The first books belong to the beginning of the printing era when the fonts used and the layouts of the pages were very close to those of the handwritten books. The collection of the CESR is pan European: coming from France, Germany, Italy, Switzerland and Holland. The languages used, Latin or French, bring an additional factor of vari-ability to the books. The Renaissance typographies used have significant variability. Some examples of images of these precious books are presented in Fig. 1.
The technical materials used impose particular pre-sentations in the layout of the pages. The variability of page layouts is much larger than in the current books due either to inaccuracies or to liberties taken by the printer. Most of the time, the text part represents the majority of the page, with notes in the margins on either side of the text. The page can also contain graphical parts of various sizes and some ornamental patterns . In the text, we can find known structures like the titles and the sub-titles, the paragraphs, the page numbers and other more particular structures like the catchwords . These are the words that are placed at the bottom of the page and rep-resent the page number; rather they act as a connection between two pages and usually correspond to the first word of the next page. The catchwords help the print-ers to organize the manuscript correctly. The styles used can alternate, normal style, justified or aligned on the left. Another characteristic of old printed books comes from weak separations between blocks of text (notes at the margins and body text for example). Lastly, on some pages, current page layout rules are not complied with: for example an illustration can overflow into the margins (Fig. 1). In the Renaissance books, the illustrations were generally printed using metal or wood plates, engraved and inked to reproduce the image. They were generally included in a white rectangle often surrounded by text. 2.1.2 Characteristics due to image acquisition techniques Other characteristics are due to the digitalization pro-cesses. They relate to the lack of lighting in the binding, to the curve of the text lines, to page distortion and to the imperfect elimination of the stains. Much research was undertaken, in particular at the time of the Debora project, to correct these defects [29,20]. 2.1.3 Selected invariant characteristics This study enables us to extract a list of characteristics, which are essential to take into account while designing algorithms for layout analysis of complex documents. Here is the prior knowledge that we used to implement our algorithms: characteristics depending on printing materials and layout conventions of the Renaissance:  X  Complex page layout, which can present several col- X  No editorial style or identifiable logical structure.  X  The presence of printed or handwritten notes at the  X  The presence of location indicators: line numbers,  X  Use of specific and multiple fonts.  X  Frequent use of ornaments (non-textual blocks) such  X  Variable location of graphical illustrations and the  X  Absence of leading bringing contacts between char- X  Non-constant spaces between characters, words and  X  Text blocks are not rectangular.

Image characteristics connected to the nature of the documents:  X  Images still degraded even after restoration (appear- X  Presence of superposition of information layers 2.2 Evaluation of existing methods on images Typography and printing technology have progressed enormously and current books meet quite a different standard of presentation than historical books. Conse-quently, the software built to recognize current documents is often inadequate when processing books of theRenaissanceperiod. Themethods of layout extrac-tion employed by this software can be classified into three main categories: bottom-up, top-down and hybrid methods [5].  X  To be classified among the bottom-up methods, a  X  In contrast, the top-down approaches use knowledge  X  The hybrid approaches try to use advantages of both
Document analysis systems of these two last catego-ries employ, often in an implicit way, a set of typesetting rules and have been designed to operate in a specific domain, mainly due to the test images used. This fact gives rise to a variety of methods, which are character-ized by different objectives and different assumptions about input type, layout structure and expected data types. A weak aspect, which is common in all these algo-rithms is the lack of information about the handling of regions that do not belong to one of the expected cate-gories.

Thus, before describing our system, we present the results of some experiments, which enabled us to evalu-ate the various traditional methods used for segmen-tation of traditional documents on historical printed documents. 2.2.1 A few words about pre-processing Despite the many efforts spent on the development of skew estimation algorithms, every year new algo-rithms are proposed in literature. This is mainly due to the need of both accurate and computationally efficient algorithms, and of methods that do not make strong assumptions about the specific class of documents they can deal with. Today, commercial solutions exist and are even regarded as satisfactory tools (Book Restorer [8]), although some problems still remain. For example, the correction of page distortion (e.g., the skew) can involve degradation on the borders of the text and the image areas. This is probably due to the general assumption that the text represents the most relevant part of the document image. So, performance decreases due to the presence of other components like graphics or pictures. Furthermore, a major part of the algorithms assumes dealing with documents with a clearly dominant skew angle, and only a few methods can deal with documents having multiple skew angles [26,18,31,23]. In this work, we decided not to spend time on working with these low-level problems, but rather to set up algorithms able to deal with these specificities. 2.2.2 Morphological and differential filtering algorithms Morphological methods have been implemented and frequently tested mainly for the fusion of characters in words, then in lines and for the elimination of the noise [7,27]. Figure 2 illustrates the results we can obtain using this type of method. The traditional Run Length Smearing Algorithm (RLSA) [35] can also be used; it works and provides similar results to all those morpho-logical methods.

It is also possible to work directly on grey level images by using differential filters [19]. The idea is to use filters that detect and agglomerate the variations of intensity, periodically produced by the contours of the characters, and then to look for horizontal alignments correspond-ing to text areas. 2.2.3 Methods based on the analysis of the connected These methods transform each page into a set of con-nected components [33]. Figure 3 shows that the size, the proximity and the relative position of the connected components can be used to extract the physical structure of a document. The bounding boxes of the connected components frequently overlap themselves in the graph-ical parts, but not so much in the text parts. Furthermore, the graphical areas correspond to the connected com-ponents whose dimensions (width or height) exceed a fixed threshold.

A text block is a set of small, connected components in close proximity to one another. Two characters are known as neighbours if the distance separating them is lower than a maximum threshold. O X  Gorman proposed to use only the neighbourhood between components to locate the text areas in an image [26]. Considering the same idea, we implement a method, which seeks for the components in the four principal directions of each text component. A component that does not have a neigh-bour in at least one direction is considered as a part of the contour of the block. Then, a closed circular chaining translated the border of a text area (Fig. 4).
K. Kise [17] proposes to use an area Vorono X  diagram instead of directly connected components to extract text blocks. The area Voronoi diagram enables in building a neighbour-graph, which represents neighbours of the characters. Then, the task of line extraction is considered to be a path selection problem in the graph.

Like the methods based on filtering, these methods are not suitable for historical books processing because of the proximity between some of the text areas in the pages. For the localization of the graphical parts, the thresholds on the size of the connected components must correspond to a size slightly larger than the great-est character contained in the book. However, even if statistical studies can make it possible to automate the selection of these thresholds, these methods hardly sup-port variability in size of the characters and variabil-ity in page layout, which can frequently appear in the old books. The bad quality of the images (holes, spots, etc.) can also raise many problems when using such methods.

In conclusion, it seems quite difficult to segment cor-rectly the text paragraphs and the graphical parts in the old documents by using bottom-up approaches, mainly because they use only the local criteria. 2.2.4 Background analysis Some other methods are based on the assumptions that the white spaces act as separators of the different regions in the document [2,6,3]. In practice, they consist of a hierarchy of constraints on the relative size of spacings between different objects, e.g., spacings between char-acters are smaller than spacings between words, which, in turn, are smaller than spacings between lines and so on. In [6,3], the search for the white separations could be achieved by using horizontal and vertical projections. Then, the localization of white separations is done by an analysis, of the histogram, of the number of black pixels of the lines and the columns of the image. Next, one can regard the significant difference between successive val-ues in the histogram as delimitation between two blocks of text. The problem is to estimate the significant differ-ences between the histogram values. Generally, a priori knowledge about the document to be analysed (number of columns, margins, etc.) is used to locate separations more easily. In the old documents, the tests we have done using such methods showed that the quality of the results is increased by blackening the bounding boxes of the connected components of the image before cal-culating the histogram.

The main drawback of these two methods is that they suppose the existence of rectangular text blocks. It is not the case for historical books. Moreover, the problem with this type of techniques relates to the separation between the local and global minima in the histogram. In historical books, certain separations between blocks cor-respond to non-significant minima (case of badly desk-ewed images). So, it is difficult to use these methods when page layout is variable or when the pages are badly rectified by pre-processing.
The method described in [2] attempts to locate in a more local way the inter-block spacings instead of the maximum size white rectangles themselves. It used white tiles to extract contours of regions. But this always sup-poses that two blocks are necessarily separated by a white space from significant surface in either a horizon-tal or a vertical direction. Unfortunately, as shown in Fig. 1, it is not the case with historical documents, which we have to process.

Among all the top-down methods suggested to seg-ment complex page layouts of composite documents (newspapers, pages of magazines, etc.), we decided to adapt the Split and Merge method suggested by Hadjar [13] during the  X  X egmentation contest X  of ICDAR  X 01. Its goal is to locate the homogeneous white blocks by splitting the image into smaller and smaller areas (Fig. 5). A priori information is needed particularly to define the stop criterion in the recursion. This method can be compared with other more or less advanced techniques like the Recursive XY-Cut ( RXYC ) [24,1] method. Readers interested in such a method can also look at [32], where Wang and Srihari compare the RLSA and RXYC approaches. In all the cases, it is the analysis of the neighbourhood (realized with the assistance of graphs or trees) of each white area discovered by split-ting that will make possible to locate and characterize the blocks contained in the page in a more or less fine manner. The major problem of such techniques is that they are suitable only for layouts that are decomposable by a sequence of horizontal and vertical subdivisions.
The top-down methods seem to be less sensitive to the noise than the bottom-up methods. When the images are well deskewed, they partly solve the problem of prox-imity between blocks and characters. Nevertheless, they are not easily applicable to documents having a variable or a non-rudimentary page layout, since they require the use of a priori knowledge [14] about the document (number of columns, splitting criteria, etc.). They are thus not adapted to the processing of old documents, which do not respect a precise editorial style. 2.2.5 Multi-resolution and texture analysis Jain and Al [11,12] present a multi-channel filtering approach to texture segmentation. The basic assump-tion is that regions of text in a document image define a unique texture, which can be easily captured by a small number of Gabor filters. As said in [21], one major problem of texture-based approaches is their high time complexity, since different filters are needed to cap-ture the desired local spatial frequency and orienta-tion of the regions. Many masks are needed to extract local features and small masks do not allow one to detect large-scale textures. To avoid this problem, some researches [10] have proposed using a multi-scale anal-ysis. Such methods improve the performance, but are quite time-consuming due to the dependence on thresh-olds and parameters. Furthermore, in cases of historical documents, different regions can be incorrectly merged always due to proximity between the different text blocks.

Totest thetexture-basedapproaches, weimplemented a method based on the extraction and the quantification of the various orientations present in different areas of the document images, based on the use of the autocorre-lation function [16]. The conclusion of our experiments (Fig. 6) is that text/non-text segmentation in Renais-sance documents can be viewed as a texture segmenta-tion problem. We also conclude that the above methods, which consist in analysing the various textures contained in the image, even with a multi-scale approach, are not easily adaptable to obtain precise contours of the text blocks when the text blocks are too close to one another. Small white spaces become undetectable when the res-olution decreases and two different text blocks do not always present any significant differences in their tex-ture. These techniques are nevertheless interesting when we try to characterize globally pre-segmented blocks using particular indices. 3 A new method of segmentation The preceding tests stressed the limits of the traditional methods to segment historical documents as well as the reasons of their failure. On this basis, we devel-oped a new method to exploit the benefits of the top-down methods and the bottom-up methods at the same time. Our method uses a map of the background of the images to highlight the separation between blocks and a map of the shapes present in the image (foreground). Then, we propose to use simultaneously the informa-tion provided by these two representations (foreground and background) to segment the image. Thus, we solve most of the difficulties mentioned in the previous chapter. 3.1 Map of the foreground (of the shapes) The connected components provide relevant informa-tion about shapes (graphical and text parts). They cor-respond to one or more characters, noise or graphical parts. Their positions, size and the overlapping of their bounding boxes provide precise information about the contents of thepages. Theycanprovidelocal information on each shape present in the image. The connected components representing the letters of a word are extremely close. Unfortunately, in old documents, the page layout and the spacing between shapes may change frequently. For example, the last letter of a line can be closer to a note in the margin than to the letter, which precedes it on the line. So this information should not be used solely.

To obtain the map of the shapes, after a binarization of the image (using the method described in [30]), we carry out a contour tracking of the shapes which allow us to extract the bounding box from each component. The position and the size of the rectangles are stored in a list, which correspond to the foreground map (Fig. 7a). According to its dimensions, each shape is labelled using one of the following labels:  X  Noise (connected components of small size),  X  Graphic (connected components of large size),  X  Text (connected components of intermediate size).
The thresholds used during this phase are chosen by the user according to the maximum and minimum size of the characters in the book. As discussed in Sect. 2.2.3, the methods based on connected components while using simply their dimensions could not fulfil the whole task. In fact, in our approach, this step provides only a first-hand labelling, which will be checked and evolved thereafter during the interactive analysis. Our experi-ment on old documents shows that it is sufficient to sep-arate text shapes from graphics, but to extract and label precisely different parts of a document (paragraphs, illustration, etc.), more elaborate processing steps are needed. These processing steps are described in Sect. 3.3. 3.2 Map of the background (separations between In the bilevel image, the background of the page is rep-resented by white pixels. Normally, a great number of white pixels can be aligned vertically or horizontally in the regions between two blocks of a page (text, graphics, etc.). In the same way, a large number of white pixels are aligned horizontally in the regions between two para-graphs. In contrast, the number of white pixels that we are able to align vertically or horizontally in a paragraph between two letters of a word and between two words of a sentence are small, the same between two lines of the same paragraph.
 Consequently, we propose to associate to each pixel P ( i , j ) of the image the summation of lgb _ h ( i , j ) , the num-ber of successive white pixels on the horizontal segment containing the current pixel (line), with lgb _ v ( i , j number of successive white pixels on the vertical seg-ment containing the current pixel (row), to build a dis-tance map. The maximum value for Ng ( i , j ) is Max = Height  X  Width (size of the image) and if neces-sary the lgb _ h and lgb _ v values can be weighted, respec-tively, by the width and the height of the image in order to not privileged one of the two directions. This summa-tion corresponds to formula (1) that allows assigning of a value to each pixel in the background of the page (white parts in the map of the shapes in which the interior of the connected components are blackened).
 Ng ( i , j ) = Max  X  lg b _ v ( i , j ) + lg b _ h ( i , j
After a normalization to 255 of Ng ( i , j ) , we obtain as shown in Fig. 7b a map with grey levels. As Fig. 7b shows, our map of the background translates the sepa-ration (more or less underlined) between the blocks of the page.

Baird [4] and Antonacopoulos [2] have proposed a method based only on information similar to that pro-vided by our map since they proposed to extract the white areas from maximum size in a page. But, because of the use of rectangles or of tiles, these methods are more sensitive to the distortion and noise problems than the one proposed here. Moreover, in our case, the white blocks constitute only one part of the information, which we exploit to carry out the segmentation (and will be combined with the foreground map). In the same way, compared with methods based only on the analysis of the neighbourhood of connected components [17], our approach uses a more global information because each value in the background map depends on the global lay-out of the page. 3.3 Fusion of the information provided by the two 3.3.1 Text/graphic separation The graphical parts do not always correspond to only one connected component in the map of the shapes. On the contrary, most of the time, they correspond to the superposition of the bounding boxes of several connected components labelled Text or Graphic In our processing, when two bounding boxes labelled Graphic overlap, they are merged in order to produce only one rectangle including the whole of the detected graphi-cal areas. It is also possible that connected components labelled Text are present inside an area labelled Graphic . These components are then labelled Text_Graphic and would be the subject of a particular analysis to deter-mine if they belong indeed to the Text class or to the Graphic class. For that, the algorithm that we developed uses, once again, the concept of map of the background to carry this classification. The algorithm used to distin-guish the Text components close to or inside a Graphic component is as follows: 1. Text areas inside Graphic areas are relabelled Text_ 2. The Text and Text_Graphic labelled blocks are col-3. Calculation of the background map in the same 4. Text_Graphic blocks positioned over the dark parts
The removed areas are located in strongly textured areas of the binarized image. We assume that these areas correspond to the presence of graphical parts in the ini-tial image (see Fig. 9). 3.3.2 Extraction of the text areas By simultaneously using the information provided by the maps of the shapes and the background, we can extract Text areas. We start from the list of the connected components labelled Text to rebuild the paragraphs of text by association of connected components likely to be characters. To carry out an association, it is necessary that the two components labelled Text are rather close and the segment between their centres of gravity ( G 1 and G 2 ) does not cross an important transition area in the map of the background (low grey level values). This multi-criterion constraint can be expressed by: d ( G 1, G 2 )  X  256  X  Min where d indicates the Euclidean distance between the centres of gravity of the two sets to be associated. When this criterion is checked for two close areas, we give them the same label (attribution of an identical number to each character of the same block of text). If the cri-terion is not respected, the association is refused. Some other multi-criterion constraints were tested (such as using the sum of the grey level values instead of the minimum, or the use of the distance between the bor-ders of the two areas instead of the centre of gravity, etc.) and we kept the best one. The Fusion_Threshold can be chosen manually by the users. The selection of this value is quite straightforward. First, the users are advised to start with a low value (which will result in less fusion of the text part) and then later during the interactive analysis, if necessary, the fusion step can be reactivated for difficult text parts. Such strategies are dis-cussed in detail in Sect. 5.2 (description of the different scenarios).

The search for the neighbours between text parts is done successively, horizontally (horizontal fusion) and then vertically (vertical fusion) in a progressive way, and stopped when no more fusions are possible. These areas are not always rectangular since they correspond simply to a gathering of the shapes having the same labels. The obtained areas can thus have an arbitrary shape. Two examples of the results are provided in Fig. 8. The Fusion_Threshold (2) used is the same for the two images. In order to visualize the obtained segmentation, each label given to a component of a given area corre-sponds to a different grey level value. In example (a), all the blocks were correctly segmented in spite of their proximity. In example (b), the layout of the page and the typesetting rules are much more complex (presence of ornamental letters, images, legends, titles) and our method tends to over-segment the image (there are too many blocks). But, we will see later, this over-segmen-tation caused by the choice of a strict initial threshold for the fusion is only temporary, since the algorithm of fusion can be applied several times with different parameters (insertion of the fusion step in the scenarios of analysis created by the users). 4 User-driven analysis of the layout Except for extremely specific applications, current block classification or logical layout analysis methods have shown their limitation. Our proposal consists of a gen-eral revision of document recognition methods to design a widely usable system (i.e., not being dedicated to a particular model of documents). For this purpose, block classification needs to be addressed in a more flexible way. So, we are proposing an architecture that drops the strict processing chain to offer an adequate level of human X  X achine cooperation. An internal representa-tion of the document structure associated with a set of rules can enable an interactive learning of the model of the documents. Furthermore, the users can use a given example of image to define this new model.

As shown in Fig. 8, the segmentation stage produces an intermediate representation of the image, providing for each page a set of blocks labelled Text , Graphic , Text_Graphic or Noise At this stage of the process, it is possible to ask the user to build different processing sequences (we will name them scenarios of structural analysis), allowing a gradual evolution of this intermedi-ate representation according to the user aims and to the characteristics of the images analysed. He and Downton have already shown that it is sometimes interesting to propose user-assisted document analysis systems [15]. In the long term, our goal is to obtain automatically the most precise labelling of the contents of the digi-tized pages of complex documents by the application of a scenario that the user builds interactively and easily (user-driven analysis). 4.1 Interactive setting of scenarios for analysis Once the initial segmentation of the image has been achieved, the architecture of the system (called AGORA) that we propose, makes it possible to continue the analysis in an interactive way on a typical image. For that, we conceived a set of interfaces that allow the user to build, in his own way, scenarios of analysis. The application of a scenario will allow the progressive evo-lution (incremental analysis) of the contents of the inter-mediate representation (initial segmentation) obtained before-hand.

The tools placed at the disposal of the user to build the scenarios are:  X  A rule editor allowing the refinement of the labels  X  The application of the algorithms for horizontal or  X  Deletion of the blocks of a particular type (e.g., the To build a scenario, the user performs (as for a macro) successive actions, which have to be recorded on a typi-cal image. These actions are applied to the image and the results are displayed in real time. We can thus validate their effectiveness. The actions (rules) are translated in a literal list permanently displayed (see Fig. 10). The user can, then, manage this list to modify the scenario by reordering or removing processing rules. Once the sce-nario is considered to be correct, it can be saved in a file for storage or for application on a more subsequent set of images (a complete book) during batch processing.
Currently, theusablerules tomakethelabels of blocks contained in the intermediate representation concern (Fig. 11):  X  the topological position of the blocks,  X  the neighbourhood relations between identified  X  the shape and the content of the blocks. 4.2 Study of the topological position It is possible to identify a number of invariants on the topological positioning of the objects present in a his-torical book, making it possible to associate a label with them. The goal is not to use extremely strict rules because page layout can be variable (it would be unwise to specify that the centre of gravity of a note in the left margin will be between the pixel of X -coordinate 205 and 213). However, one can reasonably specify that the position of the centre of gravity of the left margin text is doubtless located in the first third of the width of the page. The suggested interface (see Fig. 11a) makes it possible to take into account the topological position (left, right, top, bottom, centre) of the centre of gravity of the blocks for the evolution of the label. Of course this type of rule will only constitute a first index (pro-visional label) to correctly isolate an object of a given type. 4.3 Study of the neighbourhood It is also possible to insert rules in a scenario, which concern the neighbourhood relations between blocks. Examples of rules that allow labelling and extraction of different elements are provided in Fig. 12. For example, the second interface (Fig. 11b) allows in defining rules like:
A Graphic block will be labelled Ornamental letter if it has:  X  at the left, a block labelled Left Margin OR Column  X  AND at the right, a block labelled Legend OR Col- X  AND above, a block labelled Legend OR Column  X  AND below, a block labelled Legend OR Column 4.4 Study of shapes and contents Another interface (Fig. 11c) makes it possible to use intrinsic properties of shape to refine labelling. The actual and non-exhaustive proposed criteria are  X  the size of the block ( width and height );  X  the Height/Width ratio of the block, for example, for  X  the number of connected components (elements)  X  the ratio of block Height/average Height of the con-To finish this part, let us recall that, in addition to these rules allowing evolution of the labels of blocks, it is pos-sible to insert rules of fusion and suppression of blocks in the scenarios. Examples of scenarios built by users, maybe not specialists in image processing, can be seen in the following part. 5 Experimentation and results Our software was made available to the CESR, which currently uses it in an intensive way to process, analyse, index and make available on-line their historical books. Training in the use of the software (segmentation and creation of scenarios) was given to the potential users (historians, librarians, keyboard operators, etc.), so that they could produce scenarios and consequently test our user-driven system on numerous images. This collabo-ration between the CESR and our laboratory makes it possible to improve and complete the interfaces of the software, taking into account the needs of the final users.

Several experiments were undertaken either by the staff of the CESR or within our laboratory. The first experiment was devoted to an analysis of the robust-ness of the block segmentation with respect to the skew. This test was achieved using ten images of current doc-uments and ten images of historical books on which we applied rotations from 1  X  to 20  X  . A second experiment was carried out in our lab on a complete historical book with 250 pages. The objective of this test was to validate the usability of the proposed interfaces to build differ-ent scenarios of analysis for text/graphic separation and classification.

AGORA was then provided to the real end-users at the CESR where numerous experiments have been done. The results of some of them are presented in this article: the first one was aimed at evaluating whether only one scenario could be used to analyse different books or if it was preferable to use one scenario for each book. To conclude this part, two different scenarios are used to illustrate the flexibility of the proposed system. All these experiments have led to the results presented in the next sections. 5.1 Initial segmentation The result obtained after the first application of the algo-rithm of fusion on the connected components, according to the foreground and the background maps, which pro-vide a set of blocks labelled Text , Graphic , Text_Graphic and Noise , is called  X  X nitial segmentation X . This segmen-tation is not the final one (since it will evolve during the scenarios of incremental analysis). So, we evaluate, in this part, only the resistance of the first step to the skew problems. Next, the results obtained after the complete application of a scenario will prove the global perfor-mance of the AGORA system.

We tested the resistance of our segmentation algo-rithm to the skew of images on old documents and on images of current periodicals. The results obtained using 20 images are summarized in Figs. 13 and 14 and indicate a sufficient tolerance for the old documents and a strong tolerance for more structured documents. If the docu-ments are badly positioned at the time of scanning or if the images are badly illuminated, it is preferable to apply a correction of the geometrical defects before using the segmentation algorithm. When the image scans are cor-rect, the obtained results are very satisfactory without pre-processing. 5.2 User-driven structural analysis During a first experiment carried out in our laboratory, a scenario allowing the labelling of seven types of histor-ical elements was set up:  X  X eft Margin (MG) X ,  X  X ight Margin (MD) X ,  X  X rnamental letter (Ltr) X ,  X  X ight Page Number (NPD) X ,  X  X eft Page Number (NPG) X ,  X  X rin-cipal Title (TP) X  and  X  X itle (T) X . For this test of block classification, we used an image database of 250 pages coming from a complex historical book provided by the CESR. Previous examples of pages used to illustrate this article were extracted from this book. The obtained results are provided in Fig. 15. The tiresome work of checking the assigned class for each zone was carried out in a visual way on each analysed page. Table 1 shows the results obtained during the batch processing, which lasted 9h and 20min on an Athlon barton 2.5GHz with 768MB of RAM.

These results are dependent on the initial segmen-tation and on the robustness of the rules constituting the scenario built by the users. The aim of the user who conceived this scenario was not to have any bad classi-fication (minimal risk) for the Ornamental Letters and the Margins .

The scenario used was thoughtful, but did not imple-ment  X  X edundant rules X , which could have involved additional risk on certain criteria. These  X  X edundant rules X  aid in the verification of the high-level elements (e.g., ornamental letters) that have already been extracted with the help of the processing rules. For exam-ple, once ornamental letters have been located using rules describing their size and positions in the image, the user can specify a redundant rule to verify that no ornamental letter is found to the right of another ornamental letter (impossible in real images). If found, for example, the redundant rule will change the label  X  X rnamental letter X  with the label  X  X nknown graphic X . Indeed, the labels generated at a given moment can be validated later using other rules. With this scenario, 92% of the Margins are extracted correctly without any wrong detection. This scenario could be enhanced in order to locate the missing Margins , once again, by add-ing more rules when the other high-level objects have been localized (evolution of the context to facilitate new extractions). Still with the same scenario, 99% of the Ornamental letters were well classified. This is satisfac-tory since the labelling of the Ornamental letters uses the result of the classification of the Margins . Table 1 shows that the Page numbers were difficult to extract or that the rules used to locate them were not sufficient. Many other experiments were carried out at the CESR by the staff following the training in the use of the software. In addition to the recognition rates, it is inter-esting to notice the way in which the produced scenarios are structured and applied.

At the CESR, all the tests were carried out on images resulting from a digital camera that provides 1,200  X  2,000 Gy level pixel images. The segmentation parame-ters were as follows:  X  automatic binarization;  X  minimum height of a large component: 60;  X  minimum width of a large component: 60;  X  maximum height of a small component: 5;  X  maximum width of a small component: 5;  X  threshold of horizontal fusion: 500;  X  threshold of vertical fusion: 500.
 The first result relates to the initial segmentation, which did not produce any error of labelling. All the Text and Graphic blocks were correctly detected in all the books selected; however, sometimes an over-segmentation of some blocks resulted from the use of strict fusion thresh-olds (500). But, one can notice that more fusions can be achieved later during scenarios of classification to cor-rect this over-segmentation. 5.3 First scenario A first scenario was carried out on 1,452 images from five different books. The objective of this scenario (shown below) was to identify the various types of graphical blocks present in the five different books. As explained in Sect. 3.1, at the end of the first step of the processing, the representations of the image contain a list of blocks labelled as Text, Noise or Graphic. Then, the user has to settle scenarios to make these initial labels evolve using specific rules as shown below:  X  Text on the left of the page with 15% becomes Left  X  Text on the right of the page with 15% becomes Right  X  Vertical Fusion of the Left and of the Right Margins  X  Graphic with at the Left : Left Margin OR Nothing  X  Graphic with Width/Height Ratio between 3 &lt;&lt; 10  X  Graphic in the centre of the page becomes Portrait.  X  Portrait with above : Portrait OR below : Portrait OR  X  Graphic with at the Left : Text OR at the right : Text  X  Suppression of the Text and the Left and Right Margin Here, we can notice that the first rules often use only topological properties of the blocks to change their labels. This probably means that these kinds of rules are easy to understand and manage (but perhaps not very efficient). When Margins are detected, it is possible to run for another time the fusion step (used during the segmentation) to merge only Margin blocks. In this sce-nario, neighbourhood and shapes rules are used to locate Ornamental Letters and Borders . The images were pro-cessed in 11h and 22min and the results obtained are shown in Table 2.

Taking into account the diversity of the selected old books (size, layout, etc.) and the simplicity of this sce-nario, the results appear quite good. Nevertheless, to obtain better detection rates and to extract more ambig-uous objects, it is often preferable to adapt the scenario to each book. Thus, the continuation of the tests was achieved on images coming from a unique book. Some pages of this book are presented in Fig. 16. 5.4 Second scenario A secondtest was carriedout on180pages of theselected book. Because some images were badly illuminated or some text lines had an important curvature, Book Restorer [8]) was used for the pre-processing of the images (lighting correction for the spots and the shade of the binding, geometrical correction of the curvature):  X  Graphic with Width/Height Ratio between 3 &lt;&lt; 10  X  Graphic with at the left : Left Margin OR Nothing  X  Vertical Fusion of Text with a threshold of 2,000.  X  Text on the left of the page with 25% becomes Left  X  Vertical Fusion of the Margins with a threshold of  X  Horizontal Fusion of the Text with a threshold of  X  Text on top of the page with 10% AND with a number  X  Text on bottom of the page with 25% AND with a  X  Signature with Text below, on the left or on the right  X  Vertical Fusion of the Text with a threshold of 3,000.  X  Horizontal Fusion of the Text with a threshold of  X  Text with height in pixel between 0 &lt;&lt; 100 AND on  X  Horizontal Fusion of the Title with a threshold of  X  Suppression of the Title if the number of elements is  X  Suppression of the Title if the Width/Height Ratio is  X  Suppression of the Title if it is on the left of the page  X  Suppression of the Text.
 This scenario is again quite simple. All kinds of rules (topological, shape, neighbourhood) are equally used. Fusion steps are used very often to merge specific block like Margin and Text . The processing lasted 2h and 32min and produced the following results:  X  Detection of the Borders with 100%.  X  Detection of the Ornamental Letters with 96%.  X  Detected Pagination with 79%.  X  Identification of 100% of the Titles but with 4% of 5.5 Third scenario A last test was applied on 1,202 images without pre-processing, with the objective of only labelling the Text blocks. The following scenario was used:  X  Suppression of the Graphics  X  Horizontal Fusion of the Text with a threshold of  X  Text in the left of the page with 25% becomes Left  X  Vertical Fusion of the Margins with a threshold of  X  Margins with a Width/Height Ratio between 4 &lt;&lt; 30  X  Text on bottom of the page with 25% AND with the  X  Vertical Fusion of the Text with a threshold of 3,000  X  Text with Width/Height average in 0 &lt;&lt; 2 AND on  X  Horizontal Fusion of the type Titles with a threshold After 10h and 52min of processing time, all the Titles and Signatures were detected correctly. The extraction of the Margins functioned, but produced many spurious elements (shades, spots, etc.) 5.6 Discussion To conclude this experimental stage, we can first notice that the average processing time for one image is 30s. The obtained results are better on pre-processed images. The automatic binarizatrion (by Sauvola algorithm [30]) increases the processing time by 25 X 35% as well as increasing the precision of the detection. Lastly, it is preferable not to merge the blocks too much at the beginning (initial segmentation) in order to be able, in certain cases, to isolate small elements (e.g., Signature or Pagination ) before carrying out additional fusions later in the scenario of analysis. The presented scenarios show that the users prefer to implement several scenarios (one for the text part and one for the graphical part) instead of using only one global scenario, even if it would proba-bly have provided better results. It probably means that it is not as easy to build thoughtful scenarios as one may initially think.

Seeing these experiments and results, we think that the main advantages coming from the use of AGORA are:  X  Robustness to proximity between blocks during the  X  Working even if the blocks are not rectangular and  X  Robustness to normal skew (for a skew higher than  X  Powerful separation of Text/Non-text areas even for  X  Of course, genericity due to the use of a  X  X ser-driven  X  The only weak point of AGORA shown by our 6 Conclusion In the first part of this article, we highlighted the sources of errors in the traditional methods of page decom-position using a characterization of page layout in the historical books. We noticed that each type of method (bottom-up and top-down) required different informa-tion, which one should have been aware of to reach a segmentation of quality. For that, our recommendation consists of using first a hybrid algorithm based on the construction of two representations of the image: the map of the shapes that is focused on the connected com-ponents and the background map that provides informa-tion on white spaces separating the blocks constituting the page. The joint analysis of the contents of these two maps makes it possible to lead to a robust initial seg-mentation of the image. The results obtained with this method are very interesting; the adjustment of the nec-essary parameters is straightforward and not sensitive to variations.

Second, the originality of our approach lies in the opportunity, which we offer to the users to be able to build, in an interactive way, scenarios of incremental analysis. We propose to call this new method  X  X ser-driven analysis X  in opposition to data-driven or model-driven methods. The goal is, on the basis of the initial segmentation, to be able to make the representation of the images evolve in a progressive way to lead to the finest possible characterization of its contents accord-ing to the user objectives and to the type of images to be analysed. The CESR has processed several complete books using AGORA prototype and their own scenar-ios of block classification. Thus, the CESR has increased the number of books offered to the users in its virtual library (see http://www.bvh.univ-tours.fr). Even if the system produced some errors, the processing and time saved as compared to manual processing is considerable (e.g., the manual indexation of the page layout of an his-torical book of 300 pages lasts  X  2days instead of only 2h when using Agora), thus providing to the specialists of historical books, a useful tool, which they had never imagined.
 References
