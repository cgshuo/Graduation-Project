 Online social networking services distribute various types of messages geared towards providing increased value to their members. Common types of messages include news, con-nection requests, membership notifications, promotions, and event notifications. Such communication, if used judiciously, can provide an enormous value to the members. However sending a message for every instance of news, connection re-quest, or the like can result in an overwhelming number of messages in a member X  X  mailbox. This may result in reduced effectiveness of communication if the messages are not suffi-ciently relevant to the member X  X  interests, and potentially a poor brand perception. In this paper, we discuss our strat-egy and experience with regard to the problem of email vol-ume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system, and experimental results from online A/B tests. Machine learning, optimization, email
LinkedIn and other online social networking services pro-vide members with an easy way to connect and exchange in-formation. They facilitate exchange of information by giving members the opportunity to send private messages to other members, upload and share rich media with other members, self-organize into interest groups around a subject matter and ask/answer questions, subscribe to channels delivering news on topics of interest, etc. The networking services help members keep up with this information through pull-model based in-app services like feed and in-app notifications, as well as push-model based content distribution services like email and push notifications. Conventionally, members can subscribe to receive email communication regarding news, connection requests, membership and event notifications. At times the social networking service may also use the email communication channel for the purpose of promotions or product marketing. Such email communication is of special interest to a networking service because of its potential in providing valuable information to its members without re-quiring them to be actively logged in to the mobile or web applications of the networking service. Email communica-tion, if used judiciously, can help provide enormous value to the members and as a consequence increase engagement and value to the networking service. However, sending an email for every instance of news, connection request, or the like can result in a member receiving an overwhelming num-ber of email messages. This can hamper the effectiveness of communication if the email messages are not sufficiently rel-evant to the member X  X  interests, as the member might start to indiscriminately disregard all messages received from the networking service (including the important ones).

For a social networking service, email communication can help drive mid and long-term value to members. This value can be measured in various ways, including member engage-ment. Member engagement often relates to members visit-ing the mobile or web applications of the networking service more often. However since optimizing for member visit fre-quency is often difficult due to long-term feedback loops, proxies that are readily measurable with little feedback de-lay are often optimized in practice. Most of the industrial research and development in the area of email optimization has been in the context of driving up click-through rates of email marketing campaigns [1]. These approaches were inadequate for our problem setting.

Email communication from LinkedIn to members com-prises of several different types of emails, with each type corresponding to a product vertical as illustrated in Figure 1. Some sample email templates are shown in Figure 2. As highlighted in the email template of Figure 2(b), LinkedIn allows members to control their communication experience of a particular type of email through the change frequency or the unsubscribe options on any email of that type. Al-ternatively, members can personalize their communication experience of all types of emails through the communica-tion settings panel under the user account settings tab on the web or mobile application, as shown in Figure 3. In this panel, for each type of email, a member can choose to turn off communication completely, or to receive commu-nication at event-occurrence, daily, weekly or recommended cadence. The recommended setting is designed to enable the member to receive email communication regarding only the most relevant information in a timely manner. LinkedIn op-timizes the communication experience of the member when the member opts-in to this setting.
Optimizing the communication experience of a member entails optimization on several fronts, such as, volume op-timization, delivery time optimization, frequency control, and channel selection. Each of these problems pose unique challenges which merit careful attention to detail. One of these problems, namely volume optimization, is discussed at length in this paper.

Since email communication can provide a large value to the members, it is worth investing in methods that can max-imize this value and at the same time minimize unnecessary email. Naive reduction of email volume can have adverse ef-fects on member value and growth of the service. Hence, in-telligent algorithms are required to identify the emails which may be dropped (not sent). The decision to send or drop an email needs be based on the likelihood of the targeted user deriving a high value from the email. We can indirectly infer the value based on the user X  X  reaction to the email. The user may interact positively with the email by either viewing the email or also by clicking a link within the email which will typically bring her to the application. Alternatively, the user may react negatively to the email by either ignoring the email, or by clicking on the unsubscribe option within the email, or by reporting the email as spam to her email service provider. Ideally the networking service would like to cut down on email volume in a way such that the pos-itive interactions are maximized and negative interactions are minimized. It is evident that this would require opti-mizing across multiple different objectives. This paper is about the challenges, approaches, and experiences relating to email volume optimization at LinkedIn. Our contribu-tions are summarized below:
Whittaker et al. [2] published the first user experience study on email overload in 1996. The study exposed the problem of information management, with the subjects find-ing it challenging to organize their cluttered inboxes and identifying important emails. Sixteen years later, Grevet et al. [3] conducted a quantitative analysis of users X  inboxes on Gmail and concluded that large email volume and poor email organization continue to remain problems. In a unique study published in [4], the authors were able to quantita-tively establish the advantages of cutting down emails in a workplace. These included improved work quality and lower stress levels, among others.

In light of the above, several ideas and tools have been conceived to help users manage email overload. Some of the most promising research has been targeted at automated classification of emails into different priority levels or con-tent categories. Early attempts at email prioritization can be found in [5] where the authors train a linear classifier on a small email corpus, using only email features such as text, message length, etc. More recently, Yoo et al. [6] have pro-posed methods for a more personalized email prioritization where the feature vector is enriched with personal features inferred from the social graph. Lu et al. [7] on the other hand make use of explicit personal features in the form of a user X  X  interaction history with messages.

While email prioritization tries to address email overload from the recipient side, email volume optimization is an at-tempt to address email overload from the sender side. If the sender had a mechanism to estimate the priority of each email message for its targeted recipient then the sender could decide to drop the messages with low priority, as the recipi-ent is likely to ignore those emails anyway. However the de-cision to drop messages in the recipient X  X  interest can conflict with the sender X  X  interest. Hence, email volume optimiza-tion requires the sender to make a trade-off between multi-ple different objectives. We have not been able to trace any published literature on email volume optimization per se , but there is literature demonstrating effectiveness of multi-objective optimization in various problems pertaining to web applications. Agarwal et al. have employed multi-objective optimization for personalized click shaping in online content recommendation in [8] and for displaying promotional wid-gets on web applications under business constraints in [9].
In order to empirically examine the cost and benefit of emails for LinkedIn, we set up a small user bucket as fol-lows. For each member in this bucket, we selected a random number in [0 , 1] as the probability with which each email message generated for this member was dropped. We de-note this bucket as the random-drop bucket. The remaining users received all emails generated for them. We denote this bucket as the send-all bucket.

We collected data from this experiment over a period of one week. Over this period, a large number of unique users in the random-drop bucket received several different types of emails. Each type of email contained information corre-sponding to a particular product-vertical and contained at least one link, clicking on which would bring the user to a page under the product-vertical on the mobile or web ap-plication. Important user interactions with emails, such as email view, email click, unsubscribing from an email type and reporting an email as spam were tracked. In addi-tion, user sessions were tracked and categorized into two categories, viz., downstream (email triggered) and organic, based on the following attribution model. Any session that a user started within 15 minutes of clicking a link within an email was attributed to that email, and categorized as an email triggered downstream session. Any page view and in-app action performed within a downstream session, such as profile view, job view, search, invitations-to-connect, feed like/comment/share, etc., was also categorized as downstream. All remaining sessions were categorized as organic.
Emails were found to drive a significant percentage of in-app engagement. As tabulated in Table 3.1, we observed 2.6% less page views from members in the random-drop bucket compared to the members in the send-all bucket. In other words, members who received all email messages generated for them performed 2.6% more page views than the members who randomly received about half of the email messages which were generated for them. Similarly, a sub-stantial loss in page views was observed for pages under various product verticals.
 Table 1: Delta in page views for various product-verticals. random-drop vs. send-all Figure 4: % increase in active members with in-crease in emails sent to member Figure 5: % increase in downstream page views by a member with increase in emails sent to member
To understand this better we performed a fine grained analysis of members in the random-drop bucket. We di-vided the members into four segments based on the fre-quency of their visits to the mobile or web application be-fore the start of the experiment. These four segments were: daily-active (visited everyday), weekly-active (visited once a week), monthly-active (visited once a month) and dormant (remaining). This segmentation is also meaningful from an-other point of view: about the same number of emails were being generated for members within a given segment. No emails were being generated for a vast majority of members within the dormant segment. As shown in Figure 4, it was found that the number of active members (who visited the application at least once over the course of the experiment) increased with an increase in the number of email messages sent to members. For example, from the plot for monthly-active members, we can say that if x of these members are active when no emails are sent to these members, then more than 2 x members will be active if a large number of emails are sent to these members. This trend is evident for all member segments except the daily-active segment, in which 100% of the members visit organically without the need for any email triggers. However even within this segment mem-bers were found to perform more downstream page views with an increase in the number of emails sent, as shown in Figure 5.

To analyze the impact of downstream page views on in-app actions we computed the contribution of downstream in-app actions towards the overall in-app actions. We found that downstream in-app actions can constitute 10% to 40% of the total in-app actions for certain important actions, such as viewing a job, sending an invitation to connect, etc. These findings are shown in Figure 6. Also shown in the same figure is the percentage of members who performed specific in-app actions only as downstream actions. For example, of all the members who performed action1 on the mobile or web application, about 12% of those performed that action only after clicking a link within an email which brought them to the application. This implies that a significant number of members rely on the receipt of email to carry out certain tasks on LinkedIn.
 Figure 6: Contribution of downstream in-app ac-tions to overall in-app actions
We found that excessive email communication could have several negative consequences. We observed 45% more num-ber of negative responses to emails in the send-all bucket compared to the random-drop bucket. These negative re-sponses were in the form of clicking the unsubscribe option within an email, or reporting an email as spam to the email service provider. This was not surprising since sending more number of emails is likely to result in more number of re-sponses, positive and negative. However, unlike a positive response to an email, the scope of a negative response to an email does not end at that particular email. If a member clicks the unsubscribe option within an email, we lose the ability to send any emails of that type to that member in the future. If a large number of members report emails from a particular sender as spam to their email service providers, then this can result in an email service provider blocking and filtering all emails from that sender [10]. In general, such deliverability issues are not easy to resolve.
Excessive email communication was also found to result in the members missing important emails. Owing to lim-ited time and attention spans in humans, user attention was found to get divided across important and less important emails. Here is how we discovered this. We picked four types of emails that were designed specifically to serve an im-portant purpose for LinkedIn. We selected members in the random-drop bucket who received an equal number (=4) of these important emails but different number of total emails. For this set of members we plotted the average number of clicks on these important emails by a member against the total number of emails received by that member. This plot Figure 7: %  X  in clicks on important emails by mem-ber with increase in total emails sent to member is shown in Figure 7 for two groups of members, viz., mem-bers for whom a small number of emails were generated, and members for whom a large number of emails were generated. A downward trend is readily visible in both these groups.
Irrelevant email communication (irrelevant to the recipi-ent) can at times also affect the brand perception of a cor-poration.
As established in the previous section, on the one hand email is an important driver in-app engagement, but on the other hand excessive email can also have severe negative consequences. It goes without saying that the sender needs a way to identify those emails which are important for the sender, and at the same time relevant to the recipient. Then the sender can send only these emails, dropping the rest. In other words, the sender would like to minimize the number of emails sent, in a way so as to maximize the positive outcome and minimize the negative outcome.

In this section, we describe how we formulate email vol-ume optimization at LinkedIn as a Multi-Objective Opti-mization (MOO) problem in order to obtain optimal trade-off points involving positive and negative outcomes of send-ing emails. We also describe how the MOO problem is solved and how the solution to the problem is actually used in a production system. But first, we will explain why we think MOO is the right approach.
For the purpose of simplicity of exposition, let X  X  consider only one kind of positive consequence and only one kind of negative consequence of each email. Let X  X  consider a down-stream session as the positive outcome and an unsubscribe or reporting of an email as spam (denoting either one as a complaint) as the negative outcome. Then, on one hand we would like to maximize the number of downstream sessions and on the other hand we would like to minimize the number of emails sent and the number of resulting complaints. For each generated email e , let X  X  assume we can obtain estimates for
P sess ( x e ) = Pr(downstream session from e | e is sent) and P comp ( x e ) = Pr(complaint from e | e is sent) by using email features x e in the utility prediction models (a) Thresholding on P sess (b) Thresholding on P comp Figure 8: Trade-off achieved using naive threshold-ing approach from Section 4.3. Since each email e is targeted to a partic-ular user, we are assuming that x e will also include features of the targeted user.

One simple and intuitive approach for achieving our goal could be to rank-order emails based on P sess and only send the top emails for which P sess is greater than some threshold T sess . Another simple approach could be to only send those emails for which P comp is smaller than a threshold T comp
In Figure 8 we plot the fraction of sessions and complaints obtained at various threshold values, using these two naive approaches. The points on the curves are computed by sim-ulating the thresholding process with various values of T and T comp . Both x and y axes are normalized by the simu-lation result at the point where no threshold is applied, i.e., the point where no email is dropped.

As we can see from Figure 8(a), thresholding on P achieves a very nice trade-off between sessions and sends: we could choose a point on the trade-off curve where the send volume is reduced dramatically with little loss in ses-sions. For example, we could send only 60% of the emails and still maintain roughly 93% of the maximum achievable sessions. However, this thresholding approach achieves a poor trade-off between complaints and sends: complaints are proportional to sends, which is no better than dropping emails at random. On the other hand thresholding on P comp achieves a good trade-off between complaints and sends but a poor one between sessions and sends, as shown in Figure 8(b).

Clearly, none of these naive thresholding approaches yields a satisfactory trade-off point where the send volume is re-duced with a minimal loss in sessions and a significant drop in complaints, simultaneously. Loosely speaking, an ideal solution would achieve a sessions-sends trade-off similar to the blue curve in Figure 8(a), and a complaints-sends trade-off similar to the red curve in Figure 8(b). More formally, we would like a solution to have a mechanism that can output optimal combinations of sends, sessions and complaints.
In this section we develop on our running example of find-ing an optimal trade-off between sends, sessions and com-plaints and present our MOO methodology for email volume optimization. While the following discussion is based on the example of three utilities it should become clear that the for-mulation presented here readily generalizes to an arbitrary number of utilities of similar nature.
Consider any given window of time starting from now, say, the week starting today. Let X  X  assume we have the entire set of emails which will be generated over this week and denote it as E . Also assume that each email e  X  E will be one of T different email types. Let t = 1 ,...,T be an index over the email types and E t denote the set of emails of type t . Then e  X  E t would mean that email e is of email type t .
We assign a decision variable z e = Pr(sending e ) to each email e . The set z = { z e : e  X  E } is referred to as the serv-ing plan, which is to be optimized. We now formulate this optimization problem as a constrained linear programming problem (LP) as follows. min
In this formulation:
It is worth noting here that: 1. Not all constraints (2)-(5) need to be specified. Only 2. Although in the example LP formulation above we
At first glance, solving the above LP may seem trivial: at the beginning of the upcoming week, gather all emails E which will be generated, solve the LP with any standard LP solver, and use the optimal solution obtained to serve the emails in the week. However, this is unrealistic in practice due to the following challenge: we do not know in advance the set of emails E that will be generated in the upcoming week.

Fortunately, we observe that the distribution of our gen-erated emails does not change significantly week over week. So we use the set of emails generated in the past week as a forecast for E . However, similarity in distribution does not provide us the solution z e for every email that will ac-tually be generated in the upcoming week. To that end, we make use of the primal-dual technique introduced in [8]. We first add a quadratic regularization term to the objec-tive to make the problem strongly convex. This allows easy conversion from dual solutions to primal solutions. where q  X  [0 , 1] is some prior on the send probability, and  X  &gt; 0 is a regularization parameter.

We now solve the dual of the primal problem above for the set of emails generated in the past week. Standard QP solvers are unable to handle a problem of our scale due to the large number of variables in the dual problem corresponding to the probability constraints of (6) (of the order of millions at LinkedIn). We employ an in-house large scale implemen-tation of the Operator Splitting algorithm [11] to solve this QP. Let  X  global ,  X  t ,  X  global and  X  t be the solutions to the dual problem corresponding to the global session constraint (2), the local session constraints (3), the global complaint constraint (4) and the local complaint constraints (5) re-spectively. Then z e for any newly generated email can be efficiently computed on-the-fly without the dual solutions corresponding to (6) using Algorithm 1 from [8]. That algo-rithm can be condensed into the following equation for our binary (send or drop) decision case. For an email e of type t : for the projection onto [0 , 1]. If we set  X   X  0 + send/drop decision is further simplified to the following de-terministic rule. For an email e of type t : We use this simplified decision rule in our current implemen-tation. This rule requires us to maintain a set of just a few coefficients {  X   X  t ,  X   X  t } to make a send/drop decision for each individual email. We call these the MOO coefficients.
The solution to the MOO problem above depends on the availability of downstream session prediction P sess ( x e complaint prediction P comp ( x e ) for each generated email. In this section we outline our process for training response prediction models. This includes data collection, large-scale logistic regression model training and model evaluation.
Collecting unbiased training data for our use case is a non-trivial exercise. A naive approach for collecting train-ing data is to reserve a small fraction of user base to always receive all generated emails, bypassing any volume optimiza-tion logic. The responses (downstream session/complaint/ no-interaction) thus collected could then form the training dataset. However, this method of data collection can suffer from serving bias due to the following reason. A user X  X  re-sponse to an email depends not just on the attributes of that particular email but also on the past experience of the user with similar emails. For example, we have observed that a user X  X  propensity of clicking on an email is correlated with the number of emails received by the user per week over the past few weeks. The click probability is high if the user re-ceived few emails in the previous week, but the correlation becomes weaker if the user consistently received few emails each week over the past four weeks. If the serving scheme is unable to produce instances with varying number of emails sent over the past few weeks then we will not be able to capture user behavior in such scenarios. At LinkedIn, at least one email is generated for a vast majority of active members every week which implies that the naive serving scheme will rarely, if ever, create a scenario where an active member received zero emails in the previous week, although it is likely to result in a higher probability of engagement with a candidate email this week.

We developed the following serving scheme to explore the feature space and overcome the aforementioned sparsity prob-lem. For every member, we choose a random number in [0 , 1] which is the probability with which a message is dropped for this member, denoted as P drop . We refresh P drop every 4 weeks so as to capture both novelty as well as burn-in effects. As an illustration, suppose that 10 emails are gen-erated for Alfred every week and that the random number generator produces P drop = 0 . 8 for Alfred, then this would mean that Alfred will receive 2 messages per week on aver-age from week 1 to week 4. If the random number generator produces P drop = 1 . 0 for Alfred at the end of week 4, then Alfred will receive 0 messages per week from week 5 to week 8. We do not refresh the random numbers for all members at the same epochs (beginning of week 1, week 5) but rather spread the refresh epochs as described in Algorithm 1. This provides us with the flexibility of choosing any time frame (say one week) for training data collection with adequate coverage of feature space.

Algorithm 1: Choosing P drop for each member if memberId % 4 == weekOfYear % 4 then 2 choose a new P drop for memberId else 4 keep using the previous P drop for memberId
We deploy this model for 1% of our members.
Once we have collected appropriate training data, we train two logistic regression models with ` 2 regularization, one for each of the two utilities, viz., downstream session and com-plaint. Let y denote the response to an email e described by feature vector x e , then the corresponding Bernoulli random variable Y is modeled as where  X  is the parameter vector which we want to learn. Once  X  has been estimated from the training data, for any new example x new e we simply use the mean of the Bernoulli distribution as the predicted response, i.e.,
We include 4 broad categories of features in the feature vector x e , viz., 1. Targeted member X  X  profile features such as member X  X  2. Targeted member X  X  in-app activity features such as 3. Targeted member X  X  past experience with emails such 4. Email message features such as type of message, length,
In addition, we also include interactions between the above features.

We train these logistic regression models on Spark [12] using in-house optimization libraries which implement a dis-tributed version of TRON [13], a trust region Newton method. The distributed optimizer enables model training with a large number of features and training examples on the Spark framework which allows rapid iterations.
We compute 2 metrics to measure the performance of our response prediction models: 1. Area under the receiver operating characteristic curve 2. Observed to expected ratio (O/E ratio): This is to
The AUC and O/E ratio metrics for our best utility pre-diction models are tabulated in Table 2.

Table 2: Validation metrics for prediction models
As described in Section 4.2.2 we use the set of emails gen-erated in the past week to obtain the MOO coefficients, and then use those for making send/drop decisions for emails which will be generated in the upcoming week. Since there is an element of uncertainty here, we use a simple replay methodology to evaluate the effectiveness of our volume op-timization models (response prediction + MOO coefficients) before deploying them to the online serving system.
We collect one week of data from our send-all bucket, starting from a point in time after the period over which the training data was collected. The emails e thus collected are annotated with the observed user responses, namely, ob-served complaints (oc = 0/1) and observed downstream ses-sions (os = 0/1). Now Algorithm 2 is used to compute the replay results. Note that we are only maintaining counters for the global utilities in this algorithm, but its extension to include local level utilities should be obvious.

The sends, sessions and complaints ratios thus produced should closely match the expected ratios as per (1)-(6). For example, at the global level the following should hold true: Algorithm 2: Replay algorithm for global utilities
Data : d = ( x e , oc, os) complaints = sessions = sends = 0 maxComplaints =maxSessions = maxSends = 0 foreach d do 4 maxComplaints = maxComplaints + oc 5 maxSessions = maxSessions + os 6 maxSends = maxSends + 1 7 if type( e ) == t then 8 score( e ) =  X   X  t P sess ( x e )  X   X   X  t P comp ( x e )  X  1 9 if score( e ) &gt; 0 then 10 complaints = complaints + oc 11 sessions = sessions + os 12 sends = sends + 1 sendsRatio = sends/maxSends sessionsRatio = sessions/maxSessions complaintsRatio = complaints/maxComplaints
Although this replayer is a simple and efficient tool for obtaining an estimate of a model X  X  performance in an on-line A/B test, it is not always accurate due to a shortcom-ing of this replayer. This replayer need not capture all the dynamics of the population on which the model is going to be deployed. For example, say we are going to deploy our new model to a population which is being served by another model which drops emails with a fixed probabil-ity of P drop =0.9. Then the distribution of P sess P comp ( x e ) for this population is likely to be different from the send-all population used in the replayer.
In this section, we outline the important components of our system, as illustrated through a block diagram in Fig-ure 9. The system consists of an online serving system and an offline training system.

Depending on a member X  X  message subscription settings, the message generator will produce a message for the mem-ber. For example, if the member subscribes to network up-dates, then a message is generated once a week which con-tains important updates pertaining to the member X  X  con-nections, such as job changes, work anniversaries, profile updates, etc. This message is passed down to the utility pre-diction engine along with the ID of the target user. The util-ity prediction engine extracts the message features from the message to create a partial feature vector and appends mem-ber features of the targeted user from a tracking data store. This feature vector is used in the utility prediction models to predict the expected utilities as per (8). These predicted utilities are passed to the Volume Optimization (VO) deci-sion engine which makes the final send/drop decision based on the MOO coefficients as per (7). The member X  X  inter-actions with the received emails, along with member profile and activity data are recorded in the tracking data store. Hourly snapshots of this database are loaded into Hadoop. The snapshot data is used in the utility model trainer for training response prediction models on Spark. These models are fed into the utility prediction engine as well as the MOO solver. The MOO solver employs these models on the train-ing data for predicting expected utilities, which are used in the optimizer to produce optimal MOO coefficients. Note that the beauty of the design lies in the ability to make a send/drop decision for each individual email independently, based on the learned response prediction models and a few MOO coefficients. Further, this simple but effective design is what enables us to scale the online decision making to handle millions of message requests every week.
We set up an online A/B test experiment to evaluate the performance of our volume optimization approach. Mem-bers in the treatment bucket received emails based on the output of the volume optimization decision engine. The MOO coefficients used in the decision engine were dual so-lutions to a MOO problem with constraints listed under the  X  X onstraint X  column of Table 3. The experiment covered several email types which constitute about 70% of the total volume of emails sent by LinkedIn. We collected several met-rics of interest from this bucket and compared them against the control send-all bucket in which members received all emails generated for them. Table 3 summarizes the offline replay and A/B test results. The  X  X ontribution X  column shows the contribution of each email type towards the total sends, complaints and down-stream sessions in the send-all bucket. The  X  X onstraint X  column lists the complaint tolerances and session targets at global and local levels. In this experiment, at the global level we are willing to tolerate no more than 60% of the maximum possible complaints (when all emails are sent) and target at least 98.5% of the maximum achievable sessions. It is quite obvious from Figure 8 that it is impossible to achieve this kind of a trade-off through the naive threshold-ing approaches. We have also specified local level constraints for three important email types. There are no local level constraints for the remaining email types. The  X  X OO Co-eff X  column contains the dual solutions (  X  global ,  X  t ,  X  and  X  t ) corresponding to the constraints of the optimiza-tion problem. The  X  X ffline Replay X  and  X  X /B Test X  columns show the sends, complaints and downstream sessions results achieved through our volume optimization model in offline replay and online A/B test respectively. The A/B test re-sults are relative to the send-all bucket.

We draw the following observations from the table: 1. The MOO coefficients  X  1 and  X  2 corresponding to the 2. The MOO coefficient  X  2 corresponding to the com-3. All constraints are being satisfied in the A/B test re-4. The offline replay is making reasonable predictions about
Weekly trends for the global metrics that we directly op-timize for, viz., sends, sessions and complaints are shown in Table 4: Change in number of unique users visiting certain product pages due to volume optimization Figure 10. The red line in each plot corresponds to the A/B test result while the blue line corresponds to the constraint. Table 3 above was a detailed presentation of results from week 8.

We also saw an indirect impact of volume optimization on certain metrics that we did not explicitly optimize for. The impact on one such metric, namely, the number of unique users who visited a page is tabulated for some of the prod-uct pages in Table 4. A  X - X  entry in the table indicates that the impact was not statistically significant. We do not ob-serve any statistically significant loss in the total number of unique users visiting the mobile or web applications. This was expected because of our strict constraint on global ses-sions target. However, we do observe a reshaping of user visits across various products. This is a nice byproduct of volume optimization which eliminates less relevant and poor quality emails. If poor quality emails are dropped, the total email volume received by a member is reduced allowing the member to devote more time and attention to the good qual-ity emails. So although certain products lose out on unique users, the more relevant products gain. This is consistent with our observations in Section 3.2 where we had seen that user attention gets divided between important and less im-portant emails. Here we would like to point out once again that this kind of interaction between emails is ignored in our MOO formulation where we assume that the send/drop decision for an email does not affect the probability of a session or complaint from any other email. Relaxing this in-dependence assumption is the primary target of our future research. On a lighter note, the large drop in unique users visiting the HelpCenter pages is rewarding and also amusing, for reasons which should be obvious to the reader!
To the best of our knowledge, this is the first work on email volume optimization for a large online social network-ing service. We have introduced the challenging nature of the problem through a comprehensive cost-benefit analysis of email communication. It has been found that on the one hand emails deliver significant value to the members thereby driving in-app engagement, but on the other hand excessive email volume results in increased negative responses such as a member unsubscribing from an email type or reporting an email as spam to her email service provider. Excessive email has also been found to result in the user X  X  attention drifting away from important emails. We have illustrated why naive approaches to volume optimization result in sub-optimal so-lutions, and presented our formulation of the problem as a multi-objective optimization problem, with the objective of minimizing the number of emails sent in a way so as to maximize the positive outcome and minimize the neg-ative outcome of sending emails. We have discussed the challenges faced in solving the multi-objective optimization problem and our workaround. We have described how we evaluate our volume optimization models offline before de-ploying them in production. The major components of our volume optimization system, comprising of an online serv-ing system and an offline training system have been outlined. Finally, we have demonstrated the effectiveness of our ap-proach through online A/B test experiments.
This work would not have been possible without the in-valuable insights from our team at LinkedIn comprising of Ankan Saha, Carl Cummings, Deepak Agarwal, Kinjal Basu, Liang Zhang, Rishi Jobanputra, Shaunak Chatterjee and many others. [1] Sarah Goliger. 9 Critical Components for Optimized [2] Steve Whittaker and Candace Sidner. Email overload: [3] Catherine Grevet, David Choi, Debra Kumar, and [4] Gloria Mark, Stephen Voida, and Armand Cardello.  X  X  [5] Eric Horvitz, Andy Jacobs, and David Hovel.
 [6] Shinjae Yoo, Yiming Yang, Frank Lin, and Il-Chul [7] Shinjae Yoo, Yiming Yang, and Jaime Carbonell. [8] Deepak Agarwal, Bee-Chung Chen, Pradheep Elango, [9] Deepak Agarwal, Shaunak Chatterjee, Yang Yang, [10] SpamCop Blocking List. [11] Eric Chu, Brendan O X  X onoghue, Neal Parikh, and [12] Apache Spark. http://spark.apache.org/. [13] Chih-Jen Lin and Jorge J. Mor  X e. Newton X  X  method for
