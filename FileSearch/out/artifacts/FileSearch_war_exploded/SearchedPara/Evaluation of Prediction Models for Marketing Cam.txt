 We consider prediction-model evaluation in the context of marketing-campaign planning. In order to evaluate and compare models with specific campaign objectives in mind, we need to concentrate our attention on the appropriate evaluation-criteria. These should portray the model's ability to score accurately and to identify the relevant target population. In this paper we discuss some applicable model-evaluation and selection criteria, their relevance for campaign planning, their robustness under changing population distributions, and their employment when constructing confidence intervals. We illustrate our results with a case study based on our experience from several projects. Model Evaluation, Marketing Campaigns, Performance Measures, Confidence Intervals. When dealing with marketing applications, such as campaign management, the issue of evaluating prediction models is twofold. First, the evaluation has to be statistically sound, allowing us to compare models, choose among them and estimate their expected future performance. Second, and perhaps more important, we need to evaluate models with regard to the way they will be utilized from a business perspective. For example, suppose we are building a scoring model to predict voluntary churn (customer's propensity for disconnecting services) in order to identify the target population for a retention campaign. If in the campaign we intend to contact only the 2% of our customers who are at highest churn risk, it seems unreasonable to evaluate a suggested model using accuracy over a full test data set. The model's performance on 98% of the population is irrelevant to the campaign goal. [6] and [8], among others, present flexible and efficient techniques for evaluating models with regard to a wide variety of goal functions. However, we have found the statistical analysis of the most relevant scores for planning campaigns to be lacking, and have compiled an array of tools and techniques to fill the gaps. requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391 -x /01/08...$5.00 In this paper we discusses some of the approaches we take when evaluating model-performance in the context of campaign planning and executing. We also present statistical issues that arise when attempting to combine relevance and rigor in the evaluation process. The main results we present are:  X  Description of the requirements from appropriate evaluation  X  Methodology for applying some of the evaluation measures  X  A case study (Section 4), illustrating the importance and We begin our discussion at the point where a scoring model has been constructed. We disregard the method or algorithm that were used to create the model, and concentrate on the means for evaluating it, given the campaign objectives. A different approach would be to consider the objectives while constructing the model ([1] and [4]). Once we have a candidate model, we want to estimate its expected performance on unlabeled data. Our standard model evaluation methodology is: 1. Evaluate the models' performance on an independent test set 2. Adjust the models' score to fit the full population We focus our discussion on the performance measures, which are of interest for campaign planning and analysis, and their statistical properties. When planning a campaign, one seeks to identify individuals most likely to respond to the campaign. Due to budget restrictions the number of individuals to be approached in the campaign is limited. Thus there is a need for a good model for selecting the target segment and its performance on the rest of the population is measured by the amount ofresponders captured within the targeted population. This amount can be measured in two different ways:  X  How much better are we doing by using our model to select the target population relative to a random selection of the target population. This measure is known as the L/ft. For example: instead of reaching 2% of the responders when approaching randomly 2% of the population, we could reach 16% of the responders by approaching the top model-scored 2% of the population. In this ease we are improving the random model by 8 times, i.e. the lift at 2% is 8.  X  How frequently do we expect to encounter a responder when running our campaign? This measure is expressed by the 
Response Rate. For example: when carrying out a telemarketing campaign, we may be interested in knowing once in how many calls we should expect a responder. 
These two measures capture the essence of a models' usefulness for campaign planning, from two business perspectives. The measures are also mathematically equivalent but have a different behavior in the face of changing population distribution, as described in Section 2.2.1. 
Having established the need for adapted model evaluation in the context of campaign planning and mentioned some useful measures, we now concentrate on the statistical properties of evaluation measures, and consider their robustness in changing population distributions. We roughly divide them into two categories: overall performance measures and measures ealculated per cutoff points. The evaluation process commences with sorting all test entities according to their model-produced scores. This ranked list serves as the basis for calculation of all possible performance measures, together with the following terminology:  X  A, B -total number of responders and non-responders, respectively. * A j, By -total number of responders and non-responders, respectively, in thej-th top quantile.  X  j. (.4 +B) or (Aj + By ) -all cases in thej-th top quantile  X  A/(A +B) -overall response rate 2.2.1 Measures at Pre-Specified Cutoff Points Response Rate 
The Response Rate (RR) represents the responders' percentage you reach out of all the customers approached in a campaign. 
Customers approached in a campaign are thej-th top quantile and the RR is the response rate within that quantile: 
This measure is useful for calculating the expected profit from a campaign, however, it is extremely sensitive to the overall response rate. It drops almost linearly with the drop of the overall response rate in the population. Thus, models created based on populations with different response rates can not be compared using this measure, without applying an appropriate normalization. Furthermore, if the response rate in the "future" population on which a campaign is going to be run is unknown, there is no way to get a reliable estimate of the RR for that campaign. Lift 
The Lift measures the ratio between the RR and the overall response rate: 
The Lift shows directly how much better would be a campaign based on the model than a campaign based on a random selection. 
Thus it is also a useful measure for campaign evaluation, from a different perspective than RR. The Lift is somewhat sensitive to the overall response rate (it mildly increases as the overall response decrease), but much less than the RR. Despite this slight disadvantage it is an intuitive evaluation criterion and a common (and sensible) choice. Response Non-Response Ratio 
The Response to Non-Response Ratio (RNR) is the ratio between the percentage of all responders and the percentage of all non-respondersinthetopj-thquantile:RNR(j)=(+l/(-B~) 
Statistically the RNR is robust and independent of the overall response rate. Thus, it is easy to compare model performance with it. However, there is no intuitive way to define the RNR in terms campaign effectiveness. 
Table 1 summarizes the advantages and limitations of each measure. RR Extremely Sensitive Frequency of encountering a responder Lift Somewhat Sensitive Improvement over 
RNR Invariant No immediate interpretation 2.2.2 Overall measures 
In some cases it is difficult or undesirable to decide in advance on a specific size for the target population, or a specific model may be used for many campaigns. In such cases, it may be preferable to estimate the models' performance simultaneously with regard to a whole range of potential targets. Misclassification Rate responder or a non-responder, it is necessary to set a threshold score. The Miselassification Rate (MCR) is the percentage of entities classified incorrectly among all entities (an alternative suggested by [5] is the Misclassification Cost that weights costs into the error calculation). In the campaign-planning context the 
MCR is usually inappropriate since a campaign inherently focuses on some small sub-populations and not the entire population. 
Further discussion of its inadequacy can be found in [6]. Receiver Operating Characteristic (ROC) Curve In order to define the ROC Curve we first present the following definitions: Sensitivity -the percent ofresponders classified as responders. Specificity -the percent of non-responders classified as non-responders. When classifying, increasing the cutoff-point increases and decreases Specificity. ROC curve is a plot of the against 1-Specificity at many cut-points. The area under the curve (AUC) is a measure of a models' ability to separate responders from non-responders. When comparing two models by their ROC curves, we're actually comparing their RNR at all possible cutoff points simultaneously. Further discussion can be found in [6], who also introduce the ROC convex hull method, for comparing a large number of classifiers. Gain Chart A Gain Chart (a.k.a Cumulative Lift chart) is a graph displaying the proportion of all responders vs. the proportion of the population (the quantile) sorted according to the model scores. Had the population been sorted randomly we would have expected each quantile to include the same response proportion. Similarly to the ROC curve, a Gain Chart displays the Lift in all quantiles simultaneously. The area under the curve is a measure of the models' relative ability to identify responders. In the direct marketing domain this chart is sometimes referred to as the Curve since it expresses a similar notion as the "80/20 rule" [7]. The relationship between the two graphs ROC and Gain Chart are methods for evaluating the ranking performance of models, thus they make good criteria when the aim is to identify high or low tendency among the whole population. [8] demonstrates that the two diagrams are equivalent and presents several significance tests for the difference between AUCs that can be used to determine differences between overall performance of models. As with the RNR and the Lift, ROC Curve is good for comparing models, especially when response rate may vary, and the Gain Chart is good for evaluating campaign targeting effectiveness. much more balanced. Suppose we sort the TS according to the model-scores and calculate certain measures at each percentile. Due to the different response rates, TS percentiles do not correspond to percentiles in the FP. It is therefore necessary to translate the percentiles from the TS to the FP and we call this procedure the "Inverse Transformation". [9] introduce a similar issue -they account for the differences between the training-set consideration test-set distributions when measuring error rate. The following quantities are given prior to performing the transformation: A, B -the number of responders and non-responders in the FP, respectively. a, b -the number of responders and non-responders in the TS, respectively. a;, bi -the number of responders and non-responders in percentile i in the TS, respectively. to Each extrapolated percentile pair (-'It,/~i) does not add up to a FP percentile, so TS percentiles are merged or split in order to attain FP percentiles. Lets assume that instead of TS single pereantiles we are using accumulated TS percentiles and we want to transform them into accumulated FP percentiles. Let dO) be the number of top TS percentiles necessary to comprise the top j FP percentiles. Note that d(lO0) = 100 and that dO) might not be integer. The estimator for the number of responders and non-responders in the top j FP percentiles are: 
BJ = E B, = -ffbd(j) (2) Thus, the estimated lift for the FP at percentile j is and the estimated RR for the FP at percentile j is The performance measures, discussed in details in the previous section, are usually calculated on a test sample data set. These measures need to be adjusted to the full population, in case its distribution is different from the sample distribution used for training and test (the sample may be biased due to intentional under-sampling of the larger class). The transformation method is presented in Section 3.1. The next stage after the transformation is to calculate reliable predictions based on the performance measures for future data. We do that by building proper Confidence Intervals (CI's). Methods for building exact and approximate CIs are introduced in Section 3.2. We limit the discussion to the Lift and RR measures. Note that in the previous section lift was defined as (Aj/A)/j. Assuming j is fixed and to simplify the formula, this section refers to (A/A) as the lift. Models are usually evaluated on a test set (TS) put aside from the sample population. In many eases the response rate in the full population (FP) is very small but in the sample the 2 classes are Percentile point-estimators are not sufficient for evaluating the model predictive ability. When attempting to predict a model's performance on future data we also build confidence intervals. There are two main uses for CI's in the context of model evaluation:  X  The Lower Bound (LB) of a one-sided CI can be used to give  X  A two-sided CI for the difference between scores of two Here we present the generation of one-sided CI's for a single model, but the extensions to two-sided CI's are trivial. The proportions we are interested in estimating are: Lift: p~.l) = _~ (3) In Section 3.2.1 we discuss the exact distribution of the estimators and in Section 3.2.2 we propose alternative distribution approximations to use when constructing CIs for these proportions. Section 3.2.3 compares the different approaches with empirical results. Our aim is to construct CIs for the lit~ and the RR based on the sample lift and sample RR. Sample Lift: /30) = ad (5) know the distribution ofa a . Since we're not dealing with infinite populations but rather the total test population sizes a and b are given, we are in a "hyper-geometric"-like setting. The hyper-geometric (HG) distribution is inappropriate, of course, since it inherently assumes we are selecting a sub-population at random. We are selecting a sub-population according to our model, which we hope and assume is non-random. What we need is a "biased hyper-geometric" (BHG) distribution: Which would have the mean a. p~)) compared to a. d in the HG distribution, and variance approximately a.p~.O(1-p~))), representing our knowledge of the overall sums a and b. We have not found an appropriate distribution in the statistics literature. We are currently working on formulating such distribution and consider this an interesting research issue. Since the formula of the exact distribution is not known explicitly, in practice we can use either Binomial or HG "like" approximations for constructing CIs. Lift Confidence Interval From (1) and (5): /3 (.1) = a---La = ~j Thus, we can directly calculate the LB of a one-sided CI for the lift (using)~)) instead of the unknown p~.l)in the variance): Response Rate Confidence Interval The sample RR (6) can also be expressed as following: The LB of a matching one-sided CI is: But we are interested in a different proportion: from (9): b a = 1 -)(:2) so from (9) and (11): )(2*) = 1 )5 2) --^(2) b(2*) is a monotonic increasing function of) (2) , therefore it is 
J possible to calculate a LB for ~,j-(2*) based on the LB for p~2) That is the proper way for constructing this CI, since our uncertainty is at the TS level. Thus, the CI should be calculated based on the TS quantities and then "inverse transformed" to the FP. The Connection between the two Binomial CI's 
The sample lift and the sample RR can each be represented as a monotonic increasing function of the other. It is therefore possible to calculate the CI for each based on the CI of the other. Under certain conditions, doing that might result in a shorter CI. We RR CI, but it obviously works in the opposite direction as well. 
From (5) and (6): )~.1)=g()~2)) d(a+b)^(2) (13) = From (13) and (10): Comparing (6) and (12): The altematweLB~, l) will achieve a shorter CI under the following condition: Obviously, this shift from one proportion to another would produce the same LB if we used the exact distribution. However, since the binomial approximations are conservative (with larger variance), we believe that given a precentile, it's acceptable to choose the method that yields a shorther CI. When applying the normal approximation, a less conservative approach would be, to use the HG-like approximate variance instead of the binomial variance: 
Just as demostrated for the binomial based CIs we construct parallel HG based CIs LB~. D and LB~ 2.) for the Lift and the RR, respectively. where In this case constructing a LB for the lift based on the RR (and vice versa) will produce the same LB as constructing it directly. Our experience shows that the CIs built based on approximations are generally satisfactory. Furthermore, we made an experiment with real data that gave an idea of the quality of the practical methods we suggest. We implemented bootstrap sampling [2] and compared the various CIs to the empirical bootstrap distribution. Table 2 displays the Lift 99% CI-LB based on the binomial approximation directly (B), via the RR (B*), and based on the HG-like approximation (HG). These LBs are compared to the 1% bootstrap quantile statistic. For example, at the 3 rd percentile the Lift LBs obtained by the B, B* and HG approximations are 0 X 170, 0.239, 0.234, respectively, and all three are more conservative than the Lift LB obtained by the bootstrap -0.278. The experiment conclusions are:  X  All methods generated very close LBs -not seen in Table 2,  X  For most percentiles, at least 99% of the lift observations This work was done by the Data Mining group, which is part of the R&amp;D Business Insight unit at Amdocs Ltd. Amdocs is a leading provider of CRM, Billing and Order Management solutions to the communications and IP industry worldwide. The Amdocs Business Insight TM suite includes solutions for customer retention, reduction of bad debt, credit scoring, collection optimization, customer profitability analysis, sales analysis (cross/up sell) and relationship optimization. In this section we describe, our experience in projects where we have found some of the concepts described in this paper to be useful. The type of model we consider is a prediction model for a retention campaign, in which responders are potential chumers and the overall response rate is the overall chum rate. The performance of a suggested new model was compared to that of a legacy model X  Initially the legacy model's RR at 10% was 2.75 times better than the new model and thus it was concluded (mistakenly) that the legaey's prediction is better. However, when the actual evaluation process was investigated, it turned out that the two models were evaluated based on different test populations. The population used to evaluate the legacy's model had a 4.5 times higher overall chum rate. Given that fact, the RR was not the appropriate comparison criterion, since it is biased in favor of the model evaluated based on a population with higher chum rate. We considered the lift as an alternative, since we use it often. When we calculated the lilt, it turned out that the new model's litt was 1.62 times higher. Yet, as stated previously this measure too is not completely objective and independent of the population distribution. The lift for a smaller chum rate population is slightly higher. In order to compare the legacy and the new model, we had to use a more robust measure so we chose the RNR, which for the new model was 1.57 times greater than the legacy model. Table 3 summarizes the values of each measure for each model. 
To demonstrate the principle difference between the measures we introduce a model we built for chum prediction. For this single model we calculate each measure and display its value after performing the inverse transformation (twice according to two different chum rates). Figure 1 demonstrates the extreme sensitiveness of the RR, the relative robustness of the lift, and the high robustness of the RNR. 
RR, Lift and RNR are portrayed vs. the ranked population (by percentiles). The two lines show the same model results transformed 
In this paper we discussed a few model-evaluation criteria, their robustness under changing population distributions, and their relevance for campaign planning. We demonstrated how problematic a comparison based on a non-robust measure like RR may be, and commended the use of the L/fl and RNR measures. 
Still in many cases RR is what the user expects. Discussing the robustness of a measure is usually not the best strategy for convincing the end-user which model to choose, therefore we introduced the inverse transformation. Inverse transformation, which was previously described for the purpose of transforming test-set class-distribution into full-population distribution, helps create a uniform presentation. After performing the proper transformation, it is possible and correct to compare models (with no bias) based on the RR. Such uniform presentation would be more intuitive for decision-makers. We would like to thank Gadi Pinkas, Aron Inger and Isabel 
Sasoon of Amdocs (Israel) Ltd. for their counseling. [ 1 ] Bhattacharyya, S. (1998). Direct Marketing Response [2] Eft'on, B. and Tibshirani, R. (1993). Introduction to the [3] Ling, C. X.; Li, C. (1998). Data Mining for Direct [4] Masand, B. and Piatetsky-Shapiro, G. (1996). A Comparison [5] Pazzani, M.; Merz, \c.; Murphy, P.; Ali, K.; Hume, T. and [6] Provost, F. and Fawcett, T. (1997). Analysis and [7] Roberts, M. L. and Berger, P. D. (1999). Direct Marketing [.~ Rosset, S. (1999). Ranking -Methods for Flexible [.~ Weiss, G. M. and Provost, F. (2001). The Effects of Class 
