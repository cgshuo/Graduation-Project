 Value function representations are dominant in algorithms for dynamic programming (DP) and rein-forcement learning (RL). Ho we ver, linear programming (LP) methods clearly demonstrate that the value function is not a necessary concept for solving sequential decision making problems. In LP methods, value functions only correspond to the primal formulation of the problem, while in the dual kno wn LP duality , dual representations have not been widely explored in DP and RL. Recently , we Unfortunately , [4] did not analyze the con vergence properties nor implement the proposed ideas. In this paper , we investig ate the con vergence properties of these newly proposed dual solution tech-niques, and sho w how the y can be scaled up by incorporating function approximation. The proof techniques we use to analyze con vergence are simple, but lead to useful conclusions. In particular , even in the presence of function approximation and off-polic y updating. The dual approach appears to hold an adv antage over the standard primal vie w of DP/RL in one major sense: since the funda-mental objects being represented are normalized probability distrib utions (i.e., belong to a bounded oscillation) in the very circumstance where primal updates can and often do diverge: gradient-based off-polic y updates with linear function approximation [5, 6]. We consider the problem of computing an optimal beha vior strate gy in a Mark ov decision process vector r and a discount factor , where we assume the goal is to maximize the infinite horizon discounted reward r strate gy can always be expressed by a stationary policy , whose entries of taking action a in state s . Belo w, we represent a polic y by an equi valent representation as an j S j j S jj A j matrix where that the matrix product P gives the state-to-state transition probabilities induced by the polic y in the environment P , and that P gives the state-action to state-action transition probabilities induced by polic y in P . The problem is to compute an optimal polic y given either (a) a complete to the environment through observ ed states and rewards and the ability to select actions to cause Traditionally , DP methods for solving the MDP planning problem are typically expressed in terms of the primal value function. Ho we ver, [4] demonstrated that all the classical algorithms have natural duals expressed in terms of state and state-action probability distrib utions.
 vector q = action polic y evaluation, one considers the linear system d &gt; = (1 ) &gt; + d &gt; P , where is one considers the follo wing j S jj A j j S jj A j matrix H = (1 ) I + P H . The matrix H that entries H (1 ) q = H r . That is, given H we can easily reco ver the state-action values of .
 For polic y impro vement, in the primal representation one can deri ve an impro ved polic y 0 via the update a ( s ) = arg max of the polic y update can be expressed in terms of the state-action matrix H for is a ( s ) = We first investig ate whether dynamic programming operators with the dual representations exhibit swered in the affirmati ve. In the tab ular case, dynamic programming algorithms can be expressed by equation. Consider two standard operators, the on-polic y update and the max-polic y update. For a given polic y , the on-polic y operator O is defined as for the primal and dual cases respecti vely . The goal of the on-polic y update is to bring current representations closer to satisfying the polic y-specific Bellman equations, but instead applies a greedy max update to the current approximations where [ q ] Bellman equations q = r + P [ q ] and H = (1 ) I + P 4.1 On-policy con vergence case, by establishing a contraction property of O with respect to a specific norm on q vectors. In particular , one defines a weighted 2-norm with weights given by the stationary distrib ution deter -mined by the polic y and transition model P : Let z 0 be a vector such that z &gt; P = z &gt; ; that is, z is the stationary state-action visit distrib ution for P . Then the norm is defined as and kO q of
O in the space of vectors q . Therefore, repeated applications of the on-polic y operator con verge to a vector q such that q = O q ; that is, q satisfies the polic y based Bellman equation. Analogously , for the dual representation H , one can establish con vergence of the on-polic y oper -ator by first defining an approximate weighted norm over matrices and then verifying that O is a contraction with respect to this norm. Define satisfies the triangle inequality . This weighted 2-norm is defined with respect to the stationary distrib ution z , but also the reward vector r . Thus, the magnitude of a row normalized matrix is determined by the magnitude of the weighted reward expectations it induces.
 as the primal case. We can have k P H k Moreo ver, the on-polic y operator is a contraction with respect to k k Lemma 1 kO H Proof: kO H Thus, once again by the contraction map fix ed point theorem there exists a fix ed point of O among row normalized matrices H , and repeated applications of O will con verge to a matrix H such that O H = H ; that is, H satisfies the polic y based Bellman equation for dual representations. This argument sho ws that on-polic y dynamic programming con verges in the dual representation, without making direct reference to the primal case. We will use these results belo w. 4.2 Max-policy con vergence case, but involv es working with a dif ferent norm. Instead of considering a 2-norm weighted by the visit probabilities induced by a fix ed polic y, one simply uses the max-norm in this case: k q k be easily established in the primal case: kM q on-polic y case, contraction suf fices to establish the existence of a unique fix ed point of M among vectors q , and that repeated application of M con verges to this fix ed point q such that M q = q . To establish con vergence of the off-polic y update in the dual representation, first define the max-norm for state-action visit distrib ution as Then one can simply reduce the dual to the primal case by appealing to the relationship (1 ) M q = M H r to pro ve con vergence of M H .
 Lemma 2 If (1 ) q = H r , then (1 ) M q = M H r .
 Proof: (1 ) M q = (1 ) r + P [(1 ) q ]) = (1 ) r + P [ H r ] = (1 ) r + P Thus, given con vergence of M q to a fix ed point M q = q , the same must also hold for M H . Ho we ver, one subtlety here is that the dual fix ed point is not unique. This is not a contradiction because the norm on dual representations k k That is, the relationship between H and q is man y to one, and several matrices can correspond to the same q . These matrices form a con vex subspace (in fact, a simple x), since if H and H restricted to 0 1 to maintain nonne gativity . The simple x of fix ed points f H : M H = H g is given by matrices H that satisfy H r = (1 ) q . Primal and dual updates exhibit strong equi valence in the tab ular case, as the y should. Ho we ver, when we begin to consider approximation, dif ferences emer ge. We next consider the con vergence properties of the dynamic programming operators in the conte xt of linear basis approximation. We focus on the on-polic y case here, because, famously , the max operator does not always have a fix ed point when combined with approximation in the primal case [8], and consequently suf fers the risk of divergence [5, 6].
 Note that the max operator cannot diverge in the dual case, even with basis approximation, by boundedness alone; although the question of whether max updates always con verge in this case remains open. Here we establish that a similar bound on approximation error in the primal case can be pro ved for the dual approach with respect to the on-polic y operator .
 In the primal case, linear approximation proceeds by fixing a small set of basis functions, forming a j S jj A j k matrix , where k is the number of bases. The approximation of q can be expressed by a linear combination of bases ^ q = w where w is a k 1 vector of adjustable weights. This is to
H can be expressed as vec( ^ H ) = w , where the vec operator creates a column vector from a matrix by stacking the column vectors of the matrix belo w one another , w is a k 1 vector of To ensure that ^ H remains a nonne gative, row normalized approximation to H , we simply add the where the operator is the Kroneck er product.
 approximations stay representable in the given basis. Then we consider their composition with the on-polic y and off-polic y updates, and analyze their con vergence properties. For the composition of the on-polic y update and projection operators, we establish a similar bound on approximation error in the dual case as in the primal case. 5.1 Pr ojection Operator Recall that in the primal, the action value function q is approximated by a linear combination of bases in . Unfortunately , there is no reason to expect O q or M q to stay in the column span of , so a best approximation is required. The subtlety resolv ed by Tsitsiklis and Van Ro y [7] is to identify a particular form of best approximation X  X eighted least squares X  X hat ensures con vergence combined update operator is not guaranteed to be the best representable approximation of O  X  X  fix ed representable approximation.
 We summarize a few details that will be useful belo w: First, the best least squares approximation is computed with respect to the distrib ution z . The map from a general q vector onto its best approxi-mation in col span() is defined by another operator , P , which projects q into the column span of , P q = argmin erator in k k theorem. Approximate dynamic programming then proceeds by composing the two operators X  X he on-polic y update O with the subspace projection P  X  X o compute the best representable approxima-tion of the one step update. This combined operator is guaranteed to con verge, since composing a non-e xpansion with a contraction is still a contraction, i.e., k q Linear function approximation in the dual case is a bit more complicated because matrices are being represented, not vectors, and moreo ver the matrices need to satisfy row normalization and nonne g-ativity constraints. Ne vertheless, a very similar approach to the primal case can be successfully applied. Recall that in the dual, the state-action visit distrib ution H is approximated by a linear combination of bases in . As in the primal case, there is no reason to expect that an update lik e O H should keep the matrix in the simple x. Therefore, a projection operator must be constructed that determines the best representable approximation to O H . One needs to be careful to define this projection with respect to the right norm to ensure con vergence. Here, the pseudo-norm k k defined in Equation 1 suits this purpose. Define the weighted projection operator P over matrices The projection could be obtained by solving the abo ve quadratic program. A key result is that this projection operator is a non-e xpansion with respect to the pseudo-norm k k Theor em 1 kP H k Proof: The easiest way to pro ve the theorem is to observ e that the projection operator P is really a composition of three orthogonal projections: first, onto the linear subspace span( ) , then onto the subspace of row normalized matrices span( ) \ f H : H 1 = 1 g , and finally onto the space of nonne gative matrices span( ) \ f H : H 1 = 1 g \ f H : H 0 g . Note that the last projection into the nonne gative halfspace is equi valent to a projection into a linear subspace for some hyperplane tangent to the simple x. Each one of these projections is a non-e xpansion in k k a generalized Pythagorean theorem holds. Consider just one of these linear projections P Since the overall projection is just a composition of non-e xpansions, it must be a non-e xpansion. As in the primal, approximate dynamic programming can be implemented by composing the on-polic y update O with the projection operator P . Since O is a contraction and P a non-e xpansion, case, this fix ed point is only unique up to H r -equi valence, since the pseudo-norm k k distinguish H of equi valent solutions. For simplicity , we denote the simple x of fix ed points for P O by some representati ve H to the primal bound, which bounds the approximation error between H approximation to the on-polic y fix ed point H = O H .
 Theor em 2 k H Proof: First note that k H kP H H k z ; r by generalized Pythagorean theorem. Then since H + = P O H + and P is a non-e xpansion operator , we have k H Finally , using H = O H and Lemma 1, we obtain kO H operators do not preserv e the tight relationship between primal and dual updates. That is, even if The most obvious dif ference comes from the fact that in the dual, the space of H matrices has bounded diameter , whereas in the primal, the space of q vectors has unbounded diameter in the natural norms. Automatically , the dual updates cannot diverge with compositions lik e P O and P M ; 5.2 Gradient Operator In lar ge scale problems one does not normally have the luxury of computing full dynamic pro-gramming updates that evaluate complete expectations over the entire domain, since this requires kno wing the stationary visit distrib ution z for P (essentially requiring one to kno w the model of the MDP). Moreo ver, full least squares projections are usually not practical to compute. A key in-termediate step toward practical DP and RL algorithms is to formulate gradient step operators that only approximate full projections. Con veniently , the gradient update and projection operators are to the degree that divergence is a common phenomenon (much more so than with full projections). Composing approximation with an off-polic y update (max operator) in the primal case can be very dangerous. All other operator combinations are better beha ved in practice, and even those that are not kno wn to con verge usually beha ve reasonably . Unfortunately , composing the gradient step with an off-polic y update is a common algorithm attempted in reinforcement learning (Q-learning with function approximation), despite being the most unstable.
 are probability distrib utions. We start by considering the projection objecti ve The unconstrained gradient of the abo ve objecti ve with respect to w is follo wed directly because we need to maintain the constraints. The constraint w &gt; 1 = 1 can be maintained by first projecting the gradient onto it, obtaining w = ( I 1 weight vector can be updated by where is a step-size parameter . Then the gradient operator can then be defined by (Note that to further respect the box constraints, 0 h 1 , the stepsize might need to be reduced and additional equality constraints might have to be imposed on some of the components of h that are at the boundary values.) programming update, this gives the composed updates Thus far, the dual approach appears to hold an adv antage over the standard primal approach, since con vergence holds in every circumstance where the primal updates con verge, and yet the dual up-dates are guaranteed never to diverge because the fundamental objects being represented are nor -malized probability distrib utions (i.e., belong to a bounded simple x). We now investig ate the con-vergence properties of the various updates empirically . To investig ate the effecti veness of the dual representations, we conducted experiments on various domains, including randomly synthesized MDPs, Baird X  s star problem [5], and on the mountain car problem. The randomly synthesized MDP domains allo w us to test the general properties of the algorithms. The star problem is perhaps the most-cited example of a problem where Q-learning with linear function approximation diverges [5], and the mountain car domain has been prone to divergence with some primal representations [9] although successful results were reported when bases are selected by sparse tile coding [10].
 For each problem domain, twelv e algorithms were run over 100 repeats with a horizon of 1000 steps. primal and the dual. The discount factor was set to = 0 : 9 . For on-polic y algorithms, we measure determined fix ed-point. For off-polic y algorithms, we measure the dif ference between the values updates was 0 : 1 for primal representations and 100 for dual representations. The initial values of visit distrib utions H are chosen uniformly randomly with row normalization. Since the goal is to investig ate the con vergence of the algorithms without carefully crafting features, we also choose random basis functions according to a standard normal distrib ution for the primal representations, and random basis distrib utions according to a uniform distrib ution for the dual representations. Randomly Synthesized MDPs. For the synthesized MDPs, we generated the transition and re-ward functions of the MDPs randomly X  X he transition function is uniformly distrib uted between 0 and 1 and the reward function is dra wn from a standard normal. Here we only reported the results of random MDPs with 100 states, 5 actions, and 10 bases, observ ed consistent con vergence of the dual representations on a variety of MDPs, with dif ferent numbers of states, actions, and bases. In with the circle mark er) blo ws up (di verges), while all the other algorithms in Figure 1 con verge. the approximate error of the corresponding primal algorithm P O q ( 4 : 23 10 2 ), even though their theoretical bounds are the same (see Figure 1(left)). Figure 1: Updates of state-action value q and visit distrib ution H on randomly synthesized MDPs The Star Pr oblem. The star problem has 7 states and 2 actions. The reward function is zero for each transition. In these experiments, we used the same fix ed polic y and linear value function state-action visit distrib ution matrix H are uniformly distrib uted random numbers between 0 and 1 with row normalization. The gradient off-polic y update in the primal case diverges (see the dotted algorithms con verge. The Mountain Car Pr oblem The mountain car domain has continuous state and action spaces, which we discretized with a simple grid, resulting in an MDP with 222 states and 3 actions. The number of bases was chosen to be 5 for both the primal and dual algorithms. For the same reason as before, we chose the bases for the algorithms randomly . In the primal representations with linear function approximation, we randomly generated basis functions according to the standard normal distrib ution. In the dual representations, we randomly pick ed the basis distrib utions according to is also considerably smaller than P O q ( 3 : 26 10 2 ) in the primal.
Figure 3: Updates of state-action value q and visit distrib ution H on the mountain car problem Dual representations maintain an explicit representation of visit distrib utions as opposed to value functions [4]. We extended the dual dynamic programming algorithms with linear function ap-proximation, and studied the con vergence properties of the dual algorithms for planning in MDPs. We demonstrated that dual algorithms, since the y are based on estimating normalized probability distrib utions rather than unbounded value functions, avoid divergence even in the presence of ap-proximation and off-polic y updates. Moreo ver, dual algorithms remain stable in situations where standard value function estimation diverges.

