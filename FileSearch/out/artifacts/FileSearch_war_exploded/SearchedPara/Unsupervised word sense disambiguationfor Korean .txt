 1. Introduction
The ambiguity of a word sense can be defined as a natural phenomenon in which a lexical form has more than two senses. WSD is meant to solve this ambiguity of a word sense, that is to say, to assign the appro-words, but it is very important in the application fields of natural language processing such as machine translation (MT) and information retrieval (IR); for example, we should choose the word of a target lan-guage corresponding to a polysemous word in a source context for MT, and the appropriate sense of poly-semous words in a query in order to retrieve accurate information from a text for IR. Despite the importance of WSD, the WSD system is rarely used in application fields, for the system learns only a few target words and the performance of the system is very low. In addition to this, it takes long to imple-ment the WSD system because of the need to create sense-labeled examples and construct a thesaurus: this is referred to as a knowledge acquisition bottleneck .

Therefore, we present a way to construct a WSD system, which can be easily implemented by learning all polysemous words at once, while covering all polysemous words which are listed in MRD, and escape from a knowledge acquisition bottleneck , while showing the appropriate accuracy. 2. Related work
Several techniques for WSD have been reported. It has been common to use two kinds of resources: a dictionary and corpora. The first resource, a dictionary, is chosen based on the premise that the headwords of a dictionary are closely associated with their corresponding sense definition. Lesk (1986) used the number of common words among the sense definition of a polysemous word and the sense definitions of its context words. Wilks et al. (1990) defined the related words as frequently co-occurring words with the words in a sense definition of an MRD. After extending the related words, Wilks et al. (1990) used the number of the common words which are shared between the related words of a sense def-inition and words in a context. Yarowsky (1995) extracted the decision list from corpora automatically using sense definitions of an MRD, and proposed the bootstrapping method for applying iteratively the decision list.

Another resource for WSD is corpora. Corpus-based methods are categorized into two types: supervised learning and unsupervised learning. Brown, Della Pietra, and Mercer (1991) and Gale, Church, and Yarow-sky (1993) used bilingual corpus for WSD. Yarowsky (1992) disambiguated polysemous words by the
Bayesian categorizing method using a Roget  X  s thesaurus. Pedersen and Bruce (1997) regarded the WSD as a categorization problem and tried to solve the problem using several categorizing algorithms. Wilks and Stevensen (1997) proposed a practical model for WSD using POS, field information, selection restric-tion information, co-occurrence, and sense definition.

In the case of Korean WSD, Park (1997) collected relational words using statistical methods, and then used the number of the words which are commonly shared between the relational words of its context and those of a target word. Park (1997) reported the performance of the system as 77.97% precision on six tar-get words. Cho (1998) used both the distribution of nouns and that of verbs which limit the sense of nouns.
For relieving the data sparseness problem, Cho (1998) proposed corpus normalization, which converts all passive sentences to their corresponding active sentences. Cho (1998) reported that 80% recall and 80% pre-cision on C23 corpus were logged with 10 target words after corpus normalization. In addition, Cho (1998) showed that the performance of the system linearly increased according to the size of a corpus. Lee (1999b) proposed the classification information model composed of a feature extractor, a learner, and a classifier. individual decisions and their weights, and the classifier determines the proper sense of a polysemous word within an input by means of the set of classification information. Lee (1999b) reported that the proposed method resulted in 88.06% on 16 target words on average. 3. Learning step
The goal of the learning step is to gain two resources which are a similarity matrix from a corpus and vector representations of sense definitions from an MRD. Lee (1999a) constructed a useful knowledge base with an MRD,  X  -WooriMalKeunSajeon  X  .
 Fig. 1 shows the architecture of the learning step.

The first step is for acquiring co-occurrence probabilities from a POS tagged corpus. The POS tagged corpus is automatically generated by Sogang POS tagger. The Sogang POS tagger assigns the most appro-priate POS on each morpheme after analyzing each eojeol into morphemes in sentences. An eojeol is a token unit separated by white space in Korean. Due to the automatic POS tagging process, the POS tagged cor-pus in the learning stage holds some POS tagging errors. The co-occurrence probabilities from the POS tagged corpus are transformed into the form of the matrix. This is called the similarity matrix. This matrix suggests a way to represent all words with their own vector; furthermore, the vector representations of words can provide a method to transform all sense definitions in MRD into vectors. As a result, two resources, similarity matrix and vector representations of sense definitions are obtained. 3.1. Similarity matrix
In order to establish the similarity matrix, we should retrieve co-occurrence probabilities of word pairs from a corpus. A corpus can be explained as a pool for retrieving statistical information. For our system, we used Chosun-Illbo newspaper from 1996 to 1997 as a training corpus. Table 1 shows the composition of the training corpus.

In case of Korean WSD, we used only three POSs such as verb, noun, and adjective, because most adverbs are derived from adjective; few essential adverbs exist. Thus, we only consider those three POSs; verb, noun, and adjective.

Based on the training corpus, we calculated all co-occurrence probabilities for all word pairs. A co-occurrence probability denotes the probability that two different words will occur simultaneously in a sentence. Eq. (1) implies the co-occurrence probability.
 X co-occurring with word Y in a sentence.

Eq. (1) gives us the matrix for word pairs. The matrix for co-occurrence probability is conceptually shown in Fig. 2 .

We denote j w j as the number of words in the training corpus; in particular, f ( w This matrix is symmetric and all diagonal elements are one.

The matrix for the co-occurrence probability has the data sparseness problem, so we used Park (1997) method for that problem. Simply Park  X  s method is summarized as an n -step contextual similarity. Park (1997) used the property of the transitivity among words; Park (1997) determined the relation of w w considering the relation of w i and w k and that of w k and w Fig. 3 shows the conceptual image of an n -level contextual relationship.

In particular, the notation, S 1 , denotes a 1-level contextual similarity. Just as S contextual similarity. In the course of calculating S m between w the m -level contextual chain, which indicates the minimum value among S
S ( w m 1 , w j ). Because there exist as many as m -level contextual chains between w determined by the larger value between the contextual similarity of ( n 1)-step and that of n -level.
After reducing the data sparseness problem using an n -step contextual similarity, we can obtain the similarity matrix, with which we can convert each word into its own vector. 3.2. Word vector
The similarity matrix for word pairs supplies us with a way to represent each word by a vector. We used the vector space in order to transform each word into its own vector.
 Each word in a vector space is represented as the co-occurrence value with other words, as shown in
Fig. 4 . If any two words have a similar co-occurrence pattern, the meanings of these words seem to be sim-ilar. The similarity between two words follows up the inner product value of two vectors which represent the words.

For making word vectors, the dimension of vector space should be determined; in particular, highly fre-quent words in the training corpus are selected for the dimension. For implying f ( w f ( w j w j ), where f ( w x ) is a frequency of a word w ilarity matrix by the dimension, w 1 , w 2 , ... , w j F j
Fig. 5 shows the conceptual image of word vectors from a similarity matrix. These word vectors supply a method to represent the sense definitions in an MRD with their own vector. 3.3. Vector representations of sense definitions in MRD
Vector representations of sense definitions in an MRD can be determined by a combination of word vectors; that is to say, each vector of words occurring in a sense definition of MRD can be combined by
Eq. (2) . definition; thus, all of sense definitions are converted into vectors. 4. Disambiguation step
The disambiguation step is composed of two phases: constructing the AWD and assigning the most appropriate sense to a polysemous word. In the disambiguation step, the input sentences not used in the previous learning step are provided. We should choose k context words in an input sentence for efficiently disambiguating a polysemous word. By using k context words and the target word, the system processes an input sentence by building an acyclic weighted digraph over its words senses with each edge weighted by the cosine between the sense vectors. The system finds the optimal path through the AWD using the Viterbi algorithm, and thus the optimum set of word senses. 4.1. The definition of acyclic weighted digraph our system defines that a graph, G 0 consists of two sets, V which represent sense categories of words. E 0 is a set of pairs of vertices; in particular, E as the directed graphs having no directed cycles. Two nodes i and j are adjacent in G if category of the ( t + 1)th word in an input word sequence. In addition to this, each edge contains its own weight on tran-sition, and G 0 does not have any directed cycles. Assuming START and END are just dummy nodes for representing the start of a sentence and the end of a sentence respectively, all weights on the edges from START to all the sense categories of first word in the input word sequence are 1; in the same way, all weights on the edges from all the sense categories of the last word in input word sequence to END are also 1. 4.2. Selecting context words
Prior to constructing the AWD, we should select context words in an input sentence. The context words are the most useful words in a sentence for disambiguating a target word. These words can be determined by the similarity matrix. We selected k context words in a context by means of the similarity matrix; for instance, when given I deposit some money in the bank , assuming we disambiguate the sense of bank , the context words can be both deposit and money using the similarity matrix. In the case of the previous exam-ple, the k words which have a high similarity with bank are selected as the context words. We used k as 2, for depicting the conceptual model of the AWD in Fig. 6 . 4.3. Constructing the acyclic weighted digraph
The AWD is built with a target word and the k context words. Each node of the AWD is represented by each sense definition vector of the target word and that of k context words; furthermore, the edges between two nodes exist only on the transition from the i th sense definition of the t th word in the input word sequence and the j th sense category of the ( t + 1)th word. Fig. 6 provides a conceptual construction for the AWD.

Each weight on the edges is calculated using the cosine similarity, Eq. (3) . This equation just implies the weight of transition from one node to another. x and y indicate the sense definition vector. 4.4. Searching the optimal path on the acyclic weighted digraph
The Viterbi algorithm is well-known as the searching algorithm using dynamic programming skill, so we used the Viterbi algorithm with only modifying probability computation steps. Actually, Viterbi algorithm uses the value of the transition probability multiplied by observation probability in hidden Markov model (HMM). In order to use the original Viterbi algorithm, we should have architected the system by a super-vised learning; we could not calculate the observation factor because the training corpus does not have any sense-labels. Because of the unlabeled training corpus, we devised a method for searching the optimal path using a Viterbi algorithm which ignores the observation factors. Even though observation factors are missed, the computation complexity is exactly the same with the original Viterbi algorithm. Fig. 7 indicates the Viterbi algorithm applied to our system ( Allen, 1994 ).
 As illustrated in Fig. 7 , we did not consider the observation factor for calculating SEQSCORE . 5. Experiment 5.1. Data set
In order to test the performance of our system, we built test data sets with the most semantically ambig-uous Korean words; the target words are consisted of four nouns ( ( Bae ), ( Gogae ), ( Jeongi ), ( Sagi )), and two verbs ( ( Bbajida ), ( Tada )). They have 3.83 senses per target word on average.
For the test data sets, we randomly collected POS error-free 869 sentences from Chosun-Illbo newspaper from 1996 to 1997; therefore, our data set might be biased to one sense which is commonly used in news area. Certainly, sentences for testing are not supplied in the learning step.

However, Senseval , the international organization devoted to the evaluation of WSD systems, provides some data sets for evaluation; the test set for Korean is composed of 10 noun words and each target word has 4.8 senses on average. It provides 574 samples for evaluation over 10 target words. How-ever, it is somewhat difficult to evaluate our system on the Korean data set of Senseval. First, our sys-tem is completely devised for disambiguating senses of a polysemous word not occurring in a group of sentences but occurring in a sentence. In other words, the unit for an input of our system is only one sentence, while the Senseval data set suggests us a group of sentences for Korean. More serious problem is that the morpheme separation results for a lexical item of Senseval data set are different with those of our POS tagger. For example of Senseval data, the target word  X  ( mal )-saying  X  of  X  + ( mal + sseum )-respectful expression of saying  X  cannot be evaluated, because our morpheme analyzer cannot pro-duces  X  ( mal )+ ( sseum )  X  for the given word X   X  ( malsseum )  X  . Thus, our system cannot understand the sense of  X  mal  X  of the  X  malsseum  X  , and our system searches only the meaning of  X  malsseum  X  in the
MRD. Finally, Senseval data for Korean is somewhat deficient in that it provides only 57.4 samples per target word compared to 144.8 samples per target word in our test set. In these reasons, our system cannot be exactly evaluated on the Senseval data set, so we had no choice but to create our own test set for our system. 5.2. Compared systems
Our method is compared with both a system to choose the most frequent sense in a test data and a system with a Lesk algorithm. In the WSD field, any perfect method for comparing with other systems has not been suggested yet. Because of this, a commonly used method for comparison of the performances is using both a system to choose the most frequent sense in a test data set and a system with a Lesk algorithm.

Especially, the original Lesk algorithm disambiguates words in short phrases ( Banerjee &amp; Pedersen, 2002 ). Given a word to disambiguate, the dictionary definition or gloss of each of its senses is compared number of words in common with the glosses of the other words. The algorithm begins anew for each word and does not utilize the senses it previously assigned. 5.3. System results
In the course of disambiguating all target words in actual sentences, the number of context words, k , should be determined. Fig. 8 reveals the results of some experiments for assuring the number of context words in a sentence; Fig. 8 shows how many context words cause the best performance. We evaluated the performances of our system, average accuracy, over all target words under the same condition varying the number of context words, k , from 1 to 5. The accuracy means the ratio of the number of matched exam-ples and the number of given examples. The accuracy is calculated by Eq. (4) . T is denoted by the number of examples, and C by the number of matched examples.
 When we used three context words, the system showed the best performance.

Table 2 illustrates the performance of our system under the condition of k = 3, in which the best perfor-mance of our system is obtained. Table 2 is showing both accuracy per each sense of target words and accu-racy per each target word.

The performance of our system over all the target words shows 76.4% accuracy. By accuracy, our system performs 10% better than a system choosing a high frequent sense, and about 30% better by accuracy than the Lesk algorithm.

As shown in Table 2 , the performance of the system choosing the most frequent sense is relatively good proposed system, although the degree of the ambiguity of  X  ( Tada )  X  is seven. However, the test sentences of another domain may cause the performance of the baseline to be low.

The performance of the Lesk algorithm on our test set is even poorer than the system to choose the most frequent sense. This is because the Lesk algorithm did not consider any relation between each sense of context words and the senses of the target word; that is to say, because the Lesk algorithm considers all sense definitions for each context word, it ignores the relation between each sense of each context word and the sense of the target word. In addition to this, the Lesk algorithm tends to prefer the sense which has long description, so in case of  X  ( tada )  X  , since the sense description for  X  to the sense for  X  Ride  X  .

Particularly, in the case of  X  ( Tada )-sensitive  X  , the accuracy of our system is zero, because the sense definition of MRD is not suitable for the head word,  X  ( Tada )-sensitive  X  ; in other words, the words occurring in sense definition of MRD is not related to the context words in actual sentences. While we expect that  X  ( Tada )-sensitive  X  will occur with  X  (Bukkeureoum )-shy  X  ,  X  ( Chuwi )-cold  X  , and
 X  ( Noyeoum )-anger  X  based on the sense definition of MRD,  X  ( Tada )-sensitive  X  is actually co-occur-which do not describe the headword properly cause the performance of the system to be low. Because of this, the accuracy for  X  ( Tada )-sensitive  X  is zero.

In addition to this, it is difficult to disambiguate the words used temporal expressions; for instance, the also related to tense. Therefore, in order to disambiguate  X  ( Jeongi )-former term  X  , other clue words tense for WSD. All of the matched examples of  X  (Jeongi )-former term  X  has its own clue words in a sentence; however, most sentences limit the sense of  X  ( Jeongi )-former term  X  with tense, so accuracy of words used in temporal expressions is low.

The performance for  X  ( Jeongi )-turning point  X  is also low, because  X  ( Jeongi )-turning point  X  fre-quently co-occurs with  X  ( Doeda )-get  X  .  X  ( Doeda )-get  X  does not have any effect on WSD, because it has so many senses, and it is frequently used in sentences with no specific sense. Thus,  X  ( Doeda )-get  X  and  X  ( Hada )-do  X  are not good clue context words in sentences. 5.4. Experiment on English
We proved the applicability of our method to English based on Senseval -3 evaluation data. Senseval -3 evaluation data provides 3944 lexical samples for evaluation. For convenient evaluation, we used about a half of the data X  X andomly collected 2049 samples.

For English, we used English-to-Korean dictionary not English-to-English dictionary in order to utilize our resources. English-to-Korean dictionary provides the sense definitions in Korean, so we can utilize the similarity matrix which has been built for Korean words.

The accuracy of the system is 30.7% on average. This result is very low; there are some reasons as fol-lows. First, context words are not appropriate although context words are very important in that they decide which sense of the target word might be the best. We tried to use English similarity matrix acquired from Penn Treebank corpus for selecting context words, but many important content words were not selected as context words, because the relation between desired context words and target words was unseen in the Penn tree bank corpus. Second, mapping English senses to Korean for using English X  X orean dictio-nary leads to some loss of information. This loss of information results in a low performance of our system. to the Korean-to-Korean dictionary which reflects many examples of Korean word. These Korean exam-ming process disturbed us to search the right root of the verb in the MRD. Especially, the stemming results of irregular verbs were not confident. 6. Conclusion and future work
We proposed a way for WSD using the acyclic weighted digraph. We built the similarity matrix for word pairs using POS tagged corpus which was obtained from a raw text corpus by Sogang POS tagger, and rep-resent all of sense definitions from MRD with its own vector using word vectors from similarity matrix.
Based on two resources, the similarity matrix and vector representations of sense definitions, we structured the AWD and were able to assign the most appropriate sense to all polysemous words using a Viterbi algorithm.

Although our system showed bad results on English, our system resulted in suitable performances, 76.4% by accuracy, over the semantically ambiguous Korean words. The most important factor of our work is to cover all polysemous words through learning only raw text corpus and MRD, and to provide a method for considering the senses of context words in semantic level, not their lexical form. Our system avoided inten-sively laborious works for the implementation of WSD system such as creating sense-tagged corpus and words to all polysemous words.

Despite the advantages of our system, we should devise some methods for dynamically selecting context words without fixing the number of context words as three, and filtering less informative words in the sense definitions of MRD. In addition to this, we must devise a way for temporal reasoning. Temporal reasoning will improve the performance for disambiguating senses of words which are used in temporal expressions.
Moreover, we will apply our system to the application fields such as machine translation and information retrieval, and will continue trying to apply our method to other languages by studying language characteristics.
 References
 1. Introduction
The ambiguity of a word sense can be defined as a natural phenomenon in which a lexical form has more than very important in the application fields of natural language processing such as machine translation (MT) and information retrieval (IR); for example, we should choose the word of a target language corresponding to a polysemous word in a source context for MT, and the appropriate sense of polysemous words in a query in order to retrieve accurate information from a text for IR. Despite the importance of WSD, the WSD system tem is very low. In addition to this, it takes long to implement the WSD system because of the need to create sense-labeled examples and construct a thesaurus: this is referred to as a knowledge acquisition bottleneck .
Therefore, we present a way to construct a WSD system, which can be easily implemented by learning all polysemous words at once, while covering all polysemous words which are listed in MRD, and escape from a knowledge acquisition bottleneck , while showing the appropriate accuracy. 2. Related work
Several techniques for WSD have been reported. It has been common to use two kinds of resources: a dic-tionary and corpora. The first resource, a dictionary, is chosen based on the premise that the headwords of a dictionary are closely associated with their corresponding sense definition. Lesk (1986) used the number of common words among the sense definition of a polysemous word and the sense definitions of its context words. Wilks et al. (1990) defined the related words as frequently co-occurring words with the words in a sense definition of an MRD. After extending the related words, Wilks et al. (1990) used the number of the common words which are shared between the related words of a sense definition and words in a context. Yarowsky (1995) extracted the decision list from corpora automatically using sense definitions of an MRD, and proposed the bootstrapping method for applying iteratively the decision list.

Another resource for WSD is corpora. Corpus-based methods are categorized into two types: supervised learning and unsupervised learning. Brown, Della Pietra, and Mercer (1991) and Gale, Church, and Yarowsky (1993) used bilingual corpus for WSD. Yarowsky (1992) disambiguated polysemous words by the Bayesian categorizing method using a Roget X  X  thesaurus. Pedersen and Bruce (1997) regarded the WSD as a categori-zation problem and tried to solve the problem using several categorizing algorithms. Wilks and Mark (1997) proposed a practical model for WSD using POS, field information, selection restriction information, co-occur-rence, and sense definition.

In the case of Korean WSD, Park (1997) collected relational words using statistical methods, and then used the number of the words which are commonly shared between the relational words of its context and those of a target word. Park (1997) reported the performance of the system as 77.97% precision on six target words. Cho sparseness problem, Cho (1998) proposed corpus normalization, which converts all passive sentences to their corresponding active sentences. Cho (1998) reported that 80% recall and 80% precision on C23 corpus were logged with 10 target words after corpus normalization. In addition, Cho (1998) showed that the performance information. Lee (1999b) reported that the proposed method resulted in 88.06% on 16 target words on average. 3. Learning step
The goal of the learning step is to gain two resources which are a similarity matrix from a corpus and vector representations of sense definitions from an MRD. Lee (1999a) constructed a useful knowledge base with an MRD,  X  -WooriMalKeunSajeon  X .
 Fig. 1 shows the architecture of the learning step.

The first step is for acquiring co-occurrence probabilities from a POS tagged corpus. The POS tagged cor-pus is automatically generated by Sogang POS tagger. The Sogang POS tagger assigns the most appropriate
POS on each morpheme after analyzing each eojeol into morphemes in sentences. An eojeol is a token unit separated by white space in Korean. Due to the automatic POS tagging process, the POS tagged corpus in the learning stage holds some POS tagging errors. The co-occurrence probabilities from the POS tagged cor-to represent all words with their own vector; furthermore, the vector representations of words can provide a method to transform all sense definitions in MRD into vectors. As a result, two resources, similarity matrix and vector representations of sense definitions are obtained. 3.1. Similarity matrix
In order to establish the similarity matrix, we should retrieve co-occurrence probabilities of word pairs from a corpus. A corpus can be explained as a pool for retrieving statistical information. For our system, we used
Chosun-Illbo newspaper from 1996 to 1997 as a training corpus. Table 1 shows the composition of the train-ing corpus.

In case of Korean WSD, we used only three POSs such as verb, noun, and adjective, because most adverbs are derived from adjective; few essential adverbs exist. Thus, we only consider those three POSs; verb, noun, and adjective.

Based on the training corpus, we calculated all co-occurrence probabilities for all word pairs. A co-occur-rence probability denotes the probability that two different words will occur simultaneously in a sentence. Eq. (1) implies the co-occurrence probability.
 co-occurring with word Y in a sentence.

Eq. (1) gives us the matrix for word pairs. The matrix for co-occurrence probability is conceptually shown in Fig. 2 .

We denote j w j as the number of words in the training corpus; in particular, f ( w This matrix is symmetric and all diagonal elements are one.

The matrix for the co-occurrence probability has the data sparseness problem, so we used Park (1997) method for that problem. Simply Park X  X  method is summarized as an n -step contextual similarity. Park (1997) used the property of the transitivity among words; Park (1997) determined the relation of w considering the relation of w i and w k and that of w k and w Fig. 3 shows the conceptual image of an n -level contextual relationship.

In particular, the notation, S 1 , denotes a 1-level contextual similarity. Just as S textual similarity. In the course of calculating S m between w contextual chain, which indicates the minimum value among S there exist as many as m -level contextual chains between w between the contextual similarity of ( n 1)-step and that of n -level.

After reducing the data sparseness problem using an n -step contextual similarity, we can obtain the simi-larity matrix, with which we can convert each word into its own vector. 3.2. Word vector
The similarity matrix for word pairs supplies us with a way to represent each word by a vector. We used the vector space in order to transform each word into its own vector.
 Each word in a vector space is represented as the co-occurrence value with other words, as shown in Fig. 4 .
If any two words have a similar co-occurrence pattern, the meanings of these words seem to be similar. The similarity between two words follows up the inner product value of two vectors which represent the words.
For making word vectors, the dimension of vector space should be determined; in particular, highly fre-quent words in the training corpus are selected for the dimension. For implying f ( w where f ( w x ) is a frequency of a word w x , we can extract each word vector from each row of the similarity matrix by the dimension, w 1 , w 2 , ... , w j F j .

Fig. 5 shows the conceptual image of word vectors from a similarity matrix. These word vectors supply a method to represent the sense definitions in an MRD with their own vector. 3.3. Vector representations of sense definitions in MRD
Vector representations of sense definitions in an MRD can be determined by a combination of word vec-tors; that is to say, each vector of words occurring in a sense definition of MRD can be combined by Eq. (2) . inition; thus, all of sense definitions are converted into vectors.
 4. Disambiguation step
The disambiguation step is composed of two phases: constructing the AWD and assigning the most appro-priate sense to a polysemous word. In the disambiguation step, the input sentences not used in the previous learning step are provided. We should choose k context words in an input sentence for efficiently disambigu-ating a polysemous word. By using k context words and the target word, the system processes an input sen-tence by building an acyclic weighted digraph over its words senses with each edge weighted by the cosine between the sense vectors. The system finds the optimal path through the AWD using the Viterbi algorithm, and thus the optimum set of word senses. 4.1. The definition of acyclic weighted digraph represent the sets of vertices and edges, respectively, of graph G . In a directed graph which is also called gories of words. E 0 is a set of pairs of vertices; in particular, E category of the t th word in input word sequence and the j th sense defined as the directed graphs having no directed cycles. In addition to this, each edge contains its own weight on transition, and G any directed cycles. Assuming START and END are just dummy nodes for representing the start of a sentence and the end of a sentence respectively, all weights on the edges from START to all the sense categories of first word in the input word sequence are 1; in the same way, all weights on the edges from all the sense categories of the last word in input word sequence to END are also 1. 4.2. Selecting context words
Prior to constructing the AWD, we should select context words in an input sentence. The context words are the most useful words in a sentence for disambiguating a target word. These words can be determined by the similarity matrix. We selected k context words in a context by means of the similarity matrix; for instance, when given I deposit some money in the bank , assuming we disambiguate the sense of bank , the context words can be both deposit and money using the similarity matrix. In the case of the previous example, the k words which have a high similarity with bank are selected as the context words. We used k as 2, for depicting the conceptual model of the AWD in Fig. 6 . 4.3. Constructing the acyclic weighted digraph
The AWD is built with a target word and the k context words. Each node of the AWD is represented by each sense definition vector of the target word and that of k context words; furthermore, the edges between and the j th sense category of the ( t + 1)th word. Fig. 6 provides a conceptual construction for the AWD.
Each weight on the edges is calculated using the cosine similarity, Eq. (3) . This equation just implies the weight of transition from one node to another. x and y indicate the sense definition vector.
 4.4. Searching the optimal path on the acyclic weighted digraph
The Viterbi algorithm is well-known as the searching algorithm using dynamic programming skill, so we used the Viterbi algorithm with only modifying probability computation steps. Actually, Viterbi algorithm uses the value of the transition probability multiplied by observation probability in hidden Markov model (HMM). In order to use the original Viterbi algorithm, we should have architected the system by a supervised learning; we could not calculate the observation factor because the training corpus does not have any sense-labels. Because of the unlabeled training corpus, we devised a method for searching the optimal path using a
Viterbi algorithm which ignores the observation factors. Even though observation factors are missed, the com-putation complexity is exactly the same with the original Viterbi algorithm.
 Fig. 7 indicates the Viterbi algorithm applied to our system ( Allen, 1994 ).

As illustrated in Fig. 7 , we did not consider the observation factor for calculating SEQSCORE . 5. Experiment 5.1. Data set In order to test the performance of our system, we built test data sets with the most semantically ambiguous
Korean words; the target words are consisted of four nouns ( ( Bae ), ( Gogae ), ( Jeongi ), ( Sagi )), and two verbs ( ( Bbajida ), ( Tada )). They have 3.83 senses per target word on average. For the test data sets, we randomly collected POS error-free 869 sentences from Chosun-Illbo newspaper from 1996 to 1997; therefore, our data set might be biased to one sense which is commonly used in news area. Certainly, sentences for testing are not supplied in the learning step.

However, Senseval , the international organization devoted to the evaluation of WSD systems, provides some data sets for evaluation; the test set for Korean is composed of 10 noun words and each target word has 4.8 senses on average. It provides 574 samples for evaluation over 10 target words. However, it is some-what difficult to evaluate our system on the Korean data set of Senseval. First, our system is completely devised for disambiguating senses of a polysemous word not occurring in a group of sentences but occurring in suggests us a group of sentences for Korean. More serious problem is that the morpheme separation results for target word  X  ( mal )-saying  X  X f X  + ( mal + sseum )-respectful expression of saying  X  cannot be evaluated, because our morpheme analyzer cannot produces  X  ( mal )+ ( sseum ) X  for the given word X  X  ( malsseum ) X .
Thus, our system cannot understand the sense of  X  mal  X  of the  X  malsseum  X , and our system searches only the meaning of  X  malsseum  X  in the MRD. Finally, Senseval data for Korean is somewhat deficient in that it provides only 57.4 samples per target word compared to 144.8 samples per target word in our test set. In these reasons, our system cannot be exactly evaluated on the Senseval data set, so we had no choice but to create our own test set for our system. 5.2. Compared systems
Our method is compared with both a system to choose the most frequent sense in a test data and a system with a Lesk algorithm. In the WSD field, any perfect method for comparing with other systems has not been suggested yet. Because of this, a commonly used method for comparison of the performances is using both a system to choose the most frequent sense in a test data set and a system with a Lesk algorithm.
 Especially, the original Lesk algorithm disambiguates words in short phrases ( Banerjee &amp; Pedersen, 2002 ). of every other word in the phrase. A word is assigned that sense whose gloss shares the largest number of words in common with the glosses of the other words. The algorithm begins anew for each word and does not utilize the senses it previously assigned.
 5.3. System results
In the course of disambiguating all target words in actual sentences, the number of context words, k , should be determined. Fig. 8 reveals the results of some experiments for assuring the number of context words in a sentence; Fig. 8 shows how many context words cause the best performance. We evaluated the performances of our system, average accuracy, over all target words under the same condition varying the number of context words, k , from 1 to 5. The accuracy means the ratio of the number of matched examples and the number of given examples. The accuracy is calculated by Eq. (4) . T is denoted by the number of examples, and C by the number of matched examples.
 When we used three context words, the system showed the best performance.

Table 2 illustrates the performance of our system under the condition of k = 3, in which the best perfor-mance of our system is obtained. Table 2 is showing both accuracy per each sense of target words and accu-racy per each target word.

The performance of our system over all the target words shows 76.4% accuracy. By accuracy, our system performs 10% better than a system choosing a high frequent sense, and about 30% better by accuracy than the Lesk algorithm.

As shown in Table 2 , the performance of the system choosing the most frequent sense is relatively good in example, the accuracy of the baseline system for  X  ( Tada ) X  is 80%, and it is even higher than the proposed system, although the degree of the ambiguity of  X  ( Tada ) X  is seven. However, the test sentences of another domain may cause the performance of the baseline to be low.

The performance of the Lesk algorithm on our test set is even poorer than the system to choose the most frequent sense. This is because the Lesk algorithm did not consider any relation between each sense of context words and the senses of the target word; that is to say, because the Lesk algorithm considers all sense defini-tions for each context word, it ignores the relation between each sense of each context word and the sense of algorithm performs well on our test set which is biased to the sense for  X  ride  X .

Particularly, in the case of  X  ( Tada )-sensitive  X , the accuracy of our system is zero, because the sense def-inition of MRD is not suitable for the head word,  X  ( Tada )-sensitive  X ; in other words, the words occurring in sense definition of MRD is not related to the context words in actual sentences. While we expect that  X  ( Tada )-sensitive  X  will occur with  X  (Bukkeureoum )-shy  X ,  X  ( Chuwi )-cold  X , and  X  ( Noyeoum )-anger  X  based on the sense definition of MRD,  X  ( Tada )-sensitive  X  is actually co-occurring with  X  ( Ttae )-the headword properly cause the performance of the system to be low. Because of this, the accuracy for  X  ( Tada )-sensitive  X  is zero.

In addition to this, it is difficult to disambiguate the words used temporal expressions; for instance, the accuracy of  X  ( Jeongi )-former term  X  is very low.  X  ( Jeongi )-former term  X  is associated with time; time is also related to tense. Therefore, in order to disambiguate  X  ( Jeongi )-former term  X , other clue words related to temporal expressions such as  X  ( Hugi )-later term  X  are required in a sentence, because we ignore tense for WSD. All of the matched examples of  X  (Jeongi )-former term  X  has its own clue words in a sen-tence; however, most sentences limit the sense of  X  ( Jeongi )-former term  X  with tense, so accuracy of words used in temporal expressions is low.

The performance for  X  ( Jeongi )-turning point  X  is also low, because  X  ( Jeongi )-turning point  X  frequently co-occurs with  X  ( Doeda )-get  X .  X  ( Doeda )-get  X  does not have any effect on WSD, because it has so many senses, and it is frequently used in sentences with no specific sense. Thus,  X  ( Doeda )-get  X  and  X  ( Hada )-do  X  are not good clue context words in sentences. 5.4. Experiment on English
We proved the applicability of our method to English based on Senseval -3 evaluation data. Senseval -3 evaluation data provides 3944 lexical samples for evaluation. For convenient evaluation, we used about a half of the data X  X andomly collected 2049 samples.

For English, we used English-to-Korean dictionary not English-to-English dictionary in order to utilize our resources. English-to-Korean dictionary provides the sense definitions in Korean, so we can utilize the simi-larity matrix which has been built for Korean words.
 The accuracy of the system is 30.7% on average. This result is very low; there are some reasons as follows.
First, context words are not appropriate although context words are very important in that they decide which sense of the target word might be the best. We tried to use English similarity matrix acquired from Penn
Treebank corpus for selecting context words, but many important content words were not selected as context words, because the relation between desired context words and target words was unseen in the Penn tree bank corpus. Second, mapping English senses to Korean for using English X  X orean dictionary leads to some loss of information. This loss of information results in a low performance of our system. Most of translated senses of target words in the dictionary were not observed in the test data. It is contrast to the Korean-to-Korean dictionary which reflects many examples of Korean word. These Korean examples for describing the sense of English word had a bad effect on our system. Finally, the errors of the stemming process disturbed us to search the right root of the verb in the MRD. Especially, the stemming results of irregular verbs were not confident. 6. Conclusion and future work
We proposed a way for WSD using the acyclic weighted digraph. We built the similarity matrix for word pairs using POS tagged corpus which was obtained from a raw text corpus by Sogang POS tagger, and rep-resent all of sense definitions from MRD with its own vector using word vectors from similarity matrix. Based on two resources, the similarity matrix and vector representations of sense definitions, we structured the AWD and were able to assign the most appropriate sense to all polysemous words using a Viterbi algorithm.
Although our system showed bad results on English, our system resulted in suitable performances, 76.4% by accuracy, over the semantically ambiguous Korean words. The most important factor of our work is to cover all polysemous words through learning only raw text corpus and MRD, and to provide a method for considering the senses of context words in semantic level, not their lexical form. Our system avoided inten-sively laborious works for the implementation of WSD system such as creating sense-tagged corpus and mak-ing thesaurus; furthermore, it could be sufficiently applied to application fields by extending the target words to all polysemous words.

Despite the advantages of our system, we should devise some methods for dynamically selecting context words without fixing the number of context words as three, and filtering less informative words in the sense definitions of MRD. In addition to this, we must devise a way for temporal reasoning. Temporal reasoning will improve the performance for disambiguating senses of words which are used in temporal expressions.
Moreover, we will apply our system to the application fields such as machine translation and information retrieval, and will continue trying to apply our method to other languages by studying language char-acteristics.
 References
