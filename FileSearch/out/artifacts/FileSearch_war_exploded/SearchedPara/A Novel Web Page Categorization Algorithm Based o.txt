 Classifying web pages into meaningful semantic categories plays an important role in the domain of web mining. The content-based classification algorithms only concern word occurrence statistics of document samples; the link-based categorization meth-ods utilize the relationships between different web pages; and the traditional query log analysis methods improve the performance of categorization using the associations between queries and web pages. However, all these categorization techniques have further weaken the categorization performance. formation among web pages to emphasize each page X  topic, and hence improve the categorization result. We noticed that web pages are usually composed by multiple units, including paragraphs, tables, lists, headings and so on. We denote these units as believe, the reason for user to click one page associated with one query is that he was association should be looked upon as query-block association essentially. Our opinion is that if we could find users X  really interested blocks in web pages according to their queries, and explore the association between queries and blocks, we would deeper utilize the query log information to further improve the performance of categoriza-tion. Our experiment results well prove this opinion. 
Based on query-block association, we proposed a novel model to make one page original pages. 
The contributions of our work are: 1. We explored a query-block relationship and thus deeper mining web informa-2. We proposed Block Propagation Categorization (BPC) algorithm to propagate 3. We optimized the BPC algorithm to reduce the time usage of Block Propaga-To evaluate our algorithm, the experiments are performed on the Open Directory Project (ODP) data set together with the click-through log from MSN search engine. The experiments show that, BPC achieves a significant improvement over traditional will be reported later. 
The rest of the paper is organized as follows. In section 2, we review some related work on traditional classification techniques. In section 3, we propose our BPC algo-nally, we conclude the whole paper in section 5. most popular ones. k NN [7] is one of the most well-known classifiers, which is based [10], Lang [9] and Joachims [11] designed a document classifier by Na X ve Bayes Classifier, and Joachims proposed the methods of using Support Vector Machines (SVM) [6, 12] to classify documents. However, all these approaches only consider the content information, while neglecting the ubiquitous relationships among interrelated domain. 
In the link-based techniques, learning algorithms are applied to handle both text in-explored the hyperlink topology using an extended HITS algorithm. Similarly, Cohn et al. [5] and Glover et al. [8] improved the classification performance by combining incorporating words from neighboring page might not improve the categorization pages to enrich the pages X  representation and ignore noisy pages that drift the topic of web pages. 
Analyzing query log information, Beeferman et al. [1] proposed an innovative query clustering method based on click-through data. They treat click-though data sets as a bipartite graph and identify the mapping between queries and the associated [4] propose a technique for categorizing web query terms from the click-through logs query and web page clusters, while Xue et al. [18] categorize the query and web page by iterative reinforcement technique. However, all these approaches treat web page as basic unit, and bring the noisy or biased information into other web pages, and hence reducing the categorization performance. tion. In order to tackle these problem, we propose a Block Propagation Categorization (BPC) Algorithm which deeply uses the relationship between the heterogeneous web objects (queries, web pages and blocks), and propagates only useful information composing web pages, such as paragraphs, tables, lists and headings. Compared with web pages, blocks usually have more centralized topic. We will take advantage of this property to improve categorization performance. 
We first define an interrelated objects model for this problem. Then we explain the algorithm in detail. Finally, optimization is designed to improve the efficiency. 3.1 Problem Definition and blocks composing these pages. Besides, there also exist query-page, query-block and page-block relationships. without blocks (only consist of queries and web pages). q more relevance). The Web Model with Blocks. Now, the feature of blocks will be added into the p ) to represent the blocks composing p i and their contribution to p i . 
Figure 2 shows the interrelation between queries, web pages and blocks. But, there is a relation we haven X  X  defined in the figure  X  that is the query-block association. association exists when b jk contains some keywords of q i . 
Figure 3 illustrates the query-block association. The web page on the right contains about Michael Jordan and the country Jordan. Since there is a query-page association between the query and the page and the last two blocks contains the keyword blocks. 
Based on the web model we defined above, our problem is how to deeply utilize the relationships between queries, web pages and blocks to enhance the performance aim to classify the web pages P into a set of predefined categories. 3.2 Block Propagation Categorization Algorithm After removing stop words and feature selection, we establish the interrelated model between queries, web pages and blocks, as depicted in Section 3.1 and Figure 3. 
We propose a novel block propagation method to enhance the topic of each page, and thus improving the performance of categorization. Our basic idea is that when a contains some keywords of the query where query-page association already exists), the block has high possibility to share the same topic with the page. Hence, propagat-ing the block to the page will reinforce the topic of the page in most cases. threshold to propagate the most similar blocks, together with their contributions (See after propagation process. contents and contribution of blocks. The block propagation categorization algorithm is described as follow: 
Block Propagation Categorization (BPC) Algorithm 2. For each block b , 4. Classify the virtual pages based on their contents and contribution of blocks. Figure 4 illustrates the changes of graph after block propagation process. The Equation 1), and p 2 obtains a new block b 12 with the contribution ns 12 . In the following, we will describe the process in detail. Degree of Similarity (Contribution) Between Blocks and Web Pages. The degree measurement in our algorithm. A popular technique to quantify this measurement is is defined as the cosine of the angle between p i and b ik , pages based on their content together with their contribution. We use a weighted contribution of 0.5, we would say that the word appears 0.5 time in the block. Thus, NBC is converted to weighted NBC as Here, c NB represents the categorization result of weighted NBC, a i represents a word in some block of a page, s i represents the contribution of the block, and  X  is a parameter. 3.3 Optimization In the experiments, propagating block content could be rather inefficiency. Based on mathematic knowledge, we convert the problem to propagate some other information, such as vector space or probability dist ribution, instead of block content. 
According to Na X ve Bayes Classifier, for blocks b i , the classifier works as following combination of two blocks) Here, i w ~ represents the contribution of block b i to the new page. 
From Equation 3, we observe that we could only propagate the blocks X  probability distribution and lead to the same accuracy result. Thus, there is no need for propagat-ing contents. Hence, the time and space complexity got much reduced. 4.1 Data Set To evaluate the performance of our algorithm, we performed the experiments on a set of classified web pages from the Open Directory Project (ODP) (http://dmoz.org). human experts into 17 top level categories ( Arts, Business and Economy, Computer and Internet, Games, Health, Home, Kids and Teens, News, Recreation, Reference, Regional, Science, Shopping, Society, Sports, Adult and World ). We removed the Regional and World categories, because the web pages in Regional category are also remain 15 categories in our experiments. 
We collected a real MSN query click-through log as our experiment data set. The August 2003. 
Some preprocesses have been applied to the raw queries and web pages. First, we converted the queries into lower case, and stemmed them using the Porter algorithm, and URL are merged into a single one, with the frequencies summed up. Then, we removed, from the data set, the web pages which are not associated with some query sessions. Figure 5 shows the distribution of the web pages in 15 categories. 4.2 Feature Selection We used a popular feature selection method, Document Frequency (DF) Thresholding [20], to cut down the number of features, and speed up the classification. Based on Y. Yang et al. [20], DF thresholding is sugg ested, as the method, which has comparable performance with IG or CHI, is simplest with lowest cost in computation. In our ex-periments, we set the DF threshold as 3. 4.3 Evaluation Criteria The performance of the algorithms was evaluated by precision, recall and F 1 measures [21], while micro-average and macro-average [21] were applied to get single per-formance value over all classification tasks. 4.4 Performance We use the pure content-based Na X ve Bayes Classifier (NBC) as the baseline. Be-sides, the traditional  X  X uery + Content X  (QC) method [18], which use the query meta-evaluate the effect of block propagation, we also compare our Block Propagation Categorization (BPC) algorithm with the Link-based Page Propagation (LBPP) method [14] which we have mentioned in the section 2. 
We fixed several parameters in our experiments. First, when selecting features, we set the DF threshold as 3. Second, when propagating blocks, the threshold of similar-ity is set as 0.5. Third, the parameter  X  in Equation 2 is set as 0.2. Using these values, our experiment gave a good performance. Table 1 shows the performance of each classification algorithms. 
The result in Table 1 shows that the content-based classification method (NBC) gives poor result, which indicates it is not sufficient to only concern text contents for classifying web pages. The performance of  X  X uery + Content X  (QC) method is much better, since it utilizes the information of queries. Link-based Page Propagation (LBPP) has a comparable performance with QC, as it collects the neighborhood pages X  contents to enrich each page X  X  topic. However, our BPC algorithm archives the because BPC deeper uses the information of contents and relationships in the web. It filters much more noise than most traditional methods during the reinforcement proc-ess (Block Propagation Process). 
We conduct the further experiment to show the other performance of our algo-rithm. Figure 6 shows that NBC has a poor performance when the lengths of files are sufficient text information for NBC to give confident predictions; when the length is affect the categorization results. These web pages may have unclear or confused top-ics. BPC propagates useful blocks to emphasize the topic of the each page, and hence emphasizes the topics of these pages. We see that, in Figure 6, BPC achieves bigger the less improvement is given for the files with normal lengths. We run our BPC algorithm on the Pentium IV 2.4G PC with 1GB memory. Figure 7 shows the executed CPU time by BPC before and after optimization (Section 3.3). We see that before optimization, BPC (Na X ve BPC) consumes huge amount of time, while after optimization, BPC (Optimized BPC) has a time complexity which is approxi-ability for large data. query-block association, and hence deeply utilize both the contents and relationships in the web. We proposed a block propagation algorithm to emphasize the pages X  topic, which enhance the performance of categorization. The experiments on ODP and real MSN query click-though log datasets show that our algorithm thoroughly classification algorithm to speed up it. And the experiment shows that our algorithm is scalable well for large web data. 
In this paper, we only propagate the blocks for one step. Maybe propagating for more steps will further improves the performance of classification. But, the difficulty is how to update the query-block association after block propagation, which could take huge amount of time and space. Thus, in order to propagate the blocks for more than one step, we have to design an efficient algorithm to real-time update the query-block association. 
We have already shown that block propagation could improve the traditional web page classification algorithm. How about the web page clustering? 
