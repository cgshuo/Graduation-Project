 We introduce a new EM framework in which it is possible not only to optimize the model parameters but also the num-ber of model components. A key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the EM framework. While the classical EM algorithm estimates model parameters em-pirically using the data points themselves, we estimate them using nonparametric density estimates.

There exist many possible applications that require opti-mal adjustment of model components. We present experi-mental results in two domains. One is polygonal approxi-mation of laser range data, which is an active research topic in robot navigation. The other is grouping of edge pixels to contour boundaries, which still belongs to unsolved prob-lems in computer vision.
 I.5 [ Pattern Recognition ]: General Algorithms, Performance, Experimentation EM, Expectation Maximization, Kullback-Leibler divergence
Our goal is to approximate t he ground-truth density q ( x ) with a member p  X  ( x ) of a parametric family { p  X  ( x ): X  S} of densities. We use Kullback-Leibler divergence (KLD) to measure dissimilarity between the ground-truth and para-metric family of densities. By definition, the KLD between Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. the ground truth q ( x )andthedensity, p  X  ( x )is: The data itself, being noisy, do not directly correspond to the ground truth density. We demonstrate below that the ground-truth density q ( x ) can be estimated from the data. The use of Kullback-Leibler divergence (KLD) enables us to fit an optimal model to the ground truth rather than the noisy data.

Observe that KLD is able to approximate the optimal number of model components of p  X  . This is due to the fact that KLD D ( q || p  X  ), viewed as a functional on the space  X  p minimum. However, this minimum does not have to be a fi-nite mixture of Gaussians, since the space of finite Gaussian mixtures is not closed. On the other hand, the set of finite Gaussian mixtures is dense in the space of continuous func-tions. Therefore, we can approximate the minimum with any required precision when we minimize KLD in the space of finite Gaussian mixtures. In particular, this means that we can estimate the number of mixture components, but it is impossible to determine the optimal number of com-ponents, since this number may be large or infinite (e.g., some ground truth model components could be very small). Therefore, using KLD we are able to correctly estimate the number of  X  X arge X  (or significant) model components.
It is known that the Expectation Maximization (EM) al-gorithm can be derived from KLD (Section 2). However, in the EM framework the number of model components must be known and fixed. This is due to the fact that the log like-lihood function that is optimized in the EM framework in-creases when the number of model components is increased. Thus, when optimizing the log likelihood, we cannot esti-mate the number of model components.

An important question which arises is: why is the ability to estimate the optimal number of model components lost in the derivation of EM from KLD? In this paper we pro-vide an answer to this question and derive a new EM target function from KLD that allows us to optimize not only the model parameters but also to estimate the number of the model components. Moreover, in the proposed framework, EM converges to an optimal solution even if the initial values of model parameters are not close to the global optimum.
There exist many possible applications that require op-timal adjustment of model components. We illustrate our approach on polygonal approximation of laser range data and object contours in digital images. Polygonal maps ob-tained by polygonal approximation of laser range data are very attractive means to represent range scan data due to their very compact size and simplicity. Hence they lead to huge data compression and make it easier to access higher level features. Therefore, several approaches have been pro-posed to obtain such maps, the most recent ones being [17, 11, 8]. An excellent overview can be found in [17]. Although approximation with higher order curves is possible, approxi-mation with lines is more stable in the presence of noise (e.g., see Ch. 5 in [14]), which is the case for laser range scans. Therefore, we focus on polygonal approximation in this pa-per. However, the proposed EM framework has a broader scope of possible applications. Polygonal approximation of edge pixels in digital images can be interpreted as grouping of edge pixels to parts of object contours, which belongs to unsolved problems of computer vision. The approaches to grouping of object contours date back to the the first results of Gestalt psychology in the beginning of 20th century [18], and they remain an active research topic in computer vision. An overview of techniques for polygonal approximations of curves, which require that the order of data points is known, can be found in [10].

The main difficulty of fitting polylines in the above appli-cations is that the segmentation (or correspondence) of data points to line segments as well as the order of data points are unknown. The Expectation Maximization (EM) algorithm [2] provides a particullary useful framework to solve this cor-respondence problem. Actually EM applied to line fitting is known as the Healy-Westmacott procedure in statistics, and predates EM by many years [6]. However, polygonal ap-proximation of point data requires that not only the model parameters but also the number of model components (line segments) are estimated, but as observed above in the EM framework the number of model components must be known and fixed. Moreover, EM produces an optimal solution only if the number of model components is well estimated and the initial values of model parameters are close to the global optimum.

We give a simple example that illustrates the fact that EM yields a locally optimal solution if the initial values of model parameters are not close to globally optimal values. In Fig. 1 we see data points that follow the horizontal and vertical lines in a cross like pattern. Fig. 1(a) shows two diagonal lines that form the initial configuration of the standard EM algorithm. The number of model components (two lines) is correctly initialized, but their initial position is not close to the global optimum. Fig. 1(b) shows the final, locally optimal, result obtained by the classical EM algorithm. Fi-nally, Fig. 1(c) shows the globally optimal approximation obtained by the proposed method on the same input.
Due to the problem of getting stuck in local optima, a correct estimation of the number of components and the pa-rameter values of a statistical model is crucial in all EM applications, and therefore, belongs to one of the most chal-lenging problems in statistical reasoning. Before we describe the proposed approach, we review existing solutions. The existing solutions can be divided into two categories. The first category is based on using penalty functions like the Bayesian Information Criterion (BIC), or alternatively, the Minimum Description Length (MDL), and Akaike Infor-mation Criterion (AIC), to det ermine the optimal number of model components. The approaches in this category require that EM is run until it converges, whatever the initial num-ber of components assumed, with the goal of selecting the components exemplified by the ground truth. As we show below, approaches of this sort cannot be guaranteed to cor-rectly estimate the optimal number of model components because EM may get stuck in local optima.

In [1] the use of BIC and AIC to estimate the number of model components is discussed. We focus here on BIC but our arguments also apply to AIC and MDL. For a fixed number of data points, which is the case in our application at each given time t , the use of BIC represents a trade-off between emphasizing the importance of model complexity and the likelihood of the data. Typically a model that has the greatest BIC values is selected by repeatedly comparing these values for all possible numbers of model components. The problem with this approach is that its success depends on the convergence of the EM algorithm to the global op-timum whatever the initial number of model components assumed. If, for some given initial starting configuration, EM gets stuck in a local optimum, the BIC estimate will in-correctly estimate the ground truth number of model com-ponents. For example, the correct number of model com-ponents could not be determined (using BIC methodology) for the situation in Fig. 1(a,b). Since EM got stuck in a local optimum in (b), the likelihood of the model with two components is very low, and consequently the ground-truth model with two components is not selected. To the best of our knowledge this problem is not addressed by any ex-isting approach designed to estimate the number of model components.

Moreover, in practice there is a hidden parameter that is manually adjusted to obtain the desired number of model components in BIC. This parameter is the standard devi-ation of the measurement process. In BIC this standard deviation acts as a tradeoff weighting factor between the likelihood of the data points and the model complexity. As determined experimentally on ground-truth data in [1], BIC tends to over weight the penalty on model complexity, which leads to a too small number of model components.

The second category of approaches to estimate the opti-mal number of components is based on steps involving split-ting and merging of EM model components after each algo-rithm iteration. Our approach belongs to this category. It is important to mention that the approaches in the second category yield a quicker convergence since they adapt the number of model components and model parameters to the given environment after every algorithm iteration while BIC requires convergence for each given initial number of model components.

We will first show that the existing split and merge ap-proaches cannot be guaranteed to correctly estimate the op-timal number of model components due to the fact that they cannot recognize locally optimal solutions that are not globally optimal.

In 1995 Green [3] proposed a solution based on iterative merging and splitting components of a mixture model with the goal of obtaining a better mixture model in the case of univariate normal mixtures. Green X  X  solution is based on a fully Bayesian mixture analysis that makes use of jump Markov chain Monte Carlo (MCMC) methods. The jumps are realized by split and merge moves that are reversible. Since Green X  X  merge move is evaluated using the data points, method. it requires an additional penalty for the number of model components. The number of model components depends largely on this penalty, which is not directly related to the model quality assessment, as is the case in our approach. Green X  X  approach is used to fit polygons to contours in dig-ital images in [9]; in this setting split moves correspond to inserting a new vertex into the polygon and merge moves correspond to removing a vertex. Greens algorithm requires a huge number of iterations (Green reported the need for 20,000 iterations). This is due to a random selection of ver-tices, which is counterintuitive from the point of view of human visual perception. Humans are able to identify good and bad fitting parts of a given polygon by visual inspec-tion. In consequence of this, it makes more sense to base algorithm moves on local visual inspection rather than on random selection.

In 2000 Ueda et al. [16] proposed a split and merge ex-tension of the EM framework for mixture models. Their split and merge rules do not require any penalty as is the case for Greens approach. However, as we will now show, their approach is not able to recognize some locally opti-mal solutions that are not globally optimal. Their merge criterion is based on posterior probabilities associated with the model components. Two model components  X  i and  X  j are merged if they have almost equal posterior probabilities over the data points; this means that the probability of be-ing generated by either component is approximately equal for all data points (formula (15) in [16]). Defining model components as line segments, this means that data points are approximately the same distance to either one of compo-nents that are under consideration to be merged. Observe that the two model components (diagonal line segments) in Fig. 1(a) are merged by their rule. This, however, incor-rectly results in a single line segment that cannot provide good support for the cross-shaped data points.

A single model component is split if the local data density is significantly different from the global density; both den-sities are estimated using the actual component parameters of this component (formula (16) in [16]). This split criterion fails in our application, where the model components are line segments. The single line segment in Fig. 2 is not split by this criterion, since both densities are identical (i.e., match perfectly). However, clearly two line segments are needed to obtain an optimal fit to the data points. This critique also applies to the approach in [19] that uses the same split criterion.
 The above problems also explain why the algorithm by Ueda et al. [16] needs a relatively large number of itera-tions to converge. [16] reports that about 350 iterations are needed to fit lines to data points. The proposed algorithm usually converges in less than 20 iterations. Figure 2: Clearly two line segments are needed to obtain an optimal fit do the depicted data points.
We observe that all the split and merge steps presented in the literature optimize different target function than the function optimized by the classical M step of the EM algo-rithm. Here we propose split and merge steps that optimize the same target function (Sections 3 and 4).
It can be easily derived that the parameters b  X  minimizing (1) are given by We obtain the classical maximum likelihood estimator by applying the MC (Monte Carlo) integral estimator to (2) un-der the assumption that the observations x 1 , ..., x n are i.i.d. (independently and identically distributed) sample points selected from the ground truth distribution q ( x ). However, as we derive below (equation (9)), if some propor-tion of the observations x 1 , ..., x n is noisy, a more accurate estimator of  X  in (2) is given by: where sdd is called the smoothed data density and is defined in Section 5 by the means of nonparametric density estimation.

Equation (4) is the basis of the proposed approach. To demonstrate the significance of (4), we consider the problem of estimating the optimal number of model components by minimizing the KLD D ( q ( x ) || p  X  ( x )) in  X . It is well known that (3) cannot be used to estimate the correct number of model components, since (3) increases when the number of model components increases. In contrast, we are able to determine the correct number of model components when using (4) to estimate the KLD, D ( q ( x ) || p  X  ( x )). Thus, the modified EM algorithm that maximizes (4) is not only able to estimate model parameters but also the right number of model components.
 One of the key steps in the derivation of (4) is the Monte Carlo (MC) estimate of the integral given by the right hand side of equation (1). Let x 1 ,...,x n be i.i.d. sample points drown from the probability density function (pdf) q ( x ). Then we can approximate the integral of a continuous function f by its MC estimate: Z In the usual approach to inference, it is a commonly ac-cepted assumption that sample data points x 1 ,...,x n are distributed according to the (estimated) density q ( x ). This assumption is the key to insuring that maximum likelihood estimators are appropriate for purposes of estimating pa-rameters of interest. However, in all real applications, the sample data points are corrupted by a certain amount of noise. Usually the proportion of noisy points does not de-crease when the number of sample points is increased. We quantify this corruption by assuming that the data follow a distribution consisting of a mixture of an unknown ground-truth distribution q ( x ) and an unknown noise distribution  X  ( x ). Let u ( x )=  X q ( x )+(1  X   X  )  X  ( x )denotethismixture distribution. The quantity,  X  is the probability that an ob-servation comes from the ground-truth distribution q ( x )and (1  X   X  ) is the probability that it comes from the noise distri-bution. Since the observed sample data points do not follow the ground truth distribution q ( x ) but the mixture of noise and true distribution u ( x ), we obtain a more accurate MC estimate of the integral in (5) In Section A we show that equation (6) leads to a substan-tially smaller mean squared error in the estimation of the integral than equation (5). The ratio is equivalent to the conditional probability, P ( ground truth that an observed data point x is selected from the ground truth density q ( x ). We note that large values of P ( ground truth | x ) indicate that the data point x is of significant interest for inference purposes; small values indicate the reverse.
In Section 5 we show that it is possible to estimate a ratio proportional to (7) with the smoothed data density sdd ( x ). Consequently, By identifying sdd ( x i ) with its normalized value sdd ( for i =1 , ..., n , we can rewrite equation (8) in the form Finally equation (4) clearly follows from (9) and (2).
We introduce latent variables z 1 , ..., z n which serve to prop-erly label the respective data points x 1 , ..., x n sumed that the pairs ( x i ,z i )for i =1 ,...,n are i.i.d. with common (unknown) joint (ground truth) density, q ( x, z )= q ( x ) q ( z | x ); q ( x ) is the marginal x-density and q ( z conditional density of the label z given x . In this new frame-work, the KLD between the joint density q ( x, z ) and a para-metric counterpart density p  X  ( x, z )is =
Z
We are now ready to introduce the expectation (E) and maximization (M) steps. Both steps aim at minimizing the same target function (10) in our framework. The expecta-tion step yields the standard EM formula; considerations discussed above lead to a different solution for the maxi-mization step.
 Expectation Step: For a fixed set of parameters  X , we want to find a conditional density q ( z | x ) that minimizes D ( q ( x, z ) || p  X  ( x, z )). Since KLD is always nonnegative, and the second summand in (10) is minimized for q ( z | x )= p (in which case it is equal to zero), we obtain from (10) that In particular, for given sample points x 1 ,...,x n ,weobtain = p ( x i where  X  l = p ( z i = l |  X ) and  X  j = p ( z i = j |  X ) are the prior probabilities of component labels l and j correspondingly. Maximization Step: For the fixed marginal distribution q ( z | x )= p  X  ( z | x ), we want to find a set of parameters  X  that maximizes (10). Substituting q ( z | x )= p  X  ( z | x ) in (10), we obtain D ( q ( x, z ) || p  X  ( x, z )) = Thus, minimizing D ( q ( x, z ) || p  X  ( x, z )) in  X  is equivalent to minimizing D ( q ( x ) || p  X  ( x )) in  X . Using the estimate derived in equation (4), minimizing (13) in  X  is equivalent (in the MC setting discussed above) to maximizing the weighted marginal density where  X  l = p ( z i = l |  X ) are the prior probabilities of compo-nent labels l =1 ,...,k .
 Now we explicitly use the incremental update steps of the EM framework. Using the prior probabilities of component we obtain from (14) that an update of WM ( X ) is estimated by maximizing
WM ( X ;  X  (t) )= in  X  with  X  ( t ) denoting the value of  X  computed at stage t of the algorithm. The crucial difference between this and the standard EM update is that our target function is weighted with terms sdd ( x i ). We note that the known convergence proofs for the EM algorithm apply in our framework, since adding the weights sdd ( x i ) in (15) does not influence the convergence.
The proposed split and merge steps adjust the number of model components by performing component split and merge steps only if they increase the value of our target function (15). Since the proposed split and merge steps are computed in the sparse EM framework, the convergence of our algorithm follows from [7].

Our framework is very general in that it allows many pos-sible selections of the candidate components for the split and merge steps. We present specific selection methods of the candidate components in Section 8. They are based on a Maximum A Posteriori principle. In the following formulas, we assume that the candidate components are given. Split: Assume that we are given two candidate model com-ponents l 1 ,l 2 ; we consider replacing the model component l with components l 1 ,l 2 . Since our goal is maximizing QM ( X ;  X  ) in formula (15), we simply need to check whether replacing l with l 1 ,l 2 increases WM ,where j  X  X  1 ,...,k } : &lt; + p ( x i | z i = l 1 ,  X )  X  ( t ) l 1 + p ( x i | z i = l We only need to perform  X  X ocal X  computation to perform this test, i.e., we only need to compute the corresponding probabilities for the candidate components l 1 ,l 2 ,subjectto estimated following the sparse EM step in Neal and Hinton [7], (see equation (15)). In accordance with the results of [7] this local computation guarantees that the target function increases after each iteration (if (16) holds). Convergence is also guaranteed in this way.
 Merge: Given a candidate component l ,wemergetwoex-isting model components l 1 ,l 2 to l if for j  X  X  1 ,...,k &gt; + p ( x i | z i = l 1 ,  X )  X  ( t ) l 1 + p ( x i | z i = l Again we only need to perform  X  X ocal X  computations to per-form this test. For merge, we only need to compute the corresponding probabilities for the candidate component l , subject to the same constraint  X  ( t ) l =  X  ( t ) l 1 +  X  holds and we replace l 1 ,l 2 with l , the convergence of our algorithm follows from the results of [7].

We note that the proposed split and merge steps do not work in the classical EM framework. To see this, consider sdd ( x i ) = 1 for all the data points ( i =1 ,...,n ). The merge inequality (17) is not satisfied even if the ground truth model is assumed to be a single component, since multiple com-ponents can better fit the data, and consequently have a larger log likelihood value. Analogously, if the split inequal-ity (16) holds for a reasonable selection of candidate compo-nent models, the classical EM framework incorrectly splits ground truth components. Thus, a mixture model of larger number of components is always prefered in the classical EM framework. In the proposed framework, sdd represents an estimated density of the data points (estimated in a non-parametric way as described in Section 5). Consequently, in the proposed split and merge steps, the divergence of para-metric components l, l 1 ,l 2 from the ground truth is evalu-ated with respect to this nonparametric density.
In this section, we construct the function sdd ( x )thates-timates the ratio (7). Following the assumption made in calculating bootstrap samples, we can estimate the density, u ( x ) on the observed i.i.d. sample points x 1 ,...,x n from u ( x )by b u ( x 1 )=  X  X  X  = b u ( x n )= 1 n .
We use a kernel estimate, which is the most widely-used nonparametric density estimation method, to estimate the ground truth density q ( x ). Thus, under the assumption that x ,...,x n are i.i.d. sample points, we estimate the ratio (7) with a smoothed data density obtained by where proportionality refers to the fact that d ( x, y ) is the Euclidean distance, and G ( d ( x, y ) , 0 ,h )isa Gaussian with mean zero and the standard deviation (std) h . An intuitive motivation for (18) is as follows:
If a given data point x j was sampled from the true dis-the ratio is proportional (see equation (7)) to the proba-bility, P ( groundtruth | x j ), this too would be large. As a consequence of this latter fact, x j would be likely to lie in a dense region of the observed sample points and consequently sdd ( x j ) would be large.

If a given data point x j were sampled from the noise dis-analogous reasons, this implies that x j would be likely to lie in a sparse region of the sample space, and consequently sdd ( x j ) would be small.

To estimate the bandwidth parameter h ,wecandraw from a large literature on nonparametric density estimation [12, 13]. As we show in the presented experimental results, an accurate bandwidth estimation in not crucial in our ap-proach. It is also possible to use variable bandwidth [15].
In equation (15) of Section 2 it was shown that mini-mizing the Kullback Leibler Divergence in the parameters  X  amounts to maximizing the weighted marginal density WM ( X ). We use this fact throughout the discussion below.
The goal of this section is to show that formulas for maxi-mizing (15) are analogous, except for multiplication by sdd , to log likelihood maximization in the standard EM algo-rithm. To illustrate this we compute a partial derivative of (15) over one of the model parameters  X  j from the parameter vector  X  that is a parameter of j X  X h model component. The transitions from (20) to (21) and from (21) to (22) are based on The transition from (22) to (23) is based on the Bayes rule.
For example, in the 1D case when  X  j is the mean of one of the Gaussian mixture components, we can substi-tute p ( x i | z i = j,  X ) = exp( ( x i  X   X  j ) zero: Then we obtain in the 1D case
Below, we use the notation, G ( x ;  X  ;  X  ) for the Gaussian density at x with mean  X  and standard deviation  X  .We generated a 1 dimensional data set x 1 , ..., x n (with n=500) from the noisy density, See Fig. 3(a) for a plot of the generated data with groundtruth groups marked with different symbols.

We employed a split and merge algorithm with 5 ini-tial groups with group labels chosen randomly. For each component considered for possible splitting, our algorithm searched for a component point, whose density, as measured by sdd , is more than 1 standard deviation below the average component density. If no such point exists, the component is not split. Splits are accepted if they cause the objective function to increase its value. All pairs of components are considered for possible merging. Splits and merges are ac-cepted if they cause the objective function to increase from its former value in accord with formulas 16 and 17, corre-spondingly.

The results obtained by the proposed algorithm are illus-trated in Fig. 4. To illustrate the relationship between the smoothing bandwidth h of sdd and robustness properties of the parameter estimates, we repeat our algorithm for differ-ent values of h . Smaller values of (the bandwidth) h result in less smoothing; larger values result in more smoothing. The bandwidth of h =1 . 2, calculated using least squares cross-validation (see [5]), is optimal in this setting. This fol-lows from a general theorem relating the optimal bandwidth to the standard deviation and sample size. The point labels obtained by our algorithm for h =1 . 2areshowninFig. 3(b).

Observe a large stability of our algorithm with respect to the bandwidth h illustrated by plots in Fig. 4. For each h value, the algorithm was initialized with a randomly selected group labels consisting of 5 groups. Our algorithm always converged to the correct number of three signal model com-ponents. Small bandwidths did not adequately discriminate between noise and signal. Already moderately large band-widths demonstrate adequate discrimination in that com-ponent means  X  j and weights  X  j ( j =1 , 2 , 3) are accurately estimated.
We present specific details concerning our use of line seg-ments as EM model components in the applications pre-sented below. We stress that this section applies also to hy-per planes in any dimensions, but the presentation is given in terms of line segments for purposes of simplification.
The proposed approach requires a minor extension of EM line fitting to work with line segments, which we will call Ex-pectation Maximization Segment Fitting (EMSF). The dif-ference between EMSF and EM line fitting is that our model components are line segments (rather than lines). The in-put, for our model, is a set of line segments and a set of data points. As with EM the proposed EMSF is composed of two steps: (1) E-step The EM probabilities are computed based on points with labels to which the EM algorithm converges. (2) M-step Given the probabilities computed in the E-As in the case of EM line fitting, the output of the M-step is a new set of lines (not line segments). Since we need line segments as input to the E-step, we trim lines to line segment based on their support in the sample data. This is done by the split process described in Section 8.2.
Now we describe the specific details related to line seg-ments for steps (1) and (2). In order to derive the solution of (23) for EM model components being line segments, we introduce so called EM weights. In the classical EM, the that point x i corresponds to segment s l for l =1 ,...,k .We use the notation  X  l for the parameters of the line segment s itself. In our framework and the weights are normalized so that each i . After the E-step associated with the t  X  X h iteration is accomplished, we obtain a new matrix ( w ( t ) il ). Intuitively, each row i =1 , ..., n of this matrix corresponds to weighted probabilities that the data point x i is associated with the corresponding line segments; each column l =1 , ..., k can be viewed as weights representing the influence of each point on the computation of new line positions in the M-step. Below, we use the notation x i =( x ix ,x iy )with( i =1 , ..., n )for the coordinates of the observed data points, and (  X  x,  X  y )for the coordinate averages. The line L l ,constructedbelow, is constructed to go through the point (  X  x,  X  y ). To obtain the solution of (23), we perform an orthogonal regression weighted with the matrix ( w il ). The solution is given as the normal vector to line L l , which is the vector corresponding to the smallest eigenvalue of the matrix M l defined as  X  P
P Finally the parameters  X  ( t +1) l are given as parameters of the line segment s ( t +1) l obtained by trimming the line L l data points.

We are now ready to introduce particular realization of split and merge for EM model components being line seg-ments. The proposed split and merge EM segment fitting (SMEMSF) algorithm iterates the following four steps (1) EMSF (2) Split (3) EMSF (4) Merge Split step is presented in detail in Section 8.2 while Merge step is described in Section 8.1. Split evaluates the support in the data points of lines obtained by EMSF and removes the parts that are weakly supported. Since we have a finite set of data points, this has the effect of trimming the lines to line segments. Finally the merge step merges similar line segments. Thus, split and merge steps adjust the number of model components to better fit the data.
If inequality (17) holds, we merge two model components represented by parameters l 1 ,l 2 into one model componet given by parameter l . While components l 1 ,l 2 are present at step t (they are line segments s l 1 ,s l 2 ), we did not yet specify how to compute the candidate component l .Now we describe a particular method to generate a candidate component l in the particular case in which the model com-ponents are line segments. We stress that other methods are possible and that inequality (17) applies to them too.
A support set S ( s j ) for a given line segment s j (model component l ) is defined as set of points whose probability of supporting segment s j is the largest, i.e., This maps each data point to a unique segment using the Maximum A Posteriori principle. Given two line segments s ,s l 2 , the merged segment s l is obtained by trimming the straight line obtained by regression on data points in S ( s S ( s l 2 ). Trimming is performed by line split described in Section 8.2.
A classical case of EM local optimum problem is illus-trated in Fig. 5(a), where the line segment is in a locally optimal position. Clearly, the problem here is that we have a model consisting of one line only, while two line segments are needed. Fig. 5(b) illustrates a split operation described in this section. It is based on removal of subsegments that do not have sufficient support in the data points. As the result we obtain two line segments. Finally, Fig. 5(c) shows the globally optimal approximation of the data points obtained by EM applied to the two segments.

The main idea is that higher point density along a segment indicates the presence of a linear structure in the data points around the segment. The amount of support that a line segment has is measured by the density of points around it. Each line or line segment is examined regarding whether it has sufficient support in the data. Only parts of segments that have this support are allowed to remain. This leads to a splitting of existing lines or segments.

We use the nonparametric density estimation sdd to ob-tain the density along each segment. Although we defined sdd only at the sample data points in (18), it is actually defined at every point sdd ( x )  X  serve that sdd | s l restricted to a segment s l is a one di-mensional function. We obtain split point candidates (and consequently model segment candidates) as local minima of
An example application of our approach in robot mapping is outlined in Fig. 6. (a) shows an original data set of laser range scan points aligned with the algorithm presented in [4]. The original set is composed of 395 scans, each with 361 points. Thus, the original i nput map is compo sed of 142,595 points. We initialize our algorithm with only two segments, the two diagonals, as model components. (b) shows the output of the second iteration of our algorithm. The final polygonal map in (d), obtained after 12 iterations, is com-posed of 49 segments, i.e., of 98 points. Thus, the proposed approach yields the data compression ratio of 1455:1. The mean distance of scan points to the closest line segments is 5cm. We selected this map, since it contains surfaces of curved objects. The obtained polylines in (d) illustrate that the proposed approach is well suited to approximate linear as well as curved surfaces.

Now we apply the proposed approach to grouping edge pixels to polygonal curves representing object contours in digital images. Two example applications of this kind are outlined in Fig. 7. (a) shows an original input toy image. (b) shows the edges obtained by Canny edge detector with a substantial amount of incorrect edge pixels, and the ini-tial model for our algorithm. It consists of only two line segments. (c) shows an intermediate step of our algorithm. The final polygonal approximation obtained after 27 itera-tions is shown in (d). (e) shows a simulated image obtained by sampling 3 ground truth segments (150 points) with a substantial amount of noise (2000 points). (f) shows the initial model segments for our algorithm. We present the results of our algorithm after 8 in (g) and 19 iterations in (h). We stress that we have only 150 signal points in com-parison to 2000 background noise points.

We use the notation q ( x ) for the ground truth density of the data. We assume that the data (including noise) is dis-tributed as, u ( x )andlet b u ( x ) be a standard density estimate As a result of Theorem 1 we obtain that mean squared er-ror (MSE) for estimating the integral H ing H m =
Theorem 1. If x 1 , ..., x n are data generated from the noisy density u ( x ) , then the approximate MSE for estimating the integral to order O (1 /n ) , The MSE for estimating the integral f ( x i ) is up to order O (1 /n ) , Proof: The variances of either Monte Carlo approximation are of order O (1 /n ). Hence, the MSE X  X  in either case corre-spond up to order O (1 /n ) to the squares of the bias X  X . The asymptotic bias for the Monte Carlo approximation H sdd is, via the delta method, equivalent to: Due to the normalization, the rightmost term of equation (31) in square brackets is 0. Additionally, it follows from the Central Limit Theorem that  X  (1 /n ) It follows from equations (31) and (32) that Hence, by equation (33) and the remarks at the beginning of the proof, The asymptotic bias for the monte carlo approximation H m is, equivalent to:
BIAS ( H m )= By the law of large numbers, up to order O(1/ Hence, by equations, (35) and (36), it follows that, up to order O (1 / As a consequence, it follows from equation (37) that up to order O (1 /n ), The result follows.
 [1] M. J. Beal and Z. Ghahramani. The variational [2] A. Dempster, N. Laird, and D. Rubin. Maximum [3] P. Green. Reversible jump markov chain monte carlo [4] G. Grisetti, C. Stachniss, and W. Burgard. Improving [5] W. Hardle. Smoothing Techniques with [6] M. J. R. Healy and M. Wesmacott. Missing values in [7] R. Neal and G. Hinton. A view of the em algorithm [8] S. T. Pfister, S. I. Roumeliotis, and J. W. Burdick. [9] A. Pievatolo and P. Green. Boundary detection [10] P. L. Rosin. Techniques for assessing polygonal [11] D. Sack and W. Burgard. A comparison of methods [12] D. W. Scott. Multivariate Density Estimation: [13] B. W. Silverman. Density Estimation for Statistics [14] J. S. Simonoff. Smoothing Methods in Statistics . [15] G. R. Terrell and D. W. Scott. Variable kernel density [16] N. Ueda, R. Nakano, Z. Ghahramani, and G. E. [17] M. Veeck and W. Burgard. Learning polyline maps [18] M. Wertheimer. Untersuchungen zur lehre von der [19] Z. Zhang, C. Chen, J. Sun, and K. L. Chan. Em the approximation accuracy is 5cm.
