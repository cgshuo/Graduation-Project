 The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs a few words  X  X pple X ,  X  X oogle X  and  X  X BM X  , and the system outputs  X  X icrosoft X ,  X  X ace-book X  and  X  X ntel X .

Many set expansion algorithms are based on boot-strapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of  X  X emantic drift X . Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) pro-posed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with en-tities and attributes. Espresso alleviates semantic-drift by a sophisticated scoring system based on pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift.
Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it. Topic information means the genre of each document as estimated by statisti-cal topic models. In this paper, we effectively uti-lize topic information in three modules: the first generates the features of the discriminative mod-els; the second selects negative examples; the third prunes incorrect examples from candidate examples for new entities. Our experiments show that the pro-posal improves the accuracy of the extracted entities.
The remainder of this paper is organized as fol-lows. In Section 2, we illustrate discriminative boot-strapping algorithms and describe their problems. Our proposal is described in Section 3 and experi-mental results are shown in Section 4. Related works are described in Section 5. Finally, Section 6 pro-vides our conclusion and describes future works. Some previous works introduced discriminative methods based on the logistic sigmoid classifier, which can utilize arbitrary features for the relation extraction task instead of a scoring function such as Espresso (Bellare et al., 2006; Mintz et al., 2009). Bellare et al. reported that the discriminative ap-proach achieves better accuracy than Espresso when the number of extracted pairs is increased because multiple features are used to support the evidence. However, three problems exist in their methods. First, they use only local context features. The dis-criminative approach is useful for using arbitrary features, however, they did not identify which fea-ture or features are effective for the methods. Al-though the context features and attributes partly re-duce entity word sense ambiguity, some ambiguous entities remain. For example, consider the domain broadcast program (PRG) and assume that PRG X  X  attribute is advertisement . A false example is shown here:  X  Android  X  X  advertisement employs Japanese popular actors. The attractive smartphone begins to target new users who are ordinary people. X  The en-tity Android belongs to the cell-phone domain, not PRG, but appears with positive attributes or contexts because many cell-phones are introduced in adver-tisements as same as broadcast program . By us-ing topic, i.e. the genre of the document, we can distinguish  X  X ndroid X  from PRG and remove such false examples even if the false entity appeared with positive context strings or attributes. Second, they did not solve the problem of negative example se-lection. Because negative examples are necessary for discriminative training, they used all remaining examples, other than positive examples, as negative examples. Although this is the simplest technique, it is impossible to use all of the examples provided by a large-scale corpus for discriminative training. Third, their methods discriminate all candidates for new entities. This principle increases the risk of gen-erating many false-positive examples and is ineffi-cient. We solve these three problems by using topic information. 3.1 Basic bootstrapping methods In this section, we describe the basic method adopted from Bellare (Bellare et al., 2006). Our system X  X  configuration diagram is shown in Figure 1. In Figure 1, arrows with solid lines indicate the basic process described in this section. The other parts are described in the following sections. After N s positive seed entities are manually given, every noun co-occurring with the seed entities is ranked by PMI scores and then selected manually as N a positive attributes. N s and N a are predefined ba-sic adjustment numbers. The entity-attribute pairs are obtained by taking the cross product of seed en-tity lists and attribute lists. The pairs are used as queries for retrieving the positive documents, which include positive pairs. The document set D e,a in-cluding same entity-attribute pair { e,a } is regarded as one example E e,a to alleviate over-fitting for con-text features. These are called positive examples in Figure 1. Once positive examples are constructed, discriminative models can be trained by randomly selecting negative examples.
 Candidate entities are restricted to only the Named Entities that lie in the close proximity to the positive attributes. These candidates of documents, including Named Entity and positive attribute pairs, are regarded as one example the same as the train-ing data. The discriminative models are used to cal-culate the discriminative positive score, s ( e,a ) , of each candidate pair, { e,a } . Our system extracts N n types of new entities with high scores at each iter-ation as defined by the summation of s ( e,a ) of all positive attributes ( A P ); we do not iteratively extract new attributes because our purpose is entity set expansion. 3.2 Topic features and Topic models In previous studies, context information is only used as the features of discriminative models as we de-scribed in Section 2. Our method utilizes not only context features but also topic features. By utiliz-ing topic information, our method can disambiguate the entity word sense and alleviate semantic drift. In order to derive the topic information, we utilize statistical topic models, which represent the relation between documents and words through hidden top-ics. The topic models can calculate the posterior probability p ( z | d ) of topic z in document d . For example, the topic models give high probability to topic z =  X  cell-phone  X  in the above example sen-tences 1 . This posterior probability is useful as a global feature for discrimination. The topic feature value  X  t ( z,e,a ) is calculated as follows. In this paper, we use Latent Dirichlet Allocation (LDA) as the topic models (Blei et al., 2003). LDA represents the latent topics of the documents and the co-occurrence between each topic.

In Figure 1, shaded part and the arrows with bro-ken lines indicate our proposed method with its use of topic information including the following sec-tions. 3.3 Negative example selection If we choose negative examples randomly, such ex-amples are harmful for discrimination because some examples include the same contexts or topics as the positive examples. By contrast, negative examples belonging to broad genres are needed to alleviate se-mantic drift. We use topic information to efficiently select such negative examples.

In our method, the negative examples are cho-sen far from the positive examples according to the measure of topic similarity. For calculating topic similarity, we use a ranking score called  X  X ositive topic score X , PT ( z ) , defined as follows, PT ( z ) =  X  itive documents and p ( z | d ) is topic posterior prob-ability for a given positive document. The bottom 50% of the topics sorted in decreasing order of pos-itive topic score are used as the negative topics. Our system picks up as many negative documents as there are positive documents with each selected negative topic being equally represented. 3.4 Candidate Pruning Previous works discriminate all candidates for ex-tracting new entities. Our basic system can constrain the candidate set by positive attributes, however, this is not enough as described in Section 2. Our candi-date pruning module, described below, uses the mea-sure of topic similarity to remove obviously incor-rect documents.

This pruning module is similar to negative exam-ple selection described in the previous section. The positive topic score, PT , is used as a candidate con-straint. Taking all positive examples, we select the positive topics, PZ , which including all topics z sat-isfying the condition PT ( z ) &gt;th . At least one topic with the largest score is chosen as a positive topic when PT ( z )  X  th about all topics. After se-lecting this positive topic, the documents including entity candidates are removed if the posterior prob-ability satisfy p ( z | d )  X  th for all topics z . In this paper, we set the threshold to th = 0 . 2 . This con-straint means that the topic of the document matches that of the positive entities and can be regarded as a hard constraint for topic features. 4.1 Experimental Settings We use 30M Japanese blog articles crawled in May 2008. The documents were tokenized by JTAG (Fuchi and Takagi, 1998), chunked, and labeled with IREX 8 Named Entity types by CRFs using Mini-mum Classification Error rate (Suzuki et al., 2006), and transformed into features. The context features were defined using the template  X (head) entity (mid.) attribute (tail) X . The words included in each part were used as surface, part-of-speech and Named En-tity label features added position information. Max-imum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times.

In the experiments, we used three domains, car ( X  X AR X ), broadcast program ( X  X RG X ) and sports or-ganization ( X  X PT X ). The adjustment numbers for ba-sic settings are N s = 10 ,N a = 10 ,N n = 100 . Af-ter running 10 iterations, we obtained 1000 entities in total. SVM light (Joachims, 1999) with second order polynomial kernel was used as the discrimina-tive model. Parallel LDA, which is LDA with MPI (Liu et al., 2011), was used for training 100 mix-ture topic models and inference. Training corpus for topic models consisted of the content gathered from 14 days of blog articles. In the Markov-chain Monte Carlo (MCMC) method, sampling was iterated 200 times for training with a burn-in taking 50 iterations. These parameters were selected based on the results of a preliminary experiment.

Four experimental settings were examined. First is Baseline; it is described in Section 3.1. Second is the first method with the addition of topic features. Third is the second method with the addition of a negative example selection module. Fourth is the third method with the addition of a candidate prun-ing module (equals the entire shaded part in Fig-ure 1). Each extracted entity is labeled with cor-rect or incorrect by two evaluators based on the re-sults of a commercial search engine. The  X  score for agreement between evaluators was 0 . 895 . Because the third evaluator checked the two evaluations and confirmed that the examples which were judged as correct by either one of the evaluators were correct, those examples were counted as correct. 4.2 Experimental Results Table 1 shows the accuracy and significance for each domain. Using topic features significantly improves accuracy in the CAR and SPT domains. The nega-tive example selection module improves accuracy in the CAR and PRG domains. This means the method could reduce the risk of selecting false-negative ex-amples. Also, the candidate pruning method is ef-fective for the CAR and PRG domains. The CAR domain has lower accuracy than the others. This is because similar entities such as motorcycles are extracted; they have not only the same context but also the same topic as the CAR domain. In the SPT domain, the method with topic features offer signif-icant improvements in accuracy and no further im-provement was achieved by the other two modules.
To confirm whether our modules work properly, we show some characteristic words belonging to each topic that is similar and not similar to target do-main in Table 2. Table 2 shows characteristic words for one positive topic z h and two negative topics z l and z e , defined as follow.  X  z  X  z  X  z For a given topic, z , we chose topmost three words in terms of topic-word score. The topic-word score is the unigram probability of v , which was estimated by maximum likelihood estimation. For utilizing candidate pruning, near topics including z h must be similar to the domain. By contrast, for utilizing neg-ative example selection, the lower half of topics, z l , z e and other negative topics, must be far from the domain. Our system succeeded in achieving this. As shown in  X  X AR X  in Table 2, the nearest topic includes  X  X haken X  ( automobile inspection ) and the farthest topic includes  X  X aika X  ( internal medicine ) which satisfies our expectation. Furthermore, the ef-fective negative topic is similar to the topic of drifted entity sets ( digital device ). This indicates that our method successfully eliminated drifted entities. We can confirm that the other domains trend in the same direction as  X  X AR X  domain. Some prior studies use every word in a docu-ment/sentence as the features, such as the distribu-tional approaches (Pantel et al., 2009). These meth-ods are regarded as using global information, how-ever, the space of word features are sparse, even if the amount of data available is large. Our approach can avoid this problem by using topic models which are clustering methods based on probabilistic mea-sures. By contrast, Pas  X ca and Durme (2008) pro-posed clustering methods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use ex-tended LDA to model selectional preferences. Al-though their approach is similar to ours, our ap-proach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods.
The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping al-gorithms because the positive seed set is too small to accurately select negative examples. Our method uses topic information to efficiently solve both the problem of extracting global information and the problem of selecting negative examples. We proposed an approach to set expansion that uses topic information in three modules and showed that it can improve expansion accuracy. The remaining problem is that the grain size of topic models is not always the same as the target domain. To resolve this problem, we will incorporate the active learning or the distributional approaches. Also, comparisons with the previous works are remaining work. From another perspective, we are considering the use of graph-based approaches (Komachi et al., 2008) in-corporated with the topic information using PHITS (Cohn and Chang, 2000), to further enhance entity extraction accuracy.

