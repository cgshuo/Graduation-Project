 Spatial data have traditional numeric and categorical attributes as well as spatial attributes that describe the spatial information of the objects, such as location and shape. The assumption of independent and identical distribution (IID) is no longer valid for spatial data. In the spatial domain, everything is related to everything else but nearby objects are mo re related than distant objects [1]. For example, houses in nearby neighborhoods tend to have similar prices which are affected by one another. In r emote sensing images, close pixels usually belong to the same landcover type: soil, forest, etc.

Traditional model based clustering al gorithms, such as the Expectation-Maxi-mization (EM) algorithm [2], do not take spatial information into consideration. To this end, Ambroise et al. [3] proposed the Neighborhood Expectation-Maximization (NEM) algorithm, which extends EM by adding a spatial penalty term into the objective function. Such a spatial penalty favors those solutions where neighboring sites are assigned to the same class. The performance of NEM depends mainly on two factors. One is the choice of the spatial coefficient, which is used to weigh the penalty term in the objective function and specifies the degree of spatial smoothness in the clustering solution. Another one is the initial state of cluster separation, from which NEM starts iterative refinement.
For the choice of the spatial coefficient, NEM employs a fixed coefficient that has to be determined a priori and is often set empirically in practice. However, it may not be appropriate to assign a fixed coefficient to every site, regardless of whether it is in the class interior or on t he class border. When estimating pos-terior probabilities, sites in the class interior should receive stronger influence from its neighbors than those on the border. In addition, for initialization, it is usually impossible for NEM to achieve the global optimization, which has been shown to be NP-hard [4]. The clusterin g performance of NEM is very sensitive to the initial state of cluster separation. As a result, a proper initialization is of great value for the success of finding a better sub-optimal solution in prac-tice. Nevertheless, existing initialization methods for NEM and other EM-style clustering algorithms do not account for such spatial information.

To address the above mentioned challe nges, in this paper, we propose a vari-ant of NEM: Adaptive Neighborhood EM with spatial augmented Initializa-tion (ANEMI) for spatial clustering. ANEMI exploits a site-sensitive coefficient, which is determined by the correlation of ex planatory attributes inside the neigh-borhood. In addition, the refinement process of ANEMI starts from the initial state returned by the spatial augmented initialization method. Indeed, by push-ing spatial information further into the whole clustering process, our experimen-tal results on both synthetic and real datasets show that ANEMI generally leads to better clustering performance than traditional NEM.
 Overview. The remainder of this paper is organized as follows. Section 2 intro-duces the problem background and relat ed work. In Section 3, after reviewing the basic concepts of NEM, we presen t the ANEMI algorithm. Experimental results are reported in Section 4, where both the augmented initialization and the adaptive coefficient assignment of ANEMI are evaluated thoroughly. Finally, in Section 5, we draw conclusions and suggest future work. In this section, we first introduce the background by formulating the problem. Then, we briefly review related work. 2.1 Problem Formulation The goal of spatial clustering is to partition data into groups so that pairwise dissimilarity, in both non-spatial space and spatial space, between those as-signed to the same cluster tend to be smaller than those in different clusters. In detail, we are given a spatial framework of n sites S = { s i } n i =1 ,whicharede-scribedwithanobservableset X = { x i  X  x ( s i ) } n i =1 of random variables. Note that we overload notation and use X to refer to both the given dataset and their corresponding random variables. Often it is enough to know the neigh-borhood information, which can be represented by a contiguity matrix W with W ij =1if s i and s j are neighbors and W ij = 0 otherwise. We need to infer the unobservable (hidden) set Y = { y i  X  X  1 , 2 , ..., K }} n i =1 of random variables, corresponding to the cluster label of x i . Due to the spatial constraint, the re-sulting random field defined over Y is a Markov Random Field (MRF), where P ( 2.2 Related Work There are roughly two categories of work that are related to the main theme of this paper: spatial clustering and the cluster initialization methods for iterative refinement clustering.

Most conventional clustering method s in the literature treat each object as a point in the high dimensional space and do not distinguish spatial attributes from non-spatial attributes. These clustering methods can be divided into the following groups: distance-based [5], density-based [6], hierarchy-based [7], etc.
In the field of spatial clustering, some methods only handle 2-dimensional spatial attributes [8] and deal with problems like obstacles which are unique in clustering geo-spatial data [9]. To incorporate spatial constraints, the sim-plest method is to directly add spatial information, e.g., spatial coordinates, into datasets [10]. Others achieve this goal by modifying existing algorithms, e.g., al-lowing an object assigned to a class if th is class already contains its neighbor [11]. Another class, where our algorithm falls, selects a model that encompasses spatial information. This can be achieved by modifying a criterion function that includes spatial constraints [12], which mainly comes from image analysis where MRF and EM-style algorithms were intensively used [13,14].

Clustering using mixture models with conventional EM does not account for spatial information. NEM extends EM by adding a weighted spatial penalty term in the objective function. The clustering performance of NEM depends largely on the global fixed coefficient, the weight of the penalty. If further information about structure is available, spatially varying coefficient models can be employed, which has been mainly investigated for regression problems [15].

In practice, the subsequent cluster re finement in NEM is only a sub-task of the whole clustering, which succeeds the execu tion of a certain init ialization method. With the initialization methods returning a set of seed centers, the data are assigned to the closest center and thus an initial clustering is obtained for NEM to refine. Roughly speaking, the cluster initialization methods fall into three major families: random sampling, distance optimization and density estimation. We will examine three representati ve methods in more details later. In this section, we first introduce the basics of NEM from the MRF perspective. Then we present the cluster refinement part of the ANEMI algorithm, which exploits an adaptive scheme of coefficient assignment. Finally, we discuss the initialization methods for the ANEMI algorithm. 3.1 The MRF Framework By the Hammersley-Clifford theorem [16], the prior probability of a cluster label configuration Y = { y i } n i =1 (a realization of the MRF) can be expressed as a Gibbs distribution [13], P ( Y )= 1 Z and V ( Y ) is the overall label configuration potential function. In the clustering framework, the conditional probability of X given Y has the form P ( X | Y )= f (
X | Y,  X  ), a density function parameterized with  X  . The posterior probability becomes P ( Y | X )= 1 Z constant. Hence finding the maximum a-posteriori (MAP) configuration of the hidden MRF is equivalent to maximizing the logarithm of P ( Y | X )(scaledbya constant) 3.2 Neighborhood EM (NEM) For the potential function V ( Y ), NEM employs a soft version of the pairwise with I (true) = 1 and I (false) = 0. In detail, let P denote a set of distributions { P ik  X  P ( y i = k ) } governing { y i } . Termed  X  X patial penalty X , the potential function used in NEM is G ( P )=  X  1 2 i,j W ij K k =1 P ik P jk . One can see it becomes the Potts model if we require P ik be binary(a hard distribution). Such a model favors spatially regular partitions, which is appropriate in the case of spatial positive autocorrelation.

In NEM, the conditional density f ( x |  X  ) takes the form of a mixture model nent X  X  density function and p ( x | y = k )= f k ( x |  X  k ). Following [17], NEM X  X  soft counterpart of ln( f ( X | Y,  X  )) in Eq. (1) can be written as Note that maximizing F is also equivalent to maximizing the log-likelihood cri-terion function in the conventional mixture model [18]. Then, the new objective function in NEM becomes U ( P, X  )= F ( P, X  )+  X G ( P ), where  X  is a fixed positive coefficient to weigh the spatial penalty a nd controls the desired smoothness of output clustering. U can be maximized via the EM procedure, starting from an initial P 0 . 1. M-step: With P 2. E-step: With  X  t fixed, set P
It can be shown that in the E-step, U will be maximized at P Eq. (3), which can be organized as P certain conditions, the sequence produced by P that solution to maximize U . Hence P the estimation from its own x and the estimation from its neighbors.
 3.3 NEM with Adaptive Coefficient Assignment EM for the conventional mixture model is not appropriate for spatial clustering since it does not account for spatial information. In contrast, NEM adds in the criterion a spatial penalty weighted by a fixed coefficient  X  . However, it may not be appropriate to assign a constant coefficient to every site. For those in the class interior, the whole neighborhood is from the same class and hence the site should receive more influence from its neighbors, especially when their posterior estimates are accurate. For those on the class border, because their neighbors are from different classes, its own class membership should be determined mainly by its own explanatory attributes.

Along this line, ANEMI employs a site-sensitive spatial coefficient for the spatial penalty term. In detail, besides the original  X  that determines the global smoothness in the solution clustering, every site s i has another coefficient  X  i of its own that determines the local smoothness. Then the new penalty be-be regarded as a special case with  X  i = 1 for all sites. Let U denote the La-grangian of U : U = U + n i =1  X  i ( K k =1 P ik  X  1), which takes into account the constraints on P ik . Based on the necessary optimality Kuhn-Tucker conditions, solving  X  X  / X  P ik =0for P ik yields Then the estimation is the same as th at in NEM, except that we apply Eq. (4) in the E-step.
 What remains is to determine  X  i . In our implementation, we employ the local Moran X  X  I measure, which evaluates the local spatial autocorrelation at site s i based on the explanatory attributes inside the neighborhood [19]. Let z ip denote mean of the p -th attribute. Let  X  p denote the global standard deviation of the p -th attribute. Then, for the p -th attribute at site s i ,thelocal I is defined as I ip = binary W .Ahigh I (e.g., I&gt; 1) implies a high local spatial autocorrelation at site s i , which is likely to occur in the class i nterior. The reverse happens on the border. In ANEMI,  X  i is obtained by first averaging I ip over all attributes and then normalizing to [0 , 1], i.e., I i =mean p ( I ip ),  X  i = I i  X  min i { I i } max 3.4 Spatial Augmented Initialization Like other EM-based algorithms, ANEMI X  X  clustering solution is sensitive to the initial state and hence the study of proper initialization is another focus of this paper. In this paper, we examined three representative methods for cluster-ing initialization: random sampling, K-Means and KKZ. The random sampling method returns K seed centers by uniformly selecting K input instances. For within-cluster scatter minimization, the K-Means algorithm [20] can be regarded as a simplified hard version of EM on Gaussian mixture. While many clustering methods essentially minimize the within-cluster scatter, KKZ [21] is a greedy search method to optimize the complementary between-cluster scatter.

We can see that all initialization methods above only consider normal at-tributes without accounting for spatial information. If the positive autocorrela-tion is the major trend within data, then most sites would be surrounded by neighbors from the same class. Based on this observation, we propose to aug-ment feature vector x i of site s i with x Ni , the average of its neighbors. That is, the augmented vector becomes x i =[ x i , X  x Ni ],where  X &gt; 0 is a coefficient to weigh the impact of the neighbors, and x Ni = n j =1 W ij x i / n j =1 W ij . Then the initialization methods can be run on the augmented { x i } . In this section, we first introduce the datasets and the clustering comparison methodology used in our experiments. Then we report comparative results. 4.1 Experimental Datasets We evaluate ANEMI on five datasets, two synthetic and three real. Some data characteristics are listed in Table 1. The last row gives the spatial smoothness of the target variable y measured with contiguity ratio [19].

The synthetic image datasets are generated in the following way: First, a partition in four classes is simulated from a Potts MRF model with four-neighbor context on a 20  X  20 rectangular grid. Then, the observations are simulated from this partition based on four Gaussian densities. Fig. 1 shows two sample partitions Im1 and Im2 of different smoothness, together with their observations. The observations for both partitions are drawn from four Gaussian densities: N (0 , 0 . 5 2 ) ,N (1 , 0 . 5 2 ) ,N (2 , 0 . 8 2 ) ,N (4 , 0 . 8 2 ).
Satimage is a real landcover dataset available at the UCI repository [22]. It consists of the four multi-spectral values of pixels in a satellite image together with the class label from a six soil type set. Because the dataset is given in random order, we synthesize their spatial coordinates and allocate them in a 64  X  69 grid to yield a high contiguity ratio of 0.96 with four-neighbor context. Fig. 2 illustrates the original partition and a sample obtained partition.
The House dataset records house pri ces and their environment indices of 506 towns in Boston area [23]. The 12 explanatory variables, such as nitric oxides concentration and crime r ate, are used to predict the median value of houses, which is expected to have a small spread in each cluster of a reasonable partition. Fig. 3(a) and (b) show the true house values of 506 towns and their histogram. After normalizing the data to zero mean and unit variance, we fit two Gaussian mixtures, one with two components, the other with four components.

The Election dataset [23] records 1980 US presidential election results of 3107 counties. Originally the three attributes, fraction of population with college de-gree, fraction of population with homeownership and income, are used to predict voting rate. Here voting rate is used to evaluate clustering performance. Fig. 4(a) and (b) show the voting rates and their histogram. Again, we normalize the data and test two Gaussian mixtures with two and four components respectively. 4.2 Comparison Methodology We evaluate the clustering quality via two external validation measures. Let C, Y denote the true class label and the derived cluster label, respectively. The con-where probabilities are computed as sa mple frequencies. Analogously, for the continuous target variable C , we calculate the weighted standard deviation de-fined as S ( C | Y )= K k =1 P Y ( k )std( C | Y = k ), where std(  X  ) denotes the stan-dard deviation operator and ( C | Y = k ) denotes the C  X  X  values in cluster Y = k . Both measures are minimi zed to zero in the ideal case.

During experimentation, we concentrate on whether spatial augmented initial-ization and adaptive coefficient assignment bring any gain in the final clustering quality. For fair comparison, we first compute the augmented version of vec-tors and then randomly draw K vectors. They are treated as the initial centers returned by the random sampling method on the augmented data. The first half of these vectors are treated as those on original data. These vectors also play the role of initial centers for K-Means that runs 10 iterations. Euclidean distance is used in K-Means and KKZ. In the augmented vector x i =[ x i , X  x Ni ],often  X  = 1 led to the best results, so we only report results with  X  =1.
 The cluster refinement part of ANEMI is built upon an implementation of NEM [24]. Specifically, Gaussian mixture is employed as the model. The global coefficient  X  is tuned empirically in NEM for each dataset and the obtained value is also used in ANEMI. The number of internal iterations of E-step is set for each dataset, we report average results of 20 runs. 4.3 Results and Discussions Initialization. First, we run conventional NEM initialized in both original and augmented spaces. The clustering results are given in Table 2, where the best results are in boldface.  X  X -X X  means initialization method  X  X  X  on the augmented data. For instance, the 3rd and 4th columns show the results with random initial-ization and augmented random initialization. For the three datasets with discrete target variables, we also list in the column  X  X up X  the results under supervised mode where each component X  X  parameters are estimated with all data from a single true class. One can see that initialization using augmented data generally brings improvement. The only exception is Satimage with random sampling and KKZ, possibly because its contiguity ratio is so high that almost every site is sur-rounded by sites from the same class with very similar observations. Thus using augmented data does not make much a difference to the initialization results. Among the three initialization methods, augmented K-Means always leads to the best or sub-optimal results, though the improvement of augmented versions is often more obvious with random sampling and KKZ.
 Coefficient Assignment. Since K-Means generally provides the best initial-ization, we use it to initialize the mixt ure model for the subsequent comparison of spatial coefficient assignments. Fig. 5 presents the results corresponding to different component combinations:  X  X EM X  denotes conventional NEM with K-Means initialization in the original space;  X  X EM+I X  denotes conventional NEM with K-Means initialization in the augmented space;  X  X +NEM X  denotes NEM with adaptive coefficient and K-Means initialization in the original space;  X  X NEMI X  denotes NEM with adaptive coefficient and K-Means initialization in the augmented space. One can see that compared to conventional NEM, us-ing site-sensitive coefficients generally yields better results. The only exception is Satimage again. The reasons may be that Satimage X  X  contiguity ratio is so high that almost every site is surrounded by sites from the same class. Thus it may be always beneficial to assign sites in the neighborhood to the same class. Compared to spatial augmented initialization, the adaptive coefficient assign-ment has a greater impact on the final clustering quality. The best results are always achieved by combining them two together. In this paper, we introduced an Adaptive Neighborhood Expectation X  Maximization with spatial augmented Initialization (ANEMI) algorithm for spa-tial clustering. ANEMI is an extension of the NEM algorithm, which is built on top of the EM algorithm by incorporating a spatial penalty term into the criterion function. This penalty term is weighed by a spatial coefficient that de-termines the global smoothness of the solution clustering. Unlike NEM, which assigns an equal weight to every site, ANEMI exploits an adaptive site-sensitive weight assignment scheme, which is determined by the local smoothness in-side the neighborhood for each site. In addition, to provide a good initial state for clustering, we proposed to push spatial information early into the initial-ization methods. Along this line, we also examined three representative initialization methods in the spatial augmented space. Finally, we evaluated the impact of spatial augmented initialization and adaptive coefficient assignment in ANEMI against NEM on both synthetic and real-world datasets. Empirical re-sults showed that with adaptive coefficient assignment, ANEMI using augmented K-Means initialization generally leads to better clustering results than NEM. The gain is most obvious when they are run on datasets with low contiguity ratio.
As for future work, we plan to investigate stochastic versions of NEM to reduce dependence on the algorithm initialization. Also, other optimization techniques, such as genetic algorithms [25], are worth trying to speed up the convergence rate and to improve the final clustering quality further.
 Acknowledgments. This work was partially supported by SRF for ROCS, Sci. Tech. Plan Foundation of Guangdon g (No. 20070328005), a nd Sci. Tech. Plan Foundation of Don gguan (No. 2007108101022).

