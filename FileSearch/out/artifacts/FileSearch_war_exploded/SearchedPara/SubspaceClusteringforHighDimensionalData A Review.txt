 Subspace clustering is an extension of traditional cluster-ing that seeks to  X nd clusters in di X eren t subspaces within a dataset. Often in high dimensional data, many dimen-sions are irrelev ant and can mask existing clusters in noisy data. Feature selection remo ves irrelev ant and redundan t dimensions by analyzing the entire dataset. Subspace clus-tering algorithms localize the searc h for relev ant dimensions allowing them to  X nd clusters that exist in multiple, possi-bly overlapping subspaces. There are two major branc hes of subspace clustering based on their searc h strategy . Top-down algorithms  X nd an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, it-erativ ely impro ving the results. Bottom-up approac hes  X nd dense regions in low dimensional spaces and combine them to form clusters. This paper presen ts a survey of the various subspace clustering algorithms along with a hierarc hy orga-nizing the algorithms by their de X ning characteristics. We then compare the two main approac hes to subspace cluster-ing using empirical scalabilit y and accuracy tests and discuss some potential applications where subspace clustering could be particularly useful.
 clustering survey, subspace clustering, projected clustering, high dimensional data Cluster analysis seeks to disco ver groups, or clusters , of sim-ilar objects. The objects are usually represen ted as a vector of measuremen ts, or a point in multidimensional space. The similarit y between objects is often determined using distance measures over the various dimensions in the dataset [44; 45]. Technology advances have made data collection easier and faster, resulting in larger, more complex datasets with many objects and dimensions. As the datasets become larger and more varied, adaptations to existing algorithms are required to main tain cluster qualit y and speed. Traditional clustering algorithms consider all of the dimensions of an input dataset  X  Supp orted in part by grants from Prop 301 (No. ECR A601) and CEINT 2004. in an attempt to learn as much as possible about each ob-ject describ ed. In high dimensional data, however, many of the dimensions are often irrelev ant. These irrelev ant dimen-sions can confuse clustering algorithms by hiding clusters in noisy data. In very high dimensions it is common for all of the objects in a dataset to be nearly equidistan t from each other, completely masking the clusters. Feature selec-tion metho ds have been emplo yed somewhat successfully to impro ve cluster qualit y. These algorithms  X nd a subset of dimensions on which to perform clustering by remo ving ir-relev ant and redundan t dimensions. Unlik e feature selection metho ds which examine the dataset as a whole, subspace clustering algorithms localize their searc h and are able to unco ver clusters that exist in multiple, possibly overlapping subspaces.
 Another reason that many clustering algorithms struggle with high dimensional data is the curse of dimensionality . As the number of dimensions in a dataset increases, distance measures become increasingly meaningless. Additional di-mensions spread out the points until, in very high dimen-sions, they are almost equidistan t from each other. Figure 1 illustrates how additional dimensions spread out the points in a sample dataset. The dataset consists of 20 points ran-domly placed between 0 and 2 in each of three dimensions. Figure 1(a) shows the data projected onto one axis. The points are close together with about half of them in a one unit sized bin. Figure 1(b) shows the same data stretc hed into the second dimension. By adding another dimension we spread the points out along another axis, pulling them fur-ther apart. Now only about a quarter of the points fall into a unit sized bin. In Figure 1(c) we add a third dimension which spreads the data further apart. A one unit sized bin now holds only about one eighth of the points. If we con-tinue to add dimensions, the points will continue to spread out until they are all almost equally far apart and distance is no longer very meaningful. The problem is exacerbated when objects are related in di X eren t ways in di X eren t subsets of dimensions. It is this type of relationship that subspace clustering algorithms seek to unco ver. In order to  X nd such clusters, the irrelev ant features must be remo ved to allow the clustering algorithm to focus on only the relev ant di-mensions. Clusters found in lower dimensional space also tend to be more easily interpretable, allowing the user to better direct further study .
 Metho ds are needed that can unco ver clusters formed in var-ious subspaces of massiv e, high dimensional datasets and high dimensional data extremely sparse. represen t them in easily interpretable and meaningful ways [48; 66]. This scenario is becoming more common as we strive to examine data from various perspectiv es. One exam-ple of this occurs when clustering query results. A query for the term \Bush" could return documen ts on the presiden t of the United States as well as information on landscaping. If the documen ts are clustered using the words as attributes, the two groups of documen ts would likely be related on dif-feren t sets of attributes. Another example is found in bioin-formatics with DNA microarra y data. One population of cells in a microarra y experimen t may be similar to another because they both produce chloroph yll, and thus be clus-tered together based on the expression levels of a certain set of genes related to chloroph yll. However, another popula-tion migh t be similar because the cells are regulated by the circadian clock mechanisms of the organism. In this case, they would be clustered on a di X eren t set of genes. These two relationships represen t clusters in two distinct subsets of genes. These datasets presen t new challenges and goals for unsup ervised learning. Subspace clustering algorithms are one answ er to those challenges. They excel in situations like those describ ed above, where objects are related in multiple, di X eren t ways.
 There are a number of excellen t surveys of clustering tech-niques available. The classic book by Jain and Dubes [43] o X ers an aging, but comprehensiv e look at clustering. Zait and Messatfa o X er a comparativ e study of clustering algo-rithms in [77]. Jain et al. published another survey in 1999 [44]. More recen t data mining texts include a chapter on clustering [34; 39; 45; 73]. Kolatc h presen ts an updated hierarc hy of clustering algorithms in [50]. One of the more recen t and comprehensiv e surveys was published by Berhkin and includes a small section on subspace clustering [11]. Gan presen ted a small survey of subspace clustering metho ds at the Southern Ontario Statistical Graduate Studen ts Semi-nar Days [33]. However, there was little work that dealt with the subject of subspace clustering in a comprehensiv e and comparativ e manner.
 In the next section we discuss feature transformation and feature selection techniques which also deal with high di-mensional data. Each of these techniques work well with particular types of datasets. However, in Section 3 we show an example of a dataset for which neither technique is well suited, and introduce subspace clustering as a potential so-lution. Section 4 contains a summary of many subspace clustering algorithms and organizes them into a hierarc hy according to their primary characteristics. In Section 5 we analyze the performance of a represen tativ e algorithm from each of the two major branc hes of subspace clustering. Sec-tion 6 discusses some potential real world applications for subspace clustering in Web text mining and bioinformatics. We summarize our conclusions in Section 7. Appendix A contains de X nitions for some common terms used through-out the paper. Techniques for clustering high dimensional data have in-cluded both feature transformation and feature selection techniques. Feature transformation techniques attempt to summarize a dataset in fewer dimensions by creating com-binations of the original attributes. These techniques are very successful in unco vering laten t structure in datasets. However, since they preserv e the relativ e distances between objects, they are less e X ectiv e when there are large numbers of irrelev ant attributes that hide the clusters in sea of noise. Also, the new features are combinations of the originals and may be very di X cult to interpret the new features in the con-text of the domain. Feature selection metho ds select only the most relev ant of the dimensions from a dataset to reveal groups of objects that are similar on only a subset of their attributes. While quite successful on many datasets, feature selection algorithms have di X cult y when clusters are found in di X eren t subspaces. It is this type of data that motiv ated the evolution to subspace clustering algorithms. These algo-rithms take the concepts of feature selection one step further by selecting relev ant subspaces for each cluster separately . Feature transformations are commonly used on high dimen-sional datasets. These metho ds include techniques such as principle comp onen t analysis and singular value decomp o-sition. The transformations generally preserv e the original, relativ e distances between objects. In this way, they sum-marize the dataset by creating linear combinations of the attributes, and hopefully , unco ver laten t structure. Feature transformation is often a prepro cessing step, allowing the clustering algorithm to use just a few of the newly created features. A few clustering metho ds have incorp orated the use of such transformations to identify importan t features and iterativ ely impro ve their clustering [24; 41]. While of-ten very useful, these techniques do not actually remo ve any of the original attributes from consideration. Thus, information from irrelev ant dimensions is preserv ed, mak-ing these techniques ine X ectiv e at revealing clusters when there are large numbers of irrelev ant attributes that mask the clusters. Another disadv antage of using combinations of attributes is that they are di X cult to interpret, often mak-ing the clustering results less useful. Because of this, fea-ture transformations are best suited to datasets where most of the dimensions are relev ant to the clustering task, but many are highly correlated or redundan t. Feature selection attempts to disco ver the attributes of a dataset that are most relev ant to the data mining task at hand. It is a commonly used and powerful technique for re-ducing the dimensionalit y of a problem to more manageable levels. Feature selection involves searc hing through various feature subsets and evaluating each of these subsets using some criterion [13; 54; 63; 76]. The most popular searc h strategies are greedy sequen tial searc hes through the feature space, either forward or backward. The evaluation criteria follow one of two basic models, the wrapper model and the  X lter model [49]. The wrapp er model techniques evaluate the dataset using the data mining algorithm that will ulti-mately be emplo yed. Thus, they \wrap" the selection pro-cess around the data mining algorithm. Algorithms based on the  X lter model examine intrinsic prop erties of the data to evaluate the feature subset prior to data mining. Much of the work in feature selection has been directed at supervised learning. The main di X erence between feature se-lection in supervised and unsup ervised learning is the eval-uation criterion. Supervised wrapp er models use classi X -cation accuracy as a measure of goodness. The  X lter based approac hes almost always rely on the class labels, most com-monly assessing correlations between features and the class labels [63]. In the unsup ervised clustering problem, there are no universally accepted measures of accuracy and no class labels. However, there are a number of metho ds that adapt feature selection to clustering.
 Entropy measuremen ts are the basis of the  X lter model ap-proac h presen ted in [19; 22]. The authors argue that en-tropy tends to be low for data that contains tight clusters, and thus is a good measure to determine feature subset rel-evance. The wrapp er metho d prop osed in [47] forms a new feature subset and evaluates the resulting set by applying a standard k -means algorithm. The EM clustering algorithm is used in the wrapp er framew ork in [25]. Hybrid metho ds have also been developed that use a  X lter approac h as a heuristic and re X ne the results with a clustering algorithm. One such metho d by Devaney and Ram uses category util-ity to evaluate the feature subsets. They use the COBWEB clustering algorithm to guide the searc h, but evaluate based on intrinsic data prop erties [23]. Another hybrid approac h uses a greedy algorithm to rank features based on entropy values and then uses k -means to select the best subsets of features [20].
 In addition to using di X eren t evaluation criteria, unsup er-Figure 2: Sample dataset with four clusters, each in two dimensions with the third dimension being noise. Points from two clusters can be very close together, confusing many traditional clustering algorithms. vised feature selection metho ds have emplo yed various searc h metho ds in attempts to scale to large, high dimensional datasets. With such datasets, random searc hing becomes a viable heuristic metho d and has been used with many of the aforemen tioned criteria [1; 12; 29]. Sampling is another metho d used to impro ve the scalabilit y of algorithms. Mitra et al. partition the original feature set into clusters based on a similarit y function and select represen tativ es from each group to form the sample [57]. Subspace clustering is an extension of feature selection that attempts to  X nd clusters in di X eren t subspaces of the same dataset. Just as with feature selection, subspace clustering requires a searc h metho d and an evaluation criteria. In ad-dition, subspace clustering must someho w limit the scope of the evaluation criteria so as to consider di X eren t subspaces for each di X eren t cluster. We can use a simple dataset to illustrate the need for sub-space clustering. We created a sample dataset with four hundred instances in three dimensions. The dataset is di-vided into four clusters of 100 instances, each existing in only two of the three dimensions. The  X rst two clusters exist in dimensions a and b . The data forms a normal distribution with means 0.5 and -0.5 in dimension a and 0.5 in dimen-sion b , and standard deviations of 0.2. In dimension c , these clusters have  X  = 0 and  X  = 1. The second two clusters are in dimensions b and c and were generated in the same manner. The data can be seen in Figure 2. When k -means is used to cluster this sample data, it does a poor job of  X nding the clusters. This is because each cluster is spread out over some irrelev ant dimension. In higher dimensional datasets this problem becomes even worse and the clusters become impossible to  X nd, suggesting that we consider fewer dimensions.
 As we have discussed, using feature transformation tech-niques such as principle comp onen t analysis does not help in this instance, since relativ e distances are preserv ed and the e X ects of the irrelev ant dimension remain. Instead we migh t try using a feature selection algorithm to remo ve one or two dimensions. Figure 3 shows the data projected in a single dimension (organized by index on the x -axis for ease of interpretation). We can see that none of these projections of the data are su X cien t to fully separate the four clusters. Alternativ ely, if we only remo ve one dimension, we produce the graphs in Figure 4.
 However, it is worth noting that the  X rst two clusters (red and green) are easily separated from each other and the rest of the data when viewed in dimensions a and b (Figure 4(a)). This is because those clusters were created in dimensions a and b and remo ving dimension c remo ves the noise from those two clusters. The other two clusters (blue and pur-ple) completely overlap in this view since they were created in dimensions b and c and remo ving c made them indistin-guishable from one another. It follows then, that those two clusters are most visible in dimensions b and c (Figure 4(b)). Thus, the key to  X nding each of the clusters in this dataset is to look in the appropriate subspaces. Next, we explore the various goals and challenges of subspace clustering. The main objectiv e of clustering is to  X nd high qualit y clus-ters within a reasonable time. However, di X eren t approac hes to clustering often de X ne clusters in di X eren t ways, making it impossible to de X ne a universal measure of qualit y. Instead, clustering algorithms must make reasonable assumptions, often somewhat based on input parameters. Still, the results must be carefully analyzed to ensure they are meaningful. The algorithms must be e X cien t in order to scale with the increasing size of datasets. This often requires heuristic ap-proac hes, which again make some assumptions about the data and/or the clusters to be found.
 While all clustering metho ds organize instances of a dataset into groups, some require clusters to be discrete partitions and others allow overlapping clusters. The shap e of clusters can also vary from semi-regular shap es de X ned by a center and a distribution to much more irregular shap es de X ned by units in a hyper-grid. Subspace clustering algorithms must also de X ne the subset of features that are relev ant for each cluster. These subspaces are almost always allowed to be overlapping. Many clustering algorithms create dis-crete partitions of the dataset, putting each instance into one group. Some, like k -means, put every instance into one of the clusters. Others allow for outliers, which are de X ned as points that do not belong to any of the clusters. Still other clustering algorithms create overlapping clusters, al-lowing an instance to belong to more than one group or to be an outlier. Usually these approac hes also allow an in-stance to be an outlier, belonging to no particular cluster. Clustering algorithms also di X er in the shap e of clusters they  X nd. This is often determined by the way clusters are rep-resen ted. Algorithms that represen t clusters using a center and some distribution function(s) are biased toward hyper-elliptical clusters. However, other algorithms de X ne clusters as combinations of units of a hyper-grid, creating poten-tially very irregularly shap ed clusters with  X  X t faces. No one de X nition of clusters is better than the others, but some are more appropriate for certain problems. Domain speci X c knowledge is often very helpful in determining which type of cluster formation will be the most useful.
 The very nature of unsup ervised learning means that the user knows little about the patterns they seek to unco ver. Because of the problem of determining cluster qualit y, it is often di X cult to determine the number of clusters in a particular dataset. The problem for subspace algorithms is comp ounded in that they must also determine the dimen-sionalit y of the subspaces. Many algorithms rely on an input parameter to assist them, however this is often not feasible to determine ahead of time and is often a question that one would like the algorithm to answ er. To be most useful, algo-rithms should strive for stabilit y across a range of parameter values.
 Since there is no universal de X nition of clustering, there is no universal measure with which to compare clustering re-sults. However, evaluating cluster qualit y is essen tial since any clustering algorithm will produce some clustering for every dataset, even if the results are meaningless. Cluster qualit y measures are just heuristics and do not guaran tee meaningful clusters, but the clusters found should represen t some meaningful pattern in the data in the context of the particular domain, often requiring the analysis of a human expert.
 In order to verify clustering results, domain experts require the clusters to be represen ted in interpretable and meaning-ful ways. Again, subspace clusters must not only represen t the cluster, but also the subspace in which it exists. Inter-pretation of clustering results is also importan t since clus-tering is often a  X rst step that helps direct further study . In addition to producing high qualit y, interpretable clusters, clustering algorithms must also scale well with respect to the number of instances and the number of dimensions in large datasets. Subspace clustering metho ds must also be scalable with respect to the dimensionalit y of the subspaces where the clusters are found. In high dimensional data, the number of possible subspaces is huge, requiring e X cien t searc h algorithms. The choice of searc h strategy creates di X eren t biases and greatly a X ects the assumptions made by the algorithm. In fact, there are two major types of subspace clustering algorithms, distinguished by the type of searc h emplo yed. In the next section we review subspace clustering algorithms and presen t a hierarc hy that organizes existing algorithms based on the strategy they implemen t. This section discusses the main subspace clustering strate-gies and summarizes the major subspace clustering algo-rithms. Figure 5 presen ts a hierarc hy of subspace clustering algorithms organized by the searc h technique emplo yed and the measure used to determine locality. The two main types of algorithms are distinguished by their approac h to searc h-ing for subspaces. A naive approac h migh t be to searc h through all possible subspaces and use cluster validation techniques to determine the subspaces with the best clus-ters [43]. This is not feasible because the subset generation problem is intractable [49; 14]. More sophisticated heuris-tic searc h metho ds are required, and the choice of searc h technique determines many of the other characteristics of an algorithm. Therefore, the  X rst division in the hierarc hy (Figure 5)) splits subspace clustering algorithms into two groups, the top-do wn searc h and bottom-up searc h meth-ods.
 Subspace clustering must evaluate features on only a subset of the data, represen ting a cluster. They must use some mea-sure to de X ne this context. We refer to this as a \measure of locality". We further divide the two categories of sub-space clustering algorithms based on how they determine a measure of locality with which to evaluate subspaces. clusters are group ed together in each of the three dimensions. are impossible to completely separate. The bottom-up searc h metho d take advantage of the down-ward closure prop erty of densit y to reduce the searc h space, using an APRIORI style approac h. Algorithms  X rst create a histogram for each dimension and selecting those bins with densities above a given threshold. The downward closure prop erty of densit y means that if there are dense units in k dimensions, there are dense units in all ( k  X  1) dimen-sional projections. Candidate subspaces in two dimensions can then be formed using only those dimensions which con-tained dense units, dramatically reducing the searc h space. The algorithm proceeds until there are no more dense units found. Adjacen t dense units are then combined to form clusters. This is not always easy, and one cluster may be mistak enly reported as two smaller clusters. The nature of the bottom-up approac h leads to overlapping clusters, where one instance can be in zero or more clusters. Obtaining meaningful results is dependen t on the prop er tuning of the grid size and the densit y threshold parameters. These can be particularly di X cult to set, especially since they are used across all of the dimensions in the dataset. A popular adap-tation of this strategy provides data driven, adaptiv e grid generation to stabilize the results across a range of densit y thresholds.
 The bottom-up metho ds determine locality by creating bins for each dimension and using those bins to form a multi-dimensional grid. There are two main approac hes to ac-complishing this. The  X rst group consists of CLIQUE and ENCLUS which both use a static sized grid to divide each di-mension into bins. The second group contains MAFIA, Cell Based Clustering (CBF), CLTree, and DOC. These meth-ods distinguish themselv es by using data driven strategies to determine the cut-p oints for the bins on each dimension. MAFIA and CBF use histograms to analyze the densit y of data in each dimension. CLTree uses a decision tree based strategy to  X nd the best cutpoint. DOC uses a maxim um width and minim um number of instances per cluster to guide a random searc h. CLIQUE [8] was one of the  X rst algorithms prop osed that at-tempted to  X nd clusters within subspaces of the dataset. As describ ed above, the algorithm combines densit y and grid based clustering and uses an APRIORI style technique to  X nd clusterable subspaces. Once the dense subspaces are found they are sorted by coverage , where coverage is de X ned as the fraction of the dataset covered by the dense units in the subspace. The subspaces with the greatest coverage are kept and the rest are pruned. The algorithm then  X nds adjacen t dense grid units in each of the selected subspaces using a depth  X rst searc h. Clusters are formed by combin-ing these units using using a greedy growth scheme. The algorithm starts with an arbitrary dense unit and greedily grows a maximal region in each dimension until the union of all the regions covers the entire cluster. Redundan t regions are remo ved by a repeated procedure where smallest redun-dant regions are discarded until no further maximal region can be remo ved. The hyper-rectangular clusters are then de X ned by a Disjunctiv e Normal Form (DNF) expression. CLIQUE is able to  X nd many types and shap es of clusters and presen ts them in easily interpretable ways. The region growing densit y based approac h to generating clusters al-lows CLIQUE to  X nd clusters of arbitrary shap e. CLIQUE is also able to  X nd any number of clusters in any number of dimensions and the number is not predetermined by a pa-rameter. Clusters may be found in the same, overlapping, or disjoin t subspaces. The DNF expressions used to repre-sent clusters are often very interpretable. The clusters may also overlap each other meaning that instances can belong to more than one cluster. This is often advantageous in subspace clustering since the clusters often exist in di X eren t subspaces and thus represen t di X eren t relationships. Tuning the parameters for a speci X c dataset can be di X -cult. Both grid size and the densit y threshold are input parameters which greatly a X ect the qualit y of the cluster-ing results. Even with prop er tuning, the pruning stage can sometimes eliminate small but importan t clusters from consideration. Like other bottom-up algorithms, CLIQUE scales well with the number of instances and dimensions in the dataset. CLIQUE (and other similar algorithms), how-ever, do not scale well with number of dimensions in the output clusters. This is not usually a major issue, since subspace clustering tends to be used to  X nd low dimensional clusters in high dimensional data. ENCLUS [17] is a subspace clustering metho d based heav-ily on the CLIQUE algorithm. However, ENCLUS does not measure densit y or coverage directly , but instead measures entropy. The algorithm is based on the observ ation that a subspace with clusters typically has lower entropy than a subspace without clusters. Clusterabilit y of a subspace is de X ned using three criteria: coverage, densit y, and corre-lation. Entropy can be used to measure all three of these criteria. Entropy decreases as the densit y of cells increases. Under certain conditions, entropy will also decrease as the coverage increases. Inter est is a measure of correlation and is de X ned as the di X erence between the sum of entropy mea-suremen ts for a set of dimensions and the entropy of the multi-dimension distribution. Larger values indicate higher correlation between dimensions and an interest value of zero indicates indep enden t dimensions.
 ENCLUS uses the same APRIORI style, bottom-up ap-proac h as CLIQUE to mine signi X can t subspaces. Prun-ing is accomplished using the downward closure prop erty of entropy (below a threshold ! ) and the upward closure prop-erty of interest (i.e. correlation) to  X nd minimally correlated subspaces. If a subspace is highly correlated (above thresh-old  X  ), all of it's superspaces must not be minimally corre-lated. Since non-minimally correlated subspaces migh t be of interest, ENCLUS searc hes for interesting subsp aces by cal-culating interest gain and  X nding subspaces whose entropy exceeds ! and interest gain exceeds  X  0 . Once interesting sub-spaces are found, clusters can be identi X ed using the same metho dology as CLIQUE or any other existing clustering algorithm. Like CLIQUE, ENCLUS requires the user to set the grid interval size,  X . Also, like CLIQUE the algorithm is highly sensitiv e to this parameter.
 ENCLUS also shares much of the  X  X xibilit y of CLIQUE. EN-CLUS is capable of  X nding overlapping clusters of arbitrary shap es in subspaces of varying sizes. ENCLUS has a simi-lar running time as CLIQUE and shares the poor scalabilit y with respect to the subspace dimensions. One impro vemen t the authors suggest is a pruning approac h similar to the way CLIQUE prunes subspaces with low coverage, but based on entropy. This would help to reduce the number of irrelev ant clusters in the output, but could also eliminate small, yet meaningful, clusters. MAFIA [35] is another extension of CLIQUE that uses an adaptiv e grid based on the distribution of data to impro ve e X ciency and cluster qualit y. MAFIA also introduces paral-lelism to impro ve scalabilit y. MAFIA initially creates a his-togram to determine the minim um number of bins for a di-mension. The algorithm then combines adjacen t cells of sim-ilar densit y to form larger cells. In this manner, the dimen-sion is partitioned based on the data distribution and the re-sulting boundaries of the cells capture the cluster perimeter more accurately than  X xed sized grid cells. Once the bins have been de X ned, MAFIA proceeds much like CLIQUE, using an APRIORI style algorithm to generate the list of clusterable subspaces by building up from one dimension. MAFIA also attempts to allow for parallelization of the the clustering process.
 In addition to requiring a densit y threshold parameter, MAFIA also requires the user to specify threshold for merging ad-jacen t windo ws. Windo ws are essen tially subset of bins in a histogram that is plotted for each dimension. Adjacen t windo ws within the speci X ed threshold are merged to form larger windo ws. The formation of the adaptiv e grid is de-penden t on these windo ws. Also, to assist the adaptiv e grid algorithm, MAFIA takes a default grid size as input for di-mensions where the data is uniformly distributed. In such dimensions, the data is divided into a small,  X xed number of partitions. Despite the adaptiv e nature of the grids, the algorithm is rather sensitiv e to these parameters. Like related metho ds, MAFIA  X nds any number of clus-ters of arbitrary shap e in subspaces of varying size. It also represen ts clusters as DNF expressions. Due to paralleliza-tion and other impro vemen ts, MAFIA performs many times faster than CLIQUE on similar datasets. It scales linearly with the number of instances and even better with the num-ber of dimensions in the dataset (see section 5). However, like the other algorithms in this category , MAFIA's running time grows exponen tially with the number of dimensions in the clusters. Cell-based Clustering (CBF) [16] attempts to address scal-abilit y issues associated with many bottom-up algorithms. One problem for other bottom-up algorithms is that the number of bins created increases dramatically as the num-ber of dimensions increases. CBF uses a cell creation algo-rithm that creates optimal partitions by repeatedly exam-ining minim um and maxim um values on a given dimension which results in the generation of fewer bins (cells). CBF also addresses scalabilit y with respect to the number of in-stances in the dataset. In particular, other approac hes often perform poorly when the dataset is too large to  X t in main memory . CBF stores the bins in an e X cien t  X ltering-based index structure which results in impro ved retriev al perfor-mance.
 The CBF algorithm is sensitiv e to two parameters. Section threshold determines the bin frequency of a dimension. The retriev al time is reduced as the threshold value is increased because the number of records accessed is minimized. The other parameter is cell threshold which determines the min-imum densit y of data points in a bin. Bins with densit y above this threshold are selected as potentially a mem ber of a cluster. Like the other bottom-up metho ds, CBF is able to  X nd clusters of various sizes and shap es. It also scales linearly with the number of instances in a dataset. CBF has been shown to have slightly lower precision than CLIQUE but is faster in retriev al and cluster construction [16]. CLTree [53] uses a modi X ed decision tree algorithm to select the best cutting planes for a given dataset. It uses a decision tree algorithm to partition each dimension into bins, sepa-rating areas of high densit y from areas of low densit y. Users can browse the tree and re X ne the  X nal clustering results. CLTree follows the bottom-up strategy , evaluating each di-mension separately and then using only those dimensions with areas of high densit y in further steps. The decision tree splits corresp ond to the boundaries of bins and are cho-sen using modi X ed gain criteria that essen tially measures the densit y of a region. Hypothetical, uniformly distributed noise is \added" to the dataset and the tree attempts to split the read data from the noise. The noise does not actually have to be added to the dataset, but instead the densit y can be estimated for any given bin under investigation. While the number and size of clusters are not required ahead of time, there are two parameters. The  X rst, min y , is the minim um number of points that a region must contain to be considered interesting during the pruning stage. The second, min rd , is the minim um relativ e densit y between two adja-cent regions before the regions are merged to form a larger cluster. These parameters are used to prune the tree, and results are sensitiv e to their settings. CLTree  X nds clusters as hyper-rectangular regions in subspaces of di X eren t sizes. Though building the tree could be expensiv e ( O ( n 2 )), the algorithm scales linearly with the number of instances and the number of dimensions of the subspace. Densit y-based Optimal projectiv e Clustering (DOC) is some-what of a hybrid metho d that blends the grid based ap-proac h used by the bottom-up approac hes and the iterativ e impro vemen t metho d from the top-do wn approac hes. DOC prop oses a mathematical de X nition for the notion of an op-timal projectiv e cluster [64]. A projectiv e cluster is de X ned as a pair ( C;D ) where C is a subset of the instances and D is a subset of the dimensions of the dataset. The goal is to  X nd a pair where C exhibits a strong clustering tendency in D . To  X nd these optimal pairs, the algorithm creates a small subset X , called the discriminating set, by random sampling. Ideally , this set can be used to di X eren tiate be-tween relev ant and irrelev ant dimensions for a cluster. For a given a cluster pair ( C;D ), instances p in C , and instances q in X the following should hold true: for each dimension i in D , j q ( i )  X  p ( i ) j &lt; = w , where w is the  X xed side length of a subspace cluster or hyper-cub e, given by the user. p and X are both obtained through random sampling and the algorithm is repeated with the best result being reported. The results are heavily dependen t on parameter settings. The value of w determines the maxim um length of one side of a cluster. A heuristic is presen ted to choose an optimal value for w . DOC also requires a parameter  X  that speci X es the minim um number of instances that can form a cluster.  X  along with w determine the minim um cluster densit y. An-other parameter  X  is provided to specify the balance between number of points and the number of dimensions in a cluster. The clusters are hyper-rectangular in shap e and represen ted by a pair ( C;D ), meaning the set of data points C is pro-jected on the dimension set D . The running time grows linearly with the number of data points but it increases ex-ponen tially with the number of dimensions in the dataset. The top-do wn subspace clustering approac h starts by  X nd-ing an initial appro ximation of the clusters in the full fea-ture space with equally weighted dimensions. Next each dimension is assigned a weight for each cluster. The up-dated weights are then used in the next iteration to regener-ate the clusters. This approac h requires multiple iterations of expensiv e clustering algorithms in the full set of dimen-sions. Many of the implemen tations of this strategy use a sampling technique to impro ve performance. Top-do wn al-gorithms create clusters that are partitions of the dataset, meaning each instance is assigned to only one cluster. Many algorithms also allow for an additional group of outliers. Parameter tuning is necessary in order to get meaningful results. Often the most critical parameters for top-do wn algorithms is the number of clusters and the size of the sub-spaces, which are often very di X cult to determine ahead of time. Also, since subspace size is a parameter, top-do wn al-gorithms tend to  X nd clusters in the same or similarly sized subspaces. For techniques that use sampling, the size of the sample is another critical parameter and can play a large role in the qualit y of the  X nal results.
 Locality in top-do wn metho ds is determined by some ap-proximation of clustering based on the weights for the di-mensions obtained so far. PROCLUS, ORCLUS, FINDIT, and  X  -Clusters determine the weights of instances for each cluster. COSA is unique in that it uses the k nearest neigh-bors for each instance in the dataset to determine the weights for each dimension for that particular instance. PROCLUS [5] was the  X rst top-do wn subspace clustering algorithm. Similar to CLARANS [58], PROCLUS samples the data, then selects a set of k medoids and iterativ ely impro ves the clustering. The algorithm uses a three phase approac h consisting of initialization , iteration , and cluster re X nement . Initialization uses a greedy algorithm to se-lect a set of potential medoids that are far apart from each other. The objectiv e is to ensure that each cluster is rep-resen ted by at least one instance in the selected set. The iteration phase selects a random set of k medoids from this reduced dataset, replaces bad medoids with randomly cho-sen new medoids, and determines if clustering has impro ved. Cluster qualit y is based on the average distance between in-stances and the nearest medoid. For each medoid, a set of dimensions is chosen whose average distances are small compared to statistical expectation. The total number of dimensions associated to medoids must be k  X  l , where l is an input parameter that selects the average dimension-ality of cluster subspaces. Once the subspaces have been selected for each medoid, average Manhattan segmen tal dis-tance is used to assign points to medoids, forming clusters. The medoid of the cluster with the least number of points is thrown out along with any medoids associated with fewer than ( N=k )  X  minD eviation points, where minD eviation is an input parameter. The re X nemen t phase computes new dimensions for each medoid based on the clusters formed and reassigns points to medoids, remo ving outliers. Like many top-do wn metho ds, PROCLUS is biased toward clusters that are hyper-spherical in shap e. Also, while clus-ters may be found in di X eren t subspaces, the subspaces must be of similar sizes since the the user must input the average number of dimensions for the clusters. Clusters are repre-sented as sets of instances with associated medoids and sub-spaces and form non-o verlapping partitions of the dataset with possible outliers. Due to the use of sampling, PRO-CLUS is somewhat faster than CLIQUE on large datasets. However, using a small number of represen tativ e points can cause PROCLUS to miss some clusters entirely . The cluster qualit y of PROCLUS is very sensitiv e to the accuracy of the input parameters, which may be di X cult to determine. ORCLUS [6] is an extended version of the algorithm PRO-CLUS[5] that looks for non-axis parallel subspaces. This algorithm arose from the observ ation that many datasets contain inter-attribute correlations. The algorithm can be divided into three steps: assign clusters, subspace determi-nation, and merge. During the assign phase, the algorithm iterativ ely assigns data points to the nearest cluster cen-ters. The distance between two points is de X ned in a sub-space E , where E is a set of orthonormal vectors in some d -dimensional space. Subspace determination rede X nes the subspace E associated with each cluster by calculating the covariance matrix for a cluster and selecting the orthonor-mal eigen vectors with the least spread (smallest eigen val-ues). Clusters that are near each other and have similar di-rections of least spread are merged during the merge phase. The number of clusters and the size of the subspace dimen-sionalit y must be speci X ed. The authors provide a general scheme for selecting a suitable value. A statistical mea-sure called the cluster sparsity coe X cient , is provided which can be inspected after clustering to evaluate the choice of subspace dimensionalit y. The algorithm is computationally intensiv e due mainly to the computation of covariance matri-ces. ORCLUS uses random sampling to impro ve speed and scalabilit y and as a result may miss smaller clusters. The output clusters are de X ned by the cluster center and and an associated partition of the dataset, with possible outliers. A Fast and Intelligen t Subspace Clustering Algorithm us-ing Dimension Voting, FINDIT [74] is similar in structure to PROCLUS and the other top-do wn metho ds, but uses a unique distance measure called the Dimension Oriente d Distanc e (DOD) . The idea is compared to voting whereb y the algorithm tallies the number of dimensions on which two instances are within a threshold distance,  X  , of each other. The concept is based on the assumption that in higher di-mensions it is more meaningful for two instances to be close in several dimensions rather than in a few [74]. The algo-rithm typically consists of three phases, namely sampling phase , cluster forming phase , and data assignment phase . The algorithms starts by selecting two small sets generated through random sampling of the data. The sets are used to determine initial represen tativ e medoids of the clusters. In the cluster forming phase the correlated dimensions are found using the DOD measure for each medoid. FINDIT then incremen ts the value of  X  and repeats this step until the cluster qualit y stabilizes. In the  X nal phase all of the instances are assigned to medoids based on the subspaces found.
 FINDIT requires two input parameters, the minim um num-ber of instances in a cluster, and the minim um distance be-tween two clusters. FINDIT is able to  X nd clusters in sub-spaces of varying size. The DOD measure is dependen t on the  X  threshold which is determined by the algorithm in a iterativ e manner. The iterativ e phase that determines the best threshold adds signi X can tly to the running time of the algorithm. Because of this, FINDIT emplo ys sampling tech-niques like the other top-do wn algorithms. Sampling helps to impro ve performance, especially with very large datasets. Section 5 shows the results of empirical experimen ts using the FINDIT and MAFIA algorithms. The  X  -Clusters algorithm uses a distance measure that at-tempts to capture the coherence exhibited by subset of in-stances on subset of attributes [75]. Coheren t instances may not be close in many dimensions, but instead they both fol-low a similar trend, o X set from each other. One coheren t instance can be deriv ed from another by shifting by the o X -set. PearsonR correlation [68] is used to measure coherence among instances. The algorithm starts with initial seeds and iterativ ely impro ves the overall qualit y of the clustering by randomly swapping attributes and data points to impro ve individual clusters. Residue measures the decrease in co-herence that a particular attribute or instance brings to the cluster. The iterativ e process terminates when individual impro vemen t levels o X  in each cluster.
 The algorithm takes as parameters the number of clusters and the individual cluster size. The running time is partic-ularly sensitiv e to the cluster size parameter. If the value chosen is very di X eren t from the optimal cluster size, the algorithm could take considerably longer to terminate. The main di X erence between  X  -Clusters and other metho ds is the use of coherence as a similarit y measure. Using this measure makes  X  -clusters suitable for domains such as mi-croarra y data analysis. Often  X nding related genes means  X nding those that respond similarly to environmen tal condi-tions. The absolute response rates are often very di X eren t, but the type of response and the timing may be similar. Clustering On Subsets of Attributes (COSA) [32] is an it-erativ e algorithm that assigns weights to each dimension for each instance, not each cluster. Starting with equally weighted dimensions, the algorithm examines the k near-est neigh bors ( knn ) of each instance. These neigh borhoods are used to calculate the respectiv e dimension weights for each instance. Higher weights are assigned to those dimen-sions that have a smaller dispersion within the knn group. These weights are used to calculate dimension weights for pairs of instances which are in turn used to update the dis-tances used in the knn calculation. The process is then repeated using the new distances until the weights stabilize. The neigh borhoods for each instance become increasingly enric hed with instances belonging to its own cluster. The dimension weights are re X ned as the dimensions relev ant to a cluster receiv e larger weights. The output is a distance ma-trix based on weighted inverse exponen tial distance and is suitable as input to any distance-based clustering metho d. After clustering, the weights of each dimension of cluster mem bers are compared and an overall importance value for each dimension for each cluster is calculated.
 The number of dimensions in clusters need not be speci X ed directly , but instead is controlled by a parameter,  X  , that controls the strength of incen tive for clustering on more di-mensions. Each cluster may exist in a di X eren t subspaces of di X eren t sizes, but they do tend to be of similar dimen-sionalit y. It also allows for the adjustmen t of the k used in the knn calculations. Friedman claims that the results are stable over a wide range of k values. The dimension weights are calculated for each instance and pair of instances, not for each cluster. After clustering the relev ant dimensions must be calculated based on the dimension weights assigned to cluster mem bers. In this section we measure the performance of represen tativ e top-do wn and bottom-up algorithms. We chose MAFIA [35], an advanced version of the bottom-up subspace clustering metho d, and FINDIT [74], an adaptation of the top-do wn strategy . In datasets with very high dimensionalit y, we ex-pect the bottom-up approac hes to perform well as they only have to searc h in the lower dimensionalit y of the hidden clusters. However, the sampling schemes of top-do wn ap-proac hes should scale well to large datasets. To measure the scalabilit y of the algorithms, we measure the running time of both algorithms and vary the number of instance or the number of dimensions. We also examine how well each of the algorithms were able to determine the correct subspaces for each cluster. The implemen tation of each algorithm was provided by the respectiv e authors. To facilitate the comparison of the two algorithms, we chose to use synthetic datasets so that we have control over of the characteristics of the data. Synthetic data also allows us to easily measure the accuracy of the clustering by compar-ing the output of the algorithms to the known input clusters. The datasets were generated using specialized dataset gener-ators that provided control over the number of instances, the dimensionalit y of the data, and the dimensions for each of the input clusters. They also output the data in the formats required by the algorithm implemen tations and provided the necessary meta-information to measure cluster accuracy . The data values lie in the range of 0 to 100. Clusters were generated by restricting the value of relev ant dimensions for each instance in a cluster. Values for irrelev ant dimensions were chosen randomly to form a uniform distribution over the entire data range. We measured the scalabilit y of the two algorithms in terms of the number of instances and the number of dimensions in the dataset. In the  X rst set of tests, we  X xed the number of dimensions at twenty. On average the datasets contained  X ve hidden clusters, each in a di X eren t  X ve dimensional sub-space. The number of instances was increased  X rst from from 100,000 to 500,000 and then from 1 million to 5 mil-lion. Both algorithms scaled linearly with the number of instances, as shown by Figure 6. MAFIA clearly out per-forms FINDIT through most of the cases. The superior performance of MAFIA can be attributed to the bottom-up approac h which does not require as many passes through the dataset. Also, when the clusters are embedded in few dimensions, the bottom-up algorithms have the advantage that they only consider the small set of relev ant dimensions during most of their searc h. If the clusters exist in high numbers of dimensions, then performance will degrade as the number of candidate subspaces grows exponen tially with the number of dimensions in the subspace [5].
 In Figure 6(b) we can see that FINDIT actually outp erforms MAFIA when the number of instances approac hes four mil-lion. This could be attributed to the use of random sam-performance even with datasets containing  X ve million records. pling, which eventually gives FINDIT a performance edge with huge datasets. MAFIA on the other hand, must scan the entire dataset each time it makes a pass to  X nd dense units on a given number of dimensions. We can see in Sec-tion 5.3 that sampling can cause FINDIT to miss clusters or dimensions entirely .
 In the second set of tests we  X xed the number of instances at 100,000 and increased the number of dimensions of the dataset. Figure 7 is a plot of the running times as the num-ber of dimensions in the dataset is increased from 10 to 100. The datasets contained, on average,  X ve clusters each in  X ve dimensions. Here, the bottom-up approac h is clearly superior as can be seen in Figure 7. The running time for MAFIA increased linearly with the number of dimensions in the dataset. The running time for the top-do wn metho d, FINDIT, increases rapidly as the the number of dimensions increase. Sampling does not help FINDIT in this case. As the number of dimensions increase, FINDIT must weight each dimension for each cluster in order to select the most relev ant ones. The bottom-up algorithms like MAFIA, how-ever, are not adversely a X ected by the additional, irrelev ant dimensions. This is because those algorithms project the data into small subspaces  X rst adding only the interesting dimensions to the searc h. In addition to comparing the algorithms scalabilit y, we also compared how accurately each algorithm was able to de-termine the clusters and corresp onding subspaces in the dataset. The results are presen ted as a confusion matrix that lists the relev ant dimensions of the input clusters as well as those output by the algorithm. Table 1 and Table 2 show the best case input and output clusters for MAFIA and FINDIT on a dataset of 100,000 instances in 20 dimen-sions. The bottom-up algorithm, MAFIA, disco vered all of the clusters but left out one signi X can t dimension in four out of the  X ve clusters. Missing one dimension in a cluster can be caused by prematurely pruning a dimension based on a coverage threshold, which can be di X cult to determine. This could also occur because the densit y threshold may not be appropriate across all the dimensions in the dataset. The results were very similar in tests where the number of Figure 7: Running time vs. number of dimensions for FINDIT (top-do wn) and MAFIA (bottom-up). MAFIA consisten tly outp erforms FINDIT and scales better as the number of dimensions increases. instances was increased up to 4 million. The only di X er-ence was that some clusters were reported as two separate clusters, instead of prop erly merged. This fracturing of clus-ters is an artifact of the grid-based approac h used by many bottom-up algorithms that requires them to merge dense units to form the output clusters. The top-do wn approac h used by FINDIT was better able to identify the signi X can t dimensions for clusters it unco vered. As the number of in-stances was increased, FINDIT occasionally missed an entire cluster. As the dataset grew, the clusters were more di X -cult to  X nd among the noise and the sampling emplo yed by many top-do wn algorithms cause them to miss clusters. Tables 4 and 3 show the results from MAFIA and FINDIT respectiv ely when the number of dimensions in the dataset is increased to 100. MAFIA was able to detect all of the clusters. Again, it was missing one dimension for four of the  X ve clusters. Also higher dimensionalit y caused the same problem that we noticed with higher numbers of instances, MAFIA mistak enly split one cluster into multiple separate clusters. FINDIT did not fare so well and we can see that FINDIT missed entire clusters and was unable to  X nd all of the relev ant dimensions for the clusters it did  X nd. The top-do wn approac h means that FINDIT must evaluate all of the dimensions, and as a greater percen tage of them are irrelev ant, the relev ant ones are more di X cult to unco ver. The use of sampling can exacerbate this problem, as some clusters may be only weakly represen ted.
 While subspace clustering seems to perform well on syn-thetic data, the real test for any data mining technique is to use it to solve a real-w orld problem. In the next sec-tion we discuss some areas where subspace clustering may prove to be very useful. Faster and more sophisticated com-puters have allowed many  X elds of study to collect massiv e stores of data. In particular we will look at applications of subspace clustering in information integration systems, web text mining, and bioinformatics. Limitations of curren t metho ds and the application of sub-space clustering techniques to new domains drives the cre-ation of new techniques and metho ds. Subspace clustering is especially e X ectiv e in domains where one can expect to  X nd relationships across a variety of perspectiv es. Some areas where we feel subspace clustering has great potential are in-formation integration system, text-mining, and bioinformat-ics. Creating hierarc hies of data sources is a di X cult task for information integration systems and may be impro ved upon by specialized subspace clustering algorithms. Web searc h results can be organized into clusters based on topics to make  X nding relev ant results easier. In Bioinformatics, DNA microarra y technology allows biologists to collect an huge amoun t of data and the explosion of the World Wide Web threatens to drown us in a sea of poorly organized in-formation. Much of the knowledge in these datasets can be extracted by  X nding and analyzing the patterns in the data. Clustering algorithms have been used with DNA microarra y data in identi X cation and characterization of disease sub-types and for molecular pathway elucidation. However, the high dimensionalit y of the microarra y and text data makes the task of pattern disco very di X cult for traditional clus-tering algorithms [66]. Subspace clustering techniques can be leveraged to unco ver the complex relationships found in data from each of these areas. Information integration systems are motiv ated by the fact that our information needs of the future will not be satis- X ed by closed, centralized databases. Rather, increasingly sophisticated queries will require access to heterogeneous, autonomous information sources located in a distributed manner and accessible through the Internet. In this sce-nario, query optimization becomes a complex problem since the data is not centralized. The decen tralization of data poses a di X cult challenge for information integration sys-tems, mainly in the determination of the best subset of sources to use for a given user query [30]. An exhaustiv e searc h on all the sources would be a naive and a costly solu-tion. In the following, we discuss an application of subspace clustering in the context of query optimization for an infor-mation integration system developed here at ASU, Bib X nder [60].
 The Bib X nder system main tains coverage statistics for each source based on a log of previous user queries. With such in-formation, Bib X nder can rank the sources for a given query . In order to classify previously unseen queries, a hierarc hy of query classes is generated to generalize the statistics. For Bib X nder, the process of generating and storing statistics is proving to be expensiv e. A combination of subspace cluster-ing and classi X cation metho ds o X er a promising solution to this bottlenec k. The subspace clustering algorithm can be applied to the query list with the queries being instances and the sources corresp onding to dimensions of the dataset. The result is a rapid grouping of queries where a group represen ts queries coming from the same set of sources. Conceptually , each group can be considered a query class where the classes are generated in a one-step process using subspace cluster-ing. Furthermore, the query classes can be used to train a classi X er so that when a user query is given, the classi X er can predict a set sources likely to be useful. The explosion of the world wide web has prompted a surge of researc h attempting to deal with the heterogeneous and unstructured nature of the web. A fundamen tal problem with organizing web sources is that web pages are not ma-chine readable, meaning their contents only convey seman tic meaning to a human user. In addition, seman tic heterogene-ity is a major challenge. That is when a keyword in one do-main holds a di X eren t meaning in another domain making information sharing and interop erabilit y between heteroge-neous systems di X cult [72].
 Recen tly, there has been strong interest in developing on-tologies to deal with the above issues. The purp ose of on-tologies is to serve as a seman tic, conceptual, hierarc hical model represen ting a domain or a web page. Curren tly, on-tologies are manually created, usually by a domain expert who identi X es the key concepts in the domain and the inter-relationships between them. There has been a substan tial amoun t of researc h e X ort put toward the goal of automat-ing this process. In order to automate the process, the key concepts of a domain must be learned. Subspace clustering has two major strengths that will help it learn the concepts. First, the algorithm would  X nd subspaces, or sets of key-words, that were represen ted importan t concepts on a page. The subspace clustering algorithm would also scale very well with the high dimensionalit y of the data. If the web pages are presen ted in the form of a documen t-term matrix where the instances corresp ond to the pages and the features cor-responds to the keywords in the page, the result of subspace clustering will be the identi X cation of set of keywords (sub-spaces) for a given group of pages. These keyword sets can be considered to be the main concepts connecting the cor-responding groups. Ultimately , the clusters would represen t a domain and their corresp onding subspaces would indicate the key concepts of the domain. This information could be further utilized by training a classi X er with domains and rep-resen tativ e concepts. This classi X er could then be used to classify or categorize previously unseen web pages. For ex-ample, supp ose the subspace clustering algorithm identi X es the domains Hospital, Music, and Sports, along with their corresp onding concepts, from a list of web pages. This infor-mation can then be used to train a classi X er that will be able to determine the category for newly encoun tered webpages. DNA microarra ys are an exciting new technology with the potential to increase our understanding of complex cellular mechanisms. Microarra y datasets provide information on the expression levels of thousands of genes under hundreds of conditions. For example, we can interpret a lymphoma dataset as 100 cancer pro X les with 4000 features where each feature is the expression level of a speci X c gene. This view allows us to unco ver various cancer subtypes based upon re-lationships between gene expression pro X les. Understanding the di X erences between cancer subtypes on a genetic level is crucial to understanding which types of treatmen ts are most likely to be e X ectiv e. Alternativ ely, we can view the data as 4000 gene pro X les with 100 features corresp onding to particular cancer specimens. Patterns in the data reveal information about genes whose products function together in pathways that perform complex functions in the organ-ism. The study of these pathways and their relationships to one another can then be used to build a complete model of the cell and its functions, bridging the gap between genetic maps and living organisms [48].
 Curren tly, microarra y data must be prepro cessed to reduce the number of attributes before meaningful clusters can be unco vered. In addition, individual gene products have many di X eren t roles under di X eren t circumstances. For example, a particular cancer may be subdivided along more than one set of characteristics. There may be subtypes based on the motilit y of the cell as well as the cells prop ensit y to divide. Separating the specimens based on motilit y would require examining one set of genes, while subtypes based on cell division would be disco vered when looking at a di X eren t set of genes [48; 66]. In order to  X nd such complex relationships in massiv e microarra y datasets, more powerful and  X  X xible metho ds of analysis are required. Subspace clustering is a promising technique that extends the power of traditional feature selection by searc hing for unique subspaces for each cluster. High dimensional data is increasingly common in many  X elds. As the number of dimensions increase, many clustering tech-niques begin to su X er from the curse of dimensionality , de-grading the qualit y of the results. In high dimensions, data becomes very sparse and distance measures become increas-ingly meaningless. This problem has been studied exten-sively and there are various solutions, each appropriate for di X eren t types of high dimensional data and data mining procedures.
 Subspace clustering attempts to integrate feature evaluation and clustering in order to  X nd clusters in di X eren t subspaces. Top-do wn algorithms simulate this integration by using mul-tiple iterations of evaluation, selection, and clustering. This process selects a group of instances  X rst and then evaluates the attributes in the context of that cluster of instances. This relativ ely slow approac h combined with the fact that many are forced to use sampling techniques makes top-do wn algorithms more suitable for datasets with large clusters in relativ ely large subspaces. The clusters unco vered by top-down metho ds are often hyper-spherical in nature due to the use of cluster centers to represen t groups of similar in-stances. The clusters form non-o verlapping partitions of the dataset. Some algorithms allow for an additional group of outliers that contains instances not related to any cluster or each other (other than the fact they are all outliers). Also, many require that the number of clusters and the size of the subspaces be input as parameters. The user must use their domain knowledge to help select and tune these settings. Bottom-up algorithms integrate the clustering and subspace selection by  X rst selecting a subspace, then evaluating the instances in the that context. Adding one dimension at a time, they are able to work in relativ ely small subspaces. This allows these algorithms to scale much more easily with both the number of instances in the dataset and the number of attributes. However, performance drops quickly with the size of the subspaces in which the clusters are found. The main parameter required by these algorithms is the densit y threshold. This can be di X cult to set, especially across all dimensions of the dataset. Fortunately , even if some dimen-sions are mistak enly ignored due to improp er thresholds, the algorithms may still  X nd the clusters in a smaller subspace. Adaptiv e grid approac hes help to alleviate this problem by allowing the number of bins in a dimension to change based on the characteristics of the data in that dimension. Often, bottom-up algorithms are able to  X nd clusters of various shap es and sizes since the clusters are formed from various cells in a grid. This means that the clusters can overlap each other with one instance having the potential to be in more than one cluster. It is also possible for an instances to be considered an outlier and not belong any cluster. Clustering is a powerful data exploration tool capable of un-covering previously unkno wn patterns in data. Often, users have little knowledge of the data prior to clustering anal-ysis and are seeking to  X nd some interesting relationships to explore further. Unfortunately , all clustering algorithms require that the user set some parameters and make some assumptions about the clusters to be disco vered. Subspace clustering algorithms allow users to break the assumption that all of the clusters in a dataset are found in the same set of dimensions.
 There are many potential applications with high dimen-sional data where subspace clustering approac hes could help to unco ver patterns missed by curren t clustering approac hes. Applications in bioinformatics and text mining are partic-ularly relev ant and presen t unique challenges to subspace clustering. As with any clustering techniques,  X nding mean-ingful and useful results depends on the selection of the ap-propriate technique and prop er tuning of the algorithm via the input parameters. In order to do this, one must under-stand the dataset in a domain speci X c context in order to be able to best evaluate the results from various approac hes. One must also understand the various strengths, weaknesses, and biases of the potential clustering algorithms. [1] D. Achlioptas. Database-friendly random projections. [2] C. C. Aggarw al. Re-designing distance functions and [3] C. C. Aggarw al. Towards meaningful high-dimensional [4] C. C. Aggarw al, A. Hinneburg, and D. A. Keim. On the [5] C. C. Aggarw al, J. L. Wolf, P. S. Yu, C. Procopiuc, and [6] C. C. Aggarw al and P. S. Yu. Finding generalized pro-[7] C. C. Aggarw al and P. S. Yu. Outlier detection for high [8] R. Agra wal, J. Gehrk e, D. Gunopulos, and P. Ragha-[9] N. Alon, S. Dar, M. Parnas, and D. Ron. Testing of [10] D. Barbar X  a, Y. Li, and J. Couto. Coolcat: an entropy-[11] P. Berkhin. Surv ey of clustering data mining tech-[12] E. Bingham and H. Mannila. Random projection in di-[13] A. Blum and P. Langley . Selection of relev ant features [14] A. Blum and R. Rivest. Training a 3-node neural net-[15] Y. Cao and J. Wu. Projectiv e art for clustering data [16] J.-W. Chang and D.-S. Jin. A new cell-based clustering [17] C.-H. Cheng, A. W. Fu, and Y. Zhang. Entropy-based [18] G. M. D. Corso. Estimating an eigen vector by the power [19] M. Dash, K. Choi, P. Scheuermann, and H. Liu. Feature [20] M. Dash and H. Liu. Feature selection for clustering. [21] M. Dash, H. Liu, and X. Xu. `1 + 1 &gt; 2': merging dis-[22] M. Dash, H. Liu, and J. Yao. Dimensionalit y reduc-[23] M. Devaney and A. Ram. E X cien t feature selection in [24] C. Ding, X. He, H. Zha, and H. D. Simon. Adaptiv e di-[25] J. G. Dy and C. E. Brodley. Feature subset selection [26] S. Epter, M. Krishnamo orthy, and M. Zaki. Clusterabil-[27] L. Ert X oz, M. Stein bach, and V. Kumar. Finding clusters [28] D. Fasulo. An analysis of recen t work on clustering al-[29] X. Z. Fern and C. E. Brodley. Random projection for [30] D. Florescu, A. Y. Levy , and A. O. Mendelzon. [31] D. Fradkin and D. Madigan. Experimen ts with random [32] J. H. Friedman and J. J. Meulman. Clustering objects [33] G. Gan. Subspace clustering for high dimensional [34] J. Ghosh. Handb ook of Data Mining , chapter Scalable [35] S. Goil, H. Nagesh, and A. Choudhary . Ma X a: E X cien t [36] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. Clus-[37] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. Cluster-[38] G. Hamerly and C. Elkan. Learning the k in k -means. [39] J. Han, M. Kam ber, and A. K. H. Tung. Geographic [40] A. Hinneburg, D. Keim, and M. Wawryniuk. Using pro-[41] A. Hinneburg and D. A. Keim. Optimal grid-clustering: [42] I. Inza, P. Larraaga, and B. Sierra. Feature weight-[43] A. K. Jain and R. C. Dubes. Algorithms for clustering [44] A. K. Jain, M. N. Murt y, and P. J. Flynn. Data clus-[45] M. K. Jiawei Han. Data Mining : Conc epts and Tech-[46] S. Kaski and T. Kohonen. Exploratory data analysis [47] Y. Kim, W. Street, and F. Menczer. Feature selection [48] I. S. Kohane, A. Kho, and A. J. Butte. Microarrays for [49] R. Koha vi and G. John. Wrapp ers for feature subset [50] E. Kolatc h. Clustering algorithms for spatial databases: [51] T. Li, S. Zhu, and M. Ogihara. Algorithms for cluster-[52] J. Lin and D. Gunopulos. Dimensionalit y reduction by [53] B. Liu, Y. Xia, and P. S. Yu. Clustering through deci-[54] H. Liu and H. Moto da. Feature Selection for Know ledge [55] A. McCallum, K. Nigam, and L. H. Ungar. E X cien t [56] B. L. Mileno va and M. M. Camp os. O-cluster: scalable [57] P. Mitra, C. A. Murth y, and S. K. Pal. Unsup ervised [58] R. Ng and J. Han. E X cien t and e X ectiv e clustering [59] E. Ng Ka Ka and A. W. chee Fu. E X cien t algorithm for [60] Z. Nie and S. Kam bhampati. Frequency-based coverage [61] A. Patrik ainen. Projected clustering of high-[62] D. Pelleg and A. Moore. X-means: Extending k-means [63] J. M. Pena, J. A. Lozano, P. Larranaga, and I. Inza. Di-[64] C. M. Procopiuc, M. Jones, P. K. Agarw al, and T. M. [65] B. Raskutti and C. Leckie. An evaluation of criteria [66] S. Raychaudh uri, P. D. Sutphin, J. T. Chang, and R. B. [67] S. M. R X  uger and S. E. Gauc h. Feature reduction for [68] U. Shardanand and P. Maes. Social information  X l-[69] L. Talavera. Dependency-based feature selection for [70] L. Talavera. Dynamic feature selection in incremen tal [71] C. Tang and A. Zhang. An iterativ e strategy for pattern [72] H. Wache, T. V X ogele, U. Visser, H. Stuckenschmidt, [73] I. H. Witten and E. Frank. Data Mining: Pratical Ma-[74] K.-G. Woo and J.-H. Lee. FINDIT: a Fast and Intel-[75] J. Yang, W. Wang, H. Wang, and P. Yu.  X  -clusters: [76] L. Yu and H. Liu. Feature selection for high-dimensional [77] M. Zait and H. Messatfa. A comparativ e study of clus-Dataset -A dataset consists of a matrix of data values. Instance -An instanc e refers to an individual row in a Dimension -A dimension refers to an attribute or feature Cluster -A group of instances in a dataset that are more Clustering -The process of disco vering groups (clusters) Subspace -A subsp ace is a subset of the d dimensions of Clusterabilit y -A measure of how well a given set of in-Subspace Clustering -These clustering techniques seek Measure of Lo calit y -The measure used to determine a Feature Transformation -Techniques that generate new Feature Selection -The process of determining and se-
