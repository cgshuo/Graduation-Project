 In real-life databases, missing value is a common problem due to many unpredictable circumstances such as human carelessness, failing to import data because of violating usually unable to deal with missing values directly. 
Instead of discarding all missing data, imputation is a more desirable way to handle datasets before analysis. The simplest way to fill in missing values is to replace them many circumstances. In addition to this na X ve method, more advanced methods have categories. statistical area. Many existing models can be applied, such as expectation-maximization rior method this method is its high computation cost. Machine Learning Method: The nature of learning from existing information em-methods[9-11], clustering methods[6,7] and neural network methods[12] are all wide-ly researched. The common procedure of these methods is training model from complete data subset and then predicting missing values based on that model. 
Even though many methods have been proposed, this problem is not solved throughout. Most of the proposed methods are only suitable for numeric data and it is difficult to simply extend them to the categorical data, which is also an important part of missing value problem. Thus we focus on categorical attributes in this paper. commonly used technique [9-11]. Such meth ods have following drawbacks: (1) Gen-means only one attribute X  X  value could be estimated. This is inefficient when we want high dimensional data. Involving irrelevant attributes may cause unnecessary compu-comparison, it is hard to measure the quality or accuracy of the imputed results. 
This paper aims to solve the above problems. We propose a novel Bayesian net-work construction algorithm different from k2 algorithm[17], then use the probabilis-not fix any target attribute during the construction of BN. All nodes are added to the net according to their relativity with the others and the most influencing one comes to reasoning for one attribute. The conditional independence helps to cut off all the irre-levant attributes. To solve problem (3), we assign a probability according to the result the imputation quality from its probability information. 
The rest of this paper is organized as follows: Section 2 reviews some basic know-conclusions. about Bayesian network and probabilistic reasoning, which are used in our algorithm. 2.1 Bayesian Network A Bayesian Network (BN for brief) is a directed acyclic graph whose nodes represent variables and edges represent causal relationships. The causality is given by the con-ents. In the assumption of conditional independence, the joint probability distribution reduces the model complexity greatly and makes it easier for reasoning process, espe-cially when dealing with high dimensional data. 2.2 Probabilistic Reasoning of some variables and (2) maximum a posteriori hypothesis(MAP) problem, which finds the most possible value combination of some variables. This paper handles the latter problem, whose formal definition is given in Eq (2) E=e . More about probabilistic reasoning are discussed in [16]. ues. We build a BN in the first phase. Dependency relationships and relevant parame-ters will be computed in this phase, as discussed in Section 3.1. Next, a probabilistic reasoning is used to complete the estimation part. Section 3.2 shows the details. 3.1 Construction of Bayesian Network been proposed to learn a BN automatically fr om data, such as k2 and hill climb. But dent nodes which is not suitable for efficient reasoning. 
As an example for this point, Table 1 gives the BN constructed from k2 on one of our experimental dataset from UCI. For simplicity, we only use part of this dataset. It is observed that without any target attribute, all nodes are isolated, which disables the reasoning process. This is because the score function used in k2 algorithm is too strict to connect some nodes with correlativity. To solve this problem, we change the score for imputation, we try to explore as strong dependencies as possible, so that the miss-ing nodes can get as much information as possible from the most relevant neighbors.  X  -coefficient according to Table 2.  X   X  X  ,  X   X  X  and  X  in Table 2 represents  X   X   X  X  X  tivel y . Algorithm1 shows the main procedure of constructing a BN.

NO name parents NO name parents 0 class -1 0 class 3 1 cap-color -1 1 cap-color 5,4,0 2 bruises -1 2 bruises 3 Algorithm1: BNConstruction Input : dataset D ={  X   X   X ,  X   X ,......,  X  }  X   X  is the i th attribute Output : Bayesian network 1. for each pair of  X   X   X  and  X  ( i X  X  ) 2. t  X  X  =  X  -coefficient of  X   X   X  and  X  3. where N is the data size) in descending order. 4. for i =1 to n 5. create node for  X   X   X  6. old-score =0 7. parent ={} 8. while true 9. for j=1 to i-1 10. add-node ={} 11. new-score =  X  -coefficient of  X   X   X   X  X  X  X  X  X  X  X  and  X   X  12. if new-score&gt;old-score 13. old-score=new-score 14. add-node=  X  X  X  X  X  X  X  X  X  X   X   X  15. if add-node !={} 16. add add-node to parent 17. else 18. break 19. add edges from parent to  X   X   X  20. computes p(  X   X   X   X  X  X  X  X  X | )
In Algorithm 1, line 1-3 give an order of attributes for building the net according to their coefficient matrix. Line 4-18 find parents for each node that can help to improve its classification power. Line 20 computes the corresponding conditional probability. 0  X  0.007 0.252 0.254 0.205 0.049 0.036 0.803 4 1 0.048  X  0.047 0.107 0.119 0.115 0.135 0.571 6 2 0.252 0.003  X  0.266 0.219 0.033 0.001 0.774 5 3 0.346 0.033 0.331  X  0.335 0.112 0.021 1.178 3 5 0.265 0.094 0.243 0.304 0.278  X  0.237 1.421 1 6 0.046 0.011 0.005 0.011 0.015 0.029  X  0.117 7 
We use the data in Table 1 to demonstrate our algorithm. Table 3 is the relativity coefficient matrix computed in line 1-2. The last column shows the ordered numbers 9-18 go through {5} for finding 4 X  X  parent . We get  X   X  5,4  X   X  X  X  X =0.278 X 0=  X  X  X  X  X  X  X  X  X  , which stops the iteration. The similar procedure goes for the rest nodes. In the algorithm, we set an upper bound for the size of parent set, to avoid unneces-which means more relationships are explored compared to k2. Complexity Analysis. Each computation of the coefficient needs one pass scan of the up, the total complexity of Algorithm1 is  X   X   X   X   X   X  . 3.2 Missing Value Imputation Using Probabilistic Reasoning After all dependencies are computed as BN, the next step is to estimate each missing value through probabilistic reasoning. The main idea is to find the most possible subs-titution for a missing value with given evidences. For each missing value ( H ) of each main derivation of our imputation method. set  X   X  X  X   X  X  X  X  X  X = X   X  ="?" X  and a observed set O=  X   X  X  X   X  X   X | X  X   X   X "?" X  , solve: Eq (1): After eliminating the constant term and integrate all h in H- X   X  , we get  X  X  X  X  X  X  X   X   X  X  X  X   X  X  X  X  X  X  X  X  (8) The above transformation helps to reduce the computation complexity greatly. It only reserves the observed parent nodes, child nodes and their parent nodes which form the mate each missing value separately, which means we abandon the relationship be-code of probabilistic reasoning is shown in Algorithm2. 
After each reasoning, we assign a probability to the imputation value. The probabil-ity obtained from reasoning process cannot be used directly, because these probabili-ties may have great difference due to the number of conditional probabilities involved in Eq (8).We choose the relative probability here: Where  X  X  X   X   X =  X   X  is the probability obtained from reasoning process. Eq(9) describes the certainty of the imputed value comparing to the rest candidates. Algorithm2: Probabilistic reasoning Input : Dataset D with missing values Output : completed dataset  X   X  1. for each r in D 2.  X | X  X =  X  X  X  X  X  X   X  ="?" X  3. for each i in misSet 4. pSet ={} 5.  X  X  X  X  X  X | X  X  X  X  X  X  X  X =  X  X  X  X   X   X  X  X  X   X  6. for each i  X  X  child-node ch 7. if ch is missing 8.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X =  X  X  X  X   X  X  X   X  X  X  X   X  9. else 10.  X  X  X  X  X  X  X  X | X  X  X  X  X  X  X  X  X =  X  X  X  X   X  X  X   X  X  X  X   X  Line 5-10 are the major part, which collect the related probabilities for each node. Line 5 handles the parent nodes and line 6-10 deal with the child nodes. For explanation, we take the BN shown in Fig 1 as an example. For some instance probability from C  X  X  observed parent node, whose result is P(C|A) . Line 6-10 handle C  X  X  child nodes { D,F } one by one. Since D is missed, line 8 is executed, which adds and solves  X  X  X  X  X  X   X   X   X   X   X   X  X   X   X  |  X   X   X  X | X  X  X  X  in line 11. one, its time complexity is  X   X   X  X  X  X  X | X | |  X  X  X  X  | X  , where m is the number of miss-algorithm 2 can be considered as  X   X  | X |  X  , which means algorithm2 is linear with data size. In this section, we show the effectiveness of the proposed algorithm on real datasets. All algorithms are implemented with c++. The algorithms are run in a PC with 2GH cpu and 1G main memory. The operation system is WinXP. The basic information of datasets we used in this paper is shown in Table 4. They are from UCI machine learn-size given by Table 4 are those after removing. 4.1 Effectiveness of BN In this part, we test the effectiveness of our BN. For comparison, we build another BN dataset as representatives. For each attribute, we eliminate 10% values and use these two methods to predict them. The results are shown in Fig2. 
The experiment proves that comparing with multiple executions of sep-BNs, the them are classified in one net at the same time. 
As we have discussed in 3.1, in most circumstances, the k2 algorithm tends to form a na X ve BN, in which the target node is connected to almost all the other nodes. While the results, their effects are almost equal. The reduction in computation does not sacri-appropriately for each node. 4.2 Estimation Accuracy We compare our algorithm with two most commonly used methods most frequent value. In kNN, we choose candidate with the maximum probability as attributes used in 4.1 and generate missing values with different missing rates. For the rest 4 datasets, we generate missing values among the whole dataset in the assumption Table 5 gives their imputation precisions. the more missing values, the fewer information it used for clustering. So it is more sen-sitive to the missing rate. In the  X  X ix X  test set, where we remove the impact of weakly related attributes, the superiority of the sin-BNs is shown out under comparison. 4.3 Effectiveness of Probabilistic Reasoning In this part, we are going to show the effectiveness of the probability given in Eq(9) relatively high probability and in most cases, it gets the maximum probability. On the other hand, we are going to show that an attribute X  X  average probability can represent its imputation accuracy. observed from the result, the reasoning process helps to discover the truth value. 
In the second experiment, we eliminate 10% values of  X  X ushroom X  to compare its Table 7, the average probability almost follows the imputation accuracy, which means imputation quality from its probability. algorithm could be used to represent the validity of an imputation. stalk-surface-below-ring 0.69 0.73 processed on a BN, which is constructed in a different way to explore more correla-tions among all attributes in our algorithm. Experimental results show that (1)our BN performs well on classification problem,(2) the missing rate affects little on our impu-tation algorithm when the missing values are randomly distributed and(3) the proba-comparisons. 2012CB316200, NSFC grant 61003046 and NGFR 863 grant 2012AA011004. Doc-toral Fund of Ministry of Education of China (No.20102302120054). the Fundamen-tal Research Funds for the Central Universities(No. HIT. NSRIF. 2013064). 
