 ORIGINAL PAPER Jinying Chen 1  X  Huaigu Cao 2  X  Premkumar Natarajan 3 Abstract Automaticallyaccessinginformationfromuncon-strained image documents has important applications in business and government operations. These real-world appli-cations typically combine optical character recognition (OCR) with language and information technologies, such as machine translation (MT) and keyword spotting. OCR out-put has errors and presents unique challenges to late-stage processing. This paper addresses two of these challenges: (1) translating the output from Arabic handwriting OCR which lacks reliable sentence boundary markers, and (2) searching named entities which do not exist in the OCR vocabu-lary, therefore, completely missing from Arabic handwriting OCR output. We address these challenges by leveraging natural language processing technologies, specifically con-ditional random field-based sentence boundary detection and out-of-vocabulary (OOV) name detection. This approach significantly improves our state-of-the-art MT system and achieves MT scores close to that achieved by human seg-mentation. The output from OOV name detection was used as a novel feature for discriminative reranking, which sig-nificantly reduced the false alarm rate of OOV name search on OCR output. Our experiments also show substantial per-formance gains from integrating a variety of features from multiple resources, such as linguistic analysis, image layout analysis, and image text recognition.
 Keywords Image document analysis  X  Machine translation  X  Keyword search  X  Natural language processing  X  Optical handwriting recognition Automatic analysis of unconstrained image documents is extremely challenging. Earlier work was confined to office-quality printed documents and constrained tasks such as check processing or mail sorting due to the limitation of optical character recognition (OCR) systems [ 13 , 19 ]. Only with the recent advances in handwritten document recogni-tion have real-world applications that automatically access information from unconstrained image text become possible [ 5 , 10 ]. These are typically end-to-end systems, with lan-guage and information processing components built upon image document recognition and video text detection (see Fig. 1 ).

The error-prone nature of OCR output presents unique challenges to their high-level applications. In this paper, we present two such challenges we have faced in a government-sponsored project for image document analysis and trans-lation, and provide our solutions by integrating natural language processing (NLP) technologies into image docu-ment analysis.
The first challenge is to translate the output from Ara-bic handwriting OCR. Machine translation (MT) is a key step in analyzing foreign text and requires sentence-level processing. A lack of reliable sentence boundary markers in the output of Arabic handwriting OCR poses great chal-lenges for MT in both translation speed and accuracy. Our solution is a conditional random field (CRF)-based sentence boundary detector using rich features from automatic image layout and linguistic analyses. This approach significantly outperforms a rule-based method using punctuation match-ing and boosts the MT performance to a level close to that achieved by human segmentation. Our work is the first to show that MT on recognized image documents can be signif-icantly improved by automatic sentence boundary detection (SBD).

The second challenge comes from recognition-based key-word spotting, a task that retrieves recognized speech, image documents, and video text using keywords. This application has gained increasing popularity in multimedia information retrieval in recent years. In our work, we retrieve named enti-ties from the output of Arabic handwriting OCR. Keyword search on error-prone OCR output is much more challenging than on clean text. In an extreme (but not unusual) situa-tion, when a keyword occurs in an original document but is out of vocabulary (OOV) for the recognizer, it will be missing from the OCR output and a naive search algorithm will fail with zero recall. Our solution is to convert the OCR word lattices into character confusion networks (c-nets) and approximately match OOV keywords from c-nets. The out-put generated by this approach tends to have a high FAR (i.e., contain many irrelevant retrieved regions/segments). We propose a new method that improves search relevance by discriminative reranking using novel features. We show, by using CRF with linguistic and recognition features (e.g., OCR confidence scores), OOV name detection can provide a novel reranking feature that effectively improves both the local (within query) and global (cross queries) search rele-vance.

Furthermore, we investigated the effects of using differ-ent types of features for SBD and OOV name detection. Our experiments show substantial performance gains from com-bining features from multiple resources including linguistic analysis, image layout analysis and image text recognition. The remainder of this paper is organized as follows. In Sect. 2 , we introduce our general approach that uses CRFs to combine features from multiple resources for SBD and OOV name detection. In Sect. 3 , we introduce the specific challenges in SBD for Abrabic handwriting OCR and the rich features we designed, including novel image layout fea-tures, for this task. In Sect. 4 , we present our method that uses OOV name detection to generate a novel reranking fea-ture OOV name probability to improve OOV name search on OCR output. We introduce our experimental settings and results in Sects. 5 and 6 , respectively, and conclude our work in Sect. 7 . We chose linear-chain CRF models for both SBD and OOV name detection. Linear-chain CRFs are a common probabilistic framework for labeling sequential data [ 22 ] and have been successfully applied to a variety of NLP tasks [ 32 , 40 , 48 ]. A linear-chain CRF with parameters  X  ,..., X  label sequence y = y 1 ,..., y n given an input sequence x = x P ( y | x ) = where Z x isthenormalizationconstanttoensure y P ( y | x ) = 1. f j ( y i  X  1 , y i , x , i ) is the binary-valued feature function computed from the input x , the labels of the i th position in x ( y )anditspreviousposition( y i  X  1 ).Forexample,inSBD,the input is the sequence of words in each document, with labels end of sentence (EOS) and not end of sentence (NEOS). An example feature is defined as being 1 at position i if and only if y i is EOS, y i  X  1 is NEOS and x i is  X . X 
The most probable labeling sequence for an input x is defined by Eq. ( 2 ), which can be efficiently calculated by dynamic programming algorithms such as the Viterbi algo-rithm. y  X  = argmax The model parameters can be estimated by maximizing the log-likelihood of the training set. We used an open-source CRF toolkit 1 which implements efficient large-scale opti-mization with L-BFGS [ 25 ]. 2.1 Using CRFs for SBD and OOV name detection We treat SBD as a sequential labeling task, based on the key observation that two sentence boundaries are unlikely to be next to each other. A global inference on labels, such as sequential labeling, can reduce errors from predicting con-secutive sentence boundaries. In addition, CRFs have proved quite successful in speech segmentation [ 27 , 38 ].
For OOV name detection, since OOVs always cause OCR errors, detecting OOV names has the mixed nature of detect-ing named entities and detecting errors. CRFs have been successfully applied to named entity recognition [ 32 ] and show promising results in speech error detection [ 11 ]. There-fore, we believe CRF is suitable for this task.

Our preliminary experiments compared CRF with struc-tural support vector machines on SBD and compared CRF with a maximum entropy (MaxEnt) model on OOV name detection. These experiments confirmed our choice of the CRF model. 2.2 Using CRFs to combine features from multiple Conditional random fields allow for an easy integration of various knowledge sources. Under this framework, we were able to integrate a variety of features, including lexical and morphological features from local linguistic context and fea-tures from image analysis and text recognition, into a single model easily.

We use rich features from multiple resources for SBD to improve the model X  X  robustness when it takes on degraded input (Arabic handwriting OCR output in our case). For example, the Arabic OCR output does not always have a period  X . X  at the end of a sentence (as discussed in Sect. 3.2 ). Single-word lexical features will not work in this situation, while contextual features and image layout features (e.g., whether the line ends earlier than its previous line) can pro-vide useful cues for sentence boundaries.

In OOV name detection, we use different features to char-acterize different aspects of OOV name. Specifically, the linguistic (lexical and morphological) features are indicative for named entities and the recognition features are indicative for OCR errors. Automatic analysis of foreign image documents usually requires MT on OCR output. 3.1 Machine translation system Our MT system (Fig. 2 ) is a state-of-the-art system based on a string-to-tree translation model [ 41 ]. It uses two cate-gories of features. The first includes a small set of features, such as word and rule translation probabilities, language model scores, and target side dependency scores. The sec-ond includes a large number of features, similar to [ 12 ]. The system used a 3-gram language model (LM) for decoding and a 5-gram LM for rescoring. Both LMs were trained on billions of words of English news and web blogs. Features are combined by a log-linear model, and feature weights were set by optimizing the BLEU score on the MT development set. To adapt this system to our data in the legal filing domain, we augmented the translation model and the LMs with 0.5M in-domain data by assigning the in-domain data a high weight (5:1 for translation model and 3:1 for LMs). 3.2 Sentence boundary detection on OCR output Machine translation is typically done at sentence level. MT systems using tens of thousands of features, such as [ 5 , 12 ], are extremely slow and even exhaust memory when translat-ing un-segmented long text.

The OCR output for image documents is segmented line by line. Our preliminary experiments show that simply feeding the MT system with recognized lines degrades per-formance significantly compared to feeding it well-formed sentences (more than one point worse in BLEU and TER, respectively).

Detecting sentence boundaries on the output from Arabic handwriting OCR is more challenging than for clean English text, due to three reasons.
First, unlike languages which use punctuation to mark the end of a sentence (e.g., English), there is no consistent use of punctuation marks in Arabic writing systems. Some Arabic writers treat punctuation marks as redundant and do not write them; some use different symbols (e.g., using number zero as a period) [ 2 ].

Second, there is a high level of ambiguity of common sentence or clause connectors, such as  X   X  (and) [ 18 ].
Third, the Arabic OCR output has errors, especially for punctuation. Punctuation marks, such as  X - X  and  X . X  (writ-ten in the same way as in English), contain little glyph information and cause more confusion to OCR than con-tent words. Our OCR system has a 53% word error rate (WER) on the punctuation mark period ( X . X ), in contrast to the 25% WER on all the words. The errors can be deletion (e.g., suppressed some punctuation marks by noise remov-ing in preprocessing), insertion (e.g., misrecognized residue salt-and-pepper noise as punctuation), or substitution (e.g., confused a punctuation mark with other words). These OCR errors make punctuation-based SBD more susceptible to errors.

To overcome the above difficulties, we adopted supervised learning and used CRF to combine rich features. Specifically, we designed three types of features: lexical, morphological, and image layout.

The lexical features include the target word and its neigh-boring words within a window of size n . To smooth values for features seen in the test set but unseen in training, we made two specific treatments. First, we used regular expressions to identify words that contain Arabic or English numbers and classified them into: Date , Number , Number_like , and Word_with_Number . Second, we replaced low-frequency words (frequency &lt; m in the training data) with a single token UNK .Both m and n are set to 3 by experiments on the development set.

To generate morphological features, we ran MADA 2 on the OCR output. The output of the MADA analyzer for each input sentence is a sequence of tokens (by segmenting crit-ics from the words) with parts-of-speech (POS) tags. We designed two types of morphological features. The first rep-resentseachwordbythePOStagofitsbaseword.Thesecond represents each word by the POS tags of its critic(s) and base word. We use morphological features corresponding to the lexical features (i.e., the target word and its neighboring words).

Zimmermann [ 51 ] found that line breaks were indica-tive for sentence boundaries. We moved along this line by exploring novel page layout features, specifically relative line position features. Intuitively, an earlier ending of the current line compared with its preceding line is a strong indicator for the end of a sentence. Similarly, if the two end points of the current line are distant (horizontally or vertically) from those of the next line, the left end of the current line is likely to be a sentence boundary (Arabic is written from right to left). We designed six features to capture such information. is_end_of_line : whether it is the left endpoint of the cur-rent line left_x_rel_to_prev :the x coordinate of the left endpoint of the current line minus that of the left endpoint of the preced-ing line left_x_rel_to_next :the x coordinate of the left endpoint of the current line minus that of the left endpoint of the next line right_x_rel_to_next :the x coordinate of the right endpoint of the current line minus that of the right endpoint of the next line y_rel_to_next :the y coordinate of the current line minus that of the next line length_of_line : the physical length of the current line
Our image preprocessing module represents each line by a rectangular box with ( x , y ) coordinates of its four vertices. We estimate the y coordinate of a line by averaging the y coordinates of the four vertices and estimate the x coordinate of its left (right) endpoint by averaging the x coordinates of the two vertices on the left (right).

Except for is_end_of_line , which is 0 or 1, the other five features are real-valued. For each one, we discretize its values into 20 bins with each bin containing an almost equal number of training instances.

We also explored conjugate features such as concatena-tion of neighboring lexical and/or morphological features. Adding these features did not improve SBD on the develop-ment set, so we continue using the simpler model, which has a significantly reduced (by two orders) feature space and is also less susceptible to overfitting. 3.3 Related work Despite the rich literature in SBD [ 16 , 20 , 33 , 34 , 37 , 49 ], research in Arabic SBD has received little attention. Touir et al. [ 47 ] introduced a rule-based method based on semantic and cue phrase analysis. Khalifa et al. [ 18 ] disambiguated the different uses of connector  X   X  (and) by using support vec-tor machines. Both methods heavily rely on semantic and/or syntactic analysis of the Arabic text. Our approach is dif-ferent in that we only resort to lexical features and features from automatic morphological and image analyses to avoid tedious human analysis of Arabic text.

Al-Subaibin et al. [ 1 ] investigated the use of a list of sen-tence delimiters, including punctuation marks and newlines , to segment online colloquial Arabic text. We used a similar approach as our rule-based baseline.
Sentence segmentation has been well studied for auto-matic speech recognition (ASR), where prosodic cues from the speech recognizer and lexical cues are often used jointly to achieve the best results [ 27 , 42 ]. Such techniques have proved beneficial to MT on recognized speech [ 30 , 31 ]. Sim-ilar to the state of the arts in SBD for ASR, we used lexical cues. Unlike speech, image text does not have prosodic fea-tures. Instead, we developed novel image layout features to enhance SBD performance. Roark et al. [ 38 ] used reranking and syntactic features to improve SBD on English speech. We did not use syntactic features due to the lack of a robust Arabic parser.

Finally, Zimmermann [ 51 ] showed promising results from combiningahidden-eventLMandaMaxEntmodelandusing lexical, capital and line break features to segment the output form English OCR.

In the broader context of improving MT on noisy input, our work is also related to works that improve Arabic-to-English MT by automatic OCR error reduction [ 14 ] and by tight coupling of ASR and MT [ 52 ], although these works did not investigate the impact of SBD on MT. Our OCR system relies on LMs to reduce recognition errors. Investi-gating more, advanced technologies in error reduction and their effects on SBD and MT is out of the scope of this paper and can be an interesting topic for future research. In summary, SBD for Arabic is more challenging than for English. To our knowledge, our work is the first to address SBD for Arabic handwriting OCR and to provide a formal evaluation of its effects on MT. Keyword search (also called keyword spotting) is another way to access information from foreign image documents. For example, given a foreign name, the system returns a ranked list of image documents containing this name for manual review or automatic analysis (e.g., MT). Keyword spotting is different from statistical MT-based information retrieval. The latter first translates the foreign documents into English and then retrieves the documents by English keywords. The approaches of keyword spotting fall into two broad categories: OCR based and image feature based. The firsttypeofapproachsearcheskeywordsonoutputfromOCR and relies on a large-vocabulary, high-quality OCR system to generate the document pool. The second type is OCR-free, based on matching between feature vectors of document word images and query word image templates.

We chose the OCR-based approach for keyword spotting on Arabic handwritten documents because: (1) our large-vocabulary OCR system has state-of-the-art performance on Arabic handwriting recognition; (2) the OCR-based approach allows us to preprocess and index image docu-ments by in-vocabulary (IV) keywords off-line in advance, which reduces a tremendous on-line computational cost; (3) this approach avoids creating image-based models of query words (templates), a procedure required by image feature-based approaches and not scalable to scripts with multiple writers and a large number of queries; and (4) this approach can be further enhanced by advanced NLP technologies.
We focus on name spotting because named entities are common keywords for indexing audio [ 3 , 21 ], videotext [ 43 ] and image documents [ 44 ]. They are also important in infor-mation extraction (e.g., extracting the biography information for a person). Like other OCR applications, the performance of OCR-based keyword spotting is affected by recognition errors. OOV words, which are often names, are a major source of errors for current large-vocabulary OCR systems. Therefore, improving keyword spotting for OOV names is crucial for robust high-performance OCR-based search applications. 4.1 Keyword search system Our keyword search system (Fig. 3 ) is a hybrid system built on c-nets transformed from OCR word lattices [ 10 ]. The sys-tem indexes document images for in-vocabulary (IV) words by searching the words in the word c-nets. Given a query word, it checks whether it is in the OCR dictionary. If so, the indexed top-1000 documents for this query are returned. If not, it approximately matches the word on the character c-nets.

We chose c-nets because they offer compact representa-tions of lattices and can be searched more efficiently [ 28 ]. Our preliminary experiments on IV word search have also indicated that the c-net-based search has better performance (precision and recall trade-off) than the lattice-based search. Out-of-vocabulary queries are more challenging than IVs. OOV words never occur in the output of OCR systems that use a word lexicon to constrain and improve recognition out-put, such as our system. A naive search of OOV queries will fail with zero recall. To solve this problem, we approximately match OOVs on character c-nets. Approximate matching tends to have high recall, but a high FAR as well. To improve search relevance, we propose a new method that uses a Max-Ent model to rerank the retrieved regions with a set of novel features, including an NLP feature X  X he probability of the detected region being an OOV name. 4.2 OOV name detection We developed a CRF-based named entity detection model to learn the probabilities of IV and OOV names together in a single pass for 1-best OCR hypotheses extracted from word c-nets. The model learns three class labels: IV_name , OOV_name , and None . We collapse Person , Organization , GPE , and Location into a single category name after com-paring the effects of different label sets on the OOV name detection performance (measured by average precision) by preliminary experiments on the development set.

We designed three types of features: lexical, morpholog-ical, and recognition. The first two types of features are designed in the same way as SBD, except that words around the target word within a window of size 2 (as determined by experiments on the development set) are used as contextual features. The recognition features include the c-net posterior probability of the target word and the number of alternative c-net arcs associated with the target word.

Similar to the SBD experiments, we explored conju-gated features such as the concatenation of two neighboring linguistic and/or recognition features in our preliminary experiments. We did not see improvement on the develop-ment set, so we did not use them in our later experiments. 4.3 Discriminative reranking The top-n candidate regions (retrieved from character c-nets by dynamic programming-based approximate matching) for each query are used for reranking. Since a candidate is either relevant or not, we treat reranking as a classification problem which we solve by applying a MaxEnt model. The MaxEnt model outputs the probability of a candidate region being rel-evant conditioned on the observed features, which naturally reflects the degree of relevance and is suitable for the purpose of reranking.
 We designed six novel features for reranking:
OOV name probability : the probability of the candidate region being an OOV name
String matching score : the confidence score of approxi-mate string matching on character c-nets
Adjusted string matching score : the string matching score of the candidate region minus the string matching score of the top-1 candidate for the same query
Matching rank : the rank of the string matching score of the candidate region (measured for each individual query)
Estimated posterior probability : a weighted sum of the posterior probabilities of the 1-best arcs in the word c-net that overlap with the candidate region (called associated arcs ). The weights are the overlap ratios for the candidate region and its associated arcs
Maximum overlap ratio : the maximum value chosen from the overlap ratios between the candidate region and its asso-ciated arcs or their concatenation.
 The first feature is obtained by applying the CRF-based OOV name detection, as calculated by Eqs. ( 3 ) and ( 4 ). P OR ( c ,w) = where c is the retrieved candidate region; w is any word from the 1-best OCR hypotheses extracted from the word c-nets; span(.) is the span of a region or word in the document image, as detected by the OCR system; OR(.) is the overlap ratio of two spans; P oo v _ name (w) is the output, i.e., the probability of w being an OOV name, from the CRF OOV name detector.
The second, third, and fourth features represent the quality of the approximate matching across or within queries. To compute the matching rank feature, instead of using the rank numbers directly, we group them into different levels. An analysis of search output on the development set suggests that top matching ranks are much more indicative than low ranks; therefore, we group the top-10 matching ranks into four groups with fine granularity ({1}, {2}, {3, 4, 5}, {6, 7, 8, 9, 10}) and group the lower ranks into equal size groups with 10 members in each group.

The last two features are used to estimate how likely the detected region is a word, and they are calculated using Eqs. ( 5 ) and ( 6 ), respectively.
 P where w is defined as in Eq. ( 3 ) and P c  X  net (w) is the posterior probability of the word on the word c-net. max _ o v erlap _ ratio = max where r is an arc or the concatenation of consecutive arc sin the word c-nets.

Except for the matching rank feature, all the other features have real values. For each one, we discretize its values into 100bins with each bin containing an almost equal number of training instances. 4.4 Related work Little attention has been paid to OOV keyword spotting in the literature. OOV queries are simply rejected by many search engines. Methods using character (or sub-character) lattices, such as those described in [ 4 , 7 , 17 , 43 , 46 , 50 ], can support OOV search. However, these methods often suffer from a high FAR and high computational cost [ 43 , 46 ]. In fact, OOV word spotting has never been the focus of these works.
There has been significant work in detecting OOV regions inautomaticspeechrecognition.Ourworkisrelevanttothose that use features from the recognition output, such as con-fidence scores [ 45 ], misalignment between word and phone lattices [ 23 ], contextual information [ 35 ] and fragment pos-teriors [ 36 ]. The major difference between our work and those works is that we not only detect OOV regions, but also retrieve segments that contain a specified OOV query word.
Our approach is novel in three aspects: (1) it searches OOV names from character c-nets for efficiency; (2) it uses word boundary information from word lattices to reduce FAR; and (3) it reranks retrieved regions using novel features to further improve the search relevance. Due to the scope and space lim-itations, we only report our experiments on reranking using the OOV name probability feature here. Details of matching OOV queries on character c-nets can be found in [ 10 ]. 5.1 Data sets The data we experiment with are from the challenge track of the MADCAT project. It contains thousands of image documents with unconstrained handwritten and typewrit-ten text collected from the field (legal filing) domain in the 1980s (see Fig. 4 for two examples). These documents were scanned monochromatically at 200dpi and were transcribed and annotated by LDC.

Fromthiscorpus,thehumanannotatorsselectedhandwrit-tendocumentsifmorethan90%ofthewordsinthedocument are handwritten. Multiple genres, such as memos, letters, and tables, are identified from the dataset. The collection demon-strates most of the typical challenges in unconstrained offline handwriting recognition, such as cursive writing, multiple writers, and diversified contents. We focus on handwritten databecauseit is morechallengingfor OCRandits high-level applications. Our OCR system has a much higher WER on handwritten data than typewritten data in this corpus (25% vs. 10%). 5.2 OCR system for Arabic handwriting recognition We use a state-of-the-art OCR system [ 5 , 6 , 39 ] to recognize Arabic handwritten documents. The system is based on the hidden Markov model (HMM) and statistical LMs.

Before training and recognition, preprocessing steps are applied to remove the noise and artifacts and detect line images [ 5 ]. A presegmented text line image is represented as a time series of features. Script-independent features such as image intensity percentile, orientation of the stroke, frame energy, gradient, concavity, and Gabor filter (48-dim/frame, four orientations, center of frequency matches stroke) responses are computed for each sliding window of the image and projected into 17-dimensional features using linear discriminative analysis (LDA). We use Gabor filters because they have proved useful for character recognition, especially for degraded text [ 8 , 24 ].

A 14-state, context-dependent HMM with left-to-right state transition is used to model each individual charac-ter. Each state of the HMM has an output probability distribution over the features modeled as a Gaussian mix-ture. Every three consecutive characters are used to model ligatures associated with the center character; this is moti-vated by observing that the shape of the character glyph often varies depending on its neighboring characters in Ara-bic handwritten text. Character HMMs are concatenated to form a larger HMM, which can be trained by text lines in arbitrary length without word/character boundary annota-tion.

During training, a character-tied HMM (CTM) is first applied to creating state labels of the training data, which are needed in estimating the LDA feature transform. With the LDA matrix, a state-tied HMM (STM) is trained and used as the final glyph model.

For recognition, the glyph model and a 3-gram LM trained on3.2millionwordsofin-domaindatawitha600Kworddic-tionary are used to generate the n -best hypotheses ( n =300) and the word lattice for each line image. The n -best is reranked by optimizing the weights of the glyph model and the LM using a held-out development set. For each page, the reranked best hypotheses are taken as the reference to adapt the HMM using maximum likelihood linear regres-sion (MLLR) [ 15 ]. The page is then decoded again using the adapted HMM.

Figure 5 shows the major components and the training and decoding processes of our OCR system. 5.3 Experimental settings for SBD and MT experiments The data sets we used to train and test our OCR system, MT system and sentence boundary detector are summarized in Table 1 . The data for OCR training and test were created by LDC in support of the challenge track of the MADCAT project. The image data were also annotated with sentence boundaries by LDC to enable MT evaluation. We merged the transcriptions of the OCR training data and the OCR output of the OCR development data and then randomly split them into the training and development sets for SBD. The MT development and test sets are subsets of the OCR/SBD test set.

We used 50M news and web data from the GALE pro-gram and 0.5M in-domain data to train our MT system. The in-domain data included 349K LDC translations and 164K BBN translations of Arabic handwritten documents from the same source as the OCR training data. We ensure that there is no overlap among the training, development and test sets for each task, so our system will not overfit and degrade in the final evaluation by the government.

We set the CRF model X  X  regularization parameter for each experimental condition, respectively, by testing a range of To segment the OCR output for the MT test set, we use the threshold that corresponds to the 0.01 FAR on the SBD devel-opment set. If the CRF detector assigns a probability higher than this threshold to a word in the OCR output, the word is regarded as a sentence boundary. The 0.01 FAR value is chosen by MT experiments on the development set, in which we tried a range of values of (0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05).

We measure the standalone performance of SBD by DET (detection error trade-off) curves [ 29 ]. A DET curve plots the miss probability (1  X  recall) versus the FARs (the number of false positive/the number of true negative), using the normal deviate scale. DET curves are better for illustrating the sys-tem differences in SBD rather than using the traditional ROC curves [ 26 ]. In addition to DET curves, we also compare the highest F -scores of different SBD methods.

We compare our CRF sentence boundary detector with two baselines: rule_seg : segment the documents by punctuation marks  X . X ,  X ! X ,  X ? X  and Arabic question mark. This is the method used by our current MT system gold_seg : the ground-truth segmentation provided by LDC. This can be seen as an upper bound.

To evaluate MT on the automatically segmented OCR output, we resegment the MT output into the same num-ber of segments as in the ground-truth translations by using an approach similar to [ 31 ]. The difference is that we apply MT on the OCR output marked by ground-truth sentence boundaries and use this MT output as the reference to align other MT output with it. Data analysis suggests that this methodis abletoreducealignment errors causedby X  X seudo X  translation errors (e.g., when the MT system systematically chooses different words than the ground-truth translations to express the same meaning). Concretely, we follow four steps: 1. concatenate segments in each document for both the tar-2. align each target-reference document pair at word level 3. break the target MT output at positions aligned with the 4. compute sentence-level TER and BLEU. 5.4 Experimental settings for OOV name search We annotated 0.4M OCR ground-truth transcriptions from LDC with named entities. To generate the training data for OOV name detection, we ran the OCR system on images associated with this data set and automatically aligned the 1-besthypothesesfromtheOCRwordc-netswiththeannotated transcriptions by using the NIST SCLITE scoring package. Table 2 summarizes the data used in our experiments, and Table 3 provides the OOV name statistics.

The performance of CRF-based OOV name detection is measured by the model X  X  detection rate at 0.1 FAR and average precision (AP). We set the model X  X  regularization parameter to 0.2 after testing (0.05, 0.1, 0.2, 0.5, 1) on the development set.
The search output (top-n per query, n =100 or 1000) on the development set is used to train the MaxEnt reranking model. Because the OOV name queries are unseen during training, it is impossible to train query-specific reranking models. Instead, we trained a single model using the search output of hundreds of OOV names from the development set. OOV names occur only one or two times; therefore, the input for reranking needs to have a high recall to ensure that most querieshaveatleastonehit.Therecallonthedevelopmentset is 0.81 for top-100 output and 0.93 for top-1000. We evaluate reranking performance for these two cutoff values. To train the reranking model, we create a held-out (development) set by randomly picking 1/4 positive (relevant) instances and 1/4 negative instances from the training set described above. We repeat this procedure 10 times to train 10 MaxEnt models and use their average score to rerank the test data. The reg-ularization parameter for each MaxEnt model (the Gaussian prior) was set separately using their own development set.

We measure reranking performance by DET curves and mean average precision (MAP) at sentence level. 6.1 SBD and MT experiments As shown in Fig. 6 , the CRF model using all features (blue dotted line, lex+morph+layout ) outperforms the rule-based method (pink star) significantly for segmenting the OCR out-put. The highest F -score it achieves is also much higher than the rule-based method (0.837 vs. 0.293; Table 4 , column 1, rows 4 and 1).

Figure 7 separates the effects of image layout features (red solid line, layout ) and linguistic (black dash-dotted line, lex+morph ) features. As we can see, the DET curves of these two models cross at 0.02 FAR. Their highest F -scores (Table 4 , column 1, rows 2 and 3) are similar (0.775 vs. 0.754) and are both lower than their combination (0.837). This suggests that both image and linguistic features con-tribute significantly to the best performance achieved by the full model.

In Fig. 7 , the DET curve for the full model (blue dotted line) crosses with the partial models (red solid and black dash-dotted lines) at the area with low FAR (0.001) and high miss probability ( &gt; 0.7). However, our experiments on the MT development set suggest that a miss probability lower than 0.5 on SBD is needed to achieve good MT performance. Table 4 shows the effects of SBD on MT performance. The three CRF detectors (rows 2, 3, and 4) all perform sig-nificantly better than the rule-based method ( p &lt; 0 . They also achieve performance close to that produced by human segmentation. The performance difference between the CRF full model (row 4) and the human segmentation (row 5) is not significant ( p &lt; 0 . 085). The full model has performance gains of about 0.5 TER and 0.2 BLEU over the two partial models (Table 4 , row 4 vs. rows 2 and 3). Though not large, this difference is significant ( p &lt; 0 . 01).
On a 3.00GHz Intel Xeon Processor, our SBD model has a speed of 5ms/word (200words/s) and a peak memory use of 60M. 6.2 OOV name search experiments Table 5 shows the standalone performance of OOV name detection. As we can see, the CRF OOV name detector using the full feature set has a 75 . 4 % detection rate (i.e., recall) at 0.1 FAR and 21.3% AP on the test set. Its performance is also much higher than the two partial models (Table 5 ,row 1 vs. rows 2 and 3).

In the reranking experiments, we use the best system in our search experiments (which restricts c-net search by word boundaryinformationina soft way)asthebaseline.Asshown in Table 6 , the discriminative reranking improves MAP by 2.8% (51.0 % vs. 48.2 %; absolute gain) for cutoff 100 and 2.3% (49.3% vs. 47.0%) for cutoff 1000 (row 3 vs. row 1). The improvement is consistent when measured by the DET curves (Fig. 8 , black dash-dotted line vs. red solid line).
To test the effect of OOV name probability on rerank-ing, we compare the reranking performance between using and not using this feature. The results show that the MAP of reranking without using this feature (row 2 in Table 6 ) dropped by 2.6% (absolute loss) for cutoff 100 and 1.5% for cutoff 1000 compared with the full model (row 3 in Table 6 ).
The DET curves in Fig. 8 (blue dotted line vs. black dash-dotted line) show the similar trend in performance drop when notusingthisfeature.DETcurvesmeasuretheglobalranking quality for all involved queries [ 29 ]. MAP is more sensitive to local ranking quality, especially the quality of top-ranked candidates for each query. Our experimental results suggest that our features for reranking can capture multiple aspects of search ranking quality. Specifically, the OOV name prob-ability feature is useful for improving both global and local ranking qualities.

On a 3.00GHz Intel Xeon Processor, our OOV name detector has a speed of 5ms/word (200words/s) and a peak memory use of 5M. Reranking OOV name search output by combining 10 MaxEnt models takes 0.2s per query and 8M memory. In practice, single model or parallel computing can be used to improve speed further. We have presented two real-world applications of Arabic handwritten document analysis, in which we effectively inte-grate NLP with document image analysis.

In the first application, our CRF-based SBD model using rich features achieves significantly better performance than the rule-based method, as evaluated by both the standalone measures and the MT scores. Both linguistic and image lay-out features contribute significantly to the best performance. This suggests that a machine learning-based approach using features from multiple resources is beneficial and necessary for automatic segmentation of the output from Arabic hand-writing OCR.

The performance difference between different SBD meth-ods is less obvious when measured by MT scores than measured in a standalone way. This is expected because the MT scores are also affected by other factors such as OCR errors and translation errors. It is possible that a portion of the SBD improvements did not transfer to MT due to these errors. For the same reason, although the MT performance achieved by the best automatic segmentation (i.e., the CRF full model) is close to human segmentation, it does not imply that automatic segmentation is perfect. In fact, the highest F-score of the automatic method is 0.837, still much lower than 1. Though complicated by other factors, the in vivo eval-uation is necessary in order to know whether an SBD method can benefit MT. Showing positive evidence from such an evaluation is one highlight that distinguishes our work from previous work that applied SBD to OCR output.

In the second application, we use a MaxEnt model that integrates an NLP feature, the OOV name probability , with other recognition-based features to rerank OOV name search output. In the literature, recognition-based features such as confidence scores are predominantly used in recognition-basedkeywordspotting.Thoughalsoheavilyrelyingonthese features, our work is original in that, by integrating the NLP feature, it effectively improves both the global and the local searchrelevance.Furthermore,weshowthattheperformance of CRF-based OOV name detection benefits from combining linguistic features and recognition features.

The two applications described in this paper provide good examples of how image document analysis creates new chal-lenges and research opportunities for NLP. SBD and MT become more challenging due to OCR errors. Keyword spotting, which is trivial on clean text, becomes a difficult problem when applied to error-prone OCR output.

We set the scope of our discussions within the scenarios of machine translation and keyword spotting for Arabic hand-written documents. However, the approaches and techniques introduced in this paper are general and can be extended to other genres and problems. We have successfully applied the same approach to SBD for Arabic news, scientific journals, and typewritten documents, and to predicting OCR errors [ 9 ].
