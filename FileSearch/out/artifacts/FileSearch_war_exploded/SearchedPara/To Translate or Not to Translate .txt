 Query translation is an important task in cross-language in-formation retrieval (CLIR) aiming to translate queries into languages used in documents. The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. Some untranslated terms cause irreparable performance drop while others do not. We propose an approach to estimate the translation probability of a query term, which helps decide if it should be translated or not. The approach learns regression and classification models based on a rich set of linguistic and statistical properties of the term. Experiments on NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks demonstrate that the proposed approach can significantly improve CLIR performance. An in-depth analysis is also provided for dis-cussing the impact of untranslated out-of-vocabulary (OOV) query terms and translation quality of non-OOV query terms on CLIR performance.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithm, Experimentation, Performance Query Translation, Translation Quality, Query Term Per-formance, Cross-language Information Retrieval
Query translation, which aims to translate queries in one language into another used in documents, has been widely adopted in CLIR. Conventional approaches to query trans-lation have focused mainly on correctly translating as many query terms as possible, including translation disambigua-tion [3, 8, 9], phrasal translation [17, 11], and unknown
Figure 1: AP for each untranslated query term. words translation [7]. Such approaches pursue the reduction of erroneous or irrelevant translations in hope that the CLIR performance could approach to that of monolingual informa-tion retrieval (MIR). However, the accuracy of query trans-lation is not always perfect. Each query term has a risk of being translated incorrectly. Some incorrect translations can be remedied in the process of MIR but others may cause ir-reparable retrieval performance drop. In other words, query translation may cause deterioration of CLIR performance. This phenomenon motivates us to explore whether a query term should be translated or not.

Consider the query:  X  X eru President, Fujimori, bribery scandal, the 2000 election, exile abroad, impeach, Congress of Peru X , which is obtained based on the description field from a NTCIR-5 English-Chinese CLIR topic (after stop words removal). Its correct Chinese translations result in av-erage precision (AP) of 0.5914 for CLIR. Figure 1 shows that if one of the query terms is not translated (x-axis), how the corresponding AP (y-axis) changes using the correct transla-tions of the rest of terms as a query. It is observed that with-out correct translation of  X  X ujimor X  or  X  X ribery scandal X , we are far from satisfying retrieval performance, compared to AP of 0.5914 (dash line). However, on the other hand, we find interestingly that if the correct translation of  X  X eru President X  or  X  X ongress of Peru X  is ignored, a better AP can be even achieved. Still the missing of the translation such as  X  X he 2000 election X ,  X  X xile abroad X , or  X  X mpeach X  seems to be tolerable. This observation reveals that some untranslated terms cause irreparable performance drop while others do not. That is to say, the query terms are not equally impor-tant for translation, and it is not always the case that all translations are required.

In the above example, term  X  X ujimori X  seems to bear more important semantics and thus should be translated. It might appear OOV terms always need perfect translations. Take into account the query from another NTCIR-5 English-Chinese topic (after stop words removal):  X  X hinese-American, scien-tist, Wen-Ho Lee, suspect, steal, classified information, nu-clear weapon, US X  X  Los Alamos National Laboratory X . It could be found that the AP decreases 45.9% when  X  X en-Ho Lee X  is not translated, whereas untranslated  X  X S X  X  Los Alamos National Laboratory X  conversely helps improve 39.6% of AP. Although missing the translation of  X  X S X  X  Los Alamos National Laboratory X  loses some information about the query, we notice that term  X   X   X   X   X  (laboratory) luckily emerges in its (post-translation) feedback documents, which alleviates the problem. Moreover, there are many possible translitera-tions of  X  X os Alamos X  in Chinese such as  X   X   X   X   X   X  and  X   X   X   X   X  X  X   X , which introduce a further mismatching prob-lem in MIR and are harmful to the retrieval. This example illustrates that sometimes leaving an OOV term untrans-lated would probably be a reasonable choice.

Conventional approaches to query translation mostly put efforts in finding accurate translations [15] or examining how translation resources affect CLIR performance [12, 16]. Few did pose the problem of predicting CLIR performance or whether to translate a query term or not. Our most rel-evant work [10] presented a method to predict the perfor-mance of CLIR according to translation quality and ease of queries. Yet [10] focused merely on evaluating the perfor-mance of a whole query and did not give insight into the effect of translation for each query term. Also, [10] did not propose the issue of translation necessity which potentially helps improve retrieval performance.

The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. We are interested in realizing (1) the possibility of predicting a query term to be translated or not; (2) whether the prediction can effectively improve CLIR performance; and (3) how untranslated OOV and various translations of non-OOV terms affect CLIR performance.

We propose an approach to estimate the translation prob-ability of a query term according to its effect on CLIR. The translation probability serves as a basis for the de-cision to translate the query term or not. The proposed approach learns classification and regression models, where comprehensive factors that are essential in determination of CLIR performance are considered, inclusive of linguistic, statistical, and CLIR features in source and target language corpora. Experiments on NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks show that CLIR performance can be significantly improved based on our approach. Such effec-tiveness is consistent across different translation approaches as well as benchmarks. An in-depth analysis of how untrans-lated OOV terms and translation quality of non-OOV terms influence CLIR performance is also provided. We highlight that query terms needing no translation may result from in-trinsical ineffectiveness in CLIR, semantic recovery by query expansion, or poor translation quality.
Improving translation accuracy is important for query trans-lation. Phrasal translation approach [17, 11] was inspected for improving CLIR performance. Disambiguation of multiple-sense terms by estimating co-occurrence for each chandi-date[3] has also shown evident accuracy enhancement. Some others utilized statistical properties in parallel corpus [5, 13] as well as query expansion techniques [2] in search of better translation accuracy. Machine translation techniques [18, 14] are effective for long sentences, but they are not suit-able for short, context-inadequate queries. Though these works have brought significant improvement in translation accuracy, they eventually tried to translate as many terms as possible, which we believe is not always an effective ap-proach in CLIR.
 Our work is also related to term selection from a query. The focus of previous works[1, 4] did key-term selection in the mono-lingual environment; however, our discovery of various causes such as pre-and post-translation query expansion would influence the preference of translation in CLIR.

Our most relevant work [10] has developed regression mod-els for predicting the performance of CLIR. The translation quality and ease of query were taken into account. Their concern was evaluated on a whole query, whereas we think every single term has its own impact on CLIR performance. Moreover, their method focused on the goodness of their regression models, while we aim to learn the need of trans-lation for each term and bring up CLIR performance.
Given a query topic Q s = { s 1 ,s 2 ,...,s n } in source lan-guage, conventional query translation methods endeavor to find a set of translated terms Q t = { t 1 ,t 2 ,...,t m } in target language. Various translation methodologies such as phrasal translation or sense disambiguation have brought significant improvements in CLIR. Particularly, they incorporate dic-tionaries, bilingual corpora, or the Web to estimate the prob-ability of translation p ( t j | s i ,Q s ). This probability shows how good it will be to translate s i to t j given topic Q shown in Fig. 2. p ( t j | s i ,Q s ) also means the translation de-pends on not only s i and t j but the rest of terms in Q s simplicity, some previous works focused on p ( t j | s assumption that the probability is irrelevant from Q s .
As illustrated in Fig. 1, the effect of query term translation may differ from one to another. Noticing this point, the goal of this paper does not focus on seeking high translation ac-curacy. Rather, we are interested in realizing, given a source term s i , whether it should be translated or not. This can be casted as a classification problem by introducing a binary variable T  X  { 0 , 1 } . T = 1 and T = 0 represent should-be-translated and should-not-be-translated, respectively, w.r.t. a given source term. When bringing variable T in estima-tion of p ( t j | s i ,Q s ), we get the following: p ( t j | s i ,Q s ,T = 0) equals 0 because the probability of translation from s i to t j is 0 given that s i should not be translated. The final probability is composed of two parts. p ( T = 1 | s i ,Q s ) estimates if it is suggested that s be translated, whereas p ( t j | s i ,Q s ,T = 1) shows how proper it is to translate s i to some particular t j . This point is il-lustrated in Fig. 3 with the newly introduced variable T , where s i is mapped to t j only if it is worth being trans-lated ( T = 1). The focus of previous work lies in generat-ing translation equivalences based on p ( t j | s i ,Q s ,T = 1) or p ( t j | s i ,Q s ), since every s i is required to be translated by de-fault. Yet the goal of this paper is to predict the probability p ( T = 1 | s i ,Q s ), which concerns whether to translate s not.

Given Q s = { s 1 ,s 2 ,...,s n } , we formulate our problem by seeking a classifier c : Q s  X  T , and the classification gives probabilities of 0 or 1 in the prediction process. To estimate real numbered probabilities, we resort to finding regression function r : Q s  X  R which predicts a value indicating the necessity of translating each s i . With the regressed value, say r ( s i ), as input of the Sigmoid function, we can obtain the probability of translating or not-translating ranged within [0 , 1]. Mathematically,
With the probabilities (either binary or real numbered) in hand, we could use them in CLIR retrieval models by inte-simply use p ( T = 1 | s i ,Q s ) to translate worthily-translated terms from Q s . This approach enjoys the flexibility and extendability across various frameworks, because translat-ing some portion of query terms is independent of what retrieval models are adopted. The probabilities based on Sigmoid function rank the source terms with a permutation
Based on classifier c , query terms are easily classified ac-cording to their needs of translation. Similarly, based on re-gression r , top k query terms { s  X  (1) ,s  X  (2) ,...,s  X  ( k ) selected to be translated. Four different translation strate-gies and various threshold k X  X  have been examined in Sec-tions 4 and 5. Here, we apply support vector machine (SVM) and support vector regression (SVR) [6] to do classification and regression; other alternatives can also be adopted. Finally, we define that the regressed value r ( s i ) equals to LR CLIR ( s i ) with the following formula: where  X  CLIR ( q ) is the AP measure for query q in CLIR. The larger the loss ratio LR CLIR ( s i ) is, the more importantly we translate s i due to its better effectiveness in CLIR.
We develop the regression function r : Q s  X  R by learn-ing examples in the form of &lt;f ( s i ) ,LR CLIR ( s Section 3.2. For classifier c : Q s  X  T , the form would be &lt;f ( s i ) ,sign ( LR CLIR ( s i )) &gt; , where sign ( x ) converts LR into non-negative and negative classes based on x.
We utilize linguistic (Ling), statistical (Stat), and CLIR features f ( s i ) of query term s i to capture its characteristics from different aspects. We denote t j as the corresponding translation of s i in target language.

Table 1 shows all features adopted. Linguistic features in-clude parts of speech (POS), named entities (NE), acronym, phrase, and size (number of words in a term). More pre-cisely, the POS features contain noun, verb, adjective, and adverb, while the NE features comprise person names, loca-tions, organizations, event, and time. POS and NE in our experiments are binary and are labeled manually.

Statistical features are good predictors from the view-points of document corpora. In our experiment, both source and target language corpora are used. Here, we consider co-occurrence, context, and TFIDF features. Co-occurrence features reveal the degree of how often a term co-exists with others, and hence the degree of semantic substitutions by them. The more a term can be replaced by others, the less likely it needs to be exactly translated. Pointwise mutual information (PMI) is adopted for calculation in pre-and post-translation corpora. Given two sets of terms x and y, we measure their co-existence level by
In addition, context features are helpful for low frequency query terms that yet share common contexts in search re-sults. The context vector v ( t ) takes term(s) t as input, and is composed of a list of search-result pairs &lt; document ID, relevance score &gt; returned by retrieval systems. Given two vectors x and y, we measure their cohesive level by
For those pairwisely computed sets in co-occurrence or context features, we extract their maximal, minimal, and average values as the features for the corresponding term.
TFIDF features show a term X  X  capability of distinguish-ing relevant documents from irrelevant ones. We compute TFIDF in both source and target language corpora for each term.

CLIR features are the key to learning what characteristics make a term favorable or adverse for translation. We define translation, expansion, and replacement features.

Translation features such as the number of translations a term encompasses measure the degree of ambiguity accord-ing to dictionary knowledge. Also, we use binary feature isOOV to indicate if a term exists within the coverage of dictionary.

Expansion features express if the losing information from an untranslated term can be recovered by the semantics from the rest of terms with query expansion. Query expansion in source language reserves the room for untranslated terms by including relevant terms in advance. Also, query expansion in target language recovers the semantics loss by inspect-ing the rest well-translated terms. Here we denote QE ( q ) as the set of expanded terms obtained from the search re-sults of query q by query expansion, and  X  can either be pmi () or cos () functions. From formulas in Table 1, the mea-surements estimate the similarity between expanded terms derived with or without term s i . The same calculation is re-Type Feature Description Ling Stat CLIR peated in target language for each t j . It is inferred that the more the two expanded sets of terms resemble each other, the more likely the loss information from untranslated s i be made up.

Lastly, replacement features estimate if the rest of terms, ( Q s  X  X  s i } ), within the same topic together with its expanded terms set, QE ( Q s  X  X  s i } ), can take the place of s i replacement intension is strong, it implies translation of only the rest of terms is sufficient for retrieval. In other words, QE ( Q s  X  X  s i } ) replaces the position of s i in original query Q , while QE ( Q t  X  X  t j } ) substitutes the semantics of t query Q t . The data used in the experiments includes NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks, whose statistics in the title and description fields of English topics can be found in Table 2 (after data clean). The poorly-performing queries whose AP is below 0.02 are filtered to ensure the quality of our training data for classification and regression models. Table 3 shows the numbers of OOV and non-OOV terms in detail for each task. Note  X  X erm X  refers to manual segmenta-tion on original topic words after stop words removal, which forms a set of semantic-rich building blocks. We construct the vector space model (TFIDF), the language model (In-dri), and the probabilistic model (Okapi) using the Lemur Toolkit 1 . Both queries and documents are stemmed with Porter stemmer and filtered with standard stop words lists. We use mean average precision (MAP) as performance met-ric evaluating over top 1000 documents retrieved. To avoid inside test, 5-fold cross validation is used through the entire experiments.

We use correct translations in the benchmarks to train the regression and classification models. The correct transla-tions are available since NTCIR-4 and NTCIR-5 CLIR tasks
Lemur Project: http://www.lemurproject.org provide both English and Chinese topics at the same time. Usage of correct translations shall help reveal the necessity of translation. NTCIR-4 and NTCIR-5 CLIR tasks also pro-vide English and Chinese documents, which are used as the source and target language corpora, respectively. Note that the English and Chinese documents are not parallel texts. Table 2: Data set of English topics(after data clean) NTCIR4 title 44 216 4.90 desc 58 865 14.90 NTCIR5 title 35 198 5.65 desc 47 623 13.20 Table 3: Numbers of OOV and non-OOV terms NTCIR4 title 154 15 (9.8%) 139 (90.2%) desc 298 15 (5.0%) 283 (95.0%)
NTCIR5 title 131 27 (20.6%) 104 (79.4%) desc 277 36 (13.0%) 241 (87.0%)
The coefficient of determination R 2 measures how well future outcomes are likely to be predicted by the statistical models. A higher R 2 gives us more confidence in prediction. We train and test the regression models under a variety of features and document collections. Table 4 demonstrates the R 2 results.

Averagely speaking, the best regression performance can be achieved when both pre-and post-translation corpora is used, as query expansion is often helpful in CLIR. Also, it shows that a higher R 2 can be found in post-translation corpus than in pre-translation one. This is because post-translation corpus can provide more effective expanded terms for MIR in the set of target documents. Moreover, within each corpus setting, we go into details to inspect the ef-fectiveness using different features. Statistical features con-sistently achieve better R 2 than CLIR features, which are followed by linguistic features ( R 2 of linguistic features is the same across different corpora since such properties re-main still despite change of languages). It is caused by that statistical features reflect the underlying distribution of translated terms in the document collection, and also that CLIR features reveal the degree of translation necessity. Fi-nally, a larger R 2 can be achieved by including more features for training. We also review the classification accuracy un-der the same settings, where similar results can be found. Roughly speaking, overall classification accuracy climbs up to 80 . 15% when all features are adopted. As linguistic, sta-tistical and CLIR features are complementary, we use all of the features in the following experiments.
By inspecting correlation between the features and MAP, we may have better understanding of the effectiveness of our features. Three standard measurements inclusive of Pear-son X  X  product-moment, Kendall X  X  tau and Spearman X  X  rho are adopted.
Figure 4 depicts a wholesome picture of all features, where the absolute value of correlation using Okapi on NTCIR-4 data is shown. Clearly, classic TFIDF features show its dis-criminative power in identifying terms that need translation. Context features are effective through inspecting retrieval results, but such features meantime suffer from higher cost of computation. Another group of useful features are CLIR features. As mentioned previously, CLIR features are crucial for estimation of semantic recovery, which is captured by ex-pansion and replacement features. It is worth noticing that the  X  X sOOV X  feature is evidently correlated to retrieval per-formance. It again assures that efforts in translating OOV terms are significant for CLIR, as indicated by previous work [7]. Lastly,  X  X rans-size X , which records the number of trans-lations for each term, is negatively correlated to MAP (posi-tive in Fig. 4 because of absolute value). The more senses (or translations) a term contains, the more challenging correct translation can be detected.

Linguistic features such as NE are relatively important for search. It is not always the case yet is usually true. A named entity often has unshirkable responsibility in describ-ing the information needs especially for short queries. And this is why NE is much more correlated to MAP than POS is. Overall, statistical features are more powerful than lin-guistic ones. Specifically, context features are more effective than co-occurrence features. CLIR features also contribute substantially in translation estimation.
In this section, we show the effectiveness of our approach for CLIR. We use NTCIR-4 and NTCIR-5 English-Chinese tasks for evaluation and consider both &lt; title &gt; and &lt; desc &gt; fields as queries. We use 5-fold cross-validation and ensure that a test instance would not appear in the training set.
Table 5 shows the MAP results using translated queries for search. Based on the pre-trained model, we X  X  like to test if we can improve the CLIR performance with 4 different translation strategies. Each strategy generates its own t given source term s i .  X  X orrect Trans X  gives the standard translations in the benchmark, which is also recognized as MIR;  X  X oogle Dict top1 X  extracts the first translation from Google Dictionary 2 ;  X  X oogle Dict all X  combines all possi-ble translations from Google Dictionary for a given term; finally  X  X oogle Trans X  returns the translations from Google Translation 3 . Moreover, for each setting, we show its base-line and upper bound performance. The baseline methods (BL) suppose entire terms in Q s need to be translated, and simply combine all the translated terms in Q t as one query total 2 m  X  1 possible subclasses of Q t , considering that each s in Q s can be translated to t j or not. We construct the upper bounds (UB) by discovering the subclass (sub-query as well) with the highest AP. We also run the two-sample pairwise significance test against BL.

From Table 5, our classification (C) and regression (R) models consistently outperform the baseline methods using different translation strategies. The retrieval result proves
Google Dictionary: http://www.google.com.tw/dictionary
Google Translation: http://translate.google.com/?hl=en# Figure 5: MAP with various k (number of top terms translated) on different dataset. our observation that some terms are  X  X eant to be X  trans-lated while others are not. It is also worth noticing that the improvement rate of description queries is larger than title queries. As longer queries have more chances to encompass noisy terms, we can thereby improve retrieval performance by not translating them. Short queries such as Web queries, on the other hand, lose a great amount of information if a term cannot be well translated. Further, comparing the im-provement rate between different translation strategies, we find that  X  X oogle Dict all X  leaves the most room for improve-ment. We attribute this to the ambiguity it involves in due to inclusion all translations in dictionary. Fig. 5 illustrates the impact of the variable k.
In this section, we discuss the effect of translating OOV and non-OOV query terms on CLIR. We will explore what factors make a query term favorable for translation (T=1), adverse for translation (T=0), or just somewhere in between.
Given a query topic Q s = { s 1 ,s 2 ,...,s n } , we denote its correct translation as Q 0 t = { t 0 1 ,t 0 2 ,...,t 0 n correct translation of s j . In the following, we will stick to LR MIR ( s j ) which indicates the necessity of translation based on s j , and is defined as where  X  MIR ( q ) is the AP measure for query q in MIR. LR MIR ( s j ) tells the influence of translating s j to t Table 6: Average raking percentages (ARP)(x100%) and proportion of effective and ineffective OOV terms (N4: NTCIR-4, N5: NTCIR-5) N4 ARP 0.6794 0.8750 0.2996 0.3095 N5 ARP 0.6507 0.6667 0.2705 0.5067 Table 7: Classification accuracy with selected fea-tures (N4: NTCIR-4, N5: NTCIR-5)
Non-OOV 77.33% 69.21% 78.13% 72.20% with positive LR MIR ( s j ) are thought intrinsically-effective in target language and had better be translated.
We discuss how untranslated OOV terms affect CLIR per-formance, and why some OOV terms are not required to be perfectly translated. All of the OOV terms appearing in &lt; title &gt; and &lt; desc &gt; from both NTCIR-4 and NTCIR-5 are collected. Table 3 shows the numbers in detail.

Firstly, based on the term ranking lists generated by re-gression function r , we calculate the ranking percentage for each OOV term. For example, if an OOV term is ranked at top 2 in a list of size 5, its ranking percentage equals (2/5)*100%. Following this manner, it is expected that effective terms ( LR MIR &gt; 0), i.e., the terms need to be translated, are usually ranked in front of ineffective ones ( LR MIR  X  0) and thus have smaller average ranking per-centage. Table 6 reveals the reliability of our ranking lists. In addition, it is worth noting that for longer queries such as &lt; desc &gt; , we can earlier discover should-be-translated terms, as the ranking percentage in &lt; desc &gt; is often smaller. The result is somehow not surprising since longer queries usually contain more noises.

By calculating the proportions of effective terms and in-effective terms, Table 6 tells that the less number of terms a query such as &lt; title &gt; includes, the more effective each query term is for retrieval (83.3%:16.7% vs 76.5%:23.5%). The result is consistent with [7]. The untranslated OOV terms in short queries, especially for title queries or Web queries, crucially influence retrieval performance.
Moreover, we show what factors (features) distinguish ef-fective and ineffective OOV terms. We collect all OOV in-stances from the entire dataset to train a classifier. The up-per part of Table 7 shows the cross-validation classification accuracy for OOV terms. When applying the greedy-hill-climbing-based method to decide the best feature set, we get top-ranked features, including replacement, expansion and TFIDF features. We propose the following formula to reveal the capability of these features in predicting the need of translation for OOV terms, r 1 =
Measure r 1 estimates how possible the loss semantics caused by untranslated s j can be recovered by other terms together with its post-translation expanded terms (expansion and re-placement features). Figure 6 shows the relations between LR MIR and r 1 for each s j . Interestingly, a negative cor-relation exists between the two variables. If OOV term s is slightly effective ( LR MIR is positive but close to 0) and cannot be translated, the semantics it carries may be res-cued by expansion of the rest terms. An extremely-effective OOV term s j ( LR MIR 0) is the term whose semantics cannot be recovered well ( r 1 0). For those ineffective OOV terms ( LR MIR &lt; 0), not-translating such terms is beneficial to CLIR performance.

Measure r 2 captures the relevance of OOV term s j to the rest of the terms. Figure 7 shows a positive correlation be-tween LR MIR and r 2. Reasonably, an effective OOV term often has higher distinguishing power (TFIDF feature) in locating relevant documents compared to others. Conse-quently, our features explain why some OOV terms need to be translated while others do not. Especially, a difficult query term with low distinguishing power had better not
Figure 8: LR MIR (x-axis) versus LR TD  X  (y-axis). be translated even if its correct translation can be obtained. Also, for those which are effective for search, some terms are not necessary to appear in the query. To understand the impact of different translations for non-OOV term s j , we define a translation loss ratio as follows:
LR TD ( s j ) = tion resource D (Google Dictionary in our case). LR TD ( s tells the influence of translating s j to t k D ( s lations with non-negative LR TD are regarded having good translation quality, as they perform as well as or better than correct translation in the benchmarks.

We first focus on the most effective translation t  X  D ( s has the highest LR TD (i.e., LR  X  TD ) for each s j . From Fig. 8, LR MIR and LR  X  TD exhibit a positive relation for each s We can see that an effective term ( LR MIR &gt; 0) usually has good translation quality ( LR  X  TD &gt; 0) if one is able to find the best translation t  X  D ( s need to be translated. It also means that we will gain more performance boost ( LR  X  TD &gt; 0) if we translate the should-be-translated terms ( LR MIR &gt; 0).

In addition, we apply feature selection to non-OOV terms as what we do in the OOV analysis, and we find that statis-tical features are the most important, including context and co-occurrence features. The lower part of Table 7 demon-strates the classification accuracy for all non-OOV terms. Figure 9 shows the relations between LR TD and cos( Q 0 t { t that translation t k D ( s in CLIR, it can be inferred that t k D ( s similar from Q 0 t . We can see that a worse translation usually comes along with a weaker similarity to the original topic. Context features indeed help distinguish diverse translation qualities and help in prediction of T .
In this paper, we propose an approach to estimate the translation probability of query term s i , p ( T = 1 | s dicating if s i should be translated or not. One of our merits is that we consider comprehensive factors including linguis-tic, statistical, and CLIR aspects to predict T . It shows that T is influenced by intrinsic ineffectiveness, semantic recovery by query expansion, or poor translation quality. We also ver-ify that translating should-be-translated terms indeed helps improve CLIR performance across various translation meth-ods, retrieval models, and benchmarks.

Realizing what factors determine translation necessity is important. For an OOV term, we discover that it does not always need translation, as sometimes translations in tar-get corpus are ineffective or are irrelevant to original query. Specifically, leaving s i untranslated could be a wise choice if its semantics could be recovered by pre-or post-translation expansion. For a non-OOV term, we show that if there ex-ists an effective translation in dictionaries, it is suggested that translating s i would help CLIR performance. Context features are useful for predicting translation quality. If the translations tend to deviate original intention, we X  X  better leave s i untranslated. It is not worth taking a risk to trans-late a term if the term probably perform poorly in CLIR.
In brief sum,  X  X o-translate-or-not-to-translate X  is influenced by various and complicated causes. Some should-not-be-translated terms inherently suffer from their ineffectiveness in CLIR. Still others are affected by the translation quality obtained. These findings are consistent with [10]. Our ap-proach could minimize the efforts of translation by selecting terms that really need it. This is especially helpful under condition that lexicon coverage or time constraint is lim-ited, we can translate terms according to the ranking lists. Further, our approach can be easily extended to predict the effect of translating whole queries on CLIR as in [10].
We plan to train different models for OOV and non-OOV terms instead of a universal one, as they are intrinsically different from each other. We also want to explore how to automatically choose the best value for parameter k, which is anticipated to optimize the retrieval performance for each query topic. Despite the difficulty of automatic determina-tion of k, it turns out that a fixed value 2 in &lt; title &gt; and 4 in &lt; desc &gt; work acceptably in our experiments. Finally, we need the Web corpus for calculating statistical features before applying our method to Web applications. We leave these limitations as our future work. This work was supported by the National Science Council, Taiwan, under contract NSC97-2221-E-002-222-MY2. [1] J. Allan, J. Callan, W. B. Croft, L. Ballesteros, [2] L. Ballesteros and W. B. Croft. Dictionary methods [3] L. Ballesteros and W. B. Croft. Resolving ambiguity [4] M. Bendersky and W. B. Croft. Discovering key [5] J. Carbonell, Y. Yang, R. Frederking, R. Brown, [6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [7] P.-J. Cheng, J.-W. Teng, R.-C. Chen, J.-H. Wang, [8] M. Federico and N. Bertoldi. Statistical cross-language [9] J. Gao, J.-Y. Nie, E. Xun, J. Zhang, M. Zhou, and [10] K. Kishida. Prediction of performance of [11] J. Kupiec. An algorithm for finding noun phrase [12] P. McNamee and J. Mayfield. Comparing [13] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. [14] D. Oard. A comparative study of query and document [15] D. Oard and A. Diekema. Cross-language information [16] A. Pirkola. The effects of query structure and [17] F. Smadja, K. McKeown, and V. Hatzivassiloglou. [18] J. Zhu and H. Wang. The effect of translation quality
