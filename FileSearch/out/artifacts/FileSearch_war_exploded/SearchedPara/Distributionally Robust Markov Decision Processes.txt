
ECE, University of Texas at Austin huan.xu@mail.utexas.edu Sequential decision making in stochastic dynamic environm ents, also called the  X  X lanning prob-lem, X  is often modeled using a Markov Decision Process (MDP, cf [1, 2, 3]). In practice, parameter efforts have been made to reduce such performance variation under the robust MDP framework known set (termed the uncertainty set ), and solutions are ranked based on their performance under the (respective) worst parameter realizations.
 In this paper we extend the robust MDP framework to deal with p robabilistic information on uncer-tain parameters. To motivate the problem, let us consider th e following example. Suppose that an only when some unusual event (whose chance is less than, say, 10% ), such as a storm, happens, and otherwise the passing time is reasonable, then avoiding A may be overly pessimistic. The statement be incorporated into the decision making paradigm. Indeed, it was observed that since the robust MDP framework ignores probabilistic information, it can pr ovide conservative solutions [11, 12]. A different approach to embeding prior information is by ado pting a Bayesian perspective on the parameters of the problem; see [11] and references therein. However, a complete Bayesian prior to the model parameters may be difficult to conjure as the deci sion maker may not have a reliable generative model to the uncertainty. For example, in the pat h planning problem above, the decision maker may not know how to assign probabilities to the model dy namics when a storm occurs. Our approach offers a middle ground between the fully Bayesian a pproach and the robust approach: we want the decision maker to be able to use prior information but we do not require a complete Bayesian interpretation.
 We adapt the distributionally robust approach to MDPs under parameter uncertainty. The distri-butionally robust formulation has been extensively studie d and broadly applied in single stage op-timization problems to effectively incorporates a-priori probabilistic information of the unknown known set C . The objective is then formulated based on the worst-case an alysis over distributions in unknown parameter, distributionally robust optimization solves max deed, such approach has also been developed in the mathemati cal finance community, usually in the to be equivalent to a distributionally robust formulation.
 From a decision theory perspective, the distributionally r obust approach coincides with the cele-brated MaxMin Expected Utility framework [21, 22], which st ates that if a preference relationship cognitive bias [23], i.e., the tendency to completely disregard probabili ty when making a decision under uncertainty. Two extreme cases of such biases are the normalcy bias , which roughly speak-the uncertain one. It is easy to see that the nominal approach and the robust approach suffers from normalcy bias and zero-risk bias, respectively.
 We formulate and solve the distributionally robust MDP with respect to the nested uncertainty set . The nesting structure implies that there are n different levels of estimation, that is, C 1 representing the possible parameters of the problem. The pr obability that the parameters of state s belong to C i optimal policy satisfies a Bellman type equation, and can be s olved via backward iteration. Motivating example. The nested-set formulation is motivated by the  X  X ulti-scen ario X  setup, where the path planning example, the uncertainty of the passing ti me of A can be modeled as a nested-set with two uncertainty sets: the parameters with at least 90% belong to a small uncertainty set corre-sponding to  X  X o storm, X  and guaranteed to belong to a large wo rst-case uncertainty set representing  X  X torm X  with probability of at most 10% . In fact the multi-layer formulations allows the decision maker to handle more than two scenarios. For example, a plane can encounter scenarios such as an  X  X ptimistic X  estimation and a  X  X onservative X  estimatio n. Additionally, the nested-set formula-tion also results from estimating the distributions of para meters via sampling. Such estimation is often imprecise especially when only a small number of sampl es is available. Instead, estimating uncertainty sets with high confidence can be made more accura te, and one can easily sharpen the approximation by incorporating more layers of confidence se ts (i.e, increase n ). A (finite) MDP is defined as a 6-tuple &lt; T,  X , S, A horizon;  X   X  (0 , 1] is the discount factor; S is the finite state set; A p is the transition probability; and r is the expected reward. That is, for s  X  S and a  X  A denote the set of all history-dependent randomized policie s by  X  HR , and the set of all Markovian denotes the vector form of rewards associated with state s , and  X  and  X ( s ) to denote the probability simplex on A p = N s  X  S p s . For a policy  X  , we denote the expected (discounted) total-reward under pa rameters p , r by u (  X , p , r ) , that is, In this paper we propose and solve distributionally robust policy under parameter uncertainty, which incorporates a-prior information of how parameters are dis tributed. Suppose it is known that p and r follows some unknown distribution  X  that belongs to a set C by its expected performance under the (respective) most adv ersarial distribution of the uncertain parameters, and a distributionally robust policy is the opt imal policy according to this measure. Definition 1. A policy  X   X   X   X  HR is distributionally robust with respect to C all  X   X   X  HR , Next we specify the set of admissible distributions of uncer tain parameters C paper. Let 0 =  X  following set of distributions C We briefly explain this set of distributions. For a state s , the condition  X   X  els). Note that N parameters among different states are independent. Throug hout this paper we make a standard as-sumption (cf [5, 6, 8]) that P i In this section we show how to solve distributionally robust policies to MDPs having finitely many to a state can be treated as visiting different states, which leads to the Assumption 1 without loss of generality (by adding dummy states). Thus, we can partiti on S according to the stage each state belongs to, and let S non-stationary model.
 Assumption 1. (i) Each state belongs to only one stage; (ii) the terminal re ward equals zero; and (iii) the first stage only contains one state s ini .
 We next define sequentially robust policies through a backward induction as a policy that is robust in every step for a standard robust MDP. We will later shows that sequentially robust pol icies are also distributionally robust by choosing the uncertainty s et of the robust MDP carefully. Definition 2. Let T &lt;  X  and let P A standard game theoretic argument implies that sequential ly robust actions, and hence sequentially sequentially robust policy is the solution to the robust MDP where the uncertainty set is N (w.r.t. a specific uncertainty set)  X   X  is distributionally robust.
 Theorem 1. Let T &lt;  X  . Let Assumption 1 hold, and suppose that  X   X  is a sequentially robust policy w.r.t. N Then Theorem 2. Denote  X  where m = | A pends on the structure of the sets P i computing the sequentially robust action is tractable. Thi s claim is made precise by the following corollary. We omit the proof that is standard.
 Corollary 1. The sequentially robust action for state s can be found in polynomial-time, if for each i = 1 , , n , P i s has a polynomial separation oracle. Here, a polynomial separation oracle of a convex set H  X  R n is a subroutine that given x  X  R n , reports in polynomial time whether x  X  H , and if the answer is negative, it finds a hyperplane that separ ates x and H . 3.1 Proof of Theorem 1 We prove Theorem 1 in this section. The outline of the proof is as follows: We first show that for a given policy, the expected performance under an admissible  X  depends only on the expected value of the parameters. Then we show that the set of expected param eters is indeed N distributionally robust MDP reduces to the robust MDP with N Finally, by applying results from robust MDPs we prove the th eorem. Some of the intermediate results are stated with proof omitted due to space constrain ts.
 Let h to represent the probability of choosing an action a at state s ( h history h With an abuse of notation, we denote the expected reward-to-go under a history as: For  X   X   X  HR and  X   X  C  X  ( Lemma 1. Fix  X   X   X  HR ,  X   X  C then we have: w (  X ,  X , h t ) = From Lemma 1, by backward induction, one can show the followi ng lemma holds, which essentially means that for any policy, the expected performance under an admissible distribution  X  only depends on the expected value of the parameters under  X  . Thus, the distributionally robust MDP reduces to a robust MDP.
 Lemma 2. Fix  X   X   X  HR and  X   X  C w  X ,  X , ( s ini ) = u (  X , p , r ) .
 Next we characterize the set of expected value of the paramet ers.
 Lemma 3. Fix s  X  S , we have { E Note that Lemma 3 implies that { E Theorem 1 using the equivalence of distributionally robust MDPs and robust MDPs where the un-certainty set is N that for robust MDPs, a saddle point of the minimax objective exists (cf [5, 8]). More precisely, there exists  X   X   X   X  HR , ( p  X  , r  X  )  X  N Moreover,  X   X  and ( p  X  , r  X  ) can be constructed state-wise:  X   X  = N N It follows that  X   X  policy. From Lemma 3, there exists  X   X  N This leads to sup part (ii) of Theorem 1 holds. Note that part (ii) immediately implies part (i) of Theorem 1. Remark: Lemma 1 holds for a broader class of distribution sets than we discussed here. Indeed, the only requirement of C for Lemma 1 to hold is the state-wise decomposibility. There fore, the results presented in this paper may well extend to distribut ionally robust MDPs whose parameters exponential, binomial etc) with the distribution paramete r not precisely determined. In this section we show how to compute a distributionally rob ust policy for infinite horizon MDPs. horizon MDPs, and show that it is distributionally robust in an appropriate sense. Definition 3. Let T =  X  and  X  &lt; 1 . Denote the uncertainty set by  X  P = N following: contraction for kk Furthermore, given any v , applying L is equivalent to solving a minimax problem, which by Theo-resulting value vector will converge to the sequentially ro bust value  X  v exponentially fast. is to treat the system as having infinitely many states, each v isited at most once. Therefore, we consider an equivalent MDP with an augmented state space, wh ere each augmented state is defined will be visited at most once, which leads to the following set of distributions. The second formulation, termed stationary model , treats the system as having a finite number of then each time the distribution (of uncertain parameters)  X  adapt the augmented state space as in the non-stationary mod el, and requires that  X  depend on t . Thus, the set of admissible distributions is The next theorem is the main result of this section; it shows t hat a sequentially robust policy is distributionally robust to both stationary and non-statio nary models. Theorem 3. Given T =  X  and  X  &lt; 1 , any sequentially robust policy w.r.t. N and with respect to  X  C Due to space constraints, we omit the proof details. The basi c idea for proving the  X  C  X  nation reward  X  v butionally robust solution is proven to be stationary.
 tion of uncertain parameters, evolves with time, then the no n-stationary model is more appropriate; expected performance under the non-stationary model provi des a lower bound to that of the station-ary model since  X  C When the horizon approaches infinity, such approximation be comes exact, as we showed in this sec-a minimax problem. information, the distributional robust approach handles u ncertainty in a more flexible way, which often leads to a better performance than the nominal approac h and the robust approach. We consider a path planning problem: an agent wants to exit a 4  X  21 maze (shown in Figure 1) place where the agent needs one time unit to pass through. A sh aded box represents a  X  X haky X  place. To be more specific, we consider two setups. The first one is the uncertain cost case, where the true (yet unknown to the planning agent) time for the agent to pass through a  X  X haky X  place equals approaches are formulated as follows: the nominal approach takes the most likely value (i.e., 1 ) as each parameter set. The results are reported in Figure 2 (a).
 transition becomes unpredictable  X  in the next step with pro bability 20% it will make an (unknown) jump. The three approaches are set as follows: The nominal ap proach neglects this random jump. The robust approach takes a worst-case analysis, i.e., it as sumes that with 20% the agent will jump stay in the current position for a time unit ( X  X tuck X ). The re sults are reported in Figure 2 (b). approach outperforms the other two approach over virtually the whole range of parameters. This distributionally robust approach. In this paper we proposed a distributionally robust approac h to mitigate the conservatism of the robust MDP framework and incorporate additional a-prior pr obabilistic information regarding the unknown parameters. In particular, we considered the neste d-set structured parameter uncertainty achieves maximum expected utility under the worst admissib le distribution of the parameters. Such formulation leads to a policy that is obtained through a Bell man type backward induction, and can be solved in polynomial time under mild technical condition s.
 A different perspective on our work is that we develop a princ ipled approach to the problem of carefully designed single uncertainty set that depends on t he a-priori knowledge. A natural question is how can we take advantage of the distrib utionally robust approach and solve (exactly) a full-blown Bayesian generative model MDP? The p roblem with taking an increasingly ative model), the corresponding distributionally robust p olicies provide performance bounds on the optimal policies in the, often intractable, Bayesian model .
 Acknowledgements We thank an anonymous reviewer for pointing out relevant ref erences in mathematical finance. H. Xu would like to acknowledge the support from DTRA grant HDTR A1-08-0029. S. Mannor would like to acknowledge the support the Israel Science Foundati on under contract 890015. [1] M. L. Puterman. Markov Decision Processes . John Wiley &amp; Sons, New York, 1994. [2] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming . Athena Scientific, 1996. [3] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction . MIT Press, 1998. [6] A. Bagnell, A. Ng, and J. Schneider. Solving uncertain Ma rkov decision problems. Technical [7] C. C. White III and H. K. El Deib. Markov decision processe s with imprecise transition prob-[8] G. N. Iyengar. Robust dynamic programming. Mathematics of Operations Research , [9] L. G. Epstein and M. Schneider. Learning under ambiguity . Review of Economic Studies , [10] A. Nilim and L. El Ghaoui. Robustness in Markov decision problems with uncertain transition [11] E. Delage and S. Mannor. Percentile optimization for Ma rkov decision processes with param-[12] H. Xu and S. Mannor. The robustness-performance tradeo ff in Markov decision processes. In [13] H. Scarf. A min-max solution of an inventory problem. In Studies in Mathematical Theory of [15] P. Kall. Stochastic programming with recourse: Upper b ounds and moment problems, a review. [16] A. Shapiro. Worst-case distribution analysis of stoch astic programs. Mathematical Program-[17] I. Popescu. Robust mean-covariance solutions for stoc hastic optimization. Operations Re-[18] E. Delage and Y. Ye. Distributional robust optimizatio n under moment uncertainty with appli-[19] A. Ruszczy  X nski. Risk-averse dynamic programming for Markov decision processes. Mathe-[21] I. Gilboa and D. Schmeidler. Maxmin expected utility wi th a non-unique prior. Journal of [22] D. Kelsey. Maxmin expected utility and weight of eviden ce. Oxford Economic Papers , 46:425 X  [23] J. Baron. Thinking and Deciding . Cambridge University Press, 2000.
