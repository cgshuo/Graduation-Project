 The ubiquity of smartphones has led to the emergence of mobile crowdsourcing tasks such as the detection of spatial events when smartphone users move around in their daily lives. However, the credibility of those detected events can be negatively impacted by unreliable participants with low-quality data. Consequently, a ma-jor challenge in quality control is to discover true events from di-verse and noisy participants X  reports. This truth discovery problem is uniquely distinct from its online counterpart in that it involves uncertainties in both participants X  mobility and reliability. Decou-pling these two types of uncertainties through location tracking will raise severe privacy and energy issues, whereas simply ignoring missing reports or treating them as negative reports will signifi-cantly degrade the accuracy of the discovered truth. In this paper, we propose a new method to tackle this truth discovery problem through principled probabilistic modeling. In particular, we in-tegrate the modeling of location popularity, location visit indica-tors, truth of events and three-way participant reliability in a uni-fied framework. The proposed model is thus capable of efficiently handling various types of uncertainties and automatically discover-ing truth without any supervision or the need of location tracking. Experimental results demonstrate that our proposed method out-performs existing state-of-the-art truth discovery approaches in the mobile crowdsourcing environment.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing ; H.4 [ Information Systems Applications ]: Miscellaneous Mobile crowdsourcing; quality control; graphical models
The growing smartphone user base has enabled mobile crowd-sourcing applications on a large scale [15]. Several commercial markets such as Field Agent [3], Gigwalk [4] and TaskRabbit [5] have emerged, which represent the mobile equivalent of online crowd-sourcing markets such as the Amazon Mechanical Turk [1]. Crowd-sourced detection of spatial events is one such application where participants detect events while moving around in their daily lives. These events are arbitrary phenomena that the task requester is in-terested in, e.g., potholes on streets [14], graffiti on walls [17] and bike racks in public places [23].

Consider the task of detecting the locations of potholes as an ex-ample, where Figure 1a shows a user interface for task instruction. Since the number of possible event locations is huge and most loca-tions normally do not have an event (e.g., no potholes), a participant uses her smartphone to make a report (tagged with time and loca-tion as shown in Figure 1b) only when she detects an event. In other words, a participant either reports a detection (a positive report) or does not report at all (a missing report), but never reports a  X  X ack of an event X  (a negative report). As participants may erroneously report events due to misunderstanding, confusion, carelessness, in-competence or even intent to deceive (Figure 1c), there is a demand for efficient algorithms to handle these diverse and noisy partici-pants X  reports and automatically discover the truth (Figure 1d).
This truth discovery problem is uniquely distinct from its on-line counterpart in that it involves uncertainties in both participants X  mobility and reliability. Since participants only sporadically reveal their locations when reporting events for the geotagging purpose, we cannot obtain their detailed trajectories. This imposes a signif-icant challenge in interpreting missing reports at candidate event locations, which consequently impacts the quality of truth discov-ery (shown in Figure 1e where a positive and a missing report are annotated as a  X 1 X  and a blank space respectively). A missing re-port is ambiguous since it can be due to either the mobility issue that a participant did not visit a location and thus could not assess the event there, or a negative event assessment when she visited that location. It is important to distinguish these two cases, as the former does not carry any information about the truth of the event and the participant reliability while the latter does.

A possible solution to this problem is to continuously track par-ticipants X  locations such that the missing reports corresponding to unvisited locations are ignored and those corresponding to visited locations are treated as negative. We illustrate this strategy in Fig-ure 2a, where reports that are taken into consideration (as partic-ipants visited the corresponding locations) are marked with gray backgrounds and the inferred  X  X egative X  reports are annotated as  X -1 X  X . After eliminating the uncertainty in mobility, we can apply existing truth discovery methods for online crowdsourcing. How-ever, location tracking is impractical as it raises severe privacy and energy issues [6, 30]. Alternatively, one can try to reconstruct a participant X  X  mobility path. Nevertheless, machine learning-based path reconstruction methods [16] require historical location traces l (a) Location tracking (b) Ignoring missing (c) Missing as neg. Figure 2: Different strategies of handling missing reports. A entry with a gray (white) background means that the corre-sponding report is (not) taken into consideration for truth dis-covery. A  X 1 X  indicates a positive report, a  X -1 X  indicates that the missing report is treated as negative and a blank space in-dicates that the missing report is ignored. (a) Tracking partic-ipants X  locations. (b) Ignoring all missing reports. (c) Treating all missing reports as negative reports. which can only be obtained through tracking and such methods will easily fail when a participant deviates from her usual paths. Map-matching-based path reconstruction methods [13] require road net-work information and they will easily fail if the time interval be-tween consecutively revealed locations is larger than 5 minutes.
Strategies used for tackling missing data in related domains may be useful. For example, Raykar et al. [22] simply ignore the miss-ing data for online crowdsourced binary image classification. This is because online crowd workers are required to provide either a positive or a negative response, and the missing data simply imply that workers did not choose the images to work on. By applying this strategy to crowdsourced event detection, however, we will end up with only positive reports without any conflict (Figure 2b). This will lead to a trivial conclusion that every reported event is true, which is obviously erroneous. In tackling conflicting Web infor-mation for data integration, Zhao et al. [29] treat missing reports as negative reports if a source did not make claims on some of the facts (e.g., did not claim that Emma Watson is a cast) but on others (e.g., claimed that Daniel Radcliffe is a cast) about an entity (e.g., the movie Harry Potter). By applying this strategy to crowdsourced event detection, we can regard the spatial area of interest as an en-tity and events inside it as multiple facts, each can be either true or false. Each missing report will then become a negative report and imply a lack of an event (Figure 2c). If none of the events receives positive reports from more than half of the participants due to mo-bility issues, we will then conclude that all the events are false by majority voting, which is again erroneous. A work on social sens-ing [24] similarly treats missing reports as negative reports.
In this paper, we propose a new method to tackle the truth discov-ery problem in crowdsourced event detection through principled probabilistic modeling. We observe that a participant X  X  likelihood of reporting an event depends on three factors: 1) whether the par-ticipant visited the event location, 2) whether the event at that loca-tion is true or false, and 3) how reliable the participant is. Based on these observations, we model that each event location has certain popularity, which influences the possibility of a randomly selected participant to visit that location. This is motivated by the fact that some locations (e.g., shopping malls) naturally attract more people while others (e.g., country roads) attract fewer. Moreover, we treat the truth of events as latent variables and model three-way partic-ipant reliability, including true positive rate and false positive rate while present at a location and reporting rate while absent from a location. By doing so, positive and missing reports become random variables generated by conditioning on all these factors. Our ap-proach thus directly incorporates the mobility issues in the model, can efficiently handle missing reports and can automatically infer the truth of events and different aspects of participant reliability. Moreover, it is unsupervised and avoids location tracking.
In summary, this paper makes the following contributions: 1. We propose to address the truth discovery problem in crowd-2. We propose an unsupervised Bayesian probabilistic approach 3. We develop an efficient algorithm for model inference via
The remainder of this paper is organized as follows. We for-malize the problem in Section 2. We then introduce our proposed model and develop the inference algorithm in Sections 3 and 4. Experimental setup and results are presented in Sections 5 and 6. Several possible improvements and related work are discussed in Sections 7 and 8. Finally, we conclude the paper in Section 9. We formally define the truth discovery problem in this section. Consider a scenario where a group of participants joins a task to report a specific type of spatial events (e.g., potholes). A participant uses her smartphone to make a report r upon detection. Each report r =( u, l, t ) contains the participant ID u , the location l of the event (e.g., by GPS) and the time t of the report.

The set of related reports within a time window T and a spatial region of interest S is given by The proper sizes of T and S are application-dependent and can be specified via domain knowledge or through data-driven spatio-temporal clustering. From these reports, we can extract the set of all participants U and the set of all reported event locations We use u i and l j to denote the i th participant and the j th event location respectively. We assume that all events last for the duration of the time window and thus can be distinguished by their locations. We denote M = |U| and N = |L| .

We construct a report matrix X = { x i,j } from U and L as fol-lows which indicates that participant u i made a report claiming that an event was detected at location l j ; x i,j =0 otherwise. We term x i,j =1 as a positive report and x i,j =0 as a missing report. A missing report is ambiguous since it can be due to either the mo-bility issue that u i did not visit l j and could not assess the event there, or a negative event assessment made by u i when she visited l . The former case does not relate to the event truth and the par-ticipant X  X  reliability, while the latter does. However, participants X  detailed mobility traces, which can be used to distinguish these two cases, are not available due to privacy and energy issues.
Our problem of truth discovery in mobile crowdsourced detec-tion of spatial events is to infer 1) which reported events with loca-tions in L are true and which are false and 2) which participants in U are reliable, based on the report matrix X with only positive and missing reports. This problem is visually illustrated in Figure 1d.
In this section, we present our proposed probabilistic graphical model for the truth discovery problem in crowdsourced detection of spatial events. We first discuss our intuitions and the model com-ponents, and then illustrate some properties of the proposed model.
We consider the process of how a report is generated. In order to make a report, a participant first needs to be physically present at a location, observes whether there is any target event, and then decides to make a report or not based on her judgment. If a par-ticipant is not present at a location, she cannot make a report there since her location is recorded by her mobile device when reporting.
This process motivates us to model the following aspects: 1) lo-cation popularity, 2) participant X  X  location visit indicators, 3) event labels, 4) participant reliability and 5) reports on events. Figure 3 shows the graphical structure of our proposed model. Each node in the graph represents a random variable. Dark shaded nodes indicate observed variables, and light nodes represent latent variables and model parameters. Hyperparameters that correspond to the prior distributions are omitted for simplicity. A plate with a number such as M as its label means that there are M nodes of this kind. For ease of illustration, we list the notations used in this paper in Table 1. We summarize the generation process of our model in Algorithm 1 and detail its components below. Algorithm 1 Generation process Figure 4: (a) Location heat map in the northeast part of San Francisco (latitude: 37 . 75 to 37 . 79 ; longitude:  X  122  X  122 . 40 ; each point represents an approximately 20 m by grid cell). Each point shows the number of distinct people (to-tally 100) that visited that location. (b) Distribution (PDF and CDF) of the number of people that visited a specific location.
It is clear that in the physical world, participants do not randomly visit different locations but with certain patterns. For example, shopping malls are generally visited by a large number of people, but a residence area will only be visited by a few people who live there. This motivates us to model location popularity, which is the probability that a randomly chosen participant will visit a location.
Our intuition is supported by the findings from a public mobility dataset which contains time-stamped GPS location traces for 536 taxicabs over a span of roughly one month in the city of San Fran-cisco [20]. Figure 4a plots the location heat map in the northeast part of the city generated from the dataset, where each point shows the number of distinct people (we randomly choose 100) who have visited that location. It is clear that some locations are visited by more people while some by much fewer. We find that locations with high popularity mostly correspond to gas stations, crossroads and popular highways. Figure 4b plots the distribution of the num-ber of people that visited a specific location. It can be observed that around 80% of locations are visited by at most 20% of all the people. This suggests that mobility issues could result in a large proportion of missing reports in crowdsourced event detection.
Formally, we model that each event location l j has a location popularity g j , representing the probability that a randomly chosen participant will visit it. g j is generated from a Beta distribution with hyperparameters (  X  g j, 1 , X  g j, 0 ) , representing the prior counts of the number of distinct participants visited and did not visit location l respectively from a population.
We use h i,j =1 and h i,j =0 to denote that participant u visited and did not visit location l j respectively. We then model that a participant X  X  location visit indicator h i,j is generated from a Bernoulli distribution parameterized by the location popularity g i.e., h i,j  X  Bernoulli ( g j ) . In this way, a participant has a higher chance to visit more popular locations.
Since each reported event can be either true or false, we view them as binary random variables. We model that each event has a prior probability s of being true, and s is generated from a Beta dis-tribution. We use z j =1 and z j =0 to denote that the ground truth label of the event at l j is true and false respectively. The binary la-bel z j is then modeled as being generated from z j  X  Bernoulli
In crowdsourced detection of spatial events, a participant X  X  reli-ability depends on two factors: h i,j (a participant X  X  location visit indicator) and z j (an event X  X  true label), where the former factor does not exist in an online setting. It is desirable to model different aspects of participant reliability due to the following reasons.
First, it is likely that different participants will have different at-titudes towards reporting true and false events upon observation. A reliable participant will mostly report detection for true events but seldom make reports for false events, which results in a high true positive rate and a high true negative rate. On the other hand, a conservative participant is likely to report only when she is very confident that an event is true or when she is willing to report, which results in a low true positive rate but a high true negative rate. In other words, it is not reasonable to use a single correct rate (e.g., as that in [19, 27, 28]) to model participant reliability in crowdsourced event detection. Moreover, the true positive rate and the true negative rate in crowdsourced event detection make sense only with respect to the reports for events that a participant has an opportunity to observe (i.e., visited the event locations).
Second, as has been discussed previously, if a participant did not visit a location l j , she cannot make a report there. As a conse-quence, such a missing report is due to the participant X  X  mobility issue rather than her bias or carelessness when judging an event X  X  label. Therefore, it is desirable to use a parameter to characterize the participant X  X  reporting rate without visiting a location.
Formally, we model three-way participant reliability as follows. 1) True positive rate while present (TPR): We use a i to de-note the probability that participant u i reports that the event at l is true when she is present at l j and the event there is indeed true, i.e., a i = p ( x i,j =1 | h i,j =1 ,z j =1) . The TPR a i eled to be generated from a Beta distribution with hyperparameters (  X  a i, 1 , X  a i, 0 ) , representing the prior counts of positive and missing reports when u i is present at an event location and the event there is true. It is clear that TPR makes sense only when a participant really visited an event location ( h i,j =1 ). Without such a con-sideration, missing reports resulted from mobility issues can easily bias a participant X  X  TPR. 2) False positive rate while present (FPR): We use b i to de-note the probability that participant u i reports that event at l true when she is present at l j and the event there is actually false, i.e., b i = p ( x i,j =1 | h i,j =1 ,z j =0) . The choice to model the false positive rate rather than the true negative rate is for the ease of illustration. The FPR b i is modeled to be generated from a Beta distribution with hyperparameters (  X  b i, 1 , X  b i, 0 ) , representing the prior counts of positive and missing reports when u i is present at an event location and the event there is false. Similarly, FPR makes sense only when a participant really visited an event location. Oth-erwise, missing reports attributed to mobility issues can also easily bias a participant X  X  FPR. 3) Reporting rate while absent (RRA): We use c i to denote the probability that participant u i reports that event at l j cannot evaluate the event label when she is not at the event loca-tion, we model this probability to be independent of the event label z . The RRA c i is modeled to be generated from a Beta distri-bution with hyperparameters (  X  c i, 1 , X  c i, 0 ) , representing the prior counts of positive and missing reports when u i is absent from an event location. Since the participant X  X  location is recorded when a report is made, the probability c i that u i made a report with a geo-tag l j but was not physically at l j (within the localization accuracy bound) should be close to zero. Therefore, we specify a large  X  and a small  X  c i, 1 to make c i conform to such real-world physical constraints. The introduction of this probability also ensures that the model inference procedure only allows the presence of the case ( x i,j =0 | h i,j =0) (missing reports due to mobility issues) but not ( x i,j =1 | h i,j =0) (positive reports without visiting locations).
As can be seen, the modeling of TPR a i , FPR b i and RRA c can fully specify the confusion matrix for reports under different combinations of h i,j and z j . Note that each participant may have different reliability in different types of crowdsourcing tasks.
Finally, we consider how reports are generated. Take a missing report from a participant as an example. It can be resulted from several cases: i) the participant visited the event location which had a target event, but she did not report, ii) the participant visited the event location which did not have any target event, and she did not report, and iii) the participant did not visit the event location and thus could not report. Therefore, we model each report x as a Boolean random variable generated from a Bernoulli distribu-tion that depends on the participant X  X  location visit indicator h and the event label z j , and is parameterized by different participant reliability a i , b i and c i .

Formally, we model
We discuss several properties of our proposed model below. 1) Missing reports are well explained. According to the model structure shown in Figure 3, the probability of a missing report from participant u i on event at l j is given by This expression clearly captures the composite effect of various factors that can result in a missing report. When the location popu-larity g j  X  1 ,wehave p ( x i,j =0)  X  (1  X  s )(1  X  b i )+ It indicates that for a very popular location, the probability of ob-serving a missing report is mainly due to the event X  X  truth and a participant X  X  TPR and FPR. On the other hand, when the location popularity g j  X  0 ,wehave p ( x i,j =0)  X  1  X  c i . It indicates that for a very unpopular location, the probability of observing a missing report is then mainly due to a participant X  X  limited mobil-ity and RRA. In more general scenarios, these two possibilities for a missing report are combined through g j . Positive reports can be explained similarly. 2) Location tracking is avoided. Our model does not require continuous location tracking for each participant to disambiguate the cause of missing reports, and thus it avoids the privacy and en-ergy issues. Instead, we model location popularity which is the probability that a randomly chosen participant will visit a loca-tion. On the one hand, location popularity can be directly estimated through domain knowledge. For example, we can specify proper prior parameters (  X  g j, 1 , X  g j, 0 ) to impose a high location popular-ity for shopping malls, gas stations, and popular highways. On the other hand, since location popularity is a collective rather than a personal measure, its prior counts can be estimated once from any other resources where location tracking is not a concern (e.g., ex-periments about taxis or for studying human mobility). Moreover, location popularity for each specific task can be jointly estimated with other model parameters from the corresponding data. In con-trast, location tracking needs to be performed repeatedly for all the participants in each specific task (as new participants may join and existing participants may change mobility paths over time). 3) Different aspects of participant reliability are handled. We model three-way participant reliability which covers all the cases conditioned on different combinations of h i,j and z j . As a conse-quence, our model separates the effect of mobility and the effect of character on participants X  reports. It can also efficiently handle dif-ferent aspects of participants X  attitudes towards reporting true and false events upon observation. 4) Prior belief can be easily incorporated. We take a Bayesian approach and specify prior distributions for model parameters. This allows us to easily incorporate domain knowledge in the truth dis-covery process. In the absence of such knowledge, we can simply use uniform priors. The Beta distribution is utilized as the prior distribution because it is the conjugate prior of the Bernoulli dis-tribution. It can lead to posterior distributions having the same functional form as the prior, resulting in a greatly simplified and efficient Bayesian analysis [7].
In this section, we discuss how to perform inference to estimate 1) latent variables: event labels and participant X  X  location visit indi-cators and 2) model parameters: participant reliability and location popularity from the model, given the report matrix X . Algorithm 2 summarizes the model inference procedure.
 Algorithm 2 Model Inference 1: { Initialization } 6: { Sampling for K rounds } 7: for t =1: K do 8: { Update every z j } 14: end for 15: { Estimate event labels and location visit indicators } 18: { Estimate model parameters }
Given the data matrix X and the model, we need to find the optimal configuration of the random variables that maximize the posterior probability, i.e., using the maximum a posterior (MAP) estimator [7]. For example, to infer the event labels, we need to solve
Given the complex form of the joint distribution (refer to Figure 3), direct optimization is difficult to perform, especially when z and h can take on only integers. Therefore, we resort to an efficient algorithm, which is the collapsed Gibbs Sampling [11], for model inference. In our implementation, we integrate out all the model parameters and only sample the latent variables z j and h 1) Sampling z j . We first iteratively sample the label for each event according to the following update rules. The meaning of the notations is listed in Table 1. where f
In the above expressions, the counts n  X  j i,k,q,v reflect the reliabil-ity of u i based on her reports on events other than that at l first part in (1) carries information from other event labels and the second part carries information from the reports made by all the participants on other events (except that at l j ). Note that, in f and f i, 0 ,d , the counts only relates to h =1 . This is because only when a participant visited an event location and had an opportunity to assess the event label, that report (either positive or missing) car-ried information about the true event label. Otherwise, that report should not be taken into consideration. 2) Sampling h i,j . After sampling all z j , we then iteratively sam-ple each participant X  X  location visit indicators h i,j according to the following update rules. Note that, we only need to sample h when x i,j =0 , i.e., for those missing reports. Since the location is recorded when x i,j =1 , we can directly infer h i,j =1 if x The first part in (2) carries information from other participants X  (ex-cept u i  X  X ) location visit indicators at l j and the second part carries information from the reports made by u i on other events (except that at l j ).

The sampling procedure is performed for K rounds. To obtain  X  p ( z j =1) and  X  p ( h i,j =1) , we discard the first K 1 samples in the burn-in period, and then for every K 2 samples in the remainder we calculate their average (thinning), which is to prevent correlation in the samples. Finally, if  X  p ( z j =1)  X  0 . 5 , we output otherwise, we have  X  z j =0 . The estimation of h i,j is similar.
After we have obtained the estimation of event labels z j cation visit indicators h i,j , we can estimate the participant relia-bility using the MAP estimator by treating these inferred values as observed data. This results in a closed-form estimation as follows. where E ( n i,k,q,v ) is the expected count of tuples ( h q,x = v ) related to participant u i . This count depends on the probability of the location visit indicators, the probability of the event labels and the actual reports. Formally, c is not estimated since the setting of its prior counts will make it almost 0.
Similarly, we can estimate the location popularity as where E ( n j, 1 )= i  X  p ( h i,j =1) is the expected number of par-ticipants that visited event location l j .

These estimated model parameters can be used to select partici-pants and to compute p ( z | X ) in future tasks.
In this section, we describe the truth discovery methods com-pared, the evaluation metrics used and the experiments conducted.
We term our proposed method as Truth finder for Spatial Events ( TSE ) and compare it with the following state-of-the-art methods: 1) MV : the widely applied Majority Voting method, which predicts the event to be true if the proportion of positive reports exceeds 0.5; 2) TF : the Truth Finder proposed in [28], which utilizes the inter-dependency between source trustworthiness and claim confidence to find truth; 3) GLAD : the Generative model of Labels, Abilities, and Difficulties proposed in [27] for online crowdsourced image Table 2: Statistics of reports in Area 1 (the left half) and Area 2 (the right half) in Figure 4a.
 classification (the authors X  code is used); 4) LTM : the Latent Truth Model proposed in [29] for conflicting web information; 5) EM : the Expectation and Maximization method proposed in [24] for social sensing. Except our proposed TSE , other methods do not model location popularity, location visit indicators and three-way partic-ipant reliability. In dealing with missing data, they either ignore them or treat them as negative (for binary truth discovery). As has been discussed, the former treatment will result in consistent infor-mation and will lead to a trivial conclusion that every event is true, we thus use the latter treatment for all these compared methods.
We also compare the performance of TSE in estimating partic-ipants X  location visit indicators and location popularity with two baseline methods, where Naive0 simply assumes h i,j =0 for x i,j =0 (all the missing reports are due to mobility issues) and Naive1 simply assumes h i,j =1 for x i,j =0 (each participant visited all event locations). Actually, Naive0 and Naive1 act equivalently as ignoring missing reports and treating them as nega-tive reports respectively.

We set the hyperparameters in TSE as follows. We set  X  c i , and  X  c i , 0 =10 4 , since by domain knowledge we have  X   X  prevent the model from flipping the inference while still achieving a high likelihood. The setting of  X  g j , 1 and  X  g j , 0 in each experiment. We set all other hyperparameters to 5 hyperparameters can be set much larger for large datasets or set to different values if prior domain knowledge is available.
We use the following metrics to evaluate the performance of these methods. 1) Precision, recall and F1 score: pre = TP TP + FP , rec F (an algorithm infers an event is true when it is indeed true). We use them to evaluate the performance of the estimation  X  z j on event la-bels. The higher these metrics, the better a method performs. 2) Mean absolute error (MAE): mae ( a )= 1 N N i =1 |  X  a We use it to evaluate the performance of the estimations  X  cation visit indicators,  X  g j on location popularity,  X  a TPRs and  X  b i on participants X  FPRs. The lower this metric, the bet-ter a method performs. Ground truth of g j , a i and b i are calculated based on their definitions (Table 1). mae ( h ) is calculated only for missing reports x i,j =0 whose h i,j need to be estimated.
We conduct three sets of experiments to evaluate the perfor-mance of the compared methods. The first set is to detect the lo-cations of traffic lights, where reports were  X  X ade X  by participants driving vehicles when they were waiting at a location for a certain time. The second set is to detect image-based events. It is cre-ated by combining a mobility dataset and three online image-based event detection datasets to examine the influence of mobility in ad-dition to participants X  bias. The third set is to further examine the methods X  performance when participants with specific kinds of re-liability (such as conservative) are present through simulations. We describe their details below.
Dataset: We use the mobility dataset (denoted as M ) provided in [20] as our experiment dataset. It contains time-stamped GPS location traces for 536 taxicabs in San Francisco with successive location updates recorded 1-60 seconds apart. We choose a re-gion shown in Figure 4a as our spatial area of interest, which spans roughly 3 . 5 km  X  4 . 4 km (an area with a reasonable size such that it is possible for participants to visit all the event locations inside it). We partition this area into approximately 40 m  X  40 m grid cells and then project the large number of distinct GPS locations into a much smaller set of cells. We further vertically divide this area into two subareas of equal size, where participants more densely visited the right half (Area 2) than the left half (Area 1).

Task: The crowdsourcing task that we consider is to detect the locations of traffic lights. We observe that vehicles usually wait at traffic light locations for a few seconds to a few minutes. Therefore, by processing the waiting behaviors of vehicles driven by various participants, we will be able to crowdsource the locations of traffic lights. However, the waiting behavior is a noisy indicator of traf-fic lights since a car can also stop at stop signs or anywhere else on the road due to traffic jam or crossing pedestrians (false posi-tive). Moreover, a car does not stop at traffic lights that are green (false negative). Furthermore, different drivers have different driv-ing behaviors such as a careless driver may pass stop signs without stopping and a careful driver may stop at stop signs for a relatively long time. These factors make the waiting behavior diverse, noisy and participant-dependent.

Reports: In our experiment, we assume there is an application on each vehicle and if the vehicle waits at a location for 15-120 seconds, such a behavior triggers the application to issue a detec-tion report (of a traffic light). Since the application uses the same criterion for data processing, when and where to issue a report is actually controlled by the participants, except that their reliability comes from their behaviors rather than mind. By randomly pick-ing out 100 participants, we obtain 25,054 and 95,856 reports in Area 1 and 2 respectively, collectively identifying 2,051 and 2,683 distinct cells respectively (listed in Table 2). To account for the lo-cation granularity of GPS devices and the fact that a vehicle may also wait at a certain distance from the traffic light due to the traffic queue, we further cluster these cells using a hierarchical clustering approach [7] with a cutoff distance of 80 m . This procedure ensures that the traffic light reports are attributed to the correct intersections and it results in 486 and 537 distinct cluster centers in Area 1 and 2 respectively. We then randomly pick out 100 of them in each area to annotate the ground truth using the Street View in Google Maps.
Data analysis: We define a participant X  X  location coverage as | l ( tions visited by u i and the total number of event locations. A loca-tion X  X  popularity can be expressed as | u ( l j ) | / | u | u | denote the number of participants that visited l j and the total number of participants. Figure 5 plots the cumulative distributions of these two metrics. As can be seen, a participant can cover at most 62% and 75% of all the event locations in Area 1 and 2 respectively. Only around 20% and around 40% of event locations have a popu-larity of over 0.8 in Area 1 and 2 respectively. Some event locations are visited by almost all the participants while some are visited by less than 5%. These results suggest that mobility is an important factor that causes missing reports in mobile crowdsourcing. Fig-ure 6 plots example positive reports for true and false events. We can observe that a majority of true events and false events receive a similar number of reports. Majority voting can easily fail in such an environment since the number of reports for true events can seldom exceed half of the number of participants. Figure 5: Participants X  location coverage ( | l ( u i ) | tions X  popularity ( | u ( l j ) | / | u | ).
We combine the mobility dataset M and three online crowd-sourced event detection datasets to evaluate the performance of the methods in a broader range of applications. This set of experiments is to mimic the scenario that participants move outdoors, observe events and make reports based on their judgments. The combina-tion procedure described below is essentially to capture the phys-ical constraint that a participant could make a report at a location only when she visited that location. It assumes nothing about mo-bility patterns, event labels and participant reliability. We created three image-based event detection tasks on Crowd-Flower [2] with clear instructions. They were to detect 1) bike racks, 2) Chinese restaurants and 3) flowering cherries respectively. For each task, M =20 crowd workers were recruited and each of them was asked to report the images with target events among N =40 images, where half of them contain the target event and half do not. We chose images with different viewing angles and distances, and also carefully selected a portion of them which may cause false alarms or missed detections. To introduce mobility, we randomly assigned each participant u i a GPS trace from M then randomly sampled N locations l j from all these assigned GPS traces to represent the event locations for the N images. Only if u  X  X  GPS trace showed that she visited l j at some time and u ported that the j th image contained the target event, we generated a positive report x i,j =1 . We term these combined dataset corre-sponding to the three tasks as B M , C M and P M respectively.
In the simulation study, we put our focus on evaluating the per-formance of different methods in the presence of specific types of participants (defined in Table 3). We are particularly interested in the following representative ones: 1) reliable participants with a large TPR a and a small FPR b , 2) unskilled participants with a and b close to 0.5, reporting almost randomly, 3) conservative par-ticipants with a small a and a small b , reporting occasionally only when they are very confident or willing to report, 4) aggressive par-ticipants with a large a and a large b , reporting over actively, and 5) malicious participants with a small a and a large b , flipping the labels most of the time. Of course, this categorization is rough. However, we can use it to gain some insights on the impact of par-ticipant reliability on the algorithm performance. We assume there are M =40 participants and N = 200 event locations where half of them contain true events and half do not. We again randomly Table 3: Categorization of participants according to their reli-ability parameters. a is the TPR and b is the FPR.
 Table 4: Precision, recall and F1 score on inferring event labels for traffic light detection.
 assign GPS traces and sample event locations, but generate reports x i,j according to the process in Section 3.1.5 with c i =0
In this section, we demonstrate the effectiveness of our proposed method compared with several top-performing truth discovery ap-proaches on both real-world and synthetic datasets.
We report results based on 20 runs of tests for each experiment by randomly sampling the corresponding number of participants and events unless all of them are used. Only participants and events with at least two reports are kept for evaluation. We utilize a dis-joint set of participants to estimate the prior counts of location pop-ularity and set these counts fixed for all the experiments. To reduce noise, we consider a participant visited an event location if she had traversed there at least twice.
Table 4 lists the precisions, recalls and F1 scores of all the meth-ods on the two datasets from Area 1 and Area 2 when M =100 N =100 . We can observe that TSE achieves the highest recall and F1 score as well as very high precision on both datasets, showing that it can better handle missing reports and more accurately infer the truth of events in crowdsourced detection of spatial events. All the other methods cannot tackle mobility issues and perform much worse. They are prone to infer that most events are false due to the large number of missing reports and thus fail to detect lots of true events, resulting in high precisions but low recalls.

MV performs badly, since it does not take into account the par-ticipant reliability in its prediction. TF contains a mechanism to assign implication scores between similar observations. However, there is no similar but only contradictory observations for binary events. The power of TF is thus lost and it also performs badly. GLAD models only a single correct rate for participant reliability and thus it is not suitable for crowdsourced event detection. The overfitting problem makes it perform similar to MV . LTM and EM perform comparably and much better than MV , TF and GLAD , since they model two-sided participant reliability. However, they are not designed to tackle the mobility issues and thus fail to detect lots of positive events, resulting in low recalls.

Figure 7 plots the F1 scores on estimating the event labels versus the number of participants and the number of events respectively in Area 2 (the trends in Area 1 are similar). We can observe that TSE Figure 7: F1 scores on estimating event labels versus (a) the number of participants M when N = 100 and (b) the number of events N when M = 100 in Area 2.
 performs much better than all the other methods in all the cases, and it can benefit from more available information to improve its performance.
Figure 8 plots mae ( h ) and mae ( g ) when M = 100 and N 100 via TSE and two baseline methods. Naive1 results in the largest mae ( h ) in both areas. This shows that mobility issues lead to lots of missing reports and simply treating them as implicit negative reports is incorrect. TSE performs better than Naive0 , showing that it indeed correctly infers some location visit indica-tors. However, TSE is not significantly better than Naive0 (e.g., mae ( h ) in Area 1 are 0.246 and 0.303 for TSE and Naive0 re-spectively), meaning that TSE still faces difficulty in reliably infer-ring a large proportion of location visit indicators. As to mae Naive1 still results in the largest error but TSE results in a signif-icantly lower error than Naive0 (e.g., mae ( g ) in Area 1 are 0.118 and 0.276 for TSE and Naive0 respectively), showing that TSE can more reliably infer location popularity g j . This is because we only statistically model h i,j  X  Bernoulli ( g j ) , so that h different realizations given the same g j . We may more accurately discover true events when h i,j can be more accurately inferred. However, if privacy is an important concern, accurately inferring location popularity g j in a collective sense but less accurately infer-ring individual location visit indicators h i,j may be more desirable. Figure 9 plots the MAEs on estimating participants X  TPRs and FPRs. We can observe that MV , TF and GLAD are biased towards more accurately inferring FPRs b . This is because they are prone to predict most events to be false and participants mostly do not report, leading to small FPRs. Since in reality, the FPRs are usu-ally small, these methods accidentally achieve good performance. However, they perform poorly in estimating TPRs a . On the other hand, LTM , EM and TSE achieve much better and more balanced performance in estimating a and b .
We use uniform prior counts for location popularity. Table 5 lists the precision, recall and F1 scores of all the methods on the three Table 5: Precision, recall and F1 score on inferring event labels for image-based event detection.
 combined datasets when M =20 and N =40 . We can observe similar characteristics as those in Table 4. TSE achieves the highest recalls and F1 scores on all the datasets, followed by LTM and EM . GLAD easily leads to overfitting, and MV and TF performs worst. However, their performance is better than that in Table 4, possibly due to different sizes of the datasets and different natures of tasks.
For the MAEs on estimating h and g , we also observe that TSE performs much better than the two naive methods and it is more capable of accurately estimating location popularity than partici-pants X  location visit indicators (figures are not shown due to space limitations). Since each participant only reports a few events, it is unreliable to directly calculate TPRs and FPRs from these datasets as ground truth and we thus do not consider the estimation of them here.
We report results based on 20 independent runs for each experi-ment where uniform prior counts for location popularity are used. Figure 10 shows the F1 scores on event labels and the MAEs on participant reliability under different combinations of reliable and other types of participants. We can observe that TSE achieves the highest F1 scores across all different scenarios, and it also results in smallest MAEs on a and b in most scenarios. LTM and EM are prone to generate large errors on estimating the TPR a , since treat-ing missing reports as negative will reduce the TPR. MV and TF per-form poorly, easily resulting in low F1 scores on estimating event labels or large errors on estimating participant reliability. Conser-vative participants have the highest impact on the performance of truth discovery among all the compared participant categories.
Dependent reports. We currently assume that participants inde-pendently make reports. However, sources can sometimes be de-pendent and such dependency can undermine the wisdom of crowd [12]. One possible solution is to apply copy detection methods be-tween sources [9]. Alternatively, we can directly incorporate source dependency in the modeling [21].

Sequential mobility modeling. Our current method models lo-cation popularity in a collective sense. Alternatively, we can also model the most likely trajectory for each participant. This may im-prove the accuracy of the estimated location visit indicators and subsequently improve the accuracy of other estimates. However, it will increase the model complexity and impose higher privacy risks. Moreover, this approach may not work well if the time inter-val between consecutive reports is large (e.g., hours) [13].
Cross-domain truth discovery. Our experiments show that event detection tasks in some domains are intrinsically more difficult than those in other domains. Difficult tasks and limited number of re-ports can deteriorate the performance of truth discovery. Leverag-ing tasks in different domains may help since it can balance the var-ied difficulties and increase the number of reports. To achieve this goal, we may need to leverage transfer learning techniques [18].
A number of unsupervised approaches have been proposed for discovering the truth from conflicting information sources. In the domain of truth discovery from conflicting Web information, Yin et al. [28] proposed truth finder, which is a transitive voting algo-rithm with rules specifying how votes iteratively flow from sources to claims and then back to sources. It has been shown to be su-perior than majority voting and the hubs and authorities algorithm [10] which was initially designed to find popular web pages. Paster-nack and Roth [19] proposed Investment and PooledInvestment al-gorithms, where sources invest their credibility in the claims they make, and claim belief is then non-linearly grown and apportioned back to the sources. Unlike these heuristics, Zhao et al. [29] pro-posed a more principled probabilistic approach which can automat-ically infer true claims and two-sided source quality.

In the domain of aggregating conflicting responses in crowd-sourcing tasks, several statistical techniques have been proposed. To name a few, Dawid and Skene [8] modeled the generative pro-cess of the responses by introducing worker ability parameters. Whitehill et al. [27] also included the difficulty of the task in the model, and Welinder et al. [26] proposed a model consisting of worker compatibility for each task. Wang et al. [24, 25] proposed an EM algorithm that models both the truth of tasks and the relia-bility of workers for social sensing.

Nevertheless, these methods are not designed to tackle truth dis-covery in mobile crowdsourced event detection where both par-ticipants X  mobility and reliability are uncertain. Moreover, none of them models location popularity, location visit indicators and three-way participant reliability. Alternative solutions, such as first continuously tracking participants X  locations and then applying ex-isting truth discovery methods, will raise severe privacy and energy issues. In contrast, our proposed model integrates mobility, reliabil-ity and latent truth in a unified framework and can jointly optimize all the model parameters without the need of location tracking.
In this paper, we have proposed a probabilistic graphical model to the problem of truth discovery in crowdsourced detection of spatial events. The proposed method integrates the modeling of location popularity, participants X  location visit indicators, truth of events and three-way participant reliability in a unified framework. We demonstrate the accuracy and efficiency with which this method can handle ambiguous missing reports caused by either mobility or reliability issues, and automatically infer the truth of events and different aspects of participant reliability without any supervision or location tracking. Experimental results on real-world and syn-thetic datasets demonstrate that our proposed method outperforms existing state-of-the-art approaches for mobile crowdsourcing. on estimating FPRs b . M =40 and N = 200 .
 This research is based upon work supported in part by the U.S. ARL and U.K. Ministry of Defense under Agreement Number W911NF-06-3-0001, and by the NSF under award CNS-1213140. Any opin-ions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily re-flect the views or represent the official policies of the NSF, the U.S. ARL, the U.S. Government, the U.K. Ministry of Defense or the U.K. Government. The U.S. and U.K. Governments are autho-rized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.
