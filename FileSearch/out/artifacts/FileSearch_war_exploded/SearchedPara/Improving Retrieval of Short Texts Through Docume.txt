 Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval (IR) for short texts based on aggressive document expansion. Start-ing from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents them-selves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also tem-poral properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these exper-iments show that applying our proposed document expan-sion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Systems Issues Algorithms, Experimentation, Performance Information retrieval, microblogs, twitter, Dublin Core, doc-ument expansion, language models, temporal IR sults from the well-known language modeling approach to IR, especially the notion of relevance models. But we also show that the single-topic focus of most short documents opens avenues for consideration of information other than language X  X or example, time.
The growing quantity of information published in small texts such as Twitter posts argues for a sustained analysis of the treatment of brief documents in IR. While this ur-gency is new, the need for improved retrieval of short texts is not. For instance, many digital libraries manage repos-itories of terse metadata records. While these repositories (such as the DCC repository we describe in this paper) are nominally searchable, the brevity of their metadata can frus-trate effective IR. Other examples of short-text IR include query-specific advertisement and product review ranking.
But retrieval from collections of short documents is dif-ficult for several reasons. First, brief documents attenuate one of the primary signals used by modern IR systems X  X erm frequency. Figure 1 can help us understand why this is the case. The figure shows data from two TREC collections (de-scribed in detail in Section 5). The TREC 8 data included a corpus of news articles. Tweets2011 is the collection of Twitter data used for the microblog track at TREC 2011. Figure 1 shows the distributions of the log-probabilities of query terms among the first 100 documents retrieved using a simple query likelihood model. That is, the points that comprise the distributions are the maximum likelihood esti-mates of log P ( q | D ) for each query term q .

Two problematic facts are clear from Figure 1. First, unlike the longer news documents, tweets lead to a distri-bution of query term probabilities that is strongly peaked. The majority of tweets that contain a query word only con-tain it once, and tweets are of similar length. Second, in the case of the TREC 8 data, the mean and median log-probability of a query term are higher in relevant documents than in non-relevant documents. This is not the case for the Twitter data. Median log P ( q | D ) for relevant TREC 8 documents is -6.92, with median -7.17 for non-relevant doc-uments. The difference in log-probability in relevant versus non-relevant TREC8 documents is statistically significant (Mann-Whitney p 0 . 001 one-sided). But log P ( q | D ) for relevant Twitter documents has median -7.82 and -7.80 for non-relevant (Mann-Whitney p = 1 one-sided). To the ex-tent that TREC 8 presents a  X  X ypical X  statistical picture, Figure 1 shows that Twitter data are qualitatively different than more familiar TREC collections.

The impact of this difference is easy to see if we consider the language modeling approach to IR [21]. In the language modeling approach, we assume that each document in our collection was generated by a probability distribution X  X  lan-guage model X  X ver terms in the vocabulary. We rank docu-ments against a query Q by the likelihood that their corre-sponding language models generated the query:
The problematic nature of IR on short documents has seen little sustained research (though [22, 27] do treat the topic explicitly). However, recent interest in social media has drawn attention to this problem [22, 10]. Familiar oper-ations such as measuring inter-document similarity, scholars have found, is difficult given the brevity of many documents in collections of user-generated content [20].

Though corpora of brief documents are increasingly com-mon, the estimation problems that our proposed expansion methods address are not new. Most similar to our own work are results from research on cluster-based IR. Clusters have been applied to various points in the IR process, including relevance feedback [14, 9], rank fusion [12], and as a sepa-rate factor used during document ranking [13]. Clusters are appealing insofar as they afford a level of information about documents that resides at a level higher than intra-document word counts, but below the generalities of the collection at large. For instance, Liu and Croft applied clusters during document language model smoothing [18]. Ramage, Dumais and Leibling address the vocabulary mismatch problem in microblog text using latent Dirichlet allocation (LDA) [23], allowing the LDA model to supplement observed term prob-abilities in document representation. Other methods of doc-ument expansion have also seen sustained work (e.g. [24]), though in contexts different from those that we study here.
The most similar work to ours was given by Tao et al. [26]. In their work, Tao et al. proposed smoothing document language models by analyzing their lexical neighborhoods. That is, the model for document D was smoothed with counts obtained from its k nearest neighbors D 1 ,...,D k , with each document X  X  influence in the smoothed model be-ing proportional to its cosine similarity with D . Like Tao et al. we propose improving the representation D by an analysis of similar documents. However, the approach we outline in Section 4.1 differs from Tao et al. in its basic the-oretical orientation, couching the estimation problem in the generative semantics of language modeling. Tao et al. define the neighborhood of a document in geometrical terms, rely-ing on the cosine similarity. With an explicit assumption of normality, they smooth the model of D based on this neigh-borhood and this metric. On the other hand, we assume that D arises from an unseen model D , much as relevance modeling assumes the influence of an unseen model of rele-vance. To estimate P ( w |D ) we combine evidence from other documents, where the influence that some document D j ex-erts on the final model is the likelihood that D j  X  X  language model generated D .

Our work also relies on findings from recent studies on information retrieval and microblogs [6, 19, 2]. The field of microblog retrieval is relatively new, but has seen increasing interest, most noticeably in the 2011 TREC microblog track.
Unlike the methods described in this section, our doc-ument expansion technique also invites extension to non-linguistic features. As we show in Section 4.2, aspects of rel-evance such as temporality fit naturally into our approach. Let D be a document consisting of | D | word tokens d 1 ,...,d | D | . Also let C be a corpus of N documents. Using to Eq. 2, we can calculate the likelihood of D given the language model we can obtain a good estimate of P ( w | D 0 ) by performing the summation in Eq. 7 over only the k documents with the highest likelihood of generating D . Thus the probabil-ity of word w under the augmented model is a weighted average of the observed probabilities of w in the top k doc-uments retrieved by submitting D as a pseudo-query, where the weights are the likelihoods of D given the retrieved doc-uments X  language models.

Having obtained our augmented representation, we may then rank documents by the likelihood that their augmented language models generated the query, P ( Q | D 0 ) which we calculate using the query likelihood model as usual, substi-tuting P ( w | D 0 ) for the maximum likelihood estimator into Eq. 3. We refer to this method by the abbreviation LExp, for lexical expansion.

As is common when using relevance models for relevance feedback, interpolating the expanded model with the orig-inally observed text is likely to be  X  X afer X  than relying on the expanded model alone. Thus we define another lexically expanded model: for a parameter  X  in [0, 1]. We may then substitute P  X  ( w | D 0 ) for the maximum likelihood estimator in Eq. 3 for retrieval. We refer to rankings based on this estimator as LExp  X  . For simplicity, when discussing LExp  X  we set  X  = 0 . 5 through-out this paper.
In addition to supplementing the lexical evidence that we store about documents, the expansion method described above can create new, extra-lexical features. For example, information in R D yields actionable information related to temporal aspects of relevance. Indeed, incorporating tem-poral evidence into IR entails a research area in its own right (cf. [1]). We pursue temporality here as an example of extra-linguistic information that our document expansion method allows.

We assume that for each document D i , we have a cor-responding timestamp t i which is the time at which the document was published. We also define t 0 as the earliest timestamp in the collection. In this paper we measure time in fractions of days, such that t i is how many days elapsed between the initial time t 0 and the publication of D i . The observed timestamp t i provides useful information for IR. But our goal is to learn additional temporal information about each document D i . We hypothesize that the empir-ical distribution of timestamps related to D i via document expansion helpfully supplements direct use of t i .
Working under a similar scenario, Jones and Diaz defined the notion of a query X  X   X  X emporal profile X  [11, 5]. The tem-poral profile of a query Q is a probability distribution over time, P ( t | Q ). Analyzing the empirical distribution of docu-ments retrieved for Q gives information helpful in estimating P ( t | Q ), as shown in [4].

Our discussion above suggests that temporal profiles may be defined not only on queries, but also on documents. For each document D , we define a temporal profile P ( t | D ). This probability distribution expresses the extent to which D is associated with events that were discussed at various mo-ments in time. Estimating P ( t | D ) follows the same logic that we use for query temporal profiles; we submit the text of D as a query, yielding R D = R D 1 ,...,R Dk . Each of TREC 2011 microblog task. Second, we use a collection of metadata records describing holdings in a large digital li-brary. Both of these collections consist mostly of brief docu-ments, though as we shall show, they have many differences.
The Tweets2011 collection uses a corpus of posts (called  X  X weets X ) made available by the microblogging service Twit-ter. Instead of distributing the microblog corpus via physical media or a direct download, TREC organizers distributed approximately 16M unique tweet ID numbers. Users of the collection downloaded these ID X  X , along with software that allowed them to fetch the tweets directly from Twitter. We obtained our data using the supplied HTML scraping tools on May 25 X 26, 2011. Our corpus contained 15,653,612 in-dexable tweets, each containing: the screen name of the au-thor, the time at which the tweet was posted, and the tweet text itself.
 Our microblog data was preprocessed in several ways. First, in conformance with the track X  X  guidelines all  X  X etweets X  were removed by deleting documents containing the string RT . Second, we made a simple pass at removing non-English tweets. We deleted tweets containing more than four charac-ters with byte values greater than 255. We also defined a list of 133 words that are common in Spanish, French and Ger-man. Tweets containing any of these were removed. After these operations, the corpus contained 8,320,421 documents.
The 2011 microblog track involved a real-time search prob-lem. Track organizers created 50 test topics 2 , each of which contained query text Q and a timestamp t Q . Only doc-uments posted prior to t Q were assessed for relevance (all others are non-relevant). Aside from returning only docu-ments published before query-time, for the sake of simplicity, in this paper we drop additional real-time strictures defined by the track organizers. The Institute for Museum and Library Studies Digital Collections and Content (IMLS DCC) is a large, federated digital library offering access to the aggregated holdings of a broad spectrum of digital collections at distributed cul-tural heritage institutions 3 . In collaboration with IMLS DCC, we obtained a collection of 578,385 brief descriptions of cultural heritage resources. IMLS DCC administrators regularly harvest descriptions of several hundred participat-ing institutions X  digital resources via the Open Archives Ini-tiative Protocol for Metadata Harvesting (OAI-PMH) [15]. Each of the 578,385 harvested documents is a Dublin Core metadata record describing a single item such as a photo-graph, manuscript, sound recording. Details of the process by which this corpus was built are given in [8].

To enable IR experimentation, we sampled 53 queries taken from the DCC search engine query logs. It must be stressed that these were not chosen at random. We used care in selecting queries to assure that no query returned zero relevant documents. Because many highly specific queries returned zero or very few items  X  making changes in IR performance difficult to test  X  queries with fewer than five NIST created relevance judgments using the standard TREC pooling method. Because it lacked relevant docu-ments, one topic was removed from the final task. http://imlsdcc.grainger.uiuc.edu/ standard deviation of document length, and the number of test queries. Tweets2011 8,320,421 19,449,151 6,506,465,256 20 21.92 7.30 49
DCC 578,385 2,793,371 114,453,512 45 197.89 556.90 53 we would like, it marks a significant improvement over the initial work quality.
Table 1 summarizes length-related aspects of our two test data sets. Not surprisingly, documents in the microblog cor-pus are very short (median=20, compared to median=328 in TREC 8), with a compressed distribution of lengths. Most DCC documents are a bit longer than microblog posts (me-dian=45), and their lengths vary more than tweet lengths do. The corpora analyzed here are similar insofar as their documents are much shorter than documents in more stan-dard IR collections. But the collections are also different from each other.
All experiments were done using the Indri search engine and the Lemur toolkit 5 . For efficiency, we used the stan-dard Indri stoplist and a custom Twitter-specific stoplist when forming document pseudo-queries. But subsequent retrievals used no stoplists or stemming except to mitigate common words X  influence in the baseline relevance feedback condition X  X eedback models were stripped of words in the Indri stoplist. Unless otherwise specified, all retrieval (both queries and pseudo-queries) used the Indri defaults of Dirich-let smoothing with  X  = 2500 . For each Tweets2011 query we retrieved 100 documents. We retrieved 50 documents per DCC query. To assess the merit of our proposed expansion methods we tested the conditions shown in Table 2.
Tables 3 and 4 list four effectiveness metrics for the con-ditions described above, on both of our test data sets 6 . The clearest result from the tables is the improvement over the QL baseline offered by our lexical expansion methods. Ex-cept for one case (P10 for LExp on DCC), the lexical ex-pansion methods always outperform the QL baseline. The positive effect of document expansion is especially strong when we interpolated the expanded model with the observed document model (i.e the TExp  X  condition).
 We can see the positive effect of document expansion in Figure 2. Each panel in the figure schematizes the distribu-tion of query terms in relevant and non-relevant documents, over a variety of data sets. Besides our two short document collections, as a point of reference we show the TREC 8 data described above and the small WT10g web collection. As we discussed earlier, the figure shows that query terms tend to http://lemurproject.org
Reported metrics are mean average precision (MAP), R-precision (Rprec), NDCG over all positions (NDCG) and precision at 10 (P10).
 Table 4: Observed IR Effectiveness on DCC Data.
 The  X  symbol indicates p &lt; 0 . 05 on a permutation test against the baseline QL. The  X  symbol indicates p &lt; 0 . 01 .
 have a higher probability in relevant documents than in non-relevant documents. The difference is especially stark for the standard (TREC 8 and WT10g) corpora. Observed query term probabilities in the unexpanded microblog corpus give no consistent evidence of relevance (among retrieved docu-ments). The value of query term probabilities in the unex-panded DCC data is higher than in the Twitter data, but less so than in TREC 8 and WT10g. However, the panels labeled Microblog (Exp) and DCC(Exp) show query term probabilities in expanded document models. In these cases the estimated probability of query terms in relevant docu-ments is significantly greater than the corresponding proba-bility in non-relevant documents (Mann-Whitney p 0 . 001 one-sided).

A surprising result of our experiments is the poor per-formance of feedback using relevance models. Inspection of the expanded queries that led to these results showed that the induced query models contained very idiosyncratic terms such as user names (Twitter) and administrative vocabulary (DCC). We include the feedback results because it is worth noting that the expanded document models are capturing se-mantics that expanded query models cannot, given the con-ditions encountered in these data. We hypothesize that this effect arises because, in addition to alleviating the vocabu-lary mismatch problem, our expansion method yields more data for estimating language models. Expanded queries al-leviate vocabulary mismatch and improve estimates of the query model, but expanded documents improve our esti-mates of the document models. In the context of short doc-ument retrieval, this effect seems to be crucial.

Figure 3 shows performance over a range of values (from 5 to 50) for k , the number of expansion documents used to fit an augmented language model D 0 (cf. Eq. 7). Con-rameter, Applied During Retrieval Based on Document Pseudo-Queries. is too high. All declines in MAP, Rprec, and NDCG from  X  = 2500 are statistically significant. This suggests that document expansion carries some risk. We hypothesize that much of this risk comes from the conjunctive and disjunctive semantics of aggressive smoothing. Weak smoothing pushes retrieval towards a Boolean AND X  X ng of query terms, while strong smoothing allows the predominance of only a few query terms to promote a document. Balancing this econ-omy is clearly important. However, it is also the case that Table 5 shows very extreme values for  X  . Because computa-tional constraints kept us from performing a full parameter sweep, we chose to test very high and very low smoothing parameters. In future work we plan to examine the  X  X afe X  region for smoothing in more depth.
Of our two data sets only Tweets2011 has a temporal com-ponent; the DCC documents lack any measurable chronol-ogy. Thus we tested the temporal expansion described in Section 4.2 only on the Twitter data. As a baseline we compared our method against the use of temporal priors introduced by Li and Croft. Effectiveness results for each condition are shown in Table 3. Both the baseline method (TPrior) and our temporal expansion (TExp) gave statisti-cally significant improvements over the non-temporally in-formed QL baseline. However, the difference in effect be-tween TPrior and TExp are small in Table 3, with both methods scoring nearly identically (the differences between them are not statistically significant).

Combining lexical and temporal expansion improves ef-fectiveness further. in Table 3 the method using both ex-pansion sources, LTExp, outperforms all other runs. For completeness, we compared LTExp against a run using the LExp  X  condition modified with a temporal prior. LTExp improved on this baseline for all metrics shown in Table 3, with p &lt; 0 . 05 for MAP and P10.

An interesting result is visible in Figure 4. The bar plot shows the difference in mean average precision (between TExp and TPrior) on a query-by-query basis. Though a few queries have near-identical scores under both TPrior and TExp, the majority of queries fare differently under each method. Both the baseline and proposed methods im-prove aggregate effectiveness on these data. But they appear to do so quite differently, suggesting that they are not in-terchangeable. The run in Table 3 labeled TBoth uses both temporal priors and temporal expansion. Though its perfor-mance is similar to temporal expansion alone, TBoth does see consistent improvement over TExp ( p is 0.83, 0.12, 0.01, 0.15 for MAP, Rprec, NDCG and P10, respectively), sug-tion of features of whatever type. Our results suggest that for the feature types explored here (lexical and temporal), these distributions convey useful information for IR.
A notable advantage of the temporal expansion method proposed here is its flexibility with respect to the seman-tics of a query. For instance, the temporal prior method can account only for  X  X ecency queries X  where the user seeks new information. However, our TExp approach assesses the similarity between the timestamps of documents retrieved by Q and those obtained from the pseudo-query of a docu-ment D . If Q retrieves documents clustered around a win-dow of time in the past, TExp will reward documents whose pseudo-queries X  results occupy the same window.

One issue that we have not addressed in this paper is the transformation of a document into a pseudo-query. In the interest of simplicity, we omitted any transformation other than removal of stopwords to improve retrieval speed. How-ever, our results suggest room for improvement by a more thoughtful approach. In particular, we consider the differ-ence between LExp and LExp  X  on the DCC data evocative on this matter. Interpolation with the original document model helped retrieval to a great extent in this case, a fact emphasized by Figure 3. We suspect that this is due to the length of some DCC documents. Many of these doc-uments are more verbose than, for example, tweets. These results suggest that longer documents do not lead to suitable pseudo-queries without some improvement.
We have proposed expanding document representations to improve retrieval effectiveness on corpora of very brief texts. Our contribution starts with the idea that short documents often make productive pseudo-queries. Because brief docu-ments tend to discuss at most one topic, the retrieved set for a document D is informative with respect to making predic-tions about the relevance of D . We proposed two methods of document expansion based on analysis of document pseudo-queries X  result sets. First, we augment the language model of D , obtaining an expanded language model D 0 . Second, we induce a model of the temporal affinity of D by analyzing the temporal profile of D  X  X  result set. We find that the like-lihood that this density generated the timestamps retrieved by a query Q yields an effective mechanism for letting time inform ranking.

In future work we plan to address several issues raised by this study: [8] Miles Efron, Peter Organisciak, and Katrina Fenlon. [9] Inna Gelfer Kalmanovich and Oren Kurland.
 [10] Giacomo Inches, Mark Carman, and Fabio Crestani. [11] Rosie Jones and Fernando Diaz. Temporal profiles of [12] Anna Khudyak Kozorovitsky and Oren Kurland.
 [13] Oren Kurland and Lillian Lee. Clusters, language [14] Oren Kurland, Lillian Lee, and Carmel Domshlak. [15] Carl Lagoze and Herbert Van de Sompel. The Open [16] Victor Lavrenko and W. Bruce Croft. Relevance based [17] Xiaoyan Li and W. Bruce Croft. Time-based language
