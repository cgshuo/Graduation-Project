 On-line news agents provide commenting facilities for readers to express their views with regard to news stories. The number of user supplied comments on a news article may be indicative of its importance or impact. We report on exploratory work that predicts the comment volume of news articles prior to publication using five feature sets. We address the prediction task as a two stage classifi-cation task: a binary classification identifies articles with the poten-tial to receive comments, and a second binary classification receives the output from the first step to label articles  X  X ow X  or  X  X igh X  com-ment volume. The results show solid performance for the former task, while performance degrades for the latter.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics Algorithms, Theory, Experimentation, Measurement Comment volume, prediction, feature engineering
As we increasingly live our life online, in the form of blogs, dis-cussion forums, comment facilities, etc., new types of data become available that can be mined for valuable knowledge. E.g., online chatter can be used to predict sales ranks of books [4]. Online news is an especially interesting data type for mining and analysis pur-poses. Much of what goes on in social media is a response to news events, as is evidenced by the large amount of news-related queries users submit to blog search engines [9]. Tracking news events and their impact as reflected in social media has become an important activity of media analysts [1]. We focus on online news articles plus the comments they generate, and attempt to predict news arti-cle comment volume prior to publication time.

One might raise the question why one should be interested in commenting behavior and the factors contributing to it. We en-visage three types of application for predicting the volume of com-ments generated by news articles. First, media and reputation anal-ysis is dependent on what users think of topics covered in the me-dia. Predicting the comment volume might help in determining the desirability of an article (e.g., regarding the influence on one X  X  rep-utation) or the timing of its publication (e.g., generate publicity and discussion during election time). Second, pricing of news articles by news agencies and ad placement strategies by news publishers could be made dependent on the expected comment volume; arti-cles that are more likely to generate comments could be priced dif-ferently. Finally, news consumers could be served only news arti-cles that are most likely to generate many comments; news sources can thus provide new services to their customers and can save con-sumers X  time in identifying  X  X mportant X  articles.

Our aim in this paper is to predict comment volume of news articles prior to publication. To this end, we seek to answer the fol-lowing two questions: (i) What are the dynamics of user generated comments on news articles? We look at article and comment statis-tics per source. (ii) Can we predict, prior to publication, whether a news story will receive any comments at all, and if so, whether it will receive few or many comments?
This work makes several contributions. First, it explores the dy-namics of user generated comments in on-line Dutch media. Sec-ond, it introduces the problem of predicting the comment volume of a news article. Third, it provides a set of surface, cumulative, textual, semantic, and real-world features that can be used to pre-dict the number of comments of a news story prior to publication. Fourth, it provides an evaluation of the introduced features. Fifth, an error analysis identifies possible causes for classification failure. Section 2 contains related work; we explore news comments in Section 3; our feature sets are introduced in Section 4; predicting comment volume is done in Section 5; Section 6 contains discus-sion, error analyses, conclusions, and future work.
Different aspects of the comment space dynamics have been ex-plored in the past. Schuth et al. [11] explore the news comments space of four on-line Dutch media, while Mishne and Glance [10] explored the weblog comment space. Kaltenbrunner et al. [6] mea-sured community response time in terms of comment activity on Slashdot stories, and discovered regular temporal patterns on peo-ple X  X  commenting behaviour. Recently, various prediction tasks and correlation studies have been considered in social media. Mishne and de Rijke [8] use textual features as well as temporal metadata of blog posts to predict the mood of the blogosphere. De Choud-hury et al. [3] correlate blog dynamics with stock market activity, and Gruhl et al. [4] perform a similar task with blogs/reviews and book sales. Szab X  and Huberman [12] predict the popularity of a story or a video on Digg or YouTube, given an item X  X  statistics over a certain time period after publication. Lerman et al. [7] forecast the public opinion of political candidates from objective news arti-cles. Finally, Tsagkias et al. [13] predict podcast preference using surface features extracted from podcast RSS feeds.

To our knowledge, no prediction tasks have been published that concern the volume of comments generated by online news articles.
Our data consists of the aggregated content from seven on-line news agents: Algemeen Dagblad ( AD ), De Pers , Financieel Dag-blad ( FD ), Spits , Telegraaf , Trouw , WaarMaarRaar ( WMR ), and one collaborative news platform, NUjij . We have chosen to in-clude sources that provide commenting facilities for news stories, but differ in coverage (regional/national), in political views, in sub-ject (general/politics/arts/entertainment), and in type. Six of the selected news agents publish daily newspapers and two, WMR and NUjij , are present only on the web. WMR publishes  X  X ddly-enough X  news and is interesting for observing the commenting behavior in this setting. NUjij is a collaborative news platform, similar to Digg, where people can submit links to news stories for others to vote for or start discussions on. We aggregate content for the period Nov 2008 X  X pr 2009, leaving us with a dataset of 290 375 articles, and 1 894 925 comments for all news sources.
 Table 1: Statistics of seven on-line news agents, and one collab-orative news platform for the period Nov 2008 X  X pr 2009.

We turn to our first research question: What are the dynamics of user generated comments on news articles? Table 1 reports article and comment statistics per source, and the time between publica-tion and the first comment, and between the first and last comment. The volume of published articles per source varies per source. At one end we find large news sites such as AD , De Pers , Telegraaf , and Trouw with more than 30 000 published articles; the other end consists of smaller news agents, such as FD , Spits , and WMR with less than 10 000 published articles. We observe similar variation in the ratio of commented news articles. In general it is two times higher compared to the ratio of commented blog posts [10]. Spits and WMR find almost all of their articles commented on, while the ratio drops to lower than 10% for Trouw . We notice that Spits allows comments from guests, saving users from the registration process, and even though WMR allows comments only from regis-tered users, the registration form needs minimal input, and is con-veniently located just below the comment section. For Trouw com-ments are enabled only for some articles, partially explaining the low number of commented articles.

The elapsed time between an article X  X  publication and its first comment is longer (6.7 hrs) compared to blogs (2.1 hrs) [10]. For some sources, comments start to arrive in the first two hours af-ter article publication (e.g., Spits and WMR ), while others receive tardy arrivals up to 10 hours after publication (e.g., FD and Trouw ). Similar patterns govern the reaction lifetime, the time between the first and last comment.
 Table 2: Listing of extracted features. The feature type is either nominal (nom), integer (int) or numeric (num).

The number of articles, the number of commented articles, the total number of comments, and the reaction times seem to be inher-ent characteristics of each source, possibly reflecting the credibility of the news organization, the interactive features they provide on their web sites, and their readers X  demographics [2]. Our features attempt to capture the differences between the sources into account.
We consider five groups of features: surface , cumulative , textual , semantic , and real-world . Table 2 summarizes all features. Surface features. Feed metadata quality plays an important role in a user X  X  decision to click a news item for reading or commenting. For example, if a news source supplies only the title of an article, but not a short summary, a user may prefer to click on a similar article from a different source that exposes more information. Cumulative features. News agents broaden their news cover-age through (inter)national news providers: A newsworthy story originating from a provider will therefore be published by multiple agents. The number of times we encounter a story in a time window is a good signal for it being interesting for multiple groups of read-ers and its exposure in multiple feeds increases its likelihood to be commented. On top of this, if the news supply is high, articles that could be commented, may not receive any comments because of the users X  fast attention shift. We encode competition for attention by recording the number of published articles in the same hour. Textual features. To collect textual features, or term sets, for each agent, we take the top-100 most discriminative unique terms us-ing log-likelihood scores. Discriminative terms indicate differences between news sources. General news sources like AD and Trouw Figure 1: Modeling comment distribution per source using the continuous log-normal distribution (black line). The grey bars represent the observed data. y -axis stands for probability den-sity, and x -axis stands for number of comments (binned). show mainly  X  X eneral X  news terms (e.g., Israeli and Palestinian ), while a financial news source ( FD ) has a clear preference for finan-cial terms: banks and Euro zone . The online news sources ( NUjij and WMR ) differ from sources with an offline presence, preferring terms like police , casino , and soccer .
 Semantic features. We apply named entity recognition to extract persons , locations , organizations , and miscellaneous entities. Sim-ilar to the discriminative terms, we select the top-50 most discrim-inative entities for each entity type. Politicians are popular entities for discussions: Geert Wilders (right-winged politician) and Jan-Peter Balkenende (Prime Minister) are among the ones attracting many comments. As to organizations, soccer clubs attract much discussion: Ajax , PSV , and Feyenoord are the three biggest soccer clubs in the Netherlands. Here again, politics is a popular topic: Hamas , PvdA , and PVV are dominant political organizations. Real-world features. The last set of features explores the poten-tial correlation between real-world environmental conditions (e.g., weather conditions) and commenting behavior. Here, for each ar-ticle X  X  publication time, we assign the median temperature in the Netherlands at that time as an indicator of good or bad weather.
We now turn to the second research question: Can we predict, prior to publication, whether a news story will receive any com-ments at all? And if it receives comments, can we predict whether it receives few or many comments? Recall that for each news agent the comment volume may vary substantially. Before defining vol-ume levels, the comment volume needs to be normalized across sources; we fit a log-normal distribution to each source, similar to the modeling of response time in Slashdot [5], and define the threshold between  X  X ow X  and  X  X igh X  volume at the inverse cumula-tive log-normal distribution function at 0.5 (see Figure 1).
We address the prediction task as two consecutive classification tasks to compensate for the highly skewed datasets. First, we seg-regate articles with regard to their potential of receiving comments. A binary classification is performed with two classes: with com-ments vs. without comments . Second, we predict the comment vol-ume level for the articles predicted to receive comments in the first step (positive class). This second classification is performed with two classes: low volume and high volume . We are not interested in optimizing classification performance, but rather in investigating whether different types of features can distinguish articles that hold potential to receive comments, and ultimately to quantify and pre-dict this potential in terms of comment volume levels. Threshold for log-normal: AD : 3, De Pers : 3, FD : 2, NUjij : 6, Spits : 36, Telegraaf : 32, Trouw : 4, WMR : 34
We report on classification experiments per news source on the following experimental conditions: a baseline, one group of fea-tures at a time, and combining all feature groups. The baseline consists of six temporal features ( month , week of the month , day of the week , day of the month , hour , and first half hour ). For each source in our dataset, we create training and test sets. The training sets contain articles published from Nov 2008 until Feb 2009, and the test sets consist of the articles published in Mar 2009. We use RandomForest, a decision tree meta classifier. For evaluation of the classification performance we report the F1-score, and the percent-age of correctly classified instances for each experimental condi-tion. Significance of results is measured with the Kappa-statistic.
Looking at Table 3, most sources show a high F1 for the negative class, while only two sources show a high F1 for the positive class. These results reflect the commented/non-commented ratio of arti-cles in each source that leads to highly skewed training sets. WMR and Spits , most of their articles having at least one comment, show a high ratio of positive examples, pushing the F1 score close to 1. As a result, for this classification experiment, the different groups of features are not expected to differ greatly for these two sources.
The baseline displays solid performance across the board. How-ever, the Kappa-statistic hovers near zero, suggesting that if we classified the articles randomly, there is chance of observing simi-lar results. Among the groups of features, textual and semantic fea-tures perform the best for most sources. This confirms that certain words and named entities trigger comments. Cumulative, surface, and real-world features perform similar to the baseline. Interest-ingly, the real-world features for AD achieve an F1 score of 0.749 for the negative class with Kappa at 0.48), and the surface features X  performance for Trouw has an F1 score of 0.952 for the negative class with Kappa at 0.36. The combination of all groups of features does not lead to substantial improvements, but hovers at similar levels when using textual features only.
For the second classification experiment, articles that have pre-viously been classified as yes comments are now classified based on whether they will receive a high or low volume of comments. Misclassified negative examples (articles without comments) from the first stage are labeled low volume . Five sources lack results for the real-world feature set due to the classifier marking all articles as negative in the first step.

In this setting, the F1 score is more equally distributed between the negative and the positive class. Textual and semantic features prove again to be good performers with non-zero Kappa, although varying almost 24% between sources ( NUjij vs. FD ). The variation suggests that the number of textual and semantic features should be optimized per source. The performance of cumulative features varies substantially between sources. E.g., for Trouw and NUjij it is among the best performing groups, but for FD it has a negative Kappa index. Looking at all groups combined, Kappa values in-crease, an indication for more robust classification. In general, the classification performance for all groups combined is better than the baseline, although the difference depends on the source. Com-paring the performance of all features and individual feature sets, we observe that in some cases performance degrades in favor of a higher Kappa-value: For Telegraaf for example, textual features alone classify 55% of the instances correct (Kappa: 0.06), while all features reach 51% correctly classified instances (Kappa: 0.14). Table 3: Binary classification of articles into articles with ( yes ) and without ( no ) comments. We report the F1-score, Kappa (K), and accuracy (Acc) for the positive and negative class.
We presented exploratory work on predicting the comment vol-ume of news articles prior to publication. We have developed a set of surface, cumulative, textual, semantic, and real-world fea-tures and report on their individual and combined performance on two classification tasks: Classify articles according to whether they will (i) generate comments, and (ii) receive few or many comments. Textual and semantic features prove to be strong performers, and the combination of all features leads to more robust classification.
To better understand our results, we look at misclassified in-stances. We identified five main types of error: (i) The event dis-cussed in the news article is prone to comments, but this particular event is happening too far away (geographically). (ii) The event may be a comment  X  X agnet, X  but is too local in this case. (iii) The news article itself is not attracting comments, but one posted com-ment sparks discussion. (iv) Shocking, touching, or in other ways surprising articles often generate more comments than can be ex-pected from the article X  X  content. (v) From the content of the arti-cle, a  X  X ontroversial X  topic might be expected, but the actual event is rather uncontroversial. Our failure analysis indicates that the fea-tures used in this paper are not the only factors involved in the pre-diction process. Future work should therefore focus on extracting more feature sets (e.g., context and entity-relations), use different encodings for current features, optimize the number of textual and semantic features per source, and explore optimized feature sets. Acknowledgments. This research was supported by the DuOMAn project (STE-09-12) carried out within the STEVIN programme and by the Netherlands Organisation for Scientific Research (NWO) under project numbers 017.001.190, 640.001.501, 640.002.501, 612.066.512, 612.061.814, 612.061.815, 640.004.802.
