 Recently, sparse representation has been a powerful technique for image represen-i.e. dictionary learning. Dictionary learning can be divided into three categories: to whether labels of signals are used while learning the dictionaries. In real applications, it is worth noting that a huge amount of unlabeled data can be more easily obtained than labeled data, and thus unsupervised dictionary learning is very useful. For example, successful unsupervised image clustering can enable the subsequent supervised learning tasks to recognize image with accuracy that would not be possible without the use of image clustering. In this paper, we focus on developing an effective unsupervised dictionary learning method for image representation and then image clustering, which is useful and potentially powerful for image mining tasks. For unsupervised dictionary learning, sparse coding [ 4 ] receives much attention in machine learning and image processing. It aims at learning a dictionary that consists of a set of basis items and the sparse coordinates with respect to the dictionary.
 Following the work of original sparse coding, graph regularized sparse coding (GSC) [ 9 ] and hypergraph Laplacian sparse coding (HSC) [ 10 ] have been devel-oped to encode the geometrical information in the data space, which can preserve the locality and similarity between the image features or instances in the sparse coding space . Even though these manifold regularized sparse coding can achieve much improvement in image processing. in general, it is nontrivial to determine the intrinsic manifold in a systematical way. Usually, cross validation-based para-meter selection does not scale up well for a huge number of possible parameters ciently select the optimal manifold to make the performance of the employed graph or hypergraph regularized sparse coding method robust, or even better. Another issue of GSC or HSC comes from the insufficient exploitation of mani-fold structure as they only add the Laplacian regularization to the objective of sparse coding.
 Therefore, in this paper, we propose a novel sparse coding scheme, named multi-hypergraph incidence Consistent Sparse Coding (MultiCSC). MultiCSC exploits the hypergraph model to regularize the sparse coding, as the hyper-graph is able to capture the high-order manifold structure of high dimen-sional data compared to simple graph. A hypergraph incidence consistency reg-ularization term (HIC) is presented to further leverage the hypergraph-based manifold, based on the assumption that the hypergraph structure can be well reconstructed by sparse codes associated with data instances. Moreover, multi-hypergraph learning term is also integrated to automatically select the optimal manifold structure. Finally, alternative optimization of MultiCSC is presented. The improved performance of image clustering on real image datasets validates the advantage of the proposed unsupervised dictionary learning method. 2.1 Graphs, Hypergraphs and the Laplacian Matrix A hypergraph G ( X, E ) consists of a set of vertices X =[ of the vertices (i.e., e j  X  X ,  X  j ). The incidence matrix H hypergraph records each hyperedge in a column, so H ij =1if H ij = 0 otherwise ( in a probabilistic way as H ij  X  [0 , 1] if x i  X  e j .
 The weight matrix W  X  R n  X  n of a hypergraph is to measure how close each two vertexes in the manifold space are, defined as the number of shared hyperedges multiplied by their weights. In matrix form, the normalized weight matrix and Laplacian matrix are defined as: where D e , D v and W e denote the diagonal matrices of hyperedge degrees, vertex degrees, and hyperedge weight, respectively , and I is the identity matrix. 2.2 Basic Idea of Sparse Coding measured on m dimensions, the goal of dictionary learning and sparse coding is to decompose the data matrix X into a dictionary matrix B =[ R m  X  r and a sparse coefficient matrix S =[ s 1 , s 2 ,..., can be reconstructed using B and S . Since r is typically much smaller than n , the sparse coefficients can represent the data objects with much lower dimen-sions. Formally, the sparse coding problem can be described as the following optimization problem: where  X  F denotes the matrix Frobenius norm (i.e., A F = i j  X  represents the tradeoff between reconstruction error and sparsity. 2.3 Hypergraph Laplacian Sparse Coding To preserve the locally geometrical structure of the data points in the original space, hypergraph Laplacian sparse coding (HSC) [ 10 ] was proposed to regularize the sparse coefficient S with the hypergraph Laplacian term, as follows: min The hypergraph Laplacian regularization term R HL is based on the manifold assumption that if two data points x i and x j are close in the intrinsic manifold space, then their corresponding sparse representations, s close to each other. This term is detailed as: where Tr (  X  ) stands for the trace of a square matrix, and L is the Laplacian matrix of the hypergraph constructed from the data objects [ 10 ]. 3.1 Hypergraph Incidence Consistency tion is that the sparse codes S should be able to well reconstruct supervised information (e.g., class labels), that is, a well classification performance using S . Inspired by this idea, for hypergraph-based unsupervised learning, we conjec-ture that data points in the same hyperedge are more likely to share the same label. Thus, if we use the incidence matrix H to provide additional information of supervision, this matrix should also be reconstructible with sparse codes S . Aiming at reconstructing the hyperedge incidence matrix H using sparse coding, we propose to minimize the reconstruction error, as measured by the following hypergraph incidence consistency regularization term (HIC): denotes the number of hyperedges (i.e., the number of columns in H ). The dictionary learned while regulating the incidence matrix H reconstruc-tion error by minimizing Eq. ( 3 ) is adaptive to the underlying structure of the dataset, which leads to a good representation of each data point in the set with strict sparsity constraints. HIC generates hypergraph-oriented discrimina-tive sparse codes, and addresses the desirable ability in hypergraph construction by clustering algorithms and classifiers, such as in [ 3 ]. The hypergraph-oriented discriminative property of sparse code is very important for the performance of unsupervised image clustering tasks. 3.2 Multi-hypergraph Learning For both hypergraph Laplacian [ 10 ] sparse coding and proposed HIC term, it is vital to construct an optimal hypergraph to represent the intrinsic manifold. Instead of using exhaustive search (that does not scale well) or cross valida-tion (that tends to overfit), multi-hypergraph learning techniques have been proposed to approximate the intrinsic manifold for hypergraph Laplacian, such as ensemble manifold regularizer [ 11 ] and the multi-hypergraph regularizer in matrix factorization [ 12 , 13 ]. Multiple hypergraph learning works well since the intrinsic manifold of the collected data points is assumed to lie in a convex hull of a set of previously given candidate manifolds, each of which indicates one kind of manifold data structure, defined as follows.
 ent weighting scheme and neighbor size parameter, the corresponding Lapla-cian matrices and incidence matrices can respectively be derived as  X  = {
L 1 ,L 2 ,...,L t cian as a linear combination of the hypergraph Laplacians of these candidate manifolds, where each candidate graph G k is associated with a coefficient  X  defined as: where  X  =[  X  1 , X  2 ,..., X  t ] is the hypergraph weight vector, s.t.  X  k -nearest neighbor selection, so the j -th hyperedge in each hypergraph G associated with the identical data point x j , which covers the nearest neighbors and x j itself, and the difference lies in the weighting scheme and neighbor size parameter. In this case, these incidence matrices have additive property as all the j -th columns in H i denotes the same hyperedge corresponding to data point x . Thus, the incidence matrix H of optimal hypergraph can be also assumed as the linear combination of the candidate hypergraphs X  incidence matrices. determining the optimal linear combination weights for a group of pre-computed graph candidates. More specifically, by substituting L in Eq. ( 4 )into L in Eq. ( 2 ), the multi-hypergraph Laplacian regularization term can be written as: incidence consistency regularization term (multiHIC) can be written as: term (i.e.,  X  2 2 ) will be added to the objective function for optimization (see Eq. ( 8 )). By minimizing the multi-hypergraph Laplacian regularization term in Eq. ( 6 ) and the multi-hypergraph incidence consistency regularization term in Eq. ( 7 ), a larger weight is expected for a hypergraph with better weighting and parameter selection scheme. 3.3 Overall Fourmulation of Multi-hypergraph Incidence Consistent Sparse Coding Adding the multi-hypergraph Laplacian regularization term in Eq. ( 6 ) and the multi-hypergraph incidence consistency regularization term in Eq. ( 7 )tothe objective function of sparse coding, we propose a new sparse representation framework called multi-hypergraph incidence consistent sparse coding (Mul-tiCSC). More specifically, the objective becomes where  X  ,  X  ,and  X  are tradeoff parameters. In particular, we call  X  the hyper-graph consistent tradeoff parameter, and  X  is the multi-hypergraph combination parameter. In summary, the MultiCSC problem can be summarized as follows: Usually the data instance vector x i and each row vector in H will be firstly normalized to be a unit norm, so the constraints b i 2 2  X  make B and Q comparable with X and H . As we can see that, HSC [ 10 ]isa special case of MultiCSC when  X  =0and  X  =[0 ,  X  X  X  , 0 , 1 , 0 ,  X  =0 ,l = k ). We define Hypergraph consistent sparse coding (CSC) as a special case of MultiCSC when  X  =[0 ,  X  X  X  , 0 , 1 , 0 ,  X  X  X  , 0] T only one single hypergraph.
 Given sparse codes S of images resulting from optimization Eq. ( 9 ), clustering can be implemented under the framework of sparse representation-based image clustering [ 1  X  3 ]. As the objective is jointly non-convex with ( B , S , Q , MultiCSC in Eq. ( 9 ) is infeasible. Fortunately, we can optimize sparse coding along with reconstruction matrix ( B , S , Q ) and hypergraph combination weights  X  by a two-step iterative algorithm, as suggested in related works using multiple manifold learning [ 12 , 13 ]. At each iteration, ( B , S , Q )and optimized while the others are fixed, and then the roles are alternately reversed. These iterations are repeated until convergence is achieved or a maximum num-ber of iterations is reached.
 Theorem 1. When  X  is fixed, ( B, S, Q ) can be optimized as a new Laplacian regularized sparse coding algorithm by the following transformation: Proof. By fixing  X  , the objective function in Eq. ( 9 ) can be rewritten as: constant when  X  is fixed.
 Therefore, when  X  is fixed, ( B , S , Q ) can be optimized as a new Laplacian regularized sparse coding algorithm with the above transformation. Now, the objective function in Eq. ( 11 ) is the same as Eq. ( 2 ) of HSC, where the optimization of Eq. ( 11 ) can be implemented by alternatingly minimizing over S or B while holding the other fixed.
 Theorem 2. When fixing ( B, S, Q ) , the optimization problem Eq. ( 9 )canbe transformed into a constrained quadratic programming problem.
 Proof. By fixing ( B, S, Q ) and eliminating irrelevant terms, the objective func-tion in Eq. ( 9 ) may be transformed into: where is a constant term since B , S ,and Q are all fixed. Let where A (:) denotes the operation of flattening a m -by-n matrix into a column vector of length m  X  n . Then, Eq. ( 12 ) can be rewritten as: which is a well-defined constrained quadratic programming problem. Since the optimization of  X  when fixing ( B, S, Q ) can be transformed into a constrained quadratic programming problem, we can now efficiently solve it using the quadric optimization solver in the CVX Matlab toolbox [ 14 ].
 BasedonTheorems 1 and 2 , the overall process of optimizing MultiCSC can be described in Algorithm 1 . In the algorithm, FeatureSign denotes the feature-sign algorithm to solve l 1 regularized convex optimization for S ,and LagrangeDual is the Lagrange dual algorithm to optimize B and HSC [ 10 ]. ConstrQuad is the standard solver for constrained quadratic opti-mization.
 Algorithm 1. The optimization procedure of MultiCSC In this section, we apply the proposed MultiCSC method to image clustering tasks, which is implemented on real word image datasets. 5.1 Benchmark Datasets Four popular real image datasets have been used in our experiments as bench-mark datasets, summarized in Table 1 1 . For each dataset, we first normalize each image vector into unit norm. Then, as suggested in [ 9 ], we use principal compo-nent analysis (PCA) to eliminate correlations among features, and take the first 64 principal components as the new transformed features. 5.2 Competing Models and Setup Our proposed model will be compared with a number of baseline models.  X  Original , KSVD and SC . The  X  X riginal X  method is to cluster image objects  X  GSC , HSC ,and CSC . GSC [ 9 ]andHSC(HSC)[ 10 ] respectively add the  X  MultiGSC , MultiHSC ,and MultiCSC . MultiGSC and MultiHSC respec-The dictionary size r of all these models is set to be 128, since several recent works on sparse coding have advocated the use of overcomplete representations for images. For GSC, HSC, and CSC, graph or hypergraph is constructed by 3-nearest neighbor search in Euclidean distance as suggested in [ 9 , 10 ]. For Multi-GSC, MultiHSC, and MultiCSC, 12 candidate hypergraphs are constructed in the experiments with different neighbor sizes and weighting schemes (detailed in [ 12 ]). All the incidence matrices of hypergraphs are normalized such that the row vector representing each image is unit norm.
 space to group image datasets, and compare the clustering results under two standard metrics: accuracy (ACC) and normalized mutual information each time with a random set of initial centers in the K-means step in [ 2 ], and the average are reported. The regularization parameters of each competed algorithm (  X  in KSVD and SC;  X  in GSC and HSC;  X  in all multiple-based learning;  X  in CSC) are tuned to get the best clustering performance (highest average of NMI in 50 runs).
 5.3 Clustering Results on Image Datasets We implement sparse coding on each dataset, and then do clustering in the sparse code space. The image clustering results are summarized in Table 2 , fur-ther explained below. According to results in Table 2 , methods with manifold regularization outperform those without manifold regularization. In other words, KSVD and SC are outperformed by all other manifold regularized dictionary learning methods. Furthermore, methods using hypergraph consistency regular-ization (i.e., CSC) perform consistently better than those using simple graph or hypergraph regularization (i.e., GSC and HSC). These two points illustrate the effectiveness of manifold structure in sparse coding and the superiority of pro-posed hypergraph incidence consistency regularization. Moreover, we specifically compare single graph/hypergraph models with multiple graph/or hypergraph models, included in Table 2 and visualized in Fig. 1 . We can see that methods using multiple manifold learning (i.e., MultiGSC, MultiHSC, and MultiCSC) outperform their respective single graph/or hypergraph counterparts (i.e., GSC, HSC, and CSC, respectively). In all datasets, whether using single-graph or multi-graph, sparse coding exploiting hypergraph incidence consistency term beats that only with hypergraph Laplacian regularization. Although only ACC by NCuts is shown in Fig. 1 due to space limitations, we observe similar results using NMI. These observations reveal that both hypergraph incidence consis-tency regularization and multiple manifold learning make difference for sparse coding, and thus, clustering results on sparse codes learned with MultiCSC are more reliable.
 5.4 Robustness on Clustering In our proposed MultiCSC algorithm, there are two parameters: the hypergraph consistent tradeoff parameter  X  , and the multiple combination parameter  X  . In this subsection, we test how clustering performance varies with these two tradeoff parameters changing. Figure 2 shows the clustering performance versus the tradeoff parameter  X  when  X  is fixed as  X   X  HIC . Figure 3 shows the clustering performance versus the hypergraph consistent tradeoff parameter  X  when  X  is fixed as  X   X  MultiHIC . From these figures, we can observe that the performance of proposed MultiCSC keeps steady with varying  X  . The performance rises with increasing  X  and stabilizes when  X  is greater than 10 (except for CMU-PIE when  X  is greater than 0 . 5). These observations prove robustness of the clustering results, and provide references for parameter settings when using our proposed sparse coding methods. This paper presents a novel sparse coding method called MultiCSC, which explic-itly considers sufficient exploitation of manifold structure and automatic opti-mal manifold selection. By introducing a hypergraph incidence consistency term, the hypergraph of input dataset can be well reconstructed using learned sparse codes, and the optimal manifold represented by hypergraph model lies in the linear combination of candidate hypergraphs, where the combination weight is learned in addition to the dictionary and the sparse codes. Experimental results on image clustering have illustrated that our proposed algorithm achieves better discriminating power and significantly enhances the clustering performance.
