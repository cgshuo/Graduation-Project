 1. Introduction uals X  rights have been violated, more and more litigation has occurred.
 vocabulary, and without the help of legal experts. In Fig. 1 , we show the proposed statute retrieval approach. ficient knowledge problem can be alleviated.  X 
In recent years, text mining research has gotten more and more attention. Basically, text mining is the procedure of Wu, 2010; Schumaker, Zhang, Huang, &amp; Chen, 2012 ), document summarization ( Goldstein, Mittal, Carbonell, &amp;
Kantrowitz, 2000; Li, Du, &amp; Shen, 2013; Wang, Zhu, Li, &amp; Gong, 2009 ), online advertisement recommendations &amp; Yamada, 2011; Yin, 2007 ), etc.

Text mining has been applied in various areas. Although a few past studies applied text mining techniques to the legal most relevant statutes with respect to the user X  X  problem.
 and the top k 2 most similar statutes are selected. Finally, by applying associative statute rules to the top k ute weight computation metric is defined, so as to obtain the most relevant statutes for the user query. created between laypeople and legal statutes, and (3) the most pertinent statutes are recommended to users. This work classifier to classify the cases into k 1 statutes. Then, from these k utes, with respect to the user query, by employing the semantic relatedness measure (i.e., Normalized Google Distance
The contributions of this research are as follows. statute prediction in the legal domain has not been attempted in any previous research. described by user. The core of the approach is to remedy the gap between lay terms and legal terms without using a synopsis. tal results show that it performs accurately and effectively.
 clusions and future directions are presented in Section 5 . 2. Literature review 2.1. Background
The legal system in Taiwan is based upon the written law, which is enacted by the legislature or the congress, and who the society.
 judge.

In our study, we utilize a collection of judgments as training documents. Two relevant parts of a judgment are employed acquire relevant statutes.
 2.2. An overview of text mining as natural language processing (NLP), and connects them using data mining, machine learning, and statistics.
In general, an IR system is composed of three components: Documents, Queries and Matching/Ranking functions. Most ilarity between queries and documents can be computed using the distance or correlation between the corresponding vec-of the most successful models and most existing information retrieval systems were designed based on it. information retrieval system was implemented and presented many important concepts in research, including the vector and more), fielded searching, multiple-index searching with merged results and two ranking models: VSM and BM25 ( Robertson &amp; Zaragoza, 2009 ).

The main concern of Question X  X nswering (QA) system is that it can automatically answer accurate questions posed by humans in a natural expression of queries. Typically, QA systems can be divided into two categories: open-domain and domain-specific. An open-domain QA system aims at returning an answer to a user X  X  question with short texts rather than (TREC) commenced to supply a standard QA evaluation track from a large collection of documents. A famous open-domain
QA system, IBM X  X  Watson system ( Ferrucci et al., 2010 ), is developed by the IBM DeepQA research team. The main design idea behind Watson is that it synthesized information retrieval, natural language processing, knowledge representation is not available to the public.

For domain-specific question answering, there have been fewer recent works developed, like a medical domain QA sys-tem, MedQA ( Lee et al., 2006 ), which employs supervised machine learning approach to perform question classification based on an evidence taxonomy built up by physicians. In engineering education domain, Diekema, Yilmazel, and Liddy (2004) developed the Knowledge Acquistion and Access System (KAAS) QA system using a user-oriented approach in a col-
QA systems in academic domain. Despite those promising QA systems proposed recently, however, to our best knowledge, there is still no well-built QA system in legal domain. 2.3. Applications of text mining
When text appears in new document types, it usually leads to novel text mining applications. For example, text mining &amp; Shih, 2012; Li &amp; Wu, 2010; Reyes, Rosso, &amp; Buscaldi, 2012; Schumaker et al., 2012 ).
Additionally, text mining methods can be used to guess the identities of anonymous authors ( Stamatatos, 2009; Zheng 2011; Yin, 2007 ). In the last few years, QA has received more attention. Community-based QA (CQA) is an emerged focus content analysis approach was proposed to help text QA acquire relevant answers by adopting multimedia information such discussion should illustrate the impact text mining has had in countless situations. 2.4. Related academic research on text mining in the legal domain
Text mining in the legal domain has been an emerging research topic in recent years. So far, only several studies have been done on this topic. Moens (2001) gave an overview of text mining methods and discussed their potential to help improve legal document retrieval. Besides, Moens (2005) also proposed several XML retrieval models which exploit the structured and unstructured legislative document information to a query. EgoIR ( Gomez-Perez, Ortiz-Rodriguez, &amp; ments based on Legal Ontology. Conrad and Schilder (2007) presented an opinion mining application on legal web blogs most relevant judgments using ordinary terminology or statements as queries.
 ute prediction system that is custom-designed for the general public. 3. Research design be regarded as a text document. However, several differences exist between legal documents and normal documents. to develop a brand new approach that can address such differences. (1) The terms used by laypeople differ from the ones that appear in legal documents, such as judgments and statutes. approach can perform automated statute prediction. Before classifying the judgments, a text preprocessing procedure is performed on all documents in the training collection. In the preprocessing procedure, we conduct CKIP (2013) for represent documents. In criminal case documents, however, other POS tags are also meaningful such as adjectives ( nouns, verbs, and adjectives as candidate terms.

For simplicity, we introduce our method based on the three phases of the online process; we do not discuss the batch process separately. However, whenever needed in discussing the online process, we will explain the procedures from the batch process.

The TPP approach is depicted in Fig. 4 . The design framework is separated into three phases: (1) select the top k each phase. These three phases are described and explained in detail below. 3.1. Phase 1: Select the top k 1 statutes ranking loss and generates a better classification model for prediction ( Tsoumakas, Katakis, &amp; Vlahavas, 2010 ). ment (VSD) model is used to represent the documents.

This research attempts to generate representative vectors for each criminal case in the gathered document set. Through-out this paper, each document (i.e., criminal judgment) is denoted by term set { t number of terms that occur in the document. The relevance of each t lowing weighting schemes for assigning weights to terms: tf ( t method ( Clare &amp; King, 2001 ) is employed, which calculates the entropy of each term t p  X  S k j t i  X  denotes the probability of statute S k given that term t
S tributed over numerous statutes.
 After feature selection, the training judgments are transformed into vector space documents (VSDs) according to TF-IDF. user query are produced. All the statutes are sorted by probabilities, and the top k a dictionary or an ontology model that can store all these mapping relations.

To overcome these difficulties, Normalized Google Distance (NGD) is employed in this phase. It utilizes a well-known search engine, Google, which can return aggregate page-count estimates for a query or keyword. Normalized Google Dis-ing. In our study, we applied the NGD method to terms transformation.

Fig. 7 demonstrates the process of query terms transformation. The preprocessing procedure is used to segment a user mula, which is defined as follows:
M denotes the total number of web pages indexed by Google Search, f ( t t , f ( u j ) denotes the number of web pages containing judgment term u independent terms have a distance value of one.

Furthermore, we can associate a query term t i with k judgment terms u ( g i ,1 , ... , g i , k )isa k -dimensional vector, where g i , j tion of g ij is as follows:
The higher the value of g i , j , the greater the similarity between term t steps.

Step1 : Calculate the similarity value g i , j between query terms t
Step2 : Let pct i , j be the proportion of g i , j and the sum of g term t i in query q . The formulas of pct i , j and lw i , q max l freq l ; q is computed over all terms that occur within query q .

Step3 : Distribute the weight of lw i , q to k judgment terms u Step4 : Sum up the weight tw i , j of each term.

Figs. 8 and 9 show an example of how to distribute the weight of a query term to judgment terms and how to add together the weights of each judgment term. Assume that the similarity values of query term T are 0.7, 0.2, and 0.4, respectively, while those of T 2 to terms U found that the percentages of U 1 , U 3 , and U 4 with respect to T
U , and U 4 to T 2 are 0.5, 0.31, and 0.19, respectively. Therefore, the weights of term T are 0.54, 0.15, and 0.31, respectively, while the weights of term T 0.114, respectively. Finally, the weights of judgment terms U 3.2. Phase 2: Select the top k 2 statutes
Using the statute probabilities from the first phase, we identified the top k ing weights to terms: TF ( l i , j ) ISF ( l i ), w ( l i we encounter the same difficulties as when transforming query terms into judgment terms. Therefore, we applied the same NGD transformation method as described in Phase 1 to solve these problems.
 segment a user query into multiple terms. Then, in order to transform the terms, we utilize the NGD method.
Similarly, by applying the transformation method described in Phase 1, we can transform a vector of r terms in a user query into a list of statute terms and retrieve the most similar k utes. Cosine similarity function measures how close user query q is to statute s equation: where 0 6 Sim cos ine  X  s j ; q  X  6 1. 3.3. Phase 3: Select the final predicted statutes mine the final predicted statutes.
 mining algorithm is employed to discover associative statute rules. Let S ={ s s is one statute, s j is another statute, and s i \ s j  X   X  ment, then we are 70% confident that s 2 is also cited.

A set of associative statute rules can be found using the Apriori mining algorithm. Let w w statutes, the following formula, called the SFW (statute final weight) formula, is used:
M denotes the number of statute rules with consequent s j s ! s j .

In accordance with the SFW formula, the SFW values of statutes are computed and ranked. Then, the top candidate pre-shown in Fig. 11 . Assume that the weight values of s 1 , s that the confidence of the association rules of s 1 , s 3 final weights of s 1 , s 3 , s 4, and s 5 with respect to s (0.823984) is acquired through the SFW formula.
 4. Experimental study 4.1. Testbed
To evaluate our TPP approach, we conducted experiments on the Chinese criminal judgments stored in the Law and Reg-the time of the judge X  X  sentence. The experiment data used in our research were gathered from the ten most common types about 3 months to gather manually. Table 2 shows the distribution of criminal judgments over the ten types of crimes. To test the performance of the TPP approach, we selected 70 examples from metropolitan civil news stories ( Udn News
Net, 2013 ) as queries. Udn News Net was selected for the query data because it is a well-known news website in Taiwan, number of recommended related statutes. Most queries contain no more than three pertinent statutes. As a result, in our lowing is an example of a query:
Query:  X   X   X   X   X   X  KTV  X   X  X  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  a female KTV clerk named Luo, who secretly wrote down guests X  credit card numbers and authentication codes to conduct misappropriated online shopping.) From this query, we obtain the following useful terms:
Terms:  X   X  (seize),  X   X  (secretly),  X   X   X  (credit card),  X   X   X   X 
After all the query and training data were collected, text preprocessing tasks were executed, including word segmenta-tion, stop-word elimination, and POS tagging. These outcomes were then treated as the input data for the next step. 4.2. Details of implementation
Experiments were conducted to examine the performance of the TPP approach. First, preprocessing was executed for the document set. A stop word list containing 7321 words was developed to remove useless words, and 8306 terms were extracted from the document set. The TF-IDF weighting schemes were then used to determine the feature weights. For fea-source multi-label SVM package, to execute the SVM learning and prediction processes.
To select optimal k 1 and k 2 values, we used 50 query examples as testing documents. Since the TPP approach has three phases, we must evaluate each phase X  X  performance. Performance was evaluated according to the statutes X  rankings, which chosen for processing. The statute terms were extracted from these top 28 statutes for each query, such as  X   X  (resulted in death), and  X   X  (property). Every query term was transformed into a different number of statute terms using the NGD transformation method. The TF-ISF weighting schemes were then used to determine the statute term weights. ranked experts X  statutes were 2 and 16, respectively. Therefore, we set k produced. Then, we applied the SFW formula to acquire the final predicted statutes. The best and worst ranking statutes ciated statutes recommended by legal professionals are covered by selecting the top 13 statutes among all queries with k = 28 and k 2 = 16. We also found that the performance continuously improved from the first phase to the third phase.
To see how k 1 and k 2 influence the performance of our TPP approach, we had to test various combinations of k set k 1 = 28 and k 2 = 16 (denoted by TPPBASE) as the baseline combination. Table 3 illustrates the combinations of k that we compared with TPPBASE. 4.3. Experimental results and evaluation answer set for each query is very small, the precision rates, which are the number of statutes recommended by experts that are included within the top N statutes. The following equation is used to compute the coverage:
N denotes the threshold on the number of output statutes. 4.3.1. Find the optimal combination
Fig. 12 illustrates the coverage of TPPBASE. As can be seen from the graph, the third phase outperforms the first two and 50% in the third phase. When N was set to 13, the third phase reached the maximum 100%, while the first phase only reached 63.6%. Coverage reached 100% for all phases, however, when N = 28.

In Figs. 13 X 15 , we present the coverage values for combinations with k mance improves when the value of k 1 decreases. Fig. 13 clearly shows that by fixing k smaller k 2 value results in a higher coverage rate.

Similarly, Fig. 14 reveals that by fixing k 1 = 30 and varying the value of k recommended statute ranked behind the top 15 in the two queries. Consequently, the coverage could not be further improved. Finally, as shown in Fig. 15 , we checked the coverage of testing queries by fixing k of k however, when the coverage for a large k 2 becomes greater than that of a small k make the input statute set of the third phase as complete as possible. Since a larger k larger k 2 will result in better coverage than a smaller k
In this experiment, we tested many combinations of k 1 and k the results of k 1 and k 2 using the TPP approach. After comparing these combinations, we find that k other combinations cannot reach 100% coverage, they are more compact and can reduce user workload when applying the suggested statutes. One of these combinations is ( k 1 = 30, k 4.3.2. Comparison
To assess the performance of our custom-designed approach, the comparison methods used a classic TF-IDF scheme to ison, where the second group contains NGD transformation while the first group does not. In each group, there are three 2 (denoted by F2) and Phase 1 + Phase 2 + Phase 3 (denoted by F3); and (2) With NGD: Phase 1 (denoted by F4), Phase 1 algorithms on statute prediction. Compared with each algorithm in different combination of k results in Table 5 , we see that our proposed method is better than all the comparative methods.
Besides, we evaluated the performance of TPP method by comparing results using three state of the art retrieval func-results of different methods. Compared to the three state of the art functions, our proposed method TPP contributes to three methods. Therefore, we can conclude that the TPP approach provides an innovative method of statute prediction. 5. Conclusions and future directions become more and more important. Due to a shortage in knowledge and legal background, the general public has found it difficult to navigate automatic law consulting systems. To remedy this gap, this paper proposed an innovative method we proposed an evaluation with various combinations to choose proper k efficient statute prediction algorithms. This paper marks just the beginning of this research line.
Future works can improve upon the performance of the current study by using data or text mining techniques to increase future research topic. Finally, the two parameters k 1 and k be improved. To this end, our current solution approach must be extended so that it can decide the best values of k k based on the characteristics/features of the query before performing the current TPP method. Appendix A. An example to demonstrate TPP method
We use the following query as our example to present our TPP method. Suppose we have ten statues, and we set k works in this query.

Query:  X   X   X   X   X   X  KTV  X   X  X  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  a female KTV clerk named Luo, who secretly wrote down guests X  credit card numbers and authentication codes to conduct misappropriated online shopping).

From this query, we obtain five query terms, including  X   X  shopping), and  X   X  (misappropriated). Let t 1 denote  X   X  , t  X   X  .
 A.1. Phase 1: Select top k 1 statutes which is selected from the training judgments. Suppose there are ten judgment terms u
Section 3.1 , g i , j is the similarity between query term t query terms and judgment terms. Also, assume that the weights of query terms t
Section 3.1 . After transformation, the weights of judgment terms u 0.45 respectively.

Next, through the SVM classification model, the probabilities of ten statutes can be determined. Assume that the proba-bilities of 10 statutes S 1 , S 2, S 3 , S 4 , S 5 , S 6 , S 0.55, 0.5, 0.45, and 0.4 respectively.
 A.2. Phase 2: Select the top k 2 statutes
From Phase 1, in accordance with the probabilities, the Top 7 statutes X  sequence is S value between query vector and Top 7 statute vectors of S 0.600538, 0.251976, and 0.679921 respectively. Consequently, we found that the new order of statutes is S and S 6 .
 A.3. Phase 3: Select the final predicted statutes
Since k 2 is 5, the output statutes of Phase 2 include S 2 shown in Table A.3 , which presents the confidence of the associative statute rule s we can see that the sequence of the final Top 3 statutes is S References
