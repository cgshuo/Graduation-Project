 For many natural language processing (NLP) tasks, particularly those involving meaning, cre-ating labeled test data is difficult or expensive. One way to mitigate this problem is with pseudo-words, a method for automatically creating test corpora without human labeling, originally pro-posed for word sense disambiguation (Gale et al., 1992; Schutze, 1992). While pseudo-words are now less often used for word sense disambigation, they are a common way to evaluate selectional preferences , models that measure the strength of association between a predicate and its argument filler, e.g., that the noun lunch is a likely object of eat . Selectional preferences are useful for NLP tasks such as parsing and semantic role labeling (Zapirain et al., 2009). Since evaluating them in isolation is difficult without labeled data, pseudo-word evaluations can be an attractive evaluation framework.

Pseudo-word evaluations are currently used to evaluate a variety of language modeling tasks (Erk, 2007; Bergsma et al., 2008). However, evaluation design varies across research groups . This paper studies the evaluation itself, showing how choices can lead to overly optimistic results if the evaluation is not designed carefully. We show in this paper that current methods of apply-ing pseudo-words to selectional preferences vary greatly, and suggest improvements.

A pseudo-word is the concatenation of two words (e.g. house/car ). One word is the orig-inal in a document, and the second is the con-founder . Consider the following example of ap-plying pseudo-words to the selectional restrictions of the verb focus : In the original sentence, focus has two arguments: a subject story and an object campaign . In the test sentence, each argument of the verb is replaced by pseudo-words. A model is evaluated by its success at determining which of the two arguments is the original word.

Two problems exist in the current use of pseudo-words to evaluate selectional preferences. First, selectional preferences historically focus on subsets of data such as unseen words or words in certain frequency ranges. While work on unseen data is important, evaluating on the entire dataset provides an accurate picture of a model X  X  overall performance. Most other NLP tasks today evalu-ate all test examples in a corpus. We will show that seen arguments actually dominate newspaper articles, and thus propose creating test sets that in-clude all verb-argument examples to avoid artifi-cial evaluations.

Second, pseudo-word evaluations vary in how they choose confounders. Previous work has at-tempted to maintain a similar corpus frequency to the original, but it is not clear how best to do this, nor how it affects the task X  X  difficulty. We argue in favor of using nearest-neighbor frequen-cies and show how using random confounders pro-duces overly optimistic results.

Finally, we present a surprisingly simple base-line that outperforms the state-of-the-art and is far less memory and computationally intensive. It outperforms current similarity-based approaches by over 13% when the test set includes all of the data. We conclude with a suggested backoff model based on this baseline. Pseudo-words were introduced simultaneously by two papers studying statistical approaches to word sense disambiguation (WSD). Sch  X  utze (1992) simply called the words,  X  X rtificial ambiguous words X , but Gale et al. (1992) proposed the suc-cinct name, pseudo-word . Both papers cited the sparsity and difficulty of creating large labeled datasets as the motivation behind pseudo-words. Gale et al. selected unambiguous words from the corpus and paired them with random words from different thesaurus categories. Sch  X  utze paired his words with confounders that were  X  X omparable in frequency X  and  X  X istinct semantically X . Gale et al. X  X  pseudo-word term continues today, as does Sch  X  utze X  X  frequency approach to selecting the con-founder.

Pereira et al. (1993) soon followed with a selec-tional preference proposal that focused on a lan-guage model X  X  effectiveness on unseen data. The work studied clustering approaches to assist in similarity decisions, predicting which of two verbs was the correct predicate for a given noun object. One verb v was the original from the source doc-ument, and the other v 0 was randomly generated. This was the first use of such verb-noun pairs, as well as the first to test only on unseen pairs.
Several papers followed with differing methods of choosing a test pair ( v, n ) and its confounder v . Dagan et al. (1999) tested all unseen ( v, n ) occurrences of the most frequent 1000 verbs in his corpus. They then sorted verbs by corpus fre-quency and chose the neighboring verb v 0 of v as the confounder to ensure the closest frequency match possible. Rooth et al. (1999) tested 3000 random ( v, n ) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. They also chose confounders randomly so that the new pair was unseen .

Keller and Lapata (2003) specifically addressed the impact of unseen data by using the web to first  X  X ee X  the data. They evaluated unseen pseudo-words by attempting to first observe them in a larger corpus (the Web). One modeling difference was to disambiguate the nouns as selectional pref-erences instead of the verbs. Given a test pair ( v, n ) and its confounder ( v, n 0 ) , they used web searches such as  X  X  Det n X  to make the decision. Results beat or matched current results at the time. We present a similarly motivated, but new web-based approach later.

Very recent work with pseudo-words (Erk, 2007; Bergsma et al., 2008) further blurs the lines between what is included in training and test data, using frequency-based and semantic-based rea-sons for deciding what is included. We discuss this further in section 5.

As can be seen, there are two main factors when devising a pseudo-word evaluation for selectional preferences: (1) choosing ( v, n ) pairs from the test set, and (2) choosing the confounding n 0 (or v 0 ). The confounder has not been looked at in detail and as best we can tell, these factors have var-ied significantly. Many times the choices are well motivated based on the paper X  X  goals, but in other cases the motivation is unclear. Most NLP tasks evaluate their entire datasets, but as described above, most selectional preference evaluations have focused only on unseen data. This section investigates the extent of unseen ex-amples in a typical training/testing environment of newspaper articles. The results show that even with a small training size, seen examples dominate the data. We argue that, absent a system X  X  need for specialized performance on unseen data, a repre-sentative test set should include the dataset in its entirety. 3.1 Unseen Data Experiment We use the New York Times (NYT) and Associ-ated Press (APW) sections of the Gigaword Cor-pus (Graff, 2002), as well as the British National Corpus (BNC) (Burnard, 1995) for our analysis. Parsing and SRL evaluations often focus on news-paper articles and Gigaword is large enough to facilitate analysis over varying amounts of train-ing data. We parsed the data with the Stan-ford Parser 1 into dependency graphs. Let ( v d , n ) be a verb v with grammatical dependency d  X  { subject, object, prep } filled by noun n . Pairs ( v d , n ) are chosen by extracting every such depen-dency in the graphs, setting the head predicate as v and the head word of the dependent d as n . All prepositions are condensed into prep .

We randomly selected documents from the year 2001 in the NYT portion of the corpus as devel-opment and test sets. Training data for APW and NYT include all years 1994-2006 (minus NYT de-velopment and test documents). We also identified and removed duplicate documents 2 . The BNC in its entirety is also used for training as a single data point. We then record every seen ( v d , n ) pair dur-ing training that is seen two or more times 3 and then count the number of unseen pairs in the NYT development set (1455 tests).

Figure 1 plots the percentage of unseen argu-ments against training size when trained on either NYT or APW (the APW portion is smaller in total size, and the smaller BNC is provided for com-parison). The first point on each line (the high-est points) contains approximately the same num-ber of words as the BNC (100 million). Initially, about one third of the arguments are unseen, but that percentage quickly falls close to 10% as ad-ditional training is included. This suggests that an evaluation focusing only on unseen data is not rep-resentative, potentially missing up to 90% of the data. Figure 1: Percentage of NYT development set that is unseen when trained on varying amounts of data. The two lines represent training with NYT or APW data. The APW set is smaller in size from the NYT. The dotted line uses Google n-grams as training. The x-axis represents tokens  X  10 8 . Figure 2: Percentage of subject/object/preposition arguments in the NYT development set that is un-seen when trained on varying amounts of NYT data. The x-axis represents tokens  X  10 8 .
The third line across the bottom of the figure is the number of unseen pairs using Google n-gram data as proxy argument counts. Creating argu-ment counts from n-gram counts is described in detail below in section 5.2. We include these Web counts to illustrate how an openly available source of counts affects unseen arguments. Finally, fig-ure 2 compares which dependency types are seen the least in training. Prepositions have the largest unseen percentage, but not surprisingly, also make up less of the training examples overall.

In order to analyze why pairs are unseen, we an-alyzed the distribution of rare words across unseen and seen examples. To define rare nouns, we order head words by their individual corpus frequencies. A noun is rare if it occurs in the lowest 10% of the list. We similarly define rare verbs over their or-dered frequencies (we count verb lemmas, and do not include the syntactic relations). Corpus counts covered 2 years of the AP section, and we used the development set of the NYT section to extract the seen and unseen pairs. Figure 3 shows the per-centage of rare nouns and verbs that occur in un-seen and seen pairs. 24 . 6% of the verbs in un-seen pairs are rare, compared to only 4 . 5% in seen pairs. The distribution of rare nouns is less con-trastive: 13 . 3% vs 8 . 9% . This suggests that many unseen pairs are unseen mainly because they con-tain low-frequency verbs, rather than because of containing low-frequency argument heads.

Given the large amount of seen data, we be-lieve evaluations should include all data examples to best represent the corpus. We describe our full evaluation results and include a comparison of dif-ferent training sizes below. Given a test set S of pairs ( v d , n )  X  S , we now ad-dress how best to select a confounder n 0 . Work in WSD has shown that confounder choice can make the pseudo-disambiguation task significantly eas-ier. Gaustad (2001) showed that human-generated pseudo-words are more difficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically am-biguous, yet related concepts. Our approach eval-uates selectional preferences, not WSD, but our re-sults complement these findings.

We identified three methods of confounder se-lection based on varying levels of corpus fre-Figure 3: Comparison between seen and unseen tests (verb,relation,noun). 24 . 6% of unseen tests have rare verbs, compared to just 4 . 5% in seen tests. The rare nouns are more evenly distributed across the tests. quency: (1) choose a random noun, (2) choose a random noun from a frequency bucket similar to the original noun X  X  frequency, and (3) select the nearest neighbor , the noun with frequency clos-est to the original. These methods evaluate the range of choices used in previous work. Our ex-periments compare the three. 5.1 A New Baseline The analysis of unseen slots suggests a baseline that is surprisingly obvious, yet to our knowledge, has not yet been evaluated. Part of the reason is that early work in pseudo-word disambiguation explicitly tested only unseen pairs 4 . Our evalua-tion will include seen data, and since our analysis suggests that up to 90% is seen, a strong baseline should address this seen portion. We propose a conditional probability baseline:
P ( n | v d ) = where C ( v d , n ) is the number of times the head word n was seen as an argument to the pred-icate v , and C ( v d ,  X  ) is the number of times v d was seen with any argument. Given a test ( v d , n ) and its confounder ( v d , n P ( n | v d ) &gt; P ( n 0 | v d ) , and n 0 otherwise. If P ( n | v d ) = P ( n 0 | v d ) , randomly choose one.
Lapata et al. (1999) showed that corpus fre-quency and conditional probability correlate with human decisions of adjective-noun plausibility, and Dagan et al. (1999) appear to propose a very similar baseline for verb-noun selectional prefer-ences, but the paper evaluates unseen data, and so the conditional probability model is not studied.
We later analyze this baseline against a more complicated smoothing approach. 5.2 A Web Baseline If conditional probability is a reasonable baseline, better performance may just require more data. Keller and Lapata (2003) proposed using the web for this task, querying for specific phrases like  X  X erb Det N X  to find syntactic objects. Such a web corpus would be attractive, but we X  X  like to find subjects and prepositional objects as well as ob-jects, and also ideally we don X  X  want to limit our-selves to patterns. Since parsing the web is unre-alistic, a reasonable compromise is to make rough counts when pairs of words occur in close proxim-ity to each other.

Using the Google n-gram corpus, we recorded all verb-noun co-occurrences, defined by appear-ing in any order in the same n-gram, up to and including 5-grams. For instance, the test pair ( throw subject , ball ) is considered seen if there ex-ists an n-gram such that throw and ball are both included. We count all such occurrences for all verb-noun pairs. We also avoided over-counting co-occurrences in lower order n-grams that appear again in 4 or 5-grams. This crude method of count-ing has obvious drawbacks. Subjects are not dis-tinguished from objects and nouns may not be ac-tual arguments of the verb. However, it is a simple baseline to implement with these freely available counts.

Thus, we use conditional probability as de-fined in the previous section, but define the count C ( v d , n ) as the number of times v and n (ignoring d ) appear in the same n-gram. 5.3 Smoothing Model We implemented the current state-of-the-art smoothing model of Erk (2007). The model is based on the idea that the arguments of a particular verb slot tend to be similar to each other. Given two potential arguments for a verb, the correct one should correlate higher with the arguments ob-served with the verb during training.

Formally, given a verb v and a grammatical de-pendency d , the score for a noun n is defined: where sim ( n, w ) is a noun-noun similarity score, Seen ( v d ) is the set of seen head words filling the slot v d during training, and C ( v d , n ) is the num-ber of times the noun n was seen filling the slot v d The similarity score sim ( n, w ) can thus be one of many vector-based similarity metrics 5 . We eval-uate both Jaccard and Cosine similarity scores in this paper, but the difference between the two is small. Our training data is the NYT section of the Gi-gaword Corpus, parsed into dependency graphs. We extract all ( v d , n ) pairs from the graph, as de-scribed in section 3. We randomly chose 9 docu-ments from the year 2001 for a development set, and 41 documents for testing. The test set con-sisted of 6767 ( v d , n ) pairs. All verbs and nouns are stemmed, and the development and test docu-ments were isolated from training. 6.1 Varying Training Size We repeated the experiments with three different training sizes to analyze the effect data size has on performance:  X  Train x1 : Year 2001 of the NYT portion of  X  Train x2 : Years 2001 and 2002 of the NYT  X  Train x10 : The entire NYT portion of Giga-6.2 Varying the Confounder We generated three different confounder sets based on word corpus frequency from the 41 test documents. Frequency was determined by count-ing all tokens with noun POS tags. As motivated in section 4, we use the following approaches:  X  Random : choose a random confounder from  X  Buckets : all nouns are bucketed based on  X  Neighbor : sort all seen nouns by frequency 6.3 Model Implementation None of the models can make a decision if they identically score both potential arguments (most often true when both arguments were not seen with the verb in training). As a result, we extend all models to randomly guess ( 50% performance) on pairs they cannot answer.

The conditional probability is reported as Base-line . For the web baseline (reported as Google ), we stemmed all words in the Google n-grams and counted every verb v and noun n that appear in Gigaword. Given two nouns, the noun with the higher co-occurrence count with the verb is cho-sen. As with the other models, if the two nouns have the same counts, it randomly guesses.

The smoothing model is named Erk in the re-sults with both Jaccard and Cosine as the simi-larity metric. Due to the large vector representa-tions of the nouns, it is computationally wise to trim their vectors, but also important to do so for best performance. A noun X  X  representative vector consists of verb slots and the number of times the noun was seen in each slot. We removed any verb slot not seen more than x times, where x varied based on all three factors: the dataset, confounder choice, and similarity metric. We optimized x on the development data with a linear search, and used that cutoff on each test. Finally, we trimmed any vectors over 2000 in size to reduce the com-putational complexity. Removing this strict cutoff appears to have little effect on the results. Finally, we report backoff scores for Google and Erk. These consist of always choosing the Base-line if it returns an answer (not a guessed unseen answer), and then backing off to the Google/Erk result for Baseline unknowns. These are labeled Backoff Google and Backoff Erk . Results are given for the two dimensions: con-founder choice and training size . Statistical sig-nificance tests were calculated using the approx-imate randomization test (Yeh, 2000) with 1000 iterations.

Figure 4 shows the performance change over the different confounder methods. Train x2 was used for training. Each model follows the same pro-gression: it performs extremely well on the ran-dom test set, worse on buckets, and the lowest on the nearest neighbor. The conditional probability Baseline falls from 91 . 5 to 79 . 5 , a 12% absolute drop from completely random to neighboring fre-quency. The Erk smoothing model falls 27% from 93 . 9 to 68 . 1 . The Google model generally per-forms the worst on all sets, but its 74 . 3% perfor-mance with random confounders is significantly better than a 50-50 random choice. This is no-table since the Google model only requires n-gram counts to implement. The Backoff Erk model is the best, using the Baseline for the majority of decisions and backing off to the Erk smoothing model when the Baseline cannot answer.

Figure 5 (shown on the next page) varies the training size. We show results for both Bucket Fre-quencies and Neighbor Frequencies. The only dif-ference between columns is the amount of training data. As expected, the Baseline improves as the training size is increased. The Erk model, some-what surprisingly, shows no continual gain with more training data. The Jaccard and Cosine simi-Baseline 91.5 89.1 79.5 Erk-Jaccard 93.9 * 82.7* 68.1* Erk-Cosine 91.2 81.8* 65.3* Google 74.3 * 70.4* 59.4* Backoff Erk 96.6 * 91.8* 80.8* Backoff Goog 92.7  X  89.7 79.8 Figure 4: Trained on two years of NYT data (Train x2). Accuracy of the models on the same NYT test documents, but with three different ways of choos-ing the confounders. * indicates statistical signifi-cance with the column X  X  Baseline at the p &lt; 0 . 01 level,  X  at p &lt; 0 . 05 . Random is overly optimistic, reporting performance far above more conserva-tive (selective) confounder choices.
 Precision 96.1 95.5* 95.0  X  Accuracy 78.2 82.0* 88.1* Accuracy +50% 87.5 89.1* 91.7* Figure 6: Results from the buckets confounder test set. Baseline precision, accuracy (the same as re-call), and accuracy when you randomly guess the tests that Baseline does not answer. All numbers are statistically significant * with p-value &lt; 0 . 01 from the number to their left. larity scores perform similarly in their model. The Baseline achieves the highest accuracies ( 91 . 7% and 81 . 2% ) with Train x10 , outperforming the best Erk model by 5 . 2% and 13 . 1% absolute on buck-ets and nearest neighbor respectively. The back-off models improve the baseline by just under 1% . The Google n-gram backoff model is almost as good as backing off to the Erk smoothing model.
Finally, figure 6 shows the Baseline X  X  precision and overall accuracy. Accuracy is the same as recall when the model does not guess between pseudo words that have the same conditional prob-abilities. Accuracy +50% (the full Baseline in all other figures) shows the gain from randomly choosing one of the two words when uncertain. Precision is extremely high. Confounder Choice : Performance is strongly in-fluenced by the method used when choosing con-founders. This is consistent with findings for WSD that corpus frequency choices alter the task (Gaustad, 2001; Nakov and Hearst, 2003). Our results show the gradation of performance as one moves across the spectrum from completely ran-dom to closest in frequency. The Erk model dropped 27% , Google 15% , and our baseline 12% . The overly optimistic performance on random data suggests using the nearest neighbor approach for experiments. Nearest neighbor avoids evaluating on  X  X asy X  datasets, and our baseline (at 79 . 5% ) still provides room for improvement. But perhaps just as important, the nearest neighbor approach facilitates the most reproducibile results in exper-iments since there is little ambiguity in how the confounder is selected.

Realistic Confounders : Despite its over-optimism, the random approach to confounder se-lection may be the correct approach in some cir-cumstances. For some tasks that need selectional preferences, random confounders may be more re-alistic. It X  X  possible, for example, that the options in a PP-attachment task might be distributed more like the random rather than nearest neighbor mod-els. In any case, this is difficult to decide without a specific application in mind. Absent such spe-cific motiviation, a nearest neighbor approach is the most conservative, and has the advantage of creating a reproducible experiment, whereas ran-dom choice can vary across design.

Training Size : Training data improves the con-ditional probability baseline, but does not help the smoothing model. Figure 5 shows a lack of im-provement across training sizes for both jaccard and cosine implementations of the Erk model. The Train x1 size is approximately the same size used in Erk (2007), although on a different corpus. We optimized argument cutoffs for each training size, but the model still appears to suffer from addi-tional noise that the conditional probability base-line does not. This may suggest that observing a test argument with a verb in training is more re-liable than a smoothing model that compares all training arguments against that test example.
High Precision Baseline : Our conditional probability baseline is very precise. It outper-forms the smoothed similarity based Erk model and gives high results across tests. The only com-bination when Erk is better is when the training data includes just one year (one twelfth of the NYT section) and the confounder is chosen com-pletely randomly. These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. Our reported results include ev-ery ( v d , n ) in the data, not a subset of particu-lar semantic roles. Our reported 93 . 9% for Erk-Jaccard is also significantly higher than their re-ported 81 . 4% , but this could be due to the random choices we made for confounders, or most likely corpus differences between Gigaword and the sub-set of FrameNet they evaluated.

Ultimately we have found that complex models for selectional preferences may not be necessary, depending on the task. The higher computational needs of smoothing approaches are best for back-ing off when unseen data is encountered. Condi-tional probability is the best choice for seen exam-ples. Further, analysis of the data shows that as more training data is made available, the seen ex-amples make up a much larger portion of the test data. Conditional probability is thus a very strong starting point if selectional preferences are an in-ternal piece to a larger application, such as seman-tic role labeling or parsing.

Perhaps most important, these results illustrate the disparity in performance that can come about when designing a pseudo-word disambiguation evaluation. It is crucially important to be clear during evaluations about how the confounder was generated. We suggest the approach of sorting nouns by frequency and using a neighbor as the confounder. This will also help avoid evaluations that produce overly optimistic results. Current performance on various natural language tasks is being judged and published based on pseudo-word evaluations. It is thus important to have a clear understanding of the evaluation X  X  characteristics. We have shown that the evalu-ation is strongly affected by confounder choice, suggesting a nearest frequency neighbor approach to provide the most reproducible performance and avoid overly optimistic results. We have shown that evaluating entire documents instead of sub-sets of the data produces vastly different results. We presented a conditional probability baseline that is both novel to the pseudo-word disambigua-tion task and strongly outperforms state-of-the-art models on entire documents. We hope this pro-vides a new reference point to the pseudo-word disambiguation task, and enables selectional pref-erence models whose performance on the task similarly transfers to larger NLP applications. This work was supported by the National Science Foundation IIS-0811974, and the Air Force Re-search Laboratory (AFRL) under prime contract no. FA8750-09-C-0181. Any opinions, ndings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reect the view of the AFRL. Thanks to Sebastian Pad  X  o, the Stanford NLP Group, and the anonymous reviewers for very helpful sugges-tions.
