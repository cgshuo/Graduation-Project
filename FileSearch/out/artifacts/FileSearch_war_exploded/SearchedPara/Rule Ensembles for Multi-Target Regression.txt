
The most commonly addressed problem in machine learn-ing is that of how to predict the value of a single target attribute or class. There exist, however, many real life problems where we would like to predict multiple target attributes at once, instead of only a single one. A typical example from the environmental sciences would be the task of predicting species communities [1]. Here we are interested in the abundances of a set of different species living in the same environment. These species represent the target attributes, which might, but need not be related. If our only goal is to achieve high predictive accuracy, a collection of single-target models should be sufficient to solve the problem. However, if we are also interested in interpretability, the collection of single-target models is much more complex and harder to interpret than a single model that jointly predicts all target attributes [2] X  X 4]. An additional benefit of the multi-target models is that they are less likely to overfit the data and are frequently more accurate than the corresponding collections of single-target models [2], [3], [5], [6].

Besides decision trees, rule sets are one of the most expressive and human readable model representations, and are frequently used when an interpretable model is desired. The majority of rule learning methods, including existing methods for learning multi-target rules, are based on the sequential covering algorithm [7], originally designed for learning ordered rule lists for binary classification domains. Unfortunately, on both single-and multi-target regression problems the accuracy of rule sets that are learned with this approach is considerably worse than that of other regression methods, i.e., regression trees (cf. [8] for an empirical comparison). An alternative rule learning method that per-forms well also on (single-target) regression problems is the approach of rule ensembles as implemented in the RuleFit method [9]. The method starts with creating an ensemble of decision trees, which are considered as an initial collection of rules. An optimization procedure is then used with the purpose of finding an optimal weight for each of these rules. During the optimization, one tries to assign as many weights as possible to zero in order to learn small and interpretable models, while not compromising their accuracy. In this paper we adopt this approach for learning multi-target regression rule ensembles, and propose an algorithm that can learn unordered rule sets that are both accurate and interpretable.
The paper is organized as follows. In Section 2, we briefly present the related work on multi-target prediction, rule learning, and rule ensembles. The newly proposed algorithm for learning multi-target regression rule ensem-bles is presented in Section 3. In Section 4, we describe the experimental evaluation setting, and in Section 5 the experimental results. The last section concludes and gives some directions for further research.

The algorithm presented in this paper is related to several existing approaches for multi-target prediction. The multi-target prediction task is defined as follows. We are given a set of learning examples E of the form ( x , y ) , where x = ( x 1 ,x 2 ,...,x K ) is a vector of K descriptive attributes and y = ( y 1 ,y 2 ,...,y T ) is a vector of T target attributes. Our task is to learn a model that, given a new unlabeled example x , can predict the values of all target attributes y simultaneously. Several standard learning methods such as neural networks, decision trees, classification rules and random forests have already been extended towards multi-target prediction [3] X  X 6], [10].

Since our method learns regression rules, it is highly related to rule learning [11]. A method for learning multi-target rules already exists [4], [8]. It employs the standard covering approach and can learn ordered or unordered rule sets for classification or regression domains. Its accuracy on classification domains is comparable to other classification methods like (multi-target) decision trees. However, on re-gression domains the accuracy is significantly worse.

An alternative approach to rule learning are so-called rule ensembles [9], [12]. 1 In this paper we adopt rule ensembles as implemented in the RuleFit method. This method starts by generating a set of decision trees in much the same way as ensembles are generated by methods like bagging [13] or random forests [14]. Because such large ensembles are hard or impossible to interpret, all the trees are transcribed into a collection of rules, and an optimization procedure is used to select a small subset of rules and to determine their weights. As a result, we get a relatively small set of weighted rules. The final prediction for a given example is obtained by taking a weighted vote of all the rules that apply (i.e., cover the example). The resulting model can thus be written as:  X  y = f ( x ) = w 0 + P M i =1 w i r i ( x ) , where w 0 is the baseline prediction, and the sum is the correction value obtained from M rules. The rules r i are functions, which have a value of 1 for all examples that they cover, and 0 otherwise. During the learning phase, all the weights w i are optimized by a gradient directed optimization algorithm.
The method presented in this paper is a generalization of RuleFit towards multi-target regression problems, and we describe it in detail in the next section.

Our algorithm for learning rule ensembles for multi-target regression problems (which we call F IRE  X  Fi tted r ule e nsembles) is based on the RuleFit method [9]; its outline is presented in Fig. 1. We start by generating a set of diverse regression trees, which we convert to rules. We then optimize the weights of rules with a gradient directed optimization algorithm. This optimization procedure depends on the gra-dient threshold parameter  X  , and we repeat the optimization for different values of  X  in order to find a set of weights with the smallest error. In the end we remove all the rules whose weights are zero.

The resulting rule ensemble is a vector function f ; given an unlabeled example x it predicts the values of all target Input: learning examples E
Output: rules R with their weights W 1: T  X  GenerateSetOfTrees( E ) 2: R  X  ConvertTreesToRules( T ) 4: for  X  = 0 . 0 to 1 . 0 with step do 5: ( W  X  , ERR  X  )  X  OptimizeWeights( R,E, X  ) 6: if ERR  X  &lt; ERR min then 8: end if 9: end for 10: ( R,W )  X  RemoveZeroWeightedRules( R,W opt ) 11: return ( R,W ) attributes: The first part ( avg ) is a constant vector with the averages over each of the targets. The sum is the contribution of M rules: each rule r i is a vector function that gives a constant prediction, if it covers the example x , or returns zero otherwise. All the weights w are being determined during the optimization phase and our goal is to have as many weights equal to zero as possible.

Example. Let the problem domain have eight descriptive attributes x = ( x 1 ,...,x 8 ) and three target attributes y = ( y 1 ,y 2 ,y 3 ) . A hypothetic rule ensemble, comprising a constant vector and two rules, that predicts all the target values of this domain simultaneously could be:
So far, we have only briefly mentioned two important aspects of our algorithm, i.e., the generation of the initial collection of trees and rules and the weight optimization procedure. We will now describe them in more detail. The basic decision tree learning method that is used within the GenerateSetOfTrees procedure of algorithm F IRE (Fig. 1) is the predictive clustering tree learning method [10] that can learn multi-target regression trees. A set of diverse trees is generated with the multi-target implementation of the random forest ensemble method [6], but in order to increase the tree variance, the maximum tree depth is limited to a random value chosen according to a probability distribution proposed by [9], and used in the original RuleFit method. The average depth of trees is specified as a parameter to the algorithm. It should be emphasized here that this parameter only limits the average depth of generated trees, trees with larger depth can still be generated.

All regression trees generated are transcribed into rules within the ConvertTreesToRules procedure. Each leaf of each tree is converted to a rule and its predictions r t are normalized in the following way. The predicted value r 0 t of each target attribute t is changed to r t = r 0 t /r 0 m , where r m is the maximum absolute value of any predicted target attribute value by this rule: m = arg max i | r 0 i | . Thus for the normalized rules it holds that | r t |  X  1 and r m = 1 . Such a normalization roughly equalizes the rules before the optimization phase.

The last part of the algorithm that deserves a detailed description is the optimization within the OptimizeWeights procedure of algorithm F IRE (Fig. 1) that determines optimal weights of the rules. For this purpose, the RuleFit method uses a gradient directed optimization method [15] and a squared loss function L t : which is applicable to a single target attribute only; here f ( x ) is the predicted value and y t is the true value. If we want to use the above optimization algorithm for multi-target problems, we have to define a loss function that is convex. A simple solution is to take the above squared loss function for each of the T target attributes and aggregate them by taking their average: Such an aggregated loss function is convex and enables efficient computation of gradients.

To equalize the contributions of different targets to the aggregated loss function, we normalize the target values by shifting their mean to zero and dividing them by 2  X  t , where  X  t is the standard deviation of a given target attribute. Assuming a normal distribution, this should put 95% of all values within the [  X  1 , 1] interval.

In this kind of optimization, we usually add to the loss function a regularization part of the form P M i =1 | w i |  X  to keep the weights smaller or zero and add stability to the optimization procedure. Popular values for  X  include  X  = 2 (L2 or ridge) and  X  = 1 (L1 or lasso). For gradient directed optimization, a very similar effect to this kind of regularization can also be achieved in a different and more efficient way [15]. Instead of adding the regularization term to the loss function, we can explicitly control the number of weights that are changed during every optimization iteration in the following way. Let M be the number of weights that we are optimizing with a gradient method. Instead of
Input: rules R , learning examples E and gradient treshold
Output: weights W and an error estimate ERR 1: W 0 = { 0 , 0 ,..., 0 } 2: ( E t ,E v )  X  SplitSet( E ) { Training and validation } 3: for i = 0 to Maximum number of iterations do 4: do every 100 iterations 5: if Error( E v ,R,W i ) increased 6: W i  X  WeightsWithSmallestError( E v ,R ) 7: ReduceStepSize() 8: end if 9: end do 10: G  X  ComputeGradients( E t ,R,W i ) 11: if Limit of allowed nonzero weights is reached then 12: G  X  X  g k  X  G | w k  X  W i : w k 6 = 0 } 13: end if 14: G max  X  X  g j  X  G | | g j | X   X  max k | g k |} 15: W i +1  X  ChangeWeightsWithStep( G max ,W i ) 16: end for 17: W  X  WeightsWithSmallestError( E v ,R ) 18: ERR  X  Error( E v ,R,W ) 19: return ( W, ERR ) allowing changes to all the weights simultaneously, we only allow changes to the weights w j whose gradients g j have a value above a threshold With  X  = 0 , we are changing all the weights during every iteration, resulting in a behavior similar to ridge regulariza-tion. On the other side, if  X  = 1 , only one gradient during every iteration is modified and the behavior is similar to lasso regularization. In our case, lasso regularization seems better, because it has been shown to lead to many weights being set to zero [16], which means simpler and more interpretable models with fewer rules. However, in practice it is hard to predict which  X  value will result in the most accurate model. We overcome this problem by trying a set of different  X  values (Fig. 1, line 5) and estimating their accuracy on a separate validation set (the validation set is the same for all  X  ). In the end, the model with the smallest validation error is selected.
 The complete optimization algorithm is presented in Fig. 2. We start with all the weights set to zero, and splitting the example set E into a training set E t and a validation set E v . The main idea is to repeat computing gradients g k for each of the weights (line 8) and then changing the selected weights w j into the most promising direction for a predefined step size (lines 12 X 13).

In addition to this base idea, there are some important details. First, on every 100-th iteration we check if we are overfitting (line 4), i.e., if the validation error starts to increase. In case the validation error increased, we do not stop the optimization (as the algorithm by [15] does), but return to the iteration with the lowest validation error and continue with a smaller gradient step size, e.g., the step size can be multiplied by 0.1 (lines 5 X 6). Second, we can define a number of nonzero weights in advance (lines 9 X 11), which makes a suitable parameter for setting the accuracy vs. simplicity trade-off. An extensive experimental evaluation of the algorithm X  X  performance is presented in the next section. In the experimental evaluation we investigate three issues. First, we compare the accuracy and model size of F IRE to regression trees [10] and random forests [14] on single-target regression data sets in order to show that our implementation is also applicable to standard regression problems. Second, we compare F IRE to the same methods on multi-target regression data sets; this is the key part of the evaluation. As described in Section III, our algorithm has a parameter that can be used for limiting the total number of nonzero weights, i.e., the number of rules. All the above experiments were done with two values of this parameter: with an arbitrary limit of 20 rules, and without any limitation on the size. If we limit the maximum size of the models we of course get less accurate models. An investigation of this issue is the focus of the third part of the experimental evaluation.
The regression trees [10] and random forests [6] used in our experiments, as well as our F IRE algorithm, are implementedin the C LUS predictive clustering framework [17]. 2 All the parameters for regression trees and random forests were set to their default values. Random forests used 100 trees. Where possible, the parameters of F IRE were set to the values used in [9]. When generating the initial set of trees (GenerateSetOfTrees procedure) we used 100 random trees with an average depth of 3. The optimization procedure (Fig. 1, line 5) was run with gradient threshold parameter  X  values ranging from 0 to 1 in 0.1 increments. In the OptimizeWeights procedure (Fig. 2), the initial learning set E was split into 2 / 3 for training ( E t ) and 1 / 3 for validation ( E v ). The maximum number of optimization iterations was 10,000. The threshold for detecting error increase (line 5) was 1 . 1 , the starting step size for changing the weights was 10 , but it was automatically multiplied by the factor 0 . 1 (i.e., reduced) if overfitting occured (Fig. 2, line 6).

The data sets used in the experiments, together with their properties and references, are presented in Table I. Fifteen single-target regression data sets are taken from standard ML data repositories. Publicly available multi-target data sets, however, are scarce; in addition to one public one, we collected ten previously analyzed data sets for which we provide references. Due to time limitations we have restricted the number of examples in each data set; for data sets with more than 2,000 examples, we used random subsample of size 2,000 and ignored the rest.

The accuracy of the learned regression models is esti-mated for each target attribute by the relative root mean squared error (RRMSE). The size of regression trees and random forests is measured as the number of tree leaves in all the trees. The size of F IRE models is measured as the number of rules. All the above measures are estimated with 10-fold cross-validation, where the folds for each data set are the same for all the algorithms.

To test whether any of the observed differences between the algorithms are significant, we followed the methodology suggested by [27]: first we use the Friedman test to check if there are any statistically significant differences between the compared algorithms. If the answer is positive, we additionally use the Nemenyi post-hoc test to figure out what these differences are, and we present them on the average ranks diagrams. These diagrams show all the compared algorithms in the order of their average ranks; the best are on the right and the worst are on the left side of the diagram. The algorithms that differ by less than a critical distance for a p-value = 0 . 05 are connected with a horizontal bar, and are not significantly different. We perform such significance testing for RRMSE and for model size.

However, when testing the differences in RRMSE for multi-target data we have two possibilities. We can either treat each of the target attributes of all data sets as an independent measurement, or we can compute the average over all targets within each data set and consider such av-erages as independent measurements. The argument against the first option is that target attributes within one data set are probably not independent and as a result our test will show more significant differences than there actually are. The argument against the second option is that when computing averages across all target attributes within a data set, we are actually summing apples and oranges, and the resulting average is probably not a valid quantity. In the absence of a better solution, we present tests of RRMSE differences for both options. The results of the experimental evaluation are presented in the next section.

As already mentioned in the previous section, we per-formed three groups of experiments. We evaluated our implementation first on single-target and then on multi-target regression data sets. The latter is the most important part, since it shows whether our generalization of rule ensembles towards multi-target regression is successful. Finally, we investigated the influence of rule ensemble size on accuracy. A. Single-Target Regression
On single-target data sets we have compared regression trees, random forests and two versions of F IRE : one without any limitation on the model size, and one with the maximum number of rules set to 20. The average RRMSEs over all 15 data sets are 0.74, 0.66, 0.67, and 0.71, respectively. The corresponding average models sizes are 26.9, 36,475, 409, and 19.5. We have omitted the detailed results due to space limitations. The Friedman test shows that the RRMSEs are statistically different with a p-value =2 . 4  X  10  X  6 and model sizes with a p-value =7 . 6  X  10  X  9 . The average ranks for all four algorithms together with the results of the Nemenyi test are given in Fig. 3, separately for RRMSE and model size. The better algorithms are the ones with higher ranks (with 1 being the highest rank) and are placed on the right-hand side of the diagram. Algorithms whose ranks differ by less than a critical distance (CD) are not significantly different with a p-value =0 . 05 .

From the RRMSE diagram (Fig. 3a) we can see that random forests are the most accurate method, followed by the unlimited and the limited versions of F IRE and regression trees. However, for the adjoining algorithms, the difference is not statistically significant. By limiting the number of F IRE rules we therefore still get reasonably accurate models. The diagram for model size (Fig. 3b) shows that regression trees and the size limited F IRE both generate significantly smaller models than the unlimited F IRE and random forests. While the unlimited version of F IRE generates smaller models than random forests, the difference is below the significance threshold. We argue that these results show that our implementation of rule ensembles performs well also on single-target regression problems.
 B. Multi-Target Regression
The detailed results of the algorithm comparison on multi-target regression data are presented in Table II. Random forests generate the most accurate models on 8, the unlimited version of F IRE on 4 and regression trees on 3 data sets (on three data sets multiple methods perform equally well). However, the differences in model sizes are very large.
The Friedman test shows that the RRMSE values of algo-rithms are significantly different with a p-value &lt; 2 . 2  X  10  X  16 , if we treat each target separately, and with a p-value = 4 . 1  X  10  X  3 , if we compare target averages over each data set. The model sizes are different with a p-value =1 . 6  X  10  X  6 . The average ranks and results of the Nemenyi test are given in Fig. 4 for RRMSE evaluated on separate targets (a), RRMSE evaluated on target averages within data sets (b), and model size (c).

Looking at diagram (a), the ranking of algorithms is similar as in the case of single-target data sets: random forests and the unlimited F IRE are more accurate than the limited F IRE and regression trees. However, due to the smaller critical distance, all differences are now significant. Evaluation over target averages within data sets (b) shows a similar picture, but because the sample size is smaller (11 data sets vs. 63 targets), the critical distance is larger and differences are less significant: only random forests are significantly better than regression trees and the limited version of F IRE . The diagram for size (c) is very similar as in the single-target case: the limited F IRE and regression trees are significantly smaller than the unlimited F IRE and especially than random forests. While the difference in size between random forests and the unlimited F IRE is not significant, the average size of a random forest is almost 65 times larger than the average size of a F IRE model (Table II). However, the difference in average accuracy is small.
 C. Model Size Limitation for F IRE
Experiments presented in the previous two subsections included two versions of the F IRE algorithm, one with the maximum model size set to 20, and one without any model size restrictions. In this subsection, we present experiments with different values of the maximum model size parameter, which show how this model size limit influences the accu-racy of models. We use values of 10, 20, 30, 40, 50, and  X  . Due to space limitations, we omit the detailed results and only present the average ranks diagrams in Fig. 5. Diagram (a) shows the results on single-target data. While all the differences in RRMSE are not significant, it is clear that increasing the model size improves the accuracy. Diagrams (b) and (c) show RRMSE on multi-target data for per-target evaluation and for per-data set target average evaluation, respectively. Because of a larger sample, there are more significant differences in (b) than in (c), however, what is common to both diagrams is the trend that models with less terms are also less accurate. The size limitation parameter can therefore be used as an accuracy for simplicity (and interpretability) trade-off setting.

Another interesting conclusion that we can draw from these diagrams is that while a model size of 20 seems enough to get models that are not significantly less accurate than the unlimited models for single-target domains, this is not the case for multi-target domains. Here, at least 50 rules are needed for an accuracy that is not significantly worse than that of the full model. Of course, the exact values depend on the domain, the number of target attributes, and relations between them. But it could be expected that the task of modeling a multi-target domain is harder than the task of modeling a single-target domain, and therefore the corresponding models have to be more complex.

In many application areas there is a need for methods that can learn interpretable multi-target models, i.e., models that predict several target attributes simultaneously. Rules are arguably one of the most interpretable model types, and we presented a method F IRE that can learn multi-target regression rule ensembles. We have adopted a rule ensembles approach and generalized it to multi-target regres-sion domains. Our implementation has a simple parameter for limiting the number of rules in the learned model. This enables us to trade accuracy for size (interpretability) of learned models. We evaluated our algorithm with two parameter values: one that limits the number of rules to maximum 20, and one that has no size restrictions, and compared it to regression trees and random forests. We also investigated how the size limit affects the accuracy.

First, we evaluated F IRE on single-target domains in order to show that our implementation of rule ensembles also works on standard regression problems. The results show that, depending on the number of rules limit, the accuracy of F IRE is between the accuracies of regression trees and random forests. However, regression trees and both versions of F IRE generate smaller models than random forests. This suggests that F IRE can offer accuracy comparable to that of random forests, which are much larger.

Second and most important, we have evaluated F IRE on multi-target domains. The results are similar to the ones on single-target domains. Random forests and the unlimited F
IRE are more accurate than the limited F IRE and regression trees. Again, model size of regression trees and the limited F IRE is significantly smaller than model size of the unlimited F
IRE and random forests. Even though the difference in size between random forests and the unlimited F IRE is not significant, the average size of a random forest is almost 65 times larger than the average size of a F IRE model. Although the unlimited F IRE tends to generate somewhat less accurate models than random forest, these models are much smaller than random forests. Therefore, we believe the unlimited F
IRE is a good choice for modeling multi-target regression problems.

Finally, the investigation of the influence of the maximum model size on the accuracy confirms that this parameter can be successfully used as an accuracy for simplicity trade-off setting. The results show the general trend of larger models having better accuracy. The fact that the trend is more evident in the multi-target domains can be attributed to multi-target tasks being more complex and demanding more complex models for optimal accuracy.

Let us conclude with some ideas for further work. The original RuleFit method can also generate models that con-sist not only of rules, but also of simple linear functions (linear terms). We believe that the additions of linear terms could further improve the accuracy of our method. The experimental evaluation could also be extended by a compar-ison to existing multi-target regression rules, i.e., predictive clustering rules [8]. The optimal model size depends on the domain at hand. We believe it should be possible to auto-matically determine the optimal model size, which would be a compromise between accuracy and interpretability.

