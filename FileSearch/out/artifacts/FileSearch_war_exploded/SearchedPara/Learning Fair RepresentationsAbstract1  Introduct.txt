 Richard Zemel zemel@cs.toronto.edu Yu (Ledell) Wu wuyu@cs.toronto.edu Kevin Swersky kswersky@cs.toronto.edu Toniann Pitassi toni@cs.toronto.edu Cynthia Dwork dwork@microsoft.com Microsoft Research, 1065 La Avenida Mountain View, CA. 94043 USA Information systems are becoming increasingly reliant on statistical inference and learning to render all sorts of decisions, including the setting of insurance rates, the allocation of police, the targeting of advertising, the issuing of bank loans, the provision of health care, and the admission of students. This growing use of automated decision-making has sparked heated debate among philosophers, policy-makers, and lawyers. Crit-ics have voiced concerns with bias and discrimination in decision systems that rely on statistical inference and learning.
 Systems trained to make decisions based on historical data will naturally inherit the past biases. These may be ameliorated by attempting to make the automated decision-maker blind to some attributes. This how-ever, is difficult, as many attributes may be correlated with the protected one. The basic aim then is to make fair decisions, i.e., ones that are not unduly biased for or against protected subgroups in the population. Two important goals of fair classification that have been articulated are: group fairness, and individual fairness. Group fairness, also known as statistical par-ity, ensures that the overall proportion of members in a protected group receiving positive (negative) classifica-tion are identical to the proportion of the population as a whole. While statistical parity is an important prop-erty, it may still lead to undesirable outcomes that are blatantly unfair to individuals, such as discriminat-ing in employment while maintaining statistical parity among candidates interviewed by deliberately choos-ing unqualified members of the protected group to be interviewed in the expectation that they will fail. Indi-vidual fairness addresses this by ensuring that any two individuals who are similar with respect to a particular task should be classified similarly.
 Only recently have machine learning researchers con-sidered this issue. Several papers, e.g., (Luong et al., 2011; Kamishima et al., 2011), aim to achieve the first goal, group fairness, by adapting standard learning ap-proaches in novel ways, primarily through a form of fairness regularizer, or by re-labeling the training data to achieve statistical parity. In a different line of work, (Dwork et al., 2011) develop an ambitious framework which attempts to achieve both group and individ-ual fairness. In their setup, the goal is to define a probabilistic mapping from individuals to an interme-diate representation such that the mapping achieves both. This construction allows the initial mapping, perhaps supervised by an impartial party or regulator concerned with fairness, to produce representations of individuals that can then be used in the second step by multiple vendors to craft classifiers to maximize their own objectives, while maintaining fairness. However, there are several obstacles in their approach. First, a distance metric that defines the similarity between the individuals is assumed to be given. This may be unrealistic in certain settings, and to some extent the problem of establishing fairness in classification (more specifically simultaneously achieving the twin goals) is reduced to the problem of establishing a fair dis-tance function. This was the most challenging aspect of their framework, as was acknowledged in their pa-per. Secondly, their framework is not formulated as a learning problem, as it forms a mapping for a given set of individuals without any procedure for generalizing to novel unseen data.
 Our work builds on this earlier framework in that we try to achieve both group and individual fairness. However, we extend their approach in several impor-tant ways. First, we develop a learning approach to solving the fairness problem. Secondly we learn a re-stricted form of a distance function as well as the in-termediate representation, thus making a step toward eliminating the assumption that the distance function is given apriori. Thirdly, we explicitly formulate the problem in a novel way that we feel deserves further study. Namely, we formulate fairness as an optimiza-tion problem of finding an intermediate representation of the data that best encodes the data (i.e., preserving as much information about the individual X  X  attributes as possible), while simultaneously obfuscates aspects of it, removing any information about membership with respect to the protected subgroup. That is, we at-tempt to learn a set of intermediate representations to satisfy two competing goals: (i) the intermediate rep-resentation should encode the data as well as possible; and (ii) the the encoded representation is sanitized in the sense that it should be blind to whether or not the individual is from the protected group. We fur-ther posit that such an intermediate representation is fundamental to progress in fairness in classification, since it is composable and not ad hoc; once such a representation is established, it can be used in a black-box fashion to turn any classification algorithm into a fair classifier, by simply applying the classifer to the sanitized representation of the data.
 The remainder of the paper is organized as follows. First we introduce our model formulation, and de-scribe how we use it to learn fair representations. Sec-tion 3 reviews relevant work, and Section 4 presents experimental results on some standard datasets, com-paring our model to some earlier ones with respect to the fairness and accuracy of the classifications. 2.1. Overview and notation The main idea in our model is to map each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. The aim of this new representation is to lose any infor-mation that can identify whether the person belongs to the protected subgroup, while retaining as much other information as possible . Here we formulate this new representation in terms of a probabilistic mapping to a set of prototypes; note, however, that this is only one of many possible forms of intermediate representation. Finally, we also optimize these representations so that any classification tasks using them are maximally ac-curate.
 To formalize the approach we first introduce some no-tation and assumptions:  X  X denotes the entire data set of individuals. Each  X  S is a binary random variable representing  X  X 0 denotes the training set of individuals.  X  X +  X  X , X + 0  X  X 0 denotes the subset of indi- X  Z is a multinomial random variable, where each  X  Y is the binary random variable representing the  X  d is a distance measure on X , e.g., simple Eu-A key property that the learned mapping attempts to ensure is that membership in the protected group is lost. We formulate this using the notion of statistical parity, which requires that the probability that a ran-dom element from X + maps to a particular prototype is equal to the probability that a random element from X  X  maps to the same prototype: P ( Z = k | x +  X  X + ) = P ( Z = k | x  X   X  X  X  ) ,  X  k (1) Given the definitions of the prototypes as points in the input space, a set of prototypes induces a natural probabilistic mapping from X to Z via the softmax:
The model is thus defined as a discriminative clus-tering model, where the prototypes act as the clusters. Each input example is stochastically assigned to a pro-totype, which are in turn used to predict the class for that example. Statistical parity induces an interesting constraint on the prototype assignments, forcing the associated probabilities to be the same in expectation for the protected and unprotected groups. 2.2. Learning fair representations The goal in our model, which we denote LFR (Learned Fair Representations), is to learn a good prototype set Z such that: 1. the mapping from X 0 to Z satisfies statistical par-2. the mapping to Z -space retains information in X 3. the induced mapping from X to Y (by first map-Each of these aims corresponds to a term in the objec-tive function we use to learn the representations. In this learning system, there are only two sets of param-eters to be learned: the prototype locations { v k } and the parameters { w k } that govern the mapping from the prototypes to classification decisions y . For convenience, we use x 1 ,.., x N to denote N samples of the training set. We also use corresponding indica-tor variables s 1 ,...,s N , to denote whether x n  X  X +  X  n  X  N . We use y 1 ,..,y N as the outcome for x 1 ,..., x in the training set. We define M n,k as the probability that x n maps to v k , via Eqn. 2: Given this setup, the learning system minimizes the following objective: where A x ,A y ,A z are hyper-parameters governing the trade-off between the system desiderata.
 In order to achieve statistical parity, we want to ensure Eqn. 1, which can be estimated using the training data as: and M  X  k is defined similarly.
 Hence the first term in the objective is: The second term constrains the mapping to Z to be a good description of X . We quantify the amount of information lost in the new representation using a simple squared-error measure: where  X  x n are the reconstructions of x n from Z : These first two terms encourage the system to encode all information in the input attributes except for those that can lead to biased decisions.
 The final term requires that the prediction of y is as accurate as possible: Here  X  y n is the prediction for y n , based on marginalizing over each prototype X  X  prediction for Y , weighted by their respective probabilities P ( Z = k | x n ): We constrain the w k values to be between 0 and 1. Hence the prototype classification predictions them-selves can be viewed as probabilities.
 In order to allow different input features to have dif-ferent levels of impact, we introduce individual weight parameters for each feature dimension,  X  i , which act as inverse precision values in the distance function: Finally, we extend the model by using different param-eter vectors  X  + and  X   X  for the protected and unpro-tected groups respectively. We optimize these param-eters jointly with { v k } K k =1 , w , to minimize the objec-tive; details on the optimization can be found below. 2.3. Explaining the model design choices The first term in the objective enforces group fair-ness, as defined by statistical parity. We note how-ever that L z is not a direct encoding of the aim that the classification decisions are fair. The motivation for this indirect approach derives from our philosophy of a two-step system construction by two parties: an impartial party attempting to enforce fairness, and a vendor attempting to classify individuals. The impar-tial party builds mapping from individuals to new rep-resentations of individuals satisfying statistical parity, and then the vendor will be restricted to mapping the representations to outcomes. These two mappings are composed in order to obtain a fair classification of the individuals.Our learning algorithm attempts to drive L z to zero. If L z at test time is small, then P k | P ( Z = k | S = 1)  X  P ( Z = k | S = 0) | , and it is not hard to show that this implies that | P ( S = 1 | Z = k )  X  P ( S = 1) | , and | P ( S = 0 | Z = k )  X  P ( S = 0) | are small. Hence the mutual information between Z and S is small, and we have accomplished the goal of obsfucating information about the protected group.
 Furthermore we can show that even though the parity constraint does not directly address classification, un-der the current model formulation the two are closely linked. The key property is that if the parity con-straint is met, then the two groups are treated fairly with respect to the classification decisions:
This property follows from the linear classification approach.
 Another key property of the model is the fact that the mapping to Z is defined for any individual x  X  X . This permits generalization to new examples distinct from those in the training set.
 Allowing the model to adapt the weights on the in-put dimensions takes a step towards learning a good distance metric. The use of the same mapping func-tion for all individuals in the group encourages indi-vidual fairness, as nearby inputs are mapped to similar representations. Adapting the weights per group al-lows the model some flexibility in encoding similarities between individuals within a group. The model can thus address the  X  X nversion X  problem (Dwork et al., 2011), where different qualities may be deemed impor-tant with respect to classification decisions for the two groups. For example, in one community high grades in economics may be a good predictor of success in university (and therefore correlated with admittance), whereas in another community excellence in sports may be a better predictor of success in university. The distance metric can then weight sports and economics grades appropriately for the two sets. Previous machine learning research into fair classifica-tion can be divided into two general strategies. One involves modifying the labels of the examples, i.e., the f ( X 0 ) values, so that the proportion of positive labels are equal in the protected and unprotected groups. A classifier is then trained with these new labels, as-suming that equal-opportunity of positive labeling will generalize to the test set (Pedreschi et al., 2008; Kami-ran &amp; Calders, 2009; Luong et al., 2011). We term this a data-massaging strategy. The second type of approach, a regularization strategy, adds a regularizer to the classification training objective that quantifies the degree of bias or discrimination (Calders &amp; Ver-wer, 2010; Kamishima et al., 2011). The system is then trained to maximize accuracy while minimizing discrimination.
 A good example from the first class is that of (Kamiran &amp; Calders, 2009), where they  X  X assage X  the training data labels to remove the discrimination with the least possible changes. The initial step involves ranking the training examples based on the posterior probabilities of positive labels obtained from a Naive-Bayes classi-fier trained on the original dataset. They then select the set of highest-ranked negatively-labeled items from the protected set and change their labels. The size of this set is chosen to make the proportion of positive labels equal in the two groups; the ranking approach is used to minimize the impact on the system X  X  accuracy in predicting the classification labels. The modified data is then used for learning a classifier for future de-cisions. They also use Naive-Bayes to learn a classifier based on the modified dataset.
 A recent example of a regularization strategy is the work of Kamishima et al (2011); they quantified the degree of prejudice based on mutual information, and added this as a regularizer in a logistic regression model. Our model more closely resembles this reg-ularization approach, as the statistical parity is a reg-ularizer incorporated into the classification objective. Two key differences are that fairness in LFR is defined in terms of the intermediate representation rather than the classification decisions, and that we explicitly at-tempt to retain information in the input attributes with the exception of membership in the protected group. This enables the intermediate representation to potentially be used for other classification decisions. A third approach in the literature is the  X  X airness Through Awareness X  work (Dwork et al., 2011). Here a mapping to an intermediate representation is ob-tained by optimizing the classification decision crite-ria while satisfying a Lipschitz condition on individ-uals, which stipulates that nearby individuals should be mapped similarly. One important difference in our model is that our approach naturally produces out-of-sample representations, whereas this earlier work left open the question of how to utilize this fair mapping for future unseen examples.
 On the computational side one notable piece of related work is the information bottleneck approach (Tishby et al., 1999). The aim in information bottleneck is to compress the information in some source variable X while preserving information about another rele-vant variable Y . The optimization is cast as finding a new representation that simultaneously maximizes the mutual information with Y while minimizing the information about X . Our method similarly attempts to learn a representation that trades off mutual in-formation, and maximizes it with a relevant variable Y . However in our formulation the representation at-tempts to minimize mutual information with only a portion of the input ( S ) while maximizing the retained information about the remainder of X .
 More broadly there is a large body of work on fairness in social choice theory, game theory, economics, and law. Among the most relevant are theories of fairness and algorithmic approaches to apportionment, e.g., Young X  X , Equity , Moulin X  X  Fair Division and Collective Welfare , Roemer X  X  Equality of Opportunity and The-ories of Distributed Justice , and Rawl X  X  A Theory of Justice . Concerns about the impact of classification in-clude: maintaining a fair marketplace, bias, impedence of autonomy and identity formation, and the fear that segmented access to information undermines shared experience and therefore the informational commons considered important to democracy. Recent papers (Dwork &amp; Mulligan, 2012; Zarsky, 2012) articulate these concerns related to classification, and point out that there are no current proposals at present to reg-ulate or build systems addressing these concerns. Finally, there is a close connection between individual fairness X  X reating similar people similarly X  X nd differ-ential privacy (Dwork et al., 2006). Differential pri-vacy is a definition of privacy designed for privacy-preserving analysis of data; it ensures that the output of any analysis is essentially equally likely to occur on any pair of databases differing only in the data of a single individual. Thus, differential privacy requires that algorithms behave similarly on similar databases, while individual fairness requires that classification outcomes will be similar for similar individuals. An-other view of this relationship is that differential pri-vacy involves a constraint with respect to the rows of a data matrix, while fairness involves a constraint with respect to the columns. The analogy is well founded, as techniques from differential privacy may be used to achieve individual fairness (Dwork et al., 2011). 4.1. Comparisons, datasets, and protocol For comparison, we implemented the four variations of the models in (Kamiran &amp; Calders, 2009), using Naive-Bayes in both the ranking and classification phase of their algorithm; the variants either use separate or combined classifiers in each phase. We denote this the FNB model, for Fair Naive-Bayes. We also imple-mented the logistic regression method with a regular-izer proposed by (Kamishima et al., 2011), and opti-mized the setting of the regularization parameter. We denote this the RLR model, for Regularized Logistic Regression. We also trained an un-regularized version of logistic regression, denoted LR, as a baseline. We defined a performance metric to apply to the val-idation set in order to determine the best variant and hyper-parameter setting for each method. We chose to focus on two measurements X  X he accuracy and dis-crimination in the model output X  X ince both of these were considered in the earlier work on the FNB method (Kamiran &amp; Calders, 2009) and the RLR approach (Kamishima et al., 2011). In addition, in order to evaluate individual fairness, we define a metric that assesses the consistency of the model classifications lo-cally in input space; values close to one indicate that similar inputs are treated similarly.
  X  Accuracy : measures the accuracy of the model  X  Discrimination : measures the bias with respect  X  Consistency : compares a model X  X  classification Here we present results of the methods on two datasets which are available from the UCI ML-repository (Frank &amp; Asuncion, 2010). The German credit dataset has 1000 instances which classify bank account holders into credit class Good or Bad . Each person is described by 20 attributes. In our experiments we consider Age as the sensitive attribute, following (Kamiran &amp; Calders, 2009). The Adult income dataset has 45,222 instances. The target variable indicates whether or not income is larger than 50K dollars, and the sensitive fea-ture is Gender , as in (Kohavi, 1996; Kamishima et al., 2011)). Each datum is described by 14 attributes. The final, considerably larger dataset we experimented with is derived from the Heritage Health Prize mile-stone 1 challenge (www.heritagehealthprize.com). It contained 147,473 patients, described using the same 139 features as the winning team, Market Makers. The goal is to predict the number of days a person will spend in the hospital in a given year. To convert this into a binary classification task, we simply pre-dict whether they will spend any days in the hospital that year. We split the patients into two groups based on Age ( &gt; 65). For details on the datasets see the Supplementary Material.
 All methods were trained in the same way for all datasets. For each variant of FNB, and each setting of the hyper-parameters in the other two methods, we evaluated the performance metrics on the validation set. For our method, LFR, we applied L-BFGS to min-imize Eqn. 4. We performed a simple grid search to find a good set of hyper-parameters in Eqn. 4: A x was 0 . 01, and we chose A y ,A z to be the values from the set S = { 0 . 1 , 0 . 5 , 1 , 5 , 10 } . We also included A For RLR, we optimized the regularization parameter  X   X  X  0 , 0 . 5 , 1 . 0 , 1 . 5 , 3 . 0 } . 4.2. Results and analysis A key issue is what measure should be used for model selection; that is, which criteria will be used to evalu-ate a model X  X  performance on the validation set with a particular setting of hyper-parameters. Here we focus on two measures. In the first the selection was based on minimizing the discrimination criteria yDisc , reflecting the primary aim of ensuring fair-ness. The second selection was based on maximizing the difference between accuracy and discrimination: Delta = yAcc  X  yDisc . In each case we compare the performance of the respective models on a test dataset, examining both the accuracy and discrimination in Y . The results are summarized in Figure 1; LR = Logis-tic Regression (a baseline method); FNB = Fair Naive Bayes; RLR = Regularized Logistic Regression; and LFR = Learned Fair Representations, our new model. In these results it is clear that our model is capable of pushing the discrimination to very low values, while maintaining fairly high accuracy. The results are con-sistent in all three datasets, and across the validation criteria.
 In particular, the Fair Naive Bayes method has dif-ficulty in maintaining low values of discrimination at test time. It performs quite well on the Adult dataset, but its performance suffers considerably when the size of the problem increases, as in the Health dataset. The Regularized Logistic Regression has more consistent success in limiting discrimination while preserving ac-curacy, but still does not match our method X  X  perfor-mance overall. This is quite surprising, since our LFR model is not directly minimizing discrimination but instead optimizing a proxy evaluated on the interme-diate representations. In addition our model is also trying to preserve information about the data. We can also compare the models with respect to indi-vidual fairness. We use the yNN measure (Eqn. 15) to evaluate the consistency of each model X  X  classification decisions. The results for the models that were se-lected based on discrimination are shown in Figure 2. For each dataset our model obtained better individual fairness; this is likely due to the optimization criteria rewarding Z  X  X  preservation of information about X . For the remainder of this section, we focus on the model that maximizes the difference between accuracy and discrimination. We can gain some insight into our method by analyzing various aspects of the learned representations. First we note that a stated aim of our learning method is to encode as much information as possible about each individual while removing in-formation about membership in the protected set. We can evaluate the degree to which the system succeeded in accomplishing this at test time, in several ways. First we can measure how much information about S is contained in the latent representation by building a predictor that learns to predict S from Z :
We optimize this predictor to minimize its difference with the actual s n , and then evaluate test predictions for S using an sAcc score analogous to yAcc :
Note that even though we optimize the predictor in effect to maximize sAcc , in contrast to yAcc which we want to be as close to 1 as possible, we want sAcc to be low. We can evaluate how much information about S is removed by the mapping to Z by comparing sAcc to its upper and lower bound. A simple upper bound (based on a linear predictor) predicts  X  s from the rest of the input vector X except for S ; a lower bound is 0 . 5, which corresponds to random guessing. Results for the three datasets of predicting S from the raw data (our simple upper bound) versus from the prototype representations in the trained models (as in Eqn. 16) are shown in Figure 3. In all cases the accuracy has moved significantly towards the lower bound of 0 . 5, showing that the information regarding learned representations has been significantly obfuscated. This demonstrates that the sensitive information has been obfuscated, but is specific to the method of pre-dicting S . Another evaluation involves looking directly at statistical parity in the representations at test time. An upper bound on how much information about S is contained in the new representations can be gained by finding the maximum amount of bias across the prototypes; in the Adult dataset for example, this is: max k | P ( Z = k | S = 1)  X  P ( Z = k | S = 0) | = 0 . 0027. Similar results were obtained for the other datasets. As discussed in Section 2.2, this implies that the mu-tual information between S and Z is very small, and thus Z represents a sanitized version of the input where S has been essentially forgotten.
 An additional interesting aspect of the model concerns the learned distance metric. The model is capable of learning different precision parameters along each in-put dimension, or feature i . When we rank the fea-tures based on the magnitude of their difference be-tween the two groups, |  X  + i  X   X   X  i | , we find that the top-ranked features can reveal something interesting about the datasets. For example, in the Adult dataset, the top-ranked feature represents whether the individual is  X  X ever married X . Here the sensitive feature is gender; the  X  for female is bigger than  X  for male on this fea-ture. Similar results were obtained in both datasets. This shows that our algorithm can adapt the distance function to an inverted scenario where awareness of the protected group may be relevant to classification. 4.3. Transferring fair representations We also investigated the ability of our model to learn multiple classifications from a common set of fair rep-resentations. This is a form of transfer learning, in which the learned fair representations may be used for several different classifications, such as by differ-ent vendors, or various decisions by the same vendor. In order to examine this, we identified another dimen-sion in the Adult dataset as the second classification variable. We trained an LFR model on a modified version of the dataset, in which this new dimension, Age, was removed from each input example X . Oth-erwise the setup was exactly as above, with Gender as the sensitive variable S and Income as the target Y . We used a validation set to again find the best setting of the hyper-parameters. After training we used the { M nk } values as the representation of each individual n , and trained a linear predictor for the new classifi-cation problem, Age.
 As a benchmark, we trained a linear predictor (LR) for Age that mapped directly from the input x . The question of interest is how much the learned repre-sentations lose in accuracy versus how much fairness is gained, relative to this direct prediction. In this experiment, we found that the transferred representa-tions suffered a small loss in accuracy ( yAcc dropped &lt; 7%) while significantly removing bias in the classi-fication ( yDiscrim dropped &gt; 97%). In this paper, we formulated fairness as an optimiza-tion problem of finding an intermediate representation of the data that best encodes the data while simultane-ously obfuscates aspects of it, removing any informa-tion about membership with respect to the protected group. Our model maps each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. Classifica-tions can be made based on these new representations. We implemented our algorithm on three data sets and showed positive results compared to other known tech-niques, and conducted an initial investigation into the ability of our model to learn multiple classifications from a common set of fair representations.
 Fairness in classification is a growing concern with tremendous societal importance, pulling in scholars from many diverse areas, such as law, economics, and public policy. For example, a new initiative on big data mining, fairness and privacy has recently been estab-lished (http://privacyobservatory.org/current/40-big-data-mining-fairness-and-privacy). Researchers from an algorithmic and machine learning perspective have an important role to play in this new area.
 A multitude of interesting and important open prob-lems remain to be solved; here we mention just two. First, all formulations of fairness thus far aim to elim-inate all bias. However, in many circumstances, the classification goal does have a direct correlation with membership in the protected group. For example, when predicting who is likely to succeed in university (presumably a key criteria in admission decisions), it might be the case that statistically individuals from a certain population are much more likely to succeed in university than the population at large. One way to nonetheless maintain balance is a quota system, i.e., ensure that the proportions of positive classifications is some value but not necessarily equal between the groups; we can readily handle this case in our frame-work. Yet it is more desirable to allow some other non-quota control. Formulating a more general framework for fairness that does not force equality or quotas is an important problem. A related problem is understand-ing how to deconstruct a given classifier to determine to what extent it is fair.
 Secondly, we would like to develop other forms of inter-mediate representations beyond prototypes, utilizing multi-dimensional distributed representations, which may offer a richer space for achieving good classifica-tions and transfer to new classifications, while main-taining fairness.
 Calders, T. and Verwer, S. Three naive bayes ap-proaches for discrimination-free classification. Data Mining and Knowledge Discovery , 21:277 X 292, 2010. Dwork, C. and Mulligan, D. Privacy and classification concerns in online behavioral targeting: Mapping objections and sketching solutions. In Privacy law Scholars Conference , 2012.
 Dwork, C., McSherry, F., Nissim, K., and Smith, A.
Calibrating noise to sensitivity in private data anal-ysis. In Theory of Cryptograph Conference (TCC) , 2006.
 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and
Zemel, R. Fairness through awareness. In Proceed-ings of Innovations of Theoretical Computer Sci-ence , 2011.
 Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.ics.uci. edu/ml .
 Kamiran, F. and Calders, T. Classifying without dis-criminating. In 2nd International Conference on
Computer, Control and Communication , pp. 1 X 6, 2009.
 Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware learning through regularization approach. In
IEEE 11th International Conference on Data Min-ing , pp. 643 X 650, 2011.
 Kohavi, R. Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining , 1996.
 Luong, B., Ruggieri, S., and Turini, F. k-NN as an im-plementation of situation testing for discrimination discovery and prevention. In Proceedings of the 17th ACM KDD Conference , pp. 502 X 510, 2011.
 Pedreschi, D., Ruggieri, S., and Turini, F.
Discrimination-aware data mining. In Proceedings of the 14th ACM KDD Conference , pp. 560 X 568, 2008. Tishby, N., Pereira, F.C., and Bialek, W. The In-formation Bottleneck method. In The 37th Annual
Allerton Conference on Communication, Control, and Computing , 1999.
 Zarsky, T. Automated prediction: Perception, law,
