 We give a statistical interpretation of Proximal Support Vec-tor Machines (PSVM) proposed at KDD2001 as linear ap-proximaters to (nonlinear) Support Vector Machines (SVM). We prove that PSVM using a linear kernel is identical to ridge regression, a biased-regression method known in the statistical community for more than thirty years. Tech-niques from the statistical literature to estimate the tuning constant that appears in the SVM and PSVM framework are discussed. Better shrinkage strategies that incorporate more than one tuning constant are suggested. For nonlin-ear kernels, the minimization problem posed in the PSVM framework is equivalent to finding the posterior mode of a Bayesian model defined through a Gaussian process on the predictor space. Apart from providing new insights, these interpretations help us attach an estimate of uncertainty to our predictions and enable us to build richer classes of models. In particular, we propose a new algorithm called PSVMMIX which is a combination of ridge regression and a Gaussian process model. Extension to the case of continu-ous response is straightforward and illustrated with example datasets. G.3 [Probability and Statistics]: Correlation and regres-sion analysis, Stochastic processes. Algorithms. Bayesian models, classification, correlation, kernel, bias-variance tradeoff, regression. permission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. 
Another benefit of our statistical interpretation is that it provides guidance into building richer classes of models. By using combinations of linear and nonlinear kernels, we introduce a new algorithm, PSVMMIX, and indicate how it could be implemented by a slight modification of PSVM. Similarly we show how the methodology easily extends itself to regression with continuous response variables. 
The paper is organized as follows: In Section 2 we prove the equivalence of PSVM with linear kernel and ridge regres-sion and discuss some commonly used statistical methods used to determine the tuning parameter [17, 28] which may prove useful when implementing SVMs or PSVMs for data mining tasks. In Section 3 we show that the minimization task posed by PSVM with nonlinear kernels is equivalent to finding the posterior mode of a Bayesian model. Section 4 discusses PSVMMIX followed by data examples in Section 5. Section 6 concludes the paper with some discussion and scope for future work. 
A word about our notation. All vectors will be column vectors. The transpose of a matrix or a vector will be indi-cated by the superscript ~T'. For an m x n matrix A and n  X  k matrix B, K(A, B) is an m x k matrix. In particular, if x and vector and K(A, A T) is an mxm matrix. For a matrix A, Ai. and A 4 will denote the ith row and jth column respectively. All probability distributions will be denoted by a single func-tion f(.). For example, f(y) will denote the probability dis-tribution of a random variable Y, f(y; z, w) will denote the conditional distribution of Y given (Z, W). E(.) and V(.) will denote the expectation and variance of a probability distribution respectively. So E(g(Y); Z, W) denotes the ex-pectationf g(y)f(y; z, w)dy and Y(g(Y); Z, W) denotes the variance f (g(y) -E(g(Y); Z, W) )2 f(y; z, w)dy of g(Y) with respect to the conditional distribution of Y given (Z, W). For any two random variables (X, Y), marginalizing over Y will mean obtaining the marginal density f(x) by integrating out y from the joint density f(x, y). X ~ N(m, V) means the random variable X follows a normal or Gaussian prob-ability distribution with expectation m, dispersion matrix V and density given by f(x) = 1/V~(27r)PlVDexp(- X (x -m)TV-l(x-m)), where p is the dimension of X. For each x, Jxl will denote the euclidean distance of x from the origin. The following development shows that the linear version of PSVM is equivalent to ridge regression where the dependent variable, z, is defined as +1 or -1 depending on the class assignment of each of the m items in the training set. From equation (9) of [13], the objective is to minimize, where, D is a diagonal m x m matrix with diag(D) = z; A is the m x n matrix of numeric predictors .(features); e is an m  X  1 vector of all l's; w is an n x 1 vector of coefficients, one for each feature, to be determined; is a scalar, to be determined; v is an arbitrary positive tuning constant for the algorithm that plays a role whenever perfect classification is impossi-strategies perhaps based on domain knowledge instead of using just one tuning parameter. Such strategies are effec-tive if the data is linearly separable but contaminated with a lot of noise. As an extreme case, we can have a different ridge parameter for each predictor. However, this will lead to an algorithm with too many tuning constants that will be difficult to implement in practice. In a Bayesian frame-work one can assume the ridge parameters have a common probability law i.e. j3 ,~ N(0, o'2diag(~l, ... , ~'n)) and the ~,~s are shrunken further by assuming they are identically and independently distributed with common distribution f(~,; a) where a is known. For instance, a possible choice of f(~,;a = {C~,C2}) is a uniform distribution on [C1,C2] or even a uniform distribution on a unidimensional grid. 
Finally we remark that extension to the case of continu-ous response variable is trivial in the statistical framework. 
Ridge regression was originally invented by statisticians as an alternative to ordinary least squares. They argued that even though the ridge estimator was biased, a small bias can often be compensated by much lower variance, such that the overall mean square error of the ridge estimator is smaller than it's ordinary least squares counterpart. Ridge estima-tion has been generalized for binary response data by [21, 20]. References [23, 27, 25] extend it further for generalized linear models. These models are fitted by means by an it-erative procedure which is computationaily more intensive. 
A subtle point to remember while implementing ridge re-gression for continuous response is to make sure that the response variable is centered at zero. An alternate strategy is to assume [/z : w] ~ N((c~,0) T,va2I) where ct is some measure of central tendency of the response variable (gen-erally the sample mean or sample median). 
In most data mining applications, the ridge parameter u is chosen by cross-validation, in which some part of the train-ing sample is held back and the value that best predicts this held-back data is our estimate. While there's nothing wrong with this grid search in low dimensions, getting the right scale for the parameter (or the interval of grid search) might turn out to be a difficult task in most data mining applica-tions. The ridge regression literature discusses a plethora of methods to estimate ~, from the training data. In practice, it may be a good idea to use them first to get the correct scale for the parameter followed by cross-validation. We dis-cuss some of these methods now and point out the relevant literature. 
The simplest method is to graph the coefficients as func-tions of the ridge parameter k leading to a choice of ~, that seem to give "sensible" coefficient estimates. See [30] for fur-ther discussion on these plots called "ridge trace plots" in the statistics literature. For data mining applications, although these trace plots may not give us the exact estimate of v, it certainly helps us to decide on a sensible range [C1, C2] in which to confine our grid search to. While [13] mention esti-mating u by using a validation sample, they.do not say how they conduct the grid search. The authors in [7] compare several methods of estimating k through a simulation study. 
Their RIDGM method uses an empirical Bayes approach to estimate k. Efron and Morris in their discussion of [7] sug-gest a further refinement based on EBMLE discussed in [11]. 
Reference [17] compares ten different methods of estimating k using a Monte Carlo study and identify three estimators -HKB, RIDGM, and GHW as giving the best overall per-formance. We give a brief description of these estimators below. 
If n is the number of predictors, the HKB estimator is given by k = ncr2/13T/~. Let j3 denote the ordinary least squares (OLS) estimator of fl and o ~ = (z -X/~)r(z -X/~)/(n -m -1), an estimate of a s. Let j3(k*) denote the ridge estimate of j3 at k = k*. At the t th iteration, ated algorithm has been implemented by applying a 10 -4 convergence criterion to successive k values and defaulting to least squares estimator if convergence is not obtained in 30 iterations. 
Let GTxTxG = diag(A1,  X  .-, An) denote the spectral de-composition of xTx and 'y = GTfl. The RIDGM approach is to choose k so that where ~ = GT/~. In practice, the solution is obtained by computing the left hand side of equation (5) for a mesh of k values. 
Both HKB and RIDGM estimators depend explicitly on least squares estimates of fl which may not be available in practice due to singularity of the feature matrix. GHW es-timator is attractive in the sense that it does not depend on the OLS estimates. A solution for k is determined by mlmmmmg In practice, the minimization is implemented by evaluating V(k) for a mesh of k values. The computations are facili-tated by the fact that (xTx + kI) -~ = XG(diag(1/(A~ + k),... , 1/(An q-k)))GTX T requiring just one matrix inver-sion for all values of k. 
Finally, it is possible to get a Bayesian estimate by putting a prior on ~, and carrying out the estimation process us-ing Markov Chain Monte Carlo (MCMC) algorithms. The MCMC algorithm most commonly used in statistics is Gibbs sampling, popularized by [15]. Such an approach can easily incorporate richer shrinkage strategies ass discussed earlier in Section 2. However, MCMC algorithms are iterative in nature and it may be hard to scale them up to massive data mining tasks. With linear kernels, the complexity of MCMC depends mainly on n (the number of predictors). If n is not too large (say &lt; 200), it is feasible to run an MCMC algorithm on the entire training dataset to estimate all the shrinkage parameters in the model. Needless to say, one should only consider taking such an approach if the number of tuning parameters involved is large. For large number of predictors, if one has to rely on MCMC tech-niques, it should be done after marginalizing over/~ in (2) i.e. z ..~ N(O, a2(l + vxxT)). Since the dispersion matrix terms of Q enabling us to exploit the sparsity of K for large m. Equation (13) can be rewritten as follows: 
Assuming the number of predictors L involved in M is small, the expressions in (14) can be evaluated easily once we have inverted Q using efficient inversion routines for sparse ma-trices (See [18] for details on using these routines in MAT-
LAB). The limit on the size of L is not too restrictive for practical applications. Using a very large L would kill the contribution of the Gaussian process to a large extent and we may end up approximately implementing a linear version of PSVM. On the other hand, if we shrink the linear terms too much by making u~ small, we end up approximately im-plementing a nonlinear version of PSVM and unnecessarily wasting computational time in the process. 
Since Q is positive definite, the actual estimation process is carried out by using a sparse Cholesky decomposition of 
Q; i.e. Q : GG T where G is a lower triangular matrix and solving a system of linear equation Qx = b for a bunch of unknown b's. Once the factorization is known, solving this system for any known b involves m forward substitutions followed by m backward substitutions. All our computa-tions were done on an SGI server with 195 MHz processor using sparse matrix routines in the meschach library, a nu-merical library of C routines for performing calculations on matrices and vectors [29]. A pseudo code for implement-ing PSVMMIX for known values of u, s (tuning parameter associated with the kernel) and u~ at a point xo not in the training sample is given below. The two main functions used are spCHfactor(P) (returns the Cholesky factorization of a sparse input matrix P) and spCHsolve(W, w) (returns the solution of Px = w using the sparse Cholesky factorization 
W = spCHfactor(P) of P). The corresponding functions for dense matrices are CHfactor and CHsolve. %*% will denote matrix multiplication and t(.) will denote the transpose op-erator for vectors and matrices. 2. xx = t(M)%*%spCHsolve(G,z); 3. for(i in i:L) OUT.i = spCHsolve(G,M.i); 4. WORK = t(M)%*%OUT + (1/vZ)*I; 5. temp --CHfactor(WORK); 6. Betaest = CHsolve(temp,xx); 7. ee --z -M%*%Betaest; 8. xx = spCHsolve(G,ee); 9. kk K(xoTK, AT); 10. Predictor = t(XoM)% * %Betaest + kk%  X  %xx; tion of an L  X  L dense matrix. This shows clearly why we can't afford to use too many predictors in the linear part of 
PSVMMIX. To implement a grid search for (u, s, v~), note that at each fixed value of (u, s), steps 1 to 3 are imple-mented once followed by looping steps 4 to 10 over values of Table 1: Training and ten-fold cross validation ac-curacy for the Ionosphere dataset for two different models use of sparse kernels. Instead of storing an m x m matrix Q we have to only store the non-zero entries of the sparse ma-trix Q which is of order O(mbw). Loosely speaking, we can think of b~, as the maximum number of nearest neighbors used by the training algorithm. Storing the dense matrix M is of order O(mL) and hence the total memory requirement is of order O(m max(L, b~,)). The main computationally in-tensive tasks performed by the algorithm are the inversion of Q which is of order O(mb~) and inversion of the L x L dense matrix in step 5 of the psuedo-code which is of order O(La). Hence, for small values of bw and L, the algorithm scales linearly both in space and time approximately. Note that the predictors used in the nonlinear part only enter the algorithm through the pairwise distance matrix used to compute the kernel matrix K which is computed at the be-ginning. Thus, we can have a very large number of predic-tors in the nonlinear part without affecting the performance of the algorithm both in terms of time and space. In sec-tion 5.6 we illustrate the performance of the algorithm on a dataset consisting of large number of observations. We have recently applied the algorithm to a dataset consisting of a large number of predictors (1200 approximately). However, we won't report that analysis here due to space constraint but can provide details on request. This publicly available dataset from the UCI Machine Learning Repository [26] consists of 351 observations and 34 predictors. All the predictors are continuous. The re-sponse variable is binary. We fitted a PSVM model with linear kernel using 69 predictors [1, Xl, "'" , X34, Xl, " " "2 The value of u was estimated by doing a grid search for k E {1, 2,... ,50}. Our final estimator was u = 1/37. We also computed the GHW estimate for u which was 1/14 in this case. Figure 1 shows separate ridge trace plots (/3 vs k = l/u). The trace plots for the linear and quadratic terms are different. The latter looks like a funnel with a bigger radius indicating it may require more shrinkage. We fit-ted a second model which we call PSVM2 with two distinct shrinkage parameters ul and v2 for the linear and quadratic terms. The parameters were estimated by means of a two dimensional grid search. The estimated values in this case were ul = 1/5, v2 --1/25 indicated by vertical lines in figure 1. The ten-fold cross-validation accuracy was computed by randomly dividing the entire data into 10 random subsets of equal size (one of them had an extra observation) and con-glomerating 9 parts into a training set while the 10th part was used as a test set. This was done 10 times (each part taken as a test set once) and the average accuracy reported in table 1. PSVM2 gives us better test set accuracy than the PSVM model in this case. 
Figure ,l: qqplots for selling price on the original and log scale for Dallas data 3)AGE Age of the dwelling in decades. 4)BATHS -Number of baths in the house. Two half bath-rooms are counted as one. 5) ISD -Independent School District to which the house be-longs. There are 13 such districts in the database. 6) pool -A 0/1 valued variable with 1 denoting the presence of a pool in the house. 7)wetbar -A 0/1 valued variable with 1 denoting presence. we have a total of 20 predictors in our database. All con-tinuous predictors were centered at zero and scaled by their respective standard deviation. Motivated by [2], we decided to use LAT and LONG in the nonlinear part of PSVMMIX with the remaining 18 predictors used in the linear part. 
To make Q sparse, we worked with the following compactly supported kernel [16] at which the correlations approach zero. quires estimating constants u, s, ~, 0 and v~. Fitting PSVM-
MIX to several small subsamples from the training and val-idation set using a Gaussian kernel helped us narrow down our grid search for u, s and u~. For the current dataset, we decided to use s E {.1,.6, .9, 1.0}, u E {20, 50, 100} and u~ E {.3, .7, 1.0}. Since we have n = 2 predictors in the non-linear part, P must be &gt; 1.5. We used ~ = 2. If Ni denotes the number of non-zero correlations of the ith data point in the training set (i.e. number of non-zero entries in the ith row of K(A, A T) or the number of nearest neighbors for the 
We have given a statistical interpretation of PSVM with linear and nonlinear kernels. These interpretations provide further evidence that shrinkage estimators are perhaps the best data mining tools around. The equivalence of ridge regression and PSVM with linear kernels enable us to take advantage of the rich statistics literature to estimate the tun-ing parameter more effectively, an issue that was ignored in earlier work. Better shrinkage strategies to control the bias-variance tradeoff in our predictions can be considered by increasing the number of tuning parameters. This may lead to better classification as illustrated by the Ionosphere dataset. With a small number of predictors, the tuning con-stants could be estimated using MCMC methods as illus-trated with the adult dataset. With nonlinear kernels, we showed PSVM is equivalent to a Bayesian model defined through a Gaussian process on the predictor space with the correlations determined by the kernel. We then introduced a new class of models called PSVMMIX that were combina-tions of linear and nonlinear PSVM's. We demonstrated how these models could be fitted to large datasets by exploiting the sparsity in K(A, AT). However, more methodological re-search is needed to efficiently choose the tuning parameters for very large datasets. This is an issue that pops up even in the context of PSVM's and SVM's and needs further investi-gation. For instance, [9] report on an empirical study where they compare several functionals that were used to tune the parameters (called hyperparameters by them) for SVM's. In our case, we used the cross-validation MSE to tune the pa-rameters which is an expensive functional to compute. We are currently investigating proxies that are cheap to com-pute and would enable us to estimate the tuning constants quicker for large datasets. One possibility is to use Ker-nelized Locally Linear Embeddings (KLLE) proposed in [6]. Although not exemplified, extension to the case of multi-category data with nominal categories is trivial. With k categories (k &gt; 2), the classification problem is solved by implementing k 2-class problems. With ordinal categories (e.g. User satisfaction on a 3 point scale: bad, satisfactory and good) the classification problem could be solved using an ordinal response model (see [3] and references therein). 
Another issue is selecting predictors to go in the linear part and nonlinear part of PSVMMIX. Putting everything in the linear and nonlinear part may not help n,uch as il-lustrated with the B3PA dataset. Increasing the number of predictors L that goes in the linear part increases com-putational time cubicly in L since the algorithm involves a Cholesky decomposition of an L x L dense matrix. In gen-eral, if there are predictors that are known to have a casual relationship with the response variable, our recommendation is to put them in the linear part. The coefficient estimates which we obtain at the end of the day would enable us to check whether the causality is respected by the algorithm or not. If we have a combination of nominal valued and continuous valued predictors (very common in data min-ing applications), our recommendation is to put all nominal valued predictors in the linear part. Centering and scaling predictors may sometime lead to bad performance as seen in the Boston Housing data example. On the other hand, it is known to work well in many situations. In complex multivariate problems, this is always an issue that pops up and there isn't any clear answer to this problem. The best thing is to try both versions and see the difference. In all our data examples with PSVMMIX(v~), we assumed v~ is a scalar. However, as with the linear case, this could very well be a vector. Further research would be needed to figure out an efficient method to estimate the extra tuning parameters. 
We are currently working on this and hope to report on this at KDD2003. Next, we showed that the statistical interpretations enable us to extend methodologies from the classification literature to the regression scenario without any extra effort. We also demonstrate the usefulness of attaching an estimate of vari-ability to our predictions. To sum up, we feel the fundamental contribution of this paper is to show that some well known KDD algorithms turn out to be equivalent to some commonly used shrinkage estimation techniques used in statistics. This equivalence opens up new research opportunities both for statisticians and computer scientists. The challenge to the statistical community is to develop methodologies that can work for massive datasets by taking advantage of the KDD litera-ture. On the other hand, the statistical insights should help the computer science community better understand their al-gorithms and improve upon them. I thank William DuMouchel for all his support and help without which this work would not be possible. It was Bill who initiated this work by pointing out the equivalence be-tween PSVM with linear kernel and ridge regression. I thank 
Daryl Pregibon for stimulating discussions from time to time that helped improve the quality of the paper. Finally, I thank Professor C.F.Sirmans and T.G.Thibedeau for pro-viding the Dallas house price data. [1] US census bureau. Adult dataset. Publicly available from: www.sgi.com/Technology/mlc/db. [2] D. Agarwal. Bayesian spatial regression analysis with large datasets, ph.d dissertation, university of connecticut, www.research.att.com/~dagarwal. 2001. [3] J. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. Journal of the 
American Statistical Association, 88:669-679, 1993. [4] H. Arthur and K. Robert. Ridge regression: Biased estimation for nonorthogonal problems. 
Technometrics, 12:55-67, 1970. [5] L. Brieman. Bagging predictors. Machine Learning, 26:123-140, 1996. [6] D. DeCoste. Visualizing mercer kernel feature spaces via kernelized locally-linear embeddings. In The 8th International Conference on Neural Information 
Processing, 2001. [7] A. Dempster, M. Schatzoff, and N. Wermuth. A simulation study of alternatives to ordinary least squares. Journal of the American Statistical 
Association, 72:77-106, 1977. [8] H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In Michael C. Mozer, Michael L Jordan, and Thomas Petsche editors, Advances in Neural Information Processing Systems -9-, pages 155-161. The MIT 
Press, Cambridge, MA, 1997. [9] K. Duan, S. Keerthi, and A. Poo. Evaluation of simple [10] W. DuMouchel and D. Pregibon. Empirical bayes [11] B. Efron and C. Morris. Data analysis using stein's [12] J. Friedman, T. Hastie, and R. Tibshirani. Additive [13] G. Fung and O. Mangasarian. Proximal support [141 3. Garcke and M. Griebel. Data mining with sparse [15] A. Gelfand and A. Smith. Sampling based approaches [16] M. Genton. Classes of kernels for machine learning: A [17] D. Gibbons. A simulation study of some ridge [18] J. Gilbert, C. Moler, and R. Schreiber. Sparse [19] T. Hsalng. A bayesian view on ridge regression. The 
