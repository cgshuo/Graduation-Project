 Categories and Subject Descriptors: H.3.3 [Information General Terms: Algorithms, Experimentation, Measurement. Keywords: topic pages, aspect models, query logs. We define topic pages as content hubs, which aggregate use-ful information on particular topics from sources all over the Web, and enable users to easily navigate to those sources. They collect and organize information pertaining to different aspects of a topic in one place, explicitly addressing redun-dancy and diversity in addition to relevance. Figure 1 shows an example topic page automatically generated by our sys-tem for the topic  X  X illiam Shatner X , which covers distinct relevant aspects, such as his acting career, famous movies, books, and TV commercials, and provides links to various Web sources for additional information on each aspect. In contrast to typical Web search result pages, whose snip-pets often provide superficial and/or redundant information, topic pages present a single high-quality summary while re-taining pointers to a multitude of information sources.
One of the key challenges in generating topic pages is defining the notion of information usefulness. Wikipedia X  X  content harvests the wisdom of the crowds, but with a bias toward what the active contributors believe is important for a topic. Multi-document summarization systems often de-fine the information usefulness of sentences based on proper-ties of the summarized documents themselves. In contrast, we employ Web search query logs to build aspect models that capture a consensus of user interests with respect to the top-ics. We then attempt to cover these aspects accurately and non-redundantly, task which includes sentence retrieval and ranking, as well as coherent aggregation of the information.
Figure 1: Example topic page for  X  X illiam Shatner X  Topic page generation could be viewed as a topic-focused multi-document summarization task. However, a cru-cial difference is that typical multi-document summarization systems utilize a pool of given text documents and employ word-based statistics from the input pool directly to deter-mine sentences that need to be added to the summary [8]. While the documents obtained through Web search by query-ing a topic are often related to that topic, they frequently contain irrelevant information and lack cohesiveness, which makes them difficult to summarize [7]. Instead of using the documents alone, Lacatusu et al. X  X  system [6] breaks down a complex query into simpler queries and produces summaries as responses to those queries. In a similar fashion, we gen-erate aspect models for a topic from query logs, and employ them to construct the topic pages, thereby avoiding the dif-ficult task of summarizing non-cohesive Web documents.
Early work on biography generation has focussed on multi-document summarization of information from news collections [10, 11]. Alani et al. [1] use pre-defined bio-graphical templates to collect biographical facts, Filatova and Prager [5] identify person-specific, occupation-specific, and general biographical events, while Biadsy et al. [3] learn a biographical sentence classifier from Wikipedia and TDT4. Conversely, we extract from query logs aspect models with varying degrees of specificity to the target topic and similar topics, without the need to explicitly capture occupational roles, templates, or contextual patterns.

In contrast to Web search results clustering (e.g., [12]), which implicitly attempts to discover sub-topics within search results, we use explicit aspects derived from query logs to retrieve relevant sentences and then organize them in a co-Figure 2: System architecture for topic page generation herent fashion, with pointers to their Web sources. We opted for aspect models with low complexity, but more sophisti-cated models, such as approaches for class attribute extrac-tion and propagation in conceptual hierarchies [9], could also be employed in our topic page generation framework. We built a reference collection from Wikipedia to develop, train, and evaluate the proposed system and its subcom-ponents. To select biographical topics, we first gathered the Wikipedia pages that were labeled with the category  X  X iving people X . We used the page titles, after removing parentheti-cals, as topic names. Less than 5% of these names were du-plicates, indicating possible ambiguity problems, and were eliminated from the candidate set to simplify the experimen-tal setup. 1 Because entertainment-related topics appeared to dominate this set, we inspected the category information for a small set of known topics to identify categories corre-sponding to diverse occupations. For example, to build a seed list of politicians, we selected pages in the  X  X iving peo-ple X  category with additional category labels such as  X  X .S. Senator X . Then, we propagated the occupation labels to other topics by using the Wikipedia category sets. Addition-ally, to avoid topics with very little user interest, we removed topics that had fewer than 20 entries in a six-month query log from the Bing search engine. From the resulting topics, we randomly selected uniformly over occupation labels three disjoint sets of 100 topics for training, development and test.
Figure 2 shows the key components of our general frame-work for generating topic pages: aspect extraction, content retrieval/selection, and content organization. Web search query logs can be seen as aggregators of the in-formation needs of a vast number of users with respect to any topic. However, the distribution of queried aspects can be heavily biased towards events occurring within the time frame of the analyzed logs. Additionally, similar information needs can be expressed using multiple lexical choices. As a result, many highly popular terms co-occurring with a topic in user queries refer to the same aspect. Our experiments showed that term clustering techniques do not address these problems in a satisfactory manner. Thus, we examined an approach that employs three types of query-log-derived as-pects: self (specific to the topic), related (common across related topics), and general (common to all topics). To build the self aspect model for a topic, we first extract queries that contain that topic. Then, we select the most frequent n terms that occur in those queries after filtering out stop-words. 2 To generate the related aspect model for a topic, we sort all topics in our pool based on the similarity of their individual self aspect models to the self aspect model of the given topic. We then combine the self aspect models of top m ranked topics and select the top n terms from the com-bined model. Finally, we build a general aspect model by
Figure 3: Aspect models for  X  X om Cruise X ,  X  X l Gore X  combining the self aspect models of all the topics in our pool. 3 Since this model is generated only once, we manually filter out biographically-meaningless terms such as  X  X fficial page X  before selecting the top n of the remaining terms.
Figure 3 exemplifies these query-log-derived aspect models for the topics  X  X om Cruise X  and  X  X l Gore X .

To empirically determine suitable assignments for param-eters m and n , we tried a range of values on the develop-ment set and compared the self and related aspect models with the concepts occurring in Wikipedia pages. To perform the comparison, we employed the title pages of Wikipedia as our concept space and the anchor text of the Wikipedia links as the vocabulary for these concepts. The related aspects have both higher precision and higher recall of Wikipedia concepts than the self aspects. Increasing the number of as-pects in the models from 10 to 30 produces recall increases from 4% to 8% for the self model and from 6% to 10% for related aspects, for a relatively smaller precision trade-off (from 31% to 27% for self and from 38% to 28% for related). We now address the task of retrieving sentences from the Web for the aspects pertaining to a topic, and focus on grammaticallity, relevance, non-redundancy and diversity. To extract sentences from Web documents, we use a sim-ple html parser and a sentence boundary detector based on regular expressions and word casing statistics. This process often results in extracting ungrammatical sentences due to html misparsing, failure of the boundary detector to han-dle html/script content, or ungrammatical content such as blog postings and user responses to well written articles. To correct this, we used lexical indicators, language modeling, and perplexity features (as displayed in Table 1) to train a logistic regression classifier, which achieved more than 80% precision at 85% recall in identifying grammatical sentences. For an aspect vector, given a pool of candidate sentences, we have to select a subset that covers the aspects of interest, preferably one sentence per aspect. An approach that ranks all candidate sentences based on their similarity with the entire aspect vector (the Full Aspect method) is prone to se-lecting long and highly redundant sentences, which mention multiple aspects. A competing straight-forward approach is to select for each individual aspect one sentence that con-tains both the topic and the aspect. However, in addition to overlooking sentences with only pronominal references, this approach cannot ensure that the focus of the selected sen-tence is the targeted aspect. The mere presence of the topic and the aspect is not always an adequate indicator of rele-vance. For example, Table 2 shows four sentences extracted for the topic  X  X drien Brody X  and the aspect  X  X he Pianist X , of which the first two do not focus on the targeted aspect.
To identify sentences that focus on the connection be-tween the topic and an aspect, we make use of aspect-specific contexts , which rely on the observation that the contexts in which an aspect occurs often differ from the contexts of other aspects, as well as the overall topic context. Therefore, we first build an aspect-specific context vector for each aspect by extracting the terms in all sentences containing the topic and the aspect. These give more weight to words related to the aspect than to noisy words from sentences that have other foci (e.g., the context vector for  X  X drien Brody X  and  X  X he Pianist X  contains entries for  X  X scar X ,  X  X oman Polan-ski X ,  X  X on X ,  X  X ward X ,  X  X tar X , etc.) Then, we interpolate the aspect-specific context vector with the entire aspect vector. Finally, we compute the vectorial similarity of the candidate sentences with the corresponding context vector. An important feature of the proposed topic pages is the non-redundant coverage of aspects. Redundancy is caused mainly by the aspect models extracted from query logs and by sentences that cover multiple aspects. Using the rele-vance scores alone may lead to selecting sentences that share aspects and common vocabulary. To remove redundancy and promote selection of novel sentences, we can adopt the techniques used in novelty detection work [2], and gather iteratively a set of sentences by adding to the set a new sen-tence based on a linear combination of its relevance score and its novelty score with respect to the current set (the Novelty method). However, direct application of such a technique does not ensure the coverage of all the aspects of interest.
To enforce both diversity and novelty in the sentence se-lection process, we also investigated the following methods: Typical : For each aspect, extract sentences that contain the aspect and the topic, and build a context vector. Re-rank all sentences based on their cosine similarity to a linear in-terpolation of the aspect vector and the new context vector. Diversity : Starting with the full aspect vector, iteratively select one sentence that is most similar to the aspect vec-tor. After each selection, modify the aspect vector by down-weighting the aspects covered in the selected sentence by  X  . Repeat until the desired number of sentences are selected. Diversity+Typical (D-T) : Start with the full aspect vector. Select an aspect from the full aspect vector. Use the Typical method to get the best candidate sentence for the aspect. Then, remove all the aspects covered by the selected sen-tence. Repeat the process until no more aspects remain in the vector or the desired number of sentences are selected. Given a set of topics and their corresponding aspect mod-els, we conduct sentence selection experiments by employ-ing Wikipedia sentences. 4 We use both term-based and sentence-level metrics. To favor diversity, we also employ modified D-metrics , for which we allow each reference sen-tence to be matched by at most one of the selected sentences. During training we allow each method to learn its own weights for interpolating the self ( S ), related ( R ) and general ( G ) aspect models: A m =  X S +  X R + (1  X  (  X  +  X  )) G . Then the weighted aspect vector is trimmed to retain only the top n = 30 aspects, number which gives the best precision-recall trade-off in the Wikipedia-based evaluation. To establish an expectation for the range of values, we first compared Web biographies against Wikipedia. For 10 ran-dom development topics, we manually picked  X  X he best X  bi-ographical page from the top 10 Web search results. We then compared the sentences in those biographies against the sentences in Wikipedia. Figure 4(a) shows the results for this comparison. All sentence-level metrics indicate a lex-ical mismatch between the Web biographies and Wikipedia. Concept-based precision and recall measures are also low. We first determined the parameter values for each sentence selection method by using exhaustive grid search on the de-velopment set and then we compared the results.

The performance numbers for the automatic sentence se-lection methods (Figure 4(b)) are comparable to those ob-tained for the Web biographies. As expected, Full Aspect obtains poor performance on the diversity based measures. Novelty and Diversity, which use Full Aspect for initial rank-ing, do not provide substantial gains, due to the poor-quality initial ranking. For Diversity, removing aspects from the as-pect vector leads to poor quality sentences being retrieved as the context for ranking gets reduced. Typical and D-T outperform the other sentence selection methods across all measures. Typical focuses on retrieving the best possi-ble sentence for each aspect by leveraging the aspect spe-cific context. Conversely, D-T improves the concept-level precision and recall measures, as explicitly promoting di-versity improves D-recall. Even though the aspect vector is trimmed at each iteration, D-T is able to handle the re-duced context better than Diversity because it interpolates the aspect vector and the aspect-specific context vector.
Based on these findings, we chose D-T with empirically-best  X  = 0 . 5 and  X  = 0 . 25, for sentence selection in our final system. Also, we obtained the corresponding aspect models interpolation parameters as being: A m = 0 . 1 S +0 . 7 R +0 . 2 G . Once sentences pertaining to the important aspects of a given topic are collected, it is desirable that they are pre-sented in a coherent manner. Biographies typically follow the natural timeline of events in a person X  X  life. Because Wikipedia biographies usually obey this rule, we use them as training data to learn how to order biographical sen-tences. Our approach is to assign precedence scores for pairs of words based on Wikipedia evidence and then use these scores to order the sentences collected for each topic.
We first build a restricted vocabulary of words with at least 5 occurrences in the Wikipedia training set. For each (a) pair in this vocabulary, we count the number of times one word was used in a sentence preceding a sentence containing the other word. Then, we assign a precedence score for each pair of sentences by combining these word-based statistics. To produce the final ordering of sentences, we compute for each sentence an overall precedence score against all other sentences by aggregating the pairwise scores, and sort the sentences in the decreasing order of overall scores.
We experimented with both binary and frequency-based combination methods, with and without back-off, for sen-tence pairwise precedence, as well as two schemes (averaging and product of likelihood ratios) for overall scoring.
We evaluated our scoring methods on the 100 develop-ment topics, by extracting individual sentences from each corresponding Wikipedia articles and then attempting to re-cover the original ordering of those sentences. Despite their simplicity, our methods achieved high Spearman rank corre-lations for this task (0.55-0.65). In particular, we found the frequency-based pairwise scoring in conjunction with prod-uct of likelihood ratios to perform the best. The automatic evaluation of our final system on the test set shows results consistent with those obtained during devel-opment (Figures 4(c) and 4(b), respectively).

Additionally, we manually evaluated the aspect models and topic pages generated by the final system for a set of 20 random topics from our test collection (Table 3). We limited the number of sentences in the topic pages to 20 in order to match the size of typical Web search result pages. The evaluation was done independently by two annotators, who first read the Wikipedia page for each topic and ex-tracted a set of aspects covering personal life, and career facts. Subsequently, the aspect models and the generated topic pages were evaluated based on the knowledge gathered from Wikipedia. When the information retrieved appeared to be new, other Web sources were consulted. We opted for a 3-point scale { 0, 0.5, 1 } to limit subjectiveness but allow some degree of uncertainty and granularity of relevance.
For aspect models, we obtained an average precision of 0.33 for self aspects and 0.30 for related aspects. The anno-tator inter-agreement rate was 86%, with a Kappa of 0.66.
The topic summaries were evaluated on multiple dimen-sions, as follows: precision (is the information important to the topic and correct?), grammaticallity (is the infor-mation conveyed accurately?), non-redundancy (how much new information is conveyed on average by each sentence?), novel information versus Wikipedia (are facts not covered in Wikipedia presented?), and recall (how well the aspects from Wikipedia are covered in the summary?).

Table 4 shows the results obtained for the 20 test topics by macro-averaging the two annotators X  scores. The grammati-callity, non-redundancy, and novelty scores were aggregated only for sentences with a non-zero precision score. Most topics (14) scored over 0.5 in precision. We observed very good inter-annotator agreement, of 89% at sentence level, with a correspondingly high Kappa coefficient of 0.77. Pre-cision is much higher for topic pages than for the aspect models, which indicates that the retrieval/selection stage of our system is able to tolerate noise in the aspect models.
To verify that our system succeeds in differentiating itself from the current search approaches, we also performed a comparative evaluation with Bing. We first compared the generated topic pages and the search result pages globally, in terms of relevant information made available to the user. For 15 out of 20 topics, we strongly preferred the topic pages, in 4 cases, the information provided was comparable, while in one case, the search engine result page was more informative. We then judged the search engine results by using the same guidelines devised for topic pages. Our system substantially outperformed the Bing baseline on all metrics (Table 4). Table 3: List of topics used in the manual evaluation. Table 4: Macro-averaged performance for 20 test topics. We investigated the automatic generation of topic pages for biographical topics and we presented a general framework for this task. Our evaluation indicates the viability of automatic generation of topic pages as an alternative to the current search-based exploration of the Web.
