 Existing adaptive filtering systems learn user profiles based on users X  relevance judgments on documents. In some cases, users have some prior knowledge about what features are important for a document to be relevant. For example, a Spanish speaker may only want news written in Spanish, and thus a relevant document should contain the feature  X  X anguage: Spanish X ; a researcher working on HIV knows an article with the medical subject  X  X ubject: AIDS X  is very likely to be interesting to him/her.

Semi-structured documents with rich faceted metadata are increasingly prevalent over the Internet. Motivated by the commonly used faceted search interface in e-commerce, we study whether users X  prior knowledge about faceted fea-tures could be exploited for filtering semi-structured docu-ments. We envision two faceted feedback solicitation mech-anisms, and propose a novel user profile learning algorithm that can incorporate user feedback on features. To eval-uate the proposed work, we use two data sets from the TREC filtering track, and conduct a user study on Ama-zon Mechanical Turk. Our experimental results show that user feedback on faceted features is useful for filtering. The new user profile learning algorithm can effectively learn from user feedback on faceted features and performs better than several other methods adapted from the feature-based feed-back techniques proposed for retrieval and text classification tasks in previous work.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation
This work was done while this author was a visiting student at UC Santa Cruz Adaptive Filtering, Content-Based Filtering, User Feedback, Faceted Feedback, Labeled Features, Semi-structured Doc-uments, Document Facets
Information filtering systems process a document stream and recommend relevant documents to individual users. Existing filtering approaches are generally categorized into content-based filtering and collaborative filtering. This pa-per focuses on the content-based adaptive filtering. In content-based filtering, the system assumes documents with similar content to what a user liked before are likely to be rel-evant. In adaptive filtering, potentially relevant documents must be delivered immediately, thus the system has no time to accumulate and rank a set of documents as a traditional retrieval system does. An adaptive filtering system usually makes a binary decision to accept or reject a newly arrived document for each individual user.

A content-based filtering system maintains a user profile for each user to represent his/her information need(s). As-suming a user provides some examples of relevant documents initially, the user profile is created based on these examples and/or the initial user query. While filtering, the profile is updated based on periodic feedback from the user. Largely influenced by the TREC filtering track, almost all existing content-based filtering approaches learn user profiles based on user-labeled documents. The documents could be news, technical reports, emails, or messages. In commercial rec-ommender systems such as Amazon, eBay, etc., the labeled documents could be the descriptions of a set of user-rated items.

Similar to many other IR applications, a filtering sys-tem usually involves a large number of document features, including terms and facet-value pairs (such as  X  X uthor: Stephen Hawking X ), etc. Usually, only a limited number of features are useful for determining whether a document is relevant or not. In many cases, users have some prior knowledge about what features are important, especially if the semantic meanings of the features are clear. For exam-ple, a researcher interested in HIV has the knowledge that the facet-value pair  X  X eSH: AIDS X  is an important feature and the documents containing this feature are very likely to be relevant. Besides, users may want to put constraints on some facets of the delivered documents, such as format, authors, language, etc. For example, a Chinese reader may only want news articles written in Chinese.

In this paper, we explore how to exploit users X  prior knowl-edge about document features for the filtering task. Specifi-cally, we focus on the filtering of semi-structured documents, which usually contain a number of faceted features (i.e., facet-value pairs). There are three major reasons why we think user feedback on faceted features (i.e., faceted feed-back) is promising. First, users might be able to provide reliable feedback on document facets. Compared with iso-lated terms, common document facets including format, au-thors, language, topics, subjects, prices, genres, etc., usu-ally represent clear semantic concepts and thus are easier for users to understand. Similar to e-commerce users, fil-tering system users might be able to provide reliable feed-back on facet-value pairs. Second, semi-structured docu-ments with faceted features are increasingly prevalent. Due to several advantages offered by semi-structured documents, many publishers and information providers are creating doc-uments in structured or semi-structured format. The devel-opment of text mining techniques (classification, clustering, information extraction, etc.) has made it possible to cre-ate facets for text documents automatically. The Semantic Web effort, social tagging web sites, and the Open Direc-tory Project also provide a good way to create faceted fea-tures using the power of folksonomy. Besides, for certain document types (pictures, movies, products, etc.), faceted features are usually more accessible and informative than terms. Third, faceted search has gained great success in e-commerce over past years, and most popular online retail-ers, such as Amazon and eBay, now provide faceted search interfaces for buyers to narrow down products by putting constraints on a group of merchandise facets, such as cat-egory, price, brand, size, etc. This strengthens our belief that users are willing and able to give reliable feedback on faceted features in order to achieve a better experience.
To use faceted feedback for filtering, we need to answer three important research questions. First, how to select a small number of feature candidates for users to provide feed-back. There are usually a large number of facet-value pairs in the whole corpus and it is important not to overwhelm users with too many candidates. Considering that a fil-tering system may have collected some labeled documents over time, we propose a feature candidate selection method based on both labeled and unlabeled documents. Secondly, how to design the user interface to help users provide re-liable feedback. We envision two alternative user interac-tion mechanisms and compare their performances in this paper. Thirdly, given different types of feedback from the user including relevance feedback on documents and faceted feedback on features, how to learn the user profile. In this paper, we propose a semi-supervised user profile learning al-gorithm that can integrate two types of user feedback in a unified framework. We also implement some other meth-ods by adapting techniques proposed for retrieval and text classification tasks and compare our algorithm with these methods.

The major contributions of this paper include: 1) we eval-uate the usage of user feedback on faceted features for the filtering task and the experimental results show that faceted feedback is useful, especially in the cold-start scenario, where the filtering process starts with few or no relevant docu-ments; 2) we propose a user profile learning algorithm that can learn from user feedback on both instances and features. The experimental results show that this algorithm performs consistently well and seems more robust than some other methods used for retrieval and text classification tasks; and 3) we envision and compare two user interaction mecha-nisms for soliciting user feedback on faceted features and observe no significant difference with respect to filtering per-formances between these two mechanisms.
Previous research on content-based filtering is largely in-fluenced by the Filtering Track in TREC 4-11 [11] [12] [6] [7] [8] [19] [17] [18], where the task is to identify documents relevant to a specific topic from a document stream. Almost all research on content-based filtering is based on learning from labeled documents, and to our knowledge, this is the first work that aims to use user feedback on features for the filtering task.

Although using user feedback on features is not studied in the context of filtering, there is some related work about using feature feedback for retrieval and text-related learning tasks such as text classification.

Relevance feedback [20] has been shown to be an effective way to help retrieval systems improve retrieval performance. Besides the commonly used relevance feedback mechanism in which users are asked to judge whether a document is relevant or not, there has been some work on soliciting user feedback on document features. The term-based feedback mechanism, in which users are asked to identify relevant terms, has been studied by several researchers [5, 22, 2, 9, 10]. Recently, faceted feedback has been proposed for users to identify suitable faceted constraints on semi-structured documents to help improve retrieval performance [25].
There has been some recent interest in incorporating user-labeled features into text classification [14, 16, 15, 4]. Most research in this area involves asking users to label terms, and exploring how to learn a classifier from labeled terms. Liu et al. [14] ask human annotators to identify highly predic-tive terms from term clusters. The unlabeled instances are then soft-labeled according to their cosine similarity to the pseudo-instances that only contain user-identified features. Raghavan, Madani, and Jones [16] interleave user feedback on instances and features in a unified learning framework called tandem learning. Their experiments demonstrate that humans can provide accurate information about features, and that it takes one fifth as long to label features as to label instances. Raghavan and Allan [15] provide several methods for training SVMs with labeled features, including adjusting the parameters of labeled features, creating pseudo-instances that only contain labeled features, and soft-labeling unla-beled instances. Dayanic et al. [3] combine domain knowl-edge with training examples in a Bayesian framework. The domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model. Druck et al. [4] propose a semi-supervised learning algorithm that uses labeled features to constrain the model X  X  predictions on un-labeled instances based on generalized expectation criteria.
This paper differs from the prior work by focusing on a different task: adaptive information filtering. Some of the techniques we tried in this paper are motivated by the prior work. The new user profile learning algorithm proposed in this paper is motivated by [4], however, with significant dif-ferences. First, our algorithm is designed to incorporate two types of user feedback, that is, to learn from labeled in-stances and features simultaneously in order to fit the filter-ing task where users may provide mixed types of feedback. In our algorithm, we use a unified loss function to combine user feedback on both instances and features. Secondly, our model is designed to capture the sufficiency and necessity of user-labeled features. The assumption of our model is users can identify important features and an important fea-ture should have a high correlation with the document label. To measure this correlation, we propose the concepts of suf-ficiency and necessity and explicitly capture them in our algorithm.
In this paper, each metadata field of semi-structured docu-ments is called a document facet, such as  X  X ate X ,  X  X uthor X ,  X  X eople X ,  X  X ource X ,  X  X opic X ,  X  X ocation X , etc. A document may be assigned with one or several values on a particular facet. We call a facet (f) with a specific value (v) a facet-value pair (f: v) or a faceted feature. Examples of faceted features are  X  X ate: 2010-12-25 X ,  X  X uthor: Stephen Hawk-ing X ,  X  X egion: United States X , etc. Faceted features convey important information about documents which may not be clearly expressed in document texts, for example, the X  X ate X  of a news article. In some cases, this information is crucial in determining the relevance of a document. For example, a user may only want news on a topic reported recently rather than years ago. To help explore user information needs, it is reasonable to ask users for feedback on faceted features. In this paper,  X  X ser feedback on faceted features X  is sometimes called  X  X aceted feedback X  for short.
In a typical feature-based feedback mechanism, users are asked to identify  X  X elevant X  features from a group of candi-dates. However, the definition of  X  X elevance X  of a feature is usually not well-defined. Instead, we expect users to select features that are predictive of document labels ( relevant or non-relevant ), or from a mathematical point of view, the cor-relation between a relevant feature and the document label should be high. To help understand how users can identify the relevant features, we roughly categorize relevant features into the following two groups: 1. sufficient features :afeatureis sufficient if all doc-2. necessary features :afeatureis necessary if all rel-According to the definition of correlation in statistics, P ( y =1 | f =1)and P ( f =1 | y = 1) are the only two factors that account for the correlation between a feature and the document label.

We envision two interaction mechanisms for soliciting user feedback on features. In the first mechanism, the system Figure 1: Two types of user feedback in a filtering system asks users to identify  X  X elevant X  features from a group of feature candidates. In the second mechanism, the system asks users to specifically identify which features are likely to be sufficient and which are likely to be necessary. If the user thinks the existence of a faceted feature is strong evidence of a document being relevant, the feature is probably suf-ficient. For example, the facet-value pair  X  X eSH: AIDS X  is probably a sufficient feature for the researcher interested in HIV. On the other hand, if the user thinks a relevant doc-ument should meet some faceted constraint as specified by a faceted feature, the feature is probably necessary. For ex-ample, the facet-value pair  X  X anguage: Spanish X  is probably a necessary feature for the Spanish speaker who only wants news articles written in Spanish.
We envision a filtering system that integrates two types of user feedback: 1) relevance feedback on documents; and 2) user feedback on features (Figure 1). Whenever a new document arrives, the system determines whether to deliver it to a user based on how well this document matches the user profile. At any time, the system can suggest a set of probably relevant features based on the current user profile. Users can choose to provide relevance feedback on delivered documents or to identify relevant features from the feature candidates. Whenever the system receives any type of feed-back from the user, it updates the user X  X  profile accordingly.
When asking for feature-based feedback, it is important not to overwhelm users with too many feature candidates. In this section, we focus on how to select a small number of feature candidates. Intuitively, good feature candidates should: 1) have a high probability of being chosen by the user; and 2) provide substantial information for user profile learning. In this paper, we focus on the first aspect and leave the second to our future work.

There has been much previous work on feature selection for text classification [23], and most of the existing ap-proaches are based on the availability of labeled documents. However, in the scenario of filtering, the number of user-labeled documents is usually small or even zero, especially at the early stage of a filtering process, which is known as the cold-start problem [21]. Thus, we need a feature selection method that fits the filtering task better.

We propose a method for feature selection based on the current user profile, a set of user-labeled documents ( L ), and a set of unlabeled documents ( C ). The first step is to classify all unlabeled documents into a positive set C and a negative set C  X  according to the current user profile. Then existing feature selection methods can be adapted here based on the set of user-labeled relevant documents ( L + the set of user-labeled non-relevant documents ( L  X  ), the set of positively classified documents ( C + ), and the set of negatively classified documents ( C  X  ).

Motivated by the well known TF*IDF method, we use the following scoring function to rank features for feature selection: where N( f, L + ) is the number of relevant documents that contain feature f (similarly for N( f, C + )), IDF( f )isthe Inverse Document Frequency of feature f ,  X  and  X  are the corresponding weights. The intuition behind this ranking function is a feature occurring rarely in the whole corpus (thus has a high IDF) while frequently in the relevant ( L and probably relevant documents ( C + ) is highly predictive of the document label.
In a filtering system with feature-based feedback enabled, the user profile learning algorithm should be able to learn from user feedback on both instances and features simul-taneously. In this section, we introduce the Generalization Constraint Model (GCM), which incorporates user-labeled features as constraints on model generalization.
We define the notations that will be used later as follows:
The assumption of our model is a user selects a feature because this feature has a high sufficiency and/or necessity. To make use of user feedback on features, we need to model a feature X  X  sufficiency P ( y =1 | f = 1) and necessity P ( f = 1 | y =1).

Logistic regression has been shown to work well for adap-tive filtering [24]. We use logistic regression to model the probability of a document label ( y ) given the document vec-tor ( d i ) and the user profile (  X  ): Assume that the probability of a document X  X  label ( y )is independent with any feature ( f ) given the user profile ( Assume the probability of a document vector ( d i ) is inde-pendent with the user profile (  X  ) given a feature ( f ): Then the sufficiency of a feature given the user profile could be derived as follows: where | C f | denotes the total number of documents in C f P ( y | d i , X  ) can be calculated using the current user profile.
According to Bayes X  theorem, the necessity of a feature given the user profile could be derived as follows: where P ( f |  X  )= P ( f )since f and  X  are independent with each other. P ( f ) can be estimated according to the occur-rence number of f in the whole corpus.
A feature labeled as  X  X ecessary X  by the user should have a high necessity, and a feature labeled as  X  X ufficient X  should have a high sufficiency. To quantify the necessity and suf-ficiency of user-labeled features, we introduce two Bernoulli distributions as the reference distributions: T y | f and T For sufficient features, the distribution P ( y | f,  X  ) should be close to the distribution T y | f ; and for necessary features, the distribution P ( f | y,  X  ) should be close to the distribu-tion T f | y . We use KL divergence to measure the distances (Equation 6).
 The parameters of the reference distributions T y | f and T f | y could be tuned using a parameter tuning set. We did not use the special distribution T ( y =1 | f =1)=1forsuf-ficient features and the special distribution T ( f =1 | y = 1) = 1 for necessary features since users usually do not have enough knowledge to accurately distinguish if a fea-ture is exactly sufficient/necessary or not. While tuning the parameters in our experiments, we found these special dis-tributions are far from optimal and the optimal values of T ( f =1 | y =1)and T ( y =1 | f = 1) tend to be relatively low.

The parameters of the reference distributions should be facet-dependent, since the reliability of user feedback on dif-ferent facets may differ significantly. Some facets, such as  X  X ime X ,  X  X ocation X , and  X  X eople X , represent very clear con-cepts and are easy for users to understand. While some other facets, such as  X  X opic X , usually do not have a clear definition and different users may disagree on what values a document should have on these facets. User feedback on the first class of facets is usually more credible than that on the second class. Thus we may want to use different reference distribu-tions for features of different facets. In our experiments, the parameters of reference distributions of different facets are tuned on a parameter tuning set, and we find the optimal reference distributions for different facets are significantly different.
We propose to use a unified loss function to combine user feedback on both instances and features. Given user-labeled documents L , user-identified sufficient features F s , and user-identified necessary features F n , the loss function is: where the first item corresponds to user feedback on docu-ments, the second and third items correspond to user feed-back on features, and the fourth item handles regularization.  X  ,  X  2 ,  X  3 ,and  X  4 are pre-set parameters that could be tuned on the parameter tuning set.

The user profile  X   X  can be obtained by minimizing the loss function:
Gradient-based optimization algorithms, such as conju-gate gradient descent, could be used to find the optimal user profile  X   X  .
We design a series of experiments to evaluate the pro-posed ideas and methods. Specifically, our experiments are designed to answer the following questions:
To answer the first and second questions, we conduct a user study on Amazon Mechanical Turk to collect user feed-back on faceted features. Then we run adaptive filtering experiments using the proposed user profile learning algo-rithm and see whether filtering performances could be im-proved compared with no faceted feedback. To answer the third question, we design two user interfaces with different tasks: one is to ask users to select relevant features and the other is to ask users to select necessary and sufficient fea-tures respectively. To answer the fourth question, we imple-ment several methods originally proposed for learning from labeled features for search or text classification task, and compare them with the proposed algorithm on the adaptive filtering task.
We use two data sets from the TREC filtering track for the adaptive filtering experiments. 1
The OHSUMED data set is used in the TREC 2000 filtering track [19]. This data set consists of a medical cor-pus, 63 topics (information needs), and the corresponding document relevance judgments. The corpus contains a to-tal of 348,566 medical articles selected from a subset of 270 medical journals covering years from 1987 to 1991. Each document of this corpus has some metadata fields including MeSH (Medical Subject Headings), Author, Date, etc., from which we can create faceted features. In our experiments, we chose to use the MeSH field, which is perhaps the most informative metadata field for the information needs in this data set.

The RCV1 data set is used in the TREC 2002 filtering track [18]. We only use the first 50 topics of this track to simulate user information needs 2 . The RCV1 (Reuters Cor-pus Volume 1) corpus [13] contains about 810,000 Reuters news stories published from 1996-08-20 to 1997-08-19. Each document of this corpus has some metadata, and we choose to use three metadata fields (Topic, geographical Region, and Industry) to create faceted features.
The filtering settings in our experiments are similar to those of the adaptive filtering task in the TREC filtering track, however, there are some changes. For each user, the filtering system starts with an initial query, some (or zero) relevant document samples, and a set of unlabeled docu-ments for training. Before the filtering process starts, the user is asked to provide the first-round faceted feedback. Then the system starts filtering the testing documents in the order of document publishing date. During the filter-ing process, relevance judgments on delivered documents are available for user profile learning in order to simulate
We are not using recommendation data sets like MovieLens, where faceted features are available as well, since it is hard to collect user feedback on faceted features due to the lack of well-defined user information needs.
The prior research shows that the other topics do not match real user information needs well. users X  immediate relevance feedback on documents. If one-third of the testing documents are processed and at least two documents have been delivered, the system will present the second-round feature candidates to the user for faceted feed-back refinement and incorporate the user X  X  faceted feedback immediately. By setting the second-round user interaction, we want to evaluate whether users are able to improve the quality of their feedback during the filtering process.
The user profile is updated whenever some user feedback is available. We also change the number of initially known relevant documents to see if faceted feedback is more useful when fewer relevant documents are initially known and if it is no longer useful when more relevant documents are available.
To evaluate whether real users are able to provide use-ful feedback on features for the filtering task, we conduct a user study on Amazon Mechanical Turk. Mechanical Turk is an online marketplace for work, where requesters can pub-lish tasks that require human intelligence and workers can choose to work on the tasks to get paid. Researchers have compared TREC assessors with Mechanical Turk workers, and demonstrated that Mechanical Turk workers are a good source for IR evaluation [1].

In our user study, Mechanical Turk workers are recruited to act as real filtering users and provide faceted feedback. To avoid careless workers and ensure the quality of the study, we restrict the qualified workers who can work on our tasks to those in the United States 3 and have an approval rate of over 95% and more than 50 approved submissions on Me-chanical Turk. Since the OHSUMED data set contains a lot of medical terms, we require that workers have common sense in medicine in order to be qualified to work on this data set.
 We design two tasks for each query: the first task ( Task I ) is to ask the user to select  X  X elevant X  features, and the second ( Task II ) is to ask users to select  X  X ecessary X  and  X  X ufficient X  features respectively. Equation 1 is used to rank features and we keep the top 20 ones as feature candidates. For each individual query, we recruit ten workers with half of them working on the first task and half of them on the second task. The users working on the first task are only asked to provide one round of feedback, and users working on the second task are asked to provide two rounds of feedback. For the first-round feedback, the topic statement (including Title, Description, and Narrative) and a group of feature candidates (we use 10 in our experiments) are shown to the user; and for the second-round feedback, a set of delivered documents are additionally shown to help users refine their feedback. In Task II, users were given the explanations of sufficient and necessary features (including examples of these two types of features). For each task, results of all five users are used and the average performance will be reported.
For each document vector, we use term features, faceted features, along with a dummy variable always equal to 1. Specifically, we use the following formula to compute a doc-
This limits the users to be native English speakers or those familiar with English. ument vector d : d ( i )= tf( i, d )
In Equation 9, tf( i, d ) is the frequency of feature i in doc-ument d . For a faceted feature, we assume its frequency is 1 if it occurs in a document, otherwise 0. df( i ) is the document frequency of feature i , N is the total number of documents. At any time of the filtering process, only documents that have been processed are considered for the computation of all statistics in Equation 9.

Both term features and faceted features are used in user profiles. To ensure a number of faceted features are kept in the user profile, we select term features and faceted features separately. For each user profile, we allow the maximum number of term features to be 30 and the maximum number of faceted features to be 10. We use the Rocchio method [20] to determine which features will be kept in the user profile. For faceted features, we assume user-labeled faceted features are contained in the original query when applying the Rocchio method.
In the TREC-9, TREC-10 and TREC-11 filtering tracks, the following utility function was used [18]: where R + is the number of relevant documents delivered, and N + is the number of non-relevant documents delivered. A normalized version of T11SU was also used in TREC-11: where MaxU = 2  X  ( R + + R  X  ) is the maximum possible utility and MinNU =  X  0 . 5.

We use T11SU as the major evaluation measure, and all algorithms are designed to optimize this measure (if appli-cable). We will also report the results on T11U ,Macro-Precision ,andMacro-Recall .
For each data set, we split the query topics into two equal-size sets for parameter tuning and testing respectively. The parameters of all reference distributions and  X  1 , X  2 , X  in Equation 7 are tuned on the parameter tuning set by maximizing the metric T11SU. For simplicity, we manually set  X  =2 , X  = 1 in Equation 1 to give a higher weight to the frequency in user-labeled relevant documents ( L + ).
The performances with and without faceted feedback are compared in Table 1. For each data set, we tried several runs starting with 0/1/2 relevant documents respectively. The baseline runs ( X  X Docs = 0/1/2 X ) learn user profiles from only relevance judgments on documents using Norm-2 reg-ularized logistic regression. To ensure the baseline methods are well implemented, we compared the performances of the baseline runs with those reported in previous work [26, 19, 18], and found our performances are comparable to them. For each run with faceted feedback ( X  X ith FFb X ), the Gen-eralization Constraint Model is used to learn user profiles from both relevance judgments on documents and user feed-back on faceted features. For runs included in Table 1, we use two rounds of user feedback collected in Task II, where users are asked to select necessary and sufficient features respectively.
 According to Table 1, we have the following findings: 1) Faceted feedback can help improve filtering perfor-mances. We find filtering performances are improved on both data sets when using faceted feedback. 2) Faceted feedback is more valuable when fewer relevant docu-ments are initially available. All measures are improved significantly 4 when no relevant document is initially avail-able on OHSUMED. Most measures are improved signifi-cantly when one relevant document is initially available on OHSUMED and zero or one relevant document is initially available on RCV1. On both data sets, only slight improve-ments are observed for most measures when starting with two relevant documents. This is not surprising since faceted feedback can provide less additional information when more relevant documents are initially available.
 Table 1: Adaptive filtering performances with and without faceted feedback. (Docs/Q) is the average number of delivered documents for each user profile.  X   X   X  indicates a statistically significant improvement over the corresponding baseline.

Setting T11SU T11U Prec Rec Docs/Q rDocs: 0 0.335 1.36 0.193 0.092 8.4 with FFb 0.371  X  6.68  X  0.322  X  0.185  X  16.3 rDocs: 1 0.358 3.81 0.271 0.190 18.4 with FFb 0.371 7.90  X  0.339  X  0.255  X  23.4 rDocs: 2 0.359 7.97 0.300 0.288 27.6 with FFb 0.370 6.68 0.349  X  0.303 25.6
Setting T11SU T11U Prec Rec Docs/Q rDocs: 0 0.379 11.92 0.315 0.149 17.5 with FFb 0.415  X  29.24 0.389 0.232  X  31.8 rDocs: 1 0.445 29.28 0.367 0.275 32.4 with FFb 0.481  X  42.40 0.455  X  0.352  X  42.1 rDocs: 2 0.502 48.60 0.483 0.389 47.9 with FFb 0.504 50.28 0.504 0.404 50.5
There have been some methods for learning from labeled features proposed in previous work. We adapt some of them to the adaptive filtering task and compare them with the proposed Generalization Constraint Model (GCM). These methods include:
We use t-tests with threshold p-value 0.05 for all signifi-cance tests in this paper.
For all above methods and our method (GCM), user feed-back collected in Task I is used; no relevant documents are initially known; and the Norm-2 regularized logistic regres-sion is used as the underlying filtering algorithm, if nec-essary. For our method (GCM), we assume user-selected features are both necessary and sufficient.
Table 2 compares the performances of different meth-ods. Although widely used in the e-commerce domain, the Boolean models (BOOL(A) and BOOL(O)) do not work well on both data sets. No significant improvement is achieved and Recall is hurt significantly on RCV1. This result is consistent with the findings reported in [25] for document retrieval task. Using user feedback for feature selection (FS) improves the performances, but not significantly. Us-ing user feedback as a pseudo-relevant document (Pseudo-D) does not work well on both data sets and hurts the measure we are focusing on (T11SU). This is not surpris-ing in our experimental settings: we use both term and faceted features while the pseudo-relevant document con-tains only faceted features. Conversely, we can understand why  X  X seudo-Q X  performs better, though no significant im-provement is observed on RCV1. The  X  X rior X  method sig-nificantly improves Recall, but the improvements on other measures are not significant. The Generalized Expectation Criteria (GEC) works well on RCV1, however, not signifi-cantly better than the baseline on OHSUMED. This is prob-ably because OHSUMED has fewer relevant documents and feature necessity (which is not captured by GEC) is more important for a document to be relevant on this data set. According to Table 2, most existing methods do not per-form consistently better than the baseline on the filtering data sets. Encouragingly, our model (GCM) significantly outperforms the baseline on both data sets.
 Table 2: Adaptive filtering performances using dif-ferent user profile learning algorithms. T11SU is the measure all algorithms try to optimize (if appli-cable).
 Algrthm T11SU T11U Prec Rec Docs/Q Baseline 0.335 1.36 0.193 0.092 8.4 BOOL(A) 0.348 1.61 0.796  X  0.032 2.1 BOOL(O) 0.335 1.32 0.219 0.079 7.1 FS 0.339 3.23 0.226 0.106 12.0 Pseudo-D 0.302  X  2.71 0.221 0.200  X  23.9 Pseudo-Q 0.362  X  4.81  X  0.278  X  0.160  X  14.0 Prior 0.344 7.58 0.220 0.166  X  19.3 GEC 0.341 3.61 0.233 0.081 9.7 GCM 0.363  X  6.13  X  0.275  X  0.156  X  14.4 Algrthm T11SU T11U Prec Rec Docs/Q Baseline 0.379 11.92 0.315 0.149 17.5 BOOL(A) 0.351 4.16 0.579  X  0.048  X  4.7 BOOL(O) 0.388 15.64 0.362 0.155 17.5 FS 0.386 14.36 0.315 0.167 20.4 Pseudo-D 0.365 24.68 0.286 0.235  X  40.0 Pseudo-Q 0.397 23.60 0.360 0.187 25.6 Prior 0.414 28.88 0.357 0.240  X  32.3 GEC 0.409  X  24.80 0.351 0.223  X  30.5
GCM 0.413  X  27.44 0.395  X  0.215  X  29.3
Two user interaction mechanisms (used in Task I and II respectively) are compared and the filtering performances using two mechanisms are reported in Table 3. The run  X  X el X  corresponds to Task I in which users are asked to se-lect relevant features and only the first-round user feedback is used;  X  X S(1r) X  corresponds to Task II in which users are asked to select sufficient and necessary features respectively and only the first-round feedback is used; and  X  X S(1&amp;2r) X  uses both two rounds of user feedback collected in Task II. All runs start with zero relevant documents. The General-ization Constraint Model (GCM) is used for all runs except the X  X aseline X . For the run X  X el X , we assume all user-selected features are both necessary and sufficient when applying the GCM.

Table 3 shows that users can provide useful feedback with both interaction mechanisms. It is somewhat surprising that there is no significant difference between X  X el X  X nd X  X S(1r) X . Table 4 shows two query examples with two users X  feedback collected in Task I and II respectively. In general, the nec-essary features the user selected look reasonable; however, the sufficient features are not. This is probably because very few facet-value pairs are sufficient (or approximately suffi-cient) on our data sets (especially on RCV1), while users tend to choose some results so their results are not rejected on Mechanical Turk.

We also compared  X  X S(1r) X  and  X  X S(1&amp;2r) X  to see if the quality of user feedback can be improved during the filtering process. However, we did not observe significant improve-ment on  X  X S(1&amp;2r) X  over  X  X S(1r) X . There are two possi-ble explanations: 1) the additional information provided to users does not help them improve feedback quality signifi-cantly; or 2) faceted feedback is no longer useful when a few documents have been labeled.
 Table 3: Adaptive filtering performances using dif-ferent user interaction mechanisms. Baseline: no faceted feedback used; Rel: faceted feedback col-lected in Task I is used; NS(1r): 1st round faceted feedback collected in Task II is used; NS(1&amp;2r): two rounds of faceted feedback collected in Task II are used.  X   X   X  indicates a significant improvement over the  X  X aseline X . No significant difference is ob-served between  X  X el X  and  X  X S(1r) X , and  X  X S(1r) X  and  X  X S(1&amp;2r) X .
 Setting T11SU T11U Prec Rec Docs/Q Baseline 0.335 1.36 0.193 0.092 8.4 Rel 0.363  X  6.13  X  0.275  X  0.156  X  14.4 NS(1r) 0.366  X  5.35  X  0.314  X  0.186  X  14.9 NS(1&amp;2r) 0.371  X  6.68  X  0.322  X  0.185  X  16.3 Setting T11SU T11U Prec Rec Docs/Q Baseline 0.379 11.92 0.315 0.149 17.5 Rel 0.413  X  27.44 0.395  X  0.215  X  29.3 NS(1r) 0.409  X  23.08 0.352 0.213  X  27.8
NS(1&amp;2r) 0.415  X  29.24 0.389 0.232  X  31.8
Users of filtering systems might be willing to interact with the system and provide some feedback in order to gain a Table 4: Examples of faceted feedback. Rel: a user X  X  faceted feedback in Task I; NS: a user X  X  2nd round faceted feedback in Task II. N: necessary features; S: sufficient features.
 Title : 35 yo with advanced metastatic breast cancer
Description : chemotherapy advanced for advanced metastatic breast cancer Rel NS Title : Tourism Great Britain
Description : Retrieve documents pertaining to tourism into Great Britain and the efforts being un-dertaken to increase it.
 Rel NS better long-term experience. Existing content-based adap-tive filtering systems learn user profiles mainly based on users X  relevance feedback on documents. We propose to exploit user feedback on faceted features for filtering semi-structured documents. We propose a feature candidate se-lection method fitting to the filtering task, and a user pro-file learning algorithm that can incorporate user feedback on both instances and features.
 We evaluate our work on two filtering data sets from the TREC filtering track and conduct a user study to collect faceted feedback on Amazon Mechanical Turk. The experi-mental results show that user feedback on faceted features is useful for filtering, especially in the cold-start settings that few or no relevant documents are provided before the filter-ing process starts. The Generalization Constraint Model we proposed is a semi-supervised learning algorithm and can ex-plicitly model the two key factors (necessity and sufficiency) that account for the correlation between a user-labeled fea-ture and the document label. The experimental results show that GCM performs consistently well on two data sets and seems more robust than several other methods. We also compared two user interaction mechanisms for soliciting user feedback on faceted features and found no significant differ-ence with respect to filtering performances. It is also ob-served that user feedback refinement in our experiments is not quite useful.

This is the first step to exploiting faceted feedback for fil-tering semi-structured documents, and the techniques pro-posed are far from optimal. The feature candidate selection method used in this paper focuses on selecting features with a high probability of being chosen by the user. However, a good feature candidate should also bring as many learning benefits if its label is known. In our future work, we will explore active learning techniques for faceted feature candi-date selection. Also, we will pay attention to the specialties of faceted features. For example, features on different doc-ument facets are not equally informative for a particular information need. In this paper, we manually choose impor-tant metadata fields (facets) for our experiments, and it is still an open question how to automatically identify useful facets for users to provide feedback.

How to use feedback on features is an important ques-tion in retrieval, text classification and filtering, and this problem has not been well researched. Existing work in this direction mainly uses some simple approaches. For example, adjusting the weights of user-labeled features using heuris-tics, converting labeled features to pseudo-labeled instances, etc. The Generalization Constraint Model proposed in this paper performs well on the filtering task, and can also be adapted for retrieval and text classification tasks in the fu-ture.

Terms are the dominating features on the data sets used in this paper, and we expect that faceted feedback will be even more useful in applications where faceted features are dominating, such as product/coupon/discount email alerts sent from Groupon.com. Evaluation on these applications is a direction we will be going with our future work. This work was funded by National Science Foundation IIS-0713111, IIS-0953908, UCSC/LANL ISSDM, and China Scholarship Council. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors X , and do not necessarily reflect those of the sponsors. [1] O. Alonso and S. Mizzaro. Can we get rid of trec [2] P. Anick. Using terminological feedback for web search [3] A.Dayanik,D.D.Lewis,D.Madigan,V.Menkov, [4] G. Druck, G. Mann, and A. McCallum. Learning from [5] D. Harman. Towards interactive query expansion. In [6] D. A. Hull. The trec-6 filtering track: Description and [7] D. A. Hull. The trec-7 filtering track: description and [8] D. A. Hull and S. Robertson. The trec-8 filtering track [9] D. Kelly and X. Fu. Elicitation of term relevance [10] D. Kelly, K. Gyllstrom, and E. W. Bailey. A [11] D. D. Lewis. The trec-4 filtering track. In The 4th [12] D. D. Lewis. The trec-5 filtering track. In The Fifth [13] D. D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A [14] B. Liu, X. Li, W. S. Lee, and P. S. Yu. Text [15] H. Raghavan and J. Allan. An interactive algorithm [16] H. Raghavan, O. Madani, and R. Jones. Active [17] S. Robertson and I. Soboroff. The trec 2001 filtering [18] S. Robertson and I. Soboroff. The trec 2002 filtering [19] S. E. Robertson and D. A. Hull. The trec-9 filtering [20] J. J. Rocchio. Relevance feedback in information [21] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [22] B. Tan, A. Velivelli, H. Fang, and C. Zhai. Term [23] Y. Yang and J. O. Pedersen. A comparative study on [24] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness [25] L. Zhang and Y. Zhang. Interactive retrieval based on [26] Y. Zhang. Using bayesian priors to combine classifiers
