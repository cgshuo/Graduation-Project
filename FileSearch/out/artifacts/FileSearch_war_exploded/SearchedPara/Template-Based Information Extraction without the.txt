 A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument). In contrast to work in relation discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation dis-covery, most template-based IE approaches assume foreknowledge of the domain X  X  templates. Very little work addresses how to learn the template structure itself. Our goal in this paper is to perform the stan-dard template filling task, but to first automatically induce the templates from an unlabeled corpus.
There are many ways to represent events, rang-ing from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowl-edge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system:
A semantic role, such as target , is a cluster of syn-tactic functions of the template X  X  event words (e.g., the objects of detonate and explode ). Our goal is to characterize a domain by learning this template structure completely automatically. We learn tem-plates by first clustering event words based on their proximity in a training corpus. We then use a novel approach to role induction that clusters the syntactic functions of these events based on selectional prefer-ences and coreferring arguments. The induced roles are template-specific (e.g., perpetrator), not univer-sal (e.g., agent or patient) or verb-specific.
After learning a domain X  X  template schemas, we perform the standard IE task of role filling from in-dividual documents, for example:
This extraction stage identifies entities using the learned syntactic functions of our roles. We evalu-ate on the MUC-4 terrorism corpus with results ap-proaching those of supervised systems.

The core of this paper focuses on how to char-acterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus X  size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by eval-uations against previous work in section 6 and 7. Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classi-fiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples X  surround-ing context for features such as nearby tokens, doc-ument position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supple-mented labeled with unlabeled data.

Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning ( PERSON won ) to ap-proximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yan-garber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a cor-pus of unknown events and unclustered documents, without seed examples.

Shinyama and Sekine (2006) describe an ap-proach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled docu-ments, and extract their fillers. Central to the al-gorithm is collecting multiple documents describ-ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same proper nouns. Learned patterns represent binary relations, and they show how to construct tables of extracted entities for these rela-tions. Our approach draws on this idea of using un-labeled documents to discover relations in text, and of defining semantic roles by sets of entities. How-ever, the limitations to their approach are that (1) redundant documents about specific events are re-quired, (2) relations are binary, and (3) only slots with named entities are learned. We will extend their work by showing how to learn without these assumptions, obviating the need for redundant doc-uments, and learning templates with any type and any number of slots.

Large-scale learning of scripts and narrative schemas also captures template-like knowledge from unlabeled text (Chambers and Jurafsky, 2008; Kasch and Oates, 2010). Scripts are sets of re-lated event words and semantic roles learned by linking syntactic functions with coreferring argu-ments. While they learn interesting event structure, the structures are limited to frequent topics in a large corpus. We borrow ideas from this work as well, but our goal is to instead characterize a specific domain with limited data. Further, we are the first to apply this knowledge to the IE task of filling in template mentions in documents.

In summary, our work extends previous work on unsupervised IE in a number of ways. We are the first to learn MUC-4 templates, and we are the first to extract entities without knowing how many tem-plates exist, without examples of slot fillers, and without event-clustered documents. Our goal is to learn the general event structure of a domain, and then extract the instances of each learned event. In order to measure performance in both tasks (learning structure and extracting in-stances), we use the terrorism corpus of MUC-4 (Sundheim, 1991) as our target domain. This cor-pus was chosen because it is annotated with tem-plates that describe all of the entities involved in each event. An example snippet from a bombing document is given here:
The entities from this document fill the following slots in a MUC-4 bombing template.

We focus on these four string-based slots 1 from the MUC-4 corpus, as is standard in this task. The corpus consists of 1300 documents, 733 of which are labeled with at least one template. There are six types of templates, but only four are modestly fre-quent: bombing (208 docs), kidnap (83 docs), attack (479 docs), and arson (40 docs). 567 documents do not have any templates. Our learning algorithm does not know which documents contain (or do not con-tain) which templates. After learning event words that represent templates, we induce their slots, not knowing a priori how many there are, and then fill them in by extracting entities as in the standard task. In our example above, the three bold verbs (use, at-tack, wound) indicate the Bombing template, and their syntactic arguments fill its slots. Our goal is to learn templates that characterize a domain as described in unclustered, unlabeled doc-uments. This presents a two-fold problem to the learner: it does not know how many events exist, and it does not know which documents describe which event (some may describe multiple events). We ap-proach this problem with a three step process: (1) cluster the domain X  X  event patterns to approximate the template topics, (2) build a new corpus specific to each cluster by retrieving documents from a larger unrelated corpus, (3) induce each template X  X  slots using its new (larger) corpus of documents. 4.1 Clustering Events to Learn Templates We cluster event patterns to create templates. An event pattern is either (1) a verb, (2) a noun in Word-Net under the Event synset, or (3) a verb and the head word of its syntactic object. Examples of each include (1)  X  X xplode X , (2)  X  X xplosion X , and (3)  X  X x-plode:bomb X . We also tag the corpus with an NER system and allow patterns to include named entity types, e.g.,  X  X idnap:PERSON X . These patterns are crucially needed later to learn a template X  X  slots. However, we first need an algorithm to cluster these patterns to learn the domain X  X  core events. We con-sider two unsupervised algorithms: Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and agglomer-ative clustering based on word distance. 4.1.1 LDA for Unknown Data LDA is a probabilistic model that treats documents as mixtures of topics. It learns topics as discrete distributions (multinomials) over the event patterns, and thus meets our needs as it clusters patterns based on co-occurrence in documents. The algorithm re-quires the number of topics to be known ahead of time, but in practice this number is set relatively high and the resulting topics are still useful. Our best per-forming LDA model used 200 topics. We had mixed success with LDA though, and ultimately found our next approach performed slightly better on the doc-ument classification evaluation. 4.1.2 Clustering on Event Distance Agglomerative clustering does not require fore-knowledge of the templates, but its success relies on how event pattern similarity is determined.

Ideally, we want to learn that detonate and destroy belong in the same cluster representing a bombing. Vector-based approaches are often adopted to rep-resent words as feature vectors and compute their distance with cosine similarity. Unfortunately, these approaches typically learn clusters of synonymous words that can miss detonate and destroy. Our goal is to instead capture world knowledge of co-occuring events. We thus adopt an assumption that closeness in the world is reflected by closeness in a text X  X  discourse. We hypothesize that two patterns are related if they occur near each other in a docu-ment more often than chance.

Let g ( w i , w j ) be the distance between two events ( 1 if in the same sentence, 2 in neighboring, etc). Let C dist ( w i , w j ) be the distance-weighted frequency of two events occurring together: where d is a document in the set of all documents D . The base 4 logarithm discounts neighboring sen-tences by 0 . 5 and within the same sentence scores 1 . Using this definition of distance, pointwise mutual information measures our similarity of two events: pmi ( w i , w j ) = P dist ( w i , w j ) / ( P ( w i ) P ( w
We run agglomerative clustering with pmi over all event patterns. Merging decisions use the average link score between all new links across two clusters. As with all clustering algorithms, a stopping crite-rion is needed. We continue merging clusters un-til any single cluster grows beyond m patterns. We briefly inspected the clustering process and chose m = 40 to prevent learned scenarios from intuitively growing too large and ambiguous. Post-evaluation analysis shows that this value has wide flexibility. For example, the Kidnap and Arson clusters are un-changed in 30 &lt; m &lt; 80 , and Bombing unchanged in 30 &lt; m &lt; 50 . Figure 1 shows 3 clusters (of 77 learned) that characterize the main template types. 4.2 Information Retrieval for Templates Learning a domain often suffers from a lack of train-ing data. The previous section clustered events from the MUC-4 corpus, but its 1300 documents do not provide enough examples of verbs and argument counts to further learn the semantic roles in each cluster. Our solution is to assemble a larger IR-corpus of documents for each cluster. For exam-ple, MUC-4 labels 83 documents with Kidnap, but our learned cluster ( kidnap , abduct , release , ...) re-trieved 3954 documents from a general corpus.
We use the Associated Press and New York Times sections of the Gigaword Corpus (Graff, 2002) as our general corpus. These sections include approxi-mately 3 . 5 million news articles spanning 12 years.
Our retrieval algorithm retrieves documents that score highly with a cluster X  X  tokens. The docu-ment score is defined by two common metrics: word match, and word coverage. A document X  X  match score is defined as the average number of times the words in cluster c appear in document d :
We define word coverage as the number of seen cluster words. Coverage penalizes documents that score highly by repeating a single cluster word a lot. We only score a document if its coverage, cvg ( d, c ) , is at least 3 words (or less for tiny clusters):
A document d is retrieved for a cluster c if ir ( d, c ) &gt; 0 . 4 . Finally, we emphasize precision by pruning away 50% of a cluster X  X  retrieved doc-uments that are farthest in distance from the mean document of the retrieved set. Distance is the co-sine similarity between bag-of-words vector repre-sentations. The confidence value of 0 . 4 was chosen from a manual inspection among a single cluster X  X  retrieved documents. Pruning 50% was arbitrarily chosen to improve precision, and we did not exper-iment with other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and re-trieved an IR-corpus for each cluster, we now ad-dress the problem of inducing semantic roles . Our learned roles will then extract entities in the next sec-tion and we will evaluate their per-role accuracy.
Most work on unsupervised role induction fo-cuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Juraf-sky, 2009) learned situation-specific roles over nar-rative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic rela-tions of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vector-based approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syn-tactic relations R C of its words. Consider the fol-lowing example: where verb:s is the verb X  X  subject, :o the object, and p in a preposition. We ideally want to cluster R C as:
We want to cluster all subjects, objects, and prepositions. Passive voice is normalized to active 2 .
We adopt two views of relation similarity: coreferring arguments and selectional preferences. Chambers and Jurafsky (2008) observed that core-ferring arguments suggest a semantic relation be-tween two predicates. In the sentence, he ran and then he fell , the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role. We applied this idea to a new vec-tor similarity framework. We represent a relation as a vector of all relations with which their argu-ments coreferred. For instance, arguments of the relation go off:s were seen coreferring with men-tions in plant:o , set off:o and injure:s . We represent go off:s as a vector of these relation counts, calling this its coref vector representation .

Selectional preferences (SPs) are also useful in measuring similarity (Erk and Pado, 2008). A re-lation can be represented as a vector of its observed arguments during training. The SPs for go off:s in our data include { bomb, device, charge, explosion } .
We measure similarity using cosine similarity be-tween the vectors in both approaches. However, coreference and SPs measure different types of sim-ilarity. Coreference is a looser narrative similarity (bombings cause injuries), while SPs capture syn-onymy (plant and place have similar arguments). We observed that many narrative relations are not syn-onymous, and vice versa. We thus take the max-imum of either cosine score as our final similarity metric between two relations. We then back off to the average of the two cosine scores if the max is not confident (less than 0 . 7 ); the average penalizes the pair. We chose the value of 0 . 7 from a grid search to optimize extraction results on the training set. 4.3.2 Clustering Syntactic Functions We use agglomerative clustering with the above pairwise similarity metric. Cluster similarity is the average link score over all new links crossing two clusters. We include the following sparsity penalty r ( c a , c b ) if there are too few links between clusters c a and c b . This penalizes clusters from merging when they share only a few high scoring edges. Clustering stops when the merged cluster scores drop below a threshold optimized to extraction performance on the training data.

We also begin with two assumptions about syntac-tic functions and semantic roles. The first assumes that the subject and object of a verb carry different semantic roles. For instance, the subject of sell fills a different role (Seller) than the object (Good). The second assumption is that each semantic role has a high-level entity type. For instance, the subject of sell is a Person or Organization, and the object is a Physical Object.

We implement the first assumption as a constraint in the clustering algorithm, preventing two clusters from merging if their union contains the same verb X  X  subject and object.

We implement the second assumption by auto-matically labeling each syntactic function with a role type based on its observed arguments. The role types are broad general classes: Person/Org , Physical Ob-ject , or Other . A syntactic function is labeled as a class if 20% of its arguments appear under the cor-responding WordNet synset 3 , or if the NER system labels them as such. Once labeled by type, we sep-arately cluster the syntactic functions for each role type. For instance, Person functions are clustered separate from Physical Object functions. Figure 2 shows some of the resulting roles.

Finally, since agglomerative clustering makes hard decisions, related events to a template may have been excluded in the initial event clustering stage. To address this problem, we identify the 200 nearby events to each event cluster. These are simply the top scoring event patterns with the cluster X  X  original events. We add their syntactic functions to their best matching roles. This expands the coverage of each learned role. Varying the 200 amount does not lead to wide variation in extraction performance. Once induced, the roles are evaluated by their entity ex-traction performance in Section 5. 4.4 Template Evaluation We now compare our learned templates to those hand-created by human annotators for the MUC-4 terrorism corpus. The corpus contains 6 template Perpetrator x x x x Victim x x x x Target x x x Instrument x x types, but two of them occur in only 4 and 14 of the 1300 training documents. We thus only evaluate the 4 main templates ( bombing , kidnapping , attack , and arson ). The gold slots are shown in figure 3.
We evaluate the four learned templates that score highest in the document classification evaluation (to be described in section 5.1), aligned with their MUC-4 types. Figure 2 shows three of our four tem-plates, and two brand new ones that our algorithm learned. Of the four templates, we learned 12 of the 13 semantic roles as created for MUC. In addition, we learned a new role not in MUC for bombings, kidnappings, and arson: the Police or Authorities role. The annotators chose not to include this in their labeling, but this knowledge is clearly relevant when understanding such events, so we consider it correct. There is one additional Bombing and one Arson role that does not align with MUC-4, marked incorrect. We thus report 92% slot recall, and precision as 14 of 16 ( 88% ) learned slots.

We only measure agreement with the MUC tem-plate schemas, but our system learns other events as well. We show two such examples in figure 2: the Weapons Smuggling and Election Templates. We now present how to apply our learned templates to information extraction. This section will describe how to extract slot fillers using our templates, but without knowing which templates are correct.
We could simply use a standard IE approach, for example, creating seed words for our new learned templates. But instead, we propose a new method that obviates the need for even a limited human la-beling of seed sets. We consider each learned se-mantic role as a potential slot, and we extract slot fillers using the syntactic functions that were previ-ously learned. Thus, the learned syntactic patterns (e.g., the subject of release ) serve the dual purpose of both inducing the template slots, and extracting appropriate slot fillers from text. 5.1 Document Classification A document is labeled for a template if two different conditions are met: (1) it contains at least one trig-ger phrase, and (2) its average per-token conditional probability meets a strict threshold.

Both conditions require a definition of the condi-tional probability of a template given a token. The conditional is defined as the token X  X  importance rel-ative to its uniqueness across all templates. This is not the usual conditional probability definition as IR-corpora are different sizes.
 where P IR t ( w ) is the probability of pattern w in the IR-corpus of template t .
 where C t ( w ) is the number of times word w appears in the IR-corpus of template t . A template X  X  trigger words are defined as words satisfying P ( t | w ) &gt; 0 . 2 . Precision .64 .83 .66 .30 Recall .54 .63 .35 1.0 F1 .58 .72 .46 .46 Trigger phrases are thus template-specific patterns that are highly indicative of that template.
After identifying triggers, we use the above defi-nition to score a document with a template. A doc-ument is labeled with a template if it contains at least one trigger, and its average word probability is greater than a parameter optimized on the training set. A document can be (and often is) labeled with multiple templates.

Finally, we label the sentences that contain trig-gers and use them for extraction in section 5.2. 5.1.1 Experiment: Document Classification The MUC-4 corpus links templates to documents, allowing us to evaluate our document labels. We treat each link as a gold label (kidnap, bomb, or attack) for that document, and documents can have multiple labels. Our learned clusters naturally do not have MUC labels, so we report results on the four clusters that score highest with each label.
Figure 4 shows the document classification scores. The bombing template performs best with an F1 score of .72. Arson occurs very few times, and Attack is lower because it is essentially an ag-glomeration of diverse events (discussed later). 5.2 Entity Extraction Once documents are labeled with templates, we next extract entities into the template slots. Extraction oc-curs in the trigger sentences from the previous sec-tion. The extraction process is two-fold: Take the following MUC-4 sentence as an example: The verb plant is in our learned bombing cluster, so step (1) will extract its passive subject bombs and map it to the correct instrument role (see figure 2). The human target, owners , is missed because intim-idate was not learned. However, if owner is in the selectional preferences of the learned  X  X uman target X  role, step (2) correctly extracts it into that role.
These are two different, but complementary, views of semantic roles. The first is that a role is de-fined by the set of syntactic relations that describe it. Thus, we find all role relations and save their argu-ments (pattern extraction). The second view is that a role is defined by the arguments that fill it. Thus, we extract all arguments that filled a role in training, regardless of their current syntactic environment.
Finally, we filter extractions whose WordNet or named entity label does not match the learned slot X  X  type (e.g., a Location does not match a Person). We trained on the 1300 documents in the MUC-4 corpus and tested on the 200 document TST3 and TST4 test set. We evaluate the four string-based slots: perpetrator, physical target, human target, and instrument. We merge MUC X  X  two perpetrator slots (individuals and orgs) into one gold Perpetrator slot. As in Patwardhan and Riloff (2007; 2009), we ig-nore missed optional slots in computing recall. We induced clusters in training, performed IR, and in-duced the slots. We then extracted entities from the test documents as described in section 5.2.

The standard evaluation for this corpus is to report the F1 score for slot type accuracy, ignoring the tem-plate type. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. This allows supervised classifiers to train on all per-petrators at once, rather than template-specific learn-ers. Although not ideal for our learning goals, we report it for comparison against previous work.
Several supervised approaches have presented re-sults on MUC-4, but unfortunately we cannot com-pare against them. Maslennikov and Chua (2006; 2007) evaluated a random subset of test (they report .60 and .63 F1), and Xiao et al. (2004) did not eval-uate all slot types (they report .57 F1).

Figure 5 thus shows our results with previous work that is comparable: the fully supervised and Patwardhan &amp; Riloff-09 : Supervised 48 59 53 Patwardhan &amp; Riloff-07 : Weak-Sup 42 48 44 Our Results (1 attack) 48 25 33 Our Results (5 attack) 44 36 40 F1 Score Kidnap Bomb Arson Attack Results .53 .43 .42 .16 / .25 weakly supervised approaches of Patwardhan and Riloff (2009; 2007). We give two numbers for our system: mapping one learned template to Attack, and mapping five. Our learned templates for Attack have a different granularity than MUC-4. Rather than one broad Attack type, we learn several: Shoot-ing, Murder, Coup, General Injury, and Pipeline At-tack. We see these subtypes as strengths of our al-gorithm, but it misses the MUC-4 granularity of At-tack. We thus show results when we apply the best five learned templates to Attack, rather than just one. The final F1 with these Attack subtypes is .40.
Our precision is as good as (and our F1 score near) two algorithms that require knowledge of the tem-plates and/or labeled data. Our algorithm instead learned this knowledge without such supervision. In order to more precisely evaluate each learned template, we also evaluated per-template perfor-mance. Instead of merging all slots across all tem-plate types, we score the slots within each template type. This is a stricter evaluation than Section 6; for example, bombing victims assigned to attacks were previously deemed correct 4 .

Figure 6 gives our results. Three of the four tem-plates score at or above .42 F1, showing that our lower score from the previous section is mainly due to the Attack template. Arson also unexpectedly scored well. It only occurs in 40 documents overall, suggesting our algorithm works with little evidence.
Per-template performace is good, and our .40 overall score from the previous section illustrates that we perform quite well in comparison to the .44-.53 range of weakly and fully supervised results. These evaluations use the standard TST3 and TST4 test sets, including the documents that are not labeled with any templates. 74 of the 200 test doc-uments are unlabeled. In order to determine where the system X  X  false positives originate, we also mea-sure performance only on the 126 test documents that have at least one template. Figure 7 presents the results on this subset. Kidnap improves most signifi-cantly in F1 score (7 F1 points absolute), but the oth-ers only change slightly. Most of the false positives in the system thus do not originate from the unla-beled documents (the 74 unlabeled), but rather from extracting incorrect entities from correctly identified documents (the 126 labeled). Template-based IE systems typically assume knowl-edge of the domain and its templates. We began by showing that domain knowledge isn X  X  necessar-ily required; we learned the MUC-4 template struc-ture with surprising accuracy, learning new seman-tic roles and several new template structures. We are the first to our knowledge to automatically in-duce MUC-4 templates. It is possible to take these learned slots and use a previous approach to IE (such as seed-based bootstrapping), but we presented an algorithm that instead uses our learned syntactic pat-terns. We achieved results with comparable preci-sion, and an F1 score of .40 that approaches prior algorithms that rely on hand-crafted knowledge.
The extraction results are encouraging, but the template induction itself is a central contribution of this work. Knowledge induction plays an important role in moving to new domains and assisting users who may not know what a corpus contains. Re-cent work in Open IE learns atomic relations (Banko et al., 2007b), but little work focuses on structured scenarios. We learned more templates than just the main MUC-4 templates. A user who seeks to know what information is in a body of text would instantly recognize these as key templates, and could then ex-tract the central entities.

We hope to address in the future how the al-gorithm X  X  unsupervised nature hurts recall. With-out labeled or seed examples, it does not learn as many patterns or robust classifiers as supervised ap-proaches. We will investigate new text sources and algorithms to try and capture more knowledge. The final experiment in figure 7 shows that perhaps new work should first focus on pattern learning and entity extraction, rather than document identification.
Finally, while our pipelined approach (template induction with an IR stage followed by entity ex-traction) has the advantages of flexibility in devel-opment and efficiency, it does involve a number of parameters. We believe the IR parameters are quite robust, and did not heavily focus on improving this stage, but the two clustering steps during tem-plate induction require parameters to control stop-ping conditions and word filtering. While all learn-ing algorithms require parameters, we think it is im-portant for future work to focus on removing some of these to help the algorithm be even more robust to new domains and genres.
 This work was supported by the National Science Foundation IIS-0811974, and this material is also based upon work supported by the Air Force Re-search Laboratory (AFRL) under prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessar-ily reflect the view of the Air Force Research Labo-ratory (AFRL). Thanks to the Stanford NLP Group and reviewers for helpful suggestions.
