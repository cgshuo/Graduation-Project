 save a substantial labeling effort.
 ones without frames are unlabeled data. multi-view learning [2], and finally, translated learning. from available resources is a key issue.
 from Web sites such as Flickr ( http://www.flickr.com/ ). x . In translated learning, the training data x space, while the test data x the learning in the source space through a Markov chain c  X  y another Markov chain c  X  y is to show how to connect these two paths, so that the new chain c  X  y distance functions to measure the relevance between two mod els. 2.1 Problem Formulation We first define the translated learning problem formally. Let X space, each instance x and Y x feature space. We have a labeled training data set L x training data set L is assumed to be small, so that L test data set U is a set of k examples { x ( i ) feature space from x ( i ) be visual images.
 To link the two feature spaces, a feature translator p ( y of which is to estimate a hypothesis h possible, by making use of the labeled training data L = L 2.2 Risk Minimization Framework to the labeled training data L = L framework in [5].
 In this work, we use the risk function R ( c, x category c . Therefore, to predict the label for an instance x which minimizes the risk function R ( c, x The risk function R ( c, x or  X  X he label of x X Note that, in Equation (2),  X  replace p (  X  with respect to the event of  X  function in Equation (2). 2.3 Estimation The risk function in Equation (2) is difficult to estimate, si nce the sizes of  X  for efficiency. First of all, the loss function L (  X  functions between the two models  X   X (  X  C ,  X  X ing L (  X  Since the sizes of  X  where  X   X  In Equation (4), p (  X   X  Algorithm 1 Risk Minimization Algorithm for Translated Learning: ( TLRisk ) translator  X  to link the two feature spaces Y Output: The prediction label h Procedure TLRisk train 1: for each c  X  X  do 2: Estimate the model  X   X  c based on Equation (6). 3: end for Procedure TLRisk test 1: for each x t  X  X  do 2: Estimate the model  X   X  x 3: Predict the label h t ( x t ) for x t based on Equations (1) and (5). 4: end for where  X (  X   X  as in [5], we formulate these two models in the target feature space Y divergence as the distance function,  X (  X   X  Our estimation is based on the Markov chain assumption where  X   X  and  X   X  where p ( y statistical observations in the labeled text data set L estimated based on L target space labeled data L For another model p ( Y where p ( y p ( x 0 t |  X   X  x in Algorithm 1.
 the time complexity of TLRisk is O ( |C||Y non-zero occurrences in feature mappings. 2.4 Translator  X  We now explain in particular how to build the translator  X  ( y ferent feature spaces. As mentioned before, to estimate the translator p ( y data in the form of p ( y naries can be considered as data in the form of p ( y instance;  X   X   X  means negative instances. sults in response to queries are examples for correlational data in the forms of p ( y and pictures) is an example for data in the form of p ( x Markov chains in the previous subsections.
 or p ( x formally, p ( y p ( y t , y s ) = R X extractors in Y the translator as p ( y a classification model in the target space. 3.1 Data Sets documents is much larger than the number of images.
 The collected data are in the form of feature-instance co-oc currence p ( y convert them to feature-level co-occurrence p ( y number of labeled images L trade-off  X  . 3.2 Evaluation Methods baseline methods in our experiments, we first simply used the labeled data L L ; we refer to this model as Search+Image .
 Leibler divergence (named KL ): R of TLRisk , because Lowerbound was trained based on images in Caltech-256, while TLRisk 3.3 Experimental Results On the other hand, compared with Lowerbound , TLRisk also shows comparable performance. spaces in the case of text-to-image classification.
 ble. This indicates that TLRisk is not quite sensitive to the size of L occurrence data. In these experiments, Lowerbound is just for reference. means the classification model mainly builds on the target sp ace training images L the auxiliary text training data L to 1, based on Figure 3. source data to help classify German documents, which are tar get data. shows marked improvements compared with the baseline metho d German Labels Only , al-algorithm TLRisk is effective on the cross-language classification problem. for English-to-German cross-language classification.
 for  X  tuning here. data in different feature spaces.
 view and test data in another view. greatly outperform the state-of-the-art baseline methods .
