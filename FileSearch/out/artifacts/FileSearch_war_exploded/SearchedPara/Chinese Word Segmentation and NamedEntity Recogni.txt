 Microsoft Research Asia GrapeCity Inc.

This article presents a pragmatic approach to Chinese word segmentation. It differs from most
Chinese words using various linguistic criteria, Chinese words in this study are defined prag-matically as segmentation units whose definition depends on how they are used and processed in realistic computer applications. Second, we propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types (i.e., morpho-logically derived words, factoids, named entities, and other unlisted words) can be performed simultaneously in a unified way. These tasks are usually conducted separately in other systems. application-independent. Instead, we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different natural language processing applications might require different granularities of Chinese words.
 called MSRSeg, which will be described in detail. It consists of two components: (1) a generic approach to the five fundamental features of word-level Chinese language processing: lexicon word processing, morphological analysis, factoid detection, named entity recognition, and new the adaptive system achieves state-of-the-art performance on all the test sets. 1. Introduction
This article is intended to address, with a unified and pragmatic approach, two funda-mental questions in Chinese natural language processing (NLP): What is a  X  X ord X  in
Chinese?, and How does a computer identify Chinese words automatically? Our ap-proach is distinguished from most previous approaches by the following three unique components that are integrated into a single model: a taxonomy of Chinese words, a unified approach to word breaking and unknown word detection, and a customizable display of word segmentation. 1 We will describe each of these in turn.
 constitutes a word in Chinese. Theoretical linguists have tried to define Chinese words using various linguistic criteria (e.g., Packard 2000). While each of those criteria pro-vides valuable insights into  X  X ord-hood X  in Chinese, they do not consistently lead us to the same conclusions. Fortunately, this may not be a serious issue in computational lin-guistics, where the definition of words can vary and can depend to a large degree upon how one uses and processes these words in computer applications (Sproat and Shih 2002).
 computational linguistics. We develop a taxonomy in which Chinese words can be categorized into one of the following five types: lexicon words, morphologically derived words, factoids, named entities, and new words. 2 These five types of words have different computational properties and are processed in different ways in our system, as will be described in detail in Section 3. Two of these five types, factoids and named entities, are not important to theoretical linguists but are significant in NLP. disambiguation and unknown word identification. In most of the current systems, these are considered to be two separate tasks and are dealt with using different components in a cascaded or consecutive manner.
 better approached simultaneously. In this article, we present a unified approach to the five fundamental features of word-level Chinese NLP (corresponding to the five factoid detection, (4) named entity recognition (NER), and (5) new word identifica-tion (NWI). This approach is based on a mathematical framework of linear mixture models in which component models are inspired by the source X  X hannel models of
Chinese sentence generation. There are basically two types of component models: a source model and a set of channel models. The source model is used to estimate the generative probability of a word sequence in which each word belongs to one word type. For each of the word types, a channel model is used to estimate the likelihood of a character string, given the word type. We shall show that this framework is models in a unified way.
 words . While words are supposed to be unambiguous and static linguistic entities, segmentation units are expected to vary from application to application. In fact, dif-ferent Chinese NLP-enabled applications may have different requirements that request different granularities of word segmentation. For example, automatic speech recog-nition (ASR) systems prefer longer  X  X ords X  to achieve higher accuracy, whereas in-532 (Wu 2003).
 mentation standard exists. We argue instead for the existence of multiple segmenta-application-specific segmenters. A better solution would be to develop a generic seg-menter with customizable output that is able to provide alternative segmentation units according to the specification that is either predefined or implied in the application data.
To achieve this, we present a transformation-based learning (TBL; Brill 1995) method, to be described in Section 6.
 ive Chinese word segmenter called MSRSeg. It consists of two components: (1) a generic segmenter that is based on the linear mixture model framework of word breaking and unknown word detection and that can adapt to domain-specific vocab-application-specific standards. Evaluation on five test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all the test sets. It thus demonstrates the possibility of a single adaptive Chinese word segmenter that is capable of supporting multiple applications.
 ous work in this field. Section 3 introduces the taxonomy of Chinese words and de-background on which our unified approach is based. Section 5 outlines the general architecture of the Chinese word segmenter, MSRSeg, and describes each of the com-ponents in detail, presenting a separate evaluation of each component where appro-priate. Section 6 presents the TBL method of standards adaptation. While in Section 5 the methods of creating training data in a (semi-)automatic manner, with minimal or no human annotation. We thus demonstrate the possibilities of unsupervised learning of Chinese words. Section 8 presents several evaluations of the system on the different corpora, each corresponding to a different segmentation standard, in comparison with other state-of-the-art systems. Finally, we conclude the article in Section 9. 2. Previous Work 2.1 Approaches to Word Segmentation
Many methods of Chinese word segmentation have been proposed: reviews include Wu and Tseng (1993); Sproat and Shih (2002); and Sun and Tsou (2001). These methods can be roughly classified as either dictionary-based or statistically-based methods, while many state-of-the-art systems use hybrid approaches.
 stored in the dictionary can be identified. One of the most popular methods is maxi-mum matching (MM), usually augmented with heuristics to deal with ambiguities in segmentation. Studies that use this method or minor variants include Chen et al. (1999) and Nie, Jin, and Hannan (1994). The performance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictio-nary, many systems also contain special components for unknown word identification.
In particular, statistical methods have been widely applied because they use a proba-bilistic or cost-based scoring mechanism rather than a dictionary to segment the text. These methods have three drawbacks. First, some of these methods (e.g., Lin et al. 1993;
Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters.
He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation.
 separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is similarly motivated models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat X  X  system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and
Duffy (2001). Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004). They also use a unified approach to word breaking and OOV identification. 2.2 More on New Word Identification
In this article, we use the term  X  X ew words X  to refer to OOV words other than named en-tities, factoids, and morphologically derived words.  X  X ew words X  are mostly domain-specific terms (e.g.,  X   X  X ellular X ) and time-sensitive political, social, or cultural terms (e.g.,  X  X hree Links X ; ^ x  X  X ARS X ). There have been two general approaches into a dictionary before word segmentation starts (e.g., Fung and Wu 1994; Nie, Jin and
Hannan 1994; Chien 1997; Gao et al. 2002). The other is to detect new words on-line, i.e., to spot new words in a sentence on the fly during the process of word segmentation (e.g., Chen 2003; Wu and Jiang 2000). These two approaches complement one another, and we use both of them in our system.
 the basic assumption is that a Chinese word should appear as a stable sequence in mutual information, term frequency, and their variants. They require a reasonably large training corpus. The new words detected are mostly proper nouns and other relatively frequent words. Unfortunately, new words, under our definition of the term, may not be detected. 534 of this article. Some recent advances in on-line NWI explore the use of machine learning approaches. For example, Li et al. (2003) define NWI as a binary classification problem and use support vector machines (SVM) to combine various linguistically motivated features to determine whether a Chinese character sequence is a word. Our method is an extension of that of Li et al. in that NWI is not a stand-alone process in our system but an integral part of word segmentation. We shall show experimentally the benefit of the integration in Section 5.5. 2.3 Standards Adaptation
As described earlier, while Chinese words are supposed to be well-defined, unambigu-ous, and static linguistic entities, we are more concerned with segmentation units that are expected to vary among different computer applications. This inspires the development of an adaptive Chinese word segmenter.
 a standard that assumes a single correct segmentation. The only adaptive system, to the best of our knowledge, is the customizable segmenter described in Wu (2003), in which the display of the segmentation output can be customized by users. adaptation method we will describe in Section 6 can be viewed as an improved version in that the adaptation rules (or transformations) are acquired automatically from application data via the TBL method (Gao et al. 2004). Though the use of TBL for
Chinese word segmentation is not new (see Palmer [1997]; Hockenmaier and Brew [1998]), none of the previous work is aimed at standards adaptation. 2.4 Evaluation
The performance of Chinese word segmenters is generally reported in terms of preci-sion and recall. However, a comparison across systems could be very difficult for two for a given sentence there are multiple plausible word segmentations. As shown in
Sproat et al. (1996), the rate of agreement between two human judges is less than 80%. To deal with this problem, Fung and Wu (1994) suggest a procedure called nk-blind that uses n blind judges X  standards. If we set k = 1, it is sufficient for a seg-mentation to be considered correct if it agrees with at least one of the n judges. If k = n , all judges must agree. Therefore, nk-blind gives a more representative performance measure by taking into account multiple judges. Similarly, Sproat et al. (1996) also uses multiple human judges. In Section 8.2, we will present our method for cross-system comparison. We do not use multiple human judges. Instead, we only consider a set of measures that are lexicon-independent and less ambiguous among different human judges and systems.
 different test sets and ground rules by many research papers. For example, some papers report precision and recall rates of 98% or 99%. But they either count only the words that are stored in the dictionary or use unrealistically simple data with a very low OOV rate.
Recently, the ACL-SIGHAN X  X ponsored First International Chinese Word Segmentation
Bakeoff alleviated the situation to some degree (Sproat and Emerson 2003). The Bakeoff released four data sets, each corresponding to a different standard, and consistent train  X  test splits. We evaluate our segmenter using those four data sets in Sections 6.2 and 8.3. 3. Chinese Words
This section de fi nes Chinese words at three levels. We begin with a taxonomy by which Chinese words are categorized into fi ve main types according to the way they are processed and used in realistic systems. Second, we develop the MSR standard, which is a set of speci fi c rules to guide human annotators in segmenting Chinese sentences. Finally, we describe the development of a gold test set and how we evaluate
Chinese word segmenters. Here, we use the term  X  gold test set  X  to refer to the manually annotated corpus, according to the MSR standard, on top of the  X  test corpus  X  that is the raw text corpus. 3.1 Taxonomy
The taxonomy of Chinese words is summarized in Table 1, where Chinese words are categorized into the fi ve types: entries in a lexicon (or lexicon words, LWs), morpho-logically derived words (MDWs), factoids (FTs), named entities (NEs), and new words 536 (NWs). These fi ve types of words have different functions in Chinese NLP and are processed in different ways in our system. For example, a plausible word segmentation for the sentence in Figure 1(a) is shown in the same fi gure. Figure 1(b) is the output of our system, where words of different types are processed in different ways: (e.g., the relation between MDWs and LWs depends on the lexicon being used); the taxonomy therefore does not give a clear de fi nition of Chinese words. We do not intend for this article to give a standard de fi nition of Chinese words. Instead, we treat Chinese word segmentation as a preprocessing step where the best segmentation units depend on how they are used in the consuming applications. The fi ve word types represent the fi ve types of Chinese words that appear in most applications. This is one of the reasons that we title this article  X  a pragmatic approach.  X  We focus on two tasks in the approach: processing of the fi ve types of Chinese word using a uni fi ed framework that can be jointly optimized (Sections 4 and 5), and adapting our system to different applications (Section 6). Now, we describe each of the fi ve word types in Table 1 in detail. out Chinese word segmentation without the use of dictionaries (e.g., Sproat and Shih nent of many applications. For example, in a machine translation system, it is de-sirable to segment a sentence into LWs as much as possible so that the candidate would also like to segment a sentence into LWs in a Chinese text-to-speech (TTS) system because the pronunciations stored in the dictionary are usually much more precise than those generated dynamically (for instance, by character-to-sound rules). In nese characters stored as single-character words. This lexicon is a combination of several dictionaries authored by Chinese linguists and used in different Microsoft applications. Thus, all LWs in theory are similar to those described in Packard (2000), language processor.  X  following two characteristics. First, MDWs can be generated from one or more LWs
MDW  X   X  t t  X  happily  X  is generated from a stem  X  t  X  happy  X  via an AABB redu-plication process. As shown in Table 1, there are fi ve main categories of morpho-logical processes, each of which has several subcategories, as detailed in Figure 2 (see Wu [2003] for a detailed description): sequences in the corpus. That is, the components within the MDWs are strongly cor-related (of high co-occurrence frequency), while the components at both ends have low correlations with words outside the sequence. We shall describe in Section 5.2 how a morph-lexicon for Chinese morphology analysis.
 Therefore, the detection and normalization of FTs can be achieved by Finite State Machines.
 son names, location names, and organization names. One cannot develop a regular grammar that rejects or accepts the constructions of NEs with high accuracy, as we can do with most FTs. In Section 5.3, we shall describe how we use both heuristics and statistical models for NER.
 entities or factoids nor derived by morphological rules. In particular, we focus on low-frequency new words, including newly coined words, occasional words, and mostly time-sensitive words (Wu and Jiang 2000). Many current segmenters simply ignore
NWs, assuming that they are of little signi fi cance in most applications. However, we argue that the identi fi cation of those words is critical because a single unidenti fi ed word can cause segmentation errors in the surrounding words. For NLP applications that require full parsing, it is even more critical because a single error would cause a whole sentence to fail. 538 3.2 MSR Standard
The taxonomy employed here has been speci fi ed in detail in the MSR standard. There are two general guidelines for the development of the standard: 1. The standard should be applicable to a wide variety of NLP tasks, of 2. The standard should be compatible with existing standards, of which and [ensures] data uniformity  X  (Huang et al. 1997; Sproat and Shih 2002). The MSR standard consists of a set of speci fi c rules that aims at unambiguously determining the word segmentation of a Chinese sentence, given a reference lexicon. The development of the standard is an iterative procedure, interacting with the development of a gold segmentation rules, based on which four human annotators label a test corpus. When-ever an interannotator con fl ict is detected (automatically), we resolve it by revising the standard (e.g., mostly by adding more speci fi c rules). The process is iterated until no character sequence can be derived from a LW via a morphological process, then the sequence is treated as an MDW candidate.  X  We then observe that both ate  X  and  X   X  m  X  already had a meal  X  are derived from the LW morphological process of splitting. While  X  m is a reasonable MDW, is debatable. We then add a rule:  X  MDW candidates with complex internal structures should be segmented.  X  We also add a set of speci fi crulestode fi ne what a complex internal structure is. An example of those rules is  X  for MDW candidates of type MS, we only consider sequences that are less than four characters long.  X  errors. We currently do not have a systematic solution to this. The complexity has to be controlled manually. That is, all new added rules are recompiled by a linguist so that the total number of rules is manageable. 3.3 MSR Gold Test Set and Training Set
Several questions had to be answered when we were developing the gold test set for evaluation. 1. How should we construct a test corpus for reliable evaluation? 2. Does the segmentation in the gold test set depend on a particular lexicon? 3. Should we assume a single correct segmentation for a sentence? 4. What are the evaluation criteria? 5. How should we perform a fair comparison across different systems using tion 3.5. First, to conduct a reliable evaluation, we select a test corpus that contains ap-proximately half a million Chinese characters that have been proofread and balanced in terms of domains and styles. The distributions are shown in Table 2. The gold test set is developed by annotating the test corpus according to the MSR standard via the iterative procedure described in Section 3.2. The statistics are shown in Table 3. Some fragments of the gold test set are shown in Figure 3, and the notation is presented in Table 1. of many applications. The segmentation of the gold test set depends upon a refer-ence lexicon, which is the combination of several lexicons that are used in Microsoft applications, including a Chinese text input system (Gao et al. 2002), ASR (Chang et al. 2001), TTS (Chu et al. 2003), and the MSR-NLP Chinese parser (Wu and Jiang 2000).
The lexicon consists of 98,668 entries. We also developed a morph-lexicon , which contains 50,963 high-frequency MDWs. We will describe how the morph-lexicon was constructed in Section 5.2.
 segmentations for a given Chinese sentence, we keep only a single gold segmentation for each sentence for two reasons. The fi rst is simplicity. The second is due to the fact 540 that we currently do not know any effective way of using multiple segmentations in the above-mentioned applications. In particular, we segment each sentence as much as possible into words that are stored in the reference lexicon. When there are multiple segmentations for a sentence, we keep the one that contains the fewest number of words.

It contains approximately 40 million Chinese characters from various domains of text such as newspapers, novels, and magazines. In our experiments, 90% of the training set is used for model parameter estimation, and the remaining 10% is a held-out set for tuning. 3.4 SIGHAN  X  s Bakeoff Standards and Corpora
As mentioned in Section 1, MSRSeg is designed as an adaptive segmenter that consists of two components: (1) a generic segmenter that can adapt to different domain vocab-ularies, and (2) a set of output adaptors, learned from application data, for adapting to different application-speci fi c standards. ternational Chinese Word Segmentation Bakeoff (or Bakeoff for brevity) (Sproat and
Emerson 2003). In the Bakeoff corpora, OOV is de fi ned as the set of words in the test corpus not occurring in the training corpus.
 we have a general prede fi ned standard according to which we create a large amount of training data. We then develop a generic word segmenter. Whenever we deploy the segmenter for any application, we customize the output of the segmenter according amount of application data (called adaptation data).
 experiments, on which the generic segmenter has been developed. The four Bakeoff standards are used as speci fi c standards into which we wish to adapt the general standard. We notice in Table 4 that the adaptation data sets (i.e., training corpora for the four Bakeoff standards) are much smaller than the MSR training set. Thus, the experimental setting is a good simulation of the adaptation paradigm described above.
In the rest of the article, we shall by default report results on the MSR data set unless otherwise stated. 3.5 Evaluation Methodology
As described earlier, we argue that Chinese words (or segmentation units) cannot be de fi ned independently of the applications, and hence a more fl exible system (i.e., an 542 adaptive segmenter such as MSRSeg) should be adopted. However, we are faced with the challenge of performing an objective and rigorous evaluation of such a system. the standard data sets. In this article, we argue that MSRSeg is a better system in two regards. First, the generic segmenter provides not only word segmentation but also word-internal structures (e.g., the tree structures of MDWs, FTs, and NEs, as will be described in Section 6) that cover all possible segmentations. Ideally, such a segmenter provides a superset of segmentation units where each different application can fi nd the subset it needs. Second, the output adaptors of MSRSeg can automatically pick different subsets (i.e., segmentation units) from the superset according to different applications.
Therefore, there are two criteria for evaluating an adaptive segmenter: how complete some application data sets (i.e., segmented texts used by different applications). How-ever, such application data are not available yet, and no other system has undergone such evaluation, so there is no way to compare our system against others in this fashion. The evaluation methodology we adopted in this article is a simulation. On the one hand, we developed a generic standard and a corresponding gold test set that simulates the generic superset that attempts to cover as many applications as possible.
We then evaluate on the data set the completeness of the generic segmenter. On the other hand, we will show that we can effectively adapt the generic segmenter to the four different bakeoff data sets, each of which simulates an application subset. performance of MSRSeg is measured through multiple precision  X  recall (P/R) pairs, and F-measures (de fi ned as 2PR/(P+R)), each for one word type. Riv is the recall of in-vocabulary words. Roov is the recall of OOV words. They are used to measure the segmenter  X  s performance in resolving ambiguities in word segmentation and detect-using the criterion proposed by Sproat and Emerson (2003).
 bination ambiguity string) errors are used to measure the segmenter  X  s performance of resolving ambiguities in word segmentation in more detail. Liang (1987) de fi nes OAS and CAS as follows.
 De fi nition 1
A character string ABC is called an overlap ambiguity string (OAS) if it can be seg-mented into two words either as AB/C or A/BC (not both), depending on context. De fi nition 2
A character string AB is called a combination ambiguity string (CAS) if A, B, and AB are words.
 in Chinese text, and the relative frequency of CASs is 12 times lower than that of OAS.
However, according to the above de fi nition, the relative frequency of CASs can be much higher because most single characters in Chinese can be words by themselves, and as a result, almost all two-character words can be CASs. However, this is not desirable.
Consider the word  X   X   X  altitude  X  . Though both  X   X  high  X  and by themselves, the segmentation  X  /  X  almost never occurs in reality. To remedy this problem, Sun and Tsou (2001) revise the de fi nition as follows: De fi nition 3 A character string AB is called a combination ambiguity string (CAS) if A, B, and
AB are words, and there is at least one context under which the segmentation A/B is plausible both semantically and syntactically.
 syntactic and semantic sense of the segmentation  X  a task where an agreement cannot be reached easily among different human annotators. Therefore, we only use the
CAS measure in a pilot study. As will be described in Section 7, the number of CAS frequency CASs.
 cannot be used in cross-system comparisons. For example, since the MSR gold test set is based on a reference lexicon, some of the measures are meaningless when we compare our system to other segmenters that use different lexicons. So in comparing different systems, we consider only the P/R/F of NER and the number of OAS errors (i.e., crossing brackets), because these measures are lexicon-independent and there is always a single unambiguous answer. 4. Theoretical Background
This section provides some theoretical background on the basis of the development of MSRSeg. We fi rst present in Section 4.1 a Chinese word segmentation frame-work that uses source  X  channel models of Chinese sentence generation. Then, in Sec-tion 4.2, we generalize source  X  channel models as linear mixture models in which a wide variety of linguistic knowledge and statistical models can be incorporated in a uni fi ed way. These models are constructed via two basic modeling tools: (1) n -gram language models (LMs; Chen and Goodman 1999), and (2) fi nite state automata (FSA; Roche and
Schabes 1997). More speci fi cally, the LMs we used are bigram and trigram backoff mod-els, where the parameters are estimated using maximum-likelihood estimation (MLE) with a particular smoothing method, called modi fi ed absolute discounting, described in Gao, Goodman, and Miao (2001). LMs are used to capture statistical information such as the likelihood of word or character sequence. FSAs are used to represent (1) the lexicon, (2) the rules for detecting FTs, and (3) the rules for generating NE candidates. 544 4.1 Source  X  Channel Models
The task of MSRSeg is to detect not only word boundaries but also word types so that words of different types can be processed as shown in Figure 1. Therefore, following the
Chinese word taxonomy in Table 1, we de fi ne a Chinese word class as a group of words that are supposed to be generated according to the same distribution (or processed in the same manner) as follows: 1. Each LW is de fi ned as a class; 2. Each MDW is de fi ned as a class; 3. Each type of FT (e.g., time expressions) is de fi ned as a class; 4. Each type of NE (e.g., person names) is de fi ned as a class; and 5. All NWs belong to a class.
 fl oor-value to those words that are not stored in the lexicons. In particular, we de fi ne six unknown word classes as follows. One class is used to represent all unknown LWs and all unknown MDWs whose type cannot be detected. The other fi ve classes are used to represent unknown MDWs, one for each of the fi ve types listed in Table 1, i.e., MP/MS,
MR, MS, MM, and MHP. The probabilities of these unknown word classes are estimated using the Good-Turing method.
 character sequence. A segmenter  X  s job is to choose the most likely word class sequence w  X  among all possible candidates into which s could have been segmented: where GEN ( s ) denotes the candidate set given s .
 tation. The models assume that a Chinese sentence s is generated as follows: First, a person chooses a sequence of concepts (i.e., word classes w ) to be output, according cept by choosing a sequence of characters, according to the probability distribution P ( s | w ).
 stochastic model estimating the probability of word class sequence. It indicates, given a context, how likely a word class occurs. For example, person names are more character string is generated given a word class. For example, the character string N  X  is more likely to be a person name than  X   X   X  Li Junsheng  X  because common family name in China while  X  is not. So P ( s model . In our system, we use only one context model (i.e., a trigram language model) and a set of class models of different types, each of which is for one class of words, as shown in Table 6. (e.g., NE models are n -gram models trained on corpora, whereas FT models use de-rivation rules and have binary values). The dynamic value ranges of different class model probabilities can be so different (some are not probabilities but scores )thatitis inappropriate to combine all models through simple multiplication as in Equation (i.e., channel model) a model weight  X  to adjust the class model score P ( s
P ( s | w )  X  . In our experiments, these weights are optimized so as to minimize the number of word segmentation errors on training data under the framework of lin-models are the rationale behind our system, e.g., the decoding process described in
Section 5.6 follows the framework. Linear models are just another representation based on the optimization algorithm of class model weights. 4.2 Linear Models
The framework of linear models is derived from linear discriminant functions widely introduced into NLP tasks by Collins and Duffy (2001). It is also related to (log-)linear models described in Berger, Della Pietra, and Della Pietra (1996), Xue (2003); Och (2003), and Peng, Feng, and McCallum (2004).
 546 described in Section 4.1 by introducing class weights (i.e., adjust P ( s taking the logarithm of all probabilities. The decision rule of Equation (1) can then be rewritten as gradient descent: an iterative procedure of adjusting the parameters of that minimizes the segmentation errors with respect to a loss function. We will present in turn the loss function and the optimization algorithm. 4.2.1 Loss Function. Assume that we can measure the number of segmentation errors in w by comparing it with a reference segmentation w R using an error function Er ( w segmentation errors over the training data is as w  X  . Equation (4) is referred to as the minimum sample risk (MSR; Gao et al. 2005) criterion hereafter. Notice that without knowing the  X  true  X  distribution of the data, the best  X   X   X  can be chosen approximately based on training samples. This is known as the principle of empirical risk minimization (ERM; Vapnik 1998): If the segmenter were trained using exactly the MSR criterion, it would converge to a Bayes risk performance (minimal error rate) as the training size goes to in fi nity.
 thus a poor candidate for optimization by any simple gradient-type numerical search.
For example, the gradient cannot be computed explicitly because Er ( . )isnotdiffer-
Therefore, we use an alternative loss function, minimum squared error (MSE) in equation (5), where Score ( . )isde fi ned in Equation (2), where s has been suppressed correct segmentation and the score of the incorrect one, summing over all training samples.
 imations to maximum likelihood solution. The quality of the approximation depends upon the form of the linear discriminant functions (e.g., Equation (2)). Due to its ap-pealing theoretical properties, the MSE criterion has received considerable attention in the literature, and there are many solution procedures available (Duda, Hart, and Stork 2001). 4.2.2 Optimization Algorithm. This section discusses the delta rule , a training algorithm for an unthresholded perceptron, following the description in Mitchell (1997). The delta rule in its component form is where  X  is the step size, and G is the gradient of MSELoss. differentiating the loss function of equation (7) with respect to  X 
G (  X  d ) =  X  MSELoss(
However, the objective function of Equation (5) in the context of our task (i.e., Chinese word segmentation) has many local minima, and thus gradient descent cannot guaran-tee fi nding the global minimum. We therefore use a stochastic approximation to gradient descent. Whereas the gradient descent computes parameter updates after summing over all training samples as shown in Equation (7), the stochastic approximation method updates parameters incrementally, following the calculation of the error for each indi-vidual training sample, as shown in Equation (8).
 function MSELoss i (  X   X   X  )de fi ned for each individual training sample i as follows 548 takes T passes over the training set. All parameters are initially set to be 1. The context model parameter  X  0 does change during training. Class model parameters are updated in a simple additive fashion: Parameters are altered according to the gradient with respect to MSELoss i (  X   X   X  ).
 rors, so Score (  X   X   X  ,s , w R )  X  Score (  X   X   X  ,s , w ) That is, the model parameters are updated when the sentence is wrongly segmented.
The update rule increases the parameter values for word classes whose models were value f ( s , w R )), and decreases the parameter values whose models were  X  overesti-dates, when iterated over all training samples, provides a reasonable approximation to descending the gradient with respect to the original loss function of Equation (5).
Although this method cannot guarantee a globally optimal solution, it is chosen for our modeling because of its ef fi ciency and because it achieved the best results in our experiments.
 key difference is that, instead of using the delta rule of Equation (8) (as shown in line 5 of Figure 4), Collins (2002) updates parameters using the rule: f ( w i ). Our pilot study shows that our algorithm achieves slightly better results. 4.3 Discussions on Robustness
The training methods described in Section 4.2 aim at minimizing errors in a training bounded and can be contaminated from those training samples far away from the decision boundary. One of many possible solutions for improving the robustness is to introduce a margin in the training procedure of Figure 4. The basic idea is to enlarge the score difference (or score margin) between a correct segmentation (i.e., w its competing incorrect segmentations (i . e . , { w ; w
Equation (8), the perceptron training algorithm of Figure 4 does not adjust parameters tinued to enlarge the score margin between the correct segmentation and the top com-peting candidate even if the input sentence had been correctly segmented, until the margin has exceeded a preset threshold. More speci fi cally, we can modify Equation (8) as follows where  X  is the desired margin that can either be an absolute value or a quantity propor-tional to the score of the correct segmentation (Su and Lee 1994). The modi fi ed training algorithm is similar to the perceptron algorithm with margins proposed by Krauth and M ` ezard (1987). We leave the evaluation of the algorithm to future work.
 margin-based learning algorithms. 5. System Description 5.1 Architecture Overview MSRSeg consists of two components: a generic segmenter and a set of output adaptors.
The generic segmenter has been developed on the basis of the mathematical models described in Section 4. It consists of several components, as shown in Figure 5. 1. Sentence Segmenter: To detect sentence boundaries using punctuation 2. Word Candidate Generator: Given an input string s , to generate all word 550 3. Decoder: To select the best (or the N best) word segmentation (i.e., word 4. Wrapper: To output segmentation results using some prede fi ned canonical the lexicon (and morph-lexicon) TRIEs to generate LW (or MDW) candidates, (2) the NE class models to generate NE candidates, (3) the fi nite-state automaton (FSA) to generate
FT candidates, and (4) the classi fi er to generate NW candidates. 5.2 Lexicon Representation and Morphological Analysis
Lexicon words are represented as a set of TRIEs (Frakes and Baeza-Yates 1992), which is a particular implementation of the FSA described in Section 4. Given a character string, all pre fi x strings that form lexical words can be retrieved by browsing the TRIE whose root represents its fi rst character.
 fi nite-state morphology), they are dif fi cult to extend to Chinese for two reasons. First,
Chinese morphological rules are not as general as their English counterparts. For ex-ample, in most cases English plural nouns can be generated using the rule  X  noun + s  X  plural noun  X  . But only a small subset of Chinese nouns can be pluralized (e.g.,
 X   X   X  friends  X  ) using its Chinese counterpart  X  noun +  X  others (e.g., W  X   X  pumpkin s  X  ) cannot. 7 Secondly, the operations required by Chinese morphological analysis, such as copying in reduplication, merging, and splitting, cannot be implemented using current fi nite-state networks. 8 described in Section 3.1 and incorporate them into the TRIE lexicon, called morph-lexicon .
The TRIEs are essentially the same as those used for lexical words, except that not only the MDW  X  s identity but also its morphological pattern and stem(s) are stored.
Candidate generation is done by applying a set of morphological rules to both the word lexicon and a large corpus. For example, the rule  X  noun + would generate candidates like  X   X  . (2) Statistical fi ltering. For each candidate, we obtain a set of statistical features such as frequency, mutual information, and left/right context dependency from a large corpus. We then use an information-gain-like metric described in Chien (1997) and Gao et al. (2002) to estimate how likely a candidate is to form a morphologically derived word and remove the  X  bad  X  candidates. The basic idea behind the metric is that a Chinese word should appear as a stable sequence in the corpus. That is, the components within the word are strongly correlated, while the components at both ends should have low correlations with words outside the sequence. (3) Linguistic selection. Finally, we manually check the remaining candidates and construct the morph-lexicon, where each entry is tagged with its morphological pattern and stem(s). The resulting morph-lexicon contains 50,963 MDWs. 5.3 Named Entities
We consider four types of named entities: person names (PNs), location names (LNs), organization names (ONs), and transliterations of foreign names (FNs). Because any character string can in principle be a named entity of one or more types, in order to limit the number of candidates for a more effective search, we generate named entity constraints (which are compiled by linguists and are represented as FSAs) to generate signed a class model probability. Class models are de fi ned as generative models that are estimated on their corresponding named entity lists using MLE, together with a backoff straints and the class models here. The Chinese person-name model is a modi fi ed version of that described in Sproat et al. (1996). Other NE models are novel, though they share some similarities with the Chinese person-name model. 5.3.1 Chinese Person Names. There are two main constraints. (1) PN patterns: We assume that a Chinese PN consists of a family name F and a given name G ,andisof the pattern F + G .Both F and G are one or two characters long. (2) Family name list: We only consider PN candidates that begin with an F stored in the family name list (which contains 373 entries in our system).
 family name substring s F , with the probability P ( s sub-string s G , with the probability P ( s G | G)(or P ( s given name, with the probability P ( s G2 | s G1 , G2). For example, the generative probabil-ity of the string N  X  given that it is a PN would be estimated as P ( P (
N | F) P (  X  | G1)P( |  X  ,G2). 5.3.2 Location Names. Unlike PNs, there are no patterns for LNs. We assume that an
LN candidate is generated given s (which is less than 10 characters long), if one of the following conditions is satis fi ed: (1) s is an entry in the LN list (which contains 30,000
LNs); (2) s ends in a keyword in a 120-entry LN keyword list (e.g.,  X  city  X  ). string s  X   X  Shamir river  X  . It is an LN candidate because it ends in an LN keyword estimated as P ( s  X  | LN) = P ( | &lt; LN &gt; ) P ( s | where &lt; LN &gt; and &lt; /LN &gt; are symbols denoting the beginning and the end of an LN, respectively. 5.3.3 Organization Names. ONs are more dif fi cult to identify than PNs and LNs because
ONs are usually nested named entities. For example, the ON
China Corporation  X  contains an LN - X   X  China  X  . 552 character string s (less than 15 characters long), if it ends in a keyword in a 1,355-entry ON keyword list (e.g., l  X   X  corporation  X  ). To estimate the generative probability of a nested ON, we introduce word class segmentations of s, w, as hidden variables. In principle, the ON class model recovers P ( s sible C: P ( s | ON) = X  w P ( s , w | ON) = X  w P ( w | ON) P ( s
P ( s | w ), we have P ( s | ON) = X  w P ( w | ON) P ( s approximated by a single pair of terms P ( w  X  | ON) P ( s probable word class segmentation discovered by Equation (5). That is, we also use our system to fi nd w *, but the source  X  channel models are estimated on the ON list.
 - X  is tagged as an LN, the probability P ( s | ON) would be estimated using a word class bigram model as: P ( - X   X  E * z l  X  | ON)  X  P (LN/  X  E /
P ( - X  | LN) = P (LN | &lt; ON &gt; ) P (  X  E | LN) P ( * z |
P ( - X  | LN), where P ( - X  | LN) is the class model probability of an LN, and &lt; ON &gt; and &lt; /ON &gt; are symbols denoting the beginning and the end of an
ON, respectively. 5.3.4 Transliterations of Foreign Names. As described in Sproat et al. (1996), FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name. Since FNs can be of any length and their original pronunciation is effectively unlimited, the recognition of such names particularly common in transliterations.
 acters stored in a transliterated name character list (which contains 618 Chinese characters). The probability P ( s | FN) is estimated using a character bigram model.
Notice that in our system an FN can be a PN, an LN, or an ON, depending on the context. Then, given an FN candidate, three named entity candidates, each for one category, are generated in the lattice, with the class probabilities P ( s type to the decoding phase where the context model is used. 5.3.5 Abbreviations. For the sake of completeness, we describe below the basic ideas for tackling NE abbreviations within our framework. This is ongoing research, and
Sun, Zhou, and Gao (2003) for more details, where marginal improvements have been reported.
 abbreviations of Chinese PNs and LNs are single-character NEs. The PN and LN models described previously cannot handle them very well because (1) single-character NEs are generated in a different way from that of multicharacter NEs, and (2) the context of single-character NEs is different from multicharacter ones. For example, single-character NEs usually appear adjacent to one another, such as  X  China-Russia trade  X  . But this is not the case for multicharacter NEs.
 noted by SCPN and SCLN, respectively. We assume that a character is a candidate of
SCPN (or SCLN) only when it is included in a prede fi ned SCPN (or SCLN) list, which contains 151 (or 177) characters. The class model probabilities are assigned by unigram models as where C( s ) is the count of the SCPN (or SCLN) s in an annotated training set and N is the size of the SCPN (or SCLN) list. In the context model, we also differentiate between PN (or LN) and SCPN (or SCLN). Therefore, SCPN and SCLN should be labeled explicitly in the training data.
 ONAs are usually multiple-character strings and can be generated from their original
ON arbitrarily. For example, the abbreviation of N ' f
N while the abbreviation of  X  ' f  X  Peking University  X  is
ONA candidate is only generated given a character string s (fewer than 6 characters in the same document, and (2) s can be derived from the ON using a generative we construct a score function to assign each ONA candidate a class model score.
Consider a string ' . The generative probability of the string, given it is an ONA, would be approximated as P ( ' | ONA)  X  P (  X  ' f | ON) P ( where P ( ' |  X  ' f ,ON)isde fi ned as a constant (0.8 in our experiments) if can be derived from  X  ' f using any generative pattern of Figure 7; 0, otherwise.
If more than one ON is detected previously in the same document and can be used to derive the ONA candidate (e.g.,  X  ' f  X  North University  X  ), only the closest ON is taken into account. We also notice that ONAs and ONs occur in similar contexts.
So we do not differentiate them in the context model. 5.4 Factoids
The types of factoids handled in MSRSeg are shown in Table 1 of Section 3.1. For each type of factoid, we generate a grammar of regular expressions. The ten regular expressions are then compiled into a single FSA. Given an input string s, we scan it from left to right, and output a FT candidate when a substring matches one of the ten 554 regular expressions. We also remove those FT candidates that are substrings of any other candidates of the same type. Consider the example in Figure 1; only the string A  X   X  A is accepted as a FT candidate (a time expression), not the substrings (e.g., A  X   X  or A  X   X  A ).
 of FT detection using only FSA is comparable with that of using MSRSeg where the contextual information of the FT is considered (i.e., in MSRSeg, FSA are used as feature functions, and the FT are detected simultaneously with other words). If we read the results carefully, we can see that the use of context information (in MSRSeg) achieves a higher precision but a lower recall  X  a small but signi fi cant difference. 5.5 New Words
New words in this section refer to OOV words that are neither recognized as named entities or factoids nor derived by morphological rules. These words are mostly domain-speci fi c and/or time-sensitive, such as  X  Three Links  X  , cation of such new words has not been studied extensively before. It is an important issue that has substantial impact on the performance of word segmentation. For ex-ample, approximately 30% of OOV words in the SIGHAN  X  s PK corpus in Table 4 are new words of this type. There has been previous work on detecting Chinese new words from a large corpus in an off-line manner and updating the dictionary before word segmentation. However, our approach is able to detect new words on-line, i.e., to spot new words in a sentence on the fl y during the process of word segmentation, where widely used statistical features such as mutual information or term frequency are not available.
 as NW 11. Other types of new words such as NW 21 (a two-character word followed with a character) and NW 12 can be detected similarly (e.g., by viewing the two-character word as an inseparable unit, like a character). These three types amount to 85% of all NWs in the PK corpus. Here, we shall describe the class model and context model for NWI, and the creation of training data by sampling. 5.5.1 Class Model. We use a classi fi er (SVM light to estimate the likelihood of two adjacent characters forming a new word. Of the great number of features with which we experimented, four linguistically motivated They are independent word probability (IWP), anti-word pair (AWP), word formation analogy (WFA), and morphological productivity (MP). We now describe each feature in turn. classi fi er is created by sampling.
 acters can be used either as independent words or component parts of multicharacter words, or both. The IWP of a single character is the likelihood of this character to appear as an independent word in texts (Wu and Jiang 2000): where C ( x , W ) is the number of occurrences of the character x as an independent word in training data, and C ( x ) is the total number of occurrences of x in training data. We assume that the IWP of a character string is the product of the IWPs of the component forms a new word. In our implementation, the training data is word-segmented. 0, otherwise.  X   X  [0, 1] is a preset threshold. Intuitively, if one of the component char-acters is very likely to be an independent word, it is unlikely to be able to form a word with any other characters. While IWP considers all component characters in a new word candidate, AWP only considers the one with the maximal IWP value.
 character (or a multicharacter string) z is called the common stem of ( x , y )ifatleastone of the following two conditions hold: (1) character strings xz and yz are lexical words the number of common stems is larger than a preset threshold. The value of WFA for a
NW 11 candidate  X  (xia4-gang3,  X  be laid off  X  ), we have WFA ( )isanaf fi x pair (they have 32 common stems such as  X  , 8 and  X  (shang4-gang3,  X  take over a shift  X  ) is a lexical word.
 productivity of a particular construction, as de fi ned here (Baayen 1989):
MP is strongly related to the Good-Turing estimate. Here, N is the number of tokens of probability that (one of) the component parts of a multicharacter string appears to be a word. For example, Sproat and Shih (2002) show that the MP values of Chinese noun af fi x- X  and verb af fi x- X  are 0.20 and 0.04, respectively, indicating that -more productive af fi x, while the MP value of single-character nouns which belong to 556 a closed and nonproductive class is close to 0. These results are in agreement with our intuition. Similarly, we fi nd some very productive characters with high MP values. For example, in our training set, there are 236 words that contain the character which 13 occur only once. 5.5.2 Context Model. The motivations for using a context model for NWI are twofold.
The fi rst is to capture useful contextual information. For example, new words are more likely to be nouns than pronouns, and POS tagging is context-sensitive. The second is more important. As described in Section 4, with a context model, NWI can be per-formed simultaneously with other word segmentation tasks (e.g., word breaking, NER, and morphological analysis) in a uni fi ed manner.
 tated because  X  we usually do not know what we don  X  tknow.  X  Our solution is Monte
Carlo simulation. We sample a set of new words from our dictionary according to the distribution  X  the probability that any lexical word w would be a new word P ( NW We then generate a new-word-annotated corpus from a word-segmented text corpus. new words are those words whose probability of appearing in a new document is lower than general lexical words. Let P i ( k ) be the probability of word w in a document. In our experiments, we assume that P ( NW | the probability of w i occurring less than K times in a new document: where the constant K (7 in our experiments) is dependent on the size of the document:
The larger the document, the larger the value. P i ( k ) can be estimated using several term distribution models (Chapter 15.3 in Manning and Sch  X  utze [1999]). Following Gao and
Lee (2000), we use K-Mixture (Katz 1996) which estimates P where  X  k ,0 =1 if k =0; 0, otherwise.  X  and  X  are parameters that can be fi tusingthe observed mean  X  and the observed inverse document frequency IDF as follows: where cf is the total number of occurrence of word w number of documents in training data in which w i occurs, and N is the total number of documents. In our implementation, the training data contain approximately 40,000 documents that have been balanced among domain, style, and time. 5.5.3 Evaluation Results. The NWI component has been constructed as an SVM clas-si fi er. This section discusses two factors that we believe have the most impact on the performance of NWI. First, we investigate the relative contribution of the four linguisti-cally motivated features in NWI. Second, we compare methods where we use the NWI component (i.e., an SVM classi fi er) as a post-processor versus as a feature function in the linear models of Equation (4).
 558 the features one at a time and recorded the scores of each ablated NWI component. It turns out that in cases of both NW 11 and NW 12, IWP is obviously the most effective feature.
 uni fi ed approaches (i.e., using the NWI component as a feature function) signi fi cantly outperform consecutive approaches (i.e., using the NWI component as a post-processor) consistently, in terms of both Roov and P/R/F of the overall word segmentation. This demonstrates empirically the bene fi ts of using the context model for NWI and the uni fi ed approach to Chinese word segmentation, as described in 5.5.2. 5.6 Decoder The decoding process follows the source  X  channel framework. It consists of three steps:
Step 1: Throughout the process, we maintain an array of word class candidates, called a lattice , which is initialized to be empty.

Step 2: Given a Chinese sentence, all possible words of different types are generated simultaneously by the corresponding channel models described in Sections 5.2 to 5.5.
For example, as shown in Table 6, the lexicon TRIE generates LW candidates; the SVM classi fi er generates NW candidates, and so on. All the generated candidates are added class tag, and s is the class model score of w assigned by its feature function in Table 6. Some examples are shown in Figure 5.

Step 3: The Viterbi (or A*) algorithm is used to search for the best word class sequence, among all candidate segmentations in the lattice, according to Equations (2) and (3). parameters: 6. Standards Adaptation
This section describes the second component of MSRSeg: a set of adaptors for adjusting the output of the generic segmenter to different application-speci fi c standards. general standard prede fi ned by ourselves. We have also created a large amount of training data that are segmented according to this general standard. We then de-velop a generic word segmenter, i.e., the system described in Section 5. Whenever we deploy the segmenter for any application, we need to customize the output of the acquired. 6.1 Transformation-Based Learning Approach
In MSRSeg, standards adaptation is conducted by a postprocessor that performs an ordered list of transformations on the output of the generic segmenter  X  removing extraneous word boundaries and inserting new boundaries  X  to obtain a word segmen-tation that meets a different standard.
 requires an initial segmentation, a goal segmentation into which we wish to transform the initial segmentation, and a space of allowable transformations (i.e., transforma-tion templates). Under the above-mentioned adaptation paradigm, the initial segmen-tation is the output of the generic segmenter. The goal segmentation is adaptation data. The transformation templates can make reference to words (i.e., lexicalized tem-plates) as well as some prede fi ned types (i.e., class-type based templates), as described below.
 comes from those words that are not typically stored in the dictionary. Those words are dynamic in nature and are usually formed through productive morphological processes. In this study, we focus on three categories: MDW, NE, and FT.
 similar to Wu (2003). The structure is a tree with  X  word class  X  as the root and  X  component
PersonName has two component types and Date has nine  X  3 as non-terminals and 6 as terminals. These internal structures are assigned to words by the generic segmenter at 560 run time. The transformation templates for words of the above three categories are of the form: is conditioned on word class and makes reference to component types, we call the templates class-type transformation templates. Some examples are shown in Figure 6. ductive morphological process. They are mostly single characters, two-character words, and 4-character idioms. 6.2 Evaluation Results The results of standards adaptation on four Bakeoff open test sets are shown in from the corresponding Bakeoff training set. For each test set, we report results using our system with and without standards adaptation (Rows 1 and 2). It turns out that performance improves dramatically across the board in all four test sets.
 ward maximum matching) greedy segmenter as a generic segmenter (Row 3), and the top 2 scores (sorted by F) that are reported in SIGHAN  X  s First International Chinese
Word Segmentation Bakeoff (Rows 4 and 5). 11 We can see that with adaptation, our generic segmenter can achieve state-of-the-art performance on different standards, showing its superiority over other systems. For example, there is no single segmenter in the Bakeoff that achieved top-2 ranks in all four test sets (Sproat and Emerson 2003). largely upon the size of adaptation data (indicated by # of Tr. Word in the tables): we outperformed the best Bakeoff systems in the AS set because of the large size of the adaptation data. To verify our hypothesis, we evaluated the adaptation results using subsets of the AS training set of different sizes and observed the same trend, as shown in Table 15. However, even with a much smaller adaptation data set (e.g., 250K words), we still outperform the best Bakeoff results. 7. Training Data Creation
This section describes (semi-)automatic methods of creating the training data based on the estimated class model probability P ( w ) (i.e., trigram probability) in Equation (1).
Ideally, given an annotated corpus, where each sentence is segmented into words that are tagged by their classes, the trigram word class probabilities can be calculated using
MLE. Unfortunately, building such annotated training corpora is very expensive. 7.1 Bootstrapping Approach and Beyond consists of three steps: (1) Initially, we use a greedy word segmenter to annotate the corpus and obtain an initial context model based on the initial annotated corpus; (2) we reannotate the corpus using the obtained models; and (3) we retrain the context 562 model using the reannotated corpus. 12 Steps 2 and 3 are iterated until the performance of the system converges. This approach is also named Viterbi iterative training, an approximation of EM training.
 upon the quality of the initial annotated corpus, which is, however, not satis fi ed due to two problems. First, the greedy segmenter cannot deal with the segmentation ambigu-ities, and even after many iterations, these ambiguities can only be partially resolved.
Second, many factoids and named entities cannot be identi fi ed using the greedy word segmenter, which is based on the dictionary.
 ities in the initially segmented training data. We classify word segmentation ambi-guities into two classes: overlap ambiguity (OA) and combination ambiguity (CA), corresponding, respectively, to OAS and CAS, de fi ned in Section 3.5.
 single token &lt; OAS &gt; . An example is shown in Figure 7. By doing so, we remove the portion of training data that are likely to contain OA errors. We thus train a context model using the reduced training set that does not contain any OASs. Intuitively, the resulting context model would resolve the ambiguities. The method has been tested on the MSR test set. Our main results are shown in Table 16. We can see that the FMM (or backward maximum matching  X  BMM) method can only resolve 73.1% (or 71.1%) of OAs, while using our method, the resulting context model can resolve 94.3% of the OAs. Our method is different from previous ones that use lexicalized rules to resolve OAS.
For example, Sun and Zuo (1998) report that over 90% of OAs can be disambiguated simply by rules. We reimplemented their method in our experiments and found that method signi fi cantly outperforms the rule-based approaches. Another advantage of our method is that it is an unsupervised approach that requires no human annota-tion. Readers can refer to Li et al. (2003) for more details.
  X  talent  X  and M /  X   X  just able  X  ), as shown in Figure 8. For each CAS, we train a binary the greedy segmenter. Then, for each occurrence of a CAS in the initial segmented training data, the corresponding classi fi er is used to determine whether the CAS should be segmented. Our experiments show that 95.7% of the CAs can be resolved. the baseline method that always chooses the more frequent case of a given CAS, and  X  VSM  X  indicates the accuracy of the VSM-inspired (vector space model) binary classi fi er, which will be described here. 13 Suppose we have a CAS, s, whose position in a sentence is i . We use its six surrounding words w in positions i i + 2, and i + 3 as features. We then de fi ne a set of feature functions to simulate the TF-
IDF scores. Each feature function is a mapping f ( s , w ) TF2( s , w )) be the term frequency of w in the case that s is a 1-word (or 2-word) string. 0.25. We also assign weight  X  for each position empirically (i.e., in our experiments, we be 1-word or 2-word by Equations (18) and (19), respectively. The CAS is a single word if Score1( s ) &gt; Score2( s ), and two words otherwise.
 564 known classi fi ers for this particular task. For example, a maximum entropy classi fi er using the same features achieved an overall accuracy of 94.1%.
 based approach, as described in Section 5.4, to detect FTs in the initial segmented corpus; our method of NER in the initial step (i.e., step 1) is a little more sophisticated. First, we manually annotate named entities on a small subset (called the seed set) of the training data. Then we obtain a context model on the seed set (called the seed model). We thus improve the context model that is trained on the initial annotated training corpus by interpolating it with the seed model. Finally, we use the improved context model relatively small seed set (e.g., 150K words) is enough to get a reasonably good context model for initialization. 7.2 Evaluation Results
To justify the methods just described, we built a large number of context models trained using the Viterbi iterative procedure until convergence, i.e., the improvement of the word segmentation performance of the resulting system, is less than a preset threshold. The results are shown in Table 17, where Row 1 (FMM) presents the seg-mentation results of using the initial corpus segmented by a greedy word segmenter  X  the basic solution described earlier; in Row 2, we resolve segmentation (overlap) am-
Rows 5 to 8, several NE annotated seed sets of different sizes are used, showing the trade-off between performance and human cost. In Rows 1 to 8, we use the raw training set containing approximately 50 million characters. For comparison, we also include in Row 9 the results of MSRSeg, whose context model has been trained on a 20-million-word manually annotated corpus. The experimental results reveal several facts.
 566 8. System Evaluation 8.1 System Results
Our system is designed so that components such as the FT detector and NE recognizer can be  X  switched on or off  X  so that we can investigate the relative contribution of each component to the overall word segmentation performance. To date, we have not done a separate evaluation of MDW recognition. We leave that to future work.
 table (Row 1) the results of using the greedy segmenter (FMM) described in Section 7.
Row 2 shows the baseline results of our system, where only the lexicon is used. It is interesting to fi nd, in Rows 1 and 2, that the dictionary-based methods already achieve quite good recall, but the precision is not very good because those methods cannot cor-rectly identify unknown words that are not in the lexicon, such as factoids and named linear mixture models outperforms the greedy approach (with a slight but statistically signi fi cant difference) because the use of context model resolves more ambiguities in segmentation. The most promising property of our approach is that the linear mixture models provide a fl exible framework where a wide variety of linguistic knowledge when components are switched on in turn by activating corresponding class models, the overall word segmentation performance increases consistently.
 and factoid detection, especially the NE abbreviations, although the tokens of these word types amount to only 8.3% in the MSR test set. The remaining 15% of errors are mainly due to new words. 8.2 Comparison with Other Systems using the MSR Test Set
We compare our system with three other Chinese word segmenters on the MSR test set: 1. The MSWS system is one of the best available products. It is released by 2. The LCWS system is one of the best research systems in mainland China.
 3. The PBWS system is a rule-based Chinese parser (Wu and Jiang 2000) that mentioned four systems only in terms of NER precision and recall and the number used by these systems, it is still very dif fi cult to compare their results automatically. For example,  X  ?  X   X  Beijing city government  X  has been segmented inconsistently as  X  / ?  X   X  Beijing city  X  +  X  government  X  or  X  / ?  X   X  Beijing  X  +  X  city government  X 
ONs in another system. Therefore, we have to manually check the results. We picked 933 sentences at random containing 22,833 words (including 329 PNs, 617 LNs, and 435 ONs) for testing. We also did not differentiate LNs and ONs in evaluation. That is, we only checked the word boundaries of LNs and ONs and treated both tags as interchangeable. The results are shown in Table 22. We can see that in this small test set, MSRSeg achieves the best overall performance of NER and the best performance of resolving OASs. 8.3 Evaluations on Bakeoff Test Sets
Table 23 presents the comparison results of MSRSeg on four Bakeoff test sets with others reported previously. The layout of the table follows (Peng, Feng, and McCallum 2004). SXX indicates participating sites in the 1st SIGHAN International Chinese Word
Segmentation Bakeoff (Sproat and Emerson 2003). CRFs indicates the word segmenter reported in Peng, Feng, and McCallum (2004), which uses models of linear-chain con-568 ditional random fi elds (CRFs). Entries contain the F-measure of each segmenter on different open runs, indicated by XXo, with the best performance in bold. Column Site-
Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg.
Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. For com-particular corpus being tested on. No other material was allowed (Sproat and Emerson 2003). Since MSRSeg uses the MSR corpus for training, our results are of open tests. training material. 16 Second, there is no single segmenter that performs best in all four data sets. Third, MSRSeg achieves consistently high performance across all four data sets. For example, MSRSeg achieves better average performance than the other particular, MSRSeg outperforms them on every data set. There are two segmenters that achieve better average F-measure than ours. One is S02, which reported results on CTB only. The other is S10, which reported results on CTB and PK. From these results, we conclude that MSRSeg is an adaptive word segmenter that achieves state-of-the-art performance on different data sets, corresponding to different domains and standards.
 be roughly grouped into two categories: ones that use a rule-based approach and ones that use a statistical approach. MSRSeg is a hybrid system that takes advantage of both approaches. Though rule-based systems (e.g., S08, S10, and S11 in Table 23) can achieve reasonably good results, they cannot effectively make use of increasingly large training data and are weak in unknown word detection and adaptation. Some statistical seg-menters (e.g., S01 and S07 in Table 23) use generative models such as HMM for Chinese word segmentation. However, it is very dif fi cult to incorporate linguistic knowledge into the (approximated or assumed) generation process of Chinese sentences, under-neath which the models are developed. Discriminative models (e.g., the linear models in MSRSeg, where though all components models are derived from generative models, they are combined using discriminatively trained weights) are free from this issue and provide a fl exible mathematical framework to incorporate arbitrary linguistic knowl-edge. They do not assume any underlying generation process. Instead, they assume that the training and test sets are generated from the same distribution, but the form of the distribution (i.e., generative process) is unknown. If we view Chinese word segmenta-tion as a classi fi cation problem, i.e., to discriminate between  X  good  X  segmentations and  X  bad  X  ones, we may prefer discriminative models to generative models. Intuitively, it is suf fi cient to fi nd directly the desired features that can differentiate good segmentations from bad ones (as in discriminative models). It is, however, not necessary to estimate the distributions based upon which Chinese sentences are generated (or segmentations) fi rst, and then use the estimated distributions to construct the desired features (as in generative models). As pointed out by Vapnik (1998):  X  When solving a given problem, solve it directly and try to avoid solving a more general problem as an intermediate step.  X  incorporate arbitrary features and can be discriminatively trained. Our models are novel in that many feature functions are derived from probabilistic or heuristic models inspired by source  X  channel models of Chinese sentence generation, as described in
Section 4.3. Therefore, these feature functions are not only potentially more reasonable but also much more informative than, for instance, the binary features used in standard maximum entropy models in NLP. 570 unknown word detection from word segmentation. Though this would make the de-velopment of the segmenter easier, it seems to be a fl awed solution in reality, as we dis-cussed earlier. The bene fi ts of integrating both tasks has also been shown empirically in
Table 23. 9. Conclusions
This article presents a pragmatic approach to Chinese word segmentation. Our main contributions are threefold. First, we view Chinese words as segmentation units whose de fi nition is pragmatic in nature and depends on how they are used and processed purely linguistic criteria. Second, we propose a pragmatic mathematical framework for
Chinese word segmentation, where various problems of word segmentation (i.e., word breaking, morphological analysis, factoid detection, NER, and NWI) are solved in a uni fi ed approach. The approach is based on linear models where component models are inspired by source  X  channel models of Chinese sentence generation. Third, we de-scribe in detail an adaptive Chinese word segmenter, MSRSeg. This pragmatic system consists of two components: (1) a generic segmenter that is based on the mathematical framework of word segmentation and unknown word detection, and that can adapt to different domain-speci fi c vocabularies, and (2) a set of output adaptors for adapting the output of the former to different application-speci fi c standards. Evaluation on fi ve test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all the test sets.
 applications. We believe that some application-speci fi c features can also be integrated into the framework. For instance, in MT, it would be interesting to investigate how to jointly optimize the performances of both word segmentation and word alignment. the richest resources. Hence, another interesting area of our future work is to explore whether the performance is attributed to a superior architecture or simply to the richer resources. We have developed a simpli fi ed version of MSRSeg, called S-MSRSeg. It estingly, S-MSRSeg achieves very similar (or slightly worse) performance on the fi ve to Chinese word segmentation. The work reported in this article represents not an end but a beginning of yet another view of Chinese word segmentation. Toward http://research.microsoft.com/  X  jfgao) for the sake of encouraging others to improve upon the work we have carried out.
 Acknowledgments References 572
