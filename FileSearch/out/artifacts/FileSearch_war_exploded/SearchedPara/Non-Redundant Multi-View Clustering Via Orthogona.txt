
Typical clustering algorithms output a single clustering of the data. However, in real world applications, data can often be interpreted in many different ways; data can have different groupings that are reasonable and interest-ing from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. Why commit to one clustering solution while all these alternative clus-tering views might be interesting to the user. In this pa-per, we propose a new clustering paradigm for explorative data analysis: find all non-redundant clustering views of the data, where data points of one cluster can belong to different clusters in other views. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) cluster-ing in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to our current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the fea-ture space. We test our framework on both synthetic and high-dimensionalbenchmark data sets, and the results show that indeed our approaches were able to discover varied so-lutions that are interesting and meaningful. keywords: multi-view clustering, non-redundant cluster-ing, orthogonalization
Many applications are characterized by data in high di-mensions. Examples include text, image, and gene data. Automatically extracting interesting structure in such data has been heavily studied in a number of different areas in-cluding data mining, machine learning and statistical data analysis. One approach for extracting information from un-labeled data is through clustering. Given a data set, typical clustering algorithms group similar objects together based on some fixed notion of similarity (distance) and output a single clustering solution. However, in real world applica-tions, data can often be interpreted in many different ways and there may exist multiple groupings of the data that are all reasonable in some perspective.

This problem is often more prominent for high dimen-sional data, where each object is described by a large num-ber of features. In such cases, different feature subspaces can often warrant different ways to partition the data, each presenting the user a different view of the data X  X  structure. Figure 1 illustrates one such scenario. In particular, Fig-ure 1a shows a scatter plot of the data in feature subspace {
F 1 ,F 2 } . Figure 1b shows how the data looks like in fea-ture subspace { F 3 ,F 4 } . Note that each subspace leads to a different clustering structu re. When faced with such a situ-ation, which features should we select (i.e., which cluster-ing solution is better)? Why do we have to choose? Why not keep both solutions? In fact both clustering solutions could be important, and provide different interpretations of the same data. For example, for the same medical data, what is interesting to physicians might be different from what is interesting to insurance companies.
Figure 1. This is a scatter plot of the data in (a) subspace { F 1 ,F 2 } and (b) subspace { F 3 ,F 4 } . Note that the two subspaces lead to different clustering structures.

Hierarchical clustering [18] presents a hierarchical grouping of the objects. However, the different cluster-ing solutions obtained at different hierarchical levels differ only in their granularity  X  objects belonging to the same cluster in fine resolutions remain in the same cluster at the coarser levels. On the other hand, cluster ensemble meth-ods [22, 10] do create a set of cluster solutions but the final goal is still to find a single clustering solution that is most consistent throughout the set.

The goal of exploratory data analysis is to find structures in data, which maybe multi-faceted by nature. Traditional clustering methods seek to find a unified clustering solution and are thus inherently limited in achieving this goal. In this paper, we suggest a new exploratory clustering paradigm: the goal is to find a set of non-redundant clustering views from data, where data points belonging to the same cluster in one view can belong to different clusters in another view .
Toward this goal, we present a framework that extracts multiple clustering views of high-dimensional data that are orthogonal to each other. Note that there are k N possible k disjoint partitioning of N data points. Not all of them are meaningful. We wish to find good clustering solutions based on a clustering objective function. Meanwhile, we would like to minimize the redundancy among the obtained solutions. Thus, we include an orthogonality constraint in our search for new clustering views to avoid providing the user with redundant clustering results. The proposed frame-work works iteratively, at each step adding one clustering view by searching for solutions in a space that is orthogonal to the space of the existing solutions. Within this frame-work, we develop two general approaches. The first ap-proach seeks ort hogonality in the cluster space, while the second one seeks orthogonality in the feature subspace. We present all the multiple view clustering solutions to the user.
In this section, we briefly review the literature related to our research in different aspects.

First, our research can be considered as performing non-redundant clustering [13, 5]. In non-redundant clustering, we are typically given a set of data objects together with an existing clustering solution and the goal is to learn an alternative clustering that captures new information about the structure of the data. Existing non-redundant clustering techniques include the conditional information bottleneck approach [5, 14, 15], the conditional ensemble based ap-proach [16] and the constrained model based approach [17]. Compared to existing non-redundant clustering techniques, the critical differences of our proposed research are: (1) Focusing on searching for orthogonal clustering of (2) Existing non-redundant clustering techniques are lim-
Our framework produces a set of different clustering so-lutions, which is similar to cluster ensembles[22, 10]. A clear distinction of our work from cluster ensembles is that we intentionally search for orthogonal clusterings and do not seek to find a consensus clustering as our end product.
Similar to cluster ensembles, multi-view clustering [3] seek a single final clustering solution by learning from mul-tiple views, whereas our non-redundant multi-view cluster-ing paradigm looks for multip le clustering solutions.
An integral part of our framework is to search a cluster-ing structure in a high-dimensional space and find the corre-sponding subspace that best reveals the clustering structure. This topic has been widely st udied, and one closely related work was conducted by [8]. In this work, after a clustering solution is obtained, a subspace is computed to best cap-ture the clustering, and the clustering is then refined using the data projected onto the ne w subspace. Interestingly, our framework works in an opposite direction. We look at a sub-space that is orthogonal to the space in which the original clustering is embedded to search for non-redundant cluster-ing solutions.

Finally, while we search for different subspaces in our framework, it is different from the concept of subspace clus-tering in [1, 21]. In subspace clustering, the goal is still to learn a single clustering, where each cluster can be embed-ded in its own subspace. In contrast, our method searches for multiple clustering solutions, each is revealed in a dif-ferent subspace. Given data X  X  R d  X  N , with N instances and d features. Our goal is to learn a set of orthogonal clustering views from the data.

There are several ways to find different clustering views [22, 10]. One can apply different objective functions, uti-lize different similarity measures, or use different density models. In general, one can apply different clustering algo-rithms, or utilize one algorithm on randomly sampled (ei-ther in instance space or feature space) data from X .Note that such methods produce each i ndividual clustering inde-pendently from all the other clustering views. While the differences in the objective functions, similarity measures, density models, or different data samples may lead to clus-tering results that differ from one another, it is common to see redundancy in the obtained m ultiple clustering views. We present a framework for successively generating multi-ple clustering views that are orthogonal from one another and thus contain limited redundancy. Below, we describe our framework for generating different clustering views by orthogonalization.

Figure 2. The general framework for generat-ing multiple orthogonal clustering views.
 Figure 2 shows the general framework of our approach. We first cluster the data (this can include dimensionality re-duction followed by clustering), then we orthogonalize the data to a space that is not covered by the existing cluster-ing solutions. Repeat the process until we cover most of the data space or no structure can be found in the remaining space.

We developed two general approaches within this frame-work: (1) orthogonal clustering, and (2) clustering in or-thogonal subspaces.

These two approaches differ primarily in how they rep-resent the existing clustering solutions and consequently how to orthogonalize the data based on existing solutions. Specifically, the first approach represents a clustering so-lution using its k cluster centroids. The second approach represents a clustering solution using the feature subspace that best captures the clustering result. In the next two sub-sections, we describe these two different representations in detail and explain how to obtain orthogonal clustering solu-tions based on these two representations. Clustering can be viewed as a way for compressing data X . For example in k-means [11, 20], the objective function is to minimize the sum-squared-error criterion (SSE): where x i  X  R d is a data point assigned to cluster C j ,and  X  j is the mean of C j .Werepresent x i and  X  j as column vectors. The outputs for k-means clustering are the cluster means and the cluster membership of each data point x i . One can consider k-means clustering as a compression of data X to the k cluster means  X  j .

Following the compression viewpoint, each data point x i is represented by its cluster mean  X  j .Given k X  j  X  X  for rep-resenting X , what is not captured by these  X  j  X  X ? Let us con-sider the space that is spanned by x i , i =1 ...N , we refer to this as the original data space. In contrast, the subspace that is spanned by the mean vectors  X  j , j =1 ...k , is con-sidered the compressed data s pace. Assigning data points to their corresponding cluster means can be essentially con-sidered as projecting the data points from the original data space onto the compressed data space. What is not covered by the current clustering solution (i.e., its compressed data space) is simply its residue space . In this paper, we define residue space as the data projected onto the space orthogo-nal to our current representation. Thus, to find alternative clustering solutions not covered in the current solution, we can simply perform clustering in the space that is orthogo-nal to the compressed data space.

Given current data X ( t ) , and the clustering solution we ter assignments), we describe two variations for represent-ing data in the residue space, X ( t +1) that are based on the  X  X ard X  and  X  X oft X  clustering views respectively.
 Hard clustering. In hard clustering, each data point be-Soft clustering. In soft clustering, each data point can par-
The algorithm for orthogonal clustering is summarized in Algorithm 1. The data is first centered to have zero mean. We then create the first view by clustering the orig-inal data X . Since most of the data in our experiments are high-dimensional, we apply principal components analy-sis [19] to reduce the dimensionality, followed by k-means. Algorithm 1 Orthogonal Clustering.

Inputs: The data matrix X  X  R d  X  N , and the number of clusters k ( t ) for each iteration.

Output: The multiple partitioning views of the data into k ( t ) clusters at each iteration.
 Pre-processing: Center the data to have zero mean. Initialization: Set the iteration number t =1 and X (1) = X .
 Step1: Cluster X ( t ) . In our experiments, we performed
PCA followed by k-means. The compressed solution are the k means,  X  ( t ) j . Each  X  ( t ) j is a column vector in R (the original feature space).

Step2: Project each x ( t ) i in X ( t ) to the space orthogonal to its cluster mean to form the residue space representa-
Step3: Set t = t +1 and repeat steps 1 and 2 until the desired number of views or until the sum-squared-error, Note that one can apply other clustering methods within our framework. We chose PCA followed by k-means because they are popular techniques. In step 2 , we project the data to the space orthogonal to the current cluster representation (using cluster centers) to obtain our residue X ( t +1) .The next clustering view is then obtained by clustering in this residue space. We repeat steps 1 (clustering) and 2 (orthog-onalization) until the desired number of views are obtained or when the SSE is very small. Small SSE signifies that the existing views have already covered most of the data.
In this approach, given a clustering solution with means  X  , j =1 ...k , we would like to find a feature subspace that best captures the clustering structure, or, in other words, discriminates these clusters well. One well-known method for finding a reduced dimensional space that discriminates classes (clusters here) is linear discriminant analysis (LDA) [12, 9]. Another approach is by applying principal compo-nent analysis (PCA) on the k mean vectors  X  j  X  X  [8].
Below we explain the mathematical differences between these two approaches. LDA finds a linear projection Y = A
T X that maximizes the where S w is the within-class-scatter matrix and S b is the between-class-scatter matrix defined as follows.
 where y i  X  X  are projected data points;  X  j  X  X  are projected clus-ter centers; n j is the total number of points in cluster j and  X  is the center of the entire set of projected data. In essence, LDA finds the subspace that maximizes the scatter between the cluster means normalized by the scatter within each cluster.

Similarly, the PCA appro ach in [8] seeks a linear projec-tion Y = A T X , but maximizes a different objective func-tion trace ( MM T ) ,where M =[  X  1  X  2  X  X  X   X  k ] and the  X  are the projected cluster centers.
 For both methods, the solution can be represented as A =[  X  1  X  2  X  X  X   X  q ] , which contains the q most important eigenvectors (corresponding to the q largest eigenvalues) of S w S b for LDA and MM
Note that the trace ( S b )= trace ( M M T ) ,where M = [  X  two approaches is the normalization by the within-class-scatter S  X  1 w and a weighting of each  X  j by n j , the number of data points in cluster C j . But, both M and M span the same space. Thus, in practice, both approaches result in similar results. For computational purposes, we choose the PCA approach on the  X  j  X  X  and set q = k  X  1 , the rank of MM T .

Once we have obtained a feature subspace A = [  X  1  X  2  X  X  X   X  k  X  1 ] that captures the clustering structure well, we project X ( t ) to the subspace orthogonal to A to obtain the residue X ( t +1) = P ( t ) X ( t ) . The orthogonal projection
Algorithm 2 presents the pseudo-code for clustering in orthogonal subspaces. In step 1 , we apply a clustering algo-rithm (PCA followed by k-means in our experiments). We then represent this clustering solution using the subspace that best separates these clusters. In step 2 , we project the data to the space orthogonal to the computed subspace rep-resentation. We repeat steps 1 and 2 until the desired num-ber of views are obtained or the SSE is very small.
In this section, we investigate whether our multi-view or-thogonal clustering framework can provide us with reason-able and orthogonal clustering views of the data. We start by performing experiments on synthetic data in Section 4.1 to get a better understanding of the methods, then we test the methods on benchmark data in Section 4.2. In these ex-periments, we chose as our base clustering  X  PCA followed by k-means clustering. This means, we first reduce the di-mensionality with PCA, keeping a dimensionality that re-tains at least 90% of the original variance, then follow PCA Algorithm 2 Clustering in Orthogonal Subspaces.

Inputs: The data matrix X  X  R d  X  N , and the number of clusters k ( t ) for each iteration.

Output: The multiple partitioning views of the data into k ( t ) clusters, and a reduced dimensional subspace for each iteration A ( t ) .
 Pre-processing: Center the data to have zero mean. Initialization: Set the iteration number t =1 and X (1) = X .
 Step1: Cluster X ( t ) . In our experiments, we performed
PCA followed by k-means. Then, find the PCA solution dimensions to obtain the subspace, A ( t ) , that captures the current clustering.

Step2: Project X ( t ) to the space orthogonal to A ( t ) produce X ( t +1) = P ( t ) X ( t ) , where the projection oper-ator P ( t ) is:
Step3: Set t = t +1 and repeat steps 1 and 2 until the desired number of views or until the sum-squared-error, with k-means clustering. B ecause we apply k-means clus-tering, we implement orthogonal clustering with the  X  X ard X  assumption. In this section, we refer to the orthogonal clus-tering approach as method 1 , and the clustering in orthogo-nal subspaces as method 2 .
We would like to see whether our two methods can find diverse groupings of the data. We generate two synthetic data.
 Data 1: We generate a four-cluster data in two dimensions Data 2: We generate a second synt hetic data in four dimen-Table 1. Confusion Matrix for Synthetic Data1 4.1.1 Results for Synthetic Data 1 The confusion matrix in Table 1 shows the experimental re-sults for synthetic data 1 for methods 1 and 2 , in two iter-ations. We can see that for the first iteration, both meth-ods grouped classes L 1 and L 3 into a single cluster C 1 classes L 2 and L 4 into another cluster C 2 . For the second iteration, the data was partitioned in a different way, which grouped classes L 1 and L 2 into one cluster, and classes L and L 4 into another cluster. Figure 3 shows the scatter plot of the clustering results of both methods in the original data space for the two iterations. Different colors are used to signify the true classes, and the ellipses show the clusters found by k-means. The figure confirms the result summa-rized in the confusion matrix. Both methods 1 and 2 have similar results as shown. In subfigure a 3 and b 3 of Fig-ure 3, we plot the sum-squared-error (SSE) as a function of iteration. Note that, as expected, SSE for both methods decreases monotonically until convergence. Moreover, the SSE at iteration 2 and after is zero meaning that the first two clustering views have covered the data space completely. 4.1.2 Results for Synthetic Data 2 Table 2 shows the confusion matrix for our clustering with the two different labelings: labeling 1 is for features 1 2 , and labeling 2 is for features 3 and 4 . High number of common occurrences means that the cluster correspond to those labels. Observe that for both methods 1 and 2 ,they found the clusters in labeling 2 (features 3 and 4 ) perfectly with zero confusion in the off-diagonal elements in the first iteration/view. In the second iteration/view, methods 1 and (a1.iteration1 for method1) (b1.iteration1 for method2) (a2.iteration2 for method1) (b2.iteration2 for method2) Figure 3. Scatter plots of synthetic data 1.

The two columns show the results of meth-ods 1 and 2 respectively. The colors repre-sent different class labels and the ellipses represent the clusters found. Row 1 and 2 show the results for iteration 1 and 2 respec-tively; Row 3 shows SSE as a function of iter-ation. 2 found the clusters in labeling 1 (features 1 and 2 )per-fectly also with zero confusion. This result confirms that indeed our multi-view approach can discover multiple clus-tering solutions in different s ubspaces. Figure 4 shows scat-ter plots of the data. The left column ( ( a 1) , ( a 2) , the plot for method 1 . ( a 1) shows the clustering in ellipses found by method 1 in iteration 1 . The left sub-figure shows the groupings in the original features 1 and 2 ,andthedata points are colored based on true labeling 1 . The right sub-figure shows the clusterings in the original features 3 and 4 , and the color of the data points are based on true label-ing 2 . ( a 2) is the same scatter plot of the original data X with the clusters found by method 1 as shown by the ellipses in iteration 2 . Similarly, ( b 1) and ( b 2) show the results of method 2 . ( a 3) and ( b 3) are the SSE for the two methods in each iteration. Method 2 converges to zero much faster than method 1 here. Note that SSE monotonically decreases with iteration and that the algorithm captures most of the information in two clustering views. From these results, in iteration 1 we found the right partition based on features 3 and 4, but group the clusters in features 1 and 2 incorrectly. On the other hand, iteration 2 groups the clusters based on features 1 and 2 correctly, but the partition for the clusters in features 3 and 4 is wrong. The results confirm that indeed our multi-view approach can discover multiple clustering solutions in different subspaces.
 Table 2. Confusion Matrix for Synthetic Data2
We have shown that our two methods work on synthetic data. Here, we investigate whether they reveal interesting and diverse clustering solutions on real benchmark data. We select data sets that have high-dimensionality and that have multiple possible partitioning.

In this section, we investigate the performance of our multi-view orthogonal clustering algorithms on four real world data sets, including the digits data set from the UCI machine learning repository [4], the face data set from the UCI KDD repository [2], and two text data sets: the mini-newsgroups data [2] and the WebKB data set [6].

The digits data is a data set for an optical recogni-tion problem of handwritten digits with ten classes, 5620 cases, and 64 attributes (all input attributes are integers from 0 ... 16 ). The face data consists of 640 face images of 20 people taken with varying pose (straight, left, right, up), expression (neutral, happy, sad, angry), eyes (wear-ing sunglasses or not). Each person has 32 images captur-ing every combination of feat ures. The image resolution is 32  X  30 . We removed the missing data and formed a 960  X  624 data matrix. Each of the 960 features represents a pixel value. The mini-newsgroups data comes from the UCI KDD repository which contains 2000 articles from 20 (a1.iteration1 for method1) (b1.iteration1 for method2) (a2.iteration2 for method1) (b2.iteration2 for method2)
Figure 4. These are scatter plots of synthetic data 2 and the clusters found by methods 1 (a1, a2) and 2 (b1, b2). The color of the data points reflect different class labels and the el-lipses represent the clusters found. a1, b1 are the results for iteration 1; a2, b2 are the results for iteration 2; a3 and b3 are SSE as a function of iteration for methods 1 and 2 re-spectively. newsgroups. The second text data is the CMU four univer-sity WebKB data set as described in [6]. Both text data sets were processed following the standard procedure, including stemming and removing stopwords. 4.2.1 Results for the Digit Data Table 3 shows the confusion matrix for the digit data for both methods 1 and 2 . Because the results for methods 1 and 2 are similar, we will concentrate on the results from method 1 . For both iterations, we partition the data into three clusters. In iteration 1 , the resulting partition clustered In iteration 2 , our method clustered { 0,3,5,7,9 } , { 2,6,8,1 and { 4 } into another set of clusters. And, in iteration clusters we found are { 0,1,8,9 } , { 2,3,6,7,4 } and { 5 results show that in each itera tion we can find a different way of partitioning the ten classes (digits).

In Figure 5, we present the mean image of each cluster obtained by method 1 in three iterations. Below each image we show the dominant digits contained in the cluster. For a digit to be considered as contained in a cluster, we require that at least 70% of its data points fall into the cluster. It is interesting to note that digits 4 and 5 were not well captured by any of the clusters in iteration 1 . In contrast, in iteration 2 , we see digit 4 well separated and captured by cluster 2 In iteration 3, we were able to capture digit 5 nicelyina single cluster. This further demonstrated that our method is capable of discovering multiple reasonable structures from data. 4.2.2 Results for the Face Data Face data is a very interesting data set because it can be grouped in several different ways (e.g., by person, pose, etc.). We design the experiment to see if we can get dif-ferent clustering information in different iterations.
First, we begin with our number of clusters K =20 in the first iteration, hopefully to find the 20 persons in the
Figure 5. The average digit for images within each cluster found by method 1 in itera-tions/views 1, 2 and 3. These clustering views correspond to different digits. database. Then, from the second iteration to the rest of the iterations, we set K =4 to see if the partitions found in the remaining iterations can tell us any useful information. Fig-ure 6 shows the average image for each cluster we find in iteration 1 . We observed from this figure and from the con-fusion matrix (not shown due to space limitations and infor-mation clutter) that iteration 1 leads to a clustering corre-sponding to the different persons in the database.The num-ber below the image is the percentage this person appears in the cluster. It is a confidence measure of the identifica-tion. And the images clearly show different persons. In the second iteration, the four clusters we found are shown in Figure 7. Each image is an average image of the images within each cluster. It is clear th at the clustering in iteration 2 groups the data based on different poses. This again sug-gested that our method can find different clustering views from the data. The more independent the important parti-tions lie in the data, the better are the results of our method. Since method 2 gave us similar images, we only provided the results by method 1 here due to space limitation. 4.2.3 Results for the Mini-Newsgroups Data The mini-newsgroups data set originally contains 20 classes. We removed the classes that are under the  X  X isc X  category because it does not correspond to a clear concept class. We also pre-processed the data to remove stop words, words that appeared in less than 40 documents, and the words that had low variance of occurrence in all documents. After the pre-processing, the data contains 1700 documents from 17 classes. Each document is represented by a 500 -dimensional term frequency vector.

Note that PCA followed by k-means does not work well
Figure 6. The average face image for each cluster found by method 1 in iteration 1. This clustering view corresponds to different per-sons. for text data. Here, we apply the spherical k-means method [7] instead, which consider s the correlation between docu-ments rather than the Euclidean distance. Our experiments showed that this method provided a reasonable clustering of thetextdatasets.

Table 5 shows the confusion matrices by method 1 for three iterations. For the first iteration, we set K =3 .The results show that cluster C 1 groups together recreation and computer categories. The ten most frequent words tell us that the documents here share i nformation related to enter-tainment. Cluster C 2 groups science and talks together, and the frequent words confirm that it groups science and the religion part of the talk. Cluster C 3 is a mixture of different topics.

In iteration 2 ,weset K =4 to see if it we can partition the data to capture the four cat egories  X  X omputer X ,  X  X ecre-
Figure 7. The average face image for each cluster found by method 1 in iteration 2.

This clustering view corresponds to different poses. ation X ,  X  X alk X  and  X  X cience X . From the confusion matrix, we see that we were able to find these high level categories. C 1 is about computers; C 2 contains news about recreation; and C 3 groups those files related to science. The last one C 4 contains documents from the  X  X alk X  category that are related to politics.

In iteration 3 , two of the  X  X omputer X  classes (graph-ics, os.ms) were grouped together with the  X  X alk X  category, the remaining three  X  X omputer X  classes were grouped to-gether with the  X  X ecreation X  category (auto, motorcycles and sports). This suggests that our method continued find-ing clustering structure that is different from the existing results. 4.2.4 Results for the WebKB Text Data This data contains 1041 html documents, from four web-page topics: course, faculty, project and student. Alterna-tively, the webpage can also be grouped based on their re-gions/universities, which include four universities: Cornell University, University of Texas Austin, University of Wash-ington and Wisconsin Madison. Following the same pre-processing procedure used for the mini-newsgroups data, we removed the rare words, stop words, and words with low variances. Finally, we obtained 350 words in the vo-cabulary. The final data matrix is of size 350  X  1041 .
The experimental results are quite interesting. For the first iteration, we see our method found the partition that mostly corresponds to the different topics, which can be seen in Table 4. Cluster 1 contains course webpages, cluster 2 is a mix of faculty and project pages, cluster 3 consists of a majority of student webpages. In the second iteration, our method found a different clustering that corresponds to the universities, as shown in Table 4.
The goal of explorative data analysis is to find the un-derlying structure from a given set of data, which may be multi-faceted by nature. Existing work on non-redundant clustering attempts to address this problem by searching for a single alternative clustering solution that is different from an existing one. Our main contribution in this paper is that we introduced a new paradigm for explorative data cluster-ing that seeks to extract all non-redundant clustering views from a given set of data (until there is only noise left in the data).

We presented a general framework for extracting mul-tiple clustering views from high dimensional data. In essence, this framework works by incorporating orthog-onality constraints into a clustering algorithm. In other words, the clustering algorithm will search for a new clus-tering in a space that is orthogonal to what has been covered by existing clustering solutions. We described two different methods for introducing o rthogonality and conducted a va-riety of experiments on both synthetic data and real world benchmark data sets to evaluate these methods. The results can be summarized as follows. 1. Using two different synthetic data sets, our proposed 2. On benchmark data sets, our methods not only found
Note that this paper uses k-means as the basic cluster-ing algorithm. However, the framework is designed with no specific algorithm in mind and can work with any clus-tering algorithm. Future directions will be to explore the framework with other clustering methods.

This research is supported by NSF CAREER IIS-0347532. Table 5. Confusion Matrix for the Mini-Newsgroups Data
