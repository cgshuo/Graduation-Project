 Community Question Answering services, e.g., Yahoo! An-swers, have accumulated large archives of question answer (QA) pairs for information and answer retrieval. An effective question retrieval model is essential to increase the accessi-bility of the QA archives. QA archives are usually organized into categories and question search can be performed within the whole collection or within a certain category.
In this paper, we explore domain-specific term weight for archived question search. Specifically, we propose a novel light-weighted term weighting scheme that exploits multi-ple aspects of the domain information. We also introduce a framework to seamlessly integrate domain-specific term weight into the existing retrieval models. Extensive experi-ments conducted on real Archived QA data demonstrate the utility of the proposed techniques.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Question Search, Domain-specific term weighting, Commu-nity Question Answering Archive
Community-based Question Answering (CQA) sites have become increasingly popular and they have accumulated very large archives of user generated question answer pairs that form valuable knowledge bases for information seekers. To effectively share the knowledge in Archived Question An-swer repository, it is essential to develop effective question search models that are capable of returning relevant ques-tions and answers for a user query.

The existing widely used information retrieval models, in-cluding TFIDF, Okapi BM25, and Language Model capture document-level and collection-level evidences in their term weighting schemes. Going beyond the traditional bag-of-word approaches, recently Xue et al. [9] exploits semantic relations between terms using translation model and Wang etal. [8]utilize sthesyntacti crelation sbetweensentences.
In this paper, we aim to explore domain-level evidence of questions to complement the existing retrieval models. As a specific application of IR, Question Search in CQA reposi-tory is distinct from the search of web pages or news articles in that questions are organized into categories. This makes it possible to extract domain-specific information to enhance question search. Intuitively, a word may be more important in some domain than other domains. For example,  X  X hutter X  is an important word in domain Consumer Electronics , es-pecially the sub-domain camera , but not in domain Health . To utilize domain-level evidence to enhance the existing re-trieval models, first we need derive domain-level evidence, which is non-trivial. It is expected that the domain-level evidence would reflect the domain topic.

To achieve this, we compute the domain-level evidence for each term by taking into account three aspects, namely, General Collection Based Evidence , Sub-domain Based Evi-dence , and Entropy Based Evidence , as complements to mea-sure term importance together with document level evidence and collection level evidence. We proceed to present them in the following section. Divergence Feature
The divergence of term distribution in a specific collection from a general collection reveals the significance of terms globally in its domain. We employ Jensen-Shannon (JS) di-vergence to capture the difference of term distribution in two collections. It is defined as the mean of the relative entropy of each distribution to the mean distribution. We examine the point-wise function for each individual term as follows: where S and G denote the domain-specific and general vo-cabularies, with their probability distributions p s ( t i p ( t i ) obtained by the Maximum Likelihood Estimator on the specific and the general vocabularies, respectively. Estimating Term Saliency from Divergence Feature We define a mapping function f n : d JS !  X  s 1 , which pro-duces as output an estimation  X  s 1 (denotes the term saliency score generated by aspect 1 general collection based evi-dence) given d JS as input. This function has a normal-ization effect on the raw score of aspect 1. We propose a heuristic evaluation function based on logistic function L ( x ) as f n ( x ) = 1 +  X  L ( x +  X  ). Therefore the final form of the aspect 1 term saliency score  X  s 1 is:
Term distribution in a domain is likely to vary from one sub-domain to another. To capture the specificity of terms with regard to a sub-domain, we propose to measure the specificity of terms within a domain by comparing the term distribution in each sub-domain to that in the domain col-lection. The terms that have different distributions in a sub-domain and the whole domain would be important to characterize the sub-domain from the general domain.
Aspect 2 term saliency score  X  s 2 is calculated the similar way as in Aspect 1. Let S denote a domain collection and S denote a sub-domain of S . Given p s ( t ) and p S s ( t ) (the prob-ability of t in a sub-domain S s ), we compute the difference of term distributions between domain S and subdomain S s by invoking Equation 1 so as to get d JS ( t, S jj S s ). Similarly, we apply the Equation 2 on d JS ( t, S jj S s ) to do the saliency estimation (normalization) to obtain  X  s 2 .
To capture the specificity of terms with regard to all the sub-domains of a domain, we compute the entropy for terms across the sub-domains in a domain. Intuitively, a term of high entropy is more likely to occur in many sub-domains while a term of low entropy tends to occur only a few sub-domains. We value the terms with low entropy since they are more distinctive.
 where p ( C j t ) = tf ( C, t ) / the frequency of the term t within a sub-domain Z .
Since low entropy terms are deemed as important, the term saliency score from Aspect 3  X  s 3 is thus defined as the inverse of the entropy: where  X  is a smoothing parameter we set as 0 . 001.
We use a linear interpolation of the three aspects to derive the domain-level evidence  X  de ( t ) =  X  1  X  s 1 +  X  2  X  s Vector Space Model
The Vector Space Model has been used widely in ques-tion retrieval [5,6]. We consider a popular variation of this model [11]: given a query q , the ranking score S q ; d of the question d can be computed as follows: Here N is the number of questions in the collection, f t is the number of questions containing the term t , and tf t; d is the frequency of term t in d .
 BM25 Model While the Vector Space Model favors short questions, the Okapi BM25 Model [7] takes into account the question length to overcome this problem. The Okapi Model is used for question retrieval by Jeon et al. [5]. Given a query q and a question d , the ranking score S q ; d is computed as follows: Here N is the number of questions in the collection; f t is the number of questions containing the term t ; tf t; d is the frequency of term t in d ; k , and b are set to 1.2 and 0.75, respectively, by following Robertson et al. [7] ; and W d question length of d and W A is the average question length in the collection.
 Language Model
The Language Model is used in previous work [2,3,5] for question retrieval. The basic idea of the Language Model is to estimate a language model for each question, and then rank questions by the likelihood of the query according to the estimated model for questions. We use Dirichlet smooth-ing [10]. Given a query q and a question d , the ranking score S q ; d is computed as follows: Here C is the collection and  X  is the smoothing parameter.
The aforementioned Bag-of-Word retrieval functions can be generalized to the following format: where t i is the i th query term, and  X  ( ) is a term weighting model. Generally,  X  ( ) is a function that takes in document level and collection level evidences of a term. Note that language model can be transformed into the general form by logarithmic transformation.

To accommodate the three-aspect domain level saliency score  X  de ( t i ), we introduce a general framework as follows: We collect questions from two top categories, Consumer Electronics ( CE ) and Heath of Yahoo! Answers. Each cat-egory encompasses a few subcategories. We view each cate-gory as a domain and the sub-categories its sub-domains.
For the query set, we randomly select 300 questions from either domain X  X  archive, with the remaining 864013 and 682747 questions as the searching corpora for CE and Health respec-tively. As in the real question search scenario, only subject is used in the queries. After preprocessing, we obtained 253 and 266 questions for either domain respectively. From the remaining questions, we randomly choose 230 for testing, and the others (23 and 36 questions, respectively) are used for development. Evaluation has been performed by pooling the top 20 results from various methods.

To evaluate the proposed term weighting scheme on dataset of different document lengths, we generate two versions of searching corpora: one contains the subject field of questions only and the average length of question is about 10 words, the other concatenates all the three fields subject + content + best answers , with average length of about 124 words.
To evaluate the performance of the proposed term weight-ing scheme, we use three sets of question search models, each set consisting of three methods. The first set is based on the Language Model (LM): (1) LM (baseline): Language Modeling approach. (2) LM@OpS: This model searches in the subdomain that the query was originally assigned in Yahoo! Answers. We view it as the  X  X ptimal X  subdomain information. (3) LM+  X  de : This model integrates the term score from LM and the proposed  X  de domain-level evidence.

Similarly, the other two sets of search models are based on Vector Space Model ( V SM ) and Okapi BM25 ( BM 25), respectively. Hence, we have another six models, V SM (the baseline of the set), V SM @ OpS , V SM +  X  de ; and BM 25(the baseline of the set), BM 25@ OpS , and BM 25 +  X  de .
The parameters in the above systems are tuned using the development queries. For the proposed method in Equa-tion 2, we fix  X  to be 1 . 0 as it does not affect the shape of the function.  X  is set as 2 . 0 as the optimal range is [2 . 0 , 3 . 0], and  X  1 ; 2 ; 3 = 1 3 The smoothing parameter  X  of LM is set to be 600, and for Okapi BM25 k = 1 . 2, b = 0 . 75.
Table 1 summarizes the experimental results in Mean Av-erage Precision (MAP) using the three set of retrieval models on the two types of data, subj and long on two domains CE and Health . We make the following observations: (1) All the three  X  de (domain-level evidence) enhanced retrieval models achieve significant improvement over their respective baseline. This shows that the proposed term weighting scheme combines well with the existing retrieval models. Among the three set of retrieval models, VSM ben-efits the most, followed by BM25 and LM. The possible rea-son, we conjecture, would be that the smoothing effect of Table 1: y indicates statistical signi cance over the respective baselines at 0 . 95 con dence interval us-ing the t-test. %chg denotes the performance im-provement in percent of each domain-level evidence enhanced model over the corresponding baseline.
 LM distinguishes between the salient and trivial terms in the collection of a domain, and therefore reflects the domain specificity of the terms to some extent. The idf component in VSM and BM25 has limited effect on reflecting the term domain specificity, and thus benefit more from a comple-mentary domain-specific term weighting factor. (2) The baseline models searching in the optimal sub-domain do not consistently improve the respective baseline searching in the whole domain. The results show that the querys X  sub-domain information does not help the retrieval performance when used directly. This may be because rele-vant questions are also contained in other sub-domains but not only in the sub-domain of query questions, and some of the questions are not correctly assigned to subdomains in the Yahoo! Answers. However, when applying  X  de to enhance retrieval models, the sub-domain of a question is implicitly obtained by the sub-domain based evidence  X  s 2 Sub-domains that have the higher accumulated  X  s 2 of the terms in the question are more likely the true sub-domain of the question. This suggests that  X  s 2 does an implicit classification of the questions searched. (3) The proposed term weighting scheme works well on both short (only subject field) and long questions (each con-sisting of subject, content and best answer). The improve-ment on long is slightly lower than that on subj field. By comparing subj and long , we observe that longer documents have higher baseline performances. The reason might be that questions with only subj contain one single sentence that both the salient and trivial terms appear only once. Given the domain-specific information, the subj questions retrieval can be even better enhanced.
 To get a better understanding of the comparing models, Table 2 gives part of the results of an example query ques-tion. As can be seen, the top 2 returned results of LM are both irrelevant though they share a large portion of words with the query. The problem is that the salient terms like  X  X ood X ,  X  X oloring X , and  X  X ye X  are treated the same as less im-portant terms like  X  X hat X ,  X  X ould X ,  X  X appen X , etc. . Though it is possible to remove  X  X hat X  and  X  X ould X  using a well con-structed stopword list, the list may be hard to adapt to all cases. And terms like  X  X appen X  may be arguable.

The top 3 results using LM@OpS are quite similar to the query but do not capture the main content. It is consistent with our earlier observation that searching in the optimal subdomain is not a good way to use sub-domain information. We see that the proposed model LM +  X  de returns questions that share mainly content words with the query. It indicates that the proposed model recognizes the domain specificity of the terms and assigns weights properly.
Question retrieval has recently been investigated for CQA data. Jeon et al. [4,5] compare four different retrieval meth-ods, i.e. , the vector space model, the Okapi model, the lan-guage model, and the translation model, for question re-trieval on CQA data, and the experimental results show that the translation model outperforms the other models. In sub-sequent work [9], they propose a translation-based language model that combines the translation model and the language model for question retrieval. Duan et al. [3] propose a so-lution that makes use of question structures for retrieval by building a structure tree for questions in a category of Ya-hoo! Answers to discover question topic and question focus. Recently, Wang et al. [8] employ a parser to build syntactic trees of questions, and questions are ranked based on the similarity between their syntactic trees and the syntactic tree of the query question.

We are also aware of the recent work by Cao et al. [1, 2] that exploits the question categories in CQA data for question retrieval. However, the proposed category-based smoothing approach [2] is tightly coupled with the langauge model and is difficult to apply to other retrieval models. Cao et al. [1] propose to compute the ranking score of a question by a linear combination of the relevance score of a query to the question and the relevance score of a query to the category containing the question. While the method [1] aims to improve question search in the whole collection, our proposed techniques aim to improve question search within a certain domain, which is offered as an option for question search by many CQA services.
In this paper, we propose a novel term weighting scheme that exploits multiple aspects of the domain information to generate domain-level evidence to enhance the existing bag-of-words information retrieval models. Extensive experi-ments conducted on real Archived QA data from Yahoo! An-swer demonstrate that the proposed techniques significantly improve the performance of the existing retrieval models.
This work opens to several interesting directions for future work. First, it is of relevance to evaluate the performance of the proposed domain-specific term weighting scheme on longer documents like web pages. Second, it would be inter-esting to build a domain-specific stopword list according to the proposed methods of computing domain-level evidences for each term. [1] X. Cao, G. Cong, B. Cui, and C. S. Jensen. A [2] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. [3] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching [4] J. Jeon, W. B. Croft, and J. H. Lee. Finding [5] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [6] V. Jijkoun and M. de Rijke. Retrieving answers from [7] S. Robertson, S. Walker, S. Jones, [8] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree [9] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [10] C. Zhai and J. Lafferty. A study of smoothing [11] J. Zobel and A. Moffat. Inverted files for text search
