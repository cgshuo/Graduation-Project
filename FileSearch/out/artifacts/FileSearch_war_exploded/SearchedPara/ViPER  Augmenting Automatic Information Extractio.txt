 In this paper we address the problem of unsupervised Web data extraction. We show that unsupervised Web data ex-traction becomes feasible when supposing pages that are made up of repetitive patterns, as it is the case, e.g., for search engine result pages. Hereby the extraction rules are generated automatically without any training or human in-teraction, by means of operating on the DOM tree respec-tively the flat tag token sequence of a single page.
Our contribution to automatic data extraction through this paper is twofold. First, we identify and rank poten-tial repetitive patterns with respect to the user X  X  visual per-ception of the Web page, well aware that location and size of matching elements within a Web page constitute impor-tant criteria for defining relevance. Second, matching sub-sequences of the pattern with the highest weightiness are aligned with global multiple sequence alignment techniques. Experimental results show that our system is able to achieve high accuracy in distilling and aligning regularly structured objects inside complex Web pages.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous X  Data Extraction, Wrapper Generation, Web Algorithms, Experimentation, Performance Data extraction, data record alignment, visual features Rife information content available on the World Wide Web is published through representation-oriented semi-struc-tured HTML pages. According to the process of their gener-ation they could be divided into static and dynamic pages. Static pages, bearing stable or rarely changing content, can easily be crawled and indexed by common search engines. Dynamic pages, on the other hand, consist of rapidly chang-ing content, e.g., news sites or pages changing their con-tent based on user requests. Typically these sites are filled with information from back-end databases and generated by predefined templates or server-sided scripts. Although they have a unique URL address, crawling and indexing becomes otiose by virtue of their volatile nature. Hence there arises the need for new information services that can help users locate information in the Hidden Web . Possi-ble solutions might be high dimensional meta-search engines with hundreds of thousands of subordinate information re-sources or Web agents scanning the Web for hidden infor-mation. Hereby both approaches have to be equipped with mechanisms which distill the relevant information and sub-sequently align the extracted data in order to provide more suitable data representation.

Under the aspect of extracting information from thou-sands of information resources manual or semi-automatic approaches become infeasible, therefore this paper presents a new fully-automatic information extraction tool named ViPER ( Vi sual P erception-based E xtraction of R ecords). The principle assumption made is that a Web page con-tains at least two multiple spatially consecutive data records building a data region which exhibits some kind of struc-tural and visible similarity. ViPER is then able to extract and discriminate the relevance of different repetitive infor-mation contents with respect to the user X  X  visual perception of the Web page. Having identified the most relevant data region the tool aligns these records utilizing a fast and robust multiple sequence alignment (MSA) technique. In our opin-ion global alignment techniques, which have not yet been applied, bear most flexible opportunities in this context. To show the efficiency and accuracy of the extraction and align-ment process we used an available third-party test bed with manually extracted and labeled data. Additionally we com-pared ViPER with existing state-of-the-art wrapping tools resulting in encouraging results.
Wrapper tools for extracting information from HTML pag-es started attracting major research interest during the mid-nineties. One significant characteristic is their degree of au-tomation, reaching from specially designed standalone wrap-per programming languages, for manual wrapper genera-tion, over machine learning, and interactive approaches with more or less human interaction to fully-automatic wrapper tools. We refer the interested reader to [12] for a brief sur-vey of different wrapping techniques. Whereas the majority of the approaches rely on user interaction to generate ex-traction rules, more recently, interest in automatically gen-erate wrappers without human involvement has grown sub-stantially. Most relevant to our approach are IEPAD[4], DeLa[16], MDR[13] and recently ViNTs[14], and DEPTA[19] which fall into the category of fully-automatic wrapper tools.
IEPAD generates extraction rules utilizing a decoded bi-nary string of the HTML tag sequence and tries to find maximal repeated patterns using a PAT tree , which is rather similar to a suffix tree. The extracted pattern becomes gen-eralized using multiple string alignment techniques. Finally the user has to choose one of these generalized patterns as an extraction rule. The approach is fairly simple and fast but tests in [13] have shown that it provides poor results when the data records have a complex, nested structure. In our system we also search for maximal repeats but instead of trying to find data records with this technique we use it in the context of data record alignment.

DeLa tries to overcome the problem of nested-structured data records recognition by multi-level continuous repeat (c-repeat) detection using suffix trees. The drawback of this approach is that repetitive structures are not that obvious contained in a data record because of optional tag elements. Finally heuristics are needed to prune the number of differ-ent patterns discovered. Inspired by this work we integrated a tandem repeat (c-repeats) identification instance into our approximate pattern search to cope with additional repeti-tive tag elements which often discards possible matches.
ViNTs [14] fully-automatically generates SRR ( search re-sult record ) extraction rules using visual context features and tag structure information. Therefore it first utilizes the visual content without considering the tag structure to iden-tify content regularities denoted as content lines and then combines them with the HTML tag structure regularities to generate wrappers. To weight the relevance of different ex-traction rules visual and non-visual features have been used. ViNTs builds a wrapper for a search engine using several re-sult pages and one no-result page. The resulting wrapper is represented by a regular expression of alternative horizontal separators tags, i.e., &lt;HR&gt; or &lt;BR&gt;&lt;BR&gt; , which segment descendants into SRRs. Due to the fact that the ViNTs sys-tem only supplies horizontal separators, which is sufficient when considering document result pages, it could not sep-arate horizontally arranged data records which will require vertical separators. Additionally at least four data records have to be present in a Web page for wrapper building and in case the data records are distributed over multiple sections only the major section is reported.

MDR [13] identifies data regions by searching for mul-tiple generalize-nodes using edit-distance similarity where generalize-nodes are a fix combination of multiple child nodes and their corresponding subtrees. MDR does not identify the most relevant data records but rather reports each repet-itive sub-region contained in a Web page. Recently the authors proposed an improvement of their system named DEPTA [19] (MDR-2) operating on a tag tree built accord-ing to visual rendering information. Additionally they men-tion that gap information is incorporated to eliminate false node combinations but nothing is said about the realization. Finally they proposed an approach for data record alignment by progressively growing a seed tag tree. The alignment is partial because only these nodes of a data record become aligned whose position for inserting into the seed tree can be uniquely determined. They tested their MDR system on a collection of handpicked sample pages with near per-fect results (precision 100% and recall 99.8%) compared to (56%,39%) OMINI [2] and (67%,39%) IEPAD. Test result reported in [14] indicated that the performance on third-party test beds yield to worse results. In [19] they showed the improvements of their new system DEPTA and reported encouraging results for the alignment process.

Our objective in this paper is to enhance the extraction technique realized in the MDR by identifying tandem re-peats and visual context information for record segmenta-tion which has not yet been proposed. Additionally we only report the relevant data records according to the visual per-ception of a Web page in contrast to MDR and DEPTA. We do not extract data records according to separator tags like ViNTs does, rather we consider the tag structure that data records consist of. Thus we could also manage Web pages containing at least two consecutive data records, ex-tract similar data records distributed over multiple sections, and extract horizontally arranged data records. The visual relevance weighting mechanism is similar to ViNTs but we computed it in a different way because our data records may range over multiple sections. Our data record align-ment method is totally different to the method proposed for DEPTA because of global matching mechanisms and incor-porated text content information.
In this section we will describe in detail our contributions made to improve the extraction power. First, we carry out a pre-processing step to enhance the recognition robustness. Next, we adapt the edit-distance metric to cope with typical HTML structures. Based on the observation in [13] that a data record is either formed by a single coherent subtree ( subtree pattern ) or ranges over multiple adjacent sibling nodes ( forest tree pattern ) the pattern search is restricted to child subtrees belonging to the same parent node. This search technique has already successfully been implemented in the MDR system. We improve this technique by variable rather than fix node combinations, and finally incorporate visual information to separate and weight the identified data regions.
Semi-structured data can be described as data which is neither raw nor very strictly typed as in traditional data-base systems. In case of HTML, predefined markup tags could be used to control the appearance of untyped text. Therefore we could formalize HTML documents as a class of labeled unordered trees . A labeled unordered tree is a di-rected acyclic graph T = ( V , E , r,  X  ) where V denotes a set of nodes with a distinguished node r called the root , E X  X  X V a set of edges between two different nodes and a label function  X  : V X L where L is a string. If ( u, v )  X  X  we call u parent node of v , and v child node of u . Each node v  X  X \{ r } has exactly one parent node in the graph. If node u is an inner node then C u = { v | ( u, v )  X  X } denotes the set of child nodes belonging to u . For a node v , we define T v as the subtree of T rooted at v , where the subgraph of T v is induced by the set of all descendants.
To enhance the pattern extraction accuracy in HTML doc-uments pre-processing has to be performed. For instance, numerous Web pages are not even well-formed with respect to their HTML syntax. We used the Open Source Mozilla Browser and its underlying rendering engine Gecko to con-vert HTML documents into valid XHTML. The reason is that most HTML authors verify their pages with standard browsers before publishing their sites. We controlled Gecko over the XPCOM interface via the Java Framework JREX (http://jrex.mozdev.org/) which enables us to use the cor-rection power of Gecko within Java. Furthermore, we have the ability to access the parsed document tree T  X  with ad-ditional rendering information whereas every tag element is augmented with bounding box information by the upper-left corner X  X  (x,y) pixel coordinates along with width and height. Furthermore, we compute the matching bounding box of text elements. To analyze T  X  we work on an abstract representation where each HTML tag is restricted to its tag name ignoring attributes. Moreover, trimmed text between two tag elements is represented by a new abstract element denoted as &lt;TEXT&gt; element tag. Additionally, &lt;TABLE&gt; related tags with colspan or rowspan attributes are regu-lated. Due to the fact that matching parts of a query string are often highlighted in search results, and the correspond-ing style tags are often spread very inhomogeneously over a document, we additionally ignore HTML style tags. These transformations are done by the function  X  : T  X   X  T . Fi-nally the pre-processed document is represented by its re-stricted tag tree T and the plain tag sequence structure S of T where each element in the tree representation has a link to the corresponding element in the sequence representation and vice versa.
One common technique to measure the similarity between two plain sequences S i , S j with length n, m , respectively, is the edit-distance [9], which computes the minimal cost to transform one sequence into the other, utilizing uniform cost operations insert , delete and rename . Using Dynamic Pro-gramming techniques we can compute an n  X  m edit-distance matrix D in O ( nm ) time. A typical characteristic of data records is that single data record instances vary in optional or repetitive subparts. For instance, optional or multiple au-thors in the description of a book data record. An obvious disadvantage of the edit-distance computation is that repet-itive and optional subparts inside the sequences S i , S j increase the edit cost, therefore causing possible matches to be discarded. Optional subparts are usually handled with an approximate similarity threshold value  X  by defining two sequences similar if their accumulated edit-distance is less or equal to a threshold value e D  X  ( S i , S j ) = e This approach has already successfully been implemented in the MDR system. To overcome the problem of accumu-lated edit costs according to repetitive subparts it is sug-gested that two sequences are similar if they have at least a similarity of 60%. By virtue of the low similarity thresh-old it is very likely that sequences match each other al-though they belong to different contexts. Moreover, many situations could be generated where actually matching se-quences are discarded because of repetitive subparts. For instance, when comparing the following data records S i = &lt;A&gt;Author1&lt;/A&gt;&lt;/P&gt; the repetitive authors make the se-quences be considered as dissimilar despite of the low sim-ilarity threshold. Therefore we identify so called tandem repeats present in both sequences and allow zero cost for delete and insert operations inside additional repetitive in-stances.
A tandem repeat contained in a sequence S is a subpart of the form  X  k with k  X  2 where  X  is a non-empty se-quence of elements. For instance, the character sequences S =ABCBCBCDD and S j =ABCDDDD contain the follow-ing tandem repeats P i = {  X  3 i, 1 = BC ,  X  2 i, 2 = CB ,  X  and P j = {  X  4 j, 1 = D ,  X  2 j, 2 = DD } and have the following re-peats in common P = P i  X  X  j = {  X  2 , 4 1 = D } . Consequently tandem repeats build an array of consecutive repeats. If we additionally claim that the repeats have to be primitive then  X  may not contain shorter repeats. As a result  X  2 j, 2 in P j becomes rejected. In the context of HTML tags we only consider primitive tandem repeats with |  X  | X  3 . For the running example we disregard this condition owing to space constraints. We implemented the algorithm described in [10] based on suffix trees to identify all z primitive tan-dem repeats in a sequence of length n in O ( n + z ) time be-fore computing the edit-distance. Next we mark each extra repetitive instance with different marker elements. If, e.g.,  X  1  X  X  and S i contains fewer consecutive repeats of  X  1 than S j , then each element in the m = k j  X  k i extra repeats of  X  1 in S j becomes marked. Additionally we mark the last element in the array of the consecutive repeated elements of  X  1 contained in S i with the same marker. Moreover, for each primitive tandem repeat  X   X  X  l \P with l  X  X  i, j } , if  X  at most occurs once in the other sequence the elements be-come marked in the same way. According to these marked tag elements the recursive computation of a single matrix entry of D is adapted as follows: for 1  X  k  X  n, 1  X  l  X  m with and c( a k , b l ) = 1 if a k 6 = b l 0 else . Consequently f( a k , b l ) = 0 if the set of markers assigned to a k and b l have at least one marker in common. This assures that additional repetitive subparts contained in both sequences do not unduly increase the edit-distance. The cost function c ( a k , b l ) equals zero if both elements belong to the same tag type which we denote as shallow equal .
The table shows the resulting edit-distance matrix of the two sequences and the different markers assigned to addi-tional repetitive elements. With respect to the modified edit-distance computation the two sequences can be mapped with zero edit cost D  X  ( S i , S j ) = 0 despite of optional repet-itive sub-elements. For instance, the first mismatch in the diagonal occurs at position (4 , 4) in the matrix. Due to the fact that element C in sequence S i shares the same marker as the second B in sequence S j zero edit cost are mapped to matrix entry (3,4). To speed up the computation of the matrix D n,m we eliminate all path hypotheses exceeding the simple strategy reduces the computation workload to almost linear time and only best matching elements are compared with each other.
According to the observations made in [13] we have to find similar subtrees T v i of child nodes v i  X  X  u of all inner nodes u  X  T . This can be done by computing the pairwise similarity of each subtree T v i , T v j , respectively, their corre-sponding sequences S v i , S v j with 1  X  i &lt; j  X |C u | . Despite of comparing fix pairs of subtree combinations we generalize the search process to variable subtree combinations. There-fore single data records inside a data region could consists of variable numbers of subtrees.

When we compute the pairwise similarity between all sub-tree sequences we obtain an upper triangular subtree sim-ilarity matrix M s u for each inner node u . To simplify the pattern discovery we do not store the edit-distance values inside the matrix. Instead, a cell entry M s u ( i, j ) becomes 1 if the edit-distance between two sequences S v i , S v j fies the condition D  X  ( S v i , S v j )  X   X  v i,j with  X  max | S v i | , | S v j | . Next we try to identify sets of adjacent sibling nodes having the highest matching frequency. The following example illustrates the problem:
Let u be an inner node in T and v 1 , . . . , v n  X  X  u its n children. The corresponding subtree similarity matrix M s is given as follows: Interpreting the matrix entries we notice that the first node v has no approximative match with its sibling nodes. Scan-ning the matrix from left to right and top-down, the first matching sequences are found at position M s u (2 , 5) = 1 . Fo-cusing on the diagonal entries starting at this position we recognize that several matches follow after node v 5 . The fact that several consecutive sibling nodes are similar to each other indicates a potential repetitive matching region. Hereby we could weight the region by counting all consecu-tive matches found inside the diagonal, starting at the corre-sponding position. In our case the matching diagonal starts at position M s u (2 , 5) and ends at position M s u (7 , 10) . Thus, the diagonal weight w (diag matches (2 , 5) ) = 6 . Next we have to split the data region into blocks forming the potential sin-gle data records. To this end, we scan the matrix hori-zontally starting at position M s u (2 , 5) for nearest matches within the range defined by w (diag matches (2 , 5) ) = 6 . In our ex-ample the nearest match in row 2 occurs at position M s u with w (diag matches (2 , 7) ) = 1 , M s u (2 , 8) with w (diag 4 and the last match within the range is M s u (2 , 11) with w (diag matches (2 , 11) ) = 1 . By comparing the diagonal weight start-ing at these positions we are able to measure the splitting possibility for the current data region, which is the likeliest for position M s u (2 , 8) in our running example. If no match exists inside the range then the data record block divides the data region evenly.
 Applying these steps iteratively to every pair of matching sequences we are able to identify potential data regions and their corresponding likeliest data block segmentations. In our example we obtain one main data region which is built by the nodes v 2 , .., v 10 where every third node matches each other. Consequently we get a forest tree pattern where a sin-gle data record inside the data region consists of three nodes and their corresponding subtrees. Finally a data region R is represented by the data record, denoted as pattern S p with the minimal pairwise distance to each remaining data.
Knowing that the matrix is symmetric we only have to compute values for the upper triangular entries. To further reduce the computation of the matrix we consider an edit-distance dependent, transitive similarity relation. Hereby, the matrix is computed line-by-line, where each match with edit-distance D ( S v i , S v j )  X  1 is stored in an array A responding to the row i . After completing the computa-tion of a row we set each pair ( l, k )  X  2 A i , which is part of the power set, to one, M s u ( l, k ) = 1 . Additionally, if | S v i | X  X  S v j | &gt;  X  v i,j and in the case the sequences share no tandem repeats, we do not compute the edit-distance and set M s u ( i, j ) = 0 . Therefore we have to compute n matrix entries in the best case (subtree pattern) and n 2 2 matrix entries in the worst case (no transitive similarity).
To enhance the data record separation process we incor-porate visual cues which bears advantages for both vertical and horizontal segmentation of data region. When a data region R k has been identified we additionally analyze the corresponding image representation defined by R k . Thus the bounding boxes of &lt;TEXT&gt; elements contained in R are used, to compute the vertical and horizontal projection profiles. This is realized by summing the width and re-spectively the height of all boxes with respect to their x/y-coordinates. Figure 1 shows the x/y-projection profiles of a data region. To demonstrate the correlation between text information as normally viewed on the Web site and bound-ing box information we represented the data region in both views. Additionally the projection profiles, exclusively com-puted according to the bounding box information, surround the data region in the figure. With respect to these pro-Figure 1: Visual data record segmentation utiliz-ing x/y-profile information of text element bounding boxes. files we analyze the probability of a potential segmentation according to characteristic valleys between peaks. Valleys between peaks corresponds to blank between text lines and the distance between two significant valleys corresponds to a potential separation of the data region into smaller data records. We establish a relationship between the valleys found in the profiles and the corresponding tag elements by a containment check. For this purpose each child element v  X  X  u , respectively the corresponding bounding box, in the data region R k which is totally or partially contained in an interval, described by a valley, becomes a potential separa-tion tag. Next we test the splitting probability with respect to the computed similarity matrix for these tag elements as described above.
After the pattern extraction process took place we com-pute the relevance of each pattern S p 1 , S p 2 , ...S p tively their result sets R l 1 p 1 , R l 2 p 2 , . . . R matching sub-sequences R l i p i := { S p i , S p i , 1 , . . . , S pattern S p i with 1  X  i  X  k contained in the complete Web page. To accomplish this task several heuristics could be used for measuring the individual weight of a region. In case of dynamically generated HTML sites, we might reward data regions which partially contain the requested keywords according to some given user request. Moreover, it is pos-sible to compute the textual coverage of the data region with respect to the pattern size. This heuristic often fails if the target pattern primarily consists of images and links or if a data region inside the menu bar has a higher textual coverage. Inspired by the work of Deng [3] who described a vision-based page segmentation, we introduce a ranking technique based on visual location. Hereby a significant fea-ture for a potential target pattern is determined by its visual location inside the page. Generally, Web pages are divided into different information regions, so-called slots , filled with navigation bars, banners, adds and the proper query result. In most cases these slots reside in appropriate locations. Ac-tually it can be observed that the slot filled with the target information has often a centered location and covers a large part of the total page. Consequently a heuristic which mea-sures the coverage and the deviation of the result set R l from the page center is used to weight pattern S p i . Given the bounding box information we ascertain the rectangle  X  R l spanned by the result set R l i p i computing the extremum coor-dinates over all single rectangles represented by each match and define its total area A tot each single rectangle area. Whereas the area of a matching instance S p i,j  X  X  l i p i is defined as the area of the rectan-gle spanned by the parent node, in case S p i,j is a subtree pattern, or by the sum of the area of all sibling nodes if it is a forest tree pattern. Computing the total area as sum of each matching rectangle area, we could handle the case having a small matching instance in the upper part and a second instance at the bottom of the page which results in a rectangle spanning over the complete page but effectively occupying only a small area. Finally the weight w S p pattern S p i is computed as follows: where c page denotes the page center coordinate, c R p the center of the result set rectangle  X  R l i Euclidian distance.
Having extracted the most relevant pattern and its corre-sponding approximate matches we try to align the multiple data records next. Hereby corresponding information ele-ments (data items) should be arranged in adequate columns belonging to the same attribute, making it easy to label and store the data records in a database, export them as XML or synchronize them with data records extracted from other Web pages. Especially in case of schema matching , i.e., dis-covering semantically attributes in different schemas, which is fundamental for enabling query mediation and data ex-change across multiple information sources, accurate data alignment plays an important role.

Typical properties of similar extracted data records are optional or multi-valued attributes or even several attributes encoded inside one single text element. These attributes of-ten lead to non-trivial alignment scenarios. To motivate the complexity of multiple data record alignment we depicted a small example scenario in Figure 2a). Here three similar data records and their appropriate abstract tag sequences are represented. Please note that &lt;TEXT&gt; elements, which abstract from the original content information contained in the Web site, have been abbreviated by the letter T to en-able a compact representation. Due to the fact that each sequence has optional sub-sequences, the alignment process becomes already difficult for such a small example. Before we start to explain our alignment technique we give a short introduction into existing multiple sequence alignment tech-niques and motivate our approach.

In the domain of bioinformatics multiple (protein) sequence (genome) alignment (MSA) is a fundamental task and also one among the most studied and difficult problems in com-putational biology. Although the notion of multiple align-ment could be easily extended from two sequences to many sequences, the score or quality of a multiple alignment can-not be simply generalized. One very intuitive candidate is the well known sum-of-pairs (SP) score function which is given by the sum of the induced pairwise alignment scores of each pair in the alignment. An optimal solution of n se-quences, each of length k , can be computed in  X ( k n ) time by dynamic programming [15]. However, such an approach is not practical for more than a few sequences. Moreover, the optimal SP alignment problem has been proven to be NP-complete [1]. It should be mentioned that in the do-main of data record alignment the quantifier optimal is not always definite, making it difficult to express the quality of the alignment by an objective score function. To decrease the computational costs, a large body of research exists for the design of efficient heuristics with sub-optimal solutions. For instance, in [8] an alignment heuristic with guaranteed error bounds, less than twice the score of the optimal SP problem, and polynomial worst-case time have been pro-posed based on pairwise sequence alignment utilizing the edit-distance computation. The key idea in this approach is based on a center-star tree alignment algorithm which first tries to find a sequence (center sequence) minimizing the overall edit cost to each remaining sequence. According to this center sequence each remaining sequence is iteratively aligned to construct the final multiple alignment. Such an edit-distance based approach has already been ported into the information extraction domain, e.g., in [4, 5]. In [4] Chang et al. proposed IEPAD which generalizes the discov-ered exact repeated patterns to allow approximate matching by adopting the center-star multiple string alignment tech-nique. Furthermore, in [5] Chang et al. proposed a semi-supervised information extraction tool called OLERA where the user could interactively generate extraction rules accord-ing to a set of training pages. OLERA is able to automati-cally discover other records similar to the enclosed examples and present the data in form of a spreadsheet by aligning the data records with the center-star alignment technique. Hereby, a string comparator function is incorporated into the alignment process to improve the alignment result. We implemented the second alignment proposal and identified several drawbacks of the center-star technique in the context of data record alignment. First, if the center sequence does not contain all optional sub-sequences, which is pretty often the case for complex data records, the multiple sequence alignment is fairly poor for these missing sub-sequences. Additionally, the alignment process lacks any global con-trol instance to resolve alignment mismatches due to the fact that unaligned sequences are not considered when align-ing new sequences to the growing multiple center sequence. Furthermore, the performance of the alignment process be-comes very slow when the number of data records increases. And finally, tree structure and HTML structure informa-tion, which provides in our opinion valuable clues, should not be ignored during the alignment process.

Therefore we utilized a new method inspired by the al-gorithms in [6, 11, 7] based on global sequence alignment using general suffix trees as an alternative to edit-distance algorithms. The main idea is to find global matches which reduce the multiple alignment problem in a Divide and Con-quer fashion. Thus, we are able to dramatically speed up the alignment process and extend it by a global control instance. Next we briefly describe the main stages.
First we try to find maximal unique matches ( MUMs ) contained in all data records. Whereas maximal denotes that we cannot simultaneously extend the given match to the left or to the right in every sequence and unique means that the matches occur only once in each of the n sequences. For instance, in Figure 2a) five such like MUMs are con-tained in the sequences, respectively, MUM 1 =T&lt;A&gt;T&lt;/A&gt;T visualize the MUM arrangement we can represent the MUMs by intervals corresponding to the starting position and num-ber of elements on horizontal lines. Figure 2b) shows the re-sulting 3 -level MUM diagram with the corresponding MUM chains . The objective is to select a MUM-sequence of non-overlapping MUMs that has maximal weight, where the weight of a MUM-sequence is defined by the sum of the weights of its members, respectively, the number of ele-ments they consist of. Solving this optimization problem could be done with algorithms from computational geom-etry in O ( k 2 ) time, for details see [7], where k denotes the number of MUMs identified. When we align the data records according to the MUM-sequence of length l with op-timal weight, the problem decomposes into l +1 smaller un-aligned sub-regions. Figure 2c) shows the result of the com-puted optimal non-overlapping MUMs-sequence MUM opt seq = [MUM 1 , MUM 2 , MUM 5 ] . In the next stage we iteratively align each unaligned sub-region between global matches sep-arately. To reflect the characteristics of optional and repet-itive elements we attenuate our definition by searching for MUMs  X  appearing in most of the sequences and maxi-mal multiple exact matches (multiMEMs) which may appear multiple times in a sequence. In Figure 2c) sub-region 0 con-tains one MUM  X  0 . 1 = &lt;P&gt; which occurs in the first and the third sequence. The matching closing tags are located in sub-region 3 labeled 3.1. Moreover, sub-region 2 contains which do not intersect and therefore build an optimal non-overlapping MUMs-sequence. The algorithm to find all mul-tiMEMs, respectively, MUMs takes O ( zn + k ) time utilizing a general suffix tree, where z is the length of the concate-nated n sequences and k is the total number of multiMEMs and MUMs. We refer the interested reader to [9] for more details. Finally, remaining gaps are closed by mapping un-aligned sub-sequences to the largest sub-sequence inside a sub-region with respect to their tree structure using a stan-dard tree mapping function. Figure 2d) visualizes the tree structure of the data records. And Figure 2e) shows the re-sult of the global alignment with the text content. In the end we assign to each group of aligned text, link and image ele-ments a separate column. This course of action guarantees that we have a global control instance over the alignment process and we are also able to reduce the computational cost significantly.
So far, we defined two elements shallow equal if they be-long to the same tag type. Due to the fact that the con-tent of &lt;TEXT&gt; elements establish a good indication for global correspondences we modify the comparison function of &lt;TEXT&gt; elements when we build the general suffix tree to find regularities contained in all sequences. Therefore we define two abstract &lt;TEXT&gt; elements A and B content sim-with respect to their original trimmed text content given a string comparator function f sim and a threshold value  X  string For instance when a new suffix S i =  X  &lt;TEXT&gt;  X  have to be inserted into the general suffix tree we have to compute the content similarity to each &lt;TEXT&gt; element leaving a node v . In case no content similarity is given a new edge has to be created, otherwise the suffix &lt;TEXT&gt;  X  is assigned to the edge having the highest content similarity value. Therefore each edge leaving a node v and starting with a &lt;TEXT&gt; element stands for an equivalence class of a text content where the representative is the first &lt;TEXT&gt; content which generated a new edge. This special treatment of abstract &lt;TEXT&gt; elements is necessary because during the align-ment process we need to guarantee that only similar text content becomes aligned in the global matching stages. Best alignment results have been achieved with the Jaro-Winkler [17] string metric and  X  string = 0 . 7 .
In this section we compare our wrapper prototype system with the two most relevant existing fully-automatic state-of-the-art information extraction systems, ViNTs and MDR. We do not compare our tool with DEPTA because at the time of this writing neither the test bed, experiments were performed on, nor the system was available. Instead we took the accessible datasets 2 and 3 generated within the ViNTs prototype for testing and comparing. Furthermore, we conducted experiments on the manually labeled Testbed for Information Extraction from Deep Web TBDW [18] Ver. 1.02 available at (http://daisen.cc.kyushu-u.ac.jp/TBDW/) to additionally measure the performance of the alignment process. The performance was measured with the standard N total defines the number of data records contained in all dynamic pages, E correct is the total number of correctly ex-tracted data records and E total denotes the total number of data records extracted by the wrapper. Due to the fact that the number of data records represented inside different Web pages varies from just a few up to hundreds of records, the final performance metrics are basically dominated by pages which consists of many data records. Therefore only the first 25 data records are used.
To test the performance of our system ViPER, we used dataset 2 from the ViNTs Web page featuring sample pages from 100 different search engines with 10 (5 test/5 training) result pages and one additional no-result page per engine. Due to the fact that our wrapper only works on a single Web page we randomly took 1 result page per search engine resulting in 100 pages and operated on these pages inde-pendently. As we can see from Table 1, the performance of our system on dataset 2 tends to result in high quality precision and recall values. We use the abbreviation SRRs search result records as introduced in [14], which is equiv-alent to our definition of data records to conform with the different notions. The ViNTs results, reported in [14], were obtained during building the wrapper on 5 sample pages per search engine using one no-result page and finally extracting SRRs according to these 5 pages. Despite of the different extracting approaches, DOM tree-based vs. identifying vi-sual separators, both techniques yield high quality results. The reason is that data records in document result pages could be easily separated by horizontal separators and the relevant data records often cover a large slice of the result page.

Next, we compare our system according to dataset 3 from the ViNTs system with the results published in [14] obtained by using MDR and ViNTs. The authors of [14] took this dataset to compare their ViNTs system with the available MDR tool, calibrating the similarity threshold value at 60%. While MDR reports all identified data regions, only the ma-jor region is considered if it is contained inside the reported data regions. In the middle of Table 1 the reported ex-traction results of the two different extraction tools ViNTs and MDR are provided, along with the results of our sys-tem ViPER. First of all it could be recognized that the per-formance of ViNTs and ViPER is considerably better than that of MDR on this dataset. This indicates that despite of the same underlying technique, used for MDR, the im-provements made by incorporating tandem repeats, visual segmentation, and visual relevance selection the final result of the extraction could be enhanced enormously. Compar-ing the performance of ViNTs with ViPER X  X , ViNTs per-forms slightly better than our system. One reason is that in [14] only the number of SRRs contained in a consecutive data region are count as correct result. If, e.g., the relevant data region is split into 2 subparts, by an advertisement for instance, ViNTs only reports and counts results from the larger part as correct. This is for example the case for the result page lycos.html. Here our system additionally re-ported the 4 data records which we had to count as false records. Finally, we performed extraction tests on data set TBDW Ver. 1.02 which have not been used in [14]. Here our system performed better than ViNTs, due to the fact that some data records have a more complex structure. Ad-ditionally, we tested the alignment quality with respect to the manually labeled field information. Where label infor-mation is provided in the data set for the first data record of each Web page. The number of data items contained in the set of labeled data records add up to 367, resulting in 4,846 data items overall. With respect to the 676 correctly extracted data records by ViPER only 11 data items could not be aligned correctly. Hereby the execution time for the alignment was always less than 150msec on a P4 2.4GHz with 768MB RAM for every page.
In this paper, we presented a fully-automatic information extraction tool called ViPER. The tool is able to extract and separate data exhibiting recurring structures out of a single Web page with high accuracy by identifying tandem repeats and using visual context information. Additionally, we pro-posed a new fast and robust data record alignment technique based on global matching information and text content in-formation. Tests performed on several third-party data sets finally underpin the high accuracy of the our system. In the future we plan to release a plugin of ViPER for the Firefox Web browser with additional interactive functionalities to extract non-repetitive data.
