 pages. X  The principal singular vectors of the adjacency ma-trix of the focused subgraph define the best authorities and hubs for the query. The HITS score is query-specific in that it computes the authority scores of the pages after it com-piles a subset of web pages. Unfortunately, Kleinberg [24] and others [4, 10] have observed that the authorities and hubs do not always match the original query due to  X  X opic drift, X  i.e., nodes in the focused subgraph are not related to the query topic. Appropriate authorities and hubs gener-ally appear in some pair of singular vectors, but Davison et al. [12] note that selecting the appropriate singular vectors is an open research question.

Both PageRank [6, 32] and HITS [24] use appropriate eigenvectors (or singular vectors) to compute the authority of web pages and can be considered as members of the same family [14]. Other methods adhere to the same basic theme. For example, SALSA is a variant on HITS that uses a sto-chastic iteration matrix [27].
 In this paper, we propose a new method called Topical Hypertext Induced Topic Selection (TOPHITS), following Kleinberg [24]. This new technique analyzes a semantic graph that combines anchor text with the hyperlink structure of the web. Anchor text is useful for web search because it behaves as a  X  X onsensus title X  [18]. Fig. 1 shows four hy-pothetical web pages and the corresponding semantic graph. The adjacency structure of a semantic graph cannot be mod-eled as a matrix without losing edge type information. In-stead, it is modeled by a three-way tensor containing both hyperlink and anchor text information; see Fig. 2. Then we apply the Parallel Factors (PARAFAC) decomposition [22], which is a higher-order analogue of the SVD, to get the most significant factors that are akin to singular vectors. Instead of pairs of vectors containing authority and hub scores, we produce triplets of vectors with authority and hub scores for the pages as well as topic scores for the terms. This is an ex-tension of Kleinberg X  X  HITS algorithm [24], which uses the singular vectors of the hyperlink matrix (a two-way tensor) to produce multiple sets of hubs and authorities. The addi-tion of the topic vector means that determining which set of singular vectors contains the answer to the query is just a that are linked using terms in the search query, while Bharat and Henzinger [4] and Li et al. [28] incorporate weighting based on the content of the web pages.
We are not the first to propose the simultaneous analysis of hyperlink structure and anchor text or page content. Dili-genti et al. [13] propose a modification of PageRank that uses a topic classifier instead of the random surfer model. Rafiei and Mendelzon [33] modify the page transition prob-abilities for PageRank based on whether or not a term ap-pears in the page. Further, they derive a propagation model for HITS and adapt the same modification in that context. Haveliwala [23] introduced a topic-sensitive PageRank that pre-computes several PageRank vectors that are biased to-wards particular topics. Richardson and Domingos [34] propose a general model that incorporates a term-based rel-evance function into PageRank. The relevance function can be defined in many ways, such as defining it to be 1 for any page that includes the term, and 0 otherwise. In an approach that is very similar in spirit to ours, though dif-ferent in the mathematical implementation, Cohn and Hof-mann [11] combine probabilistic LSI (PLSI) and probabilis-tic HITS (PHITS) so that terms and links rely on a common set of underlying factors.
Our contribution is the use of a PARAFAC decomposi-tion [22] (also known as the Canonical Decomposition or CANDECOMP decomposition [7]) on a three-way tensor representing the web graph with anchor-text-labeled edges. Tensor decompositions have a long history and have been used in applications ranging from chemometrics [36] to im-age analysis [39]. Recently, they have been applied to data-centric problems including analysis of clickthrough data us-
Figure 3. In HITS model, the SVD provides a 2-way decomposition that yields authority and hub scores. described in the next section, discovers triplets of vectors that identify a topic (described by key terms) along with its associated hubs and authorities.
The TOPHITS method produces sets of triplets { u ( i ) , v ( i ) , w ( i ) } where the u and v vectors contain hub and authority scores for the web pages as in HITS, and the w vector contains topic scores for the terms.
 Just like HITS, these scores can be computed iteratively. Let n denote the number of pages and m the number of terms. The hub, authority, and topic scores are updated as follows: Here, the notation i k  X  j means page i links to page j with anchor text k . As with HITS, we normalize after each it-eration. In words, the hub score of page i is the sum of authority scores for pages that i points to multiplied by the corresponding topic scores of the terms in the anchor text. Similarly, the authority score of page j is the sum of hub scores of all pages that point to j multiplied by the topic scores of the corresponding terms in the anchor text. The topic score of term k is the sum of hub scores for page i multiplied by the authority scores for page j over all hyper-links i  X  j that involve term k in the anchor text.
This can be written in tensor form as follows. Let A denote the n  X  n  X  m adjacency tensor of a web (sub-)graph, defined by
We tested our technique on a subset of web data, gen-erated using an in-house web crawler that includes anchor text in its output. Stop words, punctuation, and non-integer numbers were removed. Any hyperlink without anchor text was assigned the term  X  X o-anchor-text X . In order to avoid the edge effects inherent in small web crawls, it was as-sumed that URLs with no recorded outlinks were never crawled and so were excluded from the data set. Finally, we considered only host-to-host links (rather than page-to-page links) and removed all self-links that point from one host to the same host.

The three-way tensor is computed by counting the num-ber of links from host i to host j with term k and storing the result as C ijk . We perform an element-wise scaling of C , which attenuates the influence of highly linked hosts:
Working with multi-way data is a challenge due to the lack of available software. Although a few packages do exist for working with dense tensors (see, e.g., [2]), noth-ing is available for sparse tensors. Our web graph data is extremely sparse. For example, storing a host graph with 10,000 hosts and 10,000 terms in a dense tensor storage format would require one trillion entries, which rules out any type of dense storage format. Thus, in order to work with this data in sparse form, we developed the capability to mathematically manipulate sparse, large-scale tensors.
We implemented our methods in MATLAB by extend-ing our existing toolbox of dense tensor classes [2], details of which will be in a forthcoming report. We have cre-ated a sparse tensor object (or class) in MATLAB that stores the data in sparse format and can efficiently manip-ulate it. We support multiplication, scaling, accumulation across dimensions, operations on individual elements, and permutations in addition to standard operations like adding, subtracting, etc. For example, we have been able to run the greedy PARAFAC algorithm (described in Section 4.3) to work with data sets as large as 50,000 by 50,000 by 50,000 with 500,000 nonzeros on a laptop. In addition, to the sparse tensor class, we have a separate class for storing a PARAFAC decomposition.

Efficiency is achieved by carefully selecting a storage format and using built-in MATLAB functions to avoid any loops. We use a coordinate-based storage scheme in which each non-zero is stored along with its indices; e.g., we store simultaneously solves for all vectors in the same mode (e.g., all u ( i ) , 1  X  i  X  p ) [36]. However, in our experiences, such an approach is slow to converge and does not yield any significant improvements in our results over the greedy approach.
We started our web crawler from the following URL: http://www-neos.mcs.anl.gov/neos (an opti-mization web page) and allowed it to crawl 4700 pages, resulting in 560 cross-linked hosts.

Fig. 6 shows the authorities derived from the HITS ap-proach [24], using the SVD applied to the standard adja-cency matrix (i.e., A ij =1 if i  X  j ). We show results from the first several singular vectors, omitting negative entries because they were repeats of earlier sets of authorities and other sets that were also repeats (e.g., the fifth singular vec-tor contained repeats from several of the first four vectors).
Using our greedy PARAFAC algorithm from Fig. 5 on the tensor A defined by (7), we computed the first twenty factors of the scaled adjacency tensor. The cost of each it-eration is O ( N ) where N is the number of nonzeros in the tensor A . This is approximately the same cost of each iter-ation of the power method for computing the SVD because the number of nonzeros in the tensor representation is not much more than that in the matrix representation. Fig. 7 shows that we only require a few iterations for each factor.
Figures 8 and 9 show sets of topics and authorities de-rived from the TOPHITS approach. As before, we omit-ted repetitive results. For each factor, we get a ranked list of hosts that is associated with a ranked list of terms. The results are very similar to what we get from HITS, but TOPHITS includes terms that identify the topic of each set of authorities. In the simplest case, this approach can be used to correct the topic drift problem. Here, for exam-ple, we collected pages about optimization as well as other topics. It is easy to find the authorities on optimization by simply searching for key terms (in this case,  X  X ptimization X  identifies the 12th factor).

The usefulness of this new TOPHITS approach is that it automatically discovers topics along with sets of authori-ties. This can be used to extend HITS so that it can be used on large, multi-topic data sets.
Multi-way data representations and tensor decomposi-tions are a novel technique for web search and related tasks. We have introduced the TOPHITS algorithm, which ex-tends HITS [24] by identifying hubs and authorities that are associated with prominent topics. We accomplish this  X  a , could be computed as
There are many directions for future research. Currently, we are studying an alternative decomposition to PARAFAC called the Tucker model [38] for applications in information retrieval. We are also looking at even higher order data sets that go beyond three-way models.

Although some accelerations have been proposed (see, e.g., [40]), much work remains to be done on efficient com-putation of PARAFAC models for large-scale, sparse ten-sors. As a first step we have created a MATLAB toolbox for working with sparse tensors that can efficiently handle up to one million nonzeros. Extending these techniques to data sets the size of the Web is a topic of future study. While multiple vectors need to be stored, they can be sparsified, which will reduce both the overall storage cost as well as the computational cost for computing them. Further, con-vergence and stability analysis of TOPHITS should be ana-lyzed in the same way that Ng et al. [30, 31] have analyzed the stability of PageRank and HITS.

Another future topic is the use of tensor decompositions on semantic graphs to measure similarity, analogous to how Blondel et al. use the SVD to measure the similarity be-tween directed graphs [5]. Such techniques can be used in attribute prediction as has already been done using ma-trix decompositions [35] as well as probabilistic-based ap-proaches [19, 29].

This work was funded by Sandia National Laboratories, a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Depart-ment of Energy X  X  National Nuclear Security Administration under Contract DE-AC04-94AL85000.
 We are indebted to Travis Bauer, David Blackledge, and Thomas Cleal for providing the web spider in STANLEY, a text analysis library being developed as part of Sandia X  X  Cognitive Science program. We thank the referees for their advice and pointers to two additional relevant articles.
