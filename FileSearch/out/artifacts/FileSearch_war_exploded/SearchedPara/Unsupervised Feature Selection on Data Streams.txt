 Massive data streams are continuously being generated from sources such as social media, broadcast news, etc., and typ-ically these datapoints lie in high-dimensional spaces (such as the vocabulary space of a language). Timely and accu-rate feature subset selection in these massive data streams has important applications in model interpretation, compu-tational/storage cost reduction, and generalization enhance-ment. In this paper, we introduce a novel unsupervised fea-ture selection approach on data streams that selects impor-tant features by making only one pass over the data while utilizing limited storage. The proposed algorithm uses ideas from matrix sketching to efficiently maintain a low-rank ap-proximation of the observed data and applies regularized regression on this approximation to identify the important features. We theoretically prove that our algorithm is close to an expensive offline approach based on global singular value decompositions. The experimental results on a vari-ety of text and image datasets demonstrate the excellent ability of our approach to identify important features even in presence of concept drifts and also its efficiency over other popular scalable feature selection algorithms.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture Evaluation and Selection Feature Selection; Unsupervised; Streaming Algorithms
The curse of dimensionality plagues many complex learn-ing tasks. A popular approach for overcoming this problem is by reducing the dimensionality of the feature space as that Part of this work done while the author was at GE Global Research.
 directly results in a faster computation time. At the same time, it is appealing to have feature interpretability , which some of the popular dimensionality reduction methods (e.g., PCA, spectral embeddings) do not possess because of their lack of direct connection to the observed feature space. In our work, we propose a novel approach to unsupervised fea-ture selection, which is the problem of choosing a subset of important (original) features without any label information. The selected feature subset minimizes a very intuitive eval-uation criterion while accounting for noise and redundancy. This in turn could lead to better: 1) model interpretation, 2) computational efficiency, and 3) generalization ability for the learning task.

One of the most important characteristic for any good fea-ture selection approach is the ability to handle huge volumes of data. Most modern data such as documents, images, mul-timedia from the web naturally arrives in a streaming fash-ion. However, detecting an informative feature subset in a large volume of data stream is also a difficult problem due to the following reasons: 1) the data stream could be infinite, so any off-line algorithm that attempts to store the entire stream for analysis will eventually run out of memory, 2) the feature importance X  X  change dynamically over time due to concept-drift , important features may become insignificant and vice-versa, and 3) for various online applications, it is important to obtain the feature subset in close to real-time.
Although there is considerable amount of previous lit-erature on feature selection both in the batch [4, 14] and online setting [31, 23, 17], none of them handles large vol-ume data stream effectively, given limited memory and CPU time, without any prior knowledge about labels. In practice, streams often contain inherently correlated data [19], so it is possible to reduce a large volume numerical stream into just a handful of hidden basis that compactly describe the key patterns of the data. We exploit this idea to reduce the complexity of streaming feature selection analysis. Our Techniques. In this paper, we propose a streaming feature selection approach that easily adapts to the con-cept/topic drift arising in the data stream, and at every timestep provides a feature importance score (weight). Our streaming feature selection algorithm uses ideas from ma-trix sketching 1 to maintain a low-rank approximation of the entire observed data at every timestep, and this approxima-tion is continually updated as new data arrives. For matrix sketching, we modify a recent algorithm (called Frequent Directions ) proposed by Liberty [15, 2]. The Frequent Di-
A sketch of a matrix Z is another matrix Z 0 that is much smaller than Z , but still approximates it well [15]. rections algorithm operates in a streaming model and con-structs a sketch matrix using a simple idea of  X  X hrinking X  a few orthogonal vectors. However, just the low-rank ap-proximation cannot by itself provide feature weighting. In our research this low-rank approximation is exploited and at every timestep the feature importance score is generated by performing a regression analysis. A regularization is added to prevent overfitting to the data (we explain the choice of regularization in Section 3.3). The idea of using regu-larized regression for feature selection in an unsupervised setting was recently proposed by Cai et al. [4], who empiri-cally showed that it leads to a better choice of features for clustering and classification applications. Their main idea is to obtain feature importance using a regularized regres-sion where the spectral embedding of the dataset is used as the regression target. However, the formulation presented in Cai et al. [4] operates in a batch setting and requires access to the entire affinity matrix for the regression step, which is not possible in a streaming setup. Our algorithm on the other hand, requires just one pass over the data, which is an essential requirement for any  X  X rue X  streaming algorithm. Our Contributions. To the best of our knowledge, ours is the first unsupervised feature selection algorith-m operating in a true data streaming setting . Our feature selection algorithm is effective and efficient in the following ways: (a) It is space and time efficient while requiring only one pass over the data. For a stream at time t consisting of n t (  X  1) datapoints in an m -dimensional space, our algorithm requires only O ( mn t ) space (linear in the size of the input) and O ( mn t ` ) time, where the sketch matrix is of size m  X  ` . In practice, it suffices to set ` much smaller compared to m and n t . Therefore, both the memory and computation requirements are almost linear in the input size (as the input at time t is an m  X  n t matrix). (b) It easily adapts to unseen patterns on the data stream and provides at every timestep an updated identification of the informative feature subset (i.e., at every timestep t it provides a feature ranking based on all the data that have arrived till time t ), which gives it the ability of handling concept drift 2 (related experiments in Section 5.4). (c) We provide theoretical support for our algorithm (Sec-tion 3.4), and show that it has a comparable performance to an expensive offline approach that uses singular value decompositions.

Empirical studies show that our streaming approach is efficient in terms of both space and time, while approaching the performance of popular batch algorithms on a wide array of datasets from both the text and the image domains. Notation. We denote [ n ] = 1 : n . Vectors are always in column-wise fashion and are denoted by boldface letter-s. For a vector v , v &gt; denotes its transpose and k v k de-notes its Euclidean norm. For a vector ( a 1 ,...,a m )  X  d iag ( a 1 ,...,a m )  X  R m  X  m denotes a diagonal matrix with
As we discuss later, in some feature selection applications, one might wish to reweigh the points to emphasize more on the recent points than the older points, which can also be easily handled in our framework. a ,...,a m as its diagonal entries. Let I m denote an iden-tity matrix of dimension m  X  m . We use r ank ( Z ) to de-note the rank of Z . For a matrix Z  X  R m  X  n , we use z to denote its ( i,j )th element. Spectral norm is defined as k Z k = sup {k Z v k : k v k = 1 } . We also use entry-wise norms denoted by k Z k p , where p = 2 gives (Frobenius nor-m) k Z k 2 F = P ij z 2 i,j , p = 1 gives k Z k 1 = P i,j p =  X  gives k Z k  X  = max i,j | z i,j | . We use Z 0 if Z is a positive semidefinite (PSD) matrix and Z Y if Z  X  Y 0. Given a set of matrices, Z 1 ,...,Z t , we use the notation Z to denote the matrix obtained by horizontally concatenating Z ,...,Z t , i.e., Z [ t ] = [ Z 1 | ... | Z t ].

We use Svd ( Z ) to denote the singular value decomposi-tion of Z , i.e., Svd ( Z ) = U  X  V &gt; . Here U is an m  X  m orthogonal matrix,  X  is an m  X  n diagonal matrix, and V is an n  X  n orthogonal matrix. The diagonal entries of  X , where  X  1  X   X  2  X   X  X  X   X   X  m (given m  X  n ), are known as the singular values of Z . We follow the common conven-tion to list the singular values in non-increasing order. For a symmetric matrix S  X  R m  X  m , we use Eig ( S ) to denote its eigenvalue decomposition, i.e., U  X  U &gt; = Eig ( S ). Here U is an m  X  m orthogonal matrix and  X  is an m  X  m diagonal matrix whose (real) entries are  X  1 ,..., X  m are known as the eigenvalues of S (again listed in non-increasing order). The best rank-k approximation (in both the spectral and Frobenius norm sense) to a matrix Z  X  R m  X  n is Z ( k ) P of Z , with associated left and right singular vectors u i and v i  X  R n , respectively. We use Svd k ( Z ) to denote the truncated singular value decomposition of Z ( k ) , i.e., Z R R n  X  k . The following well-known theorem bounds the ap-proximation error of the best rank-k approximation.
Theorem 2.1. [Golub et al. [8]] Let Z  X  R m  X  n with n &gt; m , and let  X  1  X   X  X  X   X   X  m be the singular values of Z . Let Svd k ( Z ) = U k  X  k V &gt; k . Then
In this section, we propose an online feature selection al-gorithm that operates in a streaming setting. We start by describing the problem of feature selection on data streams. Due to space constraints, we omit detailed proofs here.
We assume that the data items arrive in streams. Let { Y t  X  R m  X  n t , t = 1 , 2 ,... } denote a sequence of stream-ing items, where Y t represents the data items introduced at timestep t . Here m is the size of feature space, and n t the number of data items arriving at time t . 3 We normalize Y such that each column in Y t has a unit L 2 -norm. Under this setup, feature selection aims at selecting the most infor-mative feature subset based on certain evaluation criteria.
One could consider, a setting where only one point comes at a time (i.e., n t = 1), but by allowing n t  X  1, we allow more flexibility in our setup.
Our main idea is based on maintaining, at each timestep t , a low-rank approximation of all the seen (till time t ) data stream. By using a regression analysis on this low rank-matrix, we can weigh each feature with an up-to-date im-portance score.

In case of unsupervised feature selection, the evaluation criteria for selecting the feature subset is not provided explic-itly, and the general idea is that we want to capture the most important characteristics of dataset without losing too much information. To this end, we perform a spectral decomposi-tion on the affinity matrix to obtain a  X  X lat X  embedding of the datapoints. the intuition being that it is much easier to cap-ture the global trends of the stream in this embedded space than in the original space. Let Y [ t ] = [ Y 1 | ... | Y Since Y [ t ] is column-wise normalized to have unit Euclidean Y
Cai et al. [4] proposed an unsupervised feature selection approach using V t as the target variable in regression. The resulting regression problem can be stated as: coefficient for different features in approximating the eigen-of Y [ t ] ). Intuitively, X measures each feature X  X  contribu-tion for representing the different clusters (obtained from the columns in V t ) in the data. Note that the solution for (1) is X t = U t  X   X  1 t (assuming all the singular values in  X  non-zero).
 Now consider a rank-k approximation of Y [ t ] as defined by Theorem 2.1 (for an appropriately chosen parameter 4 k ), let Given the low-rank approximation of Y [ t ] , instead of using V t as the regression target, we could use V t ( k ) in (1). This yields the following (least-squares) regression problem: Note that the solution for (2) is X t = U t ( k )  X   X  1 t the top-k singular values of  X  t are non-zero).

However, simply using (2) may lead to an unstable solu-tion (if the input matrix is ill-conditioned) and also over-fitting to the data [4]. Therefore, we add a regularization term, and define: where  X  is the regularization parameter that controls the trade-off between the loss function and the p -norm ( p  X  { 1 , 2 } ). Generally, a regression formulation with L 1) and L 2 -norm ( p = 2) regularization are referred to as lasso and ridge regression respectively. The general formula of (3) was first concretized by Cai et al. [4] for the case of p = 1, who referred to it as Multi-Cluster Feature Selection (MCFS) .
We defer the discussion on setting of k to later. Readers could think of k as a small number k min( m,n t ).
After we obtain X t = ( x t i,j ) from (3), we can assign fea-ture importance score w t = ( w t 1 ,...,w t m )  X  R m (with the interpretation that the higher the score, the more importan-t the feature is). One of the simplest way is by using the following equation introduced in [4]: The aforementioned prototype algorithm for feature weight-ing is documented in Algorithm 1. The subsequent feature selection process can be done by ranking the w vector (in non-increasing order) and choosing the top-h features with the largest score (given that h features are needed).
Algorithm 1: GenFeatWeight (prototype algorithm for feature weighting)
Output : Feature importance score w t  X  R m at time t Other Kernels. Although we concentrate on the cosine kernel, the above framework can be generalized to other k-ernel functions. One approach would be to use the random feature map transformations known for all radial-basis func-tion kernels [21]. For instance, a Gaussian kernel can be approximated using random Fourier features [21] such that Gaussian kernel evaluation between a pair of datapoints can be approximated by the Euclidean inner product between the transformed pair. Using this randomized feature map one could, in a streaming fashion, transform all the data-points, and then work exclusively with these transformed datapoints in a framework similar to detailed above. Windowed Inputs. In our above problem formulations, all data till time t is used for selecting the top features at time t . However, some applications might require features to be selected based on a rolling window of inputs or by providing higher weights on the recent inputs, etc. Our algorithm could be easily adapted to these scenarios by modifying the matrix sketch construction. For simplicity, we ignore these aspects in this paper.
The first efficiency issue in using (3) is due to the regu-larization type. It is well-known that the ridge regression penalizes regression coefficients, rather than accomplishing variable/feature selection, while the lasso regression to some extent automatically sets insignificant coefficients to be zero. However, there is no previous analysis in the framework of Algorithm 1 about the performance difference obtained by using either the lasso or ridge regressions. In this subsection, we investigate this important topic.

A simpler situation arises when the design matrix of the regression problem consists of orthogonal columns . In this case, it is easy to show theoretically that the lasso and ridge regressions select almost the same features (Corollary 3.2).
Lemma 3.1 (Restated from [35]). Let  X  X denote the simple least squares coefficients, or in other words, tors obtained from the lasso and ridge regressions, respec-tively. If Y &gt; has orthogonal columns, then  X  X = Y A , and we have the following closed-form expressions: where for any scalar z , z + denotes the positive part, which is z if z &gt; 0 and 0 otherwise.
 Let w R and w L be the feature importance score calculated from  X  X R and  X  X L using (4). The following corollary follows because  X  X L and  X  X R are based on thresholding or scaling
Corollary 3.2. If w L has h non-zero weight features, then under the assumption of Lemma 3.1, the ranking of the top-h features in w L coincides with the ranking of the top-h features in w R .
 The above corollary implies, under the column orthogonality constraint on the design matrix, the performances of the lasso and ridge regressions in Algorithm 1 (especially, for the important features), are almost the same. Therefore, we can potentially use the more computationally cheaper regularization without any significant loss in performance. Figure 1: Performance comparison between the ridge ( p = 2 ) and the lasso ( p = 1 ) regression in the framework of (3) . The top 1000 features are picked using (4) . In Figure 1(a), red triangles show the tests when ridge outperforms lasso, while green cir-cles show the tests when lasso outperforms ridge. These results indicate that the lasso regression is s-lightly better than the ridge regression, however the running times in Figure 1(b) show that the ridge re-gression is far more computationally efficient than the lasso regression. The parameter  X  was manually tuned here (we will further analyze the setting of  X  in Section 5).

Note that the orthogonality of the design matrix is a very rigid constraint. However, as we will see later (Algorith-m
StreamFeatWeight ) in our setting, the design matrix will be of form ( U  X ) &gt; , where U is a matrix with orthogo-nal columns and  X  is a diagonal matrix (in our case, matrix of singular values). Since such a design matrix may not be too far from having orthonormal columns, we have a much higher chance of observing similar results from lasso and ridge regressions (as in Corollary 3.2) than in a general re-gression setting. 5 We conducted the following experiments to verify this hypothesis. We randomly sampled 45 data-points from three text datasets ( X 20 Newsgroup X ,  X  X CV1 X , and  X  X euters21578 X , refer to Section 5.1 for details about these datasets). For both cases of p = 1 and p = 2 in Algo-rithm GenFeatWeight , we generated the top 1000 feature set from w L and w R respectively. We then used a simple K -means algorithm on these selected features to evaluate their effectiveness in identifying the (true) document classes (the reasoning behind performing such an evaluation is ex-plained in Section 5). Figure 1(a) shows the clustering result under the Normalized Mutual Information (NMI) measure, and the results suggest that both the lasso and ridge regres-sions have comparable performances. However, in terms of running time (Figure 1(b)), ridge regression is substantially better than the lasso regression. 6 Since our goal is to handle large streaming datasets, ridge regression appears as a better choice 7 , since it generates results very similar to the lasso re-gression in the framework of Algorithm GenFeatWeight , but with far lesser running time. Therefore, we concentrate on the ridge regression from here on.

The simple closed form solution for (3) with p = 2 is as shown in the following lemma.
 Lemma 3.3. Consider the ridge regression solution, Then we have the following: where  X  t 1 ,..., X  t k are the top-k singular values in Y
Even though Algorithm GenFeatWeight is quite sim-ple, in a streaming environment the number of data items in Y [ t ] could become extremely large, which could lead to both computational and memory bottlenecks in running the algorithm. For example, the computational complexity of computing a truncated rank-k SVD is O ( mn [ t ] k ) [8] (given Y columns in Y [ t ] becomes very large ( n [ t ]  X   X  ). Our goal in the next section is to propose and analyze an efficient approach that has similar effectiveness as Algorithm Gen-FeatWeight in identifying top features but does so by u-tilizing limited storage and just one pass over the data in a streaming setting.
Note that if U has orthogonal columns and  X  is a diagonal matrix, then ( U  X ) &gt; has orthogonal rows but depending on the singular values in  X , the columns in ( U  X ) &gt; exactly be orthogonal. For the lasso regression, we use the algorithm proposed by Cai et al. [4].
Again note that this is not to be misconstrued as a general statement on the effectiveness of ridge vs. lasso regressions in other settings. As mentioned above the main bottleneck in Algorithm Gen-FeatWeight is in generating a low-rank approximation of Y [ t ] . To overcome this problem, we propose an approach based on matrix sketching that we outline next.

In his recent paper, Liberty [15] showed that by adapt-ing the Misra-Gries approach for approximating frequency counts in a stream, one could obtain additive error bounds for matrix sketching. More formally, in the setting of [15], the input is a matrix Z  X  R p  X  d . In each step, one row of Z is processed by the algorithm (called Frequent Directions ) in a streaming fashion, and the algorithm iteratively updates a matrix Q  X  R q  X  d ( q p ) such that for any unit vector x  X  R d , k Z x k 2  X  X  Q x k 2  X  2 k Z k 2 F /q .

Recently, Ghashami and Philips [7], reanalyzed the Fre-quent Directions algorithm of Liberty [15], to show that it provides relative error bounds for low-rank matrix approx-imation. Instead of Q , their algorithm return Q k (a rank-k approximation of Q ) and their main result shows that k Z k 2 F  X  X  Q k k 2 F  X  q/ ( q  X  k )  X k Z  X  Z k k 2 F .
Our approach for constructing a low-rank approximation of Y [ t ] (outlined between Steps 1-4 in Algorithm Stream-FeatWeight ) is based on extending the Frequent Directions algorithm of [15] to a more general setting where in every timestep, we add n t  X  1 new columns. 8 As in Frequent Di-rections , our algorithm requires just one pass over the data stream. Here, B t  X  R m  X  ` is the matrix sketch. The pa-rameter `  X  k , but is generally much smaller than m or n We discuss more on the setting of ` later. The Step 5 in Algorithm StreamFeatWeight is obtained by using Lem-ma 3.3 to solve the following ridge regression problem (note represents the right singular vectors of B t ):  X  where e i  X  R ` is a vector with 1 at location i , and 0 elsewhere (i.e., standard basis vector).

Algorithm 2: StreamFeatWeight (streaming update of feature weights at time t )
Output : Feature importance score  X  w t  X  R m and 2  X  3  X  5  X  6  X  7  X  i  X  [ m ] ,  X  w t i  X  max 1  X  p  X  k |  X  x t i,p | , where
A similar sketching based low-rank matrix approximation approach was recently used in an entirely different context of anomaly detection by Hao and Kasiviswanathan [10] At any time t , the running time of Algorithm Stream-FeatWeight is O (max { mn t `,m` 2 } ) (= O ( mn t ` ) if we as-sume `  X  n t ) by using power-iteration or rank-revealing QR decomposition for SVD [8] in Step 2. This computational complexity is much smaller than the O ( mn [ t ] k ) time com-plexity of Algorithm StreamFeatWeight (since n t ` n [ t ] ). Between iterations, the algorithm only maintains the B t matrix which takes O ( m` ) storage. The overall space complexity of Algorithm StreamFeatWeight is linear in the size of the input (i.e., O ( mn t )) at every time t , com-pared to Algorithm GenFeatWeight for which, at time t , the space complexity is O ( mn [ t ] )
The major focus of the rest of this section is to provide theoretical support for Algorithm StreamFeatWeight , by showing that the  X  w t from Algorithm StreamFeatWeight is a good approximation of w t obtained from Algorithm Gen-FeatWeight (with p = 2). 3.4.1 Theoretical Comparison (Bounding k w t  X   X  w t k )
We start with observation that, Therefore, a bound on k w t  X   X  w t k follows from respective since the columns in  X  U t ( k ) are orthonormal, k  X  U Bounding k U t ( k )  X   X  U t ( k ) k F . Here we use a recent result by Huang and Kasiviswanathan [10], who established an up-per bound on k U t ( k )  X   X  U t ( k ) k F by modifying the analysis of Frequent Directions by Ghashami and Philips [7] and com-bining it with some recent matrix perturbation results. To formally state their result we need few more definitions. Let where  X  t i is the i th singular value of Y [ t ] . 1. Define  X  a as,
It is easy to establish that for all t , k Y [ t ] ( k ) k (using an analysis from [10]), and by definition  X   X  1, therefore  X  a  X  1. Furthermore, for small k  X  X  (as in our setting), typically  X  is bounded, yielding  X  a = O (1). 2. Define  X  b as,
Again it is easy to establish that Y [ t ] Y &gt; [ t ] B therefore, k B t k 2  X  k Y [ t ] k 2 . Typically  X  is also bounded away from 1, yielding  X  b = O (1).
 Proposition 3.4 (Huang and Kasiviswanathan [10]). Let  X  i denote the i th eigenvalue of Y [ t ] Y &gt; [ t ]  X  | &gt; 0 . If for  X  a ,  X  b defined in (7) , (8) respectively, then Remark: For small k  X  X , and assuming 1 &lt;  X   X  O (1) (im-plying  X  a = O (1) and  X  b = O (1)), the above bound on ` could be simplified to, The assumption of L &gt; 0 is also something that is commonly satisfied in practice, especially if m is reasonably smaller than the number of datapoints in Y [ t ] .
 singular value of Y [ t ] and B t respectively. We have, A standard application of Weyl X  X  inequality [8], along with a sition.

Proposition 3.5. Putting it all Together (Bounding k w t  X   X w t k ). The following theorem follows by combining (6) with Proposi-tions 3.4 and 3.5.

Theorem 3.1. Let Y 1 ,...,Y t be a sequence of matrices best rank-k approximation of Y [ t ] . Let  X  t k and  X   X  singular value of Y [ t ] and B t respectively. Then w t ed by the Algorithm GenFeatWeight ) and  X  w t (generated by Algorithm StreamFeatWeight ), under conditions from Proposition 3.4, satisfy: k w t  X   X  w t k X  The above theorem shows that, under reasonable assump-tions and setting of ` , both Algorithms GenFeatWeight and StreamFeatWeight generate very identical feature vector weights at every timestep t . But as we discussed ear-lier, Algorithm StreamFeatWeight is far more efficient both in space and time consumptions. A point to note is that the Algorithm StreamFeatWeight can be used with any value of ` , the above bound on ` only guarantees that its feature selection results are similar to that of Algorith-m
GenFeatWeight .
We generate V t ( k ) using a cosine affinity matrix from L norm column normalized Y [ t ] . If we follow the same con-struction with a slightly different normalization, Y [ t ] zero mean and unit variance, then we obtain as regression target, the principal components (PCA) of Y [ t ] . Each princi-pal component captures different view of Pearson correlation coefficient matrix, and these principal components might be another good choice for the regression target. However, it is unclear how to construct this zero mean and unit variance normalization of Y [ t ] in a strict streaming fashion which is desired in this paper. The works of [16, 11] use N  X  1 Y for building random walk normalized cosine affinity matrix did not use the random walk normalization, V t ( k ) is not ap-proximating the normalized graph cut any more. However, in our case, V t ( k ) is not directly used for clustering, but just an intermediate step to select the important feature subset. In our experiments (Section 5), we achieve quite similar re-sults to the MCFS approach [4], where the affinity matrix is normalized in a batch setting.
We now justify the utility of our proposed approach by briefly comparing it with a few existing methods.

Our basic idea is to use regression analysis for feature se-lection. Many feature selection algorithms based on this idea have been proposed in the past decade. These algorithms operate by minimizing some appropriately defined objective function. Weston et al. [26] added an ` 0 -norm constraint on the solution to enforce sparsity, which naturally leads to a natural variable (feature) selection. But minimizing with ` -regularization is NP-hard, therefore ` 1 -norm, as a convex relaxation to ` 0 -norm, was utilized in [27, 4]. Other norm-s on the regularization term such as ` 2 -norm [6] and ` norm [33, 14] have also been explored for feature selection. One of the latest work in this area, proposed by Zhu et al. [34], performs feature selection by transferring models learnt on external (auxiliary) data sources, but it requires the di-mensionality of target data to be high and the number of datapoints to be small, which usually is not the case in data streaming. Few other recently proposed approaches in this area include [32, 12]. Although these methods are effective and robust to some degree, they are extremely inefficient, in both time and space, to be applicable in a streaming setting.
Feature selection algorithms operating in an online setting were proposed in [31, 23, 17], but they all require multiple passes over the data to converge to a stable model, and hence are not pass-efficient . Few other efficient feature selection methods such as [9, 29, 30] seem not well-suited to oper-ate in a streaming environment. In this paper, we propose a streaming algorithm that at every timestep efficiently as-signs each feature an importance score (weight) that can be subsequently used to rank (or select) the (top) features.
Also in the streaming setting, the approach of projected clustering [1, 18] can be viewed as a technique for  X  X ocal X  feature selection. The idea here is to have each cluster spe-cific to a particular feature subset that optimizes a quality criterion for that cluster. However, the feature subsets could be quite different across different clusters, therefore it leads to a complicated interpretation of clustering. Moreover, it is also very difficult to compare different clusters since their op-1 Reuter21578 8,293 18,933 65 2 TDT2 9,394 36,771 30 3 20Newsgroup 18,846 26,214 20 4 RCV1 193,844 47,236 103 5 USPS 9,298 256 10 6 MNIST 70,000 784 10 7 Tiny 1,000,000 3,072 75,062
Table 1: Statistics of the experimental datasets. Figure 2: The effect of  X  on four text datasets. For each dataset, we randomly generate 30 subsets and record the average K -means (NMI) result on the top 1000 features. The x -axis is the index i of  X  = 2 i  X  k (where k is number of clusters in each dataset). It can be observed that the NMI results show smooth changes across different  X  , and i = 3 , 4 , 5 are reason-able choices. timized subspaces are not in the same domain. On the other hand, our proposed approach provides a single comprehen-sive feature subset that covers all the clusters. Thereby, it gives an easy interpretation for clustering different classes.
In a somewhat orthogonal setting, online feature selection operating on feature streams 9 (instead of data streams as considered in this paper) have been investigated in [20, 28, 25].
In this section, we experimentally demonstrate that our proposed Algorithm StreamFeatWeight is highly scal-able, while still providing almost similar quantitative results to other expensive batch feature selection approaches.
It would be the best to evaluate feature selection results based on ground truth feature importance. But in real world applications, we cannot easily find such ground truth be-cause: 1) it is highly subjective to select candidate features because there are many similar features/terms, and 2) fea-ture selection is typically an intermediate step for the rest of data analysis pipeline. However, we do have many datasets with ground truth cluster labels. This can be utilized to e-valuate the quality of selected feature subset, by performing an unsupervised clustering on the feature-reduced dataset. If the selected feature subset is  X  X ood X , then clustering the data restricted to just this subset of features should yield a  X  X ood X  clustering result. Therefore, we evaluate the unsu-
Roughly, in this setting, feature vectors are streamed over time, e.g., one new feature is introduced at every timestep t . pervised feature selection algorithms by performing an un-supervised K -means clustering on the selected feature space. We used the popular Normalized Mutual Information (NMI) as our evaluation metric.
 Baselines. From now on, we refer to Algorithm Stream-FeatWeight as FSDS (Feature Selection on Data Stream-s). To the best of our knowledge there is no other streaming unsupervised feature selection algorithm. We chose the fol-lowing unsupervised batch feature selection approaches as baseline methods: Multi-Cluster Feature Selection ( MCFS ) [4], LaplacianScore [9], and Algorithm GenFeatWeight with p = 2 (henceforth, referred to as GFW-p2 ). MCFS (based on lasso regression) and LaplacianScore (based on finding local manifold structure) are both batch feature selection algorithms and were selected for comparison because they capture the essence of two popular ideas for feature selec-tion.

Since we used clustering to measure the performance of feature selection, we also included for comparison the clas-sical K -means ( Kmeans ) and a recent streaming variant of K -means ( StreamKM ) [22]. Both these K -means algo-rithms are operated on the whole feature set (unlike other compared approaches).
 Datasets and Preprocessing. We evaluated the above algorithms on four popular text datasets (Reuter21578, T-DT2, 20Newsgroup, and RCV1) and three image dataset-s (USPS, MNIST, and Tiny), whose statistics are summa-rized in Table 5.1. 20Newsgroup is a balanced dataset that covers 20 news topics. Reuters21578, TDT2, RCV1 are un-balanced datasets with quite different sizes of clusters. All these datasets can be found in [3]. Both the USPS and M-NIST datasets have 10 classes of handwritten digits. Tiny is a large web-image collection for non-parametric object and scene recognition (downloaded from [24]). Among 80 million images, we randomly selected 1 million images and evaluat-ed the result on 60 , 000 labeled images that cover 100 classes (from [13]). We directly performed experiments on the Tiny images with all the 3 , 072 raw features, which are 32  X  32 color images in RGB color channels.

MCFS and LaplacianScore algorithms are space and time inefficient due to computations of normalized Laplacian ma-trix or eigenvalue decomposition (also solving lasso regres-sion for MCFS). We cannot evaluate such algorithms on large benchmark datasets. Therefore the majority of dataset-s were selected to compare the effectiveness of all the base-lines. For the three image datasets, we used the approxi-mation of Gaussian kernel using random feature maps [21]. For every timestep t , we used the same random projection basis and offset setting (refer to [11] for more details about implementing these random feature maps).
 Parameter Settings. The number of selected features, h , was set from 200 to 2400 (in increments of 200) for text datasets, and from 25 to 200 (in increments of 25) for image datasets (since the number of meaningful features in raw im-ages is usually small). In our streaming setting, the number of singular vectors k was set to be the same as the num-ber of clusters in the dataset, which was assumed to be a priori known as in prior works [9, 4], and the size of each data stream ( n t  X  X ) as 1000 (we will further analyze stabili-ty against n t in Section 5.4). Our proposed algorithm has two specific parameters: the size of matrix sketch ` (which is set as the square root of the feature size as suggested by the analysis in Section 3.4) and the regularization parameter  X  . There are prior works in deciding the best regularization parameter  X  [5]. However, given the target problem is unsu-pervised and the dataset is big, we performed the following experiments on sampled datasets to select  X  . We randomly generated 30 subsets from four text datasets respectively, and evaluated the average quality of the top-1000 selected features using Lemma 3.3 with various values of  X  . Since the singular value distributions (  X  j  X  X ) of most datasets usu-ally decay rapidly, we set  X  as 2 i  X  k for i  X  R . The average NMI result is presented in Figure 2. It can be seen that the best results appear when i is around 3 to 5. Therefore we set i = 3 by default, and set  X  = 2 3  X  k for all our experi-ments. For the random feature maps using Gaussian kernel, we set bandwidth-scale as 5000 and the projected dimension as d n/k e (following [11]).

For MCFS and LaplacianScore, we followed [4, 9] for their respective parameter settings. For the NMI evaluation step, we utilized the standard within-cluster sum of squares K -means (with 100 inner loops and 100 outer loops) to obtain stable cluster assignments.
We make the following observations based on Figure 3. (1) Compared with Kmeans/StreamKM on the whole fea-tures space, feature selection can indeed improve clus-tering performance on these high dimensional datasets.
This is an argument in favor of performing feature selec-tion (similar observations have been made elsewhere). (2) In general, the regression-based algorithms (FSDS, GFW-p2, and MCFS) perform much better than the Lapla-cianScore algorithm. It is because LaplacianScore eval-uates features individually, so that the selected feature subset may come from similar global patterns. On the other hand, the other three algorithms have more compre-hensive views due to their use of regression-based feature selection w.r.t. different global patterns (eigenvectors). (3) GFW-p2 has very comparable result with MCFS (which can be seen as further evidence supporting the lasso vs. ridge argument in Section 3.3), although the latter per-forms better than the former in some specific spots (e.g., with smaller number of features in the TDT2 dataset). (4) On average, FSDS achieves more than 99% NMI of GFW-p2 on the text datasets. It confirms our theoretical proof (Theorem 3.1) that feature weight vectors produced by
Algorithms GenFeatWeight and StreamFeatWeight are close to each other. (5) Typically, FSDS achieves about 97%  X  99% NMI of M-
CFS on the text datasets. This observation shows that in our problem setting (Sections 3.2 and 3.3), a streaming ridge-based method is capable of obtaining similar per-formance as that of a batch lasso-based method (MCFS).
The FSDS performs worse than MCFS when the number of features is small (TDT2 dataset). It could probably be because the regression target of MCFS that comes from normalized spectral analysis may boost the quality of the small number of the selected features. However, if the number of features is large enough, FSDS and MCFS have very comparable performances. (6) For the experiments on the image datasets (with approxi-mated Gaussian kernel), FSDS has similar or even better performance than MCFS (Figures 3(e) and 3(f)). (7) On large datasets, such as RCV1 (Figure 3(d)) and Tiny (Figure 3(g)), MCFS and LaplacianScore algorithms ran out of memory since they require constructing the affinity matrix (which takes O ( n 2 ) space). Memory troubles also prevented GFW-p2 from completion on the Tiny dataset.
Figure 4 shows the scalability comparison between the fea-ture selection algorithms using the Tiny dataset ( k is set as 10 here). MCFS requires solving lasso while LaplacianScore needs full-matrix operations, therefore both of them are far less scalable than GFW-p2. FSDS is designed for a stream-ing setting and the computations in it happen without using any historical data, therefore it is the most efficient tech-nique among all the compared approaches.

We observed that FSDS is on average about 10 times faster than LaplacianScore and about 50 times faster than MCFS. Also FSDS outperforms GFW-p2 when the dataset size is above 10 , 000 and the difference between their run-ning times will grow as the dataset size increases. On the 20Newsgroup dataset, FSDS takes about 23 seconds, and is about 3 , 35, and 100 times faster than GFW-p2, Lapla-cianScore, and MCFS respectively. Similarly, on the MNIST dataset, FSDS takes about 4 seconds, and is about 4 , 64, and 400 times faster than GFW-p2, LaplacianScore, and MCFS respectively. Figure 4: Scalability experiments on the Tiny dataset. Except our proposed FSDS, none of the other compared approaches could scale beyond  X  10 5 points (failing because of their extremely high mem-ory overhead).
It is well-known that streaming algorithms are generally sensitive to the order of data, or concept drift. To test the performance of FSDS in such scenarios, we used the data stream sorted by timestamps as input. The performance of FSDS in this realistic testing environment is shown in Fig-ure 5, with different sizes of feature set. We also compared against a scheme where we just used a static feature subset (with 200 features) without adapting to concept drift. This static feature subset was determined by FSDS using only the first 2 , 000 samples. For the two unbalanced datasets Reuters21578 and TDT2, the larger clusters appear in the very beginning. Therefore, initially the approach based on static feature subset performs quite close to FSDS. However, as time goes on and concept drift becomes more prominen-t, FSDS continues maintaining a good stable performance across all the three datasets, which demonstrates that FSD-S is capable of quickly adapting to concept drifts. (c) 20Newsgroup even in the presence of inherent concept drift in the data stream.
FSDS tests across different batch sizes ( n t  X  X ) and feature subset sizes indicate its very stable behavior (Figure 6).
For many streaming applications, we not only want to i-dentify the top-h features in the data but also want to store the data restricted to these top-h features at any time, to enable some further data mining analyses. In general, such analysis in streaming setting would require storing the w-hole data at each timestep as the set of the top-h features dynamically changes over time. However, using FSDS, we empirically noticed that storing the data stream restricted to top g  X  h features at each intermediate timestep, and a final selection of only those features which appear in the top g  X  h features for each of the intermediate timestep, suffices to get good results, even when g is a small number (i.e., 4). The results are shown in Figure 7. The number of features test-ed in these experiments are h = { 200 , 400 , 600 , 800 , 1000 } with g = { 1 , 2 , 3 , 4 } . We also report FSDS results where we store the entire data, and then use the data to obtain the final top h features (we call this strategy Full). Even set-ting g = 1, we already achieve about 91% NMI compared to the Full (FSDS) at h = 200. As we increase g , the results get better and it suggests that it could be enough to store data restricted to the top O ( h ) features at each timestep to enable further analyses.
We proposed an unsupervised feature selection algorith-m for handling high dimensional data points arriving in a streaming fashion. Our algorithm uses ideas from matrix sketching to generate a continuous low-rank approximation of the input, which is then used in a regularized regression framework to obtain the individual feature weights. The al-gorithm only requires one-pass over the data, utilizes limited storage, and operates in near-real time. Theoretical results and experimental validation confirm that our proposed al-gorithm is efficient in both space and time for the task of streaming unsupervised feature selection.
 This research was supported in part by BNL PD 15-025, Center for Data Driven Discovery (C3D). [1] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. A [2] C. Boutsidis, D. Garber, Z. Karnin, and E. Liberty. [3] D. Cai. Datasets. http://bit.ly/1IpMnAy. [4] D. Cai, C. Zhang, and X. He. Unsupervised feature [5] S. Chatterjee and A. S. Hadi. Regression Analysis by [6] A. Dasgupta, P. Drineas, B. Harb, V. Josifovski, and [7] M. Ghashami and J. M. Phillips. Relative errors for [8] G. H. Golub and C. F. Van Loan. Matrix [9] X. He, D. Cai, and P. Niyogi. Laplacian score for [10] H. Huang and S. Kasiviswanathan. Streaming [11] H. Huang, S. Yoo, D. Yu, and H. Qin. Diverse power [12] H. Huang, S. Yoo, D. Yu, and H. Qin. Noise-resistant [13] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-100 [14] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu.
 [15] E. Liberty. Simple and deterministic matrix sketching. [16] F. Lin and W. W. Cohen. A very fast method for [17] C. Maung and H. Schweitzer. Pass-efficient [18] I. Ntoutsi, A. Zimek, T. Palpanas, P. Kr  X  oger, and [19] S. Papadimitriou, J. Sun, and C. Faloutsos. Streaming [20] S. Perkins and J. Theiler. Online feature selection [21] A. Rahimi and B. Recht. Random features for [22] M. Shindler, A. Wong, and A. W. Meyerson. Fast and [23] Q. Song, J. Ni, and G. Wang. A fast clustering-based [24] A. Torralba, R. Fergus, and W. T. Freeman. 80 million [25] J. Wang, P. Zhao, S. Hoi, and R. Jin. Online feature [26] J. Weston, A. Elisseeff, B. Scholkopf, and M. Tipping. [27] D. M. Witten and R. Tibshirani. A framework for [28] X. Wu, K. Yu, H. Wang, and W. Ding. Online [29] S. Xiang, X. Shen, and J. Ye. Efficient sparse group [30] S. Xiang, T. Yang, and J. Ye. Simultaneous feature [31] H. Yang, M. R. Lyu, and I. King. Efficient online [32] S. Yang, L. Yuan, Y. C. Lai, X. Shen, P. Wonka, and [33] Y. Yang, H. T. Shen, Z. Ma, Z. Huang, and X. Zhou. [34] X. Zhu, Z. Huang, Y. Yang, T. H. Shen, C. Xu, and [35] H. Zou and T. Hastie. Regularization and variable
