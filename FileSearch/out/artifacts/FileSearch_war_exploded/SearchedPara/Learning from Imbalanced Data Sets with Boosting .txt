 800 King Edward Road , Ottawa, Ontario, Canada, K1N 6N5 School of Information Technology and Engineering, 800 King Edward Road , Ottawa, Ontario, Canada, K1N 6N5 Learning from imbalanced data sets, where the number of examples o f one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the min ority class. In this paper, we describe a new approach that combines boosting, an ensemble -based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoos t -IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. T he DataBoost -IM method was evaluated, in terms of the F -measures , G -mean and over all accuracy , against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers . Our results are promising and show that the DataBoost -IM me thod compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting -based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes. Dat a mining, Imbalance d data sets , En semble s of classifiers , B oosting The class imbalance problem corresponds to domains for which one class is represented by a large number of examples while the other is represented by only a few [1]. Many real world applications involve learning from imbalanced sets, such as fraud detection, telecommunications management, oil spill detection and text classification [2]. When learning from imbalanced data sets, machine learning algorithms tend to produc e high predictive accuracy over the majority class, but poor predictive accuracy over the minority class [3]. There have been several proposals for coping with imbalanced data sets [1]. Kubat et al. under -sampled examples of the majority class [5]; Ling a nd Li over -sampled examples of the minority class [3]; Chawla et al. over -sampled the minority class and under -sampled the majority class [2]; Cardie et al. weighted examples in an effort to bias the learning toward the minority class [3]; Joshi et al. eva luated boosting algorithms to classify rare classes [6]; and Chawla et al. combined boosting and synthetic data to improve the prediction of the minority class [7 ]. Over the past few years, e nsemble s have emerged as a promising technique with the ability to improve the performance of weak classification algorithms [8, 9]. Ensembles of classifiers consist of a set of individually trained classifiers whose predictions are combined to classify new instances [8, 9]. In particular, boosting is an ensemble metho d where the performance of weak classifiers is improved by focusing on hard examples which are difficult to classify. Boosting produces a series of classifiers and the outputs of these classifiers are combined using weighted voting in the final prediction of the model [10]. In each step of the series, the training examples are re -weighted and selected based on the performance of earlier classifiers in the training series. This produces a set of  X  X asy X  examples with low weights and a set of hard ones with hi gh weights. During each of the iterations, boosting attempts to produce new classifiers that are better able to predict examples for which the previous classifier X  X  performance is poor. This is achieved by concentrating on classifying the hard examples cor rectly. Recent studies have indicated that boosting algorithm is applicable to a broad spectrum of problems with great success [10, 11].
 In this paper, we discuss a novel approach for learning from imbalanced data sets, DataBoost -IM, that combines data gen eration and boosting procedures to improve the predictive accuracies of both the majority and minority classes , without forgoing one of the two classes. That is, the aim of our approach is to ensure that the resultant predictive accuracies of both classes are high. Our approach differs from prior work in the following ways. Firstly, we separately identify hard examples from , and generate synthetic examples for , the minority as well as the majority classes. Secondly, we generate synthetic examples with bias information toward the hard examples on which the next component classifier in the boosting procedures needs to focus. That is, we provide additional knowledge for the majority as well as the minority classes and thus prevent boosting over -emphasizing the hard examples. Thirdly, the class frequencies in the new training set are rebalanced to alleviate the learning algorithm X  X  bias toward the majority class. Rebalancing thus involves the utilization of a reduced number of examples from the majority and minor ity classes to ensure that both classes are represented during training. Fourthly, t he total weights of the different classes in the new training set are rebalanced to force the boosting algorithm to focus on not only the hard examples, but also the minori ty class examples. In this way, we focus on improving the predictions of both the minority and majority classes.
 This paper is organized as follows. Section 2 describes the DataBoost -IM algorithm. This is followed, in Section 3, with a comparative evaluati on of the DataBoost -IM algorithm against seventeen data sets. Finally, Section 4 concludes the paper.
 A LGORITHM D ata B oost -IM Input : S equence of m examples ) y , x ( ),..., y , x ( Initialize m / 1 ) i ( D Do for t = 1, 2, ..., T The DataBoost -IM approach extends our earlier DataBoost algorithm which was successfully used to produce highly accuracy classifiers in balanced domains containing hard to learn examples [13]. In thi s section, we describe a variation, the DataBoost -IM algorithm, which is applied to imbalanced data sets. This approach extends the original DataBoost algorithm as follows. Firstly, we separately identify hard examples from and generate synthetic examples for different classes. Secondly, the class distribution and the total weights of different classes are rebalanced to alleviate the learning algorithms X  bias toward the majority class , by choosing a reduced number of representative (seed) examples from both classes.
 Recall that boosting involves the creation of a series of classifiers which aim to correctly classify hard to learn examples, through focusing on these hard examples during training. Following this mechanism, the DataBoost -IM algorithm, as shown in Figure 1, consists of the following three stages. Firstly, each example of the original training set is assigned an equal weight. The original training set is used to train the first classifier of the DataBoost -IM ensembles. Secondly, the hard examples (so -called seed examples) are identified and for each of these seed examples, a set of synthetic examples is generated. During the third stage of the algorithm, the synthetic examples are added to the original training set and the class distribution and t he total weights of different classes are rebalanced. The second and third stages of the DataBoost -IM algorithm are re -executed until reaching a user -specified number of iterations or the current component classifier X  X  error rate is worse than a threshold value. Following the AdaBoost M1 ensemble method, this threshold is set to 0.5 [8,9]. The seed selection, data generation and re -balancing process of the DataBoost -IM algorithm are described next. Throughout this discussion, the Hepatitis data set is used as an illustrative example [12]. The Hepatitis data set contains 155 examples of Hepatitis patients, described by 19 continuous and discrete attributes. Of these cases, 123 corresponds to the patients who survived treatment (class  X  X ive X  ) and 32 examples o f mortalities (class  X  X ie  X ). The aim of the seed selection process is to identify hard examples for both the majority and minority classes. These examples are used as input for the data generation process as discussed in Section 2.2 . The seed examples are selected as follows. Firstly, the examples in the training set ( E train ) are sorted in descending order , based on their weights. The original training set E train examples from the majority class and N min examples from the minority class. The number of examples that is considered to be hard (denoted by N s ) is calculated as ( E train x Err) , where Err is the error rate of the currently trained classifier. Next, the set E which contains the N s examples with the highest wei ghts in E is created. The set E s consists of two subset of examples E E smaj, i.e. examples from the minority and majority classes, respectively. Here, E smin and E smaj contain N examples, where N smin &lt; N min and N smaj &lt; N of seed examples of the majority class in E smaj by calculating M which is equal to min (N maj /N min, N smaj ) . Correspondingly, a subset M of the minority class examples in E smin, is selected as seeds, where M S is calculated as min ( (N maj x M L ) / N values of M l and M s were found, by inspection, to produce data generation set sizes which augment the original training set well. The final sets of seed examples are placed in sets E Note that, when considering an imb alanced data set, our experimental results against seventeen data set s indicate that a very high percentage of minority class examples are hard examples with high weights . Due to this fact, experimental results show that for the seed examples, the number o f higher weighted examples from the minority class is more .
 For example, for the illustrative Hepatitis data set, assume that, in the fifth iteration of the boosting, the current trained classifier X  X  error rate is 18%. The set E s will consist of the 27 exa mples with the highest weights as selected from the sorted E hard examples, 2 correspond to the majority class  X  X ive X , and 25 examples are of the class  X  X ie X . That is, the high occurrence of examples from the minority class is due to the fact that, for imbalanced data sets, the minority class is harder to learn. M equal to 2 , calculated as M L = min (2, 3) and E mai both hard examples of the majority class  X  X ive X . M and the set E min will consist of the 8 highest weighted examples of class  X  X ie X . The output of this step is shown in the Table 1.
 T able 1 : Seed examples and their weights of the Hepatitis Data set The aim of the data generation process is to generate additional synthesis instances to add to the original t raining set E data generation process extends our earlier work, as presented in [13, 17, 18], by generating data for the majority and minority classes separately. That is, t he data generation process generates two sets of data. Firstly a total of M L sets of new majority class examples , based on each seed instance in E maj , are generated. For each attribute included in the synthetic example, a new value is generated based on the following constraints [13, 17, 18]. For Nominal attribute, the data g eneration produces a total of 
N maj attribute values for each seed in E maj . The values are chosen to reflect the distribution of values contained in the original training attribute with respect to the particular class. 
This is achieved by considering, for e ach class, the number of occurrences of different attribute values in the original data set. 
For example, the attribute  X  X ENDER X  in the Hepatitis data set has a value of either  X  X ALE X  or  X  X EMALE X . Assume that for the class  X  X ive X , the number of occurrences of  X  X ALE X  is 16 and  X  X EMALE X  is 107. The data generation creates 16 occurrences of  X  X ALE X  and 107 occurrences of  X  X EMALE X . 
These 123 values are randomly assigned to the 123 examples created during data generation.
For Continuous attribute, the data gener ation produces a total of N maj attribute values. The values are chosen by considering the range [ min, max ] of the original attribute values with respect to the seed class. Also, the distribution of original attribute values, in terms of the deviation and t he mean, is used during data generation. For example, assume that, for the  X  X LBUMIN X  attribute in the Hepatitis data set, the 123 values for class  X  X ive X  lies between 2.1 and 6.4, and the mean and deviation values are 3.817 and 0.652. The data generation r andomly generates a total of 123 values between 2.1 and 6.4, following a mean value of 3.817 and a deviation value of 0.652. 
Again, the 123 values are randomly assigned to the 123 examples generated. 
Similarly, M s different sets of new minority class exam ples, each based on a seed instance in E min , are constructed. These sets of instances are added to the original training set.
 Table 2 : The number of synthetic examples generated and the number of their seeds Majority Class 369 246 123 2 Minority Class 288 256 32 8 For the Hepatitis example, recall from Table 1 that E maj examples for the class  X  X ive X  and E min contains 8 instances for the class  X  X ie X . Followed the above -mentioned ap proach, the data generation process generates two sets of examples for the class  X  X ive X , each set contains 123 synthetic examples for each one of the seeds in E maj ,. Eight sets of examples containing a total of 32 synthetic examples of the class  X  X ie X , bas ed on the 8 seed examples in E min , will also be created. A total set consisting of 246 synthetic examples of  X  X ive X  and 256 instances of  X  X ie X  is thus newly generated, as shown in Table 2. These instances are added to the original training set, leading to a final training set containing 369 instances of the class  X  X ive X  and 288 instances of the class  X  X ie X . Note that a detailed description of the data generation process falls beyond the scope of this paper. Interested readers are referred to [13, 17, 18] for a description of this process and its evolution . In the final step prior to re -training, the total weights of the examples in the different classes are rebalanced. Recall that b oosting a ims to , during each of the iterati ons, produce new classifiers that are better able to predict examples for which the precious classifier X  X  performance is poor . This is achieved by concentrat ing on classifying the examples with high weights correctly. In an imbalanced data set, the differe nce of the total weights between the different classes is large. By rebalancing the total weights of the different classes, boosting is forced to focus on hard as well as rare examples.
 Recall that the data generation process generates sets of synthetic ex amples based on seed examples E maj and E min corresponding to the majority and minority classes. Before the generated data are added to the original data set, each of the synthetic examples is assigned an initial weight. The initial weight of each example i s calculated by dividing the weight of the seed example by the number of instances generated from it. In this way, the very high weights associated with the hard examples are balanced out. Rebalancing ensures that the boosting algorithm focuses on hard as well as minority class examples.
 When the new training set is formed, the total weights of the majority class examples (denoted by W maj ) and the minority class examples (denote by W min ) in the new training data are rebalanced as follows. If W maj &gt; W min , th e weight of each instance in the minority class is multiplied by W maj / W min , Otherwise, the weight of each instance in the majority class is multiplied by W In this way, the total weight of the majority and minority classes will be balanced. No te that, prior to training, the weights of the new training set will be renormalized, following the AdaBoost M1 method, so that their sum equals one [8, 9, 10]. For the Hepatitis example, assume that seed example x in E a weight of 9.86 and seed exa mple y has a weight of 9.62. This implies that each of the 123 synthetic examples generated based x is assigned a weight of 9.86/123 and those based on y are assigned weights of 9.62/123. Similarly, an initial weight are assigned to each of the synthetic e xamples generated based on the seed examples from E min . After adding the synthetic data to the original data set, the new training data set contains 369 examples of class  X  X ive X  and 288 cases of the class  X  X ive X . Assume that W is equal to 122.51 and W mi n equals 69.83. Since W maj of the 288 examples describing the minority class is multiplied by a constant equal to 122.51/69.83. As a result, the total weights of the majority and minority classes in the new training set are equal to 122.51, th us equally distributing the balance of the two classes . This section describes the results of evaluating the performance of the DataBoost -IM algorithm, in comparison with the C4.5 decision tree [20] , AdaBoostM1 [8, 9] , DataBoost [13], AdaCost [21], CSB2 [22] and SMOTEBoost [7] boosting algorithm s . The C4.5 algorithm, which has become a de facto standard against which new algorithms are being judged, is used as base classifier [2 3 ] . Traditionally, the performance of a classifier is evaluated by cons idering the overall accuracy against test cases [16]. However, when learning from imbalanced data sets, this measure is often not sufficient [16]. Following [3, 4, 5, 7], we employ the overall accuracy , G -Mean [5] and F -Measures [14] metrics to evaluate ou r DataBoost -IM method. The confusion matrix, as shown in Table 3, represents the typical metrics for evaluating the performance of machine learning algorithms on skew class problems. In Table 3, the TP Rate and FP Rate are calculated as TP/(FN+TP) and FP/ (FP+TN) . The Precision and Recall are calculated as TP / (TP + FP) and TP / (TP + FN) . The F -measure is defined as where  X  corresponds to the relative importance of precision versus the recall and it is usually set to 1. The F -measure incorporates the recall and precision into a single number. It follows that the F -measure is high when both the recall and precision are high [6]. This implies that the F -measure is able to measure the  X  X oodness X  of a learning algorithm on the curr ent class of interest. Note that we also use this measure for the majority class, since we are interested in measuring the performance of both classes. The ROC curve is a technique for summarizing a classifier X  X  performance over a range, by considering the tradeoffs between TP Rate and FP Rate [15]. Another criteria used to evaluate a classifier X  X  performance on skew data is the G -mean [4, 5, 7] . T he G -mean is defined as where Positive Accuracy and Negative Accura cy are calculated as TP/(FN+TP) and TN/(TN+FP) . This measure relates to a point on the ROC curve and the idea is to maximize the accuracy on each of the two classes while keeping these accuracies balanced [5]. For instance, a high Positive accurac y by a lo w Negative accuracy will result in poor G -Mean [5].
 Table 4 : Summary of the data sets used in this paper. Shown are the number of examples in the data set; the number of minority class; the number of majority class; the class distribution; the number of co ntinous , and the number of discrete input features. Data set Case Min. S ONAR 208 97 111 0.47:0.53 60 0 M ONK 2 169 64 105 0.37:0.63 0 6 I ONOSPHERE 351 126 225 0.35:0.65 34 0 B REAST -W 699 241 458 0.34:0.6 6 9 0 B REAST -CANCER 286 85 201 0.30:0.70 0 9 P HONEME 5484 1586 3818 0.29:0.71 5 0 V EHICLE 846 199 647 0.23:0.77 18 0 H EPATITIS 155 32 123 0.20:0.80 6 13 S EGMENT 2310 330 1980 0.14:0.86 19 0 GLASS 214 29 185 0.13:0.87 9 0 S ATIMAGE 6435 626 5809 0.09: 0.91 33 0 VOWEL 990 90 900 0.09:0.91 10 3 S ICK 3772 231 3541 0.06:0.94 6 23 ABALONE 731 42 689 0.06:0.94 7 1 YEAST 483 20 463 0.04:0.96 8 0 P RIMARY -TUMOR 339 14 325 0.04:0.96 0 17 OIL 937 41 896 0.04:0.96 49 0 To evaluate the performance of the Data Boost -IM , we obtained sixteen data sets from the UCI data repository [12] a s well as the O il Spill data set [4, 5] . These data sets were carefully selected to ensure that they (a) are based on real -world problems , (b) varied in feature character istics, and (c) vary extensively in size and class distribution. Table 4 presents the character istics of the data sets Actual Negative TN ( the number of Actual Positive FN (the number of used for the experiments. Shown are the number of cases , the number of the majority and mi n ority classes, the class distribution, and t he type of the features. For the Glass , V owel, V ehicle, S atimage , and P rimary -tumor data sets, we increased the degree of skew by converting all but the smallest clas s into a single class. For the S ick data sets, we deleted the 'TBG' attribute due to the hig h number of missing values. For the A balone and Yeast data set s , we respectively learned the classes  X 9 X  versus  X 18 X  in the Abalone, and the classes  X  X YT X  versus  X  X OX X  in the Yeast in order to present the DataBoost -IM algorithm highly imbalanced problems. We implemented the experiments using Weka [19] , a Java -based knowledge learning and analysis environment developed at the University of Waikato in New Zealand.
 Results for the above data sets , as shown in Table 5 and appendix A , were averaged over five standard 10 -fold cross validation experiments. For each 10 -fold cross validation the data set wa s first partitioned into 10 equal sized sets and each set wa s then in turn used as the test set while the classifier t rains on the other nine sets . A stratified sampling technique was applied here to ensure that each of the sets had the same proportion of different classes . For each fold an ensemble of ten component classifiers wa s created. In the experiments, the C4.5 de cision trees were pruned [20] .
 The experimental results for the eigth most highly imbalanced data sets , as described in Table 4 , are presented in Table 5 . We also include the results for the nine moderately imbalanced data sets in Appendix A . For each dat a set, we present the results achieved when using the C4.5, AdaBoostM1, DataBoost, AdaCost [21], CSB2 [22], SMOTEBoost [7] and DataBoost -IM methods. Also, for each algorithm, the table presents the results in terms of the G -mean , overall accuracy rates, TP rate s and F -m easure s against the majority and minority class es respectively . Following the work of Fan et al. [21], Ting [22], and Chawla et al. [7] , t he cost -adjustment functions from the AdaCost and CSB2 algorithms were chosen as follows: 5 . 0 * 5 . 0 + =  X  , where +  X  and  X   X  are the functions for correctly and mislabeled labeled examples, respectively. Also, t he three cost factors used in these two algorithms are 2, 5, and 9 . The par ameter N , which specifies the amount of synthetically generated examples in the SMOTEBoost was set to 100, 300 and 500 , respectively.
 The results, as shown in T able 5 i ndicate that the DataBoost -IM algorithm performs well against highly imbalanced data set s, in terms of the F -measures of both the minority and majority classes. In many cases, our results are comparable or slightly higher than that produced by the other al gorithms. Our approach also yields good results in terms of the G -mean and overall accuracy , when compared to the other approaches.
 T he DataBoost -IM approach achieved promising results when considering the minority class F -measure s . In some cases, such a s the Yeast and Sick data set s, the value obtained is the same as , or slightly higher than , the best value produced by the other algorithms. However, for data set s such as the Abalone , Primary -Tumor , Glass, and Satimage the minority class F -measure surpass es that of all the other techniques. Also, t he majority class F -measure against five of the eight data set s as produced by the DataBoost -IM algorithm is the highest , or the same as , that obtained by the other algorithm s . For the other three data set s, nam ely the Sick, Abalone and Primary -Tumor data set s, the values obtained are lower by only 0.1 , 0.1 and 0.9 , when compared to the best performing algorithm. O ur results when comparing the DataBoost -IM method to the base -line C4.5 and AdaBoostM1 methods, sho ws that DataBoost -IM performs well in terms of both F -Measures . I n some cases such as the Glass, Abalone, Yeast, Primary -Tumor, and Oil spill data sets , the re are large improvements . For example, in the Oil spill data set, the C4.5 and AdaBoostM1 algorithm s produced minority class F -measures of 37.6 and 38.8 , respectively . T he DataBoost -IM approach achieved a minority class F -measure of 55.0. In the Primary data set, the C4.5 and AdaBoostM1 algorithms produced minority class F -measures of 0 and 19.0 respect ively , whereas t he DataBoost -IM approach achieved a minority class F -measure of 28.5. Also, the G -mean of th e DataBoost -IM method are, for all eight data sets, the same of slightly higher than that of the other two approach. Similar results holds for the o verall accuracy , where DataBoost -IM performs slightly lower in only one case.
 W hen considering th e DataBoost -IM and the original DataBoost approach es , t h e values shown in Table 5 confirm that the DataBoost -IM approach benefit s from generating synthetic exa mples for different classes separately and rebalancing the class frequencies and the total weights from the different classes. The DataBoost -IM approach achieved higher F -measures against both classes, except for the Primary -Tumor data se t , in which the va lue against the majority class is lower by only 0.2 , and the Satimage data set , in which the value s agai nst the majority class we re the same . In many cases the improvement for the minor ity class was quite significant . For example, in the Primary -Tumor data set, the DataBoost method produced a minority class F -measure of 17.3, while The DataBoost -IM approach achieved a value of 28.5. Also, for the highly imbalanced Oil spill and Abalone data set s , the improvements achieved by the DataBoost -IM method over Dat aBoost algorithm were promising , i.e. 9.5 and 5.3 respectively . For the G -mean and overall accuracy , the DataBoost -IM method consistently produce d similar or higher results than that of the DataBoost algorithm.
 When c omparing the DataBoost -IM method with t he AdaCost, CSB2, and SMOTEBoost algorithms , the results shows that the DataBoost -IM method produced results which compare well , in terms of the G -mean , overall accuracy and F -measures . T he DataBoost -IM method achieved similar or slightly better minority a nd majority class F -measure s against the eight data sets . Importantly, our results indicate tha t our approach does not sacrific e one class, but produces high predictions against both. For example, for the Primary -Tumor data set , where the majority class F -measure v alues were lower by 0.2, when compared to the best values obtained by the other algorithm s , the improvement of minority class F -measure we re 4.2 . Similarly, f or the Abalone and G lass data set s , the improvement s in minority class F -measures were 5. 3 and 4.8 , respectively. Further analysis of the moderately imbalanced data sets , as shown in Appendix A , shows that the DataBoost -IM approach obtained the highest values against six of the nine data set s in term s of the minority class F -measures . Also, against seven of the nine data set s the majority class F -measure results are slightly higher. However, the improvements in term of F -measures are less significant than those against the highly imbalanced data sets. For the Monk2 and Breast -Cancer data set s , the majority class F -measure s decreased by more than 40.0 , when compared to the values obtained by the DataBoost -IM algorithm. The same results hold for the overall accuracy and G -mean , where DataBoost -IM produces comparable and slightly higher results i n six of the nine data sets.
 In conclusion, the results, as shown in Table 5 and Appendix A , indicate that the results obtained by the Data Boost -IM approach are comparable to that of the other techniques, when evaluated in terms of overall accuracy , G -mean and F -measures . In particular, the results against highly imbalanced data sets are promising. Importantly, f or some highly imb a lanced data sets, the DataBoost -IM approach produces the highest results in terms of both minority and majority class F -measure s . Results indicate that the DataBoost -IM technique does not sacrifice the one class to favor the other. Rather, it aims at producing an ensemble which produces high values against both.
 Fig ure 3 : ROC C urve of the Hepatitis Data Set Fig ure 4 : ROC Cur ve of ten iteration of the DataBoost -IM algorithm To better understand the achievements of the DataBoost -IM method, we also present the ROC analysis results of the Hepatitis data set. This data set was chosen since it contains both continuous and discret e features, and has a moderately imbalance degree and instance size. We repeated the previous experiment against the Hepatitis data set, using the optimal parameter values for all algorithms as shown in Table 5. We varied the decision threshold s of the C4. 5 algorithm, by varying the proportion of instances at the leaf node of the decision tree for labeling a class, to obtain the ROC curve. We produced an average ROC curve, as shown in Figure 3, by averaging the TP and FP rates over ten runs [15]. Also, we d rew the ROC curves of the ten iterations of the DataBoost -IM algorithm as shown in Figure 4. From the analysis of the ROC curves as shown in Figure 3, we conclude that the DataBoost -IM ensemble X  X  ROC curve is of a high quality . This result indicates that the DataB oost -IM method achieved a high outcome , which compares well to that of the C4.5 , AdaBoostM1 , DataBoost and SMOTEBoost algorithms over most of the threshold values of the ROC space . Further analysis of Figure 4 shows us an essential fact of the Dat aBoost -IM approach, namely that each component classifier in the DataBoost -IM ensembles was pursuing a point with both a high TP rate and a low FP rate in the ROC space . This implies that the DataBoost -IM was able to produce a series of high quality class ifier s, and each of them will be better able to predict examples for which the previous classifier X  X  performance is poor . This paper introduced a novel approach for learning from imbalanced data sets through combining boosting and data generat ion. In this approach, the class fre quencies and the total weights against different the classes within the ensemble X  X  training set , which consist of both the synthesis and the original training data, are rebalanced during all iterations of the boosting al gorithm. The DataBoost -IM algorithm was illustrated by means of seventeen data sets with various features, degrees of imbalance and sizes. The results obtained indicate that the DataBoost -IM approach performs well against imbalanced data sets . In particula r, the DataBoost -IM algorithm achieved comparable and slightly better predictions, in terms of the G -mean and F -measures metrics , against both the minority and majori ty classes, when compared with a component classifier as well as four other boosting algor ithms. Importantly, our method does not sacrifice one class for the other, but produce high predictive accuracy against both the majority and the minority class. In conclusion, these results indicate four reasons for the performance improve ments achieved by the DataBoost -IM algorithm. The first is that the additional synthetic data provide complementary knowledge for the learning process. The second is that rebalancing the class frequencies alleviates the classifiers X  learning bias toward th e majority class. The third one is that rebalancing the total weight distribution of different classes forces the boosting algorithm to focus on the hard examples as well as rare examples . The last one is that the synthetic data prevent boosting from over -emphasize the hard examples. This property is especially important when considering the minority class which contains few examples. Our future research will address some issues to extend the DataBoost -IM approach , including further investigating the optimal number of new seed examples to generate , e xperim enting with other component classif iers and consider ing the performance against noisy data . Also, other weight -assignment methods will be further investigated. Future work will also include studying the voting mechanism of the boosting algorithm using different metrics such as the ROC curve. Although the DataBoost -IM and the experiments addressed only two -class problems, we believe that a similar approach can be used in the frame of multi -class learni ng problems. This will be further investigated . Thanks are due to R.Holte f or allowing us to use the Oil Spill data set and to N. Chawla for supplying the SMOTE script. We also wish to thank the anonymous reviewers for their comments. This paper extends our earlier work as reported in [24]. [1 ] N. Japkowicz . Learning from imbalanced data sets: A [2] N. Chawla, K. Bowyer, L. Hall and W. Kegelmeyer . [3] M.A. Maloof . Learning when data sets are Imbalanced and [ 4 ] M. Kubat, R. Holte and S. Matwin . Machine Learning for the [ 5 ] M. Kub at and S. Matwin . Addressing the curse of [ 6 ] M. Joshi, V. Kumar and R. Agarwal . Eva luating boosting [7] N. Chawla, A. Lazarevic, L. Hall and K. Bowyer . [ 8 ] Y. Freund and R. Schapire . Experiments with a new [ 9 ] Y. Freund and R.E.Schapire . A decision -theoretic [ 10 ] H.Schwenk and Y. Bengio . AdaBoosting Neural Networks: [ 11 ] T.G. Dietterich . An experimental comparison of three [ 12 ] C.L. Blake and C. J. Merz . UCI Repository of Machine [ 13 ] H Guo and HL Viktor . Boosting with data generation: [ 14 ] C. J. van Rijsbergen . Information Retrieval. Butterworths, [ 15 ] F. Provost, T. Fawcett, and R. Kohavi . The case against [ 16 ] F. Provost and T. Fawcett . Analysis and visualization of [ 17 ] HL Viktor . The CILT multi -agent learning system, South [ 18 ] HL Viktor and I Skrypnik . Improving the Competency of [ 19 ] I. Witten, E.Frank . Data Mining: Practical Machine [ 20 ] JR Quinlan, C4.5 . Programs for Machine Learning, Morgan [21] W. Fan, S. Stolfo, J. Zhang, P. Chan, AdaCost: [22] K. Ting, A Comparative Study of Cost -Sensitive Boosting [ 2 3 ] C. Drummond and R. Holte . C4.5, class imbalance, and [24] H.L. Viktor and H. Guo, Multiple Classifier Prediction 
