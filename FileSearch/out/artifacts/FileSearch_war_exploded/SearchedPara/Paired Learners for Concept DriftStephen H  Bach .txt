
To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas a reactive learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the sta-ble learner performs worse than does the reactive learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the sta-ble learner to predict, since the stable learner performs bet-ter than does the reactive learner when acquiring a target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dy-namic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm ( SEA ) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: m eeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.
Researchers designing algorithms to cope with concept drift have long acknowledged the importance of balanc-ing reactivity and stability. Concept drift refers to an on-line learning task in which the target concept changes over time. Learners for such tasks must be reactive so they adapt quickly to a new target concept, but they must also be sta-ble when acquiring a target concept so as not to be affected by problems typical of online learning, such as ordering ef-fects. A learner that is too reactive may have difficulty ac-quiring any target concept, whereas one that is too stable may be overly burdened by knowledge of a previous con-cept to learn a new one.

Researchers have approached the problem of concept drift in several ways. The systems they have developed have adjusted and decayed weights (e.g., [14]), main-tained and then modified partially learned models (e.g., [6, 17]), maintained previously encountered examples (e.g., [17]), and maintained and consulted multiple models (e.g., [2, 7, 9, 15, 16]). Our work falls into this final category.
Rather than using an ensemble of unweighted or weighted batch learners (e.g., [15, 16]) or one of weighted online learners (e.g., [2, 7, 9]), we paired a stable online learner with a reactive one. The stable learner predicts based on all of its experience. The reactive learner pre-dicts based on its experience over a short, recent window of time. The novel idea is to use the interplay between these two learners and their differences in accuracy to cope with concept drift. The stable learner outperforms the re-active learner when acquiring a target concept, but the re-active learner outperforms the stable learner in the period after the target concept changes. Indeed, when the reactive learner outperforms the stable learner over a short window of time, then the method of paired learning replaces the stable learner X  X  knowledge with that of the reactive learner. The pair then continue to learn.

To evaluate paired learners, we conducted an empirical evaluation using five problems that have appeared previ-ously in the literature: the Stagger concepts [14, 17], the SEA concepts [15], the CAP data set [2, 12], electricity prediction [5], and malware detection [8]. We compared the paired learners to four methods: a single base learner, streaming ensemble algorithm [15], dynamic weighted ma-jority [9], and accuracy wei ghted ensemble [16]. Results suggest that, on the five problems, paired learners outper-formed or performed comparab ly to learners more costly in time and space. In some cases, these methods required be-tween 10 and 50 base learners to obtain high accuracy on the problems considered, but our method used two. Iron-ically, for one problem, we obtained the best performance for two methods when their ensembles had two members.
We find the notion of paired learning appealing because of how directly the method addresses desiderata for learn-ers for concept drift: reactivity and stability. While present in all algorithms X  X n weighting procedures, in maintaining alternative hypotheses, in storing previous examples from the stream X  X aired learning achieves these desiderata ex-plicitly by using a reactive learner and a stable learner. We anticipate that the method X  X  simplicity and the clarity it pro-vides will give researchers additional insight into the prob-lem of learning concepts that change over time. Indeed, since paired learning stands in marked contrast to recently proposed ensemble methods that require upwards of 50 base learners to achieve high accuracy on commonly used data sets, the outcome of this study has implications not only for the design of algorithms for concept drift, but also for the problems used to evaluate such algorithms.
Researchers have introduced a number of ensemble methods for concept drift. We review those algorithms in-cluded in our experimental study, which we selected be-cause, like paired learning, the base learners train on dif-ferent sequences of examples from the stream. The stream-ing ensemble algorithm [15], or SEA , maintains a fixed-capacity, unweighted collection of m batch learners. SEA builds a classifier using a batch of p examples. In addition to the classifiers in the ensemble, it maintains two classi-fiers in reserve, C i , constructed from the current batch, and C i  X  1 , constructed from the previous batch. If space exists in the ensemble, then SEA adds C i  X  1 .Otherwise, SEA re-places a poorer performing classifier in the ensemble with C i  X  1 , as measured on the current batch, provided that such a classifier exists. SEA predicts the majority prediction of the members of the ensemble. Using C 4.5 [13] as the base learner, the authors evaluated SEA on a problem of their de-sign, which we refer to as the  X  SEA concepts. X 
Dynamic weighted majority [9], or DWM , maintains a dynamically sized, weighted collection of online learners. It predicts based on a weighted-majority vote of the learn-ers X  predictions and decreases the weights of those learners that predict incorrectly. If the algorithm X  X  global prediction is incorrect, then it adds a new online learner to the ensem-ble. Also, it removes a learner if its weight falls below a threshold. Consequently, the size of the ensemble m can vary. Its parameter p specifies the period over which it trains the members of the ensemble, but does not update weights or add or remove learners. The authors evaluated DWM with naive Bayes as the base learner on the Stagger concepts [14] and on the SEA concepts [15]. (See Section 4).

Accuracy-weighted ensemble [16], or AW E , maintains a fixed-capacity, weighted collection of m batch learners. AW E builds a classifier from a batch of p examples and com-putes the error rate of the classifier on the examples using cross-validation. It then derives a weight for the classifier using either costs or mean-squared error; our implementa-tion uses the latter. AW E then evaluates each member of the ensemble on the new batch and adjusts its weight. It forms a new ensemble by storing the m highest-weighted classifiers. AW E predicts the weighted-majority prediction of the members of the ensemble. Using credit-card fraud data and a shifting hyperplane of their own design, the au-thors conducted an extensive evaluation that included using AW E with naive Bayes as the base learner.
 Other learners for concept drift include Stagger [14], the FLORA systems [17], winnow [2, 10], weighted majority [2, 11], the concept-drifting very f ast decision tree learner [6], additive experts [7], 1 ultra-fast forest of trees [4], and online rank [1].
A paired learner (Algorithm 1) consists of the stable learner S that predicts based on all of its experience and the reactive learner R w that predicts based on its most re-cent experience over a window of length w .Ituses S to predict and uses R w as a lagging indicator of drift. Input to the algorithm (line 1) is a collection of T training examples, the window X  X  length w , and the threshold  X  for creating a new stable learner. To classify an instance, the algorithm uses S  X  X  prediction as its prediction (lines 9 &amp; 10). Learn-ing entails passing a new example to the learning elements of S and R w (lines 22 &amp; 23). To cope with concept drift, at time t ,if S incorrectly classifies an example and R w rectly classifies it (line 12), then the paired learner sets bit t in the circular list C of w bits (line 13); otherwise, it unsets the bit (line 15). If the proportion of bits set in C surpasses the threshold  X  (line 17), then the paired learner replaces S with a new stable learner (line 18) and sets its initial concept description to that of R w (line 19). It also clears the bits of C (line 20).

A learner for concept drift must incorporate new exam-ples into its model when the target concept is stable and replace an outdated model or portions of the model when the target concept changes. We designed the algorithm for paired learning to detect change by comparing the perfor-mance of a stable learner to that of a reactive one. Since the only difference between the two is that the stable learner has learned from all examples since the last replacement and the reactive learner has learned from the w most recent exam-ples, when the reactive learner outperforms the stable one, it suggests that the examples older than t  X  w are harming the performance of the ensemble.

The method attempts to dete rmine a recent point in the data stream at which the target concept changed so the learner can focus on acquiring a new model with the most recently received examples. To do so, however, we discov-ered through pilot studies that it is not sufficient to consider only the accuracy of S and R w on the last w examples. To illustrate, consider the case in which S correctly classifies the first w 2 examples, misclassifies the next w 2 examples, and R w does the opposite (i.e., classifies the first w 2 incorrectly, and classifies the next w 2 correctly). Both S and R w have an accuracy of 50% over the window, but their comparative performance indicates that the accuracy of S is decreasing and the accuracy of R w is increasing. This suggests that the target concept may have changed, but accuracy gives no information to this effect.

To identify such situations, paired learning measures how frequently S misclassifies an example that R w classi-fies correctly over the last w examples. If this event occurs frequently enough over w , then the method replaces S with R w and learning continues. Th is replacement mechanism is advantageous because it preserves in S what R w has learned from the last w examples, and as a consequence, the method can use S immediately to classify obs ervations. Crucially, in contrast to methods that accumulate batches of examples, paired learning can apply this mechanism at any point in the stream.

Note that one can use any online algorithm as a base learner for paired learning. For this study, we used naive Bayes, which we discuss in the next section.
We implemented a paired learner using an online ver-sion of naive Bayes ( NB ) as the base learner. As its concept description, NB stores distributions for each class, C i ,and for each attribute given the cla ss. For symbolic attributes, it stores frequency counts of each value. For numeric at-tributes, it stores the sum of the values and the sum of the squared values, under an assumption of normality. The learning element estimates these distributions from training data. Given the instance x , the performance element uses the distributions to compute prior and conditional probabil-are conditionally independent, and uses Bayes X  rule to pre-dict the most probable class C :
We refer to this learner as PL -NB , and we implemented two versions using WEKA [18]. Both implementations use as the stable learner the online version of NB , but their re-active learners are different. The first rebuilds the learner X  X  model with the arrival of each new training example from it and last w  X  1 examples. An advantage of this scheme is that Algorithm 1 Paired Learner 1: Input: { x t ,y t } T t =1 , w ,  X  3: w : window size for the reactive learner 4:  X  : threshold for creating a new stable learner 5: Let S be a stable learner 6: Let R w be a w -reactive learner 7: Let C be a circular list of w bits, each initially 0 8: for t  X  1 to T do 9:  X  y S  X  S. Classify ( x t ) 10: output  X  y S 11:  X  y R  X  R w . Classify ( x t ) 12: if  X  y S = y t  X   X  y R = y t then 13: C. set ( t ) 14: else 15: C. unset ( t ) 16: end if 17: if  X &lt;C. proportionOfSetBits () then 18: S  X  new StableLearner () 19: S  X  R w . getConceptDescription () 20: C. unsetAll () 21: end if 22: S. Tr a i n ( x t ,y t ) 23: R w . Tr a i n ( x t ,y t ) 24: end for it is general and works for all learners (batch and online). A disadvantage is that running time increases with w , but this is also true for SEA [15] and AW E [16] and the parameter p .
The second version uses as the reactive learner what we call a retractable learner , for it can unlearn an exam-ple. The retractable l earner also maintains w examples, but when a new example arrives, the learner retracts the last ex-ample in the window from the model, removes the example from the window, and then adds the new example to the model and to the window.

For naive Bayes, these operations are easy and efficient to implement and involves subtracting and adding counts and values from and to the appropriate distributions. An advantage of the retractable l earner is that it is significantly faster than the first version. (We discuss timing and com-plexity issues in Sections 5.1 and 5.2.) The disadvantage is that it is not general, for not all online methods will have a retractable version. Both versions perform identically in terms of accuracy and require the same amount of memory.
To evaluate paired learning for concept drift, we con-ducted a number of experiments involving three other en-semble methods for concept drift and five problems and data sets that have appeared previously in the literature. For the base learner, as mentioned, we implemented an online ver-sion of NB . For ensemble methods, we implemented SEA [15], DWM [9], and AW E [16]. To identify an ensemble method and base learner, we write the method X  X  name fol-lowed by the base learner X  X  name. For example, SEA -NB refers to SEA with NB as the base learner.
 For the purpose of experimentation, the choice of naive Bayes as the base learner is an important one. SEA [15] and AW E [16] use batch methods as their base learners, whereas paired learners and DWM [9] use online methods. When evaluating and comparing ensemble methods, it is impor-tant to control for confounding experimental factors due to the base learners. The advantage of using naive Bayes is that its batch and online versions produce the same model from a given set of training examples irrespective of their order, since the addition operations that update its model are commutative.

In the following sections, we organize discussion around problems and data sets, which we selected because they have been used in previous evaluations of learners for con-cept drift. The malware data set is an exception, but we included it as another example of a real-world data set. The real-world data sets have no ground truth, so we can-not make strong claims about the presence or type of drift, although correlational evide nce suggests that the drops in performance for the CAP data set correspond to semester boundaries [12]. Nonetheless, we concluded that the ben-efit of evaluating the methods on these real-world data sets justified their inclusion.

We present only the best results for each method, and determined the best perform ing parameters using a grid search. We searched over two parameters by defining ranges and increments for each method and its parameters. We ran the methods on all combinations of these param-eters, and measured performance using accuracy or area under the performance curve, as described in the follow-ing sections. To better understand the parameter space, we plotted three-dimensional graphs and used this information to expand the grid over which we searched if it seemed that performance might improve. Naturally, we make no claims that these parameters are op timal, but they are consistent with those published previously. In one instance, we found better parameter settings than did the original authors.
The Stagger concepts [14, 17] consist of three attributes, each taking three values, and thr ee target concepts presented over 120 time steps. The attributes and their values are shape  X  X  triangle, circle, rectangle } , color  X  X  red, green, blue } ,and size  X  X  small, medium, large } .Forthefirst40 time steps, the target concept is color = red  X  size = small . Table 1. Results for the Stagger concepts.

Measures are average normalized area under the curve (AUC) after the first drift point and 95% confidence intervals.
 For the next 40 time steps, the target concept is color = green  X  shape = circle . And for the last 40 time steps, the target concept is size = medium  X  size = large .

While this may seem like a simple problem, we must note that the first target concept is conjunctive, the second is disjunctive, and the third is internally disjunctive (i.e., one attribute takes multiple values). Moreover, the first and sec-ond target concepts share only one positive example, and so it is almost a concept reversal. Crucially, it is not al-ways the size of the feature space or the number of exam-ples that makes a learning problem difficult. A number of researchers have used this problem to evaluate methods of concept drift (e.g., [7, 9, 14, 17]).

At each time step, one presents a single, random exam-ple to the learner and then tests it on a set of 100 random examples. We evaluated each algorithm using this protocol, repeated 50 times. We measured accuracy on the test set and computed average accuracy and 95% confidence inter-vals at each time step.

We also computed the average normalized area under the performance curves ( AUC ) after the first drift point (with 95% confidence intervals). We computed AUC using the trapezoid rule on adjacent pai rs of accuracies and normal-ized by dividing by the total area of the region. The ar-eas under the entirety of the curves were similar, but we chose to compute AUC after the first drift point because most learners perform well on the first concept, and we are most interested in performance after drift occurs. Although AUC is a convenient measure, it may not represent important as-pects of performance, such as slope and asymptote, and so we center discussion on the plots of the performance curves. Figure 1 shows the effect of paired learning on naive Bayes X  performance. We compared PL -NB to NB trained on all examples and on exampl es of each individual concept. These latter two conditions represent the worst-and best-case scenarios, respectively. The areas under these curves appear in Table 1. PL -NB (with an AUC of . 865 ) performed almost as well as NB trained on each individual concept (with an AUC of . 914 ) and performed much better than did NB trained on all examples (with an AUC of . 516 ).
That the paired learner outperformed the base learner trained on all examples is not surprising, but one must keep in mind that, compared to the base learner, which is a sta-ble learner, the paired learne r adds only a reactive learner and basic control policies for replacing the stable learner X  X  knowledge with that of the reactive learner. We found these results encouraging and supportive of our approach. Figure 2 shows the best performances for PL -NB , DWM -NB , SEA -NB ,and AW E -NB . In terms of accuracy, PL performed comparably to DWM -NB , the best performing method overall. AW E -NB and SEA -NB did not perform as well as PL -NB on this problem because they had to wait to accumulate examples into a bat ch before learning. Observe the  X  X tair steps X  in their performance curves. Smaller values for p did not improve performance. Also, notice that SEA NB performed best when its ensemble had two members.
To eva l u a t e SEA , Street and Kim [15] introduced what we call the SEA concepts, which consist of four target con-cepts presented over 50,000 time steps. The target concept changes every 12,500 time steps, and one generates a sin-gle test set for each concept consisting of 2,500 examples. There is 10% class noise for the training examples, and we evaluated the learners every 100 time steps. Each example consists of numeric attributes x i  X  [0 , 10] ,for i =1 ,..., 3 . The target concepts are hyperplanes, such that y =+ if x 1 + x 2  X   X  ,where  X   X  X  7 , 8 , 9 , 9 . 5 } for each of the four target concepts, respectively; otherwise, y =  X  . Thus, x is an irrelevant attribute. We conducted 10 trials and aver-aged accuracy on the test set. We also computed the average normalized area under the curve after the first drift point. We computed 95% confidence intervals for both measures. (Several researchers have used a shifting hyperplane to eval-uate learners for concept drift [6, 7, 9, 15, 16].) Table 2 presents the learners, their parameters, and their AUC s with 95% confidence intervals. In Figure 3, we present results for PL -NB , NB trained on all examples, and NB trained on the examples of each individual concept. PL NB performed as well as did NB trained on each concept. As shown in Figure 4, all of the methods performed well. There is virtually no difference in the performances of PL SEA -NB ,and NB trained on each concept (see Table 2). No-tice, however, that DWM -NB performed well, but required 50 base learners to achieve this performance, whereas PL -NB required only two. Also, SEA -NB with two base learners resulted in notably better performance on this task than that reported by Street and Kim [15]. Indeed, that SEA -NB and AW E -NB performed best with two members in their ensem-bles is supportive of our approach.
Figure 1. Results for PL-NB on the Stagger concepts. Measures are accuracy with 95% confidence intervals. Figure 2. Results for PL-NB, SEA-NB, DWM-NB, and AWE-NB on the Stagger concepts.

Measures are accuracy with 95% confidence intervals.
Figure 3. Results for PL-NB on the SEA con-cepts. Measures are accuracy with 95% con-fidence intervals. Figure 4. Results for PL-NB, SEA-NB, DWM-
NB, and AWE-NB on the SEA concepts. Mea-sures are accuracy with 95% confidence in-tervals. Figure 5. Accuracy on the duration task for
PL-NB, NB, and DWM-NB. Measures are aver-ages of the previous 100 predictions. Figure 6. Accuracy on the duration task for
PL-NB, SEA-NB, and AWE-NB. Measures are averages of the previous 100 predictions.

Table 2. Results for the SEA concepts. Mea-sures are average normalized area under the curve (AUC) after the first drift point and 95% confidence intervals.

For a real-world application, we evaluated the meth-ods on the calendar-apprentice ( CAP ) data set [2, 12], a calendar-scheduling task. Based on a subset of 34 sym-bolic attributes, the task is to predict a user X  X  preference for a meeting X  X  location, duration, start time, and day of week. There are 12 attributes for location, 11 for duration, 15 for start time, and 16 for day of week. We processed online the 1,685 examples for User 1 by testing on each new example, measuring accuracy, and then using it to train each learner. We present the results for the CAP data set in Table 3. Overall, PL -NB performed comparably to DWM -NB and out-performed the other methods. DWM -NB performed slightly better than did PL -NB in terms of accuracy, but required 20 base learners to do so. Not only did DWM require more memory than did PL -NB , but also it required considerably more time, since it trained each of the 20 learners in the ensemble on each new instance.

Figure 5 shows the performance curves on the duration task for PL -NB , NB ,and DWM -NB , and Figure 6 shows these curves for PL -NB , AW E -NB ,and SEA -NB . We suspect that -NB , NB ,and DWM -NB had an advantage on this task since they immediately learne d from each new example, in-stead of accumulating examples into a batch, as did SEA -NB and AW E -NB . We did try smaller ensembles and shorter periods for these learners, but the settings did not produce higher accuracy on this task.
The electricity-prediction data set consists of 45,312 ex-amples collected at 30-minut e intervals between 7 May 1996 and 5 December 1998 [5]. The task is to predict whether the price of electricity will go up or down based on five numeric attributes: the day of the week, the 30-minute period of the day, the demand for electricity in New South
Table 4. Results for the electricity-prediction task. Measures are accuracy averaged over 45,312 predictions.
 Wales, the demand in Victoria, and the amount of electricity to be transferred between the two. Roughly 39% of the ex-amples have unknown values for either demand in Victoria or the transfer amount.

To evaluate the methods, we processed the examples on-line in their temporal order by testing on a new example, measuring accuracy, and then training on the example. We computed average accuracy, which we present in Table 4, and we produced performance curves by plotting the av-erage accuracy of the previous 2,352 predictions. These curves appear in Figure 7.
 As one can see, PL -NB performed almost as well as DWM -NB did on this task. Both achieved overall accura-cies above 80%. Both required reactive parameter settings: w =1 for PL -NB and p =1 for DWM -NB .(For PL -NB ,since w =1 ,  X  was irrelevant.) SEA -NB and AW E -NB , again, may have been disadvantaged on this task be-cause they accumulated examples into a batch for learn-ing. Indeed, if the environment changes quickly enough, it may change before such a learner can produce a new batch for learning. One could reduce the size of the batch, but such a reduction may result in poor performing classifiers. Although DWM -NB performed slightly better than did PL -NB , PL -NB maintained only two learners, whereas DWM -NB maintained five. Finally, notice the large dip in NB  X  X  per-formance around time step 38,000. The methods designed for concept drift were either unaffected (e.g., SEA -NB and AW E -NB ) or affected little (e.g., PL -NB and DWM -NB the phenomenon that produced this drop in NB  X  X  accuracy.
Figure 7. Results for the electricity-prediction task. Measures are accuracy averaged over the previous 2,352 predictions.
The malware-detection task involves determining whether n -grams extracted from executables are from a benign executable or a malicious executable [9]. There are 3,622 examples, and each consists of 500 Boolean attributes indicating the presence or absence of a 4-gram of bytes. We processed the examples online by shuffling the order of the examples, testing on each example, measuring accuracy, and training on the example. We repeated this process 200 times for each of the five methods.

In Table 5, we present average accuracy, and Figure 8 shows performance curves for the five methods. As shown, all of the methods performed comparably, but PL -NB was more efficient in terms of space. Notice that SEA -NB , DWM NB ,and AW E -NB maintained 10 base learners and that SEA NB and AW E -NB each accumulated a batch of 50 examples for learning. In contrast, PL -NB maintained 2 base learners and only 9 examples.

We found it interesting that PL -NB and NB achieved the same accuracy on this problem. Subsequent analysis con-firmed that, over the 200 trials, PL -NB never replaced its stable classifier, and so the two methods produced identical results. While this suggests an absence of drift and sam-pling effects, we consider it a positive outcome that, on such atask, PL -NB performed no worse than did its base learner.
Figure 8. Results for the malware-detection task. Measures are accuracy averaged over the previous 40 predictions.
Figure 9. Results for PL-NB, stable NB, and windowed NB on the Stagger concepts.
Figure 10. Number of replacements for PL-NB on the Stagger concepts over 50 runs.

Table 5. Results for the malware-detection task. Measures are accuracy with 95% con-fidence intervals
The paired learner is one of the most direct and efficient algorithms proposed thus far for coping with concept drift. It consists of two online learners, a stable learner and a re-active learner. The paired learner uses the stable learner X  X  prediction as its global prediction. The algorithm creates a new stable learner when, over a short window of time, the reactive learner is correct and the stable learner in incorrect sufficiently often. The method performed comparably to or better than other ensemble methods designed for concept drift on five different problems, and generally did so in less time and space.

Paired learners performed well on the problems consid-ered because its comb ination of stable and reactive learners is an accurate lagging indicat or of concept drift. To sup-port this assertion, we consider the algorithm X  X  performance on the Stagger concepts, which allows for careful analysis. Figure 9 shows the accuracy over 50 runs of PL -NB ,win-dowed NB ,and NB trained on all examples. Clearly, the standard version of NB trained on all examples cannot cope with changes in the target concept and performs poorly af-ter the first drift point. In contrast, the windowed version of
NB is too reactive and learns a model with only limited accuracy. However, both versions have advantages for dif-ferent parts of the problem. The standard version is better at learning stable (i.e., individual) concepts (see Figure 1), and the reactive version is better at l earning immedi ately after a drift point.

We contend that a reactive learner outperforming a sta-ble learner is rare, unless drift has occurred. Our experi-mental results support this assertion, but to illustrate further, Figure 10 shows the number of times that PL -NB replaced S at each time step on the Stagger concepts. Note that if a replacement occurs at time step t , then the new stable learner S will learn from the examples received from time step t  X  w +1 onward. The optimal replacement times for w =6 on the Stagger concepts are t =46 and t =86 (with no other replacements). As one can see, PL -NB tended to replace S around these optimal times. If the paired learner replaces S near these times, then it should classify subse-quent examples more accurately. Figures 1, 9, and 10 show that PL -NB more closely approached optimal replacement performance on the third target concept than on the second, which resulted in accuracy closer to that of the base learner trained on each concept.
To characterize the relative running times of the meth-ods in our study, we measured combined training and test-ing times on the SEA concepts, the most time-consuming task considered. All problem and learning parameters are the same as those reported in Section 4.2. We performed the evaluations on a workstation with a two-core, 2.8 GH Z Intel Pentium DCPU with 3.5 GB of RAM using Sun X  X  Java Runtime Environment 6 and WEKA version 3.5.7.

For PL -NB , the average running time for five trials was 59.8 seconds. When PL -NB used a retractable learner for R w , rather than retraining, the average running time was 12.3 seconds. The average running time over five trials for AW E -NB was 24.5 seconds, for SEA -NB was 29.5 seconds, and for DWM -NB was 204.9 seconds. These timing results are consistent with those we observed for other problems.
Let f ( n ) be the running time required to train a base learner on n examples, and let g ( n ) be the running time required for that base learner to classify n observations. The time to train a paired learner on n examples is O ( n  X  f ( w )+ n  X  g (1)) ,where w is the size of R w  X  X  window. If the base learner retracts examples, then the time required is O ( n f (1) + n  X  g (1)) . The time to classify n instances for both versions is O ( n  X  g (1)) .

For SEA , the training time is O ( n p  X  f ( p )+ mn p  X  g ( p )) , and the classification time is O ( mn  X  g (1)) ,where m is the number of experts in the ensemble, and p is the num-ber of examples in each batch. For DWM , the training time O ( mn  X  f (1) + mn  X  g (1)) , whereas the classification time is O ( mn  X  g (1)) . Finally, for AW E , training requires quired to train the base learner on n examples using cross-validation. Its classification time is O ( mn  X  g (1)) .
In terms of space, if f ( n ) is the space required for a base learner to store a model built from n examples, then the space required for paired learning is O ( f ( n )+ w ) . DWM requires O ( m  X  f ( n )) ,and SEA and AW E require O ( m  X  f ( p )+ p ) . We should note that the bound for DWM overly pessimistic for base learners with models that grow with n . DWM dynamically adds and removes learners, and so it is unlikely that any base learner will learn from all n examples. However, this bound is accurate for base learners with constant memory, such as naive Bayes.
As we have mentioned, one can use any online learner for paired learning. We have evaluated paired learning with other base learners, such as Hoeffding trees ( HT ), an on-line method for building decision trees [3]. On the Stagger concepts, DWM -HT ( p =1 ,m =10 ) achieved an AUC of . 851 ,but PL -HT had an AUC of . 835 .Onthe CAP data set, DWM -HT ( p =1 ,m =10 ) achieved 50.68%, whereas PL -HT (  X  = . 4 ,w =12 ) obtained 55.83%. On the malware data set, PL -HT (  X  = . 2 ,w =9 ) obtained 90.3%, whereas SEA 48 ( p =50 ,m =10 ) achieved 92.9%, HT alone achieved 92.9%, and AW E -J 48 ( p =50 ,m =10 ) achieved 92.7%. ( 48 is the implementation of C 4.5 [13] in WEKA [18].)
As before, the paired learner achieved higher or compa-rable accuracy, while requiring less time and space than the other methods. We plan to expand our study to include ad-ditional ensemble methods, base learners, and data sets in an effort to better characterize and understand trade-offs in performance between paired learning and other methods for concept drift.
We conducted two additional e xperiments that were in-teresting, but require further study, and we plan to report more detailed conclusions in future publications. Firstly, we were concerned that the paired learner would be more sensitive to noise than would SEA , DWM ,and AW E ,even though we saw no such sensitivity to the 10% noise present in the SEA concepts and in other pr oblems. To investigate, we varied the level of class noise from 0% to 70% in in-crements of 10% for both the Stagger concepts and the SEA concepts, measuring AUC . As expected, the performance of all of the methods decreased with increasing noise, but they degraded at roughly the same rate, suggesting that PL -NB was equally robust to noise.

Secondly, we were interested in the extent to which the relationship between the par ameter settings of the algo-rithms and the drift points of the problems affected accu-racy. In evaluations, critical parameters, such as p or w ,are often factors of the problem X  X  drift points, which could in-fluence a method X  X  overall accuracy. We investigated by se-lecting the Stagger concepts [14] and varying over all com-binations of drift points in { 20 ,..., 60 } for the first drift point and in { 60 ,..., 100 } for the second. (The drift points for the Stagger concepts are 41 and 81.) We ran PL -NB and the other methods, and the accuracy of these methods was generally consistent across all combinations of drift points. The relatively short periods between the drift points may have been insufficient to produce a measurable effect, and we plan to study this issue further with other problems.
We contend that the paired learner is of less concern in this regard since its window slides. Moreover, compo-nents of the paired learner are not  X  X ormant X  as some are in SEA , DWM ,and AW E . For example, DWM does not update weights or add or remove experts during its period, and SEA and AW E accumulate examples during periods to build new classifiers. We intend to develop further this experiment and report its results elsewhere.
We introduced the notion of a paired learner, which uses the difference in performance between a reactive learner and a stable learner to cope with concept drift. It has a minimal ensemble in that it maintains and trains only two learners, in contrast to other ensemble methods that use an unweighted or weighted collec tion of batch learners or a weighted collection of online learners.

We anticipate that the method X  X  directness and the clar-ity it provides will give researchers further insight into the problem of learning concepts that change over time. We hope the outcome of this study has implications not only for the design of algorithms for concept drift, but also for the problems used to evaluate such algorithms. Results on five previously published problems suggest that paired learners perform comparably to or better than DWM , SEA ,and AW E , while requiring less time and space.

Questions remain about ensemble methods for concept drift. We hope to better understand the problems for which large weighted and unweighted ensembles are warranted, although it is not clear if problems or data sets presently ex-ist to support such an investigation. We also plan to study further the properties of reactive learners and their use as indicators of concept drift. We will expand our empirical investigation to include single-model methods for concept drift, such as winnow and Stagger, and other instance gen-erators, such as that used to evaluate AW E . We have already begun to characterize how paired learners and other meth-ods respond to varying drift points and varying amounts of class noise; preliminary results are encouraging.
 Acknowledgments. The authors thank Clay Shields and the anonymous reviewers for helpful comments on earlier drafts of this paper. This research was supported in part by GUROP , the Georgetown Undergraduate Research Opportu-nities Program. The authors are listed in alphabetical order.
