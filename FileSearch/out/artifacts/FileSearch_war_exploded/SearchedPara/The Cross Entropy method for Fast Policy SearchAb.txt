 Shie Mannor shie@mit.edu Reuven Rubinstein ierrr01@ie.technion.ac.il Yohai Gat yohaig@tx.technion.ac.il The Markov decision processes (MDP) model is stan-dard in artificial intelligence, machine learning, oper-ation research and related fields. When the transition probability or the reward in an MDP are unknown, the problem is referred to as a learning one. In Reinforce-ment Learning (RL) an agent learns the behavior of the system through trial-and-error with an unknown dynamic environment (see Kaelbling et al., 1996). For reviews of RL see Sutton and Barto (1998); Bertsekas and Tsitsiklis (1996); Kaelbling et al. (1996). There are several approaches for RL, which can be roughly divided to the following three classes: model-based, model-free, and policy search. In the model-based approach, first a model of the environment is con-structed. The estimated MDP is then solved using standard tools like dynamic programming (see Kearns &amp; Singh, 1998). In the model-free approach one learns a utility function, instead of learning the model. The optimal policy is to choose at each state an action, which maximizes the expected utility. The popular Q-learning (e.g. Watkins, 1989) algorithm is an exam-ple of this approach. In the policy search approach a subspace of the policy space is searched, and the per-formance of policies is evaluated based on their em-pirical performance (e.g. Barto et al., 1983; Sutton &amp; Barto, 1998). Examples to gradient-based policy search methods include the REINFORCE algorithm of Williams (1992), and certain variants of the actor-critic framework (e.g. Konda &amp; Tsitsiklis, 2003). A detailed account of policy gradient can be found in Baxter et al. (2001). For a direct search in the policy space approach see Rosenstein and Barto (2001). The learning algorithms suggested in this paper belong to the policy search approach.
 Many RL algorithms are essentially based on the clas-sic Stochastic Approximation (SA) algorithm. To ex-plain SA, assume that we need to find the unique so-lution v  X  of the nonlinear equation IE S ( v ) = 0, where S ( v ) is a random variable (noisy function) with un-known expectation IE S ( v ). The SA algorithm for esti-mating v  X  is v t +1 = v t +  X  t S ( v t ). The connection be-tween SA and Q-learning is given by Tsitsiklis (1994). This work has made an important impact on the en-tire field of RL. Standard SA is known to converge slowly, because of the requirement that (  X  t  X  0). Even if  X  t remains bounded away from 0, (and thus convergence is not guaranteed) it is still re-quired that  X  t is small in order to ensure convergence to a reasonable solution. For details see Borkar and Meyn (2000).
 The main goal of this paper is to introduce a fast learning algorithm based on the Cross Entropy (CE) method instead of the slow SA algorithms. CE has be-come a standard tool in Monte Carlo estimation and both, combinatorial and continuous multi-extremal optimization, see Rubinstein (1999); de-Boer et al. (2003) for details. As opposed to most RL methods our framework leads to fast convergence and can be easily extended to parameterized policy architecture, as outlined in Section 4. Some experiments in Sec-tion 5 with a maze world and inventory control prob-lems indicate the suggested framework converges in a small number of iterations, with high probability, to a small neighborhood of the optimal solution.
 In this section we present some background on MDPs and the CE method. 2.1. Markov Decision Process (MDP) We review briefly some of the basic definitions and con-cepts in MDP. For details see, e.g., Puterman (1994); Bertsekas (1995). An MDP is defined by a tuple ( M , A , P , r ) where: M = { 1 , . . . , M } is the set of states, which we assume to be finite; A = { 1 , . . . , A } is the set of possible actions of the decision maker, which we assume to be the same for every state to ease nota-tions; P is the transition probability matrix with the elements P ( m 0 | m, a ) presenting the transition proba-bility from state m to state m 0 , when action a is taken; and r ( m, a ) is the reward for performing action a in state m , which we assumed to be bounded by r max . At each time instance t , the decision maker observes the current state, m t , and determines the action to be taken, a t . As a result, reward r ( m t , a t ), denoted by r t , is received and a new state is chosen according to the transition probability P ( m 0 | m t , a t ). A mapping from the state-action-reward histories to the probabil-ity distribution of the decision maker X  X  actions is called a strategy . A strategy is called stationary if it depends only on the current state. The goal of the decision maker is to maximize a certain reward function. The following are standard reward criteria: 1. Finite horizon reward. We will consider the spe-2. Infinite horizon discounted reward. The objective 3. Average reward. The objective is to maximize It is well known (Puterman, 1994) that there exists a deterministic stationary optimal strategy for the above three cases. Note that if the model ( r and P ) is known, then there are several efficient methods for finding the optimal strategy (Puterman, 1994). How-ever, since the model is assumed to be unknown, a learning scheme is required. As mentioned, we shall employ in this situation the CE method instead of SA and shall demonstrate its high performance. 2.2. The Cross Entropy method In this section we review the CE method, which is a state-of-the-art method for solving combinatorial and multi-extremal continuous optimization problems. We refer the reader to de-Boer et al. (2003) and references therein for context, extensions, and applications. The main idea behind the CE method is to transform the original optimization problem to an associated stochas-tic problem (ASP) and then to tackle the ASP effi-ciently by an adaptive algorithm. By doing so one constructs a random sequence of solutions which con-verges probabilistically to the optimal or near-optimal one. As soon as the ASP is defined, the CE method employs the following two phases: 1. Generation of a sample of random data (trajecto-2. Updating the parameters of the random mecha-To proceed, suppose we wish to maximize some per-formance function S ( x ) over all x in some set X . Let us denote the unique maximum by  X   X  , thus As mentioned, we randomize our deterministic prob-lem by defining a family of auxiliary pdfs { f (  X  ; v ) , v  X  V} on X and we associate with Eq. (1) the following estimation problem where u is some known parameter ( X  X  X has a pdf f (  X  ; u )) and  X  an unknown scalar. We consider the event  X  X core is high X  to be the rare event of interest. To estimate this event, the CE method generates a sequence of tuples { ( X   X  t ,  X  v t ) } , which converges quickly (with high probability) to a small neighborhood of the optimal tuple (  X   X  , v  X  ).
 We denote by  X  the fraction, that the best (maximal values, sometimes termed elite) samples, that are used to find the threshold  X  , constitute in the entire sample. Following standard terminology in simulation theory, the process that is based on sampled data is called the stochastic counterpart as it is based on stochastic samples of data. The number of samples in each stage of the stochastic counterpart is denoted by N , which is assumed to be a fixed predefined number. The fol-lowing is a standard CE procedure for a maximization problem borrowed from de-Boer et al. (2003). We ini-tialize by setting  X  v 0 uniform, and choose a not very small  X  , say 10  X  2  X   X  , and then we proceed iteratively as follows: 1. Adaptive updating of  X  t . For a fixed v t  X  1 , let 2. Adaptive updating of v t . For a fixed  X  t and We note that if f belongs to the Natural Exponential Family (NEF; e.g., Gaussian, discrete Bernoulli), then Eq. (3) has a closed form solution (see de-Boer et al., 2003). The CE optimization algorithm is summarized in Algorithm 2.1.
 Remark: Instead of the updating the parameter vector v directly via the solution of Eq. (3) we use the following smoothed version where  X  v t is the parameter vector obtained from the so-lution of (3), and  X  is called the smoothing parameter , with 0 . 7 &lt;  X  &lt; 1. Clearly, for  X  = 1 we have our orig-inal updating rule. The reason for using the smoothed (4) instead of the original updating rule is twofold: (a) to smooth out the values of b v t , (b) to reduce the prob-ability that some component b v t,i of  X  v t will be zeros or unities at an early stage, and a the algorithm will get stuck in a local maxima. Note that for 0 &lt;  X  &lt; 1 we always have that b v t,i &gt; 0, while for  X  = 1 one might have (even at the first iterations) that either b v t,i = 0 or b v t,i = 1 for some indices i . As result, the algorithm may converge to a wrong solution.
 Remark: The performance CE method is insensitive to the exact choice of parameters. As long as  X  is not too small,  X  &lt; 1, and N is large enough, the results Algorithm 2.1 The CE Method for
Stochastic Optimization 1. Choose some b v 0 . Set t = 1 (level counter). 4. Apply (4) to smooth out the vector e v t . 5. If for some t  X  d , say d = 5 , b  X  t = b  X  t  X  1 = of the algorithm are robust. In our numerical studies below we did not tweak the parameters. We chose a typical 0 . 01  X   X   X  0 . 03, and the smoothing parame-ter  X  = 0 . 7. The sample size N was chosen so that  X  =  X N which corresponds to the number of sample performances S ( j ) , j = 1 , . . . , N , that lie in the upper 100  X  % was a few tens. It is the best samples that al-low Algorithm 2.1 to avoid local extrema and to settle down in the global maximum with high probability. Improvements of Algorithm 2.1 include the Fully Adaptive CE (FACE) variant, where the parameters N and  X  are updated online, and some alternatives to the threshold (indicator) sample functions, H , (like Boltz-mann functions). See de-Boer et al. (2003) for more details. For a convergence proof of the CE method see Homem de Mello and Rubinstein (2002). This section deals with the application of the CE method to learning in MDPs. To proceed, we define an auxiliary M  X  A probability matrix P = ( P ma ) with elements P ma , m = 1 , . . . , M ; a = 1 , . . . , A de-noting the probability of taking action a at state m , ( the above two phases (with f (  X  , v ) replaced by P ma ) can be written as: 1. Generation of random trajectories (samples) using 2. Updating of the parameters of the probability ma-The matrix P is typically initialized to a uniform ma-trix ( P ma = 1 /A .) The generation of random trajec-tories for an MDP according to the probability ma-trix P is quite straightforward. We shall show that in calculating the associated sample reward function S , one can take into the Markovian nature of the prob-lem and to speed up the Monte-Carlo process. The following three subsections discuss in more details tra-jectory generation, the sample performance calculation and the stopping criteria for the finite horizon shortest path problem, infinite horizon discounted, and average rewards. 3.1. Trajectory generation for the shortest As mentioned, in shortest path problems there is a terminal state which corresponds to zero reward. With this in mind we may either stop the trajectory when it reaches the terminal state or alternatively we can stop the trajectory if it becomes too long (and discard that trajectory, or penalize states in the trajectory, depending on the application). We will assume that every policy is proper (Bertsekas &amp; Tsitsiklis, 1996) and that the terminal state is always reached. Algorithm 3.1 Trajectory generation for the Shortest Path problem Input : P  X  action probability.

For ( i = 1 to N ): 1. Start from some given initial state m 0 , set t = 0. 2. Repeat until m t = m ter 3. Given a trajectory Output : Score S .
 rameters matrix ( P ma ) using the CE method, namely as per (3). Since for each m the entry P ma presents a discrete pdf, and thus NEF, the update formula (see de-Boer et al., 2003) is: where the event { X ( k )  X  X m } means that the trajec-visit to state m in which action a was taken. We note that the score that is assigned to two states in the same trajectory is correlated, which creates a bias. This bias is inherent to our method, but by us-ing several uncorrelated trajectories, the effect of this biased is reduced.
 We now explain how to take advantage of the Marko-vian nature of the problem. Let us think of a maze where a certain trajectory starts badly, that is the path is not efficient in the beginning, but after some time it starts moving quickly towards the goal. Accord-ing to (5), all the updates are performed in a similar manner in every state in the trajectory. However, the actions taken in the states that were sampled near the target were successful, so one would like to encour-age these actions. The Markov property suggests an efficient way to improve the above algorithm by con-sidering for each state the part of the reward from the visit to that state onwards. We therefore use the same trajectory and simultaneously calculate the score for every state in the trajectory separately. The idea here is that each choice of action in a given state affects the reward from that point on, disregarding the past. The sampling algorithm of Algorithm 3.1 does not change in steps 1 and 2. The difference is in step 3. Given a trajectory X we calculate the reward from every state until termi-nation. For every state in the trajectory the score is S similar to (5), however each state is updated separately according to the reward S m m j onwards.
 A crucial point here is to understand that in contrast to (5) the CE optimization is carried for every state separately and a different threshold parameter  X  m is used for every state m . This facilitates faster conver-gence for  X  X asy X  states where the optimal strategy is easy to find. The above trajectory sampling method can be viewed as a variance reduction method. Nu-merical results indicate that the CE algorithm with updating (6) is much faster then that with updating (5). 3.2. Trajectory generation for the discounted Sampling for the discounted reward criterion is a bit more difficult since there is no obvious reason to as-sume the existence of a terminal state. However, be-cause of the discount factor we can stop sampling when some precision level  X  is guaranteed. Indeed, recall that r max denotes a known upper bound on the im-mediate reward, then for a given discount factor  X  the time horizon which guarantees the accuracy up to  X  tion for discounted reward is similar to Algorithm 3.1, with the exception that now each trajectory is T max long ( T max is an application dependent parameter), as there may not be a natural termination time. Given a trajectory X the reward for states m 0 , . . . , m T tory as Instead of (7) one may calculate the reward based only on the effective horizon T  X  , that is S m P k = j  X  k  X  j r j , and obtain an additional speed up. The update equation remains exactly as in (6). 3.3. Average reward MDP The average reward criterion requires a more elabo-rated scheme for trajectory generations. The main problem is that there is no finite horizon to consider as in the previous cases. One may employ the following two alternatives. The first is to adopt the finite horizon reward Algorithm 3.1 and then calculate the average reward as the cumulative reward divided by the total time. The drawback of this approach is that the re-ward of actions that were performed in the distant past or future affect the score of the whole trajectory. Un-der the assumption that for every strategy there exists at least one recurrent state a second alternative may be suggested based on the concept of regeneration . Ac-cording to this concept, when reaching the recurrent state the process  X  X tarts over X  again. Let m rec  X  M be a recurrent state. According to the regenerative method the average reward can be calculated as the ratio between the expected reward per cycle and the expected cycle time. Note that a cycle is defined as the time between two consecutive visits of the process to the same recurrent state. We describe this idea in Algorithm 3.2.

Algorithm 3.2 Trajectory generation for the av-erage reward MDP
Input : P  X  Action probability; m rec  X  A recurrent state; T max  X  Trajectory length.

For ( i = 1 to N ): 1. Start from some given initial state m 0 , set t = 0. 2. Repeat until t = T max 3. Let  X  0 = 0, and let  X  ` = min { t &gt;  X  `  X  1 , s.t. m
Output : Score vector per state { S m The score of each action is taken as the average reward of the cycle. Note that for every action in a given cycle has the same score. Observe that we can calculate the score only for states from the first regeneration time (  X  1 ) until the last regeneration time (  X  last ) since the cycle reward cannot be estimated without reference to the regeneration time. The update of P remains identical to (6). Until now we assumed that the state space is finite and that sampling was performed according to the matrix P which was assumed to be M  X  A matrix. We now ex-tend our framework to a large state space. We assume that the policy is parameterized by a small number of parameters (as in Konda &amp; Tsitsiklis, 2003). The CE method replaces the traditional gradient-based method and is used for optimizing over the parameter-ized policy. Let us denote the parameter space by  X . Assume that every  X   X   X  induces a strategy  X  ( a | m,  X  ). The strategy  X  ( a | m,  X  ) is defined by the probability of choosing action a when in state m according to the parameter  X  . Instead of looking for the best strat-egy, we look for the best parameter  X   X   X . All three reward criteria that were discussed above are still rele-vant, with appropriate modifications, as the sampling algorithm is virtually the same.
 The CE optimization is performed by assuming that the parameters  X   X   X  are drawn from a distribution f (  X  ; v ) and optimization is performed by solving (3). As before, if f (  X  ; v ) belongs to a NEF, then an analyti-cal solution of Eq. (3) is available. We emphasize that by using the CE method there is no need to assume anything on the parameterization of  X  (  X | m,  X  ). Specif-ically,  X  (  X | m,  X  ) may be non-differentiable with respect to  X  (as opposed to most policy search algorithms). In this subsection we describe experiments with pro-vide two  X  X raditional X  domains. The first domain is a maze world with stochastic transitions and the second is an inventory control problem. 5.1. The Maze Problem Algorithm 3.1 for stochastic shortest path MDPs was tested for a maze problem, which presents a two-dimensional grid world. The agent moves in the grid in four possible directions. The goal of the agent is to move from the upper-left corner (the starting state) to the lower-right corner (the goal). The maze con-tains obstacles ( X  X alls X ) into which movement is not allowed. The reward for every allowed movement until reaching the goal is  X  1. In addition we introduce: 1. A small (failure) probability not to succeed mov-2. A small probability of succeeding moving in the 3. A high cost for trying to move in a forbidden di-We run the algorithm for several mazes and the opti-mal policy was always found.
 In Figure 1 we present the results for 20  X  20 maze. We set the following parameters: N = 1000,  X  = 3%,  X  = 0 . 7 and T = 1500 (maximal path length). The initial policy was a uniformly random one. The cost of moves were random variables uniformly distributed between 0 . 5 and 1 . 5 and between 25 and 75 (expected cost are equal to 1 and 50) for the allowed and forbid-den moves, respectively. The success probabilities in the allowed and forbidden states were taken 0.95 and 0.05, respectively. In Figure 1 we plot the possible ( P ma &gt; 0 . 01) trajectories of the algorithm in the end of each iteration. The convergence is quite fast and very accurate. In all our experiments CE found the target exactly, within 5-10 iterations and CPU time was less than one minute (on a 500MHz Pentium pro-cessor). Note the successive refinement of the policy in Figure 1, as the P ma of suboptimal entries was re-duced quickly to 0. 5.2. The Inventory Control Problem The inventory control (IC) is a well studied problem in the operation research community. The decision maker in this problem can be viewed as a shop owner, who buys and sells certain commodities (say, k commodi-ties). At each stage (day) t he faces the dilemma of how much stock to order from each type of commodity, provided that the customers X  demand of the i th com-modity d t ( i ) is a random variable with an unknown distribution. The decision maker X  X  expenses consist of the purchase price, a price paid for holding surplus stock, and penalty for back-logged demand. In MDP terms, the state space is the amount of available stock of each commodity ( M = IR k for continuous stock or negative stock representing unsatisfied demand. De-note by m t ( i ) the inventory of commodity i at the be-ginning of period t , by a t ( i ) the ordered stock at that period and by d t ( i ) the demand of it, then m t +1 ( i ), which is the inventory of commodity i at the begin-ning of period t + 1, is given by: There are many possible cost functions of interest. We shall assume the following linear cost function: where for the i th commodity h ( i ) is the holding cost, b ( i ) is the back-logged demand cost, and c ( i ) is the price of one unit stock. We let D max denote the max-imal number of items that can be held in stock. Ob-viously, the state and action spaces are huge even for relatively small k . For a single commodity problem and the average reward criterion, if the demand d t is IID, then there exists (Bertsekas, 1995) an optimal stationary threshold policy a  X  ( m ) such that a  X  ( m ) = max { M max  X  m, 0 } for some M max , which depends on the problem parameters. Note that in contrast to SA (see, e.g. Konda &amp; Tsitsiklis, 2003) one does not have to make any smoothness assumptions for applying CE policy search. In the case of multiple commodities, an educated guess would be to use a threshold policy for every commodity separately (but this may lead to a suboptimal solution.) We have tested the CE policy search algorithm for parameterized policies on several IC problems while minimizing the average cost. As regeneration points we took the zero state (zero stock value). We look for a threshold policy, and all we need to do is to find the optimal threshold (or thresholds if there are multi-ple commodities.) Each observation cost was averaged over ` =10 regenerative cycles to reduce variance. Ob-viously, since the threshold is non-negative and since it can not exceed D max , we can sample it from a Beta distribution, multiply it by the maximal demand and round it to the closest integer number. At the first it-eration the threshold was generated from a Beta(1,1) distribution, i.e. from a uniform distribution. At each iteration we updated the parameters in the Beta dis-tribution according to Algorithm 2.1. The update of the parameters is somewhat more complex in this case as the distribution is not NEF. However, the update equation can be efficiently solved numerically. We started with running an IC problem with a sin-gle commodity. We let c = 10, h = 5, b = 7, and D max = 100. The demand distribution for was a fixed demand function chosen randomly (in the initial phase we sampled the demand for every d from a uniform dis-tribution, and normalized.) We run the CE method with N = 100 and  X  = 1%. The number of iterations was 5-11, and the execution time 1-4 seconds. Fig-ure 2 shows the calculated average cost curves for and the range of thresholds that the algorithm converged to. The CE policy search algorithm was also tested for a multi-commodity IC problem. In this problem there are seven commodities with different costs pa-Table 1. The total storage space was D max = 30. The multi-commodity IC problem is in general a diffi-cult problem, but a reasonable (sub-optimal) heuristic is to use a threshold policy for each commodity. We run the policy search 10 times using threshold policies. The relative error (comparing to the optimal thresh-old policy, obtained using a time consuming heuristic branch and bound search) was 0.1% (  X  0 . 05%) with 20 iterations (  X  5) and average execution time of 650 seconds. The advantage of the CE method is that by understanding the problem (and the structure of the solution, in this case), a robust and efficient method for learning a nearly optimal strategy can be easily derived.
 We presented a framework for policy search when the environment is unknown. A significant advantage of the CE method which was exploited in the IC problem is that knowledge of the structure of  X  X ood X  policies can be easily exploited to facilitate an efficient search. Another advantage of the CE method is the speed of convergence and the small number of parameters that need to be tweaked for guaranteeing convergence. There are plenty of off-the-shelf optimization algo-rithms that may be considered for policy search. The advantage of using the CE method is that there is no need to estimate gradients as required by many algo-rithms (e.g. steepest or conjugate gradient). Algo-rithms that are based on proximity relation (such as simulated annealing or guided local search) are also sensitive to the sampling error. Since gradients are not used when using the CE method, the CE method is expected to be more robust than other methods. Future research includes: theoretical study of conver-gence; extension of the CE framework to a hierarchical framework; incorporating exploration mechanisms, as currently exploration is based on the initial random policy; and experimentation the CE method for multi-agent problems.
 Acknowledgements. We are grateful for three anonymous reviewers for significantly improving the presentation of this work. S.M. was partially sup-ported by the ARO under grant DAAD10-00-1-0466.
