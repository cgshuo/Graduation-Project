 Parsing is one of the major tasks which helps in understanding the natural language. It is useful in se v eral natural language applications. Machine translation, anaphora resolution, word sense di s-ambiguation, ques tion answering, su m marization are few of them. This led to the d e velopment of grammar -driven, data -driven and hybrid parsers. Due to the availability of ann o tated corpora in recent years, data driven parsing has achieved considerable success. The availabi l ity of phrase structure treebank for English (Marcus et al., 1993) has seen the development of many eff i-cient parsers. Using the depe n dency analysis, a similar large scale annotation effort for Czech, has been the Prague Dependency Treebank (H a-jicova, 19 98). Unlike English, Czech is a free -word -order language and is also morph o logically very rich. It has been suggested that free -word -order languages can be handled better using the dependency based framework than the constit u-ency based one (Hudson, 1984; S hieber, 1985; Mel X  X  X k, 1988, Bharati et al., 1995). The basic difference between a constituent based represe n-tation and a dependency repr e sentation is the lack of nonterminal nodes in the latter. It has also been noted that use of appropr i ate edge labels g ives a level of semantics. It is perhaps due to these reasons that the recent past has seen a surge in the development of dependency based tre e-banks.

Due to the availability of dependency tre e-banks, there are several recent attempts at buil d-ing dependency parsers. Two CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) were held aiming at building state -of -the -art dependency parsers for different languages. Recently in NLP Tools Contest in ICON -2009 (Husain, 2009 and references therein), rule -based, constraint based, statistical and hybrid approaches were explored towards building d e-pendency parsers for three Indian languages namely, Telugu, Hindi and Bangla. In all these efforts, state -of -the -art accuracies are obtained by two data -driven par sers, namely, Malt (Nivre et al., 2007b) and MST (McDonald et al., 2006). The major limitation of both these parsers is that they won't take linguistic constraints into account explicitly. But, in real -world applic a tions of the parsers, some basic linguist ic constraints are very useful. If we can make these parsers handle li n-guistic constraints also, then they become very useful in real -world applic a tions.

This paper is an effort towards incorporating linguistic constraints in statistical dependency parser . We consider a simple constraint that a verb should not have multiple subjects/objects as its children. In section 2, we take machine tran s-lation using dependency parser as an example and explain the need of this linguistic constraint. In section 3, we pr opose two approaches to ha n-dle this case. We evaluate our approaches on the state -of -the -art dependency parsers for Hindi and Czech and analyze the results in section 4. Ge n-eral discussion and future directions of the work are presented in section 5. We co nclude our p a-per in section 6. In this section we take Machine Translation (MT) systems that use dependency parser output as an example and explain the need of linguistic constraints. We take a simple constraint that a verb should not have multi ple subjects/objects as its children in the dependency tree. Indian La n-guage to Indian Language Machine Transtion System 1 is one such MT system which uses d e-pendency parser output. In this system the gene r-al framework has three major components. a) depende ncy analysis of the source sentence. b) transfer from source dependency tree to target dependency tree, and c) sentence generation from the target dependency tree. In the transfer part several rules are framed based on the source language dependency tree. For instance, for T e-lugu to Hindi MT system, based on the depe n-dency labels of the Telugu sentence post -positions markers that need to be added to the words are decided. Consider the following e x-ample, (1) Telugu: raamu oka pamdu tinnaadu Hindi: raamu ne eka phala khaayaa English:  X  X amu ate a fruit X .

In the above Telugu sentence,  X  raamu  X  is the subject of the ve rb  X  tinnaadu  X . While translating this sentence to Hindi, the post -position marker  X  ne  X  is added to the subject. If the dependency parser marks two subjects, both the words will have  X  ne  X  marker. This affects the comprehens i-bility. If we can avoid such inst ances, then the output of the MT system will be improved.

This problem is not due to morphological richness or free -word -order nature of the target language. Consider an example of free -word -order language to fixed -word -order language MT system like Hindi to English MT system. The dependency labels help in identifying the pos i-tion of the word in the target sentence. Consider the example sentences given below. (2a) raama seba khaatha hai (2b) seba raama khaatha hai 
Though the source sentence is different, the target sentence is same. Even though the source se n tences are different, the dependency tree is same for both the sentences. In both the cases,  X  raama X  is the subject and  X  seba  X  is the object of the verb  X  khaatha  X . This information helps in getting the correct translation. If the parser for the source sentence assigns the label  X  X ubject X  to both  X  raama X  and  X  seba  X , the MT system can not give the correct output.

There were some attempts at handling these kind of linguistic constraints using integer pr o-gramming approaches (Riedel et al., 2006; Bh a-rati et al., 2008). In these approaches dependency parsin g is formulated as solving an integer pr o-gram as McD o nald et al. ( 2006 ) has formulated depe n dency parsing as MST problem . All the linguistic constraints are encoded as constraints while sol v ing the integer program. In other words, all the parses that violate these constraints are removed from the so lution list. The parse with satisfies all the constrain t s is considered as the dependency tree for the sentence. In the fo l-lo w i ng se c tion, we d e scribe two new a p proaches to avoid mu l tiple su b jects/objects for a verb. In this section, we describe the two different a p-proaches for avoiding the cases of a verb having multiple subjects/objects as its children in the d e penden cy tree. 3.1 Naive Approach (NA) In this approach we first run a parser on the input sentence. Instead of first best dependency label, we extract the k -best labels for each token in the sentence. For each verb in the sentence, we check if there are multiple ch ildren with the d e-pendency label  X  X ubject X . If there are any such cases, we extract the list of all the children with label  X  X ubject X . we find the node in this list which appears left most in the sentence with respect to other nodes. We assign  X  X ubject X  to this node. For the rest of the nodes in this list we assign the second best label and remove the first best label from their respective k -best list of labels. We check recursively, till all such instances are avoided. We repeat the same procedure for  X  X  b-j ect X .

Main c riterion to avoid multiple su b-jects/objects in this approach is position of the node in the sentence. Consider the following e x-ample, 
Eg. 3: raama seba khaatha hai 
Suppose the parser assigns the label  X  X ubject X  to both the nouns,  X  raama  X  and  X  seba  X . Then naive approach assigns the label subject to  X  ra a-ma  X  and second best label to  X  seba  X  as  X  ra a ma  X  precedes  X  s e ba  X . 
In this manner we can avoid a verb having m ultiple children with dependency labels su b-ject/object. 
Limitation to this approach is word -order. The algorithm described here works well for fixed word order languages. For example, consider a language with fixed word order like English. English is a SV O (Subject, Verb, Object) la n-guage. Subject always occurs before the object. So, if a verb has multiple subjects, based on pos i-tion we can say that the node that occurs first will be the subject. But if we consider a free -word order language like Hindi, th is approach wouldn't work always. 
Consider (2a) and (2b). In both these exa m-ples,  X  raama  X  is the subject of the verb  X  khaatha  X  and  X  seba  X  is the object of the verb  X  khaatha  X . The only difference in these two sentences is the order of the word. In (2a), su bject precedes o b-ject. Whereas in (2b), object precedes subject. Suppose the parser identifies both  X  raama  X  and  X  seba  X  as subjects. NA can correctly identify  X  raama  X  as the subject in case of (2a). But in case of (2b),  X  seba  X  is identified as the subject. To handle these kind of instances, we use a prob a-bi l istic approach. 3.2 Probabilistic Approach (PA) The probabilistic approach is similar to naive approach except that the main criteri on to avoid multiple subjects/objects in this approach is probability of the node having a particular label. Whereas in naive approach, position of the node is the main criterion to avoid multiple su b-jects/objects. In this approach, for each node in the sentence, we extract the k -best labels along with their probabilities. Similar to NA, we first check for each verb if there are multiple children with the dependency label  X  X ubject X . If there are any such cases, we extract the list of all the children with label  X  X ubject X . We find the node in this list which has the highest probabil ity value. We assign  X  X ubject X  to this node. For the rest of the nodes in this list we assign the second best label and remove the first best label from their respective k -best list of labels. We check recu r-sively, till all such instances are avoided. We r epeat the same pr o cedure for  X  X bject X .

Consider (2a) and (2b). Suppose the parser identifies both  X  raama  X  and  X  seba  X  as subjects. Probabil i ty of  X  raama  X  being a subject will be more than  X  seba  X  being a subject. So, the prob a-bilistic approach correctly mark s  X  raama  X  as su b-ject in both (2a) and (2b). But, NA couldn't ide n-tify  X  raama  X  as subject in (2b). We evaluate our approaches on the state -of -the -art parsers for two languages namely, Hindi and Czech. First we calculate the instances of mu l-tiple subjects/objects in the output of the state -of -the -art parsers for these two languages. Then we apply our approaches and analyze the results. 4.1 Hindi Recently in NLP Tools Contest in ICON -2009 (Husain, 2009 and references herein), rule -based, constraint bas ed, statistical and hybrid approac h-es were explored for parsing Hindi. All these attempts were at finding the inter -chunk depe n-dency relations, given gold -standard POS and chunk tags. The state -of -the -art accuracy of 74.48% LAS (Labeled Attachment Score) i s achieved by Ambati et al. (2009) for Hindi. They used two well -known data -driven parsers, Malt 2 (Nivre et al., 2007b), and MST 3 (McD o-nald et al., 2006) for their experiments. As the accuracy of the labeler of MST parser is very low, they used maximum en tropy classification algorithm, MAXENT 4 for labeling.

For Hindi, dependency annotation is done u s-ing paninian framework (Begum et al., 2008; Bharati et al., 1995). So, in Hindi, the equivalent labels for subject and object are  X  karta (k1)  X  and  X  karma (k2)  X  .  X  karta  X  and  X  karma  X  are syntactico -semantic labels which have some properties of both grammatical roles and thematic roles. k1 behaves similar to subject and agent. k2 behaves similar to object and p a tient (Bharati et al., 1995; Bharati et al., 2009). He re, by object we mean only direct object. Thus we consider only k1 and k2 labels which are equivalent of subject and d i-rect object. Annotation scheme is such that there wouldn X  X  be multiple subjects/objects for a verb in any case (Bharati et al., 2009). Fo r example, even in case of coordination, coordinating co n-junction is the head and conjuncts are children of the coordinating conjunction. The coordinating conjunction is attached to the verb with k1/k2 label and the conjuncts get attached to the coo r-dinati ng conjunction with a dependency label  X  ccof  X . 
We replicated the experiments of Ambati et al. (2009) on test set (150 sentences) of Hindi and an a lyzed the outputs of Malt and MST+MaxEnt. We consider this as the baseline. In the output of Malt, there are 3 9 instances of multiple su b-jects/objects. There are 51 such instances in the output of MST+MAXENT.
 Malt is good at short distance labeling and MST is good at long distance labeling (McD o-nald and Nivre, 2007). As  X  k1  X  and  X  k2  X  are short distance labels, Mal t could able predict these l a-bels more a c curately than MST. Because of this output of MST has higher number of instances of multiple su b jects/objects than Malt.

Both the parsers output first best label for each node in the sentence. In case of Malt, we mo d-ified the implementation to extract all the poss i-ble dependency lab els with their scores. As Malt uses libsvm for learning, we couldn't able to get the probabilities. Though interpreting the scores provided by libsvm as probabilities is not the correct way, that is the only option currently available with Malt. In case of MST+MAXENT, labeling is performed by MAXENT. We used a java version of MAXENT 5 to extract all possible tags with their scores. We applied both the naive and probabilistic approaches to avoid multiple subjects/objects. We evaluated our experiments based o n unlabeled attachment score (UAS), l a-beled attachment score (LAS) and labeled score (LS) (Nivre et al., 2007a). Results are pr e sented in Table 2.
 As expected, PA performs better than NA. With PA we got an improvement of 0.26% in LAS over the previous best results for Malt. In case of MST+MAXENT we got an improvement of 0.61% in LAS over the previous best results. Note that in case of MST+MAXENT, the slight difference between state -of -the -art results of Ambati et al. (2009) and our baseline accuracy is due different MA X ENT package used. 
Improvement in case of MST+MAXENT is greater than that of Malt. One reason is because of more number of instances of multiple su b-jects/objects in case of MST+MAXENT. Other reason is use of probabilities in case MST+ MAXENT. Whereas in case of Malt, we interpreted the scores as prob a bilities which is not a good way to do. But, in case of Malt, that is the only option available. 4.2 Czech In case of Czech, we replicated the experiments of Hall et al. (2007) using latest ver sion of Malt (version 1.3.1) and analyzed the output. We co n-sider this as the baseline. The minor variation of the bas e line results from the results of CoNLL -2007 shared task is due to different version Malt parser being used. Due to practical reasons we c ouldn't use the older version. In the output of Malt, there are 39 instances of multiple su b-jects/objects out of 286 sentences in the testing data. In case of Czech, the equivalent labels for subject and object are  X  X gent X  and  X  X heme X .
 Czech is a free -word -order language similar to Hindi. So as expected, PA performed better than NA. Interestingly, accuracy of PA is lower than the baseline. Main reason for this is scores of libsvm of Malt. We explain the reason for this using the following example, consider a verb  X  X  X  has two children  X  X 1 X  and  X  X 2 X  with dependency label subject. Assume that the label for  X  X 1 X  is subject and the label of  X  X 2 X  is object in the gold -data. As the parser marked  X  X 1 X  with subject, this adds to the accuracy of the parser. While avoi d-ing multiple subjects, if  X  X 1 X  is marked as su b-ject, then the accur a cy doesn't drop. If  X  X 2 X  is marked as object then the accuracy increases. But, if  X  X 2 X  is marked as subject and  X  X 1 X  is marked as object then the accuracy drops. This could happen if prob ability of  X  X 1 X  having su b-ject as label is lower than  X  X 1 X  having subject as the label. This is because of two reasons, (a) parser itself wrongly predicted the probabilities, and (b) parser predicted correctly, but due to the limitation of libsvm, we could n't get the scores correctly. Results show th at the probabilistic approach pe r-forms consistently better than the naive a p-proach. For Hindi, we could able to achieve an improvement 0.26% and 0.61% in LAS over the previous best results using Malt and MST r e-spe c tively. We couldn X  X  able to achieve any i m-provement in case of Czech due to the limitation of libsvm learner used in Malt.

W e plan to evaluate our approaches on all the data -sets of CoNLL -X and CoNLL -2007 shared tasks using Malt. Settings of MST parser are available only for CoNLL -X shared task da ta sets. So, we plan to evaluate our approaches on CoNLL -X shared task data using MST also. Malt has the limitation for extracting probabilities due to libsvm learner. Latest version of Malt (version 1.3.1) provides option for liblinear learner also. Libli near provides option for extracting probabi l-ities. So we can also use liblinear learning alg o-rithm for Malt and explore the usefulness of our approaches. Currently, we are handling only two labels, subject and object. Apart from subject and object there ca n be other labels for which mu l-tiple instances for a single verb is not valid. We can extend our approaches to handle such labels also. We tried to incorporate one simple lingui s-tic constraint in the statistical dependency par s-ers. We can also explore the ways of incorpora t-ing other useful linguistic constraints. Statistical systems with high accuracy are very useful in practical applications. If these systems can capture basic linguistic information, then the usefulness of the statistical system improves a lot. In this paper, we presented a new method of incorporating linguistic constraints into the st a-ti s tical depende n cy parsers. We took a simple constraint that a verb should not have multiple subjects/objects as its children. We proposed two a pproaches, one based on position and the other based on probabilities to handle this. We ev a-luated our approaches on state -of -the -art depe n-dency parsers for Hindi and Czech.
 Acknowledgments I would like to express my gratitude to Prof. Jo a-kim Nivre and Prof. Rajeev Sa n gal for their gui d ance and support. I would also like to thank Mr. Samar Husain for his valu a ble sugge s tions.
