 We describe methods to search with a query by example in a known domain for information in an unknown domain by exploiting Web search engines. Relational search is an effective way to obtain in-formation in an unknown field for users. For example, if an Apple user searches for Microsoft products, similar Apple products are important clues for the search. Even if the user does not know keywords to search for specific Microsoft products, the relational search returns a product name by querying simply an example of Apple products. More specifically, given a tuple containing three terms, such as ( Apple , iPod , Microsoft ), the term Zune can be ex-tracted from the Web search results, where Apple is to iPod what Microsoft is to Zune . As a previously proposed relational search requires a huge text corpus to be downloaded from the Web, the results are not up-to-date and the corpus has a high construction cost. We introduce methods for relational search by using Web search indices. We consider methods based on term co-occurrence, on lexico-syntactic patterns, and on combinations of the two ap-proaches. Our experimental results showed that the combination methods got the highest precision, and clarified the characteristics of the methods.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Algorithms, Experimentation Web information extraction, Web mining, Relational search
Advances in Internet search engines have made it easy to search for documents despite the continuing growth of the World Wide Web. However, there is a fundame ntal problem: inputting queries for information that users do not know well, since they may not know the appropriate terms to use to obtain the desired information, is difficult. For example, if an Apple user searches for Microsoft products, the user may not know the product names or keywords to describe the desired products.

In that situation, Apple products well known by the user are im-portant clues for searching for Microsoft products. The user would probably have some knowledge of Apple products, how they work, their functions, and in what situations they are used. We can sup-pose the presence of information in an unknown domain that is similar to well-known information in a known domain. An analogy enables us to predict the unknown information mainly on the basis of relational similarity between the unknown information and the known information. For example, most Apple users would know iPod , a music player sold by Apple . If they want to find an alterna-tive by Microsoft , they can find an analogy between iPod and a mu-sic player sold by Microsoft . The ability to infe r unknown informa-tion by replacing it with known information is naturally equipped for humans. We believe that analogy makes it possible to search for unknown information even without knowledge about that infor-mation or keywords related to it; simply giving an example in a well-known domain will suffice.

We propose methods to search with a query by analogical ex-ample in a known domain for an unknown domain. Given a tuple containing three terms, e.g., ( Apple , iPod , Microsoft ), Zune can be extracted from the Web, where Apple is to iPod what Microsoft is to Zune . There are similar relations between Apple -iPod and Microsoft -Zune ; based on the relational similarity, our methods find an expected entity name. As a generalization of the query by ana-logical example, a query is a tuple containing three terms: a , b , and c . a is a well-known domain, b is an element of a or has a special relation with a ,and c is an unknown domain. Our meth-ods search for d such that the relation between c and d is closest to that between a and b . This search based on relational similar-ity is called relational search in this work. The relational search helps users search for information in an unknown domain without a descriptive phrase; they simply use a known example.

We used Web search engine indices to extract a target entity name. A previously proposed relational search [8] requires a huge corpus crawled from the Web. The crawled corpus is processable but has a high construction cost, whereas Web search engine in-dices are easy to access and always up-to-date. We can quickly obtain a text corpus by using search results without downloading Web pages themselves. Our methods require 20 Web accesses at least to a Web search engine, they can process queries fast enough for practical use by parallel processing.

We considered methods based on term co-occurrence and on lexico-syntactic patterns and also on combinations of the two ap-proaches. The methods were evaluated with a set of 300 tests, which consisted of tuples containing input terms a , b ,and c ,and an expected term d ,suchas Apple , iPod , Microsoft ,and Zune .Af-ter analysis of the results with metrics such as the mean reciprocal rank (MRR), the experimental result clarified the characteristic of the methods based on term co-occurrence and lexico-syntactic pat-terns and indicated that combinations of the two approaches were the most effective for relational search.

The rest of this paper is organized as follows. Section 2 sum-marizes related work on Web information extraction and relations between terms. Section 3 introduces our methods for searching with a query by analogical example, i.e., relational search. Section 4 presents our evaluation of the methods using a large test set and discusses our analysis of the results in detail. Section 5 presents the conclusion.
Brin [7] extracted many author-title relationships from the Web by the bootstrap method. Given exa mples of author and title pairs, the method finds a phrase consisting of  X  prefix , author, middle , ti-tle, suffix  X  from Web documents, and extracts author-title pairs by matching the phrases, supposing that the same syntactic pattern means the same relation. Snowball [1], a system using a method similar to Brin X  X , also extracts specific relationships from the Web. It weights each pattern and improves the performance of the ex-traction. Snowball was improved by Zhu et al. for better precision and recall, and the resulting system was named StatSnowball [23]. It uses Markov logic networks, which subsume logistic regression and conditional random fields, weights each pattern by maximum likelihood estimation, and enhances the performance of Snowball.
KnowItAll [10, 17] is a system for finding entity names in the same class as a given example by using several syntactic patterns, such as  X  X nd other X  and  X  X uch as X . It learns an effective pattern to extract relevant entity names from many relevant and irrelevant terms for expected entity names.
 The concept of relational search has already been described by Cafarella et al. [8]. They crawled 90 million of Web pages and constructed an extraction graph that textually represented an entity-relationship graph, which was automatically extracted from the Web pages. On the basis of the entity-relationship graph, their relational search facilitates answering several queries, such as qualified-list queries (e.g., west coast liberal arts college) and relationship queries (e.g., the relationship between Bill Clinton and Justice Ginsberg). Although Cafarella et al. X  X  search concept based on relation is broader than ours, the problem we deal with is different from theirs. More-over, the relational search they described requires a huge text cor-pus to be downloaded from the Web. Thus, it is difficult to construct the system and to keep it up-to-date.
Turney et al. [22] have proposed methods that measure the sim-ilarity of relations. They aimed to solve verbal analogy questions, where a word pair, A and B , is provided, and the problem is to select the most feasible pair, C and D , from a set of five choices, where  X  A is to B as C is to D , X  e.g.,  X  mason is to stone as carpenter is to wood  X . The similarity of relations is defined as Sim( where R 1 is the relation between terms A and B ,and R 2 is the rela-tion between terms C and D . The best performing method for cal-culating Sim( R 1 ,R 2 ) for given pairs ( A , B )and( C , D )wasbased on the vector space model. In this method, a vector is created for apair, X and Y , whose elements correspond to the frequencies of documents containing prepared lexicon patterns, such as  X  X of Y  X ,  X  Y to X  X , and  X  X for Y  X . The similarity between the relations R and R 2 is calculated on the basis of the cosine similarity between vectors for pairs ( A , B )and( C , D ). Turney [19, 21] improved this method by expanding it with latent relational analysis and achieved 56.4% precision. Another study [20] proposed retrieving lexicon patterns in the context where A and B occur, representing their re-lation. Bollegala et al. [4] also tackled the verbal analogy problem. They retrieved lexicon patterns between terms X and Y from the Web, trained a two-class support vector machine (SVM) to learn the contributions of various lexical patterns towards the relational similarity between the pair, and sped up the relation similarity cal-culation. Bollegala et al. also proposed another method to mea-sure relational similarity by clustering lexical patterns between two words, X and Y , and calculating the similarity based on a metric learning approach [5, 6].
Church et al. [9] measured relatedness between two words with mutual information. Turney [18] and Baroni et al. [2] proposed methods that calculate the level of synonyms for two words by using the number of Web documents returned by search engines. Their methods use the co-occurrences of words and mutual infor-mation. Bollegala et al. [3] computed semantic similarity by using automatically extracted lexical patterns from text snippets of Web search results and integrated these different similarity scores by us-ing SVMs for a robust semantic similarity measure.

Terms with specific relations can also be retrieved through the-sauruses. WordNet [14] is a well-known thesaurus created manu-ally. However, a thesaurus should be created automatically because a manually created one does not completely support proper nouns and new words.

There have been many studies on automatically finding words having a specific relationship in large text corpora and data sets. Hearst et al. [11] proposed a method of finding hypernyms and hy-ponyms from large text corpora focusing on a lexical pattern like such as . Lin et al. [13] proposed a method of finding synonyms from dependency-parsed large text corpora by measuring the simi-larity of words on the basis of modification relations.
Other studies on finding words in a particular relationship have been conducted. Oyama et al. [16] retrieved from the Web pairs of words in which one word describes the other, which can also be taken as a part-of relationship. Hokama et al. [12] extracted from the Web mnemonic names of people. They retrieved candidates for mnemonic names by using lexical patterns occurring before the names of people.

Various kinds of methods to extract terms in a specific relation have been proposed. The relational search we propose can inclu-sively retrieve terms in any relation by input terms. Our method generalizes the problems of finding many kinds of terms for a given term.
Given a tuple containing a , b ,and c as a query, a relational search finds d , where the relation between a and b , i.e., Relation( nearly equal to Relation( c, d ) . For example, given input terms a Apple , b = iPod ,and c = Microsoft , the search returns Zune as out-put d , where the relations Relation( Apple , iPod ) and Relation( Zune ) are nearly equal. This relational search is defined in accor-dance with the characterization of information retrieval where Q is a set of queries composed of three data inputs, a , b ,and c ,and d i is an element in target data D . Although we assume in this work that a , b , c ,and D are simply terms or entity names, they can be documents, images, and so on. Rank( q, d i ) is a ranking function such that Q  X  D  X  R , and it orders elements of data for the given query q .

We suppose that there are many relations between two terms or entities, and Relation( x, y ) is a set of relations that x and y satisfy. where R is a set of all binary relations. For example, between Apple and iPod , there are relations such as iPod being a product of Apple and Apple inventing iPod . Thus, multiple relations should be considered in measuring similarity of relation between two entities.
For input terms a , b ,and c , two phases are conducted to ex-tract a target term d ,where Relation( a, b ) is the closest relation to Relation( c, d ) .

The first phase extracts additional terms or lexico-syntactic pat-terns to determine a method to find d by using inputs a and b .For example, given a = Apple , b = iPod , and input c = Microsoft , only a query using the term Microsoft cannot identify an expected term d = Zune . Additional terms such as  X  X usic player X  or lexico-syntactic patterns such as  X  X s a music player sold by X  should be given for identifying the term d . In addition, a way to find an ex-pected term and to rank candidates for it also must be determined at the same time. As a matter of convenience, these terms or patterns and as well as a function to find and rank terms are collectively called  X  X elation extractor. X  The relation extractor obtained by in-puts a and b from the Web is denoted by E ( a, b ) .

The second phase identifies a target term d by using input c and the relation extractor E ( a, b ) . Our methods return a set of candi-dates for term d which is denoted by D , and the candidates are ranked by the likelihood of being an expected term d .Forexam-ple, we input a query by combining input c = Microsoft and terms or lexico-syntactic patterns indicated by E ( a, b ) to a Web search engine, and we search for candidates D for the expected term d Zune from results of the Web search. Finally, each candidate d D is ranked by the relational similarity between Relation( iPod ) and Relation( Microsoft ,d i ) .

An overview of our framework is shown in Fig. 1. We introduce several methods for relational search from the Web: a method based on term co-occurrence, one based on lexico-syntactic patterns, and ones based on combinations of the two.

Finding an expected term d for c as b is for a is different from measuring relational similarity between prepared pairs [4, 22], where Figure 1: Framework of relational search from the Web. The gray squares represent inputs, i.e., input terms a , b ,and c .(1) Finding a relation extractor E ( a, b ) from the Web by given in-puts a and b . (2) Extracting candidates for an expected term d and ranking terms based on relational similarity by input c and E ( a, b ) . candidates for d have been already given. If the candidates for d were limited to a small number, there would be little difference be-tween the two problems. However, as there are many candidates for the answer, it is difficult to directly apply the methods to measure relational similarity to relational search. Once the candidates are limited in our methods, the measurement would contribute further improvement of the relational search.
First, the method based on term co-occurrence extract the re-lation extractor E ( a, b ) from Web search engine indices. In this method by term co-occurrence, E ( a, b ) is a set of terms T frequently appears only in Web search results for a query using both a and b . Next, we find an expected term d in Web search results for an input term c and each term in T ( a, b ) .Itisassumedthattheex-pected term d frequently appears only in documents including in-put term c and each term in T ( a, b ) , such that Sim(Relation( Relation( c, d )) is sufficiently high. Intuitively, the set of terms T ( a, b ) implicitly represents multiple relations between a and b , and documents including terms in T ( a, b ) are expected to describe relations between the inputs a and b . Then, terms frequently ap-pearing in the documents with terms c and T ( a, b ) are likely to be a target term d ,wheretheterms c and d hold a relation similar to one between inputs a and b .
Finding a relation extractor E ( a, b ) for input terms a and b con-sists of three steps.
 Step 1 Gathering text contents that contain terms a and b using Step 2 Finding terms that frequently appear only in documents Step 3 Choosing a set of terms used in a relation extractor.
We suppose that terms which frequently appear only in docu-ments including terms a and b represent relations between a and b . We find such terms by conducting  X  2 tests to probabilities of terms occurring in Web search results for different queries.

The details of those steps are as follows. Step 1. We search for Web documents that include a but not b and include b but not a , denoted by Doc( a  X  b ) and Doc( respectively. Web documents that include both a and b ( Doc( are also retrieved, and nouns in the titles and su mmaries for each search result are obtained without downloading Web pages. Step 2. For each term t supposed to be a noun, a  X  2 test is con-ducted of the hypothesis that term t has the same probability of oc-curring in Doc( a  X  b ) and Doc( a  X  b ) . The same test for is conducted. Th e probability P ( t | a, b ) of a term t occurring in Doc( a  X  b ) is calculated as follows supposing that the occurrence of term t follows a binomial distribution: where N a  X  b is the number of returned Doc( a  X  b ) ,and N the number of returned documents that include term t .

The  X  2 test value was calculated as follows: where N a  X  b is the number of returned Doc( a  X  b ) ,and N is the number of Doc( a  X  b ) that include term t . N a  X  b number of returned documents that do not include t . In this case, the  X  2 test value follows a  X  2 distribution whose degree of freedom is 1.
 Step 3. If both of these hypotheses for term t are rejected at sig-nificance level  X  , and the occurrence of t is higher in Doc( than in both Doc( a  X  b ) and Doc( a  X  b ) ,then t is taken to be an element of a term set T ( a, b ) .

This method extracts a set of terms T ( a, b ) for inputs a and b by using Web search engine indices. T ( a, b ) is estimated to appear at a high probability only in documents including both terms a and b and to stand for relations between the inputs a and b . Obtaining the set of terms T ( a, b ) determines a method for ranking terms on the basis of relational similarity.

Some examples of T ( a, b ) are shown in Table 1. They were actu-ally extracted from the Web by the proposed three steps. Note that the terms in T ( a, b ) do not represent relations explicitly but implic-itly. For example, a term  X  parliament X  for inputs a = A ustralia and b = C anberra may not only represent the meaning that  X  X ustralian parliament house is located in Canberra, X  but may also describe that  X  X anberra is the center of political activity in Australia. X  Al-though the terms in T ( a, b ) cannot be taken as a representation of relations between input a and b , they are able to extract a term d from documents where a is to b what c is to d .
Ranking candidate terms for term d on the basis of relational similarity to input terms a , b ,and c consists of the following four steps.
 Step 1 Gathering text contents that contain terms c and t in Step 2 Finding candidates for term d that frequently appear Step 3 Scoring the candidates based on  X  2 tests for each t . Step 4 Aggregating the scores for each candidate.

We suppose that terms which frequently appear only in docu-ments including terms c and t in T ( a, b ) is expected to be a target term d . We used a proba bility of a null hypothesis in a  X  score terms in Web search engine indices.
 The details of those steps are as follows.
 Step 1. For each term t in a set of terms T ( a, b ) , Web documents are searched for that include c but not t and that include t but not c , which are denoted by Doc( c  X  t ) and Doc( c  X  t ) , respectively. Documents that include both c and t ( Doc( c  X  t ) ) are also sought. Then, for each search result, we extract each noun d i in the titles and summaries, each of which is a candidate for an expected term d .
 Step 2. For each term d i supposed to be a noun in Doc( c a  X  2 test is conducted of the hypothesis that, in Doc( c Doc( c  X  t ) , the occurrence probabilities of a term d i Thesametestfor Doc( c  X  t ) is also conducted. In the test results, the probabilitie s of the null hypothesi s are assigned to P P ( d i ) , respectively. The probability P c ( d i ) is calculated as fol-lows using the  X  2 test value in formula (5): Inthesameway, P t ( d i ) is also calculated.
 Step 3. If both of these hypotheses for term d i are rejected at significance level  X  and the occurrence of term d i is higher in Doc( c  X  t ) than in both Doc( c  X  t ) and Doc( c  X  t ) ,then P P ( d i ) P t ( d i ) .Otherwise, P c,t ( d i )=1 , which is the lowest value for our ranking score. Note that P c,t ( d i ) is no longer a proba-bility because the independence between terms c and t cannot be assumed, and just represents a score for ranking candidates d Step 4. The score Score( d i ) is the product of all P c,t terms t in the term set T ( a, b ) . Score( d i ) is a very small value to process on a computer, so it is normalized by taking its logarithm, and  X  ln Score( d i ) is assigned to the value of a ranking function Rank( q, d i ) for a query q =( a, b, c ) : Term d i is estimated to appear at a high probability only in docu-ments including both terms c and t if the Score( d i ) is low. As each term t implicitly indicates a rel ation between inputs a and b ,ifterm d frequently appears with term t , it is expected that term d satisfy a relation similar to Relation( a, b ) .
 This method for a given q =( a, b, c ) retrieves the ranked result D , which is a set of candidates for an expected term d that is to c as b is to a .
Another method for relational search is based on lexico-syntactic patterns, which was proposed by us for extraction of related terms [15]. Lexico-syntactic patterns are often used to extract signifi-cant terms from text corpora [11]. We first obtain several lexico-syntactic patterns that represent relation between a and b .Asthe relation between a and b should be similar to the relation between c and d , the obtained patterns must also represent the relation be-tween c and d . After that, candidates for term d are found by using the patterns with term c .

Consider a certain relation between two terms denoted by &lt;s&gt; typically uses lexico-syntactic patterns as follows. When &lt;s&gt; is given, candidates for &lt;t&gt; would be found by search-ing for terms that match the patterns in the text resources. The Web can be used as a text resource for this purpose through Web search interfaces.

For example, when the target relation is hyponymy where &lt;s&gt; is hyponym of &lt;t&gt; , the following lexico-syntactic pattern represents the relation. When &lt;s&gt; is  X  X ong Kong X , title s and snippets in Web search re-sults by a query  X  X uch as Hong Kong X  may contain a phrase  X  X ig cities such as Hong Kong X . In this case, we can regard  X  X ig cities X  as one of the candidates for &lt;t&gt; .

We use search results from a conventional Web search engine both to find lexico-syntactic patterns by term a and term b and to detect candidates for term d by the patterns and term c .
A relation extractor E ( a, b ) in this method consists of several lexico-syntactic patterns that represent the relation between a and b . The way to obtain the lexico-syntactic patterns consists of the following three steps.
 Step 1 Gathering text contents that contain both terms a and b Step 2 Extracting candidates for lexi co-syntactic patterns that Step 3 Examining the candidates.

As Relation( a, b ) is nearly equal to Relation( c, d ) , the lexico-syntactic patterns achieve both detecting b when a is given and de-tecting d when c is given. We call a and c as source terms that are given and denote them by &lt;s&gt; . On the other hand, we call b and d as target terms that are extracted from text contents using the patterns, and denote them by &lt;t&gt; .

Two kinds of patterns are obtained in the method, i.e., prefixes and suffixes . Prefixes are text strings that appear just before tar-get terms, and suffixes are text strings that appear just after target Table 2: Examples of scoring candidates for prefixes for  X  X icasso X - X  X ainter X .
 terms. A complete lexico-syntactic pattern that contains either a prefix or a suffix is represented as follows.
 Here, represents the concatenation of text strings. The prefix and the suffix often contain &lt;s&gt; , e.g., prefix = &lt;s&gt; are .
The following lexico-syntactic patterns are available patterns for hyponymy relation between &lt;s&gt; and &lt;t&gt; .
 Here, we find such patterns automatically that represent relation between a and b . The detail of the method is described as follows. Step 1. Text contents including both terms a and b are gathered through a Web search. They are denoted by Doc( a  X  b ) . The num-ber of the gathered search results affects the response time of the method. In the experiments, 1,000 or 100 is used for the number of the Web search results, respectively.
 Step 2. Candidates for prefixes and suffixes are extracted from Doc( a  X  b ) .

As a text string useful for a prefix should appear just before term b , candidates for prefixes are extracted from such places. They are scored using their numbers of appearances in Doc( a  X  b ) for a candidate for a prefix is represented as follows. Here, t i denotes a candidate for a prefix, PtScore ( t i for t i , PtCt ( t i ) Pre is the number of times t i appears just before term b ,and T all is a set of all the extracted text strings from b ) . The formula means that the score for a prefix t i is calculated by subtracting the maximum number of appearances of t ending with t from the number of appearances of t i .

Supposing term a is  X  X icasso X  and term b is  X  X ainter X , we find several candidates for prefixes. The candidates are partially shown in Table 2. In this case, the score for  X  X reat X  is 3 because the num-ber of appearance of  X  X reat X  is 10 and the number of appearances of  X  X  great X , which is a candidate ending with  X  X reat X , is 7.  X  X i-casso is a great X  is a special case for a prefix because it starts with term a , i.e.,  X  X icasso X . Because such a pattern generally works bet-ter than others to extract a target term, we attach a high value to it. In this case, we replace  X  X icasso X  with tag &lt;s&gt; , so the prefix is represented as  X  &lt;s&gt; is a great X . At the same time, we ignore the existence of the longer prefixes than  X  &lt;s&gt; is a great X , and multiply the score by ten. As a result, the score of the prefix  X  &lt;s&gt; is a great X  is 20.
 Candidates for suffixes are obtained in almost the same manner. They should appear frequently just after term b in Doc( a score for a candidate for a suffix t i is calculated as follows. Here, t i denotes a candidate for a suffix, PtScore ( t i for t i ,and PtCt ( t i ) Suf is the number of times t i appears just after term b . The formula means that the score for the suffix t culated by subtracting the maximum number of appearances of t starting with t i from the number of appearances of t i . Candidates for suffixes that ends with term a are treated as special cases, and the scores are multiply by ten.
 Step 3. We examine some of the obtained candidates for prefixes and those for suffixes. The number of the examined candidates affects the response time of the method. In the experiments, we examine the top ten candidates for each of prefixes and suffixes.
The examination considers the following two points. The first is that a lexico-syntactic pattern must be able to extract term b when term a is given. The number of times that the pattern can detect term b must be neither few nor too many. If it is few, the pattern is not relevant enough to a and b . If it is too many, the pattern is too specific and can only work for term b ; that is, it cannot work to extract candidates for term d when term c is given. The second point is that the hit count of a Web search for the pattern should be more than a certain number. As we extract terms from Web search results, a certain number of search results are required. In the experiments, we determine patterns whose hit counts are more than 1,000 are better than ones whose hit counts are lower.
When term a is  X  X icasso X , term b is  X  X ainter X , and the exam-ined prefix is  X  &lt;s&gt; is a great X , we first obtain Web search results for the query  X  X icasso is a great X . Then, we count the number of appearances of  X  X ainter X  just after  X  X icasso is a great X .
The score of the examination for a lexico-syntactic pattern t calculated as follows.
 where PtScore ( t i ) denotes the examined score of the pattern t and PtCt ( t i ) denotes the number of times that term b is found when the pattern t i is used with term a . The formula means that the pattern by which we can find term b for term a closer to  X  times is more effective. In the experiments, we obtained 100 search results in this step. When the number of obtained search results is 100, we use 15 for  X  .

Finally, prefixes and suffixes are selected from the candidates that have high scores and have high hit counts of over 1,000 if pos-sible. In the experiments, we select three prefixes and three suffixes for the next phase. We denote the selected prefixes by Sp and the selected suffixes by Sp Suf ( a, b ) .
The method returns ranked candidates for term d on the basis of relational similarity for input terms a , b ,and c . It consists of the following three steps.
 Step 1 Gathering Web search results for each lexico-syntactic Step 2 Extracting candidates for term d from the gathered Web Step 3 Ranking the candidates.
 Step 1. First, a query for a Web search is made for each pattern, and search results for the query are gathered. A Web query for a prefixorasuffixismadeasfollows.
 Here, WQ Pre is a query for a prefix and WQ Suf is one for a suffix. &lt;s&gt; , if it appears in the pattern, will be replaced by term c .Ifthe pattern does not contain &lt;s&gt; ,weaddterm c to the Web query.
Web search results for them are gathered. In the experiments, we gathered 100 search results in this step.
 Step 2. We next extract candidates for term d from the gathered search results. A set of candidates for term d , which is denoted by D , can be represented as follows.
 Here, Parts (WQ) is a multiset of any phrase in the obtained Web search results where WQ is issued as a query. These conditions mean that a candidate for term d should be found both when any prefix is used and when any suffix is used.
 Step 3. Each term d i  X  D should be ranked. Basically, a term found more frequently is more significant. Here, Ct Pre ( the sum number of appearances for the prefixes and Ct Suf ( notes the sum number of appearance for the suffixes. The score for term d i is calculated as the geometric average of Ct Ct This is the ranking function Rank( q, d i ) ,and d i is finally ranked in accordance with the score.
The method based on term co-occurrence sometimes cannot out-puts target terms at the top, and that based on lexico-syntactic pat-terns cannot extract a target term if it cannot find an appropriate pattern. To overcome the disadvantages of each, we improve the method based on lexico-syntactic patterns by combining it with that based on term co-occurrence. In this combined method, the relation extractor E ( a, b ) is a set of term and syntactic pattern pairs.
Finding a relation extractor E ( a, b ) for input terms a and b con-sists of three steps.
 Step 1. We extract T ( a, b ) from the Web in accordance with the method based on term co-occurrence. Step 2. For each term t in the set of terms T ( a, b ) , we search for Web documents by using a Web search engine that include all the terms a , b ,and t . By the method based on lexico-syntactic patterns, candidates for prefixes and those for suffixes are extracted from indices of the results. As mentioned in the method based on term co-occurrence, each term in a term set T ( a, b ) represents a certain relation. This works to limit documents that mainly describe relations between two input terms, and the extraction of lexico-syntactic patterns is expected to become more precise due to the additional terms in T ( a, b ) .
 Step 3. The obtained lexico-syntactic patterns are evaluated by following the method based on lexico-syntactic patterns. We select the most effective patterns to find a target term d and pair them up with the terms that are used for a Web search to obtain the patterns in step 2. A term-pattern pair is composed of term t in T ( a, b ) and syntactic pattern SP Pre or SP Suf , i.e., ( t, SP ( t, SP Suf ) .The pairs are taken as a relation extractor E
This method obtains E ( a, b ) for inputs a and b by using Web search engine indices.
Ranking candidate terms D obtained by a combination of the two methods mainly follows the lexico-syntactic pattern method. As E ( a, b ) consists of pairs ( t, SP Pre ) or ( t, SP Suf ) terms in T ( a, b ) and syntactic patterns, term t is added to and WQ Suf by the  X   X   X  operator. For example, input terms a Macromedia and b = Adobe , a set of terms {acquisition, amal-gamation} are extracted from the Web by the method based on term co-occurrence. For each term in the set, Web documents are searched for with a query such as  X  X acromedia  X  Adobe  X  acqui-sition X  and  X  X acromedia  X  Adobe  X  amalgamation X , and syntac-tic patterns are extracted from the Web search results. Obtained prefixes and suffixes for the results by the query  X  X acromedia Adobe  X  acquisition X  are paired up with the term  X  X cquisition X  like (acquisition, prefix ) or (acquisition, suffix ). In finding candidates and ranking them, a query by combination of the term and patterns is used for a Web search like  X  prefix  X  acquisition X  or  X  suffix acquisition. X 
We mention a method by merging results returned by the two methods, based on term co-occurrence and lexico-syntactic pat-terns. As discussed in the experimental results, it is sometimes the case that one method can find an expected term for an input, however, another cannot find the term. For responding to all of the inputs, we merge results by methods based on term co-occurrence and based on lexico-syntactic patterns, and rank them by both meth-ods.

Each candidate d i for an input q =( a, b, c ) has different rank-ing scores given by the two methods, and the scores are denoted by Rank TC ( q, d i ) and Rank SP ( q, d i ) for methods based on term co-occurrence and lexico-syntactic patterns, respectively. The scores Rank TC ( q, d i ) and Rank SP ( q, d i ) are normalized by the maxi-mum score for each method, which run from 0 to 1.
 A ranking function of the conjunction of the two scores, Rank CNJ ( q, d i ) is defined as follows:
Rank CNJ ( q, d i )= w 1 Rank TC ( q, d i )+ w 2 Rank SP ( where w i is scalar value between 0 and 1. We heuristically set the parameters w i as w 1 =0 . 90 , w 2 =0 . 50 ,and w 3 =0 . paper.
We manually set up a set of 300 tests that consisted of tuples containing input terms a , b ,and c , and expected output d , e.g., Apple , iPod , Microsoft ,and Zune , respectively. For each test, in-put q =( a, b, c ) was a query. The returned terms ranked by each method were evaluated by the MRR, percentage of tests obtained in the top k , and the average number of Web accesses. The experiment was conducted in Japanese; the results in English are translations. Table 4: Examples of tests where terms a , b , c were inputs and term d was expected.
 TC (0.01, 0.05) 0.357 26.3 50.0 54.0 59.0 41.1 TC (0.05, 0.05) 0.379 25.0 55.3 60.3 67.3 105.2
We manually created a test set that consisted of inputs a , b ,and c and expected output d . Test set classes are shown in Table 3; the columns represent ID number, type of terms x and y , their rela-tion, and the example of top obtained answers. The column  X  X ela-tion between x and y  X  is simply a description, not a representation. Each class includes pairs of two terms, x and y , between which a specific relation holds. We selected two pairs in the same class and assigned one to a , b and the other to c , d . Consequently, the terms a and c ,and b and d for each test were the same types. For example, class 3 includes the pairs, wine and grape, chocolate and cacao, and cheese and milk. We evaluated the combinations a wine, b = grape, c = chocolate, d = cacao, and a = cheese, b milk, c = wine, d = grape, and so on. We show examples of test set combinations in Table 4. The total number of pair combinations was 300.
We used the MRR as a metric to evaluate the ranked search re-sults for each method. MRR is the mean of the inverse ranks for each task where a relevant result primarily appears. It is calculated as follows: where rr k is an inverse rank of a relevant result at the k th task, N is the number of tasks, and the maximum of the metric values was 1 while the minimum was 0 . We also considered the process time for achieving a speed sufficient for practical use, so we counted the number of Web accesses to retrieve ranked candidates for an expected term d .

The four proposed methods were compared: that based on term co-occurrence ( TC ), that based on lexico-syntactic patterns ( SP ), a combination of the two methods ( CMB ), and a conjunction of results of two methods ( CNJ ). To parameters  X  and  X  , which are the significance levels for the  X  2 test used in the method based on TC , we assigned  X  =0 . 01 , 0 . 05 and  X  =0 . 05 . We experi-mented with each combination of the parameters, and represented them in the format TC (  X  ,  X  ). Although the significance level  X  affects the precision of results and the number of Web accesses, a primary trial had indicated that  X  is a small factor compared with  X  . This is why  X  was fixed and results were compared by changing  X  . The number of search results used in the term co-occurrence method was fixed to 100, and it required twice accesses to a Web search engine. SP (100) and SP (1000) represent methods based on lexico-syntactic patterns for 100 and 1000 Web accesses, respec-tively, to extract patterns. For the method by using both TC and SP , i.e., the combined method ( CMB ), the parameters were fixed as  X  =0 . 01 in the term co-occurrence method and 100 Web ac-cesses to extract patterns in the lexico-syntactic pattern method. The conjunction of results by the two methods ( CNJ ) used results returned by TC (0.01, 0.05) and SP (100). The results D returned as an output for inputs a  X  AC b ,and c were ranked and evaluated by several metrics.

In our implementation, we used Yahoo! Search Web Services search for documents. MeCab 2 was used for the Japanese morpho-logical analysis, where a noun sequence was treated as a compound noun. The stop word list include general stop words and symbols.
The average scores for each method are shown in Table 5. Here, @1, @5, @10, and @20 mean the percentage of tests for which the expected term d was in the top 1, 5, 10, and 20. We could see general characteristics for each method. The method based on con-junction of results ( CNJ ) got the highest MRR and also obtained expected terms in the top 1, 5, 10, and 20 for the most tests. CNJ required about 60 Web access averagely. CMB is considered to be the second best method for the proposed relational search in terms http://developer.yahoo.com/search/ http://mecab.sourceforge.net/ of precision, and the cost of Web access is lower than CNJ .The lexico-syntactic pattern method ( SP ) follows CNJ and CMB ,and got MRR comparable to CMB . There is little difference in preci-sion between SP (100) and SP (1000) despite just 100 search results used for SP (100) to find syntactic patterns. The results may indi-cate that 100 Web search results are enough to construct syntactic patterns for the relational search. Although the method based on term co-occurrence ( TC ) failed to get high MRR, it could find ex-pected terms in top 20 for the second most tests.

A histogram of ranks at which relevant terms were found is shown in Fig. 2. SP and CMB ,and CNJ could find expected terms mostly at the top. Whereas TC obtained the expected terms at a broader range, i.e., between mainly 1st and 3rd, and also more than 10th. The difference of ranks in which expected terms were found made the pattern-based methods get higher MRR comparing with the method based on term co-occurrence. The method based on lexico-syntactic patterns such as SP and CMB could identify expected terms correctly, however, TC could not find the most relevant term from many candidates. The advantage of the method based on term co-occurrence could be seen in the frequency of terms which were not found. As TC (0.05, 0.05) failed only about 20% tests, it is ex-pected that the method based on term co-occurrence was applicable to various kinds of relations. It contributes the robustness of CNJ , which actually succeeded in deterrence of misses.

The average MRR for each class is shown in Fig. 3, and the per-centage of tests that obtained a relevant result in top 20 is shown in Fig. 4. Our methods achieved high MRR scores in some test classes, particularly in the classes 1, 10, 12, 13, 23, and 25, i.e., Japanese special local products, Japan ese literatures, natural satel-lites, Japanese schools of Buddhism, m ovies and directors, and ac-quirers and acquirees. Some of them, that is, 1, 10, and 13, have a commonality that they are about something Japanese and well-known to Japanese. The experiment was conducted in Japanese, and we used Japanese pages on the Web for the relational search. Also, for others, i.e., 23 and 25, there are many descriptions about them in Web pages such as news sites. Thus, our method worked well for these classes because of plenty information on them. On the other hand, the classes got the low MRR scores; class ID num-bers 2, 4, and 14, i.e., national sports, famous snacks, and Japanese hot springs. Though we used the test set for which an answer is uniquely determined, another answer were extracted that was not selected as an expected term d in the classes 4 and 14, and the pre-cisions decreased for these tests.

Comparing the method based on term co-occurrence and that based on lexico-syntactic patterns, in the classes such as 16 and 19, that is, flowers of prefectures in Japan and flower messages, TC got higher score whereas SP failed to find expected terms. Ta-ble 6 shows examples of a set of terms used in term co-occurrence method and syntactic patterns used in lexico-syntactic pattern method. We could see the appropriate terms extracted by the TC method for the classes 16 and 19 in the upper table. Although the SP method also extracted correct syntactic patterns for prefixes such as  X  X lower message of &lt;s&gt; is X  and  X  X lower of prefecture, X  the obtained suffixes were so general such as  X  X nd X , or so specific such as  X  X estival X  to identify target terms.

On the contrary, for the classes such as 9 and 17, i.e., animal classes and islands in the world, SP achieved much higher score than TC . In the class 9, we obtained the lexico-syntactic patterns frequently used in extraction of hypernyms [11] as shown in Table 6. Though TC could get terms representing relation for the class, it failed to find the hypernyms. It is difficult to represent the relations such as is-a and located-in with nouns, and syntactic patterns are suited to extract such relations.

The combination of the two methods ( CMB ) got higher scores in some classes than both TC and SP . Particularly, in the classes 13 and 25, i.e., Japanese schools of Buddhism and acquirers and acquirees, CMB could retrieve all of the expected terms at the top. On the other hand, it did not work well in the classes 1, 6, 8, and 18, that is, Japanese special local products, natives, imagoes and embryos, and gods of Japanese shrines. The combination method tends to use specific patterns for two terms a and b , and they cannot be applied to terms c and d even if they satisfy relations similar to Relation( a, b ) .
We proposed several methods to search with a query by example in a known domain for information in an unknown domain. Given three terms, such as Apple , iPod and Microsoft , Zune can be ex-tracted from the Web, where Apple is to iPod what Microsoft is to Zune . The is an effective way to obtain information in an un-known field for users. An analogical example makes it possible to search for unknown information without any descriptive key-words. We introduced methods for relational search by using Web search indices. We proposed methods based on term co-occurrence, on lexico-syntactic patterns, and on combinations of the two ap-proaches. These methods were compared with metrics such as the mean reciprocal rank, and characteristic were clarified with large amount of test data.

Query by analogical example, i.e., the relational search is based on the concept of analogy. The ability to infer unknown informa-tion by analogy is naturally equipped for humans. We believe that analogy is an essential factor for finding new information in an un-known domain with ease.
This work was supported in part by the following projects and in-stitutions: Grants-in-Aid fo r Scientific Res earch (Nos. 18049041, 21700105 and 21700106) from MEXT of Japan, a Kyoto Univer-sity GCOE Program entitled  X  X nformatics Education and Research for Knowledge-Circulating Society, X  and the National Institute of Information and Communications Technology, Japan. [1] E. Agichtein and L. Gravano. Snowball : Extracting Relations [2] M. Baroni and S. Bisi. Using cooccurrence statistics and the [3] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring [4] D. Bollegala, Y. Matsuo, and M. Ishizuka. WWW sits the [5] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring the [6] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring the [7] S. Brin. Extracting Patterns and Relations from the World [8] M. Cafarella, M. Banko, and O. Etzioni. Relational web [9] K. W. Church and P. Hanks. Word Association Norms, [10] O. Etzioni, M. J. Cafarella, D. Downey, S. Kok, A.-M. [11] M. Hearst. Automatic Ac quisition of Hypon yms om Large [12] T. Hokama and H. Kitagawa. Extracting Mnemonic Names [13] D. Lin. Automatic Retrieval and Clustering of Similar [14] G. Miller. WordNet: A Lexical Database for English. [15] H. Ohshima and K. Tanaka. Real time extraction of related [16] S. Oyama and K. Tanaka. Query Modification by [17] S. Soderland, O. Etzioni, T. Shaked, and D. Weld. The Use [18] P. D. Turney. Mining the Web for synonyms: PMI-IR versus [19] P. D. Turney. Measuring Semantic Similarity by Latent [20] P. D. Turney. Expressing Implicit Semantic Relations without [21] P. D. Turney. Similarity of Semantic Relations.
 [22] P. D. Turney and M. L. Littman. Corpus-based Learning of [23] J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. StatSnowball:
