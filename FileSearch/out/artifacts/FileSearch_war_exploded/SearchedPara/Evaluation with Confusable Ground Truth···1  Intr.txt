 Subjective judgment by human beings has been an important way for construct-ing ground truth because in many cases the gold standard cannot be collected or computed automatically and need to be judged manually. In various styles of sub-jective judgment, human ratings is a frequently used manner. When constructing the ground truth, researchers aggregate the ratings from multiple reviewers into a single score, which can be mean, majority voting and so on, and use the aggre-gated scores as the ground truth in the experiments, for example, [ 1 , 2 ] use mean scores of multiple ratings on the similarity of document pairs.
 gated scores are same. For example, assuming that there are two rating sets with five ratings, { 3 , 3 , 3 , 3 , 3 } and { 2 , 2 , 3 , 4 , 4 tributions but the aggregated mean scores are same. The aggregation loses the information of the differences of rating distributions. We make an exploration study with approaches of document similarity computation on a document collec-tion. It shows that these differences have important influences on the evaluation results.
 assume that the human ratings by reviewers in this scenario have sufficiently high quality and these differences of rating distributions as an inherent property of a judged instance or its rating set. This property represents how confusable the reviewers are when judging this instance. We denote this property as confusabil-ity . We need to consider the confusability information in the ground truth when we carry out experiments and evaluations, while existing evaluation methods which only use aggregated scores of human ratings do not consider it. Our con-fusability metrics and confusability aware evaluation methods are not proposed to instead traditional ones. They are used with the existing ones together to supplement the quality of the evaluation results.
 There are some existing work which also consider the inconsistency of human ratings. On one hand, Cohen X  X  kappa [ 3 ] is used to measure inter-rater agreement of two reviewers on a set of instances. In our scenario, we need to measure the disagreement of a set of reviewers on one instance. On the other hand, label aggregation methods in crowdsourcing such as [ 4 ] with a probabilistic model can be used to generate more rational aggregated rating than statistical measures like mean and majority voting. However, they still output a unique rating for an instance and the confusability information of the dataset is still lost when using them for evaluation. Our work concentrates on how to include the confusability in the evaluation beyond the use of single aggregation score.
 Our contributions are as follows. First, we make an exploration study which analyzes the influences of confusability in the evaluation results. Second, we propose several confusability metrics and confusability aware evaluation methods for different purposes. We utilize a public document dataset for illustration. In a given dataset D , we denote d i as an instance . In the ground truth, we define the person who provides the ratings as reviewer . r i = { of d , and the number of ratings of instance d i is | r i | = n values and r ij  X  X  . The ratings are numerical values such as categorical values such as { X  , + } mapped to R = { 0 , 1 d generated by an approach k is s k i .
 Dataset LP50: This public document collection contains 50 news articles selected from the Australian Broadcasting Corporation X  X  news mail service [ 1 ]. It not only provides the data of human ratings with aggregated measures, but also the non-aggregated ratings of all reviewers. There are 1225 document pairs and 83 reviewers. The semantic similarity of each document pair is labeled by around 10 reviewers with ratings from 1 (highly unrelated) to 5 (highly related). 2.1 Influences of Confusability in Evaluation The confusability can be measured in various ways in our topic, while in this exploration study we measure it with rating difference, which is the difference between maximum and minimum rating of an instance d i , dif r min { r i } . When dif r i is lower, it means that the ratings of the reviewers are more consistent, the rating of d i is easier to be judged by human beings. Figure 1 shows the distribution of the rating differences dif r i on the mean ratings mear It shows that the document pairs that are easy to be judged ( dif r document pairs that are very similar ( mear i = 5) or very dissimilar ( mear When mear i is around 3, it is possible that the document semantic similarity of d i is not easy to be judged by human beings ( dif r i  X  2). In such condition, it may be also difficult for an approach k to generate consistent similarity results with the human ratings which are used as the ground truth.
 performance and the confusability. One is Latent Semantic Analysis ( LSA ) which represents documents with vectors of latent topics using singular value decomposition. The other one is Knowledge Based Graph ( KBG )[ 2 ] which rep-resents document with a entity sub-graph extracted from a knowledge base. which is a traditional metric without considering confusability for evaluation. or KBG) and mean rating set mea r = { mear i } i on a set of document pairs is the results and human ratings. The range of this metric is [ we also divide the dataset into several subsets based on rating difference and list the performance on each subset. It shows two observations. First, generally when the rating difference increases, the difficulty of document similarity judg-ment increases and the performance of both two models decrease. Second, each approach performs differently on different subsets with different confusability. in ground truth has prominent influences on the evaluation results; second, the credibility of the evaluation results changes in different confusability. Therefore, we need to consider the confusability in ground truth and evaluation. 3.1 Confusability Measures For analyzing human ratings in ground truth, the statistical measures used in traditional methods which aggregate the human ratings of a given instance can be mean ( mear i ), median ( medr i ), majority voting ( majr do not consider the confusability information. In contrast, we propose three candidate confusability measures. 1. Rating Difference : It is the largest difference of two ratings in the rating set and has been used in the study in Sect. 2.1 , dif r i = max 2. Standard Deviation : It is to qualify the amount of variation or dispersion of the ratings set of an instance. We use the sample standard deviation and define it as stdr i = n i j =1 ( r ij  X  mear i ) 2 / ( n i this uncertainty is low, it means that more reviewers can reach same ratings and the confusability is low. We define it as entr i =  X  k p i n ( k ) /n i , where n i ( k ) is the number of ratings that are equal to k . We plot these three confusability measures in Fig. 2 . It shows that these confusability measures are linearly dependent to each other in a certain degree. When all reviewers of d i reach consensus, dif r i = stdr one measure cannot describe the confusability completely, and we need several measures to describe it from multiple aspects. For example, for two rating sets with five ratings, { 5 , 5 , 5 , 5 , 1 } and { 4 , 3 , 3 , 2 , 2 (=0.72), but higher dif r i (=4) and stdr i (=1.79); the second set has a higher entr (=1.52), but lower dif r i (=2) and stdr i (=0.84). Furthermore, as shown measure has some patterns of distribution, in contrast to distributing randomly. In addition, even relatively low values of confusability measures can be sig-nificantly different from zero. Statistical significance for confusability measures makes no claim on how important is a sufficient magnitude in a given scenario or what value can be regarded as absolutely low or high confusability. 3.2 Confusability Aware Evaluation The confusability in the ground truth of a given dataset actually depends on the instance selection process of constructing this dataset. The dataset creators can follow some rules in the instance selection process. For example, only selecting the instances with low confusability or keeping a balance on the number of low-confusability and high-confusability instances. Then the dataset users can provide confusability information with the experimental results.
 We propose four candidate methods for confusability aware evaluation, which instances according to the confusabilty (instance selection based methods), i.e., the following method 1 and 2. The other is combining the confusability measures with existing evaluation metrics (confusability aware metric based methods), i.e., the following method 3 and 4. We use the LP50 dataset and same approaches in Sect. 2.1 to illustrate experimental results by using these evaluation methods. 1. Evaluation instances with different confusability respectively: to evaluate the performance on the entire dataset, we also divide the dataset into several subsets according to the confusability. The evaluation results by this method is shown in Table 1 . 2. Evaluation instances with low confusability only: condition. For example, if the condition is dif r i = 0, which means we only use the instances that all reviewers reach consensus, as shown in Table 1 ,the performance of LSA and KBG are 0.859 and 0.714; if the condition is dif r the performance of LSA and KBG are 0.645 and 0.595. They show that when confusability of instances is low, the performance of LSA is better. high credibility of the experimental results. The disadvantages of this method is that it omits some kinds of instances. For example, for LP50 dataset, it uses the instances with low or high similarities but ignores the instances with middle similarities. Our proposals do not strongly accept or reject the instances with high confusability. This decision is made according to the detailed cases. 3. Separated confusability aware metrics: ation metrics is to provide additional confusability measures without modifying existing metrics. We use the average of confusability values on all instances in the evaluated set for this purpose. For example, the performance on the entire dataset, the average rating difference is 1.89; the average standard deviation is 0.67; the average entropy is 0.99. These information of the dataset are inde-pendent with the evaluated approaches. The advantage of this method is that it is easy to be integrated with existing experimental methods and results. The disadvantage of this method is that it only provides the information, but the confusability does not influence the evaluation results. 4. United confusability aware metrics: In this method, we integrate the confusability information into the existing evaluation metrics. We use the Pearson correlation coefficient defined in Sect. 2.1 as the example of evaluation metric. We modify the formula by adding the con-fusability information as the weights of instances in the following format. We consider that the instances with higher confusability has lower credibility and should have lower influences in the evaluation results. We thus assign lower weights to the instances with higher confusability. The confusability aware for-mats for other evaluation metrics average MAP or nDCG on instances in the dataset can be formulated in same way: caCorr ( s k , st )= st i can be mear i , medr i or majr i ; ca i can be dif r i , stdr normalized into the range of [0 , 1]. For example, if we use mear as ca , we can define this metric as entropy-confusability-aware-mean correlation. The performance on the entire dataset of LSA and KBG are illustrated in Table 2 . It shows that LSA has prominently better performance than KBG when considering confusability in the evaluation metrics. In contrast, without consid-ering confusability, LSA and KBG have similar performance on the entire dataset in Table 1 . Although the performance of LSA and KBG are not distinguishable when using traditional evaluation method, they have prominent difference when using our confusability aware evaluation method. It shows the significance of confusability aware evaluation and the effectiveness of our evaluation method . In an evaluation task, an instance selection based method and a confusability aware metric based method can be combined to construct an evaluation solution. For example, if using method 2 and 4, we can evaluate the performance of LSA and KBG on the subset which contains the instances with low confusability dif r  X  2. The results are shown in Table 2 . It is consistent with the observation that LSA performs better when the confusability is lower.
 All these confusability aware evaluation metrics and methods do not raise the problem on scalability. They can also be used to save the cost in the ground truth construction by setting less reviewers to instances with low confusability. In this paper, we discuss the influence of confusability on evaluation results. We propose several confusability aware evaluation metrics and methods. In future work, we will analyze more characteristics of the confusability aware evaluations.
