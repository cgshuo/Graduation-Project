  X   X  R p in the following linear model: of
X are normalized to have  X  2 norm the relevant set of variables and to estimate  X  with bounded  X  In particular, recovery of the sparsity pattern S = supp (  X  ) := { j :  X  chosen penalization parameter  X  | time k b  X   X   X  k 2 the bound on  X  columns of X indexed by T ; similarly, let  X  as in (1.2) or the Dantzig selector as in (1.3), with  X  when  X  More definitions. For a matrix A , let  X  as a s -sparse vector. Throughout this paper, we assume that n  X  2 s and general, we also assume  X  the s -restricted isometry constant  X  for all T  X  { 1 , . . . , p } with | T |  X  s and coefficients sequences (  X  non-decreasing in s and 1  X   X  Occasionally, we use  X   X   X   X  R p such that  X   X  different from those in [5]. Consider the least square estim ator b  X  | I | X  s and consider the ideal least-squares estimator  X   X  which minimizes the expected mean squared error. It follows from [5] that for  X  Now we check if for  X  These bounds are meaningful since define a constant  X  noise and covariates of X , which we only apply to X with column  X  used by [10] and [17].
 Assumption 1.1 ( Restricted Eigenvalue assumption RE ( s, k s  X  p and a positive number k 0 , the following holds: If for the restricted eigenvalue assumption to hold as X such that s is very close to n ; See Section 4 for such examples. Let  X  Theorem 1.1 ( Variable selection under Assumption 1.1) Suppose that RE ( s, k holds, where k where B  X  1 for the DS and  X  2 for the Lasso. Let B Then with probability at least P ( T which satisfies (1.7) and (1.8) given that  X  b  X  when some components of  X  oracle inequality as in (1.7)? Before we answer this questio n, we define s such that and the ( s, s  X  ) -restricted orthogonality constant [4]  X  p . Note that  X  is non-decreasing in s, s  X  and small values of  X  covariates in X dinality at most 2 s  X  Theorem 1.2 Choose  X , a &gt; 0 and set  X  in (1.3) . Suppose  X  is s -sparse with  X  range ( C 1  X  ( where C Our analysis builds upon [5]. Note that allowing t would like, with the cost of increasing the constant C C , which indeed depends on  X  k 0 = 1 depends on the unknown value  X  theoretical analysis on variable selection. We use a penalization parameter  X  Let b S 0 , 1 : (a) Set t i = 4  X  n Return the final set of variables in b S Theorem 2.1 Let  X  estimator  X  init satisfies on T where B correct set of variables in b S Remark 2.2 Without the knowledge of  X  , one could use b  X   X   X  in  X  requirement on  X  b S Lemma 3.2 for an example); instead of having to estimate B  X  of variables indexed by { j : |  X  The General Two-step Procedure : Assume  X  Proposition 3.1 [5] Let Y = X X  +  X  , for  X  being i.i.d. N (0 ,  X  2 ) and k X and set  X   X  From this point on we let  X  :=  X  yields the following constants, where C where C We first set up the notation following that in [5]. We order the  X  Recall that s p which implies that min(  X  2 2 s Lemma 3.2 Choose  X  &gt; 0 such that  X  the constraints, for  X  := p 2 log p/n and  X  Given some constant C Then with probability at least P ( T rate as required in Theorem 1.2 so long as k  X  such that I  X  S based on I , b  X  | S
R | &lt; s larger  X  X   X  s designs that satisfy the Restricted Eigenvalue assumption s. X ij  X  N (0 , 1) X 2) Generate a vector  X  of length p according to N (0 , I b  X  always fix  X  b S 0 = { j :  X  j, init  X  0 . 5  X  n }
