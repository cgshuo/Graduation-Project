 REGULAR PAPER Jianyong Wang  X  George Karypis Abstract Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to transaction or document clustering. However, most of the frequent itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring key properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results show that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm it is very effective in clustering the categorical data and summarizing the dense transaction databases.
 Keywords Data mining  X  Frequent itemset  X  Categorical database  X  Clustering 1 Introduction Frequent itemset mining was initially proposed and has been studied extensively in some studies have also demonstrated the usefulness of frequent itemset mining in serving as a condensed representation of the input data in order for answering various types of queries [ 8 , 22 ], and the transactional data (or document) classifi-cation [ 4 , 5 , 19 , 20 ] and clustering [ 7 , 11 , 32 , 33 , 34 ].
 intermediate set of frequent itemsets (in many cases, it is the complete set of fre-quent itemsets), on which some further post-processing can be performed in order to generate the final result set which can be used for clustering purposes. In this paper we consider directly mining a final subset of frequent itemsets which can be used as a concise summary of the original database and to cluster the categor-ical data. To serve these purposes, we require the final set of frequent itemsets have the following properties: (1) it maximally covers the original database given a minimum support; (2) each final frequent itemset can be used as a description for a group of transactions, and the transactions with the same description can be grouped into a cluster with approximately maximal intra-cluster similarity. To achieve this goal, our solution to this problem formulation is that for each trans-action we find one of the longest frequent itemsets that it contains and use this longest frequent itemset as the corresponding transaction X  X  description. This set of mined frequent itemsets is called a summary set .
 sets is that it provides the possibility of designing a more efficient algorithm. We proved that each itemset in the summary set must be closed; thus, some search space pruning methods proposed for frequent closed itemset mining can be bor-rowed to accelerate the summary set mining. In addition, based on some proper-ties of the summary set , we proposed several novel pruning methods which greatly improve the algorithm efficiency. By incorporating these pruning methods with a traditional frequent itemset mining framework, we designed an efficient summary set mining algorithm, SUMMARY. Our thorough empirical tests show that SUM-MARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size. Moreover, its result set is very effective in clustering the categorical data and summarizing the dense transaction databases. problem definition and some related work, respectively. Section 4 describes the algorithm in detail. Section 5 presents the empirical results. Section 6 shows an application of the algorithm in clustering categorical data, and the paper ends with some discussions and conclusion in Section 7 . 2 Problem definition A transaction database TDB is a set of transactions, where each transaction, de-noted as a tuple tid , X , contains a set of items (i.e., X ) and is associated with a unique transaction identifier tid. Let I ={ i 1 , i 2 ,..., i n } be the complete set of distinct items appearing in TDB. An itemset Y is a non-empty subset of I and is x ... x of transactions in TDB containing itemset Y is called the (absolute) support of itemset Y , denoted by sup ( Y ) . In addition, we use | TDB | and | Y | to denote the number of transactions in database TDB, and the number of items in itemset Y , respectively.
 sup ( Y )  X  min sup. Among the longest frequent itemsets supported by transaction T , we choose any one of them and denote it by SI T i .SI T i is called the summary itemset of T i . 1 The set of the summary itemsets with respect to (w.r.t.) the transac-that the summary set of a database may not be unique, this is because a transaction may support more than one summary itemset .
 the problem of this study is to find any one of the summary sets w.r.t. TDB. Example 1 The first two columns in Table 1 show the transaction database TDB in our running example. Let min sup = 2, we sort the list of frequent items in support ascending order and get the sorted item list which is called f list. In this example f list = a :3, b :4, c :4, d :5, e :5, f :5 . The list of frequent items in each transaction are sorted according to f list and shown in the third column of Table 1 . summary set w.r.t. TDB. 3 Related research Since the introduction of the association rule mining [ 2 , 3 ], numerous fre-quent itemset mining algorithms have been proposed. In essence, SUMMARY is a projection-based frequent itemset mining algorithm [ 1 , 18 ] and adopts the natural matrix structure instead of the FP-tree to represent the (conditional) scanning its projected matrix. In [ 15 ] an algorithm was proposed to mine all most specific sentences, however, both the problem and the algorithm in this study are different from those in [ 15 ].
 pruning methods previously proposed in the closed (or maximal) itemset mining algorithms [ 6 , 10 , 21 , 23 , 25 , 27 , 30 , 35 ] can be used to enhance the efficiency of SUMMARY. Like several itemset mining algorithms with length-decreasing support constraint [ 28 , 31 ], SUMMARY adopts some pruning methods to prune the unpromising transactions and prefixes. However, as the problem formulations are different, the pruning methods in SUMMARY are different from the previous studies.
 marize the transactions and cluster the categorical data. There are many algo-rithms designed for clustering categorical data, typical examples include ROCK [ 14 ] and CACTUS [ 13 ]. Recently several frequent itemset based clustering algo-rithms have also been proposed to cluster categorical or numerical data [ 7 , 11 , 34 ]. These methods first mine an intermediate set of frequent itemsets, and some post-processing are needed in order to get the clustering solution. SUMMARY mines the final subset of frequent itemsets which can be directly used to group the trans-actions to form clusters and enables us to design more effective pruning methods to enhance the performance.
 1. We proposed a new problem formulation of mining the summary set of fre-2. By exploring the properties of the summary set , we have proposed several prun-3. Thorough performance study has been performed and shown that SUMMARY 4 SUMMARY: an efficient algorithm to summarize the transactions In this section we first briefly introduce a traditional framework for enumerating the set of frequent itemsets, which forms the basis of the SUMMARY algorithm. Then we discuss how to design some pruning methods to speed up the mining of the summary set and present the integrated SUMMARY algorithm. Finally we discuss the local item ordering schemes and how to revise SUMMARY to mine K summary itemsets for each transaction. 4.1 Frequent itemset enumeration Like most of the other projection-based frequent itemset mining algorithms, SUM-MARY employs the divide-and-conquer and depth-first search strategies [ 18 , 30 ], which are applied according to the f list order. In Example 1 , SUMMARY first mines all the frequent itemsets containing item a , then mines all frequent itemsets containing b but no a ,..., and finally mines frequent itemsets containing only f . In mining itemsets containing a , SUMMARY treats a as the current prefix, and builds its conditional database, denoted by TDB | a ={ 01 , ec , 05 , ef c , 07 , fc } (where the local infrequent items b , d ,and g have been pruned and the frequent items in each projected transaction are sorted in support ascending or-der). By recursively applying the divide-and-conquer and depth-first search meth-ods to TDB | a , SUMMARY can find the set of frequent itemsets containing a . Note instead of using the FP-tree structure, SUMMARY adopts the natural ma-trix structure to store the physically projected database [ 12 ]. This is because the matrix structure allows us to easily maintain the tids in order to determine which set of transactions the prefix itemset covers. In addition, in the above enumeration process, SUMMARY always maintains the current longest frequent itemset for each transaction T i that was discovered first so far. In the following we call it the current Longest Covering Frequent itemset w.r.t. T i (denoted by LCF T i ). 4.2 Search space pruning The above frequent itemset enumeration method can be simply revised to mine the summary set : Upon getting a frequent itemset, we check if it is longer than the current longest covering frequent itemset w.r.t. any transaction that this itemset covers. If so, this newly mined itemset becomes the current longest covering fre-quent itemset for the corresponding transactions. Notice that this na  X   X ve method is no more efficient than the traditional all frequent itemset mining algorithm. How-ever, the above algorithm for finding the summary set can be improved in two ways. First, as we will prove later in this section, any summary itemset must be closed and thus, the pruning methods proposed for closed itemset mining can be used. Second, by maintaining the length of the current longest covering itemset for each transaction during the mining process, we can employ additional branch-and-bound techniques to further prune the overall search space.
 Definition 1 ( Closed itemset ) An itemset X is a closed itemset if there exists no proper superset X  X  X such that sup ( X ) = sup ( X ) .
 Lemma 1 ( Closure of a summary itemset ) Any summary itemset w.r.t. a transac-tion T i , SI T i , must be a closed itemset.
 Proof We will prove it by contradiction. Assume SI T i is not closed, which means dicts with the fact that SI T i is the summary itemset of transaction T i . ing can be used to enhance the performance of the summary set mining. In SUM-MARY, only one such technique, item merging [ 30 ], is adopted that works as fol-lows. For a prefix itemset P , the complete set of its local frequent items that have the same support as P are merged with P to form a new prefix, and these items are removed from the list of the local frequent items of the new prefix. It is easy to see that such a scheme does not affect the correctness of the algorithm [ 30 ]. Example 2 Assume the current prefix is a :3, whose local frequent item list is e :2, f :2, c :3 , among which c :3 can be merged with a :3 to form a new prefix ac :3 with local frequent item list e :2, f :2 .
 called conditional transaction and conditional database pruning that given the set of the currently maintained longest covering frequent itemsets w.r.t. TDB, they remove some conditional transactions and databases that are guaranteed not to contribute to and generate any summary itemsets .
 sideration, sup ( P ) its support, and TDB | P ={ T P 1 , X P 1 , T P 2 , X P 2 ,..., T tions X P i (1  X  i  X  sup ( P ) ) can be empty.
 Definition 2 ( Invalid conditional transaction ) A conditional transaction T P i in TDB | P (where 1  X  i  X  sup ( P ) ), is an invalid conditional transaction if it falls into one of the following two cases: 1. | X P i | X  ( | LCF T P 2. | X P i | &gt;( | LCF T P Otherwise, T P i is called a valid conditional transaction.
 greater than the difference between its current longest covering frequent itemset and the length of the prefix itemset, whereas the second condition states that the number of conditional transactions which can be used to derive itemsets longer than LCF T P Lemma 2 ( Unpromising summary itemset generation ) If T P i is an invalid con-ditional transaction, there will be no frequent itemset derived by extending prefix P that T P i supports and is longer than LCF T P Proof Follows directly from Definition 2 . (i) If a transaction T P i is invalid be-cause of the first condition, it will not contain sufficient items in its conditional transaction to identify a longer covering itemset. (ii) If a transaction T P i is invalid because of the second condition, the conditional database will not contain a suf-ficiently large number of long conditional transactions to obtain an itemset that is longer than LCF T P summary itemsets for other valid conditional transactions w.r.t. prefix P ; thus, we cannot simply prune any invalid conditional transaction. Instead, we can safely prune some invalid conditional transactions according to the following Lemma. Lemma 3 ( Conditional transaction pruning ) An invalid conditional transac-tion, T P i , can be safely pruned, if it satisfies: Proof Consider an invalid conditional transaction T P i that satisfies Eq. ( 1 ). Then in order for a frequent itemset supported by the conditional transaction T P i and prefix P to replace the current longest covering frequent itemset of a valid condi-transaction. As a result, T P i can never contribute to the support of such an itemset and can be safely pruned from the conditional database.
 ing transactions satisfying Eq. ( 1 ) even when there exist some valid conditional transactions. However, in many cases, there may exist no valid conditional trans-actions, in this case the whole conditional database can be safely pruned. Lemma 4 ( Conditional database pruning ) Given the current prefix itemset P and its projected conditional database TDB | P , if each of its conditional transac-tions, T P i , is invalid, TDB | P can be safely pruned.
 Proof According to Lemma 2 , for any invalid conditional transaction, T P i ,we cannot generate any frequent itemsets longer than LCF T P This means that if each conditional transaction is invalid, we can no longer change the current status of the set of the currently maintained longest covering frequent itemsets w.r.t. prefix P ,  X  sup safely pruned.
 Example 3 Assume the prefix is c :4 (i.e., P = c ). From Table 1 we get that TDB | c = { 01, e , 05, def , 06, d , 07, f } ,andLCF 01 = ace :2, LCF 05 = ace :2, LCF 06 = bcd :2, and LCF 07 = acf :2. Conditional transactions 01, e , 06, d ,and 07, f fall into case 1 of Definition 2 , while 05, def falls into case 2 of Definition 2 ; thus, all the conditional transactions in TDB | c are invalid. According to Lemma 4 , conditional database TDB | c can be pruned.
 4.3 The algorithm By pushing deeply the search space pruning methods of Section 4.2 into the fre-quent itemset mining framework described in Section 4.1 ,wecanminethe sum-mary set as described in the SUMMARY algorithm shown in Algorithm 1. It first initializes the summary itemset to empty for each transaction (lines 01-02) and calls the Subroutine 1 (i.e., summary (  X  , TDB ) )tominethe summary set (line 03). Subroutine summary( pi , cd b) finds the set of local frequent items by scan-ning conditional database cdb once (line 04) and applies the search space pruning methods such as the item merging (line 05), conditional database pruning (lines 11 X 12), and conditional transaction pruning (line 13), updates the summary set information for conditional database cdb w.r.t. prefix itemset pi (lines 06 X 09), and grows the current prefix, builds the new conditional database, and recursively calls itself under the projection-based frequent itemset mining framework (lines 14 X 17). 4.4 Discussions and extensions 4.4.1 Ordering of local items In some FP-tree based frequent itemset mining algorithms [ 18 , 30 ], the support descending ordering scheme is popularly used to sort the local items w.r.t. a pre-fix, while as stated in Section 4.1 , SUMMARY adopts the support ascending or-dering scheme. We make this decision due to the following considerations. First, the support descending ordering generally helps in generating more compact FP-tree structures; thus, leads to more efficient memory usage and support count-ing. However, SUMMARY does not use the FP-tree structure to represent the conditional database; thus, it cannot get the benefit from the support descending ordering as the FP-tree based algorithms usually do. Second, a transaction may support multiple summary itemsets. By adopting the support ascending ordering scheme, SUMMARY prefers the summary itemset whose constituent items have relatively lower support. This heuristic is very important for the purpose of clus-the number of transactions in the database) are not differentiable in terms of dif-ferent classes. We will justify this heuristic with our experiments on some real datasets. 4.4.2 Mining K longest itemsets w.r.t. each transaction As mentioned above, a transaction may be covered by multiple summary itemsets in many cases. The SUMMARY algorithm described in Algorithm 1, only inserts for each transaction into the summary set one summary itemset, i.e., the one that was discovered first. We can revise SUMMARY to find K summary itemsets for each transaction which supports no less than K summary itemsets, 2 where K is a user input parameter. We denote the so-derived algorithm by SUMMARY-K. avoid the unnecessary repetition, here we mainly describe their major difference, that is, how to adapt Definition 2 and Lemma 3 to mine K summary itemsets, and will leave other minor revisions to the interested readers.
 Invalid conditional transaction for mining K summary itemsets. Similar to under consideration, sup ( P ) its support, and TDB | P ={ T P 1 , X P 1 , T P 2 , X P 2 , ..., T denote the number of the currently longest itemsets maintained by algorithm SUMMARY-K w.r.t. a conditional transaction T P i .
 Definition 3 ( Invalid conditional transaction for mining K summary itemsets )A conditional transaction T P i in TDB | P (where 1  X  i  X  sup ( P ) ), is an invalid condi-tional transaction if it falls into one of the following two cases: 1. | X P i | &lt;( | LCF T P 2. | X P i | &gt;( | LCF T P Otherwise, T P i is called a valid conditional transaction.
 Conditional transaction pruning for mining K summary itemsets. Similar to SUMMARY algorithm, an invalid conditional transaction can be used to mine summary itemsets for other valid conditional transactions and thus cannot be sim-ply pruned. As a result, we need to design new conditions in order to safely prune some invalid conditional transactions.
 Lemma 5 ( Conditional transaction pruning for mining K summary itemsets ) An invalid conditional transaction, T P i , can be safely pruned, if it satisfies: Proof Similar to the proof of Lemma 3 . 5 Experimental results In this section, we will first present a thorough experimental study to evaluate the effectiveness of the pruning methods, the overall scalability, and the efficiency of the SUMMARY algorithm. Then we will evaluate the SUMMARY-K algorithm by varying the K parameter. All the experiments except the efficiency test were performed on a 2.4 GHz Intel PC with 1 GB memory and Windows XP installed. In our experiments, we used some databases which were popularly used in eval-uating various frequent itemset mining algorithms [ 16 , 30 , 35 ], such as connect , chess , pumsb* , mushroom ,and gazelle , and some categorical databases obtained from the UCI Machine Learning repository, such as SPECT , Letter Recognition , andsoon. 5.1 Effectiveness of the pruning methods We first evaluated the effectiveness of the pruning methods by comparing SUMMARY itself with or without the conditional database and transaction prun-ing methods. Figure 1 a shows that the SUMMARY algorithm with pruning can be over an order of magnitude faster than the one without pruning for the mushroom database. This illustrates that the pruning methods proposed in this paper are very effective in reducing search space. 5.2 Scalability We also tested the algorithm scalability using the IBM synthetic database series T10I4Dx by setting the average transaction length at 10 and changing the number of transactions from 200 K to 1000 K. We ran SUMMARY at two different mini-mum relative supports of 0.2% and 1%. Figure 1 b shows that SUMMARY scales very well against the database size. 5.3 Efficiency frequent closed itemsets, from which the summary set can be further identified. Our comparison with FPclose [ 17 ], one of the most recently developed efficient tical when the minimum support is low. As we will discuss in Section 6 ,such low minimum support values are beneficial for clustering applications. The ef-ficiency comparison was performed on a 1.8 GHz Linux machine with 1 GB memory by varying the absolute support threshold and turning off the output of FPclose. The experiments for all the databases we used show consistent re-sults. Due to limited space, we only report the results for databases connect and gazelle .
 that SUMMARY scales very well w.r.t. the support threshold, and for connect database, it even runs faster at low support value of 128 than at high sup-port value of 512. This is because SUMMARY usually mines longer itemsets at lower support, which makes the pruning methods more effective in remov-ing some short transactions and conditional databases. As the FP-tree struc-ture adopted by FPclose is very effective in condensing dense databases, at high support, FPclose is faster than SUMMARY for dense databases like con-nect , but once we continue to lower the support, it can be orders of magnitude slower. While for sparse databases like gazelle , FPclose can be several times slower. 5.4 Test of SUMMARY-K algorithm We also evaluated the efficiency of the SUMMARY-K algorithm by varying the value of the K parameter from 1 to 8 against FPclose (When K equals 1, SUMMARY-K is the same as SUMMARY). Figure 3 shows the comparison result for database spect .FromFig.3awecanseethatwhen K is larger, SUMMARY-K is a little slower, but it is always faster than FPclose. Figure 3 b compares the num-ber of patterns mined by SUMMARY-K and FPclose, which shows that FPclose mines orders of magnitude more patterns than SUMMARY-K. The high efficiency of the SUMMARY-K algorithm illustrates the effectiveness of the pruning meth-ods newly proposed in this paper from another angle. 6 Application X  summary set based clustering 6.1 Clustering based on summary set One important application of the SUMMARY algorithm is to cluster the categor-ical data by treating each summary itemset as a cluster description and grouping the transactions with the same cluster description into a cluster. In SUMMARY, we adopt a prefix tree structure to facilitate this task, which has been used exten-sively in performing different data mining tasks [ 18 , 30 ]. For each transaction, T i , if its summary itemset SI T i is not empty, we sort the items in SI T i in lexicographic order and insert it into the prefix tree. The tree node corresponding to the last item of the sorted summary itemset represents a cluster, to which the transaction T i belongs.
 Example 4 The summary itemsets for the transactions in our running example are SI acf ,SI 08 = ef ,andSI 09 = bd . If we insert these summary itemsets into the prefix tree in sequence, we can get seven clusters with cluster descriptions ace , bde , df , ef , bcd , acf ,and bd , as shown in Fig. 4 . From Fig. 4 we see that transactions 01 and 05 are grouped into cluster 01, transactions 04 and 08 are grouped into cluster 04, while each of the other transactions forms a separate cluster of their own. Note that a non-leaf node summary itemset in the prefix tree represents a non-maximal frequent itemset in the sense that one of its proper supersets must be frequent. For example, summary itemset bd is non-maximal, because summary itemset bd e is a proper superset of bd . In this case, we have an alternative clustering option: merge the non-leaf node clusters with their corresponding leaf node clusters to form larger clusters. In Fig. 4 , we can merge cluster 07 with cluster 02 to form a cluster. 6.2 Clustering evaluation We have used several categorical databases to evaluate the clustering quality of the SUMMARY algorithm, including mushroom , SPECT , Letter Recognition , and Congressional Voting , which all contain class labels and are available at http: // www.ics.uci.edu /  X  xmlearn / . We did not use the class labels in mining the summary set and clustering, instead, we only used them to evaluate the clustering accuracy, which is defined by the number of correctly clustered instances (i.e., the instances with dominant class labels in the computed clusters) as a percent-age of the database size. SUMMARY runs very fast and can achieve very good clustering accuracy for these databases, especially when the minimum support is low. Due to limited space, we only show results for mushroom and Congres-sional Voting databases, which have been widely used in the previous studies [ 14 , 32 , 34 ].
 mushrooms. It has 8124 instances and two classes: poisonous and edible. Table 2 shows the clustering results for this database, including the minimum support used in the tests, the number of clusters found by SUMMARY, the number of misclus-tered instances , clustering accuracy , compression ratio ,and runtime (in seconds) for both the summary set discovery and clustering. The compression ratio is de-fined as the total number of items in the database divided by the total number of items in the summary set . We can see that SUMMARY has a clustering accuracy higher than 97% and a runtime less than 0.85 seconds for a wide range of support thresholds. At support of 25, it can even achieve a 100% accuracy. The MineClus algorithm is one of the most recently developed clustering algorithm for this type of databases [ 34 ]. Its reported clustering solution for this database finds 20 clusters with an accuracy 96.41% and in the meantime declares 0.59% of the instances as outliers, which means it misclusters about 290 instances and treats about another 48 instances as outliers. Compared to this algorithm, SUMMARY is very compet-itive in considering both of its high efficiency and clustering accuracy. In addition, the high compression ratios demonstrate that the summary set can be used as a concise summary of the original database (Note in each case of Table 2 ,the sum-mary set covers each instance of the original database, which means there is no outlier in our solution).
 sional Voting Records and has two class labels: Republican and Democrat. In our experiments, we removed four outlier instances whose most attribute values are missing and used the left 431 instances. Table 3 shows the clustering solution of SUMMARY at a minimum support of 245, at which point the clusters produced by SUMMARY covers the entire database (while a minimum support higher than 245 will make SUMMARY miss some instances), and SUMMARY only uses 0.001 s to find the six clusters with an accuracy higher than 95% and a compression ratio higher than 1164. Even we simply merge the four small clusters with the two large clusters in order to get exact two clusters, the accuracy is still higher than 93% in the worst case (e.g., clusters 3 and 5 are merged into cluster 1, and clusters 4 and 6 are merged into cluster 2), and is much better than the reported accuracy, 86.67%, of the MineClus algorithm [ 34 ]. 6.3 Comparison with ROCK ROCK is one of the most well-known categorical clustering algorithms [ 14 ]. Following we will compare SUMMARY with ROCK. For mushroom database, ROCK generates 21 clusters as shown in Table 4 . In the table we also list the 21 largest clusters generated by SUMMARY at absolute support 1400. From Table 4 , we see that although SUMMARY and ROCK are two different clustering algo-rithms, their clustering solutions are similar in many aspects. For example, each of these two algorithms only generates one impure cluster (i.e., the 21st cluster), and their top 3 largest clusters (i.e., the first, the second, and the third one) have the same size and class distribution.
 ilarly, as ROCK generates exact two clusters, here we only list the two largest clusters for SUMMARY at support 245. We see that the two clusters generated by SUMMARY contains more tuples but they are purer than the corresponding ones of ROCK. As a result, SUMMARY has better clustering solution than ROCK for this database. 6.4 Evaluation of item ordering scheme In Section 4.4.1 we analyzed why SUMMARY adopts the support ascending or-dering scheme instead of the support descending ordering for sorting the local items. Our experiments demonstrate that the support ascending ordering scheme usually leads to better clustering solution than the support descending ordering. Ta b l e 6 shows the comparison result for Congressional Voting database at abso-lute support of 245. The two different ordering schemes generate the same number of clusters, but it is very clear that the clustering computed with the support as-cending ordering scheme has better quality. 7 Discussions and conclusion In this paper we proposed to mine the summary set that can maximally cover the input database. Each summary itemset can be treated as a distinct cluster description and the transactions with the same description can be grouped to-gether to form a cluster. Because the summary itemset of a cluster is one of the longest frequent itemsets that is common among the corresponding transactions of the same cluster, it can approximately maximize the intra-cluster similarity, while different clusters are dissimilar with each other because they support dis-tinct summary itemsets . In addition, we require each summary itemset be frequent in order to make sure it is statistically significant. Directly mining the summary set also enabled us to design an efficient algorithm, SUMMARY. By exploring some properties of the summary set , we developed two novel pruning methods, which significantly reduce the search space. Our performance study showed that SUMMARY runs very fast even when the minimum support is extremely low and the summary set is very effective in clustering categorical data. In addition, we also evaluated SUMMARY-K, a variant of SUMMARY, which mines K sum-mary itemsets for each transaction. In future, we plan to explore how to choose the one among the summary itemsets supported by a transaction which can reduce the number of clusters while achieving a high clustering accuracy.
 References
