 It has been recognized that, when an information retrieval (IR) system achieves improvement in mean retrieval effec-tiveness (e.g. mean average precision (MAP)) over all the queries, the performance (e.g., average precision (AP)) of some individual queries could be hurt, resulting in retrieval instability. Some stability/robustness metrics have been proposed. However, they are often defined separately from the mean effectiveness metric. Consequently, there is a lack of a unified formulation of effectiveness, stability and over-all retrieval quality (considering both). In this paper, we present a unified formulation based on the bias-variance de-composition. Correspondingly, a novel evaluation methodol-ogy is developed to evaluate the effectiveness and stability in an integrated manner. A case study applying the proposed methodology to evaluation of query language modeling illus-trates the usefulness and analytical power of our approach. Category and Subject Descriptors: H.3.3 [Information Search and Retrieval] General Terms: Theory, Measurement, Performance Keywords: Bias-Variance, Decomposition, Effectiveness, Stability, Robustness, Evaluation
Recently, it has been noticed that when we are trying to improve the mean retrieval effectiveness (e.g., measured by MAP [3]) over all queries, the stability of performance across different individual queries could be hurt. For exam-ple, compared with a baseline (using the original query in the first round retrieval), query expansion based on pseudo relevance feedback can generally achieve better MAP, but it can hurt the performance for some individual queries [2].
In the literature, various stability (or robustness) mea-sures, e.g., R  X  Loss [2], Robustness Index [2], and &lt;Init have been proposed. R  X  Loss computes the averaged net loss of relevant documents (due to query expansion failure) in the retrieved documents. &lt; Init calculates the percent-age of queries for which the retrieval performance of a query model is worse than that of the original query model. The robustness index is defined as RI ( Q )=( n +  X  n  X  ) / | Q | where n + is the number of queries for which the perfor-mance is improved, n  X  is the number of queries hurt, over theoriginalmodel,and | Q | is the number of all queries.
These existing measures have some limitations, as shown by the example in Table 1. Let us consider model A as the original query model, as well as regard B and C as two query expansion models. The example shows that A is less effective (with a lower MAP), but more stable (with a lower &lt;Init ) than B. In this case, we can not judge which model (A or B) has the better overall performance (considering both effectiveness and stability). For comparison of B and C, both &lt; Init and robustness index can not distinguish the stability between them. The MAPs of B and C are also the same. Intuitively, B would seem more stable due to the less variance of its AP values for different queries. However, the above stability metrics do not take into account such variance (denoted as Var in Table 1). For example, one way of computing the variance of AP for model A can be [(0 . 3  X  0 . 2) 2 +(0 . 1  X  0 . 2) 2 ] / 2=0 . 01, which calculates the derivation of AP from its MAP (i.e., 0.2). In Table 1, Var distinguishes the models A, B and C: the smaller Var can indicate the better retrieval stability.

Now let us change the AP values of C to 0.32 and 0.11 (for q and q 2 respectively), then its MAP, &lt;Init and Robust-ness Index become 0.215, 0 and 1 respectively, suggesting C is more stable but less effective than B. We are not sure which one (B or C) is overall better when considering both effectiveness and stability. This is mainly because the ex-isting stability metrics are often defined separately from the effectiveness metric (MAP). There is a lack of a unified for-mulation of the effectiveness and stability to allow them to be looked at in an integrated manner.

In this paper, we present a formulation based on a fun-damental concept in Statistics, namely the bias-variance de-composition, to tackle the problem. In a nutshell, we view the unsatisfactory overall performance as one total error, which can be decomposed into bias and variance of the AP values of different queries. The bias captures the expected difference of the APs from their upper bounds (the best AP obtained by model T in Table 1). The variance has been illustrated earlier. The detailed formulation will be given in the next section. Briefly speaking, the smaller bias or variance reflects the better retrieval effectiveness or stabil-ity, respectively. The total error (denoted as Bias 2 + Var can reflect the overall quality of a model. As an illustration, in Table 1, Bias 2 + Var indicates that (1) the target model T has the best overall retrieval quality; (2) the model B is more desirable than C; (3) in comparison with the baseline A, both query expansion models B and C reduce the bias but enlarge the variance as well as the total error.
Recently, Wang and Zhu [7] studied the mean-variance analysis regarding the document relevance scores. The per-topic variance of AP (of each query) was investigated in [6]. In this paper, we explore the variance of AP (across queries) and the bias-variance decomposition.
In Statistics [1], bias and variance, which are measure-ments of estimation quality in different aspects, are decom-posed from the expected squared error of the estimated val-ues with respect to the target value. In this paper, we for-mulate the bias-variance decomposition in the IR evaluation scenario. Let us first assume there exists a target model T that can have an upper-bound performance for each query 1
We can let AP be a random variable over queries. Bias (AP) essentially calculates the average difference between the tar-get AP (i.e. the AP of the target model T, denoted AP T ) and the actual AP of a retrieval model over all different queries. Specifically, where the expectation E (  X  )isoverasetofqueriesthatare assumed to be uniformly distributed. E (AP T ) corresponds to the target MAP (i.e., the MAP of the target model T, denoted MAP T ), and E (AP) corresponds to the MAP of the retrieval model under evaluation.

It turns out that the smaller bias indicates the better mean retrieval effectiveness over queries. In Table 1, the bias for Eq. 1, it can be equivalently calculated as 0.45-0.2 = 0.25, where 0.45 is the target MAP and 0.2 is the MAP of A.
Similarly, the variance of the APs for different queries is defined as: The smaller Var (AP) indicates the better retrieval stability of AP across queries. We will relax this constraint in Sections 2.4.
 Table 2: Examples of Additional Bias-Variance (  X  ) We now add up the bias and variance, yielding:
It turns out E (AP  X  MAP T ) 2 is not exactly the expected squared error E (AP  X  AP T ) 2 . However, we will show later that E (AP  X  MAP T ) 2 is a simplified version of an expected squared error E (AP  X  1) 2 in Section 2.3. This error formula-tion will help us understand the difference between different bias-variance decompositions (see Section 2.3).
To clarify the above observation, let us first look at the decomposition of the expected squared error E (AP  X  AP T ) We need to define another random variable As an illustration, for the model A in Table 2,  X  is 0.4 (0.7-0.3) and 0.1 (0.2-0.1), for q 1 and q 2 respectively.
Let  X  T be the target value of  X  , which is indeed 0, since for model T,  X  =AP T  X  AP T =0. We need  X  T in the following analysis, since the target value is an important component in the standard definition of bias. We define where E (  X  ) is the averaged  X  over all queries. Since  X  Bias (  X  )equalsto E (  X  )= E (AP T  X  AP), which is Bias (AP).
The variance based on  X  , denoted Var (  X  ), computes the variance of the difference between the AP (of the test model) and the AP target across all queries. Formally, AsshowninTable2, Var (  X  ) of B and C are smaller than Var (  X  ) of A, indicating that B and C are more stable than A. This observation is different from &lt;Init and Var (AP) which shows that B and C are less stable than A in Table 1. Now, we derive the decomposition of E (AP  X  AP T ) 2 : The above equations show the expected squared error E (AP  X  AP T ) 2 can be exactly decomposed into bias and variance on  X  . The expected squared error E (AP  X  AP T ) always computes the error of the target model as zero, no matter whether or not the target model still has room for improvement. It is likely that the current best performance for some queries can be further advanced in the near future.
In order to further investigate two aforementioned bias-variance decompositions, we set 1 (the maximum AP) as the upper-bound AP for each query. We can have an expected squared error E (AP  X  1) 2 and its decomposition as: = E (AP  X  AP T +AP T  X  1) 2 = E (AP  X  AP T ) 2 + E (AP T  X  1) 2 +2 E (AP  X  AP T )(AP T which shows that E (AP  X  AP T ) 2 is only one part of E (AP 1) 2 , and the target model T still has an error E (AP suggesting there is still a room for improvement. We can also decompose E (AP  X  1) 2 as: It turns out the variance parts in Eq. 3 and Eq. 9 are the same (i.e., Var (AP)). The term (1  X  MAP) 2 in Eq. 9 has thesametrendas Bias 2 (AP) (i.e., (MAP T  X  MAP) 2 ), where MAP T is the upper bound of the MAP. The above observa-tions show that the decomposition in Eq. 3 can be considered as a simplified version of the decomposition in Eq. 9.
First, the bias-variance of  X  requires a target AP for ev-ery query. On the other hand, the bias-variance of AP (in Eq. 3) can be used when only a target MAP (i.e., MAP T )is given. Specifically, two biases (i.e., Bias (AP) and Bias are equivalent and both can be computed by MAP T  X  MAP. Regarding variance, the variance of  X  requires AP T for each query(seeEq.6andEq.4),whilethevarianceofAPcan be calculated without knowing AP T (see Eq. 2). Thus, the bias-variance based on AP is more flexible for practical use. Second, the bias-variance of  X  is an exact decomposition of E (AP  X  AP T ) 2 and it always regards the target model as a zero-error model. However, the decomposition of E (AP  X  1) in Eq. 8 shows that the target model can still has error. On the other hand, under the bias-variance of AP, the variance for the target model still exists, indicating that although we can assume a target model as an upper-bound at a certain stage, it can be further improved.
To carry out IR evaluation based on the proposed the bias-variance decomposition methods, one needs to choose the upper-bound settings (i.e., the target model T). There can be a number of ways. First, the upper-bound AP for each query can be simply set as 1, which corresponds to a perfect target model. Second, the upper-bound AP for each query can be obtained based on the best AP among evalu-ated retrieval models in the historic TREC results. Third, one can simply set an upper-bound MAP (i.e., MAP T ).
The first and second upper-bound settings correspond to a virtual target model. For example, in the second setting, the target model collects the best AP among many retrieval models. The third setting can correspond to a real target model (see the model settings in the experiments.) With the first and second upper-bound settings, either of bias-variance metrics (based on AP or  X  ) can be adopted. The third setting is suitable for bias-variance of AP (see Sec-tion 2.4). Once an upper-bound setting and a variable (AP or  X  ) are chosen, we can calculate bias-variance figures for a series of models and then see which model can have the minimum bias, or minimum variance, or the sum of them (i.e., the total error). We can also get an overview on the trend of bias and variance over parameters.
We use the query modeling as an example of bias-variance evaluation. Due to the page limit, we only report the eval-uation results on two standard TREC collections, i.e., WSJ (87-92) over queries 151-200 and ROBUST 2004 over queries 601-700. The title field of the queries is used. Lemur 4.7 [5] is used for indexing and retrieval. The top n =30ranked documents in the initial ranking by the original query model are selected as the pseudo-relevance feedback (PRF) docu-ments. The number of expanded terms is fixed as 100. 1000 documents are retrieved by the KL-divergence model [5].
Model Settings We customize a number of query mod-els according to three factors (i.e., model complexity, model combination, ground-truth data) that can generally influ-ence the bias-variance tradeoff [1]. The first model is the original query model which is a maximum likelihood esti-mation of original query. We also evaluate an expanded query model RM (i.e., RM1 in [4]) which is generated from feedback documents D . Compared with the original query model, the expanded query model is more complex due to the fact that it has more terms and additional assumption-s (e.g., assuming that all feedback documents are relevan-t). The above two models can be combined, leading to a combined query model (also called as RM3). Let  X  be the combination coefficient which is in the range (0,1). When  X  gets close to 0, the combined model is towards the R-M, and otherwise towards the original model. In RM, we can gradually remove a percentage (denoted by r n )ofnon-relevant documents D N along the initial ranking of feedback documents, based on the available relevance judgements (as ground-truth) [8]. We finally derive a target model which are generated by keeping only relevant documents D R in D which gives the best MAP over the aforementioned query models. In Eq. 10, the document weights are set as uni-form (different from the weights in RM), since the relevant judgements of relevant documents are the same (i.e., 1).
Bias-Variance Metrics Setting We report the bias-variance on AP in our evaluation, since the target query model used in our evaluation only guarantees the upper-bound MAP. According to the discussion in Sections 2.4 and 2.5, the bias-variance of AP is more suitable.
Evaluation Results We first look at the bias-variance results in Figure 1. Recall that the smaller bias or variance (based on AP) corresponds to the better retrieval effective-ness or stability, respectively. Figure 1(a) shows that on WSJ8792, the original query model has the smallest vari-ance (the highest stability), but the largest bias (the lowest MAP). This is a tradeoff between bias and variance. One bias and the y -axis shows the variance. reason in Statistics language is that the expanded models are all complex than the original one. The bias of the expanded model RM is much smaller, while its variance is much larg-er, than the original model. The different variants of the combined model (with  X  in [0.1, 0.9] with increment 0.1) are lying between the expanded and original models. We can also see that 2 (out of 9) combined models (when  X  =0.1 and 0.2) has smaller bias and smaller variance than the expand-ed query model RM. This suggests that the combined query model with good parameters can achieve better effectiveness and stability over RM. Among the combined query model-s, bias and variance are negatively correlated, indicating a bias-variance tradeoff occurs. On the other hand, if we re-move the non-relevant documents D N (with r n in [0.1, 1] with increment 0.1) in RM, the bias and variance are posi-tively correlated, and 9 (out of 10) models (when r n =0.2 to 1) can reduce both the bias and variance over RM. The target model has zero bias, although there is a variance of its AP across different queries. On ROBUST2004, the re-sults are similar. The differences are that: 1) by removing non-relevant documents, only 6 (out of 10) models (when r n = 0.5 to 1) reduce both bias and variance over RM; 2) there are 3 (out of 9) (when  X  = 0.1 to 0.3) variants of the combined model for which both bias and variance can be smaller than those of RM.

Figure 2 shows Bias 2 + Var results for all the concerned models. On both collections, the target method achieves the smallest Bias 2 + Var and the original query model achieves the largest Bias 2 + Var .Recallthat Bias 2 + Var represents the total error and the smaller error can reflect the better overall quality (or performance), by considering both the effectiveness and stability in one single criterion ( Bias 2 Var ). The results also suggest that the model combination and the relevance judgements are helpful to reduce Bias 2 Var over the original model and the expanded model RM.
In this paper, a novel evaluation strategy based on the bias-variance decomposition has been presented. We for-mulate two forms of the bias-variance decomposition and theoretically compare them. We also use query expansion as an example to demonstrate the use of the bias-variance for IR evaluation and analysis, in terms of bias, variance or sum of them. In the future, we will further explore the other upper-bound settings and variables discussed in Section 2.5, Figure 2: Results of Bias 2 and Bias 2 + Var of AP of all the concerned query models. The x -axis shows the squared bias and the y -axis shows the Bias 2 + Var . test with more retrieval models and explore the deep insights behind the bias-variance trends.
This work is supported in part by the Chinese Nation-al Program on Key Basic Rese arch Project (973 Program, grant No. 2013CB329304), the Na tural Scien ce Founda tion of China (grant No. 61272265, 61070044), and EU X  X  FP7 QONTEXT project (grant No. 247590).
