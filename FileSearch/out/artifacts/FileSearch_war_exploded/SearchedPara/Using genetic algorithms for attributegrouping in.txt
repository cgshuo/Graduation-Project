 Barcelona Supercomputing Center  X  BSC, Technical University of Catalonia  X  BarcelonaTECH, Catalonia, Spain CA Technologies, CA Labs, Barcelona, Spain 1. Introduction
The necessity for keeping a data set available for data mining while preserving some reasonable de-gree of privacy has given rise to statistical disclosure control techniques [10,12,28]. A common way of ensuring privacy without losing information is by ensuring that the released data is k-anonymous [24, Microaggregation [3,23] is one of the most common methods used to obtain k -anonymity for numerical data: groups of k nearest records, which are mapped as points in a multidimensional space defined by the attribute columns, are identified and substituted by their centroid.

Nowadays, the increase of information stored by most enterprises and organizations usually entails the increase of the complexity of data schemas [15]. Among other aspects, this typically involves an increase on the number of attributes per record, making the number of dimensions used to map the elements in the data set larger. The more dimensions the microaggregation problem deals with, the further the points mapped in this multidimensional space. Thus, microaggregating points that are too far implies that these are substituted by centroids which are too distant, which usually implies a significant loss of information [1].

In order to diminish the effect of high dimensionality, data sets are usually partitioned into disjoint attribute sets, so that each partition only preserves the values of certain attributes for each record. Af-terwards, microaggregation is used in each partition separately. This process is known as multivariate microaggregation . While this reduces the information loss, it prevents from guaranteeing k -anonymity: two records that are clustered together using their values on a subset of their attributes might not be clustered together if another set of attributes are used as a criterium [17]. With this, we cannot ensure that these two records will not be undistinguishable anymore. Nevertheless, partitioning techniques are commonly used in order to preserve data utility.

As studied in [18], when protecting a data set using multivariate microaggregation, the way in which the data is split to form groups is highly relevant with regard to the degree of privacy achieved. That is, assuming a data set containing subsets of correlated attributes, if we group correlated attributes together and groups are, therefore, non-correlated, applying microaggregation to each partition will probably pre-serve a high information utility, while it will probably imply a complete loss of the k -anonymity property. On the contrary, grouping non-correlated attributes, may preserve k -anonymity better, at the cost of los-ing information utility. Achieving the best trade-off between information loss and entity disclosure risk by choosing the right attribute partitioning is still an open problem.

In this paper, we present a novel approach in order to decide the optimal, or near-optimal, attribute partitioning. Our technique is based on the use of genetic algorithms in order to explore the vast space of all possible attribute groupings. We provide a set of new mutation operations that, based on the mapping of a possible attribute grouping into a chromosome, allow exploring the specific search space presented in this scenario. We present a comprehensive analysis of our operations and show that they can overcome previously used ad-hoc attribute partitioning techniques, using well-known data sets.

The remainder of this paper is organized as follows: Section 2 we introduce some related work about statistical disclosure control in general and microaggregation anonymization in particular. Later, in Sec-tion 3 we describe GOMM, our novel proposal for attribute grouping using genetic algorithms. After-wards, Section 4 presents a complete analysis of GOMM operations. Section 5 performs a large number of experiments with real datasets to analyze GOMM performance. Finally, the paper finishes with some conclusions and future work. 2. Related work
In this section, we describe the typical anonymization scenario as well as some basic concepts about microaggregation. 2.1. Problem statement
A data set R is a collection of records where each record r represents a point in a multidimen-sional space defined by the number of attributes. The attributes a i can be classified in three non-disjoint categories: identifiers which unambiguously identify the individual (e.g. the passport number), quasi-identifiers which can identify the individual when some of those attributes are combined (e.g. age or postal code) and confidential attributes which contain sensitive information about the individual (e.g. salary).

When considering this classification, a data set R is defined as R = id || a nc || a c ,where id are the Normally, before releasing a data set R with confidential attributes, a protection method  X  is applied, leading to a protected data set R . Indeed, we assume the following typical scenario depicted in Fig. 1: attributes a c are not modified, and so we have a c = a c ; (iii) the protection method itself is applied to non-confidential quasi-identifier attributes, in order to preserve the privacy of the individuals whose to have precise information on confidential data without revealing to whom the confidential data belongs.
In this scenario, as shown in Fig. 2, an intruder might try to re-identify individuals by obtaining By applying record linkage between the protected attributes ( a nc ) and the same attributes obtained from together with their confidential data ( a c ). This is what protection methods try to prevent. 2.2. Microaggregation The goal of any microaggregation method is to minimize the total Sum of the Square Error is the centroid of n i . The restriction is | n i | k ,forall i =1 ,...,n .
As we have explained before, when the number of attributes per record is large, multivariate mi-croaggregation is used for reducing the information loss at the cost of increasing the disclosure risk. However, multivariate microaggregation has two main drawbacks. On the hand one, finding the optimal microaggregation, i.e. the optimal clusters configuration, is a NP-hard problem [21]. This problem has been widely studied, and a large variety of heuristic algorithms exist. As this problem is out of the scope of this paper, we use MDAV (described later on), one of the most well-known and flexible multivariate microaggregation algorithms. On the other hand, grouping attributes properly is a problem as difficult as microaggregation itself. Since the search space grows exponentially with respect to the number of at-tributes, finding the most appropriate attribute grouping configuration is far from trivial. Opposite to mi-croaggregation, this problem has been disregarded in the literature assuming that attributes are grouped considering some background knowledge, such as attribute correlations when we are interested on min-imizing the information loss, or considering that the intruder only has access to a subset of attributes, then such attributes are grouped together.

In [17,18] two approaches were presented for attribute selection, both based on the correlations among attributes. The first approach is based on cluster in the same group attributes that are highly correlated, minimizing in this way the intra-cluster distance between the centroid and the records. This approach has a low information loss but a high disclosure risk as in the case of univariate microaggregation. The second approach is based on clustering highly correlated attributes in different groups. The goal of this second approach is to increase the resulting anonymity. The rationale of this approach is the following one: If two records r i and r j are in the same cluster for some blocks, this means that the first attribute values of these records are more or less close to each other, and the same for the second attribute of the block, etc. Then, when we consider another block, if the j -th attribute of this new block is (highly) correlated with the j -th attribute of the latter block, records r i and r j will probably be close to each other as well, with respect to the attributes in the second block. Therefore, with some non-negligible a similar level of privacy to that obtained by the basic microaggregation. Of course, the information loss of this latter approach is larger than the former one.
 MDAV algorithm. The MDAV (Maximum Distance to Average Vector) algorithm [23] is an heuristic algorithm for clustering records in a data set R , so that each cluster is constrained to contain at least k records. It works as follows. Firstly, given n records in the data set to be protected, it computes the  X  r , and forms a cluster around it (this cluster contains s together with the k  X  1 closest records to it). Then, a new cluster is formed around the most distant record to s in the same way. When both clusters are formed, all the records belonging to such clusters are removed. This process is repeated until all the so it might contain between k and 2 k  X  1 records. 3. GOMM: Genetic optimizer for multivariate microaggregation
In this section we present GOMM, the Genetic Optimizer for Multivariate Microaggregation, whose goal is to find the appropriate attribute partitioning in order to obtain a protected data set which guaran-tees both utility and privacy. With this purpose, we pr opose a way to encode any possible solution to the problem along with some genetic operations to manipulate them. In addition, we propose an efficient measure to evaluate the quality of these solutions representations.

The genetic optimizers belong to the class of Evolutionary Algorithms. There are several necessary aspects to make the use of this type of algorithm suitable: (i) the search space must be complex and it must not be well-known, (ii) it must be possible to find a suitable encoding to represent the solutions of function. In the multivariate microaggregation case, it is not clear whether there is a proper way to group the different attributes in the data set and the heuristics proposed in the literature have been shown to be suboptimal in many scenarios.

The procedure of any evolutionary algorithm described in Algorithm 1 works as follows: in general terms, a collection of instances of the alternative solutions to the problem to be optimized suffers a set of transformations to generate new instances. These are evaluated through a fitness function and they are discarded if necessary, so that after several iterations the elements that have survived to the selection process represent near-optimal solutions to the problem. This collection of elements is usually called population , the instances are called chromosomes and each iteration of the algorithm is called generation . The initial population is u sually generated at random from all the possible solutions to the problem. Every generation, new members are created by using crossover operations , which combine properties of the existing members of the population, and mutation operations , which introduce new properties to the population by transforming a single individual chosen at random. In order to keep the number of members constant, a selection method is used, which in general chooses the best-fitted members to survive for the next generation. This process is repeated iteratively until a stop condition is found and the best member of the current population is taken as the solution of the problem.
Finding the optimal grouping configuration to cluster the different attributes in multivariate microag-gregation can be seen as a grouping problem, and genetic algorithms have been proven to be a good solution since a broad variety of measures for scoring the quality of an anonymization method have been
Algorithm 1: Evolutionary Algorithm basic pseudocode begin 2 Population P , P 1 , P 2 3 while stop criterion is not met do 4 P 1  X  applyCrossoverOperations( P ) 5 P 2  X  applyCrossoverOperations( P ) 6 P  X  P  X  P 1  X  P 2 7 P 2  X  applySelectionOperation( P ) end End proposed in the literature. Following, we explain in more detail the different components which conform GOMM. 3.1. Encoding
First, we propose a way to map an attribute grouping in a chromosome. GOMM uses the same en-coding scheme as the Grouping Genetic Algorithm (GGA) [9]. Every chromosome c contains an object part c op , where each gene represents the membership of the corresponding attribute in a group, and a group part c gp , which contains the groups. For example, the chromosome shown in Fig. 3(a) represents a solution where the attributes 1, 2 and 5 are grouped together and attributes 3 and 4 are in another group, with the group part written after the thick line. The goal of this encoding is to facilitate the work of the operators, which treat groups rather than objects.

It is important to remark that this encoding allows the presence of clones , which are chromosomes that represent the same attribute grouping and have the same associated fitness. For instance, Fig. 3(b) represents a chromosome which maps exactly the same solution as the chromosome shown in Fig. 3(a). 3.2. The fitness function
After encoding a solution in a chromosome c i , a genetic algorithm needs a fitness function F ( c i ) to evaluate its quality. Chromosomes in our case represe nt attribute groupings that will generate a pro-tected data set after applying microaggregation with the MDAV algorithm. It is well-know that a good anonymization method is the one that minimizes the trade-off between information loss and disclosure risk. Usually, this trade-off is computed as the arithmetic mean of both measures to give the same im-portance to both ( IL and DR ). Therefore, we define our fitness function as follows where IL stands for information loss measure and DR stands for disclosure risk measure. This function is called score and it was defined in [5] and it has been used in other evolutionary algorithms, as for instance in [11,13]. However, because of the frequent use of the fitness function during the evolutionary process, we cannot use such a high computationally intensive function. Because of this, we will use a lighter version of the score for reducing the complexity of the original measures in our fitness function F ( c 3.2.1. Information loss
Several proposals have been used in the literature in order to calculate the information loss. Depending on the alternatives, they take into account several general parameters of the data distribution such as the average or variance vectors, covariance or correlation matrices, etc. Since the goal of microaggregation is to minimize the SSE , which is a specific information loss measure for any k -anonymity model, we have used it as the IL measure. However, the SSE itself is not suitable for the F ( c i ) formulation since it is not upper bounded. The most common way to normalize the SSE range into the [0 , 1] interval is to works [4,7]. The SST is defined as where  X  r is the centroid of all the original records. Note that, the SST can be computed only once during the execution since its value is constant. Then, the IL component in the fitness function of our genetic algorithm is computed as in this way, IL rangs between 0 and 100. 3.2.2. Disclosure risk
In order to compute DR , usually two approaches are considered. The first one is the entity disclosure risk, which considers the scenario where intruders have access to an external data set containing a subset of non-confidential quasi-identifiers a nc of the original records in R . Then, they try to link them with the corresponding protected record r  X  R . Usually, the intruders apply record linkage methods [19, 26] for this purpose. Basically, there are two families of such methods, on the one hand, those based on (in)conditional probabilities and, on the other hand, those based on distances calculations. The former (probability based family) is too inefficient to be used inside a cost function of a genetic algorithm. For this reason, we only use distance-based methods as it was done in [20] for similar reasons. This is not a problem since, as it was shown in [6], distance-based methods outperform probabilistic ones for numerical attributes. Generally, it is assumed that the intruder has several different sets of non-confidential quasi-identifiers and the risk is computed as the average risk of all those sets. However, in order to reduce the execution time of the record linkage process, we only consider the worst scenario, i.e. when the intruder has access to all quasi-identifiers a nc of all records r stored in R .Again,asweare interested in a value fitted between [0 , 100] , Distance Linkage disclosure ( DLD ) is computed as where links is the total number of correct links achieved and | R | is the number of records of the data set R . In this work, we use the Euclidean distance as in [22] for DLD computation.

The second considered disclosure risk scenario is the Interval Disclosure Risk (ID) which is the av-erage percentage of protected values falling into an int erval around their corresponding original values. This measure was introduced in [2]. Usually, the interval length is a user parameter. In our case, the interval is defined as [( r ij  X  r ij  X  10%) , ( r ij + r ij  X  10%)] as in [2].
 Finally, the overall DR is computed as 3.3. Genetic operators
Following, we describe the crossover operation and propose a new set of mutation operations and a selection operation to solve the attribute grouping problem for multivariate microaggregation. 3.3.1. Crossover
The aim of the crossover operator  X  ( c p 1 ,c p 2 ) is to generate new members by combinig properties from different chromosomes in the current population. Two parent chromosomes c p 1 and c p 2 are selected randomly from the population and two new child chromosomes c c 1 and c c 2 are produced containing information from both parents.
 Figure 4 shows the three steps of our crossover operation  X  ( c p 1 ,c p 2 ) , which is an adaptation of the Grouping Genetic Algorithm (GGA) crossover [9]. First of all, two crossing sites are selected from the group part of both parents (1). A crossing site c p gp is a subsection of the group part of a parent chromosome c p gp , defined by a lower bound and an upper bound chosen at random. For example, the crossing site of the parent chromosome c p 1 in Fig. 3 is c p 1 gp = { B,C } , with a lower bound equal to 1 and an upper bound equal to 3. Then, the groups contained in the crossing site of the first parent c p 1 are has inherited properties from both parents. For the second child c c 2 , the process is repeated exchanging the role of the parents.
 3.3.2. Mutations
As crossover operations only combine existing properties from the current population, a way to intro-duce new information is needed in order to explore the whole search space; i.e. ensuring that any possible chromosome in the search space can be generated. Mutation operations  X  ( c p ) proceed by performing random modifications to a chromosome c p , thus generating a new member c c with some characteristics not present in the current population.

We propose five different mutations in GOMM, which are represented in Figure 4; three working with the Group Part of the chromosome, and two with the Object Part. The former allows the exploration of the entire solution space as they deal with groups, while the latter works in a finer granularity and allows to polish the solution when a promising zone in the search space has been found.  X  Group Create( c c =  X  GC ( c p ) ) builds a new group of attributes randomly, which is injected in the  X  Group Eliminate( c c =  X  GE ( c p ) ) selects randomly a group from the Group Part of the parent c p and  X  Element Swap( c c =  X  ES ( c p ) ) works with the Object Part of the chromosome; two attributes are  X  Element Move( c c =  X  EM ( c p ) ) selects randomly an attribute from the Object Part of the parent c p 3.3.3. Selection
In order to preserve the number of members in the population and favor evolution, at the end of every iteration the chromosomes that will survive to the next generation are chosen. The selection method used by GOMM is elitist as it chooses the chromosomes with th e highest quality; i.e. t hose solutions with the best fitness evaluations.
 Formally, if we have two chromosomes c 1 and c 2 with an associated fitness F ( c 1 ) and F ( c 2 ) and F ( c 1 ) &lt;F ( c 2 ) ,then c 1 is more likely to survive to the next generation. Thus, the objective of our genetic optimizer is to minimize the cost function F ( c ) and obtain a solution with both low information loss and disclosure risk. 4. Genetic operations analysis
Now we perform a comprehensive analysis of the genetic operations used by GOMM, using the same analysis design as that proposed in [14]. The aim is to evaluate the effect that each operation has over the population for each generation of the execution. Four aspects have been studied: the number of chromosomes discarded without being used, the average chromosome life time and the efficacy and efficiency of the different operations. As a result of this analysis, we are able to understand the behavior of the genetic optimizer and to study new techniques to improve its performance. In this section we also propose D-GOMM, an improved version of GOMM that dynamically adapts the number of genetic operations to accelerate the whole optimization process.
 This analysis has been performed with real data extracted from two data sets available in the Internet. The first one if the Water-treatment data set, extracted from the UCI Machine Learning repository [16], which contains 38 attributes and 380 records. The second one, called Census, was extracted using the Data Extraction System of the U.S. Census Bureau [27], which contains 12 attributes and 1080 records. A complete description about the details of the construction of this data set can be found in [8]. Such data sets have been widely used in microaggregation literature, as for instance, in [17,18]. 4.1. Analysis measures
The first way to analyze the capability of a genetic operator to introduce good properties into the population is to study the number of members discarded without being used. That is, the chromosomes that are not used to generate new chromosomes in a crossover or a mutation operation. This measure is called utilization . If a genetic operation introduces interesting configurations in the population, the new members generated by this operation have a higher probability to survive and to be chosen on future generations.

Another way to evaluate the effect of a genetic operator is to analyze the average life time of the new generated chromosomes it produces, which is the number of generations they survive. On the one hand, longer life times imply a higher probability for a given chromosome to be used in the next generations. On the other hand, an average life time of zero indicates that the chromosomes generated by the operation have not passed the selection process.

The utilization and the average life time of the chro mosomes give us a first impression of the amount of useful work that a genetic operation is producing during all the algorithm execution. However, these two approaches do not show if the operations are really introducing good properties into the population; that is, if the new generated chromosomes have a better fitness than their parents, in the case of mutation operations; or a better fitness than the average of both parents, in crossover operations. This measure is called efficacy .

Finally, even though the previous analysis provide us with an approximate picture of the behavior of genetic operations, it does not directly reveal how much better or worse is the fitness of the new generated chromosomes. For this reason, we evaluate the efficiency of the operations by immediately calculating the percentage of improvement or worsement of the chromosome fitnesses after the application of the genetic operation.

To calculate the average percentage of maximum improvement and worsening for mutation operations we use Formula (7), where c c is the child chromosome and c p the parent chromosome. For crossover operations we use Eqs (8) and (9) where c p 1 and c p 2 are the parents. The idea behind the equation for the crossover operation is to calculate whether the new chromosome is better than the average fitness between both parents. 4.2. Analysis results
As we said before, this analysis has been performed for two public data sets. In this subsection we only present the analysis results obtained by executing GOMM with the Water-treatment data set since the results obtained for the Census data set are very similar. We executed the optimizer with different values for the parameter k of the MDAV algorithm, but we only present the results for clusters for k =25 , which is a very common configuration in multivariate microaggregation since it offers a good trade-off between information loss and disclosure risk. The algorithm was executed during 100 generations, using 200 members for the population and generating 100 new members for every iteration. These have been obtained by performing 25 crossover operations and 50 mutation operations (10 of each type) per generation.

In order to fully understand the results provided by this analysis we monitored the evolution of the population during the optimizer execution. We observed that, for a k value of 25 the average number of attribute groups of the members of the population quickly decreases. Thus, in this case, solutions with a lower number of groups are preferred to the ones with a higher number of attribute partitions.
Utilization . The first row of plots in Fig. 6 shows the average number of chromosomes discarded without being used in a later operation for each mutation operation. Note that  X  GC ,  X  GE and  X  GS are doing useless work more or less after generation 25 since 100% of the generated chromosomes are discarded without being used. In contrast,  X  ES and  X  EM gradually lose potential. Their best behavior is shown between generations 20 and 30, where less than half of the chromosomes produced are discarded without being used.

The results for the crossover operations are shown on the first plot of Fig. 5. The percentage of chro-mosomes discarded without being used is always around 75%, being a little lower in the first generations.
Average life time . The information on the average life time for each mutation operation is shown in the second row of Fig. 6. Each plot shows the average number of survived generations of the attribute grouping configurations created by the considered mutation for every iteration of the optimizer. Again, the plots show that  X  GC ,  X  GE and  X  GS carry out unnecessary operations after generation 25 as the average life time of all the chromosomes created by these operations is zero, that is, they die as soon as they are created. On the other hand, the other two mutations improve their performance after generation 20 since the chromosomes they create live longer. The average life time of the chromosomes produced by  X  EM does not decrease until generation 50, and in the case of  X  ES until generation 70.

Regarding crossover operations, we can see in the second plot of Fig. 7 that the average life time is always around 2 and 3, which shows the conservative behaviour of this operation.

Efficacy . The efficacy plots presented in the third row of Fig. 6 show the number of improved chro-mosomes along with the number of worsened ones for every generation. It seems reasonable to assume that the addition of the two parts should be the number of chromosomes generated for every operation in a certain generation (10 for each mutation and 50 for the crossover); nevertheless, this is not true when the generated chromosome is a clone of its parent, that is, they have the same fitness.

Again,  X  GC ,  X  GE ang  X  GC only generate chromosomes worse that those of their parents from gen-eration 25. However, the only operation that gradually loses efficacy is  X  GE ,as  X  GC and  X  GS show a peak near generations 18 X 19. This is because the average number of groups of the first population is large because it has been generated randomly, and thus, creating small groups or splitting them does not generate better chromosomes. However, as the number of groups decreases due to the creation of large groups and the elimination of others, these operations are able to generate better chromosomes. In the case of  X  ES , the plot shows this is an operation that generates a lot of clones, but until the end of the execution is capable of generating improved chromosomes. This also happens with  X  EM , where we can observe that near generations 16 X 17 the number of clones generated increases. The only case where  X  EM produces clones is when the parent chromosome has a single group, which is exactly when  X  GS and  X  GC start generating improved chromosomes.

The crossover results in the third plot of Fig. 7 show that after generation 20 the number of improve-ments is very constant, standing in the 5% of the chromosomes generated.

Efficiency . In the last row of Fig. 6, we can see the average efficiency results for each mutation oper-ation, showing the average improvement and the average worsening along with the average maximum and minimum for every generation. We observe that  X  GC and  X  GE introduce very interesting properties in the first generations, with average maximum efficiency values around 10%. This is caused by the fact that, as we said before, the members of the first population have an elevated number of groups, and as these mutations reduce it, they are able to generate much better chromosomes. Between generation 10 and 20  X  GS does some useful work because new chromosomes with a low number of groups have appeared in the population due to the effects of  X  GC and  X  GE . These three operations, as seen before, stop doing useful work after generation 25 as they only produce worse chromosomes. In the case of  X  ES and  X  EM , we notice that their efficiency values are lower than the other mutation operations, standing around 2% in their phase of best behavior. This happens because these operations work with attributes rather than groups, so the chromosomes they generate are very similar to the parent chromosome they used. That property is the one that allows  X  ES and  X  EM to continue doing useful work after generation 25, when the optimum number of groups has been found and  X  GC ,  X  GE and  X  GS do useless work.
The efficiency results for the crossover operation are presented in the last plot of Fig. 7. In the first generations, the average maximum efficiency shows a peak and afterwards it quickly decreases, showing that the new chromosomes are worse or have the same fitness as the parents. The explanation to this phenomenon is that, in the first generations, parent chromosomes with a large number of groups are crossed with others with few groups, so the resulting children have less groups than the former parents. In the next generations, when the number of groups of the members of the population has stabilized, the new chromosomes generated are very similar to their parents or have more groups, which means that they have the same or worse fitness respectively. 4.3. Improving performance
The main conclusion of the previous analysis is that some operations carry out a lot of useless work after a given generation. As the algorithm proceeds we distinguish two separated phases: firstly, the group-oriented mutations (  X  GC ,  X  GE and  X  GS ) contribute to evolve the population modifying the number of groups; secondly, these operators only generate worse chromosomes and the object-oriented muta-tions, along with the crossover, introduce improvements in a lower scale.

For this reason, we propose D-GOMM, the Dynamic Genetic Optimizer for Multivariate Microaggre-gation, which includes a control mechanism to detect the generation where the second phase starts. If the optimizer is able to detect when an operator is performing useless work, that is, generating chromosomes worse-fitted than their parents, it can stop its execution. This means that less child chromosomes will be generated, thus performing less fitness evaluations and accelerating the optimization process.
It is important to notice that D-GOMM only stops the execution of the genetic operations that per-form unnecessary work during several generations. Thus, for all the executions performed the quality of the solution given by D-GOMM is always equal to the one resulting of GOMMs execution but time performance increases. 5. Experiments In this section we study the solutions given by our genetic optimizer from a twofold perspective. Firstly, we validate the solutions provided by GOMM according to the microaggregation problem. Sec-ondly, we compare them with the ad-hoc solutions used in practice. To do that, we have executed GOMM with different values of k (from 5 to 100) to study the impact of such parameter in the configuration of the groups in the best solution. In addition, we also show in this section a performance comparison between GOMM and D-GOMM.

Solution validation . Intuitively, when k increases, the clusters size must grow producing an increment in the information loss of each clus ter. This effect combined with th e problem of dime nsionality make configurations with a large number of groups to be preferred to configurations with only one or two groups, as the value of k increases.

Tables 1 and 2 show the average solution given by GOMM using the Water Treatment data set and the Census data set, respectively. The value of parameter k is placed in the first column and the average number of groups of the solution in the last one. The other columns show the average values of the cost function parameters.

When k increases, the number of groups of the best solution also increases, specially due to an incre-ment of the IL component, as our intuition predicted. Therefore, the solutions yielded by GOMM are coherent with our intuition.
 From these tables we can also observe that finding the optimal value for parameter k is far from trivial. For instance, in the Census table, the best scores are presented by configurations with a value k between 10 and 50.
Solution comparison . In order to compare our results with the different strategies presented in the literature, we manually split both data sets following the recommendations described in [18]. The re-sulting configurations are depicted in Tables 3 and 4. As we can observe, configurations vary from few groups containing many attributes to many groups containing a few attributes. In such configurations, we also considered the correlation between attributes. On the one hand, we grouped correlated attributes together (to minimize the information loss). On the other hand, we grouped correlated attributes in dif-ferent groups (to minimize the disclosure risk). Finally, we also consider two special scenarios: in the first one, all attributes are grouped together, ensuring k anonymity. In the second one, we consider the univariate microaggregation scenario where each attribute is microaggregated separately.

The results presented in Tables 5 and 6 show the fitness value of all the ad-hoc configurations along with the fitness value obtained by GOMM for different values of k . If we compare the results obtained using the same k value, we observe that GOMM obtains always the best score, unless the cases where GOMM yields a solution containing a single group, which is one of the ad-hoc groupings we executed. For instance, if we compare the Water treatment results (Table 5) obtained with k =50 , we see that GOMM achieves a fitness (score) equal to 34.3 splitting the data set in three different groups, whilst the best ad-hoc configuration only achieves a fitness value equal to 40.9 dividing the data set into four groups. Similar results are obtained with the remaining k values.

Also, if we compare GOMM results with the Census data set, we observe very similar results. For example, with k = 25 GOMM achieves a fitness value equal to 13.0 dividing the Census data set into two groups and the best ad-hoc solution is equal to 14.3 without splitting the data set.

Performance results . The aim of this last section is to compare the two versions of our genetic approach in terms of execution time. In Section 4.3 we introduced D-GOMM, which is a version of the GOMM algorithm that detects when a genetic operation is producing useless work and in this case, it stops executing it. Therefore, the number of new generated chromosomes per generation decreases along with the number of fitness evaluations of this offspring.

The plot presented in Fig. 8 shows the execution time comparison between GOMM and D-GOMM using a data set with 38 attributes. Notice that for different values of the k parameter D-GOMM is always faster than GOMM. However, the speed-up achieved varies depending on the value of the k parameter. We computed the speed-ups in all the scenarios and the results show that as k increases, the speed-up decreases. The explanation to this phenomenon is that when k is small, the solution given by the algorithm has less groups of attributes, thus, the optimizer converges faster and the group-oriented mutations stop doing useful work before.
 6. Conclusions
In this paper, we have proposed GOMM, a genetic optimizer to find the best way to group the at-tributes when anonymizing a dataset with multivariate microaggregation, in order to obtain a protected dataset with both low disclosure risk and information loss. The experiments performed show that the solutions given by GOMM outperform other grouping strategies proposed in the literature, for different parametrizations of the multivariate microaggregation method.

In addition, we analyzed the behavior of our optimizer in order to understand the contribution of ev-ery genetic operation in the solution given. This analysis allows the study of alternatives to improve GOMM X  X  performance. One of these has been proposed in a version called D-GOMM, which dynami-cally detects when a genetic operation is doing useless work and then stops its execution. We have seen that this mechanism improves the execution time of our optimizer, while the quality of the solution given is not altered.

Regarding future work, we will study other fitness functions for GOMM based on convex optimiza-tion approaches. Since IL and DR are contradictory goals convex minimization seems a very promising research line. Apart from that, we consider two possible directions to follow to improve GOMM per-formance. The first one is to use other multivariate microaggregation methods faster than the MDAV algorithm, since we observed that nearly 60% of the optimizer execution time is spent in this function. Secondly, we want to study parallelization strategies of our approach because there are several points in GOMM where parallelism could be opened. Both future lines aim at improving GOMM X  X  performance and specially, at allowing the optimizer to deal with larger data sets.
 Acknowledgement This work is partially supported by the Ministry of Science and Technology of Spain under contract TIN2012-34557, by the BSC-CNS Severo Ochoa program (SEV-2011-00067).
 References
