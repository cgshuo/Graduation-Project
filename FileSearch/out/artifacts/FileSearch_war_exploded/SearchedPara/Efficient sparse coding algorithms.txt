 in biological neurons, thus making sparse coding a plausible model of the visual cortex [2, 5]. tion over the first subset is an L ond subset of variables is an L complete bases from natural images. We demonstrate that the resulting learned bases exhibit (i) higher-level features that can then be applied to supervised classification tasks. basis vectors ~ b ~ patterns in the input data.
 Sparse coding is a method for discovering good basis vectors automatically using only unlabeled data. The standard generative model assumes that the reconstruction error ~  X   X  P distribution for each coefficient s function and  X  is a constant. For example, we can use one of the following: In this paper, we will use the L produce sparse coefficients and can be robust to irrelevant features [9]. problem above can be written as: Assuming the use of either L problem is convex in B (while holding S fixed) and convex in S (while holding B fixed), 2 but jugate gradient). However, for the L we present a new algorithm for solving the L more efficient for learning sparse coding bases. Consider solving the optimization problem (2) with an L Notice now that if we know the signs (positive, zero, or negative) of the s ( i ) we can replace each of the terms | s ( i ) s j = 0 rithm, therefore, tries to search for, or  X  X uess, X  the signs of the coefficients s ( i ) refines the guess if it turns out to be initially incorrect.
 active set and the signs, and it computes the analytical solution  X  x between the current solution and  X  x Algorithm 1 Feature-sign search algorithm sign( x i ) =  X  i , and, (ii) If i is not in the active set, then x i = 0 . Lemma 3.1. Consider optimization problem (5) augmented with the additional constraint that x is consistent with a given active set and sign vector. Then, if the current coefficients x Step 3, the feature-sign step is guaranteed to strictly reduce the objective. Proof sketch. Let  X  x point of  X  f , we have  X  f ( X  x with the given active set and sign vector, updating  X  x :=  X  x  X  x (where any coefficient changes its sign) on a line segment from  X  x and  X  f ( X  x Lemma 3.2. Consider optimization problem (5) augmented with the additional constraint that x is consistent with a given active set and sign vector. If the coefficients x guaranteed to strictly reduce the objective.
 Proof sketch. Since x not (b); thus, in Step 2, there is some i , such that  X  k y  X  Ax k 2 i  X   X   X   X  of  X  (ii), the line search direction  X  x since  X  f ( X  x ) = f ( X  x ) when  X  x is consistent with the active set, either  X  x zero-crossing from  X  x Theorem 3.3. The feature-sign search algorithm converges to a global optimum of the optimization problem (5) in a finite number of steps.
 solution more quickly than when starting from ~ 0 . In this subsection, we present a method for solving optimization problem (3) over bases B given fixed coefficients S . This reduces to the following problem: much more efficiently solved using a Lagrange dual. First, consider the Lagrangian: where each  X  where  X  = diag( ~  X  ) . The gradient and Hessian of D ( ~  X  ) are computed as follows: Feature-sign 2.16 (0) 0.58 (0) 1.72 (0) 0.83 (0) LARS 3.62 (0) 1.28 (0) 4.02 (0) 1.98 (0) Grafting 13.39 (7e-4) 4.69 (4e-6) 11.12 (5e-4) 5.88 (2e-4) Chen et al. X  X  88.61 (8e-5) 47.49 (8e-5) 66.62 (3e-4) 47.00 (2e-4) QP solver (CVX) 387.90 (4e-9) 1,108.71 (1e-8) 538.72 (7e-9) 1,219.80 (1e-8) ( f objective value attained among all the algorithms. where e the dual formulation is independent of the sparsity function (e.g., L 5.1 The feature-sign search algorithm speech, stereo images, and natural image videos. All experiments were conducted on a Linux ma-chine with AMD Opteron 2GHz CPU and 2GB RAM.
 First, we evaluated the feature-sign search algorithm for learning coefficients with the L objective function at convergence. Table 1 shows both the running time and accuracy (measured times. Feature-sign search and modified LARS produced more accurate solutions than the other and the generic QP solver, and it was also significantly faster than modified LARS and grafting. further speedup over LARS when applied to iterative coefficient learning. 5.2 Total time for learning bases The Lagrange dual method for one basis learning iteration was much faster than gradient descent stimulus datasets. Coeff. / Basis learning natural image speech stereo video Feature-sign / LagDual 260.0 248.2 438.2 186.6 Feature-sign / GradDesc 1,093.9 1,280.3 950.6 933.2 LARS / LagDual 666.7 1,697.7 1,342.7 1,254.6 LARS / GradDesc 13,085.1 17,219.0 12,174.6 11,022.8 Grafting / LagDual 720.5 1,025.5 3,006.0 1,340.5
Grafting / GradDesc 2,767.9 8,670.8 6,203.3 3,681.9 Coeff. / Basis learning natural image speech stereo video ConjGrad / LagDual 1,286.6 544.4 1,942.4 1,461.9 ConjGrad / GradDesc 5,047.3 11,939.5 3,435.1 2,479.2 functions.
 Figure 1: Demonstration of speedup. Left: Comparison of convergence between the Lagrange dual LARS and grafting as a multiple of the running time per iteration for feature-sign search. coefficient learning methods from our experiments (feature-sign search, modified LARS and graft-ing for the L both L we observe that, for L iterative optimization, such as learning sparse coding bases. 5.3 Learning highly overcomplete natural image bases 2,000 bases (each 20  X  20 pixels).
 coefficients for a basis for different length bars. Right: Sample input image for nCRF effect. parameters to each basis, qualitatively agree with previously reported statistics [15]. 5.4 Replicating complex neuroscience phenomena Several complex phenomena of V1 neural responses are not well explained by simple linear models and our algorithms enable these phenomena to be tested with highly overcomplete bases. First, we evaluated whether end-stopping behavior could be observed in the sparse coding frame-and picked the stimulus bar which most strongly activates each basis, considering only the bases optimal point. This result is consistent with the end-stopping behavior of some V1 neurons. field (nCRF) effects [7]. We found the optimal bar stimuli for 50 random bases and checked that bases, we measured the response with its optimal bar stimulus with and without the aligned bar stimuli produced a suppression of basis activation; 42 out of 50 bases showed suppression with aligned surround input images, and 13 bases among them showed more than 10% suppression, in qualitative accordance with observed nCRF surround suppression effects. image patch ~  X  as a linear combination of just a small number of these bases ~ b is useful for a variety of tasks.
 may not have the same class labels as the labeled instances. For example, one may wish to learn to distinguish between cars and motorcycles given images of each, and additional X  X nd in practice be of cars or motorcycles only.) We apply our sparse coding algorithms to the unlabeled data to text categorization, this approach leads to 11 X 36% reductions in test error. In this paper, we formulated sparse coding as a combination of two convex optimization problems and presented efficient algorithms for each: the feature-sign search for solving the L problem to learn coefficients, and a Lagrange dual method for the L partially explain the phenomena of end-stopping and nCRF surround suppression in V1 neurons.
