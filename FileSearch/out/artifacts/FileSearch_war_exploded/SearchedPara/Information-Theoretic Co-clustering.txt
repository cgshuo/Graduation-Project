 Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contin-gency table analysis is co-clustering : simultaneous clustering of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probabil-ity distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in in-formation theory  X  the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual informa-tion by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algo-rithm works well in practice, especially in the presence of sparsity and high-dimensionality.
 E.4 [ Coding and Information Theory ]: Data compaction and compression; G.3 [ Probability and Statistics ]: Con-tingency table analysis; H.3.3 [ Information Search and Retrieval ]: Clustering; I.5.3 [ Pattern Recognition ]: Clus-tering Co-clustering, information theory, mutual information Clustering is a fundamental tool in unsupervised learning that is used to group together similar objects [14], and has practical importance in a wide variety of applications such as text, web-log and market-basket data analysis. Typically, the data that arises in these applications is arranged as a contingency or co-occurrence table, such as, word-document co-occurrence table or webpage-user browsing data. Most clustering algorithms focus on one-way clustering, i.e., clus-ter one dimension of the table based on similarities along the second dimension. For example, documents may be clus-tered based upon their word distributions or words may be clustered based upon their distribution amongst documents. It is often desirable to co-cluster or simultaneously cluster both dimensions of a contingency table [11] by exploiting the clear duality between rows and columns. For example, we may be interested in finding similar documents and their in-terplay with word clusters. Quite surprisingly, even if we are interested in clustering along one dimension of the contin-gency table, when dealing with sparse and high-dimensional data, it turns out to be beneficial to employ co-clustering. To outline a principled approach to co-clustering, we treat the (normalized) non-negative contingency table as a joint probability distribution between two discrete random vari-ables that take values over the rows and columns. We define co-clustering as a pair of maps from rows to row-clusters and from columns to column-clusters. Clearly, these maps induce clustered random variables. Information theory can now be used to give a theoretical formulation to the prob-lem: the optimal co-clustering is one that leads to the largest mutual information between the clustered random variables. Equivalently, the optimal co-clustering is one that minimizes the difference ( X  X oss X ) in mutual information between the original random variables and the mutual information be-tween the clustered random variables. In this paper, we present a novel algorithm that directly optimizes the above loss function. The resulting algorithm is quite interesting: it intertwines both row and column clustering at all stages. Row clustering is done by assessing closeness of each row distribution, in relative entropy, to certain  X  X ow cluster pro-totypes X . Column clustering is done similarly, and this pro-cess is iterated till it converges to a local minimum. Co-clustering differs from ordinary one-sided clustering in that at all stages the row cluster prototypes incorporate column clustering information, and vice versa. We theoretically es-tablish that our algorithm never increases the loss, and so, gradually improves the quality of co-clustering.
 We empirically demonstrate that our co-clustering algorithm alleviates the problems of sparsity and high dimensionality by presenting results on joint word-document clustering. An interesting aspect of the results is that our co-clustering ap-proach yields superior document clusters as compared to the case where document clustering is performed without any word clustering. The explanation is that co-clustering implicitly performs an adaptive dimensionality reduction at each iteration, and estimates fewer parameters than a stan-dard  X  X ne-dimensional X  clustering approach. This results in an implicitly  X  X egularized X  clustering. A word about notation: upper-case letters such as X , Y ,  X  X ,  X  Y will denote random variables. Elements of sets will be denoted by lower-case letters such as x and y . Quanti-ties associated with clusters will be  X  X atted X : for example,  X  X denotes a random variable obtained from a clustering of X while  X  x denotes a cluster. Probability distributions are denoted by p or q when the random variable is obvious random variable explicit. Logarithms to the base 2 are used. Let X and Y be discrete random variables that take values in the sets { x 1 , . . . , x m } and { y 1 , . . . , y n p ( X, Y ) denote the joint probability distribution between X and Y . We will think of p ( X, Y ) as a m  X  n matrix. In prac-tice, if p is not known, it may be estimated using observa-tions. Such a statistical estimate is called a two-dimensional contingency table or as a two-way frequency table [9]. We are interested in simultaneously clustering or quantizing X into (at most) k disjoint or hard clusters, and Y into (at most) ` disjoint or hard clusters. Let the k clusters of interested in finding maps C X and C Y , For brevity, we will often write  X  X = C X ( X ) and  X  Y = C
Y ( Y );  X  X and  X  Y are random variables that are a deter-ministic function of X and Y , respectively. Observe that X and Y are clustered separately, that is,  X  X is a function of X alone and  X  Y is a function of Y alone. But, the partition functions C X and C Y are allowed to depend upon the entire joint distribution p ( X, Y ).

Definition 2.1. We refer to the tuple ( C X , C Y ) as a co-clustering.
 Suppose we are given a co-clustering. Let us  X  X e-order X  the rows of the joint distribution p such that all rows mapping into  X  x 1 are arranged first, followed by all rows mapping into  X  x , and so on. Similarly, let us  X  X e-order X  the columns of the joint distribution p such that all columns mapping into  X  y are arranged first, followed by all columns mapping into  X  y , and so on. This row-column reordering has the effect of dividing the distribution p into little two-dimensional blocks. We refer to each such block as a co-cluster .
 A fundamental quantity that measures the amount of infor-mation random variable X contains about Y (and vice versa) is the mutual information I ( X ; Y ) [3]. We will judge the quality of a co-clustering by the resulting loss in mutual in-formation , I ( X ; Y )  X  I (  X  X ;  X  Y ) (note that I (  X  by Lemma 2.1 below).
 Definition 2.2. An optimal co-clustering minimizes subject to the constraints on the number of row and column clusters.
 For a fixed distribution p , I ( X ; Y ) is fixed; hence minimiz-ing (1) amounts to maximizing I (  X  X,  X  Y ).
 Let us illustrate the situation with an example. Consider the 6  X  6 matrix below that represents the joint distribution: Looking at the row distributions it is natural to group the rows into three clusters:  X  x 1 = { x 1 , x 2 } ,  X  x 2 = { x  X  x 3 = { x 5 , x 6 } . Similarly the natural column clustering is:  X  y tribution p (  X  X,  X  Y ), see (6) below, is given by: It can be verified that the mutual information lost due to this co-clustering is only . 0957, and that any other co-clustering leads to a larger loss in mutual information.
 The following lemma shows that the loss in mutual infor-mation can be expressed as the  X  X istance X  of p ( X, Y ) to an approximation q ( X, Y )  X  this lemma will facilitate our search for the optimal co-clustering.

Lemma 2.1. For a fixed co-clustering ( C X , C Y ) , we can write the loss in mutual information as where D (  X || X  ) denotes the Kullback-Leibler(KL) divergence, also known as relative entropy, and q ( X, Y ) is a distribution of the form Proof. Since we are considering hard clustering, where the last step follows since p ( x |  X  x ) = p ( x ) and 0 otherwise, and similarly for p ( y |  X  y ). tu Lemma 2.1 shows that the loss in mutual information must be non-negative, and reveals that finding an optimal co-clustering is equivalent to finding an approximating distri-bution q of the form (5) that is close to p in Kullback-Leibler divergence. Note that the distribution q preserves marginals of p , that is, for  X  x = C X ( x ) and  X  y = C Y ( y ), q ( x ) = X Similarly, q ( y ) = p ( y ). In Section 4 we give further prop-erties of the approximation q . Recall the example p ( X, Y ) in (2) and the  X  X atural X  row and column clusterings that led to (3). It is easy to verify that the corresponding ap-proximation q ( X, Y ) defined in (5) equals q ( X, Y ) = and that D ( p || q ) = . 0957. Note that the row and column sums of the above q are identical to those of p given in (2). We end this section by providing another motivation based on the theory of source coding and transmission. Let us set-up an artificial data compression problem, where we want to transmit X and Y from a source to a destina-tion. Let us insist that this transmission be done in two-stages: (a) first compute  X  X = C X ( X ) and  X  Y = C Y ( Y ), and transmit the cluster identifiers  X  X and  X  Y jointly ; and (b) separately transmit X given that the destination already knows  X  X and transmit Y given that the destination already knows  X  Y . The first step will require, on an average, at least, H (  X 
X,  X  Y ) bits, and, the second step will require, on an aver-the average number of bits that must be transmitted from the source to the destination is: However, by noting the parallel between (5) and (8), it easy to show that: Thus, to find an optimal co-clustering it is sufficient to mini-mize (8) subject to the constraints on the number of row and column clusters. Observe that (8) contains the cross-term H (  X 
X,  X  Y ) that captures the interaction between row and col-umn clusters. This underscores the fact that clustering of rows and columns must interact in a  X  X ood X  co-clustering. A naive algorithm that clusters rows without paying attention to columns and vice versa will miss this critical interaction that is the essence of co-clustering. Most of the clustering literature has focused on one-sided clustering algorithms [14]. There was some early work on co-clustering, such as in [11] which used a local greedy split-ting procedure to identify hierarchical row and column clus-ters in matrices of small size. Co-clustering has also been called biclustering and block clustering in [2] and [17] respec-tively. Recently [4] used a graph formulation and a spectral heuristic that uses eigenvectors to co-cluster documents and words; however, a restriction in [4] was that each word clus-ter was associated with a document cluster. We do not impose such a restriction in this paper; see Section 5.3 for examples of different types of associations between row and column clusters.
 Our information-theoretic formulation of preserving mutual information is similar to the information bottleneck (IB) framework [20], which was introduced for one-sided cluster-ing, say X to  X  X . IB tries to minimize the quantity I ( X, to gain compression while maximizing the mutual informa- X I (  X  X, Y ) where  X  reflects the tradeoff between compression and preservation of mutual information. The resulting algo-rithm yields a  X  X oft X  clustering of the data using a determin-istic annealing procedure. For a hard partitional clustering algorithm using a similar information-theoretic framework, see [6]. These algorithms were proposed for one-sided clus-tering.
 An agglomerative hard clustering version of the IB method was used in [19] to cluster documents after clustering words. The work in [8] extended the above work to repetitively cluster documents and then words. Both these papers use heuristic procedures with no guarantees on a global loss function; in contrast, in this paper we first quantify the loss in mutual information due to co-clustering and then pro-pose an algorithm that provably reduces this loss function monotonically, converging to a local minimum.
 Recently, when considering a clustering framework using Bayesian belief networks, [10] proposed an iterative opti-mization method that amounts to a multivariate generaliza-tion of [20], and, once again, uses deterministic annealing. A later paper [18] presented a hard agglomerative algorithm for the same problem that has advantages over [10] in that  X  X t is simpler, fully deterministic, and non-parametric. There is no need to identify cluster splits which is rather tricky X . However, [18] pointed out that their  X  X gglomeration proce-dures do not scale linearly with the sample size as top down methods do . . .  X . In this paper, we present a principled, top-down hard clustering method that scales well. Also, the results in [18] amount to first finding a word clustering followed by finding a document clustering (without any it-eration), whereas we present a procedure that intertwines word and document clusterings at all stages and continually improves both until a local minimum is found, and, hence, is a true co-clustering procedure.
 By Lemma 2.1, our co-clustering procedure is intimately re-lated to finding a matrix approximation q ( X, Y ). A soft ver-sion of our procedure is related to the PLSI scheme of [12]; however the latter uses a single latent variable model. A two-sided clustering model is given in [13] that uses maxi-mum likelihood in a model-based generative framework. We now describe a novel algorithm that monotonically de-creases the objective function (1). To describe the algorithm and related proofs, it will be more convenient to think of the joint distribution of X, Y,  X  X , and  X  Y . Let p ( X, Y, note this distribution. Observe that we can write: By Lemma 2.1, for the purpose of co-clustering, we will seek an approximation to p ( X, Y,  X  X,  X  Y ) using a distribution q ( X, Y,  X  X,  X  Y ) of the form: The reader may want to compare (5) and (10): observe that the latter is defined for all combinations of x , y ,  X  x , and  X  y . Note that in (10) if  X  x 6 = C X ( x ) or  X  y 6 = C two-dimensional marginal of p ( X, Y,  X  X,  X  Y ) and q ( X, Y ) as a two-dimensional marginal of q ( X, Y,  X  X,  X  Y ). Intuitively, by (9) and (10), within the co-cluster denoted by  X  X =  X  x and  X  following proposition (which we state without proof) estab-lishes that there is no harm in adopting such a formulation. Proposition 4.1. For every fixed  X  X ard X  co-clustering, We first establish a few simple, but useful equalities that highlight properties of q desirable in approximating p .
Proposition 4.2. For a distribution q of the form (10), the following marginals and conditionals are preserved: Thus,  X  x, y,  X  x , and  X  y . Further, if  X  y = C Y ( y ) and  X  x = C and, symmetrically, Proof. The equalities of the marginals in (11) are simple to show and will not be proved here for brevity. Equalities (12), (13), and (14) easily follow from (11). Equation (15) follows from q ( y |  X  x ) = q ( y,  X  y |  X  x ) = q ( y,  X  y,  X  x ) Equation (16) follows from where the last equality follows from (15). tu Interestingly, q ( X, Y ) also enjoys a maximum entropy prop-erty and it can be verified that H ( p ( X, Y ))  X  H ( q ( X, Y )) for any input p ( X, Y ).
 Lemma 2.1 quantified the loss in mutual information upon co-clustering as the KL-divergence of p ( X, Y ) to q ( X, Y ). Next, we use the above proposition to prove a lemma that expresses the loss in mutual information in two revealing ways. This lemma will lead to a  X  X atural X  algorithm.
Lemma 4.1. The loss in mutual information can be ex-pressed as (i) a weighted sum of the relative entropies be-tween row distributions p ( Y | x ) and  X  X ow-lumped X  distribu-tions q ( Y |  X  x ) , or as (ii) a weighted sum of the relative en-tropies between column distributions p ( X | y ) and  X  X olumn-lumped X  distributions q ( X |  X  y ) , that is, Proof. We show the first equality, the second is similar.
D ( p ( X, Y,  X  X,  X  Y ) || q ( X, Y,  X  X,  X  Y )) p ( x ) p ( y | x ) when  X  y = C Y ( y ),  X  x = C X ( x ). tu The significance of Lemma 4.1 is that it allows us to express the objective function solely in terms of the row-clustering, or in terms of the column-clustering. Furthermore, it allows type X , and similarly, the distribution q ( X |  X  y ) as a  X  X olumn-cluster prototype X . With this intuition, we now present the co-clustering algorithm in Figure 1. The algorithm works as follows. It starts with an initial co-clustering ( C (0) and iteratively refines it to obtain a sequence of co-clusterings: ( C
X , C and q ( t ) are given by: p ( t ) ( x, y,  X  x,  X  y ) = p ( t ) and q ( t ) ( x, y,  X  x,  X  y ) = p ( t ) ( X  x,  X  y ) p ( t ) p ( y | x ), and p ( x, y ), respectively, instead of p ( t ) p ( t ) ( x | y ), p ( t ) ( y | x ), and p ( t ) ( x, y ). Figure 1: Information theoretic co-clustering algo-rithm that simultaneously clusters both the rows and columns In Step 1, the algorithm starts with an initial co-clustering ( C
X , C sulting approximation q (0) (the choice of starting points is important, and will be discussed in Section 5). The algo-rithm then computes the appropriate  X  X ow-cluster proto-types X  q (0) ( Y |  X  x ). While the reader may wish to think of where |  X  x | denotes the number of rows in cluster  X  x . Rather, by (15), for every y , we write where  X  y = C Y ( y ). Note that (18) gives a formula that would have been difficult to guess a priori without the help of anal-ysis. In Step 2, the algorithm  X  X e-assigns X  each row x to closest to p ( Y | x ) in Kullback-Leibler divergence. In essence, Step 2 defines a new row-clustering. Also, observe that the column clustering is not changed in Step 2. In Step 3, using the new row-clustering and the old column clustering, the al-gorithm recomputes the required marginals of q ( t +1) . More importantly, the algorithm recomputes the column-cluster prototypes. Once again, these are not ordinary centroids, but rather by using (17), for every x , we write where  X  x = C X ( x ). Now, in Step 4, the algorithm  X  X e-assigns X  each column y to a new column-cluster whose column-Leibler divergence. Step 4 defines a new column-clustering while holding the row-clustering fixed. In Step 5, the algo-rithm re-computes marginals of q ( t +2) . The algorithm keeps iterating Steps 2 through 5 until some desired convergence condition is met. The following reassuring theorem, which is our main result, guarantees convergence. Note that co-clustering is NP-hard and a local minimum does not guar-antee a global minimum.

Theorem 4.1. Algorithm Co Clustering monotonically de-creases the objective function given in Lemma 2.1. Proof.

D ( p ( t ) ( X, Y,  X  X,  X  Y ) || q ( t ) ( X, Y,  X  X,  X  Y )) = X  X  X = X = X + X = I 1 +
X = I 1 + X = I 1 + X  X  I 1 + X where (a) follows from Lemma 4.1; (b) follows from Step 2 of the algorithm; (c) follows by rearranging the sum and from (15); (d) follows from Step 3 of the algorithm, (6) and (11); (e) follows by non-negativity of the Kullback-Leibler diver-gence; and (f) follows since we hold the column clusters fixed Lemma 4.1. By using an identical argument, which we omit for brevity, and by using properties of Steps 4 and 5, we can show that By combining (20) and (21), it follows that every iteration of the algorithm never increases the objective function. tu
Corollary 4.1. The algorithm in Figure 1 terminates in a finite number of steps at a cluster assignment that is locally optimal, that is, the loss in mutual information can-not be decreased by either (a) re-assignment of a distribution for any of the existing clusters.
 Proof. The result follows from Theorem 4.1 and since the number of distinct co-clusterings is finite. tu
Remark 4.1. The algorithm is computationally efficient even for sparse data as its complexity can be shown to be O ( nz  X   X   X  ( k + ` )) where nz is the number of nozeros in the input joint distribution p ( X, Y ) and  X  is the number of iterations; empirically 20 iterations are seen to suffice.
Remark 4.2. A closer examination of the above proof shows that Steps 2 and 3 together imply (20) and Steps 4 and 5 together imply (21). We show how to generalize the above convergence guarantee to a class of iterative algorithms. In particular, any algorithm that uses an arbitrary concate-nations of Steps 2 and 3 with Steps 4 and 5 is guaranteed to monotonically decrease the objective function. For exam-ple, consider an algorithm that flips a coin at every iteration and performs Steps 2 and 3 if the coin turns up heads, and performs Steps 4 and 5 otherwise. As an another example, consider an algorithm that keeps iterating Steps 2 and 3, until no improvement in the objective function is noticed. Next, it can keep iterating Steps 4 and 5, until no further improvement in the objective function is noticed. Now, it can again iterate Steps 2 and 3, and so on and so forth. Both these algorithms as well all algorithms in the same spirit are guaranteed to monotonically decrease the objec-tive function. Such algorithmic flexibility can allow explo-ration of various local minima when starting from a fixed initial random partition in Step 1.

Remark 4.3. While our algorithm is in the spirit of k -means, the precise algorithm itself is quite different. For as a  X  X ow-cluster prototype X . This quantity is different from the naive  X  X entroid X  of the cluster  X  x . Similarly, the column-centroid of the cluster  X  y . In fact, detailed analysis (as is evident from the proof of Theorem 4.1) was necessary to identify these key quantities. We now illustrate how our algorithm works by showing how it discovers the optimal co-clustering for the example p ( X, Y ) distribution given in (2) of Section 2. Table 1 shows a typical run of our co-clustering algorithm that starts with a random partition of rows and columns. At each iteration Table 1 shows the steps of Algorithm Co Clustering , the resulting approximation q ( t ) ( X, Y ) and the corresponding compressed distribution p ( t ) (  X  X,  X  Y ). The row and column cluster num-bers are shown around the matrix to indicate the clustering at each stage. Notice how the intertwined row and column co-clustering leads to progressively better approximations to the original distribution. At the end of four iterations the algorithm almost accurately reconstructs the original dis-tribution, discovers the natural row and column partitions and recovers the ideal compressed distribution p (  X  X,  X  in (3). A pleasing property is that at all iterations q ( t ) preserves the marginals of the original p ( X, Y ). This section provides empirical evidence to show the benefits of our co-clustering framework and algorithm. In particu-lar we apply the algorithm to the task of document clus-tering using word-document co-occurrence data. We show that the co-clustering approach overcomes sparsity and high-dimensionality yielding substantially better results than the approach of clustering such data along a single dimension. We also show better results as compared to previous algo-rithms in [19] and [8]. The latter algorithms use a greedy technique ([19] uses an agglomerative strategy) to cluster documents after words are clustered using the same greedy approach. For brevity we will use the following notation to denote various algorithms in consideration. We call the Information Bottleneck Double Clustering method in [19] as IB-Double and the Iterative Double Clustering algorithm in [8] as IDC. In addition we use 1D-clustering to denote document clustering without any word clustering i.e, clus-tering along a single dimension. For our experimental results we use various subsets of the 20-Newsgroup data( NG20 ) [15] and the SMART collection from Cornell (ftp://ftp.cs.cornell.edu/pub/smart). The NG20 data  X  x 3 .029 .029 .019 .022 .024 .024 0.10 0.05  X  x 1 .036 .036 .014 .028 .018 .018 0.10 0.20  X  x 2 .018 .018 .028 .014 .036 .036 0.30 0.25  X  x 2 .018 .018 .028 .014 .036 .036  X  x 3 .039 .039 .025 .030 .032 .032  X  x 3 .039 .039 .025 .030 .032 .032  X  x 1 .036 .036 .014 .028 .018 .018 0.20 0.10  X  x 1 .036 .036 .014 .028 .018 .018 0.18 0.32  X  x 2 .019 .019 .026 .015 .034 .034 0.12 0.08  X  x 2 .019 .019 .026 .015 .034 .034  X  x 3 .043 .043 .022 .033 .028 .028  X  x 2 .025 .025 .035 .020 .046 .046  X  x 1 .054 .054 .042 0 0 0 0.30 0  X  x 1 .054 .054 .042 0 0 0 0.12 0.38  X  x 2 .013 .013 .010 .031 .041 .041 0.08 0.12  X  x 2 .013 .013 .010 .031 .041 .041  X  x 3 .028 .028 .022 .033 .043 .043  X  x 2 .017 .017 .013 .042 .054 .054  X  x 1 .054 .054 .042 0 0 0 0.30 0  X  x 1 .054 .054 .042 0 0 0 0 0.30  X  x 2 0 0 0 .042 .054 .054 0.20 0.20  X  x 2 0 0 0 .042 .054 .054  X  x 3 .036 .036 .028 .028 .036 .036  X  x 3 .036 .036 .028 .028 .036 .036 Table 1: Algorithm Co Clustering of Figure 1 gives progressively better clusterings and approximations till the optimal is discovered for the example p ( X, Y ) given in Section 2. set consists of approximately 20 , 000 newsgroup articles col-lected evenly from 20 different usenet newsgroups. This data set has been used for testing several supervised text clas-sification tasks [6] and unsupervised document clustering tasks [19, 8]. Many of the newsgroups share similar topics and about 4 . 5% of the documents are cross posted making the boundaries between some news-groups rather fuzzy. To make our comparison consistent with previous algorithms we reconstructed various subsets of NG20 used in [19, 8]. We applied the same pre-processing steps as in [19] to all the subsets, i.e., removed stop words, ignored file headers and selected the top 2000 words by mutual information 1 . Spe-cific details of the subsets are given in Table 2. The SMART
The data sets used in [19] and [8] differ in their pre-processing steps. The latter includes subject lines while the former does not. So we prepared two different data sets one with subject lines and the other without subject lines. collection consists of MEDLINE , CISI and CRANFIELD sub-collections. MEDLINE consists of 1033 abstracts from medical journals, CISI consists of 1460 abstracts from in-formation retrieval papers and CRANFIELD consists of 1400 abstracts from aerodynamic systems. After removing stop words and numeric characters we selected the top 2000 words by mutual information as part of our pre-processing. We will refer to this data set as CLASSIC3 .
 Bow [16] is a library of C code useful for writing text analy-sis, language modeling and information retrieval programs. We extended Bow with our co-clustering and 1D-clustering procedures, and used MATLAB for spy plots of matrices. Validating clustering results is a non-trivial task. In the presence of true labels, as in the case of the data sets we use, we can form a confusion matrix to measure the effectiveness of the algorithm. Each entry( i, j ) in the confusion matrix represents the number of documents in cluster i that belong to true class j . For an objective evaluation measure we use micro-averaged-precision . For each class c in the data set we define  X  ( c,  X  y ) to be the number of documents correctly assigned to c ,  X  ( c,  X  y ) to be number of documents incorrectly assigned to c and  X  ( c,  X  y ) to be the number of documents incorrectly not assigned to c . The micro-averaged-precision and recall are defined, respectively, as: P ( X  y ) = Note that for uni-labeled data P ( X  y ) = R ( X  y ). First we demonstrate that co-clustering is significantly bet-ter than clustering along a single dimension using word-document co-occurrence matrices. In all our experiments since we know the number of true document clusters we can give that as input to our algorithm. For example in the case of Binary data set we ask for 2 document clusters. To show how the document clustering results change with the number of word clusters, we tried k = 2 , 4 , 8 , . . . , 128 word clusters. To initialize a co-clustering with k word clusters, we split each word cluster obtained from a co-clustering run with k/ 2 word clusters. Note that this does not increase the overall complexity of the algorithm. We bootstrap at k = 2 by choosing initial word cluster distributions to be  X  X aximally X  far apart from each other [1, 6]. Our initial-ization scheme alleviates, to a large extent, the problem of poor local minima. To initialize document clustering we use a random perturbation of the  X  X ean X  document, a strategy that has been observed to work well for document cluster-ing [5]. Since this initialization has a random component all our results are averages of five trials unless stated otherwise. Table 3 shows two confusion matrices obtained on the CLAS-SIC3 data set using algorithms 1D-clustering and co-clustering (with 200 word clusters). Observe that co-clustering ex-tracted the original clusters almost correctly resulting in a micro-averaged-precision of 0 . 9835 while 1D-clustering led to a micro-averaged-precision of 0 . 9432.
Dataset Newsgroups included #documents Total Table 3: Co-clustering accurately recovers original clusters in the CLASSIC3 data set.
 Table 4: Co-clustering obtains better clustering re-sults compared to one dimensional document clus-tering on Binary and Binary subject data sets Table 4 shows confusion matrices obtained by co-clustering and 1D-clustering on the more  X  X onfusable X  Binary and Bi-nary subject data sets. While co-clustering achieves 0 . 98 and 0 . 96 micro-averaged precision on these data sets respec-tively, 1D-clustering yielded only 0 . 67 and 0 . 648. Figure 2 shows how precision values vary with the number of word clusters for each data set. Binary and Binary subject data sets reach peak precision at 128 word clusters, Multi5 and Multi5 subject at 64 and 128 word clusters and Multi10 and Multi10 subject at 64 and 32 word clusters respectively. Different data sets achieve their maximum at different num-ber of word clusters. In general selecting the number of clusters to start with is a non-trivial model selection task and is beyond the scope of this paper. Figure 3 shows the fraction of mutual information lost using co-clustering with varied number of word clusters for each data set. For opti-mal co-clusterings, we expect the loss in mutual information to decrease monotonically with increasing number of word clusters. We observe this on all data sets in Figure 2; our initialization plays an important role in achieving this. Also note the correlation between Figures 2 &amp; 3: the trend is that the lower the loss in mutual information the better is the clustering. To avoid clutter we did not show error bars in Figures 2 &amp; 3 since the variation in values was minimal. Figure 4 shows a typical run of our co-clustering algorithm on the Multi10 data set. Notice how the objective func-tion value(loss in mutual information) decreases monotoni-cally. We also observed that co-clustering converges quickly in about 20 iterations on all our data sets.
 Table 5 shows micro-averaged-precision measures on all our Figure 2: Micro-averaged-precision values with var-ied number of word clusters using co-clustering on different NG20 data sets. Figure 3: Fraction of mutual information lost with varied number of word clusters using co-clustering on different NG20 data sets.
 Table 5: Co-clustering obtains better micro-averaged-precision values on different newsgroup data sets compared to other algorithms. Figure 4: Loss in mutual information decreases monotonically with the number of iterations on a typical co-clustering run on the Multi10 data set. data sets; we report the peak IB-Double and IDC preci-sion values given in [19] and [8] respectively. Similarly, in the column under co-clustering we report the peak preci-sion value from among the values in Figure 2. On all data sets co-clustering performs much better than 1D-clustering clearly indicating the utility of clustering words and doc-uments simultaneously. Co-clustering is also significantly better than IB-Double and comparable with IDC support-ing the hypothesis that word clustering can alleviate the problem of clustering in high dimensions.
 We now show the kind of structure that co-clustering can discover in sparse word-document matrices. Figure 5 shows the original word-document matrix and the reordered ma-trix obtained by arranging rows and columns according to cluster order to reveal the various co-clusters. To simplify the figure the final row clusters from co-clustering are or-dered in ascending order of their cluster-purity distribution entropy. Notice how co-clustering reveals the hidden spar-sity structure of various co-clusters of the data set. Some word clusters are found to be highly indicative of individual document clusters inducing a block diagonal sub-structure while the dense sub-blocks at the bottom of the right panel of Figure 5 show that other word clusters are more uniformly distributed over the document clusters. We observed similar sparsity structure in other data sets.
 While document clustering is the main objective of our ex-periments the co-clustering algorithm also returns word clus-ters. An interesting experiment would be to apply co-clustering to co-occurrence matrices where true labels are available for both dimensions. In Table 6 we give an example to show that word clusters obtained with co-clustering are mean-ingful and often representative of the document clusters. Table 6 shows six of the word clusters obtained with co-clustering on Multi5 subject data set when a total of 50 word clusters and 5 document clusters are obtained. The vidual newsgroups with each word cluster containing words Table 6: Word Clusters obtained using co-clustering on the Multi5 subject data set. The clusters  X  x 13 ,  X  x 14 ,  X  x 16 ,  X  x 23 and  X  x 24 represent rec.motorcycles , rec.sport.baseball , comp.graphics , sci.space and talk.politics.mideast newsgroups respectively. For each cluster only top 10 words sorted by mutual in-formation are shown. indicative of a single newsgroup. This correlates well with the co-cluster block diagonal sub-structure observed in Fig-ure 5. Additionally there are a few clusters like  X  x 47 which contained non differentiating words; clustering them into a single cluster appears to help co-clustering in overcoming noisy dimensions. We have provided an information-theoretic formulation for co-clustering, and presented a simple-to-implement, top-down, computationally efficient, principled algorithm that inter-twines row and column clusterings at all stages and is guar-anteed to reach a local minimum in a finite number of steps. We have presented examples to motivate the new concepts and to illustrate the efficacy of our algorithm. In particular, word-document matrices that arise in information retrieval are known to be highly sparse [7]. For such sparse high-dimensional data, even if one is only interested in document clustering, our results show that co-clustering is more ef-fective than a plain clustering of just documents. The rea-son is that when co-clustering is employed, we effectively use word clusters as underlying features and not individual words. This amounts to implicit and adaptive dimensional-ity reduction and noise removal leading to better clusters. As a side benefit, co-clustering can be used to annotate the document clusters.
 While, for simplicity, we have restricted attention to co-clustering for joint distributions of two random variables, both our algorithm and our main theorem can be easily ex-tended to co-cluster multi-dimensional joint distributions. In this paper, we have assumed that the number of row and column clusters are pre-specified. However, since our formu-lation is information-theoretic, we hope that a information-theoretic regularization procedure like MDL may allow us to select the number of clusters in a data-driven fashion. Finally, as the most interesting open research question, we would like to seek a generalization of our  X  X ard X  co-clustering formulation and algorithms to an abstract multivariate clus-tering setting that would be applicable when more complex interactions are present between the variables being clus-tered and the clusters themselves.
 word clusters). The shaded regions represent the non-zero entries. ported by NSF CAREER Award No. ACI-0093404 and Texas Advanced Research Program grant 003658-0431-2001. [1] P. Bradley, U. Fayyad, and C. Reina. Scaling [2] Y. Cheng and G. Church. Biclustering of expression [3] T. Cover and J. Thomas. Elements of Information [4] I. S. Dhillon. Co-clustering documents and words [5] I. S. Dhillon, J. Fan, and Y. Guan. Efficient clustering [6] I. S. Dhillon, S. Mallela, and R. Kumar. A divisive [7] I. S. Dhillon and D. S. Modha. Concept [8] R. El-Yaniv and O. Souroujon. Iterative double [9] R. A. Fisher. On the interpretation of  X  2 from the [10] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. [11] J. A. Hartigan. Direct clustering of a data matrix. [12] T. Hofmann. Probabilistic latent semantic indexing. [13] T. Hofmann and J. Puzicha. Latent class models for [14] A. K. Jain and R. C. Dubes. Algorithms for Clustering [15] Ken Lang. News Weeder: Learning to filter netnews. [16] A. K. McCallum. Bow: A toolkit for statistical [17] B. Mirkin. Mathematical Classification and Clustering . [18] N. Slonim, N. Friedman, and N. Tishby.
 [19] N. Slonim and N. Tishby. Document clustering using [20] N. Tishby, F. C. Pereira, and W. Bialek. The
