 given learning task depends not only on the type of regulariz ation penalty used (e.g., L but also (and perhaps even more importantly) on the choice of hyperparameters governing the regu-support vector machine (SVM) is usually tuned by training th e SVM with several different values of
C , and selecting the one that achieves the best performance on a holdout set. In many situations, using multiple hyperparameters gives the distinct advanta ge of allowing models with features of varying strength; for instance, in a natural language proce ssing (NLP) task, features based on word bigrams are typically noisier than those based on individua l word occurrences, and hence should be  X  X ore regularized X  to prevent overfitting. Unfortunatel y, for sophisticated models with multiple hyperparameter settings quickly grows infeasible as the nu mber of hyperparameters becomes large. neural network modeling community [20, 21, 1, 12]. More rece ntly, similar cross-validation opti-mization techniques have been proposed for other supervise d learning models [3], including sup-ods [18, 17, 39]. Here, we consider the problem of hyperparam eter learning for a specialized class of structured classification models known as conditional log-linear models (CLLMs), a generaliza-tion of conditional random fields (CRFs) [19].
 y  X  X  (where Y = { X  1 } ), the input space X and output space Y in a structured classification task generally contain complex combinatorial objects (such as s equences, trees, or matchings). Design-ing hyperparameter learning algorithms for structured cla ssification models thus yields a number of paper, we derive a gradient-based approach for optimizing t he hyperparameters of a CLLM using the loss incurred on a holdout set. We describe the required algo rithms specific to CLLMs which make the needed computations tractable. Finally, we demonstrat e on both simulations and a real-world computational biology task that our hyperparameter learni ng method can give gains over learning flat unstructured regularization priors. Conditional log-linear models (CLLMs) are a probabilistic framework for sequence labeling or pars-ing problems, where X is an exponentially large space of possible input sequences and Y is an exponentially large space of candidate label sequences or p arse trees. Let F : X X Y X  R n be a fixed vector-valued mapping from input-output pairs to an n -dimensional feature space. CLLMs Z ( x ) = P y  X   X  X  exp( w T F ( x, y  X  )) . Given a training set T = ( x ( i ) , y ( i ) ) m output pairs drawn from some unknown fixed distribution D over X X Y , the parameter learning problem is typically posed as maximum a posteriori (MAP) estimation (or equivalently, regularized logloss minimization): overfitting. Here, C is the inverse covariance matrix of a Gaussian prior on the pa rameters w . parameterized using a small number of free variables, d  X  R k , known as the hyperparameters of the model. Given a holdout set H = (  X  x ( i ) ,  X  y ( i ) )  X  m learning itself can be cast as an optimization problem: In words, OPT2 finds the hyperparameters d whose regularization matrix C leads the parameter this parameterization may be partially motivated by concer ns of hyperparameter overfitting [28], such a choice usually stems from the difficulty of hyperparam eter inference.
 when optimizing multiple hyperparameters. In this section, we lay the framework for multiple hyperpara meter learning by describing a simple yet flexible parameterization of C that arises quite naturally in many practical problems. We t hen describe a generic strategy for hyperparameter adaptation via gradient-based optimization. NLP task, individual word occurrence features may be placed in a separate regularization group from word bigram features. Formally, let k be a fixed number of regularization groups, and let Furthermore, for a vector x  X  R k , define its expansion x  X  R n as x = ( x ily positive definite, so OPT2 can be written as an unconstrai ned minimization over the variables  X  dependence of C on d for notational convenience, we have the optimization probl em For any fixed setting of these hyperparameters, the objectiv e function of OPT2 X  can be evaluated by (1) using the hyperparameters d to determine the regularization matrix C , (2) solving OPT1 using C to determine w  X  and (3) computing the holdout logloss using the parameters w  X  . In this next section, we derive a method for computing the gradient of the objective function of OPT2 X  with respect to the hyperparameters. Given both procedures for f unction and gradient evaluation, we may apply standard gradient-based optimization (e.g., conjug ate gradient or L-BFGS [30]) in order to usually sufficient to determine reasonable hyperparameter s to low accuracy. Note that the optimization objective  X  perparameters d , as implicitly defined by the gradient stationarity conditi on, Cw  X  +  X  0 . To compute the hyperparameter gradient, we will use both of these facts. 4.1 Deriving the hyperparameter gradient First, we apply the chain rule to the objective function of OP T2 X  to obtain where J simply the gradient of the holdout logloss evaluated at w  X  . For decomposable models, this may be computed exactly via dynamic programming (e.g., the forw ard/backward algorithm for chain-structured models or the inside/outside algorithm for gram mar-based models).
 vanishes when w = w  X  , so where C T derivatives. Specifically, we can differentiate both sides of (2) with respect to d where B is the n  X  k matrix whose ( i, j ) th element is I Hessian of the training logloss evaluated at w  X  . Finally, solving these equations for J 4.2 Computing the hyperparameter gradient efficiently In principle, one could simply use (6) to obtain the Jacobian matrix J the n  X  n matrix ( C +  X  2 a typical CLLM requires approximately n times the cost of a single logloss gradient evaluation. more problematic, the  X ( n 2 ) memory usage for storing the Hessian is prohibitive as typic al log-linear models (e.g., in NLP) may have thousands or even milli ons of features. To deal with these Algorithm 1 : Gradient computation for hyperparameter selection.

Input: training set T = ( x ( i ) , y ( i ) ) m
Output: hyperparameter gradient  X  problems, we first explain why ( C +  X  2 in describe an efficient procedure for computing the holdout hy perparameter gradient which avoids the expensive Hessian computation and inversion steps of the di rect method.
 model, computing the product of the Hessian with v can be done quickly, using any of the following techniques, listed in order of increasing implementation e ffort (and numerical precision): Hessian-vector products for graphical models were previou sly used in the context of step-size adap-finite-differencing, provided sufficient accuracy for our a pplication.
 gradient (CG) method to solve the matrix equation (5) to obtain J through matrix-vector products Av . In practice, few steps of the CG algorithm are generally nee ded way amounts to solving k linear systems, one for each column of the J method of forming the ( C +  X  2 the expensive  X ( n 2 ) cost of Hessian computation and matrix inversion.
 of multiple linear systems, which scales poorly when the num ber of hyperparameters k is large. (a) (b) (c) Figure 2: HMM simulation experiments. (a) State diagram of t he HMM used in the simulations. (b) Testing set performance when varying R , using M = 10 . (c) Testing set performance when varying M , using R = 5 . In both (b) and (c), each point represents an average over 10 0 independent runs of HMM training/holdout/testing set generation and CRF train ing and hyperparameter optimization. However, we can do much better by reorganizing the computati ons in such a way that the Jacobian matrix J we observe that it suffices to solve the single linear system, and then form  X  squares problems that must be solved is substantially reduc ed from k to only one. A similar trick was previously used for hyperparameter adaptation in SVMs [ 16] and kernel logistic regression [33]. ulated sequence labeling task involving noisy features, an d a real-world application of conditional log-linear models to the biological problem of RNA secondar y structure prediction. hidden Markov model (HMM) with binary-valued hidden nodes, y binary-valued features x j served features whose values were chosen based on y whose values were chosen to be either 0 or 1 with equal probabi lity, independent of y shows the graphical model representing the HMM. For each run , we used the HMM to simulate training, holdout, and testing sets of M , 10, and 1000 sequences, respectively, each of length 10. which potentials were included for the initial node y y and each x j gradient-based hyperparameter learning using three diffe rent parameter-tying schemes: (a) all hy-perparameters constrained to be equal, (b) separate hyperp arameter groups for each parameter of the model, and (c) transitions, observed features, and noise fe atures each grouped together. Figure 2b shows the performance of the CRF for each of the three paramet er-tying gradient-based optimization schemes, as well as the performance of scheme (a) when using t he standard grid-search strategy of trying regularization matrices C I for C  X  . . . , 2  X  2 , 2  X  1 , 2 0 , 2 1 , 2 2 , . . . . ter than a grid search for single hyperparameter models. Usi ng either a single hyperparameter or (a) (b) tion task. (b) Grouped hyperparameters learned using our al gorithm for each of the two folds. (c) Performance comparison with state-of-the-art methods whe n using either a single hyperparameter (the  X  X riginal X  CONTRAfold), separate hyperparameters, o r grouped hyperparameters. hyperparameter model to overfit. Enforcing regularization groups, however, gave consistently lower corresponding to a relative reduction of 16.2%.
 secondary structure prediction. Ribonucleic acid (RNA) mo lecules are long nucleic acid polymers structure plays an important role in determining the RNA X  X  f unction. Here, we focus on the task of predicting RNA secondary structure, i.e., the pattern of nucleotide base pairings which form the two-dimensional scaffold upon which RNA tertiary structur es assemble (see Figure 3a). prediction program based on CLLMs. In brief, the CONTRAfold program models RNA secondary structures using a variant of stochastic context-free gram mars (SCFGs) which incorporates features chosen to closely match the energetic terms found in standar d physics-based models of RNA struc-loops, interior loops, etc.). To control overfitting, CONTR Afold uses flat L we modified the existing implementation to perform an  X  X uter  X  optimization loop based on our al-gorithm, and chose regularization groups either by (a) enfo rcing a single hyperparameter group, (b) all features for describing hairpin loop lengths were place d in a single regularization group). experimentally-determined secondary structures were alr eady known. We divided this dataset into two folds (denoted A and B) and performed two-fold cross-val idation. Despite the small size of the training set, the hyperparameters learned on each fold w ere nonetheless qualitatively similar, ization hyperparameters correspond to properties of RNAs w hich are known to contribute strongly to the energetics of RNA secondary structure, whereas many o f the features with larger regulariza-with RNA secondary structure or sufficiently noisy that thei r parameters are difficult to determine reliably from the training data.
 (see Figure 3c). 4 Using separate or grouped hyperparameters both gave increa sed sensitivity and increased specificity compared to the original model, which was learned using a single regulariza-tion hyperparameter. Overall, the testing logloss (summed over the two folds) decreased by roughly 6.5% when using grouped hyperparameters and 2.6% when using multiple separate hyperparame-ters, while the estimated testing ROC area increased by roug hly 3.8% and 3.4%, respectively. In this work, we presented a gradient-based approach for hyp erparameter learning based on mini-error is fairly natural, in many other supervised learning m ethods besides log-linear models, other objective functions have been proposed for hyperparameter optimization. In SVMs, approaches based on optimizing generalization bounds [4], such as the r adius/margin-bound [15] or maximal discrepancy criterion [2] have been proposed. Comparable g eneralization bounds are not generally known for CRFs; even in SVMs, however, generalization bound -based methods empirically do not outperform simpler methods based on optimizing five-fold cr oss-validation error [8]. the Bayesian approach of treating hyperparameters themsel ves as parameters in the model to be es-timated. In an ideal Bayesian scheme, one does not perform hy perparameter or parameter inference, this paper, however, the computational expense of sampling -based strategies can be extremely high due to slow convergence of MCMC techniques [26].
 (ARD) [22], take the intermediate approach of integrating o ver parameters to obtain the marginal likelihood (known as the log evidence), which is then optimi zed with respect to the hyperparame-One method for doing this involves approximating the parame ter posterior distribution as a Gaussian models. An alternate approach based on using a modification o f expectation propagation (EP) [25] was applied in the context of Bayesian CRFs [32] and later ext ended to graph-based semi-supervised learning [14]. As described, however, inference in these mo dels relies on non-traditional  X  X robit-style X  potentials for efficiency reasons, and known algorit hms for inference in Bayesian CRFs are limited to graphical models with fixed structure.
 grammar-based models common in computational biology and n atural language processing. Fur-one changes the hyperparameters based on the holdout loglos s gradient. The gradient computation relies primarily on a simple conjugate gradient solver for l inear systems, coupled with the ability to compute Hessian-vector products (straightforward in an y modern programming language that al-lows for operation overloading). As we demonstrated in the c ontext of RNA secondary structure prediction, gradient-based hyperparameter learning is a p ractical and effective method for tuning hyperparameters when applied to large-scale log-linear mo dels.
 mization of hyperparameters and parameters; these results suggest that similar procedures for faster hyperparameter learning that do not require a doubly-neste d optimization may be possible.
