 Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports ("pharmacovigilance"). The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, and there is no easily linked information on drug exposure in the reporting popul ation. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret. To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKlin e collaborated to create a web-based safety data mini ng web environment. The analytical core is a high-performance implementati on of the MGPS (Multi-Item Gamma Poisson Shrinker) algorith m described previously by DuMouchel and Pregibon, with se veral significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for explor ing associations, and drilldown to case details. Substantial work was involved in preparing the raw adverse event data for mi ning, including harmonization of drug names and removal of duplicate reports. The environment can be used to explore both drug-event and multi-way associations (interacti ons, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations. H.2.8 [ Database Management ]: Database Applications  X  data mining, scientific databases. Data mining, empirical Bayes me thods, association rules, post-marketing surveillance, pharmacovigilance. It is widely recognized that ther e are practical limits on the degree to which safety profiles of therapeutic products (drugs, vaccines, medical devices) can be fully char acterized before these products are approved for marketing: pre-ma rketing studies are inherently too short, with study populations that are too small and too homogeneous, to be able to detect important but relatively rare adverse events. The opportunity for a new drug X  X  true side effect profile to reveal itself is often realized after the drug is approved and then used in conjunction with other therapies. To provide an objective basis for monitoring and assessing the safety of marketed products, pharmaceutical companies and regulatory agencies have implemented post-marketing surveillance activities ( X  X harmacovigilance X ) based in large measure on the collection of spontaneously generated adverse reaction reports. Report initiation (by health professionals and consumers) is generally voluntary; by contrast, the pharmaceutical companies are generally under legal obligation to follow up on reports that they receive and to pass them along to various regulatory authorities. As a result of these pharmacovigila nce efforts, a number of large databases of spontaneous adverse event reports have come into existence: Each major pharmaceutical company has a proprietary database of reports focused on cases in which one of the company's products was considered to be  X  X uspect X  (~500,000 reports for the larger pharmaceutical companies). In addition, there are several combined data bases maintained by government regulatory agencies and health authorities that are available in varying degrees for public use; so me of these combined databases contain as many as ~3,000,000 reports. Although the various private and public databases differ in detail, the core content is demographic record (age, gender, date of event, seriousness of event), one or more therapeutic product records (generic or trade name, suspect or concomita nt designation, route of administration, dosage), and one or more records documenting a sign, symptom or diagnosis (typ ically represented as a coded  X  X vent term X  based on a standardi zed medical coding dictionary). Individual databases may also contain narratives (from which event terms were coded), outcome s (e.g., hospitalization, death), and report source (consumer or h ealth professional, domestic or foreign). These databases of spontaneous reports represent the largest existing sources of information relating specifically to the side-effect profiles of marketed therapeutic products. Systematic analysis of this data has proved difficult both for conceptual and practical reasons: concerns about the effects of underreporting,  X  X enominator problem X ), inconsis tencies and evolution over time in naming/coding practices, and technical issues relating to database access and computational tractability. In the absence of systematic analysis methods, the emphasis historically has been on largely non-analysis-based  X  X onitoring X  through such techniques as case-by-case examination of newly reported cases, tabulations of counts of events for specific drugs, and detailed review of all the data fields (including, specifically, the free-text medical narratives) of cases associated with a possible safety concern. These traditional approaches tend to be highly dependent on the knowledgeability and alertness of individual safety reviewers. They also suffer from an absence of contextual information: in isolation, it can be very difficult to tell whether 10 cases of a specific drug-event combination is disproportionately frequent such that is  X  X nteresting X  and merits further investigation. There has been a growing interest, originating at government health authorities, in the potential use of statistical data mining ( X  X ssociation finding X ,  X  X isproportionality analysis X ) as a means of extracting knowledge from pharmac ovigilance databases. Such  X  X afety data mining X  holds the pr omise of contributing to a more systematic approach to the monitoring of drug safety and to the earlier detection of potential problem areas. In 1997, Fram and DuMouchel began a long-standing re search partnership with Dr. Ana Szarfman, Medical Officer at the Food and Drug Administration Center for Drug Evaluation and Research, to experiment with the applicati on of DuMouchel X  X  Gamma Poisson Shrinker ( X  X PS X ) and Multi-It em Gamma Poisson Shrinker ( X  X GPS X ) techniques to FDA X  s combined SRS and AERS databases and to validate results retrospectively against known drug-induced adverse reactions [1,2,3,4]. Independent, parallel efforts have proceeded at the UK Medicines Control Agency (MCA) where Dr. Stephen Evans has explored the use of proportional reporting ratios (PRR X  X )[5] and at the Uppsala Monitoring Centre where Edward s, Lindquist, Bate and others have pursued the use of techni ques based on Bayesian neural networks[6]. GlaxoSmithKline ( X  X SK X ) and Li ncoln Technologies ( X  X incoln X ) began a collaboration in 2001 to apply safety data mining techniques both to provide direct leverage in addressing GSK X  X  core business problems of pharmacovigilance and risk management and, more broadly, to create decision support tools to assist with a variety of comp lex safety-related business issues. This work began in a  X  X ervice bureau X  mode, where Lincoln staff performed data mining runs us ing the MGPS software and the publicly available FDA adverse event data ( X  X OI AERS X ) and delivered analysis results in tabular and graphical form to GSK pharmacovigilance staff. At this stage of the collaboration, data mining projects required manual integration of results from several different softwa re tools utilized for the distinct steps of data extraction/transformati on, data mining, and output visualization. Positive early scie ntific results demonstrated the practicality and utility of the overall approach, and also suggested the desirability of providing GSK pharmacovigilance staff with a means for direct access to the technology to support even wider application of the approach across a number of GSK X  X  safety surveillance sites. The focus of this paper is to report on the joint effort by Lincoln and GSK to define, design, impl ement, test, and deploy a web-based visual data mining e nvironment ( X  X ebVDME X ) that packages sophisticated KDD tec hniques for pharmacovigilance in a format that is accessible to the intended medical end-user community. The WebVDME project was undertaken on the understanding that the initial focus would be on creating a custom solution to meet GSK X  X  specific needs, but that the software would eventually become a commercial product generalized and enhanced for deployment at other pharmaceutical industry and government clients. The application would st art out being hosted at Lincoln, but could later be moved to reside on a system or systems at GSK. Lincoln and GSK also realized that the project would need to follow a documented System Deve lopment Life Cycle (SDLC) development process appropriate to a major application in a regulated industry and would also benefit from close interaction between the developers and end user s to try out and refine system features. The formal process began with a list of GSK X  X  primary business requirements for the system, which included:  X  Implementation as a  X  X hin clie nt X  web-based application (no software, controls, applets, et c., to be installed on client computer), compatible with access through GSK X  X  existing firewalls and compliant with GSK X  X  major IT standards (MS 
Windows 2000 servers, Oracle database).  X  Data mining based on MGPS, including identification of signals related to two-way (drug-event) and multi-way (drug interaction, multi-event syndrome) associations, with end-user control over choices regarding stratification and subsetting.  X  Access to the major public U.S. drug and vaccine databases, with ability to select dictionary level and combining strategy for drug and event categories (e.g., use of  X  X rand X  versus  X  X eneric X  names for drugs, lumping of closely synonymous adverse event symptom terms) .  X  A user interface suitable for direct use by medical staff.  X  Output of data mining results in graphical and tabular form by means of the web interface, including screening and subsetting of results, and also the ability to download results for use with 
Excel and with other third-party graphics and statistical packages. Based on these results, a technical architecture for the application was designed as shown belo w in Figure 1, below. The various computer system components depicted in the architecture perform the following roles:  X  Client computers (standard PC workstations)  X  support the operation of Internet Explorer to provide browser access to the data mining environment. Al so support optional software packages (e.g., Microsoft Excel) for use with data downloaded from the data mining environment.  X  Application server (Intel/Windows 2000 Server/Advanced Server or Sun Sparc/Solaris 2.8)  X  runs web server and Java 
Server Page (JSP) server to support the WebVDME application, which is implemen ted as JSP pages and supporting 
Java classes.  X  Oracle server(s) (Oracle 8i or 9i)  X  runs the Oracle database that contains the safety databa se(s), data mining results, and administrative/control in formation for WebVDME.  X  Data mining server (Intel/Windows 2000 Server/Advanced 
Server or Sun Sparc/Solaris 2.8)  X  runs MGPS data mining algorithm plus Java support classes. The present configuration hosted by Lincoln for GSK includes a dedicated 2-processor Windows server (Dell PowerEdge 2650), with 4 GB of main memory and 100 GB of disk space. As described above, there exist a variety of public and company-specific pharmacovigilance databases which share a common conceptual organization (case repor ts that contain information on demographics, drugs, and events) but differ in details of field and table naming, presence or absence of specific attributes, and use of specific medical coding dictionaries. To insulate both the application and the end users fro m superficial variation in the detailed formatting of the target safety database, we decided on a design that supports multiple database  X  X onfigurations X  each of which defines a mapping between user-visible  X  X ariables X  and specific database tables and co lumns, including specification of plausible roles for the variables (e.g., a drug name, an event name, These configurations can be set up, using a set of configuration management WebVDME web pages, by expert users who are familiar both with the target database schema and the pharmacovigilance application; me dical end-users work with the application-specific variables so defined (end-users do not need to be aware of the target database schema). Multiple configurations can be used to support user access to several different databases (e.g., to do a data mining run first on the public data and then on in-house data), and also to severa l different versions of the same database (e.g., different chronological snapshots, or versions that include or exclude so-calle d  X  X oncomitant X  medications). The specific database that was the initial focus of the GSK collaboration is the public-release version of FDA X  X  AERS database (FOI AERS), which contains approximately 3,000,000 adverse event reports collected from 1968 to the present (available as series of quarterly updates from NTIS). Substantial pre-processing of this data is re quired to make it appropriate for MGPS safety signaling (this pre-pr ocessing must be carried out with each new quarterly release of the public data). There are several distinct reasons for pre-processing. First, because the data was collected over more than three decades, using several different databa se organizations, mapping and recoding is required to present a uniform view of the data. Second, drug names are collected as free text, with substantial variation among submitting organiza tions and individuals in how trade, generic, or mixed names are entered; whether packaging, route-of-administration and dosi ng information is provided; and in punctuation and spelling; etc. Also, the desired granularity of drug information for use in data mining can vary depending on the intended analysis goal (while it is often most useful to consider drugs based on the same molecular entity as equivalent, trade names may be useful in distinguishing between drugs produced by different manufacturers or between uses of a common substance for different di sease indications). Lincoln uses an extensive set of parsing and lookup tables to make consistent generic and trade names available. Third, there is a need to remove duplicates. Adverse event databases typically contain multiple versions of the same case because of regulatory requirements to submit an initial expedited report plus a series of follow-up reports as addition information about a case because available. For purposes of statistical data mining these multiple versions need to be collapsed to a single  X  X est representative version X  of the case. Beyond this versioning problem, there are several other significant sources of case duplication: multiple reports of the same medical event by different manufacturers and repor ts arriving through different pathways. In the context of an association-finding technique, report duplication represents a common source of false positives (especially for otherwise rare highe r-order associations). Lincoln has implemented sophisticated  X  X uzzy-equality X  case-matching algorithms to identify likely duplicate reports. Based on the success of the initial project accessing FOI AERS, GSK has decided to extend the use of the data mining system to operate on its internal adverse ev ent ( X  X CEANS X ) database. This extension to working with the OCEANS data does not require software modification because of WebVDME X  X  configuration layer, described above, that ta kes care of the mapping between user-visible data mining variable names and the specific logical and physical structures used in the pharmacovigilance database. The analytical core of the WebVDME application is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm descri bed previously by DuMouchel and Pregibon [2]. MGPS is based on the metaphor of the  X  X arket basket problem X , in which a data base of  X  X ransactions X  (adverse event reports) is mined for the occurrence of interesting (unexpectedly frequent) itemsets (e.g., simple drug-event pairings or more complex combinations of drugs and events representing interactions and/or syndromes). Interestingness is related to the factor by which the observed freque ncy of an itemset differs from a nominal baseline frequency. The baseline frequency is usually taken to be the frequency that would be expected under the full independence model, in which the likelihood of a given item showing up in a report is independe nt of what other items appear in the report. (Other choices for the baseline frequency are possible; see discussion of  X  X  omparative analysis X  below.) For each itemset in the database, a relative reporting ratio RR is defined as the observed count N fo r that itemset divided by the expected count E. When usi ng the independence model as the basis for computing the expected count, MGPS allows for the possibility that the database ma y contain heterogeneous strata with significantly different item frequency distributions occurring in the various strata. To avoid concluding that an itemset is unusually frequent just because the items involved individually all tend to occur more frequently in a particular stratum ( X  X impson X  X  paradox X ), MGPS uses the Mantel-Haenszel approach of computing strata-specific expect ed counts and then summing over the strata to obtain a database-wide value for the expected count. To improve upon the estimation of  X  X rue value X  for each RR (especially for small counts), the empirical Bayesian approach of MGPS assumes that the many obser ved values of RR are related in that they can be treated as having arisen from a common  X  X uper population X  of unknown, true RR-values. The method assumes that the set of unknown RR is distributed according to a mixture of two parameterized Gamma Poi sson density functions, and the parameters are estimated from a maximum likelihood fit to the data. This process provides a  X  X ri or distribution X  for all the RR X  X , and then the Bayes rule can be used to compute a posterior distribution for each RR. Since this method improves over the simple use of each N/E as the estimate of the corresponding RR, it can be said that the values of N/E borrow strength from each other to improve the reliability of every estimate. The improved estimates of RR X  X eferred to as EBGM (Empirical Bayes Geometric Mean) values X  X  re actually derived from the expectation value of the logarithm of RR under the posterior probability distributions for each true RR. EBGM is defined as EBGM = exponential of expectation value of log(RR). EBGM has the property that it is nearly identical to N/E when the counts are moderately large, but is  X  X hr unk X  towards the average value of N/E (typically ~1.0) when N/E is unreliable because of stability issues with small counts. The posterior probability distribution also supports the calculation of lower and upper 95% confidence limits (EB05, EB95) for the relative reporting ratio. A technical summary of MGPS is included at the end of this paper. Two new extensions to the MG PS data mining algorithm were developed in response to pharmac ovigilance data mining needs: 1. Ability to shrink toward the all-2-factor model when looking 2. Highlighting period-to-period change . MGPS can make use The WebVDME project benefited substantially from the availability of an initial C++ implementation of MGPS developed over a period of years in collabora tion with DuMouchel, and also from a recent NIH-sponsored activity to develop a high-performance implementation of th e method. The association-counting phase of the algorithm (which accounts for much of the time and space required for execution) uses a modification of the  X  X  priori X  method to prune counti ng of higher-order associations when the counts for their component lower-order associations imply that the higher-order count cannot meet a minimum count threshold [7]. Counting is implem ented using a hash table to store the count data, with an ability to write out and merge partial results when working with very large problems that would exceed available physical memory or address space. In order to speed up the maximum likelihood estimation of the parameters of the prior distribution in the empirical Baye s model, a summary of the table of counts and baseline values, com puted using the data squashing methods of DuMouchel et al [8] is substituted for the much larger baseline values file. The WebVDME user interface is organized around a set of  X  X abs X  and links that lead to major components of the system. The principal tabs are: Data Mini ng (specifying and initiating data mining runs), Analyze (exploring data mining results), and Case Series (reviewing the details of specific cases identified through data mining). There are also a set of administrative functions related to creating new users and to granting privileges for using the different components of the system in a production environment. MGPS data mining runs are defined through a  X  X izard X  (a multi-step series of web page dialogs) that guides the user in selecting the variables to be used as th e source of data mining items, in setting up stratification and subs etting, and in making various other technical choices such as the maximum number of items in an itemset and the minimum c ount required to consider an itemset. The first page in the wi zard is shown in Figure 2 below. When a run definition is completed, it can be submitted for execution (either immediately or at a scheduled time). Execution proceeds in the background; the user can continue working to define or analyze other runs, or can log out. Email notification of the completion of submitted runs can be requested. Figure 2. Selecting variables for use in data mining. The results of a run are stored in an Oracle-based output table that can be accessed by clicking the Analyze tab or the Analyze link in the Run History table. The Run Results table can be quite large (e.g., 500,000 rows), and a set of filtering capabilities is provided to help focus on associations in the table of particular interest. The Filter dialog is shown in Figure 3. Through filtering, the user can select a set of specific drugs and events to show in the table (using either the primary terms for drugs and adverse events or, if supported by the specific safety database, by higher-level term cate gories in a dictionary). It is also possible to focus on a specific dimension (e.g., 2-way associations in a run that involves both 2-way and 3-way associations), or on a specific pattern of item types (e.g., only associations involving two drugs and one event). Additional selection criteria can be provided through a SQL WHERE clause (e.g., specifying EB05 &gt; 2 to focus on associations that reliably occur at least twice as frequently as would be expected from independence). By default, the columns shown include the items in the association, the observed (N) and expected (E) counts, RR, EBGM, and the 5th and 95th percentile confidence limits (EB05, EB95). Additional results columns are optionally available. Filtered tabular results can also be downloaded as a spreadsheet. By clicking on the leftmost column (with the magnifying glass), the user can drill down to a list of the specific adverse event reports behind the association. Th is list can be used to drill down, further, to full details for indivi dual case reports, or can be saved potential safety signal via a careful case-by-case review. WebVDME also supports a set of application-specific graphs. Figure 5 shows one graph type, us ed to track the emergence of safety signals over time when performing a MGPS run involving the iterative analysis of cumulative subsets. Graphs are implemented as GIF out put; display of the graph takes place wholly on the server and can be viewed on any browser. This simple approach to graphi cs simplifies deployment relative to schemes involving downloading controls or applets to the client, which can run into client computer configuration and firewall issues. Some interactivity is provided through use of  X  X ouseover X  to show the statistic s behind elements of the graph, and by providing drilldown from graphical elements to the supporting case reports. In cases where more sophisticated graphics are required, WebVDME provides for download of data mining output to graphics packages popular in the pharmaceutical industry. Additional system capabilities include site administration, including control over how batch j obs are assigned to processors, and a variety of information disp lays ( X  X udit trails X  documenting run and analysis choices that lie behind a display, descriptions of runs and case series, on-line  X  X el p X  screens describing system use). WebVDME has also been integrated with a data simulation capability (developed with support from CDC), to generate artificial background databases and signals for use in evaluating MGPS signal detection operating characteristics through Monte Carlo techniques. Over the 8-month WebVDME deve lopment period, the principal Lincoln team members included a project manager/application specialist, 3 senior software de velopers, and a senior technical writer/software quality specialist. GSK project participants included pharmacovigilance manage rs and staff and a software quality team. The month-by-month sequence of development activities in the project was as described in Table 1. pre-Feb 2002 Feb 2002 Mar 2002 Apr 2002 May 2002 Jun 2002 Jul 2002 Aug 2002 Sep 2002 We attribute the combined team X  X  ability to keep to this ambitious schedule to several factors:  X  Ability to make substantial use of prior design and implementation experience by th e team members in supporting pharmacovigilance use of data mining with predecessor tools, in developing and deploying se veral JSP-based applications and in creating, tuning, and validating our high-performance 
C++ implementation of MGPS.  X  Intense and enthusiastic partic ipation by GSK staff throughout the development process, especia lly in the hands-on testing of interim releases and the production release of the software.  X  Conservative choice of operating environment (using a well-established production environmen t: Oracle, TomCat, Internet Explorer, and solid development tools: JBuilder Enterprise, 
Visual SourceSafe). Testing re quirements were simplified by standardizing on a single SQL data base supplier (Oracle) and a single browser environment (IE 5.5 and later). Important architectural choices in the design of the system were:  X  Strict use of server-centric development technologies (HTML, 
GIF, JSP pages) that facilitate widespread deployment without concern for the details of clie nt computer configurations, network firewalls, corporate s ecurity policies, etc.  X  Maintenance of all important da ta resources (source databases, data mining results tables, configurations, etc.) in Oracle to provide for stable storage, secur ity, and rapid results retrieval.  X  Integrated batch execution of data mining runs, with end-user capabilities for submitting and monitoring runs from within the system, so compute-intensive data mining runs can take place in the background without interfe ring with interactive use. A specific technique used by the project team to support rapid design, implementation, and evaluation while still generating the written design artifacts necessary fo r validation was to create and maintain a comprehensive set of context-specific  X  X elp X  screens starting very early in the project. These  X  X elp X  screens were used simultaneously as design documentation for the user interface, as the primary source for generation of specifications, test scripts and end-user documentation, a nd as a means of technical communication among the developers , testers, and GSK users. Software construction was perform ed by a small group of expert programmers, under the direction of Lincoln X  X  software architect and chief technical officer. Code was constructed using standard development tools (JBuilder) within Lincoln X  X  JSP/Java architecture, following a common coding style. All sources were maintained in SourceSafe (accessed through the web-based SourceOffSite front end) and were checked in frequently. Programmers performed coding and unit testing in a laptop computer environment that was capable of running the application and database. Builds and insta lls on shared servers took place regularly (once or twice a week ear ly in the cycle, daily later in the cycle). Problems were entered in and tracked using the ProblemTracker software from the time that initial coding and unit testing were completed on a module. Throughout the project, performance problems were the most important cause for rework of software modules, and a con tinued effort was necessary to achieve reasonably good performance on large-scale data mining problems  X  these were generally due to technical issues such as database loading or indexing, Java garbage collection, etc., rather than to the data mining algorithm itself. A major focus of the project was testing. All developers were responsible for performing unit tes ting of software modules that they develop or modify. Auto mated regression testing was used primarily to ensure that changes to the MGPS statistical algorithm did not have unintended effect s on the computation of signal scores. An extensive suite of ma nual test scripts was developed to support formal testing requirement s. While both the regression testing and the formal manual testing were effective in detecting situations where changes broke previously-working software, we found that aggressive testing of the system on large, realistic problems was still necessary to reach a high level of reliability. GSK staff made crucial contribu tions to the testing process through design and execution of formal user acceptance tests (based on application of WebVDME to realistic problems of interest). GSK was al so involved in informal early testing of new functionality, so that poor interface choices could be caught and corrected quickly. User participation in testing was facilitated by the release of a first version of th e software several months before the production version. This release contained most of the functionality and permitted end-to-end experimentation while several of the more challenging f eatures were being completed. Lincoln has ongoing maintenance responsibility for WebVDME, which has evolved from a custom application to a pharmacovigilance software product that can be installed at a user organization or provided as an ex ternal service. Several other pharmaceutical companies have begun pilot testing of the application (including one that began testing during the latter stages of the implementation pr oject described here), and the WebVDME software has been delivered to FDA and to CDC as well. Recently, Lincoln entered into a formal Cooperative Research and Development Ag reement (CRADA) with FDA, under which WebVDME is being enhanced to serve as an internal data mining resource for medical officers and safety evaluators associated with the monitoring of marketed drugs and vaccines. At GSK, WebVDME data mining ha s been used to assist with safety screening as part of day-to-day pharmacovigilance and risk management practice, and to carry out special projects for hypothesis generation and refinement as an adjunct to formalized methods (clinical trials, pharmaco epidemiological studies). Project areas have included:  X  Comparisons of safety profiles of drugs within a drug class  X  Exploration of possible drug-drug interactions  X  Analysis of adverse events that could be attributed to one of several drugs in multi-drug therapy ( X  X nnocent bystander X /  X  X uilty bystander X  problems)  X  Peri-approval risk manageme nt planning for new drug applications  X  Study of adverse events in special populations  X  Analysis of signal evolution over time to understand trends Example scenarios of use include: Effects in Special Populations : As one input to deciding whether it was appropriate to conduct clini cal trials in a special population of a highly effective drug for a serious condition, a study can be conducted to examine whether there is evidence from data mining that a known adverse event might occur more frequently in that special population. The AERS database can be essentially divided into two data sets base d on membership in the special population, and signal scores and their 90% confidence interval values calculated for the drug-event pair in each group and compared. Analysis of Possible Drug Combination : In the evaluation of a potential new therapy based on the combination of two marketed drugs (which had in some cases been co-prescribed by physicians), data mining can be used as one tool for ascertaining whether toxicity associated with the primary drug might be exacerbated by the presence of the secondary drug. The analytical approach can be based on recoding cases in the database to distinguish between cases where only the primary drug is reported and cases where both primary and secondary drugs are reported. Analysis of Adverse Events in Polytherapy : When several products are co-prescribed, safety signals may emerge where it is difficult to discern which products are properly associated with the event of interest. Data mining can be used to conduct analyses of specific subgroups wher e each drug of interest is used ( X  X onotherapy X ); this is illustrated for hypothetical data in Figure 6 below. While these analyses mu st be interpreted with extreme caution and are intended only as a first step towards hypothesis generation, comparison of signals seen in these groups may help to clarify associations and potentially help to direct the focus of future clinical studies. 
Figure 6. Discriminating drug contributions in polytherapy We were fortunate in collecting ke y user input early in the design of this application, and many aspects of the development were accomplished more easily than anticipated. Several areas were more difficult and yielded only to sustained and repeated effort, including data preparation and data cleaning, performance measurement and optimization, a nd testing on large problems. These would be good areas for improved tools or techniques. We believe that the developm ent, deployment, and use of WebVDME demonstrates that s ophisticated KDD tools can be  X  X ackaged X  in a form where they can be used effectively by end-users to explore complex problems in a mission-critical application. These applications can be factored conveniently into functionality for staff with differe nt roles (administrator, data mining specialist, data mining re sults reviewer). Currently available mainstream web-site development technologies (web and application servers, relati onal databases) are capable of supporting the creation, with in reasonable time and budget constraints, of server-based KDD applications that can be readily deployed and maintained. Furthe r, many of the components of the application (user administrati on, batch queue operation, output filtering, table and graph displa y) are relatively general purpose and will be reused by Lincoln in other data-centric applications. For an arbitrary itemset, it is desired to estimate the expectation  X  = E[ N/E ], where N is the observed frequency of the itemset, and E is a baseline (null hypothesis) count; e.g., a count predicted from the assumption that items are inde pendent. An itemset is defined involving both items i and j , E ijk is the baseline prediction for the number of reports including the itemset triple ( i, j, k ), etc. A common model for computing baseline counts is the assumption of within-stratum independence; when E is computed under this assumption we shall often denote it by E0 . Assume that all reports are assigned to strata denoted by s = 1, 2, ..., S . Let:
P i s = proportion of reports in stratum s that contain item i n s = total number of reports in stratum s Baseline frequencies for pairs and triples are defined under independence as: E0 ij =  X  s n s P i s P j s E0 ijk =  X  s n For itemsets of size 3 or more, an  X  X ll-2-factor X  loglinear model can be defined as the frequencies E2 for the itemsets that match all the estimated pairwise two-way marginal frequencies but contain no higher-order depe ndencies. For triples, E2 ijk the estimates for the three pairs: For 4-tuples, E2 ijkl agrees with 6 such pairs, etc. Then for itemsets of size 3 or more we compare the estimated frequency to the all-2-factor prediction by simple subtraction. For example, in case of triples: The parameters  X  above are estimated by their geometric means, denoted EBGM, of their empirical Bayes posterior distributions . For simplicity, the formulas belo w use just two subscripts, for itemsets of size 2, such as the occurrence of drug i and symptom j in a medical report. Estimates for other itemset sizes are computed analogously. Let:  X  ij = the observed counts E ij = the expected (baseline) counts 
RR ij = N ij /E ij = ratio of observed to baseline We wish to estimate  X  ij =  X  ij / E ij , where  X  Assume a superpopulation model for  X  ij (prior distribution) based on a mixture of two gamma di stributions (a convenient 5-parameter family of distributions that can fit almost any empirical distribution):  X ( X ;  X  1 ,  X  1 ,  X  2 ,  X  2 , P ) = P g(  X  ;  X  1 ,  X  g(  X  ;  X ,  X ) =  X   X   X   X  X 1 e  X  X  X  /  X  (  X  ) Estimate the prior distribution from all the ( Estimate the 5 hyperparameters:  X  = ( X  1 ,  X  1 ,  X  2 ,  X  2 , P ) by maximizing the likelihood function L(  X  ) in 5 dimensions: 
L(  X  ) =  X  i,j { P f( N ij ;  X  1 ,  X  1 , E ij ) + (1  X  P ) f( N If a threshold (minimum count) fo r the observed counts is used, these formulas are m odified to condition on  X  ij &gt; n* (where n* = the threshold count).
 Given  X  , the posterior distributions of each  X  ij of gamma distributions used to create  X  X hrinkage X  estimates. Assuming that  X  and E are known, then the distribution of N is: 
Prob( N = n ) = P f(n;  X  1 ,  X  1 , E ) + (1  X  P ) f(n;  X  Let Q n be the posterior probability that  X  came from the first component of the mixture, given N = n . From Bayes rule, the formula for Q n is: Q = P f(n;  X  1 ,  X  1 , E )/[ P f(n;  X  1 .  X  1 , E )+(1  X  P ) f(n;  X  Then, the posterior distribution of  X , after observing N = n can be represented as:  X |  X  = n  X   X  (  X  ;  X  1 + n ,  X  1 + E ,  X  2 + n ,  X  where (as above):  X ( X ;  X  1 ,  X  1 ,  X  2 ,  X  2 , P ) = P g(  X  ;  X  1 ,  X  We will use the expectation value: 
E[log(  X  ij ) | N ij ,  X  ] as a means of estimating the  X  X rue X  value of  X  ij To obtain a quantity on the same scale as RR , we define the Empirical Bayes Geometric Mean: EBGM ij = e
E[  X  | N = n,  X  ] = Q n (  X  1 + n )/(  X  1 + E ) + (1 X  Q
E[log(  X  ) | N = n,  X  ] = Q n [  X  (  X  1 + n )  X  log(  X  (1 X  Q n ) [  X  (  X  where  X  (x) = d(log  X  (x))/dx. In the same way, the cumulative gamma distribution function can be used to obtain percentiles of the posterior distribution of  X  . The 5th percentile of  X  is denoted:
EB05 ij = Solution to: Prob(  X  &lt;  X  X 05 | N ij ,  X  ) = 0.05 and is interpreted as a lower 1-sided 95% confidence limit. Work on the MGPS data mining technology and its embodiment in WebVDME has been supported in part by a Small Business Innovation Research Grant to Lincoln Technologies from the NIH National Center for Research Resources (1-R44-RR17478). We acknowledge Ana Szarfman of FDA for her passionate interest in drug safety and for her inspiration and enthusiasm through many years of collaboration. We thank GSK colleagues Mark Mickunas for serving as the information technology project mana ger and for assisting with the preparation and transfer of the OCEANS data; and Eric Smith for being a true  X  X uper user X  and for contributing importantly to the definition, evaluation and testi ng of the WebVDME software. We would also like to thank the entire GSK user group for their input into the design of the system. We also express our apprecia tion to the following Lincoln Technologies colleagues: Les lie Hepburn for producing accurate and complete system documentation, help pages, and test scripts, all under the pressure of very ti ght timetables; Rick Ferris for filling in on short notice as a vers atile application developer, end-user trainer, and system tester; Renel Fredriksen for continually enhancing both the functionality and performance of the MGPS C++ code; Mike Prindle for getting the architecture right and for personally building the lion X  X  share of the web site software; Mike Yang for his cleverness, patience, and persistence in carrying out the data pre-processing necessary to turn the messy  X  X s-delivered X  FOI AERS data into a valuable safety signaling resource; and Chan Russell for his multiple technical and managerial contributions and for his invaluab le assistance in the editing and production of this paper. [1] DuMouchel W. Bayesian da ta mining in large frequency [2] DuMouchel W and Pregibon D. Empirical Bayes screening [3] Szarfman A, Machado SG, and O X  X eill RT. Use of screening [4] O'Neill RT and Szarfman A. Some FDA perspectives on data [5] Evans SJW, Waller PC and Davis S. Use of proportional [6] Bate A, Lindquist M et. al. A data mining approach for signal [7] Agrawal R and Srikant R. Fast algorithms for mining [8] DuMouchel W, Volinsky C et. al. Squashing flat files flatter. 
