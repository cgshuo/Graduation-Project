 In many supervised and unsupervised lea rning problems, one o ften needs to con-duct a preprocessing step on a feature set representing the input to make the learning task feasible. Having some insights about usefulness of the features can add value to finding better solutions. For example, imagine a classification prob-lem where the number of features is so large that it is not possible to train any satisfactory model in allotted time with available resources. In such a situation, one needs to select a subset of features on which a model can be trained in a reasonable amount of time without significant degradation in generalization performance. This problem is known as the feature selection problem [12], [9], [17]. We believe that if we can group the features according to the following two criteria then it will not only make the task of feature selection easy but also would greatly improve the quality of the features selected and hence the final generalization performance. (1) Relevant Features vs Irrelevant Features: For a classification task, a feature f i is a relevant feature if removal of f i alone will result in performance deterioration of an optimal classifier. Otherwise, we call it an irrelevant feature. See Kohavi and John [12] for a detailed discussion. (2) Substitutable vs Complementary Features: For a classification task, we characterize two features f i and f j as substitutable features if there is no signif-icant difference in the generalization p erformance of a classifier that is trained using both the features and the generalization performance of a classifier that is trained using just (any) one of these two features. On the other hand, if the gener-alization performance of the first classifier is significantly better than that of the latter one then we attribute these two features as complementary features.
It is easy to see that having the above insights about the features would greatly simplify the ta sk of feature selection -onejustneedstoselectasetof features of the desired size in such a manner that all the features in that set are relevant and complementary to each other . This insight about the features can be obtained by using the feature clustering approach proposed in this paper.
In this paper, we develop a novel game theoretic approach for clustering the features where the featur es are interpreted as play ers who are allowed to form coalitions (or clusters) 1 among themselves in order to maximize their individ-ual payoffs (defined later). In this approach, the choice of a payoff function would determine whether all the features within a cluster would be substitutable or complementary to each other. It is important to mention that although we demonstrate the feature clustering approach through the idea of substitutable and complementary features, the approach is quite generic and can be used in other problems like dimensionality reduction and community detection in web graphs [7]. For any other problem, one needs to select a payoff function appro-priately so that the desired insights become apparent in clustering the features. We believe that this game theory based approach for feature clustering is quite novel and unique till date.

The key contributions of this paper are as follows: (1) We draw an analogy be-tween a coalitional game and feature clustering problem and then show that Nash Stable Partition (NSP), a well known solution concept in the coalitional game the-ory [15], provides a natural way of clustering the features (Section 3); (2) We show how to get an NSP based clustering by so lving an integer linear program (ILP) (Section 3). The solution of this ILP gives an NSP based clustering of the features for a given payoff function under the assumption of pure hedonic setting (defined later); (3) We illustrate that depending upon how a payoff function is chosen, the NSP based clustering would have a property that either substitutable features are grouped together in one cluster or complementary features are grouped together in one cluster (Section 4). Then, one can u se any standard technique [9], [17] to choose features from these clusters. We al so suggest a simple cluster ranking based technique for selecting the features from a given set of feature clusters (Section 5); (4) Finally, we propose a hierarchical scheme for feature clustering in the scenario where the feature set size is very large and solving ILP is expensive (Section 6). Interestingly, a key result that we prove on the equivalence between a k -size NSP of a coalitional game and minimum k -cut of an appropriately constructed graph [13],[4] comes in handy for large scale problems; (5) We demonstrate the efficacy of our approach through a set of experiments conducted on real world as well as synthetic datasets (Section 7). Feature selection can be thought of as a dimensionality reduction technique. The problem of feature selectio n is to find a feature subset S with m features, which jointly have the largest dependency on the target class. Featur e selection methods can be of two types: filter methods and wrapper methods [12]. Filter methods select the relevant features by ranking all the features using some measure. On the other hand, the wrapper methods treat an induction algorithm as a black box and interact with it to assess the usefulness of a subset of features.
Different measures have been suggested in the literature for ranking the fea-tures. See Guyon and Elisseeff [9] and th e references therei n. Some commonly used measures are the absolute value of t he Pearson correlation coefficient and mutual information [17]. Peng et al [17] studied the feature selection problem using maximum statistical dependency criterion based on the mutual informa-tion. Particularly relevant to our work is the approach suggested by Cohen et al [3], which poses the problem of feature s election as a cooperat ive game problem.
Cohen et al [3] proposed to use Shapley value (a solution concept for coopera-tive games) for ranking the features. In this approach, each feature is treated as a player of a game and the idea is to evaluate the marginal contribution of every feature to the classification accuracy by using Shapley value and then eliminate those features whose marginal contribution is less than a certain threshold. Since the calculation of the Shapley value involves summing over all possible permu-tations of the features, it becomes impractical to determine the Shapley value if the feature set size is large. It was therefore proposed to use multi perturbation approach of approximating the Shapley value (proposed by Keinan et al [11]). The main drawback of this approach is that it is computationally expensive as different classifiers need to be trained using different feature subsets. We start with definitions of a few basic concepts in the coalitional game the-ory which would serve as building blocks for the rest of the paper. These con-cepts are fairly standard in the game theory literature and can be found in [8]. Definition 1 (Coalitional Games). An n -person coalitional game (under pure hedonic setting) is a pair ( N, u (  X  )) ,where N = { x 1 ,x 2 ,...,x n } is the set of tions. Utility function u i (  X  ) is defined over the set of coalitions { C  X  N | i  X  C } The pure hedonic setting characterizes a special class of coalitional games where the utility of a player due to joining a coalition depends only on the composition of the coalition. The term pure hedonic setting was coined by [6]. The examples of hedonic setting include the formation of social clubs and political parties. In the rest of this paper, we will be working with coalitional games under the pure hedonic setting, which we refer to as hedonic games.

The key question addressed by the coalitional game theory is that for a given coalitional game, what coalitional structure would emerge if the players play the game. A coalition structure C = { C 1 ,C 2 ,...,C k } is a partitioning of the players into k disjoint sets C 1 ,...,C k . Many stability concepts have been proposed in the coalitional game theory [2]. In this paper we focus only on Nash Stability . Definition 2 (Nash Stable Partition (NSP)). Given a hedonic game ( N, u (  X  )) , a partition C = { C 1 ,C 2 ,...,C k } is Nash stable if for every player i , we have u ( C C ( i ))  X  u i ( C j  X  X  i } )  X  C j  X  C  X  X   X  } ,where C C ( i ) denotes the set C j  X  C such that i  X  C j . In simple words, a partition C is an NSP if no player can ben-efit from switching his current coalition C C ( i ) given that all the other players are sticking to the coalitions suggested by the partition C . The NSP where C = { N } is trivial NSP and any other NSP is called a non-trivial NSP. A non-trivial NSP C = { C 1 ,C 2 ,...,C k } is called a k -size NSP (or k -NSP for short). The obvious question that arises next is that whether an NSP always exists and if not then under what conditions it exists. In general, it is possible that a given hedonic game may not have any NSP. However, for some classes of hedonic games, existence of an NSP i s guaranteed (e.g., the games with additively separable (AS) and symmetric preferences (defined next)).
 Definition 3 (AS and Symmetric Preferences). Aplayer i  X  X  preferences are additively separable if there exists a function v i : N  X  R such that  X  C  X  N for which i  X  C , we have u i ( C )= j  X  C v i ( j ) . Without loss of generality, we can set v i ( i )=0 which would imply that u i ( C )= u i ( C  X  X  i } )  X  C  X  N .Thus,we can say that any additively separable preferences can be equivalently represented by an n  X  n matrix v =[ v i ( j )] . Further, we say that the AS preferences satisfy symmetry iff the matrix v is symmetric, that is v i ( j )= v j ( i )= v ij  X  i, j . It turns out that the problem of deciding whether an NSP exists in a pure hedonic game with the AS preferences is NP-complete [1], [16]. However, Bogomolnaia and Jackson [2] have shown that an NSP exists in every hedonic game having the AS and symmetric preferences. It is worth mentioning that Bogomolnaia and Jackson proved the existence of an NSP. But their result does not specify anything regarding the structure of the NSP (e.g., whether the NSP would be trivial or non-trivial, whether there is a unique NSP or many, what is the size of such NSP(s), etc). However, Olsen [16] has shown that the problem of deciding whether a non-trivial NSP exists for the AS hedonic games with non-negative and symmetric preferences is NP-complete. In the rest of the paper, we will be working under the hedonic setting with the AS and symmetric preference assumptions and hence we do not have to worry about the existence of an NSP (although it could be a trivial NSP).

In what follows, we extend the notion of an NSP to an approximate NSP which we prefer to call as -Regret NSP. The motivation behind this notion comes from the facts that (1) a hedonic game with the AS and symmetric preferences may not have a non-trivial NSP (i.e., a k -NSP with k&gt; 1) but for any value of 1  X  k  X  n , there always exists an  X  0 such that the game has an -Regret k -NSP, and moreover, (2) checking for the existence of a non-trivial NSP is NP-complete (under non-negative preferences) [16].
 Definition 4 ( -Regret k -NSP). A given k -size partition of the players in a hedonic game is said to be -regret k -NSP if no player can increase his utility by more than by switching his current coalition given that no other player is switching his current coalition. 3.1 NSP Computation via Integer Linear Program (ILP) Theorem 1. Consider an n -person hedonic game having the AS and symmetric preferences given by a matrix v =[ v ij ] .Let C  X  be a partition of the set N of players. If C  X  is a solution of the following ILP then C  X  is an NSP of this game. In the above ILP,  X  ( C ) is an indicator variable that decides whether the coalition C is a part of the partition or not. The constraints basically ensure a partition of the players. The objective function is nothing but the sum of values of all the coalitions in a partition, which we call as partition X  X  value .Thus,wecansaythat any value maximizing partition is always an NSP. Note that the above theorem provides a sufficiency condition for a n NSP and it can be directly obtained from the existence proof of [2]. However , it is not necessary th at if a non-trivial partition C of N is an NSP then that would also be a solution of the above ILP.Forexample,considera4  X  4 symmetric matrix v having v ii =0  X  i ; v 12 = 10; v 13 = v 14 =1; v 23 = v 24 =1; v 34 = 5. It is easy to see that for this matrix we have C = {{ 1 , 2 } , { 3 , 4 }} as an NSP but this is not the optimal solution of the ILP. The optimal solution of the ILP is a trivial NSP. This observation can be generalized by saying that if the matrix v is nonnegative then the above ILP will give only the trivial partition as the solution. Note that solving an ILP is in general hard and quite impractical (especially for a large number of variables). Hence, one can work with an LP relaxation (0  X   X  ( C )  X  1) of the above ILP and get an approximate solution. In fact, in our feature clustering approach (proposed in the next section), we use an LP relaxation of this ILP. In this section, we show that the feature clustering problem can be effectively posed as a Nash stable partitioning problem. For this, let us consider the binary classification problem { x l ,y l } m l =1 where we have m training examples and the input vector x l isspecifiedintheformof n real valued features { f 1 ,f 2 ,...,f n } and y l  X  X  X  1 , +1 } .Let  X  ij betheestimateofthePearso n correlation coefficient between the feature f i and the feature f j . Similarly,  X  iy is the Pearson correlation coefficient between the feature f i and the class label y .Nowletussetupan analogy between a feature cluster and an NSP as follows. View each feature as a player in the game and define a payoff function v i ( j )= |  X  ij | X  i, j  X  N . that each feature f i has a preference relation i over the feature subsets such that  X  C 1 ,C 2 f i ,wehave C 1 i C 2  X  j  X  C features are allowed to form the coalit ions then every feature would tend to join a feature group which maximizes its payoff function. It is easy to see that for the chosen function v ij = |  X  ij | , substitutable features would tend to group together. The above situation can be viewed as a coalitional game with the AS and symmetric preferences for which an NSP is a reasonable solution concept to predict the final coalitional structure. Therefore, if we use an NSP of the above game as clusters of the features then it would have the property that the features are stable in their own clusters and don X  X  want to move across the clusters (which is a desired property of any clustering scheme). Thus, we can say that any NSP of the above game would be a reasonable clustering of the features where each cluster would contain substitutable features and the features across clusters would be complementary to each other. A Nash stable partition of the above game can be obtained by solving ILP given in (1). Recall, if v ij is nonnegative for all i and j then ILP does not have a non-trivial solution and moreover computing a non-t rivial NSP is NP-complete due to the result of Olsen [16]. In order to tackle this situation, we can modify the payoff functions slightly as follows: v ij = |  X  ij | X   X  where 0 &lt; X &lt; 1.  X  canbeviewedasaparameter which decides a threshold (in an implicit manner) such that any two features having v ij values higher (lower) than the threshold would qualify as substitutable (complementary) features. With this interpretation, it is easy to see that as we increase  X  , the threshold increases and hence we get more and more fine grained clusters of substitutable features.

It is interesting to note that the function v ij = |  X  ij | X   X  is not the only payoff function but there exist many other choices and depending upon the function we decide to choose, we may either get substitutable features grouped together or complementary features grouped together. For example, if we use v tend to group together and each cluster will contain relevant and complementary features. Note that the maximum possible value of the function |  X  iy | + |  X  jy | X  X   X  ij | is 2 and having  X &gt; 2 would make all v ij ,i = j negative. Because v ii =0  X  i ,this would result in a solution where each single feature would form its own cluster. This is the reason why we have restricted the value of  X  upto 2. One can also again result in substitutable features getting grouped together. In each one of these functions, one can replace |  X  ij | with m ij and |  X  iy | with m iy where m ij is the estimate of the mutual information between the features f i and f j .Below, we suggest a few functions along with their clustering nature.
 Complementary Features: ( |  X  iy | + |  X  jy | X  X   X  ij | X   X  )and( m iy + m jy  X  m ij  X   X  ). Here we discuss a few approaches for selecting a given number of features. In the first approach, we choose a payoff function that puts substitutable features in one cluster. Next, we tune the parameter  X  in such a way that we obtain m feature clusters as a solution of ILP (or relaxed ILP). Now we pick the most relevant feature from each cluster. The most relevant feature of a cluster can be chosen by several schemes such as maximum |  X  iy | or maximum m iy .Notethat in this approach, there is a risk of getting an irrelevant feature selected if all the features in that group have very low value of |  X  iy | . Therefore, we propose an alternative approach next.

In the second approach also, we choose a payoff function that puts the substi-tutable features in one cluster. Next, we choose some value of  X  and obtain the feature clusters as the solution of ILP (or relaxed ILP). In this approach, we have no control over the number of clusters as  X  is not tuned. We compute a ranking score r ( C ) for each cluster C in the partition and then pick the top cluster. From this top cluster, we pick the most relevant feature (using the criterion discussed in the previous approach) and then delete that feature from this cluster. Now we recompute the ranking score for this clu ster and perform the f eature selection step as before. We repeat this whole pr ocess until we obtain the desired num-ber of features. A few suggested ranking functions are r ( C )=( i  X  C |  X  iy | ) / | C | ; r ( C )=( i  X  C m iy ) / | C | . One can develop similar approaches by using a payoff function for grouping complementary features together. In section 4, we suggested that in most of the cases, the LP relaxation of ILP gives an approximate clustering based on NSP. However, it is easy to see that even the relaxed LP would have 2 n variables and hence running time of any LP solver is large even for as small a number of features as n = 20. In such a scenario, ILP or relaxed ILP based approaches are not feasible for feature clustering. To tackle such situations, we propose a hierarchical feature clustering approach. This approach is based on an interesting result on the equivalence between a k -NSP of a coalitional game and minimum k -cut of an appropriately constructed graph (as we prove below). 6.1 Equivalence between a k -NSP and Minimum k -Cut Consider an n -person hedonic game with the AS, symmetric, and non-negative preferences given by an n  X  n symmetric and non-negative matrix v  X  0. This game can be represented by an undirected graph G =( N, E )where N is the set of nodes which is the same as the set of players. We put an undirected edge between node i and j iff v ij = v ji &gt; 0 and assign a weight v ij to that edge. Definition 5 (Minimum k -Cut). A k -cut of a graph G is defined as any partitioning of the nodes into k (1 &lt;k  X  n ) nonempty disjoint subsets, say {
C 1 ,C 2 ,...,C k } . The capacity of such a k -cut is defined as the sum of the weights of all those edges whose end points are in different partitions and is denoted by cap ( C 1 ,C 2 ,...,C k ) . A minimum k -cut of this graph is a k -cut whose capacity is minimum across all the possible k -cuts of the graph. For any given feasible value of k , there could be multiple minimum k -cuts and the capacities of all those cuts would be the same and we denote that by cap  X  ( k ) .
 Definition 6 (Support Size). The support size s of a k -cut { C 1 ,C 2 ,...,C k } is defined as follows: s =min i =1 ,...,k | C i | .
 Theorem 2 ( k -NSP and Minimum k -Cut ). Let G be a graph representation of a hedonic game with the AS and symmetric preferences given by a matrix v  X  0 .Then, minimum k -cut of G having s&gt; 1  X  k -NSP .
 Proof: The proof is by contradiction. If possible, let { C 1 ,C 2 ,...,C k } be a min-imum k -cut with support s&gt; 1 and it is not a k -NSP. This would mean that there exists some player i  X  N who would gain by switching to some other coali-tion given that no other player is switching. Let us assume that i  X  C t for some t  X  X  1 , 2 ,...,k } .Let u prefer to switch to the coalition C z from his current coalition C t . 2 Because of the AS and symmetric preferences, we have u i ( C )= j  X  C v ij for any coalition C , and hence the cut capacity of this newly formed k -cut after the switching is given we have cap (new cut) &lt;cap ( C 1 ,C 2 ,...,C k ) which is a contradiction to the as-sumption that { C 1 ,C 2 ,...,C k } is a minimum k -cut. 3 ( Q.E.D. ) In the above proof, s&gt; 1 is required to ensure that even after node i switches, the resulting partition is a k -way cut. Theorem 2 gives a sufficiency condition for the existence of a k -NSP and hence becomes useful in the case when computing a minimum k -cut of a graph is an easy problem. However, it is a well known fact that computing a minimum k -cut of a graph is NP-hard for k&gt; 2 [20]. Therefore, this theorem would not help much for computing a k -NSP of a given game if k&gt; 2. To handle that case, it is useful to define an approximate minimum cut in the form of -Regret k -cut of a graph (defined below). In Theorem 3, we show that there exists a connection between an -Regret k -cut of a graph and an -Regret k -NSP. The proof of this theorem follows the similar line of arguments as in the proof of Theorem 2. Hence, we skip the proof.
 Definition 7 ( -Regret k -Cut). Given a graph G ,a k -cut { C 1 ,C 2 ,...,C k } is said to be an -Regret k -cut iff cap  X  ( k )  X  cap ( C 1 ,C 2 ,...,C k )  X  cap  X  ( k )+ , where cap  X  ( k ) is the capacity of the minimum k -cut.
 Theorem 3. -Regret k -cut having s&gt; 1  X  -Regret k -NSP 6.2 Hierarchical Feature Clustering From Theorem 3, it is apparent that any scheme that efficiently computes an -Regret k -cut of a graph would be an efficient scheme for computing the -Regret k -NSP also. Therefore, we propose a simple hierarchical scheme (without any theoretical bounds on the value of ) for computing an -Regret k -cut and hence an -Regret k -NSP. Note that when v has only positive entries (and under some cases with negative entries also), it becomes a polynomial time solvable problem to compute a minimum 2-cut [19]. Hence, our approach works by computing minimum 2-cuts of a graph in a hierarchical manner.

In this approach, we begin with the whole feature set and just compute a min-imum 2-cut of the underlying graph. Then we recursively compute the minimum 2-cut of each partition obtained in the previous step. This gives us a binary tree structure (not necessarily balanced) where each node is the set of features and two children of a node correspond to a minimum 2-cut of that node. We stop splitting a node further if the number of features in that node is less than some threshold value. All such nodes would form the leaf nodes. Finally, we take all the leaf nodes of such a tree as feature clusters and then apply any one of the feature selection approaches discussed earlier.

Note that at every step of the above scheme, one can use an exact algorithm for computing a minimum 2-cut (for example, O ( mn + n 2 log ( n )) algorithm by Stoer and Wagner [19] where m is the number of edges). However, it is possible to have m = O ( n 2 ) in which case finding an exact solution becomes expensive ( O ( n 3 )). In addition to this, if a graph has multiple minimum 2-cuts then we would be interested in focusing on those cuts for which the support size is more than 1. Therefore, in practice one may prefer a fast randomized algorithm (al-though approximate) as opposed to an exact algorithm because we can run it several times and generate various minimum 2-cut solutions and then pick the most balanced solution. Some of the methods that are fast and can generate multiple solutions include Karger and Stein X  X  [10] randomized mincut algorithm We conducted experiments with a SC algorithm for ease of implementation and to get a feel of the performance achievable using an approximate method. Al-though the SC algorithm has same O ( n 3 ) complexity as the Stoer and Wagner X  X  algorithm but we found that the SC algorithm was significantly faster in our experiments. The SC algorithm which we use here is as follows [13][5]: (1) Con-struct a diagonal matrix D of the size same as the matrix v in such a way that each diagonal entry of D is the sum of the entries of the corresponding row of the matrix v ; (2) Compute the graph Laplacian matrix L = D  X  v ;(3)Findthe two eigenvectors corresponding to the two lowest eigenvalues of the matrix L; (4) Apply 2-means clustering on these t wo eigenvectors. Th ese clusters would correspond to an approximate minimum 2-cut. It is important to note that in this approximate algorithm, the 2-means clustering algorithm involves a random initialization of the clusters. Depending on how the initialization is done, we may get different solutions. Among the different solutions, the final solution can be picked by using an appropriate cluster quality measure. We illustrate our approach by conducting several experiments on the feature selection problems for on e synthetic dataset and two real world datasets -Splice and Arrhythmia (details given later), which are binary classification problems. Synthetic Datasets (No. of Features = 20, Training Set Size = 1000, Test Set Size = 300): We generated synthetic datasets corresponding to 20-dimensional zero mean correlated Gaussian random variables of 15 different co-variance matrices. Each of these 15 datasets consisted 1000 training samples, denoted by X tr , and 300 testing samples, denoted by X ts .Inourexperiment, we generated a coefficient vector w of size 20 for a linear classifier and used it to obtain the class labels for the training data as follows: Y tr = sign ( w  X  X tr ). In the same way, we generated the class labels for the test data. The vector w was generated as 20 i.i.d. uniform random variables over the interval [  X  1 , 1]. Next we computed the correlation matrix  X  =[  X  ij ] for the training data. We set the payoff function v ij = |  X  ij | X   X  andthenvaried  X  from0to1sothatweget different size NSPs by solving the corresponding relaxed LPs for ILP (1). For each of these NSPs, we used |  X  iy | to pick the relevant features and trained a linear least squares classifier on the training data using these selected features. We computed the accuracy of this classi fier on the test data (with the selected features). We performed this experiment 1000 times by changing the vector w and computed the average accuracy over th ese realizations. The same experiment was repeated for 15 different covariance ma trices. The results of our experiments are summarized in Figure 1 (top panel); we have plotted the variation of aver-age accuracy for 15 different covariance matrices in the form of blue colored box plots (for our method). We have also plotted a red colored box plot along with each blue box plot. The red box plot corresponds to the variation of the average accuracy when the features were p icked from the set of features ranked in decreasing order according to |  X  iy | . We conducted statistical significance test using Wilcoxon sign rank test at the significance level of 0.05 to compare the two schemes of feature selection and fo und that for the feature set sizes of 3 to 6, the game theoretic based feature clustering approach (followed by feature se-lection) yielded better performance than selecting the features using |  X  iy | values. Splice Datasets (No. of Features = 60, Training Set Size = 1000, Test Set Size = 2175): The splice datasets are taken from http://theoval.cmp. uea.ac.uk/ ~ gcc/matlab/default.html . In this experiment our goal was to test the relevant feature identification capability of our method. Because the number of features is large, we used the hierarchical clustering approach (described in Section 6) for obtaining the clusters of the features and then we picked top 7 features using the second approach for feature selection (proposed in Section 5) by making use of the ranking function r ( S )=( i  X  S |  X  iy | ) / | S | . We repeated this experiment 1000 times with random initialization for 2-means clustering. We found that most of the times, 6 out o f the selected 7 features belong to of relevant features identified by Meil  X  a and Jordan [14]. This demonstrates the effectiveness of the proposed me thod on real world problems. Arrhythmia Dataset (No. of Features = 246, Training Set Size =280, Test Set Size = 140): This dataset can be obtained from the UCI repository. We used a version of this dataset that was modified by Perkins et al [18] and was also used by Cohen et al [3]. For this dataset, we again performed the hierarchical feature clustering followed by the feature selection (varying the size from 10 to 30) in the same way as we did for the Splice dataset case. We repeated the whole process for 100 different initial conditions as was done in the Splice datasets case. For each case, we trained a simple linear least squares classifier on the selected features and then recomputed the accura cy of the test set. We have plotted the boxplot for the variation in accuracy in Figure 1 (bottom panel). Along with each box plot, we have indicated the test set accuracy (in the form of red dash) when the same number of features are se lected from the set of features ranked in decreasing order according to |  X  iy | and the linear least squares classifier is trained on those features. We see that the median of the test set accuracy with the feature clustering is higher than 75% for each case (which was the reported performance of Perkins et al [18]). Specifically, for 21 features, we see that the median of the performance is 79% whereas the highest performance reaches beyond 85% (which is higher than 84 . 2% reported by Cohen et al [3]).
To conclude, although the proposed approach is complete in its own, there are several avenues for further investigations. For example, all the experiments were conducted using the payoff functions that put substitutable features in one cluster. One can experiment with other payoff functions which would drive complementary features in one cluster. In the case of multiple NSPs, we assumed that it is fine to choose any one of them. However, it is worth investigating the quality of other NSPs.

