 Auto face annotation plays an important role in many real-world multimedia information and knowledge management systems. Re-cently there is a surge of research interests in mining weakly-labeled facial images on the internet to tackle this long-standing research challenge in computer vision and image understanding. In this pa-per, we present a novel unified learning framework for face an-notation by mining weakly labeled web facial images through in-terdisciplinary efforts of combining sparse feature representation, content-based image retrieval, transductive learning and inductive learning techniques. In particular, we first introduce a new search-based face annotation paradigm using transductive learning, and then propose an effective inductive learning scheme for training classification-based annotators from weakly labeled facial images, and finally unify both transductive and inductive learning approaches to maximize the learning efficacy. We conduct extensive exper-iments on a real-world web facial image database, in which en-couraging results show that the proposed unified learning scheme outperforms the state-of-the-art approaches.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation web facial images, face annotation, image retrieval, sparse coding, transductive learning, inductive learning
Recent years have witnessed the rapid growth of various digital and mobile devices, powerful cloud computing facilities, web 2.0 photo sharing portals and social networks. As a consequence, mas-sive facial images have been created, distributed and shared on the internet by millions of users nowadays, in which some of the fa-cial images are associated with tags or labels while some others are completely unlabeled. The huge amount of web facial images poses many challenges and opportunities. On the one hand, the existing huge amount of weakly labeled facial images offers an important source for knowledge discovery to tackle many long-standing re-search challenges, and on the other hand, the increasingly large amount of unlabeled facial images brings a critical challenge to many multimedia retrieval and knowledge management tasks. An important technique to address this challenge is auto face annota-tion , which aims to automatically assign a face with the name of the corresponding person. This technique benefits many real-world applications. For example, it can help social media portals (e.g., Facebook) to automatically annotate users X  uploaded photos to fa-cilitate the search and management of online photo albums. Be-sides, face annotation techniques can be applied to the news video domain where faces of key persons in a video can be automati-cally detected and annotated to facilitate various multimedia man-agement tasks, such as news video summarization, retrieval and browsing [ 37 ].

F ace annotation is closely related to face detection and recogni-tion, a long-standing research challenge which has been extensively studied for years in computer vision and image processing. In gen-eral, face annotation can be formulated as a data classification prob-lem from a machine learning and data mining perspective. It thus could be solved by two types of methodologies:  X  X nductive learn-ing" and  X  X ransductive learning." Below we briefly introduce some basics and existing approaches in each type of learning methodol-ogy to attack the face annotation problem.

To solve face annotation from the view of  X  X nductive learning", one can apply some classical inductive (or model-based) face recog-nition/verification algorithms, which have been extensively studied in computer vision and pattern recognition for many years [ 2, 18 , 52 ]. The inductive learning approaches can achieve impressive re-sults when enough high quality labeled training data are available for building the models. However, such approach is often limited in several aspects: (i) it is usually time-consuming and expensive to collect a large amount of human-labeled training facial images, typically in a controlled environment; (ii) it is usually difficult to generalize the models when new training data or new persons are added, in which an intensive re-training process is often required; and (ii) last but not least, the annotation performance often scales poorly when the number of persons/classes is large.

To address the aforementioned limitations, some recent studies have attempted to explore the  X  X ransductive learning" approach by mining huge weakly labeled facial images freely available on the internet [ 43 , 44 ]. Specifically, they build a large web facial image d atabase by querying some existing web search engine according to a celebrity name list. Given the nature of web images, these fa-cial images are weakly labeled, i.e. their labels are often noisy and do not always correspond to the right human names. For the an-notation task, given a query facial image, they first retrieve top k similar images from the weakly labeled facial image database, and then annotate the query facial image using some machine learn-ing algorithm. In general, the above search-based face annotation (SBFA) scheme is a data-driven approach by exploring transduc-tive learning methods to attack the face annotation task. Despite its promising performance, the SBFA approach also has some lim-itations. For example, it may have relatively poor generalization performance of unseen faces due to its nature of exploring only local information. Besides, it also suffers from the challenge of insufficient data, i.e., the web facial image database may not have enough weakly labeled facial images for some persons who are not popular or active on the internet.

In this work, we aim to address the above limitations of both in-ductive learning approach and transductive learning approach for face annotation. In particular, we propose a unified framework of Unifying Transductive and Inductive Learning (UTIL) for mining web facial images by combining the strengths of the two learning techniques to tackle the face annotation problem. From the  X  X n-ductive learning" view, we propose a new Weak Label Laplacian Support Vector Machine (WL-LapSVM) algorithm for generating effective classification models from weakly labeled web facial im-ages; from the view of  X  X ransductive learning", we apply the state-of-the-art Weak Label Regularized Local Coordinate Coding (WL-RLCC) algorithm [ 44 ] in the search-based face annotation frame-w ork; finally, we propose an entropy-based combination scheme to combine the annotation results from the two different learning schemes to maximize the learning efficacy. As a summary, the main contributions of this paper include:
The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents the proposed Unifying Trans-ductive and Inductive Learning (UTIL) framework and gives the re-lated algorithms in detail. Section 4 shows the experimental results of performance evaluation, and Section 5 concludes this paper. Our work is closely related to several groups of research work.
The first group is face recognition and verification, a classic prob-lem in computer vision and pattern recognition that has been ex-tensively studied for many years [ 18 , 52 ]. Comprehensive reviews c an be found in [ 18 , 21 , 52 , 11 , 24 ]. Although they can be ex-t ended for face annotation [ 55 ], traditional face recognition tech-n iques often suffer from a few common drawbacks. For example, they usually require high-quality facial image databases collected in well-controlled environments, which has partially motivated the recent emerging benchmark studies of unconstrained face detection and verification techniques on the facial images collected from the web, such as the LFW benchmark [ 6, 16 , 20 , 30 ].

T he second group is related to generic image annotation tech-niques [ 17 ], which usually apply existing object recognition tech-n iques to train classification models based on human-labeled train-ing images or attempt to infer the correlation or joint probabilities between query images and annotation keywords [ 12 , 13 , 7, 17 ]. G iven limited training data, semi-supervised learning methods have been widely used for image annotation [ 41 , 33 , 39 ]. Wang et al. p roposed to refine the model-based annotation results with a la-bel similarity graph by following a random walk approach [ 41 , 32 ]. S imilarly, Pham et al. proposed to annotate unlabeled facial images in video frames with an iterative label propagation scheme [ 33 ]. A lthough semi-supervised learning approaches can leverage both labeled and unlabeled data, its performance fairly depends on the amount of labeled data. It is usually time-consuming and expensive to collect enough high-quality labeled data to achieve satisfactory performance in large-scale scenarios. Recently, the search-based image annotation paradigm by mining web images has attracted more and more attention [ 46 , 36 , 41 , 35 ]. A few studies in this a rea have attempted to develop efficient content-based indexing and search techniques to facilitate annotation/recognition tasks. For ex-ample, Russell et al. developed a large collection of web images with ground truth labels to facilitate object recognition tasks [ 36 ]. T here are also several studies that aim to address the final annota-tion process by exploring effective label propagation [ 47 , 39 , 48 , 42 , 50 ]. For example, Tang et al. presented a sparse graph-based s emi-supervised learning (SGSSL) approach to annotate web im-ages [ 39 ].

T he third group is face annotation on the collections of per-sonal or family photos. Several studies have mainly focused on the annotation task on collections of personal/family photos [ 40 , 10 , 45 , 1, 9], which often contain rich context clues, such as per-s onal/family names, social context, GPS tags, timestamps, etc. In addition, the number of persons/classess is usually quite small, making such annotation tasks less challenging. These techniques usually achieve fairly impressive annotation results. Some tech-niques have been successfully deployed in commercial applica-tions, e.g., Apple iPhoto 1 , Google Picasa 2 , Microsoft easyAl-b um [ 10 ], and Facebook face auto-tagging solution 3 .
T he fourth group addresses face annotation by mining weakly labeled facial images on the web. A few studies consider a human name as an input query, and mainly aim to refine the text-based search results by exploiting visual consistency of facial images, which is closely related to automated image re-ranking problems. For example, Ozkan and Duygulu proposed a graph-based model for finding the densest sub-graph as the most related result [ 31 ]. F ollowing the graph-based approach, Le and Satoh proposed a new local density score to represent the importance of each returned image [ 26 ]. Guillaumin et al. introduced a modification to incor-p orate the constraint that a face can only appear once in an im-age [ 14 ]. On the other hand, the generative approach such as the g aussian mixture model had also been adopted to the name-based search scheme and achieved comparable results [ 5, 14 ]. Recently, a discriminant approach was proposed in [ 15 ] to improve the gen-e rative approach and avoid the explicit computation in the graph-based approach. Inspired by query expansion [ 29 ], the performance o f name-based scheme can be further improved by introducing the images of  X  X riends" of the query name. Unlike these studies of fil-tering the text-based retrieval results, some studies have attempted to directly annotate each facial image with the names extracted http://www.apple.com/ilife/iphoto/ http://picasa.google.com/ http://www.facebook.com/ from its caption information. For example, Berg et al. propos ed a possibility model which is combined with a clustering algorithm to estimate the relationship between facial images and the names in their captions [ 4]. For the facial images and the detected names i n the same document (a web image and its corresponding cap-tion), Guillaumin et al. proposed to iteratively update the assign-ment based on a minimum cost matching algorithm [ 14 ]. In their s ubsequent work [ 15 ], they further improved the annotation perfor-m ance using distance metric learning techniques to achieve more discriminative feature in low dimensional space. However, limited progress has been reported on search-based face annotation (SBFA) scheme, which is fundamentally different from the previous stud-ies of  X  X ext-based face annotation" and  X  X aption-based face anno-tation." The SBFA scheme aims to solve a generic content-based face annotation problem, where a facial image is directly used as the input query. For example, Wang et al. proposed an Unsu-pervised Label Refinement (URL) algorithm to enhance the label matrix over the entire facial image database [ 43 ]. In their further w ork [ 44 ], the WLRLCC algorithm was proposed to fully exploit t he top-ranking similar images of the query image via a unified optimization scheme of learning both local coordinate coding and refined labels. Besides, there is also some work for mainly address-ing facial image retrieval task [ 50 ] which explores both local and g lobal features for face retrieval and re-ranking.

Our work is fundamentally different from the previous studies on text/caption based face annotation because they aim to address the assignment between the existing facial images and their names appeared in their corresponding surrounding text, and generally do not support content-based annotation of a novel query facial im-age. In contrast, our work is closer to the emerging search based face annotation scheme [ 43 , 44 ]. Unlike the previous transductive l earning approaches, the proposed unified scheme unify both trans-ductive and inductive learning approaches to maximize the learning efficacy.

The last group of related work is about machine learning tech-niques, including semi-supervised learning [ 56 , 8, 53 , 3] and mul-t imodal fusion [ 22 , 23 ]. One problem addressed in our frame-w ork is about small sample learning. It can be partially solved by Semi-supervised learning (SSL) techniques which have been exten-sively studied for several years. Among many existing approaches, Laplacian Support Vector Machines (LapSVM) [ 3] is one of state-o f-the-art techniques. To reduce the computational complexity of LapSVM, Melacci et al. [ 28 ] focused on the primary Laplacian S upport Vector Machines problem and proposed an efficient so-lution with preconditioned conjugate gradient. Weighted Margin Support Vector Machines (WMSVM) [ 49 ] is another way to solve t he small-sampling problem by generalizing the original Support Vector Machines for incorporating prior knowledge. The fuzzy membership is introduced in the fuzzy support vector machine [ 27 ] s uch that different input points can make different contributions to the learning of decision surface. In our framework, as the number of positive samples is rarely small (only 1 ) and all the facial images are assigned with weak name information, motivated by the similar methodology, we proposed a new Weak Label Laplacian Support Vector Machine (WL-LapSVM) algorithm for generating effective classification models from weakly labeled web facial images. In this section, we briefly introduce the proposed framework of Unifying Transuctive and Inductive Learning (UTIL) for auto face Figure 1: The Unifying Transductive and Inductive Learning ( UTIL) framework for auto face annotation problem. annotation. It combines both transductive and inductive learning techniques in a systematic approach. Figure 1 illustrates the sys-tem flow of the proposed framework, which consists of the follow-ing three stages: (1) Preprocess the query facial image, including face detection, face alignment and facial feature extraction; (2) Ap-ply  X  X ransductive learning" and  X  X nductive learning" respectively on the weakly-labeled face image database; and (3) Combine the annotation results from the  X  X ransductive learning" and  X  X nductive learning" steps, and output the final annotation. The details of each stage are described as follows.

The first stage, as shown in Figure 1(1), is to pre-process a query f acial image, including face detection, face alignment, and facial feature representation. In particular, for facial region detection and alignment, we adopt the unsupervised face alignment technique (DLK) in [ 54 ] which attempts to align all the facial images into a c onsistent position. We extract the GIST features [ 38 ] as the facial r epresentation. According to our empirical study, for the aligned facial image achieved by DLK algorithm, the GIST feature per-forms better than the other facial features (e.g. Gabor, color, edge, or raw image intensity). As a result, each face is represented with a 512 -dimensional vector in our framework.

The second stage, as shown in Figure 1(2), consists of two in-d ependent learning steps: (i) annotation by  X  X ransuctive learning" and (ii) annotation by  X  X nductive learning." Both are applied on the same web facial image database. To build such a large-scale facial image database, we can choose a list of desired human names and submit them to some existing web search engine (e.g., Google in our approach) for crawling their related web facial images. As the output of this crawling process, we obtain a collection of web fa-cial images, each of them is associated with a human name. Given the nature of web images and the limitation of search engine, these facial images are usually noisy, and the name labels may be in-correct or incomplete, especially for the less popular persons. We thus refer to such web facial images with noisy names as weakly labeled facial images. For each image in weakly labeled facia l im-age database, the same pre-processing step as the previous step is applied and no-face-detected images are removed.

For the  X  X ransductive learning" step, we apply the state-of-the-art Weak Label Regularized Local Coordinate Coding (WLRLCC) algorithm in a search based face annotation paradigm [ 44 ], which a ims to annotate the query image by fully exploring the top-n sim-ilar images and their corresponding labels. For this problem, two key factors affect its final annotation performance: (1) Generat-ing more represented feature for re-ranking as all the top ranking images are close to each other in the original feature space; (2) En-hancing the initial weak labels. In WLRLCC algorithm, these two problems are tackled simultaneously in one optimization problem.
For the  X  X nductive learning" step, we have to address the problem of insufficient labeled data for training effective classifiers. Since the number of images and the number of persons are both large in the web facial image database, it is impossible and impractical to label all the facial images due to the expensive human labeling costs. In our framework, we assume that only one facial image can be manually labeled for each person. On the other hand, all the facial images are weakly labeled during the crawling step. As a re-sult, the core problem is how to effectively train classifiers based on a small number of well labeled data and a large amount of weakly labeled data. To tackle issue, a natural choice is to explore semi-supervised learning techniques, e.g., semi-supervised support vec-tor machines. However, the conventional semi-supervised learning techniques cannot deal with weakly labeled data properly. In this paper, we propose the Weak Label Laplacian Support Vector Ma-chines (WL-LapSVM) algorithm to overcome the challenge.
The third step is about the combination of the annotation results of the previous trasductive and inductive learning stages. To this purpose, we evaluate several last fusion scheme to merge the two annotation results. We also proposed an entropy based weighting combination scheme, which achieve fairly good fusion result with less computation effort.
In this section, we briefly introduce the search-based face anno-tation (SBFA) scheme and the Weak Label Regularized Label Local Coordinate Coding (WLRLCC) algorithm, which are proposed in [ 44 ] and employed in the  X  X ransductive learning" step of our UTIL f ramework.
Throughout the paper, we denote the matrixes by upper case let-ters, e.g. X, D ; we denote the vectors by bold lower case letters, e.g. x , x i ; we denote the scalars by the normal letters, e.g. x X ij , where x i is the i -th element of the vector x , x ij element of the vector x i , and X ij is the element in the i -row and j -column of the matrix X .
For the SBFA scheme, consider a query facial image x q  X  R a d -dimensional feature space, we firstly retrieve its top n similar images X = { ( x i , y i ) i =1 , 2 ,...,n } from the weakly labeled facial image database, where y i  X  { 0 , 1 } m is the name label vector of its corresponding facial image x i , k y i k 0 = 1 , and m is the total number of classes (names) among all the top-n facial images. For the annotation task, one baseline algorithm is to adopts a soft-max weighted majority voting scheme with these initial label informa-tion { y 1 , y 2 , . . . , y n } , which is refereed as  X  X MW" in the follow-ing sections. The  X  X MW" method is limited in two aspects, 1). the initial label information is noisy; 2). the retrieval results can be refined in more powerful feature representation space. The WL-RLCC algorithm address these two problems in a unify framework with two iterative steps: the Coding Learning step and the Label Learning step.

The purpose of Coding Learning is to obtain a more discrimina-tive local coordinate coding representation, where the local coordi-nate coding technique is adopted [ 51 ]. For the i -th facial image x its sparse representation s i is reconstructed by solving the problem e (  X  s i ; x i ) based on the dictionary B = [ X, I ]  X  R X  X  R d  X  n is the feature matrix of the top-n similar facial images and I is an identity matrix: e (  X  s i ; x i ) = min where s i is a sub-vector of  X  s i with its top-n element:  X  s  X  is related to the noise information,  X  is the parameter for the locality constraints, and B  X  X  is the k -th column of dictionary B . As a result, the whole formulation for all the top-n facial images is as follows where  X  S  X  R ( n + d )  X  n = [ S ;  X  ] , S  X  R n  X  n is the non-negative local coordinate coding of X , and  X   X  R d  X  n is the noise matrix.
The purpose of Label Learning is to refine the initial weak label information. The new label matrix is achieved based on the graph-based label smoothness principle, which means that two similar facial images tend to share the similar labels. In the Weak Label Regularized Label Local Coordinate Coding algorithm, the visual similarity information is introduced with the locality coding repre-sentation achieved in the previous step. In particular, the j -th local coefficient s ij of facial image x i essentially encodes the locality information between x i and x j , j 6 = i . A larger value of s cates that x j is more representative of x i , as a result, a larger value of s ij implies that the name labels of x i and x j are more likely to be the same. Suppose the initial weak label matrix is  X  objective function for the refined label matrix Y is as follows:
E 2 ( Y ; S ) = min Y  X  0 where M = [ h (  X  Y ij )] is an indicator matrix: h ( x ) = 1 if x &gt; 0 and otherwise h ( x ) = 0 , and  X  denotes the Hadamard product of two matrices. s ij is the j -th local coefficient of facial image x which essentially encodes the locality information between x x , j 6 = i . As the ideal true label matrix is often very sparse, a series of extra convex sparsity constraint are introduced to take into the consideration of sparsity: k Y i X  k 1  X  1 , where i = 1 , 2 , . . . , n
To better exploit the potential of the two previous learning ap-proaches: Code Learning and Label Learning , they are further re-inforced into a unified optimization framework. Specifically, the optimization formulation of Weak Label Regularized Label Local Coordinate Coding is formulated as follows:
Q (  X  S, Y ) = E 1 (  X  S ; X ) + E 2 ( Y ; S ) = min  X  1 t r ( 1 (  X  S  X  V )) +  X  2 tr ( Y  X  LY ) +  X  3 k ( Y  X   X  s . t .  X  S ii = 0 , k Y i X  k 1  X  1 , i = 1 , 2 , . . . , n, i s all-one-element matrix with dimension n  X  ( n + d ) , and tr ( ) denotes a trace function. In the above,  X  2 tr ( Y  X  LY ) is a label smoothness regularizer which connects the label matrix and the sparse features. For the final annotation step, an effective sparse reconstruction scheme is applied.

The reasons that we adopt the WLRLCC algorithm for the  X  X rans-ductive learning" step in the proposed UTIL framework are two-fold: (i) the WLRLCC algorithm is suitable for handling large-scale problem as it is applied only to the short list of the similar images for each query image and is independent of the entire re-trieval database size; (ii) the WLRLCC algorithm fully exploits the short list of top-ranking similar images via a unifying optimization scheme and achieves the best annotation performance over a large-scale web facial image database. In this section, we present the proposed Weak Label Laplacian SVM (WL-LapSVM) algorithm for solving the  X  X nductive learn-ing" task in the proposed Unifying Transductive and Inductive Learn-ing (UTIL) scheme.
We denote the whole retrieval database and the corresponding name labels with D = { ( x i , y i ) | i = 1 , 2 , . . . ,  X  n } , where x is a d -dimensional facial feature vector, y i  X  { 0 , 1 } sponding name label vector with only one non-zero value: k y 1 ,  X  n denotes the total number of facial images in the whole database, and  X  m is the total number of unique human names. For simplicity, in the following sections, we denote the label vector y i equals the index value of the non-zero item in y i . Further, we de-note by D j = { ( x k , y k )  X  D | y k = j } the subset of the crawled images belonging to the j -th name(person).

In order to train inductive classifiers for each person, we man-ually label a small number of facial images as the preliminary set of labeled images. As a result, for the j -th name(person) in the re-trieval database, its image set D j can be further divided into two subsets: the label set L j and the unlabel set U j , where D L
S U j . In particular, in our experiment only one image is labeled for each name (person), which means | L j | = 1 , j = 1 , 2 , . . . ,  X  m . This kind of setting is reasonable, since in real-world application the number of names is very large and it is time-consuming and impractical to label a large amount of labeled data.

As there is only one positive sample for each class(person), gen-eral semi-supervised learning technique, e.g. the Laplacian SVM, can be used to solve the small-sample problem. However, it does not work well in our problem due to the limited number of posi-tive samples. We also notice all the facial images in the database are assigned with weak labels, which can be employed for prior in-formation for classifier learning. To address the previous problem, we propose a variant of Laplacian SVM algorithm, the Weak Label Laplacian Support Vector Machine (WL-LapSVM), to handle the noisy web images in our weakly labeled facial image database.
For each class(person), we will train a separate classification model f j , j = 1 , 2 , . . . ,  X  m , which determine whether a facial im-age x belongs to class j or not. In particular, if sign( f then the facial image x is supposed to be in the j -th class and be labeled with the j -th name.

Generally, for the j -class(person), we can construct its training set T j by introducing the labeled subset L j from the j -th class as the labeled positive samples, introducing the labeled subsets { L k | k 6 = j } from the other classes as the labeled negative samples, and the unlabeled subset U j from the j -th class as the unlabeled samples. However, in our application, the size of labeled positive training set L j is extremely small(only 1 ), as a result, we only col-lect a subset of the labeled negative samples into the training sets T . For simplicity, we can represent the training set T j of the j -th class as follows:
T j = { ( x 1 , z 1 ) , ( x 2 , z 2 ) , . . . , ( x l , z where z  X  { X  1 , 1 } is the class labels, l is the number of la-beled samples, and u is the number of unlabeled samples ( l + u = | T j | ). Following the traditional Laplacian SVM algorithm (LapSVM) [ 28 ], we define a kernelized target function f the j -class, which is also denoted as f ( x ) for short in the rest parts of this section: where  X  ( , ) is a kernel function,  X  = [  X  1 ,  X  2 , . . . ,  X  k ( x ) = [  X  ( x 1 , x ) ,  X  ( x 2 , x ) , . . . ,  X  ( x cian SVM problem is to minimize the following objective function with respect to the previous classification function f . where k k A is the norm in the Reproducing Kernel Hilber Space (RKHS ) H k of kernel  X  . V is a loss function on the label data, we choose the L 2 hingloss function in our experiment :  X ( f ) is an intrinsic regularizer to employ the geometry information among the label data and unlabelled data by the Laplacian matrix, which is defined as :
 X ( f ) = X where K is the kernel matrix of the instances in the training set T , L is the graph Laplacian matrix, and L = D  X  W , W is the adjacency matrix of the data graph in training set ( W  X  ( x i , x j ) , x i , x j  X  T j , we choose  X  as an RBF kernel function in our framework). D is a diagnal matrix with diagonal elements
In our application, the traditional Laplacian SVM algorithm does not work well, because the positive labeled images are very limited (only one positive reference image) and the facial images are in a high dimensional space and can not be separated linearly. In order to overcome this challenge, we propose to employ p  X  u samples from the unlabeled samples of T j as the pseudo-positive labeled samples, which means that they are not definitely positive samples. To reduce the risk of using these unlabeled samples, we propose to assign each unlabeled instance with a confidence weighting value, which could be achieved with extra information(e.g. the ranking position in the searching result).

As a result, the previous unlabeled samples { x k | k = l + 1 , l + 2 , . . . , l + u } in T j can be represented as { ( x k , z 1 , l + 2 , . . . , l + u } , where the confidence weighting value  X  [0 , 1] . The label z k is set as 1 for the collected pseudo samples, and for the uncollected samples, we can just set its confidence value as z k = 0 . Correspondingly, for the labeled instances in T j also assign a fixed confidence weights  X  k = 1 for k = 1 , 2 , . . . , l . By employing the weighted unlabeled instances into the tradi tional Laplacian SVM as an extra term, we achieve a new confidence value weighted Laplacian SVM formulation as follows: where  X  k = 1 for k = 1 , 2 , . . . , l , and  X  k  X  [0 , 1] for k = l +1 , l +2 , . . . , l + u . We refer to the proposed modified Laplacian SVM based on weak label as  X  WL-LapSVM " for short. If  X  3 the formulation reduces to a standard Laplacian SVM algorithm.
In this section, we briefly introduce the optimization algorithm for the proposed WL-LapSVM, which could be reformulated as follows: where k  X  i = k ( x i ) ,  X   X  i = 1 with i = 1 , 2 , . . . , l , and  X  [0 ,  X  3 ] with i = l + 1 , l + 2 , . . . , l + u . In order to solve this problem, we follow the Newton Method proposed in [ 28 ]. In each N ewton X  X  step, we update  X  with the following rule : where t is the iteration number, s is the step size, and  X  are the gradient vector and Hession matrix for g (  X  ) in Equation 7. For  X  , we have: where S  X  R n  X  n is a diagonal matrix and its i -th element in the main diagonal is  X   X  i . K is the kernel matrix of the instances in the corresponding training set. The Hessian matrix H could be achieved as follows: The step size s could be fixed to 1 or optimized by line searching. The iterative update is guaranteed to converge when the error vector does not change for two consecutive iterations. For each name (person) in the retrieval database, we can build a WL-LapSVM model f j , j = 1 , 2 , . . . ,  X  m , respectively. To anno-tate the query image x q , we firstly compute its prediction value y qj = f j ( x q ) , j = 1 , 2 , . . . ,  X  m ; then convert these prediction value into the probability scale: p qj = 1 1+exp(  X  y sigmoid function following the technique in [ 34 ]. Finally, the an-n otated name list is obtained by sorting the previous probability Figure 2: Annotation performance of WL-LapSVM with dif-f erent confidence weighting settings. The x -axis is the confi-dence weight  X  p of the last pseudo sample.
For the proposed WL-LapSVM algorithm, there are mainly two critical problems that highly affect its annotation performance: 1) one problem is how to collect the p pseudo positive samples from the unlabeled training samples; 2) another problem is how to set the confidence value for each pseudo positive sample. In the following, we will take the j -th class as an example, where the image set is D j = L j S U j and the training set is T j .

For the first problem, in our framework there are two ways to in-troduce the pseudo samples from the unlabeled set U j : one way is to collect the images that are close to the true positive sample in L another way is to collect the images with high Google ranking val-ues, which means that these images are at the top-ranking position in Google retrieval result. According to our experiments, we found that the performance of the second scheme is much better than the first scheme. For example, supposed p = 80 extra pseudo samples are collected, the annotation performance of the second scheme is 68% , compared with 26% of the first method.

For the second problem, it aims to introduce the prior knowledge into WL-LapSVM by setting the confidence weighting value  X  in Eq. 9. Suppose the p p seudo positive samples are { x 1 , x sorted by the Google ranking value. In our framework, we set the confidence weight of x i according to its index value i by using a monotonic decreasing function:  X  i = exp( i  X  1  X  ) , where  X  is a parameter. By choosing different  X  value, we can control the con-tribution of the pseudo positive sample set. In figure 2, for different n umber of pseudo positive samples ( p = 60 and p = 80 ), we evalu-ate different confidence weights setting by making  X  p = exp( notation performances are consistently better. As a result, we set  X  = 0 . 5 for all the following experiments.
The last step for the proposed UTIL framework is to combine the annotation results from both  X  X ransductive learning" and  X  X n-ductive learning." Generally, the annotation problem can also be formulated as a multi-class classification problem, and the combi-nation is a typical late-fusion (post-classification fusion) problem. There are numerous score late fusion methods in the literature [ 22 , 23 ], which can be classified into three levels: A bstract Level , Rank Level , and Measurement level . As there are only two  X  X lassifiers" in the UTIL framework, the voting-based abstract level fusion is unsuitable for our experiments. For the rank level fusion, each model (the  X  X ransductive learning" model and the  X  X nductive learn-ing" model) outputs a list of possible names for the query image, sorted in decreasing order of confidence. For the measurement level fusion, each model outputs the possibility (confidence) values of assigning different name to the query image.
In paretical, for a query image, the annotation results of the  X  transductive" and  X  X nductive" learning steps can be represented as two possibility(measurement) score vectors: p T , p I  X  [0 , 1] respectively. For example, p T i illustrates the possibility of assign-ing the i -th name to the query image according for the  X  X rans-ductive learning" model. We can easily generate the ranking re-sults for the two models by sorting the confidence score vector p , p I  X  [0 , 1]  X  m , respectively.
For the measurement level fusion, following the normalization scheme in [ 22 ], we use s um rule for measurement combination and adopt two kinds of normalization methods: the min-max nor-malization which is refereed as  X  X LF-MinMax" in the following experiments and the Z-score normalization which is referred as  X  X LF-Zscore" for short.
For the rank level fusion scheme, Ho et al. [ 19 ] describe three m ethods to combine the ranks assigned by the different models: highest rank method, Borda count method, and Logistic Regression method. In our experiments, we only adopt the borda count scheme for fusion as there are just two models in the UTIL framework. Instead of directly using the rank position as the rank value, we set the rank value according to a monotonic decreasing function. For the combination weights of different ranks, we propose two kinds of methods: one is based on confidence value regression ( X  X LF-Regression"), and another is based on the confidence value entropy information( X  X LF-Entropy").

For RLF-Regression, given a set of query images as the training set, we set the weight value w T for the rank of the  X  X ransductive" model as 1 if the  X  X ransductive" model achieves a better annota-tion performance than the  X  X nductive" model, otherwise, w Then we adopt the SVM algorithm to train a regression model for the weight value w T by using the possibility(measurement) scores as the feature vector. Finally, for a test query image, we use the learned regression model to predict the weighting value w rank result of the  X  X ransductive" model and generate the weighting value w I for the rank result of the  X  X nductive" model by 1  X  w
For RLF-Entropy method, we aim to avoid the computation ef-fort in the previous regression scheme and estimate the weight-ing value w T according to the entropy information of the confi-dence(measurement) vector. In particular, we define the entropy of the confidence vector p T as: S imilarly, we can achieve the entropy value  X  I for p I . It is not difficult to find that when the entropy value  X  T is large, the differ-ence of the measurement scores among different candidate names is small, which indicates that the corresponding  X  X ransductive" model is less confidence, so that we should set a small weighting value for the rank result of the  X  X ransductive" model. As a result, we set the weight w T for the  X  X ransductive" model as: where w I is the weight value for the rank result from the  X  X nduc-tive" model.
To evaluate the performance of the proposed Unifying Transduc-tive and Inductive Learning (UTIL) scheme, we conduct an exten-sive set of experiments on a large real-world weakly labeled facial images database. In the following, we first briefly introduce our experiment dataset, then discuss the parameter settings, and finally present the experimental results and discussion.
Although several web facial images databases are available, for example, LFW 4 [20 ], Pubfig 5 [25 ], Yahoo!News 6 [4, 15 ], and FAN-L arge 7 , these databases are not suitable for the performance evalua -tion of the proposed Unifying Transductive and Inductive Learning (UTIL) scheme for serval reasons, e.g., the number of facial im-ages per person is too small for the search-based face annotation. In our experiments, we adopt the weakly labeled web facial image database released in [ 44 ], which consists of four retrieval database i n different size and one query database with about 1 , 600 images. In order to train the proposed WL-LapSVM model in the  X  X nductive learning" step, for each person/class in the retrieval database, we manually label one front-view facial image as the reference image. Notice that we do not make extra collection for the reference im-age, which aims to examine the generalization performance of the proposed Unifying Transductive and Inductive Learning (UTIL) scheme. Due to the manual labeling effort, in our experiments, we only label the retrieval database  X  X DB-040K" which contains 400 persons and more than 40 , 000 facial images.

To evaluate the annotation performance, we adopt the hit rate at top-T annotated results as the performance metric, which measures the likelihood of having the true label among the top-T annotated names. Specifically, for T = 1 , the hit rate is the same with the accuracy . For the  X  X ransductive learning" step, we retrieve 40 most similar images for each query image from the retrieval database. For the  X  X nductive learning" step, as a fair comparison, we adopt the RBF kernel for all the compared algorithms with  X  = 0 . 2 . For the other parameters, we randomly divide the query(test) database into two parts of equal size, and randomly collect one part for tuning the optimal parameters by a grid search scheme.
In this experiment, we evaluate how the number of pseudo pos-itive samples affects the performance of different algorithms. We compare the proposed WL-LapSVM algorithm with three baseline algorithms: the SVM algorithm using the only one positive sam-ple, the LapSVM algorithm using the only one positive sample, and another SVM that employs p weakly labeled samples as posi-tive samples, denoted as  X  X L-SVM" for short. The experimental result is presented in Figure 3 and Table 1.
 W e can draw several observations from the results. First, for both SVM and LapSVM, the annotation performances are rather poor by using only one positive reference image, and the hit rate of svm and LapSVM at the top-1 position are only 7 . 6% and 10 . 3% , respec-tively. By introducing the manifold information, the LapSVM algo-rithm is slightly better than SVM. Second, by using the pseudo pos-itive samples, the annotation performance of SVM can be improved significantly. In particular, when the number of pseudo samples is increased from 10 to 150 , the annotation performance will also boost from 23 . 0% to 62 . 0% . The additional pseudo positive train-ing samples take credit for the performance improvement, which indicates it is important to employ extra samples in the  X  X nduc-tive learning" step. Third, the annotation performance can be fur-ther improved by adopting the proposed WL-LapSVM algorithm, Figure 3: Annotation performance of inductive learning algo -rithms with varied numbers ( p ) of pseudo positive samples. which indicates that the proposed WL-LapSVM algorithm can ef-fectively use the pseudo samples by assigning different pseudo pos-itive samples with different confidence values. For example, by us-ing only 20 pseudo positive samples, the proposed WL-LapSVM algorithm can achieve a better annotation performance than WL-SVM with 80 pseudo positive samples. This is very important in a large-scale problem, where the number of persons is huge and more training samples will take more storage space and computa-tional costs.
 Table 1: Annotation performance of inductive learning algo-rithms with varied numbers ( p ) of pseudo positive samples .

I n this experiment, we evaluate the annotation performance of the  X  X ransducitve learning" step and the  X  X nductive learning" step, respectively. For the  X  X ransductive learning" scheme, we adopt two algorithms, including a majority-voting based algorithm  X  X MW" and the state-of-the-art  X  X LRLCC" algorithm [ 44 ]. For the  X  X n-d uctive learning" scheme, we adopt the WL-SVM algorithm in the previous experiment and the proposed WL-LapSVM algorithm. For both WL-SVM and WL-LapSVM algorithms, we use p = 150 pseudo positive samples in our experiment. In Figure 4 and Ta-b le 2, both the mean and standard deviation of the annotation per-f ormance ( hit rate ) are reported with different T values, where T is the number of annotated names.

Several observations can be drawn from the above experimen-tal results. First of all, for the  X  X ransductive learning" step, the WLRLCC algorithm significantly outperforms the simple baseline algorithm  X  X MW", which is similar to the observations reported in [ 44 ]. Second, for the  X  X nductive learning" step, the proposed W L-LapSVM algorithm achieves comparable results with the stat-of-the-art WLRLCC algorithm. In particular, the WL-LapSVM algorithm performs slightly worse than the WLRLCC algorithm when only one name is annotated( T = 1 ), however, its perfor-Figure 4: Comparison of face annotation performance by dif-f erent algorithms, where T is the number of the annotated names. mance is better for large T values. It indicates that the  X  X nductive learning" algorithm WL-LapSVM has a better recall performance than the  X  X ransductive learning" algorithm WLRLCC.
In this experiment, we evaluate the annotation performance of the proposed Unified Transductive and Inductive Learning (UTIL) scheme, by combining the two annotation models with different last-fusion algorithms, including the measurement level fusion ( X  X LF-MinMax",  X  X LF-Zscore") and the rank level fusion ( X  X LF-Regression",  X  X LF-Entropy"). The average annotation performance are reported in Figure 5 and Table 2, respectively.
 Figure 5: Comparison of different last-fusion algorithms in the UTIL framework
Several observations can be drawn from the above experimental results. First, by adopting proper fusion algorithm, the proposed UTIL scheme can significantly boost the annotation performance. In particular, the annotation results of WLRLCC and WL-LapSVM are 0 . 7665 and 0 . 7624 . By using the last-fusion in UTIL, the per-formance can be boosted to 0 . 8025 by  X  X LF-Regression" fusion and 0 . 7988 by  X  X LF-Entropy" fusion. Second, in our experiments, the rank level fusion algorithms are more suitable for the proposed UTIL framework than the measurement level fusion algorithms. In particular, both  X  X LF-Regression" and  X  X LF-Entropy" consis-tently outperform the measurement level fusion ( X  X LF-MinMax" and  X  X LF-Zscore"). Furthermore, the  X  X LR-MinMax" fusion scheme even performs worse than the individual  X  X L-LapSVM" algorithm for large T values. Third, for the rank level fusion scheme, al-though  X  X LF-Regression" is slightly better, it is a supervised scheme that needs extra labeled samples and training efforts. The pr oposed entropy based fusion method  X  X LF-Entropy" can achieve a very close combination result without extra efforts. For the proposed WL-LapSVM algorithm, the parameter  X  3 in Equation 9 is fixed as 1 , and the parameters  X  1 and  X  2 by a grid search scheme. Figure 6 shows one the grid search re-notice that the performance of WL-LapSVM tends to be stable in the region  X  1  X  [0 . 01 , 0 . 2] and  X  2  X  [0 . 08 , 0 . 7] . For this grid searching result, we choose  X  1 = 0 . 1 and  X  2 = 0 . 5 for the fur-ther experiments. In our experiments, we also found that generally the WL-LapSVM algorithm performs well with the parameters lo-cated in the previous range, which indicates that the WL-LapSVM algorithm is robust in terms of the parameter setting.

This paper investigates a unifying learning scheme by combining both transductive and inductive learning techniques to mine web facial images for auto face annotation. In particular, to address the small positive sample problem in the  X  X nductive learning" scheme, we propose a Weakly Label Laplacian Support Vector Machines (WL-LapSVM) algorithm to train classifiers based on weakly la-beled data. We adopt the state-of-the-art technique WLRLCC al-gorithm for the  X  X ranductive learning" scheme. To fully exploit the two types of learning paradigms, we evaluate different last-fusion algorithms on both measurement level and rank level . We also pro-pose an entropy-based rank level fusion algorithm, which performs as well as the supervised regression-based fusion algorithm without extra training efforts. Our empirical results show that the proposed UTIL scheme can significantly outperform both the transductive and inductive annotation approaches. Future work explores the ap-plications of our techniques to solve other real-world problems. This work is supported by Singapore MOE Academic tier-1 grant (RG33/11) and Microsoft Research grant [1] D. Anguelov, K. chih Lee, S. B. G X kt X rk, and B. Sumengen. [2] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [4] T. L. Berg, A. C. Berg, J. Edwards, and D. Forsyth. Who X  X  in [5] T. L. Berg, A. C. Berg, J. Edwards, M. Maire, R. White, [6] Z. Cao, Q. Yin, X. Tang, and J. Sun. Face recognition with [7] G. Carneiro, A. B. Chan, P. Moreno, and N. Vasconcelos. [8] O. Chapelle, B. Sch X lkopf, and A. Zien, editors.
 [9] J. Y. Choi, W. D. Neve, K. N. Plataniotis, and Y. M. Ro. [10] J. Cui, F. Wen, R. Xiao, Y. Tian, and X. Tang. Easyalbum: an [11] K. Delac and M. Grgic. Face Recognition . IN-TECH, 2007. [12] P. Duygulu, K. Barnard, J. de Freitas, and D. Forsyth. Object [13] J. Fan, Y. Gao, and H. Luo. Multi-level annotation of natural [14] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. [15] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Face [16] M. Guillaumin, J. Verbeek, and C. Schmid. Is that you? [17] A. Hanbury. A survey of methods for image annotation. J. [18] E. Hjelm X s and B. K. Low. Face detection: A survey. [19] T. K. Ho, J. J. Hull, and S. N. Srihari. Decision combination [20] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. [21] R. Jafri and H. R. Arabnia. A survey of face recognition [22] A. K. Jain, K. Nandakumar, and A. Ross. Score [23] J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas. On [24] M. G. Kresimir Delac and M. S. Bartlett. Recent Advances in [25] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. [26] D.-D. Le and S. Satoh. Unsupervised face annotation by [27] C.-F. Lin and S.-D. Wang. Fuzzy support vector machines. [28] S. Melacci and M. Belkin. Laplacian Support Vector [29] T. Mensink and J. J. Verbeek. Improving people search using [30] H. V. Nguyen and L. Bai. Cosine similarity metric learning [31] D. Ozkan and P. Duygulu. A graph based approach for [32] L. Page, S. Brin, R. Motwani, and T. Winograd. The [33] P. Pham, M.-F. Moens, and T. Tuytelaars. Naming persons in [34] J. C. Platt. Probabilistic outputs for support vector machines [35] X. Rui, M. Li, Z. Li, W.-Y. Ma, and N. Yu. Bipartite graph [36] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. [37] S. Satoh, Y. Nakamura, and T. Kanade. Name-it: Naming [38] C. Siagian and L. Itti. Rapid biologically-inspired scene [39] J. Tang, R. Hong, S. Yan, T.-S. Chua, G.-J. Qi, and R. Jain. [40] Y. Tian, W. Liu, R. Xiao, F. Wen, and X. Tang. A face [41] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Image [42] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang. Multi-label [43] D. Wang, S. C. Hoi, and Y. He. Mining weakly labeled web [44] D. Wang, S. C. H. Hoi, Y. He, and J. Zhu. Retrieval-based [45] G. Wang, A. Gallagher, J. Luo, and D. Forsyth. Seeing [46] X.-J. Wang, L. Zhang, F. Jing, and W.-Y. Ma. Annosearch: [47] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. [48] F. Wu, Y. Han, Q. Tian, and Y. Zhuang. Multi-label boosting [49] X. Wu and R. Srihari. Incorporating prior knowledge with [50] Z. Wu, Q. Ke, J. Sun, and H.-Y. Shum. Scalable face image [51] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local [52] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Face [53] J. Zhu. Semi-supervised learning literature survey. Technical [54] J. Zhu, S. C. Hoi, and L. V. Gool. Unsupervised face [55] J. Zhu, S. C. Hoi, and M. R. Lyu. Face annotation by [56] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised
