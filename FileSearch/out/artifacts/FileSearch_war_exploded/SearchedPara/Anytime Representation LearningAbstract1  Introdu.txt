 Zhixiang (Eddie) Xu 1 xuzx@cse.wustl.edu Matt J. Kusner 1 mkusner@wustl.edu Gao Huang 2 huang-g09@mails.tsinghua.edu.cn Kilian Q. Weinberger 1 kilian@wustl.edu
Washington University, One Brookings Dr., St. Louis, MO 63130 USA Tsinghua University, Beijing, China Machine learning algorithms have been successfully de-ployed into many real-world applications, such as web-search engines (Zheng et al., 2008; Mohan et al., 2011) and email spam filters (Weinberger et al., 2009). Tra-ditionally, the focus of machine learning algorithms is to train classifiers with maximum accuracy X  X  trend that made Support Vector Machines (SVM) (Cortes &amp; Vapnik, 1995) very popular because of their strong generalization properties. However, in large scale industrial-sized applications, it can be as important to keep the test-time CPU cost within budget. Further, in medical applications, features can correspond to costly examinations, which should only be performed when necessary (here cost may denote actual currency or patient agony). Carefully balancing this trade-off between accuracy and test-time cost introduces new challenges for machine learning.
 Specifically, this test-time cost consists of (a) the CPU cost of evaluating a classifier and (b) the (CPU or mon-etary) cost of extracting corresponding features. We explicitly focus on the common scenario where the fea-ture extraction cost is dominant and can vary drasti-cally across different features, e.g. web-search rank-ing (Chen et al., 2012), email spam filtering (Dredze et al., 2007; Pujara et al., 2011), health-care applica-tions (Raykar et al., 2010), image classification (Gao &amp; Koller, 2011a).
 We adopt the anytime classification setting (Grubb &amp; Bagnell, 2012). Here, classifiers extract features on-demand during test-time and can be queried at any point to return the current best prediction. This may happen when the cost budget is exhausted, the classi-fier is believed to be sufficiently accurate or the pre-diction is needed urgently ( e.g. in time-sensitive appli-cations such as pedestrian detection (Gavrila, 2000)). Different from previous settings in budgeted learning, the cost budget is explicitly unknown during test-time. Prior work addresses anytime classification primarily with additive ensembles, obtained through boosted classifiers (Viola &amp; Jones, 2004; Grubb &amp; Bagnell, 2011). Here, the prediction is refined through an in-creasing number of weak learners and can naturally be interrupted at any time to obtain the current classifi-cation estimate. Anytime adaptations of other classi-fication algorithms where early querying of the evalu-ation function is not as natural X  X uch as the popular SVM X  X ave until now remained an open problem.
 In this paper, we address this setting with a novel approach to budgeted learning. In contrast to most previous work we learn an additive anytime represen-tation . During test-time, an input is mapped into a feature space with multiple stages: each stage refines the data representation and is accompanied by its own SVM classifier, but adds extra cost in terms of feature extraction. We show that the SVM classifiers and the cost-sensitive anytime representations can be learned jointly in a single optimization.
 Our method, Anytime Feature Representations (AFR), is the first to incorporate anytime learning into large margin classifiers X  X ombining the benefits of both learning frameworks. On two real world bench-mark data sets our anytime AFR out-performs or matches the performance of the Greedy Miser (Xu et al., 2012), a state-of-the-art cost-sensitive algorithm which is trained with a known test budget. Controlling test-time cost is often performed with clas-sifier cascades (mostly for binary classification) (Vi-ola &amp; Jones, 2004; Lefakis &amp; Fleuret, 2010; Saberian &amp; Vasconcelos, 2010; Pujara et al., 2011; Wang &amp; Saligrama, 2012). In these cascades, several classifiers are ordered into a sequence of stages. Each classi-fier can either (a) reject inputs and predict them, or (b) pass them on to the next stage. This decision is based on the current prediction of an input. The cas-cades can be learned with boosting (Viola &amp; Jones, 2004; Freund &amp; Schapire, 1995), clever sampling (Pu-jara et al., 2011), or can be obtained by inserting early-exits (Cambazoglu et al., 2010) into preexisting stage-wise classifiers (Friedman, 2001).
 One can extend the cascade to tree-based structures to naturally incorporate decisions about feature ex-traction with respect to some cost budget (Xu et al., 2013; Busa-Fekete et al., 2012). Notably, Busa-Fekete et al. (2012) use a Markov decision process to con-struct a directed acyclic graph to select features for different instances during test-time. One limitation of these cascade and tree-structured techniques is that a cost budget must be specified prior to test-time. Gao &amp; Koller (2011a) use locally weighted regression dur-ing test-time to predict and extract the features with maximum information gain. Different from our algo-rithm, their model is learned during test-time. Saberian &amp; Vasconcelos (2010); Chen et al. (2012); Xu et al. (2013) all learn classifiers from weak learners. Their approaches perform two separate optimizations: They first train weak learners and then re-order and re-weight them to balance their accuracy and cost. As a result, the final classifier has worse accuracy vs. cost trade-offs than our jointly optimized approach. The Forgetron (Dekel et al., 2008) introduces a clever modification of the kernelized perceptron to stay within a pre-defined memory budget. Gao &amp; Koller (2011b) introduce a framework to boost large-margin loss functions. Different from our work, they focus on learning a classifier and an output-coding matrix simultaneously as opposed to learning a feature rep-resentation (they use the original features), and they do not address the test-time budgeted learning sce-nario. Kedem et al. (2012) learn a feature represen-tation with gradient boosted trees (Friedman, 2001) X  however, with a different objective (for nearest neigh-bor classification) and without any cost consideration. Grubb &amp; Bagnell (2010) combine gradient boosting and neural networks through back-propagation. Their approach shares a similar structure with ours, as our algorithm can be regarded as a two layer neural net-work, where the first layer is non-linear decision trees and the second layer a large margin classifier. How-ever, different from ours, their approach focuses on avoiding local minima and does not aim to reduce test-time cost. Let the training data consist of input vectors { x labels { y 1 ,...,y n } X  X  +1 ,  X  1 } (the extension to multi-class is straightforward and described in section 5). We assume that during test-time, features are computed on-demand , and each feature  X  has an extraction cost c &gt; 0 when it is extracted for the first time. Since fea-ture values can be efficiently cached, subsequent usage of an already-extracted feature is free.
 Our algorithm consists of two jointly integrated parts, classification and representation learning. For the for-mer we use support vector machines (Cortes &amp; Vapnik, 1995) and for the latter we use the Greedy Miser (Xu et al., 2012), a variant of gradient boosting (Friedman, 2001). In the following, we provide a brief overview of all three algorithms.
 Support Vector Machines (SVMs). Let  X  denote a mapping that transforms inputs x i into feature vec-tors  X  ( x i ). Further, we define a weight vector w and bias b . SVMs learn a maximum margin separating hy-perplane by solving a constrained optimization prob-lem, min where constant C is the regularization trade-off hyper-parameter, and [ a ] + = max( a, 0). The squared hinge-loss penalty guarantees differentiability of (1), and simplifies the derivation in section 4. A test input is classified by the sign of the SVM predicting function Gradient Boosted Trees (GBRT). Given a contin-uous and differentiable loss function L , GBRT (Fried-man, 2001) learns an additive classifier H T ( x ) = P a limited depth regression tree (Breiman, 1984) (also referred to as a weak learner ) added to the current classifier at iteration t , with learning rate  X  t  X  0. The weak learner h t is selected to minimize the function L ( H t  X  1 +  X  t h t ). This is achieved by approximating the negative gradient of L w.r.t. the current H t  X  1 : The greedy CART algorithm (Breiman, 1984) finds an approximate solution to (3). Consequently, h t can targets for all inputs x i to an off-the-shelf CART im-plementation (Tyree et al., 2011).
 Greedy Miser. Recently, Xu et al. (2012) introduced the Greedy Miser, which incorporates feature cost into gradient boosting. Let c f ( H ) denote the test-time fea-ture extraction cost of a gradient boosted tree ensem-ble H and c e ( H ) denote the CPU time to evaluate all trees 3 . Let B f ,B e &gt; 0 be corresponding finite cost budgets. The Greedy Miser solves the following opti-mization problem: where L is continuous and differentiable. To formal-ize the feature cost , they define an auxiliary function F ( h t )  X  { 0 , 1 } indicating if feature  X  is used in tree h t for the first time, ( i.e. F  X  ( h t ) = 1). The authors show that by incrementally selecting h t according to min the constrained optimization problem in eq. (4) is (approximately) minimized up to a local minimum (stronger guarantees exist if L is convex). Here,  X  trades off the classification loss with the feature ex-traction cost (enforcing budget B f ) and the maximum number of iterations limits the tree evaluation cost (en-forcing budget B e ). As a lead-up to Anytime Feature Representations, we formulate the learning of the feature representa-tion mapping  X  : R d  X  R S and the SVM classi-fier ( w ,b ) such that the costs of the final classifica-B f ,B e . In the following section we extend this for-mulation to an anytime setting, where B f and B e are unknown and the user can interrupt the classifier at any time. As the SVM classifier is linear , we con-sider its evaluation free during test-time and the cost c originates entirely from the computation of  X  ( x ). Boosted representation. We learn a representa-tion with a variant of the boosting trick (Trzcinski et al., 2012; Chapelle et al., 2011). To differentiate the original features x and the new feature representation  X  ( x ), we refer only to original features as  X  features  X , and the components of the new representation as  X  di-mensions  X . In particular, we learn a representation  X  ( x )  X  R S through the mapping function  X  , where S is the total number of dimensions of our new rep-resentation. Each dimension s of  X  ( x ) (denoted [  X  ] s is a gradient boosted classifier, i.e. [  X  ] s =  X  P T t =0 Specifically, each h t s is a limited depth regression tree. For each dimension s , we initialize [  X  ] s with the s tree obtained from running the Greedy Miser for S iterations with a very small feature budget B f . Sub-sequent trees are learned as described in the following. During classification, the SVM weight vector w assigns a weight w s to each dimension [  X  ] s .
 Train/Validation Split. As we learn the feature rep-resentation  X  and the classifier w ,b jointly, overfitting is a concern, and we carefully address it in our learn-ing setup. Usually, overfitting in SVMs can be over-come by setting the regularization trade-off parame-ter C carefully with cross-validation. In our setting, however, the representation changes and the hyper-parameter C needs to be adjusted correspondingly. We suggest a more principled setup, inspired by Chapelle et al. (2002), and also learn the hyper-parameter C . To avoid trivial solutions, we divide our training data into two equally-sized parts, which we refer to as train-ing and validation sets, T and V . The representation is learned on both sets, whereas the classifier w ,b is trained only on T , and the hyper-parameter is tuned for V . We further split the validation set into vali-dation V and a held-out set O in a 80 / 20 split. The held-out set O is used for early-stopping.
 Nested optimization. We define a loss function that approximates the 0-1 loss on the validation set V , where  X  ( z ) = 1 1+ e az is a soft approximation of the sign (  X  ) step function (we use a = 5 throughout, similar to Chapelle et al. (2002)) and  X  y specific weight to address potential class imbalance. f (  X  ) is the SVM predicting function defined in (2). The classifier parameters ( w ,b ) are assumed to be the op-timal solution of (1) for the training set T . We can express this relation as a nested optimization problem (in terms of the SVM parameters w ,b ) and incorporate our test-time budgets B e ,B f : min According to Theorem 4.1 in Bonnans &amp; Shapiro (1998), L V is continuous and differentiable based on the uniqueness of the optimal solution w  X  ,b  X  . This is a sufficient prerequisite for being able to solve L V via the Greedy Miser (5), and since the constraints in (7) are analogous to (4), we can optimize it accordingly. Tree building. The optimization (7) is essentially solved by a modified version of gradient descent, up-dating  X  and C . Specifically, for fast computation, we update one dimension [  X  ] s at a time, as we can uti-lize the previous learned tree in the same dimension to speed up computation for the next tree (Tyree et al., in detail in section 4.2. At each iteration, the tree h t selected to trade-off the gradient fit of the loss function L
V with the feature cost of the tree, min We use the learned tree h t s to update the representa-tion [  X  ] s = [  X  ] s +  X h t s . At the same time, the variable C is updated with small gradient steps. 4.1. Anytime Feature Representations Minimizing (7) results in a cost-sensitive SVM ( w ,b ) that uses a feature representation  X  ( x ) to make classi-fications within test-time budgets B f ,B e . In the any-time learning setting, however, the test-time budgets are unknown . Instead, the user can interrupt the test evaluation at any time.
 Anytime parameters. We refer to our approach as Anytime Feature Representations (AFR) and Al-gorithm 1 summarizes the individual steps of AFR in pseudo-code. We obtain an anytime setting by steadily increasing B e and B f until the cost constraint has no effect on the optimal solution. In practice, the tree budget ( B e ) increase is enforced by adding one tree h s at a time (where t ranges from 1 to T ). The fea-ture budget B f is enforced by the parameter  X  in (8). Algorithm 1 AFR in pseudo-code. As the feature cost is dominant, we slowly decrease  X  (starting from some high value  X  0 ). For each interme-diate value of  X  we learn S dimensions of  X  ( x ) (each dimension consisting of T trees). Whenever all S di-mensions are learned,  X  is divided by a factor of 2 and an additional S dimensions of  X  ( x ) are learned and concatenated to the existing representation.
 Whenever a new feature is extracted by a tree h t the cost increases substantially. Therefore we store the learned representation mapping function and the learned SVM parameters whenever a new feature is extracted. We overload  X  f to denote the rep-resentation learned with feature f th extracted, and w f ,b f as the corresponding SVM parameters. Stor-ing these parameters results in a series of triplets (  X  1 , w 1 ,b 1 ) ... (  X  F , w F ,b F ) of increasing cost, i.e. c (  X  1 )  X   X  X  X   X  c (  X  F ) (where F is the total number of extracted features). Note that we save the map-ping function  X  , rather than the representation of each training input  X  ( x ).
 Evaluation. During test time, the classifier may be stopped during the extraction of the f +1 th feature, be-cause the feature budget B f (unknown during training time) has been reached. In this case, to make a pre-diction, we sum the previously-learned representations generated by the first f features w &gt; f P f k =1  X  k ( x ) + b This approach is schematically depicted in figure 1. Early-stopping. Updating each dimension with a fixed number of T trees may lead to overfitting. We apply early-stopping by evaluating the prediction ac-curacy on the hold-out set O . We stop adding trees to each dimension whenever this accuracy decreases. Algorithm (1) details all steps of our algorithm. 4.2. Optimization Updating feature representation  X  ( x ) requires comput-ing the gradient of the loss function L V w.r.t.  X  ( x ) as stated in eq. (8). In this section we explain how to compute the necessary gradients efficiently.
 Gradient w.r.t.  X  ( x ) . We use the chain rule to compute the derivative of L V w.r.t. each dimension [  X  ] where f is the prediction function in eq. (2). As chang-ing [  X  ] s not only affects the validation data, but also the representation of the training set, w and b are also functions of [  X  ] s . The derivative of f w.r.t. the repre-sentation of the training inputs, [  X  ] s  X  X  is where we denote all validation inputs by  X  V . For val-idation inputs, the derivative w.r.t. [  X  ] s  X  X  is Note that with |T| training inputs and |V| validation inputs, the gradient consists of |T| + |V| components. In order to compute the remaining derivatives  X  w  X  [  X  ] [  X  ] s . First, let us define the contribution to the loss of input x i as  X  i = [1  X  y i ( w  X  &gt;  X  ( x i ) + b  X  )] value w  X  ,b  X  is only affected by support vectors (inputs with  X  i &gt; 0). Without loss of generality, let us as-sume that those inputs are the first m in our ordering, x ,..., x m . We remove all non-support vectors, and let b  X  = [ y 1  X  1 ,...,y n We also define a diagonal matrix  X   X  R n m  X  n m whose diagonal elements are class weight  X  ii =  X  y then rewrite the nested SVM optimization problem within (7) in matrix form: min As this objective is convex, we can obtain the optimal solution of w ,b by setting  X  X   X  w and  X  X   X  X  to zero: By re-arranging the above equations, we can express them as a matrix equality, We absorb the coefficients on the left-hand side into a design matrix M  X  R d +1  X  d +1 , and right-hand side into a vector z  X  R d +1 . Consequently, we can ex-press w and b as a function of M  X  1 and z , and derive their derivatives w.r.t. [  X  ] s from the matrix inverse rule (Petersen &amp; Pedersen, 2008), leading to To compute the derivatives  X  M  X  [  X  ] upper left block of M is a d  X  d inner product matrix scaled by  X  and translated by I C , and we obtain the derivative w.r.t. each element of the upper left block, the chain rule in eq. (9), we also need Combining eqs. (10), (11), (12) and (13) completes the Gradient w.r.t. C . The derivative  X  X   X  X  is very similar non-zero value on diagonal elements, Although computing the derivative requires the inver-sion of matrix M , M is only a ( d + 1)  X  ( d + 1) ma-trix. Because our algorithm converges after generating a few ( d  X  100) dimensions, the inverse operation is not computationally intensive. We evaluate our algorithm on a synthetic data set in order to demonstrate the AFR learning approach, as well as two benchmark data sets from very different do-mains: the Yahoo! Learning to Rank Challenge data set (Chapelle &amp; Chang, 2011) and the Scene 15 recog-nition data set from Lazebnik et al. (2006).
 Synthetic data. To visualize the learned anytime feature representation, we construct a synthetic data set as follows. We generate n = 1000 points (640 for training/validation and 360 for testing) uniformly sampled from four different regions of two-dimensional space (as shown in figure 2, left). Each point is la-beled to be in class 1 or class 2 according to the XOR rule. These points are then randomly-projected into a ten-dimensional feature space (not shown). Each of these ten features is assigned an extraction ingly, each feature  X  has zero-mean Gaussian noise added to it, with variance 1 c feature  X  ). As such, cheap features are poorly repre-sentative of the classes while more expensive features more accurately distinguish the two classes. To high-light the feature-selection capabilities of our technique we set the evaluation cost c e to 0. Using this data, we constrain the algorithm to learn a two-dimensional anytime representation ( i.e.  X  ( x )  X  X  2 ).
 The center portion of figure 2 shows the anytime repre-sentations of testing points for various test-time bud-gets, as well as the learned hyperplane (black line), margins (gray lines) and classification accuracies. As the allowed feature cost budget is increased, AFR steadily adjusts the representation and classifier to better distinguish the two classes. Using a small set of features (cost = 95) AFR can achieve nearly perfect test accuracy and using all features AFR fully sepa-rates the test data.
 The rightmost part of figure 2 shows how the learned SVM classifier changes as the representation changes. The coefficients of the hyperplane w = [ w 1 ,w 2 ] &gt; ini-tially change drastically to appropriately weight the AFR features, then decrease gradually as more weak learners are added to  X  . Throughout, the hyper-parameter C is also optimized.
 Yahoo Learning to Rank. The Yahoo! Learn-ing to Rank Challenge data set consists of query-document instance pairs, with labels having values from { 0 , 1 , 2 , 3 , 4 } , where 4 means the document is perfectly relevant to the query and 0 means it is ir-relevant. Following the steps of Chen et al. (2012), we transform the data into a binary classification problem by distinguishing purely between relevant ( y i  X  3) and irrelevant ( y i &lt; 3) documents. The resulting labels are y  X  { +1 ,  X  1 } . The total binarized data set contains 2000, 2002, and 2001 training, validation and test-ing queries and 20258, 20258, 26256 query-document instances respectively. As in Chen et al. (2012) we replicate each negative, irrelevant instance 10 times to simulate the scenario where only a few documents out of hundreds of thousands of candidate documents are highly relevant. Indeed in real world applications, the distribution of the two classes is often very skewed, with vastly more negative examples presented.
 Each input contains 519 features, and the feature extraction costs are in the set { 1 , 5 , 10 , 20 , 50 , 100 , 150 , 200 } . The unit of cost is the time required to evaluate one limited-depth regression tree h t (  X  ), thus the evaluation cost c e is set to 1. To evaluate the cost-accuracy performance, we follow the typical convention for a binary ranking data set and use the Precision@5 metric. This counts how many documents are relevant in the top 5 retrieved documents for each query.
 In order to address the label inbalance, we add a mul-tiplicative weight to the loss of all positive examples,  X  + , which is set by cross validation (  X  + = 2). We set the hyper-parameters to T = 10, S = 20 and  X  0 = 10. As the algorithm is by design fairly insensitive to hyper-parameters, this setting was determined without need-ing to search through ( T,S, X  0 ) space.
 Comparison. The most basic baseline is GBRT with-out cost consideration. We apply GBRT using two different loss functions: the squared loss and the un-regularized squared hinge loss. In total we train 2000 trees. We plot the cost and accuracy curves of GBRT by adding 10 trees at a time. In addition to this ad-ditive classifier, we show the results of a linear SVM applied to the original features as well.
 We also compare against current state-of-the-art com-peting algorithms. We include Early-Exit (Cam-bazoglu et al., 2010), which is based on GBRT. It short-circuits the evaluation of lower ranked and un-promising documents at test-time, based on some threshold s (we show s = 0 . 1 , 0 . 3), reducing the over-all test-time cost. Cronus (Chen et al., 2012) im-proves over Early-Exit by reweighing and re-ordering the learned trees into a feature-cost sensitive cascade structure. We show results of a cascade with a max-imum of 10 nodes. All of its hyper-parameters (cas-cade length, keep ratio, discount, early-stopping) were set based on the validation set. We generate the cost/accuracy curve by varying the trade-off param-eter  X  , in their paper. Finally, we compare against Greedy Miser (Xu et al., 2012) trained using the un-regularized squared hinge loss. The cost/accuracy curve is generated by re-training the algorithm with different cost/accuracy trade-off parameters  X  . We also use the validation set to select the best number of trees needed for each  X  .
 Figure 3 shows the performance of all algorithms. Al-though the linear SVM uses all features to make cost-insensitive predictions, it achieves a relatively poor result on this ranking data set, due to the limited power of a linear decision boundary on the original feature space. This trend has previously been ob-served in Chapelle &amp; Chang (2011). GBRT with un-regularized squared hinge loss and squared loss achieve peak accuracy after using a significant amount of the feature set. Early-Exit only provides limited improve-ment over GBRT when the budget is low. This is primarily because, in this case, the test-time cost is dominated by feature extraction rather than the eval-uation cost. Cronus improves over Early-Exit signif-icantly due to its automatic stage reweighing and re-ordering. However, its power is still limited by its fea-ture representation, which is not cost-sensitive. AFR out-performs the best performance of Greedy Miser for a variety of cost budgets. Different from Greedy Miser, which must be re-trained for different budgets along the cost/accuracy trade-off curve (each resulting in a different model), AFR consists of a single model which can be halted at any point along its curve X  providing a state-of-the-art anytime classifier. It is noteworthy that AFR obtains the highest test-scores overall, which might be attributed to the better gen-eralization of large-margin classifiers.
 Scene recognition. The second data set we exper-iment with is from the image domain. The scene 15 (Lazebnik et al., 2006) data set contains 4485 images from 15 scene classes. The task is to classify the scene in each image. Following the procedure use by Li et al. (2010); Lazebnik et al. (2006), we construct the train-ing set by selecting 100 images from each class, and leave the remaining 2865 images for testing. We ex-tract a variety of vision features from Xiao et al. (2010) with very different computational costs: GIST, spatial HOG, Local Binary Pattern (LBP), self-similarity, tex-ton histogram, geometric texton, geometric color, and Object Bank (Li et al., 2010). As mentioned by the authors of Object Bank, each object detector works independently. Therefore we apply 177 object detec-tors to each image, and treat each of them as indepen-dent descriptors. In total, we have 184 different image descriptors, and the total number of resulting raw fea-tures is 76187. The feature extraction cost is the ac-tual CPU time to compute each feature on a desktop with dual 6-core Intel i7 CPUs with 2.66GHz, ranging from 0.037s (Object Bank) to 9.282s (geometric tex-ton). Since computing each type of image descriptor results in a group of features, as long as any of the fea-tures in a descriptor is requested, we extract the entire descriptor. Thus, subsequent requests for features in that descriptor are free.
 We train 15 one-vs-all classifiers, and learn the fea-ture representation mapping  X  , the SVM parameters ( w , b ,C) for each classifier separately. Since each de-scriptor is free once extracted, we also set the descrip-tor cost to zero whenever it is use by one of the 15 classifiers. To overcome the problem of different de-cision value scales resulting from different one-vs-all classifiers, we use Platt scaling (Platt, 1999) to re-scale each classifier prediction within [0 , 1]. 4 We use the same hyper-parameters as the Yahoo! data set, except we set  X  0 = 2 10 , as the unit of cost in scene15 is much smaller.
 Figure 4 demonstrates the cost/accuracy performance of several current state-of-the-art techniques and our algorithm. The GBRT-based algorithms include GBRT using the logistic loss and the squared loss, where we use Platt scaling for the hinge loss variant to cope with the scaling problem. We generate the curve by adding 10 trees at a time. Although these two methods achieve high accuracy, their costs are also significantly higher due to their cost-insensitive nature. We also evaluate a linear SVM. Because it is only able to learn a linear decision boundary on the original feature space, it has a lower accuracy than the GBRT-based techniques for a given cost. For cost-sensitive methods, we first evaluate Early-Exit . As this is a multi-class classification problem, we intro-duce an early-exit every 10 trees, and we remove test inputs after platt-scaling results in a score greater than a threshold s . We plot the curve by varying s . Since Early-Exit lacks the capability to automat-ically pick expensive and accurate features early-on, its improvement is very limited. For Greedy Miser , we split the training data into 75 / 25 and use the smaller subset as validation to set the number of trees. We use un-regularized squared hinge-loss with differ-ent values of the cost/accuracy trade-off parameter ter than the previous baselines, and our approach con-sistently matches it, save one setting. Our method AFR generates a smoother budget curve, and can be stopped anytime to provide predictions at test-time. To our knowledge, we provide the first learning al-gorithm for cost-sensitive anytime feature representa-tions. Our results are highly encouraging, in partic-ular AFR matches or even outperforms the results of the current best cost-sensitive classifiers, which must be provided with knowledge about the exact test-time budget during training.
 Addressing the anytime classification setting in a prin-cipled fashion has high impact potential in several ways: i) reducing the cost required for the average case frees up more resources for the rare difficult cases X  thus improving accuracy; ii) decreasing computational demands of massive industrial computations can sub-stantially reduce energy consumption and greenhouse emissions; iii) classifier querying enables time-sensitive applications like pedestrian detection in cars with in-herent accuracy/urgency trade-offs.
 Learning anytime representations adds new flexibility towards the choice of classifier and the learning set-ting and may enable new use cases and application areas. As future work, we plan to focus on incorpo-rating other classification frameworks and apply our setting to critical applications such as real-time pedes-trian detection and medical applications.
 Acknowledgements KQW, ZX, and MK are sup-
