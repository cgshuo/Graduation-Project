 Most of search engines have tried some form of hyperlink analysis because it significantly improves the relevance of search results. The two best-known algo-rithms that perform link analysis are HITS [10] and PageRank [2]. The latter, used in Google, employs the hyperlink structure of the Web to build a stochastic irreducible Markov chain with a transition matrix P. The transition matrix was built on the assumption that a  X  X andom surfer X  who is given a Web page at random and keeps clicking on successive links, never hits  X  X ack X  but eventually gets bored and starts on another random page. The irreducibility of the chain guarantees that the long-run stationary vector , known as the PageRank vector, exists. The calculated PageRank vector is then used to measure the importance of Web resources. From the standpoint of the PageRank algorithm, the regular PageRank of a page can be regarded as the rate at which a surfer would visit that page. The PageRank model is based only on the hyperlink structure with-out considering the textual information that is carried along the edge between connected pages, and the transition probability from a given page to its outgo-ing links is weighted with equal chance. The transition probability might become more accurate if we consider some practical aspects of human search behavior. We might describe it as follows: a user goes somewhere based on his interests. He arrives there and begins thinking about the displayed content. He may find some content of interest that is somewhat similar to what he is looking for, but that still does not satisfy his requirements based on some textual informa-tion in the page. Therefore, the user will continue searching. Where will he go? The information obtained through reading a page and the literal information of outgoing links will help the user weight further options because additional searching might provide a more literal match between the words in the user X  X  mind and the words on the page. Therefore, we propose to modify the underly-ing Markov process by giving different weights to different outgoing links from a page based not only on the hyperlink structure but also the textual clues on the Web graph. The intuition here is that inbound links from pages with a similar theme to a more significant influence on its PageRank than links from unrelated pages. Although such considerations have been discussed by some researchers, an actual system and its experimental results have not been reported yet. In this paper, we propose a link analysis model that combines the literal information resources extracted from Web pages themselves with the hyperlink structure for calculating the query-independent page importance. This paper is organized as follows: Section 2 will introduce the PageRank algorithm and its several varia-tions. We propose our algorithm in Section 3. Experiment results and analysis are presented in Section 4. Lastly, we present our conclusions in Section 5. First, let us make a brief observation about Google X  X  PageRank algorithm. Imag-ine a Web surfer jumps from Web page to page, choosing with uniform proba-bility which link to follow at each step. In order to reduce the effect of dead-end or endless cycles, the surfer will occasionally jump to a random page with some small probability d , or when the page has no out-links. To reformulate this in graphic terms, consider the web as a directed graph, where nodes represent web pages, and the edges between nodes represent links between web pages. Let N be the number of verticals on the Web graph, F i be the set of pages i links to, and B i be the set pages which link to page i . If averaged over a sufficient number of steps, the probability the surfer is on page j at some point in time is given by the formula: P R ( j )=(1  X  d )/ N + d  X  page j is denoted as P R ( j ). Because the equation shown above is recursive, it must be iteratively evaluated until P R ( j ) converges. Typically, the initial distri-bution for P R ( j ) is uniform. PageRank is equivalent to the primary eigenvector of the transition matrix A :
A =(1  X  d ) Web pages, and the assumption that outgoing links from a page are equally im-portant when calculating the transition probability is naive. Two kinds of rep-resentative approaches for the integration of themes in the PageRank technique have been proposed recently: On the one hand, the  X  X ntelligent surfer X  intro-duced by Matthew Richardson [12] and on the other hand the topic-sensitive PageRank proposed by Tather Haveliwala [7]. M. Richardson and P. Domingo employ a random surfer model in order to explain their approach to the imple-mentationofthemesinthePageRanktechnique.Insteadofasurferwhofollows links completely at random, they suggest a more intelligent surfer who, on the one hand, follows only links which are related to an original search query and, on the other hand, after  X  X etting bored X , only jumps to a page which relates to the original query. Thus, an  X  X ntelligent surfer X  only surfs relevant pages that contain the term of an initial query. To model how  X  X ntelligent X  behavior of a random surfer influences PageRank, every term occurring on the Web has to be processed through a separate PageRank calculation, and each calculation is solely based on links between pages that contain that term. Apparently, comput-ing time requirements are indeed a serious issue with this approach after viewing  X  X ntelligent surfer PageRank X . The approach of Taher Havilewala seems to be more practical for actual usage. Similar to Richardson and Domingos, Havile-wala also suggests the computation of different PageRanks for different topics. Compared with  X  X ntelligent surfer PageRank X , the Topic-Sensitive PageRank does not consist of hundreds of thousands of PageRanks for different terms, but rather of a few PageRanks for different topics. Topic-Sensitive PageRank is based on the link structure of the whole Web, according to which the topic sensitivity implies that there is a different weighting for each topic. However, there are two serious limitations in Haveliwala X  X  work on Topic-Sensitive PageRank. One is that the computation of PageRank for topics in the Web is very costly and the other is that it is difficult to determine what the  X  X opic X  of a search should be. Having a Topic-sensitive PageRank is useless as long as one does not know in which topics a user is interested in. In this paper, we concentrate on investigating a way to take the textual infor-mation of Web pages into account for link analysis with a more accurate Markov transition model. Hyperlinks in a page might serve different roles. We divide the hyperlinks in a page into two types, informative links and referential links. We are interested in the links with a literal matching between pages (informative links), because the purpose of such types of links is to point to similar, more detailed, or additional information. As for the referential links, they are the links in a page that have no literal matching with its target. Users surfing on those referential links are conducting the same behavior as drawing an unseen ball from an urn while the link transfer on those informative links are guided by a literal matching with their corresponding target. 3.1 Virtual Document One major challenge to our proposed model is how to introduce the appropriate textual information units of Web pages for the literal matching function. It is well known that the similarity measure of two connected pages using content might fail to reflect the correct relationship between them due to the heterogeneous characteristics of unstructured Web pages. An IR system is purely dependent on page content and this has proved to be weakness [13]. To avoid such problem issues, an information unit, called a virtual document (VD), is generated for Web pages. What is the VD? There may be several possible ways to define VD. It is dependent on information processing tasks. The concept of the VD was introduced by Glover [6], and they used VD for Web pages classifying and describing task. In our approach, the VD of a given page is comprised of the expanded anchor text (EAT) from pages that point to him and the title words on the page itself. The definition is shown as follows: text table that includes triple elements. . It represents that the page with to the page with has description text (DT). The DT is extracted based on DOM tree structure. The left and right sibling node with text properties of the anchor tag  X  X  X  node and the text information under it are all extracted as the description data. Considering the case that sibling node with text properties around an an-chor tag node may be over several lines long and deviate from the main author X  X  motivation due to poor page structure, in our experiment, only the text infor-mation around the anchor tag within one phrase is kept as the description data of anchor link. Next, we also extract text information of the title tag. Lastly, the VD of are organized by the integrated textual information from all associated description texts of its ingoing hyperlinks and title text of its own. Fig. 1 is a sample illustration of a VD.
 Features of Virtual Document. From the definition of the VD of a page, the information resources that it contains is from the expanded anchor text of pages that point to it and its title. Such combined resources make the VD share characteristics of both anchor and title. Recent studies [5][11] examined several aspects of anchor text. They showed evidence that anchor text provides a sum-marization of the target document within the context of the source document, and the process of creating anchor text for document might be a good approxi-mation of the type of summarization presented by users to search system in most queries. As Eiron argued in his paper[5], a searching procedure may be looked at as one way of building an optimized query formulation through an anchor text collection. What is more, a page may have several ingoing links that all carry descriptions about their own impression of that page. Such collective wisdom of what the page is about is a useful implicit resource for characterizing the page. As for the title information on a page, Jin, Haupmann, and Zhai[8] found that document titles also bear a close resemblance to queries, and that they are pro-duced by a similar mental process. Thus, it is natural to expect that both titles and anchor texts capture some notion of what a document is about. Moreover, the size of VD collection is much smaller than that of the total collected data. Thus, processing it is much faster than processing the total document data. Stopword Removal from VD Space. A collection of VD sets can be looked at as a Web-based concept database. Just as an ordinary text database has certain words that appear frequently such as  X  X he X  and  X  X f X , so does a VD. In particular, there are many commonly used words in VD such as  X  X lick here X  or  X  X ome X , or  X  X ext X , and these types of terms will decrease the quality of a VD and have a negative influence on the literal matching function. In our experiment, words with an entropy [3][9] value larger than 0.65 (the maximum word entropy in our experiment is 0.78, and the thresholds are set based on human judgment) are regarded as Stopwords and it will be removed from the VD space. The entropy calculating formula is as follows: VDTF ( w, j )=# { w | w  X  VD ( j ) } ; P ( w, j )= VDTF ( w, j ) VD E T ( w )=  X  w h ere : N is the n umber of virtua l docume n ts i n virtua l docume n tco ll ectio n . pages usually carry less information to users, and terms appearing in fewer pages likely carry more information of interest.
 Definition 1. Given an actual web page i, we have Definition 2. Given a page p and its outgoing sets , the transition odds from p to q k are determined by: Homogeneity of Virtual Document. AswementionedinSection3.1,dueto the heterogeneous characteristic of unstructured Web pages, a general similarity metric cannot capture the correct relationship between two connected pages. Our intuition about the VD space tells us that the information resources in VD are somewhat homogeneous. To test the homogeneity of VD, 100 Web pages were selected randomly from the Web corpuses that satisfy the condition that the length of their VDs was more than 18 words. The method used to measure the homogeneity of a document was: we divided the document into two sections and measured the similarity of the two sections to each other. Here we use the cosine measure for the similarity metric. Fig. 2 plots the comparative results of homogeneity between an actual document (AD) and its VD. The X-axis is thedocumentIDandtheY-axisistheanglebetweenthevectorsofthetwo sections of its correspondent document. It shows clearly that the variation in the two sections in its VD is much smaller than that of the actual page content. The average angle between the two parts of the AD is around 64 degrees and most points are above the average, whereas the average angle for VD is around 27 degrees, and most points are below the average. Such analyzed results are consistent with our intuition. This homogeneous characteristic makes VD a more reasonable information resource than actual page content for literal matching calculations. 3.2 Algorithms In this section we present formally how literal mining is combined with link anal-ysis based on the Markov transition model in our approach. The main purpose of us is to introduce a more accurate transition probability calculation method for modeling actual user search behavior. Thus, we aim to assign a reasonable link weights through literal information on the connected pages. In our intuition, surfer behavior will be influenced by not only the suffer purpose but also the page content that author wrote. We assume that a surfer prefers a relevant tar-get. The mechanisms that define how a user chooses his target after viewing the current page, on one hand, should in some way reflect how likely the surfer could satisfy his need through the page being viewed, or indicate how much influence on the surfer that page X  X  contents impose. When a user continues his search (clicking), he does have a reasonable idea of what the activated page will looks like. We assume that there is something present in the user X  X  mind (VD) about the destination page that they would like to visit. In our approach, the VD is used to represent the user X  X  idea of the actual destination page. In addition, we assume that information searching is a process that gradually approaches the user X  X  desired outcome. Accordingly, user X  X  minds are somewhat consistent in their searching path. Based on the above considerations, we propose two fac-tors to evaluate the likelihood that a user will surf from p to S k , denoted by: Tra nO dds ( p  X  S k ). The first factor is to measure how likely the VD of the ac-tivated target page S k can be generated from the page p being viewed, denoted by: S I M IR ( p, V D ( S k )). The higher the similarity between a page and the VD of its target page, the more likely the surfer will accept the recommendation from the page X  X  author. Most state-of-the-art similarity equations can be used to calculate the similarity between p and S k . In our experiment, OKAPI BM25 and TFIDF schemes are performed. The second factor is to measure the similarity between two connected VDs denoted by: S I M JACCA R D ( VD ( p ) ,VD ( S k )). This value is used to indicate the consistency of surfer X  X  view of two connected nodes in the Web graph. Because the unique word list of a VD is usually short, a set operation is feasible and an easy way to measure the similarity between them. The relationship of computing elements in our algorithm is plotted in Fig. 3. Before introducing equations for calculating these two factors, we provide some definitions (definition 1 and 2) that we will use in our approach. Hyperlinks in a page might serve different roles. In our proposed link analysis model, we divide the hyperlinks in page into two types, informative links, termed InforLink, and referential link, termed as referLink. We are interested in the links with literal matching between pages, because the purpose of such kind of links is to point to similar, more detailed, or additional information. As for the referential links, they are the links in a page that have no literal matching with its target. User surfing on referential links are conducted the same behaviors as drawing a ball from an urn while surfing on informative links are guided by the literal matching with their corresponding targets. Based on the calculated values that indicate transition likelihood for all possible connections on a page, we can assign the transition probability to them and regard them as the link weight in the Markov chain. Then we can use the same processing method as the original PageRank to calculate the principle eigen-vector of the transition Matrix. The link allocation method is shown in the following equations. Parameter  X  is used for adjusting the probability that the surfer tends to follow those links with literal matching information. In this paper, the optional value of  X  will be determined by the experiment.
 We ran experiments on NTCIR 100G Web corpus [4]. There are 10,894,819 pages in the crawled Web corpus. As for the link structure of the Web corpus,  X  X ow Tie X  structure [1] is used to describe its graph features. Over 15 million nodes expose the large-scale structure of the Web graph as having a central, strongly connected core (SCC), a sub-graph (IN) with directed paths leading into the SCC, a component (OUT) leading away from the SCC, and relatively isolated tendrils attached to one of the three large sub-graphs. The statistical information on the NTCIR Web corpus is shown in Fig. 4. The experimental steps are: Firstly, SCC shown in Figure 4 was constructed based on the link structure of the Web corpus. In our experiment, there are 5 million nodes extracted iteratively from the Web graph, which is around 30 percent of the overall Web corpus. Next, link analysis based on our approach and PageRank are both performed on the SCC of the Web corpus. Nodes in the OUT sets of the Web graph are processed iteratively based on the calculated page rank value of their source nodes in the second step. Finally, around 7 million nodes obtained their page rank value. We note that not all page entities in the Web corpus have their own page rank value. There are around 3 million pages without page rank value through link analysis. The reason is that not all Web pages of the nodes in the Web graph were downloaded into the page repository. The Web corpus that we used in our experiment is just a simulated Web environment. To measure the efficiency of our proposed link analysis, several evaluation experiments were conducted. At first, query-dependent, full-text-based information retrieval was conducted on the whole corpus. Only the top 1,000 return results of a given query were extracted for evaluation. In the Web corpus, 778 relevant documents from 42 queries were provided for conducting general precision-recall evaluation of information retrieval system. As we pointed out above, due to the incomplete link structure of Web corpus, one-fourth of the pages in the Web repository has no its correspondent page rank value. This leaves only 350 relevant documents from 38 queries left for the page sets that have page rank value. To make the comparison experiment reasonable, we remove the pages that have no page rank value from the result sets of the IR baseline. Therefore, all our evaluation experiments will be performed only on the document sets that are all have a page rank value. 4.1 Original PageRank Versus Our Approach Our first experiment was to perform a comparison between the IR precision for those documents that are returned by content information retrieval based on their original PageRank score and the IR precisions based on the score of our proposed model. We choose  X  to be 0.7 from observing the behavior of our system. To find the best literal matching model for link analysis, the comparison experiments were performed among all possible matching schemes. The results are figured out in Fig. 5. As we expected, Literal-Matching-Biased link analysis all performed better than the original PageRank for both Ave. Precision-Recall and R-precision among the top 10 return documents. In addition, we found that the literal matching model using OKAPI-BM25 for with the factor together do the best job. Therefore, the following experiments were performed based on this literal matching scheme. 4.2 Direct Rank Comparison Direct comparison experiments of the rank value for the right answer (relevant file) in the results sets between PageRank and our approach were performed. Among 350 relevant documents from 38 queries, we received 214  X  X in X , 120  X  X ail X , and 16  X  X qual X . The rough metric for win, fail and equal is defined as: To observe the different degrees clearly, the plot based on the summation of rank shows clearly that the rank values based on our approach are less than those of the original PageRank in most cases. From the results of this direct rank comparison, we can deduce that our approach does make sense for achieving more accurate page importance. 4.3 Ideal Re-ranking Experiment Evaluating the usefulness of query-independent evidence in boosting search ef-fectiveness is complicated by the need to combine the query-independent score with a query-dependent score. There is a risk that a spurious negative conclu-sion could result from a poor choice of the combining function. Accordingly, we introduced an ideal re-ranking experiment, which is used to gauge the improve-ment likelihood. The way that we use this here is: first, locate the right answer (relevant file) in the IR baseline, and then compare the IR rank value with its page rank value. If the IR rank value of the relevant file is smaller than the page rank value, we will keep it as it was, but if the IR rank of relevant file is larger than its page rank, we will make some adjustment based on the scheme showed in figure 7. The lower rank boundary is the IR rank, and the upper rank boundary is the PR rank. After this rank adjustment through our ideal re-ranking method, the traditional IR precision-recall evaluation method was used to compare their efficiency. Comparison results shows in Figure 8 and Table 1. It is evident that the query-independent analysis (original PageRank) and our approach, both boost search effectiveness significantly. Compared with original PageRank, however, our approach does a good job on the lower recall and our approach obtained 8.5 percent improvement at the lowest recall. Therefore, the ideal re-ranking experiment results indicate that our proposed model promises to determine more accurately the page importance on the Web.
 In this paper, we proposed a unified model, which combines literal matching with link structure analysis. We provide a sound mechanism for unifying literal mining and link analysis for Web information retrieval. The experiment results showed that our approach will potentially do a better job for the IR task than the original PageRank. The future research that we plan to undertake is as follows:  X  We are planning to do more investigation on the literal matching kernel used  X  The Li n kT ype ( i  X  j ) factor in our approach could be used later to distin- X  Finding an appropriate practical re-ranking scheme to evaluate the effective-
