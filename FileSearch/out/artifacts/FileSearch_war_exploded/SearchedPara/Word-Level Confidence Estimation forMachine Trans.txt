 RWTH Aachen University chine translation. These measures provide a method for labeling each word in an automatically into account. They can be applied to output from nonstatistical machine translation systems as well.
 for rescoring of translation hypotheses will be investigated. 1. Introduction
The work presented in this article deals with confidence estimation for machine trans-lation (MT). Because sentences generated by a machine translation system are often is correct.
 recently have researchers started to investigate confidence measures for machine trans-lation (Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003; Blatz et al. 2004; Quirk 2004). In this article, we will develop a sound theoretical framework for calculating and evaluating word confidence measures. Possible applications of confi-dence measures include: approach to machine translation. The phrase-based translation system, which serves as the basis for one of the direct confidence measures, will be presented. Section 3 gives an overview of related work on confidence estimation for machine translation. Moreover, word posterior probabilities will be introduced, and we will explain how they can be based methods for confidence estimation, which make use of the output of a statistical machine translation system, such as word graphs or N -best lists. In Section 5, we present confidence measures based on direct models. The combination of several confidence measures into one is described in Section 6. Experimental evaluation and comparison of the different confidence measures is provided in Section 7. Section 8 deals with the rescoring of translation hypotheses using confidence measures. The article concludes in
Section 9. 2. Statistical Machine Translation 2.1 General In statistical machine translation (SMT), the translation is modeled as a decision process: Given a source string f J 1 = f 1 ... f j ... f J ,weseekthetargetstring e maximal posterior probability:
Through this decomposition of the probability, we obtain two knowledge sources: the translation model Pr ( f J 1 | e I 1 ) and the language model Pr ( e eled independently of each other. The translation model is responsible for linking the language.

Tillmann, and Ney 1999; Koehn, Och, and Marcu 2003; Tillmann 2003; Bertoldi et al. 2004; Vogel et al. 2004; Zens and Ney 2004; Chiang 2005). A more detailed description of 10 a phrase-based approach to statistical machine translation will be given in the following section. 2.2 Review of the Phrase-Based Translation System
For the confidence measures which will be introduced in Section 5.1, we use a state-of-the-art phrase-based translation approach as described in Zens and Ney (2004). phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense. The bilingual phrases are extracted from a word-aligned bilingual training corpus.
 using a weighted log-linear combination of a language model, a phrase translation model, and a word-based lexicon model. The translation models are used for both within-phrase models as they depend only on a single phrase pair, but not on the context outside the phrase.
 lation approach. This will be done for a monotone search in order to keep the equations simple. The extension to the non-monotone case is straightforward. Let ( j segmentation of the source sentence into phrases, where j k = 1, ... , K . The corresponding (bilingual) phrase pairs are denoted as
Assume a trigram language model. The phrase-based approach to SMT is then ex-pressed by the following equation:
The phrase translation probabilities are computed as a log-linear interpolation of the relative frequencies and the IBM model 1 probability. The single word X  X ased lexicon models are denoted as p ( f j |  X  e k )and p ( e i |  X 
IBM model 1 probability of f j over the whole phrase  X  e model, respectively. c 1 is the so-called word penalty, and c assigning constant costs to each target language word/phrase. The language model is a trigram model with modified Kneser X  X ey discounting and interpolation (Stolcke 2002). The search determines the target sentence and segmentation that maximize the objective function. terpolation. The model scaling factors  X  1 , ... ,  X  5 are optimized with respect to some evaluation criterion (Och 2003) such as BLEU score.
 confidence measures. We therefore introduce the following notation: Let Q the score of the phrase pair, which consists of the phrase penalty c scores, and the two word lexicon model scores (see Equation (2)): 3. Confidence Measures for MT 3.1 Related Work
In many areas of natural language processing, confidence measures have scarcely been investigated. The exception is automatic speech recognition, where an exten-sive amount of research on the topic exists. Confidence measures are widely used in this area X  X or example, in dialogue systems and in unsupervised training. Recently, researchers have started to investigate confidence measures for machine translation (Blatz et al. 2003, 2004; Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003;
Quirk 2004; Sanchis 2004). This section gives an overview of confidence estimation for machine translation on the word level as well as the sentence level and discusses its applications.
 tion was Gandrabur and Foster (2003). Their confidence measures consist of a com-sequence of up to four words in an interactive machine translation environment. The probability of being a correct extension of a given sentence prefix is computed for this word sequence. The authors report significant improvement in quality of the predicted translations.

Speech Processing (CLSP) at Johns Hopkins University, Baltimore, MD, developed confidence measures for machine translation. The combination of several confidence features using neural networks and a naive Bayes classifier was investigated. The workshop team studied confidence estimation on the word level as well as on the sen-tence level, though the focus was on the sentence level. The features applied included new features as well as those that had previously been developed by team members (Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003). Among them were also some of the word posterior probabilities, which will be presented here. Additionally, results, see Blatz et al. (2003, 2004).
 investigation of different approaches to sentence-level confidence estimation. A set of features is computed for each sentence generated by an MT system, and these features are combined using several different methods: modified linear regression, neural nets, support vector machines, and decision trees. Many of the sentence features are similar to 12 those presented in Blatz et al. (2003); the others are specific to the underlying MT system tagged data for training the confidence measures. The author found that using a small amount of manually labeled training data yields better performance than using large quantities of automatically labeled data.
 of output on N -best lists produced by different MT systems. Word-level confidence mea-sures, namely the rank-weighted sum as described in Section 4.1 (and first introduced in Ueffing, Macherey and Ney [2003]), are used to discard low-quality system output before selecting a translation from the various MT systems.
 word level, but also for n -grams, and are successfully applied to the rescoring of MT hypotheses. 3.2 Word Posterior Probabilities
The confidence of a target word can be expressed by its posterior probability, that is, the probability of the word occurring in the target sentence, given the source sentence. Word posterior probabilities are the basis of all approaches to confidence estimation presented here. The following explains how they can be determined. The different methods can be classified into two categories: system-based methods, which make use of system output such as word graphs or N -best lists; and direct methods, which use external knowledge sources such as statistical word or phrase lexica.
 tence posterior. The posterior probability of a sentence e a generated translation. The sentence probabilities employed in the search (see Equa-tion (1)) are not normalized, which does not affect the result of the search. But for use in confidence estimation, they need to be normalized in order to obtain a probability distri-bution over all target sentences (see Equation (6)). From the sentence posterior probabil-ities, the word posterior probabilities can be calculated by summing up the probabilities of all sentences containing the target word. For an exact quantification of word posterior probabilities, we need to consider the following problem: How can we define a criterion for the occurrence of a word in a sentence? The answer to this question is not at all triv-ial. Due to ambiguities, the word position in the sentence is not fixed. Sentences can have different numbers of words because of deletions and insertions. Additionally, the words can be reordered in different ways during the translation process. The posterior proba-bility of a target word e can depend on its occurrence in position i of the target sentence, for example, or on the number of times the word is contained in the sentence. Thus, sev-eral different definitions of posterior probabilities will be introduced and investigated in the following discussion. The basic concept of calculating the posterior probability will be explained for the target word e occurring in a fixed position i of the sentence. This is tion 4 will describe several different concepts of word posterior probabilities that relax this condition.

Here, this is approximated by the probability that an SMT system assigns to the sentence pair (see Section 2). The word posterior probability of e occurring in position i is calculated as the normalized sum of probabilities of all sentences containing e in exactly this position:
Here  X  (  X  ,  X  ) is the Kronecker function. The normalization term in Equation (4) is
This definition of word posterior probabilities raises the question of how to calculate the sums over the target sentences in Equations (5) and (6). This problem can be solved by approximating the summation space via a word graph or an N -best list. The summation is then performed explicitly over all sentences given in this restricted space. In the case of an N -best list, this is straightforward because the sentences are already listed. On a word graph, the forward X  X ackward algorithm can be applied to carry out the summa-tion efficiently. In these system-based approaches, the calculation depends on the output of the SMT system that generated the translations. The sentence probabilities summed in Equations (5) and (6) are the scores assigned by the underlying SMT system. The summation space is restricted to those hypotheses that are assigned a high probability by the SMT system, and the others are not considered.

These methods do not consider the whole target sentence. The summation of prob-based word posterior probabilities are independent of the system generating the translations. They do not require the MT system to assign a probability to the translation hypothesis. Thus, they can also be used for confidence estimation on hypotheses from a non-statistical MT system or if only the single best translations without any scores are given. 3.3 Word Confidence Measures
The idea behind word-level confidence estimation is to be able to detect possible errors in the output of a machine translation system. Using confidence measures, individ-can be used in, for example, interactive TransType-style machine translation systems (Gandrabur and Foster 2003; Ueffing and Ney 2005a).
 suitable confidence features have to be computed. Second, a binary classifier has to be defined, which decides whether a word is correct or not. The word posterior proba-bilities introduced in Section 3.2 can be interpreted as the probability of a word being correct. That is, the probability can directly be used as a confidence measure. For this 14 threshold are tagged as correct, and all others are tagged as incorrect, translations. Thus, the binary classifier is defined as The threshold t is optimized on a distinct development set beforehand.
 all an easy one. We will address this issue in Section 7.2. 4. System-Based Confidence Measures occurrence of a target word in a sentence will be defined and experimentally evaluated.
These are the models that proved most promising from a theoretical viewpoint and in the experimental evaluation:
Section 4.5 will treat the issue of scaling the probabilities that the SMT system assigns to the translation hypothesis. 4.1 Approach Based on the Fixed Target Position
In this approach, the word posterior probability is determined for word e occurring in target position i as shown in Equation (4). This variant requires the word to occur exactly words e and positions i in the target sentence is obtained. This type of word posterior probability was first introduced in Ueffing, Macherey, and Ney (2003).
 for easy calculation over word graphs and N -best lists. However, this concept is rather restrictive. In practice, the target position of a word varies between different translation alternatives. The method presented here is a starting point for more flexible approaches that perform summation over a window of target positions.
 target positions are calculated over word graphs and over N -best lists.
Calculation using word graphs. A word graph represents the most promising hypotheses generated by the translation system (Ueffing, Och, and Ney 2002; Zens and Ney 2005).
It has the advantage of being a compact representation of the translation hypothesis one designated root node n 0  X  V , representing the beginning of the sentence. Each path through the word graph represents a translation candidate. The nodes of the graph contain information such as the set of covered source positions and the language model history. Two hypotheses can be recombined if their information is identical.
Recombination is carried out during decoding to accelerate the search process. If two hypotheses represent the same information with respect to translation and language significantly decreased if only the more promising of the two hypotheses is considered for further expansion. If no recombination were carried out, the word graph would have the structure of a tree.
 contain weights representing the part of the probability that is assigned to each particu-lar word as part of the target hypothesis. When multiplying the scores along a path, the probability of the corresponding hypothesis is obtained. The sentence position of a word refers to the path length in the word graph: Consider an edge ( n , n ) that is annotated with word e . If a path leading from source node n be the ( i + 1)-th word in the corresponding sentence. Note that due to recombination recombined in node n , then e will be in position i + 1 in the one resulting sentence, and in i + 1 in the other sentence.
 is the root node n 0 . The other nodes represent different states with respect to the set 16 language model is applied, that is, all paths leading into a node share the last two words.
The translation alternatives contained in this word graph represent different reorderings of the words in the sentence: The monotone translation that do as well as the correctly reordered sequence do that occur. Note that in order to limit the size of the graph and keep the presentation simple, an example was chosen where all target sentences have the same length.
 the probabilities of all paths in the graph that contain an edge annotated with e in posi-tion i of the target sentence. This summation is performed efficiently using the forward X  backward algorithm (Jurafsky and Martin 2000). This algorithm also determines the total probability mass that is needed for normalization, as shown in Equation (6). In the following, we will present the exact equations for a word graph generated by the first word of a target phrase is assigned the score for the whole phrase. That is, when translating a source phrase  X  f k by a target phrase  X  e of all sub-models for this phrase is included for the first word e words e i model score of a phrase pair as defined in Equation (3) in Section 2.2. In order to keep the notation simple, we assume a bigram language model. The extension to higher-order language models is straightforward. The forward probability  X  the probability of reaching word e i from the sentence start, where e i of the sentence. It depends on the phrase pair (  X  f k full score of this phrase pair is included at the first word e ming the probabilities of all partial hypotheses of length i calculation in ascending order of i . We obtain the following formula: tence from the current word on. It can be determined recursively in descending order of i . Again, we distinguish two cases:
 X  i ( e i ;  X  e k ,  X  f k ) =
Using the forward X  X ackward algorithm, the word posterior probability of word e in position i is determined by combining the forward and backward probabilities of all hypotheses containing e in this position. We carry out a summation over all correspond-ing phrase pairs (  X  f k ,  X  e k ). This yields be performed. The normalization term p ( f J 1 ): = mass contained in the word graph and can be calculated by summing the backward probabilities of all words that occur in the first sentence position: probability of the word can appearing in the second position of the target sentence is to be calculated. There are two edges in the graph that contain this word in the desired target position. Thus, the probabilities of the paths leading through these edges have to be summed. The forward probabilities are the probabilities of the incoming edges, shown by dashed lines. The backward probabilities are those of the paths marked by dotted lines. They are combined (separately for each edge) and then summed to obtain the word posterior probability of can in position 2.

Calculation using N -best lists. An N -best list contains the n most promising translation hypotheses generated by the statistical machine translation system. The N -best list descending order. This representation allows for easy computation of the sum given in
Equation (5). Furthermore, the calculation of more complex variants of word posterior probabilities, such as the approach based on Levenshtein alignment (see Section 4.3), is feasible.
 terior probabilities presented in Equation (4) are calculated by summing the sentence probabilities of all sentences containing target word e in target position i . The sentence determined as
The normalization term in the denominator equals the probability mass contained in the N -best list. 18 the rank-weighted frequency of a word as follows: The relative frequency of e occurring in target position i in the N -best list is computed as
The rank-weighted frequency is determined as
Here, the inverted ranks N + 1  X  n are summed up because an occurrence of the word in a hypothesis near the top of the list will score better than one in the lower ranks.
Equations (9) and (10) could also be calculated over N -best lists that do not contain the sentence probability. 4.2 Approach Based on a Window over Target Positions
One way of accounting for slight variations in the target position i of word e is the intro-duction of a window i  X  t , t  X  N , around position i . The word confidence is determined as the sum of the word posterior probabilities calculated for the positions within this window. This leads to
The window can easily be integrated both into the N -best list and the word graph X  X ased implementation: The target position-dependent word posterior probabilities are calcu-lated as stated in Equation (4), and the summation over the positions in the window is performed in an additional step. 4.3 Approach Based on the Levenshtein Alignment
Another way of accounting for variations in the target position of a word is to perform the Levenshtein alignment (Levenshtein 1966) between sentence e formed over all sentences containing e in a position Levenshtein-aligned to i (Ueffing, Macherey, and Ney 2003).
 Levenshtein alignment is performed between the hypothesis e e n ,1 contained in the N -best list individually, and then the summation is carried out.
For word graphs, no efficient way of determining the Levenshtein alignments and the resulting word posterior probabilities is known.
L ( e I 1 , e n , I n n ,1 )thatofword e in position i in e culating the Levenshtein alignment between the sentences e e n ,1 =  X  X  C G E F X  yields
Using this representation, the word posterior probability of word e occurring in a position Levenshtein-aligned to i is given by The probability depends on all target words in the hypothesis e because the Levenshtein alignment of the whole sentence,
Error Rate (WER). It can be shown that the word posterior probabilities form a part of the Bayes risk for WER: Formulating the loss function and deriving the risk yields a minimization criterion consisting of the word posterior probabilities defined previously, one term representing the sentence length, and one for the deletion operations in the
Levenshtein alignment. For more details, see Ueffing and Ney (2004) and Ueffing (2006). 4.4 Count-Based Approach
Inspired by Bayes risk for Position-independent Word Error Rate (PER), the word posterior probability can be defined by taking the counts of the words in the generated sentence into account (Ueffing and Ney 2004). The probability of target word e occurring in the sentence n times is determined as
Here, n e is the count of word e in sentence e I 1 ,and n e . Analogously,  X  n E 1 denotes the count sequence for sentence  X  e these counts will be zero, of course. The posterior probabilities can then be expressed by the distribution over the count sequences: 20 where the distribution over the count sequences is determined by summing up the probabilities of all sentences with these counts:
Using this concept, the target position of the word is not taken into account, but the first occurrence of a word in the sentence will obtain a word posterior probability different from that of the second occurrence.
 related to the count-based word posterior probabilities defined here and one term related to the posterior probability of the sentence length. We can thus expect the count-based word posterior probabilities to perform especially well if the word correctness is defined on the basis of PER. The experimental results presented in Section 7.4 will confirm this assumption.
 be determined over the word graph. The problem is that the number of occurrences information, this count cannot be determined efficiently. The normalization term in because the case n = 0isalsoincluded. 4.5 Scaling the Probabilities
During the translation process, the different sub-models (such as the language model and the lexicon model) are weighted differently. These weights or scaling factors can be optimized with respect to some evaluation criterion (Och 2003). Nevertheless, this optimization determines only the relation between the different models, and not the translation process, because the search is performed using the maximum approximation (see Equations (1) and (2)). In contrast to this, the actual values of the weights make a difference for confidence estimation, because the summation over the sentence proba-bilities is performed. To account for this and to find the optimal values of the scaling word posterior probability based on the fixed position i , for example, is then calculated according to
When determining the system-based word posterior probabilities, this scaling factor is optimized with respect to some metric for confidence estimation on a development set distinct from the test set. 5. Confidence Measures Based on Direct Models
In the following, confidence measures based on direct models will be described. These approaches model the word posterior probability directly instead of summing the prob-abilities of sentences containing the target word. Confidence measures based on IBM model 1 and phrase-based translation models were developed and will be presented here. They make use of knowledge sources such as statistical word or phrase lexica for estimating the word confidence. Unlike the system-based word posterior probabilities presented so far, these confidence measures are completely independent of the target sentence position in which the word e occurs. They determine the confidence of e being contained anywhere in the sentence. 5.1 Direct Approach to Confidence Estimation Using Phrases
The statistical models presented in Section 2.2 can be used to estimate the confidence of target words as first described in Ueffing and Ney (2005b). In contrast to the approaches context information at the sentence level, but only at the phrase level.
 of marginal probability Q ( e , f J 1 ). Therefore, we extract all source phrases f in the given source sentence f J 1 . For these source phrases, we find the possible transla-lated by summing over all phrase pairs ( f j + s j , e i + t in Section 2.2. Analogously, we define Q LM ( e i + t target phrase together with the word penalty c 1 for each word in the phrase, that is,
Note that this is the within-phrase language model probability, which does not include the context of the phrase. The language model probability at the phrase boundary is approximated by a unigram and bigram.
 containing e : where s  X  s max and t are source and target phrase lengths, s phrase length.  X  ( e , e i + t i ) denotes an extension of the Kronecker delta: 22
The value calculated in Equation (16) is not normalized. In order to obtain a probability, this value is divided by the sum over the (unnormalized) confidence values of all target words:
As shown in Equations (3) and (15), the different sub-models of the phrase-based translation approach are combined in a log-linear manner. The weights  X  the penalties c 1 , c 2 are optimized in the translation process with respect to some eval-uation criterion such as WER or BLEU. This is done using the Downhill Simplex algorithm (Press et al. 2002). The resulting values of the weights express the relation between the sub-models, but not their absolute values. They are usually normalized so that they sum to 1. For use in confidence estimation, two different aspects thus have to be considered: 5.2 Confidence Measure Based on IBM Model 1
Another type of confidence measure that does not rely on system output and is thus applicable to any kind of machine translation system is the IBM model 1 X  X ased confi-dence measure that was introduced in Blatz et al. (2003). We modified this confidence measure because we found that the average lexicon probability used there is dominated by the maximum. Therefore, we determine the maximal translation probability of the target word e over the source sentence words: where f 0 is the  X  X mpty X  source word (Brown et al. 1993). The probabilities p ( e word-based lexicon probabilities.
 report on the use of this IBM model 1 X  X ased confidence measure in a TransType-style interactive MT system. The work presented there shows that even this relatively simple confidence measure yields a significant gain in the quality of the predictions proposed by the interactive system. 6. Combination of Confidence Measures
In related work in MT as well as in speech recognition, the combination of numerous confidence features has been suggested (Gandrabur and Foster 2003; Blatz et al. 2004;
Quirk 2004; Sanchis 2004). Among the methods used for combination are multi-layer artificial neural networks, naive Bayes classifiers, and modified linear regression. different word posterior probabilities proposed here were combined with each other.
The combination was performed in a log-linear manner. Let p be the word posterior probabilities of e determined using different approaches. The word confidence resulting from their combination is calculated as
The interpolation weights  X  m are optimized with respect to some confidence evaluation metric on the development corpus using the Downhill Simplex algorithm (Press et al. 2002). With this approach, the confidence error rates were reduced over the best single confidence measure consistently on all corpora we examined. The experimental results will be presented in Section 7.4. This section also contains details on which confidence measures were combined.
 confidence measures. It was shown that they are the best single features for confidence estimation (Blatz et al. 2004). Moreover, they are closely related to Bayes risk, which yields a sound theoretical foundation (Ueffing and Ney 2004). 7. Experiments 7.1 Experimental Setting The experiments were performed on three translation tasks in different language pairs. The corpora were compiled in the EU projects TransType2 (TransType2 2005) and
TC-STAR (TC-STAR 2005), and for the NIST MT evaluation campaign (NIST 2004). The 24
They are available in three different language pairs. This domain is very specialized with respect to terminology and style. The corpus statistics are given in Table 1. The
TC-STAR corpus consists of proceedings of the European Parliament. It is a spoken language translation corpus containing the verbatim transcriptions of the speeches in the European Parliament Plenary Sessions (EPPS). The domain is basically unrestricted direction is from Spanish into English. For corpus statistics, see Table 2. The NIST corpus was compiled for the yearly MT evaluation campaign carried out since 2001. Chinese the vocabulary size and the training corpus are much larger than in the EPPS collection, as the corpus statistics presented in Table 3 show. Additionally to the bilingual data, a monolingual English corpus consisting of 636M running words was used for language model training. The SMT systems that generated the translations for which confidence estimation was performed were trained on these corpora. The same holds for the probability models that were used to estimate the word confidences.
 for testing the confidence measures:
The translation quality on the TransType2 task in terms of WER, PER, BLEU score the best results are obtained on Spanish to English translation, followed by French to
English and German to English. The reason that Systran generates translations of much lower quality than the SMT systems is due to the fact that the technical manuals are very specific in terminology. The SMT systems were trained on similar corpora so that they are familiar with the terminology. The table additionally shows the translation quality achieved by the system PBT on the NIST test set.
 26 from the phrase-based translation system. The hypotheses are generated by the version of the system that participated in the TC-STAR evaluation round in March 2005 and this article. 7.2 Word Error Measures In order to evaluate the classifier built from the confidence measures as described in
Section 3.3, reference tags are needed that define the true class of each word. In machine
Therefore, a number of different measures for identifying the reference classes for single words in a translation hypothesis were implemented (Ueffing 2006). They are inspired by different translation evaluation measures like WER and PER. All of them compare the translation hypothesis to one or X  X f available X  X everal references to determine the word errors. In this article, we will present results for the following error measures: compared to the pool of all references (in case there exist different reference translations for the development and test corpus). Second, the reference with minimum distance to the hypothesis according to the translation evaluation measure under consideration is determined. The true classes of the words are then defined with respect to this nearest reference. For example, if the PER metric is applied, the pooled variant labels all those words as correct that occur in any of the references (with this count). The second variant considers as correct only those words that are contained in the nearest reference (with this count). The latter corresponds to the procedure used for m-WER and m-PER in MT evaluation (Nie X en et al. 2000).
 different error measures on the development and test corpora of the EPPS task. It can than PER does. A comparison of the pooled and the nearest reference shows that the pooling yields a significant increase in the number of words labeled as correct. Note that the figures in the table do not directly correspond to the translation error rates for the system output. They are calculated only for the words contained in the generated translation hypotheses and do not take deleted words into account. Moreover, they are normalized by the hypothesis lengths. If WER and PER are applied as translation number of errors is divided by the number of reference words. 7.3 Evaluation Metrics
After computing the confidence measure, each generated word is tagged as either correct or incorrect, depending on whether its confidence exceeds the tagging thresh-old that was optimized on the development set beforehand. The performance of the confidence measures is evaluated using the following three measures: 28 7.4 Experimental Results
TransType2 task. Table 6 compares the classification performance of several confidence measures on the TransType2 French X  X nglish task. The CER and the IROC values are given for WER-and PER-based classification. Note that lower CER and higher IROC values express better performance. It is interesting to see that, in most of the cases, the tendencies are consistent for the two evaluation metrics: Lower CER is accompanied by higher IROC.
 clearly performs worst. This is to be expected, and the method was included only for comparison. It can be considered as a simple baseline method. The other system-based measures discriminate significantly better in both settings.
 than the direct IBM model 1. The N -best list based measure with Levenshtein alignment and the word posterior probabilities calculated over word graphs using a window perform similarly well. For WER-based classification, they are outperformed only by the direct phrase X  X ased approach, which achieves the best CER and IROC values. graphs and N -best lists: the approach based on the fixed target position and the one summing over a window of positions. In both cases, the word graph X  X ased calculation is slightly superior to that based on 10,000-best lists. However, the difference in CER is not significant.
 measure for PER-based classification. This result was to be expected because the count-based word posterior probability was derived from the Bayes risk for PER (Ueffing and Ney 2004). Even if its CER does not differ much from that of the direct phrase-based measure, there exists a clear predominance in terms of IROC. The IBM-1 X  X ased confidence measure performs rather poorly compared to the other methods. This is not surprising because the IBM model 1 is a very simple model.
 that PER is easier to learn than WER: The IROC values for PER are higher for most confidence measures. This is consistent with the results obtained in the CLSP workshop power for reference classes based on PER than for WER.
 sures, the ROC curves for some of them are given in Figure 2. In each, the diagonal shows the results for WER-based classification, and the right one for PER, respectively.
The N -best list-based method considering the fixed target position is again given for comparison. One can see that the IBM-1 X  X ased confidence measure is clearly better than this baseline for PER, but not for WER. The curves for the direct phrase-based model and the best N -best list-based method lie relatively close to each other. These two confidence measures clearly dominate all others.
 of the phrase-based translation system, we were interested in finding out whether this is due to the fact that the translation system and the confidence measure explore the same statistical models. Therefore, the system-independent confidence measures (i.e., those based on IBM model 1 and the direct phrase-based method) were tested on output from different machine translation systems, including Systran as a non-statistical MT system. The experimental results are shown in Table 7. They can be summarized as follows: 30
EPPS task. Further experiments comparing the classification performance of the differ-ent confidence measures were carried out on the EPPS data task, which is structurally plenary sessions of the European Parliament, translated from Spanish into English. The
EPPS task is more challenging than the Xerox manuals because the domain is almost unrestricted and the translation has to cope with effects of spontaneous speech. The goal of these experiments is to find out whether the confidence measures perform equally well on this challenging task as on the Xerox task. The development and test set of the
EPPS data are provided with two references each. This makes it possible to compare the two ways of handling multiple references: As explained in Section 7.2, the true class of a word can be determined either with respect to the pooled references or to the reference with minimal distance.

EPPS task. The classification with respect to m-WER and m-PER (i.e., considering only the nearest reference) as word error measures was investigated. The confidence mea-sures based on the fixed position were not calculated because the previous experiments showed that they perform significantly worse than the other measures. It can be seen in the table that the word posterior probabilities derived from the Bayes risk for the word error measures perform best: The Levenshtein-based confidence measure discriminates best for m-WER and the count-based approach for m-PER. They are clearly superior to all other confidence measures, especially in terms of IROC. For WER-based classifica-tion, the word graph-based method performs similarly well to the Levenshtein-based measure in terms of CER, but significantly worse if IROC is considered.
 good as on the Xerox data. The reason for this is that the domain of the EPPS collection capture the data as well as they do in the Xerox domain (Ueffing 2006). Nevertheless, for m-PER X  X ased classification, the direct phrase-based measures achieve the same reduc-tion in CER over the baseline as the system-based method using count information.
Because the direct phrase-based confidence measures completely disregard the target position of the word, they are better suited for PER-based classification than for WER. for reference tags defined by m-PER than for m-WER. However, it is among the methods with the worst discriminative power in both cases.

EPPS data as on the TransType2 corpora. The relative gain in CER is 15% for the best confidence measure. But because the test corpora are large X  X ith 20,000 running words they are about twice as big as the TransType2 test sets X  X ll achieved improvements are significant at the 1% level. The IROC values are comparable to those achieved on translation task as well.
 32 of the different measures. The left curve shows the results for m-WER X  X ased classifi-and the direct phrase-based confidence measures perform very similarly. There is no clear difference between these two approaches and the one calculated over a window of target positions. The discriminative power of the direct model is higher for a lower correct acceptance ratio, whereas the system-based measure performs better for a high correct acceptance ratio. The Levenshtein-based word posterior probabilities are clearly superior to all other approaches. The ROC curve lies beyond the others over the whole measures. The three other methods show relatively similar performance.
 each hypothesis to the most similar reference. As mentioned in Section 7.2, it is also possible to pool the references instead. Table 9 presents an assessment of the discrimi-native power of different confidence measures for these reference tags. The conclusions from these results are the same as for those in Table 8: The Levenshtein-based method performs best for WER, and the count-based one for PER. All reported improvements in
CER are significant at the 1% level. The IROC values for the pooled error measures are higher than for m-WER and m-PER for all confidence measures. Obviously, this method of error counting is easier to assess using confidence measures. The differences in CER are not as large here as in Table 8. However, the IROC values provide a clear indication of the differences in quality between the classifiers.

NIST task. The third translation task that was used for the evaluation of the confidence measures proposed in this article is part of the NIST MT evaluation campaign. The task here is the translation of news articles from Chinese into English. As with the EPPS data, the domain is basically unrestricted.
 that perform best on the two other tasks were evaluated on the NIST data. The results support those achieved on the EPPS collection. All confidence measures reduce by m-WER, the confidence measure using Levenshtein alignment over N -best lists other confidence measures. The count-based method achieves a CER that is 0.2% lower, which is not significant. For classification with respect to m-PER, there are two meth-ods that outperform the others: the count-based confidence measure calculated over
N -best lists and the direct phrase-based approach. They achieve CER and IROC values that differ significantly from those of the other measures. However, neither of the two approaches is clearly superior to the other: The direct phrase-based confidence measure achieves a lower CER of 27.1%, whereas the count-based confidence measure calculated over N -best lists achieves a slightly higher IROC value. The confidence measure based on IBM model 1 shows by far the worst discriminative power for both m-WER-and m-PER-based classification. The CER obtained with this method is significantly higher than those of all other measures.

Combination of features. Because feature combination yields good results in the exper-experiments. The confidence measures investigated here were combined log-linearly three translation tasks. The three single word posterior probabilities that perform best in each setting were used in the combination. For the confidence estimation with respect to reference tags defined by m-WER, these are:
If the reference tags are determined by m-PER, the features used differ slightly, depend-ing on the corpus. The measures that are combined are three of the following: 34 The experimental results for the combined confidence measures are presented in
Table 11. They show that the resulting confidence measure outperforms the best single method. The improvement in CER is up to 1.8% in absolute terms. In terms of IROC, the gain is up to 4.4 points. This is in the same range as the improvements achieved in the CLSP summer workshop (Blatz et al. 2003). However, there is one case in which the IROC decreases, namely the m-PER X  X ased classification on EPPS Spanish to English. to CER. In order to avoid this type of inconsistency, the optimization could be performed considering a combination of CER and IROC as criterion. 8. Rescoring 8.1 Approach
This section reports on the use of word posterior probabilities for rescoring of N -best lists. The rescoring is performed as follows: For every hypothesis in the N -best list, the confidence of each word in the sentence is calculated. These word posterior probabilities used as an additional model for N -best list rescoring. It serves as an indicator of the overall quality of the generated hypothesis. Additionally, the minimal word posterior probability over the sentence is determined. This can be seen as an indicator of whether combined with the existing models (such as the score assigned by the underlying SMT system and additional language model scores) in a log-linear manner. The scaling factors of all models are optimized on the development corpus using the Downhill
Simplex algorithm. This combination using the optimized factors is then applied and evaluated on the test set.
 8.2 Experimental Results
Rescoring was carried out on EPPS data using the direct phrase-based confidence measures. Within the project TC-STAR, an MT evaluation campaign was performed in March 2005 to compare the research systems of the consortium members (Ney et al. 2005). Different conditions concerning the input data were defined. In the following, that RWTH submitted to this evaluation were generated by the phrase-based translation system described in Section 2.2. N -best lists were generated for development and test corpus, with a maximum length of 20,000 and 15,000, respectively. These were then rescored with an IBM model 1, a 4-gram language model, and a deletion model based on IBM-1. The weights for all these models and for the sentence probability assigned by the SMT system were optimized with respect to BLEU score on the development corpus. For a detailed description of the system, see Vilar et al. (2005). This system was ranked first in the evaluation round according to all evaluation criteria (Ney et al. 2005). their starting points: The first one starts from the baseline system without rescoring.
The sub-model weights of this system were optimized with respect to BLEU on the
This experiment was performed to analyze the maximum improvement that can be achieved through rescoring with confidence measures. The second experiment starts from the system that has already been rescored with the three different models men-tioned above. This is the system that was used in the TC-STAR evaluation campaign, with confidence measures manages to improve upon the best available system as well.
Furthermore, it is possible to analyze whether the gains from all rescoring models are additive.

These different figures are presented here in order to separate the effect of the transla-tion and the true-casing process. The translation system was trained on a lower-cased corpus, and the true-casing is performed as an additional post-processing step. 36 output of the translation system. This system can be improved through rescoring with confidence measures by 1 BLEU point. This is only 0.1 BLEU points less than the gain achieved from rescoring with the three other models. The system from the second setup (rescored with IBM model 1, the language and the deletion model) improves the through additional rescoring with the direct phrase-based confidence measures. The improvement is consistent across all four automatic evaluation criteria. Naturally, the optimized with respect to BLEU.

The corresponding results are presented in the second block of the table. The overall translation quality is lower if case is considered. For all models applied here, the gain achieved through rescoring is not as big as in the case-insensitive evaluation. If only the confidence measures are used for rescoring, the BLEU score is increased by 0.5 points.
The NIST score and the error measures change only slightly. However, when all four rescoring models are applied, the system is significantly improved. The models used in the TC-STAR evaluation yield an increase of 0.8 BLEU points. The word posterior probabilities add another 0.3 points to this. This change is rather small, but comparable
For comparison, the translation quality of the second best system in this campaign is reported in the last row of the table. The difference in BLEU score between the RWTH system and the second best can be significantly improved through rescoring. 9. Conclusion
In this work, we set up a probabilistic framework for the computation of word posterior probabilities for machine translation. Within this framework, different concepts of word posterior probabilities were defined and analyzed. Several approaches to the calculation of word posterior probabilities were investigated and compared: system-based methods that explore information provided by the SMT system that generated the translations, and direct model-based methods that make use of statistical (translation) models. cluding their application in a rescoring scenario. The proposed confidence measures were systematically evaluated on different translation tasks and different language pairs. On all corpora, the best methods developed here reduce the confidence error rate significantly (at the 1% level). The direct confidence measures were also successfully applied to output from a non-statistical MT system.
 Acknowledgments References 38
