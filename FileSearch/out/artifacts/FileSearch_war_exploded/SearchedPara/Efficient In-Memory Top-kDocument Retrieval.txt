 For over forty years the dominant data structure for ranked doc-ument retrieval has been the inverted index. Inverted indexes are effective for a variety of document retrieval tasks, and particularly efficient for large data collection scenarios that require disk access and storage. However, many efficiency-bound search tasks can now easily be supported entirely in-memory as a result of recent hard-ware advances.

In this paper we present a hybrid algorithmic framework for in-memory bag-of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures, and evaluate the various algorithmic trade-offs for performing efficient queries entirely in-memory. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time ( TAAT ) and document-at-a-time ( DAAT ) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve effectiveness and efficiency for a va-riety of fundamental search operations.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  indexing methods ; H.3.2 [ Information Storage and Retrieval ]: Information Storage X  file organization ; H.3.3 [ Inform-ation Storage and Retrieval ]: Information Search and Retrieval X  query formulation, retrieval models, search process ; I.7.3 [ Docu-ment and Text Processing ]: Text Processing X  index generation Text Indexing, Text Compression, Data Storage Representations, Experimentation, Measurement, Performance
Top-k retrieval algorithms are important for a variety of real world applications, including web search, on-line advertising, re-lational databases, and data mining. Efficiently ranking answers to queries in large data collections continues to challenge researchers as the collection sizes grow, and the ranking metrics become more intricate. Despite recent hardware advances, inverted indexes re-main the tool of choice for processing efficiency-bound search tasks [17]. However, large memory systems also provide new opportu-nities to explore another class of indexing algorithms derived from the suffix array to potentially improve the efficiency of various in-memory ranked document retrieval tasks [25, 27].

In this paper we present a hybrid algorithmic framework for in-memory bag-of-words ranked document retrieval using a self-index derived from the FM-Index , wavelet tree , and the compressed suf-fix tree data structures [12, 20, 27, 22], and evaluate the various algorithmic trade-offs for performing efficient in-memory ranked querying. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time ( TAAT ) and document-at-a-time ( DAAT ) query processing. We show that our framework is competitive with state-of-the-art indexing struc-tures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve efficiency and effec-tiveness for various document retrieval tasks.
 Our contributions . Firstly, we propose a hybrid approach to solv-ing a subset of important top-k document retrieval problems  X  bag-of-words queries. Secondly, we present a comprehensive efficiency analysis comparing in-memory inverted indexes with top-k self-indexing algorithms for bag-of-words queries on text collections an order of magnitude larger than any other prior experimental study. To our knowledge, this is the first comparison of this new algorith-mic framework for realistic sized text collections using a standard similarity metric  X  BM25 . Finally, we describe how our algorithmic framework can be extended to efficiently and effectively support other fundamental document retrieval tasks.
In this paper, we investigate the use of self-indexing algorithms to solve the top-k document search problem . A document col-lection T is a contiguous string drawn from an alphabet  X  , where  X  = |  X  | is the number of distinct  X  X erms X  or strings. In practice,  X  can be characters ( UTF8 or ASCII ), bytes, integers, or even phrases. Each document in T is separated by a unique end of document symbol defined to be lexicographically smaller than any s  X   X  . D EFINITION 1. A top-k document search takes a query q  X   X  , an integer 0 &lt; k  X  d , and a text T  X   X  partitioned into d documents { D 1 , D 2 , . . . , D d }, and returns the top-k documents ordered by a similarity measure S ( q, D i ) .

In this work, we focus primarily on bag-of-words queries, so our baseline S ( q, D i ) ranking function is BM25 . Our S ( q, D function has the following formulation:
Here, N is the number of documents in the collection, f t number of distinct document appearances of t , f d,t is the number of occurrences of term t in document d , k 1 = 1 . 2 , b = 0 . 75 ,  X  the number of symbols in the d th document, and  X  avg is the average of  X  d over the whole collection. The free parameters k 1 be tuned for specific collections to improve effectiveness, but we use the standard Okapi parameters suggested by Robertson et al. [36].
We now present an overview of the key data structures and al-gorithms used in our framework. Here, we only outline the key properties and features of wavelet trees and suffix arrays used in our search engine; for a more in-depth tutorial, see, for exam-ple Navarro and M X kinen [30] and the references therein. Efficient operations in succinct data structures depend on two fundamental operations over a bitvector B [0 , n  X  1] : Both operations can be performed in constant time. A simple con-stant time R ANK 0/1 solution uses o ( n ) space in addition to storing B [23]. More space efficient R ANK 0/1 algorithms are possible [35].
Efficient R ANK s and S ELECT s over an alphabet of size  X  &gt; 2 can be performed using a wavelet tree [20]. A wavelet tree de-composes the R ANK s and S ELECT s operations over [0 ,  X   X  1] into R
ANK 0/1 and S ELECT 0/1 operations on a binary alphabet using a binary tree. The root of the tree represents the whole alpha-bet. Its children represent each half of the alphabet of the par-ent node. Each leaf node in the tree represents one symbol in [0 ,  X   X  1] . When answering the R ANK s query for a specific symbol s , we perform R ANK 0/1 operations at each level in the tree until we arrive at the leaf node representing s . The overall R can be computed by combining the R ANK 0/1 results at each tree level in O (log  X  ) time. Any symbol s = T [ i ] is also computed in time O (log  X  ) with a similar algorithm; we call this operation A
CCESS ( T , i ) . Using a succinct representation of R ANK S
ELECT 0/1 [35], a wavelet tree requires nH 0 + o ( n log  X  ) bits of space, where H 0  X  log  X  is the zero-order entropy of T . 1
Wavelet trees are a surprisingly versatile data structure, and have attractive time and space bounds for many primitive operations in self-indexing algorithms [14]. As a result, many top-k document retrieval approaches rely heavily on wavelet trees. A subset of im-portant wavelet tree operations include: W e assume logarithms are in base 2.

A suffix array SA [0 , n  X  1] over T stores the offsets to all suffixes in T in lexicographical order. Any pattern P of length m occurring in T is a prefix of one or more suffixes in SA . These suffixes, due to the lexicographical order within SA , are grouped together in a range SA [ sp, ep ] . To determine SA [ sp, ep ] , we perform two binary searches over SA and T . Each binary search comparison requires up to m symbol comparisons in T , for a total of O ( m log n ) time. Using additional auxiliary data structures this cost can be reduced to O ( m + log n ) [25]. Suffix array construction is a well studied problem, and many solutions with various time and space trade-offs exist in the literature [34]. However, searching for a pattern P in T using only a suffix array requires O ( n log n ) bits to store both T and SA , which in practice is at least 5 times the text size.
By replacing T with the Burrows-Wheeler Transform ( BWT ) permuted text, the key operations of a basic SA can be emulated with much less space, close to the size of T in compressed form. The Burrows-Wheeler Transform [7]  X  also known as the block-sorting transform  X  produces a permutation of a string T , denoted T BWT , by sorting the n cyclic rotations of T into full lexicographi-cal order, and taking the last column of the resulting n  X  n matrix. The resulting string T BWT tends to be more compressible as sym-bols are grouped together based on their context in T , which makes the BWT an important part in many state of the art compression sys-tems [26]. To produce T BWT for a given text T , it is not necessary to construct M as there is a duality between T BWT and the a text T : T BWT [ i ] = T [ SA [ i ]  X  1 mod n ] .

The original text T can be recovered from T BWT in linear time without the need for any additional information. To recover T from only T BWT we first recover the first column, F , in M by sort-ing the last column ( L = T BWT ), in lexicographical order. By mapping the symbols in L to their respective positions in F so L [ i ] = F [ j ] (usually referred to as the LF mapping, j = we can recover T backwards as T [ n  X  1] = T BWT [0] = $ and T [ j  X  1] = T BWT [ LF ( i )] if and only if T [ j ] = T BWT [ i ] . Since F is simply a sort of the n characters of the string in lexicographical order, it can be represented succinctly as a lookup table of alphabet characters along with the count of all symbols that appear before the current character c . The LF mapping is computed using the equation where c is the symbol T BWT [ i ] , and C [ c ] stores the number of sym-bols in T BWT smaller than c .

Performing a search in T using the BWT permuted text is straight-forward. Recall that all rows are sorted in lexicographical order in M . Therefore, for a pattern P , all occurrences of P in T must have a corresponding row in M within a range h sp, ep i . To deter-mine the range within M , we first determine the range h sp within M that corresponds to P m using C [ ] . Then, for each sym-bol j = m  X  1 . . . 0 in P , we iteratively find sp j , ep ing the number of rows within sp j +1 , ep j +1 that are preceded by the symbol P j in T . For a given row j , the LF mapping can be used to determine the row in M representing the symbol preceding j in T . The preceding row is determined by counting the number of oc-currences of c = T BWT [ j ] before the current row and ranking these occurrences within C [ s ] . Assume we have located sp j +1 which corresponds to the rows prefixed by P [ j + 1 , m ] . Then will calculate the position in F of the first occurrence of P h sp j +1 , ep j +1 i , and thus compute the start of our range of rows within M that correspond to P [ j, m ] . Similarly, we compute
Once the area h sp, ep i is determined, self-indexes offer a way to find any occurrence position SA [ j ] , for sp  X  j  X  ep . This is accomplished by sampling T at regular intervals, and marking positions of SA that point to sampled text positions in a bitmap E [0 , n  X  1] . Sampled suffix array positions are stored in an array G [ R ANK 1 ( E, j )] = SA [ j ] if E [ j ] = 1 . Given a target value the successive values i = 0 , 1 , . . . are evaluated until E [ 1 , producing the desired answer of SA [ j ] = SA [ LF i ( j )]+ i . If every  X  th text position is sampled, we guarantee i can be found for every 0  X  i &lt;  X  , and sampling requires O (( n/ X  ) log n ) extra bits for G (and for E in compressed form [35]), and computes any entry of SA within  X  applications of LF .

Similarly, in order to recover any text substring T [ l, r  X  1] (in-cluding the whole T ), we can use the same sampling of text posi-tion multiples of  X  , and store H [ i ] = SA  X  1 [ i  X   X  ] . Thus, we extend the range to T [ l, r  X   X  1] , for r  X  =  X   X   X  r/ X   X  and display from the suffix array position j = SA  X  1 [ r  X  ] . Then, we can display the area backwards as T BWT [ j ] , T BWT [ LF ( j )] , T BWT [ step requires one R ANK s and one A CCESS operation, which has the same cost as LF . Therefore, we can display T [ l, r  X  1] within O ( r  X  l +  X  ) probes of LF .
 In practice self-indexes can be reduced to a wavelet tree over T BWT with auxiliary information to emulate F (the C array) and the sampling information. This representation of a self-index is re-ferred to as an FM-Index [12, 13]. A wavelet tree built over T uses nH k ( T ) + o ( n log  X  ) bits [24] for any k  X   X  log and constant  X  &lt; 1 , so the space requirements are reasonable. Here H k ( T )  X  H k  X  1 ( T )  X  . . .  X  H 0 ( T )  X  log  X  is the k -th order entropy of T [26], a measure of the performance of any compressor using k -th order statistical modeling on T . Many other self-indexing variations exist with different time / space trade-offs [30, 15]. In principle, any of these approaches are compatible with the framework we present here, as long as the method returns a h sp, ep i range of matching suffixes.
In order to efficiently solve the top-k document search prob-lem , unadorned self-indexing algorithms are not sufficient. Two approaches to enhance the self-index have been proposed. The first is to use a document array , that is, a mapping between every suf-fix in T to its corresponding document identifier [10, 18, 27, 41]. The second is to store, in addition to the global self-index, one self-index of each individual document in the collection [22, 37]. These alternatives offer different theoretical frameworks that are not di-rectly comparable, but experimental studies [10, 31] have consis-tently shown that the first approach offers better space and time performance in practice.

Representing the document array with a single wavelet tree can provide additional important advantages. For example, the list of distinct documents where a substring P appears, with the corre-sponding term frequencies, can be obtained without any additional structure [18], in O (log d ) time per document retrieved, once the self-index has given the suffix array range of P . This information Figure 1: Precomputed top-k r esults over fixed intervals g stored in a skeleton succinct suffix tree using the HSV approach. Only the fringe leaves are processed for a given h sp, ep i range. can then be used to calculate simple TF  X  IDF based S ( q, D rics at query time [37]. In addition, several other operations such as Boolean intersection can be performed efficiently using only the wavelet tree over the document array [19].

Culpepper et al. [10] showed how to use the same wavelet tree to find the top-k documents (with raw term frequency weights) for a string P . Among all of the strategies proposed, the heuristic algo-rithm GREEDY worked best in practice. Despite the lack of worst-case theoretical guarantees, they show unadorned wavelet trees are efficient in time and space for this task. Hon et al. [22] presented a technique with worst-case performance guarantees. The HSV ap-proach builds on the same document listing strategy originally pro-posed by Sadakane [37]. While HSV was originally described us-ing individual self-indexes for each document as in Sadakane X  X  ap-proach [37], the method can be applied on top of either document-listing solution in practice. The key insight of the HSV method is to precompute the top-k results for the lowest suffix tree nodes in a predetermined sampling range. Figure 1 shows a HSV tree over D . In this example, a h sp  X  , ep  X  i range of size g is precalculated and stored in a succinct suffix tree. An arbitrary query h sp, ep i is received. The bulk of the query result is already precomputed as h sp  X  , ep  X  i . The remainder of the query can then be processed us-ing RQQ queries over the fringe ranges to generate the final top-k counts.

Using g samples guarantees that any suffix array interval h sp, ep i for a given P falls into one of three categories: (1) The range is completely covered by the sampled interval, and the top-k answer is precomputed; (2) The range is partially covered, and at most 2 g fringe leaves must to be processed at query time and merged with the sample; or (3) The range is too small to be covered. For Case (3), the complete h sp, ep i range must be processed at runtime, but is guaranteed to be smaller than 2 g .
 Navarro and Valenzuela [31] demonstrated that implementing HSV over a document array, and using the GREEDY approach of Culpepper et al. [10] to speed up document listing, is more ef-ficient than using either GREEDY or HSV in isolation. The hy-brid approach requires additional space to support HSV on top of GREEDY , but efficiency is significantly improved by limiting the number of rank queries required at query time. This approach as well as other trade-offs are explored more fully in this paper.
Traditional approaches to the top-k document search problem rely on inverted indexes . Inverted indexes have been the dominant data structure for a variety of ranked document retrieval tasks for more than four decades [44]. Despite various attempts to displace inverted indexes from their dominant position for document rank-ing tasks over the years, no alternative has been able to consistently produce the same level of efficiency, effectiveness, and time / space trade-offs that inverted indexes can provide (see, for instance Zobel et al. [45]).

Figure 2 shows a typical inverted indexing system. The system contains three key components: (1) Term Map -The vocabulary of terms, along with the number of documents containing one or more occurrence of the term ( f t ), the number of occurrences of the term in the collection ( F t ), and a pointer to the corresponding posting list. (2) Posting Lists -An ordered list of tuples, h d, f containing the document identifier and the frequency of the term in document d . For each tuple, the ordered position offsets, p also maintained in order to support phrase queries. For indexes that do not require phase queries, p  X  can be omitted. (3) Document Storage -A document map to match d to the document name, and a pointer to the document in a document cache.

Ranked document retrieval requires that only the top-k docu-ments are returned, and, as a result, researchers have proposed many heuristic approaches to improve top-k efficiency [1, 4, 5, 6, 32, 38]. These approaches can be classified in two general cat-egories: term-at-a-time ( TAAT ) and document-at-a-time ( DAAT ). Each of these approaches have various advantages and disadvan-tages.
For TAAT processing, a fixed number of accumulators are allo-cated, and the rank contribution incrementally calculated for each query term in increasing document order. When inverted files are stored on disk, the advantages of this method are clear. The in-verted file for each term can be read into memory, and processed sequentially. However, when k is small relative to the total num-ber of matching documents in collection, TAAT can be inefficient, particularly when the number of terms in the query increases, since all of the inverted lists must be processed before knowing the full rank score of each document. In early work, Buckley and Lewit [6] proposed using a heap of size k to allow posting lists to be evalu-ated in TAAT order. Processing is terminated when the sum of the contributions of the remaining lists cannot displace the minimum score in the heap.

Moffat and Zobel [29] improved on this pruning approach with two heuristics: STOP and CONTINUE . The STOP strategy is some-what similar to the method of Buckley and Lewit, but the terms are processed in order of document frequency from least frequent to most frequent. When the threshold of k accumulators is reached, processing stops. In contrast, the CONTINUE method allows the current accumulators to be updated, but new accumulators cannot be added. These accumulator pruning strategies only approximate the true top-k result list.

If approximate results are acceptable, the TAAT approach can be made even more efficient using impact ordering [1, 2, 33]. The key idea of impact ordering is to precompute the TF for each document a term appears in. Next, quantize the TF values into a variable number of buckets, and sort the buckets (or blocks) for each term in decreasing impact order. Now, the top-k representative can be generated by sequentially processing each of the highest ranking term contribution blocks until a termination threshold is reached. The authors refer to this blockwise processing method as score-at-a-time processing. Despite not using the full TF contribution for each term, Anh and Moffat [1] demonstrate that the effectiveness of impact ordered indexes is not significantly reduced, but efficiency is dramatically improved.
The alternative approach is to process all of the terms simulta-neously, one document at a time [8]. The advantage of this ap-proach is that the final rank score is known as each document is processed, so it is relatively easy to maintain a heap containing ex-actly k scores. The disadvantage is that all of the term posting lists are cycled through for each iteration of the algorithm requiring non-sequential disk reads for multi-word queries. However, our focus in this paper is in-memory ranked retrieval, so DAAT tends to work very well in practice.
 Pruning strategies to further increase efficiency also exist for is M AX S CORE . Turtle and Flood [40] observed that the BM25 TF component can never exceed k 1 +1 = 2 . 2 . So, the total score con-tribution for any term is at most 2 . 2  X  log( N/N t ) . Using this ob-servation, Turtle and Flood present an algorithm that allows post-ing values below the threshold to be skipped. As the minimum bounding score in the heap slowly increases, more and more post-ings can be omitted. Enhanced DAAT pruning strategies similar in spirit to M AX S CORE have been shown to further increase effi-ciency [4, 38]. trees. The first wavelet tree supports backwards search over the permuted text, and the second supports statistical calculations
Turtle and Flood also describe a similar approach to improve the efficiency of TAAT strategies. However, the TAAT variant is more complex than the DAAT approach as it requires an ordered candidate list of k documents to be maintained. The candidate list is used to skip document postings in each term list which could not possibly displace the current top-k documents once the heap contains k items.

Fontoura et al. [17] compare several TAAT and DAAT based in-memory inverted indexing strategies. The authors present novel adaptations of M AX S CORE and W AND [4] to significantly im-prove query efficiency of in-memory inverted indexes. The authors go on to show further efficiency gains in DAAT style processing by splitting query terms into two groups: rare terms and common terms. The exact split is based on a fixed threshold selected at query time. For our baselines, we use W AND for DAAT query processing, and M AX S CORE for TAAT query processing.
We now describe our general approach to in-memory indexing and retrieval. Figure 3 shows the key components of our retrieval system: an FM-Index and the document array wavelet tree, WT In addition, our system requires a Document Map to map docu-ment identifiers to human readable document names (or URLs). No document cache is required and the original documents or snippets around each match can be recreated directly from the FM-Index by extracting the required text positions using the suffix array sam-pling. Only the items in grey are stored and used for character-based top-k document retrieval. All other components are shown for illustration purposes only.
 1: Initialize a max-heap R  X  {} 2: for i  X  1 to q do 3: Determine h sp, ep i for term t i 4: A i  X  G REEDY ( sp, ep, k  X  ) 5: end for 6: for i  X  1 to q do 7: for j  X  1 to k  X  do 8: if A i [ j ]  X  R then 9: U PDATE ( R, A i [ j ] , score ) 10: else 11: A DD ( R, A i [ j ] , score ) 12: end if 13: end for 14: end for 15: return R [1 . . . k ]
F UNCTION G REEDY ( sp, ep, k ) 1:  X   X  WT d . root 2: A max-heap, sorted by ep  X  sp , h  X  PUSH (  X , [ sp, ep ]) 3: A priority queue PQ  X  X } . 4: i  X  0 5: while h 6 =  X  and i &lt; k do 6:  X , [ sp  X  , ep  X  ]  X  POP ( h ) 7: if  X  is leaf then 8: PQ  X  ENQUEUE (  X . docid , ep  X   X  sp  X  + 1) 9: i  X  i + 1 10: else 11: [ s 0 , e 0 ]  X  [ R ANK 0 ( B  X  , sp  X  ) , R ANK 0 ( B 12: [ s 1 , e 1 ]  X  [ R ANK 1 ( B  X  , sp  X  ) , R ANK 1 ( B 13: if e 0  X  s 0 &gt; 0 then h  X  PUSH (  X . left , [ s 0 , e 14: end if 15: if e 1  X  s 1 &gt; 0 then h  X  PUSH (  X . right , [ s 1 , e 16: end if 17: end if 18: end while 19: return PQ
A simple bag-of-words search using a self-index retrieval sy stem is outlined in Algorithm GREEDY -TAAT . Recall that the sp and ep range for any string can be found using a backwards search in the BWT permuted text using only a wavelet tree over T BWT and C . So, the h sp, ep i for each query term in Line (3) can be calculated in O ( | t i | log  X  ) time using an FM-Index. Now, a wavelet tree over the document array WT d can be used to retrieve exactly k documents in frequency order for each term using GREEDY or QUANTILE [10]. This algorithm is analogous to TAAT processing, and is referred to as GREEDY -TAAT . Note that Function GREEDY can also be augmented with HSV as described in Section 4 to further increase the efficiency of constructing A i for each query term.
We also present several variations on this general strategy. First, we consider the addition of HSV style precomputations over as described by Navarro and Valenzuela [31]. Instead of storing the top-k most frequent symbols in the skeleton suffix tree, we store i T otal Matches Average n i length of the inverted lists processed. the top-k most important symbols sorted by term impact for each interval g to improve effectiveness. In order to capture k -values commonly used in IR systems ( k = 10 , 100 , 1000 ), we prestore values of any k that is a power of 2 up to 8192 in term contribution order. Note that we go higher than 1024 since the values of k essary to ensure good effectiveness can be greater than the desired k .

Observe that in typical bag-of-words query processing over En-glish text, the size of the vocabulary is often small relative to the total size of the collection. As such, we also present a new hybrid approach to top-k bag-of-words retrieval using a Term Map and WT d . If we assume the vocabulary is fixed for each collection, then the h sp, ep i range for each term can be precalculated and retrieved using a term map, as in the inverted indexing solution. This means that the FM-Index component is no longer necessary when process-ing bag-of-words queries. We refer to these hybrid approaches as space requirements of our approach, but also limit the full func-tionality of some auxiliary operations. For example, the text can no longer be reproduced directly from the index, so snippets cannot be generated on the fly, and phrase queries are no longer natively supported. However, for classic bag-of-words queries, our hybrid approach provides an interesting trade-off to consider. Our final variation is to support a term-based self-index. We refer to this approach as FM -TERM .

It is also possible to support a DAAT query processing strategy in our retrieval system, but this would require efficiently supporting RMQ over the document array. Our approach currently supports a generalization of RMQ  X  RQQ . But, the cost of RQQ is O (log d ) per k value extracted, while constant time solutions for RMQ currently exist [16]. However, an RMQ style approach as presented by Fis-cher and Heun [16] incurs an additional 2 n bits of space, and so we do not explore the possibility further in this work.
 Also note the current top-k bag-of-words approach shown in item. This means that our current implementation only approxi-mates the top-k items. This is a well-known problem in the inverted indexing domain. This limitation holds for any character-based bag-of-words self-indexing system that does frequency counting at query time since we can not guarantee that item k + 1 in any of the term lists does not have a higher score contribution than any item currently in the top-k intermediate list. A method of term contribution precalculation is required in order to support language-model ranking. Without the term contribution scoring, W
AND and M AX S CORE enhancements are not possible, and there-fore every document in the h sp, ep i must be evaluated in order to guarantee the final top-k ordering. However, this limitation can be mitigated by using HSV since we can precalculate the impact con-tribution for each sample position and store this value instead of storing only the frequency ordering. Top-k guarantees are also pos-sible using a term-based self-indexing system where each distinct term is mapped to an integer using HSV or other succinct repre-sentations of term contribution preprocessing. In future work, we intend to fully examine all of the possibilities for top-k guarantees using self-indexes in various bag-of-words querying scenarios.
When using character-based self-indexing approaches for bag-of-words queries, there is another disadvantage worth noting. For self-indexes, there is an efficiency trade-off between locating the top-k f d,t values and accurately determining f t since the index can extract exactly k f d,t values without processing every document. For a fixed vocabulary, f t is easily precomputed, and can be stored in the term map with the h sp, ep i pairs. But, in general it is not straightforward to determine f t for arbitrary strings over out auxiliary algorithms and data structures to support calculating the value on the fly. The FM -HSV approach allows us to prestore f for each sampled interval which can be be used to calculate f over h sp, ep i more efficiently by only processing potential fringe leaves. Calculating f t using only WT d for arbitrary strings in near constant time using no additional space remains an open problem.
In order to test the efficiency of our approach, two experimental collections were used. For a small collection, we used the 7 and 8 ad hoc datasets. This collection is composed of 1 . 86 GB of newswire data from the Financial Times , Federal Register , LA Times , and Foreign Broadcast Information Service , and consists of around 528 , 000 total documents [42]. For a larger in-memory col-lection, we used the TREC WT10G collection. This collection con-sists of 10 . 2 GB of markup text crawled from the internet, totalling 1 , 692 , 096 documents [21].

All of the algorithms described in this paper were implemented using C/C++ and compiled with gcc 4.6.1 with -O3 optimizations. For our baselines, we have implemented the in-memory variant of W
AND as described by Fontoura et al. [17] for DAAT , and an in-memory variant of M AX S CORE for TAAT . Experiments were run on a single system with 2  X  Intel Xeon E5640 Processors with a 12 MB smart cache, 144 GB of DDR3 DRAM, and running Ubuntu Linux 11.10. Times are reported in milliseconds unless otherwise noted. All efficiency runs are reported as the mean and median of 10 consecutive runs of a query, and all necessary information is preloaded into memory.

Note that we do not carry out an full evaluation of the effective-ness of the algorithms presented in this paper. In previous work, we showed that the BM25 ranking and query evaluation framework used in our approach can be as effective as other state-of-the-art open source search engines when using k  X  &gt; k , and do not repeat those experiments here [11]. In these experiments, we use the min-imum k  X  values that result in retrieval performance that is compa-rable to the effectiveness obtained through exhaustive processing. In all experiments we use k  X  = 8  X  k for the TREC 7 &amp; 8 dataset, and k  X  = 2  X  k for the TREC WT10G dataset. These values for k give results for the MAP and P@10 effectiveness measures that are not statistically significantly different compared to exhaustive pro-cessing, for both collections (paired t -test, p &gt; 0 . 05 ). We intend to pursue additional efficiency and effectiveness trade-offs in future work.
In order to test the efficiency of our algorithms, queries of vary-ing length were extracted from a query log supplied by Microsoft. Each query was tested against both TREC collections, and the filter-ing criteria used was that every word in the query had to appear in at least 10 distinct documents, resulting in a total of 656 , 172 unique queries for the TREC 7 &amp; 8 collection, and a total of 793 , 334 unique queries for the TREC WT10G collection. From the resulting filtered query sets, two different query samples were derived.

First, 1000 queries of any length were randomly sampled from each set, to represent a generic query log run. The 1 , 000 sampled queries for TREC 7 &amp; 8 have an average query length of 4 . 224 , and the average query length of the WT10G sample set is 4 . 265 words per query. For the second set of experiments, 100 queries for each query length 1 to 8 were randomly sampled from the same MSN query sets. Table 1 shows the statistical properties of the sampled queries that were used in the second experimental setup, including the average number of documents returned for each query for each query length, and the average length of postings lists processed for each query, computed as ( P | q | i =1 n i ) / | q | .
In order to test the efficiency of our algorithms, two experiments were performed on each of the collections. The first experiment is designed to measure the average efficiency for each algorithm, Figure 6: Efficiency of query length of 1 to 8 on the T REC given a sampling of normal queries. For this experiment, the length of the queries was not bounded during sampling, and had an aver-age query length of just over 4 words per query as mentioned in Section 7.1.

Figures 4 and 5 show the relative efficiency of each method av-eraged over 1 , 000 randomly sampled MSN queries for TREC 8, and TREC WT10G . Each boxplot summarizes the time values as follows: the solid line indicates the median; the box shows the 25 th and 75 th percentiles; and the whiskers show the range, up to a maximum of 1 . 5 times the interquartile range, with outliers beyond this shown as separate points. In both figures, the follow-ing abbreviations are used for the algorithms: FM -GREEDY ( FM ), and TAAT .

We report the timings for all of the self-indexing methods using the character-based indexes. We also ran the same experiments using our term-based indexes, but the performance was identical. This result is not surprising since the dominant cost in the self-indexing method is traversing the wavelet tree over the document array, and is dictated by the depth of the wavelet tree and not the overall length. Since the depth depends only on the number of documents, both approaches consistently produce similar running times. So, the only efficiency difference between character-based and term-based indexes is in space-usage which is discussed in Sec-tion 7.

We see that the self-indexing methods which must calculate all frequency scores ( FM and SE ) incur the most overhead as k in-creases. This is largely due to the multiplicative effect of collating many consecutive k values. For example, when collating frequency values from WT d , the number of rank operations is proportional to the depth of the wavelet tree. In the case of the TREC WT10G lection, which contains around 1 . 6 million documents, the depth of the wavelet tree is 24 . So, the number of random rank probes in wavelet tree begins to significantly degrade the performance for larger k .
 This effect can be marginalized by augmenting WT d with HSV . Since an HSV style index has a portion of each h sp, ep i ranges, only the fringe positions for each range need to be calculated at runtime, reducing the total number of page faults. For all k , the HSV indexes are efficient and remarkably resilient to outliers. In general, the W AND variant of DAAT is more efficient for large val-ues of k , but can perform poorly for certain queries. For example, the query  X  point out the steps to make the world free of pollution  X  on the WT10G collection consistently performed poorly in our DAAT framework.
We now break down the efficiency of each of our algorithms rel-ative to two parameters: k and q , where q is the number of terms in a query. Figure 6 shows the average of 10 runs of 100 queries per query length, q . For one-word queries, for all values of k , the inverted indexing approaches DAAT and TAAT are superior. This is not surprising since only a single term posting must be traversed to calculate BM25 , and the algorithms have excellent locality of access. Still, the HSV variant is the most efficient for small k .
For | q | &gt; 1 , the results also depend on k . For k = 10 and k = 100 , the self-indexing methods are more efficient than since the methods can extract exactly k values. Since the sample rates in the lower regions of the HSV methods are close to k , very little work needs to be done by the indexes. The W AND -based DAAT method remains remarkably efficient for all values of k increases, the performance of the HSV -based approaches begins to degrade since the sample size for the precalculated top-k order-ings grows exponentially. The performance degradation at large k is equivalent to Case (3) as described in Section 4. In essence, most of the h sp, ep i ranges turn out to be much smaller than any of the samples, so the complete h sp, ep i range must be computed at runtime, reducing the performance to FM -GREEDY when an ap-propriate sample is not available. Note that the performance of HSV for TREC 7 &amp; 8 is worse than for WT10G for two reasons. First, k Figure 7: Space usage for each component in the three indexing a pproaches presented in this paper for the TREC 7 &amp; 8 collec-tion (left) and the TREC WT10G collection (right). The dashed line in both graphs represents the total space usage of the orig-inal uncompressed text for the collection. is four times larger in TREC 7 &amp; 8 resulting in fewer sample points. Secondly, if only a partial match is found, the self-index approach must retrieve 8 times more intermediate documents for scoring than in the inverted indexing approaches.
 Note that none of our self-indexing approaches currently employ M approach could also benefit from similar enhancements. We intend to explore the benefits and drawbacks of various early termination and impact scoring approaches for self-indexes in future work.
We now address the issue of space usage for the different algo-rithmic approaches. Inverted indexes are designed to take advan-tage of a myriad of different compression techniques. As such, our baselines also support several state-of-the-art byte and word aligned compression algorithms [3, 9, 28, 39, 43]. So, when we re-port the space usage for an inverted index, the numbers are reported using compressed inverted indexes and compressed document col-lections.

Figure 7 presents a break down of space usage for each compo-nent of the inverted indexing and self-indexing approaches. From a functionality perspective, there are several different componen-tization schemes to consider. First, consider the comparison of an inverted index method (including the term map, the postings list with p  X  offsets, the document map, and the compressed docu-ment cache) with an FM-Index (including WT d , the document map, and any other precomputed values  X  for instance the HSV enhance-ment). We consider these two in-memory indexes as functionally equivalent, as both can support bag-of-words or phrase queries, and can recreate snippets or even the original uncompressed document. The character based variant FM is significantly larger, but able to support a range of special character and arbitrary substring queries that term-based indexes do not support. Therefore, the term-based self-indexing variant FM -TERM is much closer to the inverted in-dexing variant in space usage and functionality.

The second alternative are indexes that support only bag-of-words queries. Now, an inverted index method requires only the term map, the postings list without p  X  offsets, and the document map. The character-based self-indexes are essentially the same, but the FM-Index component is replaced with a term map component. Note that the FM component of FM -TERM is only required for phrase queries, and can also be dropped if only bag-of-words queries are required. When considering all of the current self-indexing op-tions presented in this paper, using an FM-Index component instead of a term map appears to offer the most flexible configuration for character-based self-indexes, while the term-based variant is com-petitive in both time, space, and functionality with an inverted in-dex.
We have presented an algorithmic framework for in-memory bag-of-words query processing that is efficient in practice. We have compared and contrasted our framework with industry and aca-demic standard inverted indexing algorithms. Our approach shows great promise for advancing the state-of-the-art in exciting new di-rections.

However, several challenges must be overcome before these al-gorithms can reach widespread acceptance. For instance, recent work has dramatically reduced the space required for self-indexing algorithms, there are still opportunities to further reduce space us-age in self-indexes. Another shortcoming of bag-of-words query-ing with self-indexing algorithms is providing top-k guarantees. While good solutions exist for providing top-k guarantees on sin-gleton pattern queries, optimally merging multiple queries remains problematic.

However, self-indexing algorithms can also efficiently provide functionality that is notoriously inefficient, and sometimes even im-possible, using inverted indexes. In addition to basic bag-of-words queries, our approach has the capability to perform phrase queries of any length, as well as the ability to support complex statisti-cal calculations at query time, with no additional indexing costs. In fact, phrase queries were shown to be significantly faster us-ing FM -GREEDY than when using inverted indexing approaches in prior work [10]. Self-indexes also inherently preserve term prox-imity. So, not only can each term be found quickly, but the n -terms surrounding the keyword can quickly be extracted, tabulated, and used for on-the-fly statistical calculations. Applications of this functionality include more efficient relevance feedback algorithms, construction of higher order language models in ranking metrics, or term dependency extraction and query expansion. In summary, all of the disadvantages outlined in this paper for self-indexing pa-per warrant further research, as the potential benefits of this new approach are compelling indeed.

In future work, we will explore new algorithmic approaches to reduce space usage, and to further improve efficiency for larger val-ues of k . We also intend to investigate the combination of efficient phrase querying and proximity calculations to produce and evalu-ate novel ranking metrics. Finally, we will design and evaluate new approaches to support distributed in-memory query processing in order to scale our system to terabyte size collections.
This work was supported in part by the Australian Research Coun-cil. We thank Gonzalo Navarro and Daniel Valenzuela for insight-ful discussions on the HSV method. We also thank Alistair Moffat for valuable feedback on a draft of this paper.
 [1] V. N. Anh and A. Moffat. Pruned query evaluation using [2] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space [3] N. R. Brisaboa, A. Fari X a, G. Navarro, and M. F. Esteller. [4] A. Z. Broder, D. Carmel, H. Herscovici, A. Soffer, and [5] E. W. Brown. Fast evaluation of structured queries for [6] C. Buckley and A. F. Lewit. Optimization of inverted vector [7] M. Burrows and D. J. Wheeler. A block-sorting lossless data [8] S. B X ttcher, C. L. A. Clarke, and G. V. Cormack.
 [9] J. S. Culpepper and A. Moffat. Enhanced byte codes with [10] J. S. Culpepper, G. Navarro, S. J. Puglisi, and A. Turpin. [11] J. S. Culpepper, M. Yasukawa, and F. Scholer. Language
