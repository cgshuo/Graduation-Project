 We propose a language-model-based approach for address-ing the performance robustness problem  X  with respect to free-parameters X  values  X  of pseudo-feedback-based query-expansion methods. Given a query, we create a set of lan-guage models representing different forms of its expansion by varying the parameters X  values of some expansion method; then, we select a single model using criteria originally pro-posed for evaluating the performance of using the original query, or for deciding whether to employ expansion at all. Experimental results show that these criteria are highly ef-fective in selecting relevance language models that are not only significantly more effective than poor performing ones, but that also yield performance that is almost indistinguish-able from that of manually optimized relevance models. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation Keywords: query expansion, language models, query clar-ity, model selection, query drift
Automatic query expansion based on pseudo feedback [12] is an effective approach for addressing issues rising from the use of short queries. The idea is to construct a query model using terms from both the original query and documents that are the highest-ranked ones by some initial search.
However, since typically the documents from the initial search are not all relevant, and do not necessarily exhibit all query aspects [4], the constructed query model is often far from being  X  X ptimal X . Indeed, the resultant retrieval performance is often highly sensitive to the choice of the number of documents and the number of terms used for defining the model. Furthermore, for some queries, retrieval based on the original query results in performance superior to that resulting from the use of an expansion model.
Thus, recent research on improving the robustness of ex-pansion methods has focused on either predicting whether a given expansion will be more effective for retrieval than the original query [2, 7], or on improving the performance robustness of specific expansion methods [10, 13].
Inspired by work on combining multiple, mainly boolean-based, query representations [3], we propose a new approach form interpolation of these 100 language models with the Jelinek-Mercer approach wherein the smoothing parameter is set to 0 . 2. We then choose the M j for which D (  X  M q ||  X  M j ) is minimal ,where M q is an unsmoothed unigram language model constructed from q ,and  X  M q is constructed as above using the top-100 retrieved documents from a search with M q as the query model.

Once a query model M is selected from M by either of the two criteria just described, we can rank documents in the corpus using the KL-retrieval approach [8] with M .
To derive the set of query models M , we use either a rel-evance model [9], or an interpolated relevance model [1]  X  the latter interpolates the former with the original query model. We set the number of top-retrieved documents and the number of terms, for constructing the (interpolated) rel-evance models, to values in { 5 , 10 , 20 , 30 , 40 , 50 , 75 , 100 } and { 50 , 100 , 500 , 1000 } respectively 1 .Thus,foreachquerywe get two sets of 32 (interpolated) relevance models. For each set we identify the best and worst performing models in terms of MAP as measured over all tested queries 2 .
We ran our experiments on TREC corpora (details in Fig-ure 1) using the Lemur toolkit (www.lemurproject.org). We used topics X  titles as queries, applied the Porter stemmer and removed INQUERY stopwords. Statistical significance was determined using the two-sided Wilcoxon test at the 95% confidence level.

Figure 1 depicts the MAP performance results. 3 Our first observation is that, indeed, a poor selection of a relevance model results in performance that is much inferior to that of a manually optimized one. (Note that best outperforms worst to a statistically significant degree in all cases.)
We also see in Figure 1 that both selection criteria result in performance that is always significantly better than that of worst . Furthermore, the performance of ndrif t is in most cases statistically indistinguishable from that of best .
While the resultant performance of using ndrif t is always better than that resulting from using clr ,wenotethatthe former requires running all the different query models while the latter only requires running the selected model.
For constructing the relevance models, the value of the smoothing parameter of the top-retrieved documents X  lan-guage models is set to 0 . 2.
The interpolation parameter of all interpolated relevance models, which controls the reliance on the original query model, is fixed to the (manually optimized) value chosen for the best model.
Similar performance patterns a re obtained with respect to precision@10. The actual performance numbers are omitted due to space considerations.
