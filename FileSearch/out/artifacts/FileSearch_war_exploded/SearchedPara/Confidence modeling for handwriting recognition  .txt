 ORIGINAL PAPER John F. Pitrelli  X  Jayashree Subrahmonia  X  Michael P. Perrone Abstract Confidence scoring can assist in determining how to use imperfect handwriting-recognition output. We explore a confidence-scoring framework for post-processing recog-nition for two purposes: deciding when to reject the recog-nizer X  X  output, and detecting when to change recognition pa-rameters e.g., to relax a word-set constraint. Varied confi-dence scores, including likelihood ratios and posterior prob-abilities, are applied to an Hidden-Markov-Model (HMM) based on-line recognizer. Receiver-operating characteris-tic curves reveal that we successfully reject 90% of word recognition errors while rejecting only 33% of correctly-recognized words. For isolated digit recognition, we achieve 90% correct rejection while limiting false rejection to 13%. Keywords Confidence scoring  X  Handwriting recognition  X  Rejection  X  Recognition verification  X  Multi-pass recognition  X  Online recognition 1 Introduction High-accuracy handwriting recognition has been realized for well-constrained scenarios, such as small-vocabulary, writer-dependent or constrained-style, isolated-character or -word recognition. However, on less constrained tasks, high accuracy remains elusive. Accuracy varies widely and some errors are inevitable due to poor handwriting, inherent con-fusability among characters, constraining recognition to a limited word set, and the impossibility of modeling com-pletely the unlimited variety of styles of handwriting. There-fore, applying some form of confidence assessment to the recognition process is and will remain useful in order to gauge when or how to use the recognition results. More specifically, measures of recognition confidence can be used for: 1. Recognition verification / rejection 1 : deciding whether to 2. Prioritizing checking of recognition outputs: focusing 3. Multiple recognizers: determing dynamically the param-4. Multi-pass recognition: detecting when to adjust the pa-5. Adaptation: deciding when and to what extent to incor-scoring to help a user of recognition make best use of the recognition results; in this study we explore the first two. The remaining two uses involve extracting information from the recognition process in order to improve subsequent at-tempts at recognition; we explore the first of those two uses here.
 various recognition applications which require lower error rates than can be currently achieved on their recognition tasks. As mentioned above, some recognition errors are in-evitable due to poor handwriting and unmodeled writing styles. Therefore, we want to build essentially an additional classifier to determine whether the output of the handwrit-ing recognizer is right or wrong. This classifier typically consists of computing a measure of recognition confidence and accepting the recognizer result if confidence exceeds a rejection threshold, rejecting otherwise. The goal is to find a threshold which achieves (correct) rejection of many of the recognizer X  X  erroneous results while only rejecting (falsely) few of the correct results. In this way, we reduce the effective error rate of the recognizer, that is, the error rate on accepted results, at the cost of sacrificing automated processing of the handwriting whose results are rejected. Such a verification strategy can be extremely useful for an accuracy-sensitive application that does not require 100% automation, acting somewhat analogously to Sarkar X  X   X  X riage X  [ 20 ], though that work addresses off-line recognition at the page level. application for which only 95% accurate recognition is available. Suppose the application requires 99% accuracy on inputs processed automatically, but can tolerate abandoning automation of as many as 25% of inputs. Suppose further that a verification classifier is able to reject (correctly) 85% of errors while only rejecting (falsely) 20% of correctly-recognized inputs in this application. Then the overall sys-tem, consisting of the recognizer and the verification classi-fier, abandons processing of 20% of the correctly-recognized 95% of inputs, plus 85% of the incorrectly-recognized 5%, which totals (0 . 2  X  0 . 95 ) + ( 0 . 85  X  0 . 05 ) = 23.25% of inputs, while providing correctly-automated processing of 0 . 95  X  ( 1  X  0 . 2 ) = 76% of inputs or 0 . 76 /( 1  X  0 . 2325 99.02% of all automatically-processed inputs, thereby meet-ing the specified automation and accuracy criteria. nition results, is closely related to verification. Rather than comparing the confidence score to a rejection threshold, one can instead sort the recognition outputs according to confi-dence score, for presentation to the checker in priority order. Alternatively, when it is important to preserve context and so the items should not be reordered, relative confidence can be indicated to the checker by means of color or typeface. this study is for a multi-pass approach to recognition. Specifically, confidence measures from a first recognition pass may identify instances in which the input handwriting requires some changes to the recognition parameters, which can then be used on a subsequent recognition pass. One example is the detection of words which are out of the recognizer X  X  word set, which can be followed by re-running the recognizer with a different word-set constraint. Other similar possibilities include considering alternate casings, such as all-capitals, or changing parameters modeling intra-and inter-word spacing.
 to contend with applications whose intrinsic vocabularies are too large to be applied directly as a word-set constraint during recognition. An initial pass is made without a word-set constraint, generating a lattice of character-hypotheses representing likely segmentations of the input handwriting. Committing to the most confidently recognized characters is used to reduce the overall vocabulary, generating a word subset manageable for constraining a second recognition pass.
 pose varying constraints on how confidence modeling can be approached; Sect. 3 outlines various measures of confi-dence discussed in the recognition literature. Section 4 de-scribes the recognizer used in the experiments in this study. Section 5 details verification/prioritization experiments, fol-lowedbySect. 6 discussing multi-pass experiments. Finally, Sect. 7 and 8 provide conclusions and directions for future work. 2 Confidence-scoring scenarios We now consider the implications of the wide range of uses for confidence scoring. First, it is important to distin-guish the level of linguistic unit on which recognition is performed from the level on which confidence scoring is performed. Here, we discuss three levels of unit on which recognition may be performed: characters, words, and se-quences of words which may be referred to as phrases or sentences. Confidence measurement, however, need not be performed on the same level of unit as recognition. For ex-ample, in the case word recognition, the multi-pass scenario introduced above requires character-level confidence scor-ing, that is, a separate measure of recognition confidence performed on each character of the recognized word. In con-trast, the above scenario for verification of word recognition requires word-level confidence scores, though these could optionally be based on a character-level confidence scores. While the specific experiments described below in Sects. 5 and 6 focus on word-and character-level recognition and confidence scoring, our approach must be architected to pre-pare for a more general longer term vision including phrase recognition. Thus, our methodology is designed around the set of six scenarios arrayed in Table 1 , though the present experiments are limited to Scenarios 1 through 3 and do not address phrase recognition.
 dence modeling. Specifically, Scenarios 2, 4, and 5 invoke confidence modeling at a  X  X ower X  level of linguistic unit than that at which they invoke recognition; thus, the confidence-scoring functions used may be context-dependent ,making use of recognition scores for the handwriting surrounding the unit being confidence-scored. Scenarios 3, 5, and 6 in-voke confidence modeling at a higher level than the individ-ual character, and so may employ a nested scoring method in which character confidences may be grouped in some way to contribute toward higher level unit confidences. Scenarios 1, 3 and 6 are of primary interest for verification, in which a confidence measure is needed for the same-level unit on which recognition is run, while 2 and 4 are of primary inter-est for uses like the multi-pass paradigm.
 fidence modeling intersect with six scenarios X  constraints on the set of applicable confidence-modeling and recogni-tion techniques requires much flexibility in our approach to confidence scoring. More specifically, we require an ap-proach which is flexible with regard to:  X  what level unit is being recognized,  X  what level unit is being confidence-scored,  X  what character and/or word set may be used to constrain  X  what type of scoring the recognizer uses, e.g. , probabili- X  what types of confidence measures may be applied, and  X  other recognition parameters, such as writer-independent An example of this flexibility is that recognition algorithms must have distinct ways of treating characters and words, but confidence scoring algorithms need not. For reasons such as these, we do not integrate confidence scoring with recognition, as done in some other studies [ 7 , 10 ]; rather, we choose a post-processing approach to confidence scor-ing [ 1 ], in which scoring functions are evaluated follow-ing a pass of recognition. Further reasons to architect con-fidence scoring to be a module as independent as possible of the recognizer include code modularity and reusability, e.g. , reusing an object-oriented post-processor across var-ious recognizers and applications. The drawback of such flexibility is the loss of use of the more-detailed data from the recognition process that could have been exploited by tightly integrating confidence scoring with recognition, or by tailoring confidence scoring to the specifics of a particular recognizer.
 pute character-, word-or phrase-recognition confidence, fol-lowing one pass or occurring between two passes executed by an existing recognizer essentially not modified for this purpose. This stands in contrast with other experiments in-volving augmenting the recognizer with e.g. , garbage mod-els [ 27 ] or anti-models [ 7 ]. Therefore, the post-processor must be general in its requirements for the recognizer, only specifying what is inherent to the task of confidence scoring. Accordingly, scoring is performed on a graph containing one or more recognition hypotheses , each consisting minimally of  X  a text label, such as a word or character,  X  indices identifying which span of the input handwriting  X  X  recognition score indicating how well the recognizer The recognition score may be computed from any func-tion which increases as the recognizer judges the match be-tween label and handwriting to improve. It is typically in the form of an estimated probability or likelihood (probability density), or the logarithm of either. Recognition scores are frequently derived from component character-shape-model scores and language-model scores. Note that the structure of the hypothesis is essentially the same regardless of the size of linguistic unit it represents, as the unit size is not in-trinsically relevant to the post-processor. The post-processor must be able to take as input a lattice of such hypotheses, and from the data in those hypotheses it computes a con-fidence score , typically for the hypothesis with the highest recognition score. The confidence score is often an estimate of the probability that the hypothesized label matches the handwriting. 3 Measures of recognition confidence The literature on handwriting recognition suggests a wide variety of confidence-scoring functions. This variety results not only from the ongoing quest for more accurate verifi-cation decisions, but also from varied constraints imposed on such functions by various recognition scenarios. In ad-dition, analogous studies in speech recognition also offer many potential confidence-scoring techniques which may be imported to handwriting recognition. It is interesting to see how methods which have performed well in speech recogni-tion may apply to handwriting recognition. Therefore, both sources are considered here.
 the scenario in which they can be applied, and we categorize scenarios according to three dimensions, described in terms of the configuration of the lattice of hypotheses passed to the post-processor by the recognizer: 1. Whether or not alternate hypotheses are provided by the 2. Whether or not the recognition scenario involves nested 3. The existence of context in cases in which confidence is These three dimensions of confidence-scoring scenarios are depicted in Fig. 1 and serve to outline the discussion of func-tions in the sections below. 3.1 Non-nested, context-independent scoring functions The most constraining scenario is one in which only one level of linguistic unit is distinguished at recognition time, and only one unit is recognized at a time, for example, iso-lated-character recognition. This reduces the hypothesis lat-tice to a list of single, parallel hypotheses. In the case of a 1-best recognizer output, the only input is a single recog-nition score, which may be scaled to become a confidence estimate. Such scale factors can be conditioned on the label, e.g. , accounting for patterns like  X  m tends to score higher than i  X  which may be observed in training data. Amount of handwriting may also be a scale factor, e.g. , dividing scores by some measure of amount of handwriting, such as total length of strokes, to compensate for statistical dependence among scores for portions of the handwriting.
 hood ratios or differences/ratios of other types of scores can be used to measure confidence relative to the nearest com-peting hypothesis [ 3 , 22 ]. A recognizer generating an N -best list of hypotheses enables more complex confidence-score functions. If the N -best hypotheses are scored with proba-bilities or likelihoods, estimated posterior probabilities can be used as confidence estimates, as long as N is large enough for the list to be reasonably representative of a full set of hy-potheses [ 23 ]. Optionally, confidences may be scaled down, e.g. , by raising scores to a power less than 1, in order to compensate for over estimates of confidence owing to inac-curacies of the recognizer X  X  models, discrepancy between N and the true full set of hypotheses, etc. [ 11 , 23 ] Entropy can be computed from these probabilities as an estimate of un-certainty, so negative entropy can be used as a confidence measure [ 3 ]. 3.2 Adding context-dependence In scenarios in which confidence scoring is performed on a unit smaller than the unit being recognized, scoring functions may be expanded to include information from hypotheses for handwriting surrounding the writing whose hypothesis is being scored. One such type of function is a word confidence score which takes into account recognition scores for the hypotheses for phrases which include the word. An example is computing word posteriors derived from a word-hypothesis lattice developed during phrase recognition, sometimes involving adjustments to compen-sate for the fact that multiple hypotheses of a word spanning approximately the same portion of the recognizer X  X  input should be contribute to each others X  confidences [ 2 , 26 ]. One analogous method for speech recognition is presented by Maison and Gopinath [ 11 ], who compute posterior probabil-ities for phrase hypotheses. They then compute probabilities for each word hypothesis by summing the probabilities of all phrase hypotheses containing that word hypothesis. Finally, a confidence score is computed for the word hypothesis by averaging, across the portions of the signal spanned by that hypothesis, the sum of the probabilities of all hypotheses of that word which span each portion. They also use scaling, raising the recognizer X  X  original likelihood scores to a power less than 1 before computing the posteriors. 3.3 Adding nesting Confidence scores for any level of unit may be combined to create a confidence score for a larger unit using any of a va-riety of function, such as minimum, maximum, average, etc. For example, one possible word-confidence scoring func-tion is the average of the word X  X  character-confidence scores. Koo et al. [ 7 ] found multiple-level confidence-scoring func-tions helpful, though their study applies them during speech recognition as opposed to the present problem of post-processing handwriting recognition. 3.4 Combining measures It is reasonable to consider the possibility that such diverse confidence measures are somewhat complementary in their abilities to distinguish correct from incorrect recognition re-sults. Thus, one might seek a scoring measure or classi-fier which incorporates multiple raw measures on the same hypothesis [ 5 , 13 ]. An example is an artificial neural net-work trained to combine the information conveyed by multi-ple confidence measures. Similar techniques have been used successfully in speech recognition [ 25 ] and off-line hand-writing recognition [ 3 ].
 4 Recognition system Experiments are performed using a large-vocabulary on-line handwriting recognizer; previous papers [ 14 , 24 ] describe algorithms in detail. It should be noted that the goal of recog-nition here is merely to obtain a machine labeling of what was written, with no further requirements such as deriving some understanding of the content or taking action based on the meaning of what was written. Our system is an uncon-strained-style, hidden-Markov-model-based (HMM) recog-nizer, which seeks to label any handwriting with the high-est-probability text: using P ( t | h ) = P ( h | t ) P ( t )/ P ( h ) where h denotes the observed handwriting, t denotes a text label consisting of words or characters, T denotes the set of all possible texts, and  X  t denotes the highest-probability text. P ( h | t ) represents a statistical character-shape model of how people write a given text. P ( t ) is a statistical language model of what text people are likely to write. P ( h ) is typically dis-regarded, as h is fixed. Therefore, the likelihood P ( h | scores how well a particular text t matches given handwrit-ing h . Our recognizer actually uses the log of this likelihood for its scores.
 a stream of ( x , y ) points indexed in time. Handwriting is then size-normalized. Points are re-sampled to be spatially equi-distant. A set of local features based on distances and angles is computed for each point. Windowed groups of temporally-adjacent points are assembled around window centers, which are typically pen-down and pen-up points as well as local extrema in the x and y directions. Feature vectors of the points within a window are spliced together to form window feature vectors, which are projected onto a lower-dimensional space. The resulting vectors are called frames. Sequences of frames are used to represent spans of handwriting in recognition hypotheses.
 of four allograph HMMs. Each allograph model consists of a sequence of states; sequences vary in length, typically the mode of the distribution of the number of frames seen in training samples transcribed with that allograph. Mixture-Gaussian models, trained using an EM algorithm, repre-sent the distribution of frame vectors for each state, P ( Observation statistics are expressed as log-likelihoods. A beam search, governed by word set or character set, begins with a forward pass using single-state allograph HMMs and a character-level language model P ( t ) ; then, hypothesized words are optionally re-scored with a log likelihood from a word-unigram language model. Finally, the hypotheses are re-scored using multiple-state allograph HMMs. Note that a  X  X ecognition pass X  mentioned elsewhere in this paper con-sists of all these steps, with the  X  X ecognition score X  being the sum of the language model score and the average of the scores resulting from the single-state-and multiple-state-HMM passes.
 the recognizer on American English in writer-independent mode. Each experiment uses character models trained on a database of approximately 165,000 words plus 330,000 dis-crete characters provided by 450 writers whose writing was not used in test data sets for any of the experiments described below. 5 Recognition-verification/checking-prioritization experiments Deciding when to accept or reject the output of a recognizer is a primary use of recognition confidence scoring. A scoring method capable of assigning generally higher confidences to correct recognition results than to incorrect ones enables the use of confidence thresholding to achieve perhaps arbitrarily high system accuracy, regardless of the base accuracy of the recognizer, at the cost of abandoning recognition of some fraction of the inputs. This can be extremely useful for ap-plications in which accuracy is highly important, as long as automating the processing of fewer than all the inputs is tol-erable.
 for the entire span of handwriting submitted to the recog-nizer, though it is of course possible e.g. , to run recognition once on a word but then make separate verification decisions for its characters. So here we focus on character verifica-tion for character recognition and word verification for word recognition. 5.1 Character verification experiments The most basic recognition scenario, Scenario 1 in Table 1 , is isolated characters. Many form-filling applications call for single-character entries. Character-level recognition and confidence scoring restrict us to non-nested, context-independent scoring functions. We explore eight such confi-dence functions for this task, two using just the recognizer X  X  top hypothesis, two using the recognizer X  X  2-best, and four needing a larger N -best as input:  X  Recognition score : simply the recognition score for the  X  Recognition score per frame : the top hypothesis X  X  recog- X  Likelihood ratio : the recognition score for the top hy- X  Likelihood ratio per frame : dividing the above log-like- X  Estimated posterior probability : Our recognizer is de- X  Negative Entropy : More entropy in the recognizer X  X  N - X  Selectivity : We define the recognizer X  X  N -best list X  X  se- X  Measures with exponentiated probabilities : In practice, it  X  MLP : We input the above seven non-exponentiated con-5.1.1 Database for character verification experiments We use a test database consisting of 45,800 character occur-rences provided by 100 writers recruited to our laboratory in New York. Each writer was asked to print three or five copies of a set of 93 characters  X  26 lower-case letters, 26 upper-case letters, 10 digits and 31 other symbols, approxi-mately those on a typical American-English-language com-puter keyboard, mainly punctuation. Writers were instructed to observe baselines on the stimulus sheets, with descender characters such as j and p permitted to drop beneath them. Data were transcribed automatically using the script, and then the transcripts were manually checked using the cri-terion of whether the printing represented a readable version of the elicited character, regardless of writing style, result-ing in few character occurrences being eliminated. The data were jackknifed for the MLP, with thirds of it successively serving as training, cross-validation and test sets; the results for each third as the test set were concatenated together to enable comparison with the entire set being used as a test set for the other 10 measures. Importantly, in this way we ex-clude the recognizer X  X  training data from the data sets used to train the MLP, so that the MLP is trained on data rep-resentative of its use, that is, data previously unseen by the system [ 5 ]. 5.1.2 Evaluation methodology We treat correct and incorrect recognition outputs as the two relevant classes of input to the verification post-processor, and evaluate it according to its accuracy in classifying them accordingly for acceptance or rejection. Post-processor re-sults therefore categorize into four outcomes:  X  Correct acceptance (CA)  X  recognizer correct, post-pro- X  Correct rejection (CR)  X  recognizer incorrect, post-pro- X  False acceptance (FA)  X  recognizer incorrect, post-pro- X  False rejection (FR)  X  recognizer correct, post-processor We measure the accuracy of the post-processor in terms of its rate of erroneous behavior for each class of its input as follows:  X  Acceptance rate on characters which had been mis- X  Rejection rate on characters which had been recognized trade off; for example, raising the rejection threshold re-duces false acceptances but at the cost of increased false re-jection. Therefore, for each measure, we sweep a rejection threshold across its entire range of values, plotting the false-acceptance rate on wrongly recognized characters against the false-rejection rate on correctly recognized characters as a receiver-operating-characteristic (ROC) curve. A curve reaching closer to the origin indicates a superior confidence measure, one for which a threshold can provide low rates of both types of error simultaneously.
 nition verification, the ROC curves can easily be interpreted from the perspective of prioritizing manual checking of the recognizer output. Any chosen confidence threshold and op-erating point on the curves then represents a degree of thor-oughness of checking, with the trade-off between false ac-ceptance and false rejection representing a trade-off between uncorrected recognition errors and needlessly checked cor-rect recognition outputs. 5.1.3 Experiments We describe two conditions: digits only, and the full set of 93 characters. For the 10 digits, we have 4,000 character oc-currences, which are also jackknifed as described above for the MLP. Recognition was constrained by a  X  X ord X  set con-sisting of the 10 or 93 characters in isolation, and used a un-igram model specifying equal probabilities for each charac-ter. With our recognizer X  X  normal pruning, we obtain on av-erage a 5-best output on digits and 20-best for 93-character recognition. For the digits-only case, we computed the 11 confidence measures for each character occurrence, and gen-erated the ROC curves shown in Fig. 2 , with several mea-sures omitted for clarity because they performed similarly to other measures.
 the best, requiring only a 13% false-rejection rate to achieve a false-acceptance rate under 10%. The two confidence mea-sures using only the top score perform the worst, as the highest line represents recognition score and the second-highest, recognition score per frame. Unlike those two, the other eight confidence measures make use of alternate hy-potheses, and with this additional information perform better and approximately equally, resulting in eight curves which are nearly indistinguishable from each other, two of which are shown. The fact that these measures yield equal accu-racy does not imply that they are redundant, however, as evidenced by the superior performance of the MLP which makes use of seven individual measures together.
 is considerably more difficult, because of confusable charac-ter groups such as 1 (one), l (lower-case L), I , / (slash), and | (vertical bar), which are sometimes confused even with ! , ( ,and ) . Even when one of these characters is recognized correctly, it is difficult to achieve high confidence because competing hypotheses are likely to be assigned comparable scores by the recognizer.
 off between false acceptance and false rejection is less fa-vorable than with the digits set. To obtain a false-acceptance rate under 10% now requires increasing false rejection to 56%, achieved by the MLP. Again, the top score alone is by far the worst measure, followed by the other measure which does not incorporate any alternate-hypothesis data, recogni-tion score per frame. The remaining eight measures again perform almost indistinguishably, and this time only slightly worse than the MLP. 5.2 Word verification experiments We now turn to applying confidence modeling to verify the output of word recognition. For word-level experiments, we use a test database consisting of 1157 word occurrences ex-tracted from 75 sentences each written by a different writer. Text was scripted; subjects were told to write the sentence in their own style. Sentences were segmented into words, en-abling isolated-word recognition. Recognition is constrained by a 30,000-word lexicon and includes a matching word-unigram language model. Therefore, in addition to mis-recognitions of words in the model, the post-processor is also looking to reject unknown words. For isolated words, the recognizer typically provides a 40-best output from which to compute the confidence measures. As before, data were jackknifed by thirds for training and testing the MLP on the entire set. Results are shown in Fig. 4 .
 tion score and recognition score per frame, the only mea-sures not using even a 2-best recognizer result, again per-form the worst. The MLP again is best, requiring only a 33% false-rejection rate to achieve a false-acceptance rate below 10%. The other eight measures again perform approximately equivalently, but we observe again that despite similar per-formance, they are somewhat complementary to each other, as the MLP which incorporates seven of them outperforms each measure used singly. 6 Multi-pass recognition experiments using confidence scoring Having discussed using confidence scoring to decide when to accept the output of a recognizer, we now turn to another use of confidence scoring: to improve the output of the rec-ognizer. Specifically, our experiments consider ways to use confidence scoring from a first pass of recognition to adjust parameters for a second pass of recognition. A number of such uses may be considered:  X  Character-or word-level confidence scoring could be  X  Word-level confidence scoring could be used to hypoth- X  Character-level recognition and confidence scoring can nition situation in which the vocabulary natural to an application is too large for the recognizer under study to apply directly as a constraint during recognition. This ap-proach to very large vocabulary recognition bears some re-semblance to Liu and Nakagawa X  X  [ 9 ] approach to very-large-character-set recognition for east Asian languages. We intend this experiment to exemplify one of the many ways in which confidence scoring should be explored as an element in a multi-pass approach to recognition.
 straints significantly aid handwriting recognition accuracy, so it is undesirable simply to omit applying such constraints because of an unmanageably-large vocabulary. Instead, a first pass is constrained only at the character level, in this case with a character-N -gram language model. Character-level confidence scoring is used to commit to enough char-acters to pare down the application vocabulary to a word set below 50,000 words so that it is manageable for this par-ticular recognition platform to employ as a recognition con-straint. Then, a second pass of recognition is performed us-ing the resulting word-set constraint. 6.1 Confidence scoring function As here we are computing character confidence for word recognition, Scenario 2 in Table 1 , we are able to employ a context-dependent confidence measure. We apply the tech-nique used by Maison and Gopinath [ 11 ] as mentioned in Sect. 3.2 , except we apply it for character confidence fol-lowing recognition of handwritten words, as described with our preliminary results [ 18 ]. This score functions somewhat like a character-hypothesis exponentiated posterior probabil-ity, except that the actual computation of posterior probabil-ities is done at the word level, and the score for any given character hypothesis also takes into account other hypothe-ses of the same character whose frames overlap the given one. Isolated-word recognition is run, generating an N -best list of word hypotheses each of which is associated with a se-quence of character hypotheses. For each word hypothesis, its likelihood is raised to an exponent x , as with exponenti-ated posterior probabilities described previously. We exper-imented with several values of x , with 0.6 yielding the best results. A word posterior WP for word candidate w j is com-puted by dividing its exponentiated likelihood by the sum of the exponentiated likelihoods of all the word hypotheses W WP j = Then, for each hypothesis c im for each character i , a poste-rior CHP im is computed as the sum of the posteriors of the word hypotheses which include that character hypothesis: CHP im = frame f , CFC if , is computed as the sum of the posteriors for all hypotheses of that character spanning that frame: CFC if = Finally, we compute a confidence score for each charac-ter hypothesis c im : the average over the hypothesis X  X  NF frames of its character i  X  X  confidences CFC if for those frames: C More details are provided by Maison and Gopinath [ 11 ]; other papers [ 2 , 6 , 12 , 21 ] describe related work in speech recognition.
 acter hypotheses in Fig. 5 ; we will focus on the longer d hypothesis, and let the exponent be 0.5 for this example. Word likelihoods for dog , day , clog ,and clay are 10  X  2 2  X  10  X  3 ,10  X  3 ,and2  X  10  X  4 , respectively, obtained by mul-tiplying their character-hypothesis likelihoods and raising to the power 0.5. The word probability for dog is its exponen-tiated likelihood divided by the sum of all the words X  ex-ponentiated likelihoods, 10  X  2 /( 10  X  2 + 2  X  10  X  3 + 10 2  X  10  X  4 ) = 0 . 7576. This is also the character-hypothesis posterior for the d in dog ,as dog is the only word hy-pothesis incorporating this character hypothesis. Next, we compute the character frame confidence score for this char-acter hypothesis X  X  frames, which is the sum of the posteri-ors of all character hypotheses which span that frame and carry that character. For the sixth frame, this is the only d hypothesis, so 0.7576 is the d frame confidence. For the five preceding frames, however, we add the other d charac-ter hypothesis X  X  posterior, which is the word probability for day , the only word incorporating this character hypothesis, 2  X  10  X  3 /( 10  X  2 + 2  X  10  X  3 + 10  X  3 + 2  X  10  X  4 ) = yields a d frame confidence of 0.909 for these frames. Fi-nally, we compute character-hypothesis confidence, which is the average of character frame confidence over all its frames. Confidence for the longer d hypothesis then is the average of 0.909 for the first five frames and 0.7576 for the sixth frame, or 0.884. 6.2 Database Testing is done using the isolated-word portion of the Uni-pen database [ 4 ], Train-R01/V07. We restrict to fully-al-phabetic English and proper-name inputs X 26 lower-and 26 upper-case letters. We restrict attention to words which are four or more letters long in these initial experiments, as appropriate given that this technique is intended primarily for open-vocabulary application. The data set was further reduced randomly by a factor of three down to 8009 word occurrences. 0101116 3 6 6.3 Experimental procedure A first recognition pass is run in isolated-word mode using only character-level language modeling including a charac-ter 4-gram; no word-level constraints are applied. Search beam parameters are set to allow 150 hypotheses per frame in the character-level search lattice and 70-best word out-puts, larger than the recognizer X  X  defaults of 70 and 20, re-spectively. This pass typically yields a large lattice of char-acter hypotheses. Confidences are computed for all the char-acter hypotheses comprising the word being recognized. The most confidently recognized character is  X  X ommitted to X ; a word template is formed consisting of the character, pre-ceded by a character wild-card if the committed character is not the first character in the recognized word, and followed by a character wild-card if the committed character is not the last character in the word. A 273,000-word vocabulary is matched against this template, and if more than 50,000 words match, a new template is formed from the two char-acters which were recognized most confidently. A character wild-card is added between those two characters in the tem-plate if they are not consecutive in the recognized word, in addition to wild cards preceding and/or following the two characters as before according to whether or not they start or end the word. Then, the recognition vocabulary is fil-tered further using this new template. Committed characters are added iteratively using this procedure until fewer than 50,000 words from the vocabulary fit the template, gener-ating a word set to constrain recognition. In practice, the word-set size resulting from this procedure averages 16,234 words. Finally, a second recognition pass is run using the re-sulting word set, optionally using a word-unigram language model. As this pass is intended primarily to generate first-choice word results, a narrower search beam is used, 70 hy-potheses per frame in the character-level search lattice. using the narrower beam parameters, as appropriate for pro-ducing single-hypothesis recognition results. A single-pass  X  X ord-set X  baseline is constrained using the 30,000 most-frequently occurring words from the 273,000-word set, and optionally using a word-unigram to match the experimen-tal condition. The word-set size of 30,000 is well above the 16,234 average resulting from the template-iterating proce-dure described above, thereby precluding bias toward the iterative procedure. We also use a  X  X haracter-set X  baseline consisting of a single pass constrained only by character set, using the same character 4-gram language model. 6.4 Results Our experimental system yields a net improvement in accu-racy over both baseline systems as summarized in Table 2 . Without word unigrams, it eliminates 10% of the word-recognition errors produced by the word-set no-unigram baseline system. We attribute this improvement to the fact that the experimental system has access to nine times as many words as the word-set baseline system X  X  30,000. In fact, 64% of the errors made by the baseline system are due to words being out of the 30,000-word set. By making the 273,000-word vocabulary available, we prevent 83% of those out-of-word-set errors, or 53% of all the baseline X  X  er-rors, from being made apriori by the word-set constraint, though some errors remain due to mis-recognition. The 10% reduction shows that the experimental system X  X  advantage of having the large vocabulary available outweighs its dis-advantage of having to commit to one or more characters of the output before being able to apply a word-set constraint, compared to the baseline system exploiting its word-set con-straint from the beginning.
 made by the character-set baseline system, confirming the benefits of using word-set constraints even when they are not applied until after committing to the recognition of at least one character.
 word not being in the word set, which breaks down to 12% missing from the entire vocabulary and 40% being in the vocabulary but excluded from the recognizer X  X  word set by an erroneous committed-character pattern.
 one-character template was sufficient to filter the 273,000-word vocabulary down to fewer than 50,000 words. The other 43% of the time, two characters were sufficient; no three-character templates were needed.
 a word-unigram language model during the second pass of recognition. Accordingly, we generated new baseline con-ditions, using the same unigram. Further improvement was realized; the experimental condition eliminates 23% of the errors made by the word-set/word-unigram baseline, and 40% of errors made by the character-set/word-unigram baseline. 7 Conclusions Various types of recognizer-output confidence scoring may be used in a wide variety of ways to help overcome accu-racy limitations hindering the use of handwriting recogni-tion. Confidence scoring of the entire output of a recognizer enables verification, the decision about whether or not to ac-cept the output of the recognizer. Confidence scoring of the entire output or a subset of it can be used to adjust the pa-rameters of recognition, to tailor a second recognition pass better to fit the input handwriting.
 processor can be configured with appropriate scoring func-tions for verification at the word or character level to achieve a substantially lower false-acceptance rate than its correct-acceptance rate. In this way, a system composed of the recognizer and the post-processor achieves higher accuracy than the recognizer alone, at a cost of sacrificing recogni-tion on some fraction of the input handwriting. The same post-processor can be configured to increase the accuracy of the recognizer itself. In this case, a pass of word recog-nition which is not constrained with a word set is post-processed at the character level to help generate an appro-priate word set to constrain a second recognition pass. As this scenario involves confidence scoring at a smaller unit level, the character, than was recognized, the word, context-dependent scoring functions could be used, incorporating posterior word probabilities rather than just character-level measures in computing character-hypothesis confidence. 8 Future research  X  X ery-large-vocabulary X  applications are generally actually open-vocabulary tasks. Ideally, recognition should not be constrained by any finite word set. To this end, future work should include applying confidence measures to the re-sults of the word-set recognition pass. When confidence is low, recognition could revert back to a character-set mode, to recognize words outside the word set. Other multi-pass paradigms, as outlined in Sect. 6 , should be explored as well. In addition, varied other confidence scores should be researched for verification and multi-pass uses. Finally, ver-ification experimentation should be expanded to the phrase level.
 References
