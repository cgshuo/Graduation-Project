 Clustering is the art of partitioning a data set into groups such that objects from the same group are as similar as possible, and objects from different groups are well differentiated. To support clustering, a measure of similarity (or distance) between data objects is needed. Popular di stance measures for clustering include the class of general L p norms (which includes the Euclidean distance L 2 ), and the cosine similarity measure. The k objects most similar to a data item v are often referred to as the k -nearest-neighbor ( k -NN) set of v .

An interesting and appealing class of  X  X econdary X  similarity measures, the so-called shared-neighbor (SN) measures, can be derived from any other ( X  X ri-mary X ) similarity measure. SN measures typically are expressed as a function of the intersection size of the k -NN sets of the two objects whose similarity is to be computed, where the neighborhoods are computed using the primary similarity measure. The use of SN-based similarity in clustering can be traced back to the merge criterion of the agglomerative algorithm due to Jarvis and Patrick [1]. Other agglomerative clustering methods with SN-based merge crite-ria include the hierarchical algorithm ROCK [2] and the density-based algorithm SNN (Shared Nearest Neighbor) [3]. SNN is essentially an improved version of the well-known DBSCAN [4] clustering algorithm; like DBSCAN, SNN is able to produce clusters of different sizes, shapes and densities. However, the perfor-mance of SNN greatly depends on the tuning of several non-intuitive parameters by the user. In practice, it is difficult (if not impossible) to determine appropriate values for these parameters on real datasets.

A common requirement of most SN-based clustering methods is that a fixed neighborhood size  X  either in terms of the number of neighbors k as in SNN and in Jarvis &amp; Patrick X  X  method, or in terms of the radius r of the neighborhood ball as in ROCK  X  needs to be chosen in advance, and then applied equally to all items of the data set. However , fixed choices of neighborhood sizes k or radius r are known to lead to bias in the clustering process [5], the former with respect to the sizes of the clusters discovered, and the latter with respect to the density of the regions from which the clusters are produced.

Recently, an SN-based clustering mode l was proposed that allows the sizes of the neighborhoods to vary. The Relevant Set Correlation (RSC) model [5] defines the relevance of a data item v to a cluster C in terms of a form of  X  X et correlation X  between the memberships of | C | and the | C | -nearest-neighbor set of v . RSC quality measures can be used to evaluate the relative importance of cluster candidates of various sizes, avoiding the problems of bias found with other shared-neighbor methods that use fixed neighborhood sizes or radii. The same paper introduced a clustering algorithm based on RSC, called GreedyRSC, that generates cluster candidates in the vicinity of every object of the dataset, evaluates the quality of the candidates according to the model, and greedily selects them in decreasing order of their qu ality. GreedyRSC is a  X  X oft X  clustering algorithm, in that the clusters produced are allowed to overlap. It does not require that the user choose the neighborhood size or specify a target number of clusters; instead, the user simply specifies the minimum allowable cluster size, and the maximum allowable correlation between any two clusters. Unlike many other clustering algorithms, GreedyRSC uses only local criteria for the formation of cluster candidates  X  the c lustering process is not guided by a global optimization criterion.
 In this paper, we present a new RSC-based partitional clustering algorithm, GlobalRSC , that allows the user to specify the number of clusters to be generated, K . Unlike GreedyRSC, GlobalRSC emulates the well-known K -means clustering algorithm in that it seeks to optimize a global objective function. Given an initial clustering configuration, both K -means and GlobalRSC attempt to optimize their objective function through an iterative hill-climbing improvement process. GlobalRSC, however, replaces the Euclidean distance of K -means by a shared-neighbor similarity measure, and can therefore be applied (in principle) to any form of data, and using any appropriate similarity measure.

This paper is organized as follows. In Section 2 we review those elements of the RSC model upon which GlobalRSC is based, and introduce the GlobalRSC clustering criterion. In Section 3, we give the details of the GlobalRSC cluster-ing algorithm. Experimental results are presented in Section 4, and concluding remarks appear in Section 5. In this section, we present a brief intro duction to the Relevant Set Correlation (RSC) model for clustering, and the set correlation similarity measure upon which it is based.
 Let S be a dataset of | S | = n data items { s 1 ,s 2 ,...,s n } . Any subset A of S can then be represented as a n -dimensional zero-one c haracteristic vector, wherethevalueofthe i -th coordinate is 1 if and only if s i  X  A . The simplest SN-based similarity measure between the two sets A and B is the  X  X verlap X  or intersection size | A  X  B | , which can be expressed as the inner product between the two characteristic vectors. Another popular measure, the cosine similarity for A and B , which in turn equals the cosine of the angle between them. Values of the cosine measure lie in the range [0 , 1], with 1 attained whenever A is identical to B , and 0 whenever A and B are disjoint.

In [5], the set correlation measure was proposed as the value of the Pearson correlation between the coordinate pairs of characteristic vectors of A and B . After derivation and simplification, this expression becomes: Values of the set correlation lie in the range [  X  1 , 1]. A value of 1 indicates that A and B are identical, and a value of  X  1 indicates that A and B are complements of each other in S .

Despite their simplicity and their popularity in practice, the overlap and cosine measures both have the disadvantage of bias relative to the sizes of the two sets A and B . To see this, let A be fixed with size | A | = a ,andlet B be selected uniformly at random from the items of S with the constraint that | B | equals some fixed value b&gt; 0. Let a = | A | . Under these assumptions, the overlap is known to be hypergeometrically dist ributed with expected value E [ | A  X  B | ]= ab n ,andthe expected value of the cosine measure is therefore E [cos( A, B )] = E [ | A  X  B | ]  X  When used to rank the similarity of sets with respect to A , both measures are biased towards sets B of larger sizes. On the other hand, the expected value of the set correlation under the same assumptions can be shown to be E [ R ( A, B )] = 0, indicating no bias with respect to the sizes of A and B . Therefore, of these three SN-based similarity measures, the set correlation measure is best suited for those applications in which the neighborhood size is variable.

Under the RSC model, the quality of a given cluster candidate set A is assessed in terms of the set correlation between the candidate and neighborhood sets (the  X  X elevant sets X ) based at its members. Let Q v k denote the set of k -nearest neigh-bors of v with respect to S . The RSC model uses the set correlation R ( Q v | A | ,A ) between Q v | A | and A as a measure of relevance of item v to the cluster can-didate A . Note that in this formulation, the neighborhood size is taken to be the cardinality of A . This definition eliminates the need for specifying a fixed neighborhood size, and avoids the bias associated with such choices.
For more details concerni ng the quality measures of the RSC model, see [5]. 3.1 GlobalRSC and K -Means The GlobalRSC clustering criterion has the same general form as that of K -means. In the standard K -means formulation, a partition A = { A 1 ,A 2 ,...,A K } of the data set is sought which maximizes the following objective function: where c ( A ) is a function which returns the center of mass of a cluster A (com-puted as c ( A )= 1 | A | v  X  A v ), and the distance measure D is generally taken to be the square of the Euclidean distance. The proposed formulation of Glob-alRSC replaces the distance measure D ( v, c ( A i )) by the average set correlation between cluster A i and the neighborhood Q v | A Both D and R serve as measures of the relevance of an item to its assigned cluster. However, unlike R , D can only be computed when the data can be represented as real-valued vectors. As di scussed earlier, the use of set correlation in the formulation of GreedyRSC is preferred over that of the overlap or cosine measure, due to the bias of the latter m easures with respect to set sizes. 3.2 A Hill-Climbing Heuristic The problem of optimizing the GlobalRSC objective function (3) greatly resem-bles that of optimizing the K -means objective function (2). Despite its simple appearance, the K -means clustering problem with squared Euclidean distance is known to be NP-hard even for K = 2 [6]. Although the hardness of the Glob-alRSC optimization problem is still an open question, a heuristic approach seems to be indicated.

In this section, we propose an iterative hill-climbing solution, which we simply refer to as GlobalRSC . The core idea is as follows: at each round the algorithm iterates through the items of S looking for items whose reassignment to a differ-ent cluster leads to an improvement in the value of the objective function R .As is the case with K -means, two reassignment schemes can be employed for Glob-alRSC: incremental update ,inwhichreassignmentisperformedassoonasanim-provement is detected, and batch update , in which all the membership changes are applied only at the end of each round, when the algorithm has completed a full it-eration through all data items. The advantage of the batch update scheme is that the recomputation of R can be performed very effici ently through the use of in-verted neighborhood sets (as defined in Fig. 1). However, it is possible that delaying the reassignment of items until the end o f each round could result in a decrease in the value of the objective function, even if each reassignment would have led to an increase if it were applied individually. In practice we often observe that first few rounds of the batch update scheme quickly improves the objective value. It would therefore be beneficial to begin with several rounds of batch updating, followed by an incremental update phase to further refine the clustering.

It should be noted that in an incremental reassignment of v from cluster A i to cluster A j , the contributions R ( A )= w  X  A R ( Q w | A | ,A )to R for an individual cluster A do not need to be recomputed except for A = A i and A = A j .Toverify whether the reassignment would increase the value of R , it suffices to perform the test R ( A j  X  X  v } )+ R ( A i \{ v } )  X  X  ( A j )  X  X  ( A i ) &gt; 0.
The recomputation of R after the reassignment of a single item v would be relatively expensive if all K  X  1 possible reassignmen ts were considered. We therefore limit the tentative reassignment of v to those candidate clusters found in the vicinity of v ; that is, those clusters containing at least one element in the neighborhood Q v | A If one of these tentative reassignments of v would result in an increase in the value of R , then the reassignment that results in the greatest such increase is applied. Otherwise, v is not reassigned. If the size of the new cluster A j is larger than that of the currently-stored neighborhood of v , then that neighborhood would need to be expanded. Accordingly, whenever it is necessary to recompute a neighborhood for item v  X  A j , we choose the size to be min { (1 + b ) | A j | ,m } for some fixed real parameter values b&gt; 0and m&gt; 0. In our implementation of GlobalRSC, b is set to a default value of 0 . 5, and m is set to 50. A pseudocode description of the basic GlobalRSC heuristic is shown in Fig. 1. The heuristic can easily be shown to con verge within a finite number of steps. 3.3 Complexity Analysis The algorithm requires storage for the neighbor lists of all n data items, each of which has size proportional to that of the cluster to which it has been assigned. The total space required is of order K i =1 | A i | 2 .Let  X  and  X  be respectively the mean and standard deviation of the cluster sizes; in terms of  X  and  X  ,thespace required is proportional to K (  X  2 +  X  2 ).

At the initialization step, the neighborhood list for each data item must be calculated. A straightforward implementation requires the computation of O ( n 2 ) distances. Once computed, these distan ces can also be used to generate an ini-tial clustering. Since the neighborhoods m ust be constructed in sorted order, the = O ((  X  2 +  X  2 ) K log n ) using linear-time methods for determining order statis-where d is the cost of computing a single distance.

During each round of the batch phase, building the inverted neighbor sets requires that the values of K (  X  2 +  X  2 ) integer variables be copied. Recalculating R for the tentative reassignment of item v from cluster A i to cluster A j requires time proportional to | A i | + | A j | +2 | I v | when using the inverted neighbor lists. Assuming that v needs to be tentatively reassigned to each of the other K  X  1 clusters, the cost of reassignment is O ( n ). Adding the cost over all choices of v , the total cost of reassignment per phase is at most O ( n 2 ). The neighbor list can be reconstructed in O ( n log n + nd ) time if required; the total cost of reconstruction will be no worse than that of initialization, which is O ( dn 2 +(  X  2 +  X  ) K log n ). The worse case complexity of each batch phase iteration through the data set is therefore O ( dn 2 +(  X  2 +  X  2 ) K log n ). However, in practice we expect a much lower time cost, since there are typically only a limited number of nearby clusters for each data item, and few if any neighborhoods require reconstruction.
In the incremental phase, tentatively moving an item v from a cluster A i of the objective function. If v is tentatively reassigned to each of the other K  X  1clusters,thecostis O ( K (  X  2 +  X  2 )); over all possible choices of v (one full round), the total cost of reassignment becomes O ( nK (  X  2 +  X  2 )). Recon-struction of the neighborhood for each item, if required, can be performed in O ( dn 2 + n 2 log n ) total time. The worst case complexity of a full round is there-fore O ( dn 2 + n 2 log n + nK (  X  2 +  X  2 )). However again, the observed cost should be much lower in practice. Since the incremental phase is often considerably more expensive than the batch phase, to improve time efficiency for large data sets, we employ a  X  X educed X  variant of the incremental phase, in which a data item v is considered for reassig nment to another cluster if and only if that cluster already contains the majority of the neighbors of v . This variant scheme focuses on items with neighborhoods of low consis tency, with the worst case complexity of each round being reduced to that of the batch phase.

In practice, the standard deviation of the cluster size,  X  , is typically of the same order of the mean cluster size  X  = n/K , leading to an overall space com-plexity of O ( n 2 /K ), and a time complexity (for th e reduced incremental phase variant) of O ( dn 2 + n 2 (log n ) /K ). Optionally, if a full distance matrix is to be stored in order to speed up neighborhood list computation, then the space com-plexity would attain its worst-case value of  X  ( n 2 ). 3.4 Scalability In this section we present several techniques that can boost the scalability of GlobalRSC for large, high dimensional data sets. The challenges faced by Glob-alRSC (and other SN-based clustering algorithms) as the dimensionality in-creases are: (i) the construction of nei ghborhoods becomes more expensive, due to an effect known as the  X  X urse of dimensionality X  [8]; (ii) the optimization of the objective function becomes more difficult, as local optimization approaches such as hill-climbing are more easily trapped at local maxima that may be far from the global optimum.

In order to accelerate the construction of neighborhoods, we propose the use of the Spatial Approximation Sample Hierarchy (SASH) developed in [8].
If the data set contains many large clusters, the calculation of set correlation scores with respect to these clusters may be prohibitively expensive. One of the simplest ways of avoiding the high costs associated with large cluster candidates is through the restriction of neighborh ood sizes. In our implementation of Glob-alRSC, we restricted the maximum size of the neighborhood to be 1000. Only the average relevance score for items in cluster of size smaller than this thresh-old is calculated exactly, while the membership of larger clusters are  X  X rozen X . Items are permitted to be reassigned fro m smaller clusters to frozen clusters and vice-versa, as long as such a movement increases the average relevance score of the smaller clusters. In this section we report the results of our experiments on various real data sets taken from several domains. GlobalRS C, implemented in C++ and tested on a Pentium IV 3.2GHz workstation equipped with 4Gb of main memory, was com-pared against a MATLAB implementation of  X  X ast X  K -means [9] (available from the author X  X  website), and the  X  X isecting X  K -means algorithm available as part of the CLUTO clustering toolkit [10]. CLUTO was run with its default distance measure, the cosine similarity, using repeated bisecting clustering followed by a final global optimization phase. For large data sets, we used GreedyRSC to initialize GlobalRSC, as well as the SASH to speed up neighborhood construc-tion. We report the mean and standard deviation values of the quality metrics, plus the average execution time and the number of iterations performed by each algorithm (if known). We did not include the SNN clustering algorithm, due to the difficulty in tuning its parameters, and since it often leaves a large number of items unassigned to any cluster. For i nterested readers, a comparison between SNN, K -means and GreedyRSC on several data sets can be found in [5].
For each of the data sets considered, the clustering results are assessed against a ground-truth classification according to 5 different quality measures: the well-known Adjusted Rand Index (ARI) from statistics [11]; the recently developed Adjusted Mutual Information (AMI) [12] from information theory; the Expected Precision (EPrec), Recall (E Rec) and Cosine (ECos) measures [5] from informa-tion retrieval. For all these measures, high er values represent better clusterings, with a maximum possible value of 1. A low expected precision score is an indi-cation of cluster fusion, occurring when too few clusters are produced, whereas a low expected recall indicates cluster fragmentation, occurring when too many clusters are generated. A high expected cosine score can be taken as evidence that the clustering avoids extremes of cluster fusion and cluster fragmentation. Due to lack of space, we do not give deta ils of these measures here. Interested readers are referred to the original publications for more information. 4.1 Biological Data We tested the algorithms on several ge ne expression microarray data sets:  X  X 1: This set consists of 384 genes whose expression level peak at different  X  X 2: This set consists of 237 genes corresponding to four categories in the  X  X 3: A subset of 205 genes from the yeast galactose data set [14]. The expres-As is popular practice in microarray data analysis, the data was row-normalized to have zero mean and unit variance. Under this normalization scheme, the Euclidean distance and cosine similarity are equivalent. From the experiment results shown in Table 1, averaged over 100 runs, CLUTO appears to perform best, closely followed by GlobalRSC and then K -means. 4.2 Image Data We tested the clustering algorithms on t he Amsterdam Library of Object Images (ALOI) [15], which consists of 110,250 images of 1000 common objects. Each image is represented by a dense 641-dimensional feature vector based on color and texture histograms (see [16] for details on how the vectors were produced). The following data sets were used:  X  X 1-ALOI-var: A subset of 13943 images, generated by selecting objects  X  I2-ALOI-full: The entire ALOI library.
 Since the appearance of individual objects varies considerably with the van-tage points of the images, almost every class would be expected to generate several natural clusters. Over 20 runs, GreedyRSC estimated the number of clusters to be 843  X  8 for the I1 data set, and 3724  X  25 for the I2 data set. Fast K -means and CLUTO were executed twice with random initialization, for both the true numbers of clusters ( K = 1000) and the number of clusters as determined by GreedyRSC. GlobalRSC was initialized using GreedyRSC, and a SASH was used to construct the neighborh ood sets. All runs except those involv-ing CLUTO were conducted using the Euclidean distance measure. Over 20 runs, GreedyRSC consistently achieves good clustering quality which is then further refined by GlobalRSC, as observed in Table 1. For the ALOI-full data set, the execution of Fast K -means failed to terminate due to insufficient main memory. It can be observed that when a good initialization is provided and SASH is used, the execution time of GlobalRSC is significantly shorter, comparable to that of K -means and CLUTO. 4.3 Text Data We tested the clustering algorithms on the Reuters Corpus Volume I (RCV1), an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes [17]. The document class struc-ture was simplified into 57 distinct cl asses. We then selected a subset T1 con-sisting of 200,000 documents classified to either exactly one subtopic or exactly one meta topic. We also constructed a smaller data set T2 consisting of 20,000 documents selected uniformly at random from T1. For both T1 and T2, TF-IDF weighting was used to construct the feature vectors, resulting in sparse vectors of length 320,648. Fast K -means was excluded in this experiment due to its lack of support for sparse numerical data. Since the number of external classes of these data sets was not as reliable as of the ALOI image data set, we first ran GreedyRSC to estimate the number of natural clusters K . The clustering result of GreedyRSC was used for the initialization of GlobalRSC, while CLUTO was run with the desired number of clusters also set to K . The cosine similarity measure was used for all runs. The cluste ring scores, averaged over 20 runs, are reported in Table 1. While all the algorit hms successfully processed the small T2 set, on T1 CLUTO gave a memory failure message after a few hours of execution, leaving only the results of GreedyRSC and GlobalRSC available for evaluation. The observed low agreement b etween the clustering result and the class information in this experiment can be attributed to natural fragmentation of the classes within the data domain. 4.4 Categorical Data The mushroom data set , drawn from the UCI machine learning repository [18], contains 8124 varieties of mushrooms, each recorded with 22 different cate-gorical physical attributes (such as color, odor, size, and shape). Each record is classified as to whether its associated mu shroom is poisonous or edible. The dis-tance measure for this data set is taken as the straightforward mismatch count, with missing values treated as contributing to the count.

The mushroom data set was previously analyzed with ROCK [2], which in their paper was reported as finding 21 clusters. Most of the clusters consist of only one type of mushroom, either edible or poisonous. Only 32 mushrooms were misclassified by ROCK. On 20 runs with random initialization, GreedyRSC produced 22  X  1 clusters, with 87  X  120 mushroom instances misclassified. The result is further refined with GlobalRSC, which brought the number of mushroom species misclassified down to 46  X  97. The classification errors of the 20 runs are reported in table 2. All the algorithms greatly outperformed the traditional hierarchical clustering implemented in [2], which produced 20 clusters within which 3432 out of 8124 items were misclassified. K -means was excluded from this experiment as it can not handle categorical data. A clustering algorithm from CLUTO, which operates on the similarity matrix, was tested but did not yield competitive results, with 1339 misclassified species. It should be noted that whereas ROCK require d an estimate of the number of clusters, GreedyRSC automatically determined this number. In this paper we have introduced a novel sh ared-neighbor clustering algorithm based on the Relevant Set Correlation (RSC) model. The key difference in our approach to clustering, compared to other shared-neighbor-based approaches, is that it requires the setting of only one main parameter  X  the number of clusters. The objective function greatly resembles that of K -means, and like K -means, the GlobalRSC method aims to discover compact, globular clusters. While this class of clusters appears to be restrictive, Dasgupta [19] has shown that for high dimensional da ta, random projection can transform highly eccentric clusters into more spherical ones, which in turn can be discovered by K -means or GlobalRSC. The techniques we presented for improving the scalability of our proposed GlobalRSC algorithm allow for practical application of the method for large, high-dimensional generic data sets under any reasonable measure of similarity.
 This work was partially supported by NICTA. NICTA is funded by the Aus-tralian Government as represented by the Department of Broadband, Commu-nications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

