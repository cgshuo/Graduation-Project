 ORIGINAL PAPER Bilan Zhu  X  Xiang-Dong Zhou  X  Cheng-Lin Liu  X  Masaki Nakagawa Abstract This paper describes a robust context integra-tion model for on-line handwritten Japanese text recogni-tion. Based on string class probability approximation, the proposed method evaluates the likelihood of candidate seg-mentation X  X ecognition paths by combining the scores of character recognition, unary and binary geometric features, as well as linguistic context. The path evaluation criterion can flexibly combine the scores of various contexts and is insensitive to the variability in path length, and so, the opti-mal segmentation path with its string class can be effectively found by Viterbi search. Moreover, the model parameters are estimated by the genetic algorithm so as to optimize the holistic string recognition performance. In experiments on horizontal text lines extracted from the TUAT Kondate data-base, the proposed method achieves the segmentation rate of 0.9934 that corresponds to a f-measure and the character recognition rate of 92.80%.
 Keywords On-line Japanese text recognition  X  String recognition  X  Integrated segmentation and recognition  X  Path evaluation 1 Introduction With pen input devices of large writing areas such as tablet PCs, electronic whiteboards and digital pens (e.g., Anoto pen), people tend to write texts continuously with little con-straints. This urges the need of handwritten text (character string) recognition. Compared to isolated character recog-nition, handwritten text recognition faces the difficulty of character segmentation because characters cannot be reliably segmented before they are recognized. Moreover, in continu-ous handwriting, characters tend to be written more cursively.
Character segmentation of continuous Chinese/Japanese handwriting is difficult due to the facts that the space between characters is not obvious, many characters comprise multiple radicals with internal gaps, and some characters are con-nected in cursive writing. Dissection methods (such as [ 1  X  4 ]) attempt to segment characters solely according to geometric layout features (gaps, character size/position and inter-rela-tionship). Without character recognition cues and linguistic context, characters cannot be segmented unambiguously by dissection. A feasible way to overcome the ambiguity of seg-mentation is the so-called integrated segmentation and recog-nition [ 5 ], which is dichotomized into implicit segmentation and explicit segmentation [ 6 ]. Implicit segmentation meth-ods (also called segmentation-free methods [ 7 ]), mostly com-bined with hidden Markov model (HMM)-based recognition, simply slice the string pattern into frames of equal length and label the sliced frames (primitive segments), which are concatenated into characters during recognition. Such meth-ods do not incorporate the character shape information suf-ficiently. Explicit segmentation, attempting to split character patterns at their true boundaries and to label the split char-acter patterns, can better utilize the character shapes into recognition. It is usually accomplished in two steps: over-segmentation and path evaluation-search. The string pattern is over-segmented into primitive segments such that each segment composes a single character or a part of a character. The segments are combined to generate candidate character patterns (forming a candidate lattice [ 8 ]), which are evalu-ated by character recognition incorporating geometrics and linguistic context.

In over-segmentation-based string recognition, how to evaluate the candidate characters (lying on paths in the can-didate lattice) is a key issue. A desirable criterion should make the path of correct segmentation have the largest score. Unlike HMM-based recognition that classifies a unique sequence of feature vectors (each for a frame) on a string, the candidate lattice of over-segmentation has paths of dif-ferent lengths, each corresponding to a different sequence of feature vectors, thus the comparison of different paths can-not be based on the Bayesian decision theory as for HMM-based recognition. Instead, candidate character recognition and context scores are heuristically combined to evaluate the paths. Such heuristic evaluation criteria can be divided into summation-based ones [ 9  X  13 ] and normalization-based ones [ 5 , 14 , 15 ]. A summation criterion is the summation of char-acter-wise log-likelihood or the product of probabilistic like-lihood. Since the likelihood measure is usually smaller than one,thesummation(product)criterionisoftenbiasedtopaths with fewer characters, and so, tends to over-merge charac-ters. On the other hand, the normalized criterion, obtained by dividingthesummationcriterionbythenumberofsegmented characters (segmentation length), tends to over-split char-acters. Another problem of normalized criterion is that the optimal path is not guaranteed by Viterbi search or dynamic programming (DP), because the criterion is not monotonic with the extension of path length.

To better utilize the character shape information in HMM-based recognition while preserving the monotonicity of path evaluation criterion, the variable duration HMM of Chen et al. [ 16 ] obtains the state emission probability on a can-didate character formed by concatenating multiple frames and replaces the emission probabilities of all these frames. In effect, this corresponds to weighting each candidate char-acter with its number of primitive segments in over-segmen-tation-based recognition, and leads to improved recognition accuracy and search efficiency [ 17 ]. The over-segmentation-based method proposed in Yu et al. [ 17 ] uses the number of primitive segments to weight the character recognition scores in a summation criterion to overcome the effect of segmentation length. However, it only weights the character recognition score, and does not explain why the scores of geometric features and linguistic context are not weighted. The path evaluation criteria in Yu et al. [ 17 ] cannot be derived from either labeling primitive segments or labeling character patterns. So, we need a method to decide whether to weight each factor using the number of primitive segments or not and give the weighting degree automatically.

In this paper, we present a robust context integration model for on-line handwritten Japanese text recognition. By labeling primitive segments, the proposed method not only can integrate the character shape information into recogni-tion by introducing some adjustable parameters, but also is insensitive to the number of segmented character patterns because the summation is over the primitive segments. More-over, by optimizing with the genetic algorithm (GA), we can control whether to weight each factor using the number of primitive segments or not and get the weighting degree auto-matically. The proposed model evaluates the likelihood of candidate segmentation and its string class by combining the scores of character recognition, geometric features (char-acter pattern sizes, inner gaps, single-character positions, pair-character positions, candidate segmentation points), and linguistic context. With the proposed path evaluation crite-rion, the optimal path can be efficiently found by Viterbi search.Experimentalresultsonhorizontaltextlinesextracted from the TUAT database of HANDS-Kondate_t_bf-2001-11 (hereafter, Kondate) [ 18 ] demonstrate the superiority of our proposed string recognition model.

The rest of this paper is organized as follows: Sect. 2 gives an overview of our handwritten text recognition system. Section 3 describestheover-segmentationschemeandSect. 4 details the string recognition model. Section 5 describes the parameter optimization method. Section 6 presents the exper-imental results and Sect. 7 offers our concluding remarks. 2 Processing flow For on-line handwritten Japanese text recognition, our inte-grated segmentation and recognition system has three major steps. The input is a handwritten character string or text line composed of a sequence of strokes.

Step 1: over-segmentation. Each off-stroke (pen lift between two consecutive strokes) is classified into two classes (segmentation point (SP) and non-segmentation point (NSP)) or undecided according to some geometric features. A segmentation point separates two characters at the off-stroke, while a non-segmentation point indicates the off-stroke is within a character. The off-strokes with low classification confidence are  X  X ndecided X  points. The group of consecutive strokes between two adjacent seg-mentation/undecided points is a primitive segment, and one or more consecutive primitive segments form a candi-date character pattern.

Step 2: candidate lattice construction. By character clas-sification, each candidate character pattern is associated with a number of candidate classes with confidence scores.
The combination of all candidate patterns and character classes is represented by a segmentation X  X ecognition candidate lattice (Fig. 1 ), where each arc denotes a segmentation point and each node denotes a character class assigned to a candidate pattern.

Step 3: string recognition. The segmentation paths and corresponding string classes in the candidate lattice are evaluated by combining the scores of candidate characters and between-character compatibilities (geometric and lin-guistic contexts). By searching the candidate lattice with the Viterbi algorithm, the optimal path with maximum score gives the final result of character segmentation and recognition. We intend to improve the path evaluation cri-terion such that the optimal path better corresponds to the correct segmentation and recognition. 3 Over-segmentation We previously proposed an over-segmentation method using an SVM classifier to classify off-strokes into segmenta-tion and non-segmentation points [ 18 ]. It extracts multi-dimensional features such as the distance and overlap between adjacent strokes from off-strokes, and the SVM classifier gives fairly high classification accuracy. However, this method often misclassifies true segmentation points as non-segmentation ones when adjacent strokes are overlap-ping heavily, and misclassifies non-segmentation points as segmentation ones when adjacent strokes are spaced largely. Some examples of misclassification are shown in Fig. 2 .It is hence necessary to leave off-strokes undecided when they cannot be classified reliably. Reducing misclassification via un-decision of some off-strokes can improve the string rec-ognition rate though undecided segmentation points compli-cate the candidate lattice and consequently the computation of string recognition.

To minimize un-decision and mean while minimize the misclassification between segmentation and non-segmenta-tion points, we herein use an improved two-stage classifica-tion scheme. First, we use two geometric features to classify the off-strokes into non-segmentation points and hypotheti-cal ones. Then, an SVM classifier is used to decide some of the hypothetical segmentation points as segmentation points. The process is detailed as follows. 3.1 Hypothetical segmentation We generate hypothetical segmentation points by extract-ing two features for each off-stroke: horizontal distance and intersecting length.
 The horizontal distance feature has been used in Zhu and Nakagawa [ 18 ]. It is calculated from two bounding boxes, one for all the strokes preceding the off-stroke (denoted by BB p _all ) and one for the succeeding strokes (denoted by BB s _all ) . A distance DB x is defined as DB x = lef t _ bound ( BB s _all )  X  right _ bound ( BB p _all The horizontal distance feature f d is calculated by f where acs is the average character size, which is estimated by measuring the longer side length of the bounding box of each stroke, sorting the lengths of all the strokes and taking the average of the larger 1/3 of them.

For calculating the intersecting length feature, consider the group of all strokes preceding the off-stroke (denoted by S S intersect at a point p , calculate the length on s p from p to the right end of s p (denoted by l p ) and the length on s s from p to the left end of s s (denoted by l s ) . The intersecting length between s p and s s is defined as l ( s = and accumulated as L The overall intersecting length f i for an off-stroke is then defined as f = Figure 3 shows the distribution of the horizontal distance feature and the intersecting length feature on off-strokes of a set of training string samples. It is shown that segmenta-tion points and non-segmentation ones are well separated by these two features. As shown in Fig. 3 , we classify off-strokes as hypothetical segmentation (undecided) points if their val-ues of horizontal distance feature are greater than 0 or they are in the area of OABCDE, and as non-segmentation points otherwise. After this, we modify the classified non-segmen-tation points between two successive hypothetical segmen-tation points in the area of OFGH as undecided points, if the width between the two successive hypothetical segmentation points divided by acs is greater than a threshold.
Up to now, the off-strokes are classified into non-segmentation points and hypothetical segmentation points. The non-segmentation points are excluded from further con-sideration (the string pattern cannot be split at a non-segmen-tation point), while the hypothetical segmentation points are further classified using an SVM classifier. 3.2 SVM classification The hypothetical segmentation points are classified using an SVM classifier on 20 geometric features, among which 18 have been presented in Zhu and Nakagawa [ 18 ]asshownin Appendix. Another two features are the intersecting length feature f i defined in Sect. 3.1 and a newly introduced width feature. The width feature of a hypothetical segmentation point is defined as the width of a box bounding the strokes from the immediately preceding hypothetical segmentation point to the immediately succeeding one, and divided by the average character size acs .

We train the SVM using training patterns of off-strokes with the target value of segmentation points set to 1 and that of non-segmentation points to  X  1. On hypothetical seg-mentation points of test string patterns, the SVM outputs are transformed to probability values (as detailed in Sect. 4.2 ), which are then combined into the criterion of candidate seg-mentation-recognition paths.

From SVM classification, we can select some hypothet-ical segmentation points as segmentation points for which the SVM output is greater than a threshold and the width feature values are greater than a threshold. At the decided segmentation points, the adjacent primitive segments cannot be merged to form candidate character patterns. The reduc-tion in hypothetical segmentation points simplifies the can-didate segmentation lattice and improves the efficiency of string recognition.

Keeping all the remaining off-strokes (hypothetical seg-mentation points) as undecided, segmentation points will incur computation burden. The first stage employs a sim-ple heuristics to eliminate false segmentation boundaries. The SVM classifier increases processing time, but it elim-inates them further and improve the recognition rate when it is reflected into the path evaluation function [ 13 ]. 4 String recognition model In the candidate lattice, it is inappropriate to score the paths using posterior probabilities of characters, because different paths may have different numbers of characters. We herein present an evaluation model that combines multiple features andis theoreticallyindependent of thelengthof segmentation paths. 4.1 Path evaluation Representing a character string pattern as a sequence of prim-itive segments: X = s 1 ,..., s m , which is partitioned into character patterns Z = z 1 ,..., z n , where each candidate pat-tern contains k i primitive segments: z i = s j i ,..., s The segmented character patterns are assigned classes C = C to string class C , we extract features for scoring the primitive segments (or candidate patterns) and between-segment (or between-character) compatibilities. The features are listed below:  X  Bounding box feature b i  X  Inner gap feature q i  X  Shape feature s i or z i  X  Unarypositionfeature p u i ofsinglesegment(orcharacter)  X  Binary position feature p b i between adjacent segments (or  X  Between-segment gap feature g i , which is to be classified
Denoting by b , q , X , p u , p b , g for the sequences of fea-tures of primitive segments, the posterior probability of string class is given by: P (
C | X ) = P C | b , q , X , p u , p b , g Omitting the string class-independent denominator and rea-sonably assuming independence between different features, the string class can be equivalently evaluated by f (
X , C ) = log p b , q , X , p u , p b , g | C P ( C ) where c i denotes a character class or a hypothetical cate-gory of primitive segment (we call it hyper-category), and t denotes SP or NSP. Note that one or more consecutive c i form a character class C j .

The linguistic prior P ( C ) is represented by the tri-gram of hyper-categories for primitive segments P ( c i | c i  X  Since the tri-gram of hyper-categories is difficult to obtain, we approximate the tri-gram of hyper-categories P ( c i | c i  X  2 c i  X  1 ) by that of character classes P ( C where C i includes c i : log P ( C ) = where  X  11 and  X  12 are weighting parameters, and  X  1 is a bias for balancing the number of characters. We approximate the transition probability of start segment of a character pattern and that of non-start segment using different weights for their varying effects.

Similarly, we approximate the probabilities of the fea-tures extracted from primitive segments by the probabilities of those extracted from candidate character patterns and use different weights for the start segment and non-start segment of the other features, obtaining the path score: f ( X , C ) where P h , h = 1 ,..., 6, stand for P ( C i | C i  X  2 C p ( b i | C i ), p ( q i | C i ), p ( z i | C i ), p ( p u i | respectively.  X  in Eq. ( 9 ) embraces all the bias terms for h = 1 ,..., 6. Note that s probability p ( s i | c i ) is approximated by p ( z i | C
The weighting parameters  X  h 1 , X  h 2 ( h = 1  X  7 ) and  X  are selected using a GA to optimize the string recognition performance on a training dataset.

The path score in Eq. ( 9 ) is accumulated over the prim-itive segments, and hence, is insensitive to the number of segmented character patterns. Thus, the optimal path can be found by Viterbi search (dynamic programming).

The path evaluation scorer of Nakagawa et al. [ 11 ] and that of Yu et al. [ 17 ] can be viewed as a special case of the proposed one in Eq. ( 9 ) by setting  X  h 1 = 1 , X  h 2 ( h = 1  X  7 ) and  X  = 0 for Nakagawa et al. [ 11 ], and by setting  X  41 =  X  42 , X  h 2 = 0 ( h = 1  X  3 , 5  X  7 ), and for Yu et al. [ 17 ], respectively. 4.2 Evaluation of terms a text corpus. It is reduced to unigram or bi-gram when C is the first or second character of a sentence. The tri-gram is smoothed to overcome the imprecision of training with insufficient text [ 19 ]: P ( C =  X  where the weights (subject to  X  1 +  X  2 +  X  3 +  X  4 = 1 ) obtained by using a different text corpus.

The values of geometric features b i , q i , p u i and p b normalized with respect to the average character size acs for scaling invariance. Several geometric features are shown in Fig. 4 .

The feature vector b i comprises the height and width of the bounding box of each character pattern.
 The feature vector q i comprises six values as shown in Fig. 5 . The first three values represent the horizontal gaps of three vertical slits (partitioned from vertical projection), and the last three ones represent the vertical gaps of three horizontal slits (from horizontal projection).

The feature vector p u i comprises the vertical lengths from the center line to the top and bottom of the bounding box.The featurevector p b i hastwoelementsmeasuredfromthebound-ing boxes of two adjacent character patterns: the vertical distances between the upper bounds and between the lower super-classes according to the mean vector of the unary posi-tion features of each class on training samples. p ( p b i is then replaced by p ( p b i | C i  X  1 , C i ) , where C super-classes.

The geometric feature vectors b i , q i , p u i and p b i formed to log-likelihood scores (to be used in Eq. ( 9 )) using quadratic discriminant function (QDF) classifiers. This is similar to the way that is found in Zhou et al. [ 15 ].
The character shape score p ( z i | C i ) is given by a character recognizer, which is detailed in Sect. 6 .

The feature vector g i comprises multiple features measur-ing the relationship between two primitive segments adjacent to a candidate segmentation point. We approximate p ( g i and p ( g i | NSP ) using a SVM classifier. The SVM output is warped to obtain probabilities p ( o i | SP ) and p ( o where o i is the output of the SVM for g i . The warping func-tion is obtained from the distribution of SVM outputs on a validation dataset. p ( o 1 | SP ) is set as 1.

To warp the SVM outputs, we first obtain the histograms probabilities p ( o i | SP ) and p ( o i | NSP ) : p ( o p ( o p ( o dal functions, with the parameters estimated by minimizing squared errors. This is similar to the method of Platt [ 20 ]but uses a different criterion for sigmoidal parameter estimation. 5 Parameter optimization We train the weighting parameters  X  h 1 , X  h 2 ( h = 1  X  7  X  by a GA using training data of character string patterns to maximize the recognition rate on training data. To do this, we treat each one of  X  h 1 , X  h 2 ( h = 1  X  7 ) and  X  as an element of a chromosome. The parameters are estimated by GA in following steps: (1) Initialization: Initialize N chromosomes with random (2) Crossover: Select two chromosomes at random from N (3) Mutation: Change each element of N+M chromosomes (4) Fitness evaluation: Evaluate fitness in terms of the rec-(5) Selection: Decide the roulette probability of each (6) Iteration: Obtain the average fitness of the new N chro-We set N as 50, M as 100, P mut as 0.03, n stop as 25 and T as 10,000.

For evaluating the fitness of a chromosome, each train-ing string pattern is searched for the optimal path evaluated using the weight values in the chromosome. To save compu-tation, we first set each weight value as 1 and select the top 100 recognition candidates (segmentation-recognition paths) for each training string. We then train the weight parame-ters by GA using the selected 100 recognition candidates of each training string pattern. After some iterations, we use the updated weight values to re-select top 100 recognition candi-dates for each training string pattern. We repeat recognition candidate selection three times. 6 Experiments For evaluating the proposed character string recognition model, we trained the character recognizer and geometric scoring functions using a Japanese on-line handwriting data-base Nakayosi [ 21 , 22 ]. The character recognizer combines off-line and on-line recognition methods by normalizing the recognition scores to conditional probabilities p ( z i | For the geometric scores, four quadratic discriminant func-tion (QDF) classifiers are trained for p ( b i | C i ), p p ( p u i | C i ) and p p b i | C i  X  1 , C i , respectively.
For scoring linguistic context, we prepared an initial tri-gram table from the year 1993 volume of the ASAHI newspa-per and the year 2002 volume of the NIKKEI newspaper. We estimated the smoothing parameters  X  1 , X  2 , X  3 , X  4 using the Nakayosi database. The data size of the tri-gram was reduced to 6MB by suppressing non-occurring terms, neglecting a small number of occurrences and quantizing the logarithm values of tri-gram probabilities.

For training the weight parameters and evaluating the performance of character string recognition, we extracted horizontally written text lines from the database Kondate collected from 100 people. We used 75 persons X  text lines for training the SVM classifier for the candidate segmenta-tion point probability and the weighting parameters of path evaluation score. After training, we used the text lines of the remaining 25 persons for testing. The statistics of the training and test are listed in Table 1 . The experiments were imple-mented on a Pentium (R) 4 2.80GHz CPU with 512MB memory.

First, we compare the performance for over-segmentation by our method (two-stage classification scheme) proposed in this paper and that by direct decision according to the SVM output (one-stage classification scheme) presented in Zhu and Nakagawa [ 18 ]. For fair comparison, both methods use the same path evaluation criteria by our model proposed in this paper. The over-segmentation method by one-stage classification extracts 19 features (18 features presented in Zhu and Nakagawa [ 18 ] plus the intersecting length feature) from off-strokes and applies the SVM to the extracted fea-tures to classify each off-stroke into a segmentation point, a non-segmentation point and an undecided point. We use a character segmentation measure f (F-measure of segmenta-tion point detection), the character recognition rate C r average string recognition time T av_rec_tl to evaluate the per-formance of text line recognition. Table 2 shows the results.
From Table 2 , we can see that the over-segmentation method proposed in this paper improves the character recognition and segmentation accuracy remarkably, although it consumes more processing time than the one-stage classification scheme. The method by two-stage classifica-tion scheme leaves many of off-strokes undecided to reduce misclassification, so that it can improve the recognition per-formance, though undecided segmentation points complicate the candidate lattice and consequently the computation of string recognition.

We also compare the performance of three path evalua-tion criteria: our model proposed in this paper (Proposed), the one presented in Nakagawa et al. [ 11 ] added weighting parameters as shown in Eq. ( 12 ) (Method 1) and the one pre-sented in Zhou et al. [ 15 ]asshowninEq.( 13 ) (Method 2). For fair comparison, all the three methods use the same tri-gram for language context and same classifiers for character recognition and geometric context. The weighting parame-ters  X  h 1 , X  h 2 ( h = 1  X  7 ),  X  i ( i = 1  X  7 ) and  X  were optimized using the genetic algorithm for each method. The three methods combines the same seven terms in Eq. ( 9 )for path evaluation, but Method 1 does not use the term related to k i (number of primitive segments composing a character pattern), Method 2 normalizes the path score of Method 1 using the number of segmented characters. For Method 2, we use beam search for finding the optimal paths in the candidate lattice, because the path score is not cumulative with the character sequence. For the proposed method and the Method 1, the optimal paths are found by Viterbi search. For all the three methods, the candidate lattice retains 10 candidate classes for each character pattern.
To justify weighting parameter optimization by GA, we also draw a comparison between the proposed character recognition rate optimization by GA (CR-GA) and the mini-mum classification error (MCE) criterion [ 23 ] optimized by stochastic gradient decent [ 24 ] (MCE-SGD). MCE-SGD is to find the optimal parameter vector  X  by minimizing the fol-lowing difference between the scores of the most confusing string class and that of the correct one:  X ( x ) = ( 1 + e  X  x )  X  1
Score correct = score of the correct path
Score incorrect = scores of incorrect paths
Table 3 shows the string recognition results of the three path evaluation methods. For reference, the trained weight values of Eq. ( 9 )byCR-GAareasfollows: (  X   X  0 . 641 , 0 . 009 , 0 . 000 , 0 . 100 , 0 . 000 , 0 . 323 , 0
From the weighting parameters obtained by GA, we can see that except the character recognition score p ( z i | the non-segmentation point score p ( g i | NSP ) , the other geo-metric features and linguistic context are not weighted with the number of primitive segments (  X  h 2 = 0). This implies that except the character recognition score and the non-segmentation point score, the other geometric features and linguistic context are almost independent of the number of primitive segments of character pattern.
 From the results, we can see that either by CR-GA or by MCE-SGD, our proposed path evaluation model improves (i) the character recognition and segmentation accuracy. The Method 1 tends to over-merge characters because a shorter character sequence tends to have larger evaluation score than a longer one. On the other hand, the Method 2 (normal-ized path score) is biased to longer strings, and so, tends to over-split characters. Our proposed model overcomes these problems, and the path score is insensitive to the number of segmented characters. The three methods consume nearly the same processing time. The parameter optimization method CR-GA yields better string recognition performance than the MCE-SGD. This can be attributed to the local optimum of gradient descent for MCE-SGD. The CR-GA directly opti-mizes the character recognition rate (which is not differen-tiable) on training data and achieves a global optimum.
Figure 6 shows some examples of misrecognition and mis-segmentation given by the proposed model. For each example, the upper line is the written text, and the lower line is the recognition result followed by the correct result (ground-truth) where the recognition errors are highlighted by underlines. We observed two major sources causing seg-mentation-recognition errors. (1) Problem of character recognition: Fig. 6 a X  X  show rec-(2) Problems of path evaluation and over-segmentation: 7 Conclusion In this paper, we presented a robust on-line handwritten Japa-nese character string recognition model that can evaluate the likelihood of candidate segmentation and its corresponding string class by combining the scores of character recognition, geometric and linguistic contexts. With the path evaluation criterion balanced by the primitive segment number for the scores associated with the candidate character patterns on the path, the proposed text recognition model can effectively overcome the variable length of candidate segmentation. With the model parameters optimized by GA, the proposed system outperforms the other popular path evaluation criteria in our experiments. The optimized weighting parameters jus-tify the fact that only the character recognition score and the non-segmentation point score are dependent on the primitive segmentation number of candidate character patterns.
To further improve the segmentation and recognition per-formance is the aim of our future work. This can be achieved by incorporating more effective geometric features, exploit-ing better geometric context likelihood functions and weight-ing parameter learning methods and improving the accuracy of character recognizer. To speed up recognition is another dimension of our future work. We should consider effective methods to remove invalid patterns from the lattice. Appendix Geometric features of SVM classifier on over-segmentation in [ 18 ] First, we define the following terminology:
Then, the following 18 features of off-strokes are extracted for over-segmentation: References
