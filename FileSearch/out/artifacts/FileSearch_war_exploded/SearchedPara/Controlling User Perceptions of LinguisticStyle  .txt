 University of Cambridge University of California, Santa Cruz
Recent work in natural language generation has begun to take linguistic variation into account, developing algorithms that are capable of modifying the system X  X  linguistic style based either generation systems to scale to the production of the large range of variation observed in human dialogues. Previous work on statistical natural language generation (SNLG) has shown that the grammaticality and naturalness of generated utterances can be optimized from data; however these data-driven methods have not been shown to produce stylistic variation that is perceived by humans in the way that the system intended. This paper describes P parameterizable language generator whose parameters are based on psychological findings about the linguistic reflexes of personality. We present a novel SNLG method which uses parameter required to convey any combination of scalar values along the five main dimensions of personal-ity. A human evaluation shows that parameter estimation models produce recognizable stylistic variation along multiple dimensions, on a continuous scale, and without the computational cost incurred by overgeneration techniques. 1. Introduction
Although language can be seen as simply a method for exchanging information, it also has a social function (Goffman 1970; Dunbar 1996; Labov 2006). Speakers use linguistic cues to project social aspects of utterances, such as the speaker X  X  personality, emotions, and social group, and hearers use these cues to infer properties about the speaker.
Although some cues appear to be produced through automatic cognitive processes (Levelt and Kelter 1982; Pickering and Garrod 2004), speakers may also overload their communicative intentions to try to satisfy multiple goals simultaneously (Pollack 1991;
Stone and Webber 1998; Jordan 2000), such as projecting a specific image to the hearer while communicating information and minimizing communicative effort (Clark and
Brennan 1991; Brennan and Clark 1996). The combination of these pragmatic effects results in the large range of linguistic variation observed between individual speakers (Biber 1988).
 styles has focused on text generation applications such as journalistic writing or in-struction manuals (Hovy 1988; Scott and de Souza 1990; Paris and Scott 1994; Green and
DiMarco 1996; Bouayad-Agha, Scott, and Power 2000; Power, Scott, and Bouayad-Agha 2003; Inkpen and Hirst 2004). Recent research in language generation for dialogue appli-cations has also begun to take linguistic variation into account, developing algorithms to modify the system X  X  linguistic style based on either the user X  X  linguistic style, or other factors such as the user X  X  emotional state, her personality, or considerations of politeness strategies (Walker, Cahn, and Whittaker 1997; Lester, Towns, and Fitzgerald 1999; Lester,
There is growing evidence that dialogue systems such as intelligent tutoring systems are more effective if they can generate a range of different types of stylistic linguistic variation (Litman and Forbes-Riley 2004, 2006; Porayska-Pomsta and Mellish 2004;
Wang et al. 2005; McQuiggan, Mott, and Lester 2008; Tapus and Mataric 2008). Most of this work uses either templates or handcrafted rules to generate utterances. This guarantees high quality, natural outputs, which is useful for demonstrating the utility of stylistic variation.
 for each new application, however, leading to problems of portability and scalability (Rambow, Rogati, and Walker 2001). Statistical natural language generation (SNLG) has the potential to address such scalability issues by relying on annotated data rather than manual parameter tuning. It also offers the promise of techniques for producing continuous stylistic variation over multiple stylistic factors by automatically learning a model of the relation between stylistic factors and properties (parameters) of generated utterances (Paiva and Evans 2004, 2005). It is difficult to produce such continuous variation over multiple factors with a rule-based or template-based approach (but see
Bouayad-Agha, Scott, and Power 2000). Moreover, to date, no one has shown that humans correctly perceive the generated variation as the system intended, nor has anyone shown that an SNLG approach can produce outputs that are natural enough to be used in dialogue applications such as intelligent tutoring systems, interactive drama systems, and conversational agents, where some types of stylistic variation have already been shown to be useful.
 useful framework for modeling some types of stylistic linguistic variation. This model of human personality has become widely accepted in psychology over the last 50 years (Funder 1997). Table 1 tabulates each Big Five trait along with some of the important adjectives provide an intuitive, meaningful definition of linguistic style. In previous work we describe a rule-based version of P ERSONAGE , which here we will refer to as P ERSONAGE -RB (Mairesse and Walker 2007; Mairesse 2008). In P eration parameters are implemented, and their values are set based on correlations be-tween linguistic cues and the Big Five traits that have been systematically documented in the psychology literature (Scherer 1979; Furnham 1990; Pennebaker and King 1999;
Mehl, Gosling, and Pennebaker 2006). For example, parameters for the extraversion 456 trait include verbosity, sentence length, and the production of positive content. We showed experimentally that humans perceive utterances generated by P as conveying the extremes of all Big Five traits (e.g., neuroticism (low) vs. emotionally stable (high), see Table 1). Our evaluation uses a validated perceptual questionnaire from the personality psychology literature (Gosling, Rentfrow, and Swann 2003). high or the low end of one trait; however, psychologists measure personality traits on continuous scales (Norman 1963; Goldberg 1990; Marcus et al. 2006), and human language simultaneously manifests multiple personality traits. Some computational applications may require more than a small set of personality types, which suggests that systems adapting their linguistic style to the user would benefit from fine-grained personality models. We believe that the only way to robustly and efficiently learn such fine-grained variation is to model personality as a continuous variable, rather than using arbitrary discrete personality classes. Personality generation models should thus learn to map continuous target personality scores to discrete utterances. In order to achieve this, the handcrafted rule-based approach would require the manual examination of psycholinguistic findings, followed by testing in the application domain, to determine the appropriate range for each parameter value. Extending this approach to continuous variation that can project multiple traits simultaneously does not appear to be tractable. is trained with a novel method, and which learns to generate stylistic variation ex-pressing multiple continuous stylistic dimensions (in this case multiple personality traits).
Before presenting our method, let us review existing paradigms for statistical language generation. 1.1 Previous Statistical Language Generation Methods
Previous work on SNLG has focused on three main approaches: (a) learning statisti-cal language models (SLMs) from corpora in order to rerank a set of pre-generated utterances; (b) learning utterance reranking models from user feedback rather than corpora; and (c) learning generation parameters directly from data.
 focused on grammaticality and naturalness (Bangalore and Rambow 2000; Langkilde-Geary 2002; Chambers and Allen 2004; Nakatsu and White 2006). The seminal work of
Langkilde and Knight (1998) in this area showed that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. The SLM scoring gives a low score (rank) to any ungrammatical output produced by the rule-based generator. We will refer to this as the overgenerate and scoring (OS) approach.
 the generation of dialogues in which conversational agents with different personalities discuss movies. The SLM ranking model blends SLMs from blogs annotated with Big
Five personality traits with SLMs from Switchboard, a much larger conversational dialogue corpus. Their C R A G -2 generator discretizes the blog personality ratings into three groups (low, medium, and high), and models personality with three distinct SLM models for each trait. Each model estimates the likelihood of the utterance given the personality type. A cache model based on recently used linguistic forms can also be combined, in order to model recency effects and alignment (Pickering and Garrod 2004). This approach was integrated into a demonstrator, but it does not generate continuous variation (discretization of personality ratings), and to our knowledge it has never been evaluated to test whether the variation produced is perceivable by users.
 ing phase to replicate human judgments rather than relying on the probabilities or frequencies of a SLM. This approach typically uses higher-level syntactic, semantic, and discourse features rather than only n -grams, with typical results demonstrating that the performance of the scoring models approaches the gold-standard human rank-ing with a relatively small training set (Rambow, Rogati, and Walker 2001; Stent and
Guo 2005; Nakatsu and White 2006). An advantage of this approach is that human judgments can be based on any aspect of the output, such as stylistic differences in politeness or personality. Walker et al. (2007) showed that this technique can be used to model individual preferences in rhetorical structure, syntactic form, and content ordering.
 outputs of P ERSONAGE (Mairesse 2008). The resulting statistical generator is referred to as P ERSONAGE -OS. We randomly varied P ERSONAGE  X  X  non-deterministic decisions points to generate a large number of paraphrases. We then computed post hoc fea-tures consisting of the actual generation decisions, surface word n -grams, and content-analysis features from the Linguistic Inquiry and Word Count (LIWC) tool (Pennebaker,
Francis, and Booth 2001) and the MRC psycholinguistic database (Coltheart 1981). Ex-ample content-analysis features include the ratio of words related to positive emotions (e.g., good ), social interactions (e.g., pal ), or the average frequency of use of each word.
Scoring models trained on personality ratings of random utterances (in-domain data) outperformed the mean value baseline for all Big Five traits, with the best results for agreeableness, extraversion, and emotional stability. The models for those traits predict the ratings of unseen utterances with correlations of r = . 52, r = . 37, and r = . 29, respec-tively. We also trained models on out-of-domain data, that is, 96 personality-annotated conversation extracts (without any generation decision features). Results show that the 458 out-of-domain models perform worse for all traits, only outperforming the baseline for agreeableness and conscientiousness. We also explored several hybrid methods for training that mix and blend data from different sources. Inspired by recent work on domain adaptation, we tested whether the performance of the out-of-domain models can be improved when training includes a small amount of data from the target do-main, by applying the method of Daum  X  e (2007). Whereas adding out-of-domain data improved performance for some traits, we find that adding a single domain feature per-forms as well as Daume X  X  method. The results showed that mixing randomly generated in-domain utterances with rule-based in-domain utterances improves performance; the rule-based utterances provide a way to incorporate knowledge from the personality psychology literature into an SNLG approach. Thus, personality scoring models can be effective, although the computational cost of the OS approach remains a major drawback.
 without any overgeneration phase. If the language generator is constrained to be a gen-erative SLM, the parameters can then be learned through standard maximum-likelihood estimation. Whereas n -gram SLMs can only model local linguistic phenomena, Belz showed that a context-free grammar (PCFG) can successfully model individual differ-ences in the production of weather reports (Belz 2005, 2008). This method provides a principled way to produce utterances matching the linguistic style of a specific corpus (e.g., of an individual author) without any overgeneration phase. However, standard
PCFG generation methods require a treebank-annotated corpus, and they cannot model context-dependent generation decisions, such as the control of sentence length or the generation of referring expressions.
 model mapping generation decisions to stylistic dimensions extracted from a corpus, independently of the language generation mechanism. Factors are identified by apply-ing factor analysis to a corpus exhibiting stylistic variation, and expressed as a linear combination of linguistic features (Biber 1988). Textual outputs are generated with a rule-based generator in the target domain that is allowed to randomly vary the gener-ation parameters, while logging the parameter settings corresponding to each output.
Then the same factors found in the original corpus are measured in the random outputs, and linear regression is applied to learn which generation parameters predict the factor measurements. The generation parameters can then be manipulated to hit multiple stylistic targets on a continuous scale (because factors are measured continuously) by searching for the parameter setting yielding the target stylistic scores according to the linear models. The generator of Paiva and Evans, trained in this way, can reproduce in-tended factor levels across several factors, such as sentence length and type of referring expression, thus modeling the stylistic variation as measured in the original corpus.
Again, it has not been shown that humans perceive the stylistic differences that this approach produces. 1.2 Parameter Estimation Models
In the previous sections, we referred to two existing methods for controlling the param-eters of P ERSONAGE to produce stylistic variation: P ERSONAGE generation parameter values for every target style of interest, and P uses a statistical rescoring model to rerank a set of randomly generated utterances. The following sections develop and evaluate P ERSONAGE which uses a direct generation method inspired by Paiva and Evans X  approach (2005), to produce the stylistic variation found in personality traits, without any overgeneration phase. Whereas Paiva and Evans learn models predicting the target stylistic scores from the generation parameters, we train parameter estimation models (PE) to estimate the optimal generation parameters given target personality scores, which are then used by the base generator to produce the output utterance. As parameter estimation models learn the reverse relationship to Paiva and Evans X  regression models, there is no need to search for the optimal generation parameter values at generation time. We evaluate the PE approach using the P ERSONAGE base generator, whose parameters, architecture, and capabilities are described in Section 2. Our experimental method is described in
Section 3, together with an analysis of the data required to train our models. Section 4 analyzes some of the learned models, and evaluates the quality of the generated outputs using human judges, to compare our approach with the handcrafted P generator of our previous work. Finally, Section 5 discusses the implications of our results and suggests many areas of future work.
 ing an SNLG system that can produce multiple stylistic dimensions simultaneously, over continuous dimensions, without overgeneration or search. In order to evaluate our approach, we present the first empirical results showing that humans correctly perceive the stylistic variations (of any kind based on any utterance dimensions) that a statistical language generator intended to produce. Our evaluation of P other result that we know of for non-statistical generators (Mairesse and Walker 2007).
Our experiments show that P ERSONAGE -PE produces utterances perceived by humans as portraying different personalities, while maintaining a reasonable naturalness level (4.0 on a scale of 1 to 7). We do not know of any other human evaluation of an
SNLG system that produces stylistic variation. Additionally, we test a wide range of machine learning algorithms to determine the best model for each generation decision in Section 4.1. We are not aware of any other work on SNLG to test such a wide range of algorithms. 2. The P ERSONAGE Base Generator
The architecture of the P ERSONAGE base generator is shown in Figure 1; it is discussed in detail in (Mairesse 2008) and (Mairesse and Walker 2010), and is only briefly summa-rized here.
 generation (NLG) pipeline architecture as described in Reiter and Dale (2000), Kittredge,
Korelsky, and Rambow (1991), and Walker and Rambow (2002). We assume that the inputs to the generator are (1) a high-level communicative goal; (2) a content pool that dialogue system, the communicative goal is provided by the dialogue manager. Two types of communicative goals are currently supported by P and comparison of restaurants. P ERSONAGE  X  X  content pool is based on a database of restaurants in New York City, with associated scalar values representing evaluative ratings for six attributes: food quality , service , cuisine , location , price ,and atmosphere . which specifies the structure of the information to be conveyed. The resulting content plan tree is then processed by the sentence planner , which selects syntactic templates for expressing individual propositions, and aggregates them to produce the utterance X  X  460 full syntactic structure. The pragmatic marker insertion component then modifies the syntactic structure locally to produce various pragmatic effects, depending on the mark-ers X  insertion constraints. The lexical choice component selects the most appropriate lexeme for each content word, given the lexical selection parameters. Finally, the Real-
Pro surface realizer (Lavoie and Rambow 1997) converts the final syntactic structure into a string by applying surface grammatical rules, such as morphological inflection and function word insertion. When integrated into a dialogue system, the output of the realizer is annotated for prosodic information by the prosody assigner before being sent to the text-to-speech engine to be converted into an acoustic signal. P not currently express personality through prosody, although there are studies that could be used to develop such parameters (Scherer 1979; Furnham 1990).
 produce and control personality-based linguistic variation. The generation parameters are shown in Table 2 and organized into blocks that correspond to the modules of the architecture in Figure 1; compare Figure 1 to Table 2. As mentioned previously, all of P ERSONAGE  X  X  parameters are motivated by findings in the personality psychology literature. The mapping from a finding to parameters represents a set of hypotheses about how the finding can be implemented, however, as discussed in more detail in Mairesse and Walker (2007) and Mairesse (2008).
 does and often includes an example. For example, there are 12 content planning param-eters shown in the first block of Table 2; these control aspects of utterances such as their verbosity, rhetorical structure, content selection parameters such as positive content, and the level of redundancy and restatement (Walker 1993). Table 2 also includes 13 pragmatic marker parameters, which we believe to be completely novel. These include the introduction of HEDGES and TAG QUESTIONS . We are not aware of any other gen-erators that produce the range of pragmatic variation illustrated here. Note also that 462
Figure 1 indicates that the lexical choice parameters in Table 2 make use of multiple on-line lexical resources such as WordNet and V ERB O CEAN to support lexical variation. The LEXICAL FREQUENCY parameter is calculated with respect to a corpus.
 ance (e.g., verbosity), other parameters are highly non-linear (e.g., the effect of inserting two expletives rather than one is not as strong as the effect of inserting one expletive rather than none). Parameters are therefore modeled as having either continuous (C) or binary (B) values, as illustrated in column Type of Table 2. The models for continuous and binary parameters are trained using different algorithms. Section 3 will provide examples of learned models of both types.
 uous parameter values are generation decision probabilities; for example, the input to aggregation parameters such as CONJUNCTION is the probability that the aggrega-
CONJUNCTION aggregates two propositions with the conjunction and ). If the propo-sitions cannot be aggregated because of syntactic constraints, another aggregation operation is sampled until the aggregation is successful. Complete details on the im-plementation of individual parameters can be found in Mairesse (2008) and Mairesse and Walker (2010).
 values are normalized between 0 and 1 for continuous parameters, and to 0 or 1 for binary parameters. For example, a VERBOSITY parameter of 1 maximizes the utterance X  X  verbosity given the input, regardless of the actual number of propositions expressed.
In order to ensure naturalness over the full parameter range, the maximum value of some continuous parameters is associated with an input-independent threshold (e.g., there cannot be more than two repeated propositions per utterance). Although the goal of the base generator is to satisfy its input parameters, it cannot guarantee that all input parameter values will be reflected in the utterance due to constraints on the input content plan and other parameters. A consequence is that non-deterministic decision points are introduced to satisfy these naturalness constraints (e.g., if too many pragmatic marker parameters are enabled, only a random subset will appear in the utterance). Therefore, the only assumption we make regarding the impact of parameter intended effect over a large set of utterances. 3. Generation of Personality through Data-Driven Parameter Estimation
Whereas P ERSONAGE -RB uses handcrafted parameter settings to convey different per-sonality traits, P ERSONAGE -PE relies on parameter estimation models to estimate the parameter values in Table 2 from target personality scores. At training time, our method requires the following steps: 1. Use a base generator to produce multiple utterances by randomly varying 2. Ask human subjects to evaluate (rate) the personality/style of each 3. Train statistical models predicting the parameter values from the 4. Select the best model for each parameter via cross-validation (see
At generation time, the models are used to predict the optimal set of generation param-with the predicted parameter values. The architecture for the PE method is shown in
Figure 2. 464 parameter estimation models predict generation decisions directly from input person-ality scores, in the spirit of the approach of Paiva and Evans (2005). However, whereas
Paiva and Evans X  approach searches for the generation decisions that will yield the optimal target scores according to their model, our PE method does not involve any search, as generation decisions are assumed to be conditionally independent given the target personality, and treated as dependent variables in individual models.
 models. We first explain in Section 3.1 how we collect the judge X  X  ratings for our training set. Then Section 3.2 analyzes the coverage and naturalness of the collected data. Finally,
Section 3.3 describes how the models are trained. 3.1 Collecting Judgments of Random Sample set mapping generation decisions to personality ratings. This involves the following substeps: 1. Generate a sample of random utterances that produces examples covering 2. Log the generation decisions that were made to produce each utterance. 3. Judges rate the random sample with a standard personality test shown in
To be the basis for training a high performing statistical generator, the random sample each Big Five trait or there will not be enough training data to learn how to produce utterances manifesting those values. Second, the randomly produced utterances must be natural enough to produce stable personality judgments. The only way to verify that the random sample satisfies these properties is by first generating the random sample and then analyzing the judge X  X  ratings. We generated 160 random utterances to constitute our random sample. Table 3 shows examples of random utterances and the scalar ratings for each trait that result from the judgment collection process. questionnaires (John, Donahue, and Kentle 1991; Costa and McCrae 1992; Gosling,
Rentfrow, and Swann 2003). Figure 3 shows the Ten Item Personality Inventory (TIPI) that we used to collect the personality judgments (Gosling, Rentfrow, and Swann 2003), adapted to our domain and task. The TIPI produces a scalar rating for each of the
Big Five traits ranging from 1 (e.g., highly neurotic) to 7 (e.g., very stable), and it was shown to correlate well with longer questionnaires such as the Big Five Inventory, with convergent correlations of .87, .70, .75, .81, and .65 for extraversion, emotional stability, agreeableness, conscientiousness, and openness to experience, respectively (Gosling,
Rentfrow, and Swann 2003). Although the TIPI has mostly been used as a self-report measure of personality, it has also been used to assess personality perceptions of observers, for example, based on short social interactions (Srivastava, Guglielmo, and
Beer 2010) or social networking Web sites (Gosling, Gaddis, and Vazire 2007). The judges in our experiment were researchers and postgraduate students in psychology, history, and anthropology who were familiar with the Big Five trait theory, but not with natural language generation. They were all native speakers of English. As illustrated in Figure 3, the judges were asked to rate each utterance in the random sample using the
TIPI scale. They were instructed to rate the utterance as if it had been uttered by a friend responding in a dialogue to a request to recommend restaurants. Each judge rated the same sets of utterances corresponding to 20 communicative goals, 16 utterances per goal, one set at a time. The order of the sets and the order of the utterances within each set were both randomized. The judges were asked to read all the utterances in a set 466 before rating them. Eight utterances out of sixteen were randomly generated for each communicative goal. The remaining utterances were generated using the handcrafted parameter settings of P ERSONAGE -RB for each end of each Big Five trait (Mairesse and Walker 2007). The rule-based utterances are used as a comparison point, not for training the models. The same methodology was used to collect additional extraversion ratings for another set of 160 random and 80 rule-based utterances in a separate experiment, resulting in 320 random and 240 rule-based utterances for that trait, and 160 random utterances and 40 rule-based utterances for each of the other four traits. Examples of the resulting scalar ratings are shown in Table 3. The judges also evaluated the naturalness of each utterance on the same scale. 3.2 Generation Range and Naturalness
Analysis of the collected ratings of the random utterances shows that 67.8% of the utterances were rated as natural (rating above or equal to 4), with an average nat-perience and naturalness ratings. Figure 4a illustrates that most randomly generated utterances are not perceived as projecting an extreme personality. Table 4 examines whether randomly generated utterances can hit the extreme ends of each trait scale by tabulating the most extreme ratings obtained from the 8 random utterances generated for each communicative goal with the ratings of the rule-based utterance generated from the same goal. This comparison provides useful information regarding (a) the potential of data-driven models to outperform handcrafted methods, and (b) whether our training corpus is large enough to capture the range of behavior we intend to convey. Paired t-tests over 20 communicative goals show that on average the most extreme random utterance is significantly more extreme for the positive end of the ex-traversion, emotional stability, and agreeableness scales, and significantly more extreme for both ends of the conscientiousness and openness to experience scales (p &lt;. 05, two-468 tailed). However, random utterances are not perceived as introverted as those generated using the introvert parameter settings (see Rule-based/Low column for extraversion).
Compare the distributions of judgments for the rule-based extraversion utterances with the judgments on the random sample shown in Figure 5. Nevertheless, these results suggest that randomizing P ERSONAGE  X  X  parameters produces a wide range of variation with an utterance sample of less than 10 utterances, for any communicative goal. ances with the naturalness of the rule-based utterances produced by P (Mairesse and Walker 2007; Mairesse 2008). Results suggest that the random utterances are less natural than the rule-based utterances, and this difference is close to significance (p = . 075, two-tailed t-test).
 rater agreement between the judges. Table 5 shows that the judges agree significantly correlations ranging from .26 (conscientiousness) to .40 (agreeableness), which are high correlations for human perceptual judgments. However the agreement is lower than on the rule-based utterances. A possible explanation of both the naturalness differences and rater agreement is that the random generation decisions sometimes produce ut-terances with inconsistent personality cues, which can be interpreted in different ways by the judges. For example, the utterance  X  X rr... I am sure you would like Chanpen Thai! X  expresses markers of both introversion (filled pause) and extraversion (exclamation mark). 3.3 Training Parameter Estimation Models
Parameter estimation requires a series of pre-processing steps, in order to ensure that the models X  output is re-usable by the P ERSONAGE base generator. The initial data set includes the random sample annotated with the generation decision features shown in Table 2, together with the average judges X  ratings along each Big Five dimension, as described in Section 3.1. The following transformations are performed before the learning phase: 470
Once the data is partitioned into data sets mapping the relevant personality dimensions (the features) to each generation parameter (the dependent variable), it can be used to train parameter estimation models predicting the most appropriate parameter value given target personality scores. Parameters are estimated using either regression or classification models, depending on whether they are continuous (e.g., binary (e.g., EXCLAMATION ). Recall that Table 2 indicated for each parameter whether it is continous (C) or binary (B). In order to identify what model should be used for each parameter, we compare various learning algorithms using the Weka toolbox (Witten and Frank 2005).
 (LR), an M5 X  model tree (M5), and a model based on support vector machines with a linear kernel (SVM). As regression models can extrapolate beyond the [0, 1] interval, the output parameter values are truncated if needed X  X t generation time X  X efore being sent to the base generator. Regression models are evaluated using the correlation between the model X  X  predictions and the actual parameter values in the test data.
 parameter should be enabled or disabled . We test a Naive Bayes classifier (NB), a C4.5 decision tree (J48), a nearest-neighbor classifier using one neighbor (NN), the Ripper rule-based learner (JRIP), the AdaBoost boosting algorithm (ADA), and a support vector machines classifier with a linear kernel (SVM). Unless specified, the learning algorithms use Weka X  X  default parameter values. 4. Evaluation
This section first details some of the parameter estimation models trained on the data collected in Section 3. The models X  predictive power is then evaluated by doing a 10-fold cross-validation in Section 4.2. Finally, Section 4.3 evaluates human perceptions of utterances generated using the models. 4.1 Qualitative Model Evaluation
Before discussing our quantitative results, we use Figures 6, 7, and 8 to illustrate how the learned models predict generation parameters from input personality scores. Note that sometimes the best performing model is non-linear. For example, given input trait values, the AdaBoost model in Figure 6 outputs the class yielding the largest sum of weights for the rules returning that class. The first rule of the
Figure 6 shows that an extraversion score above 6.42 out of 7 would increase the weight of the enabled class by 1.81. The fifth rule indicates that a target agreeableness above 5.13 would further increase the weight by .42. Figure 6 also illustrates how personality traits that do not have an effect on the parameter are removed, i.e., extraversion and agreeableness are the traits that affect the use of exclamation marks. The model tree in Figure 7 lets us calculate that a low emotional stability (1.0) together with a neutral conscientiousness (4.0) and openness to experience (4.0) yield a parameter value of .62 (see bottom-left linear model), whereas a neutral emotional stability decreases the value down to .17. The full parameter range obtained when varying both emotional stability and conscientiousness is illustrated in Figure 9, which shows that the .5 cut-off 472 point can be reached for low emotional stability scores and mid-range conscientiousness scores. The linear model in Figure 8 shows that agreeableness has a strong effect on the
CONTENT POLARITY parameter (.97 weight), but emotional stability, conscientiousness, and openness to experience also influence the parameter value.
 findings in the psychology literature carry over to our domain. However, in order to optimize the overall generation performance, we rely on a quantitative analysis for selecting individual models. 4.2 Cross-Validation on Corpus of Expert Judgments
We identify the best performing model(s) for each generation parameter via a 10-fold cross-validation. For continuous parameters, Table 6 evaluates modeling accuracy by comparing the correlations between the model X  X  predictions and the actual parameter comparing the F-measures of the enabled class. The F-measure measures how well the models predict the enabled class given the small proportion of instances labelled as classification accuracy. Models producing the best cross-validation results are identified in bold for each parameter; parameters that produce a poor modeling accuracy are omit-ted. Because of the large number of parameters tested simultaneously in each training utterance, many reported accuracies are relatively low. As our training approach aims at including all parameters that can potentially convey personality, we include models with correlations or F-measures above .05 in our system, and let individual models learn the extent to which their parameter will affect the trained system.
 rately, with the SVM model in Figure 8 producing a correlation of .47 with the true parameter values in Table 6. Models of the PERIOD aggregation operation also perform well, with a linear regression model yielding a correlation of .36 when realizing a justification, and .27 when contrasting two propositions. The and VERBOSITY parameters are also modeled successfully, with correlations of .33 and .26 using a model tree. The model tree controlling the STUTTERING 474 in Figure 7 produces a correlation of .23. Concerning binary parameters, although differences between the best performing models are not significant, Table 7 suggests that the Naive Bayes classifier is generally the most accurate, with F-measures of .40 for the IN -GROUP MARKER parameter, and .32 for both the insertion of filled pauses ( err ) and tag questions. The results suggest that the AdaBoost learning algorithm performs best for predicting the EXCLAMATION parameter, with an F-measure of .38 for the model in Figure 6.
 given parameter (e.g., C ONCESSION POLARITY ). This suggests that exploring different learning algorithms for individual parameters is beneficial. While the overall modeling accuracy can seem relatively low X  X or example, there are only four parameters with correlations above r = . 25 in Table 6 X  X t is important to keep in mind that the utterances were randomly generated, hence the effect of a specific parameter is likely to be affected by other random parameter values. The next section therefore evaluates whether the models are good enough to produce reliable effects on user perceptions. 4.3 Evaluation with Naive Subjects
The evaluation presented in Section 4.2 measures how well the PE models predict the parameters, using parameter settings in the random sample and expert judge ratings as the predictors. We relied on a small number of expert judges to minimize rating inconsistencies and facilitate the learning process. In order to test whether the PE method produces high quality outputs manifesting personality, we ran an experiment with 24 native English speakers (12 male and 12 female graduate students from a range of disciplines from both the U.K. and the U.S.). We produced a set of 50 utterances for this experiment using the best performing models for each generation parameter shown in Tables 6 and 7. Given this model, we generate 5 utterances for each of 10 input communicative goals. Each utterance targets an extreme value for two traits (either 1 or 7 out of 7) and neutral values for the remaining three traits (4 out of 7). The goal is for each utterance to project multiple traits on a continuous scale. Here, we test whether
PE models can convey personality in extreme regions of the Big Five space. In order to generate a range of alternatives for each input communicative goal, all target scores are randomized around their initial value according to a normal probability distribution with a standard deviation of 10% of the full scale (see Figure 10).
 (Gosling, Rentfrow, and Swann 2003). As in the training data collection (Section 3.1), the subjects rated one set of five utterances at a time, one for each communicative goal. The communicative goals and the utterances were presented in random order.
To limit the experiment X  X  duration, only the two traits with extreme target values are evaluated for each utterance. As a result, 20 utterances are evaluated for each trait, 10 of which were generated to convey the low end of that trait, and 10 of which target the high end of that trait. Each utterance was also evaluated for its naturalness as before.
Subjects thus answered 5 questions for 50 utterances, two from the TIPI for each extreme trait and one about naturalness (250 judgments in total per subject). Subjects were not told that the utterances were intended to manifest extreme trait values.
 naive subjects for two communicative goals. For example, utterance 1.a projects a high extraversion through the insertion of an exclamation mark based on the model in Figure 6, whereas utterance 2.a conveys introversion by beginning with the filled pause err . The same utterance also projects a low agreeableness by focusing on negative propositions, through a low CONTENT POLARITY parameter value produced by the model in Figure 8. 4.3.1 Naturalness and Inter-Rater Agreement. Figure 11 shows the distribution of the nat-uralness ratings. A Shapiro-Wilk test confirms that the ratings are normally distributed ( W = . 97, p = . 31), with only one utterance out of 50 rated below 2.5 out of 7 on average.
The average naturalness is 3.98 out of 7, with a rating of 4 indicating neither agreement nor disagreement. This is lower than the naturalness scores obtained in Section 3.2 for the random training utterances collected using a small number of expert judges. 476
The differences in naturalness judgments could possibly be due to (a) the different set perceived as less natural; or (c) the fact that the expert judges made a very large number of judgments, and thus became accustomed to judging the outputs.
 the 276 pairs of judges. The level of agreement between the naive subjects reflects the difficulty of the personality recognition task for humans, thus providing an upper bound on the performance to be expected from a model trained on human data. The judges agree modestly, with correlations ranging from .17 (openness to experience) to .41 (emotional stability). This agreement is lower than that for rule-based utterances, which could be due to the nature of the personality cues conveyed by P
RB X  X  handcrafted parameters. However, this difference could also result from the use of naive judges, which we believe are less consistent in their personality judgments. 4.3.2 Modeling Accuracy. Modeling accuracy is measured using both the correlation and mean absolute error (on a scale from 1 to 7) between the target personality scores and the judges X  ratings. Given the relatively small number of distinct utterances being evaluated (20 per trait), it is important to take the non-determinism of P when evaluating the correlation between the target scores and the judges X  ratings. Eval-uating correlations over the 480 ratings of each judge and utterance pair is likely to result in an inflated significance level, as it does not account for the possibility that a specific outcome of P ERSONAGE  X  X  random generation decisions could produce the intended personality, rather than accurate personality modeling. In order to address this issue, we report correlations r avg between the target personality scores and the 20 personality number of distinct test utterances into account). Table 10 shows that extraversion is the dimension modeled the most accurately by the parameter estimation models, produc-478 ing a .80 correlation between the target extraversion and the average subjects X  ratings (p &lt;. 001). Emotional stability and agreeableness ratings also correlate strongly with the target scores, with correlations of .64 and .68, respectively (p &lt;. 005 and p &lt;. 001).
The correlation for openness to experience is also relatively strong (.41), although it is not significant at the p &lt;. 05 level (p = . 07). These correlations are unexpectedly high; in corpus analyses, significant correlations as low as .05 to .15 are typically observed between averaged personality ratings and linguistic markers (Pennebaker and King 1999; Mehl, Gosling, and Pennebaker 2006). Although each utterance is used to test two hypotheses (i.e., rated for two traits), results for extraversion, emotional stability, and agreeableness remain largely significant even after applying Bonferroni correction (p &lt;. 001, p &lt;. 005, and p &lt;. 005 respectively).
 target scores. The comparison with rule-based results in Table 11 suggests that this is not because conscientiousness cannot be exhibited in our domain or manifested in a single utterance, so perhaps this arises from differing perceptions of conscientiousness between the expert and naive judges. It is also possible that inconsistencies in the training data prevented the models from learning accurate cues for conscientiousness, as Table 5 shows that the judges disagreed the most over that trait when rating the training utterances.
 from 1 to 7. Such large errors result from the decision to ask judges to answer just the TIPI questions for the two traits that were the extreme targets, because the judges tend to use the whole scale, with normally distributed ratings (Shapiro-Wilk tests, p &gt;. 05).
This means that although the judges make distinctions leading to high correlations, the averaged ratings result in a compressed scale. This explains the large correlations despite the magnitude of the absolute error.
 on five target personality scores. Thus, the results show that individual traits are per-ceived even when utterances project other traits as well, as would be expected according to the Big Five theory.
 with ratings of 20 utterances generated with the rule-based parameter settings for each extreme of each Big Five trait (40 for extraversion, resulting in 240 rule-based utterances in total). Although rating differences could be due to the different set of judges, Table 11 suggests that the handcrafted parameter settings project a more extreme personality for 6 traits out of 10. However, the parameter estimation models are not significantly worse than the rule-based generator for neuroticism, disagreeableness, unconscientiousness, and openness to experience. Moreover, the differences between low and high average ratings for the parameter models shown in the right-hand-side of Table 11 are significant for all traits but conscientiousness (p  X  . 001). Thus parameter estimation models can be used in applications that only require discrete binary variation.
 within each group of extreme utterances. At the beginning of this section, we mentioned that the target scores used in the evaluation experiment were randomized according to a normal distribution around 1 or 7, with a standard deviation of 10% of the full scale (.60).
Figure 10 shows the distribution of the target scores for emotional stability. We compute the correlation between the target scores and the average judges X  ratings over each group of extreme target scores. 2 Table 12 shows that the naive subjects failed to detect the small variation within each group, although results are close to significance for the positive end of the emotional stability scale, with a correlation of .55 for the emotionally stable group (p = . 08). This suggests that parameter estimation models could model that trait with a high granularity given more training samples, as all that is needed is for our models to learn to trigger relevant personality markers within extreme regions of the stylistic space. For example, Figure 9 shows that the 480 predicted by the model in Figure 7 varies considerably with emotional stability. Given input conscientiousness and openness to experience scores of 4 out of 7, the parameter becomes enabled (i.e., above .5) for emotional stability scores below 1.81 (see middleleafnodeinFigure7).
 high accuracy reported in Table 10 is due to the successful modeling of large variations between each end of the scale, rather than the small-scale variations within one side of the dimension. Although these results are promising, future work should evaluate the granularity of parameter estimation models over the full range of the Big Five scales. 5. Discussion
This article presents a novel SNLG method based on parameter estimation models that estimate optimal generation parameters given target stylistic scores, which are then used directly by the base generator to produce the output utterance. We believe that dialogue applications such as spoken dialogue systems, interactive drama systems, and intelligent tutoring systems would benefit from taking individual stylistic differences into account, and that theories of human personality traits represent an appropriate set of dimensions for a computational model of this adaptation. Starting from a commu-nicative goal, we show how personality affects all phases of the language generation process, and that certain parameters such as the polarity of the content selected have a strong effect on the perception of personality. We present, to our knowledge, the first results of a human evaluation experiment using the perceptions of naive subjects to evaluate the stylistic variation of a language generator. The parameter estimation method is a computationally tractable generation method that does not require any overgeneration phase, and our results show that naive judges can recognize the in-tended system personality for most traits.
 (OS) framework (Langkilde and Knight 1998; Walker, Rambow, and Rogati 2002; Isard,
Brockmann, and Oberlander 2006), our results show that direct parameter optimization is a viable alternative for stylistic control. Given the cost of an overgeneration phase, parameter estimation methods might be the only alternative for real-time generation, which is necessary for spoken dialogue interaction. The fact that the accuracy of OS methods depends on the complexity of the overgeneration phase prevents them from scaling to the large generation space required for stylistic variation. For example, an ex-haustive search over P ERSONAGE  X  X  output space would require more than 2 ter combinations, for each of which an utterance would have to be generated and scored. ing candidates throughout the generation process, as well as by using compact data structures X  X uch as in the OpenCCG chart realizer (White, Rajkumar, and Martin 2007) X  X lthough we do not know of any SNLG system using this approach. The main advantage of the OS approach is that it can model global utterance features reflecting multiple generation decisions as well as input dependencies (e.g., sentence length). The predictive accuracy of scoring models trained on our data discussed in the introduction suggests that OS methods could be useful for stylistic SNLG when limited to a mild data-driven overgeneration phase, e.g., to rerank candidate utterances produced by stochastic parameter estimation models. The OS approach therefore can be seen as a way to relax the parameter independence assumptions required to learn robust models. domain. In contrast with out-of-domain corpus based methods (Isard, Brockmann, and
Oberlander 2006), in-domain personality annotations not only reduce data sparsity by constraining the range of outputs, they also allow us to explicitly model the relation between stylistic factors and generation decisions. The parameter estimation approach is inspired by previous work by Paiva and Evans (2005), who present a data-driven method for stylistic control that does not require an overgeneration phase. We extend this work in multiple ways. First, we focus on the control of the speaker X  X  personality, rather than stylistic dimensions extracted from corpora. Second, we present a method for learning parameter estimation models predicting generation decisions directly from input personality scores, whereas the method of Paiva and Evans (2005) requires a search for the optimal generation decision over the model X  X  input space. Third, we present a perceptual evaluation of the generated stylistic variation, using naive human judges.
 these traits are widely accepted as the most important dimensions of behavioral vari-ation within human beings (Norman 1963; Goldberg 1990). Collecting reliable per-sonality annotations for in-domain utterances is a non-trivial task, as reflected by the moderate inter-rater agreement reported in Section 4.3.1. The difficulty is increased by the use of random generation decisions, which is necessary to ensure that the learned relation between personality ratings and generation parameters is not due to artificial correlations between generation decisions. One way to increase inter-rater agreement would be to reduce the number of parameters varied simultaneously for each utterance, thereby decreasing the chances of observing inconsistent markers. While this would require collecting a larger training set to model the same number of parameters, one could filter out irrelevant parameters in a first data collection phase, to focus on data quality in a second phase. A second way to improve the quality of the annotations would be to increase the size of each text sample, by simultaneously rating multiple utterances generated from the same parameters, rather than a single utterance. This is likely to produce models that are more robust to input variation, given a small increase in annotation effort. Despite the difficulty of the annotation task, our experiments show that parameter estimation models are able to detect a large range of personality markers in our training set. The fact that those markers are successfully recognized by naive judges suggests that our approach is robust to ambiguities in the data.
 differ in terms of their impact on linguistic production. Results suggest that perceptions of agreeableness and extraversion are easier to model, whereas conscientiousness and openness to experience are more difficult. A possible explanation is that these traits are not conveyed well in P ERSONAGE  X  X  narrow domain. However, previous personality recognition results suggest that observed openness to experience is difficult to model using general conversational data as well (Mairesse et al. 2007). It is therefore possible that this trait may not be expressed through spoken language as clearly as other traits. correlate strongly with the target personality scores for all traits but conscientiousness, with correlation coefficients ranging from .41 (openness to experience) up to .80 (extraversion). It is important to note that the magnitudes of the model correlations are high compared to traditional correlations between personality and linguistic markers, which typically range from .05 to .10 (Pennebaker and King 1999; Mehl, Gosling, and Pennebaker 2006). This is possibly due to our experimental method, which simultaneously tests a small number of personality markers in a controlled experiment, whereas such markers are harder to extract from the varied language samples used in psychology studies. 482 psychological studies, some of them are not modeled successfully by the parameter es-timation approach, and thus were omitted from Tables 6 and 7. This could be due to the relatively small training set size (160 utterances to optimize 67 parameters). However, even with a larger training set, it is possible that some of the parameters in Table 2 are not perceivable in our domain. Additionally, the parameter-independence assumption of the PE method could be responsible for the poor accuracy of some models. While this issue could possibly be resolved by training statistical models that simultaneously predict multiple dependent variables (e.g., structured prediction methods), this would exponentially increase the size of the parameter prediction space and further aggravate data sparsity issues.
 jecting extreme traits than the rule-based generator in the same domain (see Sec-tion 4.3.2). We believe this is due to the lack of extreme utterances in the training data to learn from. Future work should consider whether it is possible to flatten the distribution of training data, perhaps using more knowledge or active learning methods for NLG (Mairesse et al. 2010).
 a generation dictionary containing syntactic templates that, in some cases, express various pragmatic effects, even though most of P ERSONAGE automatically. Our dictionary is currently handcrafted, but future work could build on research on methods for extracting the generation dictionary from data (Barzilay and Lee 2002; Higashinaka, Walker, and Prasad 2007; Snyder and Barzilay 2007).
 models could be improved with a larger sample of judges and random utterances at development time. A larger number of judges would also smooth out rating inconsis-tencies and individual differences in personality perception, thus allowing the direct modeling of laypeople X  X  perceptions by removing the need for expert judges.
 language generator and the dialogue manager and text-to-speech (TTS) engine, because personality also affects aspects of dialogue such as initiative and prosodic realization (Scherer 1979; Vogel and Vogel 1986). It might be possible to apply a similar methodol-ogy to the parameterization of a dialogue manager and a TTS component, in order to project a consistent personality to the user, or to use a reinforcement learning approach to train the dialogue manager (Walker, Fromer, and Narayanan 1998; Singh et al. 2002), with personality judgments as the objective function. Our results suggest that person-ality can be recognized by manipulating the linguistic cues of a single utterance, but it is likely that additional cues (e.g., characterizing the system X  X  dialogue strategy) could only make the user X  X  perception of the system X  X  personality more robust.
 handcrafted methods. Firstly, SNLG methods can scale more easily to other domains and other types of stylistic variation, as collecting data requires less effort than tuning a large number parameters. In order to apply our method to a new domain or task, the base generator would have to be modified. Although P ERSONAGE with domain independence in mind by using general parameters, handling a new com-municative goal (e.g., a user request) would require modifying the syntactic aggregation operations as well as the pragmatic marker insertion rules. Porting P a new information presentation domain would only require modifying the syntactic templates in the generation dictionary. Once a base generator is available in the target domain, it can be used to collect ratings for any stylistic dimension. Our parameter estimation method can then learn to control the generator X  X  output from data, without any further handcrafting or parameter tuning. A second advantage of our parameter estimation method over handcrafted approaches is that it can learn to target scalar combinations of the Big Five traits, whereas handcrafting all possible ways in which multiple stylistic dimensions can affect output utterances requires considerable effort.
Thirdly, our parameter estimation method can easily model continuous stylistic dimen-sions, while producing such models by hand is not tractable. For example, one could interpolate various handcrafted parameter sets to handle continuous input scales (e.g., to generate an utterance that is 75% extravert); however, the various weights can only be learned reliably from data.
 antee the naturalness of the output, but that data-driven models can be used to control various pragmatic effects. As naturalness depends on the combination of multiple generation decisions, modeling naturalness in the PE framework would require taking inter-parameter dependencies into account. At training time, this can be achieved by extending each PE model with independent variables for naturalness as well as all previous generation decisions made. During the generation process, the models would predict future generation decisions given all previous decisions and a high input natu-ralness value. Although we believe this approach is tractable, it optimizes parameters sequentially in a greedy fashion. A global optimization method would require jointly optimizing all (past and future) parameters simultaneously. However, predictions in such a large output space would require a much larger training set. Nevertheless, we believe that future work should evaluate methods for jointly optimizing multiple NLG decisions.
 system, but an important next step is to use these techniques together with personality-based user modeling methods (Mairesse et al. 2007) to simultaneously model the personality of the user and the system in dialogue. Both models provide the tools for testing various hypotheses regarding personality-based alignment, such as the similarity X  X ttraction effect (Isard, Brockmann, and Oberlander 2006). Furthermore, the optimal personality of the system is likely to be application-dependent; it would thus be useful to evaluate how the user X  X  and the system X  X  personality affect task performance in different applications.
 cam.ac.uk/  X  farm2/personage , as well as the personality-annotated corpus col-lected for our experiments. An on-line demonstrator and a tutorial for customiz-ing P ERSONAGE for a new domain can be found at http://people.csail.mit.edu/ francois/research/personage .
 Acknowledgments References 484 486
