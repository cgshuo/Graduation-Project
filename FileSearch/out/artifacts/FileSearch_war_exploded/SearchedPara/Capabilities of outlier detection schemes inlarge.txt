 REGULAR PAPER Jian Tang  X  Zhixiang Chen  X  Ada Waichee Fu  X  David W. Cheung Abstract Outlier detection is concerned with discovering exceptional behaviors tion for some important applications such as credit card fraud detection, discov-ering criminal behaviors in e-commerce, discovering computer intrusion, etc. In this paper, we first present a unified model for several existing outlier detection schemes, and propose a compatibility theory, which establishes a framework for describing the capabilities for various outlier formulation schemes in terms of matching users X  intuitions. Under this framework, we show that the density-based scheme is more powerful than the distance-based scheme when a dataset con-tains patterns with diverse characteristics. The density-based scheme, however, is less effective when the patterns are of comparable densities with the outliers. We then introduce a connectivity-based scheme that improves the effectiveness of the density-based scheme when a pattern itself is of similar density as an outlier. We compare density-based and connectivity-based schemes in terms of their strengths and weaknesses, and demonstrate applications with different features where each of them is more effective than the other. Finally, connectivity-based and density-based schemes are comparatively evaluated on both real-life and synthetic datasets in terms of recall, precision, rank power and implementation-free metrics. Keywords Outlier detection  X  Scheme capability  X  Distance-based outliers  X  Density-based outliers  X  Connectivity-based outliers  X  Performance metrics 1 Introduction Outlier detection is concerned with discovering exceptional behaviors of certain objects. Revealing these behaviors is important since it signifies that something out of the ordinary has happened and shall deserve people X  X  attention. In many cases, such exceptional behaviors will cause damages to the users and must be stopped. Therefore, in some sense detecting outliers is at least as significant as discovering general patterns. Outlier detection schemes lay a foundation in many applications, for instances, calling card fraud in telecommunications, credit card fraud in banking and finance, computer intrusion in information systems [ 13 , 15 , 25 , 32 ],tonameafew.
 outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism. X  Following the spirit of this definition, researchers have proposed various schemes for outlier detec-tion. A large amount of the work was done under the general topic of cluster-ing [ 14 , 16 , 27 , 31 , 34 ], where clustering algorithms are used to detect outliers gorithms to detect outliers is based on the understanding that outliers and clus-cluster, and a cluster object shall not be an outlier. For example, the DBSCAN clustering algorithm in [ 14 ] explicitly defines data objects outside any density-based clusters as outliers (or noises). However, the outliers discovered this way are highly dependent on the clustering algorithms used and hence subject to the clusters generated. In [ 9 ], we presented some formal study about complemen-based clustering method DBSCAN. Interestingly, we showed that in some re-stricted cases outliers and cluster objects are indeed complemental. However, algorithm.
 developed in the field of statistics [ 6 , 18 ]. These methods normally assume some knowledge about the underlying distribution of a dataset. In reality, however, prior knowledge about the distribution of a dataset is not always obtainable. Besides, these methods do not scale well for even modest number of dimensions as the size of the dataset increases.
 posed that are not subject to any clustering algorithms, and do not require any prior knowledge about the underlying distributions of the dataset. These can be basically categorized into distance-based schemes [ 2 , 5 , 23 , 24 , 28 ]and density-based schemes [ 8 , 21 ]. 1 Distance-based schemes are originated from the proposal in [ 23 ], called DB ( n ,v) -outliers, where n and v are parameters. Let D be the dataset. For any p  X  D and any positive value v ,define N v ( p ) = { o : dist ( p , o )  X  v &amp; o = p &amp; o  X  D } (called the v -neighborhood of p ). If | is not. A prominent variation of the distance-based scheme is proposed in [ 28 ], called ( t , k ) -nearest neighbor scheme. For each object, its k -distance is defined as the distance to its k -nearest neighbor(s). Among all the objects, the top t with the maximum k -distances are outliers. As will be noted in later sections, this scheme is actually a special case of DB ( n ,v) -outliers.
 The reachability distance of p with respect to o for k is defined as: The reachability distance smooths the fluctuation of the distances between p and its  X  X lose X  neighbors. The local reachability density of p for k is defined as: jects in its k -distance neighborhood. The local outlier factor (LOF) of p is defined as The value on the right side is the average fraction of the reachability densities of p  X  X  k-distance neighbors and that of p . Thus, as pointed out in [ 8 ], the lower the density of p , or the higher the densities of p  X  X  neighbors, the larger the value of LOF k ( p ) , which indicates that p has a higher degree of being an outlier. categorize the objects into either outliers or non-outliers. The LOF value of an object measures how strong it can be an outlier. However, when explicit classifi-cation is desirable, we can choose a cut-off threshold to determine whether or not an object is an outlier, depending on whether its LOF value is less than the cut-off threshold or not. In the following, therefore, we assume that the cut-off threshold is also a parameter.
 first use a generic model to represent distance-based and density-based schemes, and then propose a compatibility theory, which is a list of gradually relaxing cri-teria for describing the capabilities of a scheme to match users X  intuitions. We then introduce a new scheme, called the connectivity-based outlier factor (COF) scheme, for outlier formulation. We compare COF and LOF schemes with the cri-teria developed in the compatibility theory, and use empirical analysis to demon-strate applications with different features where one scheme is more effective than the other.
 model for outlier detection and introduce the compatibility theory. In Sect. 3, we use the compatibility theory to evaluate the capabilities of the distance-based and the density-based schemes. In Sect. 4, we introduce the connectivity-based COF scheme, and compare it with the density-based LOF scheme. In Sect. 5, we intro-duce recall, precision, rank power and implementation-free metrics for evaluating the performance of an outlier detection scheme. We report evaluation results of the COF scheme in comparison with the LOF scheme, based on experiments on both real-life and synthetic datasets. Execution time and scalability results are also pre-sented. Finally, in Sect. 6, we conclude the paper by summarizing the main results and introducing problems for future research. 2 A framework for the capabilities of outlier detection schemes 2.1 A generic model for outlier detection All the outlier detection schemes mentioned in the previous section contain some parameterized conditions applied to individual objects in a dataset. Let D be a multi-dimensional dataset. In general, we define an outlier detection scheme on D as a condition Cond(o; P) ,where o  X  D and P is a parameter set. A given assignment of values to the parameters in P is called a parameter setting .Inthe subsequent discussions, we use sets of values to denote parameter settings. (For and y = 2.) For a given parameter setting P , an object o  X  D is an outlier if an outlier detection scheme (or simply a scheme )for D .
 v = ( k-distance in the dataset. 2 nearest neighbor scheme is a special case of the DB ( n ,v) -outlier scheme, thus we will not discuss it separately. 2.2 Two levels of outlying properties Outliers are identified by their distinct properties. These properties may be con-ceptual , e.g., unauthorized usage of credit cards, or physical , e.g., the amount in-volved per transaction. Conceptual properties exist in users X  minds, while physical properties are described by input data. A programmable scheme can use physical properties for outlier detection. These are included as the operands in the condi-tions in the generic model mentioned earlier. It is worth noting here that a user having an expectation of outliers/non-outliers does not mean he/she knows where they are in the dataset. It merely implies that according to the conceptual proper-ties he/she has about the outliers, each object has a unique interpretation, either an outlier or a non-outlier.
 Definition 2.1 Let D be a finite dataset. An expectation of D is a partition D = D respectively.
 their physical properties. For example, whether the credit card usage is legal may not be corresponding to any combination of the amount involved, the timing for the transaction, the user account, etc. Thus, the outliers detected by any specific scheme may not always match what a user expects. 3 which a user invokes that scheme. In the general case, different parameter settings identify different sets of outliers in the dataset. Given a user X  X  expectation, if the sets of outliers and non-outliers can be identified by some parameter setting of a scheme, then we say that the scheme has the capability of detecting outliers as expected by the users, otherwise, it does not. Consider a dataset that contains two patterns, C 1 and C 2 , and an outlier o from the user X  X  expectation. (Thus, o does not belong to either pattern.) Suppose o has a set of physical properties P 1 that dis-tinguishes it from C 1 , and another set of physical properties P 2 that distinguishes it from C 2 . Ideally, a scheme has the ability to detect o if and only if it can use a single parameter setting to describe both properties. This may be not possible, however, if the scheme envisions some conflict between P 1 and P 2 , which can-not be resolved by any parameter setting. But in some cases, the conflict may be resolved separately by two parameter settings. This implies that the scheme has been weakened. This motivates the following framework. 2.3 Compatibility requirements Again, we let D be a dataset, and D o and D n be an expectation for D . Definition 2.2 An outlier detection scheme Cond ( o ; P ) for D is ON-compatible with the expectation D o and D n if there exists a parameter setting S, such that (1)  X  p  X  D o [ Cond ( p ; S ) = true ] , and (2)  X  p  X  D n [ Cond ( p ; S ) = false ] . izes the ideal capability that any scheme can achieve, in terms of matching the user X  X  expectation.
 Definition 2.3 An outlier detection scheme Cond ( o ; P ) for D is O-compatible with the expectation D o and D n if there exists a sequence of parameter settings
S (1)  X  p  X  D o  X  i  X  X  1 ,..., m }[ Cond ( p ; S i ) = true ] , and (2)  X  p  X  D n  X  j  X  X  1 ,..., m }[ Cond ( p ; S j ) = false ] .
 fied by all the settings, and each non-outlier is identified by at least one setting. Thus, different settings can be used to accommodate patterns with diverse charac-teristics. O-compatibility has a dual, called N-compatibility, defined as follows. Definition 2.4 An outlier detection scheme Cond ( o ; P ) for D is N-compatible with the expectation D o and D n if there exist a sequence of parameter settings
S (1)  X  p  X  D n  X  i  X  X  1 ,..., m }[ Cond ( p ; S i ) = false ] , and (2)  X  p  X  D o  X  j  X  X  1 ,..., m }[ Cond ( p ; S j ) = true ] .
 a sense O-and N-compatibilities are symmetric. With O-compatibility, all the outliers have compatible properties with respect to each pattern, whereas with N-compatibility, each outlier has compatible properties with respect to all the pat-terns. The results derived for one can be reformulated for the other. In the follow-ing discussions, therefore, we will mainly consider O-compatibility.
 Definition 2.5 An outlier detection scheme Cond ( o ; P ) for D is an O-cover for the expectation D o and D n if there exists a parameter setting S such that  X  p  X  D such that  X  p  X  D n [ Cond ( p ; S ) = false ] .
 tation, but may have  X  X alse positives. X  That is, the scheme may misclassify some non-outliers as outliers. Similarly, the N-cover implies that a scheme will detect all non-outliers for a specific expectation, but may have  X  X alse negatives. X  That is, the scheme may misclassify some outliers as non-outliers.  X  X alse positives X  or  X  X alse negatives X  may occur when the conceptual properties of the outliers grossly mis-match their physical properties. For example, if the credit card transaction made by an illegal user turns out to be quite normal when the involved amount, timing, frequency, etc. are viewed separately, then a scheme that examines these properties in an isolated fashion may not detect the transaction made by the illegal user. defined earlier.
 Lemma 2.1 If an outlier detect scheme is ON-compatible, then it is both O-compatible and N-compatible. If it is O-compatible (N-compatible), then it is an O-cover (N-cover).
 2.2  X  2.5 .
 parameter selection from the capability of a scheme. In the general case, error rates depend on specific parameter values used by a scheme, and a high error rate may simply mean that the specific parameter values are not right, not necessarily an indication of the quality of the underlying outlier detection scheme. 4 scheme is not compatible.
 Theorem 2.1 An outlier detection scheme Cond ( o ; P ) for D is not ON-compatible with any given expectation D o and D n if for any parameter setting S, there exist a  X  D o and b  X  D n such that Cond ( a ; S ) = true  X  Cond ( b ; S ) = true. the two properties in Definition 2.2 . For the setting S , the condition of the theo-rem implied that there are a  X  D o and b  X  D n such that Cond ( a ; S ) = true  X  Cond ( b ; S ) = true . Property (1) in Definition 2.2 indicates that Cond ( a ; S ) = 2.2 .
 Theorem 2.2 Let S be the set of all the parameter settings for a given outlier detection scheme Cond ( o ; P ) for D . Then the following hold: (1) Cond ( o ; P ) is not O-compatible with an expectation D o and D n if (2) It is not N-compatible with the expectation D o and D n if contrary. Then Cond ( o ; P ) is O-compatible with D o and D n .Let S 1 ,..., S m be the sequence of settings mentioned in Definition 2.3 . Thus, by property (1) of Defi-nition 2.3 ,forany i with 1  X  i  X  m and any p  X  D o ,wehave Cond ( p ; S i ) = true . For the given object a  X  D n in the first condition of the theorem, by property (2) of Definition 2.3 , there is a j with 1  X  j  X  m such that Cond ( a ; S j ) = false .Since we have Cond ( p ; S j ) = true for every object p  X  D o ,wehave Cond ( b ; S j ) = true for the particular object b  X  D o given in the first condition of the theorem. Hence, by this condition, we have Cond ( a ; S j ) = true , a contradiction to the fact Cond ( a ; S j ) = false devised earlier.
 earlier to evaluate the capabilities of various outlier detection schemes. 3 Evaluating capabilities of outlier detection schemes smallest distances, respectively, from o to the points in C . (We will simply write max { k-distance ( r ) : r  X  C } ,and k-distance min ( C ) = min { k-distance ( r ) : r  X  C } . For any two datasets C 1 and C 2 , dist ( C 1 , C 2 ) = min { dist ( p , q ) : p  X  C 1 &amp; q 3.1 Distance and density skewed datasets As discussed earlier, the effectiveness of an outlier detection scheme depends on the characteristics of datasets, the extent to which the physical properties of data can match the conceptual properties, and the way the physical properties are used by the scheme. In most practical datasets, the distances between objects, and the densities in the vicinities of objects are the two commonly available physical prop-erties. For a given dataset, if at the conceptual level, both distances and densities may not match the physical properties of the data, then such a dataset is termed a DD-skewed dataset. ( DD stands for D ensity and D istance).
 nearby dense pattern C 1 , and a more distant sparse pattern C 2 . Apparently, what distinguishes o from C 1 is the density, whereas what discriminates o from C 2 is the large distance between the two. This is because the local density of o is not lower than C 2 , and hence is not a factor to disqualify o from being a member of C 2 . DD-skewed datasets arise often in practices when data are generated from populations with a mixture of distributions. 3.2 The capability of the DB ( n ,v) -outlier scheme on DD-skewed datasets This scheme essentially uses a parameter setting to define a size for the neighbor-ing area for any object o , and an upper bound on the (average) density of the area. Intuitively, it works best for outliers possessing low neighborhood densities, but may be awkward for outliers with other properties. Thus, it is no surprising that its capability is limited on DD-skewed datasets. To make the formal derivation possible, we quantify some values in the dataset in Fig. 1 , as described in Dataset Example 1. 3.2.1 Dataset Example 1 We assume that the dataset in Fig. 1 satisfies the following three conditions. 1. 1-distance max ( C 1 )&lt; 1 2 dist ( o , C 1 ) 2.  X  d ,0 &lt; d  X  dist max ( o , C 1 ) ,  X  r  X  C 2 , | N d ( r ) | X | N d ( o ) | 3.  X  d , d &gt; dist max ( o , C 1 ) ,  X  t  X  C 1 , | N d ( t ) | X | N d ( o ) | half of the distance between o and C 1 . Condition 2 specifies a range for the radius for which o has a higher density than that of C 2  X  X  members. Condition 3 describes what happens in large neighboring areas. That is, the neighborhood density of o will exceed that for some members of C 1 . This is because the distance between o and C 2 is smaller than that between C 1 and C 2 . Thus, for areas with the radius in the specified range in condition 3, o will always get at least as many neighbors as the points in C 1 can get.
 following results.
 Result 3.1 For the DD-skewed dataset described in Dataset Example 1, the DB ( n ,v) -outlier scheme is not ON-compatible with the earlier expectation D o and D n . 6 n . Now consider the case of v&gt; dist max ( o , C 1 ) . By condition 3 of Dataset Exam-two cases mentioned earlier that for any parameter setting ( n ,v) , there are o  X  D o Result 3.1 follows from Theorem 2.1 .
 O-compatible if, compared with the dense pattern, the sparse pattern is sufficiently large and far away from o , as described in the following result.
 Result 3.2 For the DD-skewed dataset in Dataset Example 1, if in addition | DB ( n ,v) -outlier scheme is O-compatible with the expectation D o and D n . ample 1, | N v 1 ( o ) |= 0 &lt; n 1 ,andforall r  X  C 1 , | N v 1 ( r ) | X  n 1 = 1.  X | C sequence in Definition 2.3 , hence Result 3.2 follows.
 dataset if, with respect to each non-outlier pattern, all the outliers possess lower densities for at least one radius. In Fig. 1 , for example, if there is another out-lier near C 1 , and far away from C 2 , then O-compatibility would still hold, since they both would have lower densities with respect to C 1 in small radius, and also have lower densities with respect to C 2 in a larger distance (i.e., v 2 in the proof mentioned earlier). In cases where this condition is not true, the results become unpredictable. In the following, we present a dataset example where the dataset does not meet this criterion, and the DB ( n ,v) -outlier scheme is not O-compatible. Consider the dataset in Fig. 2. Its description is given in Dataset Example 2. 3.2.2 Dataset Example 2 The dataset in Fig. 2 contains two dense patterns C 1 and C 2 ,andasparsepattern C , all being uniformly distributed, and two outliers o 1 and o 2 . The dataset meets the following conditions. 1. 3 4 | C 3 | X | C 1 | &lt; | C 3 | &lt; | C 2 | 2. 4  X  1-distance max ( C 1 )&lt; dist ( o 1 , C 1 ) 3.  X  d ,0 &lt; d  X  dist max ( o 1 , C 1 ) ,  X  o  X  C 3 , | N d ( o ) | X | N d ( o 1 ) | 4. 4  X  1-distance max ( C 2 )&lt; dist ( o 2 , C 2 ) 5. diam ( { o 1 } X  C 1 )&lt; diam ( { o 2 } X  C 2 )&lt; 1 2 diam ( C 3 ) 6.  X  o  X  C 3 , | N 1 7.  X  o  X  C 3 , dist ( o , o 2 )&lt; dist ( o , C 2 ) 9. diam ( { o 1 } X  C 1  X  X  o 2 } X  C 2 )&lt; dist ( { o 1 } X  C 1  X  X  o 2 } X  C 2 , C 3 ) conditions 2 X 4, 6, and 7, constrain the relative densities of the neighboring areas of the specified objects. For instance, condition 3 states that with the specified range of radius, the neighboring area of o 1 is denser than that of any point in C 3 . Condition 6 requires that the sphere with a radius of 1 2 diam ( C 3 ) centered at any point in C 3 may not contain three quarter or more of the total points in C 3 .The second group, conditions 1, 5, 8, and 9, exert group-based constraints. For exam-ple, condition 1 describes the relative sizes of groups, while the others constrain the diameters and inter-group distances. These conditions reflect the geometric structures depicted in Fig. 2.
 dataset D in Dataset Example 2. We have the following assertion.
 Result 3.3 The D B ( n ,v) -outlier scheme for the dataset described in Dataset Example 2 is not O-compatible.
 lower densities simultaneously than C 3 for any single radius. This is because for small to medium radius, the neighborhood of o 1 is denser than C 3 , and for large radius the neighborhood of o 1 may be likely sparser than C 3 , and all the points in C 2 will fall into the neighborhood of o 2 , making it denser than C 3 .
 3.3 The capability of density-based schemes on DD-skewed datasets The cardinality of the neighboring area of an object, as used by the DB ( n ,v) -outlier scheme, can be thought as the  X  X bsolute X  density of the object. Density-based schemes, on the other hand, use the relative density of an object, i.e., the comparison of its own density with the densities of its neighbors in a locality. The properties of any points or a group of points beyond this locality will not affect its relative density. This makes it more resilient to the skewness than the DB ( n ,v) -outlier scheme. We shall use Dataset Example 2 to exhibit our evaluation of the LOF scheme.
 in the following result. The first condition is just a quantification of the assumption of the uniform distribution for C i for i = 1 , 2 , 3. The second is reasonable with respect to the geometric structure of the dataset in Fig. 2. (Note that these two additional conditions do not conflict with those specified in Dataset Example 2.) Result 3.4 Assume that the dataset in Dataset Example 2 satisfies two additional conditions: (a) 1-distance min ( C i ) 1-distance (b) | N 1 ( o j ) |= 1 for j = 1 , 2 .
 Then, the LOF scheme is ON-compatible.
 k will not change the proof in a fundamental way, but will nonetheless make the analysis more tedious. The idea is that, first, the nearest neighbor of o 1 must be-long to C 1 . Then, since the distance between o 1 and C 1 is much larger than the 1-distance of any point in C 1 ,thevalueof LOF 1 ( o 1 ) will be larger. On the other hand, since the 1-distances of points in C i ,for i = 1 , 2 , 3, are roughly the same, the LOF values of these points will be small. Based on this observation, we can set a lower bound for LOF 1 ( o 1 ) and an upper bound for the LOF value for any point in C 1 ( C 2 or C 3 ) and show that the former is larger than the latter. Similar arguments can be applied to o 2 .
 3.4 Limitation of the density-based scheme One weakness of the density-based scheme is that it may rule out outliers that are shifting from a low-density pattern. To understand the problem, let us first take a closer look at the concept of pattern. According to the Concise Oxford Dictionary , a pattern is  X  X  regular or logical form, order or arrangement of parts ...  X  We observe that although a high density can reflect such a logical form, order or arrangement, it nonetheless is not a necessary condition, at least in the form defined in the current literature. As a result, an outlier does not always have to be of a lower density than the pattern it deviates.
 points as shown in Fig. 3, where D o ={ o } X  C 2 . The pattern, C 1 , is a straight line, which is of low density in a two-dimensional space. Since the outlier o shifts away from a low-density pattern, the density-based scheme will not be very effective to identify it, unless we use a small k . On the other hand, using too small a k will rule out the outliers in C 2 , which can only be identified using a value for k larger than its cardinality. In Sect. 4.3.2, we will show the ineffectiveness of the LOF scheme in handling a similar case.
 density patterns such as the line of points in Fig. 3, while at the same time does not compromise detecting a group of staying-together outliers like those in C 2 in Fig. 3. 4 Connectivity-based outlier detection 4.1 Motivation To cope with outliers with respect to low-density patterns, we differentiate  X  X ow density X  from  X  X solativity. X  While low density normally refers to the fact that the number of points in the  X  X lose X  neighborhood of an object is (relatively) small, isolativity refers to the degree that an object is  X  X onnected X  to other objects. As a result, isolation can imply low density, but the other direction is not always true. For example, in Fig. 3 point o is isolated, while any point p in C 1 is not. But both of them are of roughly equally low density. In the general case, a low-density outlier results from deviating from a high-density pattern, and an isolated outlier results from deviating from a connected pattern.
 mensional structures. For example, a pattern shown in Fig. 3 is a line in the two-dimensional space. The isolativity of an object, on the other hand, can be described by the distance to its nearest neighbor. In the general case, we can also talk about the isolativity of a group of objects, which is the distance from the group to its nearest neighbor. 4.2 Concepts and definitions In the following definitions, the function dist () has the same meaning as that de-fined in the previous sections, and G ={ p 1 , p 2 ,..., p r } is a subset of dataset D .
 Definition 4.1 Let P , Q  X  D ,P  X  Q = X  and P , Q = X  . For any given q  X  Q, we say that q is the nearest neighbor of P in Q if dist ( q , P ) = dist ( Q , P ). Definition 4.2 A set-based nearest path, or SBN-path, from p 1 on G is a sequence p a nearest neighbor of set { p 1 ,..., p i } in { p i + 1 ,..., p r } .
 ative expansion process. In each iteration, it picks up its nearest neighbor among the remaining objects. If its nearest neighbor is not unique, we can impose a pre-defined order among its neighbors to break the tie. Thus, an SBN -path is uniquely determined. An SBN -path indicates the order in which the nearest objects are pre-sented.
 Definition 4.3 Let s = p 1 , p 2 ,..., p r be an SBN-path from p 1 on G. A set-based nearest trail, or SBN-trail, with respect to s is a sequence e 1 ,..., e r  X  1 e i an edge and the sequence dist e the SBN -trail is unique for any SBN -path.
 Definition 4.4 Let s = p 1 , p 2 ,..., p r be an SBN-path from p 1 on G, and e = e from p 1 on G, denoted by ac-dist G ( p 1 ) ,isdefinedas description of the SBN -trail for the SBN -path from p 1 on G . Since this cost de-scription is unique for p 1 , our definition is well defined. Rewriting and viewing the fraction following the summation sign as the weight, this value can then be viewed as the average of the weighted distances in the cost description of the SBN -trail. Note that larger weights are assigned to the earlier terms in the SBN -trail. Thus, the edges closer to p 1 contribute more to ac-dist G ( p 1 ) than the ones farther away. As a result, a point shifting away more from a pattern is likely to have a greater ac-dist . It is easy to see that, when all the dist ( e i ) are equal, ac-dist G ( p ) = dist ( e i ) for all p  X  G .
 notations, we let ac-dist k ( p ) = ac-dist N k ( p )  X  X  p } ( p ) .
 Definition 4.5 Let p  X  D and k be a positive integer. The connectivity-based outlier factor (COF) at p with respect to its k-nearest neighborhood is defined as We define the connectivity-based outlier detection scheme (or simply the COF scheme) under the generic outlier detection model as Cond ( p ;{ k , u } )  X  and only if Cond ( p ;{ k , u } ) is true.
 ing distance from p on N k ( p ) and the average of the average chaining distances from p  X  X  k -distance neighbors to their own k -distance neighbors. It indicates how far away a point shifts from a pattern. We now use an example to highlight the motivation behind it. 4.2.1 Dataset Example 3 Consider the dataset in Fig. 4. The pattern is a single line and two points shift away from it. Suppose dist ( 1 , 2 ) = 5 , dist ( 2 , 7 ) = 3 , and the distance between any two adjacent points in the line is 1. Let k = 10. We now calculate the average chaining distances for three sample points to show how the COF values of those sample points reflect  X  X hifting from pattern X  in an appropriate way.
 N The SBN -trail for s 1 is tr 2 . 05 .
 N The SBN -trail for s 2 is tr 1 . 46 .
 N The SBN -trail for s 3 is tr 0 . 98 .
 lated similarly. The previous results show that for points that shift more from the pattern, such as points 1 and 2, the first few items in their cost description lists (or SBN-trails) tend to be larger than those for points that shift less, such as point 7. Since earlier items in a cost description list are assigned larger weights, they con-tribute more to the corresponding average chaining distance, which is the weighted sum of the values in the cost description. Thus, strongly shifted points will have larger average chaining distances than weakly shifted ones. In the general case, most points in the k -nearest neighborhood of a strongly shifted point should have small average chaining distances. This results in a larger connectivity-based out-lier factor for such a strongly shifted point. On the other hand, for a weakly shifted point, most points in its k -nearest neighborhood should have comparable average chaining distance values, resulting in a smaller connectivity-based outlier factor for such a point. The weakest shifted points are those that belong to the pattern itself. Their connectivity-based outlier factors should be close to 1. For the three sample points in the previous example, for k = 10, we have the following: 4.3 Capabilities of the COF scheme vs. the LOF scheme We first show that, like the LOF scheme, the COF scheme is resilient to DD-skewed datasets. We then show that the COF scheme is more robust than the LOF scheme in detecting outliers with respect to low-density patterns. Finally, we use experimental results to show the strengths and weaknesses of both COF and LOF schemes. 4.3.1 Detecting outliers in DD-skewed datasets Again, we use the dataset in Fig. 2 to illustrate an example of DD-skewed datasets. Our result is given in the following assertion.
 Result 4.1 Under the same conditions given in Result 3.4 ,theCOFschemeis ON-compatible.
 It differs only in the details of calculation.
 4.3.2 Detecting outliers with respect to low-density patterns We have used the dataset in Fig. 3 as a typical example of outliers shifting away from low-density patterns. In order to simplify the analysis, we reshape the dataset but still retain its basic characteristics. The resulting dataset is shown in Fig. 5 . 4.3.3 Dataset Example 4 radius of 1. Distances between any adjacent points on the circle are the same. C 1 contains 91 points lying on two straight lines l 1 and l 2 . The two lines meet at the the x -axis. C 1 contains p and 45 points on each of the lines l 1 and l 2 . Moreover, the distance between any two adjacent points on each line is ( 23 , 0 ) . According to Hawkins X  definition, it is easy to understand that point o and the points in C 2 are outliers while others are not. Thus, the expectation is D = D Result 4.2 For the dataset given in Dataset Example 4, the LOF scheme is not ON-compatible for the expectation D o and D n .
 non-outlier points p = ( 20 , 0 ) and q = ( 65 , 45 ) from D n and two outlier points w = ( 0 , 0 ) and o = ( 23 , 0 ) from D of points in the dataset is 100. We calculated the LOF values for all those four points for k = 1 , 2 ,..., 99. The calculation was done by a C ++ program with a precision of 10 decimal digits. The computing environment is a Dell Inspiron 8100 Pentium 1GHz laptop with 512 MB RAM and 20 GB HD. The LOF values of the four points are shown in Fig. 6 .For1  X  k  X  7, we have LOF k ( q )&gt; LOF k (w) . Thus, for any value u , LOF k (w)  X  u  X  LOF k ( q )  X  u .For8  X  k  X  98, we have LOF k ( q )&gt; LOF k ( o ) . This means LOF k ( o )  X  u  X  LOF k ( q )  X  u .For k = 99, we have LOF meaning LOF k (w)  X  u  X  LOF k ( p )  X  u . Because p and q are non-outliers and o and w are outliers, by Theorem 2.1 , the LOF scheme is not ON-compatible with the given expectation.
 Result 4.3 For the dataset in Dataset Example 4, the COF scheme is ON-compatible with the expectation D o and D n as defined in the example.
 chose k = 13 and calculate COF values for all points in the dataset. The calcu-lation was done by a C ++ program with a precision of 10 decimal digits. The computing environment is the same as that for Result 4.2 . All the eight outliers in C 2 have the same COF value 1 . 1518705044 and the other outlier o has a COF value 1 . 0761474038. On the other hand, the first 15 points, starting from p on each of the two lines 1 and 2 , have COF values between 0.9941766178 and 0.9995440551; and the rest of the points in C 1 have COF value of 1. Thus, we can set a threshold of 1 . 076 to distinguish the outliers from the non-outliers. Hence, by Definition 2.2 , COF is ON-compatible with the expectation as defined in Dataset Example 4.
 compatible for the given dataset. The following assertion answers this question. Result 4.4 For the dataset and the expectation defined in Dataset Example 4, the LOF scheme is both O-compatible and N-compatible.
 liers. We then identify the maximal set of non-outliers, called covered set of non-outliers , whose LOF values are below the floor of outliers. If before k reaches the maximum (i.e., the size of the dataset D ), the union of all the covered sets identi-fied so far is the entire set of non-outliers, then the LOF scheme is O-compatible, otherwise it is not. A similar procedure is used to determine if the LOF scheme is N-compatible, where we consider the ceiling of non-outliers and covered set of outliers .
 the union of the covered outliers/non-outliers increases as k increases. The figure shows that the entire set of 91 non-outliers is covered completely when k reaches the maximum, making the LOF scheme O-compatible, while the set of 9 outliers is covered when k reaches ten, implying N-compatibility of the LOF scheme. compatibilities for the LOF scheme. Please note that each number on the bottom row in each table indicates the number of outliers (non-outliers) that are covered by the current k value but not by any of the preceding ones. Thus, the numbers of the covered outliers (non-outliers) in the same sequence sum up to the total number of outliers (non-outliers) in the dataset. 4.3.4 Detecting connected outliers Although the connectivity-based COF scheme is more effective than the density-based scheme in detecting isolated outliers, it is not as effective in detecting  X  X on-nected X  outliers as the LOF scheme. Consider the dataset described in Dataset Example 5. 4.3.5 Dataset Example 5 The dataset shown in Fig. 9 contains a disk-shaped pattern C and a line pattern L , where | C |= 180 and | L |= 20. Assume the points in C are uniformly distributed, and so are the points in L . In addition, the average of the distances between adja-cent points in L is roughly the same as the average of the 1-distance ( p ) for all p  X  C . Let the expectation be D = D have the following result.
 Result 4.5 For the dataset described in Dataset Example 5, the LOF scheme is not ON-compatible, but is both O-compatible and N-compatible with the expectation D o and with the expectation.
 result is shown in Fig. 10 .The x -axis indicates k values, and the y -axis denotes the number of non-outliers that are not covered (i.e., their LOF values are not lower than the floor of the outliers for the corresponding k ). It can be seen from Fig. 10 that for each k , the curve never touches zero. Thus, there is not a single value for k such that all the non-outliers are covered. This means that the LOF scheme is not ON-compatible. (The minimum number of non-outliers that escapes from being covered is 10. This occurs when k = 183.) The LOF scheme is, however, both O-and N-compatible. This is shown in Fig. 11 . Some sequences of k values for O/N-compatibility are given in Tables 3 and 4 .
 compatible. From the figure, when k reaches the maximum (the size of the dataset), 157 out of 180 non-outliers are covered, making COF not O-compatible, while 11 out 20 outliers are covered, making it not N-compatible.
 dataset are evenly apart from their neighbors, they have the same connectivity. So the COF scheme cannot distinguish the outliers from non-outliers, but the LOF scheme can do better since the line pattern has a lower density than the disk-shaped pattern. 4.3.6 Time complexity Although algorithmic aspects for finding the COF values is not the main concern of the present paper, we include a brief discussion about the complexity for com-pleteness.
 rithm in [ 8 ], we can compute COF values for objects in D in two major steps. The first step is preprocessing. In this step, we find all k -nearest neighborhoods and all average chain distances. Precisely, we find, for any object p  X  D ,the k -nearest neighbors and then use the Prim X  X  algorithm [ 12 ] to find the average chain distance of p . The result of this step is saved in an intermediate dataset M . We know that the number of k -nearest neighbors of an object may be more than k , and in the worst case may be as large as n  X  1. But on average it is reasonable to assume that the number of k -nearest neighbors of an object is O ( k ) . Hence, the number of ob-jects in the intermediate dataset M is nk . As in the LOF algorithm, the number of objects in M is independent of the dimensionality of the original dataset D .Since each object have d many components, the size of M is O ( nkd ) . The time com-plexity for this step is O ( n 2 d + nk 2 d ) because we need to consider every object in
D , and for each object we first need to search D to find its k -nearest neighbors, and then use the Prim X  X  algorithm to find its average chain distance from those k -nearest neighbors.
 with the help of the intermediate dataset M . The original dataset is not needed at this step, since M contains sufficient information. We scan the dataset M twice. In the first scan, we find the average chain distance ac-dist k ( p ) and the k -nearest of those k -nearest neighbors. The time complexity is O ( | M |+ k | M |+ k ) = O ( nk 2 d ) .
 COF values of all objects in the dataset D is O ( n 2 d + nk 2 d ) . This time complexity is more efficient than that of the LOF algorithm in [ 8 ], because one can show that the LOF algorithm has O ( n 2 k 3 d ) complexity. 5 Experimental results We conducted performance experiments on both real-life and synthetic datasets to evaluate our proposed COF scheme. Since it is shown in [ 8 ] that the LOF scheme is more effective than the distance-based schemes, we will focus on comparing the COF scheme with the LOF scheme. The real-life datasets were obtained from the UCI Machine Learning Repository [ 7 ]. Algorithms were implemented in C ++ . The computing environment is a Dell Inspiron 8100 Pentium 1GHz laptop with 512 MB RAM and 20 GB HD.
 tion scheme is to test the scheme on datasets to discover rare classes. The per-formance of the scheme is then measured by the percentage of data, which are from the rare classes, discovered by the scheme. This approach was adopted by [ 17 , 19 , 20 ], and was used in our evaluation experiments.
 power and implementation-free metric, for measuring the performance of an out-lier scheme. 5.1 Performance metrics 5.1.1 Precision and recall These are the two traditional performance metrics of the quality of an information system [ 4 , 30 ], and can be tailored to measure the performance of an outlier de-tection scheme. Assume that a dataset D = D o  X  D n with D o being the set of all outliers and D n being the set of all non-outliers. Given any integer m  X  1, let O m denote the set of outliers among the objects in the top m positions returned by an outlier detection scheme. Then, we define precision and recall with respect to m as follows: That is, precision measures the percentage of outliers among the top m ranked objects returned by the scheme, while recall measures the percentage of the total outlier set included within the top m ranked objects. These relative precision and recall measures have been used in evaluating performances of Web search algo-rithms (see, f.g., [ 11 ]). Note that the LOF (COF) scheme ranks objects according to the LOF (COF) values of the objects. Objects with larger LOF (COF) values are ranked higher than these with smaller values. The distance-based scheme can also rank objects according to the sparseness of these objects within a given distance. The coverage rate metric used in [ 1 , 19 , 20 ] is essentially the recall measure. 5.1.2 Rank power Precision and recall metrics certainly measure the accuracy of a scheme, but do not reflect the satisfaction level of the users, because both metrics ignore the im-portance of the placements of the outliers returned by the scheme. For example, placing three outliers in the top three positions is considered by both metrics the same as placing in the bottom three positions among m objects returned. In real-ity, users are mostly interested in top ranked results. That is, not only how many results being returned is important, but also where they are placed is critical. Rank power is a metric proposed in [ 26 ] that considers both the placements and the num-ber of results returned by a scheme, and is bounded from below by 1 2 .Asurvey of other performance metrics can also be found in [ 26 ]. Here, we give a slightly revised definition of rank power with values ranging from 0 to 1 so that a value of 1 indicates the best performance and 0 indicates the worst. Consider that a scheme returns m objects, placing from position 1 to position m . Assume that there are n outliers among these m objects. For 1  X  i  X  n ,let L i denotes the position of the i th outlier, define the rank power of the scheme with respect to m as As can be seen from the previous definition, rank power weighs the placements of the returned outliers heavily. An outlier placed earlier in the returned list adds less to the denominator of the rank power (and thus contributes more to the rank power metric) than placed later in the list.
 5.1.3 Implementation-free metric When evaluating the time performance of a proposed approach vs. the competing approaches, the evaluation results may depend on data structures and indexing methods and other concrete techniques used in implementation. It is stated in [ 22 ] that Implementation bias is the conscious or unconscious disparity in the quality of implementation of a proposed approach, vs. the quality of implementation of the competing approaches. X  It is pointed out in [ 22 ] that one possibility to avoid implementation bias is to design experiments that are free from the possibility of implementation bias. For example, in artificial intelligence, researchers often compare search algorithms by reporting the number of nodes expanded, rather than the CPU time. To evaluate the time performance of the COF scheme vs. the LOF scheme, we introduce an implementation-free metric. Given any parameter k  X  1, for any object p in the dataset, the time needed to compute COF compute LOF k ( p ) is propositional to the size of are independent of implementation techniques, and hence we can use them as implementation-free metrics to measure the time performance of COF and LOF schemes. 5.2 Wisconsin breast cancer data The first dataset used is Wisconsin Breast Cancer Dataset, which has 699 records with 9 attributes. Each record is labeled as benign or malignant . We found that there are many records occurring more than once in the dataset. In order to avoid data bias, another important issue advocated in [ 22 ] in performance evaluation, we removed all the duplicated records and records with missing attribute values, and obtained a dataset with 213 records labeled as benign and 236 as malignant .We then follow the experimental technique used in [ 1 , 19 , 20 ] to remove some of the malignant records to form a very unbalanced distribution. The resulting dataset, asshowninTable 5 , has 213 (91.4%) benign records and 20 (8.6%) malignant records.
 different values of the parameter k .Table 6 shows the performance results of the COF scheme in comparison with the LOF scheme. Here, the performance is mea-sured with the three metrics of recall, precision, and rank power. The value of the parameter k is 12, which is 5% of the number of records in the dataset. For other values of k , the performance results are consistent with Table 6 .Thevalueof m indicates top m ranked records returned by the COF (or LOF) scheme. The ratio of these top m ranked records to the size of the dataset is also given in column 1. Column 2 indicates the number of rare cases among top m ranked records re-turned by the COF scheme, while column 6 indicates the number of rare cases among top m ranked records returned by the LOF scheme. The other six columns show values of recall, precision, and rank power for both COF and LOF schemes. For example, among top 25 ranked records returned by the COF scheme, 13 are rare cases, with 65% recall, 52% precision, and 0.57 rank power; while among top 25 ranked records returned by the LOF scheme, 11 are rare cases, with 55% re-call, 44% precision, and 0.43 rank power. Among top 15 ranked records returned by the two schemes, both schemes detect eight rare cases, with the same 40% re-call and the same 53% precision. However, the COF scheme has 0.56 rank power, which is larger than the 0.49 rank power of the LOF scheme. This implies that the COF scheme places these rare cases higher than the LOF scheme, hence performs better than the LOF scheme when placements of the results are considered. The last row indicates that the COF finds all the 20 rare cases among top 56 ranked records, while the LOF scheme still misses 2. The recall and precision measure-ments exhibit that the COF scheme outperforms the LOF scheme except for the two cases of top 10 and 15 ranked records. For these two cases, both schemes performs equally well in terms of recall and precision; however, the COF scheme performs better in terms of rank power. 5.3 Image segmentation data This dataset contains 210 records with 19 attributes. These records form seven equally sized classes labeled respectively as brickface , sky , foliage , cement , win-dow , path ,and grass . There are no duplicated records nor records with missing attributes values. Following the similar approach as in [ 1 , 19 , 20 ], we removed some records from the dataset to generate rare cases. Precisely, we removed 27 records from each of the brickface , grass ,and path classes. The resulting dataset has 129 records with 9 records as rare cases (3 for each of the brickface , grass , and path classes). Table 7 shows the class distribution of the dataset. shows the performance results measured with recall, precision, and rank power. The value of the parameter k is 7, which is 5% of the size of the dataset. Consis-tent performance results are obtained for other values of k .Again,thevalueof m indicates top m ranked records (or top ratio of records) returned by the COF (or LOF) scheme. Column 2 indicates the number of rare cases among top m ranked records returned by the COF scheme, while column 6 does the same for the LOF scheme. The other six columns assess recall, precision, and rank power for both COF and LOF schemes. For example, among top 15 ranked records returned by both schemes, the COF scheme detects four rare cases, with 44% recall, 30% pre-cision, and 0.29 rank power, while the LOF scheme detects two rare cases, with 22% recall, 13% precision, and 0.25 rank power. Among top 45 ranked records, the COF scheme detects all the 9 rare cases, while the LOF scheme still misses 3. Results in Table 8 show that the COF scheme outperforms the LOF scheme except for the case of top 5 ranked records, where both schemes perform equally well in terms of three metrics. 5.4 Johns hopkins university ionosphere data This dataset has 351 records with 34 attributes. These records form two classes labeled respectively as good and bad . There are no duplicated records nor records with missing attributes values. Following the similar experimental method as in [ 1 , 19 , 20 ], we removed some records from the dataset to generate rare cases. The resulting dataset has 235 records with 215 records labeled as good and 10 labeled as bad .Table 9 shows the class distribution of the dataset.
 formance of the COF scheme vs. the LOF scheme in terms of recall, precision, and rank power. The results are given in Table 10 , where the parameter k is set to 12, which is 5% of the size of the dataset. Consistent performance results are obtained for other values of k .Onceagain,thevalueof m indicates top m ranked records (or top ratio of records) returned by the COF (or LOF) scheme. Columns 2 and 6 indicate respectively the number of rare cases among top m ranked records re-turned by COF and LOF schemes. The other six columns assess recall, precision, and rank power for both schemes. For example, among top five ranked records returned by both schemes, the COF scheme detects five rare cases, with 50% re-call, 100% precision, and 1.00 rank power, while the LOF scheme detects four rare cases, with 40% recall, 80% precision, and 0.83 rank power. Among the top 20 ranked records, the COF scheme detects all the 10 rare cases, while the LOF scheme still misses 1. Results in Table 10 shows that the COF scheme outperforms the LOF scheme in terms of recall, precision, and recall. 5.5 COF scheme performance vs. outlier isolativity In the previous section, experimental results on three real-life datasets show that the COF scheme outperforms the LOF scheme in terms of recall, precision, and rank power. As addressed in Sect. 4, one major motivation for us to introduce the COF scheme is to deal with  X  X solativity X  of outliers. Isolativity implies low den-sity, but the latter does not always imply the former. We show in Sect. 4 that the COF scheme can perform well in detecting isolated outliers that deviate from con-nected patterns, but the LOF scheme cannot do so well. Here, we examine why the COF scheme outperforms the LOF scheme on these three datasets via analyzing isolativity of outliers. Since the three datasets have respectively dimensions of 9, 19, and 34, it is impossible to generate some direct visualization of the possible isolativity of the outliers. However, for an isolated outlier o , it is easy to see that the cost description of the SBN-path of o on N k ( o ) will reflect the isolativity of o . Therefore, we will analyze the cost descriptions of outliers in the datasets. To simplify the illustration, we chose top three ranked outliers returned by the COF scheme for each of the three datasets, and plotted the related cost descriptions in Figs. 13  X  15 , respectively. In these figures, the x -axis indicates the indexes of the cost description of the SBN-path on the k -nearest neighborhood of the outlier, and the y -axis indicates the corresponding costs (i.e., the distances as defined in Definition 4.3 ), where k = 12 in Figs. 13 and 15, and k = 7inFig. 14 . Recall from Definition 4.3 that, given an outlier o , the first cost in the cost description is the distance from o to its closest object in the dataset. In general, the i th cost is the distance from o and its ( i  X  1 ) closest objects to the rest of the objects in the dataset. For example, in Fig. 14 , for the top one ranked outlier returned by the COF scheme, the first cost is 111.317, meaning that this outlier is 111.37 dis-tance away from the rest of the objects in the dataset. The second cost is 47.546, meaning that this outlier and its closest neighbor is 47.546 distance away from the rest of objects in the dataset. These costs in the three figures provide good Similar isolations also exist for other outliers. These isolation properties of the outliers somehow provide evident support to the better performance of the COF scheme. 5.6 Implementation-free performance In Sect. 5.1, we introduced an implementation-free metric to evaluate the time performance of both COF and LOF scheme. Given any parameter k  X  1andany object p in the dataset, the time needed to compute COF k ( p ) is proportional to and the time needed to compute LOF k ( p ) is propositional to respectively COF k ( p ) and LOF k ( p ) , and are independent of implementation techniques. Figures 16  X  18 show the results of these two metrics for Wisconsin Breast Cancer, Image and Ionosphere Datasets. In these three figures, the x -axis indicates the index of object p in the dataset, y -axis indicates the values of C ( k = 12, and L ( 12 , p ) is on the average 109 times larger than C ( 12 , p ) .InFig. 17 , k = 7, and L ( 7 , p ) is on the average 7 times larger than C ( 7 , p ) .InFig. 18 , k = 12, and L ( 12 , p ) is on the average 12 times larger than C ( 7 , p ) . Consistent results are obtained for other values of k . It is clear that the COF scheme outperforms the LOF scheme based on the given implementation-free metrics. Wisconsin Breast Cancer Dataset, while remaining almost stable for the other numerous overlappings occur among the neighborhoods of p and p  X  X  nearest neighbors. 5.7 Time performance and scalability Here, we report execution time performance and scalability of the COF and LOF schemes. Figure 19 shows execution times of the two schemes on Wisconsin Breast Cancer ( k = 12), Image ( k = 7), and Ionosphere Datasets ( k = 12). As mentioned earlier, the parameter k is set to 5% of the dataset size. Note that both COF and LOF schemes need, for any object in the dataset, to find its k -nearest neighbors, and there are many different methods (see, for exam-ple, [ 29 ]) with variable time performances depending on the underlying data structures and indexing methods. In order to avoid implementation bias, we use the same method to find k -nearest neighbors in both schemes. It is clear that the COF scheme has better scalability than the LOF scheme with respect to k .
 ues, we prepared a synthetic, 20-dimensional dataset with 5,000 objects. We ran both schemes for k from 10 to 250 with an increment of 10, and report the re-sults in Fig. 20 . The COF scheme has a very steady performance with respect to k , while the LOF has some rapid growth in time as k increases. We also prepared five synthetic, 20-dimensional datasets with respectively 1,000, 2,000, 3,000, 4,000, and 5,000 records. We first ran both schemes on these datasets with k = 50, and then repeated the experiments with k = 100. Figure 21 shows the scalability of both schemes for k = 50, while Fig. 22 shows the scalability for k = 100. In both experiments, the COF scheme has a slower growth in time than the LOF scheme as k increases; hence, the COF scheme has better scalability about dataset sizes. 6 Conclusions The existing outlier detection schemes are either distance based or density based. The evaluations of their capabilities are mostly ad hoc, and lack theoretical framework for effective analysis and synthesis. We propose a theoretical frame-work based on which the capabilities of various kinds of schemes can be ana-lyzed. Based on this framework, we study the capabilities of these schemes on datasets with different characteristics. We find that both the density-based and the connectivity-based schemes are more capable than the distance-based schemes. In comparing the former two schemes, we see that the density-based schemes are ef-fective in an environment where patterns possess sufficiently higher densities than outliers, while the connectivity-based scheme works better for isolated outliers that possess comparable densities with the patterns.
 shows that we should not view one scheme as being superior to the other in all aspects. To enhance the effectiveness, therefore, one scheme should be used as a compliment, not a replacement, of the other in applications with different requirements. Thus, it is interesting to develop an effective and efficient method by which the two schemes can be seamlessly integrated. Please note that simply wrapping them into one package does not work. This is because their views toward outlier formulations do not match, and sometime are conflicting. Thus, a naive ap-proach may produce contradictory results, and incurs needlessly high overhead. compatibility theory is based on the precise matching between the outlier formu-lation schemes and users X  intuitions. In reality, however, it is usually difficult to detect all the outliers that fit users X  intuitions. Thus, it is probably meaningful to incorporate such a factor as the percentage of the outliers that a scheme can return into the framework. Furthermore, from our experimental results, we observe that for some datasets, for a small number of k values a majority of the covered points can be found. (This is evidenced by Figs. 8 , 11 ,and 12 , where the sharp slopes of the curves end at a small value of k , and then followed by more flat slopes that span the rest of the k values.) Thus, by considering only a portion of the outliers, there is a high potential to enhance the performance substantially.
 density-based, and connectivity-based outlier detection schemes are not ON-the schemes are not ON-compatible.
 Appendix A: Proof of Result 3.3 Result 3.3 Appendix B: Proof of Result 3.4 Result 3.4 Appendix C: Proof of Result 4.1 Result 4.1 References
