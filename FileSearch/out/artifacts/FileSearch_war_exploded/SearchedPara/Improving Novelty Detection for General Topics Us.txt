 The detection of new informati on in a document stream is an important component of many potent ial applications. In this work, a new novelty detection approach based on the identification of sentence level information patterns is proposed. First, the information-pattern concept fo r novelty detecti on is presented with the emphasis on new information patterns for general topics (queries) that cannot be simply turned into specific questions whose answers are specific named entities (NEs). Then we elaborate a thorough analysis of sentence level information patterns on data from the TREC novelty tracks, including sentence lengths, named entities, senten ce level opinion patterns. This analysis provides guidelines in applying those patterns in novelty detection particularly for the ge neral topics. Finally, a unified pattern-based approach is presen ted to novelty detection for both general and specific topics. Th e new method for dealing with general topics will be the focus. Experimental results show that the proposed approach significant ly improves the performance of novelty detection for general t opics as well as the overall performance for all topics from the 2002-2004 TREC novelty tracks. H.3.3 [ Information Search and Retrieval ]: Query formulation and retrieval models Keywords: Novelty detection, information patterns, named entities The goal of research on novelty detection is to provide a user with a list of materials that are relevant and contain new information with respect to a user X  X  information need. The goal is for the user to quickly get useful information without going through a lot of redundant information, which is a tedious and time-consuming task. A variety of novelty measures have been described in the literature [6, 7, 22]. The definitio ns of novelty, however , are quite vague and seem only indirectly related to the intuitive notions of novelty. Usually new words appearing in an incoming sentence or document contribute to the novelty scores in various novelty measures in different ways. We believe that information patterns such as combinations of query words, named entities, phrases and other sentence patterns, which indicate the presence of possible answers, may contain more important and relevant information than single words given a user X  X  request or information need. The idea of identifying query-related named-entities (NEs) patterns in se ntences has been proved very effective in our previous study [2 5] in significantly improving the performance in novelty detection, particularly at top ranks. This approach is inspired by question answering techniques and is similar to passage retrieval for factoid questions. Each query could be treated as multiple questions; each question is represented by a few query words, and it requires a certain type of named entities as answers. Instead of extracting exact answers as in typical question answering systems [14,19,20], we have proposed to first extract interesting sentences with certain NE patterns that include both query words and required answer types, indicating the presence of potential answers to the questions, and then identify novel sentences that are more likely to have new answers to the questions. The effectiveness of the pattern-based approach has been validated by the experimental results on novelty detection on TREC 2003 and 2004 novelty tracks, with significant improvements in novelty detection for those specific topics corresponding to specific NE questions. However, queries (topics) that can be transformed into specific NE questions are only a small portion of the query sets. For example, in TREC 2003, there are only 15 (out of 50) topics that can be formulated into specific NE questions. For the rest of the topics, which will be called general topics throughout the paper since they can only be formulated into general questions, the improvement is not very significant using the patt ern-based approach merely based on general NE patterns. New and e ffective information patterns are needed in order to significantly improve the performance of novelty paper. Meanwhile, a unified fr amework of the pattern-based approach is also required to deal with both the specific and the general topics. As one of the main contributions of this work, we have found that the detection of information patterns related to opinions is very effective in improving the performan ce of the general topics. As an example, Topic N1, from the TREC novelty track 2003, is about  X  X artial birth abortion ban X . This is a query that cannot be easily converted into any specific NE questions. However, we know that the user is trying to find opinions about the proposed ban on partial birth abortions. Therefore, relevant sentences are more likely to be  X  X pinion sentences X . Let us consider the following two sentences. Sentence 1 (Relevant and Novel):  X  The court's ruling confirms that the entire campaign to ban 'partial-birth abortion ' --a campaign that has consumed Congress and th e federal courts for over three years --is nothing but a fraud designed to rob American women of their right to abortion,  X  said Janet Benshoof, president of Center for Reproductive Law and Policy. Sentence 2 (Non-relevant): Since the Senate's last partial birth vote, there have been 11 court decisions on the legal merits of partial birth bans passed by different states. Both sentence 1 and sentence 2 have five matched terms (in italic ). But only sentence 1 is relevant to the topic. Note that in addition to the matched terms, sentence 1 also has opinion patterns, indicated by the word  X  X aid X  and a pair of quotation marks. The topic is an opinion topic that requires rele vant sentences to be opinion sentences. The first sentence is relevant to the query because it is an opinion sentence and topically related to the query. However, for the example topic given above, it is very difficult for traditional word-based approaches to separa te the non-relevant sentence (sentence 2) from the relevant se ntence (sentence 1). This paper tries to attack this hard problem. The rest of the paper is organized as follows. Section 2 gives a brief overview of related work on novelty detection. Section 3 introduces the concept of the proposed information patterns for novelty detection, with emphasis on information patterns for general topics that cannot be simply turned into NE questions. Section 4 elaborates a thorough analysis of sentence level information patterns, including sentence lengths, named entities, se ntence level patterns related to opinions. The analysis is performed on the data from the TREC 2002 and 2003 novelty tracks, which provides guidelines in applying those patterns in novelty detection particularly for general topics. Section 5 describes the proposed unified pattern-based approach to novelty detection for both general and specific topics. The new method for dealing with ge neral topics will be the focus. Section 6 shows experimental results in using those information patterns for significantly improving the performance novelty detection for topics corresponding to general questions, and for improving the overall performance of novelty detection using the unified approach. Section 7 summarizes the work. Work on novelty detection at the event level arises from the Topic Detection and Tracking (TDT) res earch, which is concerned with online new event detection/first story detection [1,2,3,4,5,16,18]. Current techniques on new event detection are usually based on clustering algorithms. Some models (vector space models, language models, lexical chains, etc.) are used to represent incoming new stories/documents. Each story is then grouped into clusters. An incoming story will either be grouped into the closest cluster if the similarity score between them is above the preset similarity threshold or start a new cluster. A story which started a new cluster will be marked as the first story about a new topic, or it will be marked as  X  X ld X  (about an old event) if there exists a novelty threshold and the similarity score between the story and its closest cluster is greater than the novelty score. Research on novelty detection at the sentence level is related to the TREC novelty track for finding relevant and novel sentences given a query and an ordered list of relevant documents [7, 8, 9, 10, 11, 12, 13, 22]. In current techniques developed for novelty detection at the sentence level or document level, new words appearing in sentences/documents usually contribute to the scores that are used to rank sentences/documents. Many similarity functions used in information retrieval are also tried in novelty detection. Usually a high similarity score between a sentence and a given query will increase the relevance rank of the sentence while a high similarity score between the sentence and all previously seen sentences will decrease the novelty rank of the sentence, for example, the Maximal Marginal Relevance model (MMR) introduced by Carbonell and Goldstein [23].Novelty detection could be also performed at the document level, for example, in Zhang et al X  X  work [13] on novelty and redundancy detection in adaptive filtering, in Zhai et al X  X  work [17] on subtopic retrieval and in Dai et al X  X  work [26] on minimal document set retrieval. There are two main differences be tween our proposed approach and the approaches in the literature. First, none of the work described above treats new information as new answers to questions that aforementioned systems related to the TREC novelty track, either the title query or all the three secti ons of a query were used merely as a bag of words, while we try to form answer patterns from the query. Our previous work [25] made a first attempt in this direction, but novelty detection performance onl y increases significantly for those specific topics that can be turned into specific NE questions. We emphasize that the definition of novelty or  X  X ew X  information is crucial for the performance of a novelty detection system. Unfortunately, novelty is usually not clearly defined in the literature. Generally, new words in the text of a sentence, story or document are used to calculate novelty scores by various  X  X ovelty X  measures. However, new words are not equivalent to novelty (new information). For example, rephrasing a sentence with a different vocabulary does not mean that this revised sentence contains new information that is not covered by the original sentence. In our previous work [25], a new definition of novelty has been given as following statement: Novelty Definition:  X  Novelty or new information means new answers to the potential questions representing a user X  X  request or information need.  X  There are two important aspects in this definition. First, a user X  X  query will be transformed into on e or more potential questions for identifying corresponding query-related information patterns that include both query words and required answer types. Second, new information is obtained by detecting those sentences that include previously unseen  X  X nswers X  corresponding to the query-related patterns. Although a user X  X  informa tion need is typically represented as a query consisting of a few key words, our observation is that a user X  X  information need may be better captured by one or more questions that lead to corresponding information patterns. The novelty definition can be applied to novelty detection at different levels  X  event level, se ntence level and document level. In this work, we will study novelty detection via information pattern identification at the sentence level . Novelty detection includes two consecutive steps: first retrieving relevant sentences and then detecting novel sentences. This novelty definition is also a general one that works for novelty detection with any query that can be turned into questions. In our previous work [25], we have shown that any query (topic) in the TREC novelty tracks can be tu rned into either one or more specific NE-questions, or a general question. The NE-questions (corresponding to specific topi cs) are those whose answers are specific named entities (NEs), including persons, locations, dates, time, numbers, and etc.[21]. The general questions ( corresponding to general topics), on the other hand, require obtaining additional information patterns for effective novelty detection. This will be the focus of this paper. The identification and extraction of information patterns is crucial in our approach. The information patterns corresponding to NE-questions are called NE words patterns , related to the  X  X hen X ,  X  X here X ,  X  X ho X ,  X  X hat X  and how many X  questions. Each NE word pattern is a combination of both query words (of potential questions) and answer types (which requires named entities as potential answers). We have shown that our pattern-based approach is very effective in improving the performance of novelty detection for those specific topics (queries). For a general topic, it is very difficult (if not impossible) to identify a particular type of named entity as its answer. Any type of na med entity could be an answer as long as the answer context is related to the question. In quite some relevant and novel sentences, no named entities are included. Simply using named entities seems not very helpful for improving the performance of novelty detection for these general topics, as having been shown in [25]. Therefor e the focus of this work will be how to effectively make use of th ese named entities, and what kinds of additional and critical informa tion patterns will be effective for general topics. After analyzing the TREC data, we have found that the following three kinds of information patterns are very effective for this purpose: sentence lengths , named-entity combinations , and opinion patterns . We note that the topics in TREC 2003 and 2004 novelty tracks are either classified as event topics or opinion topics. As one of the particular interesting findings, we have found that a large portion of the general questions is about opinions . Opinions can typically be identified by looking at such sentence patterns as  X  X XX said X ,  X  X YY reported X , or as marked by quotation marks. We have identified about 20 such opinion-related sentence patterns by manually scanning through a few paragraphs related to opinion patterns (Table 1). In the following section we will provide a through data analysis to support the above observations and arguments. In this section we will perform statistics of the three types of information patterns in relevant sentences, novel sentences and non-relevant sentences. The three information patterns are: sentence lengths , named entities , and opinion patterns . The goal is to find out effective ways to use these information patterns in distinguishing relevant sentences from non-relevant ones, and novel sentences from non-novel ones. The statistics of sentence lengths in TREC 2002 and 2003 datasets are shown in Table 2. The length of a sentence is measured in the number of words after stop words are removed from the sentence. As a very useful result, the average lengths of relevant sentences from the 2002 data and the 2003 data are 15.58 and 13.1, respectively. But the average length s of non-relevant sentences from the 2002 data and the 2003 data are only 9.55 and 8.5, respectively. We have the following interesting observation: Observation #1 : Relevant sentences on average have significantly more words than non-relevant sentences.
 This feature is simple, but is very effective since the length differences between non-relevant and relevant sentences are significant. The feature is ignored in other approaches mainly because they are doing sentence retrieval with information retrieval techniques developed for document retrieval where document lengths are usually used as a pena lty factor. Thus a short document is assigned a higher rank than a long document if the two documents have same occurrences of query words. But at the sentence level, it turns out that re levant sentences have more words than non-relevant sentence on average. Therefore this observation will be incorporated into the retrieval step to improve the performance of relevance, which is crucial in detecting novel (and relevant) information. The difference between novel sentences and non-relevant sentences are slightly larger, which indicate that this incorporation of the sentence length information in relevance ranking will put the novel sentences with higher ranks in relevance retrieval. There are 22 opinion topics out of the 50 topics from the 2003 novelty track. The number is 25 out of 50 for the 2004 novelty track (there are no classification of opinion and event topics in the 2002 novelty track). We classify a sentence as an opinion sentence if it has one or more opinion patterns. Intuitively, opinion sentences are more likely to be relevant senten ces than non-opinion sentences. Opinion patterns are detected in a sentence if it includes quotation marks or one or more of the expressions indicating it states an opinion (see Table 1 for a list). These patterns are extracted from TREC 2003 novelty track by scanning through a few documents in the data collection. Note that the terms remain in their original verbal forms without word stemming, in order to more precisely capture the real opinion sentences. For example, a word  X  state X  does not necessarily indicate an opinion pattern, but the word combination  X  X tated that X  will most probably do. If a sentence includes one or more opinion patte rns, it is said to be an opinion sentence . We have run statistics of opinion patterns on the 2003 novelty track in order to obtain guidelines for using opinion patterns for both 2003 and 2004 data (note that there are no classification of opinion and event topics in the 2002 nove lty track). Statistics show that there are relatively more opinion sentences in relevant (and novel) sentences than in non-relevant sentences. According to the results shown in Table 3, 48.1% of relevant sentences and 48.6% novel sentences are opinion sentences, but only 28.4% of non-relevant sentences are opinion sentences. We summarize this into the following observation: Observation #2 : There are relatively more opinion sentences in relevant (and novel) sentences than in non-relevant sentences. This has a significant impact on separating relevant and novel sentences from non-relevant sentences. Note that the number of opinion sentences in the statistics only counts those sentences that have one or more opinion patterns shown in Table 1. We have noticed that some related work [28] has been done very recently in classifying words into opinion-bearing words and non-opinion-bearing words, using information from several major sources such as WordNet, World Street Journal, and General Inquirer Dictionary. Using opinion-bearing words may cover more opinion sentences, but how the accuracy of classifyi ng opinion words is still an issue. We believe that a more accurate classification of opinion sentences based on the integration of the results of that work into our framework will further enlarge the difference in numbers of opinion sentences between relevant senten ces and non-relevant sentences. Answers and new answers to sp ecific NE-questions are named entities. And for many of the gene ral topics (questions), named understanding the distribution of named entity patterns could be very helpful both in finding rele vant sentences and in detecting novel sentences. We also want to understand the role of certain named entities and their combinations in separating relevant sentences from non-relevant sentences, for event topics and opinion topics, respectively. The statistics of all the 21 named entities that can be identified by our system are listed in Table 4. We have found that the most frequent types of NEs are PERSON, ORGANIZATION, LCATION, DATE and NUMBER. For each of them, there are more than 25% relevant sentences, each of which has at least one named entity of the type in consid eration. Among these five types of NEs, three (PERSON, LO CATION and DATE) are more important than the other tw o (NUMBER and ORGANIZATION) for separating relevant sentences from non-relevant sentences. The discrimination capability of the ORGANIZATION type is not as significant and this has also been validated by experiments of novelty detection on real data. The role of the NUMBER type is not consistent among three TREC datase ts. This is summarized in the following observation: Observation #3 . Named entities of the three types -PERSON, LOCATION and DATE are more effective in separating relevant sentences from non-relevant sentences. Therefore only the three effective ty pes will be incorporated into the sentence retrieval step to improve the performance of relevance. Named entities of the ORGANIZAION ty pe is not used in relevant sentence detection since they almost equally appear in both relevant and non-relevant sentences. For example, the ratio is 42%:38% in the TREC 2003 novelty track. However, the ORGANIZATION type will be also used in the new pattern detection step since an NE of this type often indicates the name of a different news agency or some other organization, and a diffe rent one in a already relevant sentence may provide new informatio n. This is summarized in the following observation: Observation #4 : Named entities of the POLD types -PERSON, LOCATION ORGANIZATI ON and DATE will be used in new pattern detection; named entities of the ORGANIZATION type may provide different sources of new information. Table 4 also lists the statistics of those sentences with no NEs, with no POLD (Person, Organization, Location and Date) NEs and with no PLD (Person, Location and Date) NEs. These data show that (1) there are obvious larger differences between relevant and non-relevant sentences without PLD NE s which confirms that PLD NEs are more effective in re-ranking th e relevance score (Eq. 4); and (2) there are considerable large percentages of relevant sentences without NEs or without POLD NEs. The second point is summarized into the following observation: Observation #5: The absence of NEs cannot be used exclusively to remove sentences from the relevant sentence list. The number of the previously unseen POLD NEs only contributes part in novelty ranking. As we have known, topics in TREC 2003 and TREC 2004 novelty track data collections are classified into two types: opinion topics and event topics. If a topic can be transformed into multiple NE-questions, no matter it is an opinion or event topic, the relevant and novel sentences for this  X  X pecific X  topic can be extracted by mostly examining required named entities (NEs) as answers to these questions generated from the topic. Otherwise we can only treat it as a general topic for which no specific NEs can be used to identify those sentences as its answers. Analysis in Section 4.2 shows that we can use opinion patterns to id entify opinion sentences that are more probably relevant to opinion topics (queries) than non-opinion sentences. However, opinion topi cs only consist of part of the queries. There are only 22 opinion topics out of the 50 topics from the 2003 novelty track. The number is 25 out of 50 for the 2004 novelty track. Now the question is: how to improve the performance of novelty detection for those general, event topics? Table 5 compares the difference in the statistics of NEs between event topics and opinion topics. The observation is the following: Observation #6: PERSON, LOCATION and DATE play a more important role in event topics than in opinion topics.
 This is further verified in our experiments of relevance retrieval. In the equation for NE-adjustment (Eq. 4), the best results are achieved when  X  takes the value of 0.5 for event topics and 0.4 for opinion topics. In our definition, novelty means new answers to the potential questions representing a user X  X  information need. Given this definition of novelty, it is possi ble to detect new information patterns by monitoring how the potential answers to a question change. Consequently, we propose a new novelty detection approach based on the identification of query-related information patterns at the sentence level. In the following, we will first introduce our unified pattern-based approach for both specific and general topics (queries). Then we will focus on the new method in improving the novelty detection performance for general topics. There are two important steps in the pattern-based novelty detection approach: query analysis and new pattern detection. At the first step, an information request from users will be (implicitly) transformed into one or more po tential questions that determine corresponding query-related information patterns, which are represented by combinations of query words and required answer types to the query. At the second step, sentences with the query-related patterns are retrieved as answer sentences. Then sentences that indicate potential new answers to the questions are marked novel. A question formulation algorithm first tries to automatically formulate multiple specific questions for a query, if possible [25]. Each potential question is represented by a query-related pattern, which is a combination of a few query words and an expected answer type. A specific question would require a particular type of named entities for answers. Five types of specific questions are considered in the current system: PERSON, ORGANIZATION, LOCATION, NUMBER and DATE . If this is not successful, a general question will be generated. General questions do not require a particular type of named entities for answers. Any types of named en tities as listed in Table 4 could be answers as long as the answer c ontext is related to the questions. Answers could be in sentences w ithout any NEs. However, from our data analysis, the NEs of POLD types ( PERSON, ORGANIZATION, LOCATION, DATE ) are the most effective in detecting novel sentences (Observation #4), and three of them ( PERSON, LOCATION, DATE ) are the most significant in separating relevant sentences from non-relevant sentences (Observation #3). In addition, as we have observed in the statistics in Section 4, sentence lengths and opinion patterns are also important in relevant sentence retrieval and novel sentence extraction (Observations #1 and #2). In particular, we can use opinion patterns to identify opinion sentences that are more probably relevant to opinion topics (queries) than non-opinion sentences (Observation #2). PERSON, LOCATION and DATE play a more important role in event topics than in opinion topics (Observation #6). Therefore, for a general question, its information pattern include four entities: topic type (event or opinion, used in Eq. 4 below to adjust the  X  ), sentence length (in Eq. 3), POLD NE types (in Eqs.1 and 4), and opinion patterns (in Eq. 5 for opinion topics only). There are 49 queries in the TREC 2002 novelty track, 50 queries in the TREC 2003 novelty track and 50 queries in the TREC 2004 novelty track. Our question formulation algorithm formulated multiple specific questions for 8 queries from the TREC 2002 novelty track, 15 queries from the TREC 2003 novelty track and for 11 queries from the TREC 2004 novelty track, respectively. The remaining queries were transfor med into general questions. The new pattern detection step has two main modules: relevant sentence detection and then novel se ntence detection. First, a search engine takes the query words of the query-related pattern generated from a potential question of a query and searches in its data collection to retrieve sentences that are likely to have correct answers. Our relevant sentence de tection module filters out those sentences that do not satisfy th e query-related patterns and/or re-rank the relevance list using the required information patterns. For a specific question (topic), only a specific type of named entity that the question expects would be considered for potential answers. Thus a sentence without an expected type of named entities will be removed from the list. Then the rele vance sentences list is re-ranked by incorporating the number of di fferent types of required NEs to answer the questions derived from the specific topic in consideration [25]. For general questions (topics), all types of named entities (including no NEs) could be potential answers (Observation #5). Therefore the required information patterns are used in re-ranking the relevance list in order to improve the releva nce performance for these general topics. This means that at retrieval step, the system will revise the relevance sentence retrieval results by adjusting relevant ranking scores using sentence lengths, NEs and opinion patterns. Details will be provided in the next sub-section. Then, the new sentence detection module extracts all query-related named entities (as possible answers) from each answer sentence and detects previously unseen  X  X nswers X . For specific topics, our system will identify sentences with possib le new answers to the multiple NE questions as novel sentences (details can be found in [25]). For general topics, the novelty score is calculated with the following formula based on both Observations #4 and #5: where S n is the novelty score of a sentence S, N w is the number of new words in S that do not appear in its previous sentences, and N is the number of POLD-type named entities in S that do not appear in its previous sentences. A sentence is identified as a novel sentence if its novelty score is e qual to or greater than a preset threshold. In our experiments, the best performance of novelty detection is achieved when both  X  and  X  are set to 1 and the threshold for S n is set to 4. We want to make two notes here. (1). Named entities considered at th is step include all POLD types, i.e., PERSON, ORANIZATION, LOCATION and DATE. The ORGANIZATION type is also consid ered in this step since it often refers to the name of a news agency or some other organization, which could provide new information if it is a new one. (2) By the summation of the counts in new words and new named entities, those relevant sentences that do not include any NEs could also be selected as novel sentences. (3). The novelty score formula given in Eq. 1 is actually a general one that can also be applied to specific topics. In that case, N number of the specific answer NEs, and we set  X  to 0. The threshold for the novelty score S n is set to 1. Sentence-level information patterns, including sentence lengths, Person-Location-Date NEs, and opinion sentences, are incorporated in the relevance retrieval step fo r general topics. This sub-section details this new method of incorporating these information patterns in the relevant sentence ranking. TFIDF models are one of the typical techniques in document retrieval. TF stands for Term Frequency in a document and IDF stands for Inverse Document Frequency with respect to a document collection. The term frequency in the given document gives a measure of the importance of th e term within the particular document, which is the number of times the term appears in a document divided by the number of total terms in the document. The inverse document frequency is a measure of the general importance of the term, which is the logarithm of the number of all documents in the collection divided by the number of documents containing the term [24]. There are many different formulas to calculate TFIDF score which is used for ranking documents. We adopt the TFIDF models for the relevant sentence retrieval step in our novelty detection task simply because it was also used in other systems and was reported to be able to achieve equivalent or better performance compared to other techniques in sentence retrieval [7]. The name of our sentence retrieval model is called TFISF model, to indicate that inverse sentence frequency is used for sentence retrieval instead of inverse document frequency. The initial TFISF relevance ranking score S 0 for a sentence, modified from the LEMUR toolkit [7, 24, 27], is calculated according to the following formula where n is the total number of terms, isf (t i frequency (instead of inverse document frequency in document retrieval), tf s (t i ) is the frequency of term t i in the sentence, and tf is the frequency of term t i in the query. The inverse sentence frequency is calculated as where N is the total number of sentences in the collection, N total number of sentences that include the term t i . query (with a weight w(t i ) = 1) or in an expanded query that has more terms from pseudo feedback (with a weight w(t i ) = 0.4). With pseudo feedback, the system assumes that top 100 sentences retrieved are relevant to the query and top 50 most frequent terms within the 100 sentences are added to the original query. As a preprocessing, all the sentences have passed through the stopword removal and word stemming procedures. This score S 0 will be served as the ba seline for comparing the performance increase in relevant sentence retrieval for novelty detection. The TFISF score is adjusted usin g the following three information patterns: sentence lengths, name d entities, and opinion patterns. Following Observation #1, the leng th-adjustment is calculated as where L denotes the length of a sentence and L average sentence length. Following Observation #3, the NEs-adjustment is computed as where F person = 1 if a sentence has a person name, 0 otherwise; F location = 1 if a sentence has a location, 0 otherwise; and F sentence has a date, 0 otherwise. In addition, following Observation event topics. Finally, following Observation #2, the opinion-adjustment is computed as where F opinion = 1 if a sentence is an opinion sentence, 0 otherwise. The final adjustment step is only performed for opinion topics. A number of patterns (i.e.  X  X aid X ,  X  X rgue that X , see Table 1) are used to determine whether a sentence is an opinion sentence. We apply the three adjustments sequentially to tune the parameters on training data for best performa nce, and the same parameters are adjustments, and have found that current algorithm achieves the best performance. Incorporating information patterns at the retrieval step improves the performance of relevance and thus helps later at the novelty detection step. After applying the above three steps of adjustments on the original ranking scores, sentences with query-related information patterns are pulled up in the ranked list. For the example sentences shown in Section 1, the relevant (and novel) sentence (sentence 1) was ranked 14 th with the original ranking adjustments with the information patterns. The non-relevant sentence (sentence 2) was initially ranked 2 nd but pushed down to the 81st place after the score adjustments. Complete comparison results on TREC 2002, 2003 and 2004 are provided in the experiment section below. In this section, we present and discuss the main experimental results. The data used in our experiments are from the TREC 2002, 2003 and 2004 novelty tracks. The comparison of our approach and several baseline approaches are described. The experiments and analysis include the performance of novelty detection for general topics using the proposed information patterns, and the overall performance of novelty detection using the unified pattern based approach. We compared our information-pa ttern-based novelty detection (IPND) approach to four baselines. The first baseline (B-NN) does n ot perform any n ovelty detection but only uses the initial sentence ranking. The second baseline (B-NW) in our comparison is simply applying n ew w ord detection. Starting from the initial retrieval ranking, it keeps sentences with new words that do not appear in previous sentences as novel sentences, and removes those sentences without new words from the list. All words in the collection were stemmed and stop-words were removed. The third baseline (B-NWT) is similar to B-NW. The difference is that it counts the number of new words that do not appear in previous sentences. A sentence is identified as novel sent ence if and only if the number of new words is equal to or greater than a preset t hreshold . The best value of the threshold is 4 in our experiments. The fourth baseline B-MMR is a baseline with m aximal m arginal r elevance (MMR) [6,13,24,25]. MMR starts with th e same initial sentences ranking used in other baselines and our approach. In MMR, the first sentence is always novel and ranked top in novelty ranking. All other sentences are selected according their MMR scores. One sentences at a time. MMR scores are recalculated for all unselected sentences once a sentence is selected. We use MMR as our fourth and main baseline because MMR was reported to work well in non-redundant text summarization [23], novelty detection at document filtering [13] and subtopic retrieval [17]. For comparison, in our experiment s, the same retrieval system based on the TFISF technique adopted from the LEMUR toolkit [24] is used to obtain the retrieval results of relevant sentences in both the baselines and our approach. The evaluation measure used for performance comparison is precision at rank N. It shows the fraction of correct novel sentences in the top N sentences delivered to a user (N =5, 10, 15, 20 and 30 in Tables 7-12.). The precision values at top ranks are more meaningful in real applications where uses only want to go through a small number of sentences. Before we show the overall performance of our unified IPND approach, we will first see how information patterns can significantly improve the performance of novelty detection for those general topics that cannot be easily turned into specific NE questions that can be effectively handled by our previous NE-pattern-based approach [25]. We performed two sets of experime nts. The first set of experiments is to evaluate the improvement in relevant sentence retrieval. The second set of experiments is to evaluate the overall performance in novelty detection. As described in Section 5, three types of information patterns are incorporated in the relevance retrieval step of novelty detection for general topics. Table 6 gives the performance of relevance retrieval with the original TFISF ranking and our approach with sentence level features and information patterns for the TREC 2002, 2003 and 2004 data, respectively. The main conclusion here is that inco rporating information patterns and sentence level features into TFISF techniques can achieve much better performance than using TFISF alone. Significant improvements are obtained for the 2003 topics and the 2004 topics at top ranks. This lays a solid ground for the second step -new information detection, and therefore for improving the performance of novelty detection for those general topics. Tables 7-9 show the performance comparison of our IPND approach with the four baselines on those general topics that cannot be turned into specific NE questions. We can draw the following conclusions from the results. (1) Our IPND approach consistently outperforms all the baseline approaches across the three data sets: the 2002, 2003 and 2004 novelty tracks. The precision values for the top 20 sentences with our IPND approach for general questions of the 2002, 2003 and 2004 data are 0.21, 0.50 and 0.21, respectively (The precision is the highest for the 2003 data since this track has highest ratio of relevant to non-relevant sentences). Compared to the first baseline, the performance is increased by 11.8%, 23.9% and 14.5%, respectively and the improvements are significantly larger than the other three baselines (2-4). (2) B-NWT achieves better performance than B-NW as expected because B-NW is a special case of B-NWT when the new word threshold is set to 1. (3) MMR is slightly better th an B-NW and B-NWT on the 2002 data but is worse than B-NWT on the 2003 and 2004 data. The overall performance comparison of the unified pattern-based approach with the four baselines on all topics from the TREC 2002, 2003 and 2004 novelty tracks is shown in Tables 10, 11 and 12, respectively. The most important conclusion is that the unified pattern-based approach outperforms all baselines at top ranks. Significant improvements are seen with the 2003 topics. In the top 10 sentences delivered, our appro ach retrieves 5.9 novel sentences on average, while the four baseline approaches only retrieve 4.5, 4.8, 5.2 and 4.9 novel sentences, respectively. As anticipated, the overall performance for all topics (including specific and general ones) is slightly better than that for the general topics, since the precisions for the specific ones are slightly higher. In this paper, a unified pattern-based approach was proposed for novelty detection. Here we summarize the main features of our unified pattern-based approach for novelty detection in using the proposed information patterns at the sentence level. First, information patterns are defined and determined based on question formulation (in the query analysis step) from queries, and are used to obtain answer sent ences (in the relevant sentence retrieval step) and new answer sentences (in the novel sentence detection step). Second, NE information patterns are used to filter out sentences that do not include the specific NE word patterns in the relevance retrieval step, and information pa tterns (sentence lengths, named entities and opinion patterns) are incorporated in re-ranking the relevant sentences for favoring those sentences with the required information patterns, and therefor e with answers and new answers. Third, new information patterns ar e checked in determining if a sentence is novel or not in the novelty detection step. Note that after the above two steps, this step be comes relatively simple; however, we want to emphasize that our pattern-based approach for novelty detection include all the three steps. Experiments were carried out on the data from the TREC novelty tracks 2002-2004. The experimental results show that the proposed approach achieves significantly bette r performance at top ranks than the baseline approaches on topics from all three years. The proposed unified pattern-based approach results in significant improvement for novelty detection at the sentence level. There are more research issues in the proposed pattern-based approach. These include issues in question formulation, relevant retrieval models, and new applications. Even if we have significantly improved the performance of novelty detection for those  X  X eneral X  topics by using the proposed sentence level information patterns, the novelty detection precisions for the specific topics are slightly higher. Therefore there are two-fold solutions to this problem. First, exploring the possibilities of turning more topics into multiple specif ic questions will be of great interests. Currently, for specific topics, only NE questions are considered for query transformation. A specific topic is transformed into multiple NE questions, which may not completely cover the whole topic. Therefore some relevant or/and novel sentences may be missed because they are only related to the uncovered part of the topic, but do not contain answers to the multiple NE questions. A topic may be fully covered by multiple specific questions if other types of questions in addition to NE questions are considered thus these missed sentences may be retr ieved. Second, for general topics, the proposed three information patterns only capture part of the characteristics of the required answers. More information patterns could be helpful in further improving the performance of novelty detection for general topics. In terms of relevant sentence retr ieval models, currently, the pattern-based approach is combined with TFISF techniques, which are very simple, common and effective techniques on sentence retrieval. The pattern-based approach starts with the retrieval results from the TFISF techniques and adjust the believe scores of sentences according to sentence lengths and query-related patterns. We need to study how to combine information patterns with other retrieval approaches in addition to TFISF techniques, such as language modeling approaches, for further performance improvement. Other future work is to extend the pattern-based approach to novelty detection in other applications, such as new event detection, document filtering, cross document summarization and minimal document set retrieval etc. This work was support ed in part by the Center for Intelligent Information Retrieval, by SPAWARSYSCEN-SD grant numbers N66001-99-1-8912 and N66001-1-8903, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expr essed in this material are the author(s) and do not necessarily reflect those of the sponsor. [1] J. Allan, R. Paka, and V. Lavrenko,  X  X n-line New Event [2] Y. Yang, J. Zhang, J. Carbone ll and C. Jin,  X  X opic-conditioned [3] N. Stokes and J. Carthy,  X  X irst Story Detection using a [4] M. Franz, A. Ittycheriah, J. S. McCarley and T. Ward,  X  X irst [5] J. Allan, V. Lavrenko and H. Ji n,  X  X irst Story Detection in [6] D. Harman,  X  X verview of the TREC 2002 Novelty Track X , [7] J. Allan, A. Bolivar and C. Wade,  X  X etrieval and Novelty [8] H. Kazawa, T. Hirao, H. Iso zaki and E. Maeda,  X  X  machine [9] H. Qi, J. Otterbacher, A. Wi nkel and D. T. Radev,  X  X he [10] D. Eichmann and P. Srinivasan.  X  X ovel Results and Some [11] M. Zhang, R. Song, C. Lin, S. Ma, Z. Jiang, Y. Jin and L. [12] K.L. Kwok, P. Deng, N. Dins tl and M. Chan,  X  X REC2002, [13] Y. Zhang, J. Callan and T. Minka,  X  X ovelty and Reduncancy [14] E. M. Voorhees,  X  X verview of the TREC 2002 Question [15] S. E. Robertson,  X  X he Probability Ranking Principle in IR X , [16] Y. Yang, T. Pierce and J. Carbonell,  X  X  Study on Retro-[17] C. Zhai, W. W. Cohen and J. Lafferty,  X  X eyond Independent [18] T. Brants, F. Chen and A. Farahat,  X  X  System for New Event [19] X. Li and W. B. Croft,  X  X  valuating Question Answering [20] X. Li,  X  X yntactic Features in Question Answering X , Proc. [21] Daniel M. Bikel and Richard L. Schwartz and Ralph M. [22] I. Soboroff and D. Harman,  X  X verview of the TREC 2003 [23] J. Carbonell and J. Goldstein,  X  X he Use of MMR, Diversity-[24]  X  X emur Toolkit for Language Modeling and Information [25] X. Li &amp;W.B. Croft X  Novelty Detection Based on Sentence [26] W. Dai. and R. Srihari,  X  X inimal Document Set Retrieval, X  [27] C. Zhai,  X  X otes on the Lemur TFIDF model X , online notes at [28] S.-N. Kim, D. Ravichandran and E. Hovy,  X  X SI novelty track 
