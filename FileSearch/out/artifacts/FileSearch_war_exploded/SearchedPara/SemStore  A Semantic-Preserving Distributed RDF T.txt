 The flexibility of the RDF data model has attracted an in-creasing number of organizations to store their data in an RDF format. With the rapid growth of RDF datasets, we envision that it is inevitable to deploy a cluster of comput-ing nodes to process large-scale RDF data in order to deliver desirable query performance. In this paper, we address the challenging problems of data partitioning and query opti-mization in a scale-out RDF engine. We identify that ex-isting approaches only focus on using fine-grained structural information for data partitioning, and hence fail to localize many types of complex queries. We then propose a radically different approach, where a coarse-grained structure, namely Rooted Sub-Graph (RSG), is used as the partition unit. By doing so, we can capture structural information at a much greater scale and hence are able to localize many complex queries. We also propose a k-means partitioning algorithm for allocating the RSGs onto the computing nodes as well as a query optimization strategy to minimize the inter-node communication during query processing. An extensive ex-perimental study using benchmark datasets and real dataset shows that our engine, SemStore, outperforms existing sys-tems by orders of magnitudes in terms of query response time.
 H.2.4 [ Database Management ]: Systems X  distributed data-bases, query processing RDF, SPARQL, Partitioning, Query Processing
RDF (Resource Description Framework) is a simple yet powerful data model for representing information in the form of triple statements: (subject, predicate, object). Such sim-ple statements can be used to represent complicated rela-tionship among different resources. An RDF dataset can be modeled as a labeled and directed graph with each triple modeled as two vertices and a directed edge, labeled by the predicate, from its subject vertex to its object vertex. SPARQL is the standard query language of RDF data. Due to the flexibility and dynamicity of the RDF data model, an increasing number of communities making their data avail-able in the RDF format. The statistics from Linked Open Data Project 1 show that more than 31 billion triples had been published in RDF till Sep. 2011.

In the past decade, many researches focused on techniques for RDF data management and SPARQL query optimization in a centralized environment, such as [4,5,16,22,25]. With the rapid growth of the data sizes, a single machine would not be capable to deliver satisfactory query performance [5]. Consequently, it is highly desirable to develop techniques for distributed RDF data processing that are scalable to large RDF datasets. Recently, there exist many efforts devoted to building scale-out RDF engines [9,10,12 X 15,18,24]. In these engines, data are partitioned and allocated to multiple com-puting nodes and the processing of SPARQL queries often involves distributed join processing. As indicated in previous results [12,15,17], distributed joins are much more expensive than local joins due to their excessive communication cost. Therefore, the fundamental challenge in building a scale-out RDF engine is how to partition the RDF data across a computing cluster such that queries can be evaluated with minimum communication cost incurred by distributed joins.
The most common data partitioning algorithm is edge-based hash partitioning algorithm [9,10,14], which uses the edges in RDF graph as partition unit, and assigns the edges to each node by computing a hash key over either subject or object of the edges. More recent approaches [12, 15] use n -hop blocks as the partition unit. For each vertex v in RDF graph, an n -hop block anchored at v is formed by including all the vertices whose distance from v is less than or equal to n . These approaches then allocate the n -hop blocks to differ-ent partitions by using a hash function or a graph partition-ing algorithm. Using edges and n -hop blocks as the partition unit works well for queries with small scopes, which involve joins on a single vertex, such as star queries, or joins on ver-tices with limited distances. For queries that involve joins on vertices that are of greater distances, especially those with a long sequence of subject-object joins, they often have to resort to expensive distributed joins across multiple com-puting nodes. This can be attributed to the fact that they fail to make use of larger-scale structural information in the RDF graph but rather focusing on fine-grained information.
To address this problem, we propose a drastically different approach. Rather than using a fine-grained partition unit, such as edges or n -hop blocks, we adopt a coarse-grained unit, namely Rooted Sub-Graph ( RSG ), which is capable to capture structural information with a greater scope. We prove that any star, chain, tree and cycle queries would only involve joins on vertices within RSGs. Therefore, placing a complete RSG to only a single node would localize all these four common types of queries. In this paper, we particularly address the two major challenges in realizing this approach. First of all, as an RDF triple could appear in multiple differ-ent RSGs, there would be overlaps between RSGs and hence may potentially involve duplicate triples. To reduce the data redundancy and further localize more complex queries, we propose a k-means clustering algorithm to place the RSGs into partitions and design a suitable distance measure and centroid to capture the correlations among different RSGs. Second, for the queries where distributed joins are unavoid-able, we design a partition-aware query optimizer that can capitalize on the data partitioning strategy to generate op-timized plans at both compile time and run time.

In summary, our major contributions in this paper include the following:  X  We present a radically new RDF data partition method, which takes Rooted Sub-Graph as the partition unit, and prove that by doing so can efficiently localize the four afore-mentioned common types of SPARQL queries.  X  We present a k-means partitioning algorithm with the tailored distance measure and centroid to place the RSGs across a computing cluster. The algorithm can significantly reduce the data redundancy among the computing nodes and maximize the possibility of queries other than the four common ones to be processed locally. To deal with large datasets, we also present how to implement this algorithm using the popular MapReduce framework.  X  We provide a partition-aware query decomposition al-gorithm and a dynamic two-stage (compile-time and run-time) query optimization technique to reduce the cost of distributed join processing.  X  We introduce the architecture of SemStore, a semantic-preserving distributed triple store that implements all these techniques as integral parts, where each computing node in the cluster running an instance of a centralized RDF engine.  X  We conduct a comprehensive experimental study by comparing the performance of SemStore over two datasets, namely LUBM [8] and UniProt [2], with a number of exist-ing systems or approaches, including TripleBit [22], RDF-3X [16], Trinity.RDF [24], SHARD [18], Hybrid system [12, 15] and Hive.
RDF dataset is a set of triples in the form of (subject, predicate, object). In fact, an RDF dataset can be repre-sented as a directed labeled graph with subject and object as the vertices connected by an edge labeled by predicate .An RDF graph can be defined as follows:
Definition 2.1 (RDF Graph). An RDF graph is a di-rected labeled graph, denoted as G =( V, E, L E ) ,where V is a set of vertices, corresponding to subjects and objects, and E is a set of directed edges from the subjects to the objects. L
E is a set of edge labels, referring to the predicates associ-ated with the edges. A vertex with indegree as zero is called a source vertex .

Figure 1(a) shows an example of the RDF graph. Here we use IDs instead of URIs and Literals for simplicity. SPARQL is a standard query language for RDF data. A SPARQL query consists of triple patterns, each of which is a triple that contains variables in the subject, predicate or object. A SPARQL query Q can be represented as Q = { tp 1 , ..., tp n } ,where tp i (1 i n ) is a triple pattern and triple patterns are connected by common vertex (a constant or a variable). Similarly, a SPARQL query can be repre-sented as a directed graph. Thus, to evaluate a SPARQL query is to find all matches of a query graph pattern and each match is a subgraph of the RDF graph. Figure 1(b) shows a query graph and a match of this query graph.
We note that triple patterns are connected by shared sub-jects or objects, which means joins between triple patterns are on their shared subjects or objects. Thus, the typi-cal types of join in SPARQL are subject-subject join (S-S join), subject-object join (S-O join) and object-object join (O-O join). In this paper, we do not consider the predi-cate join, since predicate join is not common [7]. Based on the join types, we can classify the SPARQL queries into five categories, star query (only contains S-S join), chain query (only contains S-O join), tree query (contains S-S join and S-O join), cycle query (contains S-S join, S-O join and O-O join) and complex query denote the rest of more complex queries. Figure 2 shows an example. Distributed RDF Engines. With the rapid increase of the volume of RDF data, there are many recent efforts in de-signing scalable distributed RDF engines for processing big RDF datasets. To benefit from the scalability and fault tol-erance of the MapReduce framework [6], several systems [13, 18] directly store RDF data as files in HDFS (Hadoop Dis-tributed File System) and process SPARQL queries using Hadoop X  X  MapReduce programming model. The authors in [12,15] propose systems integrating Hadoop and RDF-3X, an efficient local RDF engine to achieve significant perfor-mance improvement in comparing to the pure Hadoop-based systems [13,18]. The main idea is to push as much joins as possible to the individual local RDF-3X engines by a well designed RDF data partitioning algorithm. The remaining distributed joins would be handled using MapReduce jobs. Even though such distributed joins would be much more expensive than the local ones, the authors did not provide any query optimization algorithm to improve their perfor-mance. Trinity.RDF [24] is one of the most efficient dis-tributed graph engine for web-scale RDF data, which uses main memory to store the RDF data and hence can achieve very low data access latency. Instead of using joins, the authors proposed an efficient operator, namely graph explo-ration, to perform SPARQL queries based on the MPI pro-tocol. While [24] is mainly focused on designing a scale-out system, our data partitioning algorithm can be employed in this system to further improve its performance by reducing the communication cost.
 RDF Data Partitioning Approaches. The most com-mon data partitioning approach is the edge-based hash par-titioning [9,10,14]. It distributes edges across different par-titions by computing a hash key of the subject (or object) vertex of each edge. The principle of edge-based hash parti-tioning is to group the edges that contain same subject as a  X  X tar X  X ndeachofsuchstarisplacedinasinglecomputing node. In this way, the edge-based hash scheme can local-ize subject-subject joins [12]. Figure 1(c) shows an example of the edge-based partitioning. The query in Figure 1(b) has to be decomposed into two subqueries, which only have subject-subject joins, as shown in Figure 1(c). Each of these subqueries can be executed in parallel without need for dis-tributed join. However, to obtain the final results, we should execute distributed join between the intermediate outputs produced by SQ 1 and SQ 2 on join key ? y ,whichleadsto network overhead.

In [12, 15], the authors proposed RDF data partitioning approaches using a different partition unit: n -hop blocks. For each vertex v in the RDF graph, an n -hop block an-chored at v is formed by including all the vertices whose distances from v is less than or equal to n . These approaches essentially allocate the n -hop blocks to different partitions by using a graph partitioner [12] (e.g. METIS) or a hash function [15]. These approaches can localize queries where there is a vertex whose distances to all the other vertices are at most n . Wang et al. [19] propose a more efficient graph partitioner that is able to partition billion-node graphs. Fig-ure 1(d) shows an example using the 1-hop hash partitioning method. The query in Figure 1(b) does not satisfy the con-dition that there exists a vertex whose distances to the other vertices are at most 1. Actually, this query should be decom-posed into two subqueries illustrated in Figure 1(d), both of which satisfy the condition. As proposed in [12, 15], they will be executed in parallel on the computing nodes using the local RDF database engine and a distributed join will be performed on the intermediate results to generate the final ones.

Using the methods based on n -hop blocks, in order to lo-calize more complex queries that has a greater diameter, one has to increase the value of n . However, even a moderate n value may incur a very large block size, especially for those blocks anchored at vertices with high degrees. This would lead to a large number of duplicate triples and a skewed data distribution among the partitions. The duplicate triples would incur higher query processing cost as each local engine has to process more data and the distributed joins may be executed over more intermediate results. Furthermore, the skewed data distribution may render some computing nodes become the system X  X  bottleneck. Consequently, the best n value reported in [12, 15] is around 2. More experimental results in Section 6.1 further verify these problems.
Another research direction is dynamic run-time data par-titioning, which adapts the data partitioning scheme accord-ing to the run-time changes of system workload [20,21]. Our data partitioning algorithm can also be used as the initial partitioning method in [20, 21]. Moreover, the idea of us-ing a coarse-grained partition unit, RSG , can be applied to the dynamic partitioning algorithms to further improve their performance.
SemStore is designed to provide a massively scale-out sys-tem that can run on a cluster of servers for managing big RDF datasets. Figure 3 illustrates the architectural design of the prototype of SemStore. SemStore has three main components: data partitioner, a master node and a num-ber of computing nodes. The data partitioner partitions the entire RDF dataset into multiple subsets. Each comput-ing node receives one subset of triples and builds the local data indices and statistics for local join processing and op-timization. The master node is responsible for constructing a distributed query plan and coordinating distributed data transmission and processing among the computing nodes.
In our prototype system, each computing node is running a single-node RDF engine. While our system architecture allows the embedding of different single-node RDF engines, in our experiments, we use TripleBit [22], an open-source centralized RDF engine.
 Data Partitioner. When RDF datasets are loaded into the system, we first build some mapping dictionaries to replace all strings (URIs, literals and blank nodes) by IDs. The data partitioner will partition and allocate the triples to the com-puting nodes. The partitioning method in SemStore adopts a new partition unit, called Rooted Sub-Graph (RSG). By using RSGs as the partition unit, we can effectively localize all the queries in the form of a star, a chain, a tree and a cycle, which are very common in SPARQL queries. To local-ize more queries of other types and reduce data redundancy, we leverage a k-means partitioning algorithm for allocating the highly correlated RSGs into the same computing node. After the partitioning, the data partitioner will also build a global bitmap index over the vertices of the RDF graph and collect the global statistics.
 Query Processing and Optimization. In SemStore, all SPARQL queries will be first submitted to the master node. Upon receiving a query, the master node parses the query and determines which category this query belongs to, local-query or distributed-query .IfaSPARQLquerycanbe executed in parallel and locally without collaboration be-tween computing nodes, this query is a local-query. Other-wise, it is a distributed-query, which needs to join data from multiple partitions and hence has to be evaluated collabora-tively by more than one computing nodes. For a distributed-query, The query decomposer will generate a query decom-position plan consists of several subqueries, each being a local-query. Then distributed query optimizer will generate a global query plan at compile-time which determines the joining order of results returned by the subqueries. Further-more, each local-query will be sent to the computing nodes holding its results and the local optimizer on each comput-ing node will optimize each local query at run-time based on the local statistics. In this section, we present our data partitioning method. We start by presenting the rationale behind the choice of RSG as the partition unit. Then we present a scalable k-means partitioning algorithm to assign partition units, that canbeimplementedontheMapReduceframework.
As discussed earlier, the reason that the aforementioned approaches do not work well for queries involving vertices of greater distances, such as chain, tree, cycle and some com-plex queries, is because they only take use of fine-grained structural information and fail to capture the RDF graph structure with a greater scope. Furthermore, a previous em-pirical study [7] analyzed queries generated by both humans and machine agents over two datasets, and concluded that the most common types of joins are S-S joins (60%) and S-O joins (35%). S-S and S-O joins would mainly form queries as star queries, chain queries and tree queries. Hence a good partition unit should be optimized for these types of queries.
To address this problem, we propose a new partition unit, namely Rooted Sub-Graph (RSG), defined as follows:
Definition 4.1 (Rooted Sub-Graph). Given an RDF graph G =( V, E ) and a vertex r  X  V as the root, a sub-of G rooted at r if and only if the following conditions hold: (1) for each v  X  V , if there is a directed path from r to v then v  X  V rsg ( r ) , (2) for each pair of vertices ( u, v ) ,if Algorithm 1: RSGs Generation
Below we propose an algorithm to generate a set of RSGs from a general RDF graph such that all star, chain, tree and cycle queries are localized.

The critical step of the algorithm is to select a set of root vertices. First, all the source vertices, i.e. vertices with in-degrees as zero, are included in the set of root vertices. Then all the vertices that are reachable from each root vertex will form a RSG. For example, in Figure 4, the sub-graph rooted at source vertex 4 in partition S 3 is one of the RSGs of the RDF graph.

Second, there could be some vertices that not reachable from any source vertex. For example, in Figure 4, none of the source vertices (i.e. vertex 2 , 3 , 4) can reach the directed cycle 1  X  5  X  8  X  1. To handle such situation, we choose the vertex with minimal ID in such a cycle as a root, i.e. vertex 1 in this example. Additional RSGs can be generated accordingly for the given set of roots produced by this step. Finally we can get a collection of RSGs that cover all the vertices in the RDF graph.

The benefits of our RSGs generation algorithm are two-fold. First, the algorithm can be easily parallelized, for ex-ample using the MapReduce framework. Second, by using the RSGs generated by our algorithm as the partition units, all the four types of common queries can be processed as local-queries. In the rest of this section, we first present the details of the algorithm and then prove its ability to localize complication queries.
 Algorithm. The pseudo code of the RSG generation algo-rithm is illustrated in Algorithm 1. A set of vertices of an RSG rsg r is denoted as V ( rsg r ), where r is its root. An ex-ample is given in Figure 4. It is implemented by a series of MapReduce jobs, which iteratively propagate information of each vertex v to its out-neighbors. The information include the minimum-ID of the vertices that can reach v , i.e. v m the psuedo code, and the currently chosen roots of the RSGs that can cover v , i.e. r in the psuedo code. Specifically, this algorithm involves the following steps:
Step 1 (line 1-4): generate the information attached to each vertex v in the form of ( v, ( v m ,r )). Initially v m is v itself and r contains v if v is a source vertex or otherwise r is an empty set. Then all vertices are flagged as active.
Step 2 (Map1 and Reduce1): if a vertex is active, gener-ate the candidate v m and r for all its out-neighbors.
Step 3 (Map2 and Reduce2): for each vertex v , merge all messages of v . If there is a new minimum-id or root, then set v active and update v . Otherwise, deactivate v .
Step 4 (line 6-9): iterate Step 2 and Step 3, until there is no update occurred.

After invoking Algorithm 1, the RSGs rooted at all source vertices are found. Then for the vertices in the cycles that are not reachable from any source vertex, we choose the minimum-id as the input root, then invoke Algorithm 1 again to generate the remaining RSGs.
 Theorem 4.1 (Completeness). Given an RDF graph G =( V, E ) , if it is decomposed into a set of RSGs by Al-gorithm 1, then each vertex v and edge ( u, w ) in G can be found in at least one RSG.

Proof. We prove this by contradiction. (i) Assume there exists one vertex v  X  V , which cannot be found in any RSG. If v is a source, then v is in an RSG rooted at v .Therebywe derive a contradiction. If v is not a source and there exists a source r can reach v ,then v is in the RSG rooted at r .If v is not a source and none of the sources can reach v , then there must exist a minimal-ID vertex can reach v and v is in the RSG rooted at that vertex. Again we derive a contradiction. (ii) Assume there exists one edge ( u, w )  X  E , which cannot be found in any RSG. By (i) we have that u must at least be in one RSG. we conclude that ( u, w )isalsointhatRSG by Definition 4.1. Thus, we derive a contradiction.
Theorem 4.2. Using the RSGs generated by Algorithm 1 as the partition unit, then all star, chain, tree and cycle queries are local-queries.
 Proof. For brevity, we give a brief sketch of the proof. For each type of queries, there must be a vertex v in this query that can reach all other vertices. If v is a constant, then there must be a root vertex r that can reach v .If v is a variable, for each binding of v ,theremustbearootvertex r that can reach that binding. Thus, each match of this query is a subgraph of one RSG.
In this seciton, we will present how to optimized the al-location of RSGs to the computing nodes. Firstly, a k -way RSGs partitioning plan can be defined as follows. Definition 4.2 ( K -way RSGs Partitioning Plan). Given an RDF graph G and its set of RSGs S ,a k -way RSGs partitioning plan P contains k nonempty and disjoint subsets of RSGs, P = { S 1 , ..., S k } ,where k i =1 S i
To optimize the RSGs partitioning plan, we should con-sider the following metrics.
 Balance. A well balanced data distribution plays an impor-tant role for efficient parallel query processing. A substan-tial skewed data distribution may lead to some undesirably overloaded partitions, which become the performance bot-tlenecks for the whole cluster. This is because that the over-loaded partitions may exceed the memory capacity of a sin-gle machine. To avoid the substantial overloaded partition, here, we use the number of RSGs contained in a partition S to measure its payload, which is denoted by | S i | . Data Redundancy. IftwoRSGshaveoverlapsandthey are assigned to different partitions, then duplicate triples will be stored at both partitions. Such data redundancy would cause excessive redundant computations cost. Hence, we should control the overlaps of RSGs by assigning the correlated RSGs to the same partition. In particular, we have the following observation.

Observation 4.1. If two RSGs share a vertex v ,then they also share the RSG rooted at v .

For example, in Figure 4, rsg 2 and rsg 3 share the vertex 6, then they share the RSG rooted at vertex 6, i.e., 6  X  9  X 
In addition, joins always occur on shared vertices between the triple patterns specified in an SPARQL query (recall Section 2.1). Hence, to localize more joins, we have the following observation.

Observation 4.2. If all the triples that contain shared vertices between any pair of query triple patterns in a query are allocated in the same partition, then this query is a local-query.

Based on the above observations, we know that if a ver-tex is shared by multiple different RSGs, then by placing all these RSGs in one partition can localize all the possible joins on this vertex. It will also reduce the duplicate triples connected with this vertex. Therefore, to measure the cor-relation between two RSGs, we use the difference between their sets of vertices. It is also called the distance between the two RSGs.

To better understand the distance function, we first give a simple way of extending the distance between RSGs to a real-value function. Let V ( rsg )bethesetofallthevertices in rsg . V ( rsg ) can be denoted as a | V | -dimensional binary of all the vertices in RDF graph G and v ( j ) rsg  X  X  0 , 1 v  X  V is in V ( rsg ), then the v ( j ) rsg is 1 or 0 otherwise. Then, given two RSGs rsg and rsg , their distance can be defined as follows: d ( rsg, rsg )= d ( v rsg , v rsg )=
Then the intra-correlation (called IC )ofaRSGsparti-tioning plan P is defined as:
IC ( P )= 1 A plan with a lower IC tend to be more efficient in executing SPARQL queries, since it collocates more correlated RSGs, hence localizes more joins and reduce more duplicate triples.
GivenanRDFgraph G , our goal is to find the optimal partitioning plan P = { S 1 , ..., S k } for the RSGs partitioning problem. According to the metrics, the objective function for this problem is defined as follows: Algorithm 2: RSG-based K-means Partitioning where | R | indicates the number of RSGs in this graph.
We can reduce the balanced graph partition problem, which is an NP-hard problem, into our problem (the proof is omit-ted due to the limited space). Hence our problem is also NP-hard. Our goal is to find a reasonable heuristic-based approach.
We use the k-means partitioning algorithm to find an ap-proximate solution.
 Distance and Centroid. The k-means partitioning algo-rithm requires a proper definition of the centroid of a cluster to represent all vertices belong to this cluster. Here, we use S . Different from the vector v rsg , each dimensional value of
C i is a real number in [0 , 1]. Then we can calculate the value of C i on each dimension as follows: According to the above formula, the centroid not only repre-sents all vertices belong to this cluster, but it also indicates the number of RSGs each vertex belongs to in this cluster.
In the k-means partitioning algorithm, we use the objec-tive function in Eq. 4.2 to measure the distance between RSGs and the centroid of a partition. Formally, for each partition S i , the objective function of the partitioning can be defined as follows: Full Algorithm. By combining the elements defined above, we can now present the RSG-based k-means partitioning algorithm. The pseudo code is presented in Algorithm 2, which runs in the following steps:
Step 1: Randomly assign the RSGs into k partitions, where k is the number of computing nodes in the cluster.
Step 2: For each RSG rsg , we find the nearest and min-imal loaded partition S i from the set of non-fullly loaded partitions (i.e., | S i | &lt; | R | k ).
 Step 3: For each S i ,updatethecentroid C i .
 Step 4: Repeat Step 2 and Step 3 until it converges.
Step 5: Finally, we use the partitioned sets of roots to get the final partitioning results, i.e. extending the V ( rsg a real RSG with triples.
 Figure 4 gives a partitioning result using this method. One can see that the query in Figure 1(b) can be executed locally and the duplicate triples produced are less than those produced by 1-hop based hash partitioning (Figure 1(d)).
We present a partition-aware query decomposition scheme to maximize the localization of query operations and to re-duce the number of local subqueries such that the intermedi-ate results shipped across computing nodes will be reduced significantly. A SPARQL query Q can be represented as a graph that contains variables in the subject, predicate and object. Thus, we first identify all the source vertices of query graph Q and use them as the roots to partition Q intoaset of RSGs (subqueries) SQ , each of which is a local-query (Theorem 4.2). For example, consider the following query: tp1: (&lt;AssociateProfessor0&gt; ub:teacherOf ?y) tp2: (?x ub:takesCourse ?y) tp3: (?x rdf:type ub:GraduateStudent) tp4: (?y rdf:type ub:GraduateCourse) This query can be decomposed into two subqueries. The first is { tp1, tp4 } with root  X  &lt; AssociateProfessor0 &gt;  X  X nd the second is { tp2, tp3, tp4 } with root  X  ?x  X .
In the next step, for each individual subquery in SQ ,we search for the computing nodes that contain its results. To speed up this process, we construct an index to map each vertex in the RDF graph to the computing nodes holding the partitions that contains the vertex. By using this index, the exact computing nodes that contain the results of a subquery SQ i can be retrieved as follows:
Case 1: if SQ i  X  X  root is a constant, then each computing node that contains this root has complete results of SQ i We call each of such computing nodes as a matching node to SQ i .

Case 2: if SQ i  X  X  root is a variable, then the intersection of the locations of all the constant in SQ i are the nodes holding the results. Here, if the intersection contains exactly one node, then we call it the matching node of SQ i .
Then, for each pair of subqueries, if they have a shared matching node, then we can merge these two subqueries into a new subquery, which is guaranteed to be a local-query. Finally, this will produce a minimal number of subqueries based on our partitioning algorithm, and each of such sub-queries is a local-query.
If a query is decomposed into two or more subqueries, we need to define the global execution workflow to process the distributed and local joins such that the query will be processed with minimal communication overhead. The crit-ical issue is to determine ordering of query operations [16]. Here we adopt a typical two-stage scheme for physical query plan generation. First, the join order between sub-queries is calculated by the master based on the global statistics at compile-time. Then the join ordering within each sub-query is computed locally by individual computing nodes at run-time using the statistics maintained at each computing node.
 Compile-Time Optimization. After query decomposi-tion, we represent each subquery as a bipartite graph whose vertices can be divided into two disjoint sets T and Var . Each vertex in T represents a triple pattern in the subquery and each vertex in Var represents a variable appears in the subquery. If a variable appears in a triple pattern, then an edge connects their corresponding vertices. Figure 5(a) shows an example.

In the master, we only generate the order of the joins between the subqueries and the cardinality of the results of these subqueries can be estimated by the statistics stored in the master (Section 5.3). In particular, we use dynamic programming to build a left-deep tree, in which each leaf node is a subquery. Assume that a query is decomposed into n subqueries, a subplan L k that contains k subqueries is an ordered sequence of subqueries SQ (1) , ..., SQ ( k ) the remaining set of subqueries is denoted by SQ rem .Then the state transition equation can be defined as follows: After building the left-deep tree, we will search the tree bottom-up to remove the redundant triple patterns which might lead to redundant computations. For example, in Figure 5(a), the tp 4 in SQ (2) will be removed.
 Run-Time Optimization and Evaluation. The join or-dering within each subquery will be generated locally at each computing node. Specifically, when SQ ( i ) is performed, the results of SQ ( i ) will be transferred to the computing nodes holding the results of SQ ( i +1) . In each of these computing nodes, the results of SQ ( i ) will be joined with other triple patterns. The local optimizer in each computing node will optimize the local joins involved in SQ ( i +1) independently. The join ordering problem has been well studied and is out of the scope of this paper. In our prototype, we use the join ordering algorithm proposed in TripleBit. Due to the optimization at compile-time, the size of transferred inter-mediate results is minimized. Figure 5(b) shows an example that SQ (1)  X  X  results are forwarded to node 2 and node 3 which contain the results of SQ (2) . Global Statistics. In the prototype, we build the global statistics to store the number of triples matching each possi-ble single triple pattern. In SPARQL queries, there are seven forms of triple patterns, which are (? s, ? p, ? o ), ( s, ? p, ? o ), tively. For the triple patterns that only have one variable, such as ( s, p, ? o ), we store the number of triples matching each triple pattern. For the triple patterns containing two or more variables, in addition to the number of triples match-ing this triple pattern, we also store the number of distinct bindings for each variable. Take ( s, ? p, ? o )asanexample. For each constant s in the RDF data, we will store the num-ber of triples with s as their subjects as well as the number of distinct predicates and the number distinct objects within these triples.
 Estimation of Cardinality. The cardinality of a selec-tion or a join operation is the number of results that sat-isfy the selection or join condition. Using the aforemen-tioned global statistics, we can find the cardinality of a given triple pattern tp , denoted as Card ( tp ). If two triple patterns tp 1 and tp 2 join on shared variable(s), denoted as J = Var ( tp 1 )  X  Var ( tp 2 ), then the cardinality of the results of the join between these two triple patterns is estimated in the following way:  X  If J = Var ( tp 1 )= Var ( tp 2 ), then:  X  If J = Var ( tp 1 )and J Var ( tp 2 ), then Card ( tp 1 tp 2 )= min { Card ( tp 1 )  X   X  If J Var ( tp 1 )and J Var ( tp 2 ), then where B ( J, tp ) indicates the number of distinct bindings of J in the results of tp .

The number of distinct bindings of a variable (or a set of variables) in all permutation of single triple patterns can be found in the statistics. However, the number of distinct bindings of a variable (or a set of variables) J in the results of a join between tp 1 and tp 2 , can be estimated as follows: B ( J, tp 1 tp 2 )=min { B ( Var ( tp 1 ) ,tp 1 )  X  B ( J  X  where J  X  Var ( tp 1 )  X  Var ( tp 2 )and B (  X  ,tp )=0.
We compare SemStore with six state-of-the-art RDF en-gines, including two centralized engines, RDF-3X [16] and TripleBit [22,23] and four distributed engines, SHARD [18], Hybrid system [12,15], which is an engine integrating Hadoop and RDF-3X, Hive and Trinity.RDF [24]. SHARD adopts an edge-based partitioning. Hive uses vertical partition-ing [3] and RCFile [11] techniques. For the Hybrid system, we implement two typical data partitioning algorithms as proposed in [12], which uses METIS [1] as the graph parti-tioning tool and applies undirected one-hop block and undi-rected two-hop block as the partition unit, denoted as un-one and un-two respectively. We also implement semantic hash partitioning algorithm in [15], 2-hop forward, denoted as 2f , which is reported in [15] to perform the best in most cases. Finally, SemStore is implemented using C++, com-piled with GCC, using -O2 option to optimize. We refer to the RSG-based k-means partitioning algorithm as RSG-KM . We also implement a baseline RSG-based partitioning algorithm, called RSG-Hash , which assigns each RSG to the partitions by hashing the root of the RSG.

Unless otherwise stated, our experiments use a default cluster consisting of 1 master and 16 computing nodes, each of which has one processor at 2.4GHz and 4GB RAM, and we run the centralized engines on a powerful single machine with 4 way 4-core 2.13GHz CPU and 64GB RAM.

We choose two datasets LUBM [8] and UniProt [2] for the experiments. LUBM is a benchmark generator. In the experiments, we generate LUBM datasets with 1000, 2000, 5000 and 10240 universities with the size ranging from 138 million of triples to about 1.4 billion of triples, which are denoted as LUBM-1000, LUBM-2000, LUBM-5000 and LUBM-10240 respectively. The UniProt dataset is a real-life protein dataset that contains nearly 2 billion of triples. Our experiments feature 16 queries (listed in Appendix), cover-ing most types of queries mentioned earlier in this paper, as shown in Table 1.
Due to the large memory consumption of METIS, we can-not get it to work on large datasets and we can only perform the Hybrid (un-one) and Hybrid (un-two) over the LUBM-2000 dataset.
 Data Loading Time. InTable2,wecomparethedata loading time of the different distributed engines, which in-clude the time to perform the data partitioning algorithms. Hive has the fastest data loading time. This is because the vertical partitioning scheme is simple and there is no re-dundancy. SemStore and Hybrid (2f) need more loading time than Hive, because these two algorithms are much more complex than vertical partitioning. SHARD is the slowest due to the fact that it requires to covert the RDF data to a special HDFS file format and hence incurs an excessive data loading cost. Moreover, Hybrid (un-one and un-two) spend more time than Hybrid (2f) due to the extra cost of graph partitioning and the large amount of redundant triples that needs to be stored and processed.
 Redundancy. Since 2f, un-one, un-two, RSG-KM and RSG-Hash generate overlapping partitions, we report the ratio of total number of triples produced by each partition-ing algorithm to the original datasets, as shown in Table 3. Duetotheexistenceofhighdegreevertices,thenumbers of redundant triples in un-one and un-two are very large. Moreover, with the hop count increasing from one to two, the number of duplicate triples grows rapidly. The 2f al-gorithm reduces the duplicate triples significantly, which is consistent with what is reported in [15]. Since the over-laps between RSGs are very large, using hash functions to allocate RSGs will incur redundant triples. The RSG-KM achieves a dramatic reduction on the duplicate triples in comparing to RSG-Hash, because the k-means partitioning algorithm attempts to assign highly correlated RSGs into one partition.
 Data Distribution. Table 4 shows the standard deviation of the number of triples allocated to each computing node. The results show that RSG-KM achieves the most balanced data distribution across computing nodes. RSG-Hash, 2f and Hive are not as balanced as RSG-KM, but they are better than un-one and un-two. Due to the existence of high degree vertices, the data skewness in un-one and un-two is high.
 Table 4: Data Distribution (Standard Deviation) Comparison with Centralized Engines (LUBM-2000).
 We first compare SemStore with centralized engines, TripleBit and RDF-3X over LUBM-2000. The results are shown in Figure 6(a). To better analyze the differences in perfor-mance, we classify the queries into two categories. Queries of the first type are Q4, Q5, Q6, Q9 and Q10, which are highly selective and called low-load queries. Q1, Q2, Q3, Q7andQ8arethesecondtypewithlow-selectivityanda large input, called high-load queries.

For low-load queries (Q4, Q5, Q6 and Q9), the perfor-mances of SemStore and TripleBit are better than RDF-3X. This is because these queries are local-queries for SemStore. And the intermediate results are significantly reduced by the selective triple pattern scanning. An interesting data point is Q10, which has to be decomposed into two sub-queries in SemStore. The more subqueries incur more net-work communication cost. However, benefiting from query decomposition and optimization strategies, intermediate re-sults shipped across network are reduced dramatically. Thus Q10 in SemStore takes an acceptable query response time which is less than 0.2 second.

On the contrary, high-load queries with low selectivity and big input and output sizes are executed more efficiently and effectively by SemStore. This can be attributed to the high degree of parallelism of SemStore. Even though the cluster running SemStore and the single server running the central-ized engines have the same total amount of main memory, SemStore can better take use of the more number of CPUs and the higher I/O bandwidth in the cluster.
 Comparison with Distributed Systems (LUBM-2000).
 Then, we compare SemStore with the existing distributed RDF data engines: Hybrid (un-one), Hybrid (un-two), Hy-brid (2f), Hive and SHARD. Figure 6(b) shows the experi-mental results. The first observation is that SemStore out-performs other systems dramatically, even more than an or-der of magnitude on some benchmark queries. This is be-cause, except Q4, all queries are local-queries for SemStore, such that each of them can be processed in parallel without cross-node interactions.

For the Hybrid system, queries are decomposed into mul-tiple subqueries and each subquery is processed in parallel by the individual RDF-3X engines on the computing node. Then the results of each subquery are joined by the Hadoop system to generate the final results. Benefit from perfor-mance of RDF-3X and the partitioning algorithm, the Hy-brid system with different partitioning methods can outper-form Hive and SHARD. Hive has a better performance than SHARD due to the query optimization and the RCFile [11] techniques. Without query optimization, SHARD produces a large amount of intermediate results, which lead to a sig-nificant impact on query performance.

Furthermore, the three different partitioning algorithms in the Hybrid system also have significantly different perfor-mance. First of all, different data partitioning algorithms Table 5: Performance on LUBM-10240 (Time in ms ) imply different query decomposition plans for each query, which would lead to significantly different cost for the dis-tributed joins running on the MapReduce framework. Par-ticularly, Q1, Q3, Q6, Q7, Q9 and Q10 need to be decom-posed in Hybrid (un-one). Q9 needs to be decomposed in Hybrid (un-two) and Q9 and Q10 need to be decomposed in Hybrid (2f). The second factor is the skewness of data dis-tribution and the number redundant triples. For high-load queries, such as Q1 and Q8, although they do not need to be decomposed in Hybrid (un-two), their performance in Hy-brid (un-two) is the worst among all the partitioning meth-ods in the Hybrid system. This is because un-two method causes skewed data distribution and a large amount of re-dundant (see Table 3 and Table 4), and the processing time depends on the computing node with the largest amount of triples, which is the bottleneck in the cluster.

All in all, the data partitioning algorithm has a rather de-cisive effect on the performance of distributed RDF engines. Performance on Larger Datasets. We experiment on two large dataset, UniProt and LUBM-10240, to analyze the performance of SemStore over large-scale datasets. We exclude SHARD in this section due to its generally low per-formance. For the UniProt dataset, Q1, Q2, Q3 and Q6 are low-load queries, and Q4, Q5 are high-load queries. The results are presented in Figure 6(c). One can make a conclu-sion similar to our previous experiments. Hybrid (2f) per-forms especially worse for Q3 and Q6 simply because they have to be decomposed in Hybrid (2f) and the distributed joins of the large intermediate results are pretty costly.
Table 5 summaries the experimental results over LUBM-10240 dataset. Trinity.RDF [24] is the current state-of-the-art of distributed RDF engine. However, Trinity.RDF is not openly available. To compare with Trinity.RDF, we use the the same dataset and benchmark queries over a similar clus-ter setup (5 computing nodes with 24GB RAM connected by 1GBit network bandwidth in our cluster as opposed to 5 computing nodes with 96GB RAM connected by 40GBit network bandwidth in [24]). And the query run times of Trinity.RDF in Table 5 are those reported in [24]. One can see that, SemStore outperforms other systems for all the queries. This is because that each of these queries can be executed locally in SemStore. Moreover, due to the well-balanced partitioning scheme, there is no bottleneck in the cluster. Varying Size of Cluster. This section reports the compar-ison of three engines with varying size of computing cluster. In particular, we run queries on the LUBM-2000 dataset on clusters with 1, 5, 10 and 16 computing nodes respectively. For brevity, we only use six queries, which cover all types of queries. The results are displayed in Figure 7(a), 7(b) and 7(c). We normalize the query execution time of each query to single-node execution time, and include a linear speedup line (1, 0.2, 0.1 and 0.06 for 1, 5, 10 and 16 com-puting nodes) for comparison. Hybrid (un-one) and Hybrid (un-two) have similar experimental results. Hive achieves a poor performance. Thus, we only show the Hybrid (un-two) and Hybrid (2f) in this comparison.

For SemStore in Figure 7(a), high-load queries (Q1, Q8 and Q7) achieve super linear speedup as the number of com-puting nodes increases. This is because, on one hand, each query can be performed locally without communication cost. On the other hand, due to the well-balanced partitioning scheme, there is no bottleneck computing node. In Q5 and Q9, due to the high selectivity of each query, even in a single-machine these queries can be executed efficiently (within 2 ms). Thus, the execution times from 1 to 16 computing nodes are roughly the same. Q10 is a low-load query which needs to be decomposed in SemStore. Since the interme-diate results shipped across computing nodes are reduced significantly by the query decomposer and optimizer, the performance of multi-nodes is little lower than that of the single-machine.

For Hybrid (un-two), Q1, Q7, Q5, Q8 and Q10 have few benefit when going from 1 to 16 computing nodes. This is because the data distribution of Hybrid(un-two) is extraor-dinarily skewed, as discussed in Section 6.1. The overloaded partitions become the bottleneck of query processing. For Q9, this query needs to be decomposed when processing, which leads to large amount of intermediate results trans-ferring. Therefore, the performance of multi-nodes is much lower than the performance of the single-machine which do not need to partition the dataset.

For Hybrid (2f), since 2f algorithm reduces the duplicate triples significantly and achieves a more balanced data dis-tribution in comparing to un-two, the high-load queries (i.e. Q1, Q7 and Q8) achieve linear speedup. Q9 and Q10 are low-load queries for 2f, however, they need to be decom-posed. Thus, the performance of multi-nodes is also much lower than that of the single-machine.
 Varying Data Sizes. Figure 7(d) shows the query exe-cution time of SemStore over four LUBM datasets. As the number of triples increases, the execution time of each query also increases. We observe that the increase of execution time is sublinear or nearly linear, which means SemStore achieves a good scalability. In particular, Q5, Q9 and Q10 are low-load queries and hence with growth of data size, their processing time do not increase much. Q1, Q8 and Q7 are high-load queries. Hence, benefiting from the well-balanced data distribution, low data redundancy and minimal query decomposition, the processing time increases almost linearly.
In this paper, we present SemStore, a semantic-preserving distributed RDF triple store for scalable SPARQL query processing. We design a partitioning algorithm for RDF data using RSG, a coarse-grained structure, as the parti-tion unit. Furthermore, we proposed a k-means partitioning algorithm to place highly correlated RSGs in the same par-tition. A query decomposition algorithm and a query opti-mization strategy are proposed to minimize the communica-tion cost for query execution. The experimental study shows that SemStore outperforms the state-of-the-art systems by orders of magnitude in terms of query response time.
The research is supported by the NSFC under grant No. 61133008 and National High Technology Research and De-velopment Program of China (863 Program) under grant No.2012AA011003. Ling Liu acknowledges the partial sup-port by the National Science Foundation under grants IIS-0905493, CNS-1115375, IIP-1230740, and a grant from Intel ISTC on Cloud Computing.
