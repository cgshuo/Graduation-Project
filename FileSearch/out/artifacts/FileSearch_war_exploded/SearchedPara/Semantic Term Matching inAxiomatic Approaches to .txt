 A common limitation of many retrieval models, including therecentlyproposedaxiomaticapproaches,isthatretrieval scores are solelybased on exact (i.e.,syntactic)matching of terms in the queries and documents, without allowing dis-tinct but semantically related terms to match each other and contribute to the retrieval score. In this paper, we show that semantic term matching can be naturally incor-poratedintotheaxiomaticretrievalmodel throughdefining the primitive weighting function based on a semantic simi-larityfunctionofterms. Wedefineseveraldesirableretrieval constraints for semantic term matching and use such con-straints to extend the axiomatic model to directly support semantic term matching based on the mutual information of terms computed on some document set. We show that such extension can be efficiently implemented as query ex-pansion. Experiment results on several representative data setsshowthat, withmutualinformationcomputed overthe documents in either the target collection for retrieval or an externalcollectionsuchastheWeb,oursemanticexpansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model. As a pseudo feedback method, our method also outperforms a state-of-the-art language modeling feedback method.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Experimentation Keywords: Axiomaticmodel,retrievalheuristics,constraints, query expansion, feedback
Theaxiomaticapproachtoinformationretrievalwaspro-posed recently as a new retrieval framework, in which rele-vance is modeled by term-based retrieval constraints [5, 6]. Several new retrieval functions have been derived by using this approach and shown to be less sensitive to parameter settingthanexistingretrievalfunctionswithcomparableop-timal performance; using a fixed parameter value can often achieve near-optimal performance in most test sets [6]. Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.
However,likemosttraditionalretrievalmodels,thesenew axiomatic retrieval functions also have the limitation that retrieval scores are solely based on exact (i.e., syntactic) matching of terms in the query and documents, without allowing distinct but semantically related terms to match each other and contribute to the retrieval score. Since it is unlikely that the authors of relevant documents always useexactlythesametermsasauserwoulduseinaquery, suchalimitationmakestheretrievalperformanceofexisting models non-optimal. For example, given the query  X  X ar X , intuitively, a single-term document with the term  X  X ehicle X  shouldhaveahigherscorethanasingle-termdocumentwith the term  X  X ish X  because  X  X ar X  is semantically more related to  X  X ehicle X  than  X  X ish X . However, existing retrieval models do not directly support such semantic matching and would treat both documents equally as not matching the query. Although techniques such as query expansion and pseudo-relevance feedback can support semantic term matching to certainextent,queryexpansionpurelybasedonsemanticre-lations between terms have so far not been very successful, presumably because of the difficulty in assigning appropri-ate weights to the new terms [26, 14]. Pseudo feedback is much more successful, but the semantic matching exploited is restricted by the top-ranked documents and it does not allow us to incorporate external resources.

In this paper, we show that there is a natural way to incorporate semantic term matching to the inductively de-fined axiomatic retrieval functions  X  all we need to do is to generalize the primitive weighting function to incorporate the semantic similarity of terms. Specifically, we follow the spiritofaxiomaticapproaches[5,6]andformallydefinesev-eral constraints on semantic term matching. We then use theseconstraintstoprovideguidanceonhowtocomputethe term semantic similarity and how to regularize the weights oftheoriginaltermsandthesemanticallyrelatedterms. We show that our method can be implemented as query expan-sion in the axiomatic framework, and when term similarity is computed using feedback documents, it can also be re-garded as a method for pseudo feedback in the axiomatic approaches. We conduct experiments over several represen-tativedatasets. Theresultsshowthattheproposedseman-tic expansion works well for all the six inductively defined axiomaticretrievalfunctions,significantlyimprovingthere-trievalaccuracyovertheoriginalbaselineaxiomaticretrieval functionsonallthedatasetsweexperimentedwith.More-over,theanalysisofsemantictermmatchingconstraintscan predict parameter boundaries that are consistent with the empirically discovered optimal ranges of parameters. Fur-thermore, as a pseudo feedback method, our method out-performs a state-of-the-art language modeling approach for pseudofeedback[30]duetoitscapabilityofselectingbetter terms for query expansion.

The rest of the paper is organized as follows. We discuss related work in Section 2 and briefly review the existing axiomatic approach to IR in Section 3. We then present our work on incorporating semantic term matching to the axiomatic framework in Section 4, and discuss experiment results in Section 5. Finally, we conclude in Section 6.
Manystudieshave triedtobridge thevocabulary gapbe-tweendocumentsandqueriesintraditionalretrievalmodels, mostly based on either co-occurrence-based thesaurus [11, 24, 18, 20, 10, 29, 23, 2] or hand-crafted thesaurus [26, 12]. Some researchers used both [14, 4]. Although our general strategy would be applicable to exploit both types of the-sauri, in this paper, we focus on the use of co-occurrence-basedthesaurusandleaveotherpossibilitiesasfuturework.
The earliest study of co-occurrence-based thesaurus can be traced back to the early sixties. Lesk [11] studied term expansion in the vector space model, where term similarity is computed based on the cosine coefficient [22]. Smeaton etal. [24]studiedqueryexpansionbasedonclassicalproba-bilistic model. These previous studies suggested that query expansion based on term co-occurrences is unlikely to sig-nificantly improve performance [18]. Qiu et al. [20] showed that adding terms that have the greatest similarity to the entire query, rather than individual terms, can obtain more improvement. Xu et al. [29] showed that the analysis of word occurrences and relationships on a local set of docu-ments(i.e. thetoprankeddocumentsretrievedbytheorigi-nalquery)yieldsbetterperformancethanonthewholecor-pus. Inlanguagemodelingapproaches,Bergeretal. [3]pro-posed a translation model to incorporate term relationship intolanguage modeling approaches. Caoet al. [4] extended the translation model to integrate both co-occurrence and hand-crafted thesaurus andachievereasonableperformance improvement. Bai et al. [2] showed that query expansion basedonco-occurrencescanimprovetheperformanceinlan-guage modeling approaches.

Although the motivation is similar, our work differs from the previous work in that (1) we attempt to integrate term semanticrelationshipasacomponentinourretrievalmodel; (2)wetakeanaxiomaticapproachanddefineconstraintsto guide us in the incorporation of semantic term matching. Similar to previous work [11, 24, 18, 20, 23, 2], our method can also be implemented as query expansion. Thus, when we compute term similarity based on the documents in the collection, it bears some similarity to traditional feedback methods [21, 30, 19, 16], which also select terms from doc-uments as to expand the query. But our method selects termsthataresemanticallyrelatedtoeachindividualquery term and relies on the axiomatic approaches to combine them,whilefeedbackmethodsselecttermsthatdiscriminate the feedback documents, which are not necessarily related to any individual query term. Because of this difference, our method is complementary to the traditional feedback method. Indeed, our experiment results showthat they can be combined to further improve performance.
The basic idea of the axiomatic approach to information retrieval is to search in a space of candidate retrieval func-tions for one that can satisfy a set of reasonable retrieval constraints; the assumption is that if a retrieval function satisfiesallthedesirableretrievalconstraints,itwouldlikely beeffectiveempirically[5,6]. Comparedwithotherretrieval models, thisapproachhas theadvantageofconnecting rele-vancemoredirectlywithtermsthroughformalizedretrieval constraints.

In[6],severalinterestingnewretrievalfunctionshavebeen derived using formalized retrieval constraints and an induc-tive decomposition of the function space. These new func-tions are shown to perform as well as traditional retrieval functions but with much more robust parameter setting. Theinductivedefinitiondecomposesaretrievalfunctioninto three component functions: primitive weighting function, document growth function and query growth function.
Theprimitiveweightingfunctiongivesthescoreofaone-term document { d } and a one-term query { q } . It is usually instantiated as where  X  ( q )isanIDF-likefunctionof q [6].

The query growth function describes the score change when we add a term to a query and is instantiated as The document growth function describes the score change when we add a term to a document, and is instantiated basedonsomeexistingretrievalfunctions. Theinstantiation corresponding to Okapi 1 is where  X  ( x,y )= y b length, and c ( t,D ) is the count of term t in D .
In general, a different instantiation of these component functions would result in a different retrieval function. In [6],severalsuchinductivelydefinedaxiomaticretrievalfunc-tionsarederived,andtheyareallshowntobeeffective. The following function (F2-EXP) is one of the best performing functions,whichwillbeusedinthispaperasanexampleax-iomatic retrieval function to illustrate how we can incorpo-ratesemantictermmatching;however,theproposedmethod can be easily applied to all the other derived functions. S ( Q,D )=
In this section, we show how we can naturally incorpo-ratesemantictermmatchingintotheinductivelydefinedax-iomaticretrievalmodel proposedin[6]. Followingthespirit ofaxiomaticapproaches,wefirstdefinethreeconstraintson semantic term matching.
Let s ( t,u )  X  [0 , +  X  ] be any given semantic similarity function between two terms t and u . Without loss of gen-erality, we assume that term t is semantically more similar a large value of s indicates a high similarity. Since intu-itivelyaterm hasthe highest similaritytoitself,weassume  X  u = t,s ( t,t ) &gt;s ( t,u ). We also assume that s is symmet-ric, i.e.,  X  t,u,s ( t,u )= s ( u,t ). Basedonsuchasemantic similarityfunction, we now define three constraints that we would like any reasonable retrieval function to satisfy. STMC1: Let Q = { q } be a query with only one term q . Let D 1 = { d 1 } and D 2 = { d 2 } be two single-term docu-ments, where q = d 1 and q = d 2 .If s ( q,d 1 ) &gt;s ( q,d S ( Q,D 1 ) &gt;S ( Q,D 2 ).

STMC1requiresaretrievalfunctiontogiveahigherscore toadocumentwithatermthatismoresemanticallyrelated toaqueryterm. Thus,eventhough D 1 and D 2 donotmatch thequery Q syntactically,wewouldlike D 1 tohaveahigher scorebecause term d 1 is more semanticallyrelated toquery term q than term d 2 is. Clearly, STMC1 directly constrains the primitive weighting function.
 STMC2: Let Q = { q } be a single term query and d be a non-query term such that s ( q,d ) &gt; 0. If D 1 and D two documents such that | D 1 | =1, c ( q,D 1 )=1, | D 2 | k and c ( d,D 2 )= k ( k  X  1), where c ( q,D 1 )and c ( d,D are the counts of q and d in D 1 and D 2 respectively, then S ( Q,D 1 )  X  S ( Q,D 2 ).

STMC2 requires that matching an original query term q exactly should always contribute no less to the relevance score than matching a semanticallyrelated term d ,nomat-ter how many times term d occurs in the document. STMC3: Let Q = { q 1 ,q 2 } be a query with only two query terms and d be a non-query term such that s ( q 2 ,d ) &gt; 0. Let D 1 and D 2 be two documents. If | D 1 | = | D 2 | &gt; 1, S ( { q 1 } , { q 1 } )= S ( { q 2 } , { q 2 } ), c ( q 1 ,D 1 |
D 2 | X  1and c ( d,D 2 )=1,then S ( Q,D 1 )  X  S ( Q,D 2 ).
STMC3 intends to capture the following intuition: Sup-pose we have a query with two equal ly important terms q 1 and q 2 . Suppose a document D 1 matches q 1 n ( &gt; 1) times, but does not match q 2 or any of its semantically related terms. If we change one of the occurrences of q 1 in D 1 to a term semanticallyrelated to q 2 to form a document D 2 , D should not have a lower score than D 2 ,because D 2 covers more distinct query terms than D 1 .
The constraints defined above provide some guidance on how to extend the inductively defined axiomatic retrieval functions to incorporate semantic term matching.
First, it is clear that these existing axiomatic functions violate all the three constraints we defined, simply because thesemanticsimilarityfunction s isnotpartoftheretrieval function. For example, based on the primitive weighting function shown in Equation (1), any single-term document will be assigned a zero score if the term in the document is notmatchingexactly the query term, which clearlyviolates STMC1.

TomaketheprimitiveweightingfunctionsatisfySTMC1, anaturalsolutionistodefinethefollowing generalized prim-itive weighting function basedonagivensimilarityfunction s .
 where f isamonotonicallyincreasingfunction. Notethatit is reasonable to require  X  q  X  Q,f ( s ( q,q ))=1 for any query Q ,becausethescoreofgeneralizedprimitiveweightingfunc-tionshouldbecomparablewiththescoreoftheoriginalone when the twoterms matchexactly. One waytoensure such property is to define f in terms of normalized similarity. where  X  is used to regulate the weighting of the original query terms and the semantically similar terms. The value of  X  =1)when d = q .

The generalized primitive weighting function clearly sat-isfies STMC1, and if we combine it with any existing in-stantiationsofdocumentgrowthfunctionandquerygrowth function, the derived retrieval functions would also satisfy STMC1 unconditionally. We further analyze STMC2 and STMC3 on such derived functions and find that these con-straintsaresatisfiedwhen  X  iswithinacertainrange. Specif-ically,theanalysisofSTMC2providesatighterupperbound for  X  , while the analysis of STMC3 provides a tighter lower bound. The actual values of these bounds depend on the instantiation of document growth function. As an example, the lower and upper bounds for F2-EXP is:
Weseethattheboundsof  X  dependonboththequeryand semantic similarity function s . In our experiments, on each data set, the lower bound of  X  is determined by the lowest value of s ( q,q ) s ( q,d ) forallthequerytermswhiletheupperbound of  X  is determined by the highest value of s ( q,q ) s ( q,d the minimal requirements of  X  .

Since a term can potentially have a huge number of se-manticallyrelatedterms,thecomputationofthegeneralized retrieval functions can be expensive. To reduce the compu-tation cost, we can reasonably restrict our attention to the mostsimilarterms for each query term. Such simplification is not expected toaffect the retrieval score significantly,be-causethedroppedtermswouldcontributelittletothescore anyway. Thus we redefine the generalized primitive weight-ing function as follows:
S where  X  ( q )isthesetof K most semantically similar terms of q according to the similarity function s ,  X  ( q )isasin Equation (1) and  X  ( q,d ) is defined in Equation(2).
Even with this simplification, the computation can still potentiallyinvolveenumeratingallthecombinationsofquery termsanddocumentterms. Fortunately,thereisanefficient waytocomputesucharetrievalfunctionbasedonqueryex-pansion as shown in the next section.
Letusfirstintroducesomenotations. S ( Q,D )isthescor-ingfunctionoftheoriginalinductivelydefinedaxiomaticre-trievalfunction,where onlysyntactictermmatchingiscon-sidered. S gen ( Q,D ) is the generalized inductively defined axiomaticretrievalfunctionobtainedbycombiningthegen-eralized primitive weighting function with the original doc-ument growth and query growth function.

The generalized primitive weighting function (i.e., Equa-tion (4)) can be re-written as follows.
 where  X  ( q : d )=  X  ( q )  X   X   X  s ( q,d ) s ( q,q ) .
Let  X  ( q )bethesetof K most semantically similar terms of q excluding itself, i.e.,  X  ( q )=  X  ( q ) / { q } .Let P be the set of the K most similar terms of all query terms, i.e., P = q  X  Q (  X  ( q )).  X  t  X  P ,let  X  ( t )bethesetofquery termsthataresemanticallysimilarto t . Define S suchthat  X  t  X  P , S ( { t } , { t } )=  X  (  X  ( t ): t )= u  X   X  ( t )  X  S ( Q,D )= S ( Q,D ).
 Theorem:  X  Q,D , S gen ( Q,D )= S ( Q  X  P,D ) . Proof:
The first step is based on query growth function. The second step assumesthat the relevance score ofa document can be computed as the sum of the disjoint subsets of the document, which holds for all the inductively defined ax-iomatic retrieval functions. The third step is based on the fact that S gen and S use the same document growth func-with generalized primitive function when t  X  P .
The theorem shows that scoring a document using S gen can be reduced to scoring using S with an expanded query formedbyadding,foreachqueryterm, K mostsimilarterms to the query. Note that the weight of a similar term t is computed from  X  (  X  ( t ): t ) instead of  X  ( t )asusedinthe traditional query expansion methods. The remaining challenge is to define s ( t 1 ,t 2 ) in STMC1. In general, we may exploit any knowledge and resources available to us to compute term similarity and there are many ways to compute it. For example, co-occurrences of terms obtained from the analysis of a document collection usually reflect underlying semantic relationships that exist betweenterms[23,3,4,2],andwemayusemeasuressuchas Dice similarity [1] and mutual information [25, 15, 9, 8, 13, 7] to compute term similarity. In this paper, we adopt the mutual information as the basic semantic similarity metric, leaving other choices for future work.

The mutual information (MI) of two terms t and u in a set of documents can be computed as follows [25]: I ( X t ,X u )= X t and X u are two binary random variables corresponding to the presence/absence of term t and term u in each docu-ment or segment.

Mutual information is a principled way to measure term correlations,anditsatisfiesourrequirementsaboutthesim-ilarityfunction s . Thenextchoicewehavetomakeiswhich corpus to use when computing the mutual information. A naturalchoicewouldbethedocumentcollectionfromwhich we retrieve documents. However, such a choice may not be ideal because an ambiguous term can have multiple senses inalargecorpus. Asaresult,thesemanticallyrelatedterms foundbymutualinformationcouldbeamixoftermscorre-spondingtodifferentsensesoftheoriginalterm,introducing noiseinqueryexpansion. Thus,itiscrucialtocomputemu-tual information over a  X  X lean X  corpus, where ideally only one (correct) sense of the query term occurs. How can we find such a  X  X lean X  corpus? One possibility is to use the top-M documents returned by the retrieval systems for the query. The rational is that we can reasonably assume there is only one sense of a query term in the set of relevant doc-uments, and the top-M documents are reasonable approxi-mations of the set of relevant documents. This is indeed in line with what previous work in query expansion has found  X  local document analysis tends to be more effective than global document analysis [29].

However, the top-M documents would clearly be a biased corpus, and in this sense, it is not a good corpus for com-puting mutual information. For example, it is likely that a query term occurs in all the top-M documents. The abun-dance of a query term would then cause popular terms in thetop-Mdocumentstogenerallyhaveahighmutualinfor-mation. In particular, a common term (e.g.,  X  X an X ) would have a high mutual information, even if it also occurs in many other documents where the query term does not oc-cur. To solve this problem, we need to supplement the top-M documents with some additional documents that do not necessarily contain any query term. Thus we will randomly choose r  X  M documents from the collection and combine themwiththetop-Mdocumentsasamixedcorpusforcom-puting mutual information.

Clearly, the choice of r may also affect the mutual infor-mationresults. Howdowechooseagoodvaluefor r ?Once again, constraint analysis can provide some guidance. The following notations will be used in defining the constraints: N is the total number of documents in the document col-lection. df ( t ) is the number of documents that contain t in the collection. W is the working set containing r  X  M ran-dom documents plus the top M documents returned by the system;sincethe r  X  M documentsarechosenfromthedoc-umentsrankedbelowthetop-Mdocuments, weclearlyhave M + M  X  r  X  N . df ( t 1 ,t 2 | W ) is the number of documents that contain both t 1 and t 2 in the working set W . df ( t is the number of documents that contain t in the working set W .
Intuitively, the value of r should not be very small, be-cause we need enough number of random documents to pe-nalize the common terms. Consider the scenario in Figure 1(a), where t 1 is a  X  X ruly X  semantically related term, while t is a common term. t 1 is semantically more similar to q than t 2 , although t 2 co-occurs with q in more documents than t 1 . This intuition can be captured by the following Term Semantic Similarity Constraint(TSSC).
 TSSC1: Let q be a query term and t 1 and t 2 be two non-query terms. If df ( q,t 1 | W )= M 2 , df ( t 1 | W )= df ( q | W )= M , df ( q,t 2 | W )= M , df ( t 2 | W )= M + then s ( q,t 1 ) &gt;s ( q,t 2 ).
On the other hand, the value of r should not be very large because we want to ensure that the dominant sense of a query term is the one determined by the whole query. ConsiderthescenarioinFigure1(b). Supposeaquery term q has two senses. The first sense is the one determined by the wholequery (i.e.,inthe top M documents), and a term t is semantically related to this sense of q (i.e., they co-occurinthetop M documents). Nowsupposeanotherterm t issemanticallyrelatedtoanothersenseof q (i.e.,theyco-occurintherandomdocuments). Intuitively, t 1 shouldhave a higher similarity score than t 2 . The following constraint captures this intuition.
 TSSC2: Let q be a query term and t 1 and t 2 be two non-queryterms. If0 &lt; X &lt; 1, df ( q,t 1 | W )= M , df ( t df ( q | W )= M +  X   X  r  X  M and df ( t 2 | W )= df ( q,t 2  X   X  r  X  M ,then s ( q,t 1 ) &gt;s ( q,t 2 ).  X  is the percentage of the documents that contain q in a randomsampleofthewholecollectionafterthetop M doc-uments excluded, i.e.,  X  = df ( q )  X  M N  X  M .
The above two constraints are satisfied only when the value of r is within a certain range. Indeed, TSSCs pro-vide a lower and an upper bounds for r . The value of r is collection and query dependent. For each collection, we use the median of the document frequency of all query terms to compute the upper bound of r .
We briefly summarize the high-level steps involved in the proposedmethodforincorporatingsemantictermmatching: 1. Construct a working set where term semantic similar-2. Foreveryqueryterm,findthetop L mostsimilarterms 3. Gatherthetop L similartermsforallthequeryterms, 4. Expand the original query with the K terms. Note
In the first step, the working set can be constructed over any reasonable resources in the following way: Given any collection of documents and a query, we first use the origi-nal inductively defined axiomatic retrieval function to rank the documents. We then merge the top M returned doc-uments with r  X  M random documents selected from the same collection to form a working set for computing term similarity. The collection to be used can be either the tar-getcollectionforretrieval(called internal expansion )orany othercollections(called external expansion ). Toformalarge pool of terms, L is usually fixed to 1000. Four parameters need to be tuned: the number of expansion terms (i.e., K ), the number of top documents (i.e., M ), the number of ran-dom documents (i.e., r ) and the scaling parameter  X  .The optimalvaluesof  X  and r areexpectedtobewithinacertain range basedonEquation(3)and Equation(5),whichisalso supported by our experiment results.
We conduct three sets of experiments. First, we evaluate the effectiveness of the semantic term matching. Second, we examine the parameter sensitivity of the method. Fi-nally, we compare it with a model-based feedback method in language modeling approaches [30].

All experiments are conducted over two collections used in recent Robust track [28, 27]: (1) TREC Disk 4&amp;5 (mi-nus Congressional Record) with 249 official topics of Ro-bust track in 2004. The document set has 1908MB text and 528,000 documents. This is labeled as  X  X OBUST04 X . (2)AQUAINT data with 50 official topics of Robust track in 2005. The document set has 3GB text and 1,033,461 documents. This is labeled as  X  X OBUST05 X . Some experi-ments are also conducted over six other data sets used in the previous studies [5, 6, 31]: news articles (AP88-89), technique reports (DOE), government documents (FR88-89), Web data (WEB2g), and the ad hoc data in TREC (TREC7 and TREC8). In all the experiments, we use the title-only queries, because short keyword query is the most frequently used query type by web users and semantic term matching is necessary for such short queries.

The performance is measured using the official measures inRobusttrack: MAP(meanaverageprecision)andgMAP (geometricmeanaverageprecision). gMAP[27,28]isavari-ant of the traditional MAP measure that uses a geometric mean rather than an arithmetic mean. This measure em-phasizes the performance of poorly-performing topics.
The preprocessing only involves stemming with Porter X  X  stemmer. As pointed out in the previous work [6], using a fixed parameter value ( b =0 . 5), F2-EXP can often achieve near-optimal performance in many test sets. Thus, we fix b to 0 . 5 in our experiments. We use the optimal value of b for the other five inductively defined axiomatic retrieval functions. In the firstand third sets ofexperiments, M and K are both fixed to 20 and r is fixed to 29, so that we will get a total of 600 documents in the working set. We Table 1: Performance of different axiomatic func-tions.
 tune the value of  X  and report the best performance unless otherwisestated. BL isthebaselinemethodwithoutexpan-sion (i.e., without semantic term matching). docAX and segAX aresemanticexpansionmethodswithMIcomputed based on co-occurrences in documents and 100-word seg-ments,respectively. Inalltheresulttables,  X  and  X  indicate that the improvement is statistically significant according to Wilcoxin signed rank test at the level of 0.05 and 0.1 respectively.
Table 1 shows the performance of the internal expansion for all six functions. The semantic term matching consis-tently and significantly outperforms the baseline on both data sets in terms of MAP. But, gMAP decreases in a few cases, which indicates that most of the performance im-provement comes from the easy topics. F2-EXP is the best ofallthefunctions. Wefurther testthesemanticexpansion on top of F2-EXP on six other data sets, and found that semanticexpansionoutperformsthebaseline(i.e.,F2-EXP) significantly (Table 2) on all the data sets except FR88-89. Due to the limit of space, we only report the performance of F2-EXP in the remaining experiments.

Table 3 shows the performance when semantic similar-ity is computed over the internal resource (i.e., collection itself), the external resource (i.e., a pool of Google snip-pets returned for a query), and both (i.e., first use external expansion, then do another round of internal expansion). We make the following observations. First, the expansion method improves the performance significantly in all cases. Second,theweb-basedexternalexpansionmethodisconsis-tentlymore effective than the internalexpansion method in both measures. This indicates that the use of good exter-Table 2: Performance of F2-EXP on more data sets. Table 3: Performance when using different re-sources.
 nal resources improves the effectiveness especially over the poorly-performingtopics,whichisconsistentwithwhatoth-ershaveobserved[27]. Finally,combiningbothinternaland external expansion further improves the accuracy.
Next, we study the performance sensitivity for the four parameters in the semantic expansion. Here we only show plotsonROBUST05,butsimilartrendscanbeobservedfor all the other data sets. Figure 2 shows the sensitivitycurve for r . Equation (3) gives 1 &lt;r&lt; 50 for the ROBUST05 data set. The performance is relatively stable when r is within the range, while it decreases when r is out of the range. Figure 3 shows the sensitivity curve for  X  .Equation (5) gives 0 . 27  X   X   X  3 . 8fortheROBUST05dataset.The optimalvalueisindeedwithinthepredictedrange,although docAXandsegAXhavedifferentoptimalvalueof  X  .Figure 4showsthesensitivitycurvefor K . Theperformanceisnear optimal when K is 20. The performance is relatively stable whenmoretermsareadded. Figure5showsthecurvefor M . Figure 4: Performance Sensitivity (Num of Terms) We observe the performance is optimal when M is around 20. The performance decreases when more documents are used, likely because the assumption that top M documents are all relevant is not true for larger values of M .
Bothoursemanticexpansionandtraditionalfeedbackmeth-ods select terms for query expansion. Traditional feedback methods[21,30]selecttermsthathavehigherweightinthe feedback documents, while our method selects terms that are semantically related to any query term. It would be in-teresting to compare their performance. In Table 4, we re-port the performance of the model-based feedback method inlanguagemodelingapproaches[30]. InternalPFB(IPFB) is the pseudo feedback method. External PFB (EPFB) is thefeedbackmethodwherethefeedbacktermsareobtained fromtheGooglesnippets. Weset  X  totheoptimalvaluefor each data set, the number of feedback terms to 20 and the number of documents to 20. We tune the value of feedback coefficientandthevalueofmixturenoise[30]andreportthe best performance. Comparing Tables 3 and 4 shows that Figure 5: Performance Sensitivity (Num of Docs) Table 5: Performance (MAP) of term selection (segAX) the expansion method in axiomatic framework outperforms the model-based feedback method for both internal and ex-ternal feedback. More interestingly, as shown in Table 4, our method can be combined with the traditional feedback methods to further improve performance, which shows that our method iscomplementary withthetraditionalfeedback method.

Finally, we design experiments to study whether the per-formance gain of the semantic expansion comes from better term selection or from better term weighting. Assume A and B can be either our expansion method (i.e., F2-EXP) or traditional method (i.e., KL-Div.). We use method A to selecttermsforthemethodB,whichmeansthatweexclude anytermsthatarenotnominatedbyAwhenusingB.How-ever, these terms are still weighted using B. This way we have four combinations shown in Table 5. The performance of using the terms selected by F2-EXP is consistently bet-ter than that of using the terms selected by KL-divergence method. For example, on ROBUST05 data set, the per-formance of KL-divergence method can be improved from 0.254 to 0.273 by using the terms selected by our method. The results indicate that the performance improvement of our method clearly comes more from better term selection.
Inthispaper,weproposeanaturalwaytoincorporatese-mantic term matching into axiomatic retrieval models. Fol-lowing the previous work in axiomatic retrieval, several re-trieval constraints are defined to capture intuitions on se-mantic term matching. The advantage of this method is that the constraints provide us guidance on the parameter setting and on the choice of term semantic similarity mea-sure. Ourmethodcanbeefficientlyimplementedasaquery expansion method in the axiomatic framework.

Theexpansionbasedonsemantictermmatchingwaseval-uated on several representative large retrieval collections. The results show that our method is effective for all the six inductively defined axiomatic retrieval functions. Further-more, our method works for both internal resources (e.g. collection itself) and external resources (e.g. the results re-turned by Google). The parameter sensitivity confirms the hypothesis that the constraint analysis can provide an up-per bound and a lower bound for the optimal values of r and  X  . The performance is relatively stable when the val-ues of the parameters are set within the range derived from the constraint analysis. As query expansion, our method outperforms the model-based feedback method in language modeling approach and is shown to be complementary to thetraditionalfeedbackmethodsandcanbecombinedwith them to further improve performance.

Therearemanyinterestingfutureresearchdirections. First, we can use more resources, such as WordNet [4, 26, 12, 14, 17], to compute term semantic similarity. Second, our method can also be applied to cross-lingual retrieval task. Finally,thetermsimilaritybetweenquerytermsareignored in our work. It would be interesting to study the expansion based on query concepts instead of individual query term, which is along the line of [16, 20].
This material is based in part upon work supported by the National Science Foundation under award number IIS-0347933. We thank the anonymous SIGIR reviewers for their useful comments.
