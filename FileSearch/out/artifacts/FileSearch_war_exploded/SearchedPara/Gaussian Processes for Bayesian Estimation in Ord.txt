 Department of Computer Science, Laval University, Canada Department of Computer Science, University College London, U.K. Ordinary differential equations (ODEs) are continuous time models with the interaction between variables de-function f . The task is to estimate any unknown parameters  X  of the ODEs by fitting them to observed data collected at a set of discrete observation times, t 1 ,...,t T . A principled approach to this problem is to first numerically integrate the ODEs for a given value of  X  and initial value x (0) to obtain a vector of values X  X  x t 1 ,...,x t T . Parameter estimation is achieved by finding  X  such that X closely matches the observed data. However, numerical integration is computa-tionally demanding, rendering this otherwise ideal scheme impractical in all but the smallest systems, see for example (Vyshemirsky &amp; Girolami, 2008).
 In gradient matching we avoid explicit numerical integra-tion by considering an alternative model of the data, x ( t ) = gradients of the fitted function at the observed timepoints, of the ODE and parameters  X  of the fitted function g by requiring that the gradients in both models are consistent at the observed timepoints. A review of this class of ap-proaches can be found in (Ramsay et al., 2007).
 As described in (Calderhead et al., 2008), previous gra-dient matching approaches provided only limited point-parameter estimates or can prove numerically inconsistent. Recently, Gaussian Processes (GPs) have been considered as data models within the gradient matching framework (Calderhead et al., 2008; Dondelinger et al., 2013) and for the solution of linear operator equations (Graepel, 2003). GPs provide a distribution over fitted functions and associ-ated gradients. Using priors on the parameters of the GP model and the ODE, this gives a flexible Bayesian parame-ter estimation procedure. More concretely, in (Calderhead et al., 2008) GP parameters  X  are fitted first to the data, and subsequently the parameters of the ODE  X  are estimated. The estimation accuracy is however limited by the lack of feedback from ODE parameter inference to GP parameter inference. To address this Dondelinger et al. (2013) intro-duced bidirectional interaction between ODE and GP pa-rameters, demonstrating improved parameter estimation. These GP approaches have similar computational complex-ity and can run up to two orders of magnitude faster than numerical integration. The benefits of a Bayesian approach to parameter estimation in ODEs are now well-established and we propose to improve on previous approaches by in-troducing a simpler generative model that directly links state derivatives to system observations using a GP. This plays a similar role to numerical integration but without the corresponding high computational cost. 1.1. ODE System Description We consider continuous time dynamical systems in which the motions of K states x ( t )  X  [ x 1 ( t ) ,x 2 ( t ) ,...,x are represented by a set of K ODEs where  X  is a vector of parameters of the ODE. For no-tational convenience, we additionally define the state ma-x noisy observations of X (see below), the task is to infer a posterior distribution over the parameters  X  . 1.2. Observation Model The T observations Y = [ y ( t 1 ) , y ( t 2 ) ,..., y ( t tained from the states according to independent additive noise y ( t ) = x ( t ) + ( t ) where the noise for the k -th state, k  X  { 1 , 2 ,...,K } , is Gaussian, k ( t )  X  N (0 , X  2 k gives then an observation model with p OBS ( y ( t ) | x ( t )) = N ( x ( t ) , X  2 I ) . If unknown, the parameters of the observation model (  X  in this case) form part of the parameters that need to be esti-mated. This is achieved by placing a prior over their val-ues and incorporating these parameters into the model in the standard way. This step is unproblematic and, to avoid notational clutter, we drop these observation parameters as variables in the model descriptions below (they will how-ever be included in the experiments). 1.3. Bayesian Numerical Integration Given the ODE and an assumed initial value x 0 , we can then (in principle) numerically integrate the system. For example 1 , for K = 1 , using a simple approach based on discretising time in small intervals of  X  , a numeri-cal value x 0 for the integrated path is given by x x n +  X f ( x desired end time. This can be considered as a procedure that, for a given initial value, produces (in this case a deter-ministic) distribution p ( x | x 0 , X  ) =  X  ( x  X  x 0 ( x values of the state at the observation times. Here  X  (  X  ) is the Dirac delta function. Given then a prior on  X  and the integration constant x 0 , this defines a joint distribution from which samples p (  X ,x 0 | y ) can be drawn. This ideal procedure can produce excellent results (Vyshemirsky &amp; Girolami, 2008); however the computational expense is prohibitive in larger models with the bottleneck being the explicit numerical integration that needs to be carried out for every value of  X ,x 0 of interest (Calderhead et al., 2008). As an alternative to explicit Bayesian numerical integra-tion, we propose the following generative model over states X , their derivatives  X  X , observations Y and remaining pa-rameters using a simple belief network, fig(1b), p ( Y , X ,  X  X , X   X  , X  ) = p (  X  ) p (  X   X  ) where  X   X   X  ( x 0 , X  ) . To generate data from this model we first sample parameters  X   X  ,  X  from their priors and then a state X from the GP prior p GP ( X |  X   X  ) . A state derivative is subsequently obtained by sampling from p ODE (  X  X | X , X  ) . Finally, given these state derivatives  X  X , observations Y are generated by sampling from the GP p GP ( Y |  X  X , X   X  this way we combine a smoothness prior assumption on the state X together with derivative information obtained from the ODE in a single generative model 2 .
 E The temporal evolution of the ODE is encoded in the distribution p ODE (  X  X | X , X  ) . In the deterministic ODE case (which we assume throughout) this will simply be a delta function distribution  X  (  X  X  X  f ( X , X  ))  X  Q would be straightforward to incorporate for the case of Gaussian SDEs.
 P The GP prior assumes that each state dimension is a priori independent p GP ( X |  X   X  ) = Q k p GP ( x k |  X  p and covariance function c  X  k ( t,t 0 ) .
 I eter dependencies on the r.h.s for compactness of notation) plays a key role in our model and specifies how to implicitly integrate a given state derivative curve to arrive at a distri-bution over observations. Since differentiation is a linear operation, the derivative of a GP is also a GP  X  see for example (Solak et al., 2002). Hence the joint distribution p p
GP ( X |  X   X  ) and observation model p OBS ( Y | X ) we obtain the covariance functions cov (  X  x k ( t ) ,  X  x k ( t 0 )) = cov (  X  x k ( t ) ,x k ( t 0 )) = cov ( y k ( t ) ,  X  x k ( t 0 )) = cov ( x k ( t ) ,  X  x cov ( y k ( t ) ,y k ( t 0 )) = c  X  and mean functions Given the state derivatives, the observations are then Gaus-sian distributed 4 where C evaluated at the observation times t 1 ,t 2 ,...,t T . 2.1. Parameter Estimation From (1), the conditional marginal distribution over obser-vations, latent states and parameters is given by p ( Y , X |  X   X  , X  ) This integral can be analytically evaluated in the case of Gaussian additive noise in the ODE. In the deterministic case, this reduces to simply The distribution over observations, latent states and param-eters is then given by Estimation of the parameters and latent state X can then be carried out for example by sampling from the poste-rior p ( X , X   X  , X  | Y ) , see section(3). Note that, in contrast to (Calderhead et al., 2008; Dondelinger et al., 2013), the normalisation constant of the joint distribution (2) is known which facilitates sampling.
 Previous approaches (Calderhead et al., 2008; Dondelinger et al., 2013) used a GP to compute p ( X ,  X  X | Y ) ; parame-ter estimation is achieved by requiring gradients from this to match the desired gradient f ( X , X  ) . The key difference between this and our approach is our direct link from the latent gradient  X  X to the observation Y . This term can be expressed as p ( Y |  X  X ) = R p ( Y | X ) p ( X |  X  p ( X |  X  X ) implicitly performs numerical integration, as we describe below. Since  X  X , X and Y are defined only at the measurement times, no fine time discretization is required in our model. 2.2. Informal Justification To minimise notational issues, we consider a univariate system with K = 1 . Furthermore we discretize time so that t = n X  , for integer time-index n  X  { 1 ,...,N } and real discretization interval  X  . Note that in the model in section(2) the timepoints are defined only at the observa-tion times; however in this section we need to notationally distinguish between times that the data are observed and a finer discretisation of time that could be used to carry out numerical integration. The times at which data will be observed are therefore described by a subset m  X  M of the fine time discretization; for example we measure y m at time indices M = { 1 , 10 , 20 ,... } . To emphasise that the measurements only occur at a subset of all dis-crete times, we write y M for the observed measurements. Given a curve x n , the numerical derivative is given by  X  x n = ( x n  X  x n  X  1 ) / X  . We assume that  X  x 1 = x 1 where x 0 is the constant of integration. For a vector x , the derivative vector is then given by the difference equation  X  x = Dx  X  b , where the square invertible matrix D has zero elements, except for D n,n  X  1 =  X  1 / X ,D n,n = 1 / X  , D 1 , 1 = 1 and b is the zero vector except for b 1 = x 0 . To explain the fundamental mechanism, we fix the parameters of the GP and observation model. The posterior over the discretized state is Writing x 0 for the numerically integrated curve, we have Assuming that the derivative curve is obtained by differ-encing, we can invert this relation using Bayes X  rule Since D is invertible, the GP plays no role, to give ing (3) over  X  x , we obtain the joint distribution over the observations y M and latent curve x , p ( y M , x ) = p ( y M | x ) p GP ( x ) , where p ( y M | x )  X 
Z = p OBS ( y M | x 0 M = D  X  1 ( f ( x , X  ) + b )) M ) This can be interpreted as taking a set of gradients f ( x , X  ) , integrating them numerically (via the inversion D  X  1 which performs summation of the components of f ) and taking the measurement indices of this vector. The likelihood of the observations p ( y M ) = R p ( y M | x ) p GP ( x ) d x is equiv-alent to the mass of GP curves x whose numerical deriva-tives match f ( x , X  ) weighted by how well they fit the ob-served data y M at the observation times M . Taking the limit  X   X  0 , the above difference equation becomes the dif-ferential equation (1.1) and the Gaussian over x becomes a GP, with  X  x the associated GP derivative, as specified by the model (1), see figure(2). There are a number of approaches one could take to draw samples from the GP-ODE posterior p ( X , X   X  , X  | Y ) and our philosophy was to choose the simplest that provides good results. Writing  X  = { x 0 , X , X  } for all the param-eters of the GP and observation model, we sample from p ( X , X   X  , X  | Y ) using a Gibbs procedure to produce a set of samples  X  i , X  i , X i . We initialize  X  0 ,  X  0 at random and draw X 0  X  p GP ( X | Y ,  X  0 ) . We subsequently draw sam-ples, indexed by i = 1 : L by alternately drawing from 1.  X  i ,  X  i  X  p (  X ,  X  | X i  X  1 , Y ) 2. X i  X  p ( X |  X  i ,  X  i , Y ) We present a naive approach for drawing from these condi-tionals below 5 . 3.1. Parameter sampling We draw from p (  X  i ,  X  i | X i  X  1 , Y ) using Gibbs sampling: 2. For j = 1 : L p where these conditional distributions can be obtained from the joint (2). Where there are multiple components of a pa-rameter, we again use Gibbs sampling to obtain a univariate sample of a component conditioned on the remaining com-ponents. In the experiments we assume that the parameters take values from known discrete sets (the priors are dis-crete), in which case sampling from these conditionals is particularly straightforward. 3.2. State sampling It is natural to consider drawing samples from p ( X |  X ,  X  , Y ) using Metropolis-Hastings (similar to (Dondelinger et al., 2013)) with p GP ( X |  X  , Y ) as the proposal. However, in our experience, this results in poor mixing. We therefore use Gibbs sampling in which we draw a state from p ( x ( t ) | X \ t , X ,  X  , Y ) , where X are the states except for x ( t ) , drawing samples in se-quence from times t  X  { t 1 ,...,t T } . To draw from p ( x ( t ) | X \ t , X ,  X  , Y ) we use either Metropolis-Hastings with proposal p GP ( x ( t ) | X \ t ,  X  , Y ) or Gibbs sampling for each component of the vector x ( t ) based on discrete values 6 . After L x sweeps through all timepoints, we obtain the new sample X i . 4.1. Gradient Matching (Calderhead et al., 2008) is based on matching gradients via what could be termed a  X  X ompatability X  function (for the case K = 1 and fixed  X  for notational simplicity) This is used to define using the marginal compatibility with presumably the intention that this has high value when the gradient distributions overlap 7 . The marginal compat-ibility  X  0 ( x |  X , X  ) is analytically computed since the terms under the integral are Gaussian. The authors modify the de-terministic ODE by the addition of fictitious noise to give a To ease comparison with our approach (the extension to the stochastic ODE case is trivial), we take the deterministic ODE case, for which the above reduces to The joint distribution over observations, latent states and parameters is then defined as Inference is then achieved by sampling, conditioned on the observed sequence y . The unknown normalisation term of (4) is a function of  X  and thus makes direct Gibbs sam-pling from this posterior problematic. The approach taken in (Calderhead et al., 2008) is to first refactor the joint dis-tribution in the form 8 Conditioned on y , ancestral sampling is then performed: the integral can be evaluated analytically. A disadvantage of this model is that the posterior p (  X  | y ) does not take the ODE system dynamics into consideration. Effectively, a GP is fitted to the data first (without knowledge of the sys-tem dynamics) and the parameters  X  of the ODE are subse-quently adjusted to best match the fitted GP.
 The gradient matching approach can be defined as a graph-ical model chain graph (see for example (Koller &amp; Fried-man, 2009)) distribution 9 , fig(1c), with factors The undirected link between  X  and  X  x ODE is necessary to ensure that the variables  X  x ODE ,  X  x GP ,  X  form a component of the chain graph. Marginalising this chain distribution over  X  x GP and  X  x ODE gives the marginal distribution which matches (5). We can also write this as where we define the gradient matching function 4.2. Adaptive Gradient Matching Dondelinger et al. (2013) considered a modified gradient matching approach with joint distribution and marginal A benefit of this approach is that the marginal p ( y |  X  )  X  does depend on the ODE; in contrast to (Calderhead et al., 2008) the parameters of the GP are influenced by the ODE (and vice versa). The marginal p ( y , x , X , X  ) can also be written in the same form as expression (7) but with the adaptive gradient matching function m
AGM ( x |  X , X  )  X  The improved performance of AGM over GM (Don-delinger et al., 2013) may be attributed to the fact that m
AGM is proportional to the marginal compatibility  X  0 and therefore always encourages matching between the GP and the ODE, whereas m GM less strongly encourages match-ing due to the partial cancellation of  X  0 in both the numer-ator and denominator of (8). No such issues arise in the GP-ODE approach in which the coupling between the GP and ODE parameters occurs through the implicit numerical integration mechanism, as described in section(2.2), which ensures agreement between the ODE and GP curves.
 The factors in the corresponding chain graph, fig(1d), are the same as for the gradient matching method of sec-tion(4.1). However, all variables except y form a com-ponent in the chain graph, giving the correct form for the marginal distribution on p ( y , x , X , X  ) . As for the gradient matching method, this has no natural interpretation as a generative model of the data. We illustrate our framework on two benchmark systems, Lotka-Volterra and Signal Transduction Cascade in (Don-delinger et al., 2013). To aid comparison, wherever possi-ble, we have chosen the same parameter settings and priors as the original authors. Our main interest is to study the implications of the different joint distributions specified by the competing approaches. As such we wish to make as similar as possible the sampling approaches for the three competing models in order to minimize differences due to different sampling strategies. To facilitate comparison we therefore used the same discretized sampling strategy for all methods. For the AGM and our GP-ODE approach we used Gibbs sampling for a discretized set of values, analo-gous to section(3.1) and the Gibbs scheme of section(3.2) for state samples. The cost of drawing a single sample in all competing approaches is similar, scaling O ( KT 3 ) . We stopped each sampling scheme (all written in Matlab) after a similar CPU time. Following (Dondelinger et al., 2013), we set p (  X  ) to a Gamma prior Ga (4 , 0 . 5) , p (  X  ) to a uniform prior U (0 , 100) and p (  X  ) to a Gamma prior Ga (1 , 1) . For the sampling process, the standard deviations of the obser-vation noise  X  in both models are initialized as the ground truth. For comparison we ran the Bayesian Numerical Inte-gration approach using the same discretized parameter val-ues wherever possible. 5.1. Lotka-Volterra The Lotka-Volterra model is an ecological system that is used to describe the periodical interaction between a prey species [ S ] and a predator species [ W ] : d [ S ] where  X  = [  X , X , X , X  ] T and x ( t ) = [[ S ] , [ W ]] . The ground truth data are generated using numerical integration over the interval [0, 2] with  X  = [2 , 1 , 4 , 1] and initial state val-ues [ S ] = 5 , [ W ] = 3 . The clean data are then sampled with the sampling interval 0.2. Finally clean data are cor-rupted with additive Gaussian noise N ( 0 , 0 . 5 2 I ) to form the observations Y . We chose the squared-exponential co-variance function c  X  k ( t,t 0 ) =  X  x k exp(  X  l k ( t  X  t  X  k = [  X  x k ,l k ] . Assuming a common parameter across observation dimensions, the parameter vector  X  is simpli-the ODE parameters  X  ,  X  ,  X  ,  X  over [1.5, 2.5], [0.5, 1.5], [3.5, 4.5], [0.5, 1.5] all with the interval 0.1; the param-eter  X  x is discretized over the range [0.1,1] with interval 0.1; the lengthscale l is discretized over [5, 50] with inter-val 5; the standard deviation of the noise  X  was discretized over [0.1, 1] with interval 0.1. The parameter x 0 was, for each state dimension k , discretized in the range [1 , 10] us-ing 20 uniformly spaced bins. After drawing ODE param-eters  X  from the posterior (see table(5.1)), we plot the nu-merically integrated curves (setting x 0 to the true value to aid visual comparison), see figure(3). The  X  X est X  method is that which most closely approximates the Bayesian Nu-merical Integration method of section(1.3). For small noise levels (not shown), all three competing methods produce similar results; however as the noise increases, the Adap-tive Gradient Matching and Gradient Matching approaches diverge markedly from the Bayesian Numerical Integration approach, whilst the GP-ODE approach fairs well. 5.2. Signal Transduction Cascade The Signal Transduction Cascade model is described by a 5-dimensional ODE system where  X  = [ k 1 ,k 2 ,k 3 ,k 4 ,V,Km ] and x ( t ) = are generated over the interval [0, 100] with  X  = [ S d ] = 0 , [ R ] = 1 , [ RS ] = 0 , [ Rpp ] = 0 . Then the data are sampled at t = [0, 1, 2, 4, 5, 7, 10, 15, 20, 30, 40, 50, 60, 80, 100]. Finally the drawn samples are corrupted with additive Gaussian noise N ( 0 , 0 . 1 2 I ) to construct the noisy observations. The non-stationarity is captured by the co-variance function 10 c where  X  k = [  X  x k ,a k ,b k ] . The ODE parameters are ini-cretized the ODE parameters k 1 , k 2 , k 3 , k 4 , V , Km over [0.05, 0.09], [0.4, 0.8], [0.03, 0.07], [0.1, 0.5], [0.015, 0.019], [0.1, 0.5] with the respective intervals 0.01, 0.1, 0.01, 0.1, 0.001, 0.1; the parameters  X  x , a , b over [0.1,0.9], [0.5, 2.5], [0.5, 2.5] with the respective intervals 0.2, 0.5, 0.5; the standard deviations of the noise  X  over [0.06, 0.14] with the interval 0.02. The 5 components of x 0 were dis-cretized in the intervals [0.5 1.5], [-0.1 0.1], [0.5 1.5], [-0.1 0.1], [-0.1 0.1] using 50 uniformly spaced bins. All three competing approaches were run for approximately 30mins CPU time. All three methods produce reasonable solutions and the reconstructions using numerical integration with parameters  X  sampled from the respective posteriors are similar. As such we show only the ideal Bayesian Numer-ical Integration procedure and our GP-ODE method in fig-ure(4). In table(5.1) the GP-ODE method closely matches the Bayesian Numerical Integration approach, with the Gradient Matching and Adaptive Gradient Matching ap-proaches producing broadly similar parameter estimates. Bayesian parameter estimation in ODEs using numeri-cal integration is an ideal but computationally prohibitive method for all but the smallest systems due to the high cost of explicit numerical integration required to evaluate a sin-gle point in the posterior. Any other model will necessarily make assumptions that are formally inconsistent with this ideal approach and the aim is therefore to trade the accu-racy of matching the ideal Bayesian numerical integration approach for computational speed.
 Whilst previous alternatives based on using Gaussian Pro-cesses have demonstrated some success in circumventing the high computational cost, these are not natural genera-tive models of the data. Despite the improvements in (Don-delinger et al., 2013), previous GP approaches use a heuris-tic compatibility function that leads to a complex chain graph with unknown normalisation constant.
 In contrast, our method has a natural link to numerical in-tegration and is we believe conceptually the closest match amongst competing GP approaches to the ideal numerical integration mechanism. Our GP-ODE approach is a simple generative model of data and as such is amenable to alter-native approximation techniques, other than Monte Carlo. For example, variational approximations are in principle possible to apply directly to the posterior.
 In our experience on toy benchmark problems, our GP-ODE approach performs at least as well as alternative GP approaches and sometimes significantly better, particularly in the case of observations with appreciable noise. Code is available from github.com/odegp/code.
 Acknowledgements We would like to thank Dirk Husmeier and Benn Macdon-ald for helpful discussions and provision of their code. Barber, D. Bayesian Reasoning and Machine Learning . Cambridge University Press, 2012.
 Calderhead, B., Girolami, M., and Lawrence, N. D. Accel-erating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes. In NIPS , 2008. Dondelinger, F., Filippone, M., Rogers, S., and Husmeier,
D. ODE parameter inference using adaptive gradient matching with Gaussian processes. In AISTATS , 2013. Graepel, T. Solving noisy linear operator equations by
Gaussian processes: application to ordinary and partial differential equations. In ICML , 2003.
 Koller, D. and Friedman, N. Probabilistic Graphical Mod-els: Principles and Technique . MIT Press, 2009.
 Ramsay, J., Hooker, G., Campbell, D., and Cao, J. Parame-ter Estimation for Differential Equations: A Generalized Smoothing Approach. Journal of the Royal Statistical Society: Series B , 69(5):741 X 796, 2007.
 Solak, E., Murray-Smith, R., Leithead, W. E., Leith, D. J., and Rasmussen, C. E. Derivative observations in Gaus-sian Process models of dynamic systems. In NIPS , 2002. Vyshemirsky, V. and Girolami, M. Bayesian ranking of biochemical system models. Bioinformatics , 24:833 X 
