 JUI-FENG YEH, WEN-YI CHEN, and MAO-CHUAN SU , National Chiayi University Typing involves using a keyboard to input text, which is then converted into machine-readable and machine-treatable forms. Many input methods have been combined with intelligent character selection, particularly for Asian languages. Furthermore, typing is one of the most crucial input methods in human-centered computing. Because the maturity of speech recognition technology is inadequate for developing Chinese dicta-tion for open domain applications, handwriting optical character recognition (OCR) is not a convenient alternative when a scanner is unavailable. Language is one of the most important communication channels for humans because sentences are used to convey meaningful information. Additionally, words are fundamental semantic units that play an essential role in the field of natural language understanding. Because words are building blocks in the domain of natural language processing, spelling and typographical errors usually have negative effects. Therefore, natural language, ei-ther written language or spoken, is the main communication channel in daily life. Moreover, natural language processing is useful for obtaining information from large-scale data. However, spelling and typographical errors are common in human-generated electronic text, and misspelled words can render natural language pro-cessing methods ineffective. Checking for spelling errors is a common task in every language, and automated spelling checkers are widely used. An effective spelling checker features both error detection and error correction [Mays et al. 1991]. Further-more, spelling checkers are essential for areas in computer science conclusively.
From the perspective of human X  X achine interaction, text input and sentence gen-eration are the main natural language interfaces in current use. Therefore, extracting and generating semantic structures from natural language plays an essential role in developing intelligent interfaces. Furthermore, greater attention should be given to the results obtained from inputted text. Because text may be input inaccurately, mis-understanding in natural language may occur. Spelling correction is a crucial module in natural language processing, particularly in detecting and correcting spelling or typographical errors. Spelling checkers used in alphabetic languages are more ma-ture than those used in Chinese, because such languages involve word delimiters and word length. Additionally, because Chinese involves more than 50,000 characters [HanDict 2010], it is much larger than that in alphabetic languages. However, many people using Chinese require an automatic spelling checker. In summary, the compu-tational complexity of Chinese spelling checkers is one of the essential challenges for Chinese application developers in the near term.

This article considers the problem of spelling error detection and correction as a form of document retrieval. Because sentences are the building block of syntactic analysis, a document is first decomposed into sentences. A sentence is regarded as the input query for each iteration. Accordingly, the spelling check problem was transformed to document retrieval one. Because an input sentence may contain spelling errors, we re-garded the text input as an original input query in information retrieval. Based on the visually and phonologically similar character sets provided by SigHan 7 and Liu et al. [2011], the expanded sentences were generated through word expansion and keyword substitution. An inverted index list was proposed as a framework for mapping the word to sentence. From the perspective of information theory, the scoring mechanism was composed of language and correction models. The word-based and part-of-speech (POS)-based n-gram models were used to formulate the language models. The maxi-mum entropy (ME) model was adopted to integrate the character, word, and contex-tual features in the correction model. Finally, to retrieve the hypothesized expanded sentence with the most semantic and contextual information, spelling errors were de-tected and corrected simultaneously. The main contributions of this article are briefly summarized as follows. Expansion by character substitution was achieved based on similar character sets. By pruning the undesired hypothesis, we minimized the search space. Finally, the adopted rescore mechanism accurately detected misspelled words and corrected them. A class-based trigram model was used to describe the desired in-put sentence, and it markedly improved the position detection. ME models integrating three category features focused entirely on the predetermined features of achievement were adopted to capture the effectiveness of corrections by evaluating the differences between the expansion string and desired sentence, including the character, word, and contextual features.

The remainder of this article organized as follows. Relevant works and the frame-work of the proposed method are described in Sections 2 and 3, respectively. Section 4 details the proposed inverted index list and rescoring mechanism for the spelling error detection and correction. The experiments conducted to evaluate the proposed method are reported in Section 5, and concluding remarks and suggestions for future research are provided in Section 6.
 Spelling checking function in student essays and second-language learning can aid learners in becoming immediately familiar with appropriate word usage [Xiong et al. 2014; Chen et al. 2011; Gamon 2010]. Internet search engines can check the spelling of a user query. An effective spelling checker can prevent pointless searches where the user misspells query terms. One of the key points of spell checkers is understand user intentions [Shuai et al. 2013]. Similarly, spelling checker should be able to cap-ture the user X  X  desired input string. Microsoft applied spelling correction to search en-gine queries [Silviu and Brill 2004]. Sun et al. [2010] explored phrase-based spelling error models derived from the click-through data. Ahmad and Kondrak [2005] trained a spelling error model by using search query logs. Li et al. [2006] used distributional similarity based models for query spelling correction. Gao et al. [2010] employed a ranker-based approach containing surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features for large-scale web searches.

Spell checkers play a critical role in improving the post processing of speech recog-nition [Bassil and Alwani 2012] and OCR [Rodphon et al. 2001; Al Azawi and Breuel 2014] for input interfaces. For spell checking in English, Google and Microsoft have developed a toolkit for further applications, and some application program interfaces (APIs), tools, and knowledge bases have facilitated spelling error detection and cor-rection. For example, [Google 2010] developed a Java API for Google Spelling Check Service, and Microsoft [2010] also provides Microsoft Web N-gram Services. Seobook [2010] developed an online tool for generating a corpus of keywords containing typo-graphical errors.

Over the past few decades numerous studies have been conducted on spelling error detection and correction. Most of these works on automatic error detection and correction have addressed spelling errors in English and built models of appropriate usage on native English data [Kukich 1992; Golding and Roth 1999; Remus 2014]. Arabic spelling correction has also received considerable interest among researchers [Rytting et al. 2011; Shaalan et al. 2012; Alkanhal et al. 2012; Eskander et al. 2013; Nawar and Ragheb 2014; Zerrouki et al. 2014]. Ahmad and Kondrak [2005] adopted expectation X  X aximization algorithm to enhance the performance of spelling error de-tection. Some studies have attempted to build a transformation-like model for machine translation, in which the noisy channel model was selected to describe the spelling error correction. Hidden Markov models have been used to correct spelling errors in search queries and to develop CloudSpeller [Li et al. 2011]. Bao et al. [2011] em-ployed graph theory to correct the word-and query-level errors. Ingason et al. [2009] proposed context-sensitive spelling correction and rich morphology for single words. Mitton [2010] surveyed spelling checking algorithms and systems for writing systems.
Kukich [1992] analyzed how words are corrected in text and concluded that the techniques of automatic spelling error detection and correction could be divided into the following three categories: nonword error detection, isolated-word error correction, context-dependent word correction. Golding and Roth [1999] applied machine learning technology to conduct a winnow-based approach to context-sensitive spelling correc-tion. Google one tera corpus was used by Islam and Inkpen [2009] to train a trigram model for spelling checker. Instead of focusing only on the context in which words appeared, Deorowicz and Ciura [2005] considered the possible causes of spelling er-rors when seeking a solution. Mays et al. [1991] presented a statistical technique for detecting and correcting certain errors in sentences. Pronunciation modeling was used by Toutanova and Moore [2002] to improve spelling correction.

Jiang et al. [2012] presented a rule-based approach by using knowledge resources to detect spelling and grammatical errors. Liu et al. [2011] described a statistical method to achieve significant improvement. Numerous studies have used n-gram models for detecting and correcting spelling errors and have obtained marked improve-ments [Chang 1995; Li and Wang 2002; Liu et al. 2013; Wang et al. 2013b; Chen et al. 2013b]. Huang et al. [2008] proposed a system framework integrating n-gram mod-els and Internet knowledge resources to detect spelling errors. He and Fu [2013] ad-dressed this problem by analyzing documents at the following three levels: character, word, and context. In addition to the n-gram model, Microsoft adopted a graph model for a Chinese spelling checker [Jia et al. 2013] as well as technologies for statistical machine translation [Liu et al. 2013; Chiu et al. 2013] and ME [Han and Chang 2013]. Ren et al. [2001] presented a hybrid approach that involved combining rule-based and probability-based methods for checking and correcting Chinese text. Hsieh et al. [2013] developed and applied several knowledge resources, such as CKIP-WS and G1-WS, to generate unknown words, correct characters, and validate the n-gram model. Yang et al. [2013] used a pattern matcher to detect and correct spelling errors by using a homophone dictionary and n-gram model. Previous studies have used conditional random fields for combining web information [Wang et al. 2013a; Yu et al. 2013] and developing a Chinese sentence parser [Wang et al. 2013b]. Regarding lexicon and on-tology, WordNet and FrameNet are the main knowledge representations for English [Fellbaum 1998]. Accordingly, HowNet and E-Hownet are lexicon ontologies for sim-ple and traditional Chinese, respectively [Zhendong and Dong 2006; Tai et al. 2009]. According to the word expression in E-Hownet, lexical senses are described in terms of the following two aspects: entities and relations. Thus, all taxonomic relations of lexical senses can be identified according to their definitions in E-Hownet. Regarding some of the investigations into Chinese spelling checkers in the past 20 years, Chang [1995] used bigram models to handle spelling errors, and Liu et al. [2011] and Chang et al. [2013] proposed visually and phonologically similar characters in incorrect Chinese words to address problems resulting from the computational complexity caused by the number of Chinese characters. Furthermore, Huang et al. [2008] used the Chinese phonemic alphabet to detect and correct spelling errors. Zhang et al. [2000] employed an approximate word-matching algorithm to correct typographical errors. Table I shows a brief summary about related works. This section describes the proposed system framework for detecting and correcting spelling errors. The proposed system is aimed at locating misspelled characters and correcting the corresponding errors in the original input Chinese sentences. Because one of the main causes of spelling error is confusion in phonologically and visually sim-ilar character set, the inverted index list architecture was adopted as the data struc-ture to store the hypothesized input for the Chinese spelling checker. The framework mainly involves three procedures: (1) generate an expanded sentence; (2) construct an inverted index list; and (3) apply the rescoring mechanism, which is composed of lan-guage and correction models. Interpolated class-based language models were used to estimate the probability of the expanded hypothesis, and a correction model incorpo-rating an ME model was used to integrate multiple categories of features. We combined the expanded sentence generator and inverted index list into one component. This article illustrates the system framework in two parts X  X nverted index list construction (Figure 1) and rescoring mechanism (Figure 2).

Figure 1 depicts proposed inverted index list construction, expansion, and prun-ing processes for identifying the actual characters that a user intended to input (but misspelled) without substantially increasing the computational complexity. From the perspective of processes, the proposed approach is described as follows. Because the input data is a document composed of multiple sentences, it is unsuitable as a building block for further processing because of data sparsity in statistics. First, a document decomposition module transforms the input text into several sentences according to a list of predefined punctuation marks. For each iteration, the sentence is regarded as the unit of analysis for detecting and correcting misspelled characters.

An expanded sentence-generation process further treats one sentence per iteration to generate an expanded hypothesis according to phonologically and visually simi-lar character sets. Regarding the computational complexity, the pruning method is adopted to reduce the number of expanded characters. A character lattice is obtained through expansion, which involves comparing similar character sets and pruning the improper expansion. Furthermore, the segmentation and parsing based on the CKIP autotag and E-Hownet was adopted to provide word-building functionality with POS tags. Based on the results, a word lattice containing POS information was assigned to an inverted index list. Under the assumption that no insertion or deletion error was embedded in the input text, the process of checking for spelling errors can be regarded as performing an optimal path search in the word lattice, which contains words in-put by the user as well as the corresponding to the expanded sentences. Instead of a single character substitution, the proposed approach can treat sentences with only one or multiple spelling errors by transforming the misspelling correction problem into an optimal path searching problem. Because words are the building blocks in a word lattice and the desired sentence might be one of the paths in the word lattice, an inverted index list was constructed to map the word to the corresponding expanded sentences. Finally, the rescoring mechanism in combination with interpolated class-based language and ME-based correction models X  X hich combines character, word, and contextual features X  X ere used to decide the hypothesized sentence. Finally, the correct sentence can be obtained by composing the all potential desired sentences se-quentially. Thus, the corresponding spelling error detection and correction processes are completed when each sentence obtained by applying the proposed approach itera-tively. The document composition module combines the system output containing the desired text and correction information.

The main contributions of this approach are that an inverted index list is constructed to locate spelling errors and correct typographical errors in Chinese text. The follow-ing sections further describe the process of the expanded sentence-generation pro-cess, inverted index list construction, and rescoring by using contextual and semantic information. This section discusses the technical issues involved in detecting and correcting spelling errors. The goal is to locate and correct erroneous characters in Chinese sentences. For the sake of clarity, the proposed approach is further divided into the following three parts: (1) an expanded sentence-generation process (Section 4.1); (2) inverted in-dex list construction (Section 4.2); and (3) rescoring by using language and correction models (Section 4.3). The goal of the expanded sentence-generation process is to recall the intended words that the user misspelled by comparing phonologically and visually similar character sets. However, inappropriate expansion can increase the computa-tional complexity. The inverted index list with pruning based on first-order statistics is used here to illustrate the sentence hypothesis set. Finally, the rescoring mechanism was adopted to determine which sentence hypothesis is the intended one. Because spelling error correction is considered as an information retrieval problem for selecting the appropriate sentence in this article, the problem can formulated as expressed in Equation (1): where S 0 is the input text, S i denotes the possible corrected text candidates generated by character substitution, and S  X  represents the user X  X  intended text input without spelling errors that is most likely to have been erroneously input as S Bayes X  rule, the constant denominator is dropped to obtain the formula based on an un-normalized posterior. The source and channel models presented in this equation were derived from information theory. In addition, the language model of the near optimal S is used as the source model, and the correction model for mapping the user X  X  input and correcting the text is based on the channel model. Expansion can markedly improve information retrieval processes where insufficient information is available. Expansion is effective for identifying missing information, particularly for short text queries. What is true for data mining is to a considerable extent also true for spelling error correction. Consequently, the proposed approach involves identifying misspelled through character expansion and substitution. Herein, word expansion is achieved by substituting the possible characters from phonologically and visually similar character sets. The proposed approach can handle such problems as inappropriate usage of characters and words as well as errors resulting from the method used to input Chinese text. However, the number of confusing characters de-fined in phonologically and visually similar character sets for each Chinese character is considerable. Thus, we are confronted by two difficulties, the first of which is the high computational complexity involved in identifying one spelling error in a sentence, and second one is when a sentence contains multiple errors. Examining some examples of these may clarify the efficiency of word expansion in obtaining candidate words when spelling errors appear. Keyword substitution can be used to correct the spelling er-ror, although semantic pruning is necessary to reduce the complexity significantly and render the proposed approach feasible.

The expanded sentence-generation process involves using phonologically and visu-ally similar character sets to obtain the intended words or characters that have been misspelled. Each sentence extracted from the user X  X  input string is considered as the input query S 0 . The essential knowledge resources (i.e., phonologically similar char-acter set and visually similar character set) were adopted to generate a candidate correction by using character expansion with characters that either similar in pro-nunciation or character shape. Sentence segmentation and parsing are first treated by using a traditional Chinese parser based on E-Hownet and CKIP Autotag, which were developed by Academia Sinica. A corresponding word sequence with POS tagging was obtained for further processing. Herein, cascading words with only one character can be partially merged to form a word with multiple characters if the word with multiple characters is defined in E-Hownet. The inverted index list was used to store the map connecting the words embedded in the word lattice that was constructed during the expanded sentence-generation pro-cess to its locations in the expanded sentences. The purpose of an inverted index list is to enable rapid near optimal path searches at a cost of increased computational com-plexity when a document is added. Inverted index lists are widely used data structures in information retrieval systems. Sufficient evidence exists showing that the probabil-ity of a spelling error occurring in the user X  X  text input is low. The document related to original text input is first added to the list for each word appearing in the path repre-senting the original input string. One of the most critical components in the proposed spelling checker is its reliable pruning mechanism with a confidence measure. With an accurate confidence measure for each substituted character, the pruning process can be used as a backend process for identifying characters that are suitable for substi-tution. Because the improper expanded characters are eliminated, the computational complexity is decreased because the search space has been minimized. Regarding two-sided contextual dependence, each character substitution is decided according to the confidence measure based on forward and backward bigrams at the character level. For each character in the original input, the top n candidates are selected based on an accumulative confidence measure with a probability of 95%. The remaining candidates are pruned if their confidence levels are too low to reduce the search space in the sub-sequent rescoring process. Each expanded character is examined using bidirectional bigram models [Wu and Yan 2005]. Following the character substitution process, an expanded character is removed from the expanded hypothesis if the number of for-ward and backward bigrams is zero for the preceding word and subsequent word in the original user input text. Thus far, we have described how words are selected for constructing the word lattice. In practice, each path in the word lattice represents a hypothesis word or character sequence, denoted here as expanded sentence Di. The candidate c j is assigned to the inverted index list according to whether its confidence measure achieves contextual dependence, as expressed in Equation (2).
 where CD is the contextual dependence of c j . A simplified example for the intended input sentence D  X  is  X   X  X  X  X  X  X  X  X   X  ( X  X wimming is good for your health X ), but the original input  X   X  X  X  X  X  X  X  X   X  ( X  X wimming is a health X ) contains a typographical error  X  X  .  X  The number of paths in the un-pruned and pruned character lattices are 127,008 and 1,960 respectively shown in Figure 3(a) and (b). Each path is considered as a hypothesis sentence, and the segmentation process is applied to obtain the word lattice (Figure 4). A list is further constructed for each word according to the data structure defined in Figure 5. Finally, the lists are arranged by word length, position, and frequency. Figure 6 shows an inverted index file of the obtained sentence. Instead of using two cascading stages (i.e., spelling error detection and correction), this study incorporated a rescoring mechanism that accomplishes the entire task in a single state. The spelling correction problem was further formulated as a ranking problem for the potential correc-tions and the user X  X  original input text. As shown in Equation (1), the rescoring mechanism is composed of class-based language models and ME-based correction models, which are detailed as follows. 4.3.1. Class-Based Language Models for Corrected Sentence. The n-gram language model for expanded sentence is expressed in Equation (3).

P S i = P w i 1 w i 2 w i 3 ... w i j ... w i n =
A smoothed n-gram interpolating a trigram, bigram, and unigram was adopted to estimate the probability of the expanded sentence, as shown in Equation (4): This study adopted word-based POS trigram models. For a conventional trigram model, the probability of a hypothesis word generally depends on the two words that precede it. Regarding the data sparsity and computational complexity, the tri-gram model described in this article can be estimated by observing not only the frequencies of the word pair f w i j  X  2 , w i j  X  1 and word triplet f w those of the POS for corresponding words, such as f POS w f POS w i j  X  2 , POS w i j  X  1 , w i j , as shown in Figure 7. The trigram model is formu-lated as follows: where C (  X  ) is the counting number, POS (  X  ) denotes the POS tag of a word (according to the definition in E-Hownet), and  X  is the linear combination factor, which is set to 0.9 here. As shown in Figure 8, the bigram model described in this article is linear combined with the word-based and POS based bigrams, as expressed in Equation (6). 4.3.2. Maximum Entropy-Based Correction Model. The aim of the correction model in the spelling checker is to determine the probability of corrected sentence S rect one based on the user X  X  original input S 0 . The corrected text is obtained by retriev-ing the intended text from the expanded sentence S i . In other words, a hypothesized expanded sentence must be obtained according to the user X  X  original input based on the posterior probability P S 0 | S i , which can be further decomposed into alignment and substitution. Briefly, under the assumption of no insertion and deletion error, the number of characters in the expanded sentence S i and user X  X  original input S identical. We can further transform the posterior probability at the document level into that at the word level by applying Equation (7): Since Chinese spell checking is processed in the single language, the word alignment is relatively simple compared to that in statistical machine translations. Most users subjectively avoid spelling errors. Therefore, many words with fertility one and ex-actly matched between the expanded sentence S i and the original input S to the alignment results, we can further divide the words in S gories: correctly spelled words, and misspelled words. Accordingly, Equation (8) can be rewritten as follows: where I (  X  ) is an indicator function, I w 0 p = w i q implies that w (i.e., they are the same word with the same position in S function I w 0 p = w i q has a value of 1 when the words are identical, the probability at the character level. Equation (8) can be further simplified as follows: The probability P w 0 1 w 0 2 w 0 3 ... w 0 m w i 1 w i 2 w tain the same number of characters. Under the assumption of only one character requiring substitution, the probability P w 0 1 w 0 2 w 0 rewritten at the character level as Equation (10): where I (  X  ) is an indicator function, I c 0 p = c i p means that c character. In other words, S i is expanded according to c the character at position p is substituted. They are the same word with the same position in both S 0 and S i . The symbol S 0 c represents the number of characters in S However, considering only the character-level features is inadequate for determining whether character substitution is necessary. Because the semantic and contextual is essential for spelling error detection, we reformulated Equation (10) as Equation (11): where character c joined with sentence S means that the character substitution should consider the entire document, including the word-and context-level features.
The ME model was adopted to integrate the three feature categories for spelling error detection and correction. ME modeling is a general-purpose machine learning framework that has proven to be highly expressive and powerful in natural language processing. The adopted ME model is formulated in the following equations: where f k (  X  ) is an indicator function,  X  k is the weight assigned to feature k ,and Z ( a normalization factor. Features are modeled using indicator function f of which is set to 1 when k is true for a particular class (otherwise, the value is set to 0). The model parameters were estimated using generalized iterative scaling [Darroch and Ratcliff 1972]. One of the main contributions in this article is the feature extrac-tion method used for scoring in the correction models. Table II summarizes the three feature categories features containing character-, word-, and contextual-level features adopted to obtain the desired document.

In contrast to English, each Chinese character cannot be input in one keyboard stroke. Each Chinese character is input by a sequence of keyboard strokes, accord-ing to its pronunciation and shape. Pinyin code and Cangjie code are used to encode Chinese characters. Figure 9 shows an example of the pinyin and Cangjie code for the Chinese character  X   X  symbol sequence for the target character is  X   X  X  X  stroke sequence for the pinyin code is  X  X -6 X . The shape of the target character can de-composed as  X   X  X  X  X  X  X  , X  code is  X  X -Y-S-D X  (as shown on the left-hand side of the figure).

Subsequently, some identical and similar characters remain, and the user must se-lect the one he or she needs. To handle problem, Liu et al. [2011] proposed a similarity measure by using Dice X  X  coefficient with the longest common subsequence. Based on Dice X  X  similarity measure ranking, the phonologically and visually similar character sets were obtained in that study. Furthermore, the number of typographical errors is another characteristic of the user X  X  input. The Levenshtein distance is frequently adopted for measuring the difference between the code sequence of two characters that are input using either pinyin or Cangjie code. The Levenshtein distance between two code sequences is the minimum number of single keyboard stroke edits X  X ncluding insertions, deletions, and substitutions X  X hat are required to change one word into the other one. Two characters that differed by at most one symbol in Cangjie code would be considered to be highly similar and would thus be retained. Mathematically, the Levenshtein distance based on a dynamic algorithm between two keyboard stroke sequences KSS 0 and KSS i for S 0 and S i respectively is defined by the following equation: d where s and t are the s -th and t -th keyboard stokes in KSS and I (  X  ) denotes the indicator function representing the mismatch between the s -th and t -th keyboard stokes in KSS 0 and KSS i , respectively. For instance, the wrong key-board strokes  X   X  X  X  X   X  for the character  X   X   X  will set the indicator function I ( Because S i is one of the expanded sentences, it can be separated into two categories: the intended input and erroneous ones. The intended input is defined as the string that user intended to generate and it contains any no misspelled character. For example, the Levenshtein distance between for the sentence S 0  X   X  X  X  X  X  X  X  X  sired sentence S i  X   X  X  X  X  X  X  X  X   X  is one shown in Figure 3. In other words, S S  X  are identical. From this aspect, the following sets of Levenshtein distance-based were adopted: Additionally, lexical information can effectively facilitate spelling error detection. The expended words  X   X   X  and  X   X   X  with high scores both unigram and bigram. However, the word  X   X  X  X   X  with two characters is defined in E-Hownet and obtains higher prior-ity as the substring in the desired sentence compared to  X  substitution to generate the expanded sentence, the word segment should be modi-fied according to the substitution character. Furthermore, the substitution character process occasionally forms a word contained in E-Hownet. The variables w represent newly formed words generated by the character substitution process. These words appear in both original and expanded sentences and contain the characters c and c i p , respectively. Herein, the word-level features based on lexical information are defined as follows: f f Because words are the building blocks of semantic analysis, this study adopted word-level features. Instead of stemming in English, word segmentation plays a critical role in Chinese language processing. The maximal matching criterion is invariably oper-ative in word tokenization. In other words, users prefer words with more characters. For example, the word segmentation  X   X  X  X  - X  X  - X  X  X   X  outperforms  X  for the same input character string illustrated in Figure 4 for detecting the spelling errors. These phenomena are formulated as follows. where |  X  | is the number of characters comprising a word; w containing the potential substituting character c i p and substituted character c and S i at positions P i and P 0 , respectively. Lexical information plays a crucial role in Chinese spell checking and correction. Regarding insertion errors, the proportion of repeated characters is generally large. For example,  X  error  X   X   X  appears in the first character for desired input  X  formulated as follows: Regarding word sense and usage, the contextual information is crucial for detecting and correcting spelling errors. Word-based approaches do not consider the entire con-text of the sentence. Thus, to extract the corrected word from the expanded sentences and improve the ranking score, the contextual features indicating the affinity of each word in the candidate set are defined in the entire context. For example, the desired in-put string  X   X  X  X  - X  X  - X  X  X   X  with three words is better than the expanded input string  X   X  - X  - X  - X  - X  - X   X  with six words for the same input character string illustrated in Figure 4. Regarding entire documents, the features based on the comparisons for the numbers of word in user X  X  original input S 0 or the expanded sentence S in Equations (29) and (30): where |  X  | is the number of words in a document. Another aspect of contextual infor-mation is focused on the co-occurrence of words within a sentence or document. The strength of word association can be measured using mutual information (MI). By com-puting the mutual information of a word pair, information on many effective prefer-ences can be obtained from the corpus. The point-wise MI ( PMI ) between the potential corrected word w i P The average PMI ( APMI ) is further defined as Equation (32) among the potential corrected word w i P Similarly, the PMI and the APMI are defined as shown in Equations (33) and (34), whichfocusonw 0 P The two features based on APMI , shown in Figure 10 were used to detect spelling errors, are defined in Equations (35) and (36). f f According to observations, some false alarms may be generated by words compris-ing only one character. Since the syntactic parser is absent here, some phrase-level contextual information is lost. To improve the performance, the following three fea-tures based on POS were adopted: f f f Previous studies have attempted to incorporate numerous additional features into the correction model to handle specific spelling error conditions. For the corrected language model, we employed a linear combination of word-and class-based trigram models. The proposed approach involved using a rescoring method for the expanded sentences to obtain the one that best represents the intended input. To evaluate the proposed method, a Chinese spelling checker system based on the proposed inverted index list with a rescoring mechanism was investigated. The ex-periment was aimed at evaluating the effectiveness of the proposed approach when applied to a Chinese spelling checker. The task comprised two subtasks: (1) error de-tection; and (2) error correction. Subtask 1 is performed to locate a spelling error in a sentence, and Subtask 2 locates the error and corrects the error. For Subtask 1, 1000 complete Chinese sentences with and without spelling errors were given, from which the location of incorrect characters should be obtainable. For Subtask 2, 1000 complete Chinese sentences containing spelling errors were used. In these sentences, the incor-rect characters must be located and the corresponding correct character must be iden-tified. Accomplishing these two subtasks is the entire function of a spelling checker. The errors, which generally result from phonological or morphological confusion, were gathered from essays written by students. The phonological and visually similar char-acter sets, which contain Chinese characters of similar shape and identical or similar pronunciation were described by Liu et al. [2011]. The test data preparation and simi-lar character sets were obtained from the SIGHAN 7 Bake-off 2013: Chinese Spelling Check Shared Task [Wu et al. 2013].

Since the rescoring mechanism comprises a language model and correction model, suitable parameter estimations should be obtained to improve the performance of the proposed approach. For the language model, the backoff n-gram models are adopted here by linear combination. For the word-based and POS-based bigrams and trigrams, the combination factor  X  was also added. To reduce the complexity and achieve an acceptable level of performance,  X  was first set to 0.9 based on Equations (5) and (6). Because the language model determines the probability of a sentence being the cor-rect one, the correction rate for judging a sentence with or without spelling errors was used to perform the estimation. To obtain suitable parameters in Equation (4), 200 sentences selected randomly from the training set in Subtask 1 were used to estimate the linear combination factors. Table III shows the results. Finally, the factors y, t, and r were set as 0.8, 0.16 and 0.04, respectively. The following four approaches, which were developed by the research groups that participated in the SIGHAN 7 Bake-off 2013, were used to evaluate the performance of the proposed approach: (1) the n-gram language model [He and Fu 2013]; (2) an ME model [Han and Chang 2013]; (3) a graph model [Jia et al. 2013]; and (4) machine translation [Chiu et al. 2013]. The differences between the methods, features, and parameters used in these approaches and those used in the one proposed in this study are described as follows. The n-gram based ap-proach proposed by HLJU [He and Fu 2013] can detects potential errors by analyzing the following three levels: character, word, and context. Each analysis generates error candidates independently. Substitution by using a similarity matrix based on a simi-lar character set [Liu et al. 2011] was adopted. Finally, the word trigram models were used to score the correction as formulated in the following equation: An ME approach with contextual character features for Chinese spell checking were pro-posed by Han and Chang [2013] at Peking University. Five feature templates based on the three characters preceding and following the candidate character c ployed. However, the candidate characters c i was not considered in the feature tem-plates used in that study. Jia et al. [2013] used a shortest path word-segmentation algorithm [Casey and Lecolinet 1996] to determine the optimal path in a graph. They considered the hypothesis that the sentence yielding the highest mutual information (MI) gain is the desired sentence. The MI measure for adjacent characters is expressed in the following equation: Chiu et al. [2013] detected potential errors by using word segmentation. Misspelled character typically broke the structure of word and resulted in over-segmentation by data observation. Cascade single-character words typically accompany over-segmentation. They regarded singleton word sequences as misspelled error candidates for the error detection process. Language and translation models were used to de-termine the intended sentence from several hypothesized sentences. The translation probability tp is defined in the following equation: where C (  X  ) is a counting function that returns the frequency that a term appears in the training corpus,  X  denotes the similar type weight for visual and phonological. Hereinafter, the experimental results and related discussions about Subtasks 1 and 2 are described in detail in Sections 5.1 and 5.2, respectively.
 The goals of Subtask 1 are to detect whether an input sentence contained errors and to identify the location of such errors. This subtask was divided into two pro-cesses: sentence-level detection and error-location detection. Subtask 1 was aimed at achieving detecting erroneous characters while avoiding a high false alarm rate. The sentence-level performance metrics are described as follows. There are five metrics were used to detect sentences containing errors, and four metrics were used to locate the errors. Table IV shows a contingency table of the related hypothesis.
The five metrics for detecting the sentence with errors (i.e., false alarm rate FAR, detection accuracy DA , detection precision DP , detection recall DR , and detection F1 DF1 ) are formulated as follows: The four metrics for detecting the location of errors (i.e., error location accuracy ELA , error location precision ELP , error location recall ELR , error location F1 ELF1 ) are formulated as follows:
Table V shows the performance evaluation results for the sentences with and with-out errors. According to the results, the suitability of the proposed approach for Sub-task 1, either in precision rate or recall rate, is acceptable. However, the proposed approach did not outperform the previous approaches. Because not all of the expanded words or characters were involved in the lattice, an inverted index list containing po-tential word sequences was adopted in the proposed approach. This approach provides a more flexible structure for representing the search space at the character and word levels. Expanding substituted character candidates according to a similar character set is a core problem in Chinese spell checking. Because the number of character in Chinese is considerably large, unlimited character expansion would cause Chinese spell checking systems to fail. Compared with our previous work, we found that the phonologically and visually similar character sets proposed by Liu et al. [2011] offer considerable advantages for candidate expansion. However, when considering real-time applications, performance issues cannot be overlooked. Consequently, the phono-logically similar character sets were further divided into four categories of similar sets: (1) identical pronunciation and tone; (2) pronunciation but different tone; (3) similar pronunciation and identical tone; and (4) similar pronunciation but different tone. Combining characters of similar shape in a visually similar character set gen-erates five categories of similar character type. The graph model involves using two weighted parameter sets to estimate the multiplication and linear combination of sim-ilarity and log conditional probability. The optimal results were obtained when the weights of similar pronunciation same or different tone and similar shape were high. High weight values indicate that most of the characters would be correct if the sen-tence was pronounced without tones. However, opinions on this subject were divided between the SJTU and HLJU groups. The researchers in the HLJU group compared the two character expansion conditions. Instead of using all five types, identical pro-nunciation and tone and similar shape achieved the optimal performance. A total of 13,053 computer readable traditional Chinese characters are defined in CNS11643 (http://www.cns11643.gov.tw). The characters are further divided into frequently and less-frequently used characters. The 5,401 frequently used characters cover most char-acters used in daily life. The Basic Vocabulary Table of Modern Chinese Characters comprises 3,500 characters [Ye 1987], and the coverage ratio of the 964 most-frequently used characters was 90% in 2007. In place of partitioning similar character sets, the character-level bigram-based confidence measure was adopted in our proposed ap-proach. The filter threshold of the proposed approach was set at 95%. The detection recall DR was inadequate compared with other approaches because of the strict cri-terion. If the multiplication was replaced by addition, the detection recall DR would be improved considerably. However, the forward and backward context dependencies should be related in theory. An effective smoothing function should be helpful here. Regarding the false alarm rate FAR, both the machine translation approach proposed by NTHU and the one proposed in the present study are sufficiently low. These ap-proaches take the similar phonemes and Cangjie code similarity for substituting char-acter candidate selection in the machine translation approach. To obtain the same effectiveness, the Levenshtein distance (Equation (14)) was adopted to measure the difference between the keyboard strokes between characters. However, it was imple-mented in the correction model (Subtask 2) and its utility does not enable detection (Subtask 1). The recall rate should be increased as much as possible so that we can proceed without superfluous false alarms in the detection phase. However, the lower detection recall DR renders the detection accuracy DA and detection precision DP in-adequate because of the absence of the desired substitution character. According to these results, we find that spelling errors usually involve a word with one character. This is the reason why several features employed in correction model were focused on character-level characteristics, because they can improve recall rate. Considering the entire sentence, the n-gram, graph model, machine translation, and proposed approach have adequate recall. For the proposed correction model or the ME-based approach, features play an essential role in correcting sentences containing spelling errors. Table VI shows the evaluation results of spelling error location detection. This part is aimed at locating spelling errors.

These results indicate that the proposed approach, graph model, and machine trans-lation achieve acceptable performance. The same reason causes the error location recall ELP is not as high as the previous method for detecting sentence-level errors X  some of the desired corrected pattern are filtered using the confidence measure. After analyzing the experimental data, we found that the most-desired character was cor-rectly detected when it was recalled. After replacing the multiplication with addition, the performance improved by nearly 18%. Word segmentation plays an essential role in locating the error position. Some incorrect segmentations with high n-gram scores result in errors being overlooked; in other words, some misspelled characters are in-troduced by automatically selecting characters in intelligent Chinese character input methods. A reason for missing the detection is that a character may be misspelled as another single-character-word with high frequency such as  X   X   X  (de). This condition also appears in high-frequency multicharacter words formed because of the segmen-tation boundary. Lexical information can provide marked improvements for isolated single characters. Isolated characters that cannot form a single-character-word are a likely position at which an error occurs. This assertion can improve the error loca-tion precision ELP and error location accuracy ELA considerably. Additionally, cas-cading isolated characters usually accompany such errors. Using lexical information can achieve the same effect regarding error location precision ELP and error loca-tion accuracy ELA . Compared with previous sentence-level error detection methods, locating the position of an error is a more difficult task. Basically, the error can be located by a good language model theoretically. The n-gram language model achieved an acceptable level of performance. In addition to the language model, the mapping relations between the misspelled characters and desired substitution character should be considered when locating an error. According to the results shown in Table VI, the graph model, machine translation, and proposed model with the mapping relations outperform other models that consider only the corrected sentence. BTW, the classed based n-gram models used in the proposed method improved the error location recall rate for preventing misspelled errors. Compared with the word-based language model, the class-based language model decreased the risk of misspelling a word, particularly that we have some mis-spelled errors in candidate character selection. However, the features employed in the ME model provided a more precise measure and markedly improved the error location precision ELP and error location accuracy ELA . From the perspective of system development, the error location is more practical than error sen-tence detection. After detecting the sentence with spelling errors and locating the spelling errors, one of the most crucial issues is how to correct the spelling errors automatically. The aim of Subtask 2 is to locate errors in sentences containing spelling error and correct the errors. The following three metrics were used to evaluate the performance: location accuracy LA , correction accuracy CA , and correction precision CP . The first one is for locating a spelling error and the others are for correcting it. The computations are formulated as follows.
 Table VII shows the evaluation results for the spelling error correction. These results show that the proposed approach outperformed the other approaches considerably in location accuracy LA and correction accuracy CA , indicating that the rescoring mech-anism with the class-based language models and ME model provided the anticipated utility. The correction precision CP of the proposed approach is good enough, which can be attributed to the inverted index list, which yielded a difference ratio of 0.425%. The entire evaluation in Table VII shows that the proposed method achieved the optimal performance among the examined models. Moreover, the experimental results of the proposed approach shown in Tables VI and VII differ considerably. The error location performance shown in Table VI is considerably lower than the location accuracy LA shown in Table VII. These results show that the misspelled errors in the candidate character selection also have negative affection. Subtask 2 was performed under the assumption that each sentence contained only one error. Accordingly, some misspelled errors are missed because of the drawbacks in the detection phase, such as those in-volved in Subtask 1. Besides, simultaneously considering the language model of the corrected sentence and mapping relations between the misspelled and corrected pat-terns can achieve acceptable performance in the location detection task. Empirically, the parameter estimations of the n-gram model proposed by HLJU and the approach proposed in the present study are identical for the interpolation weighting in the linear combination. We compared the proposed classed-based language model and word-based language model by using a conventional n-gram and found that the class-based model provided a smoothing effect and outperformed the conventional n-gram model. Combined with a more precise scoring method, such as the ME model, the class-based language model can improve the spelling error correction. These results show that ME can improve the correction of spelling errors, and the feature is crucial for ME. Additionally, ME provides a framework for integrating other features. An effective fea-ture set can improve the correction of spelling errors. Therefore, three category feature sets were adopted in the proposed method. Character-level features improved the pre-cision rate, particularly the location accuracy LA . The features f measure the difference in the input keystroke sequence between a misspelled charac-ter and a potentially correct character, and acceptable performance was obtained for all metrics. However, the same distance measure cannot be adopted for pinyin and Cangjie codes because the Cangjie code is more complex than the Pinin code. Herein, just one and two are considered in different intervals for the Cangjie and Pinyin codes. Consequently, some possible characters from the Cangjie code may have been missed. Fortunately, approximately 30% of characters in the intersection of phonologically and visually similar character sets. The features f 11  X  f repeated characters. However, there are no repetition errors in the test corpus. Nev-ertheless, the problem would become more complex if repetition and deletion errors were included in the Chinese spelling checker. Word-level features provide semantic and segmentation information for correcting spelling errors. Lexical information de-fined in E-Hownet was adopted by using the features f3, f5, and f6. These features can be used to handle word boundary errors resulting from segmentation. This type of error usually has a high probability of occurring in language models and is diffi-cult to detect. The same condition also appears in automatically selected characters in intelligent Chinese input methods. Lexical information can be used to retain possible character lists and be assessed in combination with other features to improve the lo-cation accuracy LA . Correction accuracy CA and correction precision CP are further improved when an error is located. The features f 7 , f 8 the length of words in the sentence. According to the data, the probability of finding the correct character within a multi-character word is higher than that of finding an isolated single-character-word, particularly when the word length is more than 2. An-other effect of f 7  X  f 10 is that the hypothesis with fewer isolated one-character-words has a higher likelihood of being selected as the intended sentence. Contextual features f 15 and f 16 mainly measure the number of words in the sentence, and they provide very little help. The features f 15 and f 16 use mutual information to determine the dis-tance dependence between words in the same sentence to capture the global depen-dencies. They can provide complementary information for language models with local dependencies. The POS-based features f 15 and f 16 can detect some typographical errors that are difficult to determine, such as  X   X  X  X   X  (need, noun),  X  (plan, noun), and  X   X  X  X   X  (plan, verb) in the word context. Moreover, an effective Chi-nese parser would outperform f 15 and f 16 . However, building a parser for Chinese is a difficult problem in natural language processing. Future research involving semantic analysis would be helpful in resolving some spelling error correction problems, such as the example given by Wu et al. [2013]  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  (I used a wooden hammer to handle this set of Chinese medicines). The proposed method cannot distin-guish  X   X  X   X  (wooden hammer) from  X   X  X   X  (mental hammer) without the semantic analysis and background knowledge.

Since the goal of Chinese spelling checkers is to detect and correct errors, the cor-rected sentence and how this can be achieved should be considered simultaneously. Practically, the system should provide acceptable performance in detecting or correct-ing errors. Instead of using a word lattice, an inverted index list was adopted in this study because we wanted the sentences with words with a higher probability of being correct word to be considered first to ensure that the desired corrected hypothesis was reasonable. Confidence measure-based pruning can achieve acceptable performance by reducing the search space. However, the criteria employed here is inadequate. Another contribution of this article comes from the rescoring mechanism, which considers the reasons for the spelling errors. Feature design in ME modeling is crucial. Although the features used in this article were effective in correcting errors, we believe that in-corporating more useful would provide further improvement. An ME-based feature-integrating platform can be achieved. According to the results shown in Tables V, VI and VII, the proposed method should offer adequate practical utility, particularly in the rescoring mechanism for spelling error correction. This study presented an inverted index list-based approach for Chinese spell check-ing. A word lattice was constructed according to word or character substitution by us-ing phonologically and visually similar character sets. Segmentation and POS tagging were further processed according to E-Hownet. A rescoring mechanism that comprised a language model and correction model was used to select a nearly optimal path in the inverted index list. The hybrid n-gram model with a word lattice and POS was first applied to construct backoff language models for the user X  X  desired input. ME was adopted to integrate the character-, word-, and context-level features simultaneously for each expanded sentence. Finally, the potential desired document with the highest score was obtained. The experimental results for evaluating the up-to-date data set, provided by SIGHAN 7 Bake-off 2013: Chinese Spelling Check Shared Task, shows that the proposed inverted index list with a language model and correction model outperformed conventional approaches in spelling error checking. The contributions of the proposed approach can be further summarized as follows. Expansion accord-ing to phonologically and visually similar character sets can enable the correction of misspelled words. By using an inverted index list and pruning the undesired paths, the computational complexity was reduced substantially. Finally, the adopted rescor-ing mechanism accurately located misspelled words and corrected them. Because of the limited training corpus, a class-based trigram model was used to describe the de-sired input sentence. The model markedly improved the position detection. ME models integrated three category features focused on the predetermined features to determine effectiveness of the correction based on the differences between the expanded string and intended sentence at the level of the character, word, and contextual features. The results also demonstrate that the proposed method can effectively improve the perfor-mance of Chinese spell checker. Furthermore, the features corresponding to the error patterns for the insertion and delection should be included in the future for practical usage.

