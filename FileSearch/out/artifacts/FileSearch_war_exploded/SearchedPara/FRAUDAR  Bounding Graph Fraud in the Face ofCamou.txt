 Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or fol-lows? Existing fraud detection methods (spectral, etc.) try to iden-tify dense subgraphs of nodes that are sparsely connected to the re-maining graph. Fraudsters can evade these methods using camou-flage , by adding reviews or follows with honest targets so that they look  X  X ormal X . Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic.
Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage-resistant, (b) provides upper bounds on the ef-fectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both cam-ouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1 . 47 billion edges, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services.
How can we detect if a politician has purchased fake follow-ers on Twitter, or if a product X  X  reviews on Amazon are genuine? More challengingly, how can we provably prevent fraudsters who sell fake followers and reviews for various web services from evad-ing our detection systems? In this paper we focus on precisely this problem  X  specifically, how can we design a fraud detection system with strong, provable guarantees of robustness?
Given the rise in popularity of social networks and other web ser-vices in recent years, fraudsters have strong incentives to manipu-late these services. On several shady websites, anyone can buy fake Facebook page-likes or Twitter followers by the thousands. Yelp, Amazon and TripAdvisor fake reviews are also available for sale, misleading consumers about restaurants, hotels, and other services and products. Detecting and neutralizing these actions is important for companies and consumers alike.
 The tell-tale sign of such fraudulent actions is that fraudsters must add many edges, creating unusually large and dense regions in the adjacency matrix of the graph (see Figure 2). Smart fraudsters will also try to  X  X ook normal X , by adding links to popular items/idols (like famous singers/actors, or well-liked products) -this behavior is called  X  camouflage  X  in the recent literature. State-of-the-art algo-rithms, such as S POK E N [23] and N ET P ROBE [21] exploit exactly the density signal, but do not account for  X  X amouflage. X 
We propose FRAUDAR, a novel approach for successfully de-tecting fraudsters under camouflage, and we give provable limits on undetectable fraud. We provide data-dependent limits on the max-imum number of edges a group of fraudulent adversaries can have without being detected, on a wide variety of real world graphs. As shown in Figure 1(a), FRAUDAR provides limits on undetectable fraud, and additionally provides novel optimizations that strengthen this bound.

Moreover, our method outperforms competitors and finds real world fraud on Twitter. In Figure 1(b) we find that FRAUDAR detects injected fraud with high accuracy, even in the case of cam-ouflage, where prior methods struggle to detect fraudulent attacks. Additionally, when tested on a Twitter graph from 2009, FRAU-DAR finds a 4031 by 4313 subgraph that is 68% dense. As shown in Figure 1(c-d), we find that a majority of the detected accounts had tweets showing that they used follower-buying services, and had gone undetected by Twitter for the 7 years since the data was collected. Finally, our method is scalable, with near linear runtime in the data size.

Thus, our main contributions are as follows:
Furthermore, FRAUDAR offers natural extensibility and can easily incorporate more complex relations available in certain con-texts such as review text, IP addresses, etc.
 Reproducibility : Our code is open-sourced at www.andrew.cmu. edu/user/bhooi/camo.zip . TweepMe or TweeterGetter . (d) Real-life results -a sample fraudster caught. Fraud detection has received significant focus in recent years. Many existing methods aim to detect fraud through review text [13, 20]. However, these approaches are typically not adversarially robust: spammers can carefully select their review texts to avoid de-tection. Even without knowledge of the detection system, they may mimic normal user reviews as closely as possible. Graph-based approaches detect groups of spammers, often by identifying unex-pectedly dense regions of the graph of users and products. Such methods are potentially harder to evade, as creating fake reviews unavoidably generates edges in the graph. Graph-based methods may be classified into global and local methods.
 Global methods: Building on singular value decomposition (SVD), latent factor models, and belief propagation (BP), these model the entire graph to find fraud. S POK E N [23] considered the  X  X igen-spokes X  pattern produced by pairs of eigenvectors of graphs, and was later generalized for fraud detection [12]. F B OX [25] builds on SVD but focuses on detecting attacks missed by spectral tech-niques. Several methods have used HITS [15]-like ideas to detect fraud in graphs [3, 6, 9, 11, 30]. BP has been used for fraud classifi-cation on eBay [21], and fraud detection [1]. All of these methods have been successful in finding fraud but they offer no guarantees of robustness. [25] performs adversarial analysis for spectral algo-rithms, showing that attacks of small enough scale will necessarily evade detection methods which rely on the top k SVD components. Local clustering methods: A different direction for fraud detec-tion focuses on local subgraphs, by analyzing the properties of egonets to detect fraud [5,22]. C OPY C ATCH [2] and G ET [12] use local search heuristics to find relevant dense bipartite sub-graphs. However, without guarantees on the search algorithm, the algorithms may not be robust to intelligent adversaries. Dense subgraph mining: Finding dense subgraphs has been an important focus of graph theory communities and has been stud-ied from a wide array of perspectives [7, 14]. Most closely related Detects dense blocks X X X X X X X Camouflage-resistant X ? ? X Theoretical guarantees X T able 1: Comparison between FRAUDAR and other fraud detec-tion algorithms. to ours is Charikar X  X  work on finding subgraphs with large aver-age degree [4], which shows that subgraph average degree can be optimized with approximation guarantees. Variants have been pro-posed to efficiently find large, dense subgraphs [27], with approx-imation guarantees. To our knowledge, however, this is the first work which adapts this theoretical perspective to the challenges of fraud detection and camouflage resistance, and achieves meaning-ful bounds for our application. Moreover, our work differs from these in its setting of bipartite graphs, and in the use of edge re-weighting to further increase accuracy.
 Social network-based Sybil defense: Multiple identity or  X  X ybil X  attacks pose problems of malicious behavior in distributed systems. SybilGuard [32] and SybilLimit [31] use a decentralized random walk approach to limit the number of Sybil attackers. SumUp [26] and Iolaus [19] adapt this to content rating settings. However, these systems rely on a separate trust network between users; our setting is fundamentally different as our approach works directly with the user-product bipartite graph.
 Handling camouflage: [8, 28] consider fraud detection methods that are robust to camouflage attacks. However, both methods focus on the time-series domain, observing changes in the behavior of fraudsters from system access logs rather than graph data.
A comparison between FRAUDAR and other fraud detection al-gorithms is summarized in Table 1. Our proposed method FRAU-DAR is the only one that matches all specifications. Consider a set of m users U = { u 1 ,...,u m } and n objects W = { w 1 ,...,w n } connected according to a bipartite graph G = U = { u 1 , ...,u m } Users W = { w 1 ,...,w n } Objects ( U  X  X  , E ) . We can consider the objects to be followees on Twit-ter or products on Amazon. Table 2 gives a complete list of the symbols we use throughout the paper. We now describe our attack model and then our problem definition.

We assume that fraudsters are hired to use users they control to add edges pointing to a subset of nodes in W . For example, a busi-ness may pay for followers on Twitter or positive reviews on Yelp. In general, fraudsters add a large number of edges, inducing a dense subgraph between the fraudster accounts and customers, as shown in the bottom right corner of each subplot of Figure 2. This gen-eral characteristic of fraud was found to be true in our experiments on real datasets, as well as in many other papers which use dense blocks to detect fraud [1, 2, 12, 21, 23].

To mask the fraud, fraudster accounts can add arbitrary  X  X amou-flage X , i.e. edges pointing from their user accounts to any of the nodes in W that are not customers. We assume that fraudsters have complete knowledge of the graph and fraud detection mechanisms, enabling worst-case camouflage for any fraud detection system we create. Examples of the possible types of camouflage are given in Figure 2: (a) adding camouflage edges to random honest users, (b) camouflage biased toward high degree nodes, (c) using hijacked ac-counts, whereby fraudster accounts have realistic patterns of cam-ouflage essentially similar to that of honest users.

While it is trivial for fraud accounts to add edges to any other node, it is more difficult for customer accounts to get honest edges. In particular, we assume that a customer would try to increase their number of incoming edges by a significant portion, and as a result a fraction,  X   X  [0 , 1] , of their incoming edges will be from fraud-sters. This assumption would manifest itself as customers wanting to boost their follower count to seem noticeably more popular or a restaurant wanting a significant number of positive ratings to shift its average  X  X umber of stars X  on Yelp. We will demonstrate how using this real world pattern significantly improves fraud detection both theoretically and in practice.

Our goal is to detect dense subgraphs in G , typically indicative of fraudulent groups of users and objects, like in the bottom-right of Figure 2.
 Given a bipartite graph, detect attacks so as to minimize the num-ber of edges that fraudsters can add pointing to customers without being detected.

Given that we want our detection algorithm to be able to handle camouflage, we define the requirements for a camouflage-resistant algorithm:
D EFINITION 1. Let ( A , B ) be a block consisting of fraudulent users and objects. A density metric g is camouflage-resistant if when any amount of camouflage is added by the adversary, g ( A X  B ) does not decrease.
 That is, fraudsters cannot make themselves less suspicious by adding camouflage. Our goal is to find a fraud detection approach satisfy-ing the following criteria: Design a class of density metrics for bipartite graphs, which can be optimized (1) in near-linear time, (2) within a constant factor of the optimum, and (3) is minimally affected by camouflage edges added by adversaries.

Obtaining theoretical guarantees on the near-optimality of the returned subgraph is important because, as we will later show, it allows us to offer guarantees against worst-case fraudsters. Given this problem definition and attack model, we now offer FRAUDAR and our theoretical analysis of FRAUDAR.
In this section, we propose a class of metrics g that have par-ticularly desirable properties when used as suspiciousness metrics. Namely, we will show that if g takes the form in (1) and (2), then it can be optimized in a way that is (a) scalable; (b) offers theoretical guarantees, and (c) is robust to camouflage.

Let A  X  U be a subset of users and B  X  W be a subset of ob-jects. Let S = A X  X  , and V = U  X  X  . For the rest of this paper, we use g to denote the density metric that the algorithm will opti-mize, i.e. the algorithm will find S to (approximately) maximize g ( S ) . Note that g has a single argument, which is the union of the users and objects whose suspiciousness we are evaluating.
We propose using density metrics g of the following form: where total suspiciousness f is: for some constants a i  X  0 and constants c ij &gt; 0 .

Intuitively, the node suspiciousness f V ( S ) is a sum of constants a corresponding to the users and objects in S , which can be thought of as how individually suspicious that particular user or object is. The edge suspiciousness f E ( S ) is a sum of constants c sponding to the edges in between S , which can be thought of as how suspicious that particular edge is (e.g. the suspiciousness of the text of a review by user i for object j ).

There are many advantages to metrics of this form. Firstly, met-rics of this form can be optimized in a way that is (a) scalable; (b) offers theoretical guarantees, and (c) is robust to camouflage, as we demonstrate in the rest of this paper. All 3 of these properties hold due to the particular chosen form in (1) and (2).

Secondly, metrics of this form obey a number of basic proper-ties (or  X  X xioms X ) that we would intuitively expect a reasonable suspiciousness metric should meet, as we next show. These basic properties are adapted from the  X  X xioms for suspiciousness met-rics, X  proposed in [10], to our setting where node and edge weights exist.

A XIOM 1 (N ODE S USPICIOUSNESS ). A subset consisting of higher suspiciousness nodes is more suspicious than one consist-ing of lower suspiciousness nodes, if the other conditions are fixed. Formally, |S| = |S 0 |  X  f E ( S ) = f E ( S 0 )  X  f V ( S ) &gt; f
A XIOM 2 (E DGE S USPICIOUSNESS ). Adding edges within a subset increases the suspiciousness of the subset if the other condi-tions are fixed. Formally, where S ( V , E ) is the subgraph induced by S in the graph ( V , E ) .
The edge density  X  ( S ) of an induced subgraph is its number of edges divided by its maximum possible number of edges.

A XIOM 3 (S IZE ). Assuming node and edge weights are all equal, larger subsets are more suspicious than smaller subsets with the same edge density. Formally, given a i = a  X  i , and c b  X  ( i,j )  X  X  : |S| &gt; |S 0 |  X  S  X  X  0  X   X  ( S ) =  X  ( S 0 )  X  g ( S ) &gt; g ( S
A XIOM 4 (C ONCENTRATION ). A subset with smaller size is more suspicious than one with the same total suspiciousness but larger size. Formally,
Density metrics g of the form defined in Equation (1) satisfy these properties:
T HEOREM 1. The density metric defined in (1) satisfies axioms 1 to 4.
 P ROOF . See appendix A.

Note some simple metrics that violate these axioms: the edge density  X  ( S ) itself, as a metric, does not increase with the size of S and hence violates axiom 3. On the opposite end, the total edge as it does not consider how concentrated the edge weight is. In contrast, g scales in a reasonable manner as its size or concentration changes.

A simple example of a metric g as defined in (1) and (2) is the bipartite graph average degree:
E XAMPLE 1. (Bipartite Graph Average Degree) Let a i = 0 , and let c ij = 1 if ( i,j )  X  E and 0 otherwise. In the expression (2) for f ( S ) , we add one term c ij for each edge ( i,j ) for which i,j are both in the subset S . Thus, f ( S ) is equal to the number of edges in the subgraph spanned by S , or half the total degree in the subgraph spanned by S . As a result, g ( S ) = f ( S ) average degree of the subgraph spanned by S .
Let f and g be as given in (1) and (2). In this section, we give an algorithm for optimizing the density metric g in near-linear time.
Algorithm 0 describes our proposed FRAUDAR algorithm, a greedy approach inspired by that of [4] but which covers our broader objective class. We start with the entire set of nodes U  X  X  , then repeatedly remove the node which results in the highest value of g evaluated on the remaining set of nodes. Formally, denote by X the current set we are optimizing over; initially we set X = U  X  X  . Let  X  i = f ( X\{ i } )  X  f ( X ) be the change in f when we remove i from the current set. At each step, we will select i to maximize  X  i.e. to leave behind the set with highest value of f . We then remove i from X . We then repeat this process: we recompute the values of  X  j , then choose the next node to delete, and so on. This leads to a shrinking series of sets X over time, denoted X 0 ,..., X of sizes m + n,..., 0 . At the end, we return the one of these that maximizes the density metric g .

The key fact that allows the algorithm to be efficient is the forms for f and g in (1) and (2). When i is removed, the only values of  X  j which need to be updated are those where j is a neighbor of i . This is because for all other j , the expressions (1) and (2) ensure that  X  j does not change. Hence, the updates are fast: for each ( i,j )  X  E , over the lifetime of the algorithm we will perform at most one such update over this edge, for a total of O ( |E| ) updates. Using appropriate data structures, as we next describe, each update can be performed in O (log |V| ) time, totalling O ( |E| log |V| ) time.
Each element i  X  X has a priority that will change as the al-gorithm progresses: the priority of element i at the t th iteration is  X  i = f ( X t \{ i } )  X  f ( X t ) . This ensures that in Line 5, the el-ement i  X  = arg max i  X  X  t g ( X t \{ i } ) we wish to find is exactly the element of highest priority, allowing us to retrieve it quickly (in O (log |V| ) time, as we explain below). Note that it does not matter if we use f or g in the arg max since the denominator of g in (1), |X t \{ i }| , is the same for all possible deletions i .

These priorities are stored in the priority tree T constructed in line 2 of Algorithm 0. This data structure is a binary tree with all |V| elements as leaves, all at the bottom level of the tree. Each in-ternal node keeps track of the maximum priority of its two children.
The priority tree supports fast retrieval of the maximum priority element (used in Line 5 of Algorithm 0); it does this by starting at the root and repeatedly moving to the child with higher priority. It also supports quickly updating priorities: since all the leaves can be stored in fixed locations, we can easily retrieve the leaf at any index to update its priority. Then, after updating that node X  X  priority, we travel up the tree to update each parent up to the root (used in Line 6). Each of these operations on T takes O (log |V| ) time.
The bottleneck is the loop in Lines 5 to 7 which runs m + n times. Lines 5 and 6 take O (log |V| ) as discussed, while Line 7 is constant time. Finally, we need |E| updates to node priorities, one for each edge. Thus the algorithm takes O ( |E| log |V| ) time.
So far, we have shown that g can be optimized in near-linear time. In this section, we will show that when f and g are of the form (1) and (2), FRAUDAR is guaranteed to return a solution of at least half of the optimum value.

T HEOREM 2. Let A , B be the set of users and objects returned by FRAUDAR . Then: where g OPT is the maximum value of g , i.e.
P ROOF . Let the elements of V be labeled v 1 ,v 2 ,...,v define  X  X eight assigned to node v i in S  X  as w i ( S ) = a i + X where a i (  X  0) indicates the weight of node v i and c ij dicates that of edge ( v i ,v j ) as in (2). Note that when node v removed from the current set S at some point in the algorithm, w ( S ) is the decrease in the value of f , since it is the sum of all terms excluded in (2) when node v i is removed.

Now consider the optimal set S  X  . For each node v i  X  S claim that w i ( S  X  )  X  g ( S  X  ) . Otherwise, removing a node with w ( S  X  ) &lt; g ( S  X  ) results in which is a contradiction.

Let v i be the node that FRAUDAR removes first among those in S  X  , and let S 0 be the set before FRAUDAR removes v i since S 0  X  S  X  , w i ( S 0 )  X  w i ( S  X  ) . Moreover, since FRAUDAR chooses to remove node v i , for each of the other remaining nodes v j  X  S 0 , w j ( S 0 )  X  w i ( S 0 ) . Since each term in f ( S assigned to at most two nodes, summing over j gives f ( S returns the best solution that it encounters. We conclude that g ( A X  X  )  X  g ( S 0 ) = f ( S
So far, we have seen that metrics of the form: g ( S ) = efficiently and with approximation guarantees. In this section, we show how we can select metrics within this class that are resis-tant to camouflage, i.e. they do not allow fraudulent users to make themselves less suspicious by adding camouflage edges , i.e. edges toward honest objects.

Recall that a i and c ij are the weights of node i and edge ij , while f ( S ) is the total node and edge weight in S . A key idea of described in Section 4.2.
 Require: Bipartite G = ( U  X  X  , E ) ; density metric g of the form in (1) 1: procedure FRAUDAR ( G,g ) 2: Construct priority tree T from U  X  X  . see Section 4.2 3: X 0  X  X   X  X  . suspicious set is initially the entire set of nodes U  X  X  4: for t = 1 ,..., ( m + n ) do 5: i  X   X  arg max i  X  X  i g ( X i \{ i } ) . exonerate least suspicious node 6: Update priorities in T for all neighbors of i  X  8: end for 10: end procedure our approach is that instead of treating every edge equally, we as-sign a lower weight c ij when the target object j has high degree. This is because objects of very high degree are not necessarily sus-picious (since highly popular objects commonly exist). Thus, this weighting allows us to put greater emphasis on objects within un-expectedly dense subgraphs, rather than just high degree objects.
If we consider the adjacency matrix with rows representing users and columns representing objects, we would like to downweight columns with high column sum (column-weighting). A simple re-sult we show in this section is that column-weightings are cam-ouflage resistant. Recall that a density metric g is camouflage-resistant if g ( A X  X  ) does not decrease when any amount of cam-ouflage is added by an adversary with fraudulent users A and cus-tomers B . Let d i be the the i th column sum, i.e. the degree of object i .

Formally, define a column-weighting as a choice of weighting in which each c ij is a function of the respective column sum, i.e. c ij = h ( d j ) for some function h .

T HEOREM 3. Let c ij be a column-weighting. Then g (as de-fined in (1) and (2) ) is camouflage resistant.

P ROOF . Adding camouflage only adds edges in the region be-tween A (fraudulent users) and B C (honest objects). It does not add or remove edges within the fraudulent block; moreover, the weights of these edges do not change either as their weights only depend on the column degrees of B , which do not change when camouflage is added. Thus the value of g does not change.
A natural follow-up question is whether camouflage-resistance also holds for row-weightings (i.e. selecting c ij to be a function of the corresponding row sum). It turns out that row-weightings are in general not camouflage resistant. This is because a fraudulent user account can add a large number of camouflage edges, thereby increasing their row sum, decreasing the weight of each of their edges. Thus g ( A X  X  ) decreases, meaning that g is not camouflage resistant.

Hence we may choose any column-weighting while ensuring camouflage resistance. The remaining question is what function to choose for the column-weighting, i.e. the function h where c ij = h ( d j ) . It should be decreasing (so as to downweight columns with high sum). It should shrink more slowly than h ( x ) = 1 /x , since h ( x ) = 1 /x allows a single edge to contribute as much as the total contribution of a column with any number of edges, causing us to catch columns with single ones rather than dense blocks.
Within the remaining space of choices, we note that a very simi-lar problem of downweighting based on column frequency appears in deciding the form of the  X  X nverse document frequency X  term of the popular heuristic tf-idf weighting scheme [24], in which loga-rithmic weighting of frequency has been empirically found to per-form well. We also show empirical results (in Section 5.1) that logarithmic weighting leads to strong theoretical bounds. For these reasons, we recommend using h ( x ) = 1 / log( x + c ) , where c is a small constant (set to 5 in our experiments) to prevent the de-nominator from becoming zero, or excessive variability for small values of x . We use the resulting density metric (denoted g our experiments.
Figure 1(a) shows curves representing our theoretical bounds on the maximum amount of fraud that can be present for each possible size of the fraudulent block, based on Theorem 2. We now explain how such bounds can be computed from Theorem 2. Assume that the fraudulent block contains m 0 user accounts and n 0 customers.
In this section, we assume that no side information is present, so we set the a i , the prior suspiciousness of each node, to 0 . Thus the j th object. Consider a fraudulent subgraph with m 0 user nodes and n 0 object nodes. Assume that each fraudulent customer has at least a certain fraction 0 &lt;  X  &lt; 1 of fraudulent edges: each fraud-ulent customer should be receiving at least a comparable fraction of fraudulent reviews to its actual honest reviews, otherwise it would not be profiting appreciably from the fraud.
 T HEOREM 4. Let (  X  A ,  X  B ) be the block detected by FRAUDAR . Then the number of edges that a fraudulent block of size ( m can have without being detected is at most 2( m 0 + n 0 ) g  X  B ) log( m 0 / X  + c ) . In other words, our algorithm will detect a fraudulent block without fail if it contains more edges than this threshold.

P ROOF . By Theorem 2, 2 g log (  X  A X   X  B ) is an upper bound on the value of g log on any subgraph of users and objects. Since the fraud-ulent block has m 0 + n 0 nodes in total, thus 2( m 0 + n is an upper bound on the value of total suspiciousness f log
Moreover, each fraudulent customer has at most m 0 fraudulent edges joined to it, and since at least  X  fraction of its edges must be fraudulent, it can have at most m 0 / X  degree in total. Hence the weight of each fraudulent edge is at least 1 log( m since the total weighted degree is at most 2( m 0 + n 0 ) g it follows that the number of fraudulent edges is at most 2( m n ) g log (  X  A X   X  B ) log( m 0 / X  + c ) .
 We apply this bound to real data in Section 5.1.
Amazon [18] 28K
Advisor [29]
Epinion [17] 264K
Wiki-vote [17] 16K We design experiments to answer the following questions: Q1. Illustration of our theorem: How strong are the bounds that FRAUDAR provides in terms of bounding undetectable fraud in the graph? Does column weighting improve those bounds? Q2. Evaluation on synthetic data: How accurately does FRAU-DAR detect injected fraud under different types of camouflage at-tacks? Does FRAUDAR outperform state-of-the-art competitors? Q3. Effectiveness in real-world data: Does FRAUDAR detect true fraud in real-world graphs? Have the fraudulent accounts al-ready been detected by previous methods? Q4. Scalability: Is FRAUDAR scalable with regard to the data size?
We implemented FRAUDAR in Python; all experiments were carried out on a 2.4 GHz Intel Core i5 Macbook Pro, 16 GB RAM, running OS X 10.9.5. The code is available for download at www. andrew.cmu.edu/user/bhooi/camo.zip . We test FRAUDAR on a variety of real world datasets. Table 3 offers details on the datasets we used.

To test the accuracy of our method, we use synthetic attacks injected into our Amazon dataset. We structure our  X  X ttacks X  as shown in Figure 2. We injected a fraudulent block of users and customers with varying densities. We assume  X  = 0 . 5 for our the-oretical bounds.
In Figure 1 (a), we showed our theoretical bounds (Theorem 4) applied to compute the maximum number of edges an adver-sary with m 0 = 50 user nodes can have for various values of n These are computed by running FRAUDAR under two weighting schemes. First, we use our g log scheme exactly as in Theorem 4 to get an upper bound 2( m 0 + n 0 ) g log (  X  A X   X  B ) log( m the number of fraudulent edges; plotting this against n 0 green region ( X  X mproved X ) in Figure 1 (a). The blue region ( X  X rig-inal X ) comes from using the analogous procedure without the log-weighting, i.e. where g ( S ) is half the average degree, as in Exam-ple 1.

In this case, we see that the log-weighted scheme provides stronger bounds, since the bound is lower, i.e. an adversary should have fewer edges in order not to be detected. Intuitively, this happens because down-weighting high degree columns decreases the weight of many of the honest high degree objects in the dataset, so groups of adversaries stand out more, resulting in stronger bounds on how many edges an adversary can have.

Next, we apply our FRAUDAR in the same way over various real-world graphs to analyze the theoretical upper bounds com-puted by FRAUDAR on the density that fraudulent blocks can Figure 3: FRAUDAR X  X  bounds on fraud are stringent, on real graphs: E.g., on TripAdvisor, the bound says that a fraudulent block containing 50 user accounts and anywhere between 100 and 1000 products must have density of &lt; 2% to avoid detection. have. We run FRAUDAR on four real-world graphs: Amazon [18], Trip Advisor [29], Epinions [17], and Wiki-vote [17]. The detailed description of each graph is in Table 3. For all datasets, Figure 3 shows the maximum number of fraudulent edges that an adver-sary can have without being detected, assuming 50 fraudulent users and varying the number of fraudulent customers. We see that we can detect fraud most easily in Trip Advisor , followed by Epinion, Wiki-vote, Amazon ; even a fairly sparse block of density around 0 . 05 would stand out strongly in the Trip Advisor graph. While density is important in determining how easy it is to detect fraud in each graph (fraudulent blocks stand out more strongly in a sparse graph), it is not the only factor. Indeed, Wiki-vote is actually denser than Amazon . In fact, the difficulty of detecting fraud in each graph is mainly determined by its densest blocks, since an adversarial block that is significantly less dense than the densest normal blocks in the graph is unlikely to be detected.
In Figure 1 (b), we demonstrated that FRAUDAR can effec-tively detect fraud under four types of camouflage attacks: 1) In-jection of fraud with no camouflage, 2) random camouflage, 3) bi-ased camouflage and 4) hijacked accounts, more accurately than competitors.

We conduct experiments based on the settings at the beginning of this section, averaged over 5 trials. For the camouflage scenar-ios 2) and 3), the amount of camouflage added per fraudulent user account was (on average) equal to the amount of actual fraudulent edges for that user. For the  X  X andom Camo X  case, for each fake user node, camouflage edges were chosen at random, with on av-erage the same number of camouflage edges as fraudulent edges, as shown in Figure 2 (a). For the  X  X iased Camo X  case, for each fake user node, camouflage edges were directed toward each object with probability proportional to the degree of the object as shown in Figure 2 (b). For the  X  X ijacked X  case, we used a random subset of existing users to form the fraudulent block.

In each case, we injected 200 fraudulent users and 200 fraudu-lent products with various edge densities to the subsetted Amazon review graph of 2000 users and 2000 products, with a density of 0 . 0006 . We compare FRAUDAR to S POK E N in their F measure experiments, we assume that no honest user added an edge to the fraudulent target (i.e. object) nodes.

As seen in Figure 1(b), the results demonstrate that FRAUDAR works robustly and efficiently against all four attacks, achieving F-attacks. measures of over 0 . 95 on all four scenarios for densities of at least 0 . 04 . On the other hand, S POK E N was able to reach its maximum performance of 0 . 9 only when fraud blocks had densities of higher than 0 . 06 and under the  X  X o camouflage X  scenario.

The experimental results in Figure 1(b) were based on the as-sumption that no honest user added an edge to the fraudulent target nodes. However, in a real-world environment, some honest users may add edges to the fraudulent target nodes (which we refer to as  X  X everse camouflage X ). To incorporate this, we conducted an-other experiment using an attack model where we add edges be-tween honest users and the fraudulent target nodes, but with sparser density compared to the fraud blocks. We added random edges to this region, with half the density of the fraud blocks. All other ex-perimental settings were unchanged. The experimental results are shown in Figure 4 (a). For FRAUDAR, the results are generally similar. In contrast, S POK E N shows slightly worse performance under this additional camouflage.

To show that FRAUDAR is effective both at catching fraudu-lent users accounts as well as fraudulent objects, we next separately evaluate the fraud detection of both fake users and fake targets us-ing F measure. The basic experimental setup is same as before, with the density of the fraudulent blocks now fixed to 0 . 03 . In Figure 4 (b), the bar plots are shown for the comparison.  X  X ser-wise X  (red) denotes the F measure of the detecting fake users, and  X  X arget-wise X  denotes the F measure of detecting fake target nodes. We see that in general, accuracy is high and fairly similar, but the performance in detecting fake users is slightly higher than that of detecting products.
In this section, we verify that FRAUDAR accurately detects a large block of fraudulent accounts in the Twitter follower-followee graph, as verified by hand labelling and by clear signs of fraud ex-hibited by a majority of the detected users. Indeed, a majority of the detected accounts had tweets advertising follower-buying services, and the tweets had not been removed or the accounts suspended for the 7 years since the data was collected. Figure 1(d) shows a sample fraudster caught by FRAUDAR.

The Twitter graph we use contains 41 . 7 million users and 1 . 47 billion follows; it was extracted in July 2009 and first used in [16]. On this graph, FRAUDAR detected a dense subgraph of size 4031 followers by 4313 followees. This subgraph is extremely dense, with 68% density, which is highly suspicious in itself.

To further investigate this block, we randomly sampled 125 fol-lowers and 125 followees in the block detected by FRAUDAR for hand labeling to determine how many of them appear fraudulent. To do this, we labeled which users were fraudulent based on the following characteristics of their profile data, chosen based on es-tablished criteria in the literature [25] summarized below.
For comparison, we also construct two control groups of size 100 containing users that were not detected by the algorithm. The first control group contains randomly selected non-detected users. For the second (degree-matched) control group, we constructed it to match the follower count of users in the detected group; we do this by repeatedly selecting a random detected user, then finding another non-detected user who has at most 10% bigger or smaller follower count. During the labelling process, we shuffled the de-tected users with the control groups randomly and hid group mem-berships from labellers, labeling users in a  X  X lind X  manner.
Additionally, we also check and report how many of these users have Tweets containing the URLs of two known follower-buying services, TweepMe and TweeterGetter , showing that they had ad-vertised these follower-buying services through tweets.

Note that this entire labelling process used only profile and tweet data and not follower-followee data, whereas our algorithm uses only follower-followee data, so the labelling is a fair estimate of the algorithm X  X  accuracy. We present two pieces of evidence which strongly indicates fraud in the detected group. Firstly, the percent-age of users with tweets advertising TweepMe or Tweetergetter is much higher among the detected users than among both control groups (Figure 5): 41% of the detected followers, and 26% for the detected followees. These rise to 62% and 42% respectively as shown in Figure 1(c) if we ignore deleted, protected and suspended accounts (for which profile information was unavailable). In the control groups, there were no mentions of TweepMe and very few mentions of TweeterGetter , as shown in Figure 5. Figure 5 shows the breakdown of our groups in terms of deleted and suspended users. Given the sparsity of TweepMe and TweeterGetter in the control groups, we see that the detected users are likely charac-Figure 5: FRAUDAR detects a large, clearly fraudulent block in Twitter. A majority of the detected accounts were either deleted, suspended, or contained known follower-buying services, TweepMe and TweeterGetter . In comparison, the control groups had much less detected fraud.
 Figure 6: Follower-buying services: a large fraction of detected accounts use TweepMe (bottom) or TweeterGetter (middle, top). terized by a large block of users using these and possibly other follower-buying services, resulting in a dense block.

Secondly, we used our hand-labelling using the above criteria to determine how many of each group appear fraudulent. 57% of the detected followers and 40% of the followees were labelled as fraud-ulent, deleted or suspended accounts, but much fewer in the control groups, with 25% for the degree-matched control group, and 12% for control group with no condition. Thus both these results sup-port the effectiveness of FRAUDAR in detecting fraudulent users in the real-world graphs.
Figure 7 shows the near-linear scaling of FRAUDAR X  X  running time in the number of edges. Here we used the Trip Advisor dataset, and subsampled user nodes in proportions of 0 . 7 0 ,..., 0 . 7 parallel to the main diagonal indicate linear growth.
In this paper, we propose FRAUDAR, a fraud detection algo-rithm which provably bounds the amount of fraud adversaries can have, even in face of camouflage. Our main contributions are as follows. Figure 7: FRAUDAR runs in near-linear time : the curve (blue) shows the running time of FRAUDAR, compared to a linear func-tion (black) . This material is based upon work supported by the National Science Foundation under Grant No. CNS-1314632, DGE-1252522, and IIS-1408924.

Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not neces-sarily reflect the views of the National Science Foundation, or other funding parties. [1] L. Akoglu, R. Chandy, and C. Faloutsos. Opinion fraud [2] A. Beutel, W. Xu, V. Guruswami, C. Palow, and [3] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro. Aiding the [4] M. Charikar. Greedy approximation algorithms for finding [5] C. Cortes, D. Pregibon, and C. Volinsky. Communities of [6] S. Ghosh, B. Viswanath, F. Kooti, N. K. Sharma, G. Korlam, [7] C. Giatsidis, D. M. Thilikos, and M. Vazirgiannis. Evaluating [8] Z. Gu, K. Pei, Q. Wang, L. Si, X. Zhang, and D. Xu. Leaps: [9] Z. Gy X ngyi, H. Garcia-Molina, and J. Pedersen. Combating [10] M. Jiang, A. Beutel, P. Cui, B. Hooi, S. Yang, and [11] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. [12] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and S. Yang. [13] N. Jindal and B. Liu. Opinion spam and analysis. In ICDM [14] G. Karypis and V. Kumar. METIS: Unstructured graph [15] J. Kleinberg. Authoritative sources in a hyperlinked [16] H. Kwak, C. Lee, H. Park, and S. Moon. What is twitter, a [17] J. Leskovec, D. Huttenlocher, and J. Kleinberg. Signed [18] J. McAuley and J. Leskovec. Hidden factors and hidden [19] A. Molavi Kakhki, C. Kliman-Silver, and A. Mislove. Iolaus: [20] M. Ott, Y. Choi, C. Cardie, and J. T. Hancock. Finding [21] S. Pandit, D. H. Chau, S. Wang, and C. Faloutsos. Netprobe: [22] B. Perozzi, L. Akoglu, P. Iglesias S X nchez, and E. M X ller. [23] B. Prakash, M. Seshadri, A. Sridharan, S. Machiraju, and [24] A. Rajaraman, J. D. Ullman, J. D. Ullman, and J. D. Ullman. [25] N. Shah, A. Beutel, B. Gallagher, and C. Faloutsos. Spotting [26] D. N. Tran, B. Min, J. Li, and L. Subramanian.
 [27] C. Tsourakakis. The k-clique densest subgraph problem. In [28] S. Virdhagriswaran and G. Dakin. Camouflaged fraud [29] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating analysis [30] B. Wu, V. Goel, and B. D. Davison. Propagating trust and [31] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao. Sybillimit: [32] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman.
P ROOF . Axiom 1 (Node Suspiciousness)
Axiom 2 (Edge Suspiciousness) Let e = ( u,v ) .
Axiom 3 (Size) Let S = A X  X  , and  X  be the edge density. which is increasing in both |A| and |B| .

Axiom 4 (Concentration)
