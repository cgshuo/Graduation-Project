 This paper addresses the scenario of multi-release anonymiza-tion of datasets. We consider dynamic datasets where data can be inserted and deleted, and view this scenario as a case where each release is a small subset of the dataset corre-sponding, for example, to the results of a query. Compared to multiple releases of the full database, this has the obvi-ous advantage of faster anonymization. We present an algo-rithm for post-processing anonymized queries that prevents anonymity attacks using multiple released queries. This al-gorithm can be used with several distinct protection princi-ples and anonymization algorithms, which makes it generic and flexible. We give an experimental evaluation of the al-gorithm and compare it to m -invariance both in terms of efficiency and data quality. To this end, we propose two data quality metrics based on Shannon X  X  entropy, and show that they can be seen as a refinement of existing metrics. H.2.8 [ Database Management ]: Database Applications-Data Mining; K.4.1 [ Computers and Society ]: Public Policy Issues-Privacy Anonymization, multiple-releases, dynamic datasets
Information is one of the biggest assets of every organiza-tion since it lies at the root of every business function. Often, this information is about real world people, who would not like to see it revealed. Nevertheless, limited usage of the data might be tolerated if well justified, e.g. for health re-search purposes or simply for the sake of a better service. This introduces a tension between keeping the privacy of the individuals represented in a dataset, and the usability of  X  such data to the business and to the quality of the service the organization provides.

Sweeney [15] showed that the removal of identifying fields such as name and credit card number may leave in place quasi-identifier attributes that uniquely identify people in a wide range of contexts. Thus, a whole line of research developed around the idea of anonymizing data by grouping data in classes of mutually indistinguishable records. The aim is to preseve real useful information about people, but discard the possibility that some adversary might associate any given individual to its sensitive data.

The initial results in literature focused on anonymizing large datasets released once [15, 11, 10], but subsequent con-tributions [3, 14, 19, 6, 4, 16] moved to the case of multiple releases with full dynamic datasets. In this paper we also go in this direction, and consider a scenario where some client application or user queries a large database at differ-ent points in time, where each query returns a small fraction of the data set. The goal is to ensure that anonymity is pre-served across all the queries, whilst avoiding an anonymiza-tion of the whole database each time, thereby resulting in faster processing for each query. We present an algorithm that preserves anonymity over multiple queries and give an experimental analysis that compares it to m  X  invariance in both execution time and data quality. We base our com-parison on two quality metrics based on Shannon Entropy, which we introduce and justify by showing that they can be seen as a refinement of existing metrics.
The attributes in a table containing personal data can be divided in three groups: identifiers, quasi-identifiers and sensitive attributes. In an anonymization scenario, we as-sume the identifier attributes have previously been removed. Anonymization procedures usually follow a common frame-work: the algorithm receives a table of original data without identifying attributes and outputs a new anonymized table where each record is a processed version of a single original record. Records are grouped in equivalence classes where it is impossible to associate each record or sensitive value to a particular person, either because sensitive values are disso-ciated from the respective record or the quasi-identifiers of all records in the class are generalized to a common value.
We denote a table to be anonymized by T , which con-tains tuples t of identities denoted by I . The schema of T , T ( Q , S ), is composed of a sequence Q of quasi-identifier atributes and a single sensitive attribute S 1 . We denote by
S c an of course be a set of sensitive attributes, and our results can be adapted to that scenario. |
X | th e cardinality of a set X , and represent the projection of a tuple t onto a set of fields F by t [ F ]. For a generalized attribute a , t [ a ] may represent a set rather than a single value. For tuples t, t  X   X  T ( Q , S ), we say t  X  generalizes t if for all attributes a  X  Q , t [ a ]  X  t  X  [ a ].

Given a relation R ( Q , S ), an equivalence class E is the set of all tuples t  X  R that agree on the attributes in Q : for any two distinct tuples t 1 , t 2 in E , t 1 [ Q ] = t a set of tuples R  X  T and a set of quasi-identifiers Q , let R  X  be the anonymized version of R , i.e., a set of disjoint equivalence classes: R  X  = t , let Id ( E ) be the set of identities represented in E the tuple that represents an identity I . For a given set of tuples X , a X = { t [ a ] : t  X  X } is the multiset of all distinct values that a assumes in X , and X a = v denotes the tuples in X for which a takes the value v .

If we view quasi-identifiers as distinct axes in space, each equivalence class E defines a particular region in that space. Anonimity models . The first approach to protecting records in a large database was k -anonymity [15]. This model pro-tects information against a link-disclosure attack, by anony-mizing a table into equivalence classes with at least k el-ements. The first principle to protect against attribute-disclosure was  X  -diversity, proposed in [11], which requires that each equivalence class have at least  X  well-represented values. This is usually accepted to mean that each sensitive value appears in at most a fraction 1 / X  of the records in the class. Other attacks were discovered later, leading to new principles, e.g., t -closeness [10] and variants of k -anonymity and  X  -diversity, still not applicable to multiple releases. This scenario began to be addressed systematically in [3]. The authors identify two basic ways of dealing with the prob-lem: anonymize and publish only the new records, or re-anonymize and publish the full dataset. The first method has the drawback of degrading the information to a higher degree if the new records are sparsely distributed, leading to wide generalizations. The second method gives better data quality, but is susceptible to several inference attacks. The authors identify some of these, but only address incremental changes to the database, where records may be inserted but not altered nor deleted between releases.

The work in [19] was the first to consider re-publication of anonymized datasets modified by insertions and deletions , proposing the principle of m  X  invariance. This enforces two kinds of protection: each equivalence class must contain ex-actly m records all having distinct values in the sensitive attribute; and, if a tuple appears in different releases of published data, it is always associated to the same set of possible sensitive values.

In [14], the authors address the case of successive pub-lications of dynamic data, but like [3] consider only inser-tions. The idea of tracking a record among successive re-leases by using an attribute that uniquely identifies each original record is introduced: this record identifier is con-served unchanged in the anonymized tuples. The work in [6] reasons about possible attacks against releases of different datasets over the same population, but focuses more on esti-mating the fraction of the population subject to intersection attacks, and how this evolves with a number of parameters. Later, [4] also focused on incremental databases, introduc-ing a new form of attack called correspondence attack and a new protection principle, called BCF-anonymity. The work in [16] also deals with the scenario of incremental updates. Its approach is to subvert inference channels by mixing data of different releases in the same anonymization. Unfortu-nately this is not well suited for scenarios where rigorous analysis of the data at a given point in time is required.
Other papers focus on a different kind of successive pub-lication called streaming publication. This scenario is differ-ent from ours, since anonymization must consider time con-straints on the records and these are generated continuously. This means it is not possible to treat the data as limited at any given point in time, thereby disallowing anonymization. Quality metrics . Anonymization is a compromise between privacy and usability. Several quality metrics have been pro-posed in the literature to evaluate the balance between these conflicting goals for different anonymization solutions. Such metrics are typically calculated after the anonymization pro-cess in order to obtain numeric values that can be used for comparative quantitative analyses. We review some of the most prominent in this section.

The precision metric was proposed in [15]. It computes the distortion of a table by averaging the distance of each generalized value to the root of its generalization hierarchy. It is thus not suitable for algorithms where such hierarchy doesn X  X  exist, like Mondrian. The discernibility metric was proposed in [1]. It is based on the idea that a tuple is less discernible if it is indistinguishable from other tuples and so penalyzes large equivalence classes. Ignoring supressed tuples, the metric computes the sum C DM = equivalence classes. The least the value of C DM , the high-est the data quality is. The notion of normalized average equivalence class size metric ( C AVG ) was proposed in [9] as a normalization to the discernibility metric, and is defined as C alence classes in the anonymized table.

The notion of information loss was proposed in [3], moti-vated by the observation that neither C DM nor C AVG address the modification of data values imposed by the generaliza-tion process. This metric works well for attributes that ad-mit a notion of distance, and measures the degradation of the information in a dataset. For an attribute a and an equivalence class E , it computes the interval covered by the records of that class in that attribute d ( a E ) and the corre-sponding interval for the whole dataset d ( a ). The informa-tion loss for a class can be written IL ( E ) = |E| X  Th e average information loss ( AIL ) is the average over all equivalence classes and serves as the metric for the whole dataset. This has been generalized as certainty in [20].
The use of the Kullback-Leibler divergence as an utility measure was proposed in [8]. The authors propose to com-pute a probability P 1 over the original data and a marginal distribution P 2 over the generalized data, and define utility as the Kullback-Leibler divergence of P 1 and P 2 . The score metric was proposed in [5] and refined in [18] by the same authors. This metric balances gain of information and loss of privacy. Score is computed as score ( v ) = InfoGain ( v ) In formation gain is defined as the difference in information before and after the specialization of v . Quantity of infor-mation can be measured, e.g., using Shannon X  X  entropy. Attacker model . The attacker we consider has full access to all the anonymized releases of data, and has unlimited stor-age and computation abilities. The attacker also has access to a public list with the identification of all the individuals in the original database throughout time, omitting of course the sensitive value. We also give the attacker full knowl-edge of the quasi-identifiers of any target she chooses, which permits finding the corresponding equivalence class. Specif-ically for the multi-release scenario, we allow the attacker to know which individuals are present in each release and to track individuals across releases. This is done by giving a unique identifier to each original record that is independent of real world data, as was done in [14, 17]. This allows the attacker to correlate records of the same individual in differ-ent releases, across different generalizations and even with a changing sensitive value via an update operation. Never-theless, we do not consider the case in which the adversary has external knowledge that the sensitive value of a target identity has changed from one release to the other. This means that updates can be modeled as insertions and dele-tions, possibly modifying sensitive values. For this reason, we do not further refer specifically to update operations.
The most common attacks in literature can be grouped in the following kinds of privacy breach: link-disclosure, attribute-disclosure and record tracing. Link disclosure is the simplest kind of attack and its goal is to find which anonymized record corresponds to a given target. The at-tacker can always find the equivalence class that contains the target, and her uncertainty will stem from the indis-tinguishability of the records in that class. The objective of attribute disclosure is to find the sensitive value, which may be done without identifying a unique record. In an attribute disclosure, the adversary wins by identifying the sensitive value of the target. In record tracing [13, 12, 2], the attacker doesn X  X  have a predefined target. Instead, she starts from the anonymized data with sensitive values and tries to match any record in a public list to some anonymized record. This attack only makes sense in a multiple-release scenario, where an attacker gradually refines her knowledge about the quasi-identifiers of anonymized records. For this it is crucial that the attacker is able to correlate records in distinct releases.
 Protection principles . The principles of k -anonymity and  X  -diversity are still at the basis of most of the anonymization processes. We review their definitions here.

Definition 1. A relation R ( Q , S ) satisfies:
The m  X  invariance criterion was the first to address dy-namic databases. We give a definition here, as we will use it as benchmark reference for our algorithm.

Definition 2 ( m  X  invariance). Consider several re-also that a given tuple of the original dataset may appear in several of these relations. For a given tuple t , let E i resent the equivalence class that contains t in R  X  i ( E if t  X  X  R  X  i ). These relations satisfy m  X  invariance if 1. for every equivalence class E in any of these relations, 2. for every tuple t and X = {E i ( t ) : 1  X  i  X  m, t  X  R Attacks with multiple releases . In the setting of static re-leases, attacks mentioned in literature typically concern the information the adversary gains about the sensitive value, when she has access to the anonymized table and an auxil-iary source. Homogeneity , similarity and proximity attacks can all be leveraged from the target X  X  equivalence class alone, and skewness attacks only need to add information about the distribution in the static table. These attacks can be addressed with different techniques proposed in the litera-ture, including  X  -diversity, t -closeness and their variants.
The introduction of dynamic updates to the data allows the appearance of new kinds of attack simply because there are more data available that can be combined in different ways. In the end, the objective of these attacks is to achieve the same kinds of disclosure (homogeneity, similarity, etc.), but there are more ways in which to combine the informa-tion, which can be described as a new family of attacks. In what follows, we consider attacks over distinct releases of the same relation corresponding to anonymizations of the data at different points in time. In general, there may have been changes by insertion, deletion or updating of records between these two releases. However, as mentioned earlier, we consider only explicitly insertion and deletion operations.
To obtain some intuition about possible attack scenarios, let us consider multiple releases of health data as shown in Table 1. This shows four patients who were admitted to the same hospital, and their quasi-identifiers ( Age and Gender ). We list Name only to track the evolution of the values, this is not published in the anonymized releases. Observe that, for a given equivalence class, some (but not all) identities are present in both releases and new ones have been added. If the attacker cannot know the identities that are in each equivalence class, then there X  X  no attack: it might just be that the individuals in each release are totally unrelated. This is why it is natural to assume that the attacker can learn who is in what release and in what equivalence class. In this case, the attacker learns two possible values for either Charlotte or Bob (Pneumonia or Migraine), increasing the possibility of disclosing the right value from 1 / 4 to 1 / 2. She can also learn easily that Fran and George, among them, have Flu and Cancer.

However, the attacker is not so lucky in comparing Re-lease 2 and Release 3 (without knowing Release 1). One of Alice and Dave must have Flu, and at least one of Ellis and Helen must suffer from HIV, but the exact possibility is im-possible to determine. The attacker can identify the cases in Table 2. In order to consider stronger adversaries that don X  X  have these doubts, we use a unique identifier for each individual, that stays constant across distinct releases. This allows the attacker to track the progress of an individual, without giving any clue to its identity. With this knowledge T able 1: Simultaneous deletions and insertions Release 1 Rel ease 2 Rel ease 3 F ran &amp; George A lice &amp; Dave El lis &amp; Helen a nd the previous releases, it would be easy for the attacker to conclude that Ellis and Hellen both had HIV. It is this sort of attack we wish to prevent.

In the sequel, we show how the attacker can infer new equivalence classes from the releases she sees. For each tar-get, she computes a candidate set of records that is guaran-teed to contain it. If such a set of records fails to satisfy the selected protection principle, then the attack is successful. We illustrate with k -anonymity and  X  -diversity. Difference attack [3] . A difference attack occurs when a target individual I is known to be in exactly one of two releases and the data can only be altered through insertions. Assume that t  X  = t ( I )  X  R  X  2 . Let E R  X  class in R  X  1 that would contain t  X  if it belonged to R the attacker can track records across releases, she can simply find the candidate set C = E R  X  principle does not hold for C , i.e. if | C | X  k or | S C k -anonymity and/or  X  -diversity are broken.

If the attacker did not know the unique identifier, she would have to consider all classes overlapping E R  X  note these by L = in this attack, Id ( E 1 )  X  Id ( L ) and t  X   X  Id ( L ) \ C = ( L  X  X  R  X 
In the above example, suppose the target belongs in E 1 2 and would have belonged in E R  X  the UID column, it X  X  easy to deduce the target has tuber-culosis and previously had flu. Without that column, the attacker can only reason that the target may be any of the three records in E i 2 . Since all the records in E R  X  sented in the three classes of the second release, the attacker reasons that only three people may correspond to his tar-get and their sensitive values can be one of { Tuberculosis, Cancer, HIV } . Intersecting with E 1 2 , the attacker reduces her uncertainty to only { Tuberculosis , Cancer } . This at-tack does not work in the presence of deletions, because we can no longer guarantee that we did not exclude any of the new ly inserted tuples: if a new tuple were equal to one of the deleted ones, even if it belonged to a different identity, then it would be impossible to tell whether the tuple was new, and therefore whether it should be one of the tuples in the candidate set. Applying the formula above would have the effect of removing a valid new tuple from the list of pos-sibilities. The inclusion of the unique identifier solves this problem for the attacker.
 Intersection attack [3, 19, 6] . An intersection attack occurs when a target individual I is known to be in two different releases. Here, the candidate set is simply the in-tersection of both equivalence classes that contain t  X  : C = E deletions and insertions: because the target is in both re-leases, it is not affected by the disappearance or inclusion of new records, as those will necessarily be absent from the intersection of both classes.
The objective of a good metric is to quantify the useful-ness of data after anonymization in a statistical analysis of anonymized data. Metrics in the literature can usually be placed in one of two groups: those that relate loss of in-formation to the number of indistinguishable records and those that relate loss of information to the necessary gener-alization of record values. Both of them have merit and we believe both should be present when evaluating the worth of an anonymization.

We propose two metrics ( FEM and VEM ) that share the same basic framework but cater to each of the two groups above. Our starting point is to take Shannon X  X  Entropy as a good measure of the amount of information in a given source. In-tuitively, if a dataset has more entropy it must have more information and therefore be more useful. This is similar to [8] and [5]. Our proposal gives an upper bound to the amount of information in a data set, before and after gen-eralization. Our metrics also deal with counterfeit records, which to our best knowledge is not explicitly addressed by earlier metrics. Due to their similarity, we will often make statements that apply to both metrics simultaneously. In this case we will simply refer metric xEM .

Metric xEM computes quality as the measure of informa-tion in the data set, and satisfies these desirable proper-ties: the data set holds maximum information in the original state; quality is minimum (0) when all records are anonymized into the same class; larger data sets may have more informa-tion; any generalization or suppression always decreases the qu ality of information. Metric xEM is defined as the average of information for points in the dataset. The crux of the def-inition is to characterize these  X  X oints X  and their probability distribution. A point is a distinguished tuple in the dataset. Originally, each record is a tuple; after anonymization, each equivalence class becomes a tuple. We give two ways of defining the probability of a tuple: by counting the number of records in each class ( FEM ) or by measuring the volume that the class generalization defines in the quasi-identifier space ( VEM ). The two metrics are not directly comparable but complement each other.
 Frequency Entropy Metric . The Frequency Entropy Met-ric of a data set R , FEM ( R ), is the average information ( Inf of the equivalence classes in the data set, under the distri-bution P . In a non-anonymized dataset, we let each record be an equivalence class. For an equivalence class E , we let P (
E , R ) = |E| / | R | and Inf F ( E , R ) =  X  log P ( E , R ). Then, FEM is formally the entropy of distribution P : V olume Entropy Metric . FEM is intuitive but does not represent the fact that larger equivalence classes convey less information about its records than tighter ones. The Volume Entropy Metric ( VEM ) takes this into account. The difference to FEM is in the information function. Instead of the cardi-nality of a class, we measure the extent of its neighborhood, and penalize the larger ones: Inf V ( E , R ) =  X  log V ( where V ( X ) represents the volume of a minimum region in the quasi-quantifier space that contains all tuples in a set X .
The V function cannot be computed as a normal volume in algebra, because individual records would define single points in space and would have volume 0, whereas we want such records to have maximum information. To prevent this, we must discretize space: each equivalence class is defined over a lattice of discrete points in the quasi-identifier space. Each quasi-identifier may be discretized in a different way, but it is essential that a minimal unit can be identified. This unit should be an integer in order to ensure that the prop-erties of entropy function are preserved, and single points should have volume 1. We achieve this by defining These definitions are unsatisfactory when some records are suppressed from the database during the anonymization pro-cess in order to prevent attacks. This is because applying the formula to the anonymized dataset in this case might lead to an increase of the information value, which we find contrary to what a good metric should do: if there are less records, there should be less information. To address this, we handle tuple suppresion by considering that they remain in the dataset, but with an information value of 0. We do this by making xEM depend on the original version of the data ( | T | ) and making the information of other tuples inde-pendent of suppressions: Whe n there is no record suppression, so T = R , we simplify notation by omitting the T argument.

Unlike FEM , VEM is not formally an entropy measure, as the term inside the logarithm does not correspond to that on the outside. However, VEM conserves all the properties we X  X e interested in. We list these in the following theorem, but omit the proofs due to lack of space. They are similar to the corresponding proofs for the entropy function.
Theorem 1. For all releases R of a dataset T , we have that
Theorem 2. If an equivalence class is suppressed from the data set, the resulting FEM and VEM is never larger than the previous value, and usually decreases.

An important property conserved by the definition of VEM is additivity, as shown in the next theorem.

Theorem 3 (Additivity of VEM ). Fix generalized data set R of original T and a set of quasi-identifiers Q = { q Let VEM q ( R , T ) denote VEM restricted to quasi-identifier q . Then, Inclusion of Counterfeit Records . The algorithm pro-posed in [19] may call for the creation of counterfeit records. These fake records are extraneous to the data set, but will appear in the generalized release. We therefore must take them into account when computing data quality. Counter-feit records should not add to the information of the dataset, since they are false data. Indeed, it seems acceptable that they decrease this information because their presence may distort the statistical meaning of the data. We take this view and consider that counterfeit records are part of the database, reducing the probability of other classes, but have an information value of 0. To do this, we define where F ( X ) is the set of counterfeit records introduced in X . Note that we add the total of counterfeits to the orig-inal size of the table, to account simultaneously for record suppression: Th e reason the logarithm does not include a |F ( R ) | term is because the information value of those classes that do not have counterfeit records would actually increase, which we find makes no sense. This definition ensures the maxi-mum information of each equivalence class stays the same, no matter how many counterfeits are added to the database.
Counterfeits affect VEM more transparently, as the infor-mation of a class is dependent only on its volume and not o n the record count. Counterfeit tuples may sometimes pro-voke the enlargement of the neighborhood and decrease in-formation, but if this is not the case, the class will keep its information value. In parallel to FEM we extend VEM to deal with counterfeits like this: Th e net effect of these extensions is that the addition of counterfeit records to an anonymized dataset lowers the data quality. This introduces one quirk: in extreme cases with many or very outlying counterfeits, the information of a class may become negative. This is unusual, but not neces-sarily bad, as we may argue that the introduction of false in-formation in a dataset actually creates counter-information that may offset the useful one. A negative value therefore would indicate an exaggerated inclusion of counterfeits. It is possible to prevent negative values with a variant definition that would allow counterfeits to increase information. For lack of space, we do not explore this issue further. Comparison to other metrics . Both FEM and VEM are in-spired by Shannon X  X  entropy and therefore have strong ties to KL-Divergence [8]. The objective in [8] is to release addi-tional information -the marginals of the anonymization  X  to allow estimating the true distribution of the original data, while keeping privacy. In this way, KL-Divergence aims to provide useful relative measure of data quality degradation with respect to a concrete original data set. Our approach is to measure the absolute quantity of information present in datasets, in order to compare the performance of anonymiza-tion algorithms. We thus avoid the computation and selec-tion of marginals, as well as reasoning about the potential risks of releasing this extra information. Our metrics are thus better suited for this simpler scenario.

The nearest metric to VEM we know of is average informa-tion loss ( AIL ). Using theorem 3, we can rewrite VEM for a single equivalence class in a similar way to AIL as Th e obvious difference is we take the logarithms of the in-verse ratios used by AIL . This makes VEM a refinement of AIL with some interesting implications. We see this notion as a variant of FEM , rather than an adaptation of AIL .
While AIL measures the degradation of information in each class, VEM measures the amount of information in in-tuitive terms, as the information is better the larger VEM is. Also, a totally generalized dataset has information 0 and the upper bound of VEM is the logarithm of the volume of the global domain. In comparison, AIL is 0 when the dataset is in its original form and it X  X  impossible to compare differ-ent pristine datasets. The upper bound is reached when all records are generalized to the global maximum range of the database, and is equal to the number of dimensions of the domain. This is not intuitive, as this value depends on the data structure and is hard to reason about.
We now propose an algorithm to prevent attacks in a mul-tiple anonymized query scenario. We consider a dynamic database that evolves over time allowing insertions and dele-tions. This also covers the case where the dataset is static over time, but different queries allow for seemingly dynamic views of it, i.e., some records may appear in one query and not in others. This case is of importance because it allows for big gains in efficiency, since we no longer need to anonymize the whole dataset at once, but can do it piece-wise as each query is submitted. We don X  X  address the ways a query can be specified, as our focus is only on how to combine their results. We wish to return only true data and thus do not use data perturbation techniques. For our purposes, a query is simply an arbitrary subset of the original data.
The attacker receives an anonymized subset in each re-lease, and our aim is that anonymity is preserved even when the attacker correlates the data from many releases. The al-gorithm we propose ( BPG ) is to be used as a post-processor, meaning that it analyses the results of an anonymized query, decides if they can be released to the user, and computes in what form that should be done. The goals of this post-processor are to be lightweight and non-intrusive, i.e., it should work with any standard anonymization algorithm and be fast enough to not degrade significantly the answer-ing of the query. BPG is not tied to any specific protec-tion principle. It can work with any principle that sat-isfies monotonicity, as defined below. There are several monotonous principles, for example, k -anonymity and dis-tinct  X  -diversity. This gives BPG wide applicability.
Definition 3. (Monotonous Principle) An anonymiza-tion principle is monotonous if the validity of a non-empty set implies the validity of all its supersets. Let P ( E ) be true iff equivalence class E satisfies principle P . We say P monotonous if  X E 2  X  X  1 )  X  , ( P ( E 1 ) = True)  X  ( P ( True) with the same parameters on both sides.

In the following discussion, recall that we let the attacker know exactly what individuals are present in each release. Each individual has a unique identifier that is consistent across releases. The sensitive value can change over time, but the quasi-identifiers are fixed. We assume that the gen-eralizations returned by the underlying anonymization al-gorithm are minimal. This reveals preciser data and thus makes the attacker stronger, so there is no loss of security in such an assumption.
 A Post-Processing Anonymization Algorithm . BPG is a query post-processor. It receives a fresh anonymized re-lease of a (part of) a dataset and a state summarizing a history of previous releases. It outputs a new version of this release that is consistent with the previous history in a way that individual records of current and past releases are not vulnerable to intersection nor difference attacks. Differ-ent anonymization algorithms can be used in each release, as long as the protection principle and parameters remain fixed and all of them produce disjoint equivalence classes.
Since anonymization runs are carried out independently, there are no guarantees that their combined results are valid according to a protection principle. A record present in two releases may be placed in equivalence classes A and B with different neighbors. This means that the sets of identities A  X  B , B  X  A and A  X  B may implicitly reveal informa-tion that breaks the protection principle. The job of BPG is to analyze these sets, identify the cases that may lead to a breach of privacy, and conservatively try to make the equivalence class safe for release. If this is not possible, an equivalence class may be omitted from the new release.
BPG k eeps track of the information that any observer hav-ing access to all the releases will have: the minimal region in quasi-identifier space that can be inferred for a given record, which we call a neighborhood , plus a record of appearances of each record in each release, which we call its signature . We denote a signature by  X  ( t ), which boils down to a new field in the database that behaves as follows: before the first release, this field takes the value of the empty string; each new release adds one bit of information to this field; after n releases, each record has an n  X  bit signature, where a 1 in position i means that record was published in the i th release.
We often need to recover all records associated with a spe-cific neighborhood. Therefore, our proposed way to store this information is to add a table of neighborhoods to the database, and a foreign key to it so that each record can be in at most one neighborhood. Since we assume that the adversary knows in which of the releases a target identity is present, our algorithm sees its signature as a quasi-identifier. The fact that we do not allow generalizations over this quasi-identifier is at the crux of our protection against attacks that explore insertions and deletions. This implies that the generated equivalence classes all contain records that have appeared in exactly the same releases. Furthermore, each equivalence class will always share the same signature infor-mation, and therefore each neighborhood also stores a sig-nature field that will match those in all associated records.
Each time the algorithm receives a release, it confronts each equivalence class with the stored record data, and finds the minimum neighborhood that can be released for that class. This process can eventually refine the stored neigh-borhoods for some records, tightening the global level of generalization, and improving the quality of released data.
The algorithm is presented in pseudo-code in Listing 1. In the description and analysis below, P ( X ) means the validity of X according to protection principle P . Furthermore,  X  denotes neighborhood information, where we overload nota-tion to let  X  ( E ) denote the quasi-identifier region associated with a class E , use  X  ( t ) to denote stored neighborhood in-formation for a tuple in db , and use db [  X  ] to denote the set of tuples associated with neighborhood  X  in the database.
BPG first tentatively updates the signatures of all records in db , adding a bit 1 to those that are in the release and a bit 0 for all others (lines 5-6). Then it loops through all classes in the release (line 7). For each record in the new release: 1. It retrieves the neighborhood  X  1 for E 1 (line 8), finds 2. For each pair of neighborhoods (  X  0 ,  X  1 ), it analyzes the 3. If any of these sets violates the protection principle, 4. If such a neighborhood is found, it is published instead 5. Otherwise, if this is not possible, the class is rejected
Ob serve that, in its final steps, BPG ensures that the neighborhoods and signatures are correctly updated for ev-ery record in the database. Also note that the Update sec-tion is part of the main loop.
 Analysis . BPG preserves the following invariant properties after each release:
By monotonicity, an invalid set can only be made valid by increasing its cardinality or making it empty. BPG finds and corrects invalid sets applying either of these two solutions according to one general design principle: it only considers true information and never creates counterfeit records, even though it may force the inclusion of additional (real) records that have been omitted from a given release. Although this may be seen as a form of counterfeiting, we believe that there exists a distinction between the two.

The above invariant excludes attacks on records that are deleted from the dataset, i.e. that do not appear in the latest release. On the other hand, for records that are freshly added to the dataset, i.e. they appear for the first time in the latest release, the underlying anonymization algorithm will automatically guarantee safety: this is because new records have a distinct signature that will force them all into self-contained equivalence classes. This leaves the records that appear in the latest release and had already been released. For these, each of the implicit sets that lead to intersection and difference attacks is analysed in turn. We discuss how BPG effectively protects these records next:
As each test passes, BPG updates the signature informa-tion in the database. If all tests pass, it also updates the neighborhoods, intersecting the released neighborhood with the current state. If some test implies rejection, no neigh-borhoods are updated, and the signatures are reverted to indicate that the records in E 1 will not be published in this release. Hence, the neighborhoods stored in the database are safe and so are the sets implicitly defined by them. Variants . The above algorithm may suppress records from the dataset by rejecting entire equivalence classes. On the other hand it may also include some records that suppos-edly should not appear in the dataset, which we call dirty records . These options are a middle-ground that seems to offer the best compromise between data quality and useful-ness. Two extreme variants can be considered:  X  X o dirty records X  and  X  X ith counterfeits X . In the first case, each time a set fails to validate, BPG simply rejects instead of trying to correct the class. In the second case, if a class does not have enough records to be safe, fake (counterfeit) records are added until it passes the test, in a manner similar to m  X  invariance. There is yet another  X  X iddle X  variant worth considering: when BPG needs to extend class E 1 to make it valid, instead of finding the union with all records in E  X  can add records individually until the class becomes valid. This is possible by monotonicity and would allow the records in Comparison with other approaches . The main advan-tage of our algorithm with respect to the literature is its in-dependence of the anonymization algorithm and protection principle used. Most of the previous work is focussed on one particular principle and usually gives an algorithm tailored specially to achieve it departing from the raw data. Like many other algorithms, we keep a history of past anonymiza-tions, but this is minimal since we only keep an aggregated view of those anonymizations, that does not grow with each new release.

The algorithm we present is suited to deal with databases that may grow or shrink dynamically, which models a broader scenario of anonymizing successive queries. In comparison, [2], [14] and [4] only consider incremental databases. In-deed, m  X  invariance was the first notion proposed to deal with deletions of records. Unlike m  X  invariance, our algo-rithm is flexible enough to allow a given record to appear in sets with different combinations of sensitive values, as long as all validity tests are met. This allows the refining of in-formation relating to already anonymized records and main-taining protection without introducing false information. In the next section, we present our experimental results and a direct comparison with m  X  invariance.

As a final note, we emphasize that BPG has been designed for efficiency, so that it can be competitive with other ex-isting solutions. This means that, rather than pursuing an optimal solution, our algorithm adopts simple heuristics. In particular, BPG anonymizes each candidate release indepen-dently, and it analyses equivalence classes sequentially. The downside is that the output of the algorithm may degrade for unfavorable inputs, such as update patterns that consis-tently lead to small intersections and difference sets. We describe the results of the experimental evaluation of BPG . Given its flexibility, we had to fix some parameters at the start. We used Mondrian as anonymization algorithm, and both k -anonymity and  X  -diversity with k =  X  = 10 as protection principles (simultaneously), since they are the most intuitive and still widely used. Mondrian was a good choice because it is fast, simple, and independent of tax-onomies, yielding very tight neighborhoods. For the split-ting heuristic in Mondrian we chose degeneracy, which af-ter some tests proved to give the best results. We per-formed a parallel comparison between our algorithm and m  X  invariance with m = 10. We used the source code in C ++ made available by the authors of [19] as reference im-plementation. We also used the same databases, named SAL and OCC from the Adult dataset, downloadable from http://ipums.org . We coded our algorithm in the same language to ensure a fair comparison. The tests were run on a machine equipped with a 3.16 GHz Intel Core2 Duo processor and 4Gb of RAM on Windows 7 (64 bits) and no workload on the computer besides the testing procedure.
Both databases have 600k tuples with four quasi-identifier attributes (age, gender, education and birthplace). The sen-sitive attribute is income in the SAL database and occu-pation in the OCC . All column values are discretized and the sensitive attribute has 50 possible values. We used the same setup as the reference implementation, by creating a dynamic table for each data set with 200k new randomly sampled tuples and made successive anonymizations based on an update rate parameter r which can take values 40k, 20k, 10k and 5k. For BPG we make an anonymization of the original table and create history state based on this. In each following release, r random new tuples replace r old ones and this process is repeated until the 600k tuples are used. We repeat the test ten times for every update rate. The results shown are the average over these runs. Benchmarking results . We first compare the time used by both algorithms. In Figure 1, we report the total time of running the whole test for m  X  invariance and for BPG (where the latter means BPG after Mondrian). Each run in-cludes 400k /r + 1 releases for either algorithm. In Figure 2, we show the average time per release. The total execution time grows exponentially as the update rate decreases, which is expected since the number of releases required to cover all 600k records doubles with each halving of the update rate. For all 8 cases, BPG is noticeably faster overall than m  X  invariance, varying from 20% faster at the longer runs to almost 40% faster for the shorter. BPG is very stable regarding the average time per release, but m  X  invariance becomes more efficient as more runs are made and approx-imates BPG  X  X  average time per run. However, BPG also showed a decrease in the time per release for the smallest update rates. This suggests that this time will eventually settle around a constant. This is to be expected since, as the number of updates tends to zero, both m  X  invariance and BPG are reducing their level of intervention and tend-ing towards static consistency checks.

We also separately analyzed the efficiency of the post-processing algorithm itself, i.e. checking the overhead over the execution time of Mondrian. As can be seen in Table 4, the impact of post-processing in the overall time slowly in-creases for decreasing update rates. This is natural because, as the total number of releases grows, the number of conflicts that potentially need to be resolved because of overlapping equivalence classes will tend to increase. Nevertheless, in our cases the overhead stayed between 1 / 5 and 1 / 4 of the total time, which indicates that BPG definitely does not con-stitute a bottleneck.

We also analysed the quality of the data produced by both algorithms. The results from all cases resulted in curves with similar general shapes, as can be checked in Figure 3. The main features are the following: Table 4: BPG impact on anonymization process
The bad performance of m  X  invariance relating to VEM can be justified as follows: m  X  invariance X  X  priority is to assign records to buckets based on their sensitive value. Indeed, the assignment of tuples to unbalanced buckets in the first stages of the algorithm does not consider the quasi-identifier values. When the equivalence classes are created in the final stages of the algorithm, the final result may already be com-promised by earlier choices. Adapting m  X  invariance to take quasi-identifier information into consideration from earlier on may improve its performance for VEM .

The conclusions we draw from the previous benchmarking results are the following. The BPG algorithm is competitive with m  X  invariance both in terms of execution time and car-dinality of the generated equivalence classes. However, our algorithm displays a significant advantage in terms of the degradation of data quality, when this takes into consider-ation the amount of generalization of the resulting quasi-identifier regions. Fi gure 3: FEM and VEM values for r = 5k and r = 40k.
Some open problems remain. The metric definitions we propose are defined for releases with strictly disjoint neigh-borhoods. Since some anonymization algorithms might pro-duce overlapping neighborhoods, it would be interesting to expand them to handle this case. Our algorithm also makes strong use of the monotonicity property of the protection principle. Some principles, like simple-and entropy- X  -diversity and t -closeness are not monotonous in the sense we defined here, but they have some related properties, in particular, the union of valid classes is valid (rather than any superset of a valid class). Also, some principles nearly satisfy mono-tonicity as we defined it, but the protection parameter must be degraded a little. Further work could propose ways of expanding our algorithm to deal with these weaker forms. An interesting consequence of dealing with the latter princi-ples is that successive releases might degrade the protection parameter, and so a maximum should be imposed on the total number of queries over a certain dataset.

We do not consider attacks based on knowledge of the anonymization algorithm used [7], i.e. Algorithm-Safe data Publishing (ASP). In [7], the authors show how existing al-gorithms can be modified to achieve ASP versions for some classical protection principles. One of these strategies, Strat-ified Pick-up, performs post-processing not unlike BPG . Ex-ploring how this can be integrated into BPG is an interesting direction for future work. [1] R. Bayardo and R. Agrawal. Data privacy through [2] J.-W. Byun, T. Li, E. Bertino, N. Li, and Y. Sohn. [3] J.-W. Byun, Y. Sohn, E. Bertino, and N. Li. Secure [4] B. C. M. Fung, K. Wang, A. W. C. Fu, and J. Pei. [5] B. C. M. Fung, K. Wang, and P. S. Yu. Top-down [6] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. [7] Xin Jin, Nan Zhang, and Gautam Das. Algorithm-safe [8] D. Kifer and J. Gehrke. Injecting utility into [9] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. [10] Ninghui Li, Tiancheng Li, and Suresh [11] A. Machanavajjhala, D. Kifer, J. Gehrke, and [12] B. Malin and L. Sweeney. Re-identification of dna [13] A. Narayanan and V. Shmatikov. Robust [14] J. Pei, J. Xu, Z. Wang, W. Wang, and K. Wang. [15] L. Sweeney. Achieving k-anonymity privacy protection [16] Y. Tao, Y. Tong, S. Tan, S. Tang, and D. Yang. [17] G. Wang, Z. Zhu, W. Du, and Z. Teng. Inference [18] K. Wang and B. Fung. Anonymizing sequential [19] X. Xiao and Y. Tao. M-invariance: towards privacy [20] J. Xu, W. Wang, J. Pei, X. Wang, B. Shi, and A. Fu.
