 The efficient execution of integration processes between dis-tributed, heterogeneous data sources and applications is a challenging research area of data management. These in-tegration processes are an abstraction for workflow-based integration tasks, used in EAI servers and WfMS. The ma-jor problem are significant workload changes during run-time. The performance of integration processes strongly de-pends on those dynamic workload characteristics, and hence workload-based optimization is important. However, exist-ing approaches of workflow optimization only address the rule-based optimization and disregard changing workload characteristics. To overcome the problem of inefficient pro-cess execution in the presence of workload shifts, here, we present an approach for the workload-based optimization of instance-based integration processes and show that signifi-cant execution time reductions are possible.
 H.2.4 [ Systems ]: Query processing; H.2.5 [ Heterogeneous Databases ]: Data translation; H.4.1 [ Office Automation ]: Workflow management Algorithms, Design, Performance Integration Processes, Optimization, Workload Adaptation
Due to the trend towards heterogeneous system environ-ments, research on integration processes has gained in im-portance. Here, integration processes are considered as an abstraction for workflow-based integration tasks, that are executed by a central integration platform. Examples for those platforms are EAI (Enterprise Application Integra-tion) servers, or WfMS (Workflow Management Systems).
Integration processes are pervasive and the overall perfor-mance of complete enterprise IT-infrastructures depends on the performance of the central integration platform. A fun-damental problem are significant workload changes during runtime. Due to dynamic workload shifts, deployed inte-gration processes may perform inefficiently over time. The majority of existing workflow optimization approaches [2, 3, 4, 5] apply only rule-based optimization techniques and disregard the cost-based optimization based on monitored execution statistics.

With the aim to overcome this profound problem, we present an model-driven approach for the workload-based optimization of instance-based integration processes. The basic idea of our approach is to monitor execution statistics during runtime (workload characteristics), and to use those statistics for periodical re-optimization of the deployed inte-gration processes. The resulting feedback loop opens oppor-tunities to dynamically adapt to changing workload char-acteristics and hence, overcomes the problem of inefficient process execution in the presence of workload shifts.
Obviously, the optimization of integration processes is re-lated to traditional distributed database optimization. How-ever, there are major differences that necessitated new op-timization techniques and algorithms. First, there are addi-tional operators (e.g., Switch, Fork, Iteration, Translation, and Split). Second, the control flow and its optimization must be taken into account. Third, integration processes are deployed once and executed many times. In fact, the opti-mization of imperative integration processes is much more complex than the optimization of declarative queries.
In this section, we give a compact overview of our devel-oped feedback loop for the workload-based optimization of integration processes. The approach is based on our inte-gration process model X  X he Message Transformation Model (MTM) [1] that contains interaction-oriented, control-flow-oriented, and data-flow-oriented operators.

Workload Monitoring and Cost Prediction. Ba-sically, workload characteristics in the means of execution times, data properties (e.g., cardinalities, null-values, or-dered criteria) and relative frequencies of alternative process paths are monitored for the single operators of the deployed process plan P . Those statistics are maintained over a slid-ing time window W ( P ). Furthermore, we have adapted a cost model from a relational DBMS to the used integration model. Here, we enriched the model with additional seman-tics for interaction-and control-flow-oriented operators. In order to estimate the costs for a rewritten process plan P we compare the costs C of the single operators weighted with the execution statistics W of the current process plan P . Thus, we estimate missing workload execution statistics
Periodical Re-optimization. According to functional aspects of the integration process model, a process plan can be rewritten X  X ased on the cost prediction approach X  X o a semantical equivalent process plan. There, we must be aware of dependencies between operators. We formally define the problem of periodical re-optimization as follows: Definition 1. Process Plan Optimization Problem (POP): A process plan P = ( N, S, F ) is optimal at timestamp T with respect to the logged workload W ( P, T k ) if no process plan P 0 = ( N 0 , S, F 0 ) with C ( P 0 ) &lt; C ( P ) exists. Other-wise, the process plan P is suboptimal . The process plan optimization problem describes the periodical creation of the optimal process plan P at timestamp T k with the period T (optimization interval). The workload W ( P ) is available for a sliding time window of size T W . An optimization time T define that T W  X  T Opt  X  T OI  X  T Opt , even in the worst case. The naive algorithm would comprise three subproblems: (i) the complete creation of alternative process plans, (ii) the cost evaluation of each created process plan, and (iii) the choice of the process plan with minimal costs. Solving the POP at timestamp T k, 1 , the result is the optimal process plan correlated to the timestamp T k, 0 .

Theorem 1. The Process Plan Optimization Problem that includes the creation of the optimal process plan is NP-hard.
Proof. The subproblem of join enumeration is used to proof the theorem. The complexity of join enumeration de-pends on parameters like the query type (tree, chain, star, cycle, clique), the join tree class, the cost function, and the use of cross products. In general, this problem is NP-hard. A process plan is a directed graph. Hence, all types of join queries are possible. If the used cost model exhibit the ASI property (Adjacent Sequence Interchange), polynomial time algorithms can be found for join enumeration. There, ranks are assigned to base relations, where the sequence of or-dered ranks is the optimal join enumeration. However, the cost model does not exhibit the ASI property. In conclusion, the process plan optimization problem is also NP-hard.
The problem of cost-based query optimization in DBMS is also NP-hard. Thus, the proof is not surprising. However, in [4], it was claimed that the optimal Web service query plan can be computed in O ( N 5 ), where n is the number of Web services. The difference is caused by the assumption of negligible local processing costs, made in [4].

Optimization Techniques. In order to reduce the com-plexity, we use a heuristic algorithm X  X he pattern matching optimization algorithm X  X hat recursively evaluates the op-erators of process plan P and rewrites that plan to P 0 using our set of workload-based optimization techniques (shown in Figure 1). Basically, we distinguish between data-flow-oriented and control-flow-oriented optimization techniques. Some of those techniques are similar to distributed query optimization while others (such as the rewriting of Switch-paths) are specific to integration processes. There, we use heuristic join enumeration restrictions that reduces the num-ber of alternative joins orderings to 2( n  X  1).
 Figure 1: Workload-Based Optimization Techniques
Workload Adaptation. The core optimization algo-rithm can be influenced by parameters. Here, we explain how a fast and a slow workload adaptation can be realized with the right choice of parameterization. Basically, work-load statistics of the current plan P are aggregated over the sliding time window. There, we distinguish the three pa-rameters (i) workload aggregation method, (ii) the sliding window size, and (iii) the optimization interval. The work-load aggregation method (i) is the method used to aggregate execution statistics over the sliding window. Here, we use moving-average-based and regression-based methods (with different sensibility properties). The sliding time window size (ii) T W is the time interval used to aggregate statistics. Hence, its length influences the sensibility of the adaptation (the larger the time window, the less sensible the adapta-tion). Finally, there is the optimization interval T represents the time interval after which the optimization is triggered and costs are estimated. Clearly, with T OI  X  0, we get a continuous workload estimation value and hence, also influence the sensibility (the larger the optimization in-terval, the less sensible the adaptation).
To summarize, we introduced the workload-based opti-mization of integration processes to overcome the main prob-lem of inefficient integration processes in the presence of workload shifts. In the area of workflow-based integration tasks, this was considered for the first time. The precon-ditions of our solution comprise the monitoring of workload and execution statistics as well as the cost prediction. Based on these preconditions, we discussed the NP-hard Process Plan Optimization Problem (POP) , including the set of used optimization techniques, approaches for search space reduc-tion as well as fast and slow workload adaptation. The ex-perimental evaluation of this approach has been shown that significant execution time reductions are possible, with low overhead for periodical re-optimization.
