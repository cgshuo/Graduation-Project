 Semi-supervised sentiment classification aims to train a classifier with a small number of labeled data (called seed data) and a large amount of unlabeled data. a big adva ntage of this approach is its saving of annotation effort by us ing the unlabeled data which is usually freely available. In this paper, we propose an approach to further minimize the annotation effort of semi-supervised sentiment classification by actively selecting the seed data. Specifically, a novel selection strategy is proposed to simultaneously select good words and documents for manual annotation by considering both of their annotation costs and informativeness. Experimental results demonstrate the effectiveness of our approach. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  Linguistic processing ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text analysis ; I.5.2 [ Pattern Recognition ]: Design Methodology  X  Classifier design and evaluation ; Opinion Mining, Seed Selection , Semi-supervised, Sentiment Classification Research in sentiment analysis has been progressed tremendously in recent years (Hu and Liu, 2004; Wiebe et al., 2005; Pang and Lee, 2008). In this research area, sentiment classification is a fundamental task which aims to identify the sentimental categories (e.g., positive or negative) of a natural language text towards a given topic (Pang et al., 2002; Turney, 2002). This task has become the core component of many important applications in sentiment analysis (Cui et al., 2006; Lloret et al., 2009; Zhang and Ye, 2008; Li et al., 2011a). In the literature, machine learni ng (ML) approaches have been proved to be promising and widely used in sentiment classification. Generally speaking, the ML approaches could be grouped into three main categorie s: (1) lexicon-based learning: which employs a lexicon containing a certain number of opinion words to conduct a classifier. The popularly used term-counting approach is a typical example of such approaches. (Turney and Littman, 2002). (2) Corpus-based learning: which employs annotated corpus containing many labeled samples to conduct a machine learning-based classifier (Pang et al., 2002). (3) Joint lexicon-corpus leaning: whic h employs both a lexicon and annotated corpus to perform sentiment classification (Melville et al., 2009). Generally, the learning approaches of the third group achieve better performances due to the full consideration of the classification knowledge in both types of resources. However, whatever ML approach is employed, the good performance mainly relies on manually labeled data, which is sometimes rather time-consuming a nd expensive to get. To cope with this problem, a promising solution is to perform a semi-supervised learning activity in which only a small amount of labeled data (i.e., the seed data) to train a classifier, together with a large amount of unlabeled data. One key issue to further minimize the annotation cost in semi-supervised learning is how to get the seed data. A simple way to achieve this is to randomly select some samples for annotation as the seed data. However, this simple selection strategy is problematic when the learning approaches of the third group are employed due to the following reasons. First, in joint lexicon-corpus learning, both words and documents are required to be annotated. The types of samples (words or documents) and the amount of them preferred to be selected respectively remains an untouched problem. Second, random se lection is possibly to select many helpless samples for sentimen t classification, e.g., the stop words like "a", "the" and "as". In contrast, the clever selection strategies should be applied to get some opinion words like "wonderful", "excellent" and "poor" which are thought to be more valuable for sentiment classification. In this paper, we focus on the seed selection issue in semi-supervised sentiment classificati on where the joint lexicon-corpus learning approaches are employed. To address the above two problematic issues, we first inve stigate the annotation costs of annotating a word or a documen t; Then, we propose a uniform measurement to define the informativeness of a word or a informativeness measur ement are taken into account to decide a selection strategy to select good words and documents for manual annotation. The remainder of this paper is organized as follows. Section 2 overviews the related work on semi-supervised sentiment classification. Section 3 proposes our strategy for selecting samples as the seed data. Sec tion 4 reports the experimental results. Finally, section 5 draws the conclusion. Although supervised learning methods for sentiment classification have been extensively studied (Pang et al., 2002), the studies on semi-supervised sentiment classification are relatively new. Most related studies applied the learning approaches belonging to the second group, i.e., corpus-based learning, such as, Dasgupta and Ng (2009), Li et al. (2010) and Li et al. (2011b). As far as the third group of learning approaches, i.e., joint lexicon-corpus learning, is concerned, Sindhwani and Melville (2008) firstly propose a semi-supervised sentiment prediction algorithm that utilizes lexical prior knowledge in conjunction with unlabeled examples based on a document-word bipartite graph. More recently, Li et al. (2009) propose a non-negative tri-factorization approach to jointly learning classifier with a lexicon, labeled and unlabeled documents for semi-supervised sentiment classification. Unlike both studies mentioned above, our work focuses on the selection strategy of the seed data when a semi-supervised learning approach is employed, which has not been addressed in sentiment classification yet. 
Table 1: The numbers of annotated words and documents by A distinguishing feature of the third-group learning approaches is its requirement of annotati ng both words and documents. Instinctively the cost of acquiring a labeled word is different from a labeled document. To investigate their real annotation costs, four annotators, named A , B , C , and D , are asked to take half an hour to annotate some words and documents. Note that all the annotators are with background know ledge of sentiment analysis, which makes the annotation wo rk reliable and fast. The documents for annotating are from the multi-domain sentiment classification corpus, collected by Blitzer et al. (2007) words for annotating are extracted from the corpus. Table 1 shows the annotation results where Word N and D oc N denote the number of the annotated words and documents respectively. From table 1, we can see that in a certain time, much more words can be annotated than documen ts. On average, the time of annotating a document equals th at of annotating 16-18 words. Informative samples are encouraged to be selected as seed data for a good performance of a semi-supervised learning approach. In sentiment classification, the informativeness of a word is influenced by two main factors. First, the POS of a word is an important prior knowledge for eval uating its informativeness. For example, adjectives are more likely to be an opinion word and thus thought to be more informative than other words like verbs and nouns. To make the POS info rmation computational, we evaluate the informativeness va lue of a sample as follows. 200 words are firstly randomly selected from the corpus and the proportion of opinion words are calculated for each POS tag proportion is served as the informativeness value of the word belonging to the same POS tag. Table 2 shows the detailed proportions. Second, the frequency of a word is another important prior knowledge for evaluating its informativeness. More frequently occurring words are believed to be more informative. In summary, the informativeness value of a word w is defined as follows: informativeness value of the POS tag x . For example, if the word is a adjective, the POS value is set to V (JJ)=0.44 as shown in corpus. As far as a document d is concerned, the informativeness value is defined as follows: Where ( ) values of all containing words. L( d ) is the length of the document, i.e., the total number of the containing words. The document containing more words possibly takes more helpless words and thus is thought to be less informative. Our strategy for selecting the samples as the seed data considers both the annotation cost and the informativeness values. Principally, the samples with less annotation cost and higher informativeness value are encouraged to be labeled. The samples are ranked according to their scores which are calculated as follows: Where Infor ( s ) is the informativeness value of the sample s and sample is a word, the informativenss value is calculated by formula (1) and the annotation cost is set to 1/16. If the sample is a document, the informativenss value is calculated by formula (2) and the annotation cost is set to 1. We systematically evaluate our seed selection approach for semi-supervised sentiment classification on the multi-domain dataset as mentioned in Section 3.1.  X  Dataset: This dataset contai ns product reviews from four  X  Features: Word unigram features are used. Each review text is  X  Classification algorithm: The semi-supervised classification To perform the lexicon-corpus learning, the polarities of the words are necessarily known before. Thus, we manually annotate the words that occurs more than twice in the corpus. The detailed statistics of the annotated words w ith polarity labels (i.e., positive, negative, and neutral) are shown in Table 3. From this table, we can see that most words are neutral. Accur acy Accuracy Accuracy
Figure 1: Performances of different selection strategies with  X  Random-D: randomly selecting only documents for  X  Uncertainty-D: employing the active learning procedure as  X  Random-W: randomly selecting only words for annotation.  X  Random-D+W: randomly selecting both documents and Figure 1 illustrates the performances of different selection strategies where 50, 100, and 200 annotation costs are used. Here, 50 annotation costs means the cost of annotating 50 documents or 800 (50*16) words. From this figure, we can see that Uncertainty-D performs no better than Random-D , which indicates that uncertainty is useless for the seed selection for semi-supervised sentiment classifica tion, although it is shown to be effective for many active learning task. Our approach significantly outperforms other approaches ( p -value&lt;0.01). Especially, when only a few annotation effort (annota tion cost equals 50) is used, our approach performs remarkably better. Taking a look into the seed samples, we find that they contains both words (most are adjectives) and documents. For example, in DVD domain, the seed samples contains 544 wo rds (including 119 positive words and 91 negative words) and 16 documents. This result demonstrates the im portance of labeling both opinion words and informative documents when a limited annotation cost is given. In this paper, we investigate the annotation costs of labeling a word and a document and propose a measurement to uniformly define the informativeness of a word and a document. The annotation cost and the inform ativeness measurement are both considered into the selection strategy for selecting good seed data for semi-supervised sentiment cl assification of document-level. Experimental results show that our selection strategy could significantly improve the perfo rmance of semi-supervised sentiment classification. The research work described in this paper has been partially supported by three NSFC grants, No.61003155, No.60873150 and No.90920004, one National High-tech Research and Development Program of China No.2012AA011102, Open Projects Program of National Laboratory of Pattern R ecognition, and the NSF grant of Zhejiang Province No.Z1110551. We also thank the two anonymous reviewers for their helpful comments. [1] Blitzer, J., Dredze, M., and Pe reira, F. 2007. Biographies, [2] Cui, H., Mittal, V., and Datar, M. 2006. Comparative [3] Dasgupta, S. and Ng, V. 2009. Mine the Easy, Classify the [4] Hu M. and B. Liu. 2004. Mi ning and Summarizing Customer [5] Li, S., Huang, C. and Zong, C. 2011a. Multi-domain [6] Li, S., Huang C., Zhou, G., and Lee, S. 2010. Employing [7] Li, S., Wang, Z., Zhou, G., and Lee, S. 2011b. Semi-[8] Li, T., Zhang, Y., and Sindhwani, V. 2009. A Non-negative [9] Lloret, E., Balahur, A., Palomar, M., and Montoyo, A. [10] Melville, P., Gryc, W., and Lawrence, R. 2009. Sentiment [11] Pang, B. and Lee, L. 2008. Opinion Mining and Sentiment [12] Pang, B., Lee, L., and Vaithyanathan, S. 2002.Thumbs up? [13] Sindhwani, V. and Melville, P. 2008. Document-Word Co-[14] Tong, S. and Koller, D. 2002. Support Vector Machine [15] Turney, P. 2002. Thumbs up or Thumbs down? Semantic [16] Turney, P. and Littman, M. 2002. Unsupervised Learning of [17] Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating [18] Zhang, M. and Ye, X. 2008. A Generation Model to Unify 
