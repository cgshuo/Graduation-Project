 Given a document or document set in one source language, cross-language document summariza-tion aims to produce a summary in a different target language. In this study, we focus on Eng-lish-to-Chinese document summarization for the purpose of helping Chinese readers to quickly understand the major content of an English docu-ment or document set. This task is very impor-tant in the field of multilingual information ac-cess. 
Till now, most previous work focuses on monolingual document summarization, but cross-language document summarization has re-ceived little attention in the past years. A straightforward way for cross-language docu-ment summarization is to translate the summary from the source language to the target language by using machine translation services. However, though machine translation techniques have been advanced a lot, the machine translation quality is far from satisfactory, and in many cases, the translated texts are hard to understand. Therefore, the translated summary is likely to be hard to understand by readers, i.e., the summary quality is likely to be very poor. For example, the trans-lated Chinese sentence for an ordinary English sentence ( X  X t is also Mr Baker who is making the most of presidential powers to dispense lar-gesse. X ) by using Google Translate is  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X . The translated sentence is hard to understand because it contains incorrect translations and it is very disfluent. If such sen tences are selected into the summary, the quality of the summary would be very poor. 
In order to address the above problem, we propose to consider the translation quality of the English sentences in the summarization process. In particular, the translation quality of each Eng-lish sentence is predicted by using the SVM re-gression method, and then the predicted MT quality score of each sentence is incorporated into the sentence evaluation process, and finally both informative and easy-t o-translate sentences are selected and translated to form the Chinese summary. 
An empirical evaluation is conducted to evalu-ate the performance of machine translation qual-ity prediction, and a user study is performed to evaluate the cross-language summary quality. The results demonstrate the effectiveness of the proposed approach. The rest of this paper is organized as follows: Section 2 introduces related work. The system is overviewed in Section 3. In Sections 4 and 5, we present the detailed algorithms and evaluation results of machine translation quality prediction and cross-language summarization, respectively. We discuss in Section 6 and conclude this paper in Section 7. 2.1 Machine Translation Quality Prediction Machine translation evaluation aims to assess the correctness and quality of the translation. Usu-ally, the human reference translation is provided, and various methods and metrics have been de-veloped for comparing the system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the trans-lation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a vari-ety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and refer-ence-based features to measure the quality of sentences. 
Transition evaluation without using reference translations has also b een investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on transla-tion output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplex-ity, etc.) and features about the translation proc-ess are leveraged. Gamon et al. (2005) investi-gate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplex-ity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into  X  X ood X  or  X  X ad X  categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. 
In this study, we further predict the translation quality of an English sentence before the ma-chine translation process, i.e., we do not leverage reference translation and the target sentence. 2.2 Document Summarization Document summarization methods can be gener-ally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extraction-based summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. 
For single document summarization, the sen-tence score is usually computed by empirical combination of a number of statistical and lin-guistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual re-inforcement principle (Zha 2002; Wan et al., 2007). 
For multi-document summarization, the cen-troid-based method (Radev et al., 2004) is a typi-cal method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select impor-tant sentences. Machine Learning based ap-proaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summariza-tion performance have been investigated in (Nenkova and Louis, 2008). Graph-based meth-ods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to com-pute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extract-ing easy-to-understand English summaries for non-native readers. 
Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary transla-tion. Leuski et al. (2003) use machine translation for English headline generation for Hindi docu-ments. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japa-nese documents into Korean documents, and then extracting summary sentences by using Ko-rean summarizer, and finally mapping Korean summary sentences to Japanese summary sen-tences. Chalendar et al. (2005) focuses on se-mantic analysis and sentence generation tech-niques for cross-language summarization. Orasan and Chiorean (2008) propose to produce summa-ries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross language query based summarization has been investigated in (Pingali et al., 2007), where the query and the documents are in different languages. Other re-lated work includes multilingual summarization (Lin et al., 2005), which aims to create summa-ries from multiple sources in multiple languages. Siddharthan and McKeown (2005) use the in-formation redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. Previous methods for cross-language summariza-tion usually consist of two steps: one step for summarization and one step for translation. Dif-ferent order of the two steps can lead to the fol-lowing two basic English-to-Chinese summariza-tion methods: Late Translation (LateTrans) : Firstly, an English summary is produced for the English document set by using existing summarization methods. Then, the English summary is auto-matically translated into the corresponding Chi-nese summary by using machine translation ser-vices. Early Translation (EarlyTrans) : Firstly, the English documents are translated into Chinese documents by using machine translation services. Then, a Chinese summary is produced for the translated Chinese documents. 
Generally speaking, the LateTrans method has a few advantages over the EarlyTrans method: 1) The LateTrans method is much more effi-cient than the EarlyTrans method, because only a very few summary sentences are required to be translated in the LateTrans method, whereas all the sentences in the documents are required to be translated in the EarlyTrans method. 2) The LateTrans method is deemed to be more effective than the EarlyTrans method, be-cause the translation errors of the sentences have great influences on the summary sentence extrac-tion in the EarlyTrans method. 
Thus in this study, we adopt the LateTrans method as our baseline method. We also adopt the late translation strategy for our proposed ap-proach. 
In the baseline method, a translated Chinese sentence is selected in to the summary because the original English sentence is informative. However, an informative and fluent English sen-tence is likely to be tran slated into an uninforma-tive and disfluent Chinese sentence, and there-fore, this sentence cannot be selected into the summary. 
In order to address the above problem of exist-ing methods, our proposed approach takes into account a novel factor of each sentence for cross-language summary extraction. Each English sen-tence is associated with a score indicating its translation quality. An English sentence with high translation quality score is more likely to be selected into the original English summary, and such English summary can be translated into a better Chinese summary. Figure 1 gives the ar-chitecture of our proposed approach. 
Seen from the figure, our proposed approach consists of four main steps: 1) The machine translation quality score of each English sentence is predicted by using regression methods; 2) The informativeness score of each English sentence is computed by using existing methods; 3) The English summary is produced by making use of both the machine translation quality score and the informativeness score; 4) The extracted Eng-lish summary is translat ed into Chinese summary by using machine translation services. In this study, we adopt Google Translate 1 for English-to-Chinese translation. Google Translate is one of the state-of-the-art commercial machine translation systems used today. It applies statisti-cal learning techniques to build a translation model based on both monolingual text in the tar-get language and aligned text consisting of ex-amples of human translations between the lan-guages. 
The first step and the evaluation results will be described in Section 4, and the other steps and the evaluation results will be described together in Section 5. 4.1 Methodology In this study, machine translation (MT) quality reflects both the translation accuracy and the flu-ency of the translated sentence. An English sen-tence with high MT quality score is likely to be translated into an accurate and fluent Chinese sentence, which can be easily read and under-stand by Chinese readers. The MT quality pre-diction is a task of mapping an English sentence to a numerical value corresponding to a quality level. The larger the value is, the more accurately and fluently the sentence can be translated into Chinese sentence. 
As introduced in Section 2.1, several related work has used regression and classification methods for MT quality prediction without refer-ence translations. In our approach, the MT qual-ity of each sentence in the documents is also pre-dicted without reference translations. The differ-ence between our task and previous work is that previous work can make use of both features in source sentence and features in target sentence, while our task only leverages features in source sentence, because in the late translation strategy, the English sentences in the documents have not been translated yet at this step. 
In this study, we adopt the  X  -support vector re-gression (  X  -SVR) method (Vapnik 1995) for the sentence-level MT quality prediction task. The SVR algorithm is firmly grounded in the frame-work of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points. Formally, given a set of training data points D ={( x i ,y i )| i =1,2,..., n }  X  R d  X  R , where x i feature vector and y i is associated score, the goal is to fit a function f which approximates the rela-tion inherited between the data set points. The standard form is: Subject to 
The constant C &gt;0 is a parameter for determin-ing the trade-off between the flatness of f and the amount up to which deviations larger than  X  are tolerated. 
In the experiments, we use the LIBSVM tool (Chang and Lin, 2001) with the RBF kernel for the task, and we use the parameter selection tool of 10-fold cross validation via grid search to find the best parameters on the training set with re-spect to mean squared error (MSE), and then use the best parameters to train on the whole training set. 
We use the following two groups of features for each sentence: the first group includes several basic features, and the second group includes several parse based features 2 . They are all de-rived based on the source English sentence. 
The basic features are as follows: 1) Sentence length : It refers to the number of 2) Sub-sentence number : It refers to the num-3) Average sub-sentence length : It refers to 4) Percentage of nouns and adjectives : It re-5) Number of question words : It refers to the 
We use the Stanford Lexicalized Parser (Klein and Manning, 2002) with the provided English PCFG model to parse a sentence into a parse tree. The output tree is a context-free phrase structure grammar representation of the sentence. The parse features are then selected as follows: 1) Depth of the parse tree : It refers to the 2) Number of SBARs in the parse tree : 3) Number of NPs in the parse tree : It refers 4) Number of VPs in the parse tree : It refers 
All the above feature values are scaled by us-ing the provided svm-scale program. 
At this step, each English sentence s i can be associated with a MT quality score TransScore ( s i ) predicted by the  X  -SVR method. The score is fi-nally normalized by dividing by the maximum score. 4.2 Evaluation 4.2.1 Evaluation Setup In the experiments, we first constructed the gold-standard dataset in the following way: 
DUC2001 provided 309 English news articles for document summarization tasks, and the arti-cles were grouped into 30 document sets. The news articles were selected from TREC-9. We chose five document sets (d04, d05, d06, d08, d11) with 54 news articles out of the DUC2001 document sets. The documents were then split into sent ences and we used 1736 sentences for evalua tion. All the sentences were automatically translated into Chinese sentences by using the Google Translate service. 
Two Chinese college students were employed for data annotation. They read the original Eng-lish sentence and the translated Chinese sentence, and then manually labele d the overall translation quality score for each sentence, separately. The translation quality is an overall measure for both the translation accuracy and the readability of the translated sentence. The score ranges between 1 and 5, and 1 means  X  X ery bad X , and 5 means  X  X ery good X , and 3 means  X  X ormal X . The correla-tion between the two sets of labeled scores is 0.646. The final translation quality score was the average of the scores provided by the two anno-tators. 
After annotation, we randomly separated the labeled sentence set into a training set of 1428 sentences and a test set of 308 sentences. We then used the LIBSVM tool for training and test-ing. 
Two metrics were used for evaluating the pre-diction results. The two metrics are as follows: 
Mean Square Error (MSE) : This metric is a measure of how correct each of the prediction values is on average, penalizing more severe er-rors more heavily. Given the set of prediction scores for the test sentences: } ,... 1 |  X  {  X  n i y Y the manually assigned scores for the sentences: is defined as 
Pearson X  X  Correlation Coefficient (  X  ) : This metric is a measure of whether the trends of pre-diction values matched the trends for human-labeled data. The coefficient between Y and Y  X  is defined as where y and y  X  are the sample means of Y and Y  X  , of Y and Y  X  . 4.2.2 Evaluation Results Table 1 shows the prediction results. We can see that the overall results are promising. And the correlation is moderately high. The results are acceptable because we only make use of the fea-tures derived from the source sentence. The re-sults guarantee that the use of MT quality scores in the summarization process is feasible. 
We can also see that both the basic features and the parse features are beneficial to the over-all prediction results. 5.1 Methodology In this section, we first compute the informative-ness score for each sentence. The score reflect how the sentence expresses the major topic in the documents. Various existing methods can be used for computing the score. In this study, we adopt the centroid-based method. 
The centroid-based method is the algorithm used in the MEAD system. The method uses a heuristic and simple way to sum the sentence scores computed based on different features. The score for each sentence is a linear combination of the weights computed based on the following three features: 
Centroid-based Weight. The sentences close to the centroid of the document set are usually more important than the sentences farther away. And the centroid weight C ( s i ) of a sentence s calculated as the cosine similarity between the sentence text and the concatenated text for the whole document set D . The weight is then nor-malized by dividing the maximal weight. 
Sentence Position. The leading several sen-tences of a document are usually important. So we calculate for each sentence a weight to reflect number of sentences in the document. Obviously, i ranges from 1 to n . 
First Sentence Similarity. Because the first sentence of a document is very important, a sen-tence similar to the first sentence is also impor-tant. Thus we use the cosi ne similarity value be-tween a sentence and the corresponding first sen-tence in the same document as the weight F ( s i for sentence s i . 
After all the above weights are calculated for each sentence, we sum all the weights and get the overall score for the sentence as follows: where  X  ,  X  and  X  are parameters reflecting the importance of different features. We empirically set  X  =  X  =  X  =1. 
After the informativeness scores for all sen-tences are computed, the score of each sentence is normalized by dividing by the maximum score. 
After we obtain the MT quality score and the informativeness score of each sentence in the document set, we linearly combine the two scores to get the overall score of each sentence. Formally, let TransScore ( s i )  X  [0,1] and Info-Score ( s i )  X  [0,1] denote the MT quality score and overall score of the sentence is: where  X   X  [0,1] is a parameter controlling the summary is extracted without considering the MT quality factor. In the experiments, we em-pirically set the parameter to 0.3 in order to bal-ance the two factors of content informativeness and translation quality. 
For multi-document summarization, some sen-tences are highly overlapping with each other, and thus we apply the same greedy algorithm in (Wan et al., 2006) to penalize the sentences highly overlapping with other highly scored sen-tences, and finally the informative, novel, and easy-to-translate sentences are chosen into the English summary. Finally, the sentences in the English summary are translated into the corresponding Chinese sentences by using Google Translate , and the Chinese summary is formed. 5.2 Evaluation 5.2.1 Evaluation Setup In this experiment, we used the document sets provided by DUC2001 for evaluation. As men-tioned in Section 4.2.1, DUC2001 provided 30 English document sets for generic multi-document summarization. The average document number per document set was 10. The sentences in each article have been separated and the sen-tence information has been stored into files. Ge-neric reference English summaries were pro-vided by NIST annotators for evaluation. In our study, we aimed to produce Chinese summaries for the English document sets. The summary length was limited to five sentences, i.e. each summary consisted of five sentences. 
The DUC2001 dataset was divided into the following two datasets: Ideal Dataset : We have manually labeled the MT quality scores for the sentences in five document sets (d04-d11), and we directly used the manually labeled scores in the summarization process. The ideal dataset contained these five document sets. 
Real Dataset : The MT quality scores for the sentences in the remaining 25 document sets were automatically predicted by using the learned SVM regression model. And we used the automatically predicted scores in the summariza-tion process. The real dataset contained these 25 document sets. We performed two evaluation procedures: one demonstrate the effectiveness of the proposed approach in real applications. been developed for English summary evaluation by comparing system summary with reference summary, such as the pyramid method (Nenkova et al., 2007) and the ROUGE metrics (Lin and Hovy, 2003). However, such methods or metrics cannot be directly used for evaluating Chinese summary without reference Chinese summary. Instead, we developed an evaluation protocol as follows: The evaluation was based on human scoring. Four Chinese college students participated in the evaluation as subjects. We have developed a friendly tool for helping the subjects to evaluate each Chinese summary from the following three aspects: 
Content : This aspect indicates how much a summary reflects the major content of the docu-ment set. After reading a summary, each user can select a score between 1 and 5 for the summary. 1 means  X  X ery uninformative X  and 5 means  X  X ery informative X . 
Readability : This aspect indicates the read-ability level of the whole summary. After reading a summary, each user can select a score between 1 and 5 for the summary. 1 means  X  X ard to read X , and 5 means  X  X asy to read X . 
Overall : This aspect indicates the overall quality of a summary. After reading a summary, each user can select a score between 1 and 5 for the summary. 1 means  X  X ery bad X , and 5 means  X  X ery good X . 
We performed the evaluation procedures on the ideal dataset and the read dataset, separately. During each evaluation procedure, we compared our proposed approach (  X  =0.3) with the baseline approach without considering the MT quality factor (  X  =0). And the two summaries produced by the two systems for the same document set were presented in the same interface, and then the four subjects assigned scores to each sum-mary after they read and compared the two summaries. And the assigned scores were finally averaged across the documents sets and across the subjects. 5.2.2 Evaluation Results Table 2 shows the evalua tion results on the ideal dataset with 5 document sets. We can see that based on the manually labeled MT quality scores, the Chinese summaries produced by our pro-posed approach are significantly better than that produced by the baseline approach over all three aspects. All subjects agree that our proposed ap-proach can produce more informative and easy-to-read Chinese summaries than the baseline ap-proach. 
Table 3 shows the evaluation results on the real dataset with 25 document sets. We can see that based on the automatically predicted MT quality scores, the Chinese summaries produced by our proposed approach are significantly better than that produced by the baseline approach over the readability aspect and the overall aspect. Al-most all subjects agree that our proposed ap-proach can produce more easy-to-read and high-quality Chinese summaries than the baseline ap-proach. 
Comparing the evaluation results in the two tables, we can find that the performance differ-ence between the two approaches on the ideal dataset is bigger than that on the real dataset, es-pecially on the content aspect. The results dem-onstrate that the more accurate the MT quality scores are, the more significant the performance improvement is. Overall, the proposed approach is effective to produce good-quality Chinese summaries for English document sets. 5.2.3 Example Analysis In this section, we give two running examples to better show the effectiveness of our proposed approach. The Chinese sentences and the original English sentences in the summary are presented together. The normalized MT quality score for each sentence is also given at the end of the Chi-nese sentence. Document set 1: D04 from the ideal dataset Summary by baseline approach : Summary by proposed approach : Document set 2: D54 from the real dataset Summary by baseline approach : Summary by proposed approach : In this study, we adopt the late translation strat-egy for cross-document summarization. As men-tioned earlier, the late translation strategy has some advantages over the early translation strat-egy. However, in the early translation strategy, we can use the features derived from both the source English sentence and the target Chinese sentence to improve the MT quality prediction results. 
Overall, the framework of our proposed ap-proach can be easily adapted for cross-document summarization with the early translation strategy. And an empirical comparison between the two strategies is left as our future work. Though this study focuses on English-to-Chinese document summarization, cross-language summarization tasks for other lan-guages can also be solved by using our proposed approach. In this study we propose a novel approach to ad-dress the cross-language document summariza-tion task. Our proposed approach predicts the MT quality score of each English sentence and then incorporates the score into the summariza-tion process. The user study results verify the effectiveness of the approach. In future work, we will manually translate English reference summaries into Chinese refer-ence summaries, and then adopt the ROUGE metrics to perform automatic evaluation of the extracted Chinese summari es by comparing them with the Chinese reference summaries. Moreover, we will further improve the sentence X  X  MT qual-ity by using sentence compression or sentence reduction techniques. This work was supported by NSFC (60873155), Beijing Nova Program (2008B03), NCET (NCET-08-0006), RFDP (20070001059) and (2008AA01Z421). We thank the students for participating in the user study. We also thank the anonymous reviewers for their useful comments. 
