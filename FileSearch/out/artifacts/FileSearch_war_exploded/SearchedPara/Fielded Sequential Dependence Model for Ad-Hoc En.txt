 Previously proposed approaches to ad-hoc entity retrieval in the Web of Data (ERWD) used multi-fielded representa-tion of entities and relied on standard unigram bag-of-words retrieval models. Although retrieval models incorporating term dependencies have been shown to be significantly more effective than the unigram bag-of-words ones for ad hoc doc-ument retrieval, it is not known whether accounting for term dependencies can improve retrieval from the Web of Data. In this work, we propose a novel retrieval model that in-corporates term dependencies into structured document re-trieval and apply it to the task of ERWD. In the proposed model, the document field weights and the relative impor-tance of unigrams and bigrams are optimized with respect to the target retrieval metric using a learning-to-rank method. Experiments on a publicly available benchmark indicate sig-nificant improvement of the accuracy of retrieval results by the proposed model over state-of-the-art retrieval models for ERWD.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Term Dependence; Entity Retrieval; Knowledge Graphs The past decade has witnessed the emergence of numer-ous large-scale publicly available knowledge bases, such as  X  Freebase 1 , DBpedia 2 , Wikidata 3 and YAGO 4 . Open source knowledge bases typically adopt Resource Description Frame-work (RDF) data model and are published as part of the Linked Open Data (LOD) 5 cloud commonly referred to as the Web of Data. A similar trend exists in the industry as well (e.g. Google X  X  Knowledge Graph, Facebook X  X  Open Graph and Microsoft X  X  Satori). Individual RDF datasets in the Web of Data can be considered as massive graphs, in which the nodes are the entities (resources) and the edges are semantic relations between the entities. Each resource describes an object in the Web of Data, which can either be a real entity (e.g. Albert Einstein or Apple, Inc. ) or an ab-stract concept ( special relativity ). The relations between the entities are represented as subject-predicate-object triples (e.g. &lt; Albert Einstein, knownFor , special relativity &gt; ).
Graph structured knowledge in general and RDF graphs in particular are well-suited for addressing the information needs that aim at finding specific entities, for which the top results returned by the search systems should consist of structured objects rather than individual documents. The analysis of Web search engine logs reported in [26] revealed that such information needs constitute more than half of search engine queries. They also proposed to classify entity-oriented queries into five classes, each of which requires dif-ferent treatment by the search systems. The demand for efficient access to knowledge graphs gives rise to the task of Ad-hoc Entity Retrieval from the Web of Data (ERWD). As opposed to the traditional information retrieval task, in which search systems return documents in response to key-word queries, the goal of ERWD is to return an entity or a list of entities based on their unconstrained (and often fairly long) keyword descriptions. This task is focused on retrieval from knowledge bases, as opposed to utilization of knowledge bases in retrieval [7, 15]. Although entities have been studied in textual data mining (e.g. [16]), ERWD is a fairly recent trend in IR. It is related to entity retrieval in the context of expert finding, for which different unigram language model based approaches have been proposed (e.g. [1]). ERWD, http://freebase.com http://dbpedia.org http://wikidata.org http://www.mpi-inf.mpg.de/yago-naga/yago/ http://lod-cloud.net/ however, provides its own unique set of challenges, such as d esigning effective entity representation methods and novel retrieval models.

While several methods [4, 21, 24] to address the first chal-lenge have been previously proposed, only standard bag-of-words retrieval models, such as BM25 [2, 31] and language modeling (LM) based ones [8, 9, 22], as well as their multi-fielded extensions [4, 5, 20, 21, 24] have been applied to the task of entity retrieval from the Web of Data. It is known, however, that retrieval models incorporating term dependen-cies (in the form of ordered and unordered n-grams) are sub-stantially more accurate than standard bag-of-words mod-els in case of ad-hoc document retrieval [12], particularly for longer, verbose queries [3]. Markov Random Field (MRF) retrieval model [18, 19] provided a theoretical foundation to account for bigrams, as well as ordered and unordered phrases in IR, by representing a query as a graph of depen-dencies between the query terms. MRF calculates the score of each document with respect to a query as a linear combina-tion of potential functions, each of which is computed based on a document and a clique in the query graph. Sequen-tial Dependence Model (SDM) is a variant of the Markov Random Field model that assumes sequential dependence for the query terms and uses three potential functions: one that is based on unigrams and the other two that are based on bigrams, either as ordered sequences of terms or as terms co-occurring within a window of the pre-defined size.
To address the second challenge, the problem of design-ing effective retrieval models for ERWD, we propose Fielded Sequential Dependence Model (FSDM), which unifies and generalizes sequential dependence [18] and the mixture of language models (MLM) [23] retrieval models and allows to account for term dependencies in structured document re-trieval. Although, in this work, we only experimented with entity retrieval from the knowledge graphs, our proposed model can be utilized for retrieval from collections of any structured documents.

The key contributions of this work are two-fold: 1. novel retrieval model for collections of structured doc-2. novel multi-fielded entity representation scheme for ERWD
The remainder of the paper is organized as follows. Sec-tion 2 provides a brief overview of previous related work. In Section 3, we present FSDM, an algorithm to optimize its parameters, and a method to construct multi-fielded en-tity representations. Experimental results are reported in Section 4 and Section 5 concludes the paper. The retrieval model proposed in this paper builds upon the previous work along the following research directions. Entity retrieval . The task of ERWD as answering arbi-trary information needs expressed as keyword queries that aim at finding nodes or aspects of nodes in RDF graphs was first introduced by Pound et al. [26]. They also proposed to classify such queries into five categories: entity queries, type queries (i.e. list search queries or telegraphic queries), attribute queries, relation queries, and other queries. Al-though multiple open entity retrieval evaluation campaigns providing test collections and query sets covering a variety of information needs and relevance judgments have been con-ducted [10], most of the previous work along this direction focused on one particular query type. For example, the Sem-Search Entity Search challenge 6 focused on finding one par-ticular entity described by a keyword query. Previous suc-cessful retrieval approaches for queries of this type involved construction of multi-fielded entity representations by group-ing entity attributes together by type [24], into title and con-tent [21], according to manually determined importance [4] or into a two-level hierarchy [20]. Several methods adopted a two-stage approach, in which the initial retrieval results obtained using standard bag-of-words retrieval models were first expanded using relations in the knowledge graph and then the expanded results were re-ranked based on entity similarity. Tonon et al. [31] used Jaro-Winkler similarity between the entity names, while Herzig et al. [11] used Jenson-Shannon divergence between the language models of entities. Zhiltsov and Agichtein [33] proposed a learning-to-rank based approach, which combines explicit entity in-formation with semantic similarity between the entities in latent space determined using a modified algorithm for ten-sor factorization.

The INEX 2009 Entity Ranking (INEX-XER) 7 track intro-duced an entity list completion task and provided an evalua-tion platform for type queries. The TREC 2009 Entity track focused on two related retrieval tasks  X  X elated entity find-ing X  (i.e. finding all entities related to a given entity query) and  X  X ntity list completion X  (i.e. finding entities with com-mon properties given some examples). The List Search track from the SemSearch challenge targets a group of entities that match a keyword query. Bron et al. [5] proposed a hybrid approach that linearly combines the scores of the mixture of language models and a structure-based method, which captures the statistics of predicate-object pairs in triples shared by entity candidates. Elbassouni et al. [9] focused on the case of structured queries consisting of RDF triples and proposed a method that constructs LMs (as multino-mial distributions over RDF triples) for both the queries and each possible result sub-graph. The ranking is based on the Kullback-Leibler divergence between the query LM and the LMs of each result sub-graph. The SemSets model [6] utilizes the relevance of entities to automatically constructed categories (semantic sets, SemSets) measured according to structural and textual similarity. Their approach combines a retrieval model with the methods for spreading activation over the link structure of a knowledge graph and evaluation of membership in semantic sets.

Question Answering over Linked Data (QALD) 8 evalua-tion campaigns aim at developing retrieval methods to an-swer sophisticated question-like queries. The majority of queries are natural language questions that are focused on finding one particular entity (or several entities) as exact an-swers to these questions. A method based on integer linear http://km.aifb.kit.edu/ws/sem search\{10|11\} http://www.l3s.de/~demartini/XER09/ http://www.sc.cit-ec.uni-bielefeld.de/qald/ programming for joint segmentation and disambiguation of qu estion-like queries was proposed in [32]. Shekarpour et al. [29] applied the Hidden Markov Model to map question terms into entities and relations in the knowledge graph and translated keyword queries into structured SPARQL queries. For comprehensive overview of approaches to entity retrieval, we refer an interested reader to the recent surveys [17] [27]. Multi-fielded retrieval models. Multi-fielded extensions of different bag-of-words retrieval models have been pro-posed for structured document retrieval. BM25F [28] al-lows to either combine the BM25 retrieval scores of indi-vidual document fields with different weights into the final document retrieval score or calculate the aggregated basic retrieval statistics, such as TF and field lengths across mul-tiple fields and use them in the original retrieval formula. Mixture of Language Models (MLM) [23] is a multi-fielded extension of the query likelihood retrieval model [25], a stan-dard language modeling based retrieval model. In MLM, the retrieval score of a structured document is a linear combina-tion of probabilities of query terms in the language models calculated for each document field. Although individual field weights in BM25F and MLM can be tuned for a particular collection, they are fixed across different query terms. To overcome this limitation, Probabilistic Retrieval Model for Semistructured Data (PRMS) [14] maps each query term into document fields using probabilistic classification based on collection statistics. Although PRMS was originally pro-posed for XML retrieval, it was later applied to ERWD [2]. Field Relevance Model (FRM) [13] is an extension of PRMS to the case of relevance feedback.
 Incorporating term dependencies into retrieval mod-els. Metzler and Croft proposed the MRF retrieval model, which based on the assumption about the dependencies be-tween the query terms, has three variants: full independence (FI), sequential dependence (SD) and full dependence (FD) models. Bendersky et al. [3] extended SDM to allow learning optimal weights for unigrams and bigrams as a linear combi-nation of features that are based on internal and external col-lection statistics. Huston and Croft [12] systematically eval-uated a large number of bigram dependence models across short and long queries and concluded that retrieval mod-els incorporating term dependencies consistently improve retrieval accuracy over the standard bag-of-words retrieval models. They also experimentally demonstrated that the performance of term dependence models can be significantly improved through parameter tuning. In this section, we introduce the Fielded Sequential Depen-dence Model (FSDM), a novel model for structured docu-ment retrieval, and describe our strategy for building multi-fielded entity description documents. One of the limitations of standard SDM for structured doc-ument retrieval is that it considers term matches in different parts of a document as equally important (i.e. having the same contribution to the final retrieval score of a document), thus disregarding the document structure. For example, in case of unigrams, the potential function looks as follows: w here q i is a query term, D is a document, tf q i ,D is the fre-quency of q i in D , | D | is the document length,  X  is a Dirichlet prior, that is usually set to the average document length in the collection, cf q i is the collection frequency of q i is the total number of terms in the collection. To adapt the MRF framework to multi-fielded entity descriptions, we pro-pose to replace a single document language model P ( q i |  X  with a mixture of language models (MLM) for each docu-ment field. Hence, our model is an extension of sequential dependence model, although the same transformation can be applied to the full dependence model.

Consequently, the potential function for unigrams in case of FSDM is:  X  f ( q i ,D ) = log X w here j = 1 , F , F is the number of fields,  X  j D is a language model of field j smoothed using its own Dirichlet prior  X  and w j are the field weights with the following constraints: P j w j = 1 ,w j &gt; 0; tf q i ,D j is the term frequency of q field j of document D ; cf j q in field j ; | C j | is the total number of terms in field j across all the documents in the collection and | D j | is the length of field j in D .

The potential functions for ordered and unordered bigrams q i,i +1 = ( q i ,q i +1 ) in the query are defined as follows:  X  f  X  f in field j of document D , cf j #1( q quency of exact phrase q i q i +1 in field j , tf # uw 8( q the number of times terms q i and q i +1 occur together within a window of 8 word positions in field j of document D , re-gardless of the order of these terms.

Having separate mixtures of language models with differ-ent weights for unigrams as well as ordered and unordered bi-grams gives FSDM the flexibility to adjust the multi-fielded document scoring strategy for matching query terms and phrases depending on the query type (entity, type, relation, etc.). Intuitively, an FSDM field weighting scheme, in which unordered bigram matches in the descriptive fields of en-tity documents are given higher weight than the matches in the name field, should be more effective for informational queries, while giving higher weight to ordered bigram matches in the name field can be beneficial for navigational queries. For example, precision of retrieval results for an informa-tional query SemSearch LS-1  X  apollo astronauts who walked on the moon X  is likely to increase when more importance is given to the matches of unordered bigrams apollo astronauts and walked moon within a window inside the descriptive fields of entity documents, rather than the name field, while giving higher weights to the matches of the same unordered bigrams in the name field is likely to have the opposite effect.
Substituting the potential functions  X  ( q i ,D ;  X ) and the document and query terms in the MRF model completes t he transformation of SDM into FSDM: P
G,  X  ( Q,D ) =
Note that for the purpose of ranking, it is sufficient to compute the following posterior probability:
C onsequently, FSDM is a term dependence retrieval model with the following ranking function:
It is easy to see from Equation 1 that MLM is a special case of FSDM, when  X  T = 1 , X  O = 0 , X  U = 0 . Overall, FSDM has 3  X  F +3 free parameters: h w T , w O , w  X  i . Before introducing the parameter estimation procedure, we would like to emphasize two properties of the ranking function of FSDM. The first property is linearity with re-spect to  X  . We therefore can apply any linear learning-to-rank algorithm to optimize the ranking function with respect to  X  . The second property is linearity with respect to w of the arguments of monotonic  X  f (  X  ) functions. Therefore, opti-mization of the arguments as linear functions with respect to w , leads to optimization of each function  X  f (  X  ). Based on the above properties, we propose a two-stage Algorithm 1 to optimize the free parameters of FSDM with respect to the target retrieval metric, which in our case is Mean Average Precision (MAP).
 Algorithm 1 A n algorithm for training FSDM parameters. 1: Q  X  T rain queries 2: for s  X  X  T,O,U } do 3:  X  = e s 4:  X  w s  X  CA ( Q,  X  ) 5: end for 6:  X   X   X  CA ( Q,  X  w T ,  X  w O ,  X  w U )
In the first stage (lines 2-5), the algorithm independently e stimates the optimal values of MLM parameters for uni-grams, ordered and unordered bigrams. The unit vectors e
T = (1 , 0 , 0), e O = (0 , 1 , 0), e U = (0 , 0 , 1) are the cor-responding settings of the parameters  X  in the formula of FSDM ranking function (Equation 1). Because of linearity of MLM ranking function with respect to w , we make use of the coordinate ascent (CA) algorithm, proposed in [19], to directly optimize MAP. The starting values for each w s are uniform, i.e., equal to 1 F e ach. Therefore, CA itera-tively optimizes MAP by performing a series of line searches Table 1: Proposed scheme for multi-fielded represen-tation of entity e .
 along one coordinate direction under the constraint of non-n egativity of w s . It repeatedly optimizes each of the param-eters w s 1 ,...,w s F , while holding all other parameters fixed. Since MLMs are optimized independently, this stage can be easily parallelized to speed-up the training process.
In the second stage (line 6), the algorithm optimizes the parameters  X  in Equation 1 on the same query set, given the optimal values of  X  w T ,  X  w O ,  X  w U . Again, since the right-hand side of Equation 1 is a linear function with respect to  X  , the CA algorithm can be applied to maximize the target metric directly. It starts with  X  = (1 , 0 , 0), which is equivalent to MLM ranking model, and iterates until the gain in MAP is less than a given threshold. To avoid the changes of signs of the document ranking scores that may distort the ranking, non-negativity constraints on  X  are enforced during CA. Since multi-fielded entity representation proved to be ben-eficial for ERWD [21, 24], we propose a novel five-field en-tity representation scheme (Table 1). Attribute values, that satisfy the provided condition, are aggregated into the cor-responding field.

The first two fields are the properties of the entities them-selves. Other fields capture information from different enti-ties that are related to the given entity. The names field contains conventional names of the entities, such as the name of a person or the name of an organization. Some of the predicates that can be used for this purpose are la-bel from RDF Schema 9 , name from FOAF Ontology 10 , or /type/object/name from Freebase 11 . The attributes field in-cludes all datatype properties, other than names. The ex-amples are values of abstract and foundingDate predicates from DBpedia. We found it to be helpful to include predi-cate names along with the predicate values, e.g.  X  X ounding date 1964 X .

The next three fields are extracted from the names field of related entities. The categories field combines classes or groups, to which the entity can be assigned. In DBpedia, the membership of entities in classes is represented using the subject property, which corresponds to the categories of a Wikipedia article. The profession and ethnicity attributes for people or the industry attribute for companies in Free-base have close semantics. The similar entity names field aggregates the names of the entities that are very similar or identical to a given entity, since it is often the case that http://www.w3.org/TR/rdf-schema/ http://www.foaf-project.org/ http://freebase.com average field weights (b) bigrams averaged over 5 folds for each query set. different knowledge bases (or even the same one) can refer to the same entity with different resource identifiers or de-scribe it according to different schemata. In DBpedia, these values can be obtained with S 1 1 SPARQL query from [31] t hat takes into account sameAs property from OWL schema along with redirect and disambiguates from DBpedia. Fi-nally, the related entity names field includes the names of the entities that are part of the same RDF triple with a given entity along with the predicate names, e.g.  X  X pouse michelle obama X . Table 2 provides an example of the entity document created using the above approach. We compare the effectiveness of FSDM with existing entity retrieval models based on a publicly available benchmark 12 [2], which combines 485 queries and the corresponding rel-evance judgments from the past entity retrieval evaluation campaigns. The knowledge graph used in our evaluation is DBPedia 3.7 13 . DBPedia is a structured version of on-line http://bit.ly/dbpedia-entity http://wiki.dbpedia.org/Downloads37 encyclopedia Wikipedia, which provides the descriptions of over 3.5 million entities belonging to 320 classes. Table 3: Statistics of the query sets used for evalua-tion.

We report the retrieval results obtained on the query sets i n Table 3:
In total, there are 13,090 available positive relevance judg-ments. Although some query sets (e.g. SemSearch) contain graded relevance judgments, for the purpose of consistency, we treat all relevance judgments as binary and report only the corresponding retrieval metrics.

For all experiments in this work, we used the multi-fielded entity descriptions constructed from DBpedia RDF graph according to the method presented in Section 3.3. Indexed terms were lower-cased, filtered using the INQUERY sto-plist, and stemmed using the Krovetz stemmer. All retrieval models used in the experiments reported in this work were implemented using the Galago Search Engine 1415 . Parame-ters of retrieval models were optimized with respect to the Mean Average Precision (MAP) using the Galago X  X  imple-mentation of the coordinate ascent learner based on 5-fold cross validation. All reported evaluation metrics were macro-averaged over 5 folds. In this section, we discuss the details of optimization proce-dure for the field weights w and SDM/FSDM parameters  X  , and provide our interpretation of the learned values.
To optimize the field weights in FSDM, we uniformly ini-tialize them and run the CA algorithm under the sum nor-malization and non-negativity constraints with 5 random restarts. We do not optimize the Dirichlet priors  X  j in lan-guage models and set them equal to the average (document or field) lengths, respectively. For optimizing the vectors of SDM parameters  X  T , X  O , X  U , we initialize them to (1 , 0 , 0) and run the CA algorithm with 3 random restarts. The un-ordered window size was set to 8 in all cases, as suggested in previous work [3, 18].
 We begin our analysis with the weights for unigram MLMs. Figure 1a depicts the distribution of the learned field weights averaged over all folds for each query set. Interestingly, while fields have similar relative importance across most query sets, some query sets (i.e. query types) are clear out-liers. We observe that, compared to other query types, the optimized model strongly favors names and similar entity names fields for named entity queries (SemSearch ES query set). The categories field is important for type queries (List-Search), while on QALD-2 query set, FSDM puts particu-larly strong emphasis on related entity names , which can be explained by the fact that this query set mostly consists of relation queries, which require capturing the related entity context. INEX-LD query set includes a large number of at-tribute queries that aim at finding entities primarily through their attributes instead of names. Thus, we observe that the model assigns higher weights to the attributes field. http://www.lemurproject.org/g alago.php source code for all retrieval models used in this work as well as run files is available at https://github.com/teanalab/ FieldedSDM
Since FSDM includes bigram language models, next we in-vestigate the field weights in MLMs for ordered (Figure 1b) and unordered (Figure 1c) bigrams. In particular, we ob-serve that higher weights are assigned to names and simi-lar entity names fields for entity queries, for which the or-dered bigram matchings in these fields is an important rele-vance factor. Another difference of bigram from unigram MLMs is the increased importance of categories field for other (non-entity) types of queries. This can be explained by the fact that bigrams are more effective in matching class names of entities than unigrams. For example, for the query QALD2 tr-89  X  give me all soccer clubs in the premier league X  , an English football club Bolton Wanderers F.C. , which is a relevant entity, is assigned to a few DBpedia cat-egories, including Premier League clubs . Given this query intent, scoring entities based on matching bigrams club pre-mier and premier league in the categories field is much more effective than scoring based on matching unigrams, since premier league is an important specification for the concept soccer club and should be considered holistically, as a phrase.
From the perspective of modeling entity descriptions, we can conclude that the attributes field is consistently consid-ered to be a very valuable context for queries of any type. The names field as well as the similar entity names field are highly important for queries aiming at finding named entities, while distinguishing categories from related entity names is particularly important for type queries.
Overall, these findings support our initial hypothesis that the field weights are dependent on query types. Next, we focus on the analysis of the learned weights  X  for SDM and FSDM, the distribution of which is depicted in Figures 2a and 2b. The optimal weights of SDM on all query sets, ex-cept SemSearch ES, are close to the standard scheme of (0.8, 0.1, 0.1), which was shown to be optimal in several previous works [3, 12, 18]. This discrepancy with SemSearch ES illus-trates the significance of bigram matches for named entity queries. The optimal weights of FSDM indicate increased importance of bigram matches on every query set, especially on QALD-2. It follows that transformation of SDM into FSDM increases the importance of bigram matches, which ultimately improves the retrieval performance, as we will demonstrate next. In this section, we present the results of comparing FSDM with the state-of-the-art models for ERWD. In particular, we use MLM-CA (unigram MLM optimized with respect to the field weights w j by CA) and SDM-CA (SDM optimized with respect to the  X  T ,  X  O ,  X  U weights by CA) as the base-lines. We also used MLM as a baseline to test whether incor-porating term dependencies leads to improvement of entity retrieval. SDM leverages only unstructured entity descrip-tions, in which all fields are merged into a single document. The difference with SDM is measured to show the impor-tance of fielded document representation for ERWD. Addi-tionally, we report the results of the PRMS model using our entity descriptions, since it has been previously shown to have good performance in case of two-fielded entity repre-sentations. Finally, we include the results recomputed from the run files of the methods used for evaluation in [2]. Table 4 summarizes the retrieval results of all models on SemSearch ES, ListSearch, INEX-LD, QALD-2 query sets, and the entire query set. MAP and a preference-based mea-sure (b-pref) are calculated at the top 100 results. As fol-lows from Table 4, the SDM-CA and MLM-CA baselines (optimized SDM and MLM) both outperform previously pro-posed models on the entire query set, most significantly on QALD-2 and ListSearch query sets. However, the perfor-mance of SDM remarkably drops on SemSearch ES query set. To make sure that SDM-CA is not overfit, we run SDM us-ing a standard weighting scheme (0.8, 0.1, 0.1) and got very close results with respect to MAP  X  0.258 on SemSearch ES, 0.196 on ListSearch, 0.114 on INEX-LD, 0.186 on QALD-2, and 0.193 on the query set including all queries. The results of PRMS are significantly worse compared to MLM in our settings, which indicates that the performance of this model degrades in case of a large number of fields in entity descriptions. Bag-of-words models, such as BM25 and LM, achieve comparable results with structured document retrieval models on the more heterogeneous INEX-LD query set, which includes the queries of different types. over MLM-CA/SDM-CA is shown in parenthesis.
 We also observe that FSDM significantly outperforms the M LM-CA baseline on all query sets and all metrics, except ListSearch. FSDM also significantly outperforms SDM-CA on SemSearch ES and the entire query set with respect to all evaluation metrics. On the remaining query sets, FSDM is more effective than SDM in all but two cases (on INEX-LD with respect of MAP and on ListSearch with respect to b-pref; however, the difference in both cases is not significant), including statistically significant improvement on INEX-LD with respect to P@20 and on QALD-2 with respect to P@10. In this section, we present the results of quantitative and qualitative analysis of errors in the search results. First, Figure 3 illustrates the per-topic differences in average pre-cision between FSDM and SDM. From Figure 3, it follows that, on the entire query set, FSDM performs better than SDM on a larger number of topics than vice versa, with the most significant difference on SemSearch ES query set. QALD-2 has the largest number of queries with no perfor-mance differences, since both FSDM and SDM fail to find any relevant results for 28 out of 140 queries from this fairly difficult query set. Figure 4: R obustness of SDM and FSDM methods with respect to the MLM-CA baseline over the en-tire query set.

As the next experiment, we analyze the robustness of SDM and FSDM compared to the MLM-CA baseline on the entire query set. Figure 4 shows the histogram for various ranges of relative decreases and increases in MAP with respect to MLM-CA and indicates that FSDM is more robust com-pared to SDM. In particular, it improves the performance of 50% of the queries with respect to MLM-CA, compared to 45% of the queries improved by SDM. At the same time, FSDM decreases the performance of only 26% of the queries, while SDM degrades the performance of 40% of the queries.
Next, we focus on a more detailed qualitative analysis of queries showing the highest relative gain in terms of MAP. In particular, we observe that a common mistake of SDM on named entity queries is overestimation of importance of matches in the fields other than names . For example, for the query SemSearch ES-22  X  city of charlotte X  , SDM mistakenly promotes the entities, such as Anthony Foxx , a former mayor of Charlotte, or Clayton Heafner , an American golfer, who lived in this city. Therefore, using SDM for type queries may result in a topic drift and decreased precision. For ex-ample, for the query QALD2 tr-89  X  give me all soccer clubs in the premier league X  , SDM ranks higher the soccer clubs from the premier leagues in Tasmania (Northern Rangers) and New Zealand (Metro F.C.). Similar effect is observed for the query INEX XER-121  X  us presidents since 1960 X  , for which SDM promotes the entities with matched bigrams and unigrams in the fields of minor importance, such as List of people on stamps of Liberia , whereas FSDM correctly em-phasizes term matches in categories and ranks the correct results, such as Gerald Ford , Theodore Roosevelt , and Ronald Reagan at the top.

We also observed that a common cause of many FSDM fail-ures is neglecting the important query terms. For example, for the TREC Entity-9 query  X  members of the beaux arts trio X  , FSDM mistakenly promotes the entities Emmanuel Pontremoli and Pierre Carron , the members of the Acad  X emie des Beaux Arts, which is caused by matching bigrams mem-bers beaux and beaux arts in the categories field. However, the query term trio provides an important clue that the query intent is about Beaux Arts Trio, a famous piano trio, which FSDM is unable to pick up. Similar reasoning applies to the QALD2 tr-15 query  X  who created goofy X  , for which FSDM drifts to cartoons rather than information about the actual character creator, and the QALD2 te-90 query  X  where is the residence of the prime minister of spain? X  , for which FSDM promotes the Spanish prime ministers in retrieval re-sults, instead of the exact answer to the question. The reason of FSDM failure on another difficult query SemSearch LS-10  X  did nicole kidman have any siblings X  is slightly different. The most precise answer, Antonia Kid-man , who is the younger sister of the actress Nicole Kid-man, does not contain any occurrences of the query term sibling . In this case, SDM ranks higher a DBpedia disam-biguation page that mentions the right entity. At the same time, FSDM tends to return the movies starring Nicole Kid-man as the top results. We hypothesize that query expan-sion with synonyms can potentially resolve these issues, how-ever we leave verification of this hypothesis to future work. Table 5: Comparison of SDM and FSDM on queries of various difficulty (according to the number of pos-itive relevance judgments).  X   X   X  indicates statistical significance with respect to the Fisher X  X  randomiza-tion test (  X  = 0 . 05 ) [30].
 To complete the analysis of retrieval performance of FSDM, T able 5 compares the performance of SDM and FSDM on queries of various levels of difficulty. To obtain the results in Table 5, we grouped all the queries into three categories: d ifficult queries (that have 3 or less positive relevance judg-ments), medium queries (with 4 to 20, i.e., potential first two SERPs) and easy queries (that have more than 20 posi-tive relevance judgments associated with them). As a result, we obtained 141, 216, and 128 queries in each category, re-spectively. From Table 5 it follows that FSDM outperforms SDM in each query category with respect to all metrics in all the cases but one (P@10 for difficult queries), with the most significant difference in performance on medium and easy queries. We, therefore, conclude that creating sophisti-cated entity descriptions is not sufficient for answering diffi-cult queries in entity retrieval scenario and better capturing the semantics of query terms is required to further improve the precision of FSDM for difficult queries. This paper proposed Fielded Sequential Dependence Model, a novel retrieval model, which incorporates term dependen-cies into structured document retrieval, and a two-stage algo-rithm to directly optimize the parameters of this model with respect to the target retrieval metric. Although we only ex-perimented with ERWD, FSDM can be applied to retrieval from collections of structured documents of any type.
We demonstrated that having different field weighting sche-mes for unigrams and bigrams is effective for different types of queries in ad-hoc entity retrieval scenario. Experimental evaluation of FSDM on a standard publicly available bench-mark showed that it consistently and, in most cases, statis-tically significantly outperforms state-of-the-art structured and unstructured retrieval models for ERWD.
 This work was partially supported by the subsidy from the government of the Russian Federation to support the pro-gram of competitive growth of Kazan Federal University among world class academic centers and universities and by the Russian Foundation for Basic Research (grants # 15-07-08522, 15-47-02472).
