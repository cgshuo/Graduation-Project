 binary problems. In [1] Crammer described a unifying method (Section 2) for reduc-ing multiclass problem to multiple binary problems. 
Recently a robust Minimax classifier (Section 3) where the probability of correct classification of future data should be maximized has been provided [2]. No further The minimax problem can be interpreted geometrically as minimizing the maxi-also available. Section 4 presents new algorithm. In sectio n 5, we report the experimental results. Finally, section 6 presents conclusions. 
 X = . A multiclass classifier is a function : H  X   X  X  that maps an instance can be yielded. We denote the vector of pr edictions of these classifiers on an instance can perform the calculations in some high dimensional inner-product space Z using a through an inner-product function : ll K  X  X  \\ \ , which satisfies Mercer conditions introduce slack variables  X  , denote by ,, 1 good matrix M can be stated as the following optimization problem: the i th component which is equal to one, and let 1 be the vector whose components are all one. We can denote by , , 1 Finally, the classifier ( ) Hx can be written in terms of the variable  X  as: 
However solving optimization problem ( ) is time-consuming. In this paper our al-gorithm solves this optimization problem heuristically. Let x and y model data from each of two classes in a binary classification problem. b  X  \ which separates the two classes of points with maximal probability with respect to all distributions having same mean and covariance matrices. This is expressed as: 
In formulation (3) the term  X  is the minimal probability of correct classification of future data. 
Learning large margin classifiers has become an active research topic. However, this margin is defined in a  X  X ocal X  way. MPM considers data in a global fashion, while SVM actually discards the global informatio n of data including geometric information and the statistical trend of data occurrence. The following natural learning problems arise, 2. Given a set of h , find a matrix M which has small empirical loss. 3. Find both a matrix M and a set h which have small empirical loss. 
The previous methods have focused mostly on the first problem. Most of these and the learning algorithm. We mainly aim to solve the 3rd problem, however it is so hard to solve the designing problem not to mention finding a  X  X ood X  classifier and a wonderful output codes simultaneously by using common optimization methods. Therefore a heuristic algorithm has been proposed instead of solving the optimization problem (1) directly. We use probability output  X  in (3) of MPM to build a heuristic framework SWS (Strong-to-Weak-to-Strong), we generalize the notion of  X  X eak X  algorithm make it realizable to solve bot h problems with acceptable time-consuming and complexion. 
Recently a number of powerful kernel-based learning machines have been pro-posed. In KPCA, kernel serves as preproce ssing while in SVM kernel has an effect on classification in the middle process. There coul d be two stages for kernel to affect the result in our algorithm. The first is in the middle process as it behaves in SVM. The second is where algorithm transforms severa l weak classifiers to a strong classifier. 4.1 Strong-to-Weak Stage In the Strong-to-Weak stage, we transform  X  X  trong X  classifier to  X  X eak X  classifier by like large margin and geometric properties. optimization. 
On the other hand, our algorithm takes the geometric difference of classes into ac-count while other methods ignore the difference because MPM uses Mahalanobis distance that involves geometric informatio n. Therefore SWS preserve the character-because the algorithm only requires  X  X eak X  learning algorithms. 4.2 Weak-to-Strong Stage dimensional spaces and finally improve the performances. According to the classifica-more  X  X trong X  learning algorithm. 
We notice that the saddle point from optim ization problem (1) we are seeking is a minimum for the primal variables with respect to i  X  . We can get 
And 1 distribution obtained by the algorithm over the labels for each example. Then we can view i  X  as the difference between the former and the later. It is natural to say that an on the correct label i y . Further we can say that only the questionable points contribute to the learning process and regard them as  X  X ritical points X . We notice that one  X  X riti-utes to only one class. It is typically assumed that the set of labels has no underlying structure, however there exist lots of different relation among category in practice. It classes or classifiers. 
Unlike other methods, our algorithm implements implicit update in high dimen-merely occurs in final discrimination from (2): In this section we test our algorithm using one-against-rest method experimentally on six data sets from the repository at University of California. 1 Iris Letter Glass Segment Vowel 
Wine plays the results of using different iterative steps with polynomial kernel of degree 2 (in  X  X eak X  classifiers and in weak-to-strong stage). 
From Table1, we can say that our algorithm (SWS) is more efficient than others in most cases. Especially our algorithm achieves significant performances in Glass data set for the algorithm takes geometric informa tion into account. It is clear that experi-ments show that the algorithm is fast to compute and efficient due to its heuristic. algorithms with most datasets although using one-against-rest method. 
