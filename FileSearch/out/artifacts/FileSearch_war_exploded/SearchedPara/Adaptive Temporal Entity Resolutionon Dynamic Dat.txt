 Entity resolution is the process of identifying, matching, and merging records that correspond to the same entities from several databases [1]. The entities under consideration commonly refer to people, such as patients or customers. The databases to be matched often do not include unique entity identifiers. Therefore, the matching has to be based on the available attributes, such as names, addresses, and dates of birth. Entity resolution is employed in many domains, the most prominent being health, census statistics, national security, digital libraries, and deduplication of mailing lists [2 X 4].

While entity resolution has traditionally been applied in batch mode and on static databases, an increasingly common scenario in many application domains is the model of data streams [5], where query records (potentially from several sources) need to be matched with (and potentially inserted into) a database that contains records of known entities.

An example application of entity resolution in such a dynamic environment is the verification of identifying infor mation provided by customers at the time of a credit card or loan application. In many countries, the financial institutions where customers apply will send the customers X  identifying information to a central credit bureau. The responsibility of this bureau is to match the query to a pre-existing credit file to retrieve that customer X  X  credit history, and to determine that the customer is the person they cl aim to be [6]. The credit bureau receives a stream of query records that contain identifying information about people, and the bureau X  X  task is to match these records (in real-time) to a large database of known validated entities. In this application, accurate entity resolution is crucial to avoid inappropriate lending and prevent credit application fraud [6].
Temporal and dynamic aspects in the context of entity resolution have only been investigated in the past three years [7 X 11]. Most similar to our work is thetechniqueproposedbyLietal.[8,9 ] on linking temporal records. Their approach assumes that all records in a database have a time-stamp attached, as illustrated in Fig. 1. These time-stamps are used to calculate decay proba-bilities for combinations of individual attributes and time differences. Based on these probabilities the similarities between records (calculated using approxi-mate string comparison functions on individual attributes [2]) are adjusted. The agreement decay was defined by Li et al. [8] as the probability that two different entities, represented by two records with a time difference of  X t ,havethesame valueinanattribute.The disagreement decay was defined as the probability that the value in an attribute for a given entity changes over time  X t . Such a change occurs if, for example, a person moves to a new address.

In Fig. 1, for the  X  X ity X  attribute and  X t  X  [0 , 1) year, there are three pairs of records by the same entity where the attribute value has changed: (r3,r4) and (r6,r8) for e1 ,and (r5,r7) for e2 , and one pair where the value did not change: (r4,r6) . The disagreement probability for this attribute and  X t  X  [0 , 1) year is therefore 75%. As both entities live in  X  X ydney X  in 2006, we can calculate an agreement probability for this value. However, due to the sparseness of this data set, we cannot calculate agreement probabilities for other  X  X ity X  values.
Li et al. [8] calculated both agreement and disagreement probabilities such that they increase monotonically as  X t gets larger. However, as Fig. 2 shows, this is not necessarily the case. Their approach also assumes that the databases from which records are matched are static; that the decay probabilities are learned once in an off-line process using a supervised learning technique (learning dis-agreement probabilities is of linear complexity in the number of entities, while learning agreement probabilities is of quadratic complexity); and that these de-cay probabilities are independent of the frequencies of individual attribute values. The experiments conducted on a bibliographic data set showed that taking such temporal information into account can improve matching quality [8].

In contrast, our approach is aimed at facilitating an efficient adaptive calcu-lation of weights that are used to adjust similarities between records. While our approach to calculate disagreement probabilities is similar to Li et al. [8], we calculate agreement probabilities that incorporate the frequency distributions of the attribute values. This is similar to frequency-based weight adjustments as ap-plied in traditional probabilistic record linkage [4]. As an example, if two records have an agreeing surname value  X  X mith X , which is common in many English speaking countries, then it is more likely that they correspond to two different entities compared to two records that h ave the uncommon surname value  X  X i-jkstra X . As our experimental study on a large real-world database shows, taking these frequencies into account can le ad to improved matching accuracy.
Our contributions are (1) an adaptive matching approach for dynamic data-bases that contain temporal information; (2) an efficient temporal adjustment method that takes the frequencies of attribute values into account; and (3) an evaluation of our approach on both synthetic data (with well controlled charac-teristics) and a large real voter database from North Carolina in the USA. Most research in entity resolution over the past decade has focused on improving quality and scalability when matching databases. Several recent surveys provide overviews of the research field [2 X 4]. Besides the work by Li et al. [8, 9] on linking temporal records (described abo ve), several other recent approaches have investigated temporal or dynamic aspects in entity resolution.

Whang et al. [10] developed an approach to matching databases where match-ing rules can evolve over time, and where a c omplete re-run of an entity resolution process is not required when new data b ecome available. The assumption is that a user provides an initial set of matching rules and over time refines these rules. While the rules in this approach are evolving, no temporal information in the records that are matched is taken into account in the matching process.
Ioannou et al. [7] proposed an entity query system based on probabilistic en-tity databases, where records and their attributes are assigned probabilities of uncertainty that are incorporated into the matching process. These probabilities correspond to the confidence one has that an attribute value has been recorded correctly for an entity. During the matching process a dynamic index data struc-ture is maintained which contains sub-sets of entities that are connected through common attribute values. Related to this work, Christen et al. [12, 13] investi-gated techniques to facilitate the real-time matching of query records to a large static database by incorporating the similarities calculated between attribute values into a novel index data structure. Both of these approaches however do not consider temporal aspects of the databases that are matched.

Yakout et al. [11] developed a matching approach for transactional records that correspond to the behaviour of entities (such as shopping baskets of indi-viduals) over time, rather than the actua l entity records. Their approach converts the transactions of an entity into a behaviour matrix. Entities are then matched by calculating similarities between condens ed representations of their behaviour matrices. While temporal information i s used to generate sequences of transac-tions for an entity, this information is not used in the matching process.
Pal et al. [14] recently presented an approach to integrate Web data from different sources where temporal information is unreliable and unpredictable (such as updates of changes are missing or incomplete). The temporal aspects of updates of entities are modelled with a hidden semi-Markov process. Results on a diverse set of data, from Twitter to climate data, showed encouraging results. The focus of this work is however to compute the correct value of an entity X  X  attributes, not to identify and match records that refer to the same entity.
A large body of work has been conducted in the areas of stream data min-ing [5], where temporal decays are commonly used to give higher weights to more recent data, and temporal data min ing [15], where the aim is to discover patterns over time in temporal data. To summarise, while either temporal in-formation or dynamic data have been co nsidered in recent approaches to entity resolution, our approach is a first to consider both, with the aim to achieve an entity resolution approach that adapts itself to changing data characteristics. We assume a stream of query records q , which are to be matched (as detailed in Sect. 4) to a database R that contains records about known entities. We denote records in R with r i ,1  X  i  X  N , with N being the number of records in R . At any point in time N is fixed, but over time new records (such as matched query records) are added to R while the existing records are left unchanged. The records in R consist of attributes, r i .a j ,1  X  j  X  M , with M the number of attributes. Examples of such attributes include name and address details, phone numbers, or dates of birth. The records also contain a time-stamp r i .t ,andan entity identifier, r i .e . All records that correspond to the same entity are assumed to have the same unique value in r i .e . In a supervised setting, the r i .e are the true known entity identifiers, while in an unsupervised setting the values of r i .e are based on the match classification, as will be described further in Sect. 4.
Assume we want to calculate the similarity between query record q and record r in R . These two records have a difference in their time-stamps of  X t = | q.t  X  r .t | . For a single attribute a j , we classify the attribute to be agreeing if both records have the same attribute value ( q.a j = r i .a j ), or if the two attribute the similarity function used to compare values for attribute a j ,and s same a minimum similarity threshold. If, on the other hand, sim j ( q.a j ,r i .a j ) &lt;s same , then we classify the attribute to be disagreeing . Similarity values are assumed to be normalised, with sim j (  X  ,  X  ) = 0 for two attribute values that are completely different, and sim j (  X  ,  X  ) = 1 for two values that are the same. Similarity functions vary by attribute and may be domain specific. For string attributes, such as names, approximate string similarity functions are commonly used [2].
To take temporal aspects into accoun t, we need to consider the following events. We denote the event that q and r i actually refer to the same entity with S , and the event that they actually refer to two different entities with  X 
S . Furthermore, we denote the event that q and r i have an agreeing value in attribute a j with A j , and the event that they have a disagreeing value in attribute a j with  X  A j .

We now consider the following two probabilities. As discussed in Sect. 3.1 below, they are used to adjust the similarities sim j (  X  ,  X  ) according to the time difference  X t between records q and r i .
 is the probability that a query and a database record that actually refer to the same entity have an agreeing value in attribute a j over  X t (i.e. the value does not change). It holds that P ( A j , X t | S )=1  X  P (  X  A j , X t | S ). is the probability that a query and a database record that actually refer to two different entities have disagreeing ( i.e. different) values in attribute a j over  X t . Clearly, P (  X  A j , X t | X  S )=1  X  P ( A j , X t | X  S ).

The probability P ( A j , X t | S ) can be learned for individual attributes and dif-ferent  X t by counting the number of agreeing and disagreeing attribute values for pairs of records of the same entity that have a time difference of  X t .
Calculating the probability P (  X  A j , X t | X  S ) is more difficult because it requires the comparison of attribute values across records that have a time difference of  X t and where the records refer to different entities. Such an approach, which is of quadratic complexity in the number of entities, was employed by Li et at. [8, 9]. Our approach, described in Sect . 3.2, is aimed at efficiently calculating the two probabilities (1) and (2) in an adaptive fashion. First we present how these probabilities are used to adjust the similarities between records. 3.1 Adjusting Similarities between Records Assume the attributes of a query record q and a database record r i have been compared using a set of similarity functions s j = sim j ( q.a j ,r i .a j ), 1  X  j  X  M , such as approximate string comparison functions [2], with s j the similarity value calculated for attribute a j ,and M the number of compared attributes.
Without taking temporal effects into account, we use s j as an estimate of the probability that two attribute values are the same, and (1  X  s j ) as the probability they are different [8] (remember we assume 0  X  s j  X  1). In a temporal setting, we aim to assign weights to individual attributes that indicate their importance according to the likelihood of a change of value in this attribute, as discussed above. Following Li et al. [8], we use (3) to adjust and normalise similarities. where s j = sim j ( q.a j ,r i .a j )and  X t = | q.t  X  r i .t | . This equation is a heuristic that attempts to adjust the similarity in a qualitatively reasonable manner. We calculate the weights w j ( s j , X t ) using the minimum similarity threshold s same : For example, as can be seen from Fig. 2, almost nobody in the NC voter database has changed their given name even ov er long periods of time, compared to changes in address attributes (such as street, suburb and postcode). There-fore, agreeing given name values are a strong indicator in this database that two records refer to the same entity. On the other hand, a low similarity in an address attribute is a weak indicator that t wo records refer to different entities.
During the matching process a query r ecord is compared with one or more database records, and the resulting adjusted similarities sim ( q,r i ) as calculated with (3) are ranked. Details of this matc hing process are presented in Sect. 4. 3.2 Learning Agreement and Disagreement Probabilities In a dynamic setting, the aim is to efficiently learn the probabilities given in (1) and (2) in an adaptive way. The agreement probability P ( A j , X t | S )canbe learned from data if it is known which records correspond to the same entity. To facilitate an efficient calculation of this probability, we discretise the time differences  X t into equal sized intervals  X t k , of durations such as weeks, months, or years (depending on the overall expected time span in an application). We keep two arrays for each attribute a j , A j [  X t k ]and D j [  X t k ]. The first array, A j [  X t k ], keeps track of the number of times a value in attribute a j is agreeing for two records of the same entity that have a discretised time difference of  X t k , while D j [  X t k ] keeps track of the number of times the values in a j are disagreeing for two records of the same entity. Usi ng these two counters, we calculate: for  X t 1 ,..., X t max , with  X t max the maximum discretised time difference en-countered between two records of the same entity in R . The update of the counters A j [  X t k ]and D j [  X t k ], and calculating (4) for a certain discretised  X t k , requires a single increase of a counte r and one division for each query record that is matched to a database record, making this a very efficient approach.
It is possible that no pair of records of t he same entity with a certain discre-tised time difference of  X t k does occur in a data set, and therefore (4) cannot be calculated for this  X t k . To overcome this data sparsen ess issue, we also calculate the average value for P ( A j , X t | S )overall  X t k for each attribute a j , and use this average value in case P ( A j , X t k | S ) is not available for a certain  X t k .
To calculate P (  X  A j , X t | X  S ), we use the probability that two records that refer to two different entities have the same value v in attribute a j , which equals to P ( A j , X t | X  S )=1  X  P (  X  A j , X t | X  S ). This probability depends upon how frequently a certain value v occurs in an attribute. The probability P j ( v )that an entity has the value v in attribute a j can be estimated from the count of how few records in R might have the rare surname  X  X ijkstra X , and so the probability that two records from different entities both have this value is very low.
We keep an array V j for each attribute a j with a counter for each distinct value v of how many records in R have this value in a j .Wethencalculate P (  X  A record q in attribute q.a j has not previously occurred in R (and thus P j ( q.a j )= 0), we set P (  X  A j , X t | X  S )=1 . 0 for this value. Updating the counters V j and probabilities P j ( v ) requires one integer increment and one division per attribute. For a single query record, the calculat ions of both agreement and disagreement probabilities are thus of complexity O (1), making this approach very efficient.
The database record to which a query reco rd is matched determines the calcu-lation of P ( A j , X t k | S ). In a supervised setting, the true match status of (a subset of) record pairs is known, allowing the calculation of this probability based on these training pairs. For a query record q , if there is a previous record r i of the same entity in R ,wecalculate  X t = | q.t  X  r i .t | and which attributes agree or disagree for these two records using sim j ( q.a j ,r i .a j )and s same , and we update the appropriate counters in A j and D j and the corresponding probabilities.
If no training data are available, then for a query record the best matching database record, i.e. the record with the h ighest adjusted similarity according to (3), is assumed to be the true match. We can then calculate the time differ-ences and update the relevant counters and probabilities in the same way as in a supervised setting. Due to limited spa ce, we only consider the supervised case. Algorithm 1 provides an overview of the main steps of our matching process. The required input is a database R init of known entities. In a supervised setting, where (some of) the true entities are known, the entity identifiers r i .e in R init allow us to generate for an entity a chain of records sorted according to the time-stamps r i .t . Using these entity chains, the counters A j and D j are populated, and the initial values for the P ( A j , X t k | S ), 1  X  j  X  M are calculated. In an unsupervised setting, no such initial database is available, and thus R =  X  .
The other input parameters required are a set of similarity functions sim j (  X  ,  X  ), one per attribute a j used, and the two thresholds s same and s match .Theformer is used to decide if two attribute values are agreeing or not (as described in Sect. 3), while the latter is use to decide if a query record is classified as a match with a database record or not (lines 8 and 9 in Algo. 1).

The algorithm loops as long as query records q are to be matched. We as-sume the case where query records are inserted into the database once they are matched (line 10). Removing this line will mean that R and the counters in A j , D j and V j are not updated, and therefore our approach becomes non-adaptive (while still taking temporal aspects into account).

In line 3, a set of candidate records C is retrieved from R using an index tech-nique for entity resolution. For example, C might be all records from R that have the same postcode value as q . In our implementation, we employ a similarity-aware index technique that has shown to be efficient for real-time matching [13]. For each candidate record c  X  C , its time difference and similarities with q are calculated in line 5 and adjusted in line 6, as described in Sect. 3.1.
The candidate record with the highest adjusted similarity, c best , is identified in line 7 by finding the candidate record that has the maximum similarity with the query record q . If this similarity is above the minimum match similarity s match , then q is assigned the entity identifier of this best match (line 8). Otherwise q is classified as a new entity and given a new unique entity identifier number in line 9. In line 10, the query record q is inserted into the database R , and in line 11 the counters in A j [  X t ], D j [  X t ], and V j , as well as the relevant probabilities, are updated for all attributes a j . Finally, the entity identifier of q ispassedontothe application that requires this information, allowing the application to extract all records from R that refer to this entity. We evaluate our adaptive temporal entity resolution approach on both synthetic and real data. Synthetic data allows us to control the characteristics of the data, such as the number of records per en tity, and the number of modifications introduced [16]. We generated six data sets, each containing 100,000 records, with an average of 4, 8, or 16 records per entity, and either having a single modification per record (named  X  X lean X  data) or eight modifications per record (named  X  X irty X  data). Modifications introduced were both completely changed attribute values, as well as single character edits (like inserts, deletes and substitutions).
As real data we used the North Carolina (NC) voter registration database [17], which we downloaded every two months since October 2011 to build a compound temporal data set. This data set contains the names, addresses, and ages of more than 2.4 million voters, as well as their unique voter registration numbers. Each record has a time-stamp attached which corresponds to the date a voter originally registered, or when any of their details were changed. This data set therefore contains realistic temporal information about a large number of people. There are 111,354 individuals with two records, 2408 with three, and 39 with four records in this data set. An exploration of this data set has shown that many of the changes in the given name attribute are corrections of nicknames and small typographical mistakes, while changes in surnames and address attributes are mostly genuine changes that occur whe n people get married or move address.
We sorted all data sets according to their time-stamps, and split each into a training and test set such that around 90% of the second and following records of an entity (of those that had more than one record) were included in the training set. We compare our proposed adaptive temporal matching technique with a static matching approach that does not take temporal information into account, and with a simple temporal matching approach where an additional temporal similarity value is calculated for a record pair as sim t ( q.t,r i .t )= | q.t  X  r i .t |  X T  X T max being the difference between the time stamps of the earliest and latest record in R . Note that we cannot compare our approach to the temporal linkage method proposed by Li et al. [8, 9], because their method is only applicable on static databases, as was described in Sect . 1. We label the non-temporal matching approach with  X  X one X ; the above described temporal attribute approach with  X  X emp Attr X ; our proposed adaptive approach described in Sect. 4 as  X  X dapt X ; and with  X  X on Adapt X  a variation of this adaptive approach that calculates the probabilities P ( A j , X t k | S )and P j ( v ) only once at the end of the training phase, but that does not adjust these probabilities for each of the following query records (i.e. line 11 in Algo. 1 is not executed for query records in the test set).
In Table 1 and Fig. 3 we report matchi ng accuracy as the percentage of true matches correctly identified, i.e. if a candidate record c best in line 7 in Algo. 1 was a record of the same entity as the entity of query record q , c best .e = q.e . For the NC voter data set we additionally report the percentage of true matches that occurred in the ten top-ranked ca ndidate records. In a real-time query environment, returning the ten top-ranked results is a realistic assumption.
We implemented our approach using Python 2.7.3, and ran experiments on a server with 128 GBytes of memory and two 6-core CPUs running at 2.4 GHz. We used the Jaro-Winkler string comparison function [2] as sim j (  X  ,  X  )tocom-pare name and address attribute values. We ran four sets of experiments with peatability, the programs and synthetic data sets are available from the authors.
The results shown in Table 1 for the synthetic data sets indicate that all four matching approaches are sensitive to the choice of parameter settings, and if the data are clean or dirty. With data that contain a larger number of variations, identifying true matches becomes muc h more difficult, as would be expected. Matching also becomes harder with mor e records per entity, because more at-tributes will have their values changed over time. The proposed approach to adjust the similarities between records is able to improve matching quality most when data are dirty. Such data are likel y to occur in dynamic entity resolution applications, where it is not feasible to apply expensive data cleaning operations on query records prior to matching them to a database of entity records.
The experiments conducted on the NC voter database show quite different results (Fig. 3), with the proposed similarity adjustment approach outperforming the two baseline approaches. Taking top ten matches into account, our approach, which adjusts the similarities between records, is able to identify nearly twice as many true matches compared to the two baseline approaches. However, the adaptive approach does not perform as well as the non-adaptive approach. This is likely because the number of true matc hes compared to all query records is very small, which distorts the calculation of the probabilities in (4).
As Fig. 4 illustrates, our approach is very efficient, even on the large real database with over 2.4 million records. The time needed to adjust the similarity values between records, as done in (3), only adds around 10% to the total time required to match a query record, wh ile the time needed to update the coun-ters A j , D j ,and V j ,and P ( A j , X t k | S ), is negligible. This makes our approach applicable to adaptive entity resolution on dynamic databases. We have presented an approach to facilitate efficient adaptive entity resolu-tion on dynamic databases by adaptively calculating temporal agreement and disagreement probabilities that are used to adjust the similarities between records. As shown through experiments on both synthetic and real data, our temporal matching approach can lead to significantly improved matching quality.
Our plans for future work include to conduct experiments for unsupervised settings as discussed in Sect. 3.2 and run experiments on other data sets, to conduct an in-depth analysis of our proposed algorithm, and to extend our ap-proach to account for attribute and value dependencies. For example, when a person moves, most of their address related attribute values change, and the probability that a person moves also depends upon their age (young people are known to change their address more often than older people). We also plan to integrate our approach with traditional probabilistic record linkage [4], and we will investigate how to incorporate constraints, such as only a few people can live at a certain address at any one time.

