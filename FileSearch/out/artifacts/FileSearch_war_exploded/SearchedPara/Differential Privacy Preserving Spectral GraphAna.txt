 There have been attempts [1 X 3] to formalize notions of differential privacy in releasing aggregate information about a statistical database and the mechanism to providing privacy protection to participants of the databases. Differential pri-vacy [1] is a paradigm of post-processing the output of queries such that the inclusion or exclusion of a single individual from the data set make no statis-tical difference to the results found. Differential privacy is usually achieved by directly adding calibrated laplace noise on the output of the computation f . The calibrating process of this approach includes the calculation of the global sensitivity of the computation f that bounds the possible change in the compu-tation output over any two neighboring databases. The added noise is generated from a Laplace distribution with the scale parameter determined by the global sensitivity of f and the user-specified privacy threshold . This approach works well for traditional aggregate functions (often with low sensitivity values) over tabular data. In [4], McSherry and Talwar introduced a general mechanism with differential privacy that comes with guarantees about the quality of the output, even for functions that are not robust to additive noise. The idea is to sample from the distribution specified by the exponential mechanism distribution. This mechanism skews a base measure to the largest degree possible while ensuring differential privacy, focusing probability on the outputs of highest value.
In this paper, we focus on differential privacy preserving spectral graph anal-ysis. Spectral graph analysis deals with the analysis of the spectra (eigenvalues and eigenvector components) of the gra ph X  X  adjacency matrix or its variants. We develop two approaches to computing the -differential private spectra, the first k eigenvalues and the corresponding eigenvectors, from the input graph G .The first approach, denoted as LNPP , is based on the Laplace Mechanism [1] that calibrates Laplace noise on the eigenval ues and every entry of the eigenvectors based on their sensitivities. We derive the global sensitivities of both eigenvalues and eigenvectors based on the matrix perturbation theory [5]. Because the out-put eigenvectors after perturbation are no longer orthogonormal, we postprocess the output eigenvectors by using the state-of-the-art vector orthogonalization technique [6]. The second approach, denoted as SBMF , is based on the exponen-tial mechanism [4] and the properties of the matrix Bingham-von Mises-Fisher density for network data spectral analysis [7]. We prove that the Gibbs sampling procedure [7] achieves differential privacy. We conduct empirical evaluation on a real social network data and compare the two approaches in terms of utility preservation (the accuracy of spectra an d the accuracy of low rank approxima-tion) under the same differential privacy threshold. Our empirical evaluation results show that L NPP generally incurs smaller utility loss. 2.1 Differential Privacy We revisit the formal definition and the mechanism of differential privacy. For differential privacy, a databa se is treated as a collection of rows ,witheachrow corresponding to an individual record. Here we focus on how to compute graph statistics (eigen-pairs) from private network topology described as its adjacency matrix. We aim to ensure that the inclusion or exclusion of a link between two individuals from the graph make no statistical difference to the results found. Definition 1. (Differential Privacy [1]) A graph analyzing algorithm  X  that takes as input a graph G ,andoutputs  X  ( G ) , preserves -differential edge privacy if for all closed subsets S of the output space, and all pairs of neighboring graphs G and G from  X  ( G ) , where  X  ( G )= { G ( V,E ) | X  !( u,v )  X  Gbut ( u,v ) /  X  G } .
 A differentially private algorithm provides an assurance that the probability of a particular output is almost the same no matter whether any particular edge is included or not. A general method for computing an approximation to any function while preserving -differential privacy is given in [1]. This mechanism for achieving differential privacy computes the sum of the true answer and ran-dom noise generated from a Laplace distribution. The magnitude of the noise distribution is determined by the sensitivity of the computation and the privacy parameter specified by the data owner. The sensitivity of a computation bounds the possible change in the computation output over any two neighboring graphs (differing at most one link).
 Definition 2. (Global Sensitivity [1]) The global sensitivity of a function f : D  X  R d ( G  X  D ),in the analysis of a graph G, is Theorem 1. (The Laplace Mechanism [1]) An algorithm A takes as input a graph G, and some  X &gt; 0, a query Q with computing function f : D n  X  R d , and outputs where the Y i are drawn i.i.d from Lap ( GS f ( G ) / X  ) . The Algorithm satisfies -differential privacy.
 Another exponential mechanism was proposed to achieve differential privacy for diverse functions especially those with large sensitivities [4]. The exponential mechanism is driven by a score function q that maps a pair of input( G )and output( r )from D n  X  R d to a real valued score( q ( G,r )) which indicates the probability associated with the output. Given an input graph G , an output r  X  R d is returned such that q ( G,r ) is approximately maximized while guaranteeing differential privacy.
 Theorem 2. (The General Exponential Mechanism [4]) For any function q : ( D n  X  R d )  X  R , based on a query Q with computing function f : D n  X  R d ,and base measure  X  over R d , the algorithm  X  which takes as input a graph G and some  X &gt; 0 and outputs some r  X  R d is defined as
 X  q ( G ):= Choosing r with probability proportional to exp (  X q ( G,r ))  X  q ( G ) gives ( 2  X  X q )-differential privacy, where  X q is the largest possible difference in q when applied to two input graphs that differ only one link, for all r . Theorem 3. (Composition Theorem [2]) If we have n numbers of -differentially private mechanisms M 1 ,  X  X  X  ,M n , computed using graph G , then any composition of these mechanisms that yields a new mechanism M is n X  -differentially private. Differential privacy can extend to group privacy as well: changing a group of k edges in the data set induces a change of at most a multiplicative e k in the cor-responding output distribution. In this paper, we focus on the edge privacy. We can extend the algorithm to achieve the node privacy by using the composition theorem [2]. 2.2 Spectral Analysis of Network Topologies Agraph G can be represented as a symmetric adjacent matrix A n  X  n with A i,j = 1 if there is an edge between nodes i and j ,and A i,j = 0 otherwise. We denote the i -th largest eigenvalue of A by  X  i and the corresponding eigenvector by u i . The eigenvector u i is a n  X  1 column vector of length 1. The matrix A can be decomposed as One major application of the spectral decomposition is to approximate the graph data A by a low dimension subspace A k that captures the main information of the data, i.e., minimizes A  X  A k F . Given the top-k eigenvalues and corresponding eigenvectors, we have a rank-k approximation to A as where  X  k is a diagonal matrix with  X  ii =  X  i and U k =( u 1 ,.., u k ).
U k belongs to the Stiefel manifold. Denoted as  X  k,n , the Stiefel manifold is defined as the set of rank-kk  X  n orthonormal matrices. One of the commonly used probability distributions on the Stiefel manifold  X  k,n is called the matrix Bingham-von Mises-Fisher density (Definition 3).
 Definition 3. (The matrix Bingham-von Mises-Fisher density [7]) The proba-bility density of the matrix Bingham-von Mises-Fisher distribution is given by where C 1 and C 2 are assumed to be symmetric and diagonal matrices, repectively. The matrix Bingham-von Mises-Fisher de nsity arises as a posterior distribu-tion in latent factor models for multivariate and relational data. Recently, a Gibbs sampling scheme was developed for sampling from the matrix Bingham-von Mises-Fisher density with application of network spectral analysis [7] based on the latent factor model(Definition 4).
 Definition 4. (The latent factor model for network data [7]) The network data is represented with a binary matrix A so that A i,j is the 0-1 indicator of a link between nodes i and j . The latent factor model with a probit link for such network dataisdefinedas: where E is modeled as a symmetric matrix of independent normal noise,  X  is a diagonal matrix and U is an element of  X  k,n ,with k generally much smaller than n . Given a uniform prior distribution for U, we have which is a Bingham distribution with parameters C 1 = Z/ 2 , C 2 =  X  and C 3 =0 . Lemma 1. [7]A uniform prior distribution on eigenvectors U and independent normal(0,  X  2 ) prior distributions for the eigenvalues  X  give where  X  X ormal( u ,  X  2 ) X  denotes the normal density with mean u and variance  X  2 . The sampling scheme by Hoff [7] ensures Lemma 1 to approximate inferences for U and  X  for a given graph topology. As suggested in [7], the prior parameter  X  2 is usually chosen as the number of nodes n since this is roughly the variance of the eigenvalues of an n  X  n matrix of independent standard normal noise. In this section, we present two approaches to computing the -differential private spectra: LNPP , which is based on the Laplace Mechanism (Theorem1), and SBMF , which is based on the exponential mechanism [4] and the properties of the matrix Bingham-von Mises-Fisher density for network data spectral analysis [7]. 3.1 LNPP: Laplace Noise Perturbation with Postprocessing corresponding eigenvectors, U k =( u 1 , u 2 ,..., u k ) , under -differential privacy with the given graph G and parameters k, . We first derive the sensitivities for the eigenvalues and eigenvectors in R esults 1, 2. We then follow Theorem 1 to calibrate Laplace noise to the eigenvalu es and eigenvectors based on the derived sensitivities and privacy parameter. Because the perturbed eigenvectors will no longer be orthogonalized to each other, we finally do a postprocess to normalize and orthogonalize the perturbed eigenvectors following Theorem 4.
 Result 1. Given a graph G with its adjacent matrix A , the global sensitivity Proof. We denote adding/deleting an edge between nodes i and j on the original graph G as a perturbation matrix P added to the original adjacent matrix A . P  X  n is a symmetric matrix where only P i,j and P j,i have value 1/  X  1andall other entries are zeros. We denote  X  i as the eigenvalue of the matrix A and  X   X  i as that of matrix A + P . We have the Euclidean norm and Frobenius norm of P respectively as P 2 =1and P F = theory [5](Chapter IV, Theorem 4.11), we have and Result 2. Given a graph G with its adjacent matrix A , the sensitivity of each eigenvector, u i ( i&gt; 1) ,is GS u i ( G )= inator is commonly referred as the eigen-gap of  X  i . Specifically, the sensitiv-ities of the first and last eigenvector are respectively GS u 1 ( G )= GS u n ( G )= Proof. We define the perturbation matrix P and other terminologies the same as those in the proof of Result 1. We denote eigenvectors of matrix A,A + P respectively as column vectors u i and  X  u i ( i  X  [1 ,k ]). Based on the matrix perturbation theory [5](Chapter V, Theorem 2.8), for each eigenvector u i ( i&gt; 1), we have Specifically for i = 1 (similarly for i = n ), Theorem 4. (Orthogonalization of vectors with minimal adjustment [6]) Given a set of non-orthogononal vectors x 1 ,..., x k , we could construct components u ,..., u k such that x i is close to u i for each i, and U T U is an identity ma-trix where U =( u 1 ,..., u k ) following where X =( x 1 ,..., x k ) is the set of n  X  1 vectors and X T X is non-singular, C is the symmetric square-root of ( X T X )  X  1 and is unique.
 Algorithm 1. LNPP: Laplace noise calibration approach Algorithm 1 illustrates our LNPP approach. We output the first k eigenvalues, #  X  -differential privacy with the given graph topology A and parameters k, .We first compute the real values of eigenvalues  X  ( k ) and eigenvectors u i ( i  X  [1 ,k ]) from the given graph adjacent matrix A (Line 1). Then we distribute the privacy parameter among  X  ( k ) and u 1 , u 2 ,..., u k respectively as 0 and 1 , 2 ,..., k where = k i =0 i (Line 2). With the derived the sensitivities for the eigenvalues and 2, next we follow Theorem 1 to calibrate Laplace noise and obtain the private 4(Line5). 3.2 SBMF: Sampling from BMF Density The SBMF approach to provide spectral analysis of network data is based on the sampling scheme proposed by Hoff [7] as an a pplication of their recently-proposed technique of sampling from the matrix Bingham-von Mises-Fisher density (Defi-nitions 3, 4). In [8], the authors investigated differentially private approximations to principle component analysis and also developed a method based on the gen-eral exponential mechanism [4]. In our w ork we focus on the eigen-decomposition of the 0-1 adjacency matrix (rather than the second moment matrix of the nu-merical data) and prove that the sampling scheme from the matrix Bingham-von Mises-Fisher density satisfies differential privacy through the general exponen-tial mechanism (Theorem 2). The sampling scheme proposed by Hoff [7] ensures Lemma 1, with the purpose to build the latent factor model (Definition 4) for network data, i.e, to approximate inferences for U and  X  . We derive the pri-vacy bounds of the output eigenvalues and eigenvectors following the sampling scheme respectively in Claims 1 and 2, based on Lemma 1. Then following the Composition Theorem (Theorem 3), we come to the conclusion that the SBMF approach satisfies -differential privacy (Theorem 5).
 Claim 1. The sampling scheme which outputs  X  ( k ) satisfies  X  -differential pri-Proof. We denote A and A as the adjacent matrix of any neighboring graph G and G . The calibrated noise to a function f from the Gaussian distribution normal (0 , X  2 ), similar as that from the Laplace distribution, provides a 2  X GS f -differential privacy [1]. Based on Lemma 1, we have for each eigenvalue  X  i ,the sampling scheme satisfies where the proof of u T i ( A  X  A ) u i  X  1 is straightforward. With the composition Claim 2. Given the graph G  X  X  adjacent matrix A , the sampling scheme which outputs U satisfies U -differential privacy where U = k 2  X  1 .
 Proof. Thesamplingschemefor U can be considered as an instance for the exponential mechanism( Theorem 2) with  X  =1and q ( A,U )= tr (  X U T AU/ 2). We have Following Theorem 2, we have U =2  X  X q ( A,U )= k 2  X  1 .
 Theorem 5. The SBMF approach to computing the spectra, the first k eigen-values and the corresponding eigenvectors of a given graph topology A satisfies In this work, we take the prior parameter  X  2 as n , which is suggested by Hoff [7] since this is roughly the variance of the eigenvalues of an n  X  n matrix of inde-pendent standard normal noise. We illustrate the SBMF approach in Algorithm 2. In the Algorithm, the parameter  X  is used to change the privacy magnitude by changing U (Theorems 2, 5). Given the input graph topology A and dimension parameter k , we acquire the eigenvalues #  X  k and corresponding eigenvectors # U k from the sampler application provided by Hoff [7] with input matrix  X A .The output satisfies =(  X  + U )-differential privacy following Theorem 5. Algorithm 2. SBMF: Sampling from BMF density approach We conduct experiments to compare the performance of the two approaches, LNPP and SBMF , in producing the differentially private eigenvalues and eigen-vectors. For the LNPP , we implement Algorithm 1. For the SBMF ,weusethe R-package provided by Hoff [7]. We use  X  X nron X  (147 nodes, 869 edges) data set that is derived from an email network 1 collected and prepared by the CALO Project. We take the dimension k = 5 since it has been suggested in previous literatures [9] that the first five eigenva lues and eigenvectors are sufficient to capture the main information of this graph. The first two rows in Table 1 show the eigenvalues and their corresponding eigen-gaps (Result 2). 4.1 Performance Comparison with  X  =1 In this section, we compare the performance of the LNPP approach with that of the SBMF approach in three aspects: the accu racy of eigenvalues, the accuracy of eigenvectors and the accuracy of graph reconstruction with the private eigen-pairs. With  X  2 = n and  X  = 1, we compute that  X  =14and U = 446 following Claims 1 and 2. Therefore the SBMF approach satisfies = 460 differential privacy following Theorem 5. On the other hand, the same is taken as the input for the LNPP approach. Different strategies have been proposed to address the distribution problem(Line 2 in Algorithm 1) in previous literatures [10, 11]. In our work, we just take one simple strategy, distributing as 0 =10tothe eigenvalues and i =90 , ( i  X  [1 ,k ]) equally to each eigenvector. Therefore LNPP approach also satisfies = 460 differential privacy.

For eigenvalues, we measure the output accuracy with the absolute error de-and SBMF are respectively 0 . 9555 and 345 . 2301. One sample o eigenvalues In the third and fourth rows of Table 1, we show the output eigenvalues from the LNPP and the SBMF approaches. We can see that the LNPP outperforms the SBMF in more accurately capturing the original eigenvalues.
 For eigenvectors, we define the absolute error as E U = | # U k  X  U k | 1 . E U for LNPP and SBMF approaches are respectively 11 . 9989 and 13 . 4224. We also define the cosine similarity to measure the accuracy of each private eigenvector cosine similarities in Table2. Note that the cosine value closer to 1 indicates better utility. We can see that LNPP generally outperforms SBMF in privately capturing eigenvectors that close to the original ones. Specifically, the LNPP approach is sensitive to eigen-gaps (second row in Table 1), i.e., it tends to show better utility when the eigen-gap is large such as for u 1 and u 2 . Thus a better strategy will be distributing privacy parameter according to magnitudes of eigen-gaps, instead of the equal distribution.
 The SBMF approach outputs much larger eigenvalues than the original ones. It does not tend to accurately approximate anyone of the original eigenvectors either. The reason is that SBMF approach is designed to provide a low rank spectral model for the original graph rather than approximating of the original eigenvalues and eigenvectors.

We consider the application of graph reconstruction using the differentially pri-vate first k eigenvalues and the corresponding eigenvectors. A k = k i =1  X  i u i u T i = U
 X  k U T k is commonly used as a rank-k approximation to the original graph topol-ogy A when A is not available for privacyreasons or A  X  X  rank istoo largefor analysis. Since A k is not an 0/1 matrix, We discretize A k as # A 1 k by choosing the largest 2 m entries as 1 and all others as 0 (so keeping the number of edges m the same as that of the original graph). We then compare the performance of the two approaches by the absolute reconstruction error defined as  X  = A  X  # A 1 k F .The  X  values for LNPP and SBMF approaches are 47 . 7912 and 34 . 1760 respectively. We can see that the result of the SBMF approach outperforms the LNPP . 4.2 Performance Comparison with Varying  X  In this section, we change the privacy magnitude to additionally study the per-formance of the LNPP and SBMF approaches.  X  denotes the amplification factor of the privacy parameter used in section 4.1. We choose the value 18 . 46 , 58 . 6 , 237 , 460 , 2244 , 4474 following Theorem 5.

We show the values of E  X  , E U and  X  for the LNPP and the SBMF approaches in Table 3. The accuracy of the LNPP approach increases significantly with  X  for both the eigenvalues( E  X  ) and graph reconstruction (  X  ). Note that the greater the  X  , the weaker privacy protection, and hence the more utility preservation. However, the accuracy of eigenvectors measured by E U is not changed much with  X  , as shown in Figure 1. This is because of the normalization of eigenvectors in the postprocess step. While the SBMF approach cannot accurately capture eigenvalues for any  X  value; as to graph reconstruction, the case of  X  =1shows the best utility. In this paper we have presented two approaches to enforcing differential privacy in spectral graph analysis. We apply and evaluate the Laplace Mechanism [1] and the exponential mechanism [4] on the differential privacy preserving eigen decomposition on the graph topology. In our future work, we will investigate how to enforce differential privacy for oth er spectral graph analysis tasks (e.g., spectral clustering based on graph X  X  Laplacian and normal matrices). Nissim et al. [3] introduced a framework that calibrates the instance-specific noise with smaller magnitude than the worst-case noise based on the global sensitivity. We will study the use of smooth sensitivity and explore how to better distribute privacy budget in the proposed LNPP approach. We will also study how differ-ent sampling strategies in the proposed SBMF approach may affect the utility preservation.
 Acknowledgments. This work was supported in part by U.S. National Science Foundation IIS-0546027, CNS-0831204, CCF-0915059, and CCF-1047621.

