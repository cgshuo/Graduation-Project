 The provision of  X  X en blue links X  has emerged as the stan-dard for the design of search engine result pages (SERPs). While numerous aspects of SERPs have been examined, lit-tle attention has been paid to the number of results displayed per page. This paper investigates the relationships among the number of results shown on a SERP, search behavior and user experience. We performed a laboratory experi-ment with 36 subjects, who were randomly assigned to use one of three search interfaces that varied according to the number of results per SERP (three, six or ten). We found subjects X  click distributions differed significantly depending on SERP size. We also found those who interacted with three results per page viewed significantly more SERPs per query; interestingly, the number of SERPs they viewed per query corresponded to about 10 search results. Subjects who interacted with ten results per page viewed and saved sig-nificantly more documents. They also reported the greatest difficulty finding relevant documents, rated their skills the lowest and reported greater workload, even though these differences were not significant. This work shows that be-havior changes with SERP size, such that more time is spent focused on earlier results when SERP size decreases. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval:Search Process; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces:Screen De-sign Search behavior; user studies; search interface; search result page
Over the years, the standard design for a search engine results pages (SERPs) has converged on a textual listing of ten results per page; this presentation style is often referred to as  X  X en blue links. X  Despite many scholars and designers questioning this convention, this standard persists. While the literature contains many examples of alternatives to this design including variable snippet styles e.g., [31] and layout methods e.g., [15], few works have questioned or considered how the number of results presented per page (SERP Size), impacts search behavior and if a different size might lead to a better user experience. Although many experimental search interfaces, in particular pre-Web interfaces, have presented more than ten results per page, and many current search interfaces allow users to configure the number of results per page, to our knowledge, the size of the SERP has only been systematically investigated in a few studies [19, 23, 25].
Understanding how SERP size influences user behavior is important for several reasons. First, users are now ac-cessing search results via a multitude of devices, including desktop computers, mobile phones, tablets, smart watches and smart glasses. Each device displays varying numbers of results above-the-fold and research has shown that this can impact click behavior [12, 18]. Thus understanding how the behaviors and performance of users changes with respect to SERP size can potentially help guide and inform researchers and practitioners who are interested in designing effective in-terfaces. Second, many IR evaluations measures [11, 22, 28] make assumptions about the number of results presented to the user and how the user will evaluate these results; specif-ically, that the user examines results linearly and the likeli-hood of the user examining a result decreases exponentially with rank. This underlying user model might be insufficient if variable SERP sizes are considered. Finally, little is known about how SERP size impacts the user experience; it might be the case that presenting fewer results per page has pos-itive psychological consequences as found by Oulasvirta et al. [23] who examined user preferences for mocked-up search result sets of varying lengths. On the other hand, fewer results may lead to increased browsing costs and increased mental load as users need to flip between result pages de-tracting from their search experience.

Thus, the purpose of this paper is to investigate the im-pact of SERP size on search behavior and user experiences. To do so, we create three search interfaces that vary accord-ing to the number of results per SERP (three, six or ten) and conduct a between-subjects laboratory experiment.
Numerous aspects of search engine result pages (SERPs) have been investigated including layout (e.g., ranked list, grid), ordering (ranking of results), components (e.g., ag-gregated search pages, query suggestion), length and con-tent of snippet summaries (e.g., query-biased), snippet style (e.g., thumbnails) and SERP size. Below we provide a brief overview of some of the research focused on SERP design and SERP size. We also present research on screen size and position effects, which are relevant to this work.
A range of experiments have been conducted investigating different snippet types, sizes and features, as well as layouts and result combinations. Perhaps the most researched as-pect of SERP design has been on the potential advantages of visual snippets (e.g., thumbnails) over textual snippets [2, 13, 24, 31, 34]. For example, Woodruff et al. [34] examined the usefulness of enhanced thumbnail images to represent webpages. The enhanced thumbnail representation magni-fied salient features of webpages so that searchers could more readily identify whether a page contained relevant material. In comparison with textual summaries and standard thumb-nails, it was found that the enhanced thumbnails reduced search time. In more recent work, Teevan et al. [31] explored a variation on generating enhanced thumbnails, referred to as visual snippets, and also found that such an approach decreased the time required to identify relevant documents. TileBars [10] is another example of how researchers have used visual information on the results page to help users make better decisions about the relevance of the result rep-resented by the snippet.

There have also been many studies on the content and size of snippets [6, 8, 32]. For example, Tombros and Sander-son [32] investigated the potential usefulness of query-biased summaries for snippets and found they improved both user accuracy and speed of relevance judgements. Clarke et al. [6] studied different aspects of result snippets which users found attractive and desirable as determined by the number of clicks the snippet received. Clarke et al. X  X  [6] study showed that well formed snippets with title, summary and URLs that were more readable, contained query terms and query phrases were seen as more desirable.

Cutrell and Guan [8] explored the effect of different snip-pet lengths (short: 1 line, medium: 2-3 lines and long: 6-7 lines) using eye tracking data. For navigational queries, they found that users examined more results as snippet length increased, but performed slightly worse. They attributed the differences to snippets being a distractor; specifically, as snippets grew longer, users would often ignore the URL. For informational queries, users performed somewhat better as snippet length increased and looked at more results when snippets were medium length.

Alternatives to the linear result list have also been ex-plored, in particular, grid-based layouts [5, 15, 26]. For ex-ample, Krammerer and Beinhauer [15] examined differences in user behavior when interacting with a standard list inter-face (single column of title, URL and snippet stacked ver-tically), a tabular interface (title, snippet and URL stacked horizontally in 3 columns for each search result) and a grid layout (search results placed in 3 columns). They found users of the grid layout scrutinized the snippets more, and speculated the interface had promise in overcoming the po-sition bias observed in [12] (see Subsection 2.4).

Finally, researchers investigating aggregated search have studied SERP design [29, 1, 36]. Both Sushmita et al. [29] and Arguello et al. [1] studied the differences between blended and tabbed search result pages. Even these more com-plicated SERP pages, which blend results from multiple sources, still tend to present about 10 results per page, pri-marily in list format. However, for specific verticals such as images, the number of results shown per page is often larger and grid formats are more common [20].
With regard to SERP size, or the number of results pre-sented per page, there has been very little systematic re-search [23, 25, 19]. Although many experimental search interfaces, in particular pre-Web interfaces, have presented more than ten results per page, and many current search interfaces allow users to configure the number of results per page, to our knowledge, the size of the SERP has only been explicitly studied as a variable in a few of studies. For ex-ample, Oulasvirta et al. [23] explored the idea that showing fewer results may be preferable to showing more. In their ex-periment, they showed participants result lists with 6 results or 24 results, as a way to investigate the so called,  X  X aradox of choice. X  They found that participants rated their satis-faction, carefulness and confidence higher for the six results per page condition. However, the study was performed us-ing paper based mock-ups of screens, where there was no pagination (i.e., 6 or 24 results were shown on one page), so it is unclear how these findings generalize to on-screen desk-top conditions, where typically only about 6-7 results are viewed at one time. Also, it is not clear whether the effect would be significantly different between 6 results per page, and the standard 10 results per page, and how the results would differ in an interactive search context.

In an industry report from Google, Linden [19] stated feedback from Google users indicated they desired more than ten results per page. However, it was observed that when the number of results were increased to 30 per page, traf-fic dropped by 20%. The drop in traffic was attributed to the increase in time to dispatch the page, since a page with 10 results took approximately 0.4 seconds to generate, while a page with 30 results took approximately 0.9 seconds. This report is nearly ten years old so it is unclear if the re-sults regarding user preference are still applicable to today X  X  searchers and search environments.
While few studies have explicitly examined the impact of SERP size on user interaction, this has been indirectly examined in the context of screen size, since screen size im-pacts the number of results that are viewable. Jones et al. [14] investigated the differences in search performance when searchers used WAP based mobile phones which dis-played 5 title snippets per page, PDA devices with 5 snip-pets per page and desktop search with 10 snippets per page. They found participants using the devices showing 5 results per page would on average examine 2-3 pages of results.
A similar experiment using more modern mobile phones and PDAs was conducted by Sweeney and Crestani [30], but the focus was on what length snippet to present to searchers on the different devices. They note that because transmis-sion costs were high (at the time), SERPs often contained fewer results per page, thus increasing page to page naviga-tion costs [14]. They varied the length of the snippet sum-mary, but kept the number of results per page on each device constant and set to 10. They hypothesized participants us-ing the larger screen would examine more documents, but found participants examined approximately the same num-ber of documents regardless of the condition.

Jaewon et al. [18] compared the differences in people X  X  interaction styles with two screen sizes (desktop vs mobile) where SERP size was set to 10; on one interface all ten could be seen, while on the other, only 3 could be seen. This latter condition simulated the screen size of a mobile phone. Jae-won et al. [18] found for both informational and navigational tasks, participants using the small screen took longer to click on a link and had longer task completion times (though only the task completion times were significantly different). They also found participants scanned slightly fewer documents on the small screen, but scrutinized each link for a longer dura-tion, especially the first three links. On the smaller screen, links 1-3 attracted more clicks for both informational and navigational tasks: 88% and 91% versus 83% and 90%, re-spectively. This suggests when interacting with restricted screen space, searchers focus more on the visible set of re-sults and make more clicks on these results.
How users interact with the standard baseline SERP has been informed by studies examining where users click on the SERP [4, 7, 12, 16, 35]. In one of the first studies of click bias, Joachims et al. [12] found users inspected 70% down to 20% of result snippets in the top 6 (above the fold), and then 10% down to 5% of results snippets at positions 7-10 (below-the-fold). Essentially, the probability of a click decayed exponentially, while the probability of examining a snippet decreased linearly (above-the-fold, and the flattened out). Similar findings have been reported by others [4, 16, 21]. Joachims et al. [12] wondered whether the distribu-tion of clicks could be used as an absolute indicator of rel-evance. However, they pointed out two problems in inter-preting click data stemming from (i) trust bias , where users trust the search engine to deliver the most relevant item first, i.e., following the probability ranking principle [27], and (ii) quality bias , where the behavior depends on the quality of the retrieval system. They concluded users are more likely to click on highly ranked documents and that quality influ-ences click behavior, such that if the relevance of the items retrieved decreases, users click on items that are less rele-vant, on average.

Craswell et al. [7] noted this as a fundamental problem with click data, and referred to the problem as position bias , where in the top 10 result lists, the probability of a click de-creases with the rank of the document. They put forward several hypotheses for why position bias is observed. From their analysis, they showed a cascade model, which assumed the user would click a result snippet with some probability ( p s ) or skip it (1  X  p s ), fit click behavior at earlier ranks, and a baseline model, which assumed the probability a user would click a result snippet was proportional to its relevance, fitted click behavior at later ranks. Of note is that the click distributions that have been observed are based on interac-tion with SERPs that display ten results per page. An open question is if these findings generalize to situations where the screen and page sizes vary.
 Figure 1: A standard search interface that has a query box and search button, houses n results per page along with pagination buttons. The number of results returned is also shown.
We conducted a between-subject experiment where sub-jects were randomly assigned to one of three interfaces that differed according to how many results were displayed per search result page. Figure 1 displays the basic design of the interfaces. One interface functioned as the baseline and dis-played 10 results per page (10RPP). The other two interfaces displayed 3 and 6 results per page (3RPP and 6RPP, respec-tively). For the 3RPP and 6RPP interfaces, all search re-sults were displayed above-the-fold (i.e., no scrolling was re-quired). For the interface displaying 10RPP, the first six re-sults were visible above-the-fold. Our decision to use 6RPP was to make our work more comparable to past research. Specifically, to Oulasvirta et al. X  X  [23] study where 6RPP were evaluated through a simulation and to Joachims et al. [12] who described click distributions for results above-the-fold (those at ranks 1-6). Although we restrict our in-vestigation to desktop search, we choose 3RPP as another comparison point similar to Jaewon et al. [18].

As shown in Figure 1 all three interfaces had a query box at the top and a search button. Below the search button was information displaying the number of retrieved results and the result surrogates. The surrogates displayed the title of the result, metadata about the result and a short summary. At the bottom of the SERP were buttons that allowed sub-jects to move forward and backward through the set of result pages. All subjects used the same desktop computer when completing the study, which was in our laboratory and under our control. Subjects used a 19 inch monitor and all aspects of the display, including font size and resolution, were held constant during the study.
The Aquaint TREC test collection of over one million newspaper articles was used [33]. We selected three search topics from this collection: 344 (Abuses of E-mail); 347 (Wildlife Extinction) and 435 (Curbing Population Growth). We selected topics that had some contemporary relevance, we thought would be of interest to our target subjects and had a similar number of relevant documents available (123, 165 and 152, respectively). Our selection was also based on evidence from previous user studies with a similar sys-tem setup [17] where it was shown that the difficulty of these topics did not significantly differ. Subjects searched all three topics and topics were rotated with a Latin-square.
To situate the search tasks, subjects were instructed to imagine they were newspaper reporters and needed to gather documents to write stories about the provided topics. Sub-jects were told that there were over 100 relevant documents in the collection for each topic and they should try to find as many of these as possible during the allotted time (10 min-utes per topic). While recall-oriented searches are not as common in the general Web population as searches where users are looking for one or a small number of items, these types of searches are still performed by many people, in par-ticular, professionals such as newspaper reporters, patent searchers and business analysts. Because we were interested in examining traditional performance measures, we choose to use a test collection. Since the collection contained news-paper articles, we created a work task situation representing one type of user model (i.e., journalist) and one type of task model (i.e., find as many relevant documents as you can to write a story) that was appropriate given the collection and to which we thought our target participants (i.e., undergrad-uate students) could relate.

The Whoosh IR Toolkit was used as the core of the re-trieval system, with BM25 as the retrieval algorithm, using standard parameters, but with an implicit ANDing of query terms to restrict the set of retrieved documents to only those that contain all the query terms (similar to BM25A used in [3]). This was chosen because most systems, such as web search engines and library catalog systems, implicitly AND terms together. However, subjects could also explicitly use OR, AND, or NOT in their queries. Subjects were not pro-vided with a tutorial of the system.
Search behavior was operationalized with three types of measures: (1) interaction; (2) performance and (3) time spent doing different search activities. All of these measures were computed from log data and TREC q-rels. Interac-tion measures included: number of queries issued, number of SERPs viewed per query, number of documents viewed, number of documents viewed per query and deepest SERP click. Performance measures included: number of docu-ments marked relevant, number of TREC-relevant docu-ments marked relevant and precision at 3, 5 and 10 doc-uments. Time-based measures included: time spent issuing queries, time spent examining SERPs per query and time spent viewing documents.
To capture user experience, subjects evaluated the search tasks before each search, the system after each search and their experienced workload after completing all search tasks.
Task evaluations were elicited via pre-search questionnaire, which contained five items: (1) How much do you know about this topic? (1=nothing; 5=I know details); (2) How relevant is this topic to your life? (1=not at all; 5=very much); (3) How interested are you to learn more about this topic? (1=not at all; 5=very much); (4) How often have you searched for information related to this topic? (1=never; 5=very often) and (5) How difficult do you think it will be to search for information about this topic? (1=very easy; 5=very difficult).

System evaluations were elicited via post-search question-naire, which also contained five items: (1) How difficult was it to find relevant documents? (1=very easy; 5=very dif-ficult); (2) How would you rate your skill and ability at finding relevant documents? (1=not good; 5=very good); (3) How would you rate the system X  X  abilities at retrieving relevant documents? (1=not good; 5=very good); (4) How successful was your search? (1=unsuccessful; 5=successful) and (5) How many of the relevant documents do you think you found? (1=a few of them; 5=all of them).

Experienced workload was elicited using a modified ver-sion of the NASA Task Load Index (TLX). This instrument elicited ratings of the following: Mental Demand, Physi-cal Demand, Temporal Demand, Performance, Effort and Frustration (Table 1). We modified the TLX statements so they matched the target task (search) and asked subjects to make their evaluations with a 7-point scale. Subjects also completed two other workload questionnaires, which were fo-Time 3RPP 6RPP 10RPP F 344 347 435 F time spent viewing and assessing documents across the session. cused on the workloads associated with navigating the search results and assessing the relevance of results. We included these two additional scales to isolate any difficulties arising from the varying number of results shown per page.
Subjects were recruited by sending an email solicitation to undergraduate students at a large research university. Thirty-six undergraduate students participated (12 per in-terface). Twenty-six subjects were female and ten were male. Their average age was 20.4 (SD=2.6). Sixty-seven percent were science majors, 14% were social science majors and 19% were humanities majors. Each subject was com-pensated with $15 USD and could earn an extra $5 USD per topic for being one of the top three performers in his/her in-terface condition.

Subjects X  search experience was measured using a modi-fied version of the Search Self-Efficacy scale [9]. This instru-ment contains 14-items describing different search-related activities. Subjects respond to each item by indicating their confidence in completing each activity on a 10-point scale (1=totally unconfident; 10=totally confident). Reliability analysis of these items demonstrated a high Cronbach X  X  al-pha (0.934), so responses were averaged. Overall, subjects reported fairly high search self-efficacy (M=7.51, SD=0.97).
Both the search behavior and user experience measures were analyzed according to interface and topic. To evaluate these data, ANOVAs were conducted using interface and topic as factors; both main effects and interaction effects were examined with alpha=0.01. Bonferroni tests were used for post-hoc analysis.
Interactions. Table 2 displays the average number of queries submitted by subjects, SERP pages viewed per query, total number of documents viewed, documents viewed per query, and average SERP depth of the last document viewed per query. Participants entered similar numbers of queries in each interface, but they viewed significantly different num-bers of SERP pages and documents. Specifically, those who interacted with 3RPP viewed significantly more SERPs than those who interacted with 10RPP and those who interacted with 6RPP viewed significantly less documents than those who interacted with 10RPP. Interestingly, the average num-ber of SERPs viewed per query for those interacting with 3RPP (3.56) corresponded to about 10 results. Table 2 also displays search interactions according to topic. Significant differences were detected for all measures; in all cases the means for Topic 344 significantly differed from those for Topic 347, as well as from Topic 435 for queries and doc-uments viewed. There were no significant interaction effects between interface and topic.

Performance. Table 3 displays the number of documents subjects marked relevant, the number of documents marked relevant that were also TREC relevant and several precision scores that were calculated using the TREC-relevant doc-uments. Subjects who interacted with 10RPP performed the best according to all performance measures, but only significantly so with respect to number marked relevant. Subjects X  performances differed significantly according to topic for three measures: number marked relevant, P@5 and P@10. Subjects marked significantly fewer documents as relevant when completing Topic 344 than Topic 347. Their P@5 scores were significantly worse for Topic 344 than the other two topics and their P@10 scores were significantly worse for Topic 344 than Topic 435. There were no signifi-cant interaction effects between interface and topic.
Time-Based Measures. Table 4 displays the amount of time subjects spent performing different search activities: issuing queries, viewing SERP pages per query and viewing documents. Subjects who interacted with 3RPP spent the most time viewing documents, those who interacted with 6RPP spent the most time issuing queries and those who interacted with 10RPP spent the most time on SERPs per query, none of these differences were statistically significant. There were also no significant differences in the amount of time spent engaged in different search activities according to topic. Finally, there were no significant interaction effects between interface and topic. 0 0.1 0.2 0.3 0.4 0 0.1 0.2 0.3 0.4 0 0.1 0.2 0.3 0.4 Figure 2: The probability of clicking on a snippet ( y -axis) for each rank i ( x  X  axis ). Top: 3RPP, Middle: 6RPP and Bottom: 10 RPP. The distributions for 3RPP and 6RPP were significantly different to the distribution on the 10RPP interface.

Click Distributions. Figure 2 shows the distribution of clicks on each of the three interfaces (3RPP, 6RPP and 10RPP, Top, Middle and Bottom, respectively). The graphs show the average probability of a user clicking on a result snippet at rank position i . Subjects who interacted with 3RPP mainly clicked on the first 3 results (0.41 down to 0.33). A noticeable drop in the probability of a click is observed when subjects went to page 2. Interestingly, the probability stays around 0.22-0.23 for results at ranks 4-6, before another step down in the probability of a click on documents at ranks 7-12. After result 12, i.e., page 5 and onwards, the probability becomes increasingly smaller. Sub-jects who interacted with 6RPP appeared to be less likely to click on documents overall, but again we observe a small step change after the first page (i.e., 6-12), and again after the second page (i.e., 13-18). In contrast, subjects interact-ing with 10RPP, clicked with higher probability across most of the ranks, and even after the first page of results (i.e., 11-20), there is no noticeable drop in click probability. To de-termine if the click distributions were drawn from the same underlying distribution a two-sample Kolmogorov-Smirnov test was used. For 3RPP and 6RPP the distributions were not significantly different (p=0.3420, k=0.2333), indicating that they were drawn from the same underlying distribution. However, the click distributions for 3RPP and 6RPP were found to be significantly different from 10RPP (p=0.0046, k=0.4333, and p=0.0006, k=0.5, respectively).
Task Evaluations. Table 5 displays subjects X  task evalu-ations. With the exception of interest in the topic, subjects X  responses to these items differed significantly according to topic. Subjects indicated they had significantly more topic knowledge about Topic 435 (Curbing Population Growth) than Topics 344 (Abuses of E-mail) and 347 (Wildlife Ex-tinction), F(2, 107)=6.53, p=0.002 and had searched more often in the past for information about this topic, F(2,107)= 13.44, p &lt; 0.0001. They expected it to be significantly more difficult to search for information about Topic 344, than the other two topics, F(2, 107)=8.79, p=0.0003. An analysis was also conducted to see if subjects X  responses differed accord-ing to interface condition. Since subjects were randomly as-signed to condition, we did not expect differences and none were found. There were also no significant interaction ef-fects.
 Table 5: Mean (sd) responses to task evaluation items. *p &lt; 0.01; **p &lt; 0.0001
System Evaluations. Table 6 displays subjects X  sys-tem evaluations. For all items, subjects X  responses to the items for Topic 344 were significantly different than their responses to the items for the other two topics (note: one subject skipped this questionnaire for Topic 344). This topic was rated as significantly more difficult, F(2, 106)=20.01, p &lt; 0.0001, and subjects rated their own skill at finding rele-number found * 2.09 (1.04) 3.00 (0.89) 2.67 (0.79) Table 6: Mean (sd) responses to system evaluation items according to topic. *p &lt; 0.0001 number found 2.69 (1.04) 2.51 (1.01) 2.56 (0.91) Table 7: Mean (sd) responses to system evaluation items according to interface. vant documents significantly lower, F(2,106)=12.56, p &lt; 0.0001, as well as the system X  X  ability at retrieving relevant docu-ments, F(2,106)=12.29, p &lt; 0.0001. They also felt they were less successful, F(2,106)=13.71, p &lt; 0.0001, and found fewer of the relevant documents, F(2,106)=7.58, p &lt; 0.0002.
An analysis was also conducted to see if subjects X  re-sponses differed according to interface condition (Table 7). Interestingly, subjects who interacted with 10RPP reported the greatest difficulty finding relevant documents and rated their skills lowest; while those who interacted with 3RPP re-ported the least difficulty. Those who interacted with 6RPP reported less successful searches. However, no significant dif-ferences were found according to interface condition. There were also no significant interaction effects between condition and topic. Taken together, these results suggest subjects had a different search experience with Topic 344 regardless of in-terface condition, and interface condition did not uniformly impact their system evaluations.

Workload. Figure 3 displays the system, navigation and assessment workloads reported by subjects. With respect to system load, subjects who interacted with 10RPP reported the highest values for all items, with the exception of phys-ical demand. The difference between 10RPP and 3 and 6 RPPs is especially pronounced for mental demand. The same general trends were observed for the navigation and as-sessment loads, with the exception of performance. Despite these differences, no significant differences were detected.
This purpose of this paper was to investigate the impact of SERP size on search behavior and user experience. We found that subjects who interacted with 3RPP viewed sig-nificantly more SERPs per query, which is perhaps to be expected, as subjects needed to paginate more to see more results. Interestingly, the average number of SERPs they examined per query corresponded to about ten results, so perhaps people have been conditioned to view ten results, even if viewing ten results requires expending a bit more effort through pagination. This finding is consistent with F igure 3: Top: System Load, Middle: Navigation Load, Bottom: Assessment Load the findings of Jones et al. [14] who found that participants who used a device that displayed five results per page exam-ined about 2-3 pages of results. However, in our study, those who interacted with 6RPP and 10RPP viewed 2.66 and 1.92 SERPs per query, respectively, which corresponds to about 20 results per query, so there might be an interaction be-tween the number of results subjects desire to see (whether consciously or subconsciously) and the effort required for pagination.

Subjects who interacted with 3RPP examined fewer doc-uments, but seemed to compensate by spending more time on each document they examined. This is interesting be-cause it suggests that the additional cost and effort of paging through results led to deeper inspection of the documents, which is consistent with Jaewon et al. X  X  [18] results. From the click distributions, it is interesting to note that the re-sults on the first page were treated more or less equally, but for each subsequent page, fewer and fewer subjects visited them. It might be that with smaller sets of results, subjects treat them more independently or treat them as a batch. Subjects who interacted with 6RPP examined significantly fewer documents, but spent more time formulating queries (although not significantly so). This suggests that subjects might have seen enough results to determine their queries were unsuccessful, and thus spent more time querying. It is interesting to consider how the SERP functions as feedback to the user about the quality of his or her query and how the provision of a smaller SERP size might disrupt or change this function.

We found those who interacted with 10RPP viewed sig-nificantly more documents than those who interacted with 6RPP. This is in contrast to Sweeney and Crestani X  X  [30] findings where there was no difference in the number of doc-uments examined according to the number of results view-able on the device. Subjects who interacted with 10RPP also marked significantly more documents as relevant. This might be because they had cheaper access to more docu-ments compared to subjects in the other conditions. Inter-estingly though, there were no significant differences in the amount of time subjects in the different conditions spent ex-amining SERP pages or documents. Given that each SERP in the 10RPP condition displayed more snippets, one would have expected this difference to be significant. Seeing more results per page might have allowed those in the 10RPP to more quickly identify or distinguish relevant documents from non-relevant documents, or since there were no sig-nificant differences in the number of TREC relevant saved, they might have been more liberal with their assessments. Those who interacted with 3RPP likely faced an additional memory burden of having to internally keep track of what they saw on different pages and make comparisons of the information, while those who interacted with 10RPP likely had fewer things to keep track of internally.

A key finding from this research is that the distribution of clicks changes with SERP size. Specifically, the click distri-butions for those interacting with 3RPP and 6RPP differed significantly from those who interacted with 10RPP. Essen-tially these participants gave more time and attention to the top results, which may have been a way to reduce the potential memory load described above. When the SERP size was small, fewer snippets were inspected and more time was spent on the top documents. These findings suggest that precision-oriented search algorithms will be of increas-ing importance as the page size decreases. While we did not test interfaces that display one or two results per page, our findings suggest that users are likely to be even more focused on the top documents in these situations. Further work is need to examine how screen size (i.e. the number of results viewable) and the page size (the number of results per page) interact with one another and how this is related to the user X  X  task and search goals.

Results regarding user experience were not as revealing, but some interesting, although not significant, trends were observed. Most notably, subjects who interacted with 10RPP reported the greatest difficulty finding relevant documents, rated their skills the lowest and reported greater workload. When considered in light of Oulasvirta et al. X  X  [23] work these findings suggest that smaller SERP sizes might have positive psychological consequences for users since results are chunked in smaller sets. Oulasvirta et al. [23] work was motivated by the idea of the  X  X aradox of choice X  and partici-pants in their study rated their satisfaction, carefulness and confidence higher when interacting with 6 results as opposed to 10. The potential psychological benefits of alternative SERP sizes is an interesting avenue for future research.
Although we tried to select search topics that would not differ according to difficulty, we found that one topic (344) was much more difficult than the other two. Not only did subjects expect this topic to be more difficult, they also de-scribed it as more difficult after they searched. They exhib-ited significantly different search behaviors for this topic and performed significantly worse according to most measures. This topic was about abuses of email at the workplace. Our subjects were likely familiar with the general topic of email, but since they were all undergraduates, their knowledge and familiarity with the workplace was likely impoverished which might explain why this topic was rated differently in the pre-search task evaluations. The differences in the system evaluations for this topic might be a result of the challenges of searching for information about email, which can be ex-pressed in a variety of ways (e.g., email, e-mail, electronic mail). While we did not find a significant interaction ef-fect between topic and interface condition, future studies might explore this more systematically with a greater range of topics from different points along the difficulty spectrum. The additional clicking required by the interface with 3RPP might intensify people X  X  feelings of stress if they are experi-encing difficulty. Alternatively, fewer results per page might help the person stay more focused and in control.
In this paper, we have studied how the number of re-sults per page impacts search behavior and search experience in the context of ad-hoc topic retrieval. We created three search interfaces that varied according to the number of re-sults per SERP (three, six or ten) and conducted a between-subjects user-centered evaluation. Our major finding was that subjects X  click distributions differed significantly de-pending on SERP size. Specifically, those interacting with 3RPP and 6RPP spent more time focused on top-ranked results and those interacting with 10RPP. This result has implications for evaluation. Many evaluation measures as-sume click distributions that neatly decay following an ex-ponential distribution [11, 22, 28]. However, this might be an artifact of the SERP size modeled, and prompts a ques-tion as to whether such measures generalize well to SERPs of different sizes. Further work will need to be conducted to determine the impact of different SERP sizes on the distri-bution of clicks. This is of particular interest as the types of devices used for search are changing from desktop to mo-bile and wearables devices (such as tablets, mobiles, watches and glasses), where the number of results displayed and the number of results viewable vary.

Furthermore, while we have empirically approached the problem, it would be of interest to formally model how users interact with different screen and page configurations, in or-der to find optimal or ideal settings for these parameters. Creating a formal model would provide researchers and prac-titioners with a compact representation of how interaction costs, behaviors and performance are affected when SERP size and display size changes. Such an analytical tool would help in reasoning about the optimal SERP size for different devices and display sizes, as well as other aspects of search context such as type of task and user.

Finally, we noted that different SERP sizes appear to have different psychological effects which may lead to a positive or negative search experience. While none of our user ex-perience measures were significant, the number of users in our study was quite low. Nor did we ask the right questions or study the right constructs to really get at this issue in detail, so further work on this front is also required.
We would like to thank to Kathy Brennan from UNC for help with data collection. [1] J. Arguello, W.-C. Wu, D. Kelly, and A. Edwards. [2] A. Aula, R. M. Khan, Z. Guan, P. Fontes, and [3] L. Azzopardi, D. Kelly, and K. Brennan. How query [4] J. Bar-Ilan, K. Keenoy, M. Levene, and E. Yaari. [5] F. Chierichetti, R. Kumar, and P. Raghavan.
 [6] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. [7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [8] E. Cutrell and Z. Guan. What are you looking for?: [9] S. Debowski, R. Wood, and A. Bandura. The impact [10] M. A. Hearst. Tilebars: Visualization of term [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [13] H. Joho and J. M. Jose. A comparative study of the [14] M. Jones, G. Marsden, N. Mohd-Nasir, K. Boone, and [15] Y. Kammerer and P. Gerjets. How the interface design [16] M. T. Keane, M. O X  X rien, and B. Smyth. Are people [17] D. Kelly, K. Gyllstrom, and E. W. Bailey. A [18] J. Kim, P. Thomas, R. Sankaranarayana, T. Gedeon, [19] G. Linden. Marissa mayer at web 2.0 , November 2006. [20] H. Liu, X. Xie, X. Tang, Z.-W. Li, and W.-Y. Ma. [21] L. Lorigo, B. Pan, H. Hembrooke, T. Joachims, [22] A. Moffat and J. Zobel. Rank-biased precision for [23] A. Oulasvirta, J. P. Hukkinen, and B. Schwartz. When [24] T. Paek, S. Dumais, and R. Logan. Wavelens: A new [25] H. Reiterer, G. Tullius, and T. M. Mann. Insyder: a [26] M. L. Resnick, C. Maldonado, J. M. Santos, and [27] S. E. Robertson. The probability ranking principle in [28] M. D. Smucker and C. L. Clarke. Time-based [29] S. Sushmita, H. Joho, M. Lalmas, and R. Villa. [30] S. Sweeney and F. Crestani. Effective search results [31] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, [32] A. Tombros and M. Sanderson. Advantages of query [33] E. M. Voorhees. Overview of the trec 2005 robust [34] A. Woodruff, R. Rosenholtz, J. B. Morrison, [35] Y. Yue, R. Patel, and H. Roehrig. Beyond position [36] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose.
