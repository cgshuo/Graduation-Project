 We investigate how the cost associated with querying in the context of information retrieval affects how users interact with a search system. Microeconomic theory is used to gen-erate the cost-interaction hypothesis that states as the cost of querying increases, users will pose fewer queries and examine more documents per query. A between-subjects laboratory study with 36 undergraduate subjects was conducted, where subjects were randomly assigned to use one of three search interfaces that varied according to the amount of physical cost required to query: Structured (high cost), Standard (medium cost) and Query Suggestion (low cost). Results show that subjects who used the Structured interface sub-mitted significantly fewer queries, spent more time on search results pages, examined significantly more documents per query, and went to greater depths in the search results list. Results also showed that these subjects spent longer gen-erating their initial queries, saved more relevant documents and rated their queries as more successful. These findings have implications for the usefulness of microeconomic theory as a way to model and explain search interaction, as well as for the design of query facilities.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval:Search Process; H.3.4 [ Information Storage and Retrieval ]: Systems and Software:Performance Evaluation Theory, Experimentation, Economics, Human Factors Search Behavior, Economic Models, Production Theory, In-teractive Information Retrieval, Query Interfaces, Query Cost
During interactive information retrieval (IIR), users per-form various interactions, such as posing queries, examining snippets and evaluating documents. Each action requires a certain amount of effort and comes at a cost, whether mental, physical, temporal, fiscal, or a combination thereof. In the early days of search system research, cost and util-ity figured prominently in both systems-centered and user-centered evaluation frameworks [6, 36]; however, less atten-tion has been devoted to costs in contemporary information search research. Fiscal costs are presumed to be less impor-tant because search tools and information are freely available online. Mental and physical costs have been incorporated into evaluation measures as fixed parameters (for example, by introducing discounting based on rank [24]) and used to characterize interactions [25], but they have rarely been studied as independent variables because they are difficult to manipulate and measure. While many interactive search studies incorporate effort-based measures, such as the num-ber of documents examined, number of queries issued and amount of time spent performing different actions [35, 25], few studies have attempted to model how interaction costs shape search behavior [27]. There have been some excep-tions, though, where information seeking and retrieval be-havior has been formally modeled using a cost-benefit frame-work [2, 31, 34].

In this work, we explore how the costs associated with querying affect search behaviors in the context of IIR. This is an important question to address because understanding the relationship between cost, behavior and performance might help explain how and why users interact with search systems in particular ways, and subsequently, enable designers of such systems to influence user interaction and search behav-ior. We ground our experiment using the recently proposed Search Economic Theory [2], which uses microeconomics to model the search process and provides a means to reason about interaction, cost and performance. Using this theory, we formulated the cost-interaction hypothesis that states: as the cost of querying increases, users will pose fewer queries and examine more documents per query. We conducted a laboratory experiment with 36 subjects and three interfaces that varied according to query cost, where cost was opera-tionalized as time required to submit a query. Our findings show that subjects who used the more costly query interface submitted significantly fewer queries and examined signifi-cantly more documents per query than subjects who used the interfaces with lower querying cost.
Cost has had a long history in IR research and has figured prominently in both systems-centered and user-centered re-search. Cost have been defined in a number of ways, includ-ing as mental, physical, temporal and fiscal cost. In the pre-Web era, research often focused on fiscal costs since search tools and online information were not free; many measures of fiscal costs focused on the amount users were willing to pay for search results [6, 9, 41]. For example, Cooper [10] pro-posed that users associate dollar amounts with search results as a way to understand subjective utility and Salton [36] proposed a number of cost-based measures related to the operational environment and response times. Today, only a few IR measures attempt to incorporate some type of cost into their computation, including nDCG [24], RBP [28], and time-biased gain [40].

While fiscal costs are no longer investigated as much, tem-poral cost, and more specifically response time, continues to be an important variable in research [12]. Since the late 1990s, a number of studies have demonstrated that network latency and download speeds impact how people interact with Web pages and their evaluations of information and website quality (e.g., [14, 23, 29]). In a recent study of time delays and search behavior, Taylor et al. [42] inves-tigated the relationship between the amount of time Web pages take to load, the number of pages people viewed and the amount of information examined per page. Taylor et al. hypothesized that as response time increased, the number of pages searched would decrease and the amount of infor-mation examined per page would increase. The researchers hypothesized a step relationship between response time and search behavior rather than a linear or curvilinear relation-ship (that is, changes in behavior would only occur after a critical time delay point). Results showed partial support for the hypothesis related to response time and number of pages examined, and full support for the hypothesis related to response time and the amount of information examined per page. While these results provide evidence that cost, as measured by response time, impacts search behavior, the task explored in this study presented participants with a set of static Web pages, and query cost and participants X  interactions with result pages were not examined.

There is some evidence that introducing time delays and query constraints can impact search behavior. Brutlag [7] re-ports in a blog about research conducted at Google that the time taken to return search results impacts the number of searches conducted by users. An increase in 400 milliseconds was shown to reduce the number of searches by 0.59% across a six week period on the Google search engine. Fujikawa et al. [19] constrained the number of queries a user could issue, ostensively making queries more valuable, and found that participants whose querying was constrained posed fewer queries and examined more documents per query, while par-ticipants who were not constrained submitted more queries and examined fewer documents per query. While this study was not focused on query cost, the results suggest that lim-itations on people X  X  abilities to input queries can impact search behavior. Several studies have examined how query input facilities impact the types and properties of queries entered by users, and how this subsequently impacts search outcomes [1, 5, 17], but these relationships were not investi-gated in a cost-benefit framework. Furthermore, these stud-ies have mainly focused on encouraging different querying behaviors, rather than understanding how the query facility and the cost of querying shapes the entire search interaction.
Recently, Baskaya et al. [3] used a simulation to study the cost of querying on two different devices. Query cost was measured as time and set to different speeds related to how long it takes the average person to enter a query using a desktop computer and smart phone. Baskaya et al. found that increasing the time to enter a query resulted in a reduction in the number of queries submitted across a variety of querying strategies and across sessions of different lengths. Essentially, this simulation suggests that as query cost increases, the number of queries issued will decrease. However, the findings have yet to be empirically validated.
Researchers have also used physical costs, or the amount of effort a person exerts during search, as a way to eval-uate IR systems and user interaction. These effort-based measures include number of queries issued, number of result pages evaluated and documents viewed [27]. However, few laboratory studies have attempted to model behavior or un-derstand if and how different costs shape behavior, or how different interfaces are associated with different costs. Stud-ies using large scale search log data have made some progress on modeling effort and search behavior (e.g., [16, 46]), but since physical cost cannot always be manipulated in oper-ational environments, these studies present only a partial view of search behavior under particular circumstances. In addition, while these studies indicate how people act with a given technology, they do not show how people might act given different interface costs.

Researchers have also tried to model search costs by ex-amining mental effort. This represents a much smaller body of work (but no less important, since searching is, by na-ture, a mental activity) presumably because of the difficulty of measuring mental effort. Both Dennis et al. [15] and Gwizdka [20] have examined the cognitive costs of differ-ent interfaces using cognitive load theory. In these works, the authors conducted experiments to estimate the mental effort required to undertake various search interactions dur-ing the search process. However, the relationship among cost, performance and interaction was not explored.
In terms of theoretical research there has been a number of proposals that model costs and search behavior within a cost-benefit framework [2, 18, 4, 31, 32, 33, 34]. Bates [4], for example, suggests one of the search tactics users adopt while interacting with a system is to make decisions about whether to pursue the current strategy or to change strategy, depending on a cost-benefit analysis. While Bates did not formally pursue this idea, Russell et al. explored this in their work on the cost of sense-making [34]. Here they analyzed the possible actions a user could take during the information seeking process in terms of the gain that would be accumu-lated over time. Then actions could be compared under the assumption that users would try to maximize gain while minimizing total cost. In this process, gain can be seen as the amount of relevant information found (or the value of the relevant information found). Information Foraging Theory (IFT) [31] provides a theoretical grounding for these ideas. IFT models how users would act and behave within het-erogeneous information environments in ecologically valid ways. Specifically, it proposes that information seekers aim to minimize effort and maximize gain as they move between information patches, follow scents and assume a particular information diet. In experiments using a clustering inter-face based on the Scatter-Gather principle, it was shown that users tend to act in an ecologically valid manner (that is, they conserve effort while seeking the most gain) [33].
Another way to formally model the information seeking process is through microeconomics. Azzopardi [2] proposes Search Economic Theory (SET) as a way to predict and ex-plain search behavior. The model consists of a gain function and a cost function, which are parameterized by the type and number of interactions performed during the search process. Like the other formal models, the model assumes people will seek to minimize costs and maximize gain. Unlike the pre-vious work, the theory was specifically developed to model the interaction between a user and an information retrieval system. As a result, the theory may provide useful insights into search behavior and can guide empirical investigation. In this paper, we explore how query costs affect search be-havior under SET and then describe an empirical study that investigates this theory.
In Azzopardi [2], the search process was modeled using an analogy to Production Theory [44]. In Production The-ory, a firm takes inputs (i.e., capital and labor) and converts them to output (i.e., widgets). When applied to search, a user with a search engine is considered the firm, the user X  X  interactions are considered inputs and the relevant informa-tion found is considered as the output (and measured by Cumulative Gain ( CG ) [24]). Azzopardi defined the inputs as the number of queries posed ( Q ) and the number of doc-uments assessed per query ( A ). A functional relationship was proposed such that performance was related to interac-tion as follows: CG = g ( Q , A ). This function (referred to as the search production function) denoted the upper bound on output given a specific input combination (i.e., the max-imum that can be produced with the given inputs). It was shown that for several standard information retrieval mod-els (such as Boolean, VSM with TFIDF and BM25 [11]) the function g ( Q , A ) could be modeled closely with a Cobbs-Douglas production function [44]: where k denoted how well the hypothetical user could con-vert actions into relevant documents using the search sys-tem, and b was a mixing parameter which regulated the in-terplay between querying and assessing. The following cost model c ( Q , A ) was then used to ascribe a total cost to the interactions undertaken [2]: where a denoted the relative cost of querying to assessing. The cost of assessing documents was assumed to be 1 (where the total number of documents assessed was Q multiplied by A ).

Given this model of interaction defined by the gain and cost functions, it is possible to explore how the search be-havior of a user would change when different variables are manipulated. We conducted a simulation to illustrate what changes in search behavior we could expect to see when query cost increases under the proposed model. We explored a range of relative querying costs ( a = 0 . 5 , 1 , 2 , 4) across var-ious search production functions where b was varied from 0.5 to 0.6 and k was set to 3. Then we set CG = g ( Q , A ) = 30 No. of Queries Total Cost Figure 1: An example production function when b=0.55 (top), and the corresponding cost curves for different levels of a (bottom). The red line indicates the minimum cost point on each cost curve. which models the situation where users are looking for 30 relevant documents (assuming binary relevance for comput-ing cumulative gain) 1 .

Figure 1 shows the search production function when b = 0 . 55 (top plot) and the corresponding cost curves (bottom plot). Each point on the production curve represents the number of queries ( Q ) and the number of assessments per query ( A ) required to find 30 relevant documents. When b = 0 . 55, there exists a number of possible combinations of inputs that would yield the desired output: a user could, for example, issue approximately 10 queries and assess 10 documents per query, or issue approximately 4 queries and assess 20 documents per query to obtain the same gain.
The bottom plot shows the corresponding cost curves as-sociated with the gain function when b = 0 . 55. The red line indicates the minimum cost point on each cost curve (i.e., the combination of inputs which minimizes the cost given the relative cost of querying). It is clear from this plot that as the relative query cost increases, the number of assessments per query also increases (and this results in a correspond-ing decrease in the number of queries issued). This trend Table 1: For different production curves character-ized by b, the A and Q that result in the minimum cost (mc) for the relative query cost a is shown. As a increases, Q decreases and A increases. is similar for different values of b . Table 1 shows a number of outcomes where b is varied between 0 . 51 and 0 . 59. The combination of Q and A that minimizes the total cost for each a is shown for each gain function with value b . These results show more generally that the model predicts the fol-lowing: as relative query cost increases (i.e. a = 0 . 5  X  4), to minimize overall cost, a user should decrease the num-ber of queries issued and increase the number of documents assessed per query.

It should be noted that as b decreases, assessing docu-ments becomes more productive (i.e. there are more relevant documents in the ranked list) and so assessing will begin to dominate the production process. That is, at some point the best strategy is to keep looking at more documents, rather than querying. This is because as b decreases the gain derived from assessing documents increases. Put more formally, as b tends to zero, then A 1  X  b tends to A . For the above cost function, this point is when b  X  0 . 5, where from then on, the combination of inputs that minimizes the total cost is to issue one query, and then continue assessing until the desired level of gain is obtained. This can be seen as a boundary case. On the other hand, if b increases, then the combination of inputs that minimizes costs tends towards issuing many queries and assessing only one document per query (again this depends on the cost model). That is, if high precision queries are very cheap then it makes sense to keep issuing them until the desired level of gain is achieved. However, as the cost of a query increases there will be a point where it is better to substitute querying for more as-sessments per query. The b values shown in Table 1 show this trade-off. Note the b values shown are representative of standard IR algorithms [2].

This model of search behavior follows the basic economic principle that if cost goes up, consumption goes down [44]. It also reflects some of the empirical and simulated observa-tions made in prior work [3, 7]. While this is promising, the model and its predictions are based on a number of assump-tions which need to be considered.

Modeling Caveats and Assumptions Firstly, the model assumes that users are rational and will behave such that they would minimize interaction costs, and maximize per-formance. This is a common modeling assumption often employed ( c.f. [4, 31, 32, 33, 34]). In the context of search-ing, an operation that users repeat and practice often, it has been shown that users adapt their behavior to systems [38], and that users do try to minimize effort and maximize per-formance [33] (i.e. they subscribe to Zipf X  X  Principle of Least Effort). While the model may overestimate how well users could use an IR system, the assumption that they will try to optimize their behavior is, at least, reasonable. On an operational level, the model assumes that users will assess a fixed number of documents per query. However a user is likely to examine a different number of documents per query depending on the performance of the query. Given this as-sumption, it means that the model is rather coarse grained, considering the average number of documents assessed per query. A second operational point is that the cost model seems to ignore other costs, like viewing snippets, and how the costs of certain interactions may increase or decrease during the information seeking process. For example, a user may begin to run out of ideas for queries, and thus, increase the amount of time to generate subsequent queries. Ulti-mately, these simplifying assumptions reduce the problem to only the most salient factors, enabling us to reason about the relationship between the two main interactions within the search process: querying and assessing. The model gen-erates a testable hypothesis, which we shall refer to as the cost-interaction hypothesis :
To test this hypothesis, we conducted an experiment, where we operationalized cost in terms of query time as done in [40]. Three interfaces were created that required different amounts of time to enter queries. Subjects were randomly assigned to use one of these interfaces (Figure 2): 1. Structured Query Interface (high cost); 2. Standard Query Interface (medium cost); 3. Query Suggestion Interface (low cost).
 Aside from the different query facilities, these interfaces were similar and displayed 10 search results per page. Each query facility occupied the same amount of vertical space. To ap-proximate the query cost in terms of time, we employed the GOMS Keystroke Level Model [8] using the timings from a search experiment by [39] (shown in Table 2). For this ap-proximation, we assumed that the average length of a query was three terms, and each term was, on average, seven char-acters in length. A summary of the GOMS analysis for these query interfaces is provided in Table 3. This analysis is dis-cussed in more detail in each sub-section below.
The Standard interface functioned as the baseline and is similar to what is provided by modern search engines. We es-timated that this interface would require a medium amount of query effort relative to the other interfaces. To issue a query using the Standard interface subjects need to: (1) Go Figure 2: The Structured query interface (be-hind), the Standard query interface (middle) and the Query Suggestion interface (front). to the search box, (2) Enter 3 query terms of seven char-acters in length (plus two spaces), (3) Submit the query by pressing the return key and then wait for a response. With respect to the GOMS analysis, the corresponding low-level actions for each of these steps are (1) MHPCH, (2) 3*7K+2K, (3) KR. Using the estimates shown in Table 2, the total amount of time is 10.9 seconds per query.
The Structured interface consisted of 15 query boxes (3 rows x 5 columns), a search button and a plus button. Sub-jects could enter one term per box, but could not press the tab or enter keys to move among boxes. Each row of search boxes provided different Boolean functionality: AND, OR and NOT. While this interface might seem overly cumber-some, such search interfaces are common in commercial ser-vices; for example, EBSCO Host and ERIC provide a struc-tured query interface similar to the one used in this study (see Hearst [22] for other examples). The plus button al-lowed query term boxes to be added. For this interface the GOMS analysis is as follows: (1) Go to a search box (MH-PCH), (2) Type in the first query term (7K), (3) Go to the next search box (MHPCH), (4) type in the second query term (7K), (5) type in third query term (MHPCH + 7K), Table 2: GOMS Keystroke Level Model (time in seconds for each low level interaction)[8].
 Table 3: Estimated GOMS total time spent querying in seconds for each interface. and then (6) Submit the query -which requires the user to click the submit button (HC) and wait for response (R). The estimated total time required to enter each query is 17.6 sec-onds. To ensure that the physical costs associated with the Structured interface were as close to the GOMS model as possible, subjects had to retype query terms if they wanted to modify their previous query, i.e., query terms were re-moved from the boxes once the query was submitted, but the query was displayed on the screen. We assumed that this interface would also increase mental effort because sub-jects would have to understand how to enter terms.
The Suggestion interface was identical to the Standard interface except query suggestions were presented after sub-jects entered their initial queries. For each topic, eight query suggestions were provided. These queries were collected in a previous study [26] and led to good results, where the queries retrieved between two and five TREC relevant doc-uments in the top ten results. To issue a query using the Suggestion interface subjects needed to perform the same ac-tions as for the Standard interface. However, for subsequent queries, subjects could select query suggestions, rather than type in queries. This resulted in the action sequence MH-PCR taking approximately 3.8 seconds per suggestion. The Suggestion interface, while decreasing the amount of time to issue a query, should also reduce the mental effort associated with querying as it provides useful predefined suggestions. The query suggestions can be considered as a type of exter-nalization, which is generally believed to reduce cognitive load in human-computer interactions [37]. Gwizdka X  X  study of cognitive load in search found that an interface that dis-played category terms reduced participants X  cognitive load during query modification [20]. Thus, we anticipated that the query suggestion interface would also require less mental effort than the other two interfaces.
A 3GB Text Retrieval Conference (TREC) test collection of over one million newspaper articles was used [45]. We se-lected three search topics from this collection: 344 (Abuses of E-mail); 347 (Wildlife Extinction) and 435 (Curbing Pop-ulation Growth). We selected topics that had some contem-porary relevance, that we thought would be of interest to our target subjects and had a similar number of relevant documents available (123, 165 and 152, respectively). Our selection was also based on evidence from previous user stud-ies with a similar system setup [26] where it was shown that the difficulty of these topics did not significantly differ. Sub-jects searched all three topics, where the topics were rotated with a Latin-square. The Whoosh IR Toolkit was used as the core of the retrieval system, with BM25 as the retrieval algorithm, using standard parameters, but with an implicit ANDing of query terms to restrict the set of retrieved docu-ments to only those that contain all the query terms (similar to BM25A used in [2]). Subjects were not provided with a tutorial of the system.
To measure the impact of the cost variations on search be-havior, the following signals were logged as subjects searched: start time of search tasks, end time of search tasks, queries issued, query suggestions used, results pages viewed, doc-uments viewed, documents marked relevant, and task de-scriptions viewed. From the log it was possible to calculate the amount of time subjects spent issuing queries; examin-ing search results pages and reformulating queries at this page; and viewing documents. It was also possible to exam-ine features of the search interaction, such as, the number of terms per query, the depth of the last document viewed in the rank list, the number of queries issued and the number of documents saved.
The NASA Task Load Index (TLX) questionnaire was used to elicit subjects X  perceptions of: Mental Demand, Phys-ical Demand, Temporal Demand, Performance, Effort and Frustration. After finishing the three search tasks, subjects completed a NASA TLX questionnaire to rate their over-all experience with the system, and then completed another NASA TLX questionnaire focused specifically on querying (Table 7). We reduced the number of scale points from 21 to 7 (the number of points on the computer version of the scale were difficult to distinguish and psychometric research shows that there is little gain in reliability beyond 11 points [30]) and we modified the factor statements so they matched the target task (in Hart X  X  2006 review of the usage of the NASA TLX it was noted that this modification was often performed by researchers [21]). Subjects were recruited from the University of Glasgow, UK 2 . Thirty-six undergraduate student subjects participated (12 subjects per interface). Twenty-one subjects were fe-male and fifteen were male. Their average age was 21.8 (SD=4.4). Forty-seven percent were science majors and 53% were humanities majors. Subjects X  search experience was measured using a modified version of the Search Self-Efficacy scale [13]. This instrument contains 14-items describing dif-ferent search-related activities. Subjects respond to each item by indicating their confidence in completing each ac-tivity on a 10-point scale (1=totally unconfident; 10=totally confident). Subjects reported fairly high search self-efficacy [M=7.26 (SD=1.69)].
Subjects were instructed to imagine they were newspaper reporters and needed to gather documents to write stories about the provided topics. Subjects were told that there were over 100 relevant documents in the collection for each topic and they should try to find as many of these as possi-ble during the allotted time (10 minutes per topic). To in-centivize subjects, monetary bonuses were given to the top three performers for each topic per condition. Each subject was compensated with  X  10 and could earn an extra  X  2.50 per topic as a bonus.
Results are presented in three sections. The first two sec-tions show how the search interface affected subjects X  search behaviors, including the amount of time they spent engaged in different search processes. The third section presents re-sults of the NASA TLX. We used ANOVA and post-hoc tests with Bonferroni X  X  correction for analysis.
The mean number of queries submitted by subjects, doc-uments assessed per query, relevant documents found and assessment depth per query is shown in Table 5. Depth is the average position at which the last document was viewed in the search results list when at least one document was viewed.
 Interactions Structured Standard Suggestion Queries (Q) 19.4  X  10.6 35.0  X  8.9 31.2  X  10.4 Query Len. 3.7  X  1.49 3.5  X  0.7 3.2  X  0.5 Assess. / Q 4.7  X  4.2 1.6  X  0.5 2.5  X  1.5 Rels. Saved 47.7  X  24.1 34.7  X  18.5 43.2  X  18.7 TREC Rels. 17.3  X  8.1 11.5  X  7.3 15.3  X  6.7 Table 5: Search behavior and performance recorded per interface.

Subjects who used the Structured interface issued signif-icantly fewer queries than those using the Standard and Suggestion interfaces ( F (2 , 33) = 7 . 59, p &lt; 0 . 01). There was no difference in query length across different interfaces. Subjects who used the Structured interface viewed signifi-cantly more documents per query and went to greater depths to view these documents. Both the number of documents examined per query ( F (2 , 33) = 3 . 26, p = 0 . 05) and the depth per query were significantly different ( F (2 , 33) = 4 . 45, p &lt; 0 . 01), but only between the Structured and Standard interfaces. Subjects who used the Structured interface saved the most documents, followed by those who used the Sug-gestion interface and those who used the Standard interface, but these differences were not significant. Of these saved documents, the number relevant was determined by using the relevance judgments included in the TREC test collec-tion. Subjects who used the Structured interface found more relevant documents than those who used the Suggestion and Standard interfaces. However, these differences were not sig-nificant.
Table 6 displays the average amount of time subjects spent issuing their first query, on search results pages (QSERPs) and on document pages. The average amount of time spent on QSERPs includes the time spent viewing snippets and formulating queries. Subjects who used the Structured inter-face spent approximately 20 seconds longer formulating their first query than subjects who used the Standard and Sug-gestion interfaces ( F (2 , 33) = 4 . 05, p = 0 . 027). Follow-up tests confirm that subjects who used the Structured interface took significantly more time to construct their initial queries than subjects who used the other two interfaces. These sub-jects also spent significantly more time on QSERPs, view-ing snippets and reformulating queries ( F (2 , 33) = 8 . 149, p &lt; 0 . 01). There were no statistically significant differences in the amount of time subjects spent viewing documents among interface conditions, although subjects who used the Standard interface spent slightly more time per document. Interactions Structured Standard Suggestion First Query 44.1  X  35.7 22.9  X  11.3 19.9  X  12.3 Documents 15.1  X  7.9 17.4  X  5.1 15.3  X  6.5 Table 6: Mean (SD) time (in seconds) spent formu-lating first queries, on search results pages viewing snippets and reformulating queries (QSERP) and viewing documents.
Table 7 (top) displays the overall NASA TLX scores for each factor for each interface. These results capture the overall workload experienced by subjects as they engaged in all search behaviors (querying, viewing snippets and assess-ing documents). Subjects using the Standard interface re-ported experiencing the highest mental demand, followed by those using the Structured and Suggestion interfaces. This ordering was consistent for physical demand and temporal demands. Subjects indicated similar levels of success (per-formance), effort and frustration. None of the differences identified above were statistically significant.

Individual factor scores were summed to arrive at a total workload score. The Standard interface received the high-est overall workload score, followed by the Structured and the Suggestion interfaces. These differences were also not significant. Table 7 also provides information about the rel-ative contributions of each factor to overall workload for each interface. For example, effort was rated as the high-est contributor to load by those who used the Structured and Suggestion interfaces, while mental demand was rated highest by those using the Standard interface. More interest-ingly, physical demand received the lowest scores regardless of interface. It was also the case that temporal demand was evaluated as the second highest contributor to load by sub-jects who used the Structured and Standard interfaces, but as the second lowest for those using the Suggestion interface. Table 7 (bottom) displays the NASA TLX ratings for System Load Interface Query Load Interface query load. These results help us understand the load expe-rienced by subjects as they queried. Subjects using the Stan-dard interface reported experiencing the highest mental de-mand when querying, followed by those using the Structured and Suggestion interfaces. There were no differences among interfaces according to physical demand. With respect to temporal demand, subjects using the Structured and Stan-dard interfaces reported higher demands than those using the Suggestion interface. Subjects who used the Structured interface described their queries as more successful (perfor-mance) than those using the Standard and Suggestion inter-faces. Subjects using the Standard interface reported greater levels of frustration and effort. None of the differences iden-tified above were statistically significant. With respect to overall query load, the Structured and Standard interfaces received similar scores, which were higher than overall query load for the Suggestion interface. These differences were not significant.
We found that subjects who used the Structured (high cost) interface submitted significantly fewer queries, exam-ined more documents per query and went to greater depths in the search results list than subjects who used the lower cost Standard and Suggestion interfaces. This finding sup-ports the cost-interaction hypothesis we generated from the microeconomic theory: as the cost of querying increases, the number of queries issued will decrease and the num-ber of documents evaluated will increase. However, counter to what we expected, we found that subjects who used the Standard interface, which was constructed to represent medium cost, issued the most queries, viewed the fewest documents per query and were the shallowest in their evaluation of the search results list. Our expectation was that these behaviors would be associated with the Suggestion interface since the cost of querying was considered lowest.

One possible explanation for this finding is that subjects who used the Suggestion interface may not have experienced any meaningful differences with respect to cost since they did not always click on the available suggestions. On average, subjects selected 7 . 31 query suggestions out of 24 query sug-gestions, which were available to them during their search sessions. The initial GOMS analysis assumed a greater up-take of suggestions, which would have resulted in much lower querying costs. If we revise our GOMS estimate, based on the actual usage statistics, then the time spent per query on the Suggestion interface would have been approximately, 9 . 24 seconds, on average 3 . This indicates that in terms of querying, the costs between these interfaces was very sim-ilar (and not as great as we originally anticipated). The empirical findings also confirmed that there was no signifi-cant difference in querying time between the Suggestion and Standard interface. However, the Suggestion interface may have introduced some added costs since subjects needed to examine and make decisions about the query suggestions. This is partially supported by the differences in the amount of time subjects spent on QSERPs evaluating snippets and formulating queries. Subjects who used the Suggestion in-terface spent slightly longer on QSERPs than subjects who used the Standard interface (34.7 and 28.6 seconds, respec-tively) and this increased time might have reflected time spent evaluating query suggestions. Though, here, the dif-ferences were not significant, which implies that the querying cost between these conditions was not different.

This raises a number of issues when applying the microe-conomic theory in practice: (1) what costs are at play and how do we estimate them within the cost function, and (2) if the query costs were not significantly different, why do we observe different behavior between the Standard and Sug-gestion interfaces. Regarding costs, our research has shown that cost and how it is operationalized within the economic model needs to be reconsidered. Is the querying cost inclu-sive of the time spent viewing snippets or not, and if not, is it a document cost? If we use the QSERP estimates to denote the query cost (i.e. time spent querying and viewing snippets per query) then the Standard interface would be the least expensive: and our findings would be consistent with the cost-interaction hypothesis. However, the theory and the cost models employed would need to be refined, perhaps, re-defined, to determine whether this is appropriate. We leave this to further work, and consider alternative explanations regarding the differences in search behavior between these two interfaces below.

We first considered whether the results with regard to the Standard and Suggestion interfaces were caused by dif-ferences in query quality and search result quality. Since the Suggestion interface provided good quality queries that had reasonably high precision, differences in query qual-ity might explain why subjects who used the Suggestion interface tended to assess more documents per query and issue fewer queries than subjects using the Standard inter-face (i.e., they might have seen better search results). It is important to note that query quality differences would not Table 8: Mean precision of queries issued across each interface. * (**) indicates significantly better than Structured (Structured and Standard). have been caused by individual ability since subjects were randomly assigned to interface condition.

To determine whether there was a difference in query qual-ity, subjects X  queries were submitted to the retrieval system and evaluated using TREC relevance judgements to bet-ter understand what types of performance subjects encoun-tered. Table 8 reports the precision values at 5, 10 and 15 documents along with the number of queries issued in each group. The results show that the quality of the queries on the Suggestion interface was higher than the other two inter-faces. Statistical testing revealed that system performance was significantly better across these three precision measures ( p &lt; 0 . 05). This finding suggests that subjects who used the Suggestion interface issued better quality queries and potentially encountered more TREC relevant documents in the ranked lists, but we note that there were no significant differences in the number of documents subjects saved or the number of TREC-relevant saved, so we cannot conclude that subjects using the Suggestion interface experienced bet-ter performance, especially considering the work of Turpin and Hersh [43] who have shown that system performance evaluated in this way does not always map to user perfor-mance.

The results from the NASA TLX also provide additional insights about how subjects experienced these interfaces, and the extent to which cost, as we have manipulated it in this study, impacted their experiences. Although many interesting differences are discussed below, it is important to keep in mind that none of these were statistically signifi-cant, so this discussion is primarily presented in the service of future research.

The NASA TLX was administered twice: once to focus on system load and once to focus on query load. The system load allowed us to understand the entire experience (query-ing, evaluating snippets and results, search result quality), while query load isolated the load introduced by the query facilities. One of the most interesting and consistent findings was that physical demand contributed the least to subjects X  overall loads. For query load, the Structured, Standard and Suggestion interfaces were rated nearly the same for this factor (2.2, 2.2, and 2.1, respectively). There were greater differences in subjects X  ratings of physical demand for sys-tem load, with the Standard interface receiving a rating of 2.7, followed by Structured (2.3) and Suggestion (1.9) inter-faces. These ratings were, in general, low, when compared to the other factors, which suggests that our manipulation of physical demand may not have been as extreme as expected.
Results also showed that subjects associated the most mental demand with the Standard interface (4.8), followed by the Structured (4.2) and Suggestion (3.7). This ordering was consistent for the mental demand associated with sys-tem load, although the values were slightly larger (5.1, 4.5 and 3.9, respectively). Compared to the ratings for physical demand, it is clear that mental demand contributed more to subjects X  overall workloads. The Suggestion interface re-ceived the lowest ratings for mental demand as expected, but the Standard received higher ratings than the Struc-tured, which was unexpected. We believe that the higher mental demand associated with the Standard interface was a result of subjects creating more queries while searching.
While one might argue that the Structured interface is clunky and less usable than the other interfaces, subjects did not report high levels of frustration with the query facil-ity. Rather, the highest levels of frustration were reported by subjects who used the Standard interface. This suggests that subjects did not find the added time costs associated with the Structured interface annoying and that subjects were more frustrated by having to enter more queries with the Standard interface. The Suggestion interface, as ex-pected, received the lowest frustration rating for query load. These results make us question how subjects would have rated the usability or aesthetics of the interfaces. We hy-pothesize that subjects would rate the Structured interface the lowest, which is interesting since they actually performed better with it. We leave this for future work.

A final difference we observed in the NASA TLX was with respect to the temporal factor, which gauged how hurried or rushed subjects felt when doing the search tasks. Subjects who used the Standard interface reported experiencing the highest levels of temporal demand (5.0), followed by those who used the Structured interface (4.6) and those who used the Suggestion interface (3.6). This difference suggests that a system that encourages the type of search behavior that subjects who used the Standard interface engaged in -is-suing more queries, evaluating fewer documents per query and shallowly evaluating search results lists -might have negative psychological consequences, assuming that feeling hurried and rushed generates stress. There were no differ-ences between the Standard and Structured interfaces for temporal demand for query load, which seems to further suggest that it was the total search strategy engaged in by those who used the Standard interface that contributed to the differences in overall temporal demand.
In this paper, we investigated how query cost affects search behavior. We used microeconomic theory to motivate our study and conducted a theoretical analysis, which generated the cost-interaction hypothesis . A laboratory study with 36 subjects was conducted to evaluate this hypothesis using three interfaces: Structured, Standard and Suggestion. We found partial support for this hypothesis: subjects who used the high cost Structured interface submitted fewer queries, spent more time on search results pages, examined more doc-uments per query, and went to greater depths in the search results list than subjects who used the lower cost Standard and Suggestion interfaces.

Our results have both theoretical and practical implica-tions. Attempts to formally model search interaction are promising, but the results of this study suggest that refine-ments to the microeconomic theory are required to improve the realism of the model. Our results also imply that at least one additional factor should be included in the gain and cost functions to account for viewing snippets, and that more sophisticated cost functions may be more appropriate. We found that in testing the theory, care needs to be taken to control each of the factors involved. Changing the costs may change other aspects of the interaction and these need to be accounted for when testing hypotheses generated by the model.

Future work will focus on further refining the search eco-nomic theory/models and exploring how alternative designs for query facilities and other search interface features might encourage users to engage in and adopt more positive and successful search behaviors.

Acknowledgments We would like to thank Kelly Mar-shall for her help in conducting the user experiments. [1] E. Agapie, G. Golovchinsky, and P. Qvardordt.
 [2] L. Azzopardi. The economics in interactive [3] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Time [4] M. J. Bates. Training and education for online. [5] N. J. Belkin, D. Kelly, G. Kim, J.-Y. Kim, H.-J. Lee, [6] N. J. Belkin and A. Vickery. Interaction in [7] J. Brutlag. Speed matters for google web search. In [8] S. K. Card, T. P. Moran, and A. Newell. The [9] M. D. Cooper. A cost model for evaluating [10] W. S. Cooper. On selecting a measure of retrieval [11] B. Croft, D. Metzler, and T. Strohman. Search [12] J. Dabrowski and E. V. Munson. 40 years of searching [13] S. Debowski, R. Wood, and A. Bandura. The impact [14] A. R. Dennis and N. J. Taylor. Information foraging [15] S. Dennis, P. Bruza, and R. McArthur. Web searching: [16] G. Dupret and B. Piwowarski. A user behavior model [17] K. Franzen and J. Kalgren. Verbosity and interface [18] N. Fuhr. A probability ranking principle for [19] K. Fujikawa, H. Joho, and S. Nakayama. Constraint [20] J. Gwizdka. Distribution of cognitive load in web [21] S. G. Hart. Nasa-task load index (nasa-tlx); 20 years [22] M. A. Hearst. Search User Interfaces . New York, NY: [23] J. A. Jacko, A. Sears, and M. S. Borella. The effect of [24] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [25] D. Kelly. Methods for evaluating interactive [26] D. Kelly, K. Gyllstrom, and E. W. Bailey. A [27] D. Kelly and C. Sugimoto. A systematic review of [28] A. Moffat and J. Zobel. Rank-biased precision for [29] F. F. H. Nah. A study on tolerable waiting time: How [30] J. C. Nunnally. Psychometic Theory . McGraw, 1978. [31] P. Pirolli and S. Card. Information foraging. [32] P. Pirolli and W. T. Fu. Snif-act: a model of [33] P. Pirolli, P. Schank, M. Hearst, and C. Diehl. [34] D. M. Russell, M. J. Stefik, P. Pirolli, and S. K. Card. [35] I. Ruthven. Interactive information retrieval. Annual [36] G. Salton. Evaluation problems in interactive [37] M. Scaife and Y. Rogers. External cognition: How do [38] C. L. Smith and P. B. Kantor. User adaptation: good [39] M. D. Smucker. Towards timed predictions of human [40] M. D. Smucker and C. L. Clarke. Time-based [41] L. T. Su. Evaluation measures for interactive [42] N. J. Taylor, A. R. Dennis, and J. W. Cummings. [43] A. Turpin and W. Hersh. Why batch and user [44] H. R. Varian. Intermediate microeconomics: A modern [45] E. M. Voorhees. Overview of the trec 2005 robust [46] E. Yilmaz, M. Shokouhi, N. Craswell, and
