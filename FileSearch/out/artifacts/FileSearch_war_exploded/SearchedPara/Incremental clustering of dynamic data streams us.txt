 1. Introduction
Stream data clustering is a challenging area of research that attempts to extract useful information from continuously arriving data [22,12] . Limitations imposed by available computational resources typically constrain our ability to process such data and restrict, if not completely remove, our ability to revisit previously seen data points. Given these conditions we still desire effective means by which to identify trends and changes in evolving stream data. Furthermore, we would like work packet classification, financial databases, e-mail analysis and multimedia. Stream data clustering has been heavily investigated in the past years and, although the architectural aspects of processing data streams has received attention [19,20,17,10,56] , most effort concentrated on the mining and clustering aspect of the problem [26]. Several algorithms have been developed to handle stream data containing clusters of different shapes, sizes and densities but only a small number can handle difficult clustering tasks without supervision.

Generally, the algorithms require expert assistance in the form of the number of partitions expected or the expected den-the clusters observed and, thus, have the disadvantage that they are required to re-learn any recurrently occurring patterns.
The aims of our work have been to develop an algorithm that can handle all types of clusters with minimal expert help and to provide a graph-based description of changes and patterns observed in the stream in order to enable a detailed analysis of the knowledge acquired. The contribution of our work is a highly accurate stream processing algorithm that can handle all types of numeric data while requiring only limited resources. The novelty of our work comes in the form of an integrated knowledge repository which is used in tandem with the stream data clustering algorithm.
In this paper, we present RepStream, a sparse-graph-based stream clustering approach that employs representative clus-ter points to incrementally process incoming data. The graph-based description is used because it allows us to model the spatio-temporal relationships in a data stream more accurately than is possible via summary statistics. Each cluster is de-ter and predictor points which are used to capture the evolving properties of the cluster.

A critical aspect of our research has been to avoid having to  X  X  X e-discover X  previously learned patterns by maximising the relevant changes occurring in the clusters over time. The use of the repository offers two major benefits.
First, the algorithm can handle recurrent changes in the clusters more effectively by storing a concise representation of to discard older data points in order to adhere to constraints in available memory and computational resources while contin-higher classification accuracy than would be possible if only the most recent cluster trends were retained.
Second, the repository provides a concise knowledge collection that can be used to rebuild a cluster X  X  overall shape and recall of historical changes is desired.

The clustering effectiveness of the algorithm has been extensively investigated and is demonstrated via experimental re-sults obtained clustering both synthetic and dynamic real world benchmark data sets. Furthermore, the clustering quality of the algorithm is shown to produce substantially better results when compared with three of the most popular current stream clustering algorithms: DenStream, HPStream and CluStream.

A preliminary version of this paper appears as [43]. In this paper, we present a more detailed explanation of the workings of the algorithm and its relationship to existing works. Furthermore, we offer an extended set of experimental results and investigate the algorithm X  X  parameter sensitivity. 2. Related work
Several important stream clustering algorithms have been introduced in recent years. One of the first data stream clus-tering methods to consider the archival of cluster information was CluStream [4]. The algorithm uses microclusters to cap-ture and record statistical summary information suitable for off-line analysis. The amount of information that is archived is controlled by a user specified maximum number of microclusters with the algorithm attempting to capture as much detail as rithm X  X  suitability for many real world data sets. CluStream has recently been enhanced to deal with uncertainty in the data stream, increasing the accuracy of the algorithm when clustering noisy data sets [7].

HPStream, a modification of CluStream to enable clustering of high-dimensional data, was proposed in [5]. The algorithm employs a data projection method to reduce the dimensionality of the data stream to a subset of dimensions that minimise the radius of cluster groupings. It was demonstrated that by projecting data onto a smaller set of dimensions both synthetic and real world data sets could be more accurately processed. As with CluStream, however, the underlying assumption re-mains that clusters in the projected space remain spherical in nature. How best to classify incoming data using the CluStream and HPStream frameworks was discussed in [6].

Birch [54] is a well known hierarchical clustering algorithm that incrementally updates summary cluster information for the data. The algorithm was later adapted for online clustering and classification by combining the secondary off-line phase with the incremental update component [27].

A k -median approximation algorithm designed for stream clustering was introduced in [48,29] while a k -means approach approaches, most notably the requirement that users supply the number of clusters to be searched for and the assumption adaptability to the distributed data clustering domain; see [13] for an example and the references contained therein.
A statistical approach to distributed data stream clustering based on expectation maximisation (EM) was recently intro-peer network. The proposed algorithm, CluDistream, employed a test-and-cluster approach to efficiently cluster points, a strategy which requires the mixture models to only be updated on a sample of points that probabilistically belong to an existing cluster. Cluster splits and merges in CluDistream are event driven and managed by a co-ordinator host which re-ceives notifications of significant changes to the processing hosts mixture models. Such notifications allow the algorithm
The work of Gao et al. [28] introduced DUCstream, a grid-based technique that seeks dense  X  X  X nits X , or regions, in a mul-tidimensional space. Clusters are discovered by applying the CLIQUE [8] algorithm to regions that are considered to be dense.
The algorithm adapts to changes in the data stream by disregarding regions whose density fades over times. This approach, cessed in blocks.
A more recent grid-based approach was introduced with the Cell Tree in [49]. Each cell in the Cell Tree stores a statistical been reached. Each cell stores the time-weighted mean and standard deviation of the data distribution within the cell, en-able to adapt to the changing data distributions by further partitioning regions when their density grows or by merging adja-cent cells as the data they represent decay.

The literature comprises a considerable number of graph-based clustering approaches [32,36,37,53] . Recent works related to our proposed approach are referred to here.
 A well known algorithm, Chameleon [41] uses the h M E T I sures of relative interconnectivity and closeness. A similar approach is used in MOSAIC [21] where representative based via a proximity map to form more complex cluster structures.

More recently, a multi-density clustering technique that extends the DBSCAN [24] density-based clustering approach to stream clustering was proposed in [18]. The algorithm, DenStream, extends DBSCAN by adapting the original density based connectivity search to a microcluster approach. The microclusters allow DenStream to efficiently summarise the overall shape of clusters as the data evolves without requiring the complete set of points to be retained in memory.
The final clustering can be obtained for any time step by applying a variant of the DBSCAN algorithm on the microclusters themselves.

An incremental version of DBSCAN was earlier proposed in [23]. As with DBSCAN, the algorithm obtains groupings based on the nearest neighbourhood connectivity of points within an a priori defined radius known as the -neighbourhood. Clus-ters are formed by identifying density-reachable chainings between sets of well connected points. Incremental DBSCAN is able to cope with arbitrary point insertions and deletions, enabling the algorithm to forget past data. No measure of useful-ness of the data is retained, however, limiting the algorithm to keeping only the most recent data points in memory. The algorithm is therefore likely to discard possibly reusable cluster information without consideration towards its value.
The ordering points to identify the cluster structure (OPTICS) algorithm [9], a multi-density extension of DBSCAN, identi-provided, the method instead requiring manual inspection of generated two-and three-dimensional plots.
Nasraoui et al. [47,46] proposed a clustering technique based on an artificial immune system in which incoming data points are presented to a network of artificial white blood cells (b-cells) that mimic the adaptive mutation observed in bio-logical immune systems. The algorithm classifies incoming data points based on the similarity of each point to existing b-cells and each cell X  X  radius of influence. Cells rapidly evolve with the data stream via an update method that simulates cell stimulation. New b-cells are created whenever no existing b-cell is stimulated by a new data point; the network of b-cells is periodically reduced by means of k -means clustering.

Two alternative measures to the often used minimum cut metric, graph expansion and conductance formed the basis for a graph partitioning scheme in [38,39] . Here graph expansion incorporates the relative size of sub-graphs into the cut mea-sure. Conductance, in contrast, gauges the importance of vertices by assigning them weightings based on their similarity to neighbouring vertices. Clustering attempts to find groupings that result in a minimum threshold for expansion or conduc-tance while simultaneously reducing the sum of the intercluster edge weights in the clustering system. This approach was expanded in [25] with a technique that combines the bicriteria used in [38,39] into a single parameter that bounds both the minimum graph expansion and the maximum intercluster edge weight.

Another technique that has been explored is the analysis of a stochastic process performing a random walk through a weighted graph in order to probabilistically isolate clusters. One such method is the algorithm in [31] which deterministi-cally analyses a similarity matrix to locate separations between vertices on a weighted graph. The Markov clustering (MCL) algorithm described in [52] similarly discovers cluster formations by updating a state transition matrix to simulate random walks. The first random walk method to be proposed, in contrast, required significant space and computational resources to generate actual walks [30].

None of the algorithms mentioned provide a means to selectively archive historical information. Those algorithms that facilitate archiving instead tend to approach the issue by storing summary statistics with which general changes in clusters can be revisited. 3. Preliminaries
Given a data stream P of time ordered points P  X f p 1 ; ... ; p erties. We define a cluster c to be a set of points c  X f p p  X f p i ; 1 ; ... ; p i ; D g of D dimensions. Let C be the set of clusters C  X f c
Let the set G  X f g 1 ; ... ; g j P j g consist of the ideal cluster assignments for points P such that the j th element g sis ), a member of a different genus. The distance between point p Euclidean or Manhattan distance between two points, although other domain specific functions may be desirable [35,1,2] .
Points are inserted into a directed k -nearest neighbour ( k -NN) sparse graph SG  X  V ; E  X  of vertices V  X f v edges E  X f e 1 ; ... ; e j E j g such that the i th vertex v
For example, a surveillance application benefits from knowing the spatio-temporal relationships in a suspect X  X  position over time as opposed to only labels and positions of locations.

Definition 1. ( Nearest neighbour ). Let NN  X  v i  X  be the set nearest neighbours of a vertex v
Let NN  X  v i ; j  X  be a function that gives the j th nearest neighbour of v
Definition 2. ( Incoming edges ). Given an ordered pair h v that Let j IE  X  v i  X j be the number of incoming edges of v i .

Definition 3. ( Outgoing edges ). Given an ordered pair h v
Definition 4. ( Reciprocally connected ). Let RC  X  v i  X  be the set of vertices that are reciprocally connected to a vertex v
Fig. 2 shows the nearest neighbours and the incoming edges of two sampled vertices from the sparse graph in Fig. 1 . Recip-rocally connected vertices along with outgoing edges of a sample vertex are demonstrated in Fig. 3 .
Let R  X f r 1 ; ... ; r j R j g be a set of representative vertices on SG such that 8 x ; r neighbour representative sparse graph which links the vertices W  X f w an ordered pair h v ; u i of vertices such that v ; u 2 R . Let NN neighbours, the nearest j th neighbour and the set of vertices that are reciprocally linked to a representative vertex r Fig. 4 depicts the representative sparse graph RSG formed from the sparse graph SG in Fig. 1 .

Definition 5. ( Predictor ). Let a representative r i be a predictor if r
Definition 6. ( Exemplar ). Let a representative r i be an exemplar if r graph SG. A vertex v i is made representative if at any time 9 = j ; v existing representative. Representatives are further categorised into a set of exemplar representatives R predictor representatives R P  X f r P 1 ; ... ; r P j R P j
Clustering decisions in RepStream are made via vertices representative of regions within the cluster space. Representative resent regions which possess a potential to become prominent cluster features but where such evidence has not yet been seen.
Connectivity between representative vertices is further governed by a measure of the relative density of the regions that they represent.

Definition 8. ( Relative density ). The density of representative vertex r RD  X  r i  X  :
Definition 9. ( Density-related ). Given a density scaler a , two representatives r conditions are true:
Definition 10. ( Singularity ). A representative vertex r reached zero and the following conditions are met: resentative vertices in cluster c 1 and the density-relation of the representatives are depicted in Fig. 5 . 4. Clustering stream data via representative points
Our cluster representation involves the use of dynamically updated sparse graphs that, when used in conjunction with a viously observed. The RepStream algorithm that we propose aims to capture such patterns in order to recall them at some future time should the change reoccur. RepStream is a single phase incremental algorithm that updates two sparse graphs of k -nearest neighbour connected vertices in order to identify clusters among data points. The first of these graphs is used to capture the connectivity relationships amongst the most recently seen data points and to select a set of representative ver-tices. The second graph is used to track the connectivity between the chosen representative vertices. The connectivity of the representative vertices on both graphs then forms the basis for the algorithm X  X  clustering decision making. An overview of the relationship between the two sparse graphs is provided in Fig. 6 . 4.1. Algorithm overview
A new vertex joins an existing cluster if it is reciprocally connected to a representative vertex v ation of the new cluster may trigger an immediate merge with an existing cluster if the conditions for merging are met.
Representative vertices are used to make clustering decisions at a higher level. This offers two major advantages. First, since representative vertices typify a set of nearby data points, decisions made at this level improve performance by requir-ing only a subset of the data to be considered. Second, representative vertices are associated with a measure of usefulness tures. This retention allows the algorithm to accurately classify new data points arriving within a region of the clustering space where the non-representative vertices have since been retired.

Cluster merges and splits are conditional on the connectivity of representative vertices on the higher level representative sparse graph along with the density of vertices on the sparse graph SG in local proximity to their representative vertices.
Since these decisions are made at the representative vertex level, larger clusters are formed when two or more representa-tive vertices are seen to be density-related. Merges can, therefore, be made immediately when an update to the connectivity of vertices on the representative sparse graph sees the creation of a new reciprocal connection between two representative imity. Merges may also be triggered when the addition of a new, or the removal of an existing, vertex affects the density of can merge if the density of one or both representative vertices reduces over time to allow such a connection to be made.
Cluster splits are similarly detected by performing a split test whenever existing density-related connections between two representative vertices are lost. Monitoring the connectivity and relative density of representatives enables the algo-rithm to evolve with changes in the data.

A significant aim of RepStream is to retain those representative vertices that prove, over time, to be useful in representing repository. Retaining the most useful representative vertices permits the algorithm to  X  X  X emember X  previously encountered possible off-line analysis if constraints prevent the algorithm from retaining them.

Processing and memory constraints require that the algorithm discard information over time. This is accomplished by
All other representative vertices remain in memory; their deletion is instead managed via the repository update procedure detailed in Section 4.6.

The removal of a vertex requires updates to the sparse graph and the representative sparse graph. Graph updates are made to ensure that any vertices with edges directed at the removed vertex are updated with a new nearest neighbour. Rep-resentative vertices are also updated to ensure that their local density is adequately maintained.
 4.2. Sparse graph SG updates
Insertion of a new vertex v i into the sparse graphs is a multistep process that initially links each new vertex into sparse graph SG prior to updating any vertices that have been affected by the insertion. Line numbers referenced here refer to Algo-rithm 1 which provides an outline of the steps involved. First, a directed edge is created from v bours (line 7). A neighbour v j is updated to link back to v the distance from v j to v i (lines 8 and 9). The creation of a directed edge from v nection between the two vertices and causes vertex v j to be queued for further processing as a vertex that has gained a re-ciprocal link (line 10). If the addition of a directed edge from v then the outgoing edge from v j to its furthest neighbour is removed and v nearest neighbour (lines 11 X 13). This ensures that the k -nearest neighbour property of the sparse graph is maintained. Fur-ther, if the removal of the furthest neighbour of v j results in the removal of a reciprocal link then v vertices that have lost a reciprocal link to a neighbouring vertex (lines 14 and 15).

The vertices that have gained a reciprocal connection to the new vertex v sity of each representative vertex reciprocally connected to v itory (line 20). Such vertices are immediately unlinked from both sparse graph SG and sparse graph RSG, and archived to disk new vertex v i will be promoted to a representative vertex is made. Immediate promotion of a new vertex occurs, if no re-ciprocal link was established between it and an existing representative vertex, otherwise the new vertex is assigned to the cluster of its nearest representative (lines 25 and 26).

Non-representative vertices that have lost a reciprocal link are revisited to identify representative vertices that require updates to their local density (lines 27 X 30). Should such an update trigger the removal of a density-related link then that representative vertex is added to the queue of representatives requiring a cluster split check (line 31). Any non-representa-representative vertex (lines 32 X 34). This ensures that each data point is linked to, or acts as, a representative. 4.3. Sparse graph RSG updates
Inserting a new representative vertex into sparse graph RSG involves a process similar to vertex insertions on sparse graph SG. Line numbers here refer to Algorithm 2 which details the steps involved.

As with sparse graph SG, each new representative vertex r ciprocal link from a nearest neighbour r j is then made to r the furthest nearest neighbour of r j (lines 6 and 7). The k -nearest neighbourhood property of r 10) which may trigger the removal of a reciprocal link (lines 11 and 12).

The representative vertices are now used to detect cluster merges, the trigger condition being the creation of a density-are performed on those representative vertices that have lost a reciprocal link to another vertex on RSG (line 14).
In terms of complexity, in order for a new representative r the representative X  X  nearest k -nearest neighbours be initially identified which involves a O  X  k log j R j X  operation when employing the KD-Tree. Creating an edge from r i to a single neighbour is accomplished in constant time. Maintaining the ordering of a the nearest neighbours of r i , however, requires an additional O  X  log k  X  operation. Next, the algorithm checks the distance of r i to each of its neighbours to test whether the neighbouring representatives need to update their own nearest neighbourhood. This requires distance calculations of complexity O  X  d  X  for each of its neighbours. If each of the nearest neighbours of r i becomes reciprocally linked to r lish an reciprocal link from a neighbour r j back to r i . The removal of the furthest nearest neighbour of r stant time.

The worst case complexity of updating the links between r
An example of the sparse graph updates made due to the arrival of a new vertex is shown in Fig. 7 . Here the existing sparse graph in Fig. 7 a is updated with a new vertex with the updated edges shown in Fig. 7 b. The new vertex is not reciprocally linked to an existing representative vertex and, as such, is promoted as a new representative as seen in Fig. 7 c.
RepStream performs periodic vertex removals. This process requires updates being made to each vertex connected to the vertex v i being removed. A total of k outgoing edges and j IE  X  v sparse graph SG. Further, if the vertex being unlinked is a representative then a further k outgoing links and j IE incoming edges will be removed from the sparse graph RSG. The complexity of these edge removals is O  X  2 k  X  X  O  X j IE  X  v i  X j X   X  O  X j IE R  X  v i  X j X  .

Each vertex that was connected to the deleted vertex must be revisited to maintain its nearest neighbourhood. Each revis-the nearest neighbours of each neighbour of v i is hence O  X  k therefore of updating the links between v i and all of its k neighbours is therefore similar to that of Eq. (11):
The complexity of the remaining operations is dependant on whether a reciprocal link has been established to a represen-tative vertex. If not then the new vertex is made into a representative, the complexity of which is given as Eq. (16).
If, however, a reciprocal link to a representative is established then processing continues as follows. First, updating the local density of each representative is done in constant time since summary information can be maintained. Repository up-sparse graph SG insertion occurs when each k neighbour of a newly inserted vertex is found to be reciprocally connected to a neighbour. In this case an additional complexity is incurred: (16)). Creating a representative vertex out of each neighbour of the neighbouring vertices of v
Our implementation of RepStream employs a KD-Tree to perform nearest neighbour searches. Although identifying k neigh-to maintain a balanced structure. Cost of rebuild is approximately O  X j V j log tised over time by only periodically rebuilding the tree.
 4.4. Representative vertex creation and promotion
Algorithm 3 outlines the steps taken to promote a vertex to a representative. A new representative vertex is classified as a predictor if its number of incoming edges is beneath a threshold that we define to be half of a vertex X  X  maximum outgoing edge count as given by the user supplied value of k (lines 2 X 7). A representative vertex is therefore a predictor if j IE else it is labelled an exemplar .

The initial steps in the creation of a new representative r whether the nearest representative of a single neighbour of r case complexity of creating a new representative is therefore into the sparse graph at the time of its promotion. In this case, a new cluster is created such that the new representative vertex is the cluster X  X  sole member (lines 8 X 10). This approach permits the formation of new clusters when the arrival of resentative sparse graph using the described graph insertion method, possibly triggering a merge between its cluster and an tex that is not reciprocally connected to the existing cluster.

The cluster membership of the nearest neighbours of v i on sparse graph SG are checked following the insertion of the new is its nearest representative and the cluster membership of the two vertices differ (lines 12 X 15).
 The density of representative vertices is affected over time by changes in the distribution of vertices on sparse graph SG.
Reciprocal links between vertices on the representative sparse graph may, therefore, need to be reclassified as being either simple reciprocal or density-related links. This is a necessary step that enables the algorithm to evolve as the density and distribution of the data changes. Algorithm 4 details the steps involved.

Limitations in available memory requires the algorithm to discard old information. A vertex v first be unlinked from its respective graphs in order to maintain nearest neighbourhood connectivity. This requires that each neighbouring vertex of v i be visited and its nearest neighbours updated.

In terms of complexity, the first steps, testing a predictor representative and possibly promoting it to an exemplar, are achieved in constant time. Checking whether a density-related relationship has been lost requires two O  X  d  X  distance calcu-of checking a representative X  X  status is therefore O  X  2 kd  X  , regardless of their density-related status. 4.5. Merging and splitting clusters
Cluster split and merge decisions are made by monitoring both the reciprocal connectivity of vertices on the representa-trigger condition for either of these events is the creation or removal of density-related links.

Cluster merges are triggered when updates to the representative sparse graph creates a density-related link between rep-smaller of the two merging clusters moved over to the larger cluster structure. Neither the sparse graph nor the represen-tative sparse graph require any further updating.

Cluster reassignment during a cluster merge is carried out on the smaller of any two clusters being merged. The worst case scenario results in a O j V j 2 cluster merge each time a density-related reciprocal link is established on RSG.
An example of a cluster merge, due to the creation of a new representative that has created a density-related link between two closely positioned clusters, is given in Fig. 9 .

Split checks are executed when the loss of a density-related link between two vertices on the representative sparse graph vertices. The change here is caused by the addition of a new representative whose inclusion has removed a reciprocal con-nection between two representative vertices. An example of a cluster split due to a change in the density of a representative tative vertices was employed to perform split checks.
 Split tests execute with complexity O  X j R j 2  X  given that a simple region growing algorithm is used in our implementation. 4.6. Repository management Central to the operation of RepStream is a repository of the most useful representative vertices used for clustering.
Definition 11. ( Representative usefulness ). The usefulness of a representative vertex r where k is a user specified decay rate and count is the representative vertex X  X  reinforcement count.
The repository is defined as an ordered vector of vertices S  X h s tion ensures a monotonic ordering of vertices in the repository with respect to the passing of time. In our implementation of
RepStream we chose to index the repository using an AVL binary search tree [42]. Algorithm 5 outlines the repository update process. Updating the reinforcement count of a representative vertex that has already been added to the repository requires only two tree operations: the removal of the vertex and then its subsequent reinsertion following an increment to its rein-forcement count (lines 2 X 4). The least useful representative vertex can be rapidly found by traversing to the AVL tree node with the lowest usefulness score.

Approximately 50% of the memory allocated to the algorithm is reserved for maintaining the representative vertex repos-slow forming clusters. Such clusters may not be evident if only the short term behaviour of the data stream were to be analysed.

New additions to the repository are made whenever a new representative vertex is created until resource constraints have been reached (lines 6 and 7). At this point only the most useful repository members are retained. This is achieved by comparing the least useful repository member with other non-repository representatives whenever their reinforcement count is incremented (lines 9 X 13).

The process of updating an existing repository member X  X  reinforcement count outlined in Algorithm 5 requires a single required to identify the least useful member. The worst case complexity of a repository update is therefore O  X  3log j S j X  . An example of how the repository retains the shape of a cluster in the presence of recurrent change is demonstrated in
Fig. 12 . 4.7. Singularities A representative vertex r i that is k -connected to its nearest neighbours and where collection of identical points that offer no new information to the clustering process, yet whose inclusion in the sparse graphs would require the retirement of otherwise useful vertices. For example, a stationary object in a GPS data stream is likely to introduce a large number of identical points that are well suited for representation by a single representative vertex.

Such vertices are referred to as singularities and occur when identically featured points are frequently observed in a data stream. New points that are identical to a singularity can therefore be immediately deleted in order to avoid the overhead of unnecessary sparse graph updates and to maintain the information value of the repository. The occurrence of such points is not lost, however, and is instead represented through the singularity X  X  reinforcement count.

Singularities are unable to be assigned non-zero density measures and as such do not lose their singularity status once it is acquired. This ensures that the presence of a singularity is permanently captured by the algorithm even though its near-est neighbours may be retired over time. Representative vertices are unable to form density-related links to singularity vertices. 5. Experimental results
The performance of RepStream 1 was evaluated using both synthetic and real world data sets. Our real world data sets con-sisted of the KDD-99 Cup network intrusion data 2 and the forest canopy type data data set cannot be regarded as true stream data, we make the assumption that the ordering of the data can be used as a sub-stitute for time. The forest data set was previously treated as stream data in [5].
 Both cluster purity [3] and cluster entropy [33] were used to measure the classification accuracy of the algorithm.
Cluster purity provides a measure of how well data is being classified in the short term over a horizon of the previous h data points. The cluster purity CP  X  c i  X  of a cluster c where r  X  v j ; k  X  is the counting function: The total clustering purity TCP  X  C  X  is then found by averaging over all clusters:
As well as measuring the purity of the clustering, cluster entropy was employed as an indicator of the homogeneity of the entire cluster system. The entropy of a cluster c i is defined as given the counting function The total cluster entropy can thus be found via
Unless noted otherwise, the algorithm was constrained to using only 10 MiB of memory and the decay factor used in all experiments was set to k  X  0 : 99. The ordering of the data sets was invariant throughout the experimentation. The KD-Tree [14,45] was used to perform nearest neighbour searches in our implementation. 5.1. Synthetic data The clustering quality of RepStream was first examined using two hand crafted synthetic data sets. The quality of Rep-
Stream was compared against an incremental version of DBSCAN [23]. DBSCAN was selected for comparison as it employs a density based method of clustering known to perform well with arbitrarily shaped clusters. The algorithm is, however, lim-
As DBSCAN relies on a priori knowledge of the optimal cluster density, we repeated each of the DBSCAN experiments using a variety of values for the -neighbourhood. The minimum number of points required to form a cluster was set to 5. The exper-iment was designed to demonstrate that RepStream is capable of clustering a difficult set containing a variety of arbitrarily shaped clusters of different densities without requiring specialised expert assistance to select the algorithm parameters.
The two data sets used, DS1 and DS2, are presented in Fig. 13 . Both data sets were presented to the two algorithms using a randomised point ordering that did not change during experimentation. The similarity between points was computed using the Manhattan distance. DS1 was made up of 9153 data points while DS2 consisted of 5458 points.

Fig. 14 a depicts the RepStream clustering of DS1 using the optimal parameter set k  X  4 and a  X  4 : 0. These results show with some minor fragmentation of clusters evident. The most notable of these is seen in the middle triangle at the top of the tion and density suggests that these points may, indeed, belong to separate clusters when compared to the remaining points. did, however, result in increasingly fragmented clustering. An increase of the neighbourhood connectivity successfully
In contrast, the single density approach of DBSCAN was found to produce well formed higher density clusters with an -neighbourhood parameter of  X  15. The lower density clusters of DS1, however, were found to be highly fragmented with the presence of a significant number of unclustered points that the algorithm treated as noise. These results are shown in
Fig. 16 a. Decreasing the density with  X  16 marginally decreased the cluster fragmentation, as seen in Fig. 16 b, though at the expense of the incorrect merging of the two top left triangular clusters.

Similar results were obtained clustering data set DS2. The RepStream produced clusters with parameters k  X  4 and a  X  3 : 0 are given in Fig. 17 a. Again, the algorithm has discovered the clusters with only some minor fragmentation visible duce increasing amounts of further fragmentation. Incrementing the neighbourhood connectivity to k  X  5 overcame the fragmentation issue although this did once again cause several nearby clusters to be incorrectly merged as seen in Fig. 17 b.
Fig. 18 shows the result of clustering DS2 with DBSCAN with a selection of values. The first of these,  X  6in Fig. 18 a, shows that although no clusters have been incorrectly merged, a significant number of points have remained unclustered.
Results from using a slightly higher -neighbourhood value of  X  7in Fig. 18 c shows that the issue of the unclustered points remains and has also introduced an incorrect merge between two closely positioned clusters. Although unclustered points still remain, optimal clustering was achieved with  X  6 : 5 as seen in Fig. 18 b.

This demonstrates the difficulty of selecting an optimal density value without prior knowledge of the final data distribu-tion in the synthetic data sets. Stream data compounds this problem as the absolute minimum density is a sensitive measure that is potentially undergoing constant change in a data stream. Finding optimal values of k and a for RepStream is easier as these parameters tend to affect the relative relationships between points rather than introducing static thresholds. The parameter sensitivity of RepStream is further discussed in Section 5.4. 5.2. Network intrusion data
The KDD Cup-99 data set features network connection data derived [50] from seven weeks of raw TCP logs consisting of able both as a complete set that contains approximately 4.9 million records and as a 10% sub-sampled set containing 494,020 tures were used for clustering and a single outlier point was removed. Accurate clustering of this data demonstrates that the algorithm is able to cope in real world situations where a data stream periodically contains bursts of unexpected and unusual data records.

RepStream was tested on the sub-sampled data set using both a purity horizon of length h  X  200 and a horizon of length h  X  1000, two common horizon lengths used in previous work to evaluate clustering accuracy in data streams [4,5].
The Manhattan distance function was used to compute the similarity of data points from features that were normalised on-the-fly. A point p i  X f p i ; 1 ; ... ; p i ; D g of D dimensions was normalised in each dimension d using the formula: where j P j refers to the number of points in memory at any given time. The nearest neighbourhood connectivity was set to k  X  5 and a density scaler of a  X  1 : 5 was employed.

The results of the purity experiment are given in Fig. 19 a and b for h  X  200 and h  X  1000, respectively. These plots, along connections.
 The accuracy of RepStream was also evaluated against published results reported on the same data set for the HPStream, DenStream and CluStream algorithms. The results of the comparisons, depicted in Figs. 20 and 21, show that in most cases RepStream was able to classify network connections as well as or with higher accuracy than HPStream, DenStream and Clu-
Stream. The data stream sample times were chosen to match those reported in [4,5]. 5.3. Forest cover data
The forest cover data set contained 581,012 records consisting of a total of 54 geological and geographical features that describe the environment in which trees were observed. Records also included a class ID providing a ground truth as to which of seven different types of canopy were present on the trees. Attributes consisted of a mixture of continuous valued and Boolean valued data, the latter taking values from the set {0,1}. Successful clustering of this data set will demonstrate that the algorithm is able to cope with a highly dynamic data set when compared to the network intrusion experiment in Section 5.2.

Dimensions were normalised on-the-fly as described in Section 5.2 and the Manhattan distance function was used to measure the similarity between points. The RepStream parameters used on this data set were k  X  5 and a  X  1 : 5.
Fig. 22 a and b shows the purity measured over the data stream for h  X  200 and h  X  1000 purity horizons. The plots show that RepStream was able to classify the canopy types with an accuracy typically P 90 % over a purity horizon of 1000 points and typically P 85 % on the 200 point horizon. The jagged appearance of the purity plots along with the entropy measure depicted in Fig. 22 c is expected given that the algorithm is coping with a dynamic data set. The degraded purity during the initial clustering results was found to be due to the presence of all seven canopy types during the initial portion of the data combined with a lack of prior evidence of cluster distributions. Examining the confusion matrix throughout the stream processing shows that this initial poor classification does not reoccur later with the concurrent reappearance of all seven classes.
 RepStream X  X  purity measurements were evaluated against HPStream and CluStream using the results published in [5]. higher accuracy than the competing algorithms. 5.4. Parameter sensitivity
The sensitivity of the algorithm with respect to the neighbourhood connectivity and the density scaler parameters was explored using different values of k and a . A purity horizon of 1000 points was used in all results shown.
Fig. 24 depicts the purity measure of RepStream on the network intrusion data for k  X  3, k  X  7, k  X  9 and k  X  11 using a fixed scaling value of a  X  1 : 5. The results show that although the clustering performance of the algorithm degrades on this connectivity values. The clusters discovered for the k  X  3 parameter were, however, highly fragmented. This fragmentation was progressively reduced as the value of k was increased with large natural clusters being discovered with k P 7. again show a gradual increase in classification error as the neighbourhood connectivity is increased using values of k  X  3, again, found to be an issue using k  X  3 although this was resolved with values k P 7.

The magnitude of the performance decrease that has been observed in both the network intrusion and tree canopy data sets suggests that the clustering accuracy of RepStream degrades gracefully when sub-optimal values of k are chosen.
Algorithm sensitivity with regards to the density scaler was tested next. Results obtained on the network intrusion data set, seen in Fig. 26 , show, on average, only marginal decreases in accuracy as the scaler value is increased using values of a  X  3 : 0, a  X  4 : 5 and a  X  6 : 0. However, contrasting this with of the algorithm degrades significantly. Lower performance is particularly evident at times marking the beginning and end of some network attacks.
 were presented in Fig. 22 b.

Results shown on both the network intrusion and tree canopy data sets suggests that the performance of RepStream is relatively stable with respect to the choice of density scaler. 5.5. Scale-up experiments
Several scale-up experiments were performed to investigate how well the execution time of the algorithm scales with respect to neighbourhood connectivity, the density scaling factor, available memory and the length of the data stream.
All experiments were run on an Intel 2.33 GHz Core 2 Duo processor under Mac OS 10.4. The RepStream implementation was and a density scaler of a  X  1 : 5 was used to process both data sets in these experiments.
 A near linear relationship between connectivity and execution time was discovered in the network intrusion results in were also discovered and these are illustrated by the results shown in Fig. 30 .

Finally, the execution time of RepStream with respect to the length of the data stream is shown in Fig. 31 . An interesting contrast between the network intrusion and tree canopy data sets is noticeable here. Whereas the tree data set in Fig. 31 b shows an expected linear relationship between the number of points processed and the execution time, the network data set in Fig. 31 a displays a significant flattening out between stream time 150,000 and 350,000 and again between 400,000 and processing of them by singularity vertices. 6. Conclusions
This paper has introduced a graph-based incremental algorithm for clustering evolving stream data. We have selected a graph-based description because it allows us to model the spatio-temporal relationships in a data stream more accurately than it is possible using only summary statistics. Specifically, the graph allows a more detailed definition of the cluster boundary (rather than using a radius based measure which implies that the cluster has circular properties) and provides an effective and detailed representation of any changes that may occur in the cluster shape over time (the changes are re-flected in the way in which the vertices and the connectivity changes over time). A key aspect of our is the use of represen-tative points within the graph description allow the algorithm to capture the general structure of the clusters without requiring the complete cluster data to be stored in memory. The most pertinent of the representative vertices are stored in-side a repository which is used to recall previously discovered cluster structures in the presence of recurrent change.
The algorithm was shown to prioritise the retention of important information within the repository by weighting the use-tures when memory constraints mandate that some data be discarded. The repository can also be used to obtain a historical
Experimental results demonstrated that the algorithm is able to effectively classify both synthetic and real world data sets. The algorithm was compared against an incremental implementation of DBSCAN and shown to robustly handle clusters of complex shapes, sizes and densities. DBSCAN, in comparison, is shown to be hampered by a static density threshold ill suited towards stream processing. Results on real world data sets showed that RepStream was able to more accurately clas-sify well known network intrusion and forest canopy data sets than three of the most popular stream data clustering algo-rithms: DenStream, HPStream and CluStream.

An integral component of the algorithm is the knowledge repository, a collection of the most useful representative ver-tices that define the shape and distribution characteristics of important cluster structures. The repository was shown to as-data characteristic of recurrent change.

Investigation into parameter sensitivity revealed that RepStream is fairly resistant to sub-optimal selections of the near-parameter were observed to result in the discovery of fragmented clusters whereas only higher values of k tended to intro-duce classification errors due to incorrect cluster merges.
 Acknowledgements The authors wish to thank Dinh Q. Phung for the DBSCAN implementation used in this paper.

References
