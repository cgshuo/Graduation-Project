 Feature selection which involves choosing an optimal subset of raw input fea-tures such that the subset does not contain any irrelevant or redundant features plays an important role in many applications. However, individual features may not reveal the structural information among raw inputs, thus we are more inter-ested in finding some important feature groups instead of individual features in some scenarios. Group Feature Selection (GFS) is a technique of making use of structural information among features to select an optimal subset of relevant features in a grouped manner. The advantages of GFS can be summarized as improving the prediction performance, reducing training and utilization time and enhancing the interpretability of learned parameters. So far, GFS has been successfully applied in a number of domains, such as multi-sensor data fusion As the most popular method employed for GFS, Group-Lasso [ 23 ]isan important extension of lasso [ 19 ]. It consists in estimating a linear regression model by minimizing the square loss function evaluated on training data, under a series of constraints which enforce sparsity at the group level. Besides square loss, a logistic loss has also been considered to address classification problems with group sparsity [ 11 ]. Though Group-Lasso models are useful, their main problem concerns the lack of meaningful variance estimates for model coefficients because these methods only provide a point estimate for model parameters. To overcome such problems, Bayesian Group-Lasso is proposed in [ 13 ] as a full Bayesian treat-ment of the Group-Lasso. In [ 7 ], the authors used a generalized spike-and-slab prior to encourage group sparsity in linear regression.
 These existing GFS models were mainly based on square loss and logistic loss for regression and classification analysis, leaving the -insensitive loss and the hinge loss popularized by Support Vector Learning (SVL) machines still unex-plored. SVL machines, such as Support Vector Machine (SVM) and Support Vector Regression (SVR) [ 4 ] are widely used in various machine learning appli-cations owing to their arguably good generalization performance and the merit of only using support vectors in decision function. Nevertheless, their performance can be seriously affected when the input data are very high dimensional, with many non-informative or noisy features. Such a situation could be alleviated if the structural information among features is exploited, e.g., by combining SVL with GFS to select the most relevant feature groups.
 In this paper, based on the pseudo likelihood and data augmentation idea [ 12 , 25 ], we propose a new Bayesian GFS framework for SVL machines. To the best of our knowledge, this is the first effort to integrate GFS into SVL with Bayesian learning, which allows us to circumvent the time consuming cross-validation for regularization parameters. Specifically, our new framework employs a group Automatic Relevance Determination (ARD) [ 20 ] prior to select the most relevant feature groups, and a pseudo likelihood term for Bayesian SVL. With the data augmentation idea, we re-express the pseudo likelihood into different forms for different SVL tasks. To derive the posterior distribution of model parameters and hyper-parameters for Bayesian estimation, we perform mean field variational inference in the augmented variable space. Finally, both regression and classification experiments conducted on synthetic and real-world data sets demonstrate that our proposed approach outperforms other state-of-the-art GFS approaches and the direct SVR and SVM learning.
 Existing GFS methods can be divided into two kinds. One kind are deterministic methods addressing the optimization problem directly, such as the Group-Lasso for logistic regression [ 11 ], the Group-Lasso for generalized linear models [ 14 ], the Group-Lasso with overlap between groups [ 9 ], the sparse Group-Lasso [ 15 ], the online learning algorithms for GFS [ 21 , 22 ], etc. Another kind are Bayesian inference based approaches which can apply different likelihoods and priors on the model conveniently. The Bayesian Group-Lasso [ 13 ] imposes multivariate Laplace priors on separate groups, with a Monte Calo sampling scheme for infer-ence. The Variational Relevant Group Selector (VRGS) [ 17 ], as an extension of the Relevance Vector Machine (RVM) [ 20 ], is similar to sparse Group-Lasso, which have sparse effects both on groups and on individual features. In [ 7 ], the authors used a generalized spike-and-slab prior to encourage group sparsity with Expectation Propagation (EP) inference. In [ 2 ], Babacan presented a general class of multivariate priors for group sparse modeling and developed Bayesian inference methods via variational Bayesian approximation.
 to their convenient form and tractable solution. The problem of them is their more risk of overfitting compared to the -insensitive loss and the hinge loss popularized by SVL machines when only small training data is available. GFS models, these methods typically didn X  X  consider the structural informa-tion among features, thus can only identify relevant features rather than feature groups. In this section, we first review the basic group sparse model, and then present our proposed framework along with the learning models. Fast variational inference procedures are developed to infer the model parameters and hyper-parameters in Bayesian manner. In the sequel, suppose we have a data matrix X consisting of N observations { x i } N i =1 in d -dimensional feature space, and a N response vector y , with y i denoting the response of the i -th observation. 3.1 Group Sparse Model Group sparse modeling is a natural generalization of the traditional sparse mod-eling methods. In group sparse model, the sparsity constraint is imposed on groups instead of the individual features. It effectively models the structural properties of the feature vector, such that dependencies among features are taken into account. A general optimization formulation for group sparse model is where w is a column vector of coefficients, ( y i , x i ; w ) denotes the loss of model w on data ( x i ,y i ),  X   X  0 is the regularization parameter controlling the strength of the enforced sparsity over groups, and w 1 , 2 = G g =1 number of groups, and w g denotes the coefficients of g -th group. Assume that the groups are not overlapping, and the size of g -th group is denoted by d that G g =1 d g = d . Obviously, the traditional l 1 norm based formulation is a special case of this formulation (when d g =1,  X  g ).
 In previous work, typically the square loss was used for regression problems, problems, i.e., ( y i , x i ; w ) = log(1+exp(  X  y i ( w T Compared with these losses, the -insensitive loss and hinge loss are usually more attractive due to their better generalization performance and the merit of only using support vectors in decision function. On the other hand, the performance of traditional SVL machines can be seriously affected when the input data contains a lot of non-informative or noisy features. Therefore, it is meaningful to exploit the structural information among features by combining SVL with group sparse model.
 In addition, an important issue in the group sparse model described above is to choose the regularization parameter  X  . Usually, the time consuming cross-validation is employed to select an optimal value for  X  . This problem can be circumvented by Bayesian inference, as shown in the following subsection. 3.2 The Proposed Framework In this subsection, we will present a Bayesian Group Feature Selection (GFS) framework for SVL machines based on the pseudo likelihood and data augmen-tation idea. The Bayesian modeling of Eq. 1 requires the definition of a joint distribution of all observed and latent variables. Typically, this joint distribu-tion includes the prior distributions over the latent variables, and the likelihood of latent variables on the observed variables.
 To impose group structure constraints on w , each group ( w multivariate Gaussian prior controlled by a distinct hyper-parameter z 1 ,...,G ). We assume a multivariate variance mixture of Gaussian prior over the coefficient vector w , where z g is the inverse of the prior variances for each component of w all components of w g have the same prior variances. Furthermore, we select a conjugate prior for z g by choosing a Gamma distribution, where  X  g and  X  g are the shape and rate parameter of Gamma distribution respec-tively.
 Relevance Determination (ARD). This group ARD formulation can easily obtain the sparsity at the group level by considering a different hyper-parameter z each group of coefficients. A variety of distributions over w can be represented in this fashion by different selections of the hyper-prior distribution p (z function to link SVL with the above Bayesian model. To this end, it is necessary to utilize the loss function of SVL. However, the -insensitive loss and hinge loss do not lend themselves to a convenient description of a likelihood function. To overcome this situation, we have to transform the loss function of SVL into a Gaussian pseudo likelihood, which has the form the former is unnormalized with respect to y . By defining ( y insensitive loss and hinge loss respectively, this general framework can handle both regression and classification problems.
 of w is directly related to the regularization parameters in traditional Support Vector Regression (SVR) and Support Vector Machine (SVM). Since z can be derived automatically by Bayesian inference, our framework can circumvent the cross-validation procedure for regularization parameters. In the following, we will present the regression and classification models (BGFS-SVR and BGFS-SVM) with linear assumption.
 BGFS-SVR. For regression model ( y i  X  R ,i =1 ,...,N ), we employ the -insensitive loss to re-express Eq. 2 , which implies a pseudo likelihood of the form p ( y | w , X )= N where is a margin of tolerance. It has been shown that p ( y scale mixture of normals representation by data augmentation [ 25 ], such that where  X  i and  X  i ( i =1 ,...,N ) are the augmented variables introduced to deal with the max function. Let  X  =[  X  1 ,..., X  N ] T ,  X  =[  X  unnormalized joint distribution of y ,  X  and  X  can be expressed as the augmented pseudo posterior distribution which has the form Directly solving for this augmented pseudo posterior is intractable, and pre-vious approximate inference for probabilistic -insensitive loss relys on Gibbs mean field variational inference method, which has attractive computational properties along with good estimation performance. Specifically, we assume Leibler divergence KL( q ( w , z ,  X  ,  X  ) p ( w , z ,  X  ,  X  ing distribution and the target posterior. To this end, we first initialize the then iteratively optimize each of the factors in turn using the current estimates for all of the other factors. Convergence is guaranteed because the KL divergence is convex with respect to each of the factors. It can be shown that when keeping all other factors fixed, the optimal distribution q  X  ( w ) satisfies where  X   X  w denotes the expectation of the term inside the angled brackets with the joint distribution of all observed and latent variables with the form Plugging all involved quantities into Eq. 3 , we can further get: where  X   X  X  = diag(  X   X  1 i +  X   X  1 i ), i.e., the diagonal elements in matrix  X   X  = diag( z g ), i.e., every z g repeats d g ( g =1 ,...,G ) times in the diagonal position of matrix  X  z ; and denote element-wise multiplication and division, respectively. Similarly, we can get the optimal distributions q q (  X  )as: where GIG (  X  ) denotes the generalized inverse Gaussian distribution. After the above variational inference procedure converges, the predicted response for a new data point x new can be computed by y new should be noted that once the model hyper-parameters  X   X  R have been tuned carefully, BGFS-SVR can infer its model parameters automat-ically via Bayesian inference. Compared with the non-Bayesian group sparse models whose regularization parameters need to be re-tuned for different data sets, BGFS-SVR can apply the chosen hyper-parameters to all data sets. There-fore our approach provides the user a simple way to select the most relevant feature groups.
 BGFS-SVM. For binary classification model ( y i  X  X  +1 ,  X  we utilize hinge loss to re-express Eq. 2 such that it has the following form p ( y | w , X )= N a tractable formulation. Fortunately, the Gaussian pseudo likelihood p ( y can be re-expressed as a location-scale mixture of normals by data augmentation [ 12 ], such that where  X  i ( i =1 ,...,N ) are the augmented variables. This data augmentation idea provides an elegant way to incorporate max-margin principle into Bayesian learning.
 expressed as and the augmented pseudo posterior distribution satisfies we also use the mean field variational inference to approximate p ( w , z ,  X  The derivation procedure is similar as that in BGFS-SVR, so we only provide the optimal distributions for the latent and augmented variables: where  X   X  = diag(  X   X  1 i ).
 { +1 ,  X  1 } ) for a new data point x new as y new =sgn( w T denotes the signum function.
 role as the regularization parameter  X  in group sparse models and SVL machines, but z is more flexible. Since each group of w is controlled by distinct z be seen as a generalization of  X  . With Bayesian inference, our models infer z automatically from the data such that the time-consuming cross-validation can be circumvented. 3.3 Computational Complexity The computational complexities of BGFS-SVR and BGFS-SVM are the same. For each iteration of our variational inference on training data, we need O( d computation, where O( d 3 ) is spent on the inversion of covariance matrix  X  Given d , our method scales linearly in the number of data samples, making it suit-able for large-scale data set. To evaluate our models, we conduct a series of experiments on both synthetic and real-world data sets. On each data set, we perform 100 independent trials for our algorithm and various competitors, and the averaged results are reported. The parameters and hyper-parameters of all algorithms were carefully tuned on our data. More specifically, the hyper-parameters of the proposed models and the other Bayesian competitors were tuned manually. For Bayesian methods, once the hyper-parameters have been tuned carefully, they can be used to infer the model parameters automatically from the data. So, in our experiments the chosen hyper-parameters were applied to all data sets for regression and classi-fication, respectively. On the other hand, the regularization parameters of the non-Bayesian methods must be re-tuned for different data sets, so we obtained them by conducting a 5-fold cross-validation on each training data set. 4.1 Regression We compare the proposed BGFS-SVR with the following algorithms: SVR, Group-Lasso (G-Lasso) [ 23 ], Bayesian Group-Lasso (BG-Lasso) [ 13 ], Bayesian Group-Sparse model with Jeffreys prior (Jeffreys) [ 2 ], a model also assumes the group ARD prior but is based on square loss (G-ARD). We use the LIBLINEAR package [ 5 ] for SVR, and the SLEP package [ 10 ] for G-Lasso. For BGFS-SVR, we set the hyper-parameters  X  g =10 3 ,  X  g =10  X  3 ( g =1 ,...,G ), the maxi-mum number of iterations T = 100, and we vary the tolerance parameter in { 10 , 10  X  4 ,..., 1 } . For BG-Lasso we assign a Gamma prior with shape and scale hyper-parameters k =10  X  6 and  X  =10  X  6 on a and then integrate out a , where a is the parameter of multivariate Laplace distribution. For G-ARD, we set the hyper-parameters  X  g =1and  X  g =10  X  5 ( g =1 ,...,G ).ForSVR,we vary the regularization parameter C in { e  X  8 ,e  X  6 ,...,e meter in { 10  X  5 , 10  X  4 ,..., 1 } . Finally, for G-Lasso we vary the regularization parameter  X  in { 10  X  4 , 10  X  3 ,..., 10 3 } .
 Synthetic Data. The performance of BGFS-SVR is first tested on a sparse signal reconstruction problem. We generate the synthetic data similar as in [ 2 ]. More precisely, we assume the length of weight vector (signal) w is d = 500 and fix the group size to 20 ( d g =20 ,  X  g ), such that the number of groups is G = 25. We assign the 20 adjacent components into one group. From the 25 groups, only 5 randomly chosen groups contain components that generated from the standard Gaussian distribution with zero mean and unit standard deviation, and the remaining groups of components are zeros. The d  X  N measurement matrix X is generated by drawing its elements from a standard Gaussian distribution and then normalizing the rows to have unit l 2 -norm. The N vector y is generated by y = X T w + n , where n =[ n 1 ,..., n are Gaussian noise with  X  n =10  X  2 .
  X  w  X  w 2 / w 2 to evaluate the reconstruction performance of different meth-ods. Here  X  w and w denote the estimated signal and the true signal, respectively. The experiments are independently repeated 100 times, i.e. in every time we have different realizations of the signal w , the measurement matrix X and the observations vector y .
 Table 1 , from which we can get two observations. First, the proposed BGFS-SVR model performs better than those square loss based group sparse model-ing approaches. Second, the baseline method SVR obtains a significantly worse reconstruction error which is due to the fact that SVR cannot make use of the grouping information.
 Gas Sensor Array Data. In order to evaluate the performance of BGFS-SVR in real-world applications, we apply it to the Gas Sensor Array (GSA) data, which is available in the UCI repository. This data set contains 13 , 910 instances from 16 chemical sensors exposed to 6 gases at different concentration levels. Each instance vector contains the 128 features extracted from 16 sensors. We regard those features from the same sensor as a group such that the number of groups is G = 16, and the group size is d g =8for g =1 ,...,G . We utilize all 3600 instances gathered in the 36-th month (600 instances of each class), and the goal is to predict the concentration levels as accurate as possible. First, the data set is normalized so that each row of the data matrix X unit l 2 norm. Then, we randomly select training instances from each class with test set. We do experiments for each class, then the averaged root-mean-square errors (RMSE) over these six regression problems are shown in Fig. 1 , and the averaged numbers of selected groups are listed in Table 2 .
 term of the RMSE, while keeping the number of selected groups comparable with those of other approaches. The advantage of BGFS-SVR over SVR demonstrates the benefit of considering grouping information of features and only retaining useful feature groups for model learning and prediction. The performance differ-ences between BGFS-SVR and those square loss based Group Feature Selection (GFS) approaches verify again that combining SVR and GFS is meaningful. 4.2 Classification We compare the proposed BGFS-SVM with the following algorithms: SVM, Group-Lasso with logistic loss (G-Lasso) [ 11 ], Sparse Group-Lasso with logis-tic loss (SG-Lasso) [ 15 ]. We use the LIBLINEAR package for SVM, and the SLEP package for G-Lasso and SG-Lasso. For BGFS-SVM, we set  X   X  =10  X  3 ( g =1 ,...,G ) and T = 100. For SVM, we vary the regularization parameter C over the grid { e  X  8 ,e  X  6 ,...,e 8 } . For G-Lasso and SG-Lasso, we vary the regularization parameter  X  in { 10  X  4 , 10  X  3 ,..., 10 increased the size of training data from a small value until it is large enough such that most algorithms perform well. This is to study the performance change of each method as training data increase.
 Synthetic Data. We generate the synthetic data set similar to [ 11 , 22 , 23 ]. Before generating the data, we first generate a true model vector w consisting of ten blocks of ten dimensions, i.e., w  X  R 100 , G = 10, d tively, with their values chosen at random from { +1 ,  X  1 the first six blocks and the remaining four blocks are set to be zero. Then, we generate N data points x i ,i =1 ,...,N by x i = L v i , where v L is the Cholesky decomposition of the correlation matrix  X . The ( i, j )-th entry all zero. We finally get the response y i by y i =sgn( w T Gaussian noise with standard deviation 4.
 the goal is to predict the weights w i  X  X  +1 ,  X  1 , 0 } ,i =1 ,..., 100. Let the weight vector. Table 3 lists the averaged results in term of accuracy and the F1 score, which aim to verify whether the learned weight has the same sign as the true model weight. We calculate the F1 scores on the tasks of +1 vs. vs. { +1 , 0 } ,and0vs. { +1 ,  X  1 } and average these three F1 scores. A larger F1 indicates a better estimation.
 with the number of training instances. Among them, BGFS-SVM gets the best results, and the performance difference is especially prominent when the number of training instances is small. Furthermore, compared with G-Lasso and SG-Lasso methods which are based on logistic loss, BGFS-SVM is based on the popular maximum margin principle, thus tend to have a good generalization performance when the number of training instances is small. Finally, BGFS-SVM is more powerful than SVM, because it can make use of the structure information among features and exclude the redundant and non-informative features better. This is in accord with the conclusion in regression analysis, i.e., we can recover the true weights more accurate by combining both the advantages of group sparse modeling and the maximum margin modeling.
 Real-World Data. The GSA data set has been described in regression exper-iment. We utilize all instances gathered in the 36-th month and our goal is to discriminate six different analytes regardless of their concentration. The Smart-phones data set [ 1 ] built from the recordings of 30 subjects performing activities of daily living has been used for human activity recognition. It contains totally 10299 instances from 6 categories. Feature vectors in Smartphones data set are obtained by calculating variables from both the time and frequency domain, and all features can be grouped into 18 groups according to the variable types. The USPS data set [ 8 ] contains totally 9298 handwritten digits from 10 categories (i.e., 0, 1, . . . , 9), and each digit is represented as a 16 by 16 matrix. Inspired by the fact that people can recognize a digit even when it loses some columns of pixels, we group these 256 pixels into 16 columns in the experiments. For multi-class classification, we choose the method proposed in [ 3 ]forSVM, and one vs. rest strategy for BGFS-SVM, G-Lasso and SG-Lasso. All data sets are normalized so that each row of data matrix X  X  R d  X  N For each data set, we randomly select training instances from each category with the size in { 50 , 100 , 200 } , and the rest instances are used as test set. The averaged predictive accuracies of various methods are reported in Table 4 ,and the averaged numbers of selected groups when the number of training instances in each category is 200 are shown in Fig. 2 (we calculate the average number of selected groups over all binary classification sub-problems).
 Several observations can be drawn from Table 4 and Fig. 2 .First,theBGFS-SVM and SVM prominently outperform G-Lasso and SG-Lasso approaches on all three data sets. This can be related to the fact that the maximum margin principle generally yields better generalization ability. Second, the numbers of selected groups of our approach are obviously less than those of other approaches, which indicates that our model is more effective in identifying and selecting the most relevant groups for classification. Finally, although SVM obtains a com-parable performance to our BGFS-SVM, it cannot identify and select the most relevant feature groups which can improve the interpretability of the learned model parameters and reduce the complexity of computing.
 We have presented a new Bayesian Group Feature Selection (GFS) framework for Support Vector Learning (SVL) machines based on the pseudo likelihood and data augmentation idea. Compared with traditional GFS models, our SVL based approach generally yields better generalization performance, and it can circum-vent the cross-validation for regularization parameters with Bayesian inference. Extensive experimental results demonstrated that our proposed models are supe-rior to several existing GFS models and the direct SVL models. There still exist some future work, e.g., to impose sparsity restriction on both the groups and the individual features, to take into account the group overlapping, and to estimate the group partition automatically when it is unknown.

