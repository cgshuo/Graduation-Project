 Ravi Kumar ravi.k53@gmail.com Google, 1600 Amphitheater Parkway, Mountain View, CA 94043 USA Daniel Lokshtanov daniello@ii.uib.no Sergei Vassilvitskii sergeiv@google.com Google, 1600 Amphitheater Parkway, Mountain View, CA 94043 USA Andrea Vattani avattani@cs.ucsd.edu Cross-validation is a classical tool in the machine learner X  X  repertoire for obtaining good estimates of a learning algorithm X  X  performance (Rosset, 2009; Mullin &amp; Sukthankar, 2000; Blockeel &amp; Struyf, 2002). By repeatedly slicing the data into test and training sets, one obtains k dependent estimates of the average error that are usually further averaged to obtain an es-timate of the performance of the algorithm. Although it was initially motivated by the lack of labeled exam-ples (and the cost in obtaining labels), cross-validation remains extremely popular and pertinent, even in the current age of  X  X ig data. X  Despite its ubiquity in practice, cross-validation has largely eluded formal analysis. Under investigation is the decrease in the variance of the generalization er-ror, i.e., the variance in the estimate of the algorithm X  X  performance. In their seminal work, Blum, Kalai, and Langford (Blum et al., 1999), proved insanity check bounds, and showed that cross-validation always helps to reduce this variance, although they did not quan-tify the magnitude of the improvement. Such a task is non-trivial since cross-validation involves subtle corre-lations between the hypotheses learned on the different overlapping slices of the input.
 Recently, Kale, Kumar, and Vassilvitskii (Kale et al., 2011) showed that under a suitable notion of stability (called mean-square stability), k -fold cross-validation achieves a significant variance reduction. Specifically, they showed that in a  X  X oisy X  setting, the variance re-duction is near-optimal (a factor of k ) and, for weakly stable algorithms, one obtains a sub-optimal O (1 / reduction (for more details, see (Kale et al., 2011)). Even though they improved upon previous results, their bound was far from tight, for example, simple calculations illustrate the gap in the bound even in the toy setting of estimating the mean of a Gaussian. In this work we introduce a new stability measure called loss stability , which is weaker than the mean-square stability and other stability measures defined in the past. Our main contribution is to show that this new notion serves as an additive factor to the op-timal variance reduction obtained by cross-validation. In other words, an algorithm X  X  loss stability precisely captures the degradation of the reduction of gener-alization errors due to the dependencies between the different folds. Thus, we obtain an improved bound on the performance of cross-validation; in addition, we prove that this bound is near-optimal. We illustrate an application of our findings to the problem of estimating the mean of a distribution and show that loss stability is able to precisely capture the variance reduction in the generalization error due to cross-validation. We then show that, in the noisy setting, many algorithms such as t -nearest neighbor rules obtain a near-optimal  X ( k ) variance reduction. Cross-validation has been studied (Moore &amp; Lee, 1994; Ng, 1997; Bengio &amp; Grandvalet, 2004; Ko-havi, 1995; Kearns, 1996), and has been used in practice for many decades; see also the references in http://masi.cscs.lsa.umich.edu/ ~ crshalizi/ notabene/cross-validation.html . However, ana-lyzing the improvements offered by cross-validation has been tricky. Blum, Kalai, and Langford (Blum et al., 1999) showed under mild assumptions on the learning algorithm that the variance of the cross-validation estimate is never more than that of a sin-gle holdout estimate. Kale, Kumar, and Vassilvit-skii (Kale et al., 2011) generalized this considerably, quantifying the variance reduction as a function of the algorithm X  X  stability. Our results can be viewed as an improvement over their work: we obtain a near-optimal connection between cross-validation and an appropriately defined notion of algorithmic stability. The notion of algorithmic stability, once again, has been studied in various contexts over the past many years. Rogers and Wagner (Rogers &amp; Wagner, 1978) and Devroye and Wagner (Devroye &amp; Wagner, 1979) implicitly defined weak hypothesis stability in their work on leave-one-out cross-validation. Kearns and Ron (Kearns &amp; Ron, 1999) and Anthony and Holden (Anthony &amp; Holden, 1998) defined weak-error stability in the context of proving sanity check bounds. Kutin and Niyogi (Kutin &amp; Niyogi, 2002) defined the weak-L 1 stability notion; see also the work of Bous-quet and Elisseeff (Bousquet &amp; Elisseeff, 2002). The notion of mean-square-stability introduced by (Kale et al., 2011) is weaker than all the previous notions except for that of weak-error stability and is closely related to the efficacy of cross-validation. Our new def-inition of loss stability is weaker still, but can be used to obtain near-optimal results. In contrast, we show that the notion of weak-error stability is not enough to obtain a significant variance reduction. Let X be the input space and Y be the set of labels and let Z = X  X  Y . Labeled examples are drawn from an unknown, fixed distribution D over Z . We will denote by D X the marginal distribution of D over X , i.e., x  X  X is sampled from D X with probability P y  X  X  Pr z [ z = ( x,y )]. To help readability, every time we refer to some examples z  X  Z or x  X  X , we will implicitly assume that z and x are respectively drawn from D and D X .
 A hypothesis is a function h : X  X  Y . The loss of a hypothesis h on an example z = ( x,y ) is defined loss function. This leads to the following two notions: the expected loss of a hypothesis h defined as  X  ` h E [ ` h ( z )] and the (empirical) loss of a hypothesis h on that the latter is an unbiased estimator of the former. As in (Blum et al., 1999), we denote the discrepancy in the estimation by disc h ( T ) = ` h ( T )  X   X  ` h . A learning algorithm A is a (symmetric) function whose input is a set S of labeled training examples and whose output is a hypothesis A ( S ). In a stan-dard k -fold cross-validation setting we are given a set of nk examples drawn from D , which we denote by U . Let T 1 ,...,T k be a random equipartition of U into k parts, called folds , with | T i | = n . We learn k different hypotheses with h i = A ( U \ T i ) the hypothesis learned on all of the data except for the i th fold; we denote by m = n ( k  X  1) the size of the training set for each of these k hypotheses. Following the work of (Blum et al., 1999) we focus on the cross-validated hypothesis, h cv , which picks one of the { h i } k i =1 uniformly at random. Specifically, we want to relate the discrepancy of h cv to that of the discrepancy of a hypothesis trained on a single fold. Formally, let disc i = of the i th hypothesis. We are interested in: disc cv = We assume that each of the hypotheses is unbiased, E
U [ disc i ] = E U [ disc cv ] = 0. Therefore, we focus on higher moments, specifically the variance of the dis-crepancy, var U ( disc cv ). Note that if each of the k hypothesis was trained on an independently drawn set of training and test examples, it would be easy to con-clude that var ( disc cv ) is smaller than var ( disc 1 ) by a factor of k . The goal of this work is to quantify the relationship between these two quantities while taking into account the dependencies between the folds. For the rest of the paper, we assume that the training set T is drawn i.i.d. from D and the loss function ` is arbitrary but fixed. To simplify notation, for a set T of examples and an example z /  X  T , we will use T z to denote the set of examples obtained by replacing an example chosen uniformly at random from T by z . We will also use shorthand notation such as P x  X  A g ( x ) to Kale, Kumar, and Vassilvitskii (Kale et al., 2011) in-troduced the concept of mean-squared stability (MSS), which was based on the expected variance of the loss on a random test example under a change of a single element in the training set. We recall their definition. Definition 1 (Mean-square stability (Kale et al., 2011)) . The mean-squared stability of a learning algorithm A trained on m exam-mss m,` ( A ) = E Since m and ` are fixed, we will drop them for clar-ity. We refine and weaken the MSS notion further and introduce the concept of loss stability, which tightly captures the variance reduction possible in cross-validation.
 We begin by considering an unbiased loss of a learning algorithm on a test set example. Given a training set T , let the unbiased loss of the algorithm on example x clear from the context we will use the shorthand ` 0 T ( z ) Definition 2 (Loss stability) . The loss stability of a learning algorithm A trained on m exam-ls m,` ( A ) = E A learning algorithm A is  X  -loss stable if ls m,` ( A )  X   X . By a simple rearrangement of the terms, and using the of a learning algorithm can be viewed as the variance of the loss due to a change in a single training example of a typical training set.
 Lemma 1. ls m,` ( A ) = 2 E As before, we will drop the subscripts m and ` . We first show that loss stability is upper bounded by mean-squared stability.
 Lemma 2. For any algorithm A , ls( A )  X  mss( A ) . Proof. Using Definitions 1 and 2, we have mss( A ) = = E = E = E  X  E where the second step follows from the facts that E x [ ` 0 A ( T ) ( z )] = 0 for any T and neither  X  ` Kale et al. (Kale et al., 2011) showed that many pre-viously studied notions of stability, for example, weak-error stability (Kearns &amp; Ron, 1999), uniform stabil-bounded MSS. Therefore all of their bounds apply to loss stability as well; for the sake of brevity, we do not repeat them.
 The discrepancy introduced earlier has a simple char-acterization using the unbiased loss.
 Lemma 3. For any learning algorithm A and i  X  [ k ] , Proof. We now prove our main result regarding the variance reduction obtained using k -fold cross-validation for an algorithm that is  X  -loss stable. We first state a char-acterization of the variance of the discrepancy when using k -fold cross-validation in terms of the variance and covariance of the discrepancy on the folds. Lemma 4 ((Kale et al., 2011)) . var U ( disc cv ) = var U ( disc 1 ) + 1  X  1 k cov U ( disc 1 , disc 2 ) . We next state our main result.
 Theorem 1. Consider any learning algorithm A that is  X  -loss stable with respect to ` . Then By Lemma 4 it is enough to analyze the covariance cov U ( disc 1 , disc 2 ) between the discrepancies on two different folds. The following key lemma provides a more manageable form for cov U ( disc 1 , disc 2 ). To simplify notation, let S = U \ ( T 1  X  T 2 ) be the set of training examples shared by the first two folds. Lemma 5. Let a fold T i consist of a single element z i and elements  X  T i . Then, cov Proof. We first reduce the covariance over the folds to the covariance over a change in a single example. By definition of unbiased loss, cov where i,j , are distributed uniformly in { 1 ,...,n } , and z , z j 1 represent the i th element of T 2 and the j th ele-ment of T 1 (for some ordering of their elements). We can now write: cov where the last step uses the fact that the elements in T 1 and T 2 are i.i.d.
 and z 1 ,z 2 ,z 0 1 ,z 0 2 i.i.d. samples from D . We have cov since similar arguments hold for the other cross-terms. The proof of our main theorem is now a simple appli-cation of the Cauchy X  X chwarz inequality.
 Proof of Theorem 1. By Lemma 4 it is enough to show that cov U ( disc 1 , disc 2 )  X   X  . By Lemma 5 and Cauchy X  X chwarz, Previous work left open the question whether an as-sumption on the stability of the algorithm is at all necessary for k -fold cross-validation to yield a signif-icant variance reduction. Intuitively, one may try to argue that discrepancy estimates obtained from very unstable algorithms are essentially independent. On the other hand, one can try to weaken the notion of stability needed to obtain O ( k ) variance reduction. In studying the effect of mean square stability, Kale et al. (Kale et al., 2011) asked whether the weakest such notion, that of weak-error 1 stability introduced in (Kearns &amp; Ron, 1999) is enough to obtain a signif-icant variance reduction.
 In this section we provide an answer to both questions by showing that there are instances that are (0 , 0)-weak-error stable for which the variance reduction is only a constant independent of k . We observe that such instances have hypothesis class with VC dimen-sion only 2, hence assumptions based only on the VC dimension are also insufficient.
 Theorem 2. For every 4 there is a loss function ` and a learning algorithm A such that var U ( disc 1 ) = 1 / n and var U ( disc cv  X ( 1 t  X  n ) . Furthermore the algorithm A is ( 2  X  ) -loss stable and (0 , 0) -weak-error stable. Proof. The input space X and labels Y are both { X  1 , 1 } , with uniform distribution over Z = X  X Y . pendent of y . (While having negative loss may be non-standard, it greatly simplifies notation, and changing the loss function to ` ( y,h ( x )) = h ( x ) + c for any con-stant c would not change the obtained results.) For b  X  { X  1 } , let h b ( x ) = b  X  x . The expected loss of h For every t  X  Z and set S = { ( x 1 ,y 1 ) ,..., ( x t ,y ( X X Y ) t , define k S k = P i  X  t x i . For a function f : { X  1 , 1 } to be defined later we consider the algorithm for all S .
 We start by computing var U ( disc 1 ) and cov U ( disc 1 , disc 2 ) = E U [ disc 1  X  disc 2 ]. By Lemma 3, the definition of ` 0 , and the fact f (  X  )  X  X  X  1 , 1 } , var We now proceed with cov U ( disc 1 , disc 2 ). In the fol-lowing we write f S,T,x as a shorthand for f ( k S k + k T k + x ). By Lemma 5 and the definition of ` 0 , we have where the last step follows as x,x 0  X  X  1 ,  X  1 } . Thus to create a setting where cross-validation per-forms poorly we need to find a function f that maxi-of functions, with one function f for every choice of a parameter 10 for different values of loss stability for A . The function f is a step-wise function: we will call x a step of f if f ( x ) 6 = f ( x  X  1).
 We now proceed with computing the covariance. De-note by s the size of the overlapping training set, s = | S | = ( k  X  2) n . We say that the set S is active if the following two properties are satisfied. (a)  X  k S k  X  Since k S k = P ( x,y )  X  S x where the x 0 s are uniform in { X  1 , 1 } , we can interpret k S k as a one-dimensional unbiased random walk of length s . The following facts are known for a random walk W of length n starting at zero 2 : 1. Pr [ W = 0]  X  1  X  n . 2. For every  X  3. Pr [  X  4. For i  X  0, Pr[ | W | = i + Let A S be the event that S is active. For 4 2 , the number of values for k S k that make S active is at least ( b 2 2 X 3 above, We will derive a bound for the covariance by simply considering the case in which S is active. k T k lands on a step of f , i.e., when k S k + k T k  X  { x  X  ,x  X  + 1 } where x  X  is a step 3 , and zero otherwise. conditioned on S , T 1 is such that k S k + k T 1 k lands tributed as a random walk of length n . When S is active, the zeroth step is at distance at most k S k and therefore properties 1 and 2 above imply that Pr ( t  X  for Pr[ E 2 j S [ T 1 ] | A S ]. We can conclude Simple algebra now gives | E T 1 [ f S,T 1 , 1  X  f S,T n . Combining this with (2) and the bound on Pr[ A S ], we have cov U ( disc 1 , disc 2 )  X  48 t  X  n . The bound on var U ( disc cv ) now follows by Lemma 4.
 It remains to analyze the loss stability and the weak error stability of the algorithm. Since both hypothe-ses output by the algorithm have expected loss 0, the algorithm is (0 , 0)-weak error stable. Finally, Inserting the definition for the loss function yields ls( A ) = E In this section we show that the bound derived in The-orem 1 is nearly tight. We begin with a discussion of our main result and then show a specific instance where it cannot be improved by more than a factor of .
 Theorem 1 shows that the reduction in the variance of the discrepancy is bounded by the loss stability of the learning algorithm. In fact, the reduction in error depends on how the variance of an average training set on a single training example compares in magnitude to the variance due to a change in a single training exam-ple on an  X  X verage X  training set. To see this, observe that using Lemma 1 and Lemma 3 we can rewrite the statement of our main theorem as var = = where is the exchange ratio . Thus the lower the exchange ratio of a learning algorithm, the higher the reduction due to cross-validation.
 Regression. We begin by considering the case of re-gression under squared loss and show that for the em-pirical risk minimization algorithm,  X  = 2, and there-fore k -fold cross-validation achieves a k/ 3 reduction in the variance of the discrepancy.
 Suppose the examples are drawn from a one-dimensional distribution and we are interested in pre-dicting the mean. We will consider the setting of a squared-loss function. Specifically, suppose that the examples are drawn from a distribution D x with mean  X  and variance  X  2 . Let h T be the hypothesis output by the algorithm A trained on T . The loss of a hypothesis on an example x is then ` A ( T ) ( x ) = ( h T  X  x ) 2 . We begin by computing the unbiased loss, ` 0 T ( x ) = ` T ( x )  X   X  ` T ( x ) = ( x  X  h T ) 2  X  E We can now proceed with computing  X  . For the nu-merator, For the denominator, we have E [ var = E = E = var We claim that the first term is non-negative which will imply that To prove the claim, define g ( z ) = var x ( x 2 4 var x ( x ) z 2  X  4 cov x ( x,x 2 ) z.
 We have that g 0 ( z ) = 0 if and only if z = z  X  cov x ( x,x 2 ) / (2 var x ( x )) and g 00 ( z ) = 8 var Hence, it is enough that g ( z  X  ) is non-negative. We have g ( z  X  ) = var where the last step follows by the well-known fact that cov ( X,Y )  X  p var ( X )  X  var ( Y ), for any two random variables X and Y .
 Therefore, the exchange ratio,  X  is bounded by: Consider the bound in Equation 3. For algorithms where the effect of a single training example on the hy-pothesis is independent of the rest of the training set, we expect that the variance of the hypothesis when replacing a single sample in the training set T is a fac-tor | T | = n ( k  X  1) smaller than the variance of the hypothesis over the whole training set. This results in  X  = 2. For instance, in case of empirical risk min-imization (ERM) algorithm that given a training set T returns hypothesis A ( T ) = h T = 1 | T | P x  X  T x , it is easy to check that: E [ var which give  X  = 2. Hence, cross-validation achieves a k/ 3 reduction in the variance of the discrepancy. On the other hand, an algorithm that considers more interactions between training examples will have a higher value of  X  .
 Lower bound. We now show that the bound in The-orem 1 is nearly optimal.
 Theorem 3. There exists a loss function ` and a  X  -loss stable learning algorithm A such that cov U ( disc 1 , disc 2 )  X   X / 2 . Hence, Proof. Consider again the setting of estimating the mean of a distribution using the ERM algorithm with squared-loss function, and assume  X  = 0. Due to sym-metry, we again focus on the covariance between the first two folds. First we write down the discrepancies. Similarly, disc 2 = 1 n P y  X  T the law of total covariance (Kale et al., 2011): cov We are concerned with the covariance over the fold T 2 , and therefore can drop terms independent of T 2 : Therefore, the first term of the covariance is: n ( k  X  1) = = To bound the second term, we want to compute: Note that the covariance is independent of T 1 , there-fore, we can rewrite as other hand, Lemma 1 implies that the loss stability of In previous work Kale et al. (Kale et al., 2011) consid-ered the noisy setting, where every label is corrupted with a small probability. We recall their definition be-low: Definition 3. An instance of the cross-validation problem composed of the algorithm A , the loss function ` , and the distribution D is  X  -volatile, if They showed that O (1 /m )-uniform stable algorithms achieve optimal variance reduction of 1 /k . However, uniform stability is a very strong assumption and many algorithms such as t -nearest neighbor learning rules do not have non-trivial uniform stability but have small ( O ( Wagner, 1979). This setting of weak L 1 stability im-plies that these algorithms are O (1 /m )-mean square stable, and thus by Lemma 2, are O (1 /m )-loss stable. The following Lemma is immediate and allows us to conclude that these learning algorithms achieve a near-optimal variance reduction.
 Lemma 6. In an  X (1) -volatile setting, a learning algorithm A that is either ( O (1 /m ) ,O (1 /m )) -weak-hypothesis or weak-L 1 stable has: We studied the decrease in the variance of cross-validation through the lens of a new algorithmic stabil-ity notion, namely, the loss stability. We also showed that this is a near-tight connection. It will be interest-ing to empirically compute the loss stability of popular learning algorithms such as decision trees and SVM, for which an analysis similar to the one in Section 7 seems daunting.
 Anthony, M. and Holden, S. B. Cross-validation for binary classification by real-valued functions: theo-retical analysis. In Proc. COLT , pp. 218 X 229, 1998. Bengio, Yoshua and Grandvalet, Yves. No unbiased estimator of the variance of k -fold cross-validation. JMLR , 5:1089 X 1105, 2004.
 Blockeel, Hendrik and Struyf, Jan. Efficient algorithms for decision tree cross-validation. JMLR , 3:621 X 650, 2002.
 Blum, A., Kalai, A., and Langford, J. Beating the hold-out: Bounds for k -fold and progressive cross-validation. In Proc. COLT , pp. 203 X 208, 1999. Bousquet, O. and Elisseeff, A. Stability and general-ization. JMLR , 2:499 X 526, 2002.
 Devroye, L. P. and Wagner, T. J. Distribution-free per-formance bounds. IEEE TOIT , 25:601 X 604, 1979. Kale, S., Kumar, R., and Vassilvitskii, S. Cross-validation and mean-square stability. In Proc. ICS , pp. 487 X 495, 2011.
 Kearns, M. A bound on the error of cross valida-tion with consequences for the training-test split. In Proc. NIPS , pp. 183 X 189, 1996.
 Kearns, M. J. and Ron, D. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. Neural Computation , 11(6):1427 X 1453, 1999.
 Kohavi, R. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proc. IJCAI , pp. 1137 X 1143, 1995.
 Kutin, Samuel and Niyogi, Partha. Almost-everywhere algorithmic stability and generalization error. In Proc. UAI , pp. 275 X 282, 2002.
 Moore, A. W. and Lee, M. S. Efficient algorithms for minimizing cross validation error. In Proc. ICML , pp. 190 X 198, 1994.
 Mullin, M. D. and Sukthankar, R. Complete cross-validation for nearest neighbor classifiers. In Proc. ICML , pp. 639 X 646, 2000.
 Ng, A. Y. Preventing  X  X verfitting X  of cross-validation data. In Proc. ICML , pp. 245 X 253, 1997.
 Rogers, W. and Wagner, T. A finite sample distribution-free performance bound for local dis-crimination rules. Annals of Statistics , 6(3):506 X  514, 1978.
 Rosset, Saharon. Bi-level path following for cross vali-dated solution of kernel quantile regression. JMLR ,
