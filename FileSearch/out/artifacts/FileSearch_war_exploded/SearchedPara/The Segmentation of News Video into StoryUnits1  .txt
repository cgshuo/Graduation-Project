 Large amount of news videos are availa ble. We need an automatic and effective tool to segment these news video into single story units. These story units are used for indexing to support further browsing and retrieval by the users. In order to characterize the video content, video structure parsing is required for indexing. Many literatures have addressed the shot boundary detection techniques, such as Ref. [1]. Some researchers have prese nted scene segmentation or extraction algorithms, for example, Ref. [2] presents a novel algorithm that uses number of interpolated macro blocks in B-frames to identify the sudden scene changes detection and statistical f eatures for gradual scene c hange detection. Ref. [3] proposes a fast scene change detection al gorithm using direct feature extraction from MPEG compressed videos. Some literatures also contain the approaches about story detection or segmentation, for instance, Ref. [4] has introduced an approach to extract story units from long programs for video browsing and navigation by time-constrained clustering of video shots and analysis of scene transition graph.
 now it is still difficult to automatically extract high-level semantic structure such as scene, story for general video programs, this problem is also not solved completely in semantic content level in late research works listed above. We should exploit more clues, not just depend on the visual information analysis. For example, Ref. [5] has used image analysis techniques to automatically pars-ing news video. Their algorithm that locates and identifies anchorperson shots is based on the a priori knowledge and assumes that each news item starts with an anchor shot followed by a sequence of news shots. Due to his assump-tion limitations, news items that start without anchorperson shots and that read by anchorperson without news shots can not be identified by their system. This paper presents an approach, which integrates visual, auditory and textual information to segment news stories. It overcomes the limitations of the algo-rithm mentioned above and is helpful for users to efficiently manage their video resources. Fig. 1 shows our overall system components. Shot boundary detection module segments video stream into a sequence of shots. Caption text detection mod-ule identifies text event to locate the c lips composed of topic caption frames. In silence clip detection module, silence segments are identified. Then audio-visual features and text information are integrated to segment news video into individual stories.
 2.1 Shot Boundary Detection Because of spot and time limitations of news video in shoot and edition, the abrupt shots of news video obtain a rate of 90%. The gradual transitions gener-ally appear in the head, tail of news programs. Even if they occur in the main body of news programs, they generally occur inside of the news story and do not locate at news story boundaries, so we just consider the detection of abrupt transition because there should be abrupt transition between two consecutive news stories to a great extent.
 paper to measure the content change between each contiguous frame pairs. We define E at as the abrupt shot transition event. 2.2 Topic Caption Text Detection A row or several rows of captions added by late edition must appear to express the meaning of news story in the start or middle, as shown in Fig. 2(a). They generally appear in the specific position (for example, bottom-left part of TV screen) and last several seconds. These captions are defined as topic captions, and the frame containing topic captions is defined as topic-caption frame. The appearance and the disappearance of these captions are defined as text event. text detection algorithm based on threshold. For L long, N  X  M size of video R R  X  is the region defined as the 1/4 field of screen bottom where text events appear. So we can get two frame difference sequences: the whole frame difference d t = F follows. T text and T shot are defined as the global threshold of text event and shot transition between frame of t and t + 1. If the backgrounds have changes when text events occur, the algorithm is done as follows. A slider detection window is defined using the following expression, d t + 1. The following parameters are chosen,  X  =2, m =2.
 interviewee may be added in the same region, as shown in Fig. 2(b). In order to avoid wrong detection, the fixed region RF within the dotted lines(as shown in Fig. 2(c)) is chosen. The color Euc lidean distances between the RGB values of RF and the standard blue and white values are calculated respectively. If the distances sum is lower than a certain defined threshold, then these frames are potential topic-caption frames. In the next step, the two horizontal edges of caption text region are detected usin g edge detection algorithm described in Ref.[7] to confirm the topic-caption frame.
 itcanbeexpressedas TC ( n )=[ V s n ,V e n ] ,n =1 , 2 ,..., and V s n &lt;V e n . V s n and V e n represent the corresponding frame number of the topic captions start and end respectively. We choose the frame at the position INT(( V s n + V e n ) / 2) as the topic-caption frame of the n th story and INT( a )is defined to get the integrate part of a . Then the array Ftc ( n ) of topic-caption frames sequence can be obtained, which is expressed as Ftc ( n )= { Ftc 1 ,Ftc 2 ,...,Ftc n } . 2.3 Silence Clip Detection According to lots of experimental observation of news video, we find out that a relatively long silence clip must exist in two continuous news story boundary. The frame containing silence clip is define d as silence-clip frame. In our system, the audio stream is defined as two layers including audio frame and audio clip. An audio frame lasts about 20ms, and continuous 30 audio frames are choosed as an audio clip.
 time energy function value of an audio frame is lower than the defined threshold T e and its short-time zero-crossing rate is lower than another defined thresh-old T zcr , then the audio frame is indexed as a silence frame SF , also define silence event. The following parameters are chosen, T e = 10000, T zcr =0 . 02,  X  = 24. Based on analysis above, we can get a series of silence clips SC ( n )= { represent the corresponding start and end frame number of SC n respectively. 2.4 News Stories Segmentation Algorithm For a news story NS k , it has only one topic-caption frame Ftc k , and there should be one or more silence clips before and after appearance of Ftc k in temporal axis. These silence-clip frames should have abrupt transition except that a whole news story is inside of an anchorperson shot and it has not live broadcasts, so we can detect shot transiti on of these specific silence-clip frames between two consecutive topic-caption frames to locate the story boundaries.
 is chosen as the story boundary between NS k and NS k +1 .2.For SC i V s E at =  X  , Esc anchorperson shot and there is no abrupt transition around news story NS k .We the frame at the position INT(( V s  X  + V e  X  ) / 2) as the story boundary between story boundary between NS k and NS k +1 . The material is three days of MPEG-1 CCTV news with frame dimension of 352  X  288 pixels and frame rate of 25 frames per second, which is selected from our video database randomly. The test data set lasts one and a half hours or so in total and contains 135,400 frames. In order to test the validity of our story segmentation algorithm, the test data are labelled manually as a standard contrast.
 cluding shot boundary and topic caption text detection). We can calculate the accuracy rate P =1  X  E/C =1  X  21 / 97 = 78 . 3% and the recall rate R =1  X  D/B =1  X  5 / 81 = 93 . 8%. This results show that only using visual streams analysis is not sufficient. Table 2 shows the results for the multimodal analysis. The precision is improved to 85 . 8%, and the recall value to 97 . 5%. This results show that multimodal analysis method is effective and robust. shots the algorithm is effective. For example, text starts at frame 3091 and text ends at frame 3198, and there is no abrupt transition around them. The silence-clip frames are located at around frame 2798 and frame 3450, and then the story boundaries are located at frame 2798 and frame 3450 respectively, which is same to the manually labelled results. Missing boundaries are mainly caused by disturbs of abrupt transitions occurring on the topic-caption frames, and text event is considered as general shot transition. False boundaries are mainly caused by disturbs of other captions such as the captions of interviewee personal introduction or the dialogue between the reporter and the intervie-wee.
 Extracting high-level semantic content from video flows such as news video is hot spot in the researches of video database. This paper presents an effective approach of story segmentation for news video from the point of view of the integration of image, auditory and textual cues. Though the approach is designed for parsing TV news, its analysis of text event detection, as well as the integration strategy of audio-visual cues, can also be applied to the scene segmentation and video retrieval of other video types in future work.
 Acknowledgements. The research was supported by the Hubei Nature Science Fund in China under Grant No. 2005ABA246.

