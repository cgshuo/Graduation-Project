 This paper describes a novel approach to product recom-mendation that is based on opinionated product descriptions that are automatically mined from user-generated product reviews. We present a recommendation ranking strategy that combines similarity and sentiment to suggest products that are similar but superior to a query product according to the opinion of reviewers. We demonstrate the benefits of this approach across a variety of Amazon product domains. H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.5 [ Online Information Services ]: Web-based services User-generated Reviews; Opinion Mining; Sentiment-based Product Recommendation
Consider the 13 X  Retina MacBook Pro . At the time of writing its product features , as listed by Amazon, cover tech-nical details such as screen-size , RAM , processor speed , and price . These are the type of features that one might expect to find in a conventional content-based recommender [9]. Often, such features can be difficult to locate and can be technical in nature, thereby limiting recommendation oppor-tunities and making it difficult for casual shoppers to judge the relevance of suggestions. However, the MacBook Pro has more than 70 reviews which encode valuable insights into a great many of its features; from its  X  X eautiful design X  and  X  X reat video editing X  capabilities to its  X  X igh price X  . These features capture more detail than a handful of technical (cat-alog) features. They also encode the opinions of real users and thus provide a basis for product comparisons.

We consider the following in this paper: can such fea-tures be used as the basis for a new type of experiential product recommendation, which is based on genuine user experiences? Are these features sufficiently rich to be a vi-able alternative to more conventional product descriptions based on meta-data or catalog features? And what type of recommendation strategies might be used?
There have been a number of efforts focused on extracting feature-based product descriptions from reviews. The work in [10] is representative in this regard and describes the use of shallow NLP techniques for explicit feature extraction and sentiment analysis; see also [6]. The features extracted, and the techniques used, are similar to those presented in this paper, although in the case of the former there was a focus on the extraction of meronomic and taxonomic features to describe the parts and properties of a product.

Zhang et al. [11] analyze the sentiment of comparative and subjective sentences in reviews on a per-feature basis to cre-ate a semi-order of products, but do not consider the recom-mendation task with respect to a query product. The work in [5] is relevant in that it uses user-generated micro-reviews as the basis for a text-based content recommender, and re-cently work in [2] has also tried to exploit user-generated content in similar ways. In [1], a manually defined ontology is used to convert opinions extracted from reviews into a structured form. This ontology, which captures both the re-viewer X  X  skill level and experience with the product, is then leveraged for recommendation.

The work in this paper uses existing techniques [4, 6 X 8] to automatically extract rich, opinionated product case rep-resentations from user-generated reviews and extends our earlier work [3] by introducing a new approach to sentiment-based recommendation and a hybrid approach for combining similarity and sentiment during recommendation.
The reviews for each product, P , are converted into a rich, feature-based, experiential case in 3 steps as follows (Figure 1). See [3] for more details.
 Extracting Review Features. Shallow NLP and statis-tical methods are used to mine features from reviews [6, 7]. We consider bi-gram features which conform to one of two part-of-speech co-location patterns: a noun preceded by an adjective ( AN ) or by a noun ( N N ). Single-noun features which frequently co-occur (  X  70% of the time) with senti-ment words in the same sentence are also considered [6]. Evaluating Feature Sentiment. We use a version of opinion pattern mining to evaluate feature sentiment [8]. For a given feature F i in sentence S j of review R k , we iden-tify the closest sentiment word w min to F i in S j ; F i beled as neutral if no sentiment words are present (sentiment words are those contained in the sentiment lexicon [6]). Next we extract the opinion pattern : the part-of-speech tags for w min , F i and any words that occur between them. After a pass over all features, the frequency of occurrence of all pat-terns is noted. For valid patterns (those which occur more than once) we assign sentiment to F i based on that of w min in the sentiment lexicon; sentiment is reversed if S j contains a negation term within a 4-word distance of w min . Features associated with invalid patterns are labeled as neutral . Generating Experiential Product Cases. For each prod-uct P we now have a set of features F ( P ) = { F 1 extracted from the reviews of P Reviews ( P ) and each fea-ture F i has an associated set of positive, negative, or neutral sentiment labels ( L 1 , L 2 , . . . ). Features which are mentioned in  X  10% of reviews for that product are only considered and overall sentiment (Equation 1) and popularity (Equa-tion 2) scores are calculated; P os ( F i , P ) resp. N eg ( F N eut ( F i , P ) denotes the number of positive (resp. nega-tive, neutral) sentiment labels for feature F i . The product case, Case ( P ), is then given by Equation 3.
 Sent ( F i , P ) = P os ( F i , P )  X  N eg ( F i , P )
Case ( P ) = { [ F i , Sent ( F i , P ) , P op ( F i , P )] : F
The above case representation leads to a content-based recommendation approach based on feature similarity to a query product. However, the availability of feature senti-ment suggests another approach in which products that offer better quality features compared to the query product are recommended. These techniques are described below. Similarity-Based Recommendation. Each product case is represented as a vector of features, where feature values represent their popularity in reviews (Equation 2) as a proxy for their importance. The cosine similarity between query product, Q , and candidate recommendation, C , is given by: Sim ( Q, C ) =
Using this approach, a set of top n recommendations are generated, ranked according to their query product similar-ity [9].
 Sentiment-Enhanced Recommendation. Rather than recommend products using similarity alone, feature senti-ment can also be used to seek products with better sentiment than the query product. Equation 5 computes a score for feature F i between query product Q and recommendation candidate C ; a positive (resp. negative) score means that C has higher (resp. lower) sentiment for F i compared to Q .
Equation 6 computes an average better score at the prod-uct level across the shared features between Q and C . How-ever, this approach ignores any residual features that are unique to Q or C . Thus, Equation 7 computes an average better score across the union of features in Q and C ; non-shared features are assigned a neutral sentiment score of 0. Combining Similarity and Sentiment. The sentiment-based approaches above prioritise products that enjoy more positive reviews across a range of features relative to the query product. However, these recommendations may not necessarily be very similar to the query product. Thus, Equation 8 ranks recommendations based on their combined (controlled by w ) similarity and sentiment with respect to Q ; B ( Q, C ) denotes B 1 or B 2, normalised to [0, 1].
Score ( Q, C ) = (1  X  w ) Sim ( Q, C ) + w B ( Q, C ) + 1 / 2 (8)
The above approaches are evaluated using data extracted from Amazon.com during October 2012. We considered 6 product domains; here we present representative results for 3 domains (Table 1). For each product with  X  10 reviews, we extracted review texts and helpfulness information, and the top n recommendations as suggested by Amazon.

The success of our approach depends on its ability to translate user-generated reviews into useful product cases. Table 1 shows the mean and standard deviation of the num-ber of features that are extracted for each domain. On aver-age, 9-26 features are extracted per product case, indicating that reasonably feature-rich cases are generated. Table 1 (last column) also shows the mean and standard deviation of the pairwise product cosine similarities. Again the results bode well because they show a relatively wide range of sim-ilarity values; very narrow ranges would suggest limitations in the expressiveness of extracted product representations.
A standard leave-one-out approach is used in our evalu-ation, comparing our recommendations for each product to those produced by Amazon. Thus, for each product (re-ferred to as the query product , Q ) in a given domain, we generate a set of top 5 recommendations using Equation 8, varying w from 0 to 1 in steps of 0.1. This produces 22 rec-ommendation lists for each Q , 11 each for B 1 and B 2, which we compare to Amazon X  X  own recommendations for Q .
We use Amazon X  X  overall product ratings as an indepen-dent measure of product quality. The ratings benefit metric compares two sets of recommendations based on their rat-ings (Equation 9), where a ratings benefit of 0.1 means that our recommendations R enjoy an average rating score that is 10% higher that those produced by Amazon ( A ).
We also compute the query product similarity , the average similarity based on mined feature representations between our recommendations and the query product. This allows us to evaluate whether our techniques produce recommenda-tions that are related to the query product and also provides a basis for comparison to Amazon X  X  recommendations.
For each domain, Figure 3(a X  X ) shows B 1 and B 2 results for top 5 recommendations. Ratings benefit scores (left y-axis, dashed lines) for B 1 (circles) and B 2 (squares) against w (x-axis), along with the corresponding query product sim-ilarity values (right y-axis, solid lines), are shown. The av-erage similarity between the query product and the Amazon recommendations is also shown, which is independent of w and so appears as a solid horizontal line in each graph.
At w = 0, Equation 8 is equivalent to a pure similarity-based approach to recommendation using cosine, because sentiment is not contributing to the overall recommenda-tion score. For this configuration there is little or no ratings benefit; the recommendations produced have very similar average ratings to those produced by Amazon. However, the recommendations that are produced are more similar to the query product, in terms of the features mentioned in re-views, than Amazon X  X  own recommendations. For example, in the Phones domain (Figure 3(b)) at w = 0, recommen-dations based on cosine have a query product similarity of 0.8 compared to 0.6 for Amazon X  X  recommendations.

At w = 1, where recommendations are based solely on sen-timent, we see a range of maximum positive ratings benefits (from 0.18 to 0.23) across all 3 product domains. B 2 out-performs B 1, except for GP S , indicating that the sentiment associated with residual (non-shared) features is important, at least for two of the three domains considered. Consider again the Phones domain (Figure 3(b)). At w = 1, we see a ratings benefit of 0.11 and 0.21 for B 1 and B 2, respectively. Thus, products recommended by B 2 enjoy ratings that are 21% higher than Amazon X  X  recommendations, an increase of almost one point on average for Amazon X  X  5-point scale.
However, these ratings benefits are offset by a drop in query product similarity. At w = 1, query product similarity falls below that of the Amazon recommendations. Thus, a tradeoff exists between ratings benefits and query product similarity.
The relative contribution of similarity and sentiment is governed by w (Equation 8). As w increases a gradual in-crease in ratings benefit for B 1 and B 2 is seen, especially at larger w , with B 2 outperforming B 1 except for GP S . The slope of the ratings benefit curves and the maximum bene-fit achieved is influenced by the ratings distribution in each domain. For example, P hones and T ablets have ratings distributions with relatively low means and high standard deviations. Thus, more opportunities for improved ratings exist and, indeed, the highest ratings benefits are seen for these domains (above 0.2 at w = 1 for B 2).

Regarding query product similarity, there is little change for w &lt; 0 . 7. But for w &gt; 0 . 7 there is a reduction as sen-timent tends to dominate during recommendation ranking. This query product similarity profile is remarkably consis-tent across all product domains and in all cases B 2 better preserves query product similarity compared to B 1. To better understand the relative performance of B 1 and B 2 with respect to the Amazon baseline as w varies, we need a point of reference for the purpose of a like-for-like com-parison. To do this we compare our techniques by fixing w at the point at which the query product similarity curve in-tersects with the Amazon query product similarity level and then reading the corresponding ratings benefits for B 1 and B 2. This is a useful reference point because it allows us to look at the ratings benefit offered by B 1 and B 2 when de-livering recommendations that have the same query product similarity as the baseline Amazon recommendations.
Figure 2 shows these ratings benefits and corresponding w values for B 1 and B 2. The results clarify the positive ratings benefits that are achieved using sentiment-based recommen-dation without compromising query product similarity. For T ablets and P hones there are very significant ratings bene-fits, especially for B 2 (resp. 15% and 21%). As stated above, B 1 outperforms B 2 for GP S , but in a relatively minor way, suggesting that the sentiment associated with residual fea-tures is not playing a significant role in this domain.
Finally, note the consistency of the w values at which the query product similarity of the sentiment-based recommen-dations matches that of Amazon. For each domain, w  X  0 . 9 (for B 2) delivers recommendations that balance query prod-uct similarity with significant ratings benefits; whether this value of w applies in general we leave to future work.
The objective of this work has been twofold: (1) to convert unstructured reviews into rich product descriptions and (2) to use these product descriptions in a recommender system that combines similarity and sentiment. Our results show clear benefits in terms of recommendation quality compared to Amazon X  X  own recommendations. In this work, we have considered one particular approach to similarity: a cosine metric calculated over the frequency of occurrence of ex-tracted product features. A question arises as to whether this approach indeed reflects an authentic notion of product similarity as judged by human assessment. A detailed ex-ploration of this matter is left to future work; however, we note that preliminary assessments attest to the validity of our approach.
This work is supported by Science Foundation Ireland un-der grant 07/CE/I1147. The INSIGHT Centre for Data An-alytics is supported by Science Foundation Ireland under Grant Number SFI/12/RC/2289. [1] S. Aciar, D. Zhang, S. Simoff, and J. Debenham. [2] G. De Francisci Morales, A. Gionis, and C. Lucchese. [3] R. Dong, M. Schaal, M. P. O X  X ahony, K. McCarthy, [4] R. Dong, M. Schaal, M. P. O X  X ahony, and B. Smyth. [5] S. Garcia Esparza, M. P. O X  X ahony, and B. Smyth. [6] M. Hu and B. Liu. Mining and summarizing customer [7] J. S. Justeson and S. M. Katz. Technical terminology: [8] S. Moghaddam and M. Ester. Opinion digger: An [9] M. Pazzani and D. Billsus. Content-based [10] A.-M. Popescu and O. Etzioni. Extracting product [11] K. Zhang, R. Narayanan, and A. Choudhary. Voice of
