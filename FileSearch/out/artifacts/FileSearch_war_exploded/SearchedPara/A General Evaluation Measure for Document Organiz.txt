 A number of key Information Access tasks  X  Document Re-trieval, Clustering, Filtering, and their combinations  X  can be seen as instances of a generic document organization prob-lem that establishes priority and relatedness relationships between documents (in other words, a problem of forming and ranking clusters). As far as we know, no analysis has been made yet on the evaluation of these tasks from a global perspective. In this paper we propose two complementary evaluation measures  X  Reliability and Sensitivity  X  for the generic Document Organization task which are derived from a proposed set of formal constraints (properties that any suitable measure must satisfy).

In addition to be the first measures that can be applied to any mixture of ranking, clustering and filtering tasks, Reli-ability and Sensitivity satisfy more formal constraints than previously existing evaluation metrics for each of the sub-sumed tasks. Besides their formal properties, its most salient feature from an empirical point of view is their strictness: a high score according to the harmonic mean of Reliability and Sensitivity ensures a high score with any of the most popular evaluation metrics in all the Document Retrieval, Clustering and Filtering datasets used in our experiments. B.8 [ Performance and Reliability ]: General Measurement, Performance IR effectiveness measures
Some key Information Access tasks can be seen as in-stances of a generic document organization problem that es-tablishes priority (ranking) and relatedness (clustering) re-lationships between documents. Let us think of a generic document organization system as a function from document pairs d, d 0 into one of these possible relationships: , which means that d has more priority than d 0 , and  X  , which means that d and d 0 have some kind of topical equivalence. We will use the notation k for the cases in which the other two rela-tions do not hold, and the notation { d 1 , d 2 . . . d n that d 1 . . . d n are all related via the topical equivalence rela-tion  X  .
 This general problems subsumes: Document Ranking . This case is illustrated in Table 1. The gold standard establishes (at least) two priority levels (relevant versus non-relevant), and the system output re-turns an ordered list with one priority level per document. For clarity, in the table we use a bold font for relevant docu-ments. Note that, in this problem, it is assumed that there is an unlimited amount of irrelevant documents, while the set of relevant documents is limited. Both the gold standard and the system output contain, implicitly, an unlimited set of documents in the last level. The gold standard contains two priority levels (with the documents manually judged) while the system output contains as many levels as returned documents.

Document Filtering . Table 2 illustrates this case; now, both the gold standard and the system output consist of two priority levels (relevant and irrelevant).

Document Clustering is exemplified in Table 3. Now there is only one priority level, and a set of clusters which contain related documents. Both the gold standard and the system output have the same form. Table 4 illustrates the variant of overlapping clustering, where a document may simultaneously appear in more than one cluster.

Evaluation metrics have been extensively discussed for each of these tasks. There are, however, many practical problems where the system must be able both to detect top-ical relationships (clustering documents) and relative prior-ities (some clusters are more relevant than others). Let us give a couple of examples:
Alert detection . A number of practical information access problems involve detecting, in an incoming stream of documents, new information that is both novel and of high priority. Online Reputation Management, for instance, involves monitoring online information about an entity (a company, brand, product, person, etc.), clustering texts into the main topics, and establishing which of them have higher priority (for instance, those that may potentially damage the reputation of the entity). Table 1: Example of Document Ranking task. Ver-tical ordering indicates relative priorities
Search Results Organization . Given the set of top ranked documents retrieved for a query, a mechanism to group related documents and assign priorities between clus-ters can be used to improve search results in many ways: (i) to enhance search results diversity by maximizing the num-ber of highly relevant topics represented in the top results; (ii) to provide keyword suggestions to refine the query, sam-pled from the most relevant clusters; (iii) to directly display search results as a ranked list of relevant topics correspond-ing to alternative query interpretations, subtopics or facets in the retrieved documents.

This type of tasks (and others such as composite retrieval)  X  which are more complex than the standard ranking and clustering problems  X  still match the generic Document Or-ganization task as we have defined it. An example is shown in Table 5. In the gold standard there is, for instance, one topic with two documents (d1  X  d2) at the top priority level, and two topics at the second level (d3  X  d4 and d5  X  d6). In the next three priority levels there are two single docu-ments and another topic. The example includes overlapping clusters: for instance, d 5 is relevant for two topics, one in the second priority level and another one in the fourth.
Finally, the (potentially long) list of documents at the bot-tom of the gold standard ranking (d 11 , d 12 , d 13 , ... d resents irrelevant documents which are not judged in terms of topical similarity, and are therefore represented via the empty relationship k .

Evaluation measures have been proposed to evaluate clus-tering outputs in the context of document retrieval [14, 8, 18] and to include the notion of diversity in search results[15, 24, 9]. But, to the best of our knowledge, no measure has been previously designed to evaluate the general document organization problem and all the tasks subsumed by this one.

In order to find appropriate evaluation measures for the generic Document Organization problem, we will focus on the specification of the formal constraints that they should satisfy in each of the subsumed tasks (filtering, clustering and ranking). Our goal is finding an evaluation measure for the general document organization task that (i) satisfies formal constraints for all the tasks; (ii) turns into suitable existing measures when mapped into each of the subsumed tasks.

Our research leads to propose Reliability and Sensitivity which are, in short, precision and recall over document pair relationships established in the gold standard, with a suit-able weighting scheme. Comparing them with state of the art measures for each particular scenario, we find that Reli-ability and Sensitivity satisfy more formal constraints than previously existing measures in most cases. In addition, its harmonic mean is stricter than previously existing measures  X  a high score implies high scores with respect to all other standard measures  X , which is an interesting property when the application scenario does not clearly prescribe a more specific evaluation measure.

We will start by establishing the set of formal constraints that any suitable metric should satisfy (Section 2); then we present our proposed measures (Section 3) and discuss its application to the different tasks (Section 4). We end by showing how the measures apply to the more complex doc-ument organization task, and discussing the main implica-tions of our results.
Our approach to measure design is to start defining a set of formal constraints, i.e. a set of formal, verifiable properties that any suitable measure should satisfy. In addition, they explain the nature of different evaluation measure families. We will start by reviewing (or proposing) formal constraints for each of the subsumed tasks, and then merge all collected constraints into a single list of properties for the generalized measures. Table 6 summarize the results of this analysis. Table 5: An example of gold standard and system output for the generic information retrieval task. MAP, DCG, Q measure 4 4 7 4 7 UTILITY, F, LAM% 4 [2] provides a detailed analysis of clustering evaluation measures (grouped by families) and the constraints they should satisfy. We briefly describe here these constraints and refer to [2] for its justification and formal description. We will say that the system output produce clusters while the gold standard is composed by classes . Q represents the quality of a clustering distribution and a, b, c, d.. are docu-ments from different classes:
Cluster Homogeneity : This restriction was firstly pro-posed in [22]. Given a certain system output document distribution, splitting documents that do not belong to the same class, must increase the output quality: Although it seems a very basic constraint, in [2] it is shown that measures based in editing distance do not satisfy it.
Cluster Completeness : The counterpart to the first constraint is that documents belonging to the same assessed class should be grouped in the same cluster [2, 22]: Measures based on set matching, such as Purity and Inverse Purity do not satisfy this contraint.

Rag Bag: This constraint states that introducing disor-der into a disordered cluster (rag bag) is less harmful than introducing disorder into a clean cluster. That is: In general, all traditional measures fail to comply with this constraint.

Cluster size vs. quantity: A small error in a big clus-ter is preferable to a large number of small errors in small clusters [2, 19, 22]. This constraint prevents the problem that measures based on counting pairs [19, 13], overweight big clusters. That is: Measures based on counting pairs are sensitive to the com-binatory explosion of pairs in big clusters, failing on this constraint.

In [2] it is shown how the above contraints discriminate between four families of evaluation measures for the clus-tering problem: measures based on set matching (e.g. F measure, Purity and Inverse Purity), entropy-based mea-sures (Entropy, class Entropy, Mutual Information), mea-sures based on counting pairs and edit distance measures. Interestingly, there is only one pair of measures, BCubed Precision and Recall, that satisfies all constraints simulta-neously, and forms an independent family. However, we will show in this paper that the extended version of Bcubed for overlapping clustering does not satisfy the last constraint.
Filtering is a binary classification problem where there is a relative priority between the two classes: the system must classify each document as positive or negative, being the positive class the one that stores the relevant information. In the filtering scenario all existing measures satisfy a basic constraint:
Priority Constraint Moving a positive (relevant) docu-ment from the predicted negative set to the predicted pos-itive set, or moving a negative (irrelevant) document from the predicted positive to the predicted negative set must in-crease the system output quality. Being d r and d  X  r a judged relevant/irrelevant document respectively: This constraint is satisfied by every standard measure. Filtering is a basic binary classification task that can be evaluated in multiple ways, but it is difficult to define objec-tively desirable boundary constraint that discriminate mea-sures. However, there are descriptive properties which are mutually exclusive and explain the nature of different mea-sure families [3]. These properties focus on how the metrics score non-informative outputs D  X  inf depending on the size of the positive set S ( D  X  inf ). A non-informative output is a random distribution of the documents that is independent on the content of the input documents. An  X  X ll positive X  baseline, for instance, is a non-informative output where the size of the positive set is equivalent to the size of the input set.

For instance, there is a large set of filtering measures that assign a fixed score to every non-informative output: Lam [11], the Macro Average Accuracy [20] and, in general, measures based on correlation such as the Kappa statistic [10]. ( Q ( D  X  inf ) = constant ). Other family of measures assumes that, if the filtering system does not know any-thing, returning all is better than removing documents ran-domly. In this case, the score for a non-informative sys-tem is correlated with the size of the positive output set: ( Q ( D  X  inf )  X  S ( D  X  inf )) This is the case of the harmonic mean of Precision and Recall over the positive class. Fi-nally, measures such as Accuracy or Utility assign relative weights to documents in each of the cases in the confusion matrix; Depending on how these parameters are set and on the distribution of classes in the input, the optimal positive class size for a non-informative output varies.
In order to define a set of constraints for the document ranking (or document retrieval) task, we have to take into account some aspects. First, documents at the top of the ranking must have more weight in the evaluation process: even if the system is able to sort all documents, the user will not be able to explore all of them. The sizes of the relevant and the irrelevant set are expected to be heavily unbalanced: potentially, the amount of irrelevant documents for a given query is (in practice) unlimited. Second, traditional docu-ment retrieval is a mixture of filtering and ranking tasks: an optimal system should not only rank the documents, but also decide on the size of the output set, depending on the amount of relevant documents in the collection and the self-assessed quality of the system output. These two features  X  which are related the fact that the gold standard and the system output take different forms, unlike the filtering and clustering problems  X  make document retrieval evaluation harder than it seems a priori.

There is a large number of proposed measures in the state of the art. Some of the most popular are: precision at cer-tain recall levels or ranking positions, AUC [12], MAP (Mean Average Precision), Discounted Cumulative Gain [16], Ex-pected Reciprocal Rank (ERR)[25], Q-measure, Binary Pref-erence [4], or Rank Biased Precision [21], among many oth-ers. Let us analyze them in terms of formal constraints.
First, the Priority Constraint from the filtering prob-lem also applies here (see previous subsection) and it is satis-fied by most measures. A notable exception is P@10 (preci-sion at the top ten documents retrieved), given that it is not sensitive to relationships after the tenth position in the sys-tem output ranking, and to internal reorderings in the top ten setWe can express this constraint in the context of Doc-ument Retrieval evaluation as, being r and  X  r relevant and irrelevant documents respectively, and being { d 1 , d 2 , d an output ranking: Deepness Constraint : The more we go to a deeper point in the output ranking, the less the probability of documents being explored by the user. Therefore, the effect of a docu-ment priority relationship in the system quality depends on the depth of the documents in the ranking. Being i &lt; j: Measures based on traditional correlation, such as AUC or Kendall, do not satisfy this contraint, because they give the same weight to all elements in the ranking. Also, P@10 obviously does not satisfy this contraint.

Deepness Threshold Constraint: Although P@10 does not satisfy the previous two constraints, one motivation for using it is that in some cases it is advisable to assume a prac-tical deepness threshold. In other words, there is a ranking area which will never be explored by the user. We can ex-press this as a constraint by saying that there exists a value n large enough such that retrieving one relevant document at the top of the rank is better than retrieving n relevant documents after n irrelevant documents:
Q ( { r 1 ,  X  r 2 ,  X  r 3 ..  X  r 2 n } ) &gt; Q ( { X  r 1 ,  X  r We will not include all the formal proofs on how measures satisfy this constraint, due to space availability; we will only discuss the proof that MAP does not satisfy it.

The score for the leftmost distribution in the constraint
N r , assuming that there are N r relevant documents in the collectionWe can prove that the score for the rightmost distribution is always higher: which is bigger than 1 N ply with this constraint either: DCG for the first distribution is 1, before normalization. And for the second distribution we have:
DCG = For instance, according to MAP or DCG, finding 1,000 rel-evant documents after 1,000 irrelevant documents is better than having only one relevant document, but at the top of the rank. This is counterintuitive in many practical settings.
The Q-measure is an extension of MAP for multigraded relevance, having a similar behavior. However, the mea-sure ERR does satisfy this constraint (as well as P@10), due to the strong relevance discount for positions deeper in the rank. Measures with similar weighting schemes also satisfy this constraint, but at the cost of failing to satisfy the next one.

Closeness Threshold Constraint: There exists a (short) ranking area which is always explored by the user. For in-stance, we can assume that the top three documents re-turned by a search engine for informational queries are al-ways inspected. We formalize this constraint as the counter part of the previous constraint: There exists a value n small enough such that retrieving one relevant document in the first position is worse than n relevant documents after n irrelevant documents:
Q ( { r 1 ,  X  r 2 ,  X  r 3 ..  X  r 2 n } ) &lt; Q ( { X  r 1 ,  X  r In the case of P@10, n is 9 (i.e. for any n lower than 9, the constraint is satisfied). ERR, on the other hand, does not satisfy the constraint: given its strong discount with ranking depth, one relevant document at the first position has always more weight than n relevant documents after the position n.
The measures RBP and the discounting function proposed by Smucker and Clarke [23] satisfy all the previous con-straints. The common characteristic of both measures is that they are based on a probabilistic user behavior model. However, all proposed measures fail on the following con-straint.

Confidence Constraint . The output ranking does not necessarily include all documents in the collection and, there-fore, the amount of documents returned is also an aspect of the system quality. The classical TREC ad-hoc evaluation does not consider this aspect, given that the length of the rank is fixed; nevertheless, there is research focused on the prediction of ranking quality, in order to determine when an output rank must be shown to the user. We include this aspect in our constraints by stating that extending the rank with irrelevant documents should decrease the output score: Given that most measures are based on accumulative rele-vance weighted by the location in the ranking [5], we can conclude that irrelevant documents at the bottom of the ranking do not affect the score and the constraint is not sat-isfied. As far as we know, current evaluation measures do not consider this aspect.

In summary, for the tasks subsumed under our document organization problem, the result of our analysis is that (i) in clustering, only the Bcubed measure satisfies all constraints, with the exception of one of them in the case of overlapping clustering; (ii) in the filtering scenario, all measures satisfy the priority constraint, but behave in very different manners with respect to how they score non-informative outputs; and (iii) Finally, in the case of document retrieval, we have de-tected a few measures that satisfy all constraints except the last one, but none that satisfies all constraints.

Our goal is finding a measure that can be applied to all of the subsumed tasks and to any combination of them (i.e. to the general document organization problem), and that satisfies all constraints coming from each of the subsumed tasks. In the next section we introduce our proposal. Our proposal consists of two complementary measures, Reliability and Sensitivity , with a straightforward initial def-inition. Let us consider a system output X and a gold standard G , which are both a set of document relationships r ( d, d 0 )  X  { ,  X  ,  X  X  . The Reliability (R) of relationships in the system output is the probability of finding them in the gold standard. Reversely, the Sensitivity (S) of predicted relationships is the probability of finding them in the sys-tem output when they appear in the gold standard. In other words, R and S are precision and recall of the predicted set of relationships with respect to the true set of relationships: We can express Reliability as a sum of probabilities pondered by the weight of each relationship in X :
We want to observe three restrictions on relationship weights: lationship is a function of the weights of the documents in-volved. In Document Retrieval, the weight of a document will be related to its probability of being inspected by the user, which is related at least to its position in the ranking; starting from a document d determines the weight of doc-ument d ; this restriction prevents the quadratic effect of counting binary relationships and is related to the  X  X ize vs quantity X  restriction for the clustering problem that we want to satisfy; and finally (iii) the contribution w ( d, d 0 d related to d should be proportional to the weight of d 0 .
Then, we can express R and S in terms of weights of single documents. Being w X ( d ) the weight of d in X and being W x,d the sum weight of documents related with d: we can compute R and S as: If all documents have the same weight in the distributions, R and S simply turn into the average R ( d ) and S ( d ) associated to each document:
In the generic document organization task we must as-sume that there exists a virtually unlimited amount of doc-uments in the collection. Therefore, we must weight docu-ments according to their priority in the system output or in the gold standard. There exist several studies on predict-ing the weight of a document in the ranking in terms of the probability to be explored by the user. Most of them are based on assumptions over user behavior [6, 7, 23]. How-ever, there is no clear consensus yet on how to model the empirical user behavior. Rather than this, we focus on ba-sic constraints and interpretability as the main criteria to choose a weighting scheme.

We model the weight of the document in the i position as the weight integration between i  X  1 and i . The first constraint is that the sum weight for all documents must be finite. Therefore, we must employ a function with a converg-ing integral. We select 1 i 2 because it is a simple, soft decay, integrable and convergent function. We leave the refinement of the discounting curve for a latter parameterization step; ideally, our evaluation measure should be as general as pos-sible, and therefore it must have parameters to establish how much of the ranking (v.g. the top 10 vs the top 100) carries on how much of the weight (v.g. 50% or 99% of the overall score) in the evaluation.

According to 1 i 2 , the weight of the document in position i in the priority ordering is: where c 1 is a normalization factor to ensure that the sum is 1. c is another parameter that moves the function in order to give more or less weight to the high priority documents. Stating that the total sum weight is one: Therefore, c = c 1 . The next constraint is that we should be able to parameterize the weighting curve in an interpretable way. Ideally, we want to be able to set two parameters n and W n which mean that the first n documents must cover a W n weight ratio of the overall evaluation score. For in-stance, we may want to state that the first 30 positions in the ranking ( n = 30) will have an 80% of the total weight in the evaluation measure ( W n = 0 . 8). Therefore: Now, we can estimate the weight of each document in the system output or in the gold-standard according to Formula 1. Documents at the same priority level share the interval weight. Being n and n = the amount of documents with more and equal relevance than d respectively we can esti-mate the weight of each document as:
Smucker and Clarke proposed a discounting model based on exploration time calibration[23] which considers addi-tional aspects such as the relevance and length of documents. Actually, this model is compatible with our proposal: we can incorporate this by replacing the i position of documents with a time function. We leave this analysis for future work.
As we mentioned earlier, a document may appear in mul-tiple clusters (corresponding, for instance, to different in-formation nuggets in the document), and therefore it may appear at multiple priority levels. If overlapping between clusters is allowed, a document has potentially a different number of occurrences in the gold and system output distri-bution. If there exists only one instance of d and d 0 in both the gold standard and the system output, the probability of coocurrence is 1 when the relationships match. Otherwise, following the extended Bcubed measure proposed in [2], we assume the best possible correspondence between relation-ships in X and G . For instance, if two documents are related in the system output less times than in the gold standard, then all the predicted relationships are assumed to be cor-rect. Otherwise, the probability is the ratio of gold relation-ships per system relationships. Formally, being | r and | r X ( d, d 0 ) | the number of occurrences of r ( d, d and X respectively:
Here we state the method to compute R and S over the general document organization task. We assume that there is a set of prioritized documents X r organized by levels and clusters and a special, bottom level containing an unlimited amount of irrelevant/discarded documents X  X  r (see Table 5). The weight of a single document in the set of prioritized documents X r is computed as in Equation 2. The weight of prioritized documents X r in the system output is: The weight of the long tail of non prioritized documents X in the system output is: The sum weight of documents priority related with d is 1 minus the sum weight of documents in the same priority level: The probability P ( r ( d ij , d kl )  X  G ) for a document relation-ship between two document occurrences in X r is computed as in Equation 3 for both the clustering and priority rela-tionships.

As for the relationships between the unlimited tail X and documents in X r , we must consider that all documents in the infinite set X  X  r have the same weight. Therefore, the finite amount of relevant documents in the long tail has no effect. Then, any relevant document in X r is correctly related with all documents in X  X  r if it appears between the prioritized documents in G r . According to Equation 3, being d ij the j occurrence of document d i in the system output and being D i its set of occurrences 1 :
According to all of this, Reliability over priority relation-ships can be computed as follows 2
X
Assuming that the documents in the long tail do not have clustering relationships with each other, Reliability over clus-tering relationships can be computed as follows: Sensitivity is computed in the same way, but exchanging X by G and x by g . The complexity of this computation is O ( n 2 ), being n the amount of ranked documents X prioritized documents G r . For the sake of simplicity, we notate P ( d ij X  X  r  X  G ) as
The first component covers the relationships within docu-ments in X r . The second and third components cover the relationship X r  X  X   X  r and X  X  r  X  X  r respectively.
As far as we know, Reliability and Sensitivity are the first measures which are applicable to the general document or-ganization task. For this reason, our comparison will focus on how R and S behave in each of the subsumed tasks (filter-ing, clustering, ranking) with respect to previously existing measures. There are many ways of meta-evaluating a new measure. The most direct one consists of comparing measure scores vs. human assessments of quality, or system results in some extrinsic task. Other meta-evaluation criteria focus on the hability to capture information from limited data sets. Some examples are discriminativeness [23], statistical significant differences between systems[21], stability method, swap method, robustness against noisy data, correlation between rankings over different data sets [5], etc. The main drawback of these methods is that a measure can be, for instance, perfectly discriminative under limited data sets without giving infor-mation about quality. As an extreme example, the rank-ing length can be perfectly discriminative but not useful for evaluation purposes. in this study we want to investigate how useful is to eval-uate measures in terms of a basic, intuitive set of formal constraints. According to this, our first meta-evaluation cri-terion is the ability to satisfy the stated formal constraints. After that formal analysis, and in order to compare mea-sures empirically over data sets, we will assume that current standard measures used by the community are, to a certain extent, reliable: we assume that all of them give some use-ful information about system quality in certain scenarios. The problem is that, in most cases, we do not know exactly the real scenarios in which the system will be employed. In previous experiments, particularly in the case of clustering and filtering, it has been shown that there can be a very low correlation between measure results [3]. Therefore, we cannot expect to find a measure which is correlated with all of them. However, we can at least ensure that a high score according to the measure implies a high score according to all measures. This is strictness . In other words, a good score in a reliable measure should ensure a good system regard-less of the environmental conditions. Note that strictness itself is not enough as a meta-evaluation criterion (a mea-sure that always scores zero is the strictest of all). Note also that strictness could easily be achieved by computing a harmonic mean of the most popular measures, but we would have to solve scale-normalization issues and we would end up with a measure that would be hard to interpret, and would not cover all quality aspects in an homogeneous man-ner. Therefore, and ad-hoc combination of metrics is not the best solution to have a strict measure.

Let us quantify Strictness in the following way: we com-pute, for each measure, all the rank positions obtained by each system output o  X  O for each topic and measure. Then we define strictness as the largest difference between a high rank assigned by our measure and a low rank assigned by a traditional measure:
In order to maintain a correspondence with standard meta-evaluation criteria, we will also compute the robustness of measures in terms of the average Spearman correlation of system scores across topics. Being Rnk ( m, t i ) the ranking of systems produced by metric m for topic t i : Robustness( m ) = Avg i,j ( Spearman ( Rnk ( m, t i ) , Rnk ( m, t
We now discuss each of the subsumed tasks.
In the non overlapped clustering scenario where all doc-uments has the same weight, R and S turn into Bcubed Precision and Bcubed Recall: Bcubed measures are the only ones that satisfy all the clus-tering constraints [2]. However, in the extended version for overlapping clustering, the Cluster Size vs Quantity restric-tion is no longer satisfied. This is due to the fact that, in the extended Bcubed version, all the relationships from one doc-ument are computed as a single unit, even when it belongs to several clusters at the same time. The result is that if two documents d and d 0 are related to each other several times (e.g. they share more than one information nugget), break-ing all these relationships is penalized only once. Therefore, splitting n clusters can have less effect than splitting one document from a cluster with size n . The solution provided by R and S consists of considering the document in each different cluster as a separate document. BCubed measures are well-known and have already been studied formally and empirically and compared with other measures in previous work [2]. Therefore, we will focus on the other subsumed tasks for our meta evaluation. Rob. 0.3 0.39 0.38 0.49 0.70 Strict. -0.91 -0.91 -0.96 -0.92 -0.78 Table 7: Robustness and Strictness achieved by Fil-tering Evaluation Measures, WEPS2 test set
The Filtering scenario consists of distributing a finite set of documents into two priority levels (binary classification). Being X r and G r the sets of prioritized documents in the system output and gold standard respectively, the correct relationships in the system output are priority relationships between relevant documents in X r and irrelevant documents in  X X r . Therefore, R corresponds with:
This corresponds with the product of precisions over pos-itive and negative sets in the output. Analogously, Sensitiv-ity corresponds with the product of Recalls over the positive and negative sets.
 Just like any other filtering measure, the combination of R and S (using the F measure) satisfies the priority constraint. Let us denotate the harmonic mean of R and S as R*S. With respect to how R*S scores non-informative outputs, its be-havior is a mixture of the other measure families: it assigns a zero-score to the all-relevant and all-irrelevant baselines, because they are not able to distinguish any useful priority relationship between documents. Other arbitrary partitions receive scores that depend on how the ground truth parti-tions the test set.
For the filtering scenario we employ the evaluation cor-pus and system results from the second task in the WePS3 competition [1]. The task consisted of classifying Twitter entries [17] that contain a company name as relevant when they do refer to the company and irrelevant otherwise. The test set includes tweets for 47 companies and the training set includes 52 company names. For each company, around 400 tweets were retrieved using the company name as query. The ratio of related tweets per company name is variable, covering both extremely low and high ratios. We will refer to a company tweet set retrieved by a query in a time slot as an input stream or topic. Five research teams partici-pated in the competition, and sixteen runs were evaluated. The organizers included two baseline systems: the placebo system (all true) and its opposite (all false).

Figure 1 shows the correspondence between R*S and stan-dard measures employed in different evaluation campaigns; F represents the harmonic mean of precision and recall over the positive class. The most relevant fact is that a high score in all standard measures is necessary to achieve a high score according to R*S. Table 7 shows the Robustness and strictness of measures (see Section 4.1) in this test set. The strictest measure is R*S (-0.78), followed by Lam% (-0.89). The measures F, Utility and Accuracy achieve -0.92 of Strict-ness. There exists a clear difference in robustness between R*S and other measures. In this scenario, robustness seems to be correlated with strictness.
Just like MAP and DCG, Reliability and Sensitivity sat-isfy the first two constraints, Priority and Deepness. Adding an incorrect relationship produces a score decrease, and the effect of an incorrect relationship depends on the deepness of the related documents in the system output ranking. How-ever, unlike MAP or DCG, Reliability and Sensitivity also satisfy the third constraint, Deepness Threshold. And, un-like MRR, they also satisfy the fourth constraint, Closeness Threshold. Due to space constraints, we do not include here the formal proofs.

Note that the parameters n and W n offer great flexibil-ity, and permit to accomodate scenarios where only the top documents in the rank matter (as in Web search) as well as recall-oriented scenarios, simply adjusting the parame-ters accordingly. The formal constraints are satisfied for any paratemer setting. For example, with the setting W 30 = 0 . 8, ranking 30 relevant documents after 30 irrelevant documents is worse than retrieving one document in the first position, but retrieving five relevant documents in the first 10 posi-tions is better than retrieving only one document at the top 1. The cut point is n=20.

As for the confidence constraint, note that the more we include irrelevant documents in the ranking, the more we in-clude incorrect priority relationships between the priorized documents and the long tail. We could also satisfy this prop-erty with measures like RBP by using a discounting score for each irrelevant document, but then the nature of RBP would change and its formal properties would not hold any longer. Table 6 summarizes the results of the formal con-straint analysis.
We have used queries 701 to 750 in the GOV-2 collection, which were employed in the TREC 2004 Terabyte Track. The GOV-2 corpus consists of approximately 25 million doc-uments. Therefore, we can assume that the amount of doc-uments in the collection is unlimited for practical purposes. Relevant documents were manually annotated for each query. We consider the results of 60 retrieval systems developed by the participants in the track. Two relevance levels (high and medium) were considered in the human annotation.
 Table 8 shows the strictness and robustness 3 of measures. We consider the strictness of all measures against the stan-dard measures MAP, DCP, P@10, MRR, RBP p =0 . 8 and RBP p =0 . 95 . The strictest measure is R*S with W (-0.58) followed by MRR (-0.63), MAP (-0.74) and P@10 (-0.75). However, R*S with W 80 = 30 achieves low robustness, while R*S with n 80 = 800 and DCG have higher robustness at the cost of strictness. It seems that there is a trade-off between strictness and robustness. A possible explanation is that the robustness of measures across test cases depends on the amount of data that is considered for the evalua-Discriminating systems that return short rankings is easy. In order to avoid this noise, we only consider in this test those systems that return at least 1000 documents Table 9: Sensitivity and Reliability examples for the generic Document Organization scenario. tion. Therefore, measures focused on the top of the ranking are less robust. Given that the max function in the strict-ness definition is very sensitive to outlier systems, we have also computed strictness considering the 10 maximum dif-ferences; results were the same.
As far as we know, there is no measure that can be di-rectly compared with R*S in the generic document organi-zation scenario as we have defined it. In this section, we illustrate the behavior of R*S across several instances of system outputs. See Table 9. The gold standard consists of seven relevant documents distributed along three priority levels. Each priority level contains two clusters (or informa-tion nuggets) and documents 4,6 and 7 appear in two clusters and priority levels simultaneously. The Reliability and Sen-sitivity over priority and clustering relationships have been computed with W 1 0 = 0 . 8, i.e., we require that the first 10 occurrences represent 80% of the score.

Of course, Reliability and Sensitivity are maximal for the gold standard. Starting from this, we can identify in the ta-ble the following behaviors: (System 1) breaking clusters at low priority levels decreases slightly S  X  , (System 2) break-ing clusters at high priority levels decreases S  X  to a greater extent; (System 3) Removing one document (d 2 ) decreases priority and clustering sensitivity; (System 4) removing and adding noisy documents (d 2 and d 8 ) decreases both sensi-tivity and reliability scores and in (System 5) we swap all priority levels. Then, clustering is perfect and the prior-ity R and S decrease, but not to zero. Notice that the 10 first documents have only a 80% of weight in the evaluation. There exists a long tail of documents from which this set is priorized.
In this paper we have discussed how some prominent Infor-mation Access tasks  X  Document Retrieval, Clustering and Filtering  X  can be subsumed under a generic Document Or-ganization task that establishes two kinds of binary relation-ships between documents: relatedness (which forms clusters) and priority (ranking). We have then introduced two evalu-ation measures for the document organization problem: Re-liability and Sensitivity, which are precision and recall of the predicted set of relationships with respect to the true rela-tionships, with a specific relative weighting scheme between relations. The main contribution of this paper is that R and S can be applied to complex tasks which involve ranking, clustering and filtering at the same time. An example task is online reputation monitoring, where systems have to (i) filter out irrelevant information, (ii) organize relevant infor-mation in topics, and (iii) decide which topics have more priority from the point of view of reputation management. R and S are able to provide a unique evaluation measure for this combined problem.

In addition, R and S satisfy all formal constraints in each of the subsumed tasks; in particular, they satisfy more for-mal constraints than any previous measure in the Document Retrieval task, and they are able to accomodate several re-trieval scenarios (from precision-oriented to recall-oriented) via two parameters that establish that the first n levels in the rank carry on a fraction W n of the overall quality score. Our empirical study indicates that R and S are stricter than standard measures, i.e., a high result with R and S ensures a high result with any other standard measure in all the subsumed tasks. That makes R and S a preferable choice in cases where the application scenario does not clearly point towards any of the previously existing measures, because it guarantees that a good result will still hold according to any other standard measure.

For its application to combined tasks, future work involves extending the set of binary relations  X  the general principle of R and S can be applied to any kind of relations  X  together with a a careful analysis on how to assign relative weights to different types of relations; for instance, in certain applica-tion scenarios one type of relation (priority or relatedness) may dominate and obscure what is going on with the other, making R and S less transparent and/or useful.
 Code to use R and S is available at http://nlp.uned.es . This work has been partially funded by EU FP7 project Limosine (grant number 288024), a Google Faculty Research Award ( Axiometrics , project Holopedia (grant from the Span-ish goverment) and project MA2VICMR (grant from the government of Comunidad de Madrid). [1] E. Amig  X o, J. Artiles, J. Gonzalo, D. Spina, B. Liu, and [2] E. Amig  X o, J. Gonzalo, J. Artiles, and F. Verdejo. A [3] E. Amig  X o, J. Gonzalo, and F. Verdejo. A comparison [4] C. Buckley and E. M. Voorhees. Retrieval evaluation [5] B. Carterette. System effectiveness, user models, and [6] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating [7] O. Chapelle and Y. Zhang. A dynamic bayesian [8] J. M. Cigarr  X an, A. Pe  X nas, J. Gonzalo, and F. Verdejo. [9] C. L. A. Clarke, M. Kolla, G. V. Cormack, [10] J. Cohen. A Coefficient of Agreement for Nominal [11] G. Cormack and T. Lynam. Trec 2005 spam track [12] G. V. Cormack and T. R. Lynam. TREC 2005 Spam [13] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On [14] M. A. Hearst and J. O. Pedersen. Reexamining the [15] B. Hu, Y. Zhang, W. Chen, G. Wang, and Q. Yang. [16] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [17] B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps [18] A. Leuski. Evaluating document clustering for [19] M. Meila. Comparing clusterings. In Proceedings of [20] T. M. Mitchell. Machine learning . McGraw Hill, New [21] A. Moffat and J. Zobel. Rank-biased precision for [22] A. Rosenberg and J. Hirschberg. V-measure: A [23] M. D. Smucker and C. L. Clarke. Time-based [24] S. Vargas and P. Castells. Rank and relevance in [25] E. M. Voorhees. The trec-8 question answering track
