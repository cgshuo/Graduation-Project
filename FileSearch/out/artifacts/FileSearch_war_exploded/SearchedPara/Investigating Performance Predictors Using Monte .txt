 The standard deviation of scores in the top k documents of a ranked list has been shown to be significantly correlated with average precision and has been the basis of a number of query performance predictors [8, 6, 3]. In this paper, we outline two hypotheses that aid in understanding this cor-relation. Using score distribution (SD) models with known parameters, we create a large number of document rankings using Monte Carlo simulation to test the validity of these hypotheses.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval: Retrieval models Keywords: Monte Carlo Simulation, Score Distributions
Many works [2, 1] have shown that document scores can be modelled using score distributions (SD), while others [5] have shown that unexpected observations in large collec-tions can be explained using SD models. Therefore, and in particular, we assume document scores can be drawn from an SD model comprised of two-lognormal distribu-tions, where f ( s | 1) and f ( s | 0) are the probability density function of the relevant (1) and non-relevant (0) document scores respectively, and where  X  is the mixing parameter of the recall-fallout convexity hypothesis (RFCH) [7], each lognormal SD model can be described using four parameters {  X  1 ,  X  0 ,  X  1 =  X  0 ,  X  } where  X  1 &gt;  X  0 and  X  1 =  X  research [4] has shown that the RFCH implies the follow-ing relationship 1 between the moments in a two-lognormal model: (E) and variance (Var) of relevant (1) and non-relevant (0)
A two-gamma model is another suitable SD model that yields similar results to those in this paper. The moment relationship is similar for a two-gamma SD model that ad-heres to the RFCH. In that model, the variance replaces the document scores respectively. It can be seen that the RFCH implies that the standard deviation of relevant scores (mono-tonically related to variance) is proportional to the expected value of the relevant scores. All else being equal, it is intu-itive to assume that the larger the expected value of the relevant scores is (i.e. E [ s 1 ]), the higher average precision will be.
We now outline two hypotheses that aim to explain the correlation between average precision and the standard de-viation in the head of a ranked list: H1: As a consequence of the RFCH, the standard deviation at the head of a ranked list is positively correlated with the mean score of relevant documents, which in turn is positively correlated with average precision.
 H2: A lower standard deviation of document scores in the head of a ranked list indicates that the separation between the relevant and non-relevant distributions is low, and there-fore there is a higher contamination of non-relevant docu-ments in the head of the list. This leads to a lower average precision.
We now investigate these hypotheses using Monte Carlo simulation. We simulate rankings by drawing samples of document scores from SD models with known parameters. In particular, to test the first hypothesis ( H1 ), we simulate rankings returned for 50 queries. Each ranking is drawn from an SD model with a different  X  1 value (variable  X  1 ranging uniformly from 1.5 to 2.5, while  X  0 = 1 . 5,  X  0 and  X  remains fixed 2 . This experimental setting changes the distribution (i.e. the mean score) of relevant documents while keeping the other parameters constant. The Kendall- X  correlation of average precision with the standard deviation-at-k documents for the 50 rankings is recorded. We repeat this process 50 times to ensure that the resultant correla-tion coefficients are not spurious. The average Kendall- X  is reported.

To test the second hypothesis ( H2 ), we simulate rankings for another 50 queries. Each of these rankings is drawn from an SD model with a different  X  0 value ranging uniformly from 1.5 to 2.5 (variable  X  0 ) while  X  1 = 2 . 5,  X  0 = 0 . 25, and  X  remains fixed. This experimental setting changes the distribution (i.e. the mean score) of non-relevant documents
T hese values were chosen by fitting a two-lognormal SD model to scores returned by a Language Model run on actual TREC data (disks 4 and 5) while keeping the remaining parameters static. This essen-t ially increases the contamination of non-relevant documents in the head of a ranking. Again we record the Kendall- X  cor-relation of average precision with the standard deviation-at-k documents and average the correlation for 50 simulations.
We perform both of these experiments, (variable  X  1 ) and (variable  X  0 ) for different values of  X  (i.e. the parameter controlling the portion of relevant documents drawn). We wanted each sample ranking to contain approximately 50 relevant documents 3 (R) and so we drew different sample sizes (N) from each SD model so that we simulate returned sets of various sizes. In particular, we set N = { 250, 500, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000 } and so the mixing parameters was  X  = 50 /N for each value of N. Figure 1: Average precision of rankings drawn from S D models with  X  0 = 0 . 25 ,  X  0 = 1 . 5 and variable  X  for various mixing levels  X  = 50 /N .

Each point in Figure 1 shows the mean average precision of 50 simulated rankings drawn from an SD model. Plotted are SD models for various  X  1 and  X  values. Intuitively, we can see that the average precision of the rankings increase as both  X  and  X  1 increase. This shows that our simulated rankings cover a large range of average precision values.
Table 1 shows the average Kendall- X  correlation of average precision with the standard deviation-at-k for the simulated rankings for both experimental settings ( variable  X  1 and variable  X  0 ). We can see that when only varying  X  1 , the correlation coefficient is high and significant for all values of  X  . This is a consequence of the RFCH (equation 1) and is similar for many values of k (not shown due to space limitations).

We can also see that there is a significant correlation be-tween standard deviation-at-100 and average precision when varying  X  0 for most settings of  X  . By examining our simu-lated rankings, we have determined that this is because the standard deviation is measured at a value k = 100, which is higher than the number of relevant documents (R) in the average ranking. Therefore, the standard deviation-at-k has
F or TREC disks 4 and 5 and for many TREC collections, this is approximately the average number of relevant docu-ments per topic. the potential to measure the score of all relevant documents but also includes the score of several non-relevant documents when k &gt; R . Thus, the lower the score of these non-relevant documents compared to the relevant documents, the better the query (i.e. the degree of separation between  X  1 and  X  measured when k &gt; R ). It can be seen that the correlation of standard deviation with average precision is lower when k = 25. Also shown is the correlation when k = 400, which shows that including too many documents in the standard deviation calculation leads to a lower correlation when only varying  X  0 . A high deviation can in some cases indicate that the non-relevant documents have contaminated the relevant documents to a high degree, due to the right-skewed nature of score distributions. This can lead to a negative correla-tion between standard deviation-at-k and average precision when  X  is low and k is high.
 Table 1: Average Kendall- X  correlation of average precision of simulated rankings with the standard deviation of scores for top k documents
W e have presented two hypothesis regarding the relation-ship between average precision and the standard deviation in the head of a ranking. Furthermore, we performed an analysis using SD models that indicates the conditions un-der which these hypotheses can be deemed true.

