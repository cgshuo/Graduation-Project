 The Relevant in Context retrieval task is document or article re-trieval with a twist, where not only the relevant articles should be retrieved but also the relevant information within each article (cap-tured by a set of XML elements) should be correctly identified. Our main research question is: how to evaluate the Relevant in Context task? We propose a generalized average precision measure that meets two main requirements: i) the score reflects the ranked list of articles inherent in the result list, and at the same time ii) the score also reflects how well the retrieved information per article (i.e., the set of elements) corresponds to the relevant information. The resulting measure was used at INEX 2006.

Traditional document retrieval returns atomic documents as an-swers, and leaves it to users to locate the relevant information in-side the document. Focused retrieval, such as practiced at INEX [2], studies ways to provide users with direct access to relevant in-formation in structured documents. INEX 2006 introduced a new retrieval task, Relevant in Context (RiC), that combines article re-trieval with XML element retrieval [1]. The RiC task is document or article retrieval with a twist, where not only the relevant articles should be retrieved but also a set of XML elements representing the relevant information within each article. Phrased differently, the system should return the relevant information (captured by a set of XML elements) within the context of the full article.
The task corresponds to an end-user task where focused retrieval results are grouped per article, in their original document order, providing access through further navigational means. This assumes that users consider the article as the most natural unit of retrieval, and prefer an overview of relevance in their context. Interactive experiments at INEX provided support for this task [4]. Moreover, the RiC task corresponds with the assessors X  task at INEX, where assessors are asked to highlight the relevant information in a pooled set of articles. The difference is that the INEX assessors can high-light sentences, whereas systems could only return XML elements.
How to evaluate the RiC task? We face two main requirements: i) the score should reflect the ranked list of articles inherent in the result list, and ii) the score should also reflect how well the retrieved information per article corresponds to the relevant information.
A submission for the task is a ranked list of articles, with for each article an unranked set of elements covering the relevant ma-terial in the article [1]. 1 Hence, the evaluation is based on a ranked list of articles, where per article-rank we obtain a score reflecting how well the retrieved set of elements corresponds to the relevant information in the article.
Per retrieved article, the text retrieved by the selected elements is compared to the text highlighted by the assessor. We calculate Pre-cision as the fraction of retrieved text (in bytes) that is highlighted; Recall as the fraction of highlighted text (in bytes) that is retrieved; and F-Score as the combination of precision and recall using the harmonic mean, resulting in a score in [0,1] per article.
More formally, let us assume that the function rel (  X  ) gives the relevant or highlighted content of an element (or article), and that ret (  X  ) gives the retrieved content. Furthermore, assume that set operations (union, intersection) are defined for elements (and rel , ret ) with the expected behavior in terms of their yield or content. Finally, assume that the function | . | gives the byte-length of an element or set of elements in terms of their yield or content. Then, for each retrieved article a , Precision and Recall are defined as if | rel ( a ) | &gt; 0 , and R ( a ) = 0 otherwise. The combination is the standard F-score, The resulting F-score varies between 0 (article without relevance, or none of the relevance is retrieved) and 1 (all relevant text is re-trieved and nothing more), matching the second requirement. We have a ranked list of articles, with for each article a score F ( a )  X  [0 , 1] . Hence, we need a generalized measure, and we opt for the most straightforward generalization of precision and re-call [3, p.1122-1123]. Over the ranked list of articles, we calculate generalized Precision as the sum of F-scores up to an article-rank, divided by the article-rank; and generalized Recall as the number of articles with relevance retrieved up to an article rank, divided by the total number of articles with relevance.

More formally, let us assume that for our topic there are in to-tal Numrel articles with relevance, and assume that the function
Due to the tree structure of XML documents, only disjoint (non-overlapping) elements were permitted.
 relart ( a ) = 1 if article a contains some relevant information, and relart ( a ) = 0 otherwise. Then, at each article-rank of the list r generalized Precision and Recall are defined as
These generalized measures are compatible with the standard precision/recall measures, matching the first requirement. Specif-ically, the generalized Recall definition leads to a very natural in-terpretation of recall as the fraction of articles with relevance that has been retrieved. In that sense, the Average generalized Preci-sion (AgP) for a topic can be calculated by averaging the gener-alized Precision at natural recall points where generalized Recall increases. When looking at a set of topics, the Mean Average gen-eralized Precision (MAgP) is the mean of the Average generalized Precision scores per topic.
We now look at the ability of the MAgP measure to distinguish between different retrieval systems. We use the top 20 submissions to the INEX 2006 Relevant in Context retrieval task, based on the MAgP score over 111 topics, and label systems by their rank. determine whether improvements are significant, we use the boot-strap method (one-tailed at significance level 0.05 over 1,000 re-samples). Table 1 shows the extent to which higher ranked systems improve significantly over lower ranked system in terms of their MAgP scores. We observe that the MAgP measure is fairly effec-tive at distinguishing between different systems: no less than 112 of the 190 pairwise comparisons are significant.
 It is interesting to compare the MAgP measure to standard MAP. First, the recall dimension in MAgP is identical to that used in stan-dard document retrieval MAP (so in fact not generalized). Second, the precision dimension differs only slightly: in case of MAgP, the calculated article score represents a (partial) F-score, while in case of MAP the article score will always be an increment of 1. We compare how the relative systems ranking based on MAgP cor-relates with the systems ranking based on MAP. That is, we de-rive an article ranking and evaluate systems using standard MAP ( trec eval ). The rank correlation (Kendall X  X  tau) between MAP and MAgP is 0.674 over the top 20 official submissions.

Table 2 shows the extent to which higher ranked systems im-prove significantly over lower ranked system in terms of their MAP scores. Note that systems are labeled by their ranks based on the
Note that this contains runs from 10 participants in total, including some very close variants of the same system.
 overall MAgP scores, so the order reflects the differences in the re-spective rankings. We see a few notable upsets: the system ranked 5th on MAgP is now ranked 1st on MAP, and the system ranked 20th on MAgP is now ranked 6th. This clearly shows that there are important differences between the tasks of document retrieval and RiC. Inspection of the table reveals that no less than 95 of the 190 pairwise comparisons are significant for the MAP measure.
When comparing the numbers of significant differences in Ta-ble 1 and Table 2, we see that MAgP is distinguishing more sys-tems. This is perhaps not surprising since we are loosing informa-tion in the abstraction toward the article level needed for MAP.
Our main aim was to develop a measure for the evaluation of the Relevant in Context task, which represents a combination of document retrieval with XML element retrieval. We developed a measure that meets two main requirements: i) the score reflects the ranked list of articles inherent in the result list, and at the same time ii) the score also reflects how well the retrieved information per article corresponds to the relevant information.

The RiC task is very similar to the INEX assessors X  task, who are highlighting relevant information in a pooled set of articles. Note that, since the assessors can highlight sentences and systems could only return XML elements, it will make it impossible for a system to obtain a perfect score of 1 (although the theoretical maximum will be close to 1). At INEX 2007, systems will be allowed to re-turn arbitrary passages that can be directly evaluated by the MAgP measure, which in turn could enable a system to receive a perfect score when exactly matching the highlighted text.
 Acknowledgments INEX is an activity of the Network of Excel-lence in Digital Libraries. Jaap Kamps was supported by the Nether-lands Organization for Scientific Research (NWO) under project numbers 612.066.513, 639.072.601, and 640.001.501), and by the E.U. X  X  6th FP for RTD (project MultiMATCH contract IST-033104).
