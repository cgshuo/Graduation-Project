 The retrieval of sentences is a core task within Information Re-trieval. In this poster we employ a Language Model that incor-porates a prior which encodes the importance of sentences within the retrieval model. Then, in a set of comprehensive experiments using the TREC Novelty Tracks, we show that including this prior substantially improves retrieval effectiveness, and significantly out-performs the current state of the art in sentence retrieval. Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation, Performance Keywords: Sentence Retrieval, Language Models
Sentence retrieval (SR) is a challenging problem area that has received a significant amount of attention recently [1, 4, 5, 7]. The main SR task consists of finding relevant sentences from a docu-ment base given a query. This task is very useful in a wide range of Information Retrieval (IR) applications, such as summarization, question answering, and opinion mining. However, the task has usually been approached by taking a document retrieval model and adapting it for SR. In fact, the model that is the state of the art in SR is known as term frequency-inverse sentence frequency (TF.ISF) , which is analogous to the traditional TF.IDF method used in doc-ument retrieval [1, 4]. While, numerous attempts to develop more sophisticated models that employ techniques such as Natural Lan-guage Processing and Clustering have been proposed [2, 3, 8], they have failed to significantly and consistently outperform the TF.ISF method. Consequently, little progress has been made in terms of improving sentence retrieval effectiveness.

In this poster we posit that a relevant sentence needs to be indica-tive of the query, but also representative and important within the context of the document; i.e. we assume that key statements within a document are more likely to be relevant, if they are on topic. With this aim, we adopt the Language Modeling framework and include a sentence based prior to encode the importance of a sentence in a document within the model. In a set of experiments performed over several TREC test collections, we compare the proposed mod-els against existing SR models and show that using an importance prior within a LM framework delivers retrieval performance that significantly outperforms the current state of the art.
 of the documents, and treat p ( d ) as a constant. The p ( s | d ) repre-sents how likely the sentence is to be generated from the document, whereas p ( s ) represents how likely the sentence is to be generated randomly. The ratio between the two expresses the importance of the sentence. Observe that p ( d | s ) will give preference to those sen-tences that are central to the document X  X  topics (i.e. high p ( s | d ) ) but also rare within the collection (i.e. low p ( s ) ). It should also be noted that this prior will implicitly tend to favor longer sentences because p ( t | d ) is greater than p ( t ) 3 . With the importance prior, in our experiments we shall refer to the extended Language Models as 3MM.IP, JM.IP, and DIR.IP.
In this paper, we adopt the same definition of the sentence re-trieval problem as proposed in the TREC Novelty Tracks. Although these tracks are mostly focused on researching redundancy filter-ing, they also involve a SR task that enables research into how to retrieve sentences that are relevant to a given query. The SR prob-lem is framed as follows: given a textual query that represents an information need, a ranked set of documents is supplied and the systems have to process this ranking to extract the sentences that are estimated as relevant to the information need.

Data : Along with this definition we used all three TREC Nov-elty Track collections 2002, 2003 and 2004 4 . Each collection was indexed using the Lemur toolkit 5 , where standard stop words were removed but stemming was not applied. The corresponding set of topics for each collection was used, where short queries were con-structed taking the title field of the TREC Topic 6 . The TREC 2002 collection was used to train and estimate the parameters of each model used, while the TREC 2003 and 2004 collections were used to test the sentence retrieval models.

Models : In this work, we used a number of baseline models: (i) the current state of the art, TF.ISF [1], (ii) BM25 [6], which closely matches the performance of TF.ISF but is parameterized [4], and (iii) the standard sentence language models, JM and DIR, as well as 3MM [4, 5]. These are compared against the extended sentence language models, JM.IP, DIR.IP and 3MM.IP.

Measures : For all of our experiments, we report the perfor-mance of each method using Mean Average Precision (MAP) and R-Prec. To compare the differences in performance between the different methods, statistical significance tests were applied using the t-test with a 95% confidence level. Here, we show the statis-tical comparisons between each model and TF.ISF and DIR (see Table 1).

Results : Table 1 shows the performance obtained for each of the different models tested. Firstly, we note that the standard sentence language models do not outperform the state of the art TF.ISF or BM25. And in fact, TF.ISF and BM25 are significantly better than DIR. However, when the prior on sentence importance is incorpo-rated within the language modeling framework, we note that these models all significantly outperform both TF.ISF and DIR, with im-provements of up to 20% in some cases. The model that performed the best overall was DIR.IP which resulted in gains of 5-8% over TF.ISF. This is a substantive gain making these extended models an attractive and stronger baseline.
