 Many learning problems can be naturally formulated in terms of multi-category classification or signals in terms of a large common overcomplete dictionary, which is a multi-response regression problem. In each case, while the details of the estimators vary from instance to instance, across high-dimensional space. How to find this common sparsity pattern is an interesting learning task. In the parametric setting, progress has been recently made on such problems using regularization based on the sum of supremum norms (Turlach et al., 2005; Tropp et al., 2006; Zhang, 2006). For example, consider the K -task linear regression problem y ( k ) i =  X  ( k ) 0 + task. Using quadratic loss, Zhang (2006) suggests the following estimator  X  X rouping X  the elements in  X  j such that they can be shrunk towards zero simultaneously. The problems of multi-response (or multivariate) regression and multi-category classification can be viewed as a special case of the multi-task regression problem where tasks share the same design matrix. Turlach et al. (2005) and Fornasier and Rauhut (2008) propose the same sum of sup-norms (2008) propose the sup-norm support vector machine, demonstrating its effectiveness on gene data. In this paper we develop new methods for nonparametric estimation for such multi-task and multi-mate smooth functions of the data, and formulate a regularization framework that encourages joint functional sparsity, where the component functions can be different across tasks while sharing a common sparsity pattern. Building on a recently proposed method called sparse additive models, or  X  X pAM X  (Ravikumar et al., 2007), we propose a convex regularization functional that can be viewed as a nonparametric analog of the sum of sup-norms regularization for linear models. Based on this regularization functional, we develop new models for nonparametric multi-task regression and classification, including multi-task sparse additive models (MT-SpAM), multi-response sparse additive models (MR-SpAM), and sparse multi-category additive logistic regression (SMALR). The contributions of this work include (1) an efficient iterative algorithm based on a functional response SpAM procedures, (2) a penalized local scoring algorithm that corresponds to fitting a sequence of multi-response SpAM estimates for sparse multi-category additive logistic regression, and (3) the successful application of this methodology to multi-category tumor classification and biomarker discovery from gene microarray data. We begin by introducing some notation. If X has distribution P X , and f is a function of x , its L ( P X ) norm is denoted by  X  f  X  2 = fine  X  v  X  2 n = 1 n  X  f that have an additive form, i.e., f ( x 1 , . . . , x p ) =  X  + 2.1 Multi-task/Multi-response Sparse Additive Models that n 1 = . . . = n K = n . We also assume different tasks are comparable and each Y ( k ) and X j has been standardized, i.e., has mean zero and variance one. This is not really a restriction of the model since a straightforward weighting scheme can be adopted to extend our approach to handle noncomparable tasks. We assume the true model is E  X  Q across different function components, we define the regularization functional K ( f ) by The regularization functional K ( f ) naturally combines the idea of the sum of sup-norms penalty sparsity in nonparametric multi-task inference. as a penalized M-estimator, by framing the following optimization problem The multi-response sparse additive model (MR-SpAM) has exactly the same formulation as in (3) except that a common design matrix is used across the K different tasks. 2.2 Sparse Multi-Category Additive Logistic Regression ( x i 1 , . . . , x ip ) T is a p -dimensional predictor vector and y i = ( y dimensional response vector in which at most one element can be one, with all the others being The multi-category additive logistic regression model is entire vectors f j = ( f (1) j , . . . , f ( K  X  1) j ) .
 Denoting as the multinomial log-loss, the sparse multi-category additive logistic regression estimator (SMALR) is thus formulated as the solution to the optimization problem where f ( k ) j  X  H ( k ) j for j = 1 , . . . , p and k = 1 , . . . , K  X  1 . these conditional expectations.
 For the j th block of component functions f (1) j , . . . , f ( K ) j , let R ( k ) j = Y ( k )  X  problem is reduced to The following result characterizes the solution to (6).
 Theorem 1. Let P ( k ) j = E s where m  X  = arg max m 1 m tion (7), which we shall denote as First, we formulate an optimality condition in terms of the G  X  ateaux derivative as follows. Here the former one denotes the subdifferential of the convex functional  X   X   X   X  evaluated at (  X  and Wets (1998) is used to characterize the subdifferential of sup-norms.
 Lemma 3. The subdifferential of  X   X   X   X  on R K is the k -th canonical unit vector in R K .
 Using Lemma 2 and Lemma 3, the proof of Theorem 1 proceeds by considering three cases for the there exists a unique k , such that  X  f ( k ) j  X  = max k  X  =1 ;:::;K  X  f ( k k  X  (2) are relatively straightforward, but for case (3) we prove the following.
 The proof of Theorem 1 then follows from the above lemmas and some calculus. Based on this sparse backfitting algorithm for multi-task and multi-response sparse additive models (MT-SpAM and MR-SpAM) is shown in Figure 2. The algorithm for the multi-response case (MR-SpAM) has j = . . . = S M Figure 2: The simultaneous sparse backfitting algorithm for MT-SpAM or MR-SpAM. For the multi-response case, the same smoothing matrices are used for each k . 3.1 Penalized Local Scoring Algorithm for SMALR We now derive a penalized local scoring algorithm for sparse multi-category additive logistic re-gression (SMALR), which can be viewed as a variant of Newton X  X  method in function space. At approximate quadratic problem in each iteration is weighted by a non-diagonal matrix in function to lower bound the log-loss, as in (Krishnapuram et al., 2005).
 second-order Lagrange form Taylor expansion to L ( f ) at b f is then 1 |
X ) , . . . , p b f ( Y ( K  X  1) = 1 | X )) T , and the Hessian is H ( e f ) =  X  diag positive-definite. Therefore, we have that The following lemma results from straightforward calculation.
 Lemma 5. The solution f that maximizes the righthand side of (10) is equivalent to the solution that minimizes 1 2 E auxiliary functional algorithm, in the finite sample case, is shown in Figure 3. neous sparse backfitting algorithm. We then apply SMALR to a tumor classification and biomarker tended to the multi-task setting: GCV(  X  ) = where df(  X  ) = of freedom for the univariate local linear smoother on the j th variable. 4.1 Synthetic Data We generated n = 100 observations from a 10-dimensional three-task additive model with four relevant variables: y ( k ) i = X j = ( W parameter  X  = 0 . 25 are summarized in Figure 4. The upper 12 figures show the 12 relevant com-(from dimension 5 to dimension 10), both the true and estimated components are zero. The middle imum empirical L 1 norm of the component functions for each variable, with the red vertical line separation between the relevant dimensions and the irrelevant dimensions becomes smaller. Using the same setup but with one common design matrix, we also compare the quantitative performance of MR-SpAM with MARS (Friedman, 1991), which is a popular method for multi-response additive identified and the mean squared error with the standard deviation in the parentheses. (The MARS 4.2 Gene Microarray Data for small round blue cell tumors (SRBCT) (Khan et al., 2001). The data consist of expression (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL), and the Ewing family of tumors (EWS). The dataset includes a training set of size 63 and a test set of size 20. These data have been analyzed by different groups. The main purpose is to identify important biomarkers, which centroids. Zhang et al. (2008) identify 53 genes using the sup-norm support vector machine. In our experiment, SMALR achieves 100% prediction accuracy on the test data with only 20 genes, which is a much smaller set of predictors than identified in the previous approaches. We follow the same procedure as in (Zhang et al., 2008), and use a very simple screening step based on the marginal correlation to first reduce the number of genes to 500. The SMALR model is then trained image id. The patients are grouped into four categories according to the corresponding tumors, 20 identified genes from our method are among the 43 genes selected using the shrunken centroids approach of Tibshirani et al. (2002). 16 of them are are among the 96 genes selected by neural network approach of Khan et al. (2001). This non-overlap may suggest some further investigation. We have presented new approaches to fitting sparse nonparametric multi-task regression models and theoretical results will be reported elsewhere. This research was supported in part by NSF grant CCF-0625879.

