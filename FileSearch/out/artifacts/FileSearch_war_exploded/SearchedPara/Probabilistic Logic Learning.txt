 The past few years have witnessed an significant interest in probabilistic logic learning , i.e. in research lying at the in-tersection of probabilistic reasoning, logical representations, and machine learning. A rich variety of different formalisms and learning techniques have been developed. This paper provides an introductory survey and overview of the state-of-the-art in probabilistic logic learning through the identi-fication of a number of important probabilistic, logical and learning concepts.
 data mining, machine learning, inductive logic program-ming, multi-relational data mining, uncertainty, probabilis-tic reasoning One of the central open questions in data mining and ar-tificial intelligence, concerns probabilistic logic learning , i.e. the integration of relational or logical representations, prob-abilistic reasoning mechanisms with machine learning and data mining principles. In the past few years, this ques-tion has received a lot of attention and various different ap-proaches have been developed, cf. [82; 41; 88; 66; 73; 45; 60; 61; 57; 81; 52; 77; 2; 55; 87]. This paper provides an intro-ductory survey and overview of those developments that lie at the intersection of logical (or relational) representations, probabilistic reasoning and learning, cf. Figure 1. While do-ing so, we identify some of the key concepts and techniques underlying probabilistic logic learning. This, in turn, we hope, should allow the reader to appreciate the differences and commonalities between the various approaches and at the same time provide insight into some of the remaining challenges in probabilistic logic learning.
 Before starting our survey, let us specify what we mean by probabilistic logic learning . The term probabilistic in our context refers to the use of probabilistic representations and reasoning mechanisms grounded in probability theory, such as Bayesian networks [78; 10; 47], hidden Markov models [85; 84; 22] and stochastic grammars [22; 64]. Such representa-tions have been successfully used across a wide range of ap-plications and have resulted in a number of robust models for reasoning about uncertainty. Application areas include ge-netics, computer vision, speech recognition and understand-Figure 1: Probabilistic Logic Learning as the intersection of Probability , Logic , and Learning . ing, diagnostic and troubleshooting, information retrieval, software debugging, data mining and user modelling. The term logic in the present overview refers to first order logical and relational representations such as those studied within the field of computational logic. The primary advan-tage of using first order logic or relational representations is that it allows one to elegantly represent complex situa-tions involving a variety of objects as well as relations be-tween the objects. Consider e.g. the problem of building a logical troubleshooting system of a set of computers. Com-puters are complex objects which are themselves composed of several complex objects. Furthermore, assume that even though the overall structure of the computers can be the same, their individual components can be highly different. Such situations can be elegantly modelled using relational or first order logic representations. E.g., we could use the relation ram ( r 1 , c 1 ) to specify that computer c 1 has a RAM component of type r 1 , and cpu ( p 2 , c 1 ) and cpu ( p note that c 1 is a multi-processor machine with two cpu X  X  p and p 3 . In addition to these base (or extensional) relations, one can also add virtual (or intensional) relations to describe generic knowledge about the domain. E.g., consider the rule which defines the concept of a multi-processor machine (as a machine that possesses two different cpu X  X ). Representing the same information employing a propositional logic would require one to specify for each machine a separate model. The term learning in the context of probabilistic logic refers to deriving the different aspects of the probabilistic logic on the basis of data. Typically, one distinguishes various learning algorithms on the basis of the given data (fully or partially observable variables) or on the aspect being learned (the parameters of the probabilistic representation or its log-ical structure). The motivation for learning is that it is of-ten easier to obtain data for a given application domain and learn the model than to build the model using traditional knowledge engineering techniques.
 So, probabilistic logic learning aims at combining its three underlying constituents: learning and probabilistic reason-ing within first order logic representations. The recent inter-est in probabilistic logic learning may be explained by the growing body of work that has addressed pairwise intersec-tions of these domains: -Indeed, various techniques for probabilistic learning such as gradient-based methods, the family of EM algorithms or Markov Chain Monte Carlo methods have been devel-oped and exhaustively investigated in different commu-nities, such as in the Uncertainty in AI community for
Bayesian networks and in the Computational Linguistic community for Hidden Markov Models. These techniques are not only theoretically sound, they have also resulted in entirely new technologies for, and revolutionary novel products in computer vision, speech recognition, medical diagnostics, troubleshooting systems, etc. Overviews of and introductions to probabilistic learning can be found in [20; 84; 44; 8; 48; 22; 64]. -Inductive Logic Programming and multi-relational data mining studied logic learning , i.e. learning and data min-ing within first order logical or relational representations.
Inductive Logic Programming has significantly broadened the application domain of data mining especially in bio-and chemo-informatics (cf. the other papers in this vol-ume) and now represent some of the best-known exam-ples of scientific discovery by AI systems in the literature.
Overviews of inductive logic learning and multi-relational data mining can be found in this volume and in [69; 23]. -Early work on incorporating probabilities into logic pro-grams was done by Clark and McCabe [9] and by
Shapiro [97]. Subsequently, researchers such as Nils-son [75], Halpern [43], Ng and Subrahmanian [71], and
Poole [82] have studied probabilistic logics from a knowl-edge representational perspective. The aim of this re-search is more a probabilistic characterization of logic than suitable representations for learning.
 Finally, in the past few years there have been some impor-tant attempts that address the full problem of probabilistic logic learning [59; 30; 67; 31; 14; 53; 51; 92; 68]. The goal of this paper is to provide an introduction and overview to these works. In doing so, we restrict our atten-tion to those developments that address the intersection of probabilistic logic learning, cf. Figure 1. Consequently, we will not discuss interesting work lying on boundaries such as Flach and Lachiche X  X  work on Na  X  X ve Bayes for structured terms [27; 62] and Craven and Slattery X  X  work on combining Na  X  X ve Bayes with a relational rule learner [11]. Giving an overview of the pairwise intersections (let alone the under-lying domains) would lead us too far (as it would require a book instead of a paper) and is therefore beyond the scope of this paper. Because probabilistic logic learning involves three domains, one can approach this topic from various directions. In this paper, motivated by the topic of this special issue, we start from a multi-relational and induc-tive logic programming perspective. The terminology and notation employed throughout this paper is based on com-putational logic rather than on relational databases because the relational model is not expressive enough to model the logical component of many of the probabilistic logical rep-resentations (e.g., [88; 66; 90; 52]). Nevertheless, we have attempted to be self-contained and to reduce the required logical machinery and background as much as possible. The paper is organized as follows: Sections 2 and 3 introduce the key concepts underlying computational logic and prob-abilistic reasoning formalisms, respectively; Section 4 then studies first-order probabilistic logics, i.e., how probabilistic reasoning and computational logic can be combined; after-wards, Section 5 then surveys various approaches to learning within those probabilistic logics, and, finally, in Section 6, we conclude. As logic programs will be used throughout this paper as the underlying knowledge representation framework, we now briefly introduce their key concepts. Afterwards, we show how state-of-the-art probabilistic representations em-ploy these concepts. The choice of logic programs as the underlying represen-tation is motivated by its expressive power and general-ity. Indeed, logic programs allow us to represent relational databases, grammars and also programs. This will turn out to be useful when discussing commonalities and differences in the probabilistic relational or logical representations later on. In addition, logic programs are well-understood, have a clear semantics and are wide-spread.
 Three example logic programs are given in Figures 2, 3 and 4.b. The first one specifies some regularities in the alarm program (inspired on Judea Pearl X  X  famous example for Bayesian networks). The genetic program is a simple model of Mendel X  X  law applied to the color of plants. Finally, the grammar example encodes a context-free grammar. The corresponding type of logic program is known under the term  X  X efinite clause grammar X .
 Let us now introduce some terminology. The predicates in the above examples include ber of arguments) has been listed explicitly. Predicates with arity 0, such as alarm / 0 are sometimes called propositions . variables are also terms . In addition, there exist structured terms such as [ barks ], shorthand for [ ]( barks , nil ), which contains the list functor [ ] / 2 , and the terms barks and nil (the empty list). Constants are often considered as functors of arity 0. Atoms are predicate symbols followed by the necessary number of terms, e.g., v ([ barks | T ] , T ) and pt ( P , white ) . We are now able to define the key concept of a definite clause . Definite clauses are formulas of the form A :  X  B 1 , . . . , B m where A and the B i are atoms and where all variables are understood to be universally quantified. E.g., Figure 4: Context-free grammars: ( a ) A context-free gram-mar and ( b ) the corresponding grammar program. read as  X  the phenotype of P is green if the m-chromosome of P contains the gene g and the p-chromosome of P contains the gene g . Because we will restrict attention to definite clauses only in the present paper, we will omit the term  X  X efinite X  as long as no ambiguities arise. We call body . Clauses with an empty body, such as v ([ barks | T ] , T ) are called facts . A clause program (or logic program for short) consists of a set of clauses. For a more thorough introduction to logic programming we refer to [63; 26]. Within logic, two types of semantics can be distinguished: a model-theoretic and a proof-theoretic one. Because this distinction will also be useful from a probabilistic point of view, let us briefly introduce these two concepts. Consider the alarm program in Figure 2. In this exam-ple, there are five propositions that occur, i.e. { alarm , earthquake , marycalls , johncalls , burglary } . This is the so-called Herbrand base HB . It consists of all facts that one can construct with the predicate, constant and function symbols in the program. The Herbrand base  X  in a sense  X  specifies the set of all possible worlds described by the program. Indeed, for the alarm program, there are 2 5 = 32 possible assignments of truth-values to the Herbrand base. These assignments are sometimes called ( Herbrand ) inter-pretations and they each describe a world.
 From a model-theoretic point of view, a logic program re-stricts the set of possible worlds. Indeed, if we interpret the clause alarm :  X  burglary , earthquake as an implica-tion, then the interpretations in which both burglary and earthquake are true, but alarm is not, do not satisfy this clause. The idea is that if we accept the clause as an axiom, then such worlds are impossible. More formally, we have that a (Herbrand) interpretation I satisfies a clause c if and only if for all substitutions  X  such that body ( c )  X   X  I also head ( c )  X   X  I . The interpretation I is called a model of c if it satisfies c . A substitution  X  = { V 1 /t 1 , . . . , V n {
P / 1mm } , is an assignment of terms t i to variables V i ing a substitution  X  to a term, atom or clause e yields the in-stantiated term, atom, or clause e X  where all occurrences of the variables V i are simultaneously replaced by the term t and the substitution  X  1 . Applying  X  1 to c yields the instan-Furthermore, the reader may want to verify that the inter-c . Finally, let us remark that an interpretation satisfies a set of clauses if it satisfies all the clauses in the set. The most important point about model theory  X  for our purposes  X  is that it specifies which worlds (i.e., interpre-tations) are possible with respect to (i.e., satisfy) a given logical theory. Another way of looking at logic programs comes from proof theory. From this perspective, a logic program can be used to prove that certain atoms, goals (see below) or clauses are logically entailed by the program. Consider, e.g., the genetic program in Figure 3. In this program, the fact pt ( P , C ) will be provable for any P and C whenever C is one of the possible colors for the plant P according to the axioms encoded in the program.
 Proofs are typically constructed using SLD-resolution pro-cedure, which we now briefly introduce. More formally, given a goal :  X  G 1 , ..., G n and a clause G :  X  L 1 , ..., L that G 1  X  = G  X  , applying resolution results in the goal tion of a goal :  X  G is then a sequence of resolution steps that leads to the empty goal :  X  . A failure does not end in the empty goal. For instance, in the genetic example, one can prove that pt ( 1f , green ) is true, because of the following resolution derivation Resolution is employed by many theorem provers (such as Prolog). Indeed, when given the goal :  X  pt ( 1f , green ), Pro-log would compute the above resolution derivation and an-swer yes . Furthermore, when given the goal :  X  pt ( 1f , X ), Prolog would answer with the substitutions that make the goal true, i.e., { X / green } . The set of possible resolution steps for a given goal is captured in an SLD-tree. Two ex-amples, one in the genetic domain, and one in the grammar domain are shown in Figure 5. At this point it is impor-tant to see the connection between an SLD-derivation for the grammar domain and a traditional derivation using the context free grammar. Indeed, there is a one to one mapping between these two concepts for our grammar. Figure 5: Examples of SLD trees. Root nodes are shaded. The upper tree is the SLD tree of querying the grammar program with :  X  s ([ the , dog , barks ] , []). The lower tree is the SLD tree of querying the genetic program with :  X  pt ( 1mm , red ). In both cases, there is only one refutation, i.e. successful derivation.
 There are thus two ways of viewing a logic program. The first view, illustrated by the alarm program, is the model-theoretic view. It determines what the possible worlds (i.e. interpretations) are that satisfy the program. The sec-ond view, illustrated by the genetic program, is the proof-theoretic view. It determines what the possible proofs are with respect to a given goal. Of course, these two views are intimately connected. Indeed, for clause programs, we have that a (ground) fact has a resolution refutation if and only if it belongs to all (Herbrand) models of the program. The model theoretic and proof theoretic views of logic are also useful from a probabilistic perspective. Indeed, many of the present probabilistic representations can be regarded as defining probabilities on possible worlds (e.g., Bayesian networks) or on proofs (e.g., stochastic context free gram-mars). As these probabilistic representations form the ba-sis for the probabilistic logics, we will now provide a short overview to these. We will use P to denote a probabil-ity distribution, e.g. P ( X ), and P to denote a probability value, e.g. P ( X = x ). Figure 6: The graphical structure of the alarm Bayesian network. The most popular formalism for defining probabilities on possible worlds is that of Bayesian networks. As an ex-ample of a Bayesian network, consider Judea Pearl X  X  fa-mous alarm network graphically illustrated in Figure 6. Formally speaking, a Bayesian network [78; 10; 47] is an augmented, directed acyclic graph, where each node corre-sponds to a random variable X i (e.g., { alarm , earthquake , marycalls , johncalls , burglary } ) and each edge indicates a direct influence among the random variables. We will denote the set of parents of a node X i by Pa ( X i ), e.g., Pa ( alarm ) = { earthquake , burglary } . A Bayesian network specifies a joint probability distribution P ( X 1 , . . . , X a fixed, finite set { X 1 , . . . , X n } of random variables. As we will  X  for simplicity  X  assume that the random variables are all boolean, i.e., they can have the domain { true, false } this actually amounts to specifying a probability distribu-tion on the set of all possible interpretations. Indeed, in our alarm example, the Bayesian network defines a probability distribution over truth-assignments to { alarm , earthquake , marycalls , johncalls , burglary } .
 A Bayesian network stipulates the following conditional in-dependency assumption: For example, marycalls is conditionally independent of earthquake and of burglary given a joint state of alarm . Be-cause of the conditional independence assumption, we can write down the joint probability density as by applying the independency assumption to the chain rule expression of the joint probability distribution. There-fore, we associate with each node X i of the graph the conditional probability distribution P ( X i | Pa ( X denoted as cpd ( X i ). The conditional probability distributions in our alarm examples would include P ( alarm | earthquake , burglary ), P ( earthquake ), etc. They are illustrated in Table 1.
 So, Bayesian networks define a probability distribution on the possible worlds, i.e., the interpretations. Furthermore, the resulting probabilities induce a probability distribution at the level of propositions and propositional formulae. In-deed, reconsider the alarm example, and assume that we
In general, the domains of a random variable can be con-tinuous or discrete as well, cf. [25].
 Table 1: The conditional probability distributions associ-ated with the nodes in the alarm network, cf. Figure 6. The distributions are specified over { true , false } . When viewing the child-parents pairs as clauses, the clauses in the alarm program, cf. Figure 2, specify some regularities of the alarm network. The corresponding conditional probability distri-butions can be associated with the clauses. are interested in the probability of P ( alarm = true ). This probability can be obtained by marginalizing over the other variables, i.e.,
P ( a = t ) = X where we have abbreviated the variables and states appro-priately. The probability of a propositional formula can be obtained similarly, it is simply the sum of the probabilities of the interpretations that are a model for the formula. The key limitation of Bayesian networks is that they lack the notion of an object. The possible worlds considered are propositional . Using Bayesian networks, it is impossible to model relations between objects, i.e., relational or logical worlds. This problem is similar to that with traditional propositional mining and learning systems. Because of the closeness of proofs of logical formulae and derivations in grammar, we will start by discussing stochastic grammars, see e.g. [1; 64]. Consider therefore the stochastic context free grammar in Figure 7 (adapted from [1]), where we have omitted many of the terminal sym-bols for brevity.
 This context-free grammar is stochastic in that every gram-mar rule has a probability associated to it. Moreover, for every non-terminal symbol T the sum of the probabilities of the rules with T on the left hand side equals 1 . 0, e.g., for NP : 0 . 2 + 0 . 8 = 1 . 0. Stochastic context-free grammars de-fine probabilities on derivations (or, when mapped onto logic programs, on proofs). Indeed, one of the proofs (with asso-ciated probabilities) of the sentence  X  Rice flies like sand  X  goes as follows: Figure 7: An example of a stochastic context-free gram-mar. The numbers associated with the grammar rules de-note probability values. Note that [ Rice ] does not denote a variable but the string Rice . Each time a rule is applied in a proof, the probability of the rule is multiplied with the overall probability. It is easy to verify that this induces a probability distribution over all possible proofs or successful derivations for the same non-terminal symbols. Failures (if any) are only due to non-matching terminals which can be considered to have 0 . 0 as probability label. Furthermore, the probabilities over the proofs induce a probability over the accepted sentences for each of the non-terminals. Indeed, consider again the sen-tence  X  Rice flies like sand  X  for which there are two proofs, one with probability p 1 (as illustrated above) and another one with probability p 2 . The sum of these probabilities spec-ifies the probability that a random sentence generated by the non-terminal symbol S is indeed  X  Rice flies like sand  X . These distributions are useful for various purposes. For in-stance, in natural language processing, one may be inter-ested in either the most likely parse tree (or derivation) when dealing with ambiguity, or the total probability that a par-ticular sentence is derived. Also, in bioinformatics, there are numerous applications of stochastic grammars, most no-tably of (hidden) Markov models [84], which are essentially stochastic regular grammars. These are e.g. used to deter-mine how likely it is that a given protein belongs to some fold [22].
 From a computational and logical perspective, the key lim-itation of stochastic context free grammars concerns their expressive power. Indeed, context free grammars (and therefore stochastic context free grammars) are not Turing-equivalent, i.e. they cannot be used as a programming lan-guage. Thus it is useful to lift the underlying grammar rep-resentation to that of logic programs, a Turing equivalent language, cf. [66]. By now, everything is in place to introduce the key repre-sentational frameworks that combine probabilistic reasoning with logical or relational representations. Furthermore, in the spirit of the above presentation, we can distinguish them according to whether they define probabilities on interpre-tations or on proofs. The first class of representations extends Bayesian networks with abilities to define probabilities on first order logical or relational interpretations, cf. [82; 41; 73; 45; 57; 81; 31; 52]. Important predecessors of these logical Bayesian networks discussed below, include the work by Breese, Charniak, Bacchus, Goldman, Poole and Wellmann [40; 6; 4; 7; 39]. These predecessors were largely based on the idea of knowl-edge based model construction (KBMC). In knowledge based model construction, a knowledge base (sometimes in the form of an annotated logic program) is used to describe sets of probabilistic models. A query then results in construct-ing a particular model that can be used to answer the query. Many of these approaches can represent Bayesian networks. However, they are not direct upgrades of Bayesian networks in the same way that the work by e.g. Haddawy [41] is. This may also explain why  X  to the authors X  knowledge  X  Bayesian network learning techniques have not been applied to these representations.
 A central contribution in combining clausal logic with Bayesian networks is due to Peter Haddawy [41]. It is based on the observation that the structure of a Bayesian network can essentially be represented as a set of (propositional) clauses, where there will be one clause for each node in the graph, cf. the alarm program and Bayesian network. This observation allows one to upgrade Bayesian networks toward first order logic by upgrading the corresponding proposi-tional clauses towards definite clauses. These clauses will specify the so-called intensional part I , i.e. the overall reg-ularities. In addition, we will need an extensional part E , which specifies the specific objects and situations under con-sideration. The distinction between extensional and inten-sional is akin to that made in database theory and compu-tational logic. E.g., in the genetic program, the extension corresponds to the facts and the intension to the non-fact clauses. Given an extensional part E and an intensional part I , the central ideas are then -to view the atoms in the Herbrand interpretation as ran-dom variables 2 ; they will constitute the nodes in the in-duced Bayesian network, -to view the instantiated clauses H :  X  B 1 , . . . , B m the probabilistic dependencies, i.e. the B i will correspond to parents of the node H ; therefore, a conditional probabil-ity distribution cpd needs to be associated to each clause. Before continuing to discuss some problems and extensions, let us first illustrate these ideas on our genetic example. In doing, so let us slightly redefine the predicates pc / 2 , mc / 2
More specifically, it only makes sense to talk about the atoms that are logically entailed by the program, i.e. that are true in all possible Herbrand interpretations, that is the atoms belonging to the so-called least Herbrand model. Figure 9: The structure of the Bayesian network induced by the truncated genetic program, see Figure 8. All nodes nodes (with a non-empty set of parents) for the same pred-icate share the same associated cpds. and pt / 2 . We will now turn these two-place predicates into predicates of arity 1, the idea being that pt ( P ) will be (in state) true if and only if the phenotype of the person P is green. The other predicates are dealt with in a similar way. Together with the facts for mother / 2 and father / 2 (listed in Figure 3), the truncated facts for mc / 1 and pc / 1 result in the truncated genetic program as shown in Figure 8. This program then induces a Bayesian network with the graphical structure shown in Figure 9. All nodes (with a non-empty set of parents) for the same predicate (i.e., mc / 1 , pc / 1 and pt / 1 ) share the same local probability model. So, the cpd X  X  for mc ( 1m ) and mc ( 1mm ) are the same.
 There are some problems with this approach. Let us dicuss the most notable ones. First, the Bayesian network obtained is required to be acyclic. This is required because the se-mantics of cyclic Bayesian networks are not well-defined. Therefore, virtually all approaches require acyclicity or en-force other restrictions that guarantee acyclicity. Second, the domains of the random variables may not be boolean, i.e., they may have more than two possible val-ues, or even be continuous. There are two ways to realize this. First, one could simply allow the domains of the ran-dom variables to be arbitrary, cf. [41; 52]. One side-effect is that boolean or logical random variables get mixed up with other ones. This may sometimes cause some confusion. Sec-ond, one could simply add an extra argument to the atoms, cf. [57; 73]. E.g., in the genetic domain, this would corre-spond to the encoding used in Figure 3. The disadvantage of this approach is that one needs to add extra constraints that would state that the phenotype must be either red or white, cf. [73]. This in turn results in a more complicated inference engine.
 Finally, it might be the case that there exist multiple ground clauses with the same head. Indeed, consider the program consisting of the following two clauses: The first clause states that Pa ( p ) = { q , r } , the second one that Pa ( p ) = { s , t } . This problem can also arise with a program consisting of a single clause. Indeed, consider p ( X ) :  X  q ( X , Y ), for which there may be multiple (true) in-stantiations of q ( X , Y ) for a single X . Various solutions which might even be combined have been developed for dealing with this situation. First, one could introduce so-called combination rules , cf. [73; 45; 52]. Combination rules are functions that would  X  in the above case  X  take the cor-responding conditional probability distributions P ( p | q , r ) and P ( p | s , t ) as inputs and would produce the desired P ( p | q , r , s , t ) as output. Classical examples of combining rules are noisy-or and noisy-and . For illustration purposes, consider the clauses together with noisy-or as combining rule. The Bayesian network induced is where noisy or ((cpd( fever :  X  flu ) , cpd( fever :  X  cold ) , cpd( fever :  X  malaria )) is associated with fever , cf. [86, page 444]. Then fever is false only if all parents are inde-pendently false. The probability of fever being true then depends on the parents which are true. More precisely, Second, one could employ an aggregate function as done in probabilistic relational models (PRMs) [57; 81]. Aggregate functions are well-known from databases; examples include min, max, sum, average, count, etc. To illustrate their use in the present context, reconsider the clause p ( X ) :  X  q ( X , Y ) and consider that for { X = a } there are multiple substitu-tions, let us say { Y = b , Y = c } , such that q ( a , Y ) holds. Then { q ( a , b ) , q ( a , c ) } are the two random variables that influence p ( a ). Furthermore, these random variables will have values, say v 1 and v 2 . An aggregate function  X  could then map this set of values { v 1 , v 2 } onto a single value v . Let us use the notation  X  ( q ( a , Y )) to denote this value v . The random variable p ( a ) thus depends on  X  ( q ( a , Y )), which is of course also a random variable. If multiple atoms appeared in the would be aggregated independently. The latter clause thus represents the probabilistic dependence of p ( X ) on  X  1 and  X  2 ( r ( X , Z )), i.e., the cpd associated with the clauses ac-tually specifies for each value in  X  1 ( q ( X , Y ))  X   X  distribution for p ( X ). The effect of aggregate functions is that they reduce the information in a natural way. This is advantageous because it reduces the size of the conditional probability distributions in a natural way, but  X  on the other hand  X  it may also result in a loss of information [46]. Above we introduced the principles underlying logical ex-tensions of Bayesian networks. Let us now briefly survey some of the key representational frameworks that have been developed around these principles.
 As mentioned before, the first approach to directly upgrade Bayesian networks to relational representations, seems due to Peter Haddawy [41]. In his initial approach, the random variables correspond to ground atoms. Furthermore, the need for combining rules was alleviated by requiring that for each random variable, there was at most one instantiation of a clause having the random variable as the head. In later work, Haddawy now together with Ngo [73], used combination rules to deal with the latter problem, and also, the values of the random variables were now written as an extra argument of the atoms, which necessitates the need to add constraints, cf. above. The language, called probabilistic-logic programs (PLPs), was further extended in the knowledge based model construction tradition by dis-tinguishing between logical and probabilistic conditions in clauses. The former then specified the (logical) conditions under which the probabilistic dependencies held. This in-creased the expressive power and was certainly useful for specifying probabilistic models by hand, but on the other hand, it also significantly extended and therefore compli-cated the description language. These complications may also explain why  X  to the authors X  knowledge  X  no (struc-tural) learning results are known for this formalism. The framework has been applied within medical domains [42; 72].
 Building on Ngo and Haddawy X  X  work, Kersting and De Raedt [52] introduced the framework of Bayesian logic programs (BLPs). This framework can be viewed as a sim-plification of that by Ngo and Haddawy in that BLPs employ a minimal set of primitives such that BLPs have both def-inite clause logic (i.e. pure Prolog) and Bayesian networks as a direct special case. This in turn facilitates the learn-ing, cf. [53; 51; 54]. Bayesian logic programs again view the atoms as the random variables. More precisely, the atoms of the (least) Herbrand model constitute the set of random variables of the induced Bayesian network. BLPs deal with continuous random variables and employ a simpler form of combination rule.
 Finally, there is the quite popular formalism of probabilistic relational models 3 (PRMs) [57; 81; 31; 33]. This formalism
Several other extensions of Bayesian network have been developed [60; 58; 61; 80; 81] by Pfeffer et al. before in-troducing probabilistic relational models. These extensions have been termed frame-based and object-oriented, in which the relational and logical issues are less direct. Therefore, we base our discussion of probabilistic relational models on does not use definite clause logic (i.e. pure Prolog) as the un-derlying representational framework, but rather the entity-relationship model. In PRMs, the idea is that the informa-tion about one entity type is stored in one relation. E.g., in the genetic illustration, this could be realized using the relation person ( Person , MC , PC , PT ). So each ground atom can store information about multiple attributes and the de-pendencies are defined at the level of these attributes. Two types of dependencies are allowed: direct dependencies from other attributes (for the same entity, e.g., the blood type of a person could depend on the two chromosomes of that per-son), or dependencies via so-called slot chains . Slot-chains are binary relations (or binary projections of other relations) that relate the attributes of one entity to that of other ones. E.g., in the genetic example, the MC attribute of one person would depend via the slot-chain mother ( Person , Mother ) on the attributes MC and PC of the entity corresponding to the Mother . Furthermore, in the basic PRM setting the relations among entities (such as Mother or Father in the genetic ex-ample) are assumed to be deterministic and given, cf. [33]. This implies that it would be impossible to specify that the probability is 0 . 70 that Jef is the father of Mary. Never-theless, some partial solutions for dealing with probabilistic relations have been developed, cf. [34; 31]. The genetic ex-ample shows that  X  at the logical level  X  PRMs essentially define via single clauses probabilistic dependencies between the attributes of various entities. It also shows that proba-bilistic dependencies between relations are harder to repre-sent and require special treatment. Within the more logical approaches sketched above, these issues are dealt with in a uniform manner because random variables are represented by ground atoms. Indeed, the more logically oriented ver-sion of the genetic example, introduced above, shows how the person / 4 relation in PRMs would be split-up (or  X  X or-malized X ) into atoms that directly correspond to the random variables. On the other hand, the advantages of PRMs are that they (1) possess an elegant graphical representation, see Figure 10, (2) may be more efficient to implement and learn, and (3) can test the acyclicity requirement more eas-ily. Other differences with the above mentioned logical no-tation include the use of aggregate functions to deal with the multiple instantiations problem and the use of a purely re-lational language, which does not allow for functors (needed for representing infinite structures), but see [79]. PRMs have been successfully applied to various problems such as rela-tional clustering [98], hypertext classification [37], selectiv-ity estimation within databases [38], and modelling identity uncertainty [76], and led to impressive results within com-putational biology [95; 93; 94].
 Recently, Getoor et al. [31; 35; 32] have introduced stochas-tic relational models (SRMs). At the syntactic level SRMs are almost identical to PRMs but SRMs possess a different semantics, which allow PRMs to answer statistical queries such as  X  X hat is the probability that a randomly chosen patient is elderly and has a contact who is a coworker? X  The key idea underlying SRMs is that probability distribu-tions are defined and used at an abstract level determined by the database schema. At this level, one is not inter-ested in specific entities (such as specific patients, contacts or co-workers) but rather of their properties (such as the probability that a randomly chosen patient is elderly, or a the presentation by Getoor et al. in [23].
 Figure 10: Graphical representation of a probabilistic rela-tional model of the genetic domain. randomly chosen contact is a coworker). Whereas PRMs define dependencies among different entities via slot-chains, SRMs define them via chains of joins. Many of the other issues SRMs are quite similar to those in PRMs, which ex-plains why we will not provide any further details on SRMs in this paper. For more information, see [31; 35; 32], who present efficient algorithms to construct the desired Bayesian network, to estimate (in particular the join) parameters and to learn the dependency structure from a given database. SRMs have also been used for selectivity estimation within databases [31]. To define probabilities on proofs, there are at least two dif-ferent options. They correspond to the approaches taken in stochastic logic programs [24; 66; 12; 13; 15] and PRISMs respectively [88; 90; 50; 89].
 Let us start by introducing stochastic logic programs (SLPs). Stochastic logic programs are a direct upgrade of stochastic context free grammars. This is realized by asso-ciating to each clause a probability value. Furthermore, as in the case of stochastic context free grammars, the sum 4 the probabilities associated to clauses of the same predicate equals 1 . 0. To illustrate stochastic logic programs, recon-sider the derivation of  X  Rice flies like sand  X  but now with the definite clause grammar (with the same associated probabili-ties). Both give the same probabilities to the same (ground) queries. However, stochastic context free grammars con-stitute very simple stochastic logic programs. Failures are only due to non-matching terminals. In general SLPs, (in-tensional) clauses may cause already failures. Consider the genetic example in Figure 3 and assume that we have as-sociated uniform probabilities to all clauses for the same predicate. Under these assumption one could e.g. show that the probability of the single proof of :  X  mc ( 1m , r ) is 0 . 2  X  0 . 5  X  0 . 2  X  0 . 2 = 0 . 004. However, computing the proba-bilities of all other ground atoms over mc / 2 shows that they do not sum to 1 . 0 but to 0 . 624 (see below) because some of the refutations fail already at the clause level. Stochastic logic programs account for failures by normalization. The normalization constant of an atom over some predicate mc / 2 is the sum of probabilities of all refutations of the most gen-
More general definitions of stochastic logic programs where e.g. the probability values do not sum to 1 . 0 are discussed by Cussens [12; 14]. Because learning of stochastic logic programs is considered only for  X  X ormalized X  programs we restrict our attention to them. eral goal over mc ( X , Y ), i.e. the sum of the probabilities of Thus, the normalized probability of :  X  mc ( 1m , r ) is (2  X  0 . 004) / 0 . 624 = 0 . 012820513. More details on inference (including approximative inference) can be found in [13; 15]. Stochastic logic programs have been used to specify declar-ative priors for probabilistic models [3].
 Whereas stochastic logic programs attach probability labels to all clauses, PRISMs and the earlier representation by David Poole [82] attach probability labels to facts. Indeed, let a logic program consist of the set of facts E (this could be considered the extensional part) and the set of proper clauses I (the intension). The frameworks by Poole and Sato would then associate a probability label to each of the facts in E . E.g., in the genetic example (where we take the truncated encoding of Figure 8), one could associate a probability of 1 . 0 to the facts for mother and father and in addition there could be probabilities associated to the facts for mc/1 and pc/1 , e.g., A labelled fact p : f then has the meaning that there is a probability of p that the fact f is true. This induces a prob-ability at the level of proofs. Indeed, consider now the atom pt ( 1mm ). The proof of pt ( 1mm ) only succeeds when both mc ( 1mm ) and pc ( 1mm ) are true. These two facts are only true with a probability of 0 . 7  X  0 . 7 = 0 . 49. Therefore, the prob-ability that the proof of pt ( 1mm ) succeeds is 0 . 49. Because probability values are associated with facts, no normaliza-tion is needed as opposed to stochastic logic programs. At this point the reader has probably observed that the probability distribution for proofs induces a probability dis-tribution for the other atoms and even for the interpreta-tions. Indeed, because there is only one possible proof for pt ( 1mm ), the probability for this atom to be true is also 0 . 49. If there would be more proofs, then one would have to sum the probabilities. This in turn defines a probability distribution at the level of interpretations and so the for-malisms by Sato and Poole, can also be interpreted from a model-theoretic perspective. Sato has called this a distribu-tional semantics , cf. [88], stressing the declarative character of PRISMs. However, the key difference with the Bayesian network approaches presented above is that the former ap-proaches rely more on logical inference methods, whereas the latter rely more on probabilistic inference mechanisms. In a sense, the assumptions (on which the proofs rely) are probabilistic, whereas in Bayesian networks the inference mechanism is. This in turn explains the strong link with Figure 11: The graph structure of a Markov model for web navigation. abductive inference methods, cf. [82]. Finally, let us remark that we discussed simplified versions of Poole X  X  and Sato X  X  frameworks. Originally, they have been introduced to deal with situations where some of the facts are mutually exclu-sive. E.g., if three genes would be involved (say a , b and c ), then the corresponding facts for mc / 2 and pc / 2 would be mutually exclusive. This could be specified using logical constraints such as A more compact notation used by Sato and Poole is notes a probability value. The above introduced frameworks integrate probabilistic models with very expressive relational representations or even logic programs. This however comes at a computa-tional cost. Therefore, some recent approaches such as re-lational Markov models (RMMs) [2], hidden tree Markov models (HTMMs) [28; 21], and logical hidden Markov mod-els (LOHMMs) [55] have tried to upgrade some well un-derstood probabilistic representations, such as (Hidden) Markov Models [84]. These approaches can also be viewed as downgrading one of the more expressive probabilistic log-ics presented above. Whereas the resulting approaches are typically less expressive than the above introduced ones, the advantages are that (1) they are closely related to the un-derlying representations, (2) they are  X  in principle  X  more efficient, and (3) that the learning algorithms for the under-lying representations can almost directly be applied. Let us illustrate this idea on Markov models (where we fol-low the ideas underlying RMMs and LOHMMs using an example by Anderson et al. [2]). A Markov model is essen-tially a finite state automaton with probabilities associated to each of the edges instead of symbols from an alphabet. Alternatively, it can be regarded as stochastic regular gram-mar 5 . A (toy) example for web navigation (within a given academic site) is given in Figure 11. The model denotes the probabilities that if one is at a web page of type X the next web page will be of type Y . E.g., in the graph listed above, the probability of moving from department to lecturer is 0 . 3. It should be clear that this model is too simple to be useful for web user modelling because the abstraction level is too high. One alternative would be to build a model with
This is a special case of a stochastic context free grammar in which all rules have exactly one symbol on the right hand. one node for each of the web pages. This model would how-ever be so huge that it would hardly be usable (let alone learnable). Therefore, a better way, supported by relational or logical Markov models is to use proper relational or log-ical atoms to represent the state instead of propositional symbols. A sequence of possible states could then be e.g. dept ( cs )  X  course ( cs , dm )  X  lecturer ( cs , pedro ) course ( cs , stats )  X  . . .
 The sequence of pages represented would include the web page of the department of cs , the dm course, its lecturer pedro and the course stats taught by the same lecturer. This type of relational and logical sequence could be mod-elled using the following type of (grammar) rules Now, starting from any ground state, e.g., dept ( cs ), one can determine the possible transitions as well as their prob-abilities. However, the above logical Markov model is only specifying the probability of a transition to an abstract state (i.e., a state that is not ground -it contains variables); in our example, the abstract states are course ( cs , C ) and lecturer ( cs , L ). To be able to determine the corresponding real states, one also needs to know (1) the domains of the various arguments (in our examples, the set of all lecturers and the set of all courses, say dom( L ) and dom( C )), and (2) a probability distribution on these domains (i.e., P L and P
C ) that specifies the probability of selecting a particular instance from these domains, e.g. the probability of select-ing pedro as a lecturer would be given by P L ( pedro ). Given these domains and their corresponding distributions, one can now instantiate the abstract transitions. E.g., starting from dept ( cs ) there would be a probability of 0 . 7  X  P of going to lecturer ( cs , pedro ). This illustrates the key ideas underlying these representations: proof steps corre-spond to time steps. Nevertheless, various differences among these approaches exist: Further possible choices for these approaches exist. One of these is concerned with the level at which to spec-ify the domain probabilities. One can specify them lo-cally, i.e., for each  X  X redicate X  independently, or glob-ally. Secondly, how should one combine these prob-ability distributions to account for larger substitutions, e.g., X = cs , Y = pedro in lecturer ( X, Y )? Either, one could take a na  X  X ve approach by assuming that the domains are independent (similar to na  X  X ve Bayes), i.e., P ( X = cs , Y = pedro ) = P ( X = cs )  X  P ( Y = pedro ), or em-ploy more advanced models such as Bayesian networks or other probabilistic models. More experience and research is necessary in order to fully understand and appreciate the different options, opportunities and limitations of these re-cent intermediate approaches.
 Finally, let us remark that the intermediate representations presented in this section should be viewed as belonging to the proof-theoretic approach as probability distributions are defined at the level of derivations and (partial) proofs re-spectively. Learning denotes the process by which one adapts (proba-bilistic) models on the basis of data. When learning within probabilistic logics (as well as within the more traditional probabilistic representations), one typically distinguishes two tasks. The first task is concerned with the probabilistic part, i.e., the question where do the numbers come from ; it is the so-called parameter estimation problem. In this task, it is assumed that the structure (i.e., the underlying logic pro-gram) of the representation is fixed and known, and the pa-rameters (i.e., the probability labels) have to be estimated. In the second task, the so-called structure learning or model selection , one assumes that neither the logic program nor the parameters are fixed but have to be learned.
 Again, it will turn out to be useful to distinguish the model-based from the proof-based representations. It will turn out that  X  from the inductive logic programming perspective  X  the distinctions are akin to those between learning from en-tailment and learning from interpretations , cf. [17]. We will therefore start this section by discussing the types of data employed for learning. Because one also needs to estimate the parameters when learning the structure of a probabilis-tic model, we will introduce parameter estimation before structure learning. The model-based and the proof-based representations em-ploy different forms of data which we will now sketch. An example or data case used for learning in a Bayesian network setting consists of the random variables specified in the network and a possibly partial assignment of values to states 6 . To illustrate this, reconsider our earlier alarm
In this paper, we will assume that all random variables are example. A possible example for this network could be This example partially describes a particular situation that arose in alarm . Judea was called by Mary after an earth-quake took place. Judea neither knew whether the alarm went off nor whether John tried to call him. Therefore, when working with logical or relational upgrades of Bayesian networks, it is natural to work with examples in the form of interpretations. Indeed, the atoms in the interpretations specify the random variables, and their values specify the state they are in. E.g., in the genetic illustration, one could have the following example: { pc ( 1f ) = true , mc ( 1f ) = true , pc ( 1m ) = true , mc ( 1m ) = ? , Remark that  X  from a logical perspective  X  there are two constraints on this type of examples. First, the random variables specified by a single example can be considered a Herbrand interpretation, and this interpretation should be a model of the unknown target program. This is akin to the learning from interpretations setting from the field of inductive logic programming. Second, the Bayesian network structure induced by the target program for this particular example should be acyclic. These two properties will turn out to be important when learning the structure. To obtain insight into the nature of the examples for the proof theoretic setting, it is a good idea to look at the na-ture of the data when learning stochastic context free gram-mars. When learning stochastic context free grammars, the examples given are strings that should be accepted (with an unknown probability) by the target grammar. E.g., when learning a stochastic context free grammar in a natural lan-guage domain for English, the examples would be English sentences. Furthermore, these sentences should be derivable by the target grammar. When working with definite clause grammars (as in Figure 4 ( a )) sentences would be repre-sented by facts. E.g., the sentence for the non-terminal symbol S would be represented as s ([ the , dog , barks ] , []) . Thus, within the proof oriented setting of probabilistic log-ics, the evidence will correspond to facts (or even clauses). Furthermore, these examples e should be logically entailed by the target program T , i.e., T | = e . This is akin to the tra-ditional learning from entailment setting in inductive logic programming, cf. [69]. Intermediate representations, such as (hidden) Markov models, are typically learned from examples that cor-respond to sequences of states that arose in particu-lar situations. E.g., the Markov model for the web site domain, could be learned from examples such as department , lecturer , course , lecturer , . . . , or in the first order case by sequences such as the one specified in Sec-known, i.e., that no latent variables, whose states are never observed have to be introduced by the learning algorithm. tion 4.3: { dept ( cs )  X  course ( cs , dm )  X  lecturer ( cs , pedro ) Such sequences correspond to a kind of (partial) trace (or derivation) of the underlying proof system. Learning from traces has also been considered in the field of inductive logic programming, see [96; 5] for two well-known examples where traces have been used to induce logic programs. Traces im-pose a strong logical constraint on the underlying program. Indeed, for each two subsequent states s 1 and s 2 in a trace (for a logical Markov model), it is required that there exists a clause b  X  h and a substitution  X  such that s 1 = b X  and s = h X  . Parameter estimation methods start from a set of exam-ples E and a given logic program L of a probabilistic log-ical model, and aims at computing the values of the pa-rameters  X   X  that best explain the data. So,  X  is a set of parameters (i.e. the quantitative part of the model) and can be represented as a vector. To measure the extent to which a model fits the data, one usually em-ploys the likelihood of the data. The likelihood of the data is the probability of the observed data as a func-tion of the unknown parameters with respect to the cur-rent model, i.e. P ( E | L,  X  ). Thus, maximum likelihood es-timation (MLE) aims at finding  X   X  = argmax  X  P ( E | L,  X  ). As argmax  X  P ( E | L,  X  ) = argmax  X  log P ( E | L,  X  ), one often works with the log-likelihood log P ( E | L,  X  ), which is eas-ier to manipulate. Often, one assumes that the examples are independently sampled from identical distributions, i.e. log P ( E | L,  X  ) = P e  X  E log P ( e | L,  X  ). Variants or extensions of the MLE criterion presented are sometimes used as well. They include: a Bayesian approach which would take into account parameter priors or the minimum description length (MDL) principle that is often used for structure learning, cf. Section 5.3.
 To illustrate MLE, let us consider tossing a coin. The coin can either be heads or tails. Assume that the evidence is (respectively {{ head } , { tail } , { tail }} in the proof-based notation), and that our current quantitative part is P ( toss = head ) = p . Then We can maximize the log-likelihood by setting the partial derivative w.r.t. to p equal to 0 . 0: Solving this give p = 1 3 , which is the  X  X ntuitive X  estimate for p namely the proportion of heads which have been seen in the examples. More formally, where count(  X  ) denotes the frequency of an event. Thus, MLE reduces to frequency counting.
 However, in the presence of missing data, the maximum likelihood estimate typically cannot be written in closed form. It is a numerical optimization problem, and all known algorithms involve nonlinear optimization The most com-monly adapted technique for probabilistic logic learning is the Expectation-Maximization (EM) algorithm [20; 65] 7 . EM is based on the observation that learning would be easy (i.e., correspond to frequency counting), if the values of all the random variables were known. Therefore, it first esti-mates these values, then uses these to maximize the like-lihood, and then iterates. More specifically, EM assumes that the parameters have been initialized (e.g., at random) and then iteratively perform the following two steps until convergence 8 : -E-Step: On the basis of the observed data and the present parameters of the model, compute a distribution over all possible completions of each partially observed data case. -M-Step: Using each completion as a fully-observed data case weighted by its probability, compute the updated pa-rameter values using (weighted) frequency counting. The frequencies over the completions are called the expected counts . Let us illustrate EM using the evidence and assume that in the current model P ( toss = head ) = p . The completions of the data case then are The updated maximum likelihood estimation of the prob-ability that the coin will be head is then 1+ p 3 . Assuming which converges to 0 . 5.
 Let us now discuss how EM has been applied within the various probabilistic logics introduced earlier. For the sake of simplicity, we will only state the key ideas and will not address any advanced issues such as efficiency concerns. Parameter estimations for probabilistic-logic programs [59], probabilistic relational models [30; 31; 33], and Bayesian logic programs [51; 54] all follow the same principle: The given data and the current model induce a Bayesian net-work explaining each data cases. Then, the parameters of the induced Bayesian network are estimated using standard Bayesian network parameter estimation methods, such as those discussed in Heckerman X  X  tutorial [44]. For instance the examples in Section 5.1.1 together with the genetic program would induce the Bayesian network shown in Fig-ure 9. All nodes of the Bayesian network, which do not occur in the evidence but are introduced by the program,
Some approaches employ gradient-based approaches in-stead of EM, e.g., Muggleton [67; 68] for structural learning of SLPs, and Kersting and De Raedt [51; 54] developed both EM-based and gradient-based approaches for parameter es-timation of BLPs.
This is only the underlying idea of the EM. More for-mally, the E-step consists of computing the expectation of the likelihood given the old parameters and the observed data. Then, the M-step consists of maximizing the expected likelihood w.r.t. the parameters. are not observed, i.e. they are assigned a question mark. Thus the evidence corresponds to a partial joint state of the resulting Bayesian network. It is completed by the E-step, which relies on traditional Bayesian network inference and which is used to determine the distribution of the values for the unobserved states. The improved estimates of the pa-rameters in a node (i.e., the values in the cpds) are then computed in the M-step, by dividing the expected counts of corresponding node-parents and parents joint states. The only difference with standard Bayesian network parameter estimation is that parameters for different nodes in the net-work  X  those corresponding to different ground instances of the same clause  X  are forced to be identical. This technique is akin to that in recurrent neural network [99] or dynamic Bayesian networks [19]. It however requires a unique as-signment of parent nodes to clauses. Aggregate functions guarantee this for PRMs as the aggregated examples con-stitute the states of the nodes in the induced Bayesian net-work. Within PLPs and BLPs, uniqueness is enforced by using decomposable combining rules [59; 51]. The effect of a decomposable combining rule can be represented using extra nodes in the induced Bayesian network. Most combin-ing rules commonly employed in Bayesian networks such as noisy or , noisy and , or linear regression are decomposable. Whereas model-based approaches complete the data in terms of a Bayesian network, proof-theoretic approaches complete the data based on refutations and failures. For stochastic logic programs, Cussens [14] introduced the failure-adjusted maximization (FAM) algorithm. In FAM, the logical part of the SLP is fixed and given, and the pa-rameters have to be learned on the basis of the evidence. The evidence or the examples are atoms for a predicate p/m . They are assumed to be generated from the target SLP ac-cording to the probability distribution that it induces over p/m . This implies that the examples are already logically entailed by the SLP. The parameters are estimated by com-puting for each example the SLD tree and treating each path from root to leaf in the SLD tree as one possible completion. For instance the evidence pt ( 1mm , red ) yields five possible completions, namely the five paths of the lower SLD tree in Figure 5, of which four are failed and one constitutes a proof 9 . Secondly, the completions are weighted with the product of probabilities presently associated with clauses used in the corresponding derivations, cf. Section 4.2. Fi-nally, the improved estimates for each clause are obtained by dividing the clause X  X  expected counts by the sum of the expected counts of clauses for the same predicate. The EM algorithm for PRISM programs [50; 91; 49; 92] is similar in spirit. The main differences are that (1) no probability values are associated to (intensional) clauses, (2) failed derivations have a probability of zero, and (3) the disjoint representation introduced in Section 4.2 is assumed. Given an example, all explanations S of the example are computed. Similar to abductive reasoning, an explanation of an example is a set of facts S that al-lows the example to be proven. For instance, { pc ( 1mm , r ) } is the only explanation for pt ( 1mm , red ), i.e. the failure derivation in Figure 5 is neglected. Thus, the explana-
The approach is called failure-adjusted maximization be-cause both failed and successful derivations are taken into account. tions constitute the completions of the data, and they are weighted with their probability P ( S ) as specified by the cur-rent parameters. The maximum likelihood estimation of the probability value associated with a fact, say pc ( 1mm , r ), is then the fact X  X  expected count over all explanations where it is true divided by the expected counts of all corre-sponding mutually exclusive facts, e.g. given the declara-pc ( 1mm , g ) are mutually exclusive. To speed up computa-tions, Sato and Kameya employ well-known tabulation tech-niques from the field of computational logic.
 As already argued above, the intermediate representations, such as LOHMMs and RMMs can be viewed as belonging to the proof-theoretic approach. Therefore, we will omit a detailed discussion of parameter estimation for these meth-ods. In structure learning, one starts from a set of examples E , a language bias L that determines the set of possible hy-potheses, and aims at computing a hypothesis H  X   X  X  such that 1. H  X  logically covers the examples E , i.e., cover( H  X  2. the hypothesis H  X  is optimal w.r.t. some scoring func-The hypotheses H are of the form ( L,  X  ) where L is the logi-cal part L and  X  the vector of parameter values as in Section 5.2. The coverage criterion used will typically depend on the type of data and representation formalisms considered, cf. Section 5.1, where we have discussed three different types of data for our three types of representation formalisms. In addition, as mentioned in Section 5.2, various scoring func-tions can be employed as well. A simple example would be score( H, E ) = P ( E | H ) = P ( E | L,  X  ), i.e., the like-lihood criterion. The type of data, the coverage criterion, the language bias and the scoring function are the central components of any structure learning method. However, in addition, various enhancements or variants can be consid-ered as well. These include a possible initial hypothesis that could serve as a starting point for guiding the search and the incorporation of background knowledge. Background knowl-edge could consist of a set of given and fixed clauses (possibly with attached probability labels) that would provide useful information about the problem domain. The use of an ini-tial hypothesis is common in many probabilistic learning as well as in theory revision approaches [100] to inductive logic programming. Background knowledge is typically employed in multi-relational data mining and inductive logic program-ming, cf. [69]. Further distinctions could be made according to whether the data are given initially and fixed or whether they are gathered incrementally. Such distinctions and en-hancements are quite similar to those made in logic learning and probabilistic learning. Therefore, we will not provide more details on these aspects as doing so would lead us too far. Instead, we will concentrate on the generalized problem sketched above.
 Nearly all (score-based) approaches to structure learning perform a heuristic search through the space of possible hypotheses as determined by L . Typically, hill-climbing or beam-search is applied until the candidate hypothesis Figure 12: Refinement operators during structure learning of Bayesian networks. We can add a proposition to the body of a clause (respectively fact) or delete it from the body. Figure 13: First-order refinement operators during structure learning. Atoms can be added to or deleted from the body of a clause. Facts can be added to and deleted from the program. (1) satisfies cover( H, E ) and (2) the score( H, E ) is no longer improving. The steps in the search-space are made through the application of so-called refinement operators [96; 74], which make perform small modifications to a hypothesis. From a logical perspective, these refinement operators typ-ically realize elementary generalization and specialization steps (usually, under  X  -subsumption). In probabilistic learn-ing, certainly when working with graphical models, larger steps are sometimes taken, see also below. Let us now il-lustrate how these refinement operators work. Assume the current candidate hypothesis H consists of the set of clauses { c 1 , . . . , c n } . Then the following results of applying a (spe-cialization) refinement operator  X  on the overall hypothesis H can be imagined. Notice that  X  operates at the hypoth-esis or theory level and also that is employs a refinement operator  X  c at clause level 10 :  X   X  ( H ) = ( H \{ c i } )  X   X  c ( c i ) where c i  X  H .  X   X  ( H ) = H  X  X  f } where f is a fact with f 6 X  H .  X   X  c ( h :  X  b 1 , . . . , b m ) = { h :  X  b 1 , . . . , b In addition, one typically also employs a dual generaliza-tion operator. This operator would generalize hypotheses by deleting atoms in clauses and facts from hypotheses. The refinement operator is illustrated in Figure 12 for proposi-tional clauses and in Figure 13 for first order ones. By now, we are able to describe some of the key approaches to structure learning in probabilistic logics.
 Bayesian networks directly correspond to propositional clauses. In the typical structure learning approach, both a propositional generalization and specialization operator are applied. In one step, atoms are either added to or deleted from clauses. Furthermore, there is exactly one
At this point, note that we employ a simplified and ide-alized refinement operator. Some essential features, such as substitutions are not taken into account for reasons of simplicity, cf. [69; 74]. clause for each of proposition. Special care is taken that the resulting network is acyclic [44]. The state-of-the-art learning algorithm is the structural EM (SEM) [29]. It adapts the standard EM algorithm for structure learning. The key idea is that the expected counts are not com-puted anew for every structure that is proposed, but only after several iterations. This leads to an improved efficiency. For PRMs, SRMs and BLPs, each application of a refine-ment operator at the hypothesis level acts as a kind of macro at the level of the induced Bayesian network. Indeed, when a literal is added to the logical hypothesis, this cor-responds to adding multiple edges to the induced Bayesian network on the data. Consequently, Bayesian network learning techniques such as the SEM have been adopted. Probabilistic relational models can be represented by  X  i denote different aggregate functions and where there is at most one clause defining a predicate (such as p/1 ). Structure learning with probabilistic relational models now occurs by refining this type of clauses [30; 36; 34; 33; 31] Aggregated literals such as  X  1 ( q ( X , Y )) can be added to or deleted from clauses. At this point, the graphical notation for probabilistic relational models (illustrated in Figure 10) is convenient for visualizing this process. Adding or deleting aggregated literals to or from a clause directly corresponds to the same operation on the arcs. This in turn clarifies the connection to traditional Bayesian network learning. As in Bayesian networks, special care is taken to guarantee that regardless of the extension used the probabilistic rela-tional model will be acyclic. Bayesian logic programs are represented by regular clauses, on which the typical refinement operators from inductive logic programming can be applied [53; 54]. However, in Bayesian logic programs, one must take into account the covers constraint. More formally, it is required that the examples are models of the Bayesian logic programs, i.e. cover ( H, e ) = true if and only if e is model of H . This requirement is needed because  X  as argued in Section 4.1  X  the set of random variables defined by a Bayesian logic program corresponds to a Herbrand model. This is akin to the learning from interpretations setting in inductive logic programming [18]. The requirement is enforced when learning the structure of Bayesian logic programs by starting from an initial Bayesian logic programs that satisfies this requirement (such a hypothesis can be computed using the clausal discovery engine in [18]) and from then on only considering refinements that do not result in a violation. In addition, acyclicity is enforced by checking for each refinement and each example that the induced Bayesian network is acyclic. The framework of stochastic logic programs is a repre-sentative of the proof-theoretic probabilistic logics. It differs from the frameworks previously discussed in this section in that the learning problem is not related to that of Bayesian networks. Nevertheless, stochastic logic programs are similar to Bayesian logic programs in that they are represented by regular clauses on which the typical refinement operators apply. However, whereas Bayesian logic programs are learned from interpretations,
Please note that the original work on PRMs does not present the learning problem in terms of clauses. stochastic logic programs are learned from entailment. Indeed, the examples in stochastic logic programs are facts and these facts must be logically entailed by the target (stochastic) logic program. Thus the coverage requirement states that H | = e for stochastic logic program. Solving the general structure learning problem for stochastic logic programs thus involves applying a refinement operator at the theory level (i.e. considering multiple predicates) under entailment. This problem has been studied within the field of inductive logic programming under the term theory revision [96; 16; 100] and is known as a very hard problem. This may explain why this most general form of structure learning in stochastic logic programs (and the related PRISMs) has not yet been addressed. So far, the only contributions to structure learning of stochastic logic programs are restricted to learning missing clauses for a single predicate. In [70; 67], Muggleton introduced a two-phase approach that separates the structure learning aspects from the parameter estimation phase. In a more recent approach [68] Muggleton presents an initial attempt to integrate both phases for single predicate learning. To learn models within the intermediate representations, one can take advantage of their simplified underlying logic pro-gram. Logical and relational Markov models can be represented by transition rules of the form B  X  H where B and H are atoms representing the abstract states. One could  X  in principle  X  consider each possible abstract transition B  X  H and estimate the corresponding transition probabil-ity directly from the data. However, the number of such transitions is way too large for this approach to work in practice. Therefore, other approaches  X  that only select the most informative transitions  X  are needed. For RMMs, this is realized by 1) employing one abstract transition for each predicate pair, and 2) by assigning a probability estima-tion tree [83], a kind of decision tree, to each such predicate. The probability estimation tree will assign to any given con-crete state in the body part of the transition a probability distribution over the possible resulting states. So, probabil-ity estimation trees encode a kind of abstraction hierarchy. Kersting et al. [56] propose a different approach, which is essentially a structural, generalized EM algorithm using re-finement operators for the LOHMM formalism. An overview and survey of the new and exciting area of prob-abilistic logic learning has been presented. It combines prin-ciples of probabilistic reasoning, logical representations and statistical learning into a coherent whole. The techniques of probabilistic logic learning were analyzed starting from a logical (or inductive logic programming) perspective. This turned out to be quite useful for obtaining an appreciation of the differences and similarities among the various frame-works and formalisms that have been contributed to date. In particular, the distinction between a model-and a proof-theoretic view was used for clarifying the relation among the logical upgrades of Bayesian networks (such as PLPs,PRMs, BLPs, etc.) and grammars (such as PRISMS and SLPs). This distinction is not only relevant at the representational level but also for learning. It turned out that one learns from interpretations in the model-theoretic approaches, from en-tailment in the proof-theoretic ones, and from traces in the intermediate ones (such as RMMs and LOHMMs). Fur-thermore, principles of both statistical learning and induc-tive logic programming (or multi-relational data mining) are employed for learning the parameters and structure of the probabilistic logics considered. The authors hope that this survey provides a useful perspective on probabilistic logic learning that may inspire the reader to contribute to this challenging and exciting research area.
 This work benefited from the European Union IST project IST-2001-33053 ( A pplication of Pr obabilistic I nductive L ogic Programming  X  APRIL). The authors thank Lise Getoor for providing the graphical representation of the PRM in Figure 10. [1] J. Allen. Natural Language Understanding . Ben-[2] C. R. Anderson, P. Domingos, and D. S. Weld. Rela-[3] N. Angelopoulos and J. Cussens. Markov chain Monte [4] F. Bacchus. Using first-order probability logic for the [5] F. Bergadano and D. Gunetti. Inductive Logic Pro-[6] J. S. Breese. Construction of Belief and decision [7] J. S. Breese, R. P. Goldman, and M. P. Wellman. In-[8] W. Buntine. A guide to the literature on learning prob-[9] K. L. Clark and F. G. McCabe. PROLOG: A Lan-[10] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. [11] M. Craven and S. Slattery. Relational Learning with [12] J. Cussens. Loglinear models for first-order proba-[13] J. Cussens. Stochastic logic programs: Sampling, in-[14] J. Cussens. Parameter estimation in stochastic logic [15] J. Cussens. Statistical aspects of stochastic logic pro-[16] L. De Raedt. Interactive Theory Revision: An Induc-[17] L. De Raedt. Logical settings for concept-learning. Ar-[18] L. De Raedt and L. Dehaspe. Clausal discovery. Ma-[19] T. Dean and K. Kanazawa. Probabilistic temporal [20] A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-[21] M. Diligenti, P. Frasconi, and M. Gori. Hidden Tree [22] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Bio-[23] S. D X zeroski and N. Lavra X c. Relational Data Mining . [24] A. Eisele. Towards probabilistic extensions of [25] William Feller. An Introduction to Probability The-[26] P. Flach. Simply logical: intelligent reasoning by ex-[27] P. Flach and N. Lachiche. 1BC: A first-order Bayesian [28] P. Frasconi, M. Gori, and A. Sperduti. A general [29] N. Friedman. The Bayesian Structural EM Algorithm. [30] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. [31] L. Getoor. Learning Statistical Models from Relational [32] L. Getoor, N. Friedman, and D. Koller. Learning [33] L. Getoor, N. Friedman, D. Koller, and A. Pf-[34] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [35] L. Getoor, D. Koller, and B. Taskar. Statistical Models [36] L. Getoor, D. Koller, B. Taskar, and N. Friedman. [37] L. Getoor, E. Segal, B. Taskar, and D. Koller. Prob-[38] L. Getoor, B. Taskar, and D. Koller. Selectivity Es-[39] S. Glesner and D. Koller. Constructing Flexible Dy-[40] R. P. Goldman and E. Charniak. Dynamic construc-[41] P. Haddawy. Generating Bayesian networks from [42] P. Haddawy, J. W. Helwig, L. Ngo, and R. A. Krieger. [43] J. Y. Halpern. An analysis of first X  X rder logics of prob-[44] D. Heckerman. A Tutorial on Learning with Bayesian [45] M. Jaeger. Relational Bayesian networks. In D. Geiger [46] D. Jensen and J. Neville. Linkage and autocorrelation [47] F. V. Jensen. Bayesian networks and decision graphs . [48] M. I. Jordan, editor. Learning in Graphical Mod-[49] Y. Kameya and T. Sato. Efficient EM learning [50] Y. Kameya, N. Ueda, and T. Sato. A graphical method [51] K. Kersting and L. De Raedt. Adaptive Bayesian Logic [52] K. Kersting and L. De Raedt. Bayesian logic programs. [53] K. Kersting and L. De Raedt. Towards Combining In-[54] K. Kersting and L. De Raedt. Principles of Learning [55] K. Kersting, T. Raiko, S. Kramer, and L. De Raedt. [56] K. Kersting, T. Raiko, and L. De Raedt. A Struc-[57] D. Koller. Probabilistic relational models. In [58] D. Koller, A. Levy, and A. Pfeffer. P-classic: A [59] D. Koller and A. Pfeffer. Learning probabilities for [60] D. Koller and A. Pfeffer. Object-oriented Bayesian [61] D. Koller and A. Pfeffer. Probabilistic frame-based [62] N Lachiche and P. Flach. 1BC2: A True First-Order [63] J. W. Lloyd. Foundations of Logic Programming . [64] C. H. Manning and H. Sch  X utze. Foundations of Sta-[65] G. J. McKachlan and T. Krishnan. The EM Algorithm [66] S. Muggleton. Stochastic logic programs. In L. De [67] S. Muggleton. Learning stochastic logic programs. [68] S. Muggleton. Learning structure and parameters of [69] S. Muggleton and L. De Raedt. Inductive logic pro-[70] S. H. Muggleton. Learning stochastic logic programs. [71] R. Ng and V. S. Subrahmanian. Probabilistic [72] L. Ngo and P. Haddawy. A Knowledge-Based Model [73] L. Ngo and P. Haddawy. Answering queries from [74] S.-H. Nienhuys-Cheng and R. de Wolf. Foundations of [75] N. J. Nilsson. Principles of Artificial Intelligence . [76] H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Sh-[77] H. Pasula and S. Russell. Approximate inference for [78] J. Pearl. Reasoning in Intelligent Systems: Networks [79] A. Pfeffer and D. Koller. Semantics and Inference [80] A. Pfeffer, D. Koller, B. Milch, and K. T. Takusagawa. [81] A. J. Pfeffer. Probabilistic Reasoning for Complex Sys-[82] D. Poole. Probabilistic Horn abduction and Bayesian [83] F. Provost and P. Domingos. Tree induction for [84] L. R. Rabiner. A Tutorial on Hidden Markov Mod-[85] L. R. Rabiner and B. H. Juang. An introduction to [86] S. J. Russell and P. Norvig. Artificial Intelligence: A [87] V. Santos Costa, D. Page, M. Qazi, and J. Cussens. [88] T. Sato. A Statistical Learning Method for Logic Pro-[89] T. Sato. Parameterized logic programs where comput-[90] T. Sato and Y. Kameya. PRISM: A Symbolic X  [91] T. Sato and Y. Kameya. A Viterbi-like algorithm and [92] T. Sato and Y. Kameya. Parameter learning of logic [93] E. Segal, Y. Barash, I. Simon, N. Friedman, and [94] E. Segal, A. Battle, and D. Koller. Decomposing Gene [95] E. Segal, B. Taskar, A. Gasch, N. Friedman, and [96] E. Y. Shapiro. Algorithmic Program Debugging . MIT [97] E.Y. Shapiro. Logic Programs with Uncertainties: A [98] B. Taskar, E. Segal, and D. Koller. Probabilistic clus-[99] R. J. Williams and D. Zipser. Gradient-Based Learn-[100] S. Wrobel. First Order Theory Refinement. In L. De
