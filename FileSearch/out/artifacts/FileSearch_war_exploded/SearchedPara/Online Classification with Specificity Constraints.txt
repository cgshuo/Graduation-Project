 use the terms tp-rate and fp-rate, as we think that they are se lf-explaining. prior knowledge on the performance of the given classifiers. The assumption is that the observed game between an agent and some abstract opponent that stands for t he collective behavior of the corresponding online classification meta-algorithm.
 formance guarantees will be expressed in the regret minimization framework. In a seminal paper, Hannan [8] introduced the optimal reward-in-hindsight r  X  tion of opponent X  X  actions, as a performance goal of an onlin e algorithm. In our case, r  X  defined as the difference between r  X  goal is only to obtain no-regret with respect to the optimal t p-rate in hindsight. as a performance goal of an online algorithm. This quantity i s the maximal average reward the the computation of both the convex hull and a calibrated fore cast.
 constraints.
 and the goal of the meta-algorithm. In Section 3 we present th e general problem of constrained final remarks. m to the agent); (ii) each classifier a  X  A outputs f to belong to class 1 , and simultaneously the agent chooses a classifier a of the instance, b data is expressed in terms of the vector z r n = r ( a n , z n ) , f n ( a n ) I { b n = 1 } and the cost of the agent at time n . Note that r label at time n will be classified correctly by the agent, while c with negative label will be classified incorrectly. Then,  X   X   X   X  fp ( n ) , P respectively.
 Our aim is to design a meta-algorithm that will have  X   X  convex combination of the m given classifiers (in hindsight), while satisfying  X   X  case of online classification. 3.1 Model Definition vironment with some abstract opponent, and therefore obtai n a repeated game formulation between comes (or actions ) of the environment; r : A X Z  X  R is the reward function; c : A X Z  X  R ` constraints, that is  X  = c  X  R ` : c agent chooses an action a observes z R . We let  X  r at time n , respectively. Let H time n . At time n , the agent chooses an action a where  X ( A ) is the set of probability distributions over the set A . The collection  X  = {  X  action of the opponent by q  X   X ( Z ) , which is the probability density over Z . In what follows, we will use the shorthand notation r ( p, q ) , P for the expected reward under mixed actions p  X   X ( A ) and q  X   X ( Z ) . The notation the agent can satisfy the constraints in expectation against any mixed action of the opponent. that c ( p, q )  X   X  .
 simply by playing the corresponding stationary strategy q .
 Let  X  q so that  X  q implying that r  X  r (introduced in [11]) in order to assess the online performan ce of the agent. there exists a strategy  X  for the agent such that, almost surely, (i) lim sup and (ii) lim point distance. Such a strategy  X  is called constrained no-regret strategy with respect to  X  . agent knew in advance that the empirical distribution of the opponents actions is  X  q mization problem: We refer to r  X  the value v efficient provided that v itself is not attainable in general. However, the (lower) convex hull of r  X  Two no-regret algorithms with respect to conv ( r  X  regret algorithms in the literature.
 online convex optimization [13, 9]  X  see [11] for a discussion on this issue. 3.2 Application to the Online Classification Problem is classifiers and q ( b ) is the probability of the label b . It is easy to check that where  X  c ( a, z ) , c ( a, z )  X   X  I { b =  X  1 } , and similarly to the reward above, we have that where  X  note that keeping the average fp-rate of the agent  X   X  (1 /n ) P n k =1 c  X  ( a k , z k )  X  0 .
 exists a  X  such that  X  demonstrated in Section 5.
 We proceed to compute the BE and CBE. Using (2), the BE is (2), and (3) we have that r  X  constrained game v order to attain the convex hull of r  X  these algorithms, we are motivated to examine more carefull y the problem of regret minimization with constraints and provide more practical no-regret algo rithms with formal guarantees. limitation, we present it directly for our classification pr oblem.
 relaxed version thereof. For  X   X  0 , set r  X  function, and we can always pick  X   X  0 large enough, such that r  X  that the value v can be smaller than v We refer to r SR We note that r SR  X  propose an algorithm which solves this trade-off for  X   X   X   X  .
 We introduce some further notation. Let k . We have that the average  X  -regret and constraints violation at time n are Using this notation, the Constrained Regret Matching (CRM) algorithm is given in Algorithm 1. We then have the following result.
 Theorem 4.1. Suppose that the CRM algorithm is applied with parameter  X   X   X   X  , where  X   X  is given in (7). Then, under Assumption 3.1, it attains r SR for every strategy of the opponent, almost surely.
 P where B ( a ) = h R  X  n  X  1 ( a ) i when the average constraints violation L p = p  X  n . Finally, when P a  X  X  h R  X  n  X  1 ( a ) i Convex Programming (OCP) framework [13, 9], since the equiv alent reward functions in our case (i.e., they should not depend on the agent X  X  actions). Algorithm 1 CRM Algorithm Parameter:  X   X  0 .
 Initialization: At time n = 0 use arbitrary action a At times n = 1 , 2 , ... find a mixed action p  X   X ( A ) such that where R  X  Remark. In practice, it may be possible to attain r SR the value of  X  online. The idea is to start from some small initial value  X  At each time step n , we would like to use a parameter  X  =  X  addition, once in a while,  X  can be reset to  X  If m = 2 , we can obtain explicit expressions for the reward envelope s and for the algorithm. In that  X  explicitly the CBE, we obtain r  X  ( q ) = q (1) Therefore, the relaxation parameter is  X  = max ple rule: (i) if P h arbitrary action with p (1)  X   X   X   X  2 We simulated the CRM algorithm with the following parameter s:  X  = 0 . 3 ,  X  this algorithm is g We studied regret minimization with average-cost constrai nts, with the focus on computationally as the reward function a weighted sum of the tp-rate and fp-ra te.
 Some remarks about our algorithm and results follow. First, the guaranteed convergence rate of large period of time , the average fp-rate of each classifier a is bounded by  X  results can be then extended to this case, by treating each su ch period as a single stage. [2] D. Blackwell. Controlled random walks. In Proceedings of the International Congress of [3] D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Math-[4] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games . Cambridge University [6] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters , 27(8):861 X 874, [7] Y. Freund and R.E. Schapire. Adaptive game playing using multiplicative weights. Games and [8] J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of [10] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-
