 Discriminative training is an active area in statistical machine translation (SMT) ( e.g., Och et al., 2002, 2003 , Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009 , Foster et al, 2010, Xiao et al. 2011 ) . Och ( 2003 ) proposed us ing a log -linear model to incorporate multiple features for translation , and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. 
While the log -linear model itself is discriminative, the phrase and lexicon translation features , which are among the most important components of SMT, are derived from either generative models o r heuristics ( Koehn et al., 20 03, Brown et al., 19 9 3 ). Moreover , the p arameter s in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to t he translation measure, e. g., bilingual evaluation understudy ( BLEU) ( Papineni et al., 2002 ) . Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality.

However, there are a large number of parameters in these models, making discriminative training for them non -trivial ( e.g., Liang et al., 20 06, Chiang et al., 20 09) . Liang et al. (2006) proposed a large set of lexical and Part -of -Speech features and train ed the model weights associated with these features using perceptron. Since many of the reference translation s are non -reachable, an empirical local updating strategy ha d to be devised to fix this problem by pick ing a pseudo reference. Many such non -desirabl e heuristics led to m oderate gain s reported in that work . Chiang et al. (2009) improve d a syntactic SMT system by adding as many as ten thousand syntactic features, and use d Margin Infused Relaxed Algorithm (MIRA) to train the feature weights . However, the number of parameters in common phrase and lexicon translation models is much larger. 
In this work, we present a new , highly effective discriminative learning method for phrase and lexicon translation models. T he training objective is a n expected BLEU score , which is closely linked to translation quality . Further, we apply a Kullback  X  Leibler (KL) divergence regularization to prevent over -fitting .

For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexico n translation probabilities . A GT is a transformation of the probabilities that guarantees strict non -decrease of the objective over each GT iteration unless a local maximum is reached . A similar GT technique has been successfully used in speech recognition ( Gopa lakrishnan et al., 19 91 , Povey , 20 04 , He et al., 20 08 ) . Our work demonstrates that it works with large scale discriminative training of SMT model as well.
Our work is based on a phrase -based SMT system. Experiments on the Europarl German -t o -English dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set , where the task is to translat e TED talk s (www.ted.com) . Our experimental results on this open -domain spoken language translation task show that the proposed method leads to significant tran slation performance improvement over a state -of -the -art baseline, and the system using the proposed method achieved the b est single system translation result in the Chinese -to -English MT track . One best known approach in discriminative training for SMT is proposed by Och ( 2003 ) . In that work, multiple features , m ost of them are derived from generative models, are incorporated into a log -linear model, and the relative weights of them are tu ned discriminatively on a small tuning set . However, i n practice , this approach only works with a handful of parameters. 
More closely related to our work, Liang et al . (2006) proposed a large set of lexical and Part -of -Speech features in addition to the phrase translation model . W eights of these features are trained using perceptron on a training set of 67K sentences . In that paper, the authors point ed out that forcing the model to update to wards the reference translation could be problematic. This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produc e a reference translation . Therefore, instead of pushing the parameter update towards the reference translation (a.k.a. bold updating ), t he author proposed a local updating strategy where the model parameters are u pdate d towards a pseudo -reference (i.e., t he hypothesis in the n -best list that gives the best BLEU score ) . Experimental results show ed that their approach outperform ed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no significant gain over a stronger baseline with a full -distortion model . In our work, we use the expectation of BLEU scores as the objective . This avoid s the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective. 
As another closely related study, Chiang et al. (2009) incorporated about ten thousand syntactic features in addition to the baseline features . T he feature weights are trained on a tuning set with 2010 sentences using MIRA . In our work, we have many more parameters to train, and the training is conducted on the entire training corpora . Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training .

As a further related work, Rosti et al . (2011) have proposed usi ng differentiable expected BLEU score as the objective to train system combination parameters . Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches ( Smith and Eisner 2006 , Tromble et al., 2008 ) and lattice -based MERT ( Macherey et al., 2008 ) . In these earlier work, however, the phrase and lexicon translation models used remained unchanged.

Another line of research that is closely related to our work is phrase t able refinement and pruning . Wuebker et al . (2010) proposed a method to train the phrase translation model using Expectation -Maximization algorithm with a leave -one -out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pair s from a particular sentence wa s excluded from the phrase table when aligning that sentenc e. However, as pointed out by Liang et al (2006), the same problem as in the bold updating exist ed , i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be un reliable. The method presented in this paper is free from this problem. The translation process of phrase -based SMT can be briefly described in three steps: segment source sentence into a sequence of phrase s , translate each 
