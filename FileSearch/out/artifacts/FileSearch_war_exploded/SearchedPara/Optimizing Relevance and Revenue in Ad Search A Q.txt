 The primary business model behind Web search is based on textual advertising, where contextually relevant ads are displayed alongside search results. We address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine, showing that optimizing ad relevance and revenue is not equivalent. Se-lecting the best ads that satisfy these constraints also natu-rally incurs high computational costs, and time constraints can lead to reduced relevance and profitability. We pro-pose a novel two-stage approach, which conducts most of the analysis ahead of time. An offline preprocessing phase leverages additional knowledge that is impractical to use in real time, and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching. Empirical evaluation shows that our method optimized for relevance matches a state-of-the-art method while improv-ing expected revenue. When optimizing for revenue, we see even more substantial improvements in expected revenue. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval General Terms: Algorithms, Economics, Performance Keywords: Online advertising, Relevance, Revenue
Online advertising has arguably become the financial back-bone of today X  X  Internet. It is a multi-billion dollar market that brings audience and potential customers to countless Web sites, and provides substantial revenue for search en-gines and numerous content providers. The bulk of ads to-day are in textual form, and are selected to be contextually relevant to actual Web content. The two main types of tex-tual advertising are sponsored search , where paid ads are displayed alongside Web search results, and content match , where ads are displayed on third-party Web sites. Previous Research performed during internship at Yahoo! Research. Copyright 2008 ACM 978-1-60558-164-4/08/07 ... $ 5.00. studies have shown that more relevant ads lead to improved user satisfaction and higher response rates [5, 20].
We focus on improving the relevance of ads in sponsored search while optimizing the revenue they yield. The chal-lenge in selecting relevant ads stems from the difficulty to adequately represent queries and ads in a sufficiently rich feature space. Web search queries are on the average only about 2.5 words long, and hence often difficult to interpret automatically. Textual ads are usually created with the pre-sentation rather than indexing needs in mind, and consist of only a few dozen words. To make ad placement easier, most search engines allow advertisers to annotate ads with designated queries for which the ad is to be displayed. These queries are called bid phrases because advertisers participate in auctions where they bid to advertise on the designated queries. By default, an exact match between an ad X  X  bid phrase and the user X  X  query is required to display the ad. However, it can be quite difficult for advertisers to come up with an exhaustive list of bid phrases pertinent to their of-fers. To circumvent this difficulty, advertisers often opt-in for broad match , where ads can be shown for queries that do not exactly match the bid phrase, but rather are related to it. Relaxing the requirement for an exact match does not, however, supply any additional information, and the broad match problem remains just as hard.

With such short text fragments, it is essential to use ad-ditional sources of information for ad selection, and indeed prior studies employed text classification and named entity extraction [2, 3]. However, the amount of external knowl-edge and computation one can use is bounded by the need to provide sub-second response times. But what if we could selectively spend more time on certain queries?
We propose a two-stage approach that fixes a large set of sufficiently frequent queries, and analyzes them in depth ahead of time in a preprocessing phase. To facilitate fast on-line matching, the preprocessing phase constructs a lookup table that maps queries into bid phrases of appropriate ads. This step effectively transforms users X  queries into alterna-tive queries that are better for selecting ads. In the sub-sequent online phase, suitable ads are retrieved nearly in-stantaneously by exact match on the substituted query. It is entirely practical to compute such a transformation ta-ble for millions of frequent and highly monetizable queries, while the process can be repeated periodically to follow the changes in query popularity and ad supply over time.
During preprocessing, we first use pseudo-relevance feed-back to expand the query representation with Web search results, and then identify a set of candidate ads. These ads provide a set of candidate bid phrases that are relevant to the query. We measure the quality of these bid phrases using several sources of additional knowledge, such as fre-quency statistics, textual similarity with respect to an ex-ternal taxonomy, and bid amounts of candidate ads. It is through this use of external knowledge without impending time constraints that we achieve improved ad revenue while maintaining high relevance.

The contributions of this paper are threefold. Our main finding is that optimizing relevance alone is not sufficient for ad matching. Prior work (notably, [12]) often produced substitute queries that were not monetizable, because no or few advertisers bid on them. Using external knowledge in the offline preprocessing phase, we achieve a substan-tial improvement in expected advertising revenue without sacrificing the relevance of ads. The use of broad match also allows us to leverage market inefficiencies, which occur when substantially equivalent queries solicit very different bid amounts. Second, we are able to find relevant ads for a larger fraction of queries than previous techniques. We demonstrate that the relevance region of our method differs from that of the previous state-of-the-art method [12], thus making it possible to design a fusion approach that provides highly relevant substitutions for an even larger fraction of input queries. Finally, our methodology combines the flex-ibility of broad match and the computational efficiency of exact match ad retrieval. In essence, we show how to trans-form any query-to-ad matching system into an ahead-of-time query-to-query substitution system, simultaneously making use of heterogenous sources of external knowledge.
This section presents more details on online textual adver-tising, followed by a brief background on query substitution.
In the sponsored search scenario, online textual ads are shown next to Web search results. Usually, an ad consists of a title (typically 3 X 5 words long), a short description (around 20 words), and a landing URL that users who click on the ad are redirected to. Each ad is also associated with one or more bid phrases, which are Web queries (typically 2 X 3 words long) that advertisers list as relevant to the ad. Each bid phrase is associated with a bid amount that indicates the maximum amount of money the advertiser is willing to pay for each click on the ad. Given a query, ads are usu-ally selected by one of the following two approaches. Exact match selects ads whose bid phrase matches the query ex-actly. Broad match attempts to find ads that match the user X  X  intent expressed by the query.

In most online advertising systems, advertisers pay the search engine every time their ad is clicked on by a user. The amount paid is determined by an auction, although the details of this auction are beyond the scope of the paper. We refer interested readers to [8].

With exact match, there are three main reasons why a query q may have poor advertising potential. First, for suf-ficiently rare queries, there may be no ads with this exact bid phrase. Second, there may only be one or two advertis-ers bidding on q . Even if the ads are relevant, the standard auction approach means that the search engine is likely to receive little revenue in the event of a click. Third, the query may be ambiguous, so that many different types of ads are possible and most ads shown are irrelevant. Our method Algorithm 1 Constructing the query substitution table 1: Input: Queries Q ; Advertisement corpus Ads ; 2: for q  X  Q do 6: for bp  X  BidP hraseP ool do 8: end for 10: end for addresses the first two reasons for poor advertising perfor-mance of the exact match method.
The goal of query substitution is, given a query q , to find a substitute query sub C ( q ) that provides better search re-sults than q on corpus C . For example, when searching for  X  X og diseases X  in a medical corpus, the query  X  X anine dis-eases X  may be more effective. In general, the substitute query may involve adding or removing terms from q , or pos-sibly transforming or replacing individual words or the whole query. Note that in our case two corpora are involved (an ad corpus and a Web corpus), so the vocabulary used for ad search ( sub C ( q )) need not be the same as that for general Web search ( q ), and can instead be optimized for retrieval on the ad corpus C .

Although there has been a vast amount of research on query substitution, we limit ourselves to a brief overview so as to place our work in context. At a high level, queries are typically either changed incrementally from the input query or transformed into an entirely different query. An exam-ple of incremental change is stemming, which removes the endings of query words, so that the query  X  X ameras X  would match documents with the word  X  X amera X  [15]. Other ex-amples of incremental changes include removing one or more query words to improve recall [11], correcting spelling [6], or using a dictionary and thesaurus to find synonyms of query words [19]. Common methods for transforming queries com-pletely include Latent Semantic Indexing (LSI) [7], which maps all queries and documents to a new feature space, and building language models [14, 22] of relevant documents for example using pseudo-relevance feedback [16, 17, 21].
Many of these techniques are commonly used by Web search engines. However, both types of transformations of-ten produce queries that are not directly useful for ad selec-tion by exact match. For example, stemming often results in incomplete words and does not work for product names and numbers. Language modeling approaches tend to generate long queries that are unlikely to be bid on. Our approach is a hybrid of exact match and broad match. In the offline phase, we fix a large set of sufficiently frequent queries, and learn a function that substitutes input queries with one or more alternatives. Then, in the online phase, we use exact match to find ads matching the substitute query. Algorithm 1 describes the offline computations for creating our query substitution table. Given a set of queries and a corpus of ads, we iterate over all the queries. First, for each query we obtain the top S results returned by a Web search engine. Using features of these Web search results (see Sec-tion 3.1) we find the k ads most related to the input query. The bid phrases of these ads are taken as a candidate pool of possible substitutions for the query. For every candidate substitution, we compute a linear combination of features that measure the match between the query and bid phrase (Section 3.2). The highest scoring bid phrase is selected as the substitution for the query.

The most related previous work is log-based substitution by Jones et al. [12], which generates substitutions using search engine query logs. The method first identifies all pairs of successive queries issued by the same user, and an-alyzes them to find common transformations. Then, given a new query such as  X  X ew york maps X , it is segmented into phrases such as  X (new york) (maps) X . To generate substi-tutions, common transformations observed earlier are then applied, for example replacing  X  X aps X  with  X  X irections X , yielding a substitute query  X  X ew york directions X . This ap-proach has several limitations. First, most of the rewrites found are not actual bid phrases and cannot be used for ad placement. Second, many queries do not have relevant transformations in the search logs. For instance, queries consisting of product codes fall into this category. This lack of data reduces the fraction of queries to which log-based substitution can be applied to around 50%. Finally, when suitable substitutions are found, these can select ads with lower revenue potential, as we will see in the evaluation.
We now describe our approach in greater detail. Given an input query, the first stage in our algorithm is to find a set of possible query substitutions. In this section, we describe this process, which we call candidate selection. We start by presenting an intuitive technique similar to Web search, followed by an enhanced version that we found slightly more effective. The similarity metrics presented here were those we found to work best, although due to space constraints we cannot discuss alternatives. A full analysis of candidate selection is in preparation [4].
Our first approach for generating candidate substitutions involves query expansion using a Web search engine. Each of the top S search results for the input query was downloaded and represented as a bag of words. After stop word removal and stemming, we selected the F most frequent words in the top results, and weighted each word using a variant of TFIDF weighting [18], (1+log( T F ))  X  log ( N/N d ), where T F is the number of occurrences of the word in the top results, N is the total number of ads in our corpus and N d is the number of those that contain the word. The specific number of search results ( S = 50) and words ( F = 50) were chosen using a validation dataset.

We performed the same word transformations on the ad corpus, which consists of a several hundred million actual ads from a major search engine. It is common for one ad to be associated with many different bid phrases, hence we replicate these ads so that each ad in the corpus has exactly one bid phrase. For each ad, we used the words in the title, description, bid phrase, and URL.

Given the representation of queries and ads as a weighted bag of words, we used cosine similarity to find 100 ads clos-est to the query. We collected the bid phrases of these ads to create the candidate pool. Note that the same bid phrase was often associated with several ads from the top 100, lead-ing to a typical pool size of 20 candidate substitutions.
In addition to using weighted words and cosine similarity to select the top 100 ads, we also added two other candidate similarity measures to improve the quality of the ad pool.
To select words to represent queries and ads that are more informative than highly weighted words in search engine re-sults, we used a variant of the Prisma tool [1] to generate related phrases for both. Given a text fragment, this tool ex-tracts named entities and common phrases, and filters them through a restricted lexicon of about 10 million phrases iden-tified through global analysis of a Web corpus. We applied Prisma on the ad text  X  X s-is X , but queries were expanded using top Web search results as explained in Section 3.1.1. We selected the 50 highest scoring phrases as features to de-scribe each query and ad. Computing the cosine similarity of these vectors provided a second measure of similarity.
Furthermore, previous work has shown that IR perfor-mance can be improved if queries and documents are classi-fied in a topic hierarchy [13, 9]. We classified queries and ads with respect to a taxonomy of over 6,000 commercial topics, arranged in a hierarchy with a median depth of 5. Follow-ing Broder et al. [2], we augmented the query representation with top search results, and then performed voting to ob-tain 5 top-scoring classes for each query; a similar process was used to classify ads. The similarity of the classification of the query and ad that Broder et al. describe served as a third similarity measure.

The final similarity of an ad to the input query is a weighted sum of the three measures, weighting the web similarity twice as much as the two new measures.
The simplest way to rank the candidate query substitu-tions is using the similarity score described above. However, such a method would completely ignore the bid information associated with each of the candidates, which is essential to optimize revenue. For example, the top ranked substitution may only have one bidding ad. Additionally, noisy matches are possible, for example selecting one candidate that may be very different from all others. Therefore, although we can consider the similarity between the query and ads as one feature, it is likely that additional features can be used to improve the quality of the query substitution found.
For these reasons, we rerank candidate substitutions using a variety of features that characterize the original query and the candidate rewrites. We use a regression support vector machine [10] with default settings to learn the weights for these features, which then allows us to compute the final score for each candidate. We then take the highest scoring candidate as the selected substitution for the input query.
To describe the quality of match between a query and can-didate substitution, we use three types of features motivated by three desirable properties. First, substitutions should be somehow similar to the original query, both in terms of lex-ical similarity (as was found by Jones et al. [12]) and se-mantic similarity. The latter can be measured, for example, by whether the original query and candidate substitution return similar Web search results. Second, the substitution eventually selected for a query should be representative of many candidate substitutions so as to ensure it is not a noisy match. Third, motivated by the advertising application, a good substitution should retrieve profitable ads.
We use the following features to quantify the lexical sim-ilarity between a query q and a substitution candidate t :  X  shareWords(q,t) : Do q and t share any words?  X  wordDistance(q,t) : The number of word changes  X  cosine(q,t) : The cosine similarity of q and t .  X  editDistance(q,t) : The number of character changes  X  trigramCosine(q,t) : The cosine similarity of q and t
For features that are not guaranteed to be in the interval [0 , 1], we first ranked all candidate substitutions by the raw feature value, and then converted the feature values to per-centile ranks. For example, if 40% of the candidates for a given query had an edit distance above 10, then a candidate with edit distance 10 from the input query would have the editDistance equal to 0.4. We used such a transformation for all the features presented in this section.

To measure semantic similarity between a query and sub-stitution, we again used a Web search engine. Intuitively, the  X  X emantic X  features capture the Web search engine X  X  ability to retrieve relevant Web results. We use top Web results as background knowledge, and construct a set of fea-tures that encode semantic meaning rather than mere tex-tual similarity measured by the lexical features:  X  maxMatchScore(q,t) : The maximum similarity score (as  X  abstractCosine(q,t) : The cosine similarity of Q and T ,  X  taxonomySimilarity(q,t) : The similarity of q to t with
Note that the second and third features are very similar to two of the similarity measures used in the enhanced pooling approach (Section 3.1.2). However, here the similarity is measured between the query and a candidate bid phrase, rather than between the query and an entire ad as before.
We would also like the top ranked substitution to be repre-sentative of the entire pool of candidate substitutions. This way, a particular substitution is less likely to be selected based on one spurious match. This is motivated by query ex-pansion using pseudo-relevance feedback, where it is usually optimal to add words common to many of the top ranked re-sults retrieved for the original query. We used the following features to capture whether many of the substitution can-didates are of interest to the same advertisers, and whether the substitutions are often used by search engine users:  X  clientCosine(clients(top-10),clients(t)) : The overlap be- X  queryFrequency(t) : The frequency with which the sub-
Finally, motivated by the goal of maximizing revenue for the search engine, we used the following features to charac-terize the revenue potential of a given substitution t :  X  maxBid(t) : The max bid of any ad with bid phrase t .  X  secondBid(t) : The second highest bid for this substitu-Table 1: Relevance scale used by human judges to eval-Table 2: Relevance scale used by human judges to eval-
The features described above measure the match between a query and a candidate substitution. To learn a query substitution function, we need to combine these features into a final score that can be used to select a specific substitution from the candidate pool for a given query.

We learned a weighted linear combination of the features using labeled training data obtained from expert judgments. Due to the time and expense necessary to obtain these judg-ments, we started with a subsample of 40 diverse Web search queries used during an earlier study [12]. For these queries, we generated candidate substitutions as described above us-ing both the simple and enhanced pooling techniques. The candidates were ranked by maxMatchScore , using an un-weighted sum of all the features above and by using log-based substitution. The top three substitutions generated by each method were collected, giving us about 100 distinct (query, substitute) pairs after removing duplicates.
The relevance of these substitutions was evaluated by ex-pert human judges. Judges were asked to score each (query, substitute) pair on the 4-point relevance scale shown in Ta-ble 1. The scale is designed to be granular enough to cap-ture different levels of relevance, while restricted to allow judgments to be reliable. In addition, each level is assigned a numeric score that measures the relative relevance that the judgment represents. By training a regression SVM [10] with default settings to the relevance values, we obtained a weighted linear combination of all the features, giving us a function for ranking the candidate query substitutions.
The ranking function learned using relevance judgments can only be expected to rank substitutions in terms of rel-evance to the original query. However, as described earlier, the substitutions selected should also produce high revenue for the search engine.

We propose to rank candidate substitutions by the prod-uct of (1) the query substitution relevance score and (2) second highest bid amount of any ad bidding on the candi-date substitution. Second price auctions are standard in ad pricing, hence the second highest bid amount is the revenue that a search engine would collect for a click on the top ad. We call this method revenue optimized .

Algorithmically, revenue optimization amends line 9 of Al-The relevance score ~w  X  ~ f bp is assumed to be proportional to the probability that a user will click on an ad for the substi-tute query after having issued the search query. Multiplying the score by the second-highest bid amount yields an esti-mate of the expected revenue from this query substitution.
In this section we evaluate the performance of our ap-proach. We start by describing the corpus of query and ad relevance judgments used for evaluation. Then we compare the expected revenue generated by using our system to pre-vious work. We follow this by evaluating the contribution of the individual components of our technique to the overall result. Finally, we analyze the weights for the features used to rank candidate substitutions.
We evaluated our approach using two kinds of expert judg-ments, which captured the quality of query substitutions directly, as well as indirectly by evaluating the actual ads retrieved through the use of substitutions.

To build the first evaluation dataset (to measure query substitution quality), we collected a list of the 10 million most frequent queries issued to the Web search engine over a one week period. We used stratified sampling to randomly select 50 queries from each decile in terms of query fre-quency, giving us 500 evaluation queries; this set contained both very frequent and less frequent queries. For each eval-uation query, we computed the top 3 substitutions using a variety of methods to allow us to compare their performance. The methods we used were (1) log-based substitution; (2) the learned ranking function using both the simple and en-hanced pooling methods; (3) ranking substitutions by the estimated relevance times the second highest bid amount for this substitution; (4) ranking the candidate substitutions by the maxMatchScore feature, both using simple pooling and enhanced pooling. In the enhanced pooling case, we also multiplied the match score by the second highest bid amount. After removing duplicates, this gave us over 5,400 (query, substitute) pairs, which were judged in the same way as before.

To allow us to evaluate the relevance of the actual ads that would be returned by these methods, we also collected a second set of relevance judgments. Taking the highest scoring substitute query returned by each of the methods listed above, we collected the three ads in our corpus with highest bids. Accounting for duplicates and substitutions with fewer than three ads, this gave us almost 4,000 (query, ad) pairs. The relevance of these ads to the original query was evaluated by the expert judges, using the scale described in Table 2.
Figure 1 plots a score that is proportional to the expected revenue as a function of coverage. We calculated expected revenue as the product of the (judged) relevance score for query substitutions and the second highest bid, averaged across all the evaluation queries. Along the horizontal axis, coverage indicates the fraction of the 500 evaluation queries for which each method generated a query substitution. To control the coverage of each method, we only used substi-tutions for which the score was above a threshold. For in-stance, a coverage of 40% indicates that substitutions are accepted for the 40% of the input queries with highest score. Comparing performance at different coverage levels is sim-ilar to comparing the performance of information retrieval systems at different recall levels.

Assuming that users X  click probabilities are proportional to the scores assigned by the human relevance judges, the revenue optimized method (which uses enhanced pooling) generates substantially more revenue than any of the other approaches. Without revenue optimization, simple pooling and enhanced pooling perform almost indistinguishably and with much lower expected revenue. The log-based substi-tution technique of Jones et al. yields substantially lower expected revenue still. For comparison, we used a baseline that substitutes the input query with the top ranked bid phrase returned by simple pooling.

In the figure, we only present the curve for log-based sub-stitutions for a limited coverage range as we were unable to practically obtain query substitutions for higher cover-age levels. The maximum level of coverage shown here for that approach is similar to that reported by Jones et al. We were also unable to create a revenue-optimized version of log-based substitution because we were unable to obtain a sufficiently large selection of candidate substitutions to rank for each input query.

It is interesting to note that both the baseline and log-based substitution methods perform particularly poorly at low coverage. This suggests that the highest scoring sub-stitutions generated, i.e., those with the highest relevance score, are rewrites that provide low revenue.
We now evaluate the performance of our approach in terms of the relevance of the ads and query substitutions found.
We start by analyzing the judged relevance of the ads se-lected by each method. Figure 2 presents the fraction of top 3 ads returned for the 500 evaluation queries that were relevant to the original query (i.e., judged as certainly at-tractive or probably attractive), as a function of the fraction
Figure 2: Fraction of relevant ads vs. query coverage. of queries for which we generate ads. In this case we varied the coverage by choosing not to display ads when the score from each system fell below varying thresholds.

The figure shows the performance of the methods de-scribed above, and also compares to the relevance of the top ads selected by searching the ad corpus directly using enhanced pooling, without using query substitution. We see that query substitution with enhanced pooling selects more relevant ads than simple pooling, and yields ads of simi-lar relevance to direct ad search. We also see that at high coverage levels enhanced pooling provides the most relevant ads, suggesting that query substitution helps avoid noisy ad matches. However, between 10% and 30% coverage, log-based substitution yields the most relevant ads. This hap-pens since log-based substitution selects substitutions just based on similarity to the original query. Yet as before, log-based substitution provides substitutions for only half the queries. In addition, some of the substitute queries have no advertisements in our corpus. We also see that, surpris-ingly, at lowest coverage log-based substitution and direct ad search both return less relevant ads.

Note also that the revenue optimized method, which per-formed by far the best when measuring revenue, performs less well in terms of ad relevance. On average, the ads re-turned are less relevant than those returned by the other methods. This happens because the bid phrases selected as substitutions are biased toward ads that have high bid amounts. Should we wish to avoid showing ads with low relevance values, a potential approach would be to interpo-late between the revenue optimized approach and another approach. For example, candidate query substitutions could be ranked by a weighted sum of the expected revenue and the ad relevance. By tuning the relative weighting, we could trade off mean relevance and expected revenue. Combined with the previous figure, these results show that optimizing for revenue and optimizing for relevance can lead to sub-stantially different learned query substitutions. In fact, it is apparent that optimizing for revenue and optimizing for relevance can be at odds with each other, where as one im-proves the other decreases.
When using query substitution for ad selection, less rel-evant ads may be returned for one of two reasons. Either, the learned substitution may not be relevant to the original query, or the substitute query may not have any relevant ads Figure 3: Fraction of query substitutions judged precise Table 3: Comparison of the relevance of substitutions in our ad corpus. To tease apart the extent to which each of these two reasons contribute to the results seen in Fig-ure 2, Figure 3 shows how the fraction of substitutions that are relevant changes as a function of query coverage. We consider substitutions judged to be a precise match or an approximate match as relevant. Log-based substitution ob-tains the most relevant substitutions although again only for about half the queries. Enhanced pooling still outperforms simple pooling, and both substantially outperform revenue optimized ranking in terms of relevance. We also compare to the same top-of-pool baseline described for Figure 1.
Note that log-based substitution does not exhibit the un-usual drop in performance at low coverage seen earlier. This suggests that the query substitutions generated with very high confidence are very relevant to the original query yet poor for selecting ads.

Additionally, by comparing to Figure 2, we see that all methods are better at finding relevant query substitutions than they are at selecting ads using query substitutions. It is likely that this is a limitation of query substitution in general, as selecting a single substitution for a user query limits the potential ads that can be shown. However, it could be addressed if we allow multiple substitutions per input query. For example, if we were to select a small number of substitutions per input query, ads could be served for any of them, increasing the supply of potential ads while remaining computationally efficient.
One of the limitations that we have seen repeatedly for log-based substitution is that it is unable to obtain substitu-tions for many of the queries. We now address the question of how to combine that method with ours to generate more relevant substitutions for a larger fraction of the queries. Table 3 shows the fraction of queries where each method ei-ther selects relevant substitutions, selects non relevant sub-stitutions and does not generate any substitutions on our evaluation set of 500 queries. Figure 4: Relevance of query substitutions using
We see that enhanced pooling performs better on the queries that log based substitution is also able to gener-ate substitutions on. However, for queries where log-based pooling is not applicable, enhanced pooling can still gener-ate substitutions with reasonable performance. In fact, just as average substitution and ad quality can be improved by reducing coverage of queries, thresholding can be used to re-duce the number of poor substitutions returned on queries uncovered by log-based substitution. For instance, enhanced pooling achieves a precision of 64% if only used to generate a substitution for 42% of the queries uncovered by log-based substitution. We plan to study such a combined substitution system in future work.
As described in Section 3, the feature weights used to rank candidate substitutions were trained using a small set of about 100 (query, substitution) pairs. We now observe that the substantially larger dataset collected for the 500 evalu-ation queries can also be used to improve the weights used to rank candidate substitutions. The feature weights can be trained either using the (query, substitution) judgments or using the (query, ad) judgments. We will now compare these two alternative training methods to determine which results in better performance.

We start by optimizing the weights to the (query, substi-tution) relevance judgments. Since the relevance of a sub-stitution should be unrelated to the ads on the substitution, we trained a model omitting the features that depend ex-plicitly on ads, again using a regression SVM with default settings. We then ranked candidate substitutions both with enhanced pooling, and revenue-optimized enhanced pooling.
To train the weights using the (query, ad) judgments, we set the target value of each (query, substitution) to the mean relevance judgment of the top three ads retrieved for that substitution. We also learned weights for two new features which were omitted in earlier experiments due to an over-sight: numberOfAds(t) capturing the number of ads bidding on t , and numberOfClients(t) , the number of distinct adver-tisers who bid on t .

To evaluate the performance of these new ranking func-tions without overfitting, we performed the entire training and evaluation using 5-fold cross validation. Figure 4 shows the fraction of top substitutions that were found relevant to the original query. Note that as we had a fixed evaluation corpus, about one third of the substitutions now returned Figure 5: Expected revenue using weights retrained on by the different methods had not been judged by the human judges, and thus were ignored.

We see that the relevance of substitutions selected by en-hanced pooling, trained on either set of judgments, both when revenue optimized and not, has markedly improved over that seen in Figure 3. In particular, enhanced pooling now returns substitutions whose relevance almost matches those returned by log-based substitution. Computing the 95% binomial confidence intervals for the performance of the top 3 methods, we find that the difference in relevance between them is not statistically significant at any coverage level. In addition, the relevance of the revenue-optimized substitutions has also improved by about a third. Interest-ingly, despite this evaluation being in terms of the relevance of substitutions, optimizing to (query, substitution) judg-ments or the (query, ad) judgments makes little difference.
Evaluating the estimated revenue using the retrained weights, Figure 5 can be compared to the results obtained for the original weights and presented in Figure 1. We see that the performance of the revenue optimized method is essentially unchanged. However, the relevance for the simple pooling and enhanced pooling methods increased at the cost of expected revenue. Although the revenue for these methods has dropped, the estimated revenue using the weights trained on query relevances is still 20% to 30% higher than that generated by log-based substitution. These results reemphasize the earlier observation that revenue and ad relevance can be at odds.

We finish by studying the weights learned for the fea-tures using the evaluation data. We retrained the models evaluated in this section using all the relevance judgments, optimizing to the (query, substitution) judgments and the (query, ad) judgments. The weights learned are listed in Table 4.

Notice that the learned substitution functions place the most weight on the semantic and lexical features. We sus-pect that negative weights in many of the features are due partly to the correlation between some of the features and partly to random noise. For example, shareWords and tri-gramCosine are strongly related. In all, the net effect is that the largest contribution is due to simple textual similarity. However, the extent to which substitutions are representa-tive of all candidate substitutions (represented by the client-Cosine feature) is also important. Particularly interestingly, maxMatchScore has almost zero weight in both cases, sug-gesting that the original scores used to create the pool of Table 4: Learned feature weights, optimized to the candidate substitutions are not very useful given all the other features. Indeed, it indicates that the features just based on the bid phrases are more informative than those based on entire ads. Also, it is interesting that the revenue features have mostly negative weights. This means that the bid amounts and query frequency are slightly negatively cor-related with substitution relevance.
We have presented a two-phase methodology for selecting advertisements to appear alongside Web search results. In an offline preprocessing phase we use several sources of ex-ternal knowledge to build a query substitution table. Then, at matching time, retrieving suitable ads is performed nearly instantaneously by finding ads whose bid phrase exactly matches the substituted query. In contrast to previous query substitution studies, we optimize both the relevance of ads and the advertising revenue collected by the search engine. Our approach combines the flexibility of broad match with the computational efficiency of exact match. We have demon-strated that the expected revenue generated by our query substitutions is substantially higher than that offered by previous approaches, while the relevance of ads produced can be on par with the previous state of the art.

We plan to extend this work by studying in more detail the tradeoff between revenue and ad relevance. In particu-lar, we plan to perform a real-world study showing adver-tisements generated by different techniques to Web search users. Furthermore, we plan to compare our method directly to revenue-optimized log-based substitution.

Finally, observe that the manual association of bid phrases to ads encodes a considerable amount of human knowledge, which we expect to be useful outside of the advertising do-main. Consequently, in future work we intend to explore the effectiveness of bid phrases for general query expansion, cross-language information retrieval, and related tasks. We would like to thank Rosie Jones, Benjamin Rey and Shaji Sebastian for providing access to the log-based query substitution implementation we used, as well as for helpful discussions about evaluation. We also thank the anonymous reviewers for useful feedback. [1] P. Anick. Using terminological feedback for web search [2] A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, [3] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel. [4] A. Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich, [5] P. Chatterjee, D. L. Hoffman, and T. P. Novak. [6] S. Cucerzan and E. Brill. Spelling correction as an [7] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [8] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [9] E. Gabrilovich and S. Markovitch. Feature generation [10] T. Joachims. Making large-scale SVM learning [11] R. Jones and D. C. Fain. Query word deletion [12] R. Jones, B. Rey, O. Madani, and W. Greiner.
 [13] P. Kowalczyk, I. Zukerman, and M. Niemann.
 [14] V. Lavrenko and W. B. Croft. Relevance-based [15] C. D. Manning and H. Schuetze. Foundations of [16] M. Mitra, A. Singhal, and C. Buckley. Improving [17] S. E. Robertson, S. Walker, S. Jones, M. M.
 [18] G. Salton and C. Buckley. Term-weighting approaches [19] E. M. Voorhees. Query expansion using lexical-[20] C. Wang, P. Zhang, R. Choi, and M. D. Eredita. [21] J. Xu and W. B. Croft. Improving the effectiveness of [22] C. Zhai and J. D. Lafferty. Model-based feedback in
