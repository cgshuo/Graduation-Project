
Sensor-based monitoring of any phenomenon creates time series data. The spacing between successive readings may be constant or varying, depending on whether the sampling is fixed or adaptive. The overall goal is to obtain an accurate picture of the phenomenon with minimum sampling effort. Examples of such observations include highway traffic monitoring, electro-cardiograms, and monitoring of oil refineries. in using data mining techniques to extract interesting patterns from temporal sequences [SA95, MTV97, 
PT96]. A standard assumption has been that the raw data collected from sensors is somehow processed to generate a sequence of events, which is then mined for interesting episodes [MTV95, HKM+96]. While there pattern of events occurring in some order, and close enough to each other in time. Recent research has developed languages for specifying temporal patterns [MT96, PT96, GWS98], and algorithms have been proposed that takes advantage of the specified pattern is of deriving an event sequence from raw sensor data. 
In some cases the rule for determining when a sensor reading should generate an event is well known, e.g. if the temperature of a boiler goes above a certain threshold, then sound an alarm. However, if the phenomenon is ill-understood or changes its behavior unpredictably, adapting the threshold such that event reporting is accurate becomes very difficult. Thus, a more systematic approach is required for processing the raw sensor data to generate an event sequence. This is the focus of our paper. changes enough over time so as to be considered a from light to heavy to congested. Another example is the change of a boiler from normal to super-heated. 
The specific problem we address is of applying data mining techniques to identify the time points at which the changes, i.e. events, occur. In the statistics literature this has been called the change point detection problem. The standard approach has been to (a) apriori determine the number of change points to be discovered, and (b) decide the model to be used for fitting the subsequence between successive change points. Thus, the problem becomes one of finding the best set of the predetermined number of points that minimizes the error in fitting the pre-decided function [SO94, Hus93, Haw76, HM73, Gut74]. [KS971 addresses the problem of approximating a sequence of sensor readings by a set of Ic linear segments as a pre-processing step. This too can be considered a version of the change-point detection problem. In the proposed approach, we address both limitations of standard approaches. First, we place no constraint on the class of functions that will be fitted to the subsequences between successive change points. Second, the number of change points is not, fixed apriori. Rather, the appropriate set is found by using maximum likelihood methods [Hud66]. 
In this paper we study two versions of the change point detection problem, namely the batch and the incremental versions. In the batch version the entire data set is available, as in the case of 24-hour data from traffic sensors, from which the best, set of change points can be determined. In the incremental version, the algorithm receives new data points one at a time, and determines if the new observation causes a new change-point to be discovered. Our contributions include l developing a general approach to change-point, l developing algorithms for both the batch and incre-l evaluating their behavior with synthetic and real l and comparing the algorithms with visual change-
This paper is organized as follows: In section 2 we formally describe the event detection problem. Section 3 presents the batch algorithm and Section 4 its performance. Section 5 describes the incremental algorithm, which is evaluated in the Section 6. Section 7 concludes the paper. In this paper we are interested in real-valued time time variable. It is assumed that the time series can be modeled mathematically, where each model is characterized by a set of parameters. The problem of event detection becomes one of recognizing the change of parameters in the model, or perhaps even the change of the model itself, at unknown time(s). 
This problem is widely known as the change-point detection problem in the field of statistics. A number of approaches have been proposed to solve the change-point. detection problem [SO94, Hus93, Haw76, HM73, Gut74]. The standard assumption is that the phe-nomenon can be approximated by a known, stationary (usually linear) model. However, this assumption may not be true in some domains, creating the need for an approach that works without this assumption. In this paper we propose an approach that simultaneously ad-dresses the issue of model selection and change-point detection. 2.1 Formal Statement of the Problem Consider a time series denoted by y(t), t = 1,2, . . ..n. where t is a time variable. We would like to find a piecewise segmented model M, given by 
An fi(t, wi) is the function (with its vector of parameters wi) that is fit in segment i. The Bi X  X  are the change points between successive segments, and ei(t) X  X  are error terms. At this point we put no constraints on the nature of fi(t, ~0 X  X . 2.2 Maximum Likelihood Estimation If all change points are specified a priori, and mod-ations gi X  X  found for each segment, then the statistical likelihood L, of the change points is proportional to 
L= Here Ic is the number of change-points, mi is the number of time points in segment i, and n is the total number of time pointsl. 
If the change points are not known, the maximum likelihood estimate (MLE) of the ei X  X  can be found by maximizing the likelihood L over all possible sets of or equivalently, by minimizing -2 log L. This function is equivalent to, 
In this paper, the term likelihood criteria will refer to function -2 logL, and will be denoted as C. Because log is a monotonically increasing function, for the ho-moscedastic error case we use the equivalent likelihood criteria of minimizing the function C, X =, rrzi~p. For each segment i, model estimation is the problem of finding the function fi(t, wi) that best approximates the data. The quality of an approximation produced by the learning system is measured by the loss function Loss(y(t),fi(t,wi)), where Bi-i &lt; t 5 0i. The expected value of loss is called risk functional &amp;(wi) = E[Loss(y(t), ji(t, wi)]. Therefore, for each segment the learning system has to find a fi(t, wi) that minimizes R(wi). 
Let us now consider the nature of fi(t, wi) X  X . Most past work has assumed that the nature of these functions is known, or can be somehow determined from domain knowledge. However, in general this cannot be done, and thus our approach allows the possibility of arbitrary functions. To provide a handle on the problem, however, we use the key result of universal approximation theory, which states, that any continuous function can be approximated by another function from a given class [CM98]. The latter class can be considered as a basis class. An example of such a basis class is the set of algebraic polynomials { to, 9, t2, . . } X  [KC96]. 
For each of the segments, the learning machine should select a model that best describes the data. Various model selection methods have been proposed, e.g. analytical model selection via penalization and model selection via re-sampling [CM98]. The re-sampling approach has an advantage of making no assumptions on the statistics of the data or the type of target function being estimated. However, its main disadvantage is high computational effort. With linear regression it is possible to compute the leave-one-out cross-validation estimate of expected risk analytically [CM98]. This has computational advantages over the re-sampling approach, since repeated parameter estimation is not required. This is the approach used in the paper. Finally, the change-point likelihood also depends on the error model used. Unless there is a known fact that the error model is heteroscedastic, it is reasonable to assume the homoscedastic error model [Kue94], which is what we do. In this section we assume that the entire data set is collected before the analysis begins. In section 5 we consider the incremental case where change-point detection proceeds concurrently with data collection. Change-point detection algorithms have been studied in the statistics literature [Haw76, HM73, Gut74]. They have worked under the assumptions that (a) a stationary known model can be used to describe (b) the number of change points is known apriori. Our approach was to start from the algorithm described in [Haw76], and remove these assumptions. 
Assume that the best model that maintains time points ti, ti+l, . . . . tj as a single segment has been selected. Let S be the residual sum of squares for this model. The number of points in this segment is m = j -i + 1. Let C(i, j) = mlog(S/m) if a heteroscedastic error model is used, and l(i,j) = S if the error model is homoscedastic. 
The key idea behind the proposed algorithm is that at every iteration, each segment is examined to see whether it can be split into two significantly different segments. The splitting procedure can be illustrated by a consideration of the first stage, since all subsequent stages consist of equivalent scaled-down problems. Let the data set cover the time points tl, ts, . . ., t,. The change point in the first stage is the j minimizing C(l,j)+ C(j + l,n), say j*. Here j* is defined as 
The range of j depends on the fact that at least p points are needed for model fitting in each segment. 
Further, the model fitted in each segment is the best possible from the space described by the basis functions, according to the model selection method used. 
At the second stage, each of the two segments is analyzed as above and the best candidate change-points cl and c2 of each are located. The better of these candidates is then selected, yielding a division of the original sequence into three segments. Without loss of generality let X  X  assume that point cl is chosen: Now, the likelihood criteria of the model becomes 
The above procedure is repeated until a stopping criterion (described in section 3.2) is reached. Figures 1, 2, 3 provide the details of the algorithm. 
It should be noted that there have been other algo-rithms [HM73, Gut741 proposed to solve the change-point problem. We chose to modify a hierarchical solu-tion because it is computationally more efficient. Since the number of change points is not known apriori, a stopping criterion must be used by the algorithm. In practice one would expect that once the algorithm has detected all  X  X eal X  change-points, adding any more change points would not change the likelihood significantly. In fact, upon the addition of a sufficient number of spurious change-points, the overall likelihood value can increase, as illustrated in Figure 4. In successive iterations of the algorithm, at first the likelihood criteria decreases dramatically until it becomes stable, and then. starts to increase slowly as spurious change-points are found. Therefore, the algorithm should stop when the likelihood criteria becomes stable or starts to increase. Formally, if in iterations k and 5 + 1 the respective likelihood criteria values are Lk and Ck+l, the algorithm should stop if where s is a user-defined stability threshold. When stability threshold s is set to O%, the algorithm stops only when the likelihood criteria starts increasing. We evaluated the behavior of our change-point detec-tion algorithm on synthetic as well as real data from highway traffic sensors. In this section we present the Figure 4: Likelihood criteria as a function of change-points results of these evaluations. In each case we measure the effectiveness of the algorithm, i.e. the quality of the change-points detected. For experimental purposes, the basis functions we selected were 1, t, t2 and t3. Note that our approach is general and can work with any class of basis functions. The data set consisted of 40 data points and was generated using the following saw-tooth function 
The noise E is Gaussian with zero mean and unit variance. The height of the function h controls the signal-to-noise ratio. The larger the value of h, the greater the signal-to-noise ratio. An example of such a function (without noise) is depicted in Figure 5. 
If the proposed algorithm is able to correctly identify all change-points, it should detect the following inter-vals: [l, 91, [lo, 191, [20, 291, [30, 391. However, due to the continuity of the saw-tooth function f(t) at the change-points, a different set of change-points can also be detected. For example, the set [l, lo], [II, 191, [20, 291, [30, 391 is also a correct set of intervals. This is because t = 10 can be interpreted as the end of the cur-rent trend or the beginning of a new one. Similarly for t = 20 and t = 30. 
The experiment was aimed at finding whether the method is able to correctly identify all change-points, and the sensitivity of the technique to the noise level. The results of the experiment are summarized in Table 1. As the signal-to-noise ratio decreases, the algorithm starts to give less accurate results. In this particular case the algorithm breaks at height h = 2. However, the algorithm works well for larger values of h. For h &gt; 8, the algorithm identifies all change points without introducing false positives and false negatives. 
The stability threshold, s, of the stopping criterion doesn X  X  affect the results when the data set does not have a lot of noise. However, when the noise in the data set is increased, higher values of s prevent the algorithm from identifying false change-points. At height h = 5, when we increased the stability threshold from 0% to 5%, the algorithm was able to stop before falsely splitting the region [30, 391 into two regions [30, 351 and [36, 391. The data used in our experiments was taken from highway traffic sensors, called loop detectors, in the Minneapolis-St. Paul metro area. A loop detector is a sensor, embedded in the road, with an electro-magnetic field around it, which is broken when a vehicle goes over it. Each such breaking, and the subsequent re-establishment, of the field is recorded as a count. Traffic volume is defined as the vehicle count per unit time. In our data set the volume data was sampled at 5 minute intervals, i.e. the vehicle count was recorded at the end of a 5 minute interval and the counter was reset to 0. Each data set is a time sequence collected over a 24-hour time period, i.e. consisting of 288 samples. 
The proposed algorithm X  X  behavior was evaluated on four different data sets, the results of which are shown in Figures 6, 7, 8, and 9. Each change point detected by the algorithm is based on the criteria defined in Section 3, i.e. the stability threshold of 0% is met for each of the points. However, some interesting observations can be made from these graphs. Segment A of Figure 7 is reported as one segment by the algorithm, whereas based on visual inspection one could argue that there are one or more change points in it. However, the likelihood calculations of the algorithm show that the variations being observed are not statistically significant and probably attributable to noise. A similar situation occurs in segment B of Figure 8, which contains a seemingly significant local minima. The converse appears in Figure 9, where C and D are reported as two separate segments, even though they visually appear to be a single segment. A reason is that we often tend to focus on straight-line segments in visual examinations [Att54]. Figure 6 represents a case where all the change points detected by the algorithm seem to agree with our intuitive notion of change-points. A crucial issue in evaluating the behavior of a change 
We were interested in how our change point detec-tion algorithm performed compared to a person doing the same task through visual inspection. The original data was very noisy, and thus in some cases it was dif-ficult to visually detect the actual change points. Es-sentially, the data had a lot of small variations, which can potentially cause a human to observe microscopic trends that are not actually present. Based on our dis-cussions with traffic engineers from the Minnesota De-partment of Transportation, i.e. the domain experts, we smoothed the data using a moving averages approach for visual inspection based change point detection by the human observer. Our algorithm was fed the origi-nal data set, i.e. without smoothing. point detection algorithm is to determine if the change points detected by it are indeed true change points. However, this raises the issue of first determining what the true change points of a function are. This is a difficult question to answer, because it in turn depends on the method employed to determine the true change points. Our approach was to examine the techniques used in the traffic domain, from which the data was taken. Traffic engineers use visual inspection for detecting change points in traffic data. Hence, we selected the data set of Figure 6 and asked four human subjects to detect change points3 in it by visual inspection. Subject Sr and Sz were given smoothed representation of the time sequence, while subjects S X  X  and SJ received the original data set. 
Figures 10 through 13 show the change points reported by subjects Sr , Sz, Ss, and S,, respectively. The change points detected by subject Sr, Figure 10, seem to be the most similar to those detected by our algorithm. Subject SZ, Figure 11, seems to be using a quadratic model for segmentation, while subject Ss, Figure 12, seems to be using a cubic model. Subject Sq, Figure 13, seems to be using a linear segmentation model. 
One thing that became clear from this experiment was that determining the true change points of a function is not at all straightforward, and human observers can have significant disagreements. Thus, a technique based on detecting change points based on some quantitative measure of likelihood is perhaps more robust than any of these. 
To quantify the quality of change-points identified by the subjects, we calculated the likelihood estimates for each of the models and compared them with the likelihood criteria of the model identified by our algorithm. The resulting ratios are shown in Table 2. The results show that statistically speaking the algorithm performed better then any of the four subjects. The batch algorithm is useful only when data collec-tion precedes analysis. In some cases, change-point de-tection must proceed concurrently with data collection, e.g. dynamic control of highway ramp metering lights. Towards this we developed an incremental algorithm. The key idea is that if the next data point collected by the sensor reflects a significant change in phenomenon, then its likelihood criteria of being a change-point is going be smaller then the likelihood criteria that it is not. However, if the difference in likelihoods is small, we cannot definitively conclude that a change did oc-cur, since it may be the artifact of a large amount of noise in the data. Therefore, we use the criteria that a change-point has been detected if and only if where 6 is a user-defined likelihood increase threshold. 
Suppose that the last change-point was detected at time tk-1. At time tl, the algorithm starts by collecting enough data to fit the regression model. Suppose at time tj a new data point is collected. The candidate change point is found by determining ti, with likelihood criterion &amp;in(k,j), such that If this minimum is significantly smaller than C(lc, j), i.e. the likelihood criteria of no change-points from tk to tj, then ti is a change-point. Otherwise, the process should continue with the next point, i.e. tj+l. The algorithm is shown in Figures 14 and 15. 
In the incremental algorithm, execution time is a significant consideration. If enough information is stored, some of the calculations can be avoided. Thus, at time tj+l to find likelihood criteria it is only necessary to calculate L(i + 1,j + l), since .C(k, i) was calculated in the previous iteration. 
It should be noted that if a change-point is not detected for a long time, the successive computations become increasingly expensive. A possible solution is to consider a sliding window of only the last w points. To study the performance of the incremental algorithm, we used data set generated by the following function where the noise E is Gaussian with zero mean and unit variance. 
The goal of this experiment was to observe if the algorithm is able to accurately recognize the change-points. Accuracy is measured both by how close the identified change-point is to the point where the actual change occurred, and by how long it takes the algorithm to recognize the change. 
The results of the experiment are shown in Table 3. The algorithm performs well for data sets with high signal-to-noise ratio. In addition, the time it takes to realize that the change occurred is small. However, for data sets with h 5 20, the algorithm starts to break. The change-point estimates become increasingly inaccurate. Moreover, the latency of recognizing that change has occurred increases. In addition, for likelihood increase threshold 6 = 35%, the algorithm identifies spurious change-points. Increasing the threshold to 45% does not eliminate spurious change-points, but eliminates a true change-point when h = 10. 
The last column in Table 3 represents results obtained by running the batch algorithm on the same data sets with stability threshold s = 5%. Note that the batch algorithm identifies change-points with very high accuracy, showing it to be much more tolerant of noise than the incremental algorithm. This is because the batch algorithm tries to achieve a global optimization of the likelihood metric, while the incremental algorithm seeks only local optimization due to unavailability of data about the future. 7 Conclusion and Future Work In this paper, we presented an approach for event detection from time series data. The approach allows US to detect a change-point by detecting the change of model (or parameters of the model) that describe the underlying data. We use a combination of change-point detection and model selection techniques. The proposed approach does not assume the availability of a model describing the data, or the number of deviation points in the time series. In addition, the technique is independent of regression and model selection methods. 
Our experimental results suggest that both algo-rithms are able to correctly identify change-points in cases where signal-to-noise ratio is not too low. In ad-dition, the proposed approach is more robust than using visual inspection by humans, at least by the likelihood measure used here. First, it is not subject to human ten-dency to segment smooth curves into piecewise straight lines. Second, while human beings find it hard to work with data that contains a lot of noise, the algorithms are able to handle such data sets (as long as the noise level doesn X  X  dominate the signal). The batch algorithm is more robust than the incremental one, since it works with the entire data set and can perform global opti-mization. 
As discussed in [Raf93], applicable Bayesian ap-proaches have been found to produce results more eas-ily than non-Bayesian ones, especially for change point detection in one dimensional stochastic processes, How-ever, a significant hurdle is the existence of a prior model that is both sophisticated enough to model the application, and computationally tractable for deriving the posterior model. In general, to make the computa-tion tractable often simplifying assumptions are made [CGS92]. Previous work [CGS92] has shown that iter-ative techniques such as Monte-Carlo methods can be used to compute the marginal posterior densities. Our approach is non-Bayesian, and hence doesn X  X  require a prior model. It would be an interesting future research to see how our approach compares with a Bayesian one for the problem of event detection. 8 Acknowledgments The research reported herein has been supported in part by NSF grant no. EHR-9554517 and ARL contract no. DAKFll-98-P-0359. [Att54] [CGS92] [CM981 [Gut741 [GWS98] [Haw76] [HKM+96] [HM73] [Hud66] [Hus93] [KC961 [KS971 [Kue94] [MT961 [MTV95] [MTV97] [PT96] [Raf93] [SA95] [SO941 
