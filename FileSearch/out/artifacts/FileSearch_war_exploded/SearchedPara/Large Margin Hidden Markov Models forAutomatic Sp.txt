 As a result of many years of widespread use, continuous densi ty hidden Markov models (CD-HMMs) are very well matched to current front and back ends for automatic speech recognition (ASR) [21]. Typical front ends compute real-valued feature vectors from the short-time power spec-tra of speech signals. The distributions of these acoustic f eature vectors are modeled by Gaussian mixture models (GMMs), which in turn appear as observation m odels in CD-HMMs. Viterbi de-coding is used to solve the problem of sequential classification in ASR X  X amely, the mapping of sequences of acoustic feature vectors to sequences of phone mes and/or words, which are modeled by state transitions in CD-HMMs.
 The simplest method for parameter estimation in CD-HMMs is t he Expectation-Maximization (EM) algorithm. The EM algorithm is based on maximizing the joint likelihood of observed feature vectors and label sequences. It is widely used due to its simplicity a nd scalability to large data sets, which are common in ASR. A weakness of this approach, however, is th at the model parameters of CD-HMMs are not optimized for sequential classification: in gen eral, maximizing the joint likelihood does not minimize the phoneme or word error rates, which are m ore relevant metrics for ASR. Noting this weakness, many researchers in ASR have studied a lternative frameworks for parame-ter estimation based on conditional maximum likelihood [11 ], minimum classification error [4] and maximum mutual information [20]. The learning algorithms i n these frameworks optimize discrim-inative criteria that more closely track actual error rates, as oppo sed to the EM algorithm for maxi-mum likelihood estimation. These algorithms do not enjoy th e simple update rules and relatively fast convergence of EM, but carefully and skillfully implemente d, they lead to lower error rates [13, 20]. Recently, in a new approach to discriminative acoustic mode ling, we proposed the use of  X  X arge margin GMMs X  for multiway classification [15]. Inspired by s upport vector machines (SVMs), the learning algorithm in large margin GMMs is designed to maxim ize the distance between labeled ex-amples and the decision boundaries that separate different classes [19]. Under mild assumptions, the required optimization is convex, without any spurious loca l minima. In contrast to SVMs, however, large margin GMMs are very naturally suited to problems in mu ltiway (as opposed to binary) clas-sification; also, they do not require the kernel trick for non linear decision boundaries. We showed how to train large margin GMMs as segment-based phonetic cla ssifiers, yielding significantly lower error rates than maximum likelihood GMMs [15]. The integrat ed large margin training of GMMs and transition probabilities in CD-HMMs, however, was left as an open problem.
 We address that problem in this paper, showing how to train la rge margin CD-HMMs in the more general setting of sequential (as opposed to multiway) clas sification. In this setting, the GMMs appear as acoustic models whose likelihoods are integrated over time by Viterbi decoding. Experi-mentally, we find that large margin training of HMMs for seque ntial classification leads to significant improvement beyond the frame-based and segment-based discriminative trainin g in [15]. Our framework for large margin training of CD-HMMs builds on ideas from many previous studies in machine learning and ASR. It has similar motivation as rec ent frameworks for sequential classifi-cation in the machine learning community [1, 6, 17], but diff ers in its focus on the real-valued acous-tic feature representations used in ASR. It has similar moti vation as other discriminative paradigms in ASR [3, 4, 5, 11, 13, 20], but differs in its goal of margin ma ximization and its formulation of the learning problem as a convex optimization over positi ve semidefinite matrices. The recent margin-based approach of [10] is closest in terms of its goal s, but entirely different in its mechanics; moreover, its learning is limited to the mean parameters in G MMs. Before developing large margin HMMs for ASR, we briefly revie w large margin GMMs for multi-way classification [15]. The problem of multiway classificat ion is to map inputs x  X  X  X  d to labels y  X  X  1 , 2 , . . . , C } , where C is the number of classes. Large margin GMMs are trained from a set of labeled examples { ( x n , y n ) } N n =1 . They have many parallels to SVMs, including the goal of marg in maximization and the use of a convex surrogate to the zero-on e loss [19]. Unlike SVMs, where classes are modeled by half-spaces, in large margin GMMs the classes are modeled by collections of ellipsoids. For this reason, they are more naturally suit ed to problems in multiway as opposed to binary classification. Sections 2.1 X 2.3 review the basic fr amework for large margin GMMs: first, the simplest setting in which each class is modeled by a singl e ellipsoid; second, the formulation of the learning problem as a convex optimization; third, the general setting in which each class is modeled by two or more ellipsoids. Section 2.4 presents resu lts on handwritten digit recognition. 2.1 Parameterization of the decision rule The simplest large margin GMMs model each class by a single el lipsoid in the input space. The ellipsoid for class c is parameterized by a centroid vector  X  c  X  X  X  d and a positive semidefinite scalar offset  X  c  X  0 . The decision rule labels an example x  X  X  X  d by the class whose centroid yields the smallest Mahalanobis distance: The decision rule in eq. (1) is merely an alternative way of pa rameterizing the maximum a posterior (MAP) label in traditional GMMs with mean vectors  X  c , covariance matrices  X   X  1 c , and prior class probabilities p c , given by y = argmin c { p c N (  X  c ,  X   X  1 c ) } .
 The argument on the right hand side of the decision rule in eq. (1) is nonlinear in the ellipsoid parameters  X  c and  X  c . As shown in [15], however, a useful reparameterization yie lds a simpler Note that  X  c is positive semidefinite. Furthermore, if  X  c is strictly positive definite, the parameters {  X  c ,  X  c ,  X  c } can be uniquely recovered from  X  c . With this reparameterization, the decision rule in eq. (1) simplifies to: The argument on the right hand side of the decision rule in eq. (3) is linear in the parameters  X  c . In what follows, we will adopt the representation in eq. (3), im plicitly constructing the  X  X ugmented X  vector z for each input vector x . Note that eq. (3) still yields nonlinear (piecewise quadra tic) deci-sion boundaries in the vector z . 2.2 Margin maximization Analogous to learning in SVMs, we find the parameters {  X  c } that minimize the empirical risk on the training data X  X .e., parameters that not only classify t he training data correctly, but also place the decision boundaries as far away as possible. The margin o f a labeled example is defined as its distance to the nearest decision boundary. If possible, eac h labeled example is constrained to lie at least one unit distance away from the decision boundary to ea ch competing class: Fig. 1 illustrates this idea. Note that in the  X  X ealizable X  s etting where these constraints can be simultaneously satisfied, they do not uniquely determine th e parameters {  X  c } , which can be scaled to yield arbitrarily large margins. Therefore, as in SVMs, w e propose a convex optimization that selects the  X  X mallest X  parameters that satisfy the large ma rgin constraints in eq. (4). In this case, the optimization is an instance of semidefinite programming [18 ]: Note that the trace of the matrix  X  c appears in the above objective function, as opposed to the tr ace of the matrix  X  c , as defined in eq. (2); minimizing the former imposes the scal e regularization only on the inverse covariance matrices of the GMM, while the latt er would improperly regularize the mean vectors as well. The constraints  X  c  X  0 restrict the matrices to be positive semidefinite. The objective function must be modified for training data tha t lead to infeasible constraints in eq. (5). As in SVMs, we introduce nonnegative slack variables  X  nc to monitor the amount by which the margin constraints in eq. (4) are violated [15]. The objecti ve function in this setting balances the margin violations versus the scale regularization: where the balancing hyperparameter  X  &gt; 0 is set by some form of cross-validation. This optimization is also an instance of semidefinite programming. 2.3 Softmax margin maximization for multiple mixture compo nents Lastly we review the extension to mixture modeling where eac h class is represented by multiple ellipsoids [15]. Let  X  cm denote the matrix for the m th ellipsoid (or mixture component) in class c . We imagine that each example x n has not only a class label y n , but also a mixture component label m n . Such labels are not provided a priori in the training data, b ut we can generate  X  X roxy X  labels by fitting GMMs to the examples in each class by maximum likelihood estimation, then for each example, computing the mixture component with the high est posterior probability. In the setting where each class is represented by multiple el lipsoids, the goal of learning is to ensure Figure 1: Decision boundary in a large margin GMM: labeled examples lie at least one unit of distance away. where M is the number of mixture components (assumed, for simplicit y, to be the same for each class). We fold these multiple constraints into a single one by appealing to the  X  X oftmax X  inequal-min m z T n  X  cm z n , we replace the M constraints in eq. (7) by the stricter constraint: We will use a similar technique in section 3 to handle the expo nentially many constraints that arise vice versa. Also, though nonlinear in the matrices {  X  cm } , the constraint in eq. (8) is still convex. The objective function in eq. (6) extends straightforwardl y to this setting. It balances a regularizing term that sums over ellipsoids versus a penalty term that sum s over slack variables, one for each constraint in eq. (8). The optimization is given by: This optimization is not an instance of semidefinite program ming, but it is convex. We discuss how to perform the optimization efficiently for large data sets i n appendix A. 2.4 Handwritten digit recognition We trained large margin GMMs for multiway classification of M NIST handwritten digits [8]. The MNIST data set has 60000 training examples and 10000 test exa mples. Table 1 shows that the large margin GMMs yielded significantly lower test error rates tha n GMMs trained by maximum likeli-hood estimation. Our best results are comparable to the best SVM results (1.0-1.4%) on deskewed images [8] that do not make use of prior knowledge. For our bes t model, with four mixture compo-nents per digit class, the core training optimization over a ll training examples took five minutes on a PC. (Multiple runs of this optimization on smaller validati on sets, however, were also required to set two hyperparameters: the regularizer for model complexity , and the termination criterion for early stopping.) In this section, we extend the framework in the previous sect ion from multiway classification to sequential classification. Particularly, we have in mind th e application to ASR, where GMMs are used to parameterize the emission densities of CD-HMMs. Str ictly speaking, the GMMs in our framework cannot be interpreted as emission densities beca use their parameters are not constrained to represent normalized distributions. Such an interpreta tion, however, is not necessary for their use as discriminative models. In sequential classification by C D-HMMs, the goal is to infer the correct In the application to ASR, the hidden states correspond to ph oneme labels, and the observations are acoustic feature vectors. Note that if an observation seque nce has length T and each label can belong to C classes, then the number of incorrect state sequences grows as O ( C T ) . This combinatorial explosion presents the main challenge for large margin meth ods in sequential classification: how to separate the correct hidden state sequence from the exponen tially large number of incorrect ones. The section is organized as follows. Section 3.1 explains th e way that margins are computed for se-quential classification. Section 3.2 describes our algorit hm for large margin training of CD-HMMs. Details are given only for the simple case where the observat ions in each hidden state are modeled by a single ellipsoid. The extension to multiple mixture com ponents closely follows the approach in section 2.3 and can be found in [14, 16]. Margin-based lear ning of transition probabilities is likewise straightforward but omitted for brevity. Both the se extensions were implemented, however, for the experiments on phonetic recognition in section 3.3. 3.1 Margin constraints for sequential classification We start by defining a discriminant function over state (labe l) sequences of the CD-HMM. Let a ( i, j ) denote the transition probabilities of the CD-HMM, and let  X  s denote the ellipsoid pa-rameters of state s . The discriminant function D ( X , s ) computes the score of the state sequence s = [ s 1 , s 2 , . . . , s T ] on an observation sequence X = [ x 1 , x 2 , . . . , x T ] as: This score has the same form as the log-probability log P ( X , s ) in a CD-HMM with Gaussian emis-sion densities. The first term accumulates the log-transiti on probabilities along the state sequence, while the second term accumulates  X  X coustic scores X  comput ed as the Mahalanobis distances to each state X  X  centroid. In the setting where each state is mod eled by multiple mixture components, the acoustic scores from individual Mahalanobis distances are replaced with  X  X oftmax X  distances of We introduce margin constraints in terms of the above discri minant function. Let H ( s , y ) denote the Hamming distance (i.e., the number of mismatched labels ) between an arbitrary state sequence s and the target state sequence y . Earlier, in section 2 on multiway classification, we constr ained each labeled example to lie at least one unit distance from the dec ision boundary to each competing class; see eq. (4). Here, by extension, we constrain the score of eac h target sequence to exceed that of each competing sequence by an amount equal to or greater than the H amming distance: Intuitively, eq. (11) requires that the (log-likelihood) g ap between the score of an incorrect se-quence s and the target sequence y should grow in proportion to the number of individual label errors. The appropriateness of such proportional constrai nts for sequential classification was first noted by [17]. 3.2 Softmax margin maximization for sequential classificat ion The challenge of large margin sequence classification lies i n the exponentially large number of constraints, one for each incorrect sequence s , embodied by eq. (11). We will use the same softmax inequality, previously introduced in section 2.3, to fold t hese multiple constraints into one, thus considerably simplifying the optimization required for pa rameter estimation. We first rewrite the constraint in eq. (11) as: We obtain a more manageable constraint by substituting a sof tmax upper bound for the max term and requiring that the inequality still hold: Note that eq. (13) implies eq. (12) but not vice versa. As in th e setting for multiway classification, the objective function for sequential classification balan ces two terms: one regularizing the scale of the GMM parameters, the other penalizing margin violations . Denoting the training sequences by { X n , y n } N n =1 and the slack variables (one for each training sequence) by  X  n  X  0 , we obtain the following convex optimization: It is worth emphasizing several crucial differences betwee n this optimization and previous ones [4, 11, 20] for discriminative training of CD-HMMs for ASR. Firs t, the softmax large margin constraint in eq. (13) is a differentiable function of the model paramet ers, as opposed to the  X  X ard X  maximum in eq. (12) and the number of classification errors in the MCE t raining criteria [4]. The constraint and its gradients with respect to GMM parameters  X  cm and transition parameters a ( , ) can be computed efficiently using dynamic programming, by a variant of the st andard forward-backward procedure in HMMs [14]. Second, due to the reparameterization in eq. (2), the discriminant function D ( X n , y n ) and the softmax function are convex in the model parameters. Therefore, the optimization eq. (14) can be cast as convex optimization, avoiding spurious local minima [14]. Third, the optimization not only increases the log-likelihood gap between correct and i ncorrect state sequences, but also drives the gap to grow in proportion to the number of individually in correct labels (which we believe leads to more robust generalization). Finally, compared to the la rge margin framework in [17], the softmax handling of exponentially large number of margin constrain ts makes it possible to train on larger data sets. We discuss how to perform the optimization efficiently in appendix A. 3.3 Phoneme recognition We used the TIMIT speech corpus [7, 9, 12] to perform experime nts in phonetic recognition. We followed standard practices in preparing the training, dev elopment, and test data. Our signal pro-cessing front-end computed 39-dimensional acoustic featu re vectors from 13 mel-frequency cepstral coefficients and their first and second temporal derivatives . In total, the training utterances gave rise to roughly 1.2 million frames, all of which were used in train ing.
 We trained baseline maximum likelihood recognizers and two different types of large margin recog-nizers. The large margin recognizers in the first group were  X  low-cost X  discriminative CD-HMMs whose GMMs were merely trained for frame-based classificati on. In particular, these GMMs were estimated by solving the optimization in eq. (8), then subst ituted into first-order CD-HMMs for se-quence decoding. The large margin recognizers in the second group were fully trained for sequential classification. In particular, their CD-HMMs were estimate d by solving the optimization in eq. (14), generalized to multiple mixture components and adaptive tr ansition parameters [14, 16]. In all the recognizers, the acoustic feature vectors were labeled by 4 8 phonetic classes, each represented by one state in a first-order CD-HMM.
 For each recognizer, we compared the phonetic state sequenc es obtained by Viterbi decoding to the  X  X round-truth X  phonetic transcriptions provided by the TI MIT corpus. For the purpose of comput-ing error rates, we followed standard conventions in mappin g the 48 phonetic state labels down to 39 broader phone categories. We computed two different type s of phone error rates, one based on Hamming distance, the other based on edit distance. The form er was computed simply from the percentage of mismatches at the level of individual frames. The latter was computed by aligning the Viterbi and ground truth transcriptions using dynamic prog ramming [9] and summing the substitu-tion, deletion, and insertion error rates from the alignmen t process. The  X  X rame-based X  phone error rate computed from Hamming distances is more closely tracke d by our objective function for large margin training, while the  X  X tring-based X  phone error rate computed from edit distances provides a more relevant metric for ASR.
 Tables 2 and 3 show the results of our experiments. For both ty pes of error rates, and across all model sizes, the best performance was consistently obtaine d by large margin CD-HMMs trained for sequential classification. Moreover, among the two diff erent types of large margin recognizers, utterance-based training generally yielded significant im provement over frame-based training. Discriminative learning of CD-HMMs is an active research ar ea in ASR. Two types of algorithms have been widely used: maximum mutual information (MMI) [20 ] and minimum classification er-mixture baseline margin margin (per state) (EM) (frame) (utterance) Table 2: Frame-based phone error rates, from Hamming distance, of different recognizers.
 See text for details. ror [4]. In [16], we compare the large margin training propos ed in this paper to both MMI and MCE systems for phoneme recognition trained on the exact same ac oustic features. There we find that the large margin approach leads to lower error rates, owing perh aps to the absence of local minima in the objective function and/or the use of margin constraints based on Hamming distances. Discriminative learning of sequential models is an active a rea of research in both ASR [10, 13, 20] and machine learning [1, 6, 17]. This paper makes contributi ons to lines of work in both commu-nities. First, in distinction to previous work in ASR, we hav e proposed a convex, margin-based cost function that penalizes incorrect decodings in propor tion to their Hamming distance from the desired transcription. The use of the Hamming distance in th is context is a crucial insight from the work of [17] in the machine learning community, and it differ s profoundly from merely penalizing the log-likelihood gap between incorrect and correct trans criptions, as commonly done in ASR. Second, in distinction to previous work in machine learning , we have proposed a framework for se-quential classification that naturally integrates with the infrastructure of modern speech recognizers. Using the softmax function, we have also proposed a novel way to monitor the exponentially many margin constraints that arise in sequential classification . For real-valued observation sequences, we have shown how to train large margin HMMs via convex optimiza tions over their parameter space of positive semidefinite matrices. Finally, we have demonstra ted that these learning algorithms lead to improved sequential classification on data sets with over on e million training examples (i.e., phonet-ically labeled frames of speech). In ongoing work, we are app lying our approach to large vocabulary ASR and other tasks such as speaker identification and visual object recognition.
 The optimizations in eqs. (5), (6), (9) and (14) are convex: s pecifically, in terms of the matrices that parameterize large margin GMMs and HMMs, the objective functions are linear, while the con-straints define convex sets. Despite being convex, however, these optimizations cannot be managed by off-the-shelf numerical optimization solvers or generi c interior point methods for problems as large as the ones in this paper. We devised our own special-pu rpose solver for these purposes. For simplicity, we describe our solver for the optimization of eq. (6), noting that it is easily extended to eqs. (9) and (14). To begin, we eliminate the slack variabl es and rewrite the objective function in terms of the hinge loss function: hinge( z )=max(0 , z ) . This yields the objective function: which is convex in terms of the positive semidefinite matrice s  X  c . We minimize L using a projected subgradient method [2], taking steps along the subgradient of L , then projecting the matrices {  X  c } back onto the set of positive semidefinite matrices after eac h update. This method is guaranteed to converge to the global minimum, though it typically converg es very slowly. For faster convergence, we precede this method with an unconstrained conjugate grad ient optimization in the square-root converges to an excellent starting point for the projected s ubgradient method.
 This work was supported by the National Science Foundation u nder grant Number 0238323. We thank F. Pereira, K. Crammer, and S. Roweis for useful discus sions and correspondence. Part of this work was conducted while both authors were affiliated with th e University of Pennsylvania.
