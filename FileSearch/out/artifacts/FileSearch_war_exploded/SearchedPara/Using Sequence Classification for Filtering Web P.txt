 Web pages often contain text that is irrelevant to their main content, such as advertisements, generic format elements, and references to other pages on th e same site. When used by automatic content-processing syst ems, e.g., for Web indexing, text classification, or information extraction, this irrelevant text often produces substantial amount of noise. This paper describes a trainable filtering system based on a feature-rich sequence classifier that removes irrele vant parts from pages, while keeping the content intact. Most of the features the system uses are purely form-related: HTML ta gs and their positions, sizes of elements, etc. This keeps the system general and domain-independent. We also experiment with content words and show that while they perform very poorly alone, they can slightly improve the performance of pur e-form features, without jeopardizing the domain-indepe ndence. Our system achieves very high accuracy (95% and above) on several collections of Web pages. We also do a series of tests with different features and different classifiers, compar ing the contribution of different components to the system pe rformance, and comparing two known sequence classifiers, R obust Risk Minimization (RRM) and Conditional Random Fields (CRF), in a novel setting. I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  text analysis. I.7.5 [ Document and Text Processing ]: Document Capture  X  document analysis Algorithms, Management, Experi mentation, Documentation. Text mining, Web page cleani ng, sequence classification. The task of removing irrelevant text from Web pages is becoming important as ever in creasing numbers of automatic content-processing systems, including both general and specialized search engines pro cess web pages. Many Web pages contain large amounts of text that one does not want to include when searching or extracting information. In addition to advertisements, many pages have a variety of headers, footers and sidebars, which are often not relevant to the main subject of the text. Inclusion of text from these blocks can be highly detrimental to automatic indexing methods. Depending on the domain, we find that roughly 20% to 45% of the text is irrelevant. Very often, many of the prominently mentioned words or topics, including headlines and short excerpts from other stories, and  X  X oolbox X  insets with in the page are irrelevant and should be removed for the purposes of indexing. People have written software that attempts to recognize the templates used in the construction of the page based on the document structure. Those template s can then be used to remove substantial amounts of the  X  X oile rplate X  material contained on these pages. However, the vast number of different formats used on different web pages makes it prohibitive to manually build such template-recognizers; machine learning must be used. Alternatively, one can try to classify individual text blocks as being indexable content or  X  X ois e X  based on their content (as with a spam filter). This, too, is hard to do in a domain-independent fashion In this paper, we build and analyze a system that preprocesses HTML pages, simplifying their stru cture, and then learns domain-independent models to filter text blocks. Domain independence is a key feature for web page cleaning; since one can neither build by hand nor manually annotate page s on all possible subjects, one can only use form-related features or very general language features. We suggest a set of modeling decisions, each of which contribute to overall system performance, which, when combined, give over 95% accuracy (F1) across a wide range of topics and web sites. Key to the performance are (1) a straightforward algorithm for simplifying the DOM (Document Object Model) tree of the document, (2) a set of features on the text blocks of the DOM tree, (3) the training of sequential classification models (e.g., a CRF) on labeled examples and (4) use of domain-independent words that occur in the text blocks. Taken t ogether, these contributions lead to superior performance, and allo w a system trained on web pages from one subject area to work equally well on other domains and web sites. We also demonstrate that utilization of automatically generated site-specific templates does not significantly improve the performance of our system, which shows that our classification method and the feature set are sufficiently rich to capture the clues produced by such templates without the necessity to download and analyze a large number of pages from the same sites as the pages being processed. We present two main results. The first, in table 1, shows that models trained on one set of topics cleans well for widely different topics. The second table shows the e ffect of using different feature sets. The datasets in our experime nts consist of HTML files, downloaded from the Web and manua lly labeled. We use five datasets, one domain-independent and four domain-specific ones. All five were created in a similar manner: a query was issued to the Google search engine and the first fifty of the returned pages that had a clear text content were collected. We discarded pages that did not have a well-defined text content, since the task of filtering is undefined for such page s. We also used at most one page from each site, in order for the results not to be dependent on site-specific elements. The general dataset was generated from the query  X  the  X , producing pages on a variety of topics. The three domain-specific datasets were generated from the queries  X  game theory  X ,  X  general relativity  X ,  X  marathon training  X , and  X  subprime lending  X . (The latter is not shown due to space limitations) The five datasets described above are used separately only in the experiments testing dom ain dependence. In the other experiments they are merged into a single combined dataset, containing 250 instances. Table 1 shows the performan ce of two baselines  X  a Zero baseline , in which all text blocks are assu med to be  X  X ontent X , and the hand-coded Baseline (removing blocks that are too short or have too many links, as described above), and our best performing method, CRF with the  X  X ull X  feature set trained on each of the five sets of HTML pages and tested on each of the same five data sets. We + Precision  X 1 ), which calculates a single measure that is a balance between recall and precision 
Training 
Zero baseline 74.4 85.3 84.1 81.1 Baseline 80.8 86.9 86.8 85.7 General 93.4* 95.7 96.4 92.3 Game Theory 92.6 94.6* 95.8 91.4 General Relativity 93.3 92.5 94.7* 92.7 Marathon 93.6 94.1 92.7 93.4*
Table 1. Cross-domain performance of our system. The values marked by (*) were produced by leave-one-out cross validation. Most of the features used by the feature extractor simply test for the presence of natural contextual features. Two groups of features check heuristics  X  that the text blocks should be sufficiently large and that they should not contain too many links. In order to investigate the depe ndence of the system performance on the features, we used several feature set variants in our experiments: Table 2 shows the effect of using different sets of each of these features on the recall, precision, and F1 scores.
 Feature Set Recall Prec F1 Zero baseline 100.0 66.6 80.0  X  1.9 Baseline 96.1 75.3 84.4  X  1.6 Unpreprocessed 97.2 89.8 93.4  X  1.3 Flat 96.5 90.8 93.6  X  1.1 Pure form 96.4 90.7 93.5  X  1.4 Pure content 84.6 85.9 85.2  X  2.4 Full 97.2 93.8 95.5  X  1.4 Full + site-specific 97.9 93.5 95.7  X  1.5 Full w/out positive words 97.0 93.8 95.4  X  1.6 We have developed a system for removing text blocks that are irrelevant to the main content of a web page. The method works well (test set F1 score of over 95%) and generalizes extremely well to new sets of web pages on different domains with vastly different content. For this task , sequential classification methods such as Robust Risk Minimization (RRM), and conditional random fields (CRFs) work much better than direct classification methods like SVMs, which sugge sts that HTML pages have essentially local structure. Using a simplified DOM tree improves the performance by six percentage points, using features based on form rather than content (words ) made nine percentage points difference, adding content to fo rm gave an addition two points benefit, using features draw n from neighboring DOM nodes gave an additional point, and adding site specific information only adds a further fraction of a percentage point. Since the final model has very large percentages reduction in the small remaining error. Our work demonstrates the feasibility of learning web page cleaning methods that can be trained once on a small number of labeled examples, but are still domain and topic-independent. Our method works well even in the absence of the use of any words as features; hence it could easily be applied to languages for which no training data exists. We also not e that if one is using words as features, the words indicative of noise are largely domain independent, while the words indicative of content pages are a mix of domain independent (e.g.,  X  a X ,  X  X hat X ,  X  X oday X ) and domain dependent words. 
