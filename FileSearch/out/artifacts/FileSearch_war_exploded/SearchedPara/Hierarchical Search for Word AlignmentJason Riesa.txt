 Automatic word alignment is generally accepted as a first step in training any statistical machine translation system. It is a vital prerequisite for generating translation tables, phrase tables, or syn-tactic transformation rules. Generative alignment models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely un-supervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system.

Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing ac-curate alignments. How can we take advantage of all of this data at our fingertips? Using fea-ture functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually di ffi cult to extend a given genera-tive model with feature functions without chang-ing the entire generative story. This di ffi culty Figure 1: Model-4 alignment vs. a gold stan-dard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Itty-cheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Lacoste-Julien et al., 2006; Moore et al., 2006).

We present in this paper a discriminative align-ment model trained on relatively little data, with a simple, yet powerful hierarchical search proce-dure. We borrow ideas from both k -best pars-ing (Klein and Manning, 2001; Huang and Chi-ang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment.

Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of pos-sible alignments. Our algorithm yields a forest of Hypotheses at the root node imply full alignment structures. word alignments, from which we can e ffi ciently extract the k -best. We handle an arbitrary number of features, compute them e ffi ciently, and score alignments using a linear model. We train the parameters of the model using averaged percep-tron (Collins, 2002) modified for structured out-puts, but can easily fit into a max-margin or related framework. Finally, we use relatively little train-ing data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. Algorithm input The input to our alignment al-gorithm is a sentence-pair ( e n 1 , f m 1 ) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = e n 1 , let T be its syntactic parse. To gener-ate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree.
 Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score. u u u  X  u 1 , u 2  X  . In this example, k cost.
 Each partial alignment comprises the columns of the alignment matrix for the e -words spanned by v , and each is scored by a linear combination of feature functions. See Figure 2 for a small exam-ple.

Initial partial alignments are enumerated and scored at preterminal nodes, each spanning a sin-gle column of the word alignment matrix. To speed up search, we can prune at each node, keep-ing a beam of size k . In the diagram depicted in Figure 2, the beam is size k = 5.

From here, we traverse the tree nodes bottom-up, combining partial alignments from child nodes until we have constructed a single full alignment at the root node of the tree. If we are interested in the k -best, we continue to populate the root node until we have k alignments. 1
We use one set of feature functions for preter-minal nodes, and another set for nonterminal nodes. This is analogous to local and nonlo-cal feature functions for parse-reranking used by Huang (2008). Using nonlocal features at a non-terminal node emits a combination cost for com-posing a set of child partial alignments.

Because combination costs come into play, we use cube pruning (Chiang, 2007) to approxi-mate the k -best combinations at some nonterminal node v . Inference is exact when only local features are used.
 Assumptions There are certain assumptions re-lated to our search algorithm that we must make: (1) that using the structure of 1-best English syn-tactic parse trees is a reasonable way to frame and drive our search, and (2) that F-measure approxi-mately decomposes over hyperedges.

We perform an oracle experiment to validate these assumptions. We find the oracle for a given ( T , e , f ) triple by proceeding through our search al-gorithm, forcing ourselves to always select correct links with respect to the gold alignment when pos-sible, breaking ties arbitrarily. The the F 1 score of our oracle alignment is 98 . 8%, given this  X  X erfect X  model. 2.1 Hierarchical search Initial alignments We can construct a word alignment hierarchically, bottom-up, by making use of the structure inherent in syntactic parse trees. We can think of building a word alignment as filling in an M  X  N matrix (Figure 1), and we be-gin by visiting each preterminal node in the tree. Each of these nodes spans a single e word. (Line 2 in Algorithm 1).

From here we can assign links from each e word to zero or more f words (Lines 6 X 14). At this level of the tree the span size is 1, and the par-tial alignment we have made spans a single col-umn of the matrix. We can make many such partial alignments depending on the links selected. Lines 5 through 9 of Algorithm 1 enumerate either the null alignment, single-link alignments, or two-link alignments. Each partial alignment is scored and stored in a sorted heap (Lines 9 and 13).

In practice enumerating all two-link alignments can be prohibitive for long sentence pairs; we set a practical limit and score only pairwise combina-Algorithm 1: Hypergraph Alignment 1 function A  X  X  X  X  X  ( e n 1 , f m 1 , T ) 2 for v  X  T in bottom-up order do 4 if  X  X  X  -P  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  N  X  X  X  X  ( v ) then 5 i  X  index-of( v ) 6 for j 7 links  X  ( i , j ) 8 score  X  w  X  h ( links , v , e n 1 , f m 1 ) 9 P  X  X  X  X  (  X  v ,  X  score , links  X  , k ) 11 links  X  ( i , j ) , ( i , k ) 12 score  X  w  X  h ( links , v , e n 1 , f m 1 ) 13 P  X  X  X  X  (  X  v ,  X  score , links  X  , k ) 17  X  v  X  G  X  X  X  X  S  X  X  X  X  (children( v ) , k ) 21 function G  X  X  X  X  S  X  X  X  X  (  X  u 1 , u 2  X  , k ) 22 return C  X  X  X  X  P  X  X  X  X  X  X  X  X  (  X   X  u 1 , X  u 2  X  , k , w , h ) tions of the top n = max n | f | 2 , 10 o scoring single-link alignments.

We limit the number of total partial alignments  X  v kept at each node to k . If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst o ff the heap and replace it with our new partial alignment if its score is better than the current worst. Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. At each nonterminal node v we wish to combine the partial alignments of its children u 1 ,..., u c . We use cube pruning (Chiang, 2007; Huang and Chi-ang, 2007) to select the k -best combinations of the partial alignments of u 1 ,..., u c (Line 19). Note Sentence 1 Figure 4: Correct version of Figure 1 after hyper-graph alignment. Subscripts on the nonterminal labels denote the branch containing the head word for that span. that Algorithm 1 assumes a binary tree 2 , but is not necessary. In the general case, cube pruning will operate on a d -dimensional hypercube, where d is the branching factor of node v .

We cannot enumerate and score every possibil-ity; without the cube pruning approximation, we will have k c possible combinations at each node, exploding the search space exponentially. Figure 3 depicts how we select the top-k alignments at a node v from its children  X  u 1 , u 2  X  . We incorporate all our new features into a linear model and learn weights for each using the on-line averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs in-spired by Chiang et al. (2008). We define: Figure 5: A common problem with GIZA ++ Model 4 alignments is a weak distortion model. The second English  X  X n X  is aligned to the wrong Arabic token. Circles show the gold alignment. where ` ( y i , y ) is a loss function describing how bad it is to guess y when the correct answer is y i . In our oracle alignment according to: where  X  X  X  X  X  ( x ) is a set of hypothesis alignments generated from input x . Instead of the traditional oracle, which is calculated solely with respect to the loss ` ( y i , y ), we choose the oracle that jointly minimizes the loss and the di ff erence in model score to the true alignment. Note that Equation 2 is equivalent to maximizing the sum of the F-measure and model score of y :
Let  X  y be the 1-best alignment according to our model: Then, at each iteration our weight update is: where  X  is a learning rate parameter. 3 We find that this more conservative update gives rise to a much more stable search. After each iteration, we expect y + to get closer and closer to the true y i . Our simple, flexible linear model makes it easy to throw in many features, mapping a given complex alignment structure into a single high-dimensional feature vector. Our hierarchical search framework allows us to compute these features when needed, and a ff ords us extra useful syntactic information.
We use two classes of features: local and non-local . Huang (2008) defines a feature h to be lo-cal if and only if it can be factored among the lo-cal productions in a tree, and non-local otherwise. Analogously for alignments, our class of local fea-tures are those that can be factored among the local partial alignments competing to comprise a larger span of the matrix, and non-local otherwise. These features score a set of links and the words con-nected by them.
 Feature development Our features are inspired by analysis of patterns contained among our gold alignment data and automatically generated parse trees. We use both local lexical and nonlocal struc-tural features as described below. 4.1 Local features These features fire on single-column spans.  X  From the output of GIZA ++ Model 4, we  X  Based on these features, we include a binary head of it X  X  sister NP.
 Table 1: A sampling of learned weights for the lex-ical zero feature. Negative weights penalize links never seen before in a baseline alignment used to initialize lexical p ( e | f ) and p ( f | e ) tables. Posi-tive weights outright reward such links.  X  We also include a measure of distortion .  X  As a lexical backo ff , we include a tag prob- X  In cases where the lexical probabilities are  X  We find that binary identity and Additionally, we include fine-grained versions of the lexical probability, fertility, and distortion fea-tures. These fire for for each link ( e , f ) and part-of-speech tag. That is, we learn a separate weight for each feature for each part-of-speech tag in our data. Given the tag of e , this a ff ords the model the ability to pay more or less attention to the features described above depending on the tag given to e . Arabic-English specific features We describe here language specific features we implement to exploit shallow Arabic morphology. Figure 7: This figure depicts the tree / alignment structure for which the feature PP-from-prep fires. The English preposition  X  X rom X  is aligned to Arabic word of the sister NP are aligned to words following English preposition structure commonly matches that of Arabic in our gold data. This family of fea-tures captures these observations.  X  We observe the Arabic prefix  X  We also include analogous feature functions 4.2 Nonlocal features These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments to create a larger span. Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang (2008).  X  Features PP-NP-head , NP-DT-head , and  X  Local lexical preference features compete  X  Finally, we have a tree-distance feature to Recent work has shown the potential for syntac-tic information encoded in various ways to sup-port inference of superior word alignments. Very recent work in word alignment has also started to report downstream e ff ects on B LEU score.

Cherry and Lin (2006) introduce soft syntac-tic ITG (Wu, 1997) constraints into a discrimi-native model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, show-ing B LEU improvement for a hierarchical phrase-based MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically mo-tivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style ap-proaches, our model can generate arbitrary align-ments and learn from arbitrary gold alignments.
DeNero and Klein (2007) refine the distor-tion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA ++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical informa-tion.

Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. We evaluate our model and and resulting align-ments on Arabic-English data against those in-duced by IBM Model-4 using GIZA ++ (Och and Ney, 2003) with both the union and grow-diag-final heuristics. We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training. We also align the test data using GIZA ++ 5 along with 50 million words of English. Figure 8: Learning curves for 10 random restarts over time for parallel averaged perceptron train-ing. These plots show the current F-measure on the training set as time passes. Perceptron training here is quite stable, converging to the same general neighborhood each time. Figure 9: Model robustness to the initial align-ments from which the p ( e | f ) and p ( f | e ) features are derived. The dotted line indicates the baseline accuracy of GIZA ++ Model 4 alone. 6.1 Alignment Quality We empirically choose our beam size k from the results of a series of experiments, setting k = 1, 2, 4, 8, 16, 32, and 64. We find setting k = 16 to yield the highest accuracy on our held-out test data. Us-ing wider beams results in higher F-measure on training data, but those gains do not translate into higher accuracy on held-out data.

The first three columns of Table 2 show the balanced F-measure, Precision, and Recall of our alignments versus the two GIZA ++ Model-4 base-lines. We report an F-measure 8.6 points over Model-4 union, and 6.3 points over Model-4 grow-diag-final. Table 2: F-measure, Precision, Recall, the resulting B
LEU score, and number of unknown words on a held-out test corpus for three types of alignments. B
LEU scores are case-insensitive IBM B LEU . We significant at the p &lt; 0 . 01 level.

Figure 8 shows the stability of the search proce-dure over ten random restarts of parallel averaged perceptron training with 40 CPUs. Training ex-amples are randomized at each epoch, leading to slight variations in learning curves over time but all converge into the same general neighborhood.
Figure 9 shows the robustness of the model to initial alignments used to derive lexical features p ( e | f ) and p ( f | e ). In addition to IBM Model 4, we experiment with alignments from Model 1 and the HMM model. In each case, we significantly outperform the baseline GIZA ++ Model 4 align-ments on a heldout test set. 6.2 MT Experiments We align a corpus of 50 million words with GIZA ++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of trans-lation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results be-low.

We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from ( e -tree, f -string, align-ment) triples as in (Galley et al., 2004; Galley et al., 2006).

We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 mil-lion English words. We tune the the parameters of the MT system on a held-out development cor-pus of 1,172 parallel sentences, and test on a held-out parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data.

Columns 4 and 5 in Table 2 show the results of our MT experiments. Our hypergraph align-ment algorithm allows us a 1.1 B LEU increase over the best baseline system, Model-4 grow-diag-final. This is statistically significant at the p &lt; 0 . 01 level. We also report a 2.4 B LEU increase over a system trained with alignments from Model-4 union. We have opened up the word alignment task to advances in hypergraph algorithms currently used in parsing and machine translation decoding. We treat word alignment as a parsing problem, and by taking advantage of English syntax and the hy-pergraph structure of our search algorithm, we re-port significant increases in both F-measure and B
LEU score over standard baselines in use by most state-of-the-art MT systems today.
 We would like to thank our colleagues in the Nat-ural Language Group at ISI for many meaningful discussions and the anonymous reviewers for their thoughtful suggestions. This research was sup-ported by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies, and a USC CREATE Fellowship to the first author.

