 Paul Weng 4 paul.weng@lip6.fr Weiwei Cheng 1 cheng@mathematik.uni-marburg.de Eyke H  X ullermeier 1 eyke@mathematik.uni-marburg.de Consider the problem of selecting the best  X  out of K random variables with high probability on the basis of finite samples, assuming that random vari-ables are ranked based on their expected value. A natural way of approaching this problem is to apply an adaptive sampling strategy, called racing algo-rithm , which makes use of confidence intervals de-rived from the concentration property of the mean estimate ( Hoe ff ding , 1963 ). This formal setup was first considered by Maron &amp; Moore ( 1994 ) and is now used in many practical applications, such as model selection ( Maron &amp; Moore , 1997 ), large-scale learning ( Mnih et al. , 2008 ) and policy search in MDPs ( Heidrich-Meisner &amp; Igel , 2009 ). Motivated by recent work on learning from qualita-tive or implicit feedback, including preference learn-preference-based reinforcement learning in particu-lar ( Akrour et al. , 2011 ; Cheng et al. , 2011 ), we in-troduce and analyze a preference-based generaliza-tion of the value-based setting of the above selection problem, subsequently denoted TKS (short for Top-k Selection) problem: Instead of assuming that the decision alternatives or options O = { o 1 ,...,o K } are characterized by real values (namely expecta-tions of random variables) and that samples provide information about these values, we only assume that the options can be compared in a pairwise manner. Thus, a sample essentially informs about pairwise preferences, i.e., whether or not an option o i might be preferred to another one o j (written o i o j ). An important observation is that, in this setting, the original goal of finding the top- X  options is no longer well-defined, simply because pairwise comparisons can be cyclic. Therefore, to make the specification of our problem complete, we add a ranking procedure that turns a pairwise preference relation into a com-plete preorder of the options O . The goal is then to find the top- X  options according to that order. More concretely, we consider Copeland X  X  ranking (binary voting), the sum of expectations (weighted voting) and the random walk ranking (PageRank) as target rankings . For each of these ranking models, we de-vise proper sampling strategies that constitute the core of our preference-based racing algorithm. After detailing the problem setting in Section 2, we introduce a general preference-based racing algo-rithm in Section 3 and analyze sampling strategies for di ff erent ranking methods in Section 4. In Sec-tion 5, a first experimental study with sports data is presented, and in Section 6, we consider a special case of our setting that is close to the original value-based one. Related work is discussed in Section 7. In this section, we first recapitulate the original value-based setting of the TKS problem and then introduce our preference-based generalization. 2.1. Value-based TKS Consider a set of decision alternatives or options O = { o 1 ,...,o K } , where each option o i is associated with a random variable X i . Let F 1 ,...,F K denote the (unknown) distribution functions of X 1 ,...,X K , respectively, and  X  i = xdF i ( x ) the corresponding expected values (supposed to be finite).
 The TKS task consists of selecting, with a predefined confidence 1  X   X  , the  X  &lt;K options with highest ex-pectations. In other words, one seeks an index set I  X  [ K ]= { 1 ,...,K } of cardinality  X  maximizing ing optimization problem: where I { X } is the indicator function which is 1 if its argument is true and 0 otherwise. This selection problem must be solved on the basis of random sam-ples drawn from X 1 ,...,X K . 2.2. Preference-based TKS Our point of departure is pairwise preferences over the set O of options. In the most general case, one typically allows four possible outcomes of a sin-gle pairwise comparison between o i and o j , namely (strict) preference for o i , (strict) preference for o j indi ff erence and incomparability. They are denoted To make ranking procedures applicable, these pair-wise outcomes need to be turned into numerical scores. We consider the outcome of a comparison between o i and o j as a random variable Y i,j which assumes the value 1 if o i o j , 0 if o i  X  o j , and 1 / otherwise. Thus, indi ff erence and incomparability are handled in the same way, namely by giving half a point to both options. Essentially, this means that these outcomes are treated in a neutral way. Based on a set of realizations { y 1 assumed to be independent, the expected value y i,j = E [ Y i,j ] of Y i,j can be estimated by the mean A ranking procedure A (concrete choices of A will be discussed in the next section) produces a complete preorder A of the options O on the basis of the relation Y =[ y i,j ] to ( 1 ), our preference-based TKS task can then be defined as selecting a subset I  X  [ K ] such that specifically, the optimality of the selected subset should be guaranteed with probability at least 1  X   X  . 2.3. Ranking Procedures In the following, we introduce three instantiations of the ranking procedure A , starting with Copeland X  X  ranking (CO); it is defined as follows ( Moulin , 1988 ): o  X  CO o j if and only if d i &lt;d j , where d i =# { k  X  [
K ] | 1 / 2 &lt;y i,k } . The interpretation of this relation is very simple: An option o i is preferred to o j when-ever o i  X  X eats X  more options than o j does. The sum of expectations (SE) ranking is a  X  X oft X  version of CO: o i  X  SE o j if and only if The idea of the random walk ( RW ) ranking is to handle the matrix Y as a transition matrix of a Markov chain and order the options based on its sta-tionary distribution. More precisely, RW first trans-forms Y into the stochastic matrix S =[ s i,j ] stationary distribution ( v 1 ,...,v K ) for this matrix (i.e., the eigenvector corresponding to the largest eigenvalue 1). Finally, the options are sorted ac-cording to these probabilities: o i  X  RW o j i ff v i &lt;v The RW ranking is directly motivated by the PageR-ank algorithm ( Brin &amp; Page , 1998 ), which has been well studied in social choice theory ( Altman &amp; Ten-nenholtz , 2008 ; Brandt &amp; Fischer , 2007 ) and rank aggregation ( Negahban et al. , 2012 ), and which is 2 Algorithm 1 PBR ( Y 1 , 1 ,...,Y K,K ,  X  ,n max ,  X  ) 1: B = D =  X  Set of selected and discarded 2: A = { ( i, j ) | i = j, 1  X  i, j  X  K } 3: Set of all pairs of options still racing 4: for i, j =1  X  K do n i,j =0 Initialization 5: while (  X  i  X  j, ( n i,j  X  n max ))  X  ( | A | &gt; 0) do 6: for all ( i, j )  X  A do 9: Update  X  Y =[  X  y i,j ] K  X  K with the new samples 10: according to ( 2 ) 11: for i, j =1  X  K do 12: Update confidence bounds, C , U , L 14: Hoe ff ding bound 16: ( A, B )= SSCO ( A,  X  Y , K,  X  , U , L ) 17: Sampling strategy for  X  CO 18: ( A, B, D )= SSSE ( A,  X  Y , K,  X  , U , L ,D ) 19: Sampling strategy for  X  SE 20: ( A, B )= SSRW (  X  Y , K,  X  , C ) 21: Sampling strategy for  X  RW 22: return B widely used in many application fields ( Brin &amp; Page , 1998 ; Kocsor et al. , 2008 ). The original racing algorithm for the value-based TKS problem is an iterative sampling method. In each iteration, it either selects a subset of options to be sampled, or it terminates and returns a  X  -sized subset of options as a (probable) solution to ( 1 ). In this section, we introduce a general preference-based racing ( PBR ) algorithm that provides the ba-sic statistics needed to solve the selection problem ( 3 ), notably estimates of the y i,j and correspond-ing confidence intervals. It contains a subroutine that implements sampling strategies for the di ff er-ent ranking models described in Section 2.3 . The pseudocode of PBR is shown in Algorithm 1 . The set A contains all pairs of options that still need to be sampled; it is initialized with all K 2  X  K pairs of indices. The set B contains the indices of the current top- X  solution. The algorithm samples those Y i,j with ( i, j )  X  A (lines 6  X  8 ). Then, it maintains ( 9  X  10 ). We denote the confidence interval of  X  y i,j [ u i,j , i,j ]. To compute confidence intervals, we apply the Hoe ff ding bound ( Hoe ff ding , 1963 ) for a sum of random variables in the usual way (see ( Mnih et al. , After the confidence intervals are calculated, one of the sampling strategies implemented as a subroutine is called. Since each sampling strategy can decide to select or discard pairs of options at any time, the confidence level  X  has to be divided by K 2 n max (line 13 ); this will be explained in more detail below. The sampling strategies determine which pairs of op-tions have to be sampled in the subsequent iteration. There are three subroutines ( SSCO , SSSE , SSRW ) in lines 16  X  21 of PBR that implement, respectively, the sampling strategies for our three ranking models, namely Copeland X  X  (CO), sum of expectation (SE) and random walk (RW). The concrete implementa-tion of the subroutines is detailed in the next section. We refer to the di ff erent versions of our preference-based racing algorithm as PBR  X  { CO , SE , RW } , de-pending on which sampling strategy is used. 4.1. Copeland X  X  Ranking (  X  CO ) The preference relation specified by the matrix Y is obviously reciprocal, i.e., y i,j =1  X  y j,i for i = j . Therefore, when using  X  CO for ranking, the opti-mization task ( 3 ) can be reformulated as follows: Procedure 2 implements a sampling strategy that optimizes ( 5 ). First, for each o i , we compute the number z i of options that are worse with su ffi ciently high probability X  X hat is, for which u i,j &lt; 1 / 2, j = (line 2 ). Similarly, for each option o i , we also com-pute the number w i of options o j that are preferred to it with su ffi ciently high probability X  X hat is, for are always at most K  X  z i options that can be bet-i is a member of the solution set I of ( 5 ) with high probability (see line 4 ). The indices of these options are collected in C . Based on a similar argument, options can also be discarded (line 5 ); their indices are collected in D . 3 Procedure 2 SSCO ( A,  X  Y , K,  X  , U , L ) 1: for i =1  X  K do 2: z i = |{ j | u i,j &lt; 1 / 2  X  i = j }| 3: w i = |{ j | i,j &gt; 1 / 2  X  i = j }| 4: C = i : K  X   X  &lt; { j | K  X  z j &lt;w i } Select 5: D = i :  X  &lt; { j | K  X  w j &lt;z i } Discard 6: for ( i, j )  X  A do 7: if ( i, j  X  C  X  D )  X  (1 / 2  X  [ i,j ,u i,j ]) then 8: A = A \ ( i, j ) Stop updating  X  y i,j 9: B = the top- X  options for which the correspond-10: return ( A, B ) In order to update A (the set of Y i,j still racing), we note that, for those options whose indices are in C  X  D , it is already decided with high probability whether or not they belong to I . Therefore, if the indices of two options o i and o j both belong to C  X  D , then Y i,j does not need to be sampled any more, and thus the index pair ( i, j ) can be excluded from A . Additionally, if 1 / 2  X  [ i,j ,u i,j ], then the pairwise relation of o i and o j is known with su ffi ciently high probability, so ( i, j ) can again be excluded from A . These filter steps are implemented in line 7 . Despite important di ff erences between the value-based and the preference-based racing approach, the expected number of samples taken by the latter can be upper-bounded in much the same way as Even-Dar et al. ( 2002 ) did for the former. 2 Theorem 1. Let O = { o 1 ,...,o K } be a set of op-tions such that  X  i,j = y i,j  X  1 / 2 =0 for all i, j  X  [ The expected number of pairwise comparison taken by PBR-CO is bounded by Moreover, the probability that no optimal solution of ( 6 ) is found by PBR-CO is at most  X  if n i,j  X  n max for all i, j  X  [ K ] . 4.2. Sum of Expectations (  X  SE ) Ranking For the SE ranking model, the problem ( 3 ) can be written equivalently as Procedure 3 SSSE ( A,  X  Y , K,  X  , U , L ,D ) 1: G = { i : i appearing in A } Active options 2: B = { 1 ,...,K }\ ( G  X  D ) Already selected 3: for all i  X  G do 6: K = | G | ,  X  =  X   X  | B | Reduced problem 7: B = B  X  i : K  X   X  &lt; { j  X  G : u j &lt; i } 8: D = D  X  i :  X  &lt; { j  X  G : u i &lt; j } 9: for ( i, j )  X  A do 10: if ( i  X  B  X  D ) then 11: A = A \ ( i, j ) Stop updating  X  y i,j 13: B = the top- X  options with the highest  X  y i values 14: return ( A, B, D ) with y i as in ( 4 ). The naive implementation would be to sample each random variable until the confi-dence intervals of the estimates  X  y i = 1 are non-overlapping. Note, however, that if the upper confidence bound of  X  y i calculated as u i = with respect to option o i do not need to be sampled anymore; instead, o i can be excluded from the so-lution set of ( 6 ) with high probability. Therefore, o can be discarded, and we can continue the run of PBR -SE with parameters K  X  1 and  X  (line 6 ). We use the set D to keep track of the discarded options. An analogous rule can be devised for the selection of options. The pseudocode of the PBR -SE sampling strategy is shown in Procedure 3 .
 We can also upper-bound the expected number of samples taken by PBR -SE. In fact, this setup is very close to the value-based one, since a single real value  X  y i is assigned to each option.
 Theorem 2. Let O = { o 1 ,...,o K } be a set of op-tions. Assume o i  X  SE o j i ff i&lt;j without loss of generality and y i = y j for all 1  X  i = j  X  K . Let b and b j = 4 1 1) outputs the optimal solution with probability at least (1  X   X  ) . 4 4.3. Random Walk (  X  RW ) Ranking We start the description of the RW sampling strat-egy with computing confidence intervals for the ele-ments of a stochastic matrix  X  S =[  X  s i,j ] fidence bounds c i,j for a given confidence level  X  for each element of the matrix  X  Y =[  X  y i,j ] Decatur ( 1998 ) provide simple bounds for propagat-ing error via some basic operations (see Lemma 1-2). Using their results, a direct calculation yields that s the stochastic matrix calculated as s i,j = y i,j P with probability at least 1  X  K  X  (since we assumed that the confidence term is  X  and each y i,j in the i th row of matrix Y must be within the confidence in-terval of  X  y i,j to meet ( 7 )). Note that the components of a particular row of matrix C =[ c i,j ] to each other, therefore C 1 = max i As a next step, we use the result of Funderlic &amp; Meyer ( 1986 ) on the updating of Markov chains. Theorem 3 (Funderlic&amp;Meyer, 1986) . Let S and S be the transition matrices of two irreducible Markov chains whose stationary distributions are v =( v 1 ,  X  X  X  ,v K ) and v =( v tively. Moreover, define the di ff erence matrix of the transition matrices as E = S  X  S . Then, the follow-ing inequality holds: where A # = a # In the PBR framework (Algorithm 1 ), we gradually decrease the confidence intervals of the entries of the matrix  X  Y , thus getting more precise estimates for Y . Let us denote the stochastic matrices derived from  X  Y and Y by  X  S and S , respectively, and their principal eigenvectors (that belong to the eigenvalue 1) by  X  v = (  X  v 1 ,  X  X  X  ,  X  v K ) and v =( v 1 ,  X  X  X  ,v K ). Moreover, let be the matrix that contains the confidence intervals of  X  S as defined in ( 7 ). Applying Theorem 3 , 3 we have v  X   X  v max  X  S  X   X  S 1  X  A # max , where  X  A # = (
I  X   X  S + 1  X  v T )  X  1  X  1  X  v T . Moreover, we have S
C 1 with probability at least 1  X  K 2  X  , since this inequality requires all s i,j to be within the confidence interval given in ( 7 ) and, therefore all y i,j must be within the confidence interval of  X  y i,j . Summarizing what we found so far, we have This upper bound suggests the minimization of
C 1 . What remains to be shown, however, is that  X 
A # max is bounded. In PBR , we gradually es-timate Y , thereby obtaining a series of estimates  X  converges componentwise to Y , then  X  A ( n )# max  X 
A # max . Moreover, based on ( Seneta , 1992 ) Eq. (7), A # max is bounded from above for a stochas-tic matrix S . In order to have a sample complexity analysis for PBR -RW, we would also need to know the rate of convergence of the series  X  A ( n )# max , which is a quite di ffi cult question.
 The inequality ( 9 ) suggests a simple sampling strategy: Since the goal is to decrease C 1 = max i,j c i,j  X  y ,i , select the pairs of random vari-ables ( i, j ) = argmax Recall our original optimization task, namely to se-lect a subset of options as follows: Let  X  be the sorting permutation that puts the ele-ments of  X  v in a descending order. Now, if |  X  v  X  v stop sampling, since | v i  X   X  v i |  X  C 1  X  A # max for 1  X  i  X  K with probability 1  X  K 2  X  ; therefore, the confidence term has to be divided by K 2 .The pseudo-code of RW sampling strategy is shown in Procedure 4 . In this experiment, we applied our preference-based racing method to sports data. We collected the scores of all soccer matches of the last ten seasons from the German Bundesliga. Our goal was to find those three teams that performed best during that time. We restricted to the 8 teams that participated in each Bundesliga season between 2002 to 2012. Ta-ble 1 lists the names of these teams and the number of their overall wins (W), losses (L) and ties (T). Each pair of teams met 20 times. For teams o i and o j , we denote the outcome of these matches 5 Procedure 4 SSRW (  X  Y , K,  X  , C ) 1: Convert  X  Y to be stochastic matrix  X  S , and calcu-2: Calculate the eigenvector  X  v of  X  S which belongs 3: Calculate  X  A # = I  X   X  S + 1  X  v T  X  1  X  1  X  v T 4: Take the  X  th and  X  + 1th biggest elements of  X  v 5: if | a  X  b | &gt; 2 C 1  X  A # max then A =  X  6: else A = { argmax i,j c i,j  X  y ,i } 7: B = the top- X  options for which the elements of 8: return ( A, B ) by y 1 quency distribution as the (ground-truth) probabil-ity distribution of Y i,j . The rankings of the teams with respect to  X  CO ,  X  SE and  X  RW , computed from the expectations y i,j = E [ Y i,j ], are also shown in Ta-ble 1 . While the team of Munich (Bayern M  X unchen) dominates the Bundesliga regardless of the ranking model, the follow-up positions may vary depending on which method is chosen.
 We run our racing algorithm on the outcomes of all matches by sampling from the distributions of the Y i,j (i.e., we sampled from each set of 20 scores with replacement). PBR was parametrized by  X  =0 . 1 ,  X  =3 ,n max = { 100 , 500 , 1000 , 5000 , 10000 } Figure 1 shows the empirical sample complexity ver-sus accuracy of di ff erent runs averaged out over 100 runs. As a baseline, we also run the PBR algo-rithm with uniform sampling meaning that in each iteration we sampled all pairwise comparisons. The accuracy of a run is 1 if all top- X  teams were found, otherwise 0. As we increase n max , the accuracy converges to 1  X   X  . This experiment confirms that our preference-based racing algorithm can indeed re-cover the top- X  options with a confidence at least 1  X   X  provided n max is large enough. Moreover, by using the sampling strategies introduced in Section 4 ,
PBR can achieve an accuracy similar to the uni-form sampling for an empirical sample complexity that is an order of magnitude smaller (if again n max is large enough). In this section, we consider a setting that is in a sense in-between the value-based and the preference-based one. Like in the former, each option o i is associated with a random variable X i ; thus, it is possible to evaluate individual options, not only to compare pairs of options. However, the random vari-ables X i take values in a set  X  that is only partially ordered by a preference relation . Thus, like in the preference-based setting, two options are not neces-sarily comparable in terms of their sampled values. Obviously, the value-based TKS setup described in Section 2.1 is a special case with  X  = R and the standard  X  relation on the reals.
 Coming back to our preference-based setting, the pairwise relation y i,j between options can now be written as
P ( X i  X  X j )+ 6 It can be estimated on the basis of random samples X from P X  X  y This estimate is known as Mann-Whitney U-statistic (also known as the Wilcoxon 2 -sample statistic ) and belongs to the family of two-sample U-statistics. Apart from  X  y i,j being an unbiased estimator of y i,j , ( 11 ) exhibits concentration properties resembling those of the sum of independent random variables. Theorem 4 (( Hoe ff ding , 1963 ),  X  5b) . 4 For any &gt; 0 , using the notations introduced above, Based on this concentration result, one can ob-tain a confidence interval for  X  y i,j as follows: for any 0 &lt;  X  &lt; 1, the interval [  X  y i,j  X  c i,j ,  X  y contains y i,j with probability at least 1  X   X  where c We can readily adapt the PBR framework to this special setup: In each iteration of PBR , those ran-dom variables have to be sampled whose indices ap-pear in A , i.e., those X i with ( i, j )  X  A or ( j, i ) Then, by comparing the random samples with re-spect to , one can calculate  X  y i,j according to ( 11 ). Finally, the confidence intervals for the  X  y i,j can be obtained based on Theorem 4 (for pseudo-code see Appendix B.1 ). 6.1. Results on Synthetic Data Recall that the setup described above is more gen-eral than the original value-based one and, therefore, that the PBR framework is more widely applicable than the value-based Hoe ff ding race ( HR ). 5 Never-theless, it is interesting to compare their empirical sample complexity in the standard numerical set-ting, where both algorithms can be used.
 We considered three test scenarios. In the first, each random variable X i follows a normal distribution N k  X  N + ; in the second, each X i obeys a uniform dis-in the third, each X i obeys a Bernoulli distribution Bern (1 / 2) + d i , where d i  X  U [0 ,k/ 5] and k  X  N + . In every scenario, the goal is to rank the distribu-tions by their means. Note that the complexity of the TKS problem is controlled by the parameter k , with a higher k indicating a less complex task; we varied k between 1 and 10. Besides, we used the parameters K = 10,  X  = 5, n max = 300,  X  =0 . 05. Strictly speaking, HR is not applicable in the first scenario, since the support of a normal distribution is not bounded; we used R = 8 as an upper bound, thus conceding to HR a small probability for a mis-bounds of the supports can be readily determined. Figure 2 shows the number of random samples drawn by the racing algorithms versus precision (percent-age of true top- X  variables among the predicted top- X  ). PBR -CO, PBR -SE and PBR -RW achieve a significantly lower sample complexity than HR , whereas its accuracy is on a par or better in most cases in the first two test scenarios. While this may appear surprising at first sight, it can be explained by the fact that the Wilcoxon 2-sample statistic is e ffi cient ( Serfling , 1980 ).
 In the Bernoulli case, one may wonder why the sam-ple complexity of PBR -CO hardly changes with k (see the red point cloud in Figure 2(c) ). This can be explained by the fact that the two sample U-statistic  X  Y in ( 11 ) does not depend on the magnitude of the drift d i (as long as it is smaller than 1). The racing setup and the Hoe ff ding race algorithm were first considered by Maron &amp; Moore ( 1994 ; 1997 ) in the context of model selection. Mnih et al. ( 2008 ) improved the HR algorithm by using the empirical Bernstein bound instead of the Hoe ff ding bound. In this way, the variance information of the mean es-timates could be incorporated in the calculation of confidence intervals.
 In the context of multi-armed bandits, Even-Dar et al. ( 2002 ) introduced a slightly di ff erent setup, where an -optimal random variable has to be cho-sen with probability at least 1  X   X  ; here, -optimality of X i means that  X  i +  X  max gorithms solving this problem are called ( ,  X  )-PAC 7 bandit algorithms. The authors propose such an al-gorithm and prove an upper bound on the expected sample complexity. In this paper, we borrowed their technique and used it in the complexity analysis of PBR -CO and PBR -SE.
 Recently, Kalyanakrishnan et al. ( 2012 ) introduced a PAC-bandit algorithm for TKS which is based on the widely-known UCB index-based multi-armed ban-dit method ( Auer et al. , 2002 ). In their formaliza-tion, an algorithm is an ( , m,  X  )-PAC bandit algo-rithm that selects the m best random variables un-der the PAC-bandit conditions. According to their definition, a racing algorithm is a (0 ,  X  ,  X  )-PAC algo-rithm. They could prove a high probability bound for the worst case sample complexity instead of the expected sample complexity. It is an interesting question whether their slack variable technique can be applied in our setup.
 Yue et al. ( 2012 ) introduce a multi-armed bandit setup where feedback is provided in the form of noisy comparisons between options, just like in our ap-proach. In their setup, however, they are aiming at a small cumulative regret, where the reward of a pair-wise comparison of o i and o j is max {  X  i  X  ,i ,  X  i  X  ,j whereas ours is a pure exploration approach. To ensure the existence of the best option o i  X  , strong assumptions are made on the distributions of the comparisons, such as strong stochastic transitivity and stochastic triangle inequality.
 In  X  X oisy sorting X  ( Braverman &amp; Mossel , 2008 ), noisy pairwise preferences are sampled like in our case, but it is assumed that there is a total order over the objects. That is why the algorithms pro-posed for this setup require in general less pairwise comparisons in expectation ( O ( K log K )) than ours. We introduced a generalization of the problem of top-k selection under uncertainty, which is based on comparing pairs of options in a qualitative in-stead of evaluating single options in a quantitative way. To tackle this problem, we proposed a general framework in the form of a preference-based racing algorithm along with three concrete instantiations, using di ff erent methods for ranking options based on pairwise comparisons. Our algorithms were ana-lyzed formally, and their e ff ectiveness was shown in experimental studies on real and synthetic data. For future work, there are still a number of theoret-ical questions to be addressed, as well as interesting variants of our setting. For example, inspired by ( Kalyanakrishnan et al. , 2012 ), we plan to consider a variant that seeks to find a ranking that is close to the reference ranking (such as  X  CO ) in terms of a given rank distance, thereby distinguishing be-tween correct and incorrect solutions in a more grad-ual manner than the (binary) top-k criterion. Moreover, there are several interesting applications of our preference-based TKS setup. Concretely, we are currently working on an application in preference-based reinforcement learning, namely a preference-based variant of evolutionary direct pol-icy search as proposed by Heidrich-Meisner &amp; Igel ( 2009 ).
 This work was supported by the German Research Foundation (DFG) as part of the Priority Pro-gramme 1527, and by the ANR-10-BLAN-0215 grant of the French National Research Agency. 8
