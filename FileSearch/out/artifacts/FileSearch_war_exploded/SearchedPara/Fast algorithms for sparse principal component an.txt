 Volodymyr Kuleshov kuleshov@stanford.edu Department of Computer Science, Stanford University, Stanford, CA A fundamental problem in statistics is to find simpler, low-dimensional representations for data. Such rep-resentations can help uncover previously unobserved patterns and often improve the performance of ma-chine learning algorithms.
 A basic technique for finding low-dimensional rep-resentations is principal component analysis (PCA). PCA produces a new K -dimensional coordinate sys-tem along which data exhibits the most variability. Although this technique can significantly reduce the dimensionality of the data, the new coordinate sys-tem it introduces is not easily interpretable. In other words, whereas initially, each coordinate x i of a data point x  X  R n corresponds to a well-understood pa-rameter (such as the expression level of a gene), in the PCA basis, the new coordinates x 0 i are weighted com-binations P i w i x i of every original parameter x i , and it is no longer easy to assign a meaning to different values of x 0 i .
 An effective way of ensuring that the new coordinates are interpretable is to require that each of them be a weighted combination of only a small subset of the original dimensions. In other words, we may require that the basis vectors for our new coordinate system be sparse . This technique is referred to in the litera-ture as sparse principal component analysis (sPCA), and has been successfully applied in areas as diverse as bioinformatics (Lee et al., 2010), natural language pro-cessing (Richt  X arik et al., 2012), and signal processing (D X  X spremont et al., 2008).
 Formally, an instance of sPCA is defined in terms of the cardinality-constrained optimization problem (1), in which  X   X  R n  X  n is a symmetric matrix (normally a positive semidefinite covariance matrix), and k &gt; 0 is a sparsity parameter. Problem (1) yields a single sparse component x  X  and for k = n , reduces to finding the leading eigenvector of  X  (i.e. to the standard formulation of PCA). Although there exists a vast literature on sPCA, most popular algorithms are essentially variations of the same technique called the generalized power method (GPM, see Journ  X ee et al. (2010)). This technique has been shown to match or outperform most other al-gorithms, including ones based on SDP formulations (D X  X spremont et al., 2007), bilinear programming (Witten et al., 2009), and greedy search (Moghaddam et al., 2006). The GPM is a straightforward exten-sion of the power method for computing the largest eigenvector of a matrix (Parlett, 1998) and can also be interpreted as projected gradient ascent on a vari-ation of problem (1).
 Although the GPM is a very simple and intuitive al-gorithm, it can be slow to converge when the covari-ance matrix  X  is large. This should not come as un-expected, as the GPM generalizes two rather unso-phisticated algorithms. In practice, eigenvectors are computed using a technique called Rayleigh quotient iteration (normally implemented as part of the QR algorithm). Similarly, in unconstrained optimization, Newton X  X  method converges orders of magnitude faster than gradient descent.
 Here, we introduce a new algorithm for problem (1) that generalizes Rayleigh quotient iteration, and that can also be interpreted as a second-order optimization technique similar to Newton X  X  method. It converges to a local solution of (1) in about eight iterations or less on most problem instances, and generally requires between one and two orders of magnitude fewer float-ing point operations (flops) than the GPM. Moreover, the solutions it finds tend to be of as good or of bet-ter quality than those found by the GPM. Conceptu-ally, our algorithm fills a gap in the family of eigen-value algorithms and their generalizations that is left by the lack of second-order optimization methods in the sparse setting (Table 1). We refer to our technique as generalized Rayleigh quo-tient iteration (GRQI). GRQI is an iterative algorithm that converges to local optima of (1) in a small num-ber of iterations; see Algorithm 1 for a pseudocode definition.
 Briefly, GRQI performs up to two updates at every iteration j : a Rayeligh quotient iteration update, fol-lowed by an optional power method update. The next iterate x ( j +1) is set to the Euclidean projection P k (  X  ) on the set {|| x || 0  X  k } X  X || x || 2  X  1 } of the result x of these steps.
 The first update is an iteration of the standard Rayleigh quotient iteration algorithm on the set of working indices W = { i | x ( j ) i 6 = 0 } of the current iterate x ( j ) . The indices which equal zero are left unchanged. Rayleigh quotient iteration is a method that com-putes eigenvectors by iteratively performing the up-date x + = ( X   X   X I )  X  1 x for  X  = x T  X  x/x T x . To get an understanding of why this technique works, observe that when  X  is close to an eigenvalue  X  i of  X , the i -th eigenvalue of ( X   X   X I )  X  1 tends to infinity, and after the Algorithm 1 GRQI( X , x 0 , k , J , ) j  X  0 repeat Algorithm 2 SPCA-GRQI( X ,  X  , J , , K ,  X  ) for k = 1 to K do end for Algorithm 3 GPower0( D , x 0 ,  X  , )
Let S 0 : R n  X  R  X  R n be defined as j  X  0 repeat Algorithm 4 GPower1( D , x 0 ,  X  , )
Let S 1 : R n  X  R  X  R n be defined as j  X  0 repeat Equivalent optimization method Gradient descent Newton X  X  method multiplication x + = ( X   X   X I )  X  1 x , x + becomes almost parallel to the i -th eigenvector. Since  X  is typically a very good estimate of an eigenvalue of  X , this happens after only a few multiplications in practice.
 The second update is simply a power method step along all indices. This step starts from where the Rayleigh quotient update ended. In addition to im-proving the current iterate, it can introduce large changes to the working set at every iteration, which helps the algorithm to find a good sparsity pattern quickly.
 Finally, the projection P k ( x ) on the intersection of the l and l 2 balls ensures that x ( j +1) is sparse. Note that this projection can be done by setting all but the k largest components of x (in absolute value) to zero and normalizing the resulting vector to a norm of one. For the sake of simplicity, we defined Algorithm 1 to perform power method updates only at the first J iterations, with J  X  [0 ,  X  ] being a parameter. We also implemented more sophisticated ways of combin-ing Rayleigh quotient and power method updates, but they offered only modest performance improvements over Algorithm 1, and so we focus here on a very sim-ple but effective method. 2.1. Convergence In all our experiments, we simply set J =  X  , and we recommend this choice of parameter for most appli-cations. However, to formally guarantee convergence, J must take on a finite value, in which case we can establish the following proposition.
 Proposition. Let  X   X  R n  X  n such that  X  =  X  T , x 0  X  R n , k &gt; 0 , J &lt;  X  be input parameters to Algorithm 1. There exists an x  X  such that || x  X  || 0  X  k and such that the iterates ( x j )  X  j =1 generated by Algorithm 1 converge to x  X  at a cubic rate: This proposition follows from the fact that when j  X  J , Algorithm 1 reduces to Rayleigh quotient iteration on a fixed set of k non-zero indices, and from the fact that standard Rayleigh quotient iteration converges at a cubic rate (Parlett, 1998). 2.2. Starting point We recommend setting x 0 to the largest column of  X , as previously suggested in Journ  X ee et al. (2010). Interestingly, we also observed that we could initialize Algorithm 1 randomly, as long as the first Rayleigh quotient  X  (0) was close the largest eigenvalue  X  1 of  X . In practice, an accurate estimate of  X  1 can be obtained very quickly by performing only a few steps of Power iteration (O X  X eary et al., 1979). 2.3. Multiple sparse principal components Most often, one wants to compute more than one sparse principal component of  X . Algorithm 1 eas-ily lends itself to this setting; the key extra step that must be added is the deflation of  X . In order to avoid describing variation in the data that has already been explained by an earlier principal component x , we re-place  X  at the next iteration by  X  + =  X   X  ( x T  X  x ) xx Thus, if a new principal component x + explains any variance in the direction of x (i.e. if x T x + 6 = 0), that variance will not count towards the new objective func-tion x T +  X  + x + .
 In practice, there are settings where one may not want to perform a full deflation of  X , but instead only give variance explained in the direction of x less weight. This can be done by changing the deflation step to  X  + =  X   X   X  ( x T  X  x ) xx T , where 0  X   X   X  1 is a parame-ter. For example, this partial deflation turns out to be useful when analyzing image features, as we show in Section 5. In addition, there exist several other defla-tion schemes which could be used with our algorithm; we refer the reader to the survey by Mackey (2009) for details. The full version of our method for computing K principal components can be found in Algorithm 2. Finally, we would like to point out that block algo-rithms that compute several components at once, like the ones in Journ  X ee et al. (2010), can also be derived from our work. Essentially, the GRQI algorithm modifies the power method by replacing the power iteration step by Rayleigh quotient iteration, followed by an optional step of the power method. Rayleigh quotient iteration is a simple technique that converges to an eigenvector much faster than power iteration, and is responsible for the high speed of modern eigenvalue algorithms. Perhaps our most interesting observation is that this technique dramatically improves the performance of algorithms in the sparse setting as well.
 Existing algorithms for solving (1) essentially do two tasks at once: (a) identifying a good sparsity pattern, and (b) computing a good eigenvector within that pat-tern. Intuitively, our algorithm works well because Rayleigh quotient iteration converges to high-variance eigenvectors within the current working set of indices W much faster than the power method. Therefore, it solves task (b) much more quickly than the GPM, at the cost of some extra computation. 3.1. Rayleigh quotient iteration as Newton X  X  Interestingly, the effectiveness of Rayleigh quotient it-eration can also be explained by the fact that it is equivalent to a slight variation of Newton X  X  method (Tapia &amp; Whitley, 1988). By the same logic, one can view GRQI as a second-order projected optimization method on the objective (1).
 In fact, Algorithm 1 resembles in some ways the pro-jected Newton methods of Gafni &amp; Bertsekas (1984). Both approaches take at every iteration a gradient step and a Newton step; in both cases, Newton steps are restricted to a subspace to avoid getting stuck at bad local optima (see Bertsekas (1982) for an example). Unfortunately, projected Newton methods can only be used with very simple constraints, unlike those in (1). 3.2. Computational complexity Perhaps the chief concern with second order methods is their per-step cost. Algorithm 1 performs at every iteration only O ( k 3 + nk ) flops, where k is the number of non-zero components. For comparison, the GPM requires O ( n 2 + nk ) flops (see next section and Algo-rithms 3 and 4). Thus, our algorithm has an advan-tage when k n , which is precisely when the principal components are truly sparse. 3.3. Comparison to the generalized power The generalized power method is a straightforward ex-tension of the power method for computing eigenval-ues; in its simplest formulation, it alternates between power iteration and projection steps until convergence. According to Journ  X ee et al. (2010), its most effective formulations are two algorithms called GPower0 and GPower1 (Algorithms 3 and 4). Both algorithms are equivalent to subgradient ascent on the objective func-tion where D is the data matrix,  X  &gt; 0 is a sparsity pa-rameter and || X || can be the l 0 norm (in the case of GPower0), or the l 1 norm (in the case of GPower1). Our method has several immediate advantages over Algorithms 3 and 4. For one, the user can directly specify the desired cardinality of the solution, instead of having to search for a penalty parameter  X  . In fact, we found that varying  X  by &lt; 1% in equation (2) some-times led to changes in the cardinality of the solution of more than 10%. Moreover, the desired cardinality is kept constant at every iteration of our algorithm; thus one can stop iterating when a desired level of variance has been attained.
 Other algorithms for sPCA are often variants of the generalized power method, including some highly cited ones, such as Zou et al. (2006) or Witten et al. (2009). Methods that are not equivalent to the GPM include the SDP relaxation approach of D X  X spremont et al. (2007); since it cannot scale to more than a few hun-dred variables, we do not consider it in this study. Another class of algorithms includes greedy methods that search the exponential space of sparsity patterns directly (Moghaddam et al., 2006); such algorithms have been shown to be much slower than the GPM (Journ  X ee et al., 2010), and we don X  X  consider them ei-ther. We now proceed to compare the performance of gen-eralized Rayleigh quotient iteration to that of Algo-rithms 3 and 4. We evaluate the algorithms on a series of standard tasks that are modeled after the ones of Journ  X ee et al. (2010). The experiments in this sec-tion are performed on random positive semidefinite matrices  X   X  R n  X  n of the form  X  = A T A for some A  X  N (0 , 1) n  X  n . Although we show results only for n = 1000, our observations carry over to other matrix dimensions.
 Throughout this section, we use as our convergence cri-terion the condition || x  X  x prev || &lt; 10  X  6 . Note that we cannot use criteria based on the objective functions (1) and (2), as they are not comparable: one measures car-dinality and one doesn X  X . Furthermore, we set J =  X  in all experiments, in which case Algorithm 1 performs at every iteration a Rayleigh quotient step, followed by a full power method step. 4.1. Convergence rates As a demonstration of the rapid convergence of gen-eralized Rayleigh quotient iteration, we start by plot-ting in Figure 1 the variance, sparsity, and precision at every iteration of GRQI and GPower1. We set the sparsity parameter  X  of GPower1 to 5, and set the pa-rameter k to match the sparsity of the final solution returned by that algorithm, which was k = 44 (4.4% sparsity). We found that this setting was representa-tive of what happens at other sparsities.
 We observe that GRQI converges in six iterations, which is quite typical for the problem instances we con-sidered. This observation is consistent with the speed of regular Rayleigh quotient iteration and appears to support empirically the cubic convergence guarantees of our algorithm. Interestingly, the rapid convergence in Figure 1 was achieved without setting J &lt;  X  , as the convergence argument formally requires. 4.2. Time complexity analysis To further evaluate the speed of GRQI, we counted the number of flops that were required to compute a sparse component on a number of random matri-ces. We chose to measure algorithm speed in flops be-cause unlike time measurements, they do not depend on various implementation details. In fact, when im-plemented in a popular high-level language like MAT-LAB, GRQI would have an unfair advantage over the GPM, as it spends relatively more time in low-level subroutines.
 To measure flops, we counted the number of matrix operations performed by each algorithm. For every k  X  n matrix-vector multiplication, we counted kn flops, whereas for k  X  k matrix inversions, we counted k 3 + 2 k 2 . The latter is the complexity of inverting a Hermitian matrix through an LDL T factorization. Operations taking o ( n 2 ) time were ignored, as their flop count was negligible.
 Results of this experiment appear in Figure 2. GRQI uses between one and two orders of magnitude fewer flops at sparsities below 20%; at sparsities below 5%, it used a hundred times fewer flops. However, for large values of k , the complexity of GRQI outgrows that of the GPM. In this case, we found that we could bring down the number of GRQI flops to at least the level of the GPM by letting J be small; however at those levels of k the problem is not truly sparse, and so we do not give details.
 In practice, the computational requirements of the GPM would appear somewhat smaller if we used a loose convergence criterion, such as || x  X  x prev || &lt; 10  X  2 . However, even in that case, GRQI required at least an order of magnitude fewer flops. On the other hand, the GPM in our experiments needed about 3-4 restarts before we could find a penalty term that yielded the desired number of non-zero entires in the principal component. We did not count these restarts in our flop measurements.
 4.3. Variance Finally, we demonstrate that the quality of the solu-tions to which our algorithm quickly arrives matches that of the solutions obtained by the GPM. In Figure 3, we plot the tradeoff between the sparsity and the variance of a solution for both algorithms. The two curves are essentially identical, and this observation could be also made for other matrix dimensions. Thus far, our methods have been presented only in the context of positive semidefinite covariance matrices. However, they can be easily extended to handle an arbitrary rectangular m  X  n matrix R . In that setting, the problem we solve becomes a generalization of the singular value decomposition (SVD); hence, we refer to it as the sparse singular value decomposition (sSVD). Sparse SVD generalizes objective (1) as follows. Problem (3) can be reduced to problem (1) by exploit-ing the well-known observation that Thus we can solve (3) by running Algorithm 1 on the ( m + n )  X  ( m + n ) matrix  X . Fortunately, this can be done without ever explicitly forming ( m + n )  X  ( m + n ) matrices. Using blockwise inversion, we compute the inverse of  X   X   X I as follows. where S = RR T  X   X I is the Schur complement of  X I . When R is a k 1  X  k 2 submatrix with k 1  X  k 2 ,  X   X   X I can be inverted using O ( k 2 1 k 2 + k 3 1 ) floating point operations. Note that when the matrix R is square, this complexity reduces to that of Algorithm 1. Details may be found in Algorithm 5.
 Algorithm 5 SSVD( R , u 0 , v 0 , k u , k v , J , ) j  X  0 repeat 5.1. Gene expression data We test the performance of Algorithm 5 on gene ex-pression data collected in the prostate cancer study by Singh et al. (2002). The study measured expres-sion profiles for 52 tumor and 50 normal samples over 12,600 genes, resulting in a 102  X  12 , 600 data matrix. Figure 4 shows the variance-sparsity tradeoffs of GRQI and GPower for the first principal component of the data matrix, as well the number of flops they use. Both algorithms explain roughly the same variance (in fact, GRQI explains 2.35% more for small sparsities), whereas time complexity is again significantly smaller for Algorithm 1. Although the relative advantage is not as large as in Figure 2, GRQI still uses about an order of magnitude fewer flops at small sparsity levels. Yao et al. (2012) have shown that the principal com-ponents of the above data matrix map to known path-ways associated with cancer. Our algorithms can iden-tify these pathways at a fraction of the computational cost of existing methods. 5.2. Application in deep learning/machine As a further example of how our methods can be ap-plied in practice, we use Algorithm 5 to perform un-supervised feature learning on the STL-10 dataset of images.
 Briefly, we sampled 100,000 random 3  X  8  X  8 RGB image patches and computed sparse principal compo-nents on the resulting 196  X  100 , 000 matrix using a deflation parameter  X  = 0 . 2. This partial deflation al-lowed us to recover 400 principal components, which is more than twice the rank of the data matrix. Curiously, we observe that when visualized as 8  X  8 color matrices, these principal components take on the shape of little  X  X dges X , as shown in Figure 5. We found it challenging to reproduce the above results using the GPower algorithms because of the difficulties in tuning the sparsity parameter  X  ; therefore we do not report a speed comparison on this data.
 In the vision literature, the edges shown in Figure 5 are known as Gabor filters , and are used as a basis for rep-resenting images in many machine vision algorithms. Their use considerably improves these algorithms X  per-formance.
 In the past several years, learning such features auto-matically from data has been a major research topic in artificial intelligence. The fact that sparse principal component analysis can be used as a technique for un-supervised feature extraction may lead to new feature learning algorithms that are simple, fast, and that use matrix multiplications that can be parallelized over many machines.
 More generally, sparse principal component analysis addresses a key challenge in statistics and thus has applications in many other areas besides bioinformat-ics and machine vision. We believe that developing fast, simple algorithms for this problem will spread its use to an even wider range of interesting application domains.
 We wish to thank Stephen Boyd for helpful discussions and advice, as well as Ben Poole for reviewing an early draft of the paper.
 Bertsekas, D. P. Projected Newton Methods for Opti-mization Problems with Simple Constraints. SIAM
Journal on Control and Optimization , 20(2):221 X  246+, 1982.
 D X  X spremont, Alexandre, El Ghaoui, Laurent, Jordan,
Michael I., and Lanckriet, Gert R. G. A Direct For-mulation for Sparse PCA Using Semidefinite Pro-gramming. SIAM Rev. , 49(3):434 X 448, 2007.
 D X  X spremont, Alexandre, Bach, Francis, and Ghaoui, Laurent El. Optimal Solutions for Sparse Principal
Component Analysis. J. Mach. Learn. Res. , 9:1269 X  1294, 2008.
 Gafni, Eli M. and Bertsekas, Dimitri P. Two-Metric Projection Methods for Constrained Optimization.
SIAM Journal on Control and Optimization , 22(6): 936 X 964, 1984.
 Journ  X ee, Michel, Nesterov, Yurii, Richt a rik, Pe-ter, and Sepulchre, Rodolphe. Generalized Power Method for Sparse Principal Component Analysis. J. Mach. Learn. Res. , 11:517 X 553, 2010.
 Lee, Donghwan, Lee, Woojoo, Lee, Youngjo, and Paw-itan, Yudi. Super-sparse principal component analy-ses for high-throughput genomic data. BMC Bioin-formatics , 11(1):296, 2010.
 Mackey, Lester. Deflation Methods for Sparse PCA.
In Advances in Neural Information Processing Sys-tems , pp. 1017 X 2024. MIT Press, 2009.
 Moghaddam, Baback, Weiss, Yair, and Avidan, Shai. Spectral Bounds for Sparse PCA: Exact and Greedy
Algorithms. In Advances in Neural Information Pro-cessing Systems , pp. 915 X 922. MIT Press, 2006. O X  X eary, Dianne P., Stewart, G. W., and Vandergraft, James S. Estimating the Largest Eigenvalue of a
Positive Definite Matrix. Mathematics of Computa-tion , 33(148):pp. 1289 X 1292, 1979.
 Parlett, Beresford N. The symmetric eigenvalue prob-lem . Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1998.
 Richt  X arik, Peter, Tak  X ac, Martin, and Ahipasaoglu, Selin Damla. Alternating Maximization: Unifying
Framework for 8 Sparse PCA Formulations and Ef-ficient Parallel Codes. CoRR , abs/1212.4137, 2012. Singh, Dinesh, Febbo, Phillip G, Ross, Kenneth, Jack-son, Donald G, Manola, Judith, Ladd, Christine,
Tamayo, Pablo, Renshaw, Andrew A, D X  X mico, An-thony V, Richie, Jerome P, Lander, Eric S, Loda, Massimo, Kantoff, Philip W, Golub, Todd R, and
Sellers, William R. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell , 1(2): 203 X 209, 2002.
 Tapia, R. A. and Whitley, David L. The Projected Newton Method has Order 1 + 2 for the Symmetric Eigenvalue Problem. SIAM Journal on Numerical Analysis , 25(6):pp. 1376 X 1382, 1988.
 Witten, D M, Tibshirani, R, and Hastie, T. A pe-nalized matrix decomposition, with applications to sparse principal components and canonical correla-tion analysis. Biostatistics , 10(3):515 X 534, jun 2009. Yao, Fangzhou, Coquery, Jeff, and Le Cao, Kim-Anh.
Independent Principal Component Analysis for bi-ologically meaningful dimension reduction of large biological data sets. BMC Bioinformatics , 13(1):24, 2012.
 Zou, Hui, Hastie, Trevor, and Tibshirani, Robert. Sparse Principal Component Analysis. Journal of
Computational and Graphical Statistics , 15(2):265 X 
