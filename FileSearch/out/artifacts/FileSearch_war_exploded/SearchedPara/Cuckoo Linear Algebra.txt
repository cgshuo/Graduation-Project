 In this paper we present a novel data structure for sparse vectors based on Cuckoo hashing. It is highly memory ef-ficient and allows for random access at near dense vector level rates. This allows us to solve sparse ` 1 programming problems exactly and without preprocessing at a cost that is identical to dense linear algebra both in terms of memory and speed. Our approach provides a feasible alternative to the hash kernel and it excels whenever exact solutions are required, such as for feature selection.
 H.1 [ Information Systems ]: Models and Principles; E.2 [ Data ]: Data Storage Representations Linear Models; Hashing; Sparse Vectors
High dimensional sparse estimation [4] is one of the key tools in machine learning. It forms the basis of algorithms for document analysis, bioinformatics, user personalization, recommendation, and the vast array of Bayesian nonpara-metric models with their hierarchical and variable data struc-tures. Unfortunately, these tools require expensive manipu-lation of sparse vectors. This is particularly costly whenever the sparsity pattern changes over time, e.g., by insertion of previously-unseen tokens, user actions, topics, or clusters. Usually this requires custom-built data structures to ma-nipulate these events efficiently. In its place we propose the use of a general-purpose sparse data structure, the Cuckoo hash, as proposed by [ 20 ] with refinements due to [25 , 10 ]. We adapt it for linear algebra and demonstrate its efficacy for sparse generalized linear models.

The motivation for our approach arises from the task of penalized risk minimization with sparsity penalty. That is, we are interested in a family of problems of the form For instance, penalized logistic regression follows this tem-plate. Here R [ w ] is the empirical risk for a given dataset { ( x i ,y i ) } with Such problems are common, e.g., in spam filtering and com-putational advertising [ 5]. Similar problems arise in the con-text of sparse decoding where R [ w ] = Pw for a random inco-herent projection matrix P , and in the context of web page tiering [14 ].

One of the challenges for such high-dimensional problems is that w is often not simply a vector in R d with well-defined dimensionality but rather a sparse vector of (key,value) pairs. As a result, one either needs to maintain a dictionary dy-namically or it requires substantial work for preprocessing the data. In fact, the preprocessing cost can exceed the cost of solving the problem both in terms of memory footprint and in terms of computation.

To make matters more concrete, consider the case of (per-sonalized) spam filtering [ 24]. In it, each document is repre-sented as a bag of words. Hence, it is a sparse vector with an identifier for each unique word w  X  W . Moreover, for per-sonalization purposes one computes the tensor product with the identifier associated with each user u  X  U . This leads to a space that is potentially of size |W| X |U| , i.e., given by the product of the total number of users and words. Typ-ical commercial e-mail systems support in the order of 100 million users and vocabularies of size 100,000 are not un-common, thus requiring a 10 13 dimensional space. This is clearly infeasible and unrealistic since most users use only a significantly smaller set of words and user participation follows a power law. Hence the set of unique keys is several orders of magnitude smaller.

One way of solving the associated ` 1 programming prob-lem of ( 1) is to preprocess all data by mapping unique combi-nations of attributes and user identifiers to integers such as to form a contiguous list, i.e., to form a perfect hash. This re-quires sorting all entries, something that is typically accom-plished by sorting all combinations, e.g., by using MapRe-duce [8 ]. Given m observations the cost is O ( dm log dm ), where d is the typical number of nonzero terms per docu-ment (we need to sort all words). Once this is accomplished, one maps all keys into integers and henceforth analysis pro-ceeds as usual. This is the approach that LibLinear [11 ] and many related algorithms take. An alternative is to build a nontrivial data structure in memory to dynamically allocate storage according to the sparsity pattern, as was proposed by [ 21] for Latent Dirichlet Allocation. However, this is less memory-efficient than the above preprocessing step for ` penalized models.

Unfortunately preprocessing is sometimes impossible with limited memory. For example when we use string kernels [ 13 ] for protein classification, the memory needed to store the ex-panded substring data may exceed physical memory. In this case we need a data structure that supports generating fea-tures on the fly. Also preprocessing fails whenever we are in a online setting, that is, new features keep on occurring over time. Even worse, in a distributed online setting, different features can be received by different machines and we need to have a mechanism for reconciling these subsets without the need for a common key space coordination mechanism since the latter is likely to be more costly to implement than solving the original inference problem.

In short, dealing with (key,value) pairs in machine learn-ing is a nontrivial problem. Distributed, parallel, and online inference are substantially impeded by having to maintain a separate data structure for key management. Most impor-tantly, in cases where the variable space already taxes the memory footprint of the system, we can ill afford sacrificing the lion X  X  share of memory for an auxiliary data structure.
An alternative way to solve the problem exactly without approximation is to store terms compactly, or to employ domain specific variable compression. Below we review three major approaches and discuss their relative merit.
 Hash Kernels: This is the most obvious alternative to Cuckoo Conditional Random Sampling is an attractive strat-Suffix Trees: When dealing with string kernels it is possi-
In this paper we propose an alternative to the above strate-gies that uses Cuckoo hashing as the underlying data struc-ture. Its benefits are as follows:
These desirable properties allow us to solve ` 1 penalized optimization problems and similar sparsity constrained prob-lems without preprocessing or approximation, and makes the data structure an important component for distributed (online) inference algorithms on sparse vectorial data.
Furthermore, in Table 1 we compare Cuckoo with other methods in various aspects such speed, memory, accuracy and abilities.
To serve as a self-contained description, we briefly overview the theory and practice of Cuckoo hashing. Since its first description by Pagh et al. [20 ], it has received considerable attention both in theory and practice. At its heart, Cuckoo hashing relies on on the  X  X ower of two choices X : when per-forming randomized load balancing, being able to choose between two (nearly random) buckets dramatically reduces the maximum load on any single bucket.
Our work uses the recently proposed  X  X artial-key X  Cuckoo variant [25 ], which is fast and achieves high (over 90%) table occupancy. We describe the data structure in two steps: first by explaining basic associative Cuckoo hashing, and then by explaining the partial-key variant.

Like all hash tables, a Cuckoo hash table consists of an array of memory broken into  X  X uckets. X  A hash function H maps a key to be looked up or inserted to to a number in the range (0 , | buckets | ).

In the  X 2,4-Cuckoo X  variant of Cuckoo hashing, two hash functions are applied to keys, producing h 1 and h 2 each bucket contains 4  X  X lots X  for individual key/value items. Each slot can contain any item that maps to that bucket. The process of searching for an item in a Cuckoo hash table is simple: Get(key) : Compute h 1 and h 2 , retrieve buckets b and b 2 , and examine the 2  X  4 = 8 potential items found in them. If the key stored in the slot matches the key being searched for, return the associated value.

The theoretical challenge of Cuckoo hashing is on item insertion: For key k insert , if all slots in buckets b are full, then insert selects an item, k victim in one of these buckets for displacement. It inserts k insert in its place. It then re-inserts k victim into the alternate bucket for k victim If that alternate bucket is full, the process recurses, displac-ing yet another key, and so on, until the items have been successfully inserted. If the process does not succeed within several hundred displacements, the table is full, and a resize procedure is invoked. The theory of Cuckoo hashing indi-cates that this process works with high probability, allows extremely high table occupancy, and has amortized O (1) time to insert (though some individual insertions may be slow).

The partial-key variant permits the table to not store the full key in the slot. Instead, the table stores only some number of bits of the (hashed) key. Cuckooing works as normal. Retrieval, however, may experience false positives : A slot may appear to contain the desired key when it does not. For a 2,4 Cuckoo table with 8 possible slots for each key, a false positive can occur with probability 8 nbits is the number of key bits stored in the table.
Using this mechanism as a building block, we face the chal-lenge of efficiently implementing sparse vector operations atop it. A naive use of hash tables as a sparse vector repre-sentation requires excessive amounts of random access when, e.g., computing the products of vectors or adding them to-gether: Because the same key may be stored at different locations in each hash table, one cannot simply sequentially traverse both data structures.

In nearly all vector-vector operations, at least one of the two vectors can be traversed sequentially (i.e., in random key order, but sequentially through memory). Such a traversal is fast: modern CPUs have extremely high sequential memory bandwidth. However, if the values of interest are not stored in the same order in the second vector (because of cuckooing, or because they are of different sizes), these values need to be accessed randomly.

We describe below our algorithms for sparse vector oper-ations using partial-key Cuckoo hashing. These algorithms take advantage of two optimizations:
Prefetching : When traversing a vector sequentially, cre-ate a  X  X ipeline X  of k items: x 1 ,x 2 ,x 3 ,...,x k . Issue mem-ory prefetches for items 1 ,...,k before returning to item x ,...,x n to perform the actual operation. While these are still random memory fetches, this strategy hides the latency of access to memory, and substantially boosts performance.
First-Biased Insertion: In our Cuckoo hash tables, the insertion procedure defaults to inserting item x at its first somewhat more sparse than is required, and by using wider associativity (e.g., 2,8-Cuckoo), more items can be inserted into their first-hash location. This will induce a larger sim-ilarity of memory locality between two same-sized sparse vectors with many shared items, which can modestly speed the throughput of, e.g., vector addition.
We now adapt the basic data structure to perform efficient linear algebra. Our primary focus is on probabilistic Cuckoo hashing: representations that have a small but nonzero prob-ability of collision. The operations of primary interest are inner products  X  w,x  X  , vector updates w  X  w +  X  , and traver-sal of the nonzero elements of a vector; these are the tools needed to design linear models. In linear algebra these are referred to as Level 1 BLAS subroutines [ 9]. For simplicity of exposition, we omit from the description the handling of the (typically four) associative slots per hash bucket. Their handling is straightforward X  X imply checking four locations at a time X  X nd the performance reduction is minimal, be-cause the slots map to the same CPU cache line. (Four or eight-way associativity is key to achieving high occupancy.) To compute the k x k p p norm we must scan all entries z [ j ]. Because summation is commutative, the scan can traverse x in the order the entries are stored in the Cuckoo hash table: norm  X  0 for j = 0 to 2 d  X  1 do end for Likewise, finding the largest element is order-invariant. Hence a single pass in the order in which elements are stored in x suffices.

Cuckoo hashes have the attractive property that any point-wise operation can be carried out by a simple scan, i.e., in O ( n ) time, given n nonzero entries. This is useful, e.g., when we want to apply the prox operator in algorithms such as FISTA [3 ]. There one applies a gradient element-wise to the entries and subsequently all entries are shrunk back towards 0, hence the name of the algorithm (Fast Iterative Successive Thresholding). 2.2.2 Dot product  X  x,y  X  Computing the dot product requires iterating over all match-ing nonzero entries of x and y . It suffices to check, for each non-zero entry in the vector with the fewest entries, whether that entry matches in the more dense vector. Without loss of generality, assume that x is more sparse. This algorithm is the first that uses biased insertion to optimize its retrieval: When examining item j in vector x , it first tests the equiv-alent slot j in vector y . These two retrievals are both a sequential traversal of their respective vectors. If the item is not found in its primary location, the algorithm will search its alternate location in y , which requires a random access: dot  X  0 for j = 0 with | x [ j ] . value 6 = 0 | to 2 d  X  1 do end for There is obvious benefit in prefetching whenever the signa-tures do not match, i.e., whenever and whenever x is a shorter vector than y even for Hence we look up keys in a non-blocking fashion by issuing a prefetch request, carry on with the current workload and return to the queued-up terms once they are available. As before, the total cost is O (min( n x ,n y )) since the procedure must iterate only over the smaller of the two vectors. 2.2.3 Vector addition z = ax + y
Vector addition is analogous to the dot product, but it cannot skip non-zero entries in either vector, and requires a scalar multiplication of every element in x by a .

If y has fewer elements than x , it is best to accomplish the addition in a separate step. First, set z  X  x , and multiply it by a : for j = 0 with | z [ j ] . value 6 = 0 | to 2 d  X  1 do end for
And then traverse y (the sparse vector), and add its ele-ments into z : for j = 0 with | y [ j ] . value 6 = 0 | to 2 d  X  1 do end for
The case where x is more sparse than y is similar, except that the multiplication and addition can be merged. z is instead initialized as a copy of y , and the non-zero elements in x are multiplied by a and pushed into z . While a copy of a sparse Cuckoo vector is fast (a sequential bulk memcpy), if the destination vector is one of the source vectors, the copy step can be omitted, as is done in the BLAS SAXPY operation.

As before, this can be carried out in linear time, albeit this time in O ( n x +min( n y ,n y )) operations due to the point-wise multiplication x  X  ax . For stochastic gradient descent procedures, the number of nonzeros in x is low since we need to implement w  X  w  X   X  t g t where g t is the gradient computed e.g., on a given document. As a result the updates are essentially as fast as performing O (1) random access to the parameter vector.
The previous two sections motivated the need for a new data structure and showed how it could be constructed ef-ficiently. To demonstrate its versatility, we now apply it to three representative machine learning settings: batch infer-ence, online inference, and distributed inference.
We use LibLinear [ 11 ], a popular machine learning pack-age for solving sparse linear optimization problems, as the baseline for comparison. In a nutshell, LibLinear solves the optimization problem ( 1) with logistic loss function ( 2) as risk R [ w ]. The gradient and Hessian of the risk R [ w ] can be written as  X  w R [ w ] = 1  X  2 w R [ w ] = 1 Here we use only the diagonal component of the Hessian  X 
R [ w ] as a preconditioner. This yields the search direction which is used for line search via Here g is the search direction,  X   X  (0 , 1) is the step size and  X  is the regularization constant.

LibLinear implements an improved GLMNET algorithm analogous to what was summarized above: in its implemen-tation all features are preprocessed and indexed by integers for direct memory access. C-style arrays are used to store features related data such as weight w and gradient  X  w R [ w ].
Extending LibLinear to Cuckoo hashing is straightfor-ward: we only need to substitute all feature related C-style arrays with Cuckoo hash tables. In doing so, the algorithm can use the original training data directly as input.
Cuckoo hashing excels particularly when processing vari-able feature sets in an online fashion. Moreover, it is very amenable to element-wise operations. Hence we implemented the follow-the-regularized-leader proximal [18 ] algorithm (FTRL-Proximal) using Cuckoo hashing and hash kernels as alter-native strategies.

FTRL-Proximal algorithm is a sparse online algorithm. It proceeds as follows: given a sequence of gradients, g t  X  R it performs the update where  X  s is the learning-rate. Using  X  &gt; 0, FTRL-Proximal introduces excellent sparsity. If we store z t  X  1 = g P is solved in the following closed form on a per-coordinate bases [19 ] Note that this operation can be carried out in the Cuckoo hash space since the problem decomposes into d scalar op-timization problems. Hence it can be implemented very ef-ficiently.
Distributed optimization and inference is a prerequisite for solving large scale machine learning problems. There are several open source distributed machine learning frameworks available, such as Parameter Server [15 ], YahooLDA [ 1], GraphLab [17 ], Petuum[ 7], Mahout [2 ], and MLBase [12 ]. We use the Parameter Server as the motivating example, but the discussion can be extends to other frameworks as well. Figure 1: A simplified architecture of parameter server [15 ]
In parameter server, there are three different nodes, which are shown in Figure 1. The globally shared parameters w are partitioned and stored in the server nodes. Each worker node solves a subproblem of R [ w ] and communicates with the server nodes in two ways: to push local results such as gradients or parameter updates to the servers, and to pull recent parameter (changes) from the servers.

In the distributed environment, preprocessing data locally in worker node is impossible, because we may assign differ-ent local indices to the same feature in different work nodes, and then we can not match them in the server node. Global data preprocessing, however, requires extensive data com-munication, which could be a bottleneck of the system.
To implement the FTRL-Proximal algorithm discussed in the previous section on parameter server, we implemented a Cuckoo hashing based server nodes to handle feature keys pushed by the worker nodes. Therefore we can get rid of the expensive global preprocessing.
We use the sparse logistic regression discussed in Sec-tion 1.1 as the benchmark algorithm. We evaluate Cuckoo linear algebra in the following three settings discussed in Sec-tion 3: batch inference, online inference, and distributed in-ference. The Cuckoo linear algebra implementation is based on libcuckoo library 1 .

To evaluate the performance, we use two binary classifi-cation datasets. The first one is a DNA dataset from the Pascal Large Scale Learning Challenge 2 . Each instance in the dataset is represented by an ASCII string of length 200 with symbols { A, C, G, T } . There are 50 million training in-stances and 1 million validation instances. As the labels are extremely unbalanced (a positive/negative ratio of 0.0028), we use Area Under the Curve (AUC) as our metric.

The second dataset is an anonymous online advertising click-through rate (CTR) dataset. In this dataset each in-stance is a display of an ad, and the label is 1 if it is clicked by a user and 0 otherwise. For single machine experiments we sampled about 8.3 million instances and 100 million fea-tures, and for distributed experiments we sampled about 20 million instances and 200 million features. Each feature is represented by a 64-bit signature. Since this dataset can-not be used by LibLinear directly, we first do preprocess-ing to map feature signatures to continuous indices. Note, however, this dataset can be used directly by Cuckoo linear algebra and hash kernel.

All experiments were carried on a university cluster. Each machine is equipped with one Intel Xeon E5620 2.4GHz CPU, 64GB memory, and 1 Gigabyte Ethernet. All com-pared methods are implemented by C/C++ and compiled with GCC 4.8. Setup. In this experiment we compare the following three methods.
 LibLinear: LibLinear uses a dense array implementation. https://github.com/efficient/libcuckoo http://largescale.ml.tu-berlin.de/ Cuckoo: We substituted all feature related dense arrays in Hash Kernels: As a second alternative, we implemented We evaluated these three methods on the CTR dataset. For each method, we report the time, memory consump-tion, and feature reconstruction. The feature reconstruction is measured by the Jaccard similarity. In other words, we first obtained a baseline model from LibLinear, then we ran each method several times and compared the nonzero entries between the baseline model and the model of each method. Denote the nonzero feature sets of two model by A and B , the Jaccard similarity between these two model is We ran each method 10 times and reported the average Jac-card similarity. The results are shown in Table 2 3 . Speed. We first observe that the preprocessing step for LibLinear with dense array takes about 500 seconds, which is roughly 20% of the total running time. In the preprocess-ing step we need to read all the training data, construct a feature vocabulary, map features to continuous indices and then write the whole training data to the disk. Both map-ping and disk IO are time consuming.

The next step is data transformation, which transforms the sparse training matrix from row-major format into column-major format. Taking the advantage of continuous indices, LibLinear with dense array uses the least time.
 In the following training part, it is quite surprising that Cuckoo outperforms the dense array implementation. The reason are twofold: first, the time complexity for accessing
In the process of our evaluation, we discovered and fixed several memory leaks in LibLinear. As a result, the ver-sion of LibLinear we use for evaluation uses, without any algorithmic changes, 30% less memory than the originally-released version [ 11]. an element in Cuckoo hash table is O (1), and with well op-timized technologies such as prefetching as we mentioned before, the performance of Cuckoo hashing is close to dense array. Second, the structural optimization used by Cuckoo hash table induces an more compact representation of sparse vectors and therefore improves the sparse locality. A sim-ilar results can be observed from hash kernel: A small bit length ( N = 20) is almost 5 times faster than a large bit length ( N = 27), since the 20-bit hash kernel fits into L3 cache and has better sparse locality.

Memory. In terms of memory, Cuckoo used about the same memory as LibLinear. The reason is that Cuckoo hash table can have an occupancy ratio greater than 90%, so its memory consumption is almost the same as a dense array. On the other hand, hash kernel with a small bit length can effectively compress the weight and therefore used less mem-ory than others.
 Accuracy. When comparing model performance, Lib-Linear, Cuckoo and hash kernel ( N = 27) provided nearly the same accuracy. This is reasonable since LibLinear and Cuckoo linear algebra both provide exact solution, and when N = 27, hash kernel X  X  key space is slightly larger than the number of features in the dataset (100 million, or about 2 26 . 5 ). Unfortunately, with this accuracy, hash kernel was slower than Cuckoo and consumed more memory than both Cuckoo and dense array. For N = 20, hash kernel was the fastest within the four methods, took only 421 seconds, and consumed only 23 GB memory. However, the accuracy dropped significantly to 90 . 96%.

Feature Reconstruction. One very useful attribute of ` 1 regularizer is feature selection, therefore feature recon-struction is also an important merit. The reconstruction rate of LibLinear is 0 . 8777, which is less than 1 because the ` 1 regularizer does not guarantee unique solution and the inference algorithm (GLMNET) is a randomized method. The reconstruction rate of Cuckoo is 0 . 8773, which is nearly identical to LinLinear, indicating that Cuckoo has perfect feature reconstruction ability. The results of hash kernel is 0 . 2638 for N = 27 and 0 . 0052 for N = 20, which is signifi-cantly worse. Note that even though 27-bit hash kernel used more memory than dense array, the unavoidable conflict de-stroys the reconstruction.

In summary, Cuckoo linear algebra is comparable with the dense array in terms of memory consumption, accuracy, and feature reconstruction while it is faster than the latter. Hash kernel, on the other hand, can trade off between the memory/speed and accuracy, however it cannot win on both sides. In addition, it sacrifices the interpretability.
Setup. In the online inference experiment, we generated the features on the fly. In particular, we used the DNA dataset and chose the substrings of the original DNA strings as features. In keeping with substring kernels of [ 13 ] we in-stantiated all substrings of length 1 to 16, that is, 3,080 substrings per string. On average each string has 2,477 dis-tinct substrings, and the total number of distinct substrings in the dataset is about 800 million. Each of these terms was weighted exponentially according to its length In other words, long strings are penalized further.
Using the DNA dataset we simulated the situation where there are a huge number of potential features and generating a vocabulary for preprocessing would be infeasible. On the other hand, both Cuckoo hashing and hash kernel can sup-port generating features during the training phase without a vocabulary.

Figure 2 shows that the number of unique features in-creases exponentially with the maximum substring length. With maximum length equals to 16, 800 million features are generated.

Besides Cuckoo linear algebra and hash kernel, we added an implementation with C++ STL hash for reference. Fig-ure 3 shows the memory footprint required to store all the features as the maximum substring length increases from 1 to 16. As we can see, with 800 million features, dense ar-ray consumed only 12 GB memory, which is the least among the three. This is reasonable since dense array does not have any overhead storing these features. With the benefit of high occupancy ratio, Cuckoo hash table consumed only 18 GB memory while STL hash table consumed more than 32 GB memory. Also, on average Cuckoo hash table consumed half as much as the memory used by STL Hash as the number of feature increased.

In our implementation of Cuckoo hash table, we allocate the slots as power of 2, so if the number of features exceed 2 , we reallocate 2 k +1 slots. On average Cuckoo hash table consumed 1.7 times more memory than dense array, however if the number of features is less than and close to 2 k , then the occupancy ratio of Cuckoo hash table is high and the memory usage is close to dense array. For example, when the number of feature is about 450 million, Cuckoo hash table Figure 2: The number of unique features versus the maximum length of substring on dataset DNA.
Figure 3: Comparison of the memory usage among all methods by different the number of features. consumed about 9 GB memory and dense array consumed about 6.8 GB memory, while STL hash table consumed 2 times more.

Varying maximal substring length. We compared these methods by varying the maximum substring length. The results when maximum length equals to 14 and 16 are shown in Table 3. As can be seen, preprocessing on this dataset is impossible: on average each instance has 2,477 features, and assume each feature is encoded as a 32-bit integer, then it takes about 460 GB to store all these 50 million training instances, so it cannot be fitted into the memory and trained with LibLinear.

The accuracy of Cuckoo and STL Hash are similar, how-ever, Cuckoo uses 40% less memory and is 1 . 7 times faster than the STL hash implementation. Hash kernel can reduce the memory footprint and time further, however it loses ac-curacy.

Limited memory or time. We evaluated the accuracy of these methods under maximum allowable memory con-straints by varying the maximum substring length. Figure 4 shows the results. Given the same memory constraint, Cuckoo was more memory efficient and can store more fea-tures than STL hash, therefore it obtained higher accuracy. We also varied the bit length of the hash kernel, and re-ported the best results on different substring length. Using small bit length, the superiority of hash kernel allows it to process more features and therefore was comparable with the STL hash. However, when using a larger bit length it performed worse.

Next, we evaluated these methods under maximum allow-able time constraints with substring length fixed to 16. The results is shown in Figure 5. Cuckoo outperformed STL hash as it is faster and able to process more examples. It is worth noting that hash kernel is comparable with Cuckoo, the rea-son is that hash kernel also has a constant time complexity and thus can process more examples in the given time to compensate the loss due to feature conflict.

In summary, both Cuckoo, STL hash and hash kernel sup-port generating features on the fly and thus online inference. Table 4: AUC of distributed FTRL-Proximal algo-rithm with 6 server nodes and 12 worker nodes and CTR dataset.
 However Cuckoo is more CPU and memory efficient than STL hash and it provides exact solution, thus under a fixed memory constraint or time constraint greater than 1600 sec-onds, Cuckoo outperformed both STL Hash and hash kernel and achieved the highest AUC.
Setup. We use the parameter server [15] as our machine learning framework to examine Cuckoo linear algebra in the distributed inference. We directly modified the built-in im-plement of FTRL-Proximal algorithm in parameter server. In other words, we substituted the STL hash used by the server node with Cuckoo linear algebra and hash kernel in our experiments.

We fixed the number of machines to 6, and each machine has 2 workers. This configuration can fully utilize the com-putational power. We varied the number of server nodes between 1 and 12. Decreasing the number of server nodes will increase the workload of each server. For hash kernel, we fix the bit length to 24.

Vary the number of server nodes. Experiment results are shown in Figure 6. Compared with STL hash implemen-tation, Cuckoo reduced the total training time of the whole system by more than 30% when the server nodes are more than 2. The significant performance boost also indicates that the server nodes are the bottleneck of the parameter server. In terms of memory, Cuckoo used 25% less mem-ory than STL hash when the number of server is 3 or 6, and with Cuckoo we can serve all workers with only 1 or 2 server nodes, while the STL hash implementation ran out of memory.
AUC Figure 4: AUC under different memory constraints by varying the maximum length of substring from 1 to 16. We also varied the bit length of hash kernel and reported the best results on all substring length. AUC 0.68 0.72 0.74 0.76
Figure 5: AUC under different time constraints by fixing the maximum length of substring to 16. time (seconds) memory (GB) Hash kernel, on the other handle, has similar speed as Cuckoo. It can further reduce the memory requirement as expected. However, it decreased the accuracy. Table 4 shows the AUC achieved by these three methods. As can be seen, Cuckoo is comparable with STL hash, and outperforms hash kernel by . 2% AUC. Due to the commercial importance of ads click prediction, even an . 1% improvement brings signif-icant revenue improvement for the whole company.

As a summary, Cuckoo linear algebra is a perfect sub-stitute for hash table used by distributed machine learning frameworks to improve both CPU and memory efficiency.
In this paper we proposed to use Cuckoo hash as the un-derlying data structure for sparse vectors. It is highly mem-ory efficient and allows for random access at near dense vector level rates. We defined linear algebra operations based on this representation, and showed it can be easily applied to various machine learning algorithms, including batch inference, online inference, and distributed inference. We demonstrated its efficacy by comparing with commonly used methods, such as dense array with data preprocess-ing, standard STL hash, and hash kernel with different bit length. The experimental results showed that Cuckoo lin-ear algebra outperform the others either in speed/memory, accuracy or both.
 The authors thank Paul Bradley, Tyler Johnson, and Car-los Guestrin for inspiring discussions and experimental in-vestigation in the context of hash kernels and sparsity and Quoc Le regarding optimization. Parts of this work were supported by grants from Google and Amazon. [1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, [2] Apache Foundation. Mahout project, 2012. [3] A. Beck and M. Teboulle. A fast iterative [4] P. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous [5] A. Z. Broder. Computational advertising and [6] A. Z. Broder, M. Charikar, A. M. Frieze, and [7] W. Dai, J. Wei, X. Zheng, J. K. Kim, S. Lee, J. Yin, [8] J. Dean and S. Ghemawat. MapReduce: simplified [9] J. J. Dongarra, J. D. Croz, S. Hammarling, and R. J. [10] B. Fan, D. G. Andersen, and M. Kaminsky. The [11] R.-E. Fan, J.-W. Chang, C.-J. Hsieh, X.-R. Wang, and [12] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griffith, [13] C. Leslie, E. Eskin, and W. S. Noble. The spectrum [14] G. Leung, N. Quadrianto, A. J. Smola, and [15] M. Li, D. G. Andersen, J. Park, A. J. Smola, [16] P. Li, K. Church, and T. Hastie. Conditional random [17] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [18] B. McMahan. Follow-the-regularized-leader and mirror [19] H. B. McMahan, G. Holt, D. Sculley, M. Young, [20] R. Pagh and F. F. Rodler. Cuckoo hashing. Journal of [21] A. J. Smola and S. Narayanamurthy. An architecture [22] E. Ukkonen. On-line construction of suffix trees. [23] S. V. N. Vishwanathan and A. J. Smola. Fast kernels [24] K. Weinberger, A. Dasgupta, J. Attenberg, [25] D. Zhou, B. Fan, H. Lim, M. Kaminsky, and D. G.
