 Web search queries provide a surprisingly large amount of in-formation, which can be potentially organized and converted into a knowledgebase. In this paper, we focus on the problem of automatically identifying brand and product entities from a large collection of web queries in online shopping domain. We propose an unsupervised approach based on adaptor gram-mars that does not require any human annotation efforts nor rely on any external resources. To reduce the noise and normalize the query patterns, we introduce a query standard-ization step, which groups multiple search patterns and word orderings together into their most frequent ones. We present three different sets of grammar rules used to infer query structures and extract brand and product entities. To give an objective assessment of the performance of our approach, we conduct experiments on a large collection of online shop-ping queries and intrinsically evaluate the knowledgebase generated by our method qualitatively and quantitatively. In addition, we also evaluate our framework on extrinsic tasks on query tagging and chunking. Our empirical studies show that the knowledgebase discovered by our approach is highly accurate, has good coverage and significantly improves the performance on the external tasks.
Queries collected by web-scale search engines provide a large amount of useful information. This information can be organized to knowledgebase [5] which can impact and im-prove the performance of a wide variety of applications such as parsing, coreference resolution, entity linking. Typically, researchers focus on extracting entities for movie, music, per-son, location, and organization names from natural language texts [22]. Others have shown that knowledgebase can be easily built from such types by consulting external resources, such as IMDB [9], DBpedia [10], or Wikipedia [24]. However, one bigger challenge is how to learn knowledgebase for more rare yet useful entity types such as brand and product . These entities are important and crucial for commercial search engines and online advertising to function properly. Formally, we define the brand and product entity types as:
Definition 1. A brand entity is defined as a named entity phrase whose master 1 business is to manufacture, provide or sell one or more major products or services. It also applies to subsidiaries or even divisions that operate as a separate company and master manufacturer producing or selling their own products, for instance,  X  Lexus  X  (owned by  X  Toyota  X ) and  X  Chevrolet  X  (owned by  X  GM  X ).

Definition 2. A product entity is defined as an entity phrase specifying generic product terms, e.g.,  X  jeans  X ,  X  smart-phone  X ,  X  running shoes  X . Note that a product does not include any attributes, but may include necessary generic specification. For example,  X  wedding dress  X  and  X  dress  X  are considered two different products, since they are generically different products, but  X  black dress  X  is not considered a stan-dalone product, since it is an actual product ( X  dress  X ) with color attributes ( X  black  X ). 2 To the best of our knowledge, there are no publicly available knowledgebase with updated brand and product entity lists.
Fortunately, online shopping queries collected at commer-cial search engines provide large amount of information, which can be organized into such knowledgebase [26]. For example, the query  X  calvin klein white t shirts  X  contains explicit information about the brand ( X  calvin klein  X ) and product ( X  t shirts  X ) with arbitrary attributes or constrains ( X  white  X ). Utilizing this information is able to greatly reduce human annotation time and editorial labeling efforts, which can be both expensive and time consuming to acquire.
However, extracting brand and product entities from web-scale query logs in online shopping domain is fundamentally different from classic named entity recognition [23, ner frameworks. Past entity extraction works focus on natural language text [22, 30] or external web resources [20, 31]. They target on identifying entity phrases, such as person, location and organization, etc, using some indicative fea-tures (e.g., capitalization), trigger words (e.g.,  X  Mrs  X ,  X  Inc  X ) or some grammar properties (e.g., structural patterns, like a location address usually follows the pattern of street +
The definition  X  X aster X  is to distinguish the concept of brand from other named entity types, such as product family of organization . For example,  X  nexus  X  (owned by  X  Google  X ) is not a brand , but a product family . It does not apply to branding or advertisement carriers neither, e.g.,  X  Manchester United  X  is considered organization .
This definition can be empirically more generic and fine-grained than the extended named entity hierarchy at nlp.cs. nyu.edu/ene/. city + state | province + zip codes ). In addition, classic frameworks usually operate on a collection of sentences or paragraphs, which contains rich contextual information and syntactic structures, such as part-of-speech ( pos ) tagging, dependency parsing, etc.

On the other hand, online shopping queries often do not have indicative features nor do shopping entity types contain any trigger words. In terms of grammar properties, majority of web search queries consist of proper nouns or noun phrases only [3, 27], which often lacks explicit syntactic dependency structure. In addition, they are usually short and noisy (e.g., misspelling, arbitrary word ordering). All of these factors rise severe difficulties in learning brand and product entities from web query logs.

In this paper, we focus on a fundamentally challenging yet interesting problem of automatic extracting brand and prod-uct entities from web-scale online shopping queries. Ideally, the extraction framework should satisfy following: 1: It is unsupervised and automatic, which does not re-2: It is robust to various kinds of noise and dependency 3: It is domain-independent , and easily generalizable to 4: It is data-driven , i.e., the numbers of brand and product
In this work, we propose a novel framework which fulfills all these requirements. Our contributions in this paper lies in the following aspects. We first formulate the entity extrac-tion problem, in its general form, as an unsupervised shallow parsing problem with probabilistic context-free grammars [21, pcfg ] on a collection of online shopping queries. This allows our model to be unsupervised and domain-independent , and hence easily generalizable to multiple domains. We intro-duce a query standardization step to reduce the sparsity of the word orderings and query patterns in data. Our proposed model relies on adaptor grammars [15] X  X  nonparametric counterpart of pcfg  X  X hich is completely data-driven and robust to noise. We subsequently propose three different grammar rule sets to regulate different pattern variants in the data. We then compare our model against strong baselines and conduct comprehensive evaluations on the extracted knowledgebase both qualitatively and quantitatively. Intrin-sically, we demonstrate that our framework is able to discover high quality knowledgebase. We also extrinsically evaluate the extracted knowledgebase on applications of tagging and chunking. Our result indicates that knowledgebase generated by our method significantly improves the performance.
We organize our paper as following. In Section 2, we review the adaptor grammar model, and discuss its inference procedure using Markov chain Monte Carlo ( mcmc ) method. We introduce our unsupervised entity extraction framework based on adaptor grammars in Section 3. To give an objective assessment on our proposed method, we conduct extensive experimental analysis and empirical study on web-scale query dataset collected from online shopping domain in Section 4. In Section 5, we review some past approaches on related problem. Finally, we conclude the paper and point out some possible future research directions in Section 6.
 Figure 1: An example of a phrase-structure tree and un-derlying probabilistic context-free grammar ( pcfg ). pcfg assume the set of grammar rules are defined a priori and fixed. Therefore, rewriting operations are independent given the nonterminal, i.e., disregarding the yields and structure of the derivation tree. For example, in the query dataset, the highlighted derivation trees may appear several times X  possibly with different brand or product  X  X uch derivation trees should be cached as a new grammar rule. Adaptor grammars [15, ag ] break this independence assumption by jointly modeling the context and grammar, i.e., a nonpara-metric pcfg , allow to incorporate new context-dependent grammar rules from data.
In this section, we discuss probabilistic context-free gram-mars and adaptor grammars.
Probabilistic context-free grammars ( pcfg ) define probabil-ity distributions over derivations of a context-free grammar. A pcfg G is defined as a set of variables  X  W , N , R ,  X   X  . Given a collection of terminals W and a collection of nonterminals N , G is described by a set of probabilistic grammar rules  X  R ,  X   X  . Let us denote the collection of all the rules rewrit-ing a nonterminal c is R ( c ). For a nonterminal c , each of the grammar rules r 7 X   X   X  R ( c ) X  X ommonly referred to as a production  X  X s associated with a probability  X  r 7 X   X  P
For any given sentence, a pcfg starts with the unique start symbol S  X  N , recursively rewrites a nonterminal into its derivations according to the probabilistic grammar rules  X  R ,  X   X  . This builds a hierarchical derivation tree structure , starting from a nonterminal to a sequence of terminals, i.e., leaf nodes. This sequence of terminals often referred to as the yield of the derivation tree. Figure 1 illustrates an example of a derivation tree, its yields and underlying probabilistic context-free grammar ( pcfg ). Interested readers may refer to [21] for a more detailed description of pcfg s. pcfg s assume the set of grammar rules are defined a priori and fixed. Therefore, rewriting operations are inde-pendent given the nonterminal, i.e., disregarding the yields and structure of the derivation tree. This context-freeness assumption often can be too strong for modeling natural language. Referring to Figure 1, in the query dataset, the derivation tree Brand 7 X  \ pottery barn may appear over and over again, possibly with different products. If every time, such tree is constructed from raw pcfg rules, it is both time consuming and possibly error prone.

Adaptor grammars [15] break this independence assump-tion by jointly modeling the context and the grammar. It
Figure 2: Illustration of the Chinese restaurant franchise. is a nonparametric version of pcfg , which allows the model to incorporate new context-dependent grammar rules in a completely data-driven fashion. It specifies a set of adapted nonterminals, often called the adaptors . For each adaptor c , the model subsequently imposes a nonparametric prior on the distribution over its parse trees. This allows the model to dynamically learn more meaningful derivation trees and expand the rule set according to data. In general form, adap-tor grammars use Pitman-Yor process [29, pyp ] prior, which is also called the Pitman-Yor adaptor grammar ( pyag ).
One realization of pyp is via the view of the Chinese restau-rant process [13, crp ], parametrized by scale parameter a , discount factor b and base distribution G c . The crp as-sumes a set of tables, each of which serves a dish (a derivation tree in our context) randomly drawn from the base distribu-tion, i.e., z c  X  G c . Each customer (an adapted nonterminal c in our context) entering the restaurant chooses a table to sit based on the choice of all previous customers (Figure 2). Let us denote K as the number of tables occupied by all past n  X  1 customers, and the sequence x 1 , ... ,x n  X  1 represents the table indices that they sit at, i.e., x i  X  X  1 , ... ,K } . The n -th customer choose to sit at table x n where m k is the number of customers sit at table k , i.e., P k =1 m k = n  X  1. Variable  X  K +1 stands for the case that a new table is chosen, i.e., a new instance of derivation tree is sampled. Therefore, the n -th customer chooses to sit at a new table K + 1 with probability Kb + a n  X  1+ a and an existing
According to exchangability and de Finetti X  X  theorem, all customers in crp are mutually exchangeable, and do not alter the distribution, i.e., all m k  X  X  remain the same. Therefore, for a given distribution over number of customers per table n = { n 1 ,...,n K } , its probability is governed by pyp where K is the number of tables occupied and n is the number of observed samples drawn from crp , i.e., total number of customers. The crp formulation of pyp enables us to infer the latent variables in pyag using Markov chain Monte Carlo ( mcmc ) method, which we will discuss in Section 2.3.
Formally, a Pitman-Yor adaptor grammar pyag A extends a pcfg and defined as following variables: 1: a collection of terminals W , nonterminals N and context-2: Dirichlet prior  X  c for the pcfg production probabilities 3: a set of non-recursive adapted nonterminals M  X  N ; 4: pyp parameters a c ,b c for each adaptor c  X  M . In following section, we discuss in details about the inference process using a Metropolis-Hastings sampler.
The inference process is to learn the latent variables from observed data, i.e., p ( T | X ). To accomplish this, we first write down the joint distribution over their derivation trees T given a collection of sentences X , is where n c ( T ) represents the frequency vector of all adapted rules for nonterminal c being observed in T , and f c ( T ) repre-sents the frequency vector of all pcfg rules for nonterminal c being observed in T . The posterior probability for Dirichlet p dir ( f |  X  ) is where K = | R ( c ) | is the number of pcfg rules associated with c , and variables f and  X  are both vectors of size K .
Given an observation string x i , in order to compute the posterior distribution over its derivation trees, we need to normalize p ( T |  X  , a , b ) over all derivation trees that has yields x . This probability is, unfortunately, intractable to com-pute, so we turn to Markov chain Monte Carlo ( mcmc method and iteratively sample t i  X  p ( t i | x i , T  X  i struct an  X  X uxiliary X  pcfg variable G 0 that emulates the pyag behavior, and approximate the conditional distribution with it. The pcfg approximation G 0  X   X  W , N , R 0 ,  X  0 be viewed as a static snapshot of the pyag given all the derivation trees T  X  i . Let us assume T  X  i are observations from a pyag A  X   X  W , N , R ,  X  , M , a , b  X  , the rule set R pcfg approximation G 0 is defined as where z c represents the set of all derivation subtrees observed in T  X  i that rooted at c , and yields ( z ) represents the yields of derivation tree z . Their corresponding rule probability is where f c 7 X   X  ( z c ) is the frequency count of observing produc-tion c 7 X   X  in all derivation tree set z c , and n z is the frequency count of observing a particular tree z in all z c . Variable m is the number of unique derivation tree, i.e., m c = | z c n c is the frequency count of observing all derivation trees rooted at c , i.e., n c = P z  X  z
The approximation pcfg G 0 offers an efficient sampling alternative using Metropolis-Hastings to generate draws from the conditional p ( t i | x i , T  X  i ) [11]. We use it as our proposal distribution in a Metropolis-Hastings algorithm. Let us assume t i is the current derivation tree, and t 0 i is a sample from p ( t i | x i , T  X  i ), we accept the proposal with probability in formational n avigational t ransactional Table 1: Some sample patterns and words for general domain-independent query filtering and preprocessing. We ignore all queries mapping to any of these pattern to reduce sparsity and modeling noises in query dataset.
 The overall inference procedure iteratively updates G 0 ,  X  t until convergence, which follows the standard paradigm: 1: Randomly initialize parse trees for all observations. 2: while model not converged do 3: Randomly choose observation x i and its derivation 4: Construct pcfg approximation G 0 from T  X  i , i.e., up-5: Sample a parse tree t 0 i from G 0 and accept the proposal 6: Adjust the associated counts with rules and grammars.
Our approach consists of three different steps. The first preprocessing step (Section 3.1) focuses on cleaning  X  X ut-of-domain X  queries in different taxonomies (i.e., informational, navigational, and transactional queries) and regularizing the  X  X n-domain X  queries (filtering out domain-dependent stop-words). After that, we apply a query standardization step X  which we discuss in Section 3.2 X  X n all queries to further decrease their sparsity. Finally, in Section 3.3, we describe the grammar rules used for the adaptor grammar.
One universal problem of query processing and understand-ing is the diversity of the queries, which introduces large amount of noise to modeling. The noise is due to possi-bly multiple reasons, for example, misspelling, tokenization. In addition to misspelling noise, following the general web search taxonomy [4], queries are usually of different forms, including informational (e.g.,  X  X hat is gnc whey protein X ), navigational (e.g.,  X  X ww bobbi brown com X ), and transac-tional (e.g.,  X  X oreal eye serum versus estee lauder eye cream X  or  X 10 best man cologne X ). Table 1 shows some of the general patterns and words for each category. In our settings, we ignore any queries matching these forms.

To reduce the noise during our modeling, we also collect some non-conventional stopwords commonly used in shop-ping queries. For example, marketing events (e.g.,  X  X ale X ,  X  X romo X ,  X  X eal X ), shopping fashion (e.g.,  X  X nline X ,  X  X hipping X ), price range (e.g.,  X  X iscount X ,  X  X heap X ), working condition (e.g.,  X  X sed X ,  X  X efurbished X ), popularity (e.g.,  X  X opular X ,  X  X ot X ,  X  X ool X ), quality control(e.g.,  X  X ood X ,  X  X ice X ), etc. These stop-words can be applied universally on all domains, for example X  in the scope of this paper X  apparel , electronics , health and beauty , home and garden , and sporting goods . In practice, these non-conventional stopwords can be extracted using statistical methods, for instance, inverse document frequency. Besides such preprocessing step, to further reduce the spar-sity over search patterns and word orderings, we subsequently perform a query standardization step.
We collect a set of online shopping queries sampled during a 9-months interval. There are large number of queries that have different word orderings despite the fact that people were searching for the same brand and/or product. For example, the queries  X  florajen probiotics  X  and  X  probiotics florajen  X  lead to the same product ( X  probiotics  X ) from the brand ( X  florajen  X ). We refer to the set of queries containing exactly the same bag of words as one unique query family .
Theoretically, given n words, there can be n ! number of unique word orderings available. However, in practice, we do not observe all possible word sequences in the search log for that particular query. In fact, we discover that the distribu-tion over all possible word sequences for any particular query family is very sparse, regardless of different query length n . In practice, we find out that approximately more than 90% of the query families appear in exact one word ordering in our search log, and less than 2% exhibit more than two word orderings, regardless of the query length. In addition to being sparse, the distribution over the word orderings tends to be highly skewed, regardless of the query length or the number of word orderings. Namely, for a query of length n , the number of word orderings actually being observed is significantly smaller than the number of all of its possible word orderings, i.e., n !. For instance, significant fraction (approximately 70% to 90%) of them have one predominant word ordering which accounts for more than 60% of the total traffic against all possible word orderings. This well suggests that, in online shopping domain, one query family usually searches for one particular brand and/or product. 3
Often, users also keep entities as continuous phrases during search, i.e., do not interleaving across each other. For in-stance, in our random sample, we observe following different word orderings for  X  miracle gel  X  from  X  sally hansen  X : where all other possible sequence combinations of these words do not appear in the collected search queries at all. Note that, in this case, the product entity  X  miracle gel  X  gets rewritten to  X  gel miracle  X  in a small fraction of queries, but it remains continuous as one product entity. Therefore, it is fairly unlikely to see queries or word orderings like  X  sally miracle gel hansen  X  or  X  miracle sally hansen gel  X , etc. The sparsity property in word orderings reveals substantial information about online search queries, and essentially enables us to model them using adaptor grammar.

However, these less frequent word orderings certainly intro-duce a lot of noise during modeling. To address this problem, we perform a query standardization step to reduce the spar-sity and group all the word orderings in a query family to the most frequent one. The idea of query standardization is
This phenomena is frequently hold within some particular domain, but not necessarily across different domains. For instance, the query  X  paris hilton  X  is often referred to the person in celebrity domain, whereas  X  hilton paris  X  often searches for hilton hotel in the city of paris in travel domain.
The number after shows the probability of observing the corresponding word ordering in search logs. summarized as following: 1: for every query family do 2: Collect distribution { w i : c i } , where c i is the frequency 3: Find the word ordering with highest frequency, i.e.,
Take the above examples as illustrations, query standard-ization step reduces multiple search patterns and word or-derings to their most frequent ones like following: sally hansen miracle gel (96 . 60%) miracle gel sally hansen (3 . 37%) sally hansen gel miracle (0 . 03%) It greatly reduces the sparsity in search patterns and word orderings, hence, significantly improve the parsing perfor-mance. In our experiments, our study shows that on average about half of the unique word orderings are aggregated to their predominant patterns.
As previously discussed, users tend to construct their queries in online shopping domain with continuous entity phrases (e.g., brand , product ), and do not interleave them across each other. This allows us to formulate the problem into a chunking problem, which is also commonly referred to as the shallow parsing of a sentence [1]. However, there are still challenges remaining on identifying brand and product entities from online shopping queries. One particular chal-lenge in this case, is that, unlike sentences or paragraphs, they often lack of the grammatical information or syntactic structure, since majority of them consist of proper nouns or noun phrases [3]. For instance, part-of-speech ( pos ) tagging information, in this case, is often less accurate than struc-tured sentences or documents. Therefore, classical chunking models do not often work very well.

In addition to word orderings being sparse and highly skewed for each query family, queries often adhere to some generic patterns in online shopping domain, for example,  X 
Brand Product  X . These query search patterns are pre-served from aggregated query families. Similar to the dis-tribution over word orderings, the distribution over these patterns is also sparse and highly skewed. Out of the queries searching for the same brand and/or product, we find that significant amount of them follow the pattern of  X  Brand Product  X , despite the variants of word ordering in each en-tity type. In fact, for all search queries in shopping domain, pattern  X  Brand Product  X  is more predominant than any other patterns, such as  X  Product Brand  X . Such a property inspires us to solve the problem with an approach akin to shallow grammar parsing, and we want our chunking model to be as  X  X ontext-free X  as possible.

However, as discussed in Section 2, vanilla probabilistic context-free grammars ( pcfg ) model imposes strong inde-pendence assumption between grammar rules and internal structures of derivation trees, such context-freeness hypothe-sis may often lead to less accurate language modeling. Unlike pcfg  X  X ts parametric counterpart X  X daptor grammars con-stantly cache the derivation trees and dynamically expands the grammar rule set in a completely data-driven fashion. Therefore, it provides a more flexible modeling option than pcfg . In addition, it is an unsupervised method, and hence does not require any human annotation efforts to extract the brand and product entities from a large collection of online search queries.

We start with a simple grammar that decomposes a query into a brand , a product or a simple combination of both: Qu ery  X  Brand P roduct  X  W ords Query  X  Product W ords  X  Word Words Query  X  Brand P roduct W ords  X  Word Brand  X  W ords Word  X  . . .
 The underlined nonterminals refer to the adaptors, i.e., the adapted nonterminal nodes, such that derivation trees rewrit-ing them will be cached by the framework, and subsequently added to the grammar rule set. We refer this grammar as the chunk grammar.

However, in practice, we found that it is not enough to cap-ture some other common patterns with prepositional phrases, for example,  X  sephora shea butter for lips  X ,  X  chin strap to stop snoring  X ,  X  massage oils at walgreens  X ,  X  nail polish bottles with brush  X , etc. Therefore, we propose the chunk+prep grammar to explicitly modeling prepositional phrases: Qu ery  X  Brand B rand  X  W ords Query  X  Product P roduct  X  W ords Query  X  Brand P roduct W ords  X  Word Words Query  X  Query PrepPhrase W ords  X  Word PrepPhrase  X  P rep Words Word  X  . . .
 Prep  X  for | to | by | with | at | ...
 These prepositional phrases are sometimes informative, such that they reveal possible brand entity, e.g.,  X  at walgreens  X ,  X  at cvs  X . On the other hand, they can also be noisy, e.g.,  X  at home  X ,  X  at night  X . This grammar explicitly models all prepo-sitional phrases. We leave the study of utilizing prepositional phrases to identify brand and product entities as future work.
This grammar, again, overlooks another important infor-mation, i.e., it does not capture the modifier information about the product. These modifiers could be descriptions, but more likely are model names or product families. For example, the word  X  professional  X  in the query  X  andis pro-fessional hair clippers  X  is a general description. The phrase  X  despicable me minion  X  in the query  X  despicable me minion hoodie  X  modifies a product. The word  X  hdmi  X  specifies a product family in the query  X  iogear wireless hdmi transmitter and receiver  X . The phrase  X  galaxy s1  X  in the query  X  sam-sung galaxy s1 cell phone  X  is a model number. The word  X  tv  X  specifies the purpose of the product in the query  X  ge coaxial tv cable  X . To address this, we further extend it to the chunk+prep+mod grammar: We found this grammar works best in practice. In Section 4, we conduct experimental study on the performance under different grammars, and show empirical evaluation of our approach against the vanilla pcfg approach.
In this section, we report the empirical result of our ap-proach on a large collection of web shopping queries. To the best of our knowledge, there are no off-the-shelf unsupervised methods available to automatically identify brand and prod-uct entities from online shopping search queries. We compare our approach against the vanilla probabilistic context-free grammar [21, pcfg ] with the same parsing grammar. Differ-ent than adaptor grammars, which imposes a Pitman-Yor process prior on the distributions over grammar rules, we assume a Dirichlet distribution as the prior for pcfg . We learn the pcfg using Bayesian inference with a mcmc sam-pler [16]. This pcfg approach is akin to the shallow parsing approach [1] as discussed in Section 3.3, so serves as a com-parable unsupervised baseline for our model. In addition, we also compare against a semi-supervised approach [25, cikm07 ]. For every domain and entity type, we seed the model with a collection of 5 popular entities as suggested in [25]. The seeded list for corresponding brand and product entities for each domain are shown as below: We first conduct intrinsic evaluation of our model against both pcfg and cikm07 on knowledge discovery. Then, we assess the quality of the inferred knowledgebase extrinsically by using them as features on two external tasks.
The data we use are raw queries in shopping domain col-lected from a commercial search engine during a 9-month time interval. We retrieve all queries that have at least one online ads click, and randomly sample them. Based on the most frequent product category of their clicked ads, these queries are classified into following 5 different domains: Ap-parel , Electronics , Health &amp; Beauty , Home &amp; Garden , and Sporting Goods , which contains 906 K , 444 K , 625 K , 1 . 1 M and 412 K unique queries respectively. For both pcfg and our unsupervised approaches, we train them based on adap-tor grammars on all available data. During our experiments, we let all mcmc samplers running for 2000 iterations to make sure all models are fully converged. For our unsu-pervised approach, we examine hyperparameter settings of a = { 0 . 01 , 0 . 05 , 0 . 1 } and b = { 100 , 500 , 1000 } for the pcfg approach, recall that Dirichlet distribution is de-fined by its scaling parameter  X  , we examine different settings of { 0 . 01 , 0 . 05 , 0 . 1 } in our experiments. For both unsuper-vised approaches, we report the performance on the model with highest data likelihood. We collect the distributions over all discovered entities and rank them according to their probabilities. For semi-supervised approach cikm07 , we rank all entities by Jensen-Shannon as suggested. We evaluate all approaches on the precision measure at different rank of the retrieved entity types.
 of our approach, we looked into the precision of the retrieved entities at different rank for different approaches with differ-ent grammars. Precision measures the percentage of retrieved entities which correctly belong to an entity type. The preci-sion is evaluated by professional editors judging if a retrieved entity belongs to the corresponding type. Figure 3 captures the performance on precision at different ranks of both brand and product entities discovered in dataset of each domain. Our approach constantly achieves better performance than cikm07 and pcfg under all grammars. For grammars, we notice that chunk+prep+mod grammar yields better or comparable performance than other two grammars. We find that the semi-supervised approach cikm07 often discovers en-tities in a greedy manner. For instance, in addition to  X  apple  X , it also ranks high on  X  apple mac  X  and  X  hdmi cable for apple  X  as brand names. Besides  X  dress  X , it also identifies  X  calvin klein dress  X  or  X  black dress  X  as product . In addition, we also find cikm07 model takes much more memory resource and longer computation time to train, partly due to the large variance in search patterns. Subsampling the dataset increases the speed, but would downgrade the performance. In contrast, our proposed approach (and all pcfg -based approaches, in general) utilizes the context-free grammar rules, which are more compact, efficient and memory-friendly than flat repre-sentations. For sporting goods domain, all approaches yield low precision in identifying brand entity types. This is mainly due to the reason that our models discover a lot of sports team and club names (refer to Table 2 for examples). By our guidelines in Definition 1, even though these sport clubs are the branding or advertisement carriers for many products, they are considered organizations , rather than the actual brand . Instead, their respective sponsors X  X uch as  X  nike  X ,  X  under armour  X , etc X  X re.
 Discovered Entities. Table 2 lists the most frequent brands , products , prepositional and modifier phrases identified by our approach based on adaptor grammars. Our approach is able to correctly identify complicated brand entities (e.g.,  X  tiffany and co  X ,  X  at t  X ,  X  bath and body works  X ,  X  bed bath and beyond  X  and  X  green bay packers  X ) and product entities (e.g.,  X  engage-ment ring  X ,  X  remote control  X ,  X  shampoo and conditioner  X ,  X  coffee table  X  and  X  hoodie  X ) of arbitrary length in a com-pletely unsupervised data-driven fashion. One interesting observations is that our model discovers some joint phrases, such as  X  samsung galaxy  X , which is more like a combination of a brand entity ( X  samsung  X ) with a model modifier  X  galaxy  X . This again is due to the nonparametric nature of the model-ing framework, i.e., if the model sees a large amount of such phrases appear in the dataset, it would identify the entire phrase as a segment. On the other hand, pcfg is less capable in discovering such phrases, simply because it imposes too much independent assumption on the underlying grammar. One thing worth to note is, for the sporting goods category, we discover a lot of sports team names, these are due to the reason that queries fall into this domain are often looking for team jerseys, jackets, hats, etc.

In addition, our approach also discovers a lot of popular prepositional phrases for each shopping domain, e.g.,  X  for little girls  X ,  X  with answering machine  X ,  X  for oily skin  X ,  X  by the yard  X  and  X  for beginners  X , as well as some common modifier under each domain, e.g.,  X  halloween  X ,  X  all in one  X ,  X  anti aging  X ,  X  stainless steel  X  and  X  super bowl  X . These prepositional and modifiers phrases are modeled jointly with the brand and product entities, and provides additional information or constraints to the products.
In addition to intrinsic evaluation, we also conduct extrinsic empirical study on the quality of our discovered knowledge-base to external tasks. We use knowledgebase extracted from different approaches as features to external tasks and evalu-ate its effectiveness. For the purpose, we build, train and test two supervised algorithms X  X  query tagging model based on conditional random fields [18, crf ], and a query chunking model based on maximum entropy method [17, maxent ].
For every category, we reserve a collection of 3 K random samples, and manually annotate them with professional edi-tors. In addition to brand and product , the label space also includes other entity types, such as model number, prod-uct family, attribute specification, etc. We split the labeled dataset by 80% for training and 20% for testing. For both crf and maxent models, we use contextual features includ-ing words and their lemmas in a 5-word surrounding window; and lexical features including surface, pattern, shape and length of previous, current and next word. In addition, we also use the knowledgebase lookup features by examining if we find a match of the target word in any entity inside our extracted knowledgebase. We gradually increase the size of the knowledgebase, and evaluate the performance of crf and maxent at different rank. We choose all parameters using 5-fold cross validation on the training data, and report the performance of the best settings. call, and F 1 score of crf model on different entity types in 5 domains against the rank (size) of the knowledgebase used to generate features. These knowledgebases are extracted by different approaches with different grammars. We grad-ually increase the knowledgebase by the step size 50. The precision measure drops initially, but after we increase the knowledgebase to a certain threshold, which has sufficient coverage over all entity types, it starts to improve. On the recall measure, the performance of crf tagger constantly improves when we reveal more entries from the discovered knowledgebase to the model. This implies the knowledgebase discovered by our model provides a good coverage to this tasks. The overall F 1 score demonstrates consistent improve-ments against entity rank. In addition, we show that the crf sequence tagger constantly yields better performance when using knowledgebase extracted by our model based on adaptor grammar, as oppose to using the one discovered by the pcfg or cikm07 . This well indicates the knowledgebase discovered by our model is of good quality and hence can be used to improve the performance of external models. call, and F 1 score of maxent model in 5 domains against the rank (size) of the knowledgebase used to generate fea-tures. We observe a similar behavior with previous query tagging task, such that the performance on recall and F 1 measure of maxent model constantly improves. This is possibly due to the reason that, as we reveal more entities from the knowledgebase discovered by our approach to the model, it establishes more sufficient coverage to the chun-ker. In contrast, the knowledgebases extracted by pcfg and cikm07 models are not effective enough, and the respective performance on recall and F 1 measure does not change much. This again well implies the knowledgebase discovered by our model provides much better coverage than the ones extracted by pcfg or cikm07 approach.
The objective of this work is to automatically identify brand and product entities from query logs in a complete unsupervised way. This is very different from classic named entity recognition ( ner ) problem, and can be significantly more challenging. First, unlike person names, organization titles or location labels, almost all of the products are not notable entity names. They often do not carry very indicative surface features (e.g., capitalization) nor grammar properties (e.g., structural patterns). For example,  X  X r. X  and  X  X r. X  may very well indicate person names;  X  X nc X  and  X  X td X  are strong indicators for organization titles; a location address usually follows the pattern of street + city + state or province + zip codes . Second, classic ner frameworks usually works on a collection of sentences or paragraphs, which contains rich contextual information and semantic structures. In our case, we operate the framework on a collection of web search queries, which are usually short and noisy (e.g., misspelling, arbitrary ordering, etc).

To the best of our knowledge, there have been very limited research efforts attempting to extract brand and product entities from query logs automatically. We summarize a few past approaches for ner in queries, either in a supervised or semi-supervised way.

The problem of open-domain entity recognition and extrac-tion from web search queries is originally proposed in [25], which they rely on a seed-based weakly-supervised method. The method starts with a set of human labeled named entities X  X ommonly referred to as seeds X  X nd iteratively ac-quires new named entities explicitly from web queries. Li et al. [19] propose a semi-supervised approach based on con-ditional random fields method to extract entities from user queries. Guo et al. [12] introduce another weakly-supervised method, using partially labeled named entities as seeds, and train topic models on different domains. Pantel et al. [28] extend this work to jointly model user intent as latent vari-ables in the framework. However, both of these methods are limited to the queries containing only one named entity.
Later, [14] propose a multi-stage method on different do-mains. The method starts with extracting named-entity candidates from query logs using surface-level properties, e.g., capitalization, etc. It then filters out the uncertain candidates and applies clustering based on different feature space. It works considerably well on domains like celebrities, cities, diseases, movies, etc, partly because queries of these domains usually carry along very indicative surface features.
In another study, Du et al. [7] focus on the domain of car models and use the entire search session as additional contextual information. They train both conditional random fields and topic models with new contextual features and demonstrate significant improvement. More recently, Alasiry et al. [2] determine named entities using grammar annotation and query segmentation with the help of additional snippets from web resources. Eiselt and Figueroa [8] propose a su-pervised two-step approach to identify named entities from open-domain search queries.

Unfortunately, all these works rely on semi-supervised methods with additional human annotations, which can be costly to acquire. To address this problem, we propose a completely unsupervised method based on adaptor gram-mar [15]. It does not require any human annotation efforts (e.g., named entity seeds or labeled data) which typically can be expensive to acquire. Our method also does not rely on additional web resources, such as query sequences in search sessions or web snippets.
Adaptor grammars [15] offer a great flexibility and model-ing advantage in probabilistic context-free grammar parsing. In this paper, we apply the adaptor grammar model on a col-lection of queries in online shopping domain to extract brand and product entities efficiently and effectively. We propose a three step approach. The first step preprocesses the noisy query data and cleans up all stopwords. We then conduct a query standardization step, which aggregates and groups less frequent word orderings, to further reduce the noise and reg-ularize the queries. Finally, we propose three different sets of grammar rules to infer the query structures and extract the entities in a completely data-driven fashion. We compare our model against the vanilla pcfg approach X  X  variant akin to the shallow parsing approach X  X nd demonstrate significant better performance on precision measure on retrieved brand and product entities. We also evaluate the effectiveness of the discovered knowledgebase on two external supervised tasks X  X  query tagging model based on conditional random fields and a query chunking model based on maximum en-tropy model. We show that the knowledgebase discovered by our framework is of high quality and significantly improves the overall performance of both models.

We would also like to point out some possible future di-rections of our work. One possible extension is to use the variational Bayesian inference [6] as oppose to the mcmc method in this paper. It offers a more scalable alternative and easier amendable to online learning [33] and paralleliza-tion [32]. One limitation of our work is that we do not explicitly model the product families and model numbers dur-ing parsing. Instead, we subsume them all into the adaptor nonterminal Modifier . Such information can be critical for a query understanding system to better understand the constraints and prompt more desirable results to users during search and ranking. In the future, we would like to explore along this direction to jointly model these entities in adaptor grammar framework. [1] S. P. Abney. Parsing by chunks . Springer, 1992. [2] A. Alasiry, M. Levene, and A. Poulovassilis. Detecting [3] C. Barr, R. Jones, and M. Regelson. The linguistic [4] A. Broder. A taxonomy of web search. SIGIR Forum , [5] M. J. Cafarella, D. Downey, S. Soderland, and [6] S. B. Cohen, D. M. Blei, and N. A. Smith. Variational [7] J. Du, Z. Zhang, J. Yan, Y. Cui, and Z. Chen. Using [8] A. Eiselt and A. Figueroa. A two-step named entity [9] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, [10] P. Exner and P. Nugues. Entity extraction: From [11] J. T. Goodman. Parsing Inside-out . PhD thesis, [12] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity [13] H. Ishwaran and L. F. James. Generalized weighted [14] A. Jain and M. Pennacchiotti. Open entity extraction [15] M. Johnson, T. L. Griffiths, and S. Goldwater. Adaptor [16] M. Johnson, T. L. Griffiths, and S. Goldwater. [17] R. Koeling. Chunking with maximum entropy models. [18] J. Lafferty, A. McCallum, and F. C. Pereira. [19] X. Li, Y.-Y. Wang, and A. Acero. Extracting structured [20] B. Liu, R. Grossman, and Y. Zhai. Mining data records [21] C. D. Manning and H. Sch  X  utze. Foundations of [22] D. Nadeau. Semi-supervised named entity recognition: [23] D. Nadeau and S. Sekine. A survey of named entity [24] J. Nothman, J. R. Curran, and T. Murphy.
 [25] M. Pa  X sca. Weakly-supervised discovery of named [26] M. Pa  X sca. Queries as a source of lexicalized [27] M. Pa  X sca. Interpreting compound noun phrases using [28] P. Pantel, T. Lin, and M. Gamon. Mining entity types [29] J. Pitman and M. Yor. The two-parameter [30] L. Ratinov and D. Roth. Design challenges and [31] T.-L. Wong, W. Lam, and T.-S. Wong. An [32] K. Zhai, J. Boyd-Graber, N. Asadi, and M. L. [33] K. Zhai, J. Boyd-Graber, and S. Cohen. Online adaptor
