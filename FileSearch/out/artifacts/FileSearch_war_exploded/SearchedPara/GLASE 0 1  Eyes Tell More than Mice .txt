 This paper proposes a protot ype system called Gaze-Learning-Access-and-Search-Engine 0.1 (GLASE), which can perform image relevance ranking based on gaze data and within-session learning. We developed a search user interface that uses an eye-tracker as an input device and employed a relevance re-ranking algorithm based on the gaze length. The preliminary experimental results showed that using our gaze-driven system reduced the task completion time an average of 13.7% in a search session. H.3.3. [ Information Search and Retrieval ]: Relevance feedback H.5.2. [ User Interfaces ]: Input devices and strategies Experimentation, Measurement Image search, gaze-based interaction, within session learning The interactivity of most search user interfaces (UI) is mostly supported by a keyboard and a mouse. Our prototype system, GLASE 0.1, provides a gaze-drive n search UI and personalizes the search results based on within-session learning. Gaze data can be used as feedback for learning to rank and re-rank based on the dynamic interest and attention of users. Since the provided data are continuous, comp ared to discre te mouse click events, learning can be performed in continuous time rather than by discrete iterations. Gaze conv eys information that mouse clicks don't, including the gaze length, the direction, and the speed of movements. The data are more accurate and can capture the user attention and interest toward the items that were not clicked on; (this phenomena is called "good abandonment" [4]) and can also convey such information as whether users are actually looking at the screen while they are performing multiple tasks. Mouse "dwell time" fails to track this. Gaze-d riven navigation is a great support when a user's hands are occupied during the search process. Finally, such input can greatly be nefit people with sensory system, motor system, or cognitive disorders. The search process has a high cognitive load, especially during query formulation and relevant document tagging [1]. Using implicit feedback, which requires nothing more from users than looking at items on a search engi ne results page (SERP), may lower the cognitive load and simplify the search task. Users are freed from clearly formulating their intents, if the system can learn them fast during within-session interaction. This can be especially effective in exploratory search for images. Many related researches exist that prove that eye-tracking is an effective means both as an input device in an interactive UI as well as a data source for deter mining object relevance [2,3,5]. An experiment explored the eff ects of using eye-tracker as an input device in an image search user interface. Six Japanese university students from various majors were recruited for the experime nt; two were female. Each participant performed four search tasks: 2 with an eye-tracker-driven (ET) UI and 2 with a mouse-driven UI. We used two types of relevance recalculation methods for the mouse-driven interface: "binary" and "dwell-time". The experiment organizers assigned the participants to either the "binary" (MB) or "dwell-time" (MD) interfaces. Th e tasks were counterbalanced among the participants. Before performing the tasks, the sy stem was briefly explained and a training session was held. After that the participants performed search tasks. The participants we re asked to search for the images, that are similar to the sample images (these are the images from MIRFLICKR with the following IDs: 859626, 585623, 926255, 781859). The search session started with an input of a query. Depending on the task, the navigation had to be performed using gaze or a mouse. There was no time restriction for the tasks; however the participants were told that the tasks should not take longer than five minutes and that the overall experiment duration should not exceed 1 hour. The task completion time means an d standard deviations, shown in the Table 1 are floored down to the nearest integer. The task completion time for ET is smaller than for the mouse-driven UI by 13.7%. 
Table 1. Means and standard deviations of task completion We generated heat maps of pop-up-counts and pop-up-view-lengths of the images on the UI (F ig. 1). The SERP consisted of five lines of images, seven images per line. The total number of pop-ups on gaze-driven UI was 1.35 times bigger than on the mouse UI, but the total duration of the pop-ups was 1.24 times smaller on the gaze-driven UI. The cases of "good abandonment," when users looked at the images without clicking on them were captured by the gaze-driven UI. So the system learned this action; however the m ouse-driven UI failed to capture these events. Thus, th e system satisfied the information needs of users in a shorter time. The total time effort of the users is smaller on the gaze-driven UI, but the computational cost requi red for relevance recalculation is increase is, however, negligible and was not noticed by the users during the experiment. Using an eye-tracker as a releva nce feedback input device can help save time in image sear ch tasks. The information of relevance that is otherwise lost due to the phenomena of "good abandonment" can be retrieved during search sessions and used to learn the user's search intent. User experiments showed that the total duration of a search session can be decreased by an average of 13.7% using our proposed system, the gaze-driven UI of GLASE 0.1. 
