 Weicong Ding dingwc@bu.edu Mohammad H. Rohban mhrohban@bu.edu Prakash Ishwar pi@bu.edu Venkatesh Saligrama srv@bu.edu We consider a corpus of M documents composed of words chosen from a vocabulary of W distinct words indexed by w = 1 , . . . , W . We adopt the classic  X  X ags of words X  modeling paradigm widely-used in proba-bilistic topic modeling ( Blei , 2012 ). Each document is modeled as being generated by N independent and identically distributed drawings of words from an un-known W  X  1 document word-distribution vector. Each document word-distribution vector is itself modeled as an unknown probabilistic mixture of K &lt; min( M, W ) unknown W  X  1 latent topic word-distribution vectors that are shared among the M documents in the cor-pus. Documents are generated independently. For fu-ture reference, we adopt the following notation. We denote by the unknown W  X  K topic-matrix whose columns are the K latent topic word-distribution vec-tors. denotes the K  X  M weight-matrix whose M columns are the mixing weights over K topics for the M documents. These columns are assumed to be iid samples from a prior distribution. Each column of the W  X  M matrix A = corresponds to a document word-distribution vector. X denotes a realization of A : a W  X  M matrix whose columns are the empiri-cal word-frequency vectors of the M documents. Our goal is to estimate the latent topic word-distribution vectors ( ) from the empirical word-frequency vectors of all documents ( X ).
 A fundamental challenge here is that words-by-documents distributions ( A ) are unknown and only a realization is available through sampled word fre-quencies in each document. Another challenge is that even when these distributions are exactly known, the decomposition into the product of topic-matrix, , and topic-document distributions, , which is known as Nonnegative Matrix Factorization (NMF) , has been shown to be an NP -hard problem in general ( Vavasis , 2009 ). In this paper, we develop computationally effi-cient algorithms with provable guarantees for estimat-ing for topic matrices satisfying the separability con-dition ( Donoho &amp; Stodden , 2004 ; Arora et al. , 2012 ). Definition 1. (Separability) A topic matrix  X  R
W  X  K is separable if for each topic k , there is some word i such that i,k &gt; 0 and i,l = 0 ,  X  l  X  = k . The condition suggests the existence of  X  X ovel X  words that are unique to each topic. Our algorithm has three main steps. In the first step, we identify novel words by means of data dependent or random projections. A key insight here is that when each word is associated with a vector consisting of its occurrences across all docu-ments, the novel words correspond to extreme points of the convex hull of these vectors. A highlight of our approach is the identification of novel words based on data-dependent and random projections. Our idea is that whenever a convex object is projected along a ran-dom direction, the maximum and minimum values in the projected direction correspond to extreme points of the convex object. While our method identifies novel words with negligible false and miss detections, evi-dently multiple novel words associated with the same topic can be an issue. To account for this issue, we apply a distance X  X ased clustering algorithm to cluster novel words belonging to the same topic. Our final step involves linear regression to estimate topic word frequencies using novel words.
 We show that our scheme has a sample complexity that matches the state-of-art such as ( Arora et al. , 2013 ). On the other hand, the computational complexity of our scheme can scale as small as O ( M N K + W K ) for a corpora containing M documents, with an average of N words per document from a vocabulary containing W words. We then present a set of experiments on synthetic and real-world datasets. The results demon-strate qualitative and quantitative superiority of our scheme in comparison to other state-of-art schemes. The literature on topic modeling and discovery is ex-tensive. One direction of work is based on solving a nonnegative matrix factorization (NMF) problem. To address the scenario where only the realization X is known and not A , several papers ( Lee &amp; Seung , 1999 ; Donoho &amp; Stodden , 2004 ; Cichocki et al. , 2009 ; Recht et al. , 2012 ) attempt to minimize a regularized cost function. Nevertheless, this joint optimization is non-convex and sub-optimal strategies have been used. Unfortunately, when N  X  W which is often the case, many words do not appear in a single document and such methods often fail in these cases.
 Latent Dirichlet Allocation(LDA) ( Blei et al. , 2003 ; Blei , 2012 ) is an example of probabilistic topic model-ing approach. In this approach the columns of are modeled as iid random drawings from some prior dis-tributions such as Dirichlet. The goal is to compute MAP (maximum aposteriori probability) estimates for the topic matrix. This setup is inherently non-convex and MAP estimates are computed using variational Bayes approximations of the posterior distribution, Gibbs sampling or expectation propagation.
 A number of methods with provable guarantees have also been proposed. ( Anandkumar et al. , 2012 ) de-scribe a novel method of moments approach. While their algorithm does not impose structural assumption on topic matrix , they require Dirichlet priors for matrix. One issue is that such priors do not permit cer-Li &amp; McCallum , 2006 ). Also their algorithm is not ag-nostic since it uses parameters of the Dirichlet prior. Furthermore, the algorithm suggested involves finding empirical moments and singular decompositions which can be cumbersome for large matrices.
 Our work is closely related to recent work of ( Arora et al. , 2012 ) and ( Arora et al. , 2013 ) with some important differences. In their work, they describe methods with provable guarantees when the topic ma-trix satisfies the separability condition. Their al-gorithm discovers novel words from empirical word co-occurrence patterns and then in the second step the topic matrix is estimated. Their key insight is that when each word, j , is associated with a W dimensional vector 1 the novel words correspond to extreme points of the convex hull of these vectors. ( Arora et al. , 2013 ) present combinatorial algorithms to recover novel words with computational complexity scaling as O ( M N 2 + W 2 ). One issue with their method is that empirical estimates of joint probabilities in the word-word co-occurrence matrix can be unreliable, es-pecially when M is not large enough. Another issue is they require linear independence of the extreme points of the convex hull. This can be a serious problem in some datasets where word co-occurrences lie on a low dimensional manifold.
 Major Differences: Our work also assumes the ex-istence of novel words. We associate each word with a M -dimensional vector consisting of the word X  X  fre-quency of occurrence in the M -documents rather than word co-occurrences as in ( Arora et al. , 2012 ; 2013 ). We also show that extreme points of the convex hull of these cross-document frequency patterns are as-sociated with novel words. While these differences appear technical, it has important consequences. In several experiments our approach appears to signifi-cantly outperform ( Arora et al. , 2013 ) and mirror per-formance of more conventional methods such as LDA ( Griffiths &amp; Steyvers , 2004 ). Furthermore, our ap-proach can deal with degenerate cases found in some image datasets where the extreme points can lie on a lower dimensional manifold than the number of topics. At a conceptual level our approach appears to hinge on distinct cross-document support patterns of novel words belonging to different topics. This is typically robust to sampling fluctuations when support pat-terns are distinct in comparison to word co-occurrences statistics of the corpora. Our approach also differs algorithmically. We develop novel algorithms based on data-dependent and random projections to find ex-treme points efficiently. Recall that , , A , and X denote, respectively, the topic matrix, the weight matrix, the document word distribution matrix, and the empirical document word-frequency matrix and A = . We assume that satisfies the Separability condition (Definition 1 ). Let e A := diag( A1 )  X  1 e A denote the  X  1 row-normalized A matrix and e and e X the  X  1 row-normalized and X matrices respectively. Then e A = e e with e := diag( A1 )  X  1 diag( 1 ). Let X i (resp. A i ) denote the i th row of X (resp. A ) which represents the cross-document pattern of word i . Let C k be the set of novel words of topic k and C 0 be the non-novel words. Our approach is motivated by the following simple geomet-ric structure: Proposition 1. Suppose is separable. For all i  X  X  j combination of e j  X  X , j = 1 , . . . , K .
 Proof: Since e A = e e , e A and e are row-stochastic by construction, and is separable, it follows that e is row-stochastic and for all i  X  X  j and all j  X  = 0, e ij = 1. The key idea of Proposition 1 is illustrated in Fig. 1 . Without loss of generality, we assume that no row of e is in the convex hull of the remaining rows. The problem of identifying novel words reduces to finding extreme points of all e A i  X  X . Recovering the topic matrix is straightforward given all K distinct novel words: Proposition 2. The topic matrix can be recovered using W constrained linear regressions given the ma-trix A and K distinct novel words { i 1 , . . . , i K } . Proof: Since e = ( A  X  i e A ing a linear regression. can be obtained by column normalizing  X  since  X  = diag( A1 ) e = diag( 1 )  X  1 . In practice, we are not given A but a sampled real-ization X with limited number of samples per docu-ment ( N ). However, by collecting enough documents ( M  X  X  X  ), one can asymptotically estimate  X  to arbi-trary precision.
 Following Proposition 1 and 2 , our proposed approach consists of three main steps: (1) Novel Word Detection: Given X , extract a set of novel words I . To this end, we provide algorithms based on data-dependent and random projections in Sec. 4.1 . 4.2 . (2) Novel Word Clustering: Given a set of novel words I with |I| &gt; K , cluster them into K groups corresponding to K topics and pick a representative sample from each group. We provide a distance based clustering algorithm for this purpose (Sec. 4.3 ). (3) Topic Estimation: The topic matrix is esti-mated as suggested by Proposition 2 (Sec. 4.4 ). 4.1. Data Dependent Projections (DDP) Fig. 1 illustrates the key insight of our approach to identify novel words as extreme points of some convex object. If we project every point of a convex body onto some direction d , the maximum and minimum corre-spond to extreme points of the convex object. Our two algorithms both exploit this fact. They only differ in the choice of projected directions.
 To simplify analysis we randomly split each document into two subsets, and get two statistically independent document collections X and X  X  distributed as A , and then row normalize to obtain e X and e X  X  . For a word i , we project all e X  X  l  X  X  onto d = e X i .  X  e X i , e X  X  be the maximum if i is a novel word.
 Multiple novel words for a single topic is problematic guishable. Therefore, for some threshold d to be spec-ified later, and for each word i , we construct a set, J i of all words that are sufficiently different from word i in the following sense: We then declare word i as a novel word if all words j  X  J margin,  X / 2 to be specified later.
 These steps could asymptotically detect all the novel-words as M  X   X  under technical assumptions, as is justified in Sec. 5 .
 Algorithm 1 NovelWordDetection-DDP 1: Input 2: Output : A set of the novel words I 3: C  X  M 4: I  X  X  X  5: for all 1  X  i  X  W do 6: J i  X  All indices j  X  = i : C i,i  X  2 C i,j + C j,j  X  d 7: if  X  j  X  J i : C i,i  X  C i,j  X   X / 2 then 8: I  X  X  X  X  i } 9: end if 10: end for The algorithm is elaborated in Algorithm 1 . The run-ning time of the algorithm is polynomial in N, M, W : Proposition 3. The running time of Algorithm 1 is O ( M N 2 + W 2 ) . 4.2. Random Projection (RP) DDP uses W different directions e X i  X  X  to find all the extreme points. Here we use random directions in-stead. This significantly reduces the time complexity by decreasing the number of required projections. The Random Projection Algorithm (RP) uses roughly P = O ( K ) random directions drawn uniformly iid over the unit sphere in R M . For each direction d , we project all e X i  X  X  onto it and choose the maximum and mini-mum. If there are multiple maximums/minimums as a result of multiple novel words for a single topic, we choose all of them. Note that e X i d will converge to e A d conditioned on d and as M increases. More-over, only for novel words i as extreme points, e A i d can be the maximum or minimum projection value. This provides intuition of consistency for RP. Since the directions are independent, we expect to find all the novel words using P = O ( K ) number of random projections.
 The algorithm is summarized in Algorithm 2 . It is completely agnostic and parameter-free. Moreover, it significantly reduces the computational complexity: Algorithm 2 NovelWordDetection-RP 1: Input 2: Output : A set of the novel words I 3: I  X  X  X  4: for all 1  X  j  X  P do 5: Generate d  X  Uniform(unit-sphere in R M ) 6: i max = arg max 8: end for Proposition 4. Running time of Algorithm 2 is O ( M N K + W K ) . 4.3. Novel Word Clustering There may be multiple novel words for a single topic which is often the case. In such case our DDP or RP algorithm extract multiple novel words for each topic. This necessitates clustering step. Conceptually, the cross-document frequency patterns for two topics, hence for the novel words of them, should be different. This motivates our simple distance-based clustering. To be precise, we construct a graph whose vertices are the novel words extracted in the first step. Word i and j is connected if they are close enough, i.e., j /  X  J i defined in Eq. 1 . Clustering therefore reduces to finding K connected components of this graph. The procedure is described in Algorithm 3 .
 Algorithm 3 NovelWordsClustering 1: Input : I , 2: Output : A set J of K distinct novel words 3: C  X  M 4: B  X  a zero matrix of size |I| X |I| 5: for all i, j  X  X  , i  X  = j do 7: end for 8: J  X  X  X  9: for all 1  X  j  X  K do 10: c  X  index of a representative of the j th con-11: J  X  X   X  X  c } 12: end for We can show the clustering scheme is asymptotically consistent under some technical assumptions : Proposition 5. Let C i,j , M e X i e X  X  X  X  j , D i,j , C i,i 2 C i,j + C j,j . Suppose assumptions in Sec. 5 holds. Then, as M  X   X  , D i,j converges to 0 in probabil-ity when i and j are novel words of the same topic, and converges to some strictly positive value greater than some constant d in probability otherwise. Constant d is the same as in Algorithm 1 . In principle we can choose any point in a cluster as the represen-tative for that cluster to estimate in the next step. In practice, we use the average of data points in each cluster. This turns out to be more noise resilient than choosing a single point, as the comparison of perfor-mance of our algorithm against ( Arora et al. , 2013 ) in Sec. 6 indicates. 4.4. Topic Matrix Estimation Given K distinct novel words of different topics, we estimate as suggested in Proposition 2 . This is described in Algorithm 4 . This step is similar to other topic modeling algorithms, which exploit separability ( Arora et al. , 2013 ; Recht et al. , 2012 ). While our algorithm works with cross-document word-frequency patterns (conceptually up to the 2 nd order moments occurs in regression), the provable algorithm in ( Arora et al. , 2013 ) works with word co-occurrence patterns (up to the 4 th order moments). We justify the consistency of this step in Sec. 5 .
 Algorithm 4 TopicMatrixEstimation 1: Input : J = { j 1 , . . . , j K } , X , X  X  2: Output : 3: Y = ( 4: for all 1  X  i  X  W do 6: end for 7: column normalize In this section, we present the sample complexity bound for each steps of our algorithm. Specifically, we provide guarantees for DDP Algorithm 1 and novel word clustering Algorithm 3 under some mild condi-tions. The analysis of the random projection algorithm 2 is much more involved and requires elaborate argu-ments and we will omit it in this paper.
 In order to prove consistency of the proposed novel word detection and clustering algorithms, we assume that the correlation matrix R and expectation a of the prior distribution over satisfy : (1) The min. entry of a is lower bounded by a  X  &gt; 0; (2) There exists a positive value  X  such that for dis-tinct i and j , R i,i / ( a i a i )  X  R i,j / ( a i a j ) The second condition can be interpreted as the re-quirement that any two novel words of different top-ics appear in substantial number of distinct docu-ments. To see this note that if i  X  C 1 , j  X  C 2 , then M e this requirement means that M ( e X i  X  e X j ) should be fairly distant from the origin, which in turn implies that the number of documents these two words (thus two topics) occur in, with similar probabilities, should be small. This is a reasonable assumption, since other-wise we can group two related topics into one. In fact, we show in the supplementary section it holds for the Dirichlet distribution, which is a traditional choice for the prior distribution in topic modeling. Moreover, we have tested the validity of it numerically for the logistic normal distribution (with non-degenerate co-variance matrices), which is used in Correlated Topic Modeling ( Blei &amp; Lafferty , 2007 ).
 The above assumptions in turn justify the steps of DDP as given by Eq. 1 , 2 .
 Proposition 6. Suppose conditions (1) and (2) above are satisfied. Then there exist positive constants d and  X  such that with high probability, i is a novel word if and only if Eq. 2 is satisfied.
 We further denoting  X   X  to be positive lower bounds on non-zero elements of , and R i;i a prove the consistency and sample complexity of the DDP algorithm: Theorem 1. For parameter choices d = 2  X a 2  X   X  2  X  and  X  =  X a  X   X   X  the DDP Algorithm 1 is consistent as M  X  X  X  . Specifically, true novel and non-novel words are asymptotically declared as novel and non-novel, re-spectively. Furthermore, for where C 1 is constant, Algorithm 1 finds all novel words without any outlier with probability at least 1  X   X  1 . We can also prove the consistency and sample com-plexity of the novel word clustering algorithm: Theorem 2. For parameter choice d = 2  X a 2  X   X  2  X  , given all true novel words as the input, the clustering Algo. 3 asymptotically (as M  X  X  X  ) recovers K distinct novel words of different topics. Furthermore, for where C 2 is a constant, Algorithm 3 clusters all novel words correctly with probability at least 1  X   X  2 . We also provide an analysis for the topic estima-tion Algorithm 4 under the same assumption as in ( Arora et al. , 2013 ) that R is positive definite. R &gt; 0 is not needed for novel words detection and clustering. Theorem 3. If we further assume that R is positive definite with its eigenvalues lower bounded by  X   X  , then given K distinct novel words, the output of Algorithm 4 b p  X  X  X  element-wise up to a column permutation. Specifically, if least 1  X   X  3 , for  X  &lt; 1 and C 3 being a constant. Implementation Details: DDP requires two param-eters d and  X  . In practice, we apply DDP without knowing them adaptively and agnostically. Note that we use d to construct J i . We can otherwise construct J by finding r &lt; W words that are maximally distant from e X i in the sense of Eq. 1 . To bypass  X  , we rank the values of min j  X  J i M  X  e X i , e X  X  i  X  X  X  M  X  e X i all i and declare the topmost s indices as novel words. d is also used in Algo. 3 to threshold the 0-1 graph. We could avoid hard thresholding by using say and apply spectral clustering. Typically the size of I is
O ( K ). The sorting and spectral clustering requires additional O ( W 2 log ( W )) and O ( K 3 ) time. For the experiments in Sec. 6.1 &amp; 6.3 we use the ag-nostic variants with r = W/ 2 and s = 10  X  K .  X  is chosen so that maximum weight is fixed. For the im-age dataset we used d = 1 and  X  = 3. For RP, we set the number of projections P  X  50  X  K . 6.1. Synthetic Dataset In this section, we validate our algorithm on synthetic examples. We generate a W  X  K separable topic ma-trix with W 1 /K &gt; 1 novel words per topic as follows: first, iid 1  X  K rows-vectors corresponding to non-novel words are generated uniformly on the probability sim-plex. Then, W 1 iid Uniform[0 , 1] values are generated for the nonzero entries in the rows of novel words. The resulting matrix is then column-normalized to get one realization of . Next, M iid K  X  1 column-vectors are generated for the  X  matrix according to a Dirich-let prior c 2004 ), we set  X  i = 0 . 1 for all i . Finally, we obtain X by generating N iid words for each document. For different settings of W 1 /W , K , M and N , we cal-culate the  X  1 distance of the estimated topics to the ground truth after finding the best matching between two sets of topics. For each setting we average the er-ror over 50 random samples. For RP &amp; DDP we set parameters as discussed in the implementation details. We compare the DDP and RP against the Gibbs sam-pling approach ( Griffiths &amp; Steyvers , 2004 ) (Gibbs), a state-of-art NMF-based algorithm ( Tan &amp; F  X evotte , 2013 ) (NMF) and the most recent practical provable algorithm in ( Arora et al. , 2013 ) (RecoverL2). The NMF algorithm is chosen because it compensates for the type of noise in our topic model. Figure 2 de-picts the estimation error as a function of the num-ber of documents M (Upper) and the number of words/document N (bottom). RP and DDP have sim-ilar performance and are uniformly better than compa-rable techniques. Gibbs performs relatively poor in the first setting and NMF in the second. RecoverL2 per-form worse in all the settings. Note that M is relatively small (  X  1 , 000) compared to W = 500. DDP/RP out-perform other methods with fairly small sample size. Meanwhile, as is also observed in ( Arora et al. , 2013 ), RecoverL2 have very bad performance with small M . The error of RecoverL2 decreases and became compa-rable to the other method as M is 10 times larger than the maximum in the plot ( M  X  10 , 000). 6.2. Swimmer Image Dataset In this section we apply our algorithm to the synthetic swimmer image dataset introduced in ( Donoho &amp; Stodden , 2004 ). There are M = 256 bi-nary images each of W = 32  X  32 = 1024 pixels. Each a b c d e image represents a swimmer composed of four limbs, each of which can be in one of 4 distinct positions, and a torso. We interpret pixel positions ( i, j ) as words. Each image is interpreted as a document composed of pixel positions with non-zero values. Since each posi-tion of a limb features some unique pixels in the image, the topic matrix satisfies the separability assumption with K = 16  X  X round truth X  topics that correspond to 16 single limb positions.
 Following the setting of ( Tan &amp; F  X evotte , 2013 ), we set body pixel values to 10 and background pixel val-ues to 1. We then take each  X  X lean X  image, suitably normalized, as an underlying distribution across pix-els and generate a  X  X oisy X  document of N = 200 iid  X  X ords X  according to the topic model. Examples are shown in Fig. 3 . We apply RP and DDP algorithms to the  X  X oisy X  dataset and compare against Gibbs 2013 ), and RecoverL2 ( Arora et al. , 2013 ). Results are shown in Figs. 4 and 5 . We set the parameters as discussed in the implementation details.
 This dataset is a good validation test for different algo-rithms since the ground truth novel words are known and are unique. As we see in Fig. 5 , both Gibbs and NMF produce topics that do not correspond to any pure left/right arm/leg positions. Indeed, many of them are composed of multiple limbs. Nevertheless, as shown in Fig. 4 , no such errors are realized in RP and DDP and our topic-estimates are closer to the ground truth images. In the meantime, RecoverL2 algorithm failed to work even with the clean data. Although it also extracts extreme points of a convex body, the algorithm additionally requires these points to be lin-early independent. It is possible that extreme points of a convex body are linearly dependent (for example, a 2-D square on a 3-D simplex). This is exactly the case in the swimmer dataset with dimension of convex body in clean images being 13 &lt; K = 16. As we see in the last row in Fig. 4 , RecL2 produces only a few topics close to ground truth. Its extracted topics for the clean images are shown in Fig. 5 . Results of Re-coverL2 on noisy images are no close to ground truth as shown in Fig. 5 . 6.3. Real World Text Corpora In this section, we apply our algorithm on two real world text corpora from ( Frank &amp; Asuncion , 2010 ). The smaller corpus is NIPS proceedings dataset with M = 1 , 700 documents, a vocabulary of W = 14 , 036 words and an average of N  X  900 words in each docu-ment. Another large corpus is New York (NY) Times article dataset, with M = 300 , 000, W = 102 , 660, and N  X  300. The vocabulary is obtained by re-moving a standard  X  X top X  word list used in computa-tional linguistics, including numbers, individual char-acters, and some common English words such as  X  X he X . In order to compare with the practical algorithm in ( Arora et al. , 2013 ), we followed the same pruning in there experiment setting to shrink the vocabulary size to W = 2 , 500 for NIPS and W = 15 , 000 for NY Times. Following typical settings in ( Blei , 2012 ) and ( Arora et al. , 2013 ) , we set K = 40 for NIPS and K = 100 for NY Times. We set other algorithm pa-rameters as discussed in implementation details. We compare DDP and RP algorithms against Recov-erL2 ( Arora et al. , 2013 ) and a practically widely suc-cessful algorithm ( Griffiths &amp; Steyvers , 2004 )(Gibbs). Table 1 and 2 2 depicts typical topics extracted by the different methods. For each topic, we show its most frequent words, listed in descending order of the esti-mated probabilities. Two topics extracted by different algorithms are grouped if they are the closest in  X  1 distance.
 Different algorithms extract some fraction of similar topics which are easy to recognize. Table 1 indicates most of the topics extracted by RP and DDP are simi-lar and are comparable with that of Gibbs. We observe that the recognizable themes formed with DDP or RP topics are more abundant than that by RecoverL2. For example, topic on  X  X hip design X  as shown in the first panel in Table 1 is not extracted by RecoverL2, and topics in Table 2 on  X  X eather X  and  X  X motions X  are missing in RecoverL2. Meanwhile, RecoverL2 method produces some obscure topics. For example, in the last panel of Table 1 RecoverL2 contains more than one theme, and in the last panel of Table 2 RecoverL2 pro-duce some unfathomable combination of words. More details about the topics extracted are given in the sup-plementary material. Anandkumar, A., Foster, D., Hsu, D., Kakade, S., and
Liu, Y. K. A spectral algorithm for latent dirichlet allocation. In Advances in Neural Information Pro-cessing Systems 25 , pp. 926 X 934, Lake Tahoe, NV, Dec. 2012.
 Arora, S., Ge, R., and Moitra, A. Learning topic mod-els  X  going beyond SVD. In 53rd IEEE Annu. Symp. Foundations of Computer Science , pp. 1 X 10, New Brunswick, NJ, Oct. 2012.
 Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A.,
Sontag, D., Wu, Y., and Zhu, Michael. A practical algorithm for topic modeling with provable guaran-tees. In the 30th Int. Conf. on Machine Learning , Atlanta, GA, Jun. 2013.
 Blei, D. Probabilistic topic models. Commun. of the ACM , 55(4):77 X 84, 2012.
 Blei, D. and Lafferty, J. A correlated topic model of science. The Ann. of Applied Statistics , 1(1):17 X 35, 2007.
 Blei, D., Ng, A., and Jordan, M. Latent dirichlet al-location. J. Mach. Learn. Res. , 3:993 X 1022, Mar. 2003.
 Cichocki, A., Zdunek, R., Phan, A. H., and Amari,
S. Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation . Wiley, 2009.
 Donoho, D. and Stodden, V. When does non-negative matrix factorization give a correct decomposition into parts? In Advances in Neural Information Pro-cessing Systems 16 , pp. 1141 X 1148, Cambridge, MA, 2004. MIT press.
 Frank, A. and Asuncion, A. UCI ma-chine learning repository, 2010. URL http://archive.ics.uci.edu/ml .
 Griffiths, T. and Steyvers, M. Finding scientific topics.
Proceedings of the National Academy of Sciences , 101:5228 X 5235, Apr. 2004.
 Lee, D. and Seung, H. Learning the parts of objects by non-negative matrix factorization. Nature , 401 (6755):788 X 791, Oct. 1999.
 Li, W. and McCallum, A. Pachinko allocation: Dag-structured mixture models of topic correlations. In
Proc. the 23rd Int. Conf. on Machine learning , pp. 577 X 584, Pittsburgh, PA, Jun. 2006.
 Recht, B., Re, C., Tropp, J., and Bittorf, V. Factor-ing nonnegative matrices with linear programs. In
Advances in Neural Information Processing Systems 25 , pp. 1223 X 1231, Lake Tahoe, NV, Dec. 2012. Tan, V. Y. F. and F  X evotte, C. Automatic relevance determination in nonnegative matrix factorization with the beta-divergence. IEEE Transactions on
Pattern Analysis and Machine Intelligence, in press , 2013.
 Vavasis, S. On the complexity of nonnegative ma-trix factorization. SIAM J. on Optimization , 20(3):
