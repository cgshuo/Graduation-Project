 A main challenge in applying tr anslation language models to information retrieval is how to estimate the  X  X rue X  probability that a query could be generated as a translation of a document. The state-of-art methods rely on doc ument-based word co-occurrences to estimate word-word translation probabilities. However, these methods do not take into account the proximity of co-occurrences. Intuitively, the proximity of co-occurrences can be exploited to estimate more accurate translation probabilities, since two words occur closer are more likely to be related. In this paper, we study how to explicitly incorporate proximity information into the existing translation language m odel, and propose a proximity-based translation language model, called TM-P, with three variants. In our TM-P models, a new concept (proximity-based word co-occurrence frequency) is introduced to model the proximity of word co-occurrences, which is then used to estimate translation probabilities. Experimental results on standard TREC collections show that our TM-P models achieve significant improvements over the state-of-t he-art translation models. H.3.3 [Information Search and Retrieval]: Retrieval Models Information Retrieval; Proximity Information; Translation Language Models In the past decade and a half, language modeling for information retrieval has become a promising ar ea of research because of its elegant mathematical model and excellent empirical results reported in the literature [9][14]. The basic language modeling approach is primarily based on exact matching of words between documents and queries. Since que ries are short and relevant documents might use different vo cabulary, such an approach suffers from vocabulary gap whic h might lead to non-optimal performance. In order to reduce the semantic gap between documents and queries, statistical translation models have been proposed for information retrieval to capture semantic word relations [1]. The basic idea of translation language models is to estimate the probabilities of translating a word in a document to query words. Since a word in a document could be translated into different words in the query, translation language models can avoid exact matching of words between documents and queries. As a result, translation language models have achieved significant improvements over the ba se language model. The key part for translation language model is to learn word-word translation probabilities. The quality of the word-word translation probabilities can directly affect the performance of the translation models. The state-of-art methods use document-based word co-occurrence to estimate translation probabilities [4][5]. In reality, a document such as news article often discusses multiple topics. The information of document-based word co-occurrences may contain much noise. Intuitively, the closer the proximity of co-occurrences, the more likely the two words are relevant. Word proximity is an effective measure for word associations, which has been studied extensively in the past few years [2][6][7][8][10][11][12]. In these papers , various methods have been proposed to integrate proximity information into differ ent retrieval models, and it has been proven that word proximity is useful to improve retrieval performance. The paper in hand makes a systematically study on how to explicitly incorporate proximity information into the translation language model. The main contributi on of this paper is as follows First, we study how to adapt the existing translation model for proximity information, and propos e a proximity-based translation language model, called TM-P, in which the proximity of co-occurrences is take into acc ount. Second, we introduce a new concept (proximity-based word co-occurrence frequency) to model the proximity of word co-occurrences. Finally, extensive experiments on standard TREC co llections have been conducted to evaluate the proposed models. Experimental results show that our TM-P models achieve sign ificant improvements over the state-of-the-art translation models. In this section, we give a brief review to statistical translation language model. Reader can refer to [9][14] for more information about the basic language modelling approach. In the basic language modelling approach, documents are ranked by the probability that the query text could be generated by the document language model. Given a query q X  X   X , q  X , ...q document d, the query likelihood scoring function is as follows: probability of generating word w in document d, and  X  is the vocabulary set. In translation language modellin g approach, the document model  X   X   X  |  X   X  can be calculated by usin g the following  X  X ranslation document model X :  X  . In this way, a word can be translated into its semantically related words with non-zero probability, which allows us to score a document by counting the matches between a query word and semantically related words in document. The key part for translation language model is estimating translation probabilities. Berger and Lafferty [1] proposed a method to estimate translatio n probabilities by generating synthetic query. This method is inefficient and does not have good coverage of query words. In order to overcome these limitations, Karimzadehgan and Zhai [4] pr oposed an effect ive estimation method based on Mutual Information. Recently, Karimzadehgan and Zhai [5] defined four constraints that a reasonable translation la nguage model should satisfy, and proposed a new estimation method which is shown to be able to better satisfy the constraints. Th is new estimation method, namely conditional context analysis, is described in formula 4. Previous studies have shown th at translation language model works better with Dirichlet Prior smoothing [4][5]. Therefore, in the rest of the paper, we furthe r focus on the translation language model with Dirichlet prior smoothing only as follows: In the conditional context analysis method proposed in [5], the probability of translating word u into word w can be estimated as follows: where c  X  w,u  X  is the co-occurrences of word u with other words in the vocabulary, and |V| is the size of the vocabulary. In this method, any co-occurrence within the document is treated in the same way, no matter how far they are from each other. In reality, closer words usually ha ve stronger relationships, thus should be more relevant. Therefore, we introduce a new concept, namely proximity-based word co-occurrence frequency ( pcf ) to model the proximity feat ure of co-occurrencies. Recently, density functions based on proximity are proven to be effective to characterize term in fluence propagation. A number of term propagation functions (e.g. Gaussian, Triangle, Cosine and Circle) have been proposed [7][8] . In this section, we adopted Gaussian functions because it has been shown to be effective in most cases. The Gaussian-based pcf can be calculated as follows: set of documents which contain both  X  and  X  , and  X  X , X , X  X  X  X  X  X  is the distance score of word w and u in document d . In this paper, three commonly used distance measures are adopted to calculate  X  X , X , X  X  X  X  X  X  . We will use the following short document d as an example to explain how to calculate distance score in the three distance measures. d ={ w u c k w u k e w g } Minimum pair distance: It is de fi ned as the minimum distance between any occurrences of w and u in document d . In the example,  X  X , X , X  X  X  X  X  X  is 1 and can be calculated from the position vectors. Average pair distance: It is de fi ned as the average distance between w and u for all position combinations in d . In the example, the distances from the first occurrence of w (in position 1) to all occurrences of u are: {1 and 5}. This is computed for the next occurrence of w (in position 5) and so on.  X  X , X , X  X  X  X  X  X  for the example is (((2-1) + (6-1)) + ((5-2) + (6-5)) + ((9-2) + (9-6)))/(2  X  3) = 20/6 = 3.33. Average minimum pair distance: It is defined as the average of the shortest distance between each occurrence of the least frequently occurring word and any occurrence of the other word. In the example, u is the least frequently occurring word so  X  X , X , X  X  X  X  X  X  = ((2  X  1)+(6  X  5))/2 =1. Then, the probability of translating word u into word w can be estimated as follows: where  X  is a smoothing parameter in order to account for unseen all  X  X  X  values in collection. In order to satisfy the constrai nts defined in [5], we adjust translation language model as follows: where parameter s is a constant value that could be set to 0.5 &lt;= s Then p  X   X  w|u  X  could replace p  X   X  w | u  X  in Equation 3. The experiments in this sect ion use three main document collections: (1) ad hoc data in TREC7 with TREC topics 351-400 and 528,155 articles (2) WSJ news articles with TREC topics 51-100 and (3) technical reports in DOE abstracts with TREC topics 51-100. Each document is processed in a standard way for indexing. Words are stemmed (using porter-s temmer), and stop words are removed. In the expe riments, we only use title of the queries because semantic word matching is necessary for such short queries. The performance is measured using two standard measures: MAP (mean average precision) and precision P10 (precision at 10). In this paper, three different proximity measures are adapted to measure the distance score of two words in document. The corresponding models based on th ese measures, TM-P1, TM-P2 and TM-P3, are evaluated on three standard TREC collections. The methods used for the experiments are: BL: baseline, i.e., basic query likelihood method. TM-MI : translation language model wi th mutual information [4]. TM-CCON : translation language model with conditional context analysis [5]. TM-P1 : proximity-based translation language model with minimum pair distance. TM-P2 : proximity-based translation language model with average pair distance. TM-P3 : proximity-based translation language model with average minimum pair distance. As we can see from all the TM-P models used in our experiments, there are several controlling parameters to tune. In order to make the comparison fair, we do cross validation to learn these parameters for the TM-P models . For the baseline, parameter  X  in the Dirichlet smoothing is set to an optimal value of 1500. The results of TM-MI, TM-CCON are directly from [5]. Table 1 shows the cross-validation results for these models with Dirichlet prior smoothing. Comp aring the rows in the table indicates that our TM-P models outperform the other three models according to both MAP and P@10. The results indicate that our TM-P models are more effective than the previous translation models on all the three collections. In addition, the results confirm our hypothesis that the proximity of co-occurrence is helpful for estimating the strength of association between two words. Comparing the three variants of TM-P, TM-P3 is more effective and robust than TM-P2 and TM-P1. It also 
Figure 1. TM-P1, TM-P2, TM-P3 over TREC7 with  X  values 
Figure 2. TM-P3 model over TREC7, WSJ, and DOE with  X  indicates that TM-P3 model can ca pture the proximity feature of co-occurrences better than the other two models. The significance test results using Wilcoxon signed -rank test indicate that the differences between our models and the TM-CCON are statistically significant. An important issue that may aff ect the robustness of the TM-P and  X  (in Equation 5). The parameter  X  controls the amount of self-translation probabilities. The kernel parameter  X  determines the distance in which words are considered to be related. In this section, we study how sensitive these two parameters are to MAP measure. We investigate a large range of  X  from 10 to 1000. Generally, the value of  X  affects the performance of all TM-P models extensively. The experimental results show that the influence of  X  is collection-based. For the th ree TM-P models, their curves fluctuate similarly on the same collection. However, the best  X  values for these TM-P models are not the same. For example, on the TREC7 collection, optimal  X  value for TM-P1 is 80, and the corresponding value for TM-P2 is 150. Thus, the optimal values of  X  depend on the proximity measur es and the collections. Figure 1 plots the evaluation metrics MAP obtained by the three TM-P models over  X  values ranging from 10 to 1000 on TREC7 The experimental results also show that the influence of  X  is collection-based. For one collection, the best  X  values for all TM-P models are the same. Specifically, the best  X  values are 0.7, 0.5, and 0.8 for TREC7, WSJ, and DOE , respectively. Figure 2 shows how the performance of the TM-P3 model changes with parameter  X  on the three collections. To investigate how the Dirichlet prior parameter  X  affects the performance of our TM-P models, we also chan ge the settings of the smoothing parameters for them. The results indicate that the optimal smoothing parameters are the same (equals to 1000) for all the three TM-P models on all collections. The empirical successes of language modelling approaches have made it a formidable text retrieval model [9][14]. The key challenge in language model-base d information retrieval is to estimate a better document model [ 13]. Different heuristics have been proposed to improve the es timations of document models [14]. To reduce the vocabulary gap between documents and queries, Berger and Lafferty firstly adopt statistical translation model for information retrieval [1].They us e synthetically generated query-document pairs to train translati on model. An alternation way of estimating the translation model is based on document titles [3]. Recent works have relied on document-based word co-occurrences to estimate word-word translation probabilities [4][5]. Word-word proximity, as a strong indicator of word relevance, has been studied extensively in the past few years. In these papers, various methods have been pr oposed to integrate proximity information into different retr ieval models. Keen [6] firstly attempted to import term proximity in the Boolean retrieval model by introducing a  X  X EAR X  operator. Buttcher et al. [2] proposed an integration of term proximity sc oring into Okapi BM25 and obtain improvements on several collections . Tao et al. systematically studied five proximity measures and compared their performance in various retrieval models [10]. Zhao et al. [11] used a query term X  X  proximity centrality as a hyper parameter in Dirichlet language model under the language modelling framework. Lv and Zhai [7] integrated the position and proximity information into the language model by a different way. Zhao et al. [12] introduce a pseudo term, namely cross term, to model term proximity for boosting retrieval model. Miao et al. [8] has attempted to incorporate proximity information into the Rocchio X  X  model. In this paper, a new type of translation language model, called TM-P, is proposed by explicit ly incorporating proximity Specifically, we model the proximity of co-occurrences by introducing a new concept pcf. Thr ee proximity measures are then adopted for calculating the distance score of two words in document. The corresponding models based on these measures, TM-P1, TM-P2 and TM-P3, are evaluated on three standard TREC collections. Our experiment results indicate that our TM-P models are more effective than the state-of-art translation models. Comparing the three variants of TM-P, TM-P3 is more effective than TM-P1 and TM-P2. In the futu re, we will try to study how to apply TM-P in other text processing tasks. This work was supported by the NSF of China (No. 61100133). [1] Berger, A. and Lafferty, J. 1999. Information retrieval as [2] B X ttcher, S., Clarke, C. L. A. , and Lushman, B. 2006. Term [3] Jin, R., Hauptmann, A. G. and Zhai, C. X. 2002. Title [4] Karimzadehgan, M. and Zhai , C. X. 2010. Estimation of [5] Karimzadehgan, M. and Zh ai, C. X. 2012. Axiomatic [6] Keen, E. M. 1992. Some aspects of proximity searching in [7] Lv, Y. and Zhai, C. X. 2009. Positional language models for [8] Miao, J., Huang, X. J. and Ye, Z. 2012. Proximity-based [9] Ponte, J. M. and Croft, W. B. 1998. A language modeling [10] Tao, T. and Zhai, C. X. 2007. An exploration of proximity [11] Zhao, J., and Yun, Y. 2009. A proximity language model for [12] Zhao, J., Huang, X. and He, B. 2011. CRTER: using cross [13] Zhai, C.X. and Lafferty, J. 2001. A study of smoothing [14] Zhai, C.X. 2008. Statistical Language Models for 
