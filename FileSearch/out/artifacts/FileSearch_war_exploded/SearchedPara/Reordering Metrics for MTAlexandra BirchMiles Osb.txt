 Research in machine translation has focused broadly on two main goals, improving word choice and im-proving word order in translation output. Current machine translation metrics rely upon indirect meth-ods for measuring the quality of the word order, and their ability to capture the quality of word order is poor (Birch et al., 2010).

There are currently two main approaches to eval-uating reordering. The first is exemplified by the B
LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the refer-ence and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of match-ing words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, ME-TEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment be-tween the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. When block moves are allowed the search space is very large, and matching stems and synonyms in-troduces errors. Importantly, none of these metrics capture the distance by which words are out of order. Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. More sophisticated metrics, such as the RTE metric (Pad  X  o et al., 2009), use higher level syntactic or semantic analysis to determine the grammaticality of the out-put. These approaches require annotation and can be very slow to run. For most research, shallow metrics are more appropriate.
 We introduce a novel shallow metric, the Lexical Reordering Score (LRscore), which explicitly mea-sures the quality of word order in machine trans-lations and interpolates it with a lexical metric. This results in a simple, decomposable metric which makes it easy for researchers to pinpoint the effect of their changes. In this paper we show that the LRscore is more consistent with human judgements than other metrics for five out of eight different lan-guage pairs. We also apply the LRscore during Mini-mum Error Rate Training (MERT) to see whether in-formation on reordering allows the translation model to produce better reorderings. We show that hu-mans prefer the output of systems trained with the LRscore 52.5% as compared to 43.9% when train-ing with the B LEU score. Furthermore, training with the LRscore does not result in lower B LEU scores.
The rest of the paper proceeds as follows. Sec-tion 2 describes the reordering and lexical metrics that are used and how they are combined. Section 3 presents the experiments on consistency with human judgements and describes how to train the language independent parameter of the LRscore. Section 4 re-ports the results of the experiments on MERT. Fi-nally we discuss related work and conclude. In this section we present the LRscore which mea-sures reordering using permutation distance metrics. These reordering metrics have been demonstrated to correlate strongly with human judgements of word order quality (Birch et al., 2010). The LRscore com-bines the reordering metrics with lexical metrics to provide a complete metric for evaluating machine translations. 2.1 Reordering metrics The relative ordering of words in the source and tar-get sentences is encoded in alignments. We can in-terpret alignments as permutations which allows us to apply research into metrics for ordered encodings to measuring and evaluating reorderings. We use dis-tance metrics over permutations to evaluate reorder-ing performance. Figure 1 shows three permutations. Each position represents a source word and each value indicates the relative positions of the aligned target words. In Figure 1 (a) represents the identity permutation, which would result from a monotone alignment, (b) represents a small reordering consist-ing of two words whose orders are inverted, and (c) represents a large reordering where the two halves of the sentence are inverted in the target.

A translation can potentially have many valid word orderings. However, we can be reasonably cer-tain that the ordering of the reference sentence must be acceptable. We therefore compare the ordering of a translation with that of the reference sentence. Where multiple references exist, we select the clos-est, i.e. the one that gives the best score. The un-derlying assumption is that most reasonable word orderings should be fairly similar to the reference, which is a necessary assumption for all automatic machine translation metrics.

Permutations encode one-one relations, whereas alignments contain null alignments and one-many, many-one and many-many relations. We make some simplifying assumptions to allow us to work with permutations. Source words aligned to null are as-signed the target word position immediately after the target word position of the previous source word. Where multiple source words are aligned to the same target word or phrase, a many-to-one relation, the target ordering is assumed to be monotone. When one source word is aligned to multiple target words, a one-to-many relation, the source word is assumed to be aligned to the first target word. These simplifi-cations are chosen so as to reduce the alignment to a bijective relationship without introducing any extra-neous reorderings, i.e. they encode a basic monotone ordering assumption.

We choose permutation distance metrics which are sensitive to the number of words that are out of order, as humans are assumed to be sensitive to the number of words that are out of order in a sen-tence. The two permutations we refer to,  X  and  X  , are the source-reference permutation and the source-translation permutation. The metrics are normalised so that 0 means that the permutations are completely inverted, and 1 means that they are identical. We re-port these scores as percentages. 2.1.1 Hamming Distance
The Hamming distance (Hamming, 1950) mea-sures the number of disagreements between two per-mutations. It is defined as follows: d (  X , X  ) = 1  X  where n is the length of the permutation. The Hamming distance is the simplest permutation dis-tance metric and is useful as a baseline. It has no concept of the relative ordering of words. 2.1.2 Kendall X  X  Tau Distance
Kendall X  X  tau distance is the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another (Kendall, 1938). It represents the percentage of pairs of ele-ments which share the same order between two per-mutations. It is defined as follows: d k (  X , X  ) = 1  X  where z ij =
Kendalls tau seems particularly appropriate for measuring word order differences as the relative or-dering words is taken into account. However, most human and machine ordering differences are much closer to monotone than to inverted. The range of values of Kendall X  X  tau is therefore too narrow and close to 1. For this reason we take the square root of the standard metric. This adjusted d k is also more correlated with human judgements of reorder-ing quality (Birch et al., 2010).

We use the example in Figure 1 to highlight the problem with current MT metrics, and to demon-strate how the permutation distance metrics are cal-culated. In Table 1 we present the metric results for the example permutations. The metrics are calcu-lated by comparing the permutation string with the monotone permutation. (a) receives the best score for all metrics as it is compared to itself. B LEU and METEOR fail to recognise that (b) represents a small reordering and (c) a large reordering and they assign a lower score to (b). The reason for this is that they are sensitive to breaks in order, but not to the actual word order differences. B LEU matches more n-grams for (c) and consequently assigns it a higher score. METEOR counts the number of blocks that the translation is broken into, in order to align it with the source. (b) is aligned using four blocks, whereas (c) is aligned using only two blocks. TER counts the number of edits, allowing for block shifts, and ap-plies one block shift for each example, resulting in an equal score for (b) and (c). Both the Hamming distance d h and the Kendall X  X  tau distance d k cor-rectly assign (c) a worse score than (b). Note that for (c), the Hamming distance was not able to re-ward the permutation for the correct relative order-ing of words within the two large blocks and gave (c) a score of 0, whereas Kendall X  X  tau takes relative ordering into account.

Wong and Kit (2009) also suggest a metric which combines a word choice and a word order compo-nent. They propose a type of F-measure which uses a matching function M to calculate precision and recall. M combines the number of matched words, weighted by their tfidf importance, with their posi-tion difference score, and finally subtracting a score for unmatched words. Including unmatched words in the M function undermines the interpretation of the supposed F-measure. The reordering component is the average difference of absolute and relative word positions which has no clear meaning. This score is not intuitive or easily decomposable and it is more similar to METEOR, with synonym and stem functionality mixed with a reordering penalty, than to our metric. 2.2 Combined Metric The LRscore consists of a reordering distance met-ric which is linearly interpolated with a lexical score to form a complete machine translation evaluation metric. The metric is decomposable because the in-dividual lexical and reordering components can be looked at individually. The following formula de-scribes how to calculate the LRscore: The metric contains only one parameter,  X  , which balances the contribution of the reordering metric, R , and the lexical metric, L . Here we use B LEU as the lexical metric. R is the average permutation dis-tance metric adjusted by the brevity penalty and it is calculated as follows: Where S is a set of test sentences, d s is the reorder-ing distance for a sentence and BP is the brevity penalty.
 The brevity penalty is calculated as: where t is the length of the translation, and r is the closest reference length. If the reference sentence is slightly longer than the translation, then the brevity penalty will be a fraction somewhat smaller than 1. This has the effect of penalising translations that are shorter than the reference. The brevity penalty within the reordering component is necessary as the distance-based metric would provide the same score for a one word translation as it would for a longer monotone translation. R is combined with a system level lexical score.

In this paper we apply the B LEU score as the lex-ical metric, as it is well known and it measures lexi-cal precision at different n-gram lengths. We experi-ment with the full B LEU score and the 1-gram B LEU score, B LEU 1, which is purely a measure of the pre-cision of the word choice. The 4-gram B LEU score includes some measure of the local reordering suc-cess in the precision of the longer n-grams. B LEU is an important baseline, and improving on it by in-cluding more reordering information is an interest-ing result. The lexical component of the system can be any meaningful metric for a particular target lan-guage. If a researcher was interested in morpholog-ically rich languages, for example, METEOR could be used. We use the LRscore to return sentence level scores as well system level scores, and when doing so the smoothed B LEU (Lin and Och, 2004) is used. Automatic metrics must be validated by compar-ing their scores with human judgements. We train the metric parameter to optimise consistency with human preference judgements across different lan-guage pairs and then we show that the LRscore is more consistent with humans than other commonly used metrics. 3.1 Experimental Design Human judgement of rank has been chosen as the of-ficial determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al., 2009). We used human ranking data from this workshop to evaluate the LRscore. This consisted of German, French, Spanish and Czech translation systems that were run both into and out of English. In total there were 52,265 pairwise rank judgements collected.

Our reordering metric relies upon word align-ments that are generated between the source and the reference sentences, and the source and the trans-lated sentences. In an ideal scenario, the transla-tion system outputs the alignments and the refer-ence set can be selected to have gold standard hu-man alignments. However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. We used version two of the Berkeley alignment model (Liang et al., 2006), with the posterior threshold set at 0.5. Our Spanish-, French-and German-English alignment models are trained using Europarl version 5 (Koehn, 2005). The Czech-English alignment model is trained on sec-tions 0-2 of the Czech-English Parallel Corpus, ver-sion 0.9 (Bojar and Zabokrtsky, 2009).

The metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, Ger-man, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The sys-tem output along with human evaluations can be downloaded from the web 1 .

The B LEU score has five parameters, one for each n-gram, and one for the brevity penalty. These pa-rameters are set to a default uniform value of one. METEOR has 3 parameters which have been trained for human judgements of rank (Lavie and Agarwal, 2008). METEOR version 0.7 was used. The other baseline metric used was TER version 0.7.25. We adapt TER by subtracting it from one, so that all metric increases mean an improvement in the trans-lation. The TER metric has five parameters which have not been trained.

Using rank judgements, we do not have absolute scores and so we cannot compare translations across different sentences and extract correlation statistics. We therefore use the method adopted in the 2009 workshop on machine translation (Callison-Burch et al., 2009). We ascertained how consistent the auto-matic metrics were with the human judgements by calculating consistency in the following manner. We take each pairwise comparison of translation output for single sentences by a particular judge, and we recorded whether or not the metrics were consistent with the human rank. I.e. we counted cases where both the metric and the human judge agreed that one system is better than another. We divided this by the total number of pairwise comparisons to get a per-centage. We excluded pairs which the human anno-tators ranked as ties. We present a novel method for setting the LRscore parameter. Using multiple language pairs, we train the parameter according to the amount of reordering seen in each test set. The advantage of this approach is that researchers do not need to train the parameter for new language pairs or test do-mains. They can simply calculate the amount of re-ordering in the test set and adjust the parameter ac-cordingly. The amount of reordering is calculated as the Kendall X  X  tau distance between the source and the reference sentences as compared to dummy monotone sentences. The amount of reordering for the test sentences is reported in Table 2. German-English shows more reordering than other language pairs as it has a lower d k score of 73.9. The language independent parameter (  X  ) is adjusted by applying the reordering amount ( d k ) as an exponent.  X  is al-lowed to takes values of between 0 and 1. This works in a similar way to the brevity penalty. With more re-ordering, the d k becomes smaller which leads to an increase in the final value of  X  .  X  represents the per-centage contribution of the reordering component in the LRscore:
The language independent parameter  X  is trained once, over multiple language pairs. This procedure optimises the average of the consistency results across the different language pairs. We use greedy hillclimbing in order to find the optimal setting. As hillclimbing can end up in a local minima, we per-form 20 random restarts, and retaining only the pa-rameter value with the best consistency result. 3.2 Results Table 3 reports the optimal consistency of the LRscore and baseline metrics with human judge-ments for each language pair. The LRscore vari-ations are named as follows: LR refers to the LRscore,  X  X  X  refers to the Hamming distance and  X  X  X  to Kendall X  X  tau distance.  X  X 1 X  and  X  X 4 X  refer to the smoothed B LEU score with the 1-gram and the complete scores. Table 3 shows that the LRscore is more consistent with human judgement for 5 out of the 8 language pairs. This is an important result which shows that combining lexical and reordering information makes for a stronger metric than the baseline metrics which do not have a strong reorder-ing component.
 METEOR is the most consistent for the Czech-English and English-Czech language pairs, which have the least amount of reordering. METEOR lags behind for the language pairs with the most reorder-ing, the German-English and English-German pairs. Here LR-KB4 is the best metric, which shows that metrics which are sensitive to the distance words are out of order are more appropriate for situations with a reasonable amount of reordering. Automatic metrics are useful for evaluation, but they are essential for training model parameters. In this section we apply the LRscore as the objective func-tion in MERT training (Och, 2003). MERT min-imises translation errors according to some auto-matic evaluation metric while searching for the best parameter settings over the N-best output. A MERT trained model is likely to exhibit the properties that the metric rewards, but will be blind to aspects of translation quality that are not directly captured by the metric. We apply the LRscore in order to im-prove the reordering performance of a phrase-based translation model. 4.1 Experimental Design We hypothesise that the LRscore is a good metric for training translation models. We test this by eval-uating the output of the models, first with automatic metrics, and then by using human evaluation. We choose to run the experiment with Chinese-English as this language pair has a large amount of medium and long distance reorderings. 4.1.1 Training Setup The experiments are carried out with Chinese-English data from GALE. We use the official test set of the 2006 NIST evaluation (1994 sentences). For the development test set, we used the evalu-ation set from the GALE 2008 evaluation (2010 sentences). Both development set and test set have four references. The phrase table was built from 1.727M parallel sentences from the GALE Y2 train-ing data. The phrase-based translation model called MOSES was used, with all the default settings. We extracted phrases as in (Koehn et al., 2003) by run-ning GIZA++ in both directions and merging align-ments with the grow-diag-final heuristic. We used the Moses translation toolkit, including a lexicalised reordering model. The SRILM language modelling toolkit (Stolcke, 2002) was used with interpolated Kneser-Ney discounting. There are three separate 3-gram language models trained on the English side of parallel corpus, the AFP part of the Gigaword corpus, and the Xinhua part of the Gigaword cor-pus. A 4 or 5-gram language model would have led to higher scores for all objective functions, but would not have changed the findings in this paper. We used the MERT code available in the MOSES repository (Bertoldi et al., 2009).

The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5.

We first extracted the LRscore Kendall X  X  tau dis-tance from the monotone for the Chinese-English test set and this value was 66.1%. This is far more re-ordering than the other language pairs shown in Ta-ble 2. We then calculated the optimal parameter set-ting, using the reordering amount as a power expo-nent. Table 4 shows the parameter settings we used in the following experiments. The optimal amount of reordering for LR-HB4 is low, but the results show it still makes an important contribution. 4.1.2 Human Evaluation Setup
Human judgements of translation quality are nec-essary to determine whether humans prefer sen-tences from models trained with the B LEU score or with the LRscore. There have been some recent studies which have used the online micro-market, Amazons Mechanical Turk, to collect human anno-tations (Snow et al., 2008; Callison-Burch, 2009). While some of the data generated is very noisy, in-valid responses are largely due to a small number of workers (Kittur et al., 2008). We use Mechanical Turk and we improve annotation quality by collect-ing multiple judgements, and eliminating workers who do not achieve a certain level of performance on gold standard questions.

We randomly selected a subset of sentences from the test set. We use 60 sentences each for compar-ing training with B LEU to training with LR-HB4 and with LR-KB4. These sentences were between 15 and 30 words long. Shorter sentences tend to have uninteresting differences, and longer sentences may have many conflicting differences.

Workers were presented with a reference sen-tence and two translations which were randomly ordered. They were told to compare the transla-tions and select their preferred translation or  X  X on X  X  Know X . Workers were screened to guarantee reason-able judgement quality. 20 sentence pairs were ran-domly selected from the 120 test units and anno-tated as gold standard questions. Workers who got less than 60% of these gold questions correct were disqualified and their judgements discarded.
After disagreeing with a gold annotation, a worker is presented with the gold answer and an expla-nation. This guides the worker on how to perform the task and motivates them to be more accurate. We used the Crowdflower 2 interface to Mechanical Turk, which implements the gold functionality.
Even though experts can disagree on preference judgements, gold standard labels are necessary to weed out the poor standard workers. There were 21 trusted workers who achieved an average accuracy of 91% on the gold. There were 96 untrusted work-ers who averaged 29% accuracy on the gold. Their judgements were discarded. Three judgements were collected from the trusted workers for each of the 120 test sentences. 4.2 Results 4.2.1 Automatic Evaluation of MERT
In this experiment we demonstrate that the re-ordering metrics can be used as learning criterion in minimum error rate training to improve parameter estimation for machine translation.

Table 5 reports the average of three runs of MERT training with different objective functions. The lexi-cal metric B LEU is used as an objective function in LRHB4 31.1 32.2 41.3 60.6 55.7 LRKB4 31.0 32.2 41.2 61.0 55.8 isolation, and also as part of the LRscore together with the Hamming distance and Kendall X  X  tau dis-tance. We test with these metrics, and we also report the TER and METEOR scores for comparison.

The first thing we note in Table 5 is that we would expect the highest scores when training with the same metric as that used for evaluation as MERT maximises the objective function on the develop-ment data set. Here, however, when testing with B LEU , we see that training with B LEU and with LR-HB4 leads to equally high B LEU scores. The reordering component is more discerning than the B
LEU score. It reliably increases as the word order approaches that of the reference, whereas B LEU can reports the same score for a large number of different alternatives. This might make the reordering metric easier to optimise, leading to the joint best scores at test time. This is an important result, as it shows that by training with the LRscore objective function, B LEU scores do not decrease, which is desirable as B LEU scores are usually reported in the field.
The LRscore also results in better scores when evaluated with itself and the other two baseline met-rics, TER and METEOR. Reordering and the lexi-cal metrics are orthogonal information sources, and this shows that combining them results in better per-forming systems. B LEU has shown to be a strong baseline metric to use as an objective function (Cer et al., 2010), and so the LRscore performance in Ta-ble 5 is a good result.

Examining the weights that result from the dif-ferent MERT runs, the only notable difference is that the weight of the distortion cost is considerably lower with the LRscore. This shows more trust in the quality of reorderings. Although it is interesting to look at the model weights, any final conclusion on the impact of the metrics on training must depend on human evaluation of translation quality. LR-KB4 96 79 5 LR-HB4 93 79 8 4.2.2 Human Evaluation
We collect human preference judgements for out-put from systems trained using the B LEU score and the LRscore in order to determine whether training with the LRscore leads to genuine improvements in translation quality. Table 6 shows the number of the times humans preferred the LRscore or the B LEU score output, or when they did not know. We can see that humans have a greater preference for the out-put for systems trained with the LRscore, which is preferred 52.5% of the time, compared to the B LEU score, which was only preferred 43.9% of the time.
The sign test can be used to determine whether this difference is significant. Our null hypothesis is that the probability of a human preferring the LRscore trained output is the same as that of prefer-ring the B LEU trained output. The one-tailed alter-native hypothesis is that humans prefer the LRscore output. If the null hypothesis is true, then there is only a probability of 0.048 that 189 out of 347 (189 + 158) people will select the LRscore output. We therefore discard the null hypothesis and the hu-man preference for the output of the LRscore trained system is significant to the 95% level.

In order to judge how reliable our judgements are we calculate the inter-annotator agreement. This is given by the Kappa coefficient ( K ), which balances agreement with expected agreement. The Kappa co-efficient is 0.464 which is considered to be a moder-ate level of agreement.

In analysis of the results, we found that output from the system trained with the LRscore tend to produce sentences with better structure. In Table 7 we see a typical example. The word order of the sentence trained with B LEU is mangled, whereas the LR-KB4 model outputs a clear translation which more closely matches the reference. It also garners higher reordering and B LEU scores.

We expect that more substantial gains can be made in the future by using models which have more powerful reordering capabilities. A richer set of re-ordering features, and a model capable of longer distance reordering would better leverage metrics which reward good word orderings. We introduced the LRscore which combines a lexi-cal and a reordering metric. The main motivation for this metric is the fact that it measures the reorder-ing quality of MT output by using permutation dis-tance metrics. It is a simple, decomposable metric which interpolates the reordering component with a lexical component, the B LEU score. This paper demonstrates that the LRscore metric is more con-sistent with human preference judgements of ma-chine translation quality than other machine trans-lation metrics. We also show that when training a phrase-based translation model with the LRscore as the objective function, the model retains its perfor-mance as measured by the baseline metrics. Cru-cially, however, optimisation using the LRscore im-proves subjective evaluation. Ultimately, the avail-ability of a metric which reliably measures reorder-ing performance should accelerate progress towards developing more powerful reordering models.
