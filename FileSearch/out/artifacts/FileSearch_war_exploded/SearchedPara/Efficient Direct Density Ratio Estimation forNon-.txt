 since the importance can be used for various succeeding task s, e.g., for efficient computation [4].
 computational efficiency in model selection scenarios.
 Formulation and Notation: Let D X  ( R d ) be the data domain and suppose we are given inde-pendent and identically distributed (i.i.d.) training sam ples { x tr with density p assume p from { x tr p te ( x ) Least-squares Approach: Let us model the importance w ( x ) by the following linear model: where  X  denotes the transpose,  X  = (  X  parameters,  X  ( x ) = (  X  0 element-wise manner. Note that b and {  X  models are also allowed. We explain how the basis functions {  X  We determine the parameter  X  so that the following squared error is minimized: where C = 1 where H = R  X  ( x )  X  ( x )  X  p and taking into account the non-negativity of the importanc e function w ( x ) , we obtain where c H = 1 for avoiding overfitting,  X   X  0 , and 1 n mentarity condition, and (b) n and all possible test samples of size n Theorem 1 guarantees that LSIF converges to the ideal soluti on with order n  X  1 explicitly obtain the coefficient of the term of order n  X  1 parameter  X  and basis functions {  X  samples: First, the training samples { x tr subsets {X tr as b number of samples is reduced due to data splitting.
 Heuristics of Basis Function Design: A good model may be chosen by cross-validation, given Gaussian kernel model centered at the test input points { x te The reason why we chose the test input points { x te input points { x tr if the training input density p Gaussian centers at the test input points { x te Alternatively, we may locate ( n increased the computational cost. When n propose using a subset of { x te where c In the experiments shown later, we fix the number of template p oints at b = min(100 , n all  X  ) can be computed efficiently based on the parametric optimization technique [4]. common pitfall of solution path tracking algorithms in gene ral. However, since we dropped the non-negativity constraint  X   X  0 could be negative. To compensate for this approximation err or, we modify the solution by where I method unconstrained LSIF (uLSIF). Due to the  X  0 Below, we theoretically analyze the approximation error of uLSIF.  X   X  = max( 0 Theorem 2 Assume that (a)  X   X  Then we have E [ J ( b  X  )] = J (  X   X  ) + O n  X  1 Theorem 2 guarantees that uLSIF converges to the ideal solut ion with order n  X  1 explicitly obtain the coefficient of the term of order n  X  1 two sets of samples, { x tr plicity, we assume that n held out at the same time; the test samples { x te mula for computing matrix inverses X  b  X  ( i ) A without repeating hold-out loops. tion. KDE can be used for importance estimation by first estim ating b p from { x tr likelihood cross validation. However, KDE may suffer from t he curse of dimensionality. demanding since a quadratic programming problem has to be so lved. advantage over KMM. However, LogReg and KLIEP are computati onally rather expensive since non-linear optimization problems have to be solved.
 highly contributes to reducing the computation time in the m odel selection phase. Importance Estimation: Let p covariance identity; let p We fixed the number of test input points at n the number n (b) d = 10 and n each method, and evaluate the quality of the importance esti mates { b w
Figure 1: NMSEs averaged over 100 trials in log scale. normalized to be one, respectively.
 the training sample size n errors than KDE and KMM.
 sample size n and 3(b) show that uLSIF is approximately 5 to 10 times faster than LogReg. accuracy, but is computationally more efficient than LogReg . according to the importance. In addition to training input s amples { x tr input density p that training output samples { y tr predict the outputs for test inputs.
 We use the kernel model for function learning, where K domly chosen from { x te importance weighted regularized least-squares (IWRLS): It is known that IWRLS is consistent when the true importance w ( x tr f ( x ) is not realizable by the model b f ( x ) [8].
 ( x i , y by IWRLS and its mean test error for the remaining samples Z tr where loss ( b y, y ) is ( b y  X  y ) 2 in regression and 1 while unweighted CV with misspecified models is biased due to covariate shift. and n repeated 100 times for each dataset and evaluate the mean test error : 1 datasets taken from IDA.
 adaptation with low computational cost.
 Table 1: Covariate shift adaptation. Mean and standard deviation of test error over 100 trials (smaller is better). values (i.e., close to zero).
 regard the positive samples as regular and the negative samp les as irregular. to uLSIF both in AUC and computation time. LOF and KDE work rea sonably well in terms of computationally efficient also in outlier detection. importance estimation could be used as a new versatile tool i n machine learning.
