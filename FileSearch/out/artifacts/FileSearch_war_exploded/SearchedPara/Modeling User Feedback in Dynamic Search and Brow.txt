  X  Information systems  X  Retrieval models and rank-ing; Users and interactive retrieval; Session Search; Dynamic Search; User Feedback Modeling
Nowadays, searching for complicated information is a more and more common task. User X  X  information needs are usu-ally vague or consist of multiple sub-topics, and they must formulate many different queries to achieve their search X  X  goal. A lot of work has been done to improve user satisfac-tion during the search process. Riccho etc. are developed to help find similar relevant documents. They help the user fo-cus on and exploit the current search topic. xQuAD [5] etc. are efficient diversification algorithms that help the user ex-plore multiple search topics. None of these approaches alone work well in session searches because none of them treat the search session as a whole. They can X  X  answer the question of when to explore and when to exploit.

In our previous work [3], we argue that it is suitable to model session searches as a Dual-Agent Stochastic Game, which essentially is a Partially Observable Markov Decision Process (POMDP) [2] with two agents. Our model treats session search as a  X  X rial-and-error X  process and uses user feedback as learning signals to adjust its search strategies, such as exploration and exploitation.

The major feedback we considered in our previous works were query reformulations and user clickthrough datas. I first extend our work by introducing more user feedbacks into our framework. We implement a new search engine UI which allows users to explicitly mark out relevant passages and irrelevant documents. With these explicit feedbacks, I can improve the exploitation algorithm. I use the relevant text to reform a new query for retrieval. The irrelevant doc-uments are then used to re-rank the retrieved documents. For exploration, my algorithm is inspired by xQuAD. The subtopics are found by the user during a search process. I run the algorithm n times in order to recommend n diver-sified documents. For each round, I sample one subtopic to recommend based on xQuAD.

The second research question I addressed is the decision of when to explore and when to exploit. In [6], the authors explore during the initial phase, and then choose one path to exploit based on user feedback. [4] applies exploration whenever it encounters a diversified query. I argue that their approaches only capture some specific user behavior models. [1] applies exploration as default behavior and in-terprets user click as a desire to exploit similar documents. However this assumption is not accurate. I use -greedy as a naive approach to balance Exploration and Exploitation. If the user has tried exploitation multiple times, then we should switch to exploration, and vice versa. Another pos-sible solution could be to pick one search strategy at first, such as  X  X xploitation X , and if no positive feedback is received from the user, then we switch to the other search strategy.
The final challenge in this thesis is how to properly eval-uate session search algorithms. We propose a new sophisti-cated evaluation metric, Cube Test. This new metric is able to emphasize subtopic coverage, novelty, and retrieval accu-racy at the same time. It also emphasizes minimizing user effort by encouraging short sessions over long sessions. I plan to use this metric, MAP, nDCG,  X  -nDCG and nERR-IA to evaluate my algorithms as well as other well-known session based retrieval algorithms. I hope these metrics can reveal different aspects of retrieval algorithms and eventually help us to distinguish good session search algorithms from others. [1] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic [2] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. [3] J. Luo, S. Zhang, and H. Yang. Win-win search: [4] K. Raman, P. N. Bennett, and K. Collins-Thompson. [5] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting [6] M. Sloan and J. Wang. Dynamic information retrieval:
