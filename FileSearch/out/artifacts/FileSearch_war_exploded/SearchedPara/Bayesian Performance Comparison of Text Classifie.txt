 How can we know whether one classifier is really better than the other? In the area of text classification, since the publi-cation of Yang and Liu X  X  seminal SIGIR-1999 paper, it has become a standard practice for researchers to apply null-hypothesis significance testing (NHST) on their experimen-tal results in order to establish the superiority of a classifier. However, such a frequentist approach has a number of inher-ent deficiencies and limitations, e.g., the inability to accept the null hypothesis (that the two classifiers perform equally well), the difficulty to compare commonly-used multivariate performance measures like F 1 scores instead of accuracy, and so on. In this paper, we propose a novel Bayesian approach to the performance comparison of text classifiers, and ar-gue its advantages over the traditional frequentist approach based on t -test etc. In contrast to the existing probabilistic model for F 1 scores which is unpaired, our proposed model takes the correlation between classifiers into account and thus achieves greater statistical power. Using several typical text classification algorithms and a benchmark dataset, we demonstrate that the our approach provides rich information about the difference between two classifiers X  performances. Bayesian inference; hypothesis testing; performance evalua-tion; text classification
Text classification (aka categorisation) [25] is a fundamen-tal technique in information retrieval (IR) [19]. It has many important applications, including topic categorisation, spam filtering, sentiment analysis, message routing, language iden-tification, genre detection, authorship attribution, and so on. In fact, most modern IR systems for search, recommenda-tion, or advertising contain multiple components that use some form of text classification.

How can we know whether one classifier is really better than the other? Is it possible that they perform equally well? Sure we should be able to evaluate their classification performances on some benchmark datasets using some per-formance measures. However, given any finite amount of test results, we can never be completely certain that one classi-fier works better than the other or vice versa: the observed difference between their performance scores do not neces-sarily reflect their intrinsic qualities. The central question here is how to reliably tell if classifier A indeed outperforms classifier B, given a set of test results. Perhaps the simplest solution is to apply k -fold cross-validation [21] and then cal-culate the sample variance of performance scores over mul-tiple  X  X olds X  of the dataset. This method tends to yield poor estimations though: the sample variance can approximate the true variance well only if we have a large number of folds, but when the dataset is divided into many folds, the size of each fold is likely to be too small to give a meaning-ful performance score (especially for complex multivariate performance measures like F 1 [26]). Hence it is desirable to derive the uncertainty of performance scores directly from all the atomic document-category classification results.
To address this problem, Yang and Liu defined in their seminal SIGIR-1999 paper [27] a suite of null-hypothesis sig-nificance testing (NHST) methods which aim to verify how strongly the experimental results support the claim that one particular classifier is more accurate than another classifier. That paper has been influential within and beyond the realm of text classification. Since its publication, it has received about 3,000 citations (according to Google Scholar). Today, it is almost compulsory for researchers to validate the su-periority of their proposed text classification algorithms by means of NHST and report the p -values in their papers.
Although NHST has proven to be useful in assessing text classifiers and its adoption has greatly improved the rigour of performance evaluation in IR, such a frequentist approach has many inherent deficiencies and limitations which we shall elaborate on later. In this paper, we propose a novel approach to performance comparison of text classifiers based on Bayesian estimation [15], and argue its advantages over the traditional frequentist approach based on t -test etc. Us-ing a few representative text classification algorithms and a benchmark dataset, we demonstrate that the our approach provides rich information about the difference between two classifiers X  performances.
The traditional frequentist approach to comparing clas-sifiers is to use NHST [21]. The usual process of NHST consists of four steps: (1) formulate the null hypothesis that the observations are the result of pure chance and the alternative hypothesis H 1 that the observations show a real effect combined with a component of chance variation; (2) identify a test statistic that can be used to assess the truth of H ; (3) compute the p -value, which is the probability that a test statistic equal to or more extreme than the one observed would be obtained under the assumption of hypothesis H 0 ; (4) if the p -value is less than an acceptable significance level, the observed effect is statistically significant, i.e., H out and H 1 is valid.

Specifically for performance comparison of text classifiers, the usage of NHST has been presented in detail by Yang and Liu in their SIGIR-1999 paper [27]. In summary, on the doc-ument level (micro level), sign-test can be used to compare two classifiers X  accuracy scores (called s-test), while unpaired t -test can be used to compare two classifiers X  performance measures in the form of proportions, e.g., precision, recall, error, and accuracy (called p-test); on the category level (macro level), sign-test and paired t -test can both be used to compare two classifiers X  F 1 scores [26] (which are called S-test and T-test respectively).

In spite of being useful and influential, such a frequen-tist approach unfortunately has many inherent deficiencies and limitations [14,15]. First, NHST is only able to tell us whether the experimental data are sufficient to reject the null hypothesis (that the performance difference is zero) or not, but there is no way to accept the null hypothesis. If we fail to reject the null hypothesis, we cannot conclude that it is true, but only recognise that the null hypothesis is a pos-sibility. That is to say, it is impossible for us to use NHST to confidently claim that two classifiers perform equally well . Second, NHST will reject the null hypothesis as long as the experimental data suggest that the performance difference is non-zero, even if the performance difference is too slight to have any real effect in practice. Third, complex performance measures such as the F 1 score can only be compared on the category level but not on the document level, which seri-ously restricts the statistical power of NHST as the number of categories is usually much much smaller than the number of documents. Fourth, using sign-test, those pairs of identi-cal classification outcomes are completely discarded, which is undesirable because the probability that the two classi-fiers are essentially equal would be substantially underesti-mated. Fifth, using unpaired t -test, the correlation between the classifiers in comparison are totally ignored, which is unreasonable because in reality both classifiers are likely to do well on  X  X asy X  test documents, and badly on  X  X ifficult X  test documents, not to mention that those classifiers could be just different versions of the same machine learning algo-rithm.

The other NHST methods that have been applied to com-pare classifiers include ANOVA test [10], Friedman test [24], McNemar X  X  test [6], and Wilcoxon signed ranks test [4]. Due to their frequentist nature, no matter which specific test they use, more or less they suffer from the above mentioned perils (especially the first three). It has been loudly advocated in recent years that the Bayesian approach to comparing two groups of data has many advantages over the frequentist NHST [14,15]. How-ever, to our knowledge, almost all the existing models of Bayesian performance comparison deal with continuous val-ues (that can be described by Gaussian or t distributions) but not discrete classification outcomes, and they produce estimations for simple statistics (such as the average differ-ence between the two given groups) but not complex perfor-mance measures (such as the F 1 score).

Probably the most closely related work is that of Goutte and Gaussier [8]. Their F 1 score model constructed using a couple of Gamma variates is not as expressive and flexible as ours. For example, generalising their model to the F  X  measure (  X   X  0) [19, 26] with  X  = 1 would end up with a complex equation involving three Gamma variates, but that would be trivial in our approach. It seems that their model is restricted to a single F 1 score for binary classification with two classes only, due to its reliance upon the special proper-ties of the Gamma distribution. In contrast, our approach is a probabilistic graphical model [13] which opens up many possibilities for adaptation or extension (see Section 5).
Our previous work on this topic [28, 29] has ignored any possible connection between the predictions from the two classifiers in comparison. Although this is totally fine when those two classifiers are evaluated separately each on a differ-ent test dataset, it is not the optimal solution in the common situation when those two classifiers are evaluated on exactly the same test dataset. In this paper, we extend such a sim-plistic  X  X npaired X  model to the more general  X  X aired X  model which takes the correlation between classifiers into account, and demonstrate that the former has much less statistical power than the latter (see Section 4.1).
Let us consider a text classifier which has been tested on a collection of N labelled test documents, D .Foreach document x i ( i =1 ,...,N ), we have its true class label as well as the predicted class label  X  y i .

If this classifier is actually a Bayesian model, in princi-ple there should be a direct way to assess the suitability of model M in explaining the experimental data by computing Pr[ M|D ]  X  Pr[ M ]  X  Pr[ D|  X  , M ] Pr[ X  |M ]d X . However, here we would like to consider the general situation where the true and predicted class labels are the only information presumed to be available.

In the most basic setting, binary classification , a docu-ment belongs to either the positive class or the negative class. Without loss of generality, we use integer 1 as the ID of the positive class and integer 0 as the ID of the negative class. Furthermore, for the sake of clarity, we will also denote the true positive and negative classes using notations + and Table 1: The classification results from one binary classifier.  X  respectively which should be regarded as interchangeable synonyms of class IDs 1 and 0.

The test documents can usually be considered as  X  X nde-pendent trials X , so we regard both their true class labels and their predicted class labels  X  y i as independent and iden-tically distributed (i.i.d.) random variables.

Table 1 lists all the possible classification results and their corresponding probabilities for a test document using one binary classifier. It is worth noting that in our model a classifier is allowed to exhibit different prediction accuracies on documents from different true classes. This flexibility is necessary to reflect the reality and facilitate the estimation of complex performance measures that take class imbalance into account.

Given a test document x i ,weuse  X  to represent the prob-ability that its true class label y i is positive. Obviously the probability that y i is negative would therefore be 1  X   X  This means that y i follows a Bernoulli distribution with pa-rameter  X  : y i  X  Bern(  X  ), i.e., Pr[ y i |  X  ]=  X  y i (1 would then be convenient to use the Beta distribution (which is conjugate to the Bernoulli distribution) as the prior dis-tributionofparameter  X  . More specifically,  X   X  Beta(  X  ), parameter  X  =(  X  + , X   X  ) encodes our prior belief about each class X  X  proportion. If we do not have such knowledge, we can simply set  X  =(1 , 1) that yields a uniform distribution, as we did in our experiments.

When a test document x i with true class label y i is clas-sified, we anticipate that it will be classified as positive with example,  X   X  is the probability that a negative (  X  )docu-ment is classified to be positive (1). Hence we can say that  X  y follows a Bernoulli distribution with parameter  X  + when y is positive and  X   X  when y i is negative. In other words,  X  y i  X  Bern(  X  + )if y i = + and  X  y i  X  Bern(  X   X  )if y i = It would then be convenient to use the Beta distribution as the prior distribution of parameter  X  + and  X   X  .Morespecifi-cally,  X  +  X  Beta(  X  + ), i.e., Pr[  X  + ]=  X (  X  codes our prior belief about the classifier X  X  prediction accu-racy on positive test documents. In the same way, we have  X   X   X  Beta(  X   X  ), where  X   X  =  X   X  1 , X   X  0 .Ifwedonothave any prior knowledge, we can simply set  X  + =  X   X  =(1 , 1) that yields a uniform distribution, as we did in our experi-ments.

Once the parameters  X  ,  X  + and  X   X  have been estimated, it will be easy to calculate the contingency table of  X  X xpected X  classification results: true positive ( tp ), false positive ( true negative ( tn ), and false negative ( fn ). For example, the anticipated number of true positive predictions of the clas-sifier should be the number of positive test documents N X  times the rate of being predicted by the classifier as positive Figure 1: The probabilistic graphical model for a binary text classifier X  X  performance. . The equations to calculate the contingency table for a classifier are listed as follows.
With the contingency table for a classifier available, we can compute not only the accuracy, but also more complex performance measures such as the F 1 score for that classifier. The precision P ,recall R , and their harmonic mean F 1 score could be computed as follows.
 It can be seen that N is cancelled out in the calculation of the precision, the recall, and the F 1 score.

Such a model is quite general to accommodate various performance measures (see Section 5), though in this paper we focus on the F 1 score only to illustrate the usage of our model. Let  X  denote the chosen performance measure, then it is simply a function that depends on  X  ,  X  + and  X   X  only:  X 
This model describes a generative mechanism of a clas-sifier X  X  test results. It is summarised as follows, and also depicted in Figure 1a as a probabilistic graphical model [13] using common notations.
In the above model, each true class label y i is regarded as an individual sampling event, and each prediction  X  y i treated as an individual sampling event too. If we aggre-gate the occurrences of such individual sampling events into the counts of their occurrences, the model could be greatly simplified.

Let n + represent the total number of positive test doc-uments and n  X  = N  X  n + represent the total number of negative test documents, then n + is known to follow the Binomial distribution with parameters N and  X  : n +  X  Bin( N, X  ), i.e., Pr[ n + | N, X  ]= N n +  X  n + (1  X   X  ) N  X  n
Let c + represent the count of positive predictions ( X  y i produced on positive test documents ( y i = + ), then c + is known to follow the Binomial distribution with param-eters n + and  X  + : c +  X  Bin( n + , X  + ), i.e., Pr[ c + | n + , X  + Bin( n  X  , X   X  ).

The parameters  X  ,  X  + and  X   X  are the same as before and their prior distributions remain the same. The deterministic variable  X  also stays unchanged.

This compact model is equivalent to the original model, but it will be computationally much more efficient due to the drastic reduction of sampling events. So hereafter the compact model will be used instead of the original model for our work on performance comparison.

The compact model is summarised as follows and depicted in Figure 1b. The usage of conjugate priors (e.g., Beta for Bernoulli or Binomial) is not obligatory in our model. Actually any rea-sonable probability distribution can be used as the prior of  X  ,  X  + or  X   X  . If we insist on using conjugate priors, it is pos-sible to simplify the model even further by computing the posterior probability distributions of our model parameters analytically and then sampling from the posterior probabil-ity distributions directly. However, this will only bring mod-erate improvement to computational efficiency, and more importantly it will make the model less flexible as some ex-tensions to the model (such as hierarchical modelling) will be obstructed. So we shall not go down that direction in this paper.
In the unpaired model for performance comparison, the predictions from the two classifiers A and B being compared are assumed to be independent with each other [28,29]. Ac-tually the two classifiers could be evaluated each on a dif-ferent test dataset as long as the data come from the same distribution (e.g., with the same proportion of positive test examples). So we can simply pool the two probabilistic mod-els for those two classifiers together, and introduce a deter-ministic variable  X  to capture the difference between their performance scores  X  A and  X  B . Figure 2: The unpaired model for performance comparison. Figure 3: The paired model for performance comparison.
The unpaired model consisting of two separate sub-models for two classifiers A and B is depicted in Figure 2, where most of the sub-model for B is omitted as it is symmetric to that of A.
Although the unpaired model is simple and effective, its underlying assumption that the predictions from two clas-sifiers A and B are independent of each other is unrealistic when those two classifiers are evaluated on the same test dataset. In contrast to the existing work for classification performance comparison (see Section 2), we would like to avoid this unrealistic assumption by modelling the two clas-sifiers X  predictions jointly as pairs. This is indeed crucial to assessing the real significance of the two classifiers X  perfor-mance difference, as we demonstrate later in our experiments (see Section 4.1).

Considering two classifiers A and B evaluated on the same document collection, we have for each document x i ( i = 1 ,...,N ) a prediction outcome pair o i =( X  y A i ,  X  y B i  X  i and  X  y B i are the predicted class labels given by A and B respectively.

Table 2 lists all the possible classification results and their corresponding probabilities for a test document using two binary classifiers. Since for each of the two possible y i there are four possible o i values { (1 , 1) , (1 , 0) , (0 this table has 2  X  4=8entriesintotal.

When a test document x i with true class label y i is clas-sified by the two classifiers A and B, we anticipate that each possible prediction outcome pair o i will occur with a certain the probability that a positive ( + ) document is classified to be negative (0) by the classifier A and positive (1) by Table 2: The classification results from two binary classifiers. the classifier B. If we let  X  + denote the vector of parame-ters  X  + o i and similarly let  X   X  denote the vector of parameters i , then we can say that tion with parameter  X  + when y i is positive and  X   X  when y is negative. In other words, o i  X  Cat(  X  + )if y i = + and o i  X  Cat(  X   X  )if y i =  X  .Itwouldthenbeconve-nient to use the Dirichlet distribution (which is conjugate to the Categorical distribution) as the prior distribution of pa-rameter  X  + or  X   X  . More specifically,  X  +  X  Dir(  X  + ), i.e., the classifier X  X  prediction accuracy on positive test docu-ments. Inthesameway,wehave  X   X   X  Dir(  X   X  ), where knowledge, we can simply set  X  + =  X   X  =(1 ,..., 1) that yields a uniform distribution, as we did in our experiments. ent types of prediction outcome pairs produced on positive test documents, then c + is known to follow the Multinomial distribution with parameters n + and  X  + : c +  X  Mult( n + ,  X  + .
 Inthesameway,wehave c  X   X  Mult( n  X  ,  X   X  ).

Once the parameters  X  ,  X  + and  X   X  have been estimated, it will be easy to calculate, for each classifier, the contingency table of X  X xpected X  X lassification results as before by noticing the following facts: Thus the performance scores  X  A and  X  B ,aswellastheir difference  X  could be estimated.

The paired model is summarised as follows and depicted in Figure 3.
Given a probabilistic model of the chosen performance measure, we can consider the comparison of two classifiers as a model selection problem and utilise the Bayes factor to address it [1,2].

In our context, the Bayes factor is the marginal likelihood of classification results data for the null model Pr[ D|M (where two classifiers perform equally well) relative to the marginal likelihood of classification results data for the al-ternative model Pr[ D|M 1 ] (where one classifier works better than the other classifier): BF = Pr[ D|M 0 ] / Pr[ D|M 1 ]. As the BF becomes larger, the evidence increases in favour of model M 0 over model M 1 . The rule of thumb for interpret-ing the magnitude of the BF is that there is  X  X ubstantial X  evidence for the null model M 0 when the BF exceeds 3, and similarly,  X  X ubstantial X  evidence for the alternative model M 1 when the BF is less than 1 3 [11].

Although for simple models the value of Bayes factor can be derived analytically as shown by [1,2], for complex mod-els it can only be computed numerically using for exam-ple the Savage-Dickey (SD) method [5]. The SD method assumes that the prior on the variance in the null model equals the prior on the variance in the alternative model at the null value: Pr[  X  2 |M 0 ]=Pr[  X  2 |M 1 , X  =0]. From this it follows that the likelihood of the data in the null model equals the likelihood of the data in the alternative model at the null value: Pr[ D|M 0 ]=Pr[ D|M 1 , X  =0]. Thus, the Bayes factor can be determined by considering the alternative hypothesis alone, because it is just the ra-tio of the probability density at  X  = 0 in the posterior relative to the probability density at  X  = 0 in the prior: BF = Pr[  X  =0 |M 1 , D ] / Pr[  X  =0 |M 1 ].
Instead of relying on the Bayes factor which is a single value, we can make use of the entire posterior probability distribution of  X  , the performance difference between two classifiers, for their comparison. This Bayesian (parameter) estimation approach to performance comparison is said to be more informative and more robust than using the Bayes factor [14,15].

Given the posterior probability distribution of  X  ,wecan then reach a discrete judgement (decision) about how those two classifiers A and B compare with each other by exam-ining the relationship between the 95% Highest Density In-terval (HDI) of  X  and the user-defined Region of Practical Equivalence (ROPE) of  X  [14,15]. The 95% HDI is a useful summary of where the bulk of the most credible values of  X  falls: by definition, every value inside the HDI has higher probability density than any value outside the HDI, and the total mass of points inside the 95% HDI is 95% of the distri-bution. The ROPE of  X  ,e.g.,[  X  0 . 05 , +0 . 05], encloses those values of  X  deemed to be negligibly different from its null value for practical purposes. Using the HDI together with the ROPE, the performance comparison decisions could be made as follows: The need to specify the ROPE may sound like an extra bur-den on users compared to NHST, but in fact it is only mak-ing a hidden problem  X  how much performance difference would really matter for practical purposes (such as customer satisfaction and business profit)  X  explicit. The determina-tion of the ROPE requires only knowledge about the appli-cation domain but not expertise in statistics. When the HDI is far away from or tightly surrounding the null value, the ex-act ROPE is inconsequential as any reasonable ROPE would lead to the same decision. Furthermore, in many situations, the exact ROPE can be left indeterminate. By reporting the HDI and other summary information about the full pos-terior distribution of  X  , readers can apply whatever ROPE appropriate for them to make their own decisions.
The purpose of building these models for classification re-sults is to assess the Bayesian posterior probability of  X  the performance difference between two classifiers A and B. An approximate estimation of  X  can be obtained by sam-pling from its posterior probability distribution via Markov Chain Monte Carlo (MCMC) [15] techniques.
 We have implemented our models with an MCMC method Metropolis-Hastings sampling [15]. The default configura-tion is to generate 50,000 samples, with no  X  X urn-in X ,  X  X ag X , or  X  X ultiple-chains X . It has been argued in the MCMC liter-ature that those tricks are often unnecessary: it is perfectly right to do a single long sampling run and keep all sam-ples [13, 18]. In fact, the approximation accuracy of our program is very high: its Monte Carlo error (MC error) was usually close to 0 and never went beyond 0.002 in all our experiments (see Section 4). Figure 4 shows an exam-ple MCMC trace of our program in the experiments which clearly demonstrates the convergence of MCMC sampling.
In order to calculate the Bayes factor using the SD method (see Section 3.2.1), we approximate the posterior density Pr[  X  =0 |M 1 , D ] and the prior density Pr[  X  =0 |M 1 ]byfit-ting a smooth function to the corresponding MCMC samples via kernel density estimation (KDE).
 The program is written in Python 3 utilising the module PyMC3 1 [22] for MCMC based Bayesian model fitting. The source code is made open to the research community as on-line supplementary material 2 . It is free, easy to use, and extensible to more sophisticated models (see Section 5).
We should mention that this program for Bayesian perfor-mance comparison runs much slower than standard frequen-tist NHST techniques. On a machine with Intel x64 Core i7 CPU 2.30GHz, a sign-test or t -test would normally finish in less than 0.02 seconds, but our program could take up to 20 seconds for one comparison. Most of the time is spent on the computationally expensive MCMC sampling as it does re-quire a decent number of samples to achieve high-fidelity ap-proximation of probability distributions. Nevertheless, such a speed should be perfectly acceptable for the purpose of comparing classifiers because the classification experiments would usually take much longer time. Therefore the pro-gram is still very practical. Moreover, the program would be greatly accelerated if GPUs could be used by Theano ,the underlying computational engine for PyMC3 .
To demonstrate the advantage of our paired model over unpaired model, we perform power analysis using simula-tions. The statistical power is the probability of achieving the goal of a planned empirical study, if a suspected under-lying state of the world is true [15]. As the power increases, there are decreasing chances of a Type II error aka the false negative rate  X  since the power is equal to 1  X   X  .
We consider the following two scenarios where the two hypothetical classifiers A and B are somewhat correlated. The scenario (a): Pr[ + ]=  X  =0 . 5 Pr[(1 , 1) | + ]=  X  + (1 , 1) =0 . 3, Pr[(1 , 0) | + ]=  X  + Pr[(0 , 1) | + ]=  X  + (0 , 1) =0 . 2, Pr[(0 , 0) | + ]=  X  + Pr[  X  ]=1  X   X  =0 . 5 Pr[(1 , 1) |  X  ]=  X   X  (1 , 1) =0 . 2, Pr[(1 , 0) |  X  ]=  X   X  Pr[(0 , 1) |  X  ]=  X   X  (0 , 1) =0 . 3, Pr[(0 , 0) |  X  ]=  X   X  It is easy to see that F A 1 =0 . 6while F B 1 =0 . 5, so the goal here is to detect  X  X  B X .
 The scenario (b): Pr[ + ]=  X  =0 . 5 Pr[(1 , 1) | + ]=  X  + (1 , 1) =0 . 3, Pr[(1 , 0) | + ]=  X  + Pr[(0 , 1) | + ]=  X  + (0 , 1) =0 . 2, Pr[(0 , 0) | + ]=  X  + Pr[  X  ]=1  X   X  =0 . 5 Pr[(1 , 1) |  X  ]=  X   X  (1 , 1) =0 . 3, Pr[(1 , 0) |  X  ]=  X   X  Pr[(0 , 1) |  X  ]=  X   X  (0 , 1) =0 . 2, Pr[(0 , 0) |  X  ]=  X   X  It is easy to see that F A 1 =0 . 5and F B 1 =0 . 5, so the goal here is to detect  X  X   X  B X . Please note that this goal is infeasible using the frequentist NHST.
 The power analysis results are shown in Table 3 and also Figure 5, which clearly indicate the superiority of the paired model to the unpaired model in terms of statistical power.
The reason why the unpaired model does not have as much statistical power as the paired model is because the former cannot tell whether the prediction differences (or http://pymc-devs.github.io/pymc3/ http://www.dcs.bbk.ac.uk/  X dell/publications/dellzhang sigir2016 supp.html agreements) between the two classifiers are consistent or not while the latter can. Inconsistent prediction differences yield a larger variability than consistent ones, and consequently more are required to exhibit statistical significance. Sup-pose that classifier A has a higher F 1 score than classifier B. If A almost always makes better predictions than B when they disagree, a relatively small amount of such consistent differences could give us enough confidence to assert statis-tical significance, which is recognised by the paired model but not the unpaired model.
We have conducted experiments on a standard benchmark dataset for text classification, 20newsgroups [16], of which the results are reported here. In order to ensure the re-producibility of our experimental results, we choose to use not the raw document collection, but a publicly-available ready-made  X  X ectorised X  version 3 , as in [28, 29]. We have also done experiments on other  X  X ectorised X  datasets includ-ing the classic Reuters-21578 [27], but due to the space limit those experimental results are reported only as online supplementary material together with our program X  X  source code (see Section 3.3).

In the experiments, we have applied our proposed ap-proach to carefully analyse the performances of two well-known supervised machine learning algorithms that are widely used for real-world text classification tasks: Naive Bayes (NB) and linear Support Vector Machine (SVM) [19]. For the former, we consider its two common variations: one with the Bernoulli event model (NB Bern ) and the other with the Multinomial event model (NB Mult ) [20]. For the latter, we consider its two common variations: one with the L 1norm penalty (SVM L 1 ) and the other with the L 2 norm penalty (SVM L 2 ) [7, 30]. Thus we have four different classifiers in total. Obviously, the classification results of NB Bern and NB Mult would be highly correlated, and those of SVM L 1 and SVM L 2 as well. Among them, SVM L 2 is widely re-garded as the state-of-the-art text classifier [17,25,27]. It is also worth to notice that the NB algorithms will be applied not to the raw bag-of-words text datasets as people usually do, but on the vectorised 20newsgroups dataset which has http://scikit-learn.org/stable/datasets/twenty newsgroups.html already been transformed by TF-IDF term weighting and document length normalisation.

We have used the off-the-shelf implementation of these classification algorithms provided by a Python machine learn-ing library scikit-learn 4 in our experiments, again for the reproducibility reasons. The smoothing parameter  X  for the NB algorithm and the regularisation parameter C for the linear SVM algorithm have been tuned via grid search with 5-fold cross-validation on the training data for the macro-averaged F 1 score. The optimal parameters found are: NB Bern C =2 2 ,SVM L 2 with C =2 1 .

Table 4 shows the results of performance comparison be-tween NB Bern and NB Mult , based on which we can confi-dently say that for most of the target categories, NB Bern outperformed by NB Mult . Such results confirm the finding of [20] on this harder dataset.

Table 5 shows the results of performance comparison be-tween SVM L 1 and SVM L 2 , based on which we can confi-dently say that for most of the target categories, SVM L 1 and SVM L 2 have no practical difference on classification ef-fectiveness as measured by the F 1 score (given the ROPE [  X  0 . 05 , +0 . 05]), though the former may have its advantages in terms of sparsity. Such results are complementary to those reported in [30].

Table 6 shows the results of performance comparison be-tween NB Mult and SVM L 2  X  the better performing classi-fiers from the NB and SVM camps. It can be clearly seen that for most of the target categories, the competition be-tween NB Mult and SVM L 2 is too close to call: more test data would be needed to make a reliable judgement which one works better. Nevertheless, for six out of the eight target categories on which we can indeed make reliable judgements, NB Mult and SVM L 2 are practically equivalent (given the ROPE [  X  0 . 05 , +0 . 05]). This phenomenon somewhat sup-ports the claim of [23] that NB Mult , if properly enhanced by TF-IDF term weighting and document length normalisa-tion, can reach a comparable performance as SVM L 2 .
On the micro (document) level, no NHST method exists for the comparison of F 1 scores. So in the above tables we show the results of using NHST to compare classification accuracies instead: the column  X  X ign-test X  and  X  t -test X  con-tain the two-sided p -values of micro level sign-test (called s-test in [27]) and unpaired t -test (called p-test in [27]) re-spectively. The symbol indicates that the accuracy differ-ence between A and B is statistically significant ( p&lt; according to NHST. When NHST fails to reject the null hy-pothesis that the two classifiers work equally well, no con-clusion can be drawn from the comparison.

In all those tables, our proposed Bayesian performance comparison method has offered rich information about the difference between two classifiers X  F 1 scores: in addition to the final judgement ( X  X ecision X ), we have shown the pos-terior  X  X ean X , standard deviation ( X  X td X ), the Bayes factor estimated by the SD method ( X  X F SD  X ), the percentage lower or greater than the null value 0 ( X  X G pct X ), the percentage covered by the ROPE ( X  X OPE pct X ), and the 95%  X  X DI X . By contrast, the frequentist NHST would lead to a far less complete picture: it has only the p -values (and maybe also the confidence intervals) to offer. Furthermore, note that the judgements made by the Bayesian estimation on sev-http://scikit-learn.org/stable/ eral cases are different from those made by the frequentist NHST (e.g., at the significance level 0.05). So even if in some researchers X  opinion the superiority of the former over the latter is still debatable, there is no doubt that the former can at least be complementary to the latter.

Figure 6 illustrates the visualisation of Bayesian perfor-mance comparison results produced by our program: the  X  X osterior plot X  sub-graph shows the posterior probability distribution of the performance difference variable  X  ;and the X  X actor plot X  X ub-graph shows the estimation of the Bayes factor by the SD method.
The proposed Bayesian approach to performance compar-ison has been described above in the most basic setting for concreteness and simplicity, but it is in fact readily extensi-ble to the following more general scenarios.

Multiple classes . It would be straightforward to extend our model to multi-class classification (either single-label or multi-label): we will need one  X  parameter and a pair of  X  parameters for each class. Thus we are able to measure each classifier X  X  overall performance using micro-averaged or macro-averaged F 1 scores [27], and compute their difference as the deterministic variable  X  in the model [28]. Note that here  X  is estimated using a large number of prediction out-comes for all test documents, rather than just a small num-ber of F 1 scores for test categories as in [27] (see Section 2.1). It would be promising to go further to develop a Bayesian hi-erarchical model [15] where the classifier X  X  parameters  X  different classes are governed by a higher-level overarching hyper-parameter  X  (e.g., representing the overall probabil-ity of making correct predictions) and thus able to  X  X hare statistical strength X  [29]. A potential problem, though, is the explosive growth of possible prediction outcome combi-nations along with the increase of class numbers, which in the worst situation may force us into backing off to the as-sumption of independence between classifiers so as to keep the model computationally tractable.

Other performance measures . To compare classifiers using a performance measure different from the F 1 score, we would only need to replace the function f (  X ,  X  + ,  X   X  computing  X  , as long as that performance measure could be calculated based on the classification contingency table alone [12]. For example, it would be straightforward to extend our model to handle the more general F  X  measure (  X   X  0) [19,26] with  X  = 1: we just need to substitute the F  X  formula for the F 1 formula in the function of  X  . For another example, the Area Under the ROC Curve (AUC) is essentially the proportion of correctly ranked document pairs [9, 12], so it couldbemodelledinasimilarway.
 Other tasks . More generally, the idea of building a Bayesian probabilistic graphical model to make comprehen-sive performance comparison could be applied to not just classifiers, but also search systems (see the ICTIR-2015 best paper [3]), recommender systems, and advertising systems.
This paper tries to address the problem of comparing text classifiers X  performances by appealing to Bayesian reason-ing. Although we ourselves believe that Bayesian statistics is X  X he way it should be X , we understand that not everyone is a Bayesian or wants to become a Bayesian. Our argument is not whether being a Bayesian is philosophically better than being a frequentist, but that our Bayesian estimation based approach to performance comparison of text classifiers avoids all the aforementioned practical weaknesses of NHST (see Section 2.1) and it provides much richer information about the difference between two classifiers X  performances than NHST does, therefore it can supersede or at least com-plement the currently popular frequentist approach.
We thank the anonymous reviewers for their very helpful comments. This work was partly supported by the NSFC grants (61472141 and 61321064) as well as the Shanghai Knowledge Service Platform Project (ZF1213).
