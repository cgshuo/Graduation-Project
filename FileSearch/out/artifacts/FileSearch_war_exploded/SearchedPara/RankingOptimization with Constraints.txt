 This paper addresses the problem of post-processing of rank-ing in search, referred to as post ranking. Although impor-tant, no research seems to have been conducted on the prob-lem, particularly with a principled approach, and in prac-tice ad-hoc ways of performing the task are being adopted. This paper formalizes the problem as constrained optimiza-tion in which the constraints represent the post-processing rules and the objective function represents the trade-off be-tween adherence to the original ranking and satisfaction of the rules. The optimization amounts to refining the origi-nal ranking result based on the rules. We further propose a specific probabilistic implementation of the general formal-ization on the basis of the Bradley-Terry model, which is theoretically sound, effective, and efficient. Our experimen-tal results, using benchmark datasets and enterprise search dataset, show that the proposed method works much better than several baseline methods of utilizing rules. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Post Ranking; Ranking Optimization; Bradley-Terry Model
Recent years have observed a significant progress in re-search and development on learning to rank, i.e., creation of ranking models in search using machine learning tech-niques. Now, it becomes a common practice to exploit the learning technologies to construct the basic ranking model of a search system. This paper is concerned with post pro-cessing of ranking , which we call post ranking. Post ranking
Cu rrently affiliated with Institute of Computing Technolo-gy, Chinese Academy of Sciences.
 is normally conducted at web search engines in ad-hoc man-ners. The paper aims to provide a principled approach to post ranking, which does not seem to have been seriously studied previously.

In practice, there are many situations in which one wants to further  X  X wist X  the search results given by the basic rank-ing model, i.e., to conduct post ranking. Post ranking is widely adopted in practice, under the names of re-ranking, final ranking, etc. For example, the query is about a hot topic and one wants to boost a webpage about the topic from news channels to the top three positions, no matter how the ranking model does (note that it is usually hard to add such control into a learning to rank model). In an-other example, a web page is reported to be likely a spam page, and an immediate action is required to demote the position of the page, without change of the ranking mod-el. (See more examples in Section 3.) Post ranking needs to be carried out not only from the viewpoint of enhancing search quality, but also due to operational, commercial, and even political reasons. There are other situations in which post ranking appears to be necessary, such as diversification of search result [6, 27, 7], context aware ranking [29], per-sonalized ranking [25, 26, 23]. Therefore, post ranking is a necessary and important process for search.

Post ranking has the following characteristics. (1) It is usually query dependent, user dependent, or context depen-dent. (2) The effects of it may not be achieved by using the basic ranking model, because it is usually difficult, costly, or even impossible to implement in the basic ranking model. (3) It does not need learning or training.

The key challenge for post ranking lies in the difficulty of formalizing the problem in a theoretically sound, effective, and efficient way. The original search result might be very different from the rules of post processing, and the rules might also be contradictory to each other. Thus, it is not easy to incorporate the complicated controls into a single framework. Moreover, the process needs to be conducted online and thus must be very efficient.

This paper proposes formalizing ranking optimization as a constrained optimization problem. Given the ranking result of a query by the ranking model, we perform re-ranking on the result, by minimizing an objective function under a number of constraints, where the constraints represent the rules which we want to use for post processing, for example, boost a page to the top k position, and the objective function represents the trade-off between adherence to the original ranking and satisfaction of the constraints.
As the first study, we propose a method for ranking opti-mization. Our method adopts the Bradley-Terry model [2] for calculating the probability of a ranking list. It realizes the optimization problem as minimizing the negative log conditional probability of the original ranking list and the negative log conditional probability of the constraints giv-en a Bradley-Terry model. The optimization problem has a simple form and is guaranteed to have a global optimal solution. Our method employs gradient decent to find the optimal solution, with linear order time complexity. It is thus a theoretically sound, effective, and efficient method.
We have conducted experiments using the LETOR bench-mark datasets [14, 22] and a dataset from an enterprise search engine. We take as baselines several methods which adjust ranking results with heuristic rules. Experimental re-sults indicate that our method consistently and significantly outperform the baseline methods in terms of NDCG on all datasets, indicating that it is better to employ our method in post ranking.

The contributions of the paper include (1) formalization of ranking optimization, (2) proposal of a method of ranking optimization, (3) empirical study of ranking optimization.
The rest of the paper is organized as follows. After an in-troduction to related work in Section 2, we describe the for-mulation of ranking optimization with constraints in Section 3. We describe the proposed method based on Bradley-Terry model in Section 4. Experimental results and discussions are given in Section 5. Section 6 concludes this paper and gives future work.
Construction of ranking model is one of the key problems in IR. Given a query, the ranking model assigns relevance scores to the retrieved documents and sorts the documents based on the scores, and thus it plays an important role in search. Traditionally, a ranking model is defined based on a small number of factors, e.g., term frequency, inversed document frequency, and document length. BM25 [24] and LMIR (Language Models for Information Retrieval) [21, 12] are such models. Recently, machine learning techniques are applied to construction of ranking model usually using a large number of features and a large amount labeled training data, referred to as learning to rank. Methods of learning to rank are categorized as  X  X ointwise X  [18, 13],  X  X airwise X  [9, 8, 3, 4], and  X  X istwise X  [5, 30, 31, 32, 28] methods, depending on the loss functions used. In this paper, we adopt the listwise method of LambdaMART [28] to train the basic ranking model.

As explained above, basic ranking is not enough, and post ranking is necessary in many cases. In practice, post ranking is conducted by using heuristic rules, and there has not been research on post ranking itself, as far as we know. There are several other ranking issues, which can be addressed through post ranking as well, such as search result diversification, personalized ranking, and context aware ranking.

In recent years, search result diversification arises as a hot topic in IR, in which the search system returns a list of documents which are not only relevant to the query but also cover many subtopics of the query. A common practice for diversification is to see diversification as a post ranking step after the ranking list based on relevance is created [6, 27, 7]. This is because it is usually hard to model relevance and diversity in a single framework.

Personalized search may also be realized in post ranking, for example, in what is called client side re-ranking [25, 26, 23]. Specifically, the ranking result by the basic ranking model is sent to the client side and re-ranking of the result based on the user X  X  interest is conducted. One advantage of the approach is that re-ranking is carried out entirely on the client side and the privacy of user can be protected. Xiang et al. [29] have proposed context aware ranking. For example, if the user has clicked a URL in the previous search in the same session, then it is very likely she will not click the same URL again when it appears in the result of the current search. That means that ideally the URL should be demoted in the current search result.

Probabilistic models for ranking have been studied in s-tatistics and related fields from many years ago. The most popular ones include Plackett-Luce model [20, 15], Mallows model [16], and Bradley-Terry model [2, 10]. Plackett-Luce model is a stagewise generative model, which decomposes the process of generating a permutation of n objects into n sequential stages. Mallows model is a distance-based model, which defines the probability of a permutation according to its distance to a centroid permutation. Bradley-Terry model calculates the probability of a permutation by pairwise com-parisons. See [17] for a review on the topic. The Plackett-Luce model has been utilized in learning to rank [5], and the Bradley-Terry model is utilized in ranking optimization in this paper.
Let us describe the process of post ranking. Given a query, a ranking list of documents is first created by the basic rank-ing model, presumably built by learning to rank. Post rank-ing may be then conducted, depending on the query, user, or context, which means a refinement of the original rank-ing list, in which some documents are moved up and some are moved down. The original ranking list is created mainly from the viewpoint of relevance between query and docu-ments. The refined ranking list is further created from the viewpoint of quality, diversity, personalization, contextual-ization, and so on.

The actions of post processing can be realized by using heuristic rules, which is a common approach in practice. A rule can be  X  X f the query is in a list of terminologies, then always have the Wikipedia page of the terminology on the top one position X . Another rule can be X  X f the documents are retrieved by both the original query and refined queries, then have at least one document retrieved by the original query ranked at the top three positions X (to reduce the risk of topic shift). Yet another rule can be  X  X f the query is  X  X residential election debates X , then make all the top webpages of different rounds of the debates grouped together in the ranking list X . How to use rules for post ranking is not a trivial issue. First, the rules may not be hard rules and they only rep-resent a guideline for refining the initial ranking list. For example, a rule may be  X  X he document should be ranked at top three positions X , in which no specific position is decided. Second, multiple rules might be applied at the same time, and the rules might be contradictory to each other. Third, different orders of applications of rules might yield different final ranking results and thus the order of applications needs also be considered. Finally, the rules usually only affect a Fi gure 1: Illustration of ranking optimization with constraints. small number of documents, it is important to make a bal-ance between application of rules and preservation of the original ranking list.

In this paper, we formalize post ranking as constrained optimization, referred to as ranking optimization with con-straints. The constraints represent the rules for post rank-ing, defined as functions over sets of permutations. The ob-jective function represents the trade-off between adherence to the original ranking list and satisfaction of the constraints. The constraints are in fact soft constraints in the formula-tion. Post ranking is naturally performed by conducting the optimization problem. Therefore, the issues described above can all be naturally solved in the framework.

Suppose that  X  denotes the original ranking list, C denotes the set of constraints, and  X  denotes the final ranking list of post ranking. The optimization can be written as follows where L denotes agreement between  X  and  X  , R denotes satisfication of C by  X  ,  X  denotes the tradeoff coefficient, and  X  N denotes the set of all permutations (ranking lists) on the N documents in the current search. In the optimization process, we need to find the optimal final ranking  X  .
Suppose the set of constraints C is defined as C = { c i (  X 
N  X  X  0 , 1 } . If c i (  X  ) = 1, then permutation  X  violates con-straint c i , otherwise, c i (  X  ) = 0. The subset of permutations which do not violate the constraints are good candidates for the final ranking list.
 We can define several types of constraints.
 Top-k constraint: A document must be at top k positions. Not-top-k constraint: A document cannot be at top k Clustering constraint: Two or more documents should Diversity constraint: Two or more documents should not
Figure 1 illustrates the problem of ranking optimization with constraints (note that the figure is only for illustration purposes; the set of permutations forms a discrete set, not a Euclidean space.).  X  N is the set of all possible permutations for N documents.  X  C is the subset of permutations satisfy-ing the constraints in C .  X  is the original permutation given by the basic ranking model. In ranking optimization, we aim to find the optimal permutation (ranking list)  X  which is as close to  X  as possible and in the meantime as within  X 
C as possible.
In this section, we propose a method of ranking optimiza-tion on the basis of the Bradley-Terry model. The method only makes use of the top k constraint and the not top k constraint. We leave to future work the study of adding other constraints to the method.
We consider a probabilistic approach to ranking optimiza-tion. We assume that there exists a probabilistic rank-ing model M which gives rise to the ranking list  X  , by  X  = arg max  X  P (  X  | M ) and we incorporate M into the opti-mization problem (1) to obtain That is to say, we turn the optimization problem (1) with re-spect to  X  into an optimization problem with respect to both  X  and M . Let L (  X ,  X , M ) =  X  log P (  X  | M ) and R ( C  X  log P ( C| M ), where P (  X  | M ) is the probability of generat-ing the permutation  X  given M and P ( C| M ) is the proba-bility of generating all of the constraints in C given M . We first solve and then solve where  X  denotes the optimal ranking list.

The interpretation of the method is as follows. Given the ranking list  X  by the basic ranking model as well as the set of constraints C , we want to first find a probability model M that can best explain the ranking list as well as the constraints (i.e., the product of the probabilities P (  X  and P ( C| M ) is the largest, with a trade-off coefficient). After M is determined, we want to find the best ranking list given by M (i.e., the probability P (  X  | M ) is the largest).
In this paper, we choose the Bradley-Terry model for cal-culation of P (  X  | M ) and P ( C| M ).
The Bradley-Terry model represents the probability dis-tribution of permutation of N documents (in general items) by making comparison among all pairs of documents. It as-sumes that the ranking model M is parameterized with a set of N scores  X  = (  X  1 ,  X  X  X  ,  X  N ), each corresponding to a document. Furthermore, the parameters are assumed to be positive and sum to one, i.e.,  X  i &gt; 0 for i = 1 ,  X  X  X  i =1  X  i = 1. In Bradley-Terry model, the probability of a preference pair ( i, j ) (document i be ranked higher than j ) is defined as
Gi ven the permutation  X  , the Bradley-Terry model defines the probability P (  X  | M ) as being proportional to the product of probabilities of i ranked higher than j for all preference pairs ( i, j ):
Gi ven the constraint set C , the Bradley-Terry model de-fines the probability P ( C| M ) based on the preference pairs derived from C : wh ere P c is the set of preference pairs derived from the constraint c .

The methods for deriving preference pairs depend on the types of constraints. Here, we give methods for the top-k constraint and not-top-k constraint. Given a top-k con-straint c , the set of preference pairs P c is defined as where i is the document to promote in constraint c . Sim-ilarly, Given a not-top-k constraint c , the set of preference pairs is where i is the document to demote in constraint c . We note that the top-k and not-top-k constraints can only be defined on the original ranking list  X  , because only after the ranking list is given one can perform post ranking on it, i.e., impose constraints on it. Thus, the top-k and not-top-k constraints restrict the positions of a document in the original ranking  X  .
Thus, the optimization in Equation (2) becomes min where  X  c &gt; 0 is the weight for constraint c . Note that pa-rameter  X  in Equation (2) has been merged to parameters  X   X  X  in Equation (4).

The final ranking  X  , then, can be obtained via the max-imization in Equation (3). With the use of Bradley-Terry model, it can be simplified to sorting of the documents in descending order of scores in  X .
It is easy to demonstrate that f ( X ) = f (  X   X   X ) for any  X  &gt; 0 (note that log  X  i  X  get rid of the constraint is because for any solution we can normalize the  X  i  X  X  by dividing them with satisfy the constraint and keep f unchanged.

Furthermore, the constraints of  X  i &gt; 0 can be discard-ed by replacing  X  i with e s i for i = 1 ,  X  X  X  , N . Thus, the optimization problem in (4) becomes the following uncon-strained optimization problem: min S f ( S ) = where S = { s 1 , s 2 ,  X  X  X  , s N } is the set of parameters. The objective function f ( S ) in Equation (5) is convex, as stated in Theorem 4.1.
 Theorem 4.1. f ( S ) is a convex function.
 Proof of the theorem can be found in Appendix. Theorem 4.1 indicates that the objective function f can be efficiently optimized with gradient descent. The gradient w.r.t. S can Al gorithm 1 Ranking Optimization Algorithm Re quire: Initial ranking  X  , constraints C , and shrinkage 1: S (0)  X  random values 2: t  X  1 3: repeat 5:  X   X  1 6: while f ( S ( t 1)  X   X   X  X  ) &gt; f ( S ( t 1) )  X   X  2  X   X  X   X  2 7:  X   X   X  X  8: end while 10: t  X  t + 1 11: until convergence 12: return  X  = { e s 1 Z ,  X   X  X  , e s N Z } , where Z = df + Thus, the updating criterion for gradient descent is where t is the iteration number,  X  ( t ) is the optimal step size at the i -th iteration which is determined by backtracking. Algorithm 1 shows the pseudo code of the optimization al-gorithm.

The final document ranking  X  , then, is obtained by sort-ing with the scores in  X  returned in Algorithm 1.
We analyze the convergency of Algorithm 1 and have the following theorem:
Theorem 4.2. Algorithm 1 converges in nite steps and the convergence rate is O ( 1  X  ) , where  X  &gt; 0 is the tolerance. Proof of the theorem can be found in Appendix. Theorem 4.2 implies that Algorithm 1 can return an optimal ranking model in a reasonably short time, which makes it possible to apply the algorithm online. The gradient in Equation (6) has an intuitive explanation. The gradient consists of two parts, one based on the original ranking list  X  and the other based on the constraint set C Both parts contribute to the gradient and are calculated on the basis of preference pairs.

Given a preference pair ( i, j ), there will be a force that pushes the preferred document i upward, by subtracting a that in gradient descent s i is updated with negative gradi-ent). The term also indicates that the force is related to the Fi gure 2: Intuitive explanation of the gradient. Given a preference pair ( i, j ) , document i will be pushed upward (green solid arrow) and document j will be pushed downward (red dashed arrow). The strengths of the forces are identical.
 d ataset # queries #documents #relevance levels M Q2007 1692 69623 3 MQ2008 784 15211 3
OHSUMED 106 16140 3 .Gov 50 49058 2
En terprise 183 5464 3 d ifference between the scores. A small s i  X  s j , which means the current scores s i and s j do not agree with the prefer-ence pair, leads to a strong force to promote document i . In the same time, the un-preferred document j will be pushed downward, by adding the same term to gradient df ds 2 illustrates the forces that respectively push the document i and document j upward and downward.
 Given all of the preference pairs derived from  X  and from C , the overall forces that promote (or demote) a document i are jointly determined by all the preference pairs related to document i .
We conducted experiments to test the performances of our method for post ranking (ranking optimization).
We know of no existence of public data available for post ranking. As approximation, we used relevance datasets for the experiments. That is, we assume that we know that some documents are relevant (i.e., should be ranked high) and boost the ranks of the documents in post ranking.
We made use of the following subsets of the LETOR bench-mark dataset: MQ2007, MQ2008, OHSUMED and .Gov, as well as a dataset of enterprise search, denoted as Enterprise. Table 1 gives the statistics of the four LETOR datasets [14, 22] and the Enterprise dataset.

The Enterprise dataset consists of 183 queries; each query is associated with about 30 documents. In total, the dataset contains 5464 query-document pairs. Each query-document pair is assigned with a label representing relevance at three levels: Good, Fair, or Bad. The dataset is split into training data (130 queries) and test data (53 queries).

The ranking models were trained using LambdaMART [28], which is a state-of-the-art method in learning to rank. The standard features in LETOR datasets were utilized and we also defined 18 features for Enterprise, including BM25 [24] and word level edit distance 1 , etc.

Two types of constraints were tested in our experiments: the top-k constraints and not-top-k constraints. For each test query, we generated one top-k ( k = 1 , 3 , 5) constraint and one not-top-k ( k = 5 , 10) constraint based on the labels h ttp://en.wikipedia.org/wiki/Edit di stance of documents with respect to the query 2 . Specifically, we sorted the documents according to their labels (to obtain a perfect ranking) and randomly selected one document i from the top k positions. Then we created a constraint c which states that the selected document i should be ranked to top k positions in the final ranking. Similarly, we also created a not-top-k constraint by randomly select a document j from positions after k in the perfect ranking.

For simplicity, we assumed that all top-k (and all not-top-k ) constraints are equally important and take the values of  X  (and  X  n ). Thus, the ranking optimization of Equation (5) becomes
As for baseline methods, we use the following four heuris-tics for modifying the original ranking: Radical For the top-k constraint, Radical always ranks the Moderate For the top-k constraint, Moderate always ranks Conservative For the top-k constraint, Conservative al-Proportional The above three baselines do not consider
As evaluation measures, Normalized Discounted Cumula-tive Gain (NDCG) [11] at positions 1, 3, and 5 were used.
In all the four datasets in LETOR, the queries and as-sociated documents were split to 5 subsets, and 5-fold cross validations were conducted. The performances reported here are the averages over 5 trials.

For each dataset, we used the training data to learn the basic ranking model, the validation data to tune the param-eters, and the test data to perform ranking optimization. There are two parameters  X  t and  X  n tuned with the validate
Ra nking optimization can be conducted or not conducted depending on queries. Here for experimentation purpose, ranking optimization is assumed to be carried out for all queries. t =10,  X  n =10 (a ) top-3, not-top-5,  X  t =100,  X  n =10 =100,  X  n =10 (a ) top-3, not-top-5,  X  t =200,  X  n =0 =200,  X  n =10 t =50,  X  n =50 (a ) top-3, not-top-5,  X  t =60,  X  n =40 t =60,  X  n =40 Table 2: Average time (in milliseconds) of ranking optimization in setting of (top-5 , not-top-10 ). time 4.24 6.85 134.53 70.06 6.45 set . The performances on the test sets are those based on the best performing parameters.

We tested all the six combinations of top-k constrain-t ( k = 1 , 3 , 5) and not-top-k constraint ( k = 5 , 10) on all of the four datasets. Figures 3, 4, 5, and 6 report the ex-perimental results on MQ2008, MQ2007, OHSUMED, and .Gov, respectively. Our ranking optimization method is de-noted as  X  X ankOpt X  in the figures. The best performing parameters of  X  t and  X  n are also shown. The performances of the combinations (top-3, not-top-5), (top-3, not-top-10), and (top-5, not-top-10) are reported.

From the results, we can see that our method outperforms the baselines as well as the basic ranking model of Lamb-daMART on all datasets. The experimental results for (top-3, not-top-5) and (top-3, not-top-10) are very similar. This is because all the methods focus on the top of ranking list. The changes of k on the not-top-k constraint have less im-pact on the final ranking list. Our method and the baselines perform equally well, when k is 1 for the top-k constrain-t. This is because the top-1 constraint means promoting the document to the top one position, and our method can function as a hard rule in such case and produce the same ranking result as the baselines (we omit the result because of space limitation). We will have discussions on the effect of different k values in Section 5.3.

We conducted significant tests (t-test) on the improve-ments of our method over all the baselines. The results in-dicate that all the improvements are statistically significant (p-value &lt; 0.05), except (top-5, not-top-10) over  X  X ropor-tional X  on MQ2008 in terms of NDCG@5, (top-3, not-top-5) and (top-3, not-top-10) over  X  X adical X  on OHSUMED and .Gov in terms of NDCG@3 and NDCG@5. Note that when top-1 constraint is adopted, our method performs equally well with the baselines.

Table 2 reports average running time of ranking optimiza-tion by our method (with top-5 and not-top-10 constraints) in milliseconds on a Laptop PC with 2.4GHZ CPU and 4G-B memory. We can see that our method runs very fast, even on an unpowerful machine. We also observed that for most queries the algorithm converges within 10 iterations. Similar experimental results were also observed in other ex-periments. The results empirically verify the conclusion of Theorem 4.2.
In the experiment, we utilized the training data to learn the basic ranking model and to tune the parameters  X  t and  X  . The test data was utilized to perform ranking optimiza-tion. We tested all the six combinations of top-k constraint ( k = 1 , 3 , 5) and not-top-k constraint ( k = 5 , 10) on the Enterprise dataset. The performances of the combination-s (top-3, not-top-5), (top-3, not-top-10), and (top-5, not-top-10) are reported in Figure 8. Our ranking optimization method is denoted as  X  X ankOpt X  in the figures. From the results, we can see that our method outperforms the base-lines as well as the basic ranking model of LambdaMART on all datasets. The experimental results for (top-3, not-top-5) and (top-3, not-top-10) are very similar, as in the exper-iments on LETOR datasets. We conducted t-test on the improvements of our method over the baselines in terms of NDCG@1, NDCG@3 and NDCG@5. The improvements are statistically significant. Again, our method and the baselines perform equally well when the top-1 constraint is adopted. We omit the result because of space limitation.

We also tested the running time of our method of ranking optimization, in the setting of top-5 and not-top-10 con-straints. The last column of Table 2 reports average time of our method in milliseconds. We can see that our method runs very fast. Similar experimental results were obtained for other combinations of top-k and not-top-k constraints.
We first investigated the reasons that our method of rank-ing optimization outperforms the baseline methods. We found that in general our method can really work better than the baselines to make a good comprise between using the original ranking and using the constraints.

Here, we use the result of MQ2008 dataset with regard to two queries to illustrate why our method is superior to the baselines. Figure 8(a) shows the original ranking by LambdaMART and the final rankings by different methods with regard to one query. The empty blocks, grid blocks, and filled blocks represent not relevant documents, partially relevant documents, and relevant documents, respectively. The documents selected by top-k constraints and not-top-k constraints are marked with  X  X  X  and  X  X  X , respectively. From the results, we can see that our method of ranking optimiza-tion can really promote the relevant document (marked with  X  X  X ) and demote the not relevant documents (marked with  X  X  X ). In this case, our method outperforms the baselines of Moderate, Conservative, and Proportional. The results with regard to the other query are reported in Figure 8(b), which is a noisy case in which the document selected by the top-k constraint is actually not relevant (noises may also exist in the rules in practice). Our method of ranking optimization also takes into account the position of the document in the original ranking list given by LambdaMART and thus does not promote it too much. In other words, our method can make a good trade-off between the constraints and the orig-inal ranking. On the other hand, the baselines of Radical and Moderate do not have such consideration and cannot rank the document satisfactorily. Therefore, our method is more capable for post ranking than the baselines.
We further conducted experiments to see the impact of different k values on different methods. For the top-k con-straint, larger k means a softer control on the final rank-ing list. When k = 1, the constraint becomes a hard rule. From the results reported in Figure 9(a), we can see that our method of ranking optimization always works better than or as well as the baselines for all k values. When k gets close to one, our method will perform similarly as the baselines; when k gets larger, the improvements of our method over the baselines will also be larger. The results indicate that our method is more robust than the baselines. We also note that the baseline of Radical hurts the basic ranking results when k gets large. This is because Radical is very sensitive to the noise in the constraints (rules). The results indicate that it is risky to directly apply Radical to post ranking though it outperforms the other baselines in most of the experiments.
On the other hand, for the not-top-k constraint, smaller k means a softer control on the final ranking list. When k = N  X  1, the constraint becomes a hard rule. From the re-sults reported in Figure 9(b), we can see that our method of ranking optimization always works better than or as well as the baselines for all k values. When k gets close to one, our method will perform similarly as the baselines, because the not-top-k constraint has a soft control on the final ranking list. When k gets larger, the improvements of our method over the baselines will also become larger. There is a peak Figure 9: Performances of ranking optimization with respect to different k values in terms of NDCG@1. for performance of our method around k = 5. The perfor-mance will drop after k = 5. This is because the not-top-k constraint impacts more on the tail of the ranking list, and a larger k will have less impact on the top of the ranking list, which is more important in ranking evaluation.
We also investigated the influence of different types of con-straints. Specifically, we tested performances of our method of rank optimization without the constraints (LambdaMART only), with top-5 constraint only, with not-top-10 constraint only, and with both types of constraints. Figure 10 reports the results. From the figure we can see that the two type-s of constraints can individually improve the ranking per-formances if they are adopted. The performances can be further improved if they are used simultaneously. The re-sults indicate that our method of ranking optimization can leverage multiple types of constraints within one framework. The top-5 constraint outperforms the not-top-10 constraint, because the evaluation measure of NDCG emphasizes the importance of top ranking, which is also the focus of the top-k -constraint.

Finally, we evaluated how sensitive our method of ranking optimization is to the parameter settings. In the experiment, two parameters  X  t and  X  n were tested, which are weights of the top-k constraint and not-top-K constraint, respectively. Fi gure 10: Performances of ranking optimization with different constraint types. Figure 11: Performances of ranking optimization with respect to different parameter settings.
 We changed one parameter and fixed the other to its optimal value. Figure 11(a) and Figure 11(b) show the performances of ranking optimization of our method in terms of NDCG at the positions of 1, 3, and 5. From the results, we can see that our method is not sensitive to the parameter settings, and thus is quite robust.
In this paper, we have studied the problem of post pro-cessing of ranking in search, which we call post ranking. Practices on post ranking in reality tend to be heuristic and we have, perhaps for the first time, formalized the problem as an optimization problem. In the formulation, we repre-sent the post processing rules as constraints and manage to minimize the objective function denoting the trade-off be-tween the original ranking and the constraints. As a result, one can perform post ranking through solving the optimiza-tion issue.

We have also given a specific probabilistic implementa-tion of the optimization formulation. Bartley-Terry model is employed to calculating the probability of ranking list. The objective function is defined based on the conditional probability of the original ranking and the conditional prob-ability of the constraints given the model. The optimiza-tion amounts to minimization of the sum of the negative log probabilities.

We have compared the performances of our method with several baselines in experiments using a number of datasets including benchmark datasets. The baseline methods repre-sent practical methods of using rules. The results show that it is always better to employ our method in post ranking than the baselines.

There are still many open questions with regard to rank-ing optimization. We plan to conduct more research on the problem in the future. The open questions include (1) whether there exists a more general framework for ranking optimization, (2) how to define and incorporate other type-s of constraints into the framework, (3) how to naturally add diversification of results, etc. into the framework, (4) whether there are more effective and efficient methods for the task. [1] S. Boyd and L. Vandenberghe. Convex Optimization . [2] R. A. Bradley and M. E. Terry. The rank analysis of [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. [6] J. Carbonell and J. Goldstein. The use of mmr, [7] Z. Dou, S. Hu, K. Chen, R. Song, and J.-R. Wen. [8] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [9] R. Herbrich, T. Graepel, and K. Obermayer. Large [10] R. Hunter. Mm algorithms for generalized [11] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [12] J. Lafferty and C. Zhai. Document language models, [13] P. Li, C. J. C. Burges, and Q. Wu. Mcrank: Learning [14] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [15] R. D. Luce. Individual Choice Behavior: A theoretical [16] C. L. Mallows. Non-null ranking models. i.
 [17] J. I. Marden. Analyzing and Modeling Rank Data . [18] R. Nallapati. Discriminative models for information [19] Y. Nesterov and I. Nesterov. Introductory Lectures on [20] R. L. Placket. The analysis of permutations. Applied [21] J. M. Ponte and W. B. Croft. A language modeling [22] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A [23] F. Radlinski and S. Dumais. Improving personalized [24] S. Robertson and D. A. Hull. The trec-9 filtering track [25] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [26] J. Teevan, S. T. Dumais, and E. Horvitz.
 [27] E. Vee, U. Srivastava, J. Shanmugasundaram, P. Bhat, [28] Q. Wu, C. J. Burges, K. M. Svore, and J. Gao. [29] B. Xiang, D. Jiang, J. Pei, X. Sun, E. Chen, and [30] J. Xu and H. Li. Adarank: A boosting algorithm for [31] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. [32] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A Proof of Theorem 4.1.

Proof. The objective function f can be decomposed as f = f a  X  f b , where f a = f = is the union of all preference pairs. b i  X  0 for i = 1 , is also convex for any ( i, j )  X  P , because log-convexity can be persevered under addition (cf., [1]). Hence, f a is convex because a ( i,j )  X  0 holds for all ( i, j )  X  X  .
Finally, we conclude that f = f a  X  f b is convex because f is linear.
 Pro of of Theorem 4.2.
 pair ( i, j ), obviously the objective function f in Equation (5) has a lower bound of 0. Also, according to Theorem 4.1, f is convex. Thus, there exits one and only one optimal solution for the problem of Equation (5), denoted as S .

Since gradient descent and backtracking are adopted for the optimization, the following inequality holds (cf., [19]): wh ere t is the number of iterations,  X  min = min { 1 ,  X /L L is Lipschitz constant for f , and  X  is the shrinkage rate. Thus, Algorithm 1 can converge within O ( 1  X  ) to reach the tolerance  X  .
