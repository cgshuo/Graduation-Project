 1. Introduction
With the rapid growth of the World Wide Web, the task of classifying natural language documents into a pre-defined set of semantic categories has become one of the key methods for organizing online information. This task is commonly referred to nizing this overwhelming amount of data is to classify them into topical categories.

Since the machine learning paradigm emerged in the 1990s, many machine learning algorithms have been applied to text Vector Machine (SVM) ( Joachims, 2001 ).

However, the major bottleneck of the supervised learning algorithms is that they require a large number of labeled train-to E-mails and newsgroup postings, it is also a difficult task to create training data for each application area (Nigam, manually labeling task.

In this paper, we propose a new text classification method based on unsupervised or semi-supervised learning. The pro-technique . The input to the bootstrapping process is a large amount of unlabeled documents and a small amount of seed mation. To automatically build up a text classifier with unlabeled documents, we must solve two problems; how we can automatically generate labeled training documents ( machine-labeled data ) from only a title word, and how we can handle incorrectly labeled documents in the machine-labeled data. This paper provides the solutions of both the problems. For the former, we employ the bootstrapping technique and, for the latter, we use the TCFP ( Text Categorization using Feature
Projections ) classifier with robustness from noisy data (Ko &amp; Seo, 2002 ). 1.1. How can an automatic text classifier be built from unlabeled documents?
Do you think that it is possible to build a text classifier with only unlabeled documents? Maybe we cannot gain any infor-mation from unlabeled documents for building a text classifier because the unlabeled documents do not contain the most occurrence information between the title word ( X  automobile  X ) and the other words. In the proposed method, context is de-category which include at least one among the title word and the keywords. The extracted contexts are called by centroid-contexts because they are regarded as contexts with the core meaning of each category. We can obtain many words directly occurrence cannot sufficiently describe the meaning of the category, we must collect more contexts by measuring similar-ities between centroid-contexts and remaining contexts; the remaining contexts do not have any title word and any key-words. The collected contexts contain the words in the second-order co-occurrence with the title word and the keywords. classifier can assign each unlabeled document its label, the labeled training documents are obtained automatically; it is called by machine-labeled data .

When the machine-labeled data is used to build up supervised mannered text classifiers, there is an additional problem in that the data has more incorrectly labeled documents than manually labeled data does. Thus we develop and employ the TCFP classifier with robustness from noisy data for learning from the machine-labeled data.
 strapping technique to create machine-labeled data. Section 4 describes the TCFP classifier to learn from the machine-la-beled data. Section 5 is devoted to the analysis of empirical results. In Section 6, we discuss the proposed method and results. Finally, we describe conclusions and future work. 2. Related work that learn from labeled and unlabeled documents (Ghani, 2002; Lanquillon, 2000; Nigam, 2001), models that perform a par-Sona, 2003; Slonim, Friedman, &amp; Tishby, 2002 ).

The studies to support the manual labeling of documents focus on the labeling task of a restricted set of documents. Then they fulfill to a requirement of a minimum amount of labeled data for each category using unlabeled data. Nigam et al. (1998) studiedanExpectedMaximization(EM)techniqueforcombininglabeledandunlabeleddatafortextclassificationinhis dissertation. Ghani (2002) developed a framework to incorporate unlabeled data in the Error-Correcting OutputCoding (ECOC) setup by first decomposing multiclass problems into multiple binary problems and using Co-Training to learn the individual binary classification problems. Lanquillon (2000) presented another approach for learning from labeled and unlabeled data. clustering methods. Jeonand Landgrebe (1999) proposed a new partially supervised classification method using unsupervised clustering,and Liuetal.(2002) studiedtheproblemofclassificationwithonlypartialinformation,oneclassoflabeled(positive) documents, and a set of mixed documents. Roy and McCallum (2001) presented an active learning method that directly opti-
In the other hand, there are several studies which used the clustering algorithms to text classification for not doing any documents by a new clustering method, the sequential Information Bottleneck (sIB) algorithm. Adami et al. (2003) proposed a semi-automatic process whose aim is to minimize the work required to the administrators when creating, modifying, and maintaining taxonomy with labeled documents.
 Basically, the problem we are attacking can be conceived as a bootstrapping technique using keywords for each category.
Several bootstrapping techniques using keywords have studied in this literature (McCallum, Nigam, Rennie, &amp; Seymore, 2000; Urena-lopez, Buenaga, &amp; Gomez, 2001). McCallum et al. (2000) presented a bootstrapping method for construction chy, keyword, and phrases can be useful. With a category hierarchy and human-provided keywords, a rule-list classifier can be built and it preliminarily can label unlabeled documents. The bootstrapping iterations are EM steps that used unlabeled of word sense disambiguation (WSD). However, this method required the whole labeled training documents and did not ex-ploit any unlabeled document. 3. The bootstrapping technique to generate machine-labeled data
The bootstrapping process consists of three modules as shown in Fig. 1 : a module to preprocess unlabeled documents, a
Each module is described in the following sections in detail. 3.1. Preprocessing
The preprocessing module has two main roles: extracting content words and reconstructing unlabeled documents into as content words.

Generally, the supervised learning approach with labeled data regards a document as a unit of meaning. However, since machine-labeled data is created from only a title word, context is defined as a new unit of meaning, and it is used as the the bootstrapping process is to build up labeled training documents from only title words automatically. Hence, the middle size processing unit, between word and document, is required. A sequence of 60 content words within a document is re-garded as the window size for one context; we believe that words co-occurred with any keyword in a context have important technique applied to a WSD problem. Yarowsky recommended the optimal window size to be between 40 words and 100 words in his paper (Yarowsky, 1994 ). Thus we conducted simple experiments using various window sizes (40 X 100 words) and consequently chose 60 words for the window size. To extract the contexts from a document, we use a sliding window document in the size of the window (60 words) and with the interval of each window (30 words). That is, two successive contexts are overlapped in rear 30 words of the previous window and front 30 words of the next window. Therefore, the final output of preprocessing is a set of context vectors that are represented as content words of each context. 3.2. Constructing a context-cluster as the training data of each category
At first, keywords are automatically generated from a title word for each category using co-occurrence information. Then word and keywords. It is regarded as one of the most informative contexts for each category. Furthermore, more information of each category is obtained by assigning remaining contexts to each context-cluster by a similarity measure technique. 3.2.1. Creating keyword lists semantic similarity is estimated for extracting keywords by using co-occurrence information between the title word and other words in the unlabeled documents.

The score of semantic similarity between a title word, T , and a word, W , is calculated by the cosine metric as follows: or 1, and n is the total number of documents in the unlabeled documents. This formula calculates the similarity score be-tween words based on the degree of their co-occurrence in the same document.

The most important criterion for good keywords of a category is a similarity with the title word of each topic category, ilarity with title words of two or more categories, the word must be excluded from keywords because it does not have the assigned word is recalculated by using the following formula: where T max is a title word with the maximum similarity score of a word W , c
T secondmax is other title word with the second high similarity score of the word W .

This formula means that a word in high ranking has a high similarity score with the title word (sim( T similarity score difference with other title words (sim( T sorted out according to the score calculated by formula 2 in a descending order. Then top m words are chosen as keywords in the category. Table 1 shows the list of keywords (top 5) for each category in the WebKB data set. 3.2.2. Extracting and verifying centroid-contexts
A context with a keyword or a title word of any category is selected as a centroid-context. From the selected contexts, we centroid-contexts, some contexts could not have effective features of a category even though they include a keyword or a title word of the category. Thus the importance score of each centroid-context is measured and it is ranked according to egory and Inverse Category Frequency (ICF) (Cho &amp; Kim, 1997 ). Using word weights ( TW ( S )in j th category ( c j ) is computed as follows: where N is the number of content words in each centroid-context.
 The centroid-contexts of each category are sorted in a descending order according to their calculated importance scores.
This order of the centroid-contexts is used in the following section. 3.2.3. Creating the context-cluster of each category
We here gather the second-order co-occurrence information by assigning remaining contexts to the context-cluster of each category. For the assigning criterion, we calculate similarities between remaining contexts and the centroid-contexts of each category. Thus we employ the similarity measure algorithm by Karov and Edelman (1998) . In the proposed method, a part of this algorithm is reformed for our purposes, and remaining contexts are assigned to each context-cluster by this algorithm. the rows and columns of WSM are labeled by all the content words encountered in the centroid-contexts of each category and input remaining contexts, and the rows of CSM correspond to the centroid-contexts and the columns to the remaining contexts. Each category has one WSM and one CSM. In each iteration n ,WSM 3 as it is recommended by Karov and Edelman (1998) .
 enough. 1. Update the context similarity matrix CSM n , using the word similarity matrix WSM 2. Update the word similarity matrix WSM n , using the context similarity matrix CSM relation between words and contexts is expressed as affinity and is represented by aff some affinity to every word, reflecting the similarity of X to the contexts involving that word. to a context X :
In the above formulae, n denotes the iteration number, and the similarity values are defined by WSM has some affinity to a context, and the context can be represented by a vector indicating the affinity of each word to it. (3) Similarity formulae. The similarity of W 1 to W 2 is the average affinity of the contexts that include W ilarity of a context X 1 to X 2 is a weighted average of the affinity of the words in X follows:
The weights in formula (6) are calculated by a methodology described in Appendix A . Since each weight in formula (7) is a responding entries of WSM n and CSM n . (4) Assignment of remaining contexts to a category. The similarity value of each remaining context for each category is decided by using the following formula:
In formula (8), X is a remaining context, C ={ c 1 , c 2 , ... , c category c i .

Each remaining context is assigned to a category with the maximum similarity value. However, there may exist remain-ing contexts which do not belong to any category. To remove these remaining contexts, we set up a dropping threshold using normal distribution of similarity values as follows (Ko &amp; Seo, 2000 ): where X is a remaining context, l is an average of similarity values, sim and h is a numerical value corresponding to the threshold (%) in normal distribution table.

Finally, a remaining context is assigned to the context-cluster of any category, when the category has a maximum sim-a validation set. 3.3. Learning a Naive Bayes classifier using context-clusters ument but from words distribution within each category. Therefore, the Naive Bayes classifier is constructed by estimating
Thus better reflect uncertainty than those produced by Naive Bayes. A document d where n is the number of words in document d i , w t is the t th word in the vocabulary, N(w document d i . 4. Using a feature projection technique for handling the noisy data of the machine-labeled data The labeled data of a documents unit is finally obtained through the bootstrapping process, machine-labeled data . bootstrapping method, it generally includes more incorrectly labeled documents than the human-labeled data. In order to
TCFP showed better performance than other conventional classifiers in using machine-labeled data. 4.1. The TCFP classifier with robustness from noisy data voting scores of all features.
 projections of a feature must become more useful classification criteria for the feature, only elements with TF-IDF values above the average TF-IDF value are used for voting. The selected elements participate in proportional voting with the same importance as the TF-IDF value of each element. Thus, the voting ratio of each category c following formula:
In formula (11), f mi denotes the projection element for a feature f document d i , V m denotes a set of elements selected for the voting of a feature f egory for an element f mi is equal to c j , the output value is 1. Otherwise, the output value is 0.
Next, since each feature separately votes on feature projections, contextual information is missing. Thus co-occurrence
That is, terms with a high co-occurrence frequency value and a low category frequency value have higher term weights as the following formula: where fw ( f i , d) denotes a modified term weight assigned to term f categories in which f i and f j co-occur, co ( f i , f j ) is a co-occurrence frequency value for f value among all co-occurrence frequency values. Note that the weight of feature f f instead of f i and every co ( f i , f j ) is calculated in the training phase.

Finally, the voting score of each category c j in a feature f where fw ( f m , d) denotes a modified term weight by the co-occurrence frequency and v 2 the calculated v 2 statistics value of f m in each category. These v a two-way contingency table of a word f m and a category c where A is the number of times f m and c i co-occur, B is the number of times f occurs without f m , D is the number of times neither c i The outline of the TCFP classifier is as follows: 5. Empirical evaluation 5.1. Data sets and experimental settings
To test the proposed method, we used three different kinds of data sets: UseNet newsgroups (20 Newsgroups), web pages (WebKB), and newswire articles (Reuters 21578). For fair evaluation in Newsgroups and WebKB, we employed the five-fold is the same for all classifiers. Therefore, all the results of our experiments are averages of five runs.
The Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discus-dows.misc) and one duplicate meaning category (comp.sys.ibm.pc.hardware). and on a stop word list, the resulting average vocabulary from five training data has 43,249 words (no stemming).
The second data set comes from the WebKB project at CMU ( Craven et al., 2000 ). This data set contains web pages gath-ered from university computer science departments. The pages are divided into seven categories: course, faculty, project, words.
 The Reuters 21578 Distribution 1.0 data set consists of 12,902 articles and 90 topic categories from the Reuters newswire. list and no stemming. The vocabulary from training data has 12,001 words.

About 25% of documents from training data of each data set were selected for a validation set. After all parameter values of our experiments were set from the validation set, we evaluated the proposed method using these parameter values. Pedersen, 1997 ).

As performance measures, we followed the standard definition of recall, precision, and F formance average across categories, we used the micro-averaging method (Yang, 1999 ). Results on Reuters are reported as precision-recall breakeven points, which is a standard information retrieval measure for binary classification ( Joachims, 2001; Yang, 1999 ).

The title words in our experiment are selected from the category names of each data set for fair evaluation; because each egory in the ReadMe file. 5.2. Experimental results
We tested the proposed method the following steps. First, using the validation set of each data set, we observed the per-formance according to the number of keywords and verified our similarity measure technique for assignment of remaining vised learning method, and a supervised learning method. 5.2.1. Observing the performance according to the number of keywords
First of all, the number of keywords is determined to be used in the proposed method. The number of keywords is limited of keywords (from 0 to 20) in each data set. If we use zero keywords, it means that only the title word is used. As shown in Fig. 2 , we obtained the best performance at two keywords in the Newsgroups data set, at five keywords in the
WebKB data set, and at three keywords in the Reuters data set. As a result, we use the number of keywords with the best performance in each data set. Generally, we recommend the number of keywords to be from two to five. 5.2.2. Verifying our similarity measure algorithm for assignment of remaining contexts through comparing with the K-means algorithm
We here verify our similarity measure algorithm for assignment of remaining contexts as mentioned in Section 3.2.3.To verify the efficiency of our similarity measure algorithm, we exploit the standard K -means algorithm that uses the cosine metric as a similarity measure under a vector space model. But the K -means algorithm in this paper has a different point
Fig. 3 shows the performance curve of each algorithm. As shown in Fig. 3 , our similarity measure algorithm shows better
Fig. 3 . 5.2.3. Comparing the proposed method using TCFP to those using other classifiers data with much noisy data such as the machine-labeled data. As shown in Table 2 , the best performance was obtained in the machine-labeled data as training data. The same manner is applied for the other classifiers. 5.2.4. Comparing our method with a clustering technique
In related work, there have been two approaches using unlabeled data in text classification; one approach combines unla-method does not use any labeled data, it cannot be fairly compared with the former approaches. Therefore, the proposed same experimental settings as those in Slonim X  X  experiments and conduct experiments, we verify that the proposed method outperforms the sIB algorithm. In these experiments, the micro-averaging precision is used as performance measure and two according to Slonim X  X  paper as follows:
In revised_NG, the categories of the Newsgroups data set were united with respect to 10 meta-categories: five comp cat-five big meta-categories.
 The revised_Reuters used the 10 most frequent categories in the Reuters 21578 corpus under the ModApte split.
These experiments were conducted as a close test. That is, all the documents were used as test data as well as training method show relative improvement as high as 8.36% in revised NG and 3.73% in revised Reuters over sIB . methods. For semi-supervised learning, Lanquillon X  X  (2000) experimental results are compared to our method because his experimental settings of the WebKB data set are nearly same as ours. He proposed the semi-supervised method based on of an unlabeled set of 2500 pages and a non-overlapping labeled set. According to his experimental results with 20 labeled was 61%. Finally, with 400 labeled training documents, the semi-supervised NB classifier achieved 76%. This classification accuracy is almost similar to our method X  X  performance (75.47%) even though he used 400 labeled training documents.
Moreover, he reported that the performance gain achieved by the semi-supervised learners decreases as the number of la-developers barely benefit from incorporating unlabeled documents through semi-supervised learning if they use more than a certain amount of labeled documents.

For supervised learning, the training data consists of 500 different documents randomly chosen from the appropriate cat-we consider not only the labeling task using a part of unlabeled documents but also the labeling task using the whole of them. As a result, the following table reports performances from two kinds of NB classifiers which are learned from 500 training documents and the whole training documents respectively.

In Table 4 , the results of the proposed method are shown to be higher than those of NB (500) and they are comparable to that of NB(All ). Note that NB(All) learned from the whole labeled training data. 6. Discussion
We here discuss the weakness of the proposed method and propose the hybrid keywords extraction method to overcome this weakness. Then we observe how many human-labeled documents are required to obtain the performance of the pro-posed method in each data set. 6.1. Enhancing the proposed method from choosing keywords by human developers
The main problem of the proposed method is that its performance depends on the quality of the keywords and title words. As shown in Table 2 , we obtained the worst performance in the WebKB data set. In fact, title words and keywords of each category in the WebKB data set also have high frequency in other categories. We think these factors contribute to a comparatively poor performance of the proposed method. If keywords as well as title words are supplied by humans, the proposed method may be able to achieve better performance. However, choosing the proper keywords for each cat-egory is a much difficult task. Moreover, keywords from developers, who have insufficient knowledge about an applica-tion domain, do not guarantee a high degree of performance. In order to overcome this problem, we propose a hybrid method for choosing keywords. That is, a developer obtains 10 candidate keywords from the keyword extraction method and then he/she can choose proper keywords from them. Table 5 shows the results from the hybrid method in three data sets.

Especially, we could achieve significant improvement in the WebKb data set. Thus we find that the hybrid method for choosing keywords is more useful in a domain with confused keywords between categories such as the WebKB data set. 6.2. The effect of learning from small labeled training data to whole labeled training data
We here observe how many human-labeled documents are required to obtain the performance of the proposed method in lected when the number of human-labeled training documents is varied. The horizontal axes indicate the number of human-to one document per category and 12,800 training documents means that we made use of all labeled training documents. The vertical axes indicate the classification performance on the test sets.

Notice that the achieved performances vary across the different data sets and different amounts of labeled data. A reason specific level when the training sets contain some hundred examples per category.

As shown in Fig. 4 , for the Newsgroups data set, the similar performance to that of the proposed method is achieved when using about 3500 labeled training documents: about 600 labeled documents for the WebKB data set and about 5000 labeled some thousand documents for training is still a tedious and time-consuming task. Moreover, the performances from the supervised classifiers with whole labeled training data do not show much difference in comparison with those of the proposed method. 7. Conclusions and future work
This paper has addressed a new unsupervised or semi-supervised text classification method. Though the proposed method uses only title words and unlabeled data, it shows reasonably comparable performance to the supervised Naive
Bayes classifier. Moreover, it outperforms a clustering method, sIB . Labeled data is expensive while unlabeled data is inexpensive and plentiful. Therefore, the proposed method is useful for low-cost text classification. Furthermore, if some text classification tasks require high accuracy, the proposed method can be used as an assistant tool for easily creating training data.

Since the proposed method depends on title words and the number of keywords, we need additional studies for the characteristics of candidate words for title words and the number of input keywords according to different kinds of data set.
 Acknowledgement This paper was supported by Dong-A University Research Fund in 2008.
 Appendix A
We here explain how to calculate the word weight of formula (6) in Section 3.2.3. The weight of a word in formula (6) is a changed in their process of iterations.

Global frequency : Frequent words in total contexts are less informative of context similarity as follows: where maxfreq is the value of the highest frequency in total contexts.
 than in total contexts. The log-likelihood factor captures this tendency as follows: where Pr( w i ) is estimated from the frequency of w i in the total contexts, and Pr( w contexts. 1 is assigned to the words which do not appear in centroid-contexts.

Part of speech : Each part of speech is considered as a weight. The weight (1.0) is assigned to proper noun, common noun, and foreign word, and the weight (0.6) is assigned to verb.
 words in a context as follows: References
