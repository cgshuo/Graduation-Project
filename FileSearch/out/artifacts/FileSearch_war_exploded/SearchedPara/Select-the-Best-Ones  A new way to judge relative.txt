 1. Introduction As the Web contains a huge amount of information, it is common that many thousands of documents are returned in a
Web search for a query. Based upon user experience, relevant documents are never equally desirable. The criteria for ranking such scenarios, more discriminative judgments are in demand for improving both evaluation and ranking.

Previous works ( Borlund, 2000; Burges et al., 2005; Saracevic, 1969 ) extend binary judgments, i.e., irrelevant/rel-evant, to three-grade judgments, e.g., irrelevant/relevant/highly-relevant, and even five-grade judgments that are more discriminative. However, the methods are still absolute. Human assessors learn what the grades mean in advance and assign an appropriate grade to a document, independent of any other documents. We call the corresponding judgments absolute relevance judgments . Graded relevance, rather than binary relevance, allows for finer distinctions among documents; however, it is difficult to define clear and sufficient grades. For example, sometimes, although one document is obviously better than another, they have to be labeled in the same grade, given the limited scale.
However, if too many gradations are specified, it will be unfeasible for assessors to distinguish between some adjacent grades and thus some tricky pairs of documents will be produced when the assessors arbitrarily assign grades to the documents on the boundary.
 Pairwise preference judgments are investigated as a good alternative to absolute judgments ( Joachims, 2002 ; Carterette, ier for an assessor to judge a preference for one document over another than to assign a pre-defined grade to each of them.
Moreover, they propose some measures such as ppref and wpref to evaluate search engines over preference or relative judg-ments. Research on preference judgments is also motivated by learning to rank. The majority of learned retrieval models are the noise associated with difficulty in distinguishing between different levels of relevance may be reduced X  (p. 17).
In this paper, we extend preference judgments to relative relevance judgments . First, we formulate the relative judgment problem formally then we propose a new strategy that is different from pairwise methods; we call it the Select-the-Best-
Ones (SBO) strategy. By conducting user studies, we make some interesting comparisons between our proposed method, a pairwise method, and an absolute method. We find that the relative judgment methods can generate 15 X 30% more pairs for learning than the absolute method. Furthermore, our proposed SBO method outperforms the pairwise method in a few aspects: it increases the agreement among assessors from 95% to 99%; it reduces the labeling time by about half; and it re-duces half of the number of pairs that are discordant to experts X  judgments.

The rest of the paper is organized as follows: Section 2 surveys related work. We reformulate the absolute relevance judg-ment problem and define the relative relevance judgment problem in a formal way in Section 3 . We also introduce two pre-vious representative judgment methods, respectively, as baselines. Section 4 describes our proposed relative judgment method called Select-the-Best-Ones. We conduct a user study to compare the three methods in Section 5 , and evaluate judg-ment quality by comparing the judgments made in the user study with experts X  judgments in Section 6 . We summarize the 2. Related work
We group related work into three parts. First, we review the literature on the concept of relevance in order to clarify the definition of relevance in our work. The main related work on absolute and relative relevance, including judgments, evalu-ation, and applications, is described in the second part. Finally, we briefly introduce the research on disagreement between human assessors. 2.1. The concept of relevance
The concept of relevance is a fundamental issue to the evaluation of information retrieval (IR) systems, because the main son, 1986 ): (1) objective or system-oriented relevance and (2) subjective or user-oriented relevance. The system-oriented approach treats relevance as static and objective, whereas the user-oriented approach considers relevance to be a subjective individualized mental experience ( Swanson, 1986 , pp. 390 X 391). From another perspective, Saracevic (1996, p. 214) distin-guishes between five basic types of relevance. These are: (1) algorithm relevance, which describes the relation between the query and the collection of information objects expressed by the retrieved objects; (2) a topical-like type, associated with aboutness, i.e., an intellectual assessment, usually judged by an assessor, on how an information object corresponds to the topical area required; (3) pertinence or cognitive relevance, which represents the intellectual relationship between intrinsic human information and the information objects as currently interpreted or perceived by the cognitive state of a lying the information need of a user and (5) motivational and affective, which describes the relationship between the inten-tions, goals, and motivations of a user and the information objects. The judgments made by assessors in common IR experiments, such as TREC (Text Retrieval Conference), CLEF (Cross Language Evaluation Forum), and NTCIR (NII Test Collec-tion for IR Systems) , are of a typical-like type and intended to be of an objective nature at the same time. Borlund (2003) relevance in this paper refers to this type of relevance. 2.2. Degrees of relevance
Relevance is a multi-level phenomenon, i.e., some documents are more relevant than others to a query. Relevance judg-ments have traditionally been made on a binary scale: a document is either relevant to a query or not. However, since the early days of IR, researchers have never given up exploring non-binary relevance because the binary relevance seems to be too coarse to express the differences between documents that users can perceive in many scenarios, such as in a Web search.
In this subsection, we mainly describe the research on non-binary relevance. According to whether judging the relevance of a document is independent of other documents or not, we divided the previous work into two groups: absolute relevance and relative relevance. 2.2.1. Absolute relevance
The most popular way to obtain absolute relevance judgments is category rating. The category rating approaches usually pre-define the number of rating categories. Assessors assign a document to a category in terms of how relevant the docu-ment is to a given query. The judging is explicitly independent of other documents. Of course, the number of categories and the presentation of categories can vary from test to test. For instance, Rees and Schultz (1967) employed an 11-point
Saracevic (1969), Borlund (2000) , and TRECs (since 2000). A five-category scale with all categories defined was also em-ployed in recent works on learning to rank, e.g., ( Burges et al., 2005 ).

Some work has tried to figure out the recommendable number of relevance categories to employ. Jacoby and Matell (1971, suggest that seven is the optimal number of categories when only two end categories are labeled. However, Tang, Shaw, and
Vevea recommended a replication of their own study because previous research indicates that  X  X  X here is no single number of stead of pre-defining the number of relevance categories, we propose a new strategy that allows assessors to decide how many categories, between which they perceive difference, are applied for an individual query and the documents for judging. 2.2.2. Relative relevance
Some previous work has studied how to obtain the relevance judgment of a document relative to other documents. Con-tinuous scales, such as magnitude estimation, were investigated by some researchers ( Bruce, 1994; Eisenberg, 1988 ). The magnitude-ratio scale is a procedure developed in the field of psychophysics ( Stevens, 1966 ), in which assessors express judgments as ratios relative to an arbitrary number applied to the first judgment. However, Eisenberg and Barry (1988) found that the magnitude estimation is affected by the order in which documents are presented. Later, Rorvig (1990) pro-posed substituting the usual relevance judgments with  X  X  X reference X  judgments, i.e., judgments that prefer one document to another. He showed some experimental results that confirmed the reliability of this approach. One concern with using pair-wise methods is that they are less efficient than absolute methods because the number of pairs of documents is polynomial in the number of documents. Carterette et al. (2008) propose to reduce the number of pairs for judging by using transitivity of relevance among documents, assuming every non- X  X  X ad X  page would be preferred to any page with  X  X  X ad X  judgments, and estimating whether the sequence of two documents is useful in evaluating engines. In this paper, we dramatically increase the efficiency of relative relevance judgments by using a new strategy, which is not pairwise.

Using relative judgments poses a new challenge for evaluation measures because the frequently used measures for graded judgments, such as NDCG (Normalized Discounted Cumulative Gain) proposed by J X rvelin and Kek X l X inen (2000) , the system ranked i above j . The other measure is called wpref that weights the pairs in ppref . wpref is then the sum of w  X  1 = log 2  X  j  X  1  X  . These measures enable the direct application of relative judgments, including the judgments obtained through our proposed method in this paper, to evaluating search systems.
 Relative judgments become popular when researchers use them to learn retrieval models. Early in 1987, Bollmann and
Wong (1987) developed two linear retrieval models based on an axiomatic approach. Their system learns the preference relation from examples of decisions made by the user. Documents are then retrieved and ranked according to the inferred decision function. Lacking enough judgments for training, minimal research had been carried out in this respect until 2002.
Many researchers apply supervised learning approaches to solve ranking problems in search because huge amounts of click 2002 ). In general, these kinds of approaches take document pairs as training samples and regard ranking as a classification problem. In the training phase, the approaches minimize the number of pairs that are wrongly classified by applying differ-relative judgments are preferred ahead of absolute judgments in the learning to rank applications. That motivates us to pur-sue more reliable relative relevance judgments with finer discriminative power by designing better judgment methods. 2.3. Disagreement between assessors
Regardless of absolute or relative judgments are used, disagreements exist between different human assessors and thus some measures are proposed to quantify how human judgments agree/disagree ( Hoffman, 1965; Mizzaro, 1999 ). The work done by O X  X onnor (1967, 1696) indicates that unclear or ambiguous requests were found to create more disagreement be-tween assessors. A discussion among judges was proposed as a way to resolve these disagreements ( O X  X onnor, 1969 ) and (1968) defined a strong hypothesis (the difference in relevance judgments cannot affect the assessment of retrieval perfor-mance) and a weak hypothesis (the difference in relevance judgments cannot affect the comparison of performances of dif-ferent retrieval methods). Both hypotheses were supported by experimental data in ( Lesk &amp; Salton, 1968 ). However, the strong hypothesis was not supported by ( Voorhees, 1998 ) and the weak one was not supported by ( Harter, 1996 ). Therefore, no consistent conclusion on how disagreement affects the reliability of evaluation has yet been drawn. Nevertheless, we use the agreement between assessors as one measure in comparing different judgment methods because a higher agreement does suggest a more consistent understanding of relevance among assessors by using the judgment method. 3. From absolute to relative relevance judgments
In this section, we reformulate traditional absolute relevance judgments in a formal way and later propose our formula-tion of the relative relevance judgment problem. The difference and relationship between the two methodologies is also dis-cussed. To compare the previous work with ours, we describe two implementations of previous methods: a five-grade absolute relevance judgment tool and a pairwise tool to judge relative relevance. 3.1. Traditional absolute relevance judgments 3.1.1. Problem formulation According to the previous work, we reformulate the problem of absolute relevance judgment as follows:
Given a query q and a document d i in the set D for judging, we assign a rating r with respect to q : irrelevant categories. 3.1.2. Five-grade relevance judgments
In this paper, we implement a five-grade relevance judgment tool as a baseline method. There are three reasons for which we choose five instead of other number of rating categories. First, five grades have been widely adopted by most recent work datasets like TREC data, five grades are more discriminative than three grades in identifying different degrees of relevance in nature. It would be fairer when we compare the absolute method with relative judgment methods in terms of discrimi-native power. Third, although Tang, Shaw, and Vevea (1999) found that the optimal number of categories is seven in their user studies on bibliography search, their experimental environment is different from ours as they used an undefined cat-ences between in words. Therefore, we choose five that have been used in practice and close to the optimal number of undefined categories found by Tang, Shaw, &amp; Vevea.

For each query, a document is manually rated from a  X  X erfect X  match to a  X  X ad X  match corresponding to the ratings from 5 to 1. We provide users with a document on specifications of these five grades with examples and on-site training. Here is a the term; (4) Fair: a correlation between the query and the page can be seen, but the amount of content does not adequately answer the query; (5) Bad: content is too far removed from the query to be useful, or content is out of date.
In the labeling phase, an assessor goes through the documents one by one and gives a rating to each document. A judg-then listed for selection. Below those are the previous/next buttons and a button linked to a review page. The document is better understand it.

If the assessor would like to change the label later, he/she can jump back to the document from a link on a review page label displayed before the URL. The review page is the only way to move onto the next queries. Thus users must pass this page before submitting all judgments for the query. 3.2. Relative relevance judgments 3.2.1. Problem formulation
When considering the result given by judging the relative relevance between documents, we formulate the judgement problem as follows:
Given a query q and a set of documents D for judging, we take the relative relevance between documents as the basis of making a judgment and aim to order the documents in D into a sequence like: where ation that means  X  X ore relevant than X , e.g., the documents in fg ber of discriminative relevance degrees that D has.

Mizzaro (1999) states that such a formulation corresponds to the total order judgments with equality. | D | could be small, e.g., being 20 or 30, for web applications as Sanderson and Zobel suggest that one can consider using the time saved by a collection would be better at assessing new retrieval systems than a collection formed from deep pools, such as TREC (Text Retrieval Conference).

There are some connections between absolute and relative judgments. We can perform absolute judgments upon relative judgments if the assessors assign a relevance grade r i to the set fg we are able to compare relative judgment methods with the absolute judgment method, e.g., in terms of agreement, on the same base. Conversely, absolute relevance judgments can infer a sequence of document sets. For example, if five grades are applied, one possible judged result can be transformed as follows: Documents with the same rating belong to one set; the set with a higher rating is greater than the set with a lower rating.
However, a big difference lies between the sequences of relative judgments and those of absolute judgments. A relevance set in relative judgments is only meaningful for the given query. There is no alignment between fg query B. In contrast, a set in absolute judgments can align between different queries. For example, fg learning to rank, a ranking model is learned from the document pairs in each of which one document is more relevant than the other. For evaluation, we can apply a different methodology, rather than average precision (AP) or discounted cumulative gains (DCG), to measure the distance between two ranking lists, e.g., ppref and wpref ( Carterette et al., 2008 ). 3.2.2. Quick-Sort: a pairwise strategy sorting algorithm with a minimum of information loss in preference judgments. Here, we borrow a classic sorting algorithm named quicksort as an implementation of the pairwise strategy.

The main idea of the Quick-Sort (QS) strategy is described in Algorithm 1 ( Fig. 2 ). Given a document set D , we ran-domly select a page r as a reference. Then we compare this reference r with all the other pages in D . The comparison interface has three parts as Fig. 3 shows: the top document area, the middle functionality area and the bottom docu-ment area. We show two documents for comparison in the top and bottom document area. We name the top document as Page A and the bottom document as Page B. In the functionality area, we ask assessors to decide whether Page A is  X  X etter than X / X  X ame as X / X  X orse than X  Page B with regard to the query shown on the left. When one of the choices is checked, the  X  X ext X  button on the right is enabled and thus the assessors can go to judge the next page and the reference r . As a result of one cycle, we divide the set D into three subsets: B for the better documents, S for the documents with a similar relevance, and W for the worse documents. Now we know all the documents in B are better than those in S , whereas those in S are also better than the ones in W . We will take S as a set in the final ordering. For B and W ,we push them into two lists respectively to wait for applying the Quick-Sort-Judge algorithm again. Taking the set B as an example, we randomly choose one page from the set as the reference page and again judge all the pairs of the other pages in B and the reference. Consequently the set is split into three subsets: B not stop until all documents in a set are the same as the reference in relevance. Finally, we get the relative ordering of the sets, in each of which the documents are similarly relevant to the query. By using this Quick-Sort strategy, the aver-age complexity of comparison is N log  X  N  X  .
 4. Select-the-Best-Ones: a proposed new strategy
In this section, we propose a new strategy called Select-the-Best-Ones, which is different from the pairwise strategy but also adopts the relative relevance philosophy. We carefully design the tool that enables us to implement the strategy. 4.1. Strategy design
Pairwise approaches are criticized for a worse efficiency caused by the increased complexity from N to N log  X  N  X  . As some the Quick-Sort strategy, we find that the ordering done by a human being is actually different from the sorting performed by a machine. When a person has seen the pages, he/she may have some impressions on which documents are better than oth-ers. If the impressions can be recorded at once, it would save the laborious task of browsing some documents many times. However, no way of doing this is provided by the Quick-Sort strategy.

To speed up the judging process, we design a new strategy called Select-the-Best-Ones (SBO). We show an assessor a bunch of documents and ask him/her to select the best ones among them. The completed selection becomes the first subset the number of subsets in the final ordering is m . As assessors browse all the documents and decide which ones are the best, some comparisons are implicitly completed in the assessors X  minds. Therefore, we hypothesize that the strategy will be per-formed faster than the Quick-Sort (QS) strategy.
 strategy assessors can see only two documents at each time. Thus, the comparison is based on local information. 4.2. User interface design
We design a labeling interface for the SBO strategy. As Fig. 4 shows, for the SBO strategy, we use two monitors aligned vertically with a ratio of 16:9. On the primary monitor, we present no more than 20 documents in the selection page, with itor. There is a check-box beside the short description; when checked, the item is highlighted, and thus we can easily dis-
We design an interactive review page for user modification needs. It is used in both the QS and SBO strategies. As Fig. 5 shows, in the main results area, we show the ordered judgments by groups. A group corresponds to a subset in the sequence
URL are enough to remind users what basic information is contained in the document. At the top of the review page, five buttons are provided to adjust the ordering. These are  X  X ove Item Up X ,  X  X ove Item Down X ,  X  X ove Group Up X ,  X  X ove Group
Down X , and  X  X reate a New Group X . For example, if a user finds document A is not as good as others in the first group and should be moved to the third group, he could select document A by clicking on the item and then pushing the  X  X ove Item
Down X  button twice. To enable the comparison between relative relevance judgments and absolute relevance judgments, we also ask users to give one of the five grades for each group at the group title bar. 5. User study
We carried out a user study to validate the effectiveness of our proposed method by comparing it with the two previous methods. Three basic metrics are used in this paper: efficiency of labeling, agreement between subjects, and discriminative power of judgments. A better judgment approach is supposed to win in efficiency (as the improvement of efficiency means saving money spent on labeling for commercial search engines), agreement (as higher agreement somehow indicates less confusion of subjects in judging relevance degree), and discriminative power (as higher discriminative power means finer trustworthy data for comparison, we carefully design the user study so that we can avoid the order influence of three tools that a subject experiences and balance the difference between subjects in the meantime. 5.1. Experiment design
This experiment uses a 3 3 within-group design, in which the same group of subjects used all three tools, i.e., the five-order.

We find a problem in assigning query sets and tools to subjects. If a subject has tried a judgment tool T relevance of documents for a query set Q 1 , it is not appropriate to work on the query set Q rience another judgment tool T 2 . The reason is that most likely he/she memorizes more or less the information on the doc-uments corresponding to Q 1 and thus we cannot compare the tools independently as the order of tools influences the results.
Therefore, in order to minimize possible practice effects and order effects, we hire nine subjects and use a Latin Squares ( Latin Squares ) design to counterbalance measures of the sequence of using tools and the variance of the subjects X  produc-three different group numbers as follows:
The square shows the assignments for subject groups. For example, Group 1 will judge Q they will work on Q 2 using T 3 . Lastly, they will try T ( Q , T 1 ), ( Q 2 , T 3 ), and ( Q 3 , T 2 ). Similarly, the subjects in Group 2 experience three different combinations: ( Q and ( Q 3 , T 3 ), and the subjects in Group 3 experience another three combinations: ( Q such assignments satisfy: (1) each tool has been used to judge all three query sets; (2) each query has been judged by three subjects applying a tool and thus we can investigate the agreement between different subjects; (3) each subject has used every tool and judged every query, but there are no overlapped queries when he/she uses two different tools.
We select 30 queries from real query logs from Live Search and divide them into three sets as shown in Table 1 . We try to query sets. Furthermore, the sampled queries contain both popular queries such as  X  X  X ets X  and long-tail queries such as
Chinese subjects to judge Chinese queries and documents instead. As the language is not an influential variable, we believe the experimental results are also valid for other languages, such as English.

Different from TREC style topics, we do not describe the information needs of a query beforehand. This is a common prac-tice for search engine companies in collecting relevance judgments. Compared to TREC, search engines use a large-scale query set, e.g., 1000 X 10,000, with shallow document judgments, e.g., 30 documents per query, for evaluation. The top re-trieved documents are more relevant to a query than before for most queries. Annotators work more on distinguishing excel-lent answers from generally good answers, rather than distinguishing slightly relevant documents from irrelevant documents. Therefore, without descriptions, annotators can still understand the major meanings of a query by reading a few documents and then perform relevance judgments. Also, when a query has more than one interpretation, commercial search engines do not want to narrow the query using a pre-defined description. Instead, search engine companies ask anno-information on the marine organism coral. However, on the Web in China, coral is also the name of popular shareware. When annotators are judging documents on the shareware, they will become aware of this kind of information and judge these documents as relevant. In relative judgment methods, annotators are also allowed to regard an encyclopedia page on the marine organism coral and the homepage of the shareware coral as similar in relevance degree if they think that the two meanings are both major intents for web users.

Nine subjects, comprising six males and three females, are hired to perform this user study. The age of the subjects ranges from 23 to 26 with an average of 24. All subjects are graduate students who are familiar with Web search. All of them attend for making absolute relevance judgments. Most of the training time is spent on the five-grade absolute relevance judgment tool because the other two judgment tools do not require prior knowledge.

The PCs used in the user studies are Dell desktops (two 2.13 GHz CPUs) facilitated with two LCDs of 1050 1680 pixels in resolution per machine. 5.2. Basic evaluation metrics
We evaluate the three judgment methods from three perspectives. 5.2.1. Efficiency
Efficiency is important for judging large-scale web queries and documents continuously for commercial search engines, especially in the framework of learning to rank where a huge amount of training data is in demand. In previous academic evaluation activities, such as TREC, organizers usually control labeling costs by constructing a small set of 50/100 queries over a static corpus and pooling the top 30 documents returned from each participant for judging. Once the dataset is built, namic web collections and update their indexes once/twice a month. A huge amount of queries, more than a thousand new queries per month, are required to be judged to track the performance changes of search engines. To keep the judgment set lasting heavy workload for human assessors, which costs about a million dollars per year. That is why efficiency does matter be reduced dramatically.

In this paper, time is used as a measure of efficiency. We have done three things to ensure that time is recorded as accu-each action performed by subjects. Second, if a subject decides to take a break, he/she is asked to exit the tool at the end of judging a query. When the subject returns, he/she then resumes from the checkpoint and we avoid counting the length of break taken in the results. Third, we download the documents and related data from the Web in advance and show the local data at runtime; this reduces unpredictable downloading time. Consequently, the time for judging each query, without counting subjects X  break time and downloading time, is finally used to measure efficiency. 5.2.2. Majority Agreement
The Majority Agreement is a simple but intuitive choice to measure the agreement among subjects; it is used frequently in user studies. If a task is done by N subjects, the Majority Agreement is defined as the percentage of instances on which the majority of subjects agree. To avoid a tie, N is in general odd. The Majority Agreement is only valid when the number of choices for each instance is more than two. In our case, the number of choices is five for each document, thus the Majority a document, we count it to the numerator of the Majority Agreement. 5.2.3. Discriminative power
We propose two metrics to indicate what the granularity of judgments is, or discriminative power of judgments. The average number of relevance degrees generated could be a measure of discriminative power. For absolute relevance judg-ments, not all the pre-defined grades must be assigned to documents given a query. For example, there may be no  X  X erfect X  document for an informational query. Thus, we average the real number of grades that are assigned for queries in experi-ments as the average number of degrees. For relative relevance judgments, we average the number of subsets in ordering for queries as the average number of degrees. As Section 3.2.1 defines, m q in the set Q . Thus the average number of degrees is calculated as measure of discriminative power is the number of untied pairs generated from judgments. Given a query, the documents d and d j form an untied pair, for example, d i &gt; d j when r relative judgments. However, they are tied if both are assigned the same rating, i.e., r sure discriminative power because the machine learning based ranking models benefit from more training pairs. Therefore, when the size of documents is the same, the number of untied pairs could be used to estimate the discriminative power of judgments. 5.3. Results
We have 30 queries with 536 URLs judged by three methods: QS, SBO, and ARJ5, respectively. Each query has 17.8 URLs on average. If any two documents for a query are taken as a pair, we have 4605 pairs in total.
 5.3.1. Basic evaluation results In terms of efficiency, the proposed SBO strategy reduces about half of the labeling time compared to the QS strategy, as ative relevance judgments by leveraging a human being X  X  memory. The SBO strategy is slightly faster than absolute relevance judgment. That is an exciting achievement because in general a relative relevance judgment tool has a larger complexity than an absolute relevance judgment tool.

When measuring agreement among subjects, we find that the relative methods do not consistently improve agreement over the absolute method. SBO performs better than ARJ5, whereas, QS does worse in terms of the Majority Agreement (MA).
This indicates that relative methods may not solve the root causes that result in the disagreement of human subjects, e.g., unclear requests as O X  X onnor found (1967, 1969) .
 the time that is spent on each pair by the preference method with the time spent on each document by their baseline meth-methods. In addition, the agreement is measured by pairs in their paper, thus preference is assigned randomly when two pages are tied. We calculate agreement based upon the grade assigned for each document instead. To make three methods comparable, we ask subjects to assign one of the five grades to each group in the review page of the relative judgment methods.

We use two metrics to show the discriminative power of the three methods in Table 2 . It indicates that the relative rel-evance judgment methods significantly improve the discriminative power of the absolute method. On average, SBO gener-ates about 0.7 more degrees than ARJ5, and QS generates even more  X  about 1.3 degrees. The number of untied pairs is correlated to the average number of degrees. As a result, QS is still the champion. It generates about 29% more pairs than
ARJ5. The gap between SBO and ARJ5 is also significant. 5.3.2. Further analysis on discriminative power
We are curious about which grades in absolute relevance judgments are split into more than one subset in relative rel-evance judgments. That means a finer granularity of relevance degree is obtained through relative judgment methods. Thus, we collect statistics on how many subsets in relative judgments each pre-defined grade in absolute judgments corresponds to. Table 3 shows the results. Each cell contains two numbers: one is the average number of subsets in relative judgments corresponding to a relevance grade in absolute judgments; and the other is the percentage of queries that have a finer rel-subsets in relative judgments and 14.44% of queries experience the splitting in some way.
 not. It indicates that the concepts of  X  X erfect X  and  X  X ad X  are more coherent, but the other concepts may cover more than one relevance degree that is distinguishable. Among the three split grades,  X  X ood X  is split most. For example, when using the QS tool, about two subsets in relative relevance judgments are mapped into the  X  X ood X  grade in absolute judgments. This con-firms our hypothesis that the pre-defined relevance grades may ignore some relevance differences that can be perceived by human subjects.
 using QS and SBO and find the cause. In QS, relative relevance is assessed based on local information. Subjects may change their standard on what is better/worse. At the beginning, they judge that document A is better than document B only if the difference is obvious. When the document sets get smaller, subjects still want to give unequal decisions and they judge that is based on global comparisons. As subjects can view all the documents, they are more likely to keep the same standards when judging. Therefore, the differences between subsets in SBO are more consistent than those in QS. 6. Evaluation experiment on judgment quality
A judgment method that achieves the highest agreement or discriminative power is not necessarily the best with regards to judgment quality. There are two possible ways to evaluate judgment quality: (1) we explicitly compare the judgments with experts X  judgments. The quality of experts X  judgments can be trusted because we allow experts to leverage more web services in exploring the queries and documents, to judge partial orders of documents with high confidence, instead ries, by three tools, we can learn ranking models from the three sets of judgments respectively and then implicitly compare the judgments in terms of the effectiveness of performance of the three ranking models. Due to a limited budget, we choose to evaluate the judgments X  quality by comparing them with experts X  judgments. The more consistent they are to the experts X  judgment, the better they are. 6.1. Collecting experts X  judgments
There are some difficulties that may lead to poor quality judgments. First, an assessor may have no background informa-without enough software background, it would be hard to rank the most popular websites for downloading Kaspersky with a good degree of relevance. Second, due to the limitations of the tool, assessors may have no way to express some of their viewpoints. For example, according to the specifications, four download services websites are not rated as high as  X  X erfect X  with regards to Kaspersky, but all are rated above  X  X ood X . Among them, two websites are the most reputable, whereas the
Apple Corporation and products before the documents on a movie named  X  X pple X  or a fairy tale website named  X  X pple X . Some tricky judgments will be generated if assessors have to make total order judgments.

By considering the difficulties in reality, we propose evaluating the quality of judgments by comparing them to relatively ideal orderings created by experts. First, experts are qualified only if they have rich experience in Web search and a back-ground knowledge of queries. They will learn what a query is about by using more than one search engine. Second, experts first judge the documents individually and then discuss them with other experts to finalize agreed ideal orderings. Experts expected to have no bias to any tool. Third, for some ambiguous or multi-topic queries, experts propose to group documents into categories first and order documents within a single category. Finally, we calculate consistency between the judgments obtained by the three methods and the ideal partial ordering.
 In the experiment, five experts, comprising three males and two females, volunteered to perform relatively ideal ordering. The age of the subjects ranged from 24 to 29 with an average of 26. All experts are professionals who are working on some
Web search related projects. There is no overlap between the nine subjects in the former user study and the five experts in this evaluation experiment. On average, it takes about 36 min five experts for each query. It is so costly and time-consum-ing that we cannot undertake larger scale user studies. 6.2. Experimental results We calculate the percentage of concordant, tied, and discordant pairs and show the results in Table 4 . For a pair of pages
P and P b judged as P a &gt; P b by experts, if the relationship between the pair is also judged by assessors as P concordant pair; if P a =P b in assessor X  X  judgments, the pair is regarded as tied; otherwise, i.e., P pair. When we calculate the percentage, the denominator is the number of unequal pairs generated by experts. As there is no agreed judgment among assessors for some pairs, the sum of numbers in each column will not necessarily be one.
In terms of the results shown in Table 4 , we find that the two relative judgment methods can generate more concordant pairs than the absolute method, while the number of discordant pairs is also larger. That is partially because the absolute method has less discriminative power and thus most pages are tied, instead of ordered, as they are assigned the same rating. pared to the QS strategy, with a comparable percentage of concordant pairs. It indicates that our proposed SBO strategy im-proves the pairwise judgment method in terms of quality. In addition, we notice that even the best method achieves less than 35% concordant pairs. Such low percentages show the big gap between experts and usual assessors in performing rel-evance judgments. The assessors can only complete some coarse judgments with limited background, time, and tools. There is still room to further improve the judgment process. 7. Discussion
In this section, we summarize the advantages and limitations of the three judgment methods: the absolute relevance judgment method, the Quick-Sort method, and our proposed new method Select-the-Best-Ones. Thus one would choose the most appropriate judgment method in his/her own situations.

The absolute relevance judgment method is based on assigning a pre-defined grade to a document with the knowledge on what the grades mean. The advantage of this method is that it is fast and easy-to-implement. When a user is well-trained, it is easy for them to perform a reasonable judgment. However, we have to pay more training costs in the absolute method than the relative method. Another disadvantage is that documents with the same grade may be still different in relevance, meaning that we will lose some useful order information that is critical for learning a ranking function and evaluation.
The Quick-Sort method is based on simple comparisons between two documents. Users can easily judge which one is bet-ter without extra cognitive load. Such a method is easy to expand if we would like to judge more documents for a query. The method of showing documents at first sight is also preferred by users. The disadvantage of this method is bad efficiency.
Although the Quick-Sort strategy has reduced the complexity from O  X  N the absolute relevance judgment method. In addition, we observe that when assessors are labeling a query, the standards of how much relevance difference makes unequal pairs may change from the beginning to the end. Of course, this problem can be partially solved by training.

The Select-the-Best-Ones method is based on comparisons among a small group of documents. Its advantages are shown clearly in the user study and the evaluation experiment. First, the method is fast, even faster than the absolute relevance judgment. Second, by using this method, we would be able to collect about 15% more pairs that are useful for learning to rank approaches. Third, the quality of pairs generated by this method is higher than QS. However, the disadvantage is the heavy cognitive load for users. They have to memorize what 20 documents are talking about. As a result, the method is ity judgments for a small size of top documents. 8. Conclusions and future work
In this paper, we propose a new strategy called Select-the-Best-Ones to address the problem of relative relevance judgment. In contrast with the traditional graded/absolute relevance judgment methods that determine the relevance degree of a document independent of other documents, relative relevance judgment methods take the relative relevance of one document to other documents into consideration. Different from the previous preference judgment methods that ask an assessor to judge one pair of documents at a time, our proposed method provides an overview of the documents for judging, and thus an assessor can quickly browse the documents and select a group of documents with the best rel-evance. Through a user study and an evaluation experiment, we find that our proposed SBO method performs signifi-cantly better than the absolute method in terms of agreement and discriminative power. The SBO method dramatically improves the efficiency over the pairwise relative method Quick-Sort strategy and achieves a comparable efficiency to the absolute method. Meanwhile, the SBO method reduces half of the discordant pairs, compared to the
QS method, which means by using the SBO method we can obtain higher-quality relative judgments for evaluation and applications, such as learning to rank.

We are going to investigate relative relevance judgments in three directions for future work. First, we will try to improve the Select-the-Best-Ones tool to scale. With the limitation of screen size, only about twenty snippets can be shown. If the of both space and the assessors X  memory. Here an interesting problem arises: how could we design a new judgment tool to adapt our proposed SBO principle to judge large scale documents? Second, based on the conclusions drawn in this paper, we will try to use the better relative judgment method SBO to collect enough judgments for learning a ranking model. As we point out in Section 6 , it is meaningful to evaluate judgment quality by validating whether a better ranking model can be learnt from the relative relevance judgments than the model learnt from the absolute relevance judgments. Third, we will investigate other factors that influence judgment quality. For example, in the evaluation experiment, we have observed that it is helpful in judging relevance for experts to explore the type of query, such as being ambiguous or not, the category of information need, such as software/entertainment, and the knowledge on the category, such as which websites are the most authoritative for downloading free software. These factors result in a big gap between the quality of experts X  judgments and general assessors X  judgments. Therefore, it is useful to model these factors and further improve the judgment tools by man-aging the relevance assessment process.
 Acknowledgements We are grateful to Dwight Daniels and Matthew Callcut for edits and comments on writing the paper.
 References
