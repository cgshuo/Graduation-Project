 Jun Zhu  X  X  X  jun-zhu@mails.tsinghua.edu.cn Eric P. Xing  X  epxing@cs.cmu.edu Bo Zhang  X  dcszb@mail.tsinghua.edu.cn In recent years, log-linear models based on compos-ite features that explicitly exploit the structural de-pendencies among elements in high-dimensional in-puts (e.g., DNA strings, text sequences, image lat-tices) and structured interpretational outputs (e.g., gene segmentation, natural language parsing, scene de-scription) have gained substantial popularity in learn-ing structured predictions from complex data. Ma-jor instances of such models include the conditional random fields (CRFs) (Lafferty et al., 2001), Markov networks (MNs) (Taskar et al., 2003), and other spe-cialized graphical models (Altun et al., 2003). Adding to the flexibilities and expressive power of such mod-els, different learning paradigms have been explored, such as maximum likelihood estimation (Lafferty et al., 2001), and max-margin learning (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004). For domains with complex feature space, it is often desirable to pursue a  X  X parse X  representation of the model that leaves out irrelevant features. Learning such a sparse model is key to reduce the rick of over-fitting and achieve good generalizability. In likelihood-based estimation, sparse model fitting has been exten-sively studied. A commonly used strategy is to add an L -penalty to the likelihood function, which can also be viewed as a MAP estimation under a Laplace prior. Recent work along this line includes (Lee et al., 2006; Wainwright et al., 2006; Andrew &amp; Gao, 2007). This progress notwithstanding, little progress has been made so far on learning sparse MNs or log-linear models in general based on the max-margin principle, which is arguably a more desirable paradigm for train-ing highly discriminative structured prediction models in a number of application contexts. While sparsity has been pursued in maximum margin learning of cer-tain discriminative models such as SVM that are  X  X n-structured X  (i.e., with a univariate output), by using L -regularization (Bennett &amp; Mangasarian, 1992) or by adding a cardinality constraint (Chan et al., 2007), generalization of these techniques to structured output space turns out to be extremely non-trivial. For exam-ple, although it appears possible to formulate sparse max-margin learning as a convex optimization prob-lem as for SVM, both the primal and dual problems are hard to solve since there is no obvious way to ex-ploit the conditional independence structures within a regularized MN to efficiently deal with the typically exponential number of margin constraints. Another empirical insight as we will show in this paper is that the L 1 -regularized estimation is not so robust. Dis-carding the features that are not completely irrelevant can potentially hurt generalization ability.
 In this paper, we propose a new formalism called Structured Maximum Entropy Discrimination (SMED), which offers a general framework to com-bine Bayesian learning and max-margin learning of log-linear models for structured prediction. SMED is a generalization of the maximum entropy discrimina-tion (Jaakkola et al., 1999) methods originally devel-oped for classification to the broader problem of struc-tured learning. It facilitates posterior inference of a full distribution of feature coefficients (i.e., weights), rather than a point-estimate as in the standard max-margin Markov network (M 3 N) (Taskar et al., 2003), under a user-specified prior distribution of the coeffi-cients and generalized maximum margin constraints. One can use the learned posterior distribution of co-efficients to form a Bayesian max-margin Markov net-work (BM 3 N) that is equivalent to a weighted sum of differentially parameterized M 3 Ns, or one can ob-tain a MAP BM 3 N. We show that, by using a Laplace prior for the feature coefficients, the resulting BM 3 N is effectively a  X  X parse X  max-margin Markov network, which we refer to as a Laplace M 3 N (LapM 3 N). But unlike the L 1 -regularized maximum likelihood estima-tion, where sparsity is due to a hard threshold intro-duced by the Laplace prior (Kaban, 2007), the effect of Laplace prior in LapM 3 N is a biased posterior weight-ing of the parameters. Smaller parameters are shrunk more and thus robust estimation is achieved when the data have irrelevant features. The Bayesian formalism also makes the LapM 3 N less sensitive to regularization constants. Interestingly, a trivial assumption on the prior distribution of the coefficients, i.e., a standard (zero-mean and identity covariance) normal, reduces BM 3 N to the standard M 3 N, as shown in Theorem 3. The paper is structured as follows. The next section reviews the basic structured prediction formalism and sets the stage for our model. Sec. 3 presents the SMED formalism and basic results on BM 3 N. Sec. 4 presents LapM 3 N and a novel learning algorithm. Sec. 5 presents a generalization bound of BM 3 N. Sec. 6 shows empirical results. Sec. 7 concludes this paper. Consider a structured prediction problem such as nat-ural language parsing, image understanding, or DNA decoding. The objective is to learn a predictive func-tion h : X 7 X  Y from a structured input x  X  X (e.g., a sentence or an image) to a structured output y  X  X  (e.g., a sentence parsing or a scene annotation), where Y = Y 1  X  X  X  X  X Y l with Y i = { y 1 ,...,y m a combinatorial space of structured interpretations of multi-facet objects. For example, Y could correspond to the space of all possible instantiations of the part-of-speech (POS) tagging in the parse tree of a sentence, or the space of all possible ways of labeling entities over some segmentation of an image. The prediction y  X  ( y 1 ,...,y l ) is structured because each individual label y i  X  X  i within y must be determined in the con-text of other labels y j 6 = i , rather than independently as in a standard classification problem.
 Let F : X  X Y 7 X  R represent a discriminant function over the input-output pairs from which one can define the predictive function h . A common choice of F is a linear model, which is based on a set of feature functions f k : X  X Y 7 X  R and their weights w k , i.e., F ( x , y ; w ) = w &gt; f ( x , y ). Given F , the prediction function h is typically defined in terms of an optimization problem that maximizes F over the response variable y given input x : Depending on the specific choice of the objective func-tion C ( w ) for estimating the parameter w (e.g., likeli-hood, or margin), incarnations of the general struc-tured prediction formalism described above can be seen in models such as the CRFs (Lafferty et al., 2001), where C ( w ) is the conditional likelihood of the true structured label; and the M 3 N (Taskar et al., 2003), where C ( w ) is the margin between the true label and any other label. Recent advances in structured pre-diction has introduced regularizations of C ( w ) in the CRF context, so that a sparse w can be learned (An-drew &amp; Gao, 2007). To the best of our knowledge, ex-isting max-margin structured prediction methods uti-lize a single discriminant function F (  X  ; w ) defined by the  X  X ptimum X  estimate of w , similar to a prac-tice in Frequentist statistics. In this paper, we pro-pose a Bayesian version of the predictive rule in Eq. (1) so that the prediction function h can be obtained from a posterior mean over multiple (indeed infinitely many) F (  X  ; w ); and we also propose a new for-malism and objective C ( w ) that lead to a Bayesian M 3 N , which subsumes the standard M 3 N as a spe-cial case, and can achieve a posterior shrinkage effect on w that resembles L 1 -regulatiztion. To our knowl-edge, although sparse graphical model learning based on various likelihood-based principles has recently re-ceived substantial attention (Lee et al., 2006; Wain-wright et al., 2006), learning sparse networks based on the maximum margin principle has not yet been suc-cessfully explored. Our proposed method represents an initial foray in this important direction.
 Before dwelling into exposition of the proposed ap-proach, we end this section with a brief recapitulation of the basic M 3 N that motivates this work, and pro-vides a useful baseline that grounds the proposed ap-proach. Under a max-margin framework, given train-ing data D = { X  x i , y i  X  X  N i =1 , we obtain a point estimate of the weight vector w by solving the following max-margin problem P0 (Taskar et al., 2003): s . t .  X  i,  X  y 6 = y i : w &gt;  X  f i ( y )  X   X  ` i ( y )  X   X  where  X  f i ( y ) = f ( x i , y i )  X  f ( x i , y ) and w the  X  X argin X  between the true label y i and a predic-tion y ,  X  ` i ( y ) is a loss function with respect to y  X  is a slack variable that absorbs errors in the train-ing data. Various loss functions have been proposed in the literature (Tsochantaridis et al., 2004). In this pa-per, we adopt the hamming loss used in (Taskar et al., 2003):  X  ` i ( y ) = P | x i | j =1 I ( y j 6 = y i j ), where indicator function that equals to one if the argument is true and zero otherwise. The optimization prob-lem P0 is intractable because the feasible space for w , F 0 = { w : w &gt;  X  f i ( y )  X   X  ` i ( y )  X   X  i ;  X  i,  X  y 6 = y defined by O ( N |Y| ) number of constraints, and Y it-self is exponential to the size of the input x . Exploring sparse dependencies among individual labels y i in y , as reflected in the specific design of the feature func-tions (e.g., based on pair-wise labeling potentials), and convex duality of the objective, efficient algorithms based on cutting-plane (Tsochantaridis et al., 2004) or message-passing (Taskar et al., 2003) have been pro-posed to obtain an approximate optimum solution. As described shortly, these algorithms can be directly em-ployed as subroutines in solving our proposed model. In this paper, we take a Bayesian approach and learn a distribution p ( w ), rather than a point estimate of w , in a max-margin manner. For prediction, we take the average over all the possible models, that is: Now, the open question is how we can devise an ap-propriate objective function over p ( w ), in a similar spirit as the L 2 -norm cost over w in P0, that leads to an optimum estimate of p ( w ). Below, we present a structured maximum entropy discrimination (SMED) framework that facilitates the estimation of a Bayesian M 3 N defined by p ( w ). As we show in the sequel, our Bayesian max-margin learning formalism offers several advantages like the PAC-Bayes generalization guaran-tee and estimation robustness. 3.1. SMED and the Bayesian M 3 N Given a training set D , analogous to the feasible space F 0 for weight vector w in an M 3 N (i.e., problem P0), the feasible subspace F 1 of weight distribution p ( w ) is defined by a set of expected margin constraints: denotes the expectations with respect to p .
 To choose the best distribution p ( w ) from F 1 , the maximum entropy principle suggests that one can con-sider the distribution that minimizes its relative en-tropy with respect to some chosen prior p 0 , as mea-sured by the Kullback-Leibler divergence, KL ( p || p 0 ) =  X  log( p/p 0 )  X  p . To accommodate the discriminative pre-diction problem we concern, instead of minimizing the usual KL, we optimize the generalized entropy (Dud  X  X k et al., 2007; Lebanon &amp; Lafferty, 2001), or a regular-ized KL-divergence, KL ( p ( w ) || p 0 ( w )) + U (  X  ), where U (  X  ) is a closed proper convex function over the slack variables. This leads to the following Structured Max-imum Entropy Discrimination Model: Definition 1 (The Structured Maximum En-tropy Discrimination Model) Given training data a loss function  X  ` x ( y ) , and an ensuing feasible sub-space F 1 (defined above) for parameter distribution p ( w ) , the SMED model that leads to a prediction function of the form of Eq. (2) is defined by the following generalized relative entropy minimization with respect to a parameter prior p 0 ( w ) : The P1 defined above is a variational optimization problem over p ( w ) in a subspace of valid parameter distributions. Since both the KL and the function U in P1 are convex, and the constraints in F 1 are lin-ear, P1 is a convex program, which can be solved via applying the calculus of variations to the Lagrangian to obtain a variational extremum, followed by a dual transformation of P1. Due to space limit, a detailed derivation is given in an extended version of this paper, and below we state the main results as a theorem. Theorem 2 (Solution to SMED) The variational optimization problem P1 underlying the SMED model gives rise to the following optimum distribution of Markov network parameters w : where the Lagrangian multipliers  X  i ( y ) (corresponding to constraints in F 1 ) can be obtained by solving the dual problem of P1: where U ? (  X  ) represents the conjugate of the slack func-tion U (  X  ) , i.e., U ? (  X  ) = sup  X  P i, y  X  i (y)  X  i For a closed proper convex function  X  (  X  ), its conjugate is defined as  X  ? (  X  ) = sup  X  [  X  &gt;  X   X   X  (  X  )]. In problem D1, by convex duality, the log normalizer log Z (  X  ) can be shown to be the conjugate of the KL-divergence. If the slack function is U (  X  ) = C k  X  k = C P i  X  it is easy to show that U ? (  X  ) = I  X  ( P y  X  i ( y )  X  C,  X  i ), where I  X  (  X  ) is a function that equals to zero when its argument holds true and infinity otherwise. Here, the inequality corresponds to the trivial solu-tion  X  = 0, that is, the training data are perfectly separative. Ignoring this inequality does not affect the solution since the special case  X  = 0 is still in-cluded. Thus, the Lagrangian multipliers  X  i ( y ) in the dual problem D1 comply with the set of con-straints that P y  X  i ( y ) = C,  X  i . Another example is U (  X  ) = KL ( p (  X  ) || p 0 (  X  )) by introducing uncertainty on the slack variables (Jaakkola et al., 1999). Some other U functions and their dual functions are studied in (Lebanon &amp; Lafferty, 2001; Dud  X  X k et al., 2007). The optimum parameter distribution p ( w ) defined by Eq. (3), along with the predictive function h 1 ( x ; w ) given by Eq. (2), jointly form what we would like to call a Bayesian M 3 N (BM 3 N). The close connection of BM 3 N and M 3 N is suggested by the striking isomor-phisms of the opt-problem P1, the feasible space F 1 , and the predictive function h 1 underlying an BM 3 N, to their counterparts P0, F 0 , and h 0 , respectively, un-derlying an M 3 N. Indeed, by making a special choice of a parameter prior in Eq. (3), based on the above discussion of conjugate functions in D1, we arrive at a reduction of D1 to an M 3 N optimization problem. The following theorem makes this explicit.
 Theorem 3 (Reduction of BM 3 N to M 3 N) Assuming F ( x , y ; w ) = w &gt; f ( x , y ) , U (  X  ) = P and p 0 ( w ) = N ( w | 0 ,I ) , where I denotes an identity matrix, then the Lagrangian multipliers  X  i ( y ) are obtained by solving the following dual problem: which, when applied to h 1 , lead to a predictive function that is identical to h 0 ( x ; w ) given by Eq. (1). Proof: (sketch) Replacing p 0 ( w ) in Eq. (3) with N ( w | 0 ,I ), we can obtain the following closed-form expression of the Z (  X  ) in p ( w ): As we have stated, the constraints P y  X  i ( y ) = C are due to the conjugate of U (  X  ) = P i  X  i .
 Theorem 3 shows that in the supervised learning set-ting, M 3 N is subsumed by the SMED model, and can be viewed as a special case of a Bayesian M 3 N when the slack function is linear and the parameter prior is a standard normal. As described later, this connec-tion renders many existing techniques for solving the M 3 N directly applicable for solving the BM 3 N. Note that although the distribution p ( w ) in Eq. (3) has the same form as that of Bayesian CRFs (Qi et al., 2005), the underlying principles are fundamentally different. Recent trend in pursuing  X  X parse X  graphical mod-els has led to the emergence of regularized version of CRFs (Andrew &amp; Gao, 2007) and Markov net-works (Lee et al., 2006; Wainwright et al., 2006). Inter-estingly, while such extensions have been successfully implemented by several authors in maximum likeli-hood learning of various sparse graphical models, they have not yet been explored in the context of maxi-mum margin learning. Such a gap is not merely due to a negligence. Indeed, learning a sparse M 3 N can be significantly harder as we discuss below.
 As Theorem 3 reveals, an M 3 N corresponds to a BM 3 N with a standard normal prior for the weight vector w . To encourage a sparse model, when using zero-mean normal prior, the weights of irrelevant features should peak around zero with very small variances. However, the isotropy of the variances in all dimensions in the standard normal prior makes M 3 N infeasible to adjust the variances in different dimensions to fit sparse data. One way to learn a sparse model is to adopt the strat-egy of L 1 -SVM to use L 1 -norm instead of L 2 -norm (a detailed description of this formulation and the duality derivation is available in the extended version of this paper). However, in both the primal and dual of an L -regularized M 3 N, there is no obvious way to exploit the sparse dependencies among variables of the MN in order to efficiently deal with typically exponential number of constraints, which makes direct optimiza-tion or LP-formulation expensive. In this paper, we adopt the SMED framework that directly leads to a Bayesian M 3 N, and employ a Laplace prior for w to learn a Laplace M 3 N. When fitted to training data, the parameter posterior p ( w ) under a Laplace M 3 N has a shrinkage effect on small weights, which is similar to the L 1 -regularizer in an M 3 N. Although exact learning of a Laplace M 3 N is still very hard, we show that it can be efficiently approximated by a variational inference procedure based on existing methods. The Laplace prior is p 0 ( w ) = Q K k =1 and peaked at zero. Thus, it encodes the prior belief that the distribution of w is strongly peaked around zero. Another nice property is that the Laplace den-sity is log-convex, which can be exploited to get convex estimation problems like LASSO (Tibshirani, 1996). 4.1. Variational Learning with Laplace Prior Although in principle we have a closed-form solution of p ( w ) in Theorem 2, the parameters  X  i ( y ) are hard to estimate when using the Laplace prior. As we shall see in Section 4.2, exact integration will lead to a dual function that is difficult to maximize. Thus, we present a variational approximate learning approach.
 Our approach is based on the hierarchical interpre-tation (Figueiredo, 2003) of the Laplace prior, that is, each w k has a zero-mean Gaussian distribution p ( w k |  X  k ) = N ( w k | 0 , X  k ) and the variance  X  exponential hyper-prior density, Let p ( w |  X  ) = Q K k =1 p ( w k |  X  k ), p (  X  |  X  ) = Q archical representation and applying the Jensen X  X  inequality, we get the following upper bound: where q (  X  ) is a variational distribution which is used to approximate p (  X  |  X  ).
 Substituting this upper bound for the KL in P1, we now solve the following problem, This problem can be solved with an iterative minimiza-tion algorithm alternating between p ( w ) and q (  X  ), as outlined in Algorithm 1, and detailed below.
 Algorithm 1 Variational Bayesian Learning Step 1: Keep q (  X  ) fixed, we optimize (4) with respect to p ( w ). Taking the same procedure as in solving P1, we get the posterior distribution p ( w ) as follows, where  X  = P i, y  X  i ( y ) X  f i ( y ), L = P i, y  X  i ( y ) X  ` A = diag(  X  k ), and b = KL ( q (  X  ) || p (  X  |  X  )) is a constant. The posterior mean and variance are  X  w  X  p =  X  w =  X  respectively. The dual parameters  X  are estimated by solving the following dual problem: This dual problem can be directly solved using exist-ing algorithms developed for M 3 N, such as (Taskar et al., 2003; Bartlett et al., 2004). Alternatively, we can solve the following primal problem: It is easy to show that the solution of problem (6) leads to the posterior mean of w under p ( w ). The primal problem can be solved with subgradient (Ratliff et al., 2007) or extragradient (Taskar et al., 2006) methods. Step 2 : Keep p ( w ) fixed, we optimize (4) with respect to q (  X  ). Take the derivative of L with respect to q (  X  ) and set it to zero, then we get q (  X  ) = Q K k =1 Each q (  X  k ) is computed as follows: The normalization factor is R N ( p  X  w 2 k  X  p | 0 , X  exp(  X  1 2  X  X  k ) d  X  k = pectations  X   X   X  1 k  X  q required in calculating  X  A  X  1 calculated as follows, We iterate between the above two steps until conver-gence. Then, we use the posterior distribution p ( w ) to make prediction. For irrelevant features, the variances should converge to zeros and thus lead to a sparse esti-mation. The intuition behind this iterative minimiza-tion algorithm is as follows. First, we use a Gaussian distribution to approximate the Laplace distribution and thus get a QP problem that is analogous to that of M 3 N; then, the second step updates the covariance matrix in the QP problem with an exponential hyper-prior on the variance.
 4.2. Insights To see how the Laplace prior affects the posterior dis-tribution, we do the following calculations. Substitute the hierarchical representation of the Laplace prior into p ( w ) in Theorem 2, and we get: where  X  k = P i, y  X  i ( y )( f k ( x i , y i )  X  f k ( x additional constraint is  X  k,  X  2 k &lt;  X  . Otherwise, the integration is infinity. Using the result (8), we can get: where  X  is a column vector and  X  k = 2  X  k  X   X   X  2 K . An alternative way is using the definition of Z : Z = R p 0 ( w )  X  exp { w &gt;  X   X  L } d w . We can get: Comparing Eqs. (9) and (10), we get  X  w  X  p =  X  , that can lead to the result that in M 3 N (standard normal prior)  X  w  X  p =  X  . Figure 1 shows the posterior means (any dimension) when the priors are standard normal, Laplace with  X  = 4, and Laplace with  X  = 6. We can see that with a Laplace prior, the parameters are shrunk around zero. The larger the  X  value is, the greater the shrinkage effect. For a fixed  X  , the shape of the posterior mean is smoothly nonlinear but no component is explicitly discarded, that is, no weight is set to zero. This is different from the shape of a L 1 -regularized maximum likelihood estimation (Kaban, 2007) where an interval exists around the origin and parameters falling into this interval are set to zeros. Note that if we use the exact integration as in Eq. (8), the dual problem D1 will maximize L  X  P K k =1 log  X   X   X   X  2 Since  X  2 k appears within a logarithm, the optimization problem would be very hard to solve. Thus, we turn to a variational approximation method. The PAC-Bayes bound (Langford et al., 2001) provides a theoretical motivation to learn an averaging model as in P1 which minimizes the KL-divergence and si-multaneously satisfies the discriminative classification constraints. To apply it to our structured learning setting, we assume that the discriminant functions are bounded, that is, F  X  H : X  X Y  X  [  X  c,c ] for all w , where c is a positive constant. Recall that our aver-aging model is h ( x , y ) =  X  F ( x , y ; w )  X  p ( w ) . We define the margin of an example ( x , y ) for such a function h as M ( h, x , y ) = h ( x , y )  X  max y 0 6 = y h ( x , y the model h makes a wrong prediction on ( x , y ) only if M ( h, x , y )  X  0. Let Q be a distribution over X  X Y , and let D be a sample of N examples randomly drawn from Q . We have the following PAC-Bayes theorem. Theorem 4 (PAC-Bayes Bound of BM 3 N) Let p 0 be any continuous probability distribution over H and let  X   X  (0 , 1) . If F  X  H : X  X Y  X  [  X  c,c ] for all w , then with probability at least 1  X   X  over random samples D of Q , for very distribution p over H and for all margin thresholds  X  &gt; 0 : Here, Pr Q ( . ) stands for  X  .  X  Q and Pr D ( . ) stands for the empirical average on D . The proof follows the same structure as the original PAC-Bayes bound proof, with consideration of the margins. Due to space limit, de-tails of the proof are given in the extended paper. In this section, we present some empirical results of LapM 3 N on both synthetic and real data sets. We compare LapM 3 N with M 3 N, CRFs, L 1 -regularized CRFs ( L 1 -CRFs), and L 2 -regularized CRFs ( L 2 CRFs). We use the quasi-Newton method (Andrew &amp; Gao, 2007) to learn L 1 -CRFs. 6.1. Synthetic Data Sets 6.1.1. I.I.D Features The first experiment is conducted on synthetic se-quence data with 100 i.i.d features. We generate three types of data sets with 10, 30, and 50 relevant features. For each setting, we randomly generate 10 linear-chain CRFs with 8 binary labeling states. The feature func-tions include: a real valued state-feature function over a one dimensional input feature and a class label; and 4 (2  X  2) binary transition-feature functions capturing pairwise label dependencies. For each model we gen-erate a data set of 1000 samples. For each sample, we first independently draw the 100 features from a standard normal distribution, and then apply a Gibbs sampler to assign a label sequence with 5000 iterations. For each data set, we randomly draw a part as train-ing data and use the rest for testing. The numbers of training data are 30, 50, 80, 100, and 150. The QP problem is solved with the exponentiated gradient method (Bartlett et al., 2004). In all the following ex-periments, the regularization constant of L 1 -CRFs and L -CRFs is chosen from { 0 . 01 , 0 . 1 , 1 , 4 , 9 , 16 } by a 5-fold cross-validation in training. For LapM 3 N, we use the same method to choose  X  from 20 roughly evenly spaced values between 1 and 268. For each setting, the average over 10 data sets is the final performance. The results are shown in Figure 2. All the results of LapM 3 N are achieved with 3 iterations of the varia-tional learning. Under different settings LapM 3 N con-sistently outperforms M 3 N and performs comparably with L 1 -CRFs. But note that the synthetic data come from simulated CRFs. Both L 1 -CRFs and L 2 -CRFs outperform the un-regularized CRFs. One interesting result is that M 3 N and L 2 -CRFs perform comparably. This is reasonable because as derived by Lebanon and Lafferty (2001) and noted by Globerson et al. (2007) the L 2 -regularized MLE of CRFs has a similar con-vex dual as that of M 3 N. The only difference is the loss they try to optimize. CRFs optimize the log-loss while M 3 N optimizes the hinge-loss. As the number of training data increase, all the algorithms consistently get higher performance. The advantage of LapM 3 N is more obvious when there are fewer relevant features. 6.1.2. Correlated Features In reality, most data sets contain redundancy and the features are usually correlated. So, we evaluate our models on synthetic data sets with correlated features. We take the similar procedure as in generating the data sets with i.i.d features to first generate one linear-chain CRF model. Then, we use the CRF model to generate 10 data sets of which each sample has 30 rele-vant features. The 30 relevant features are partitioned into 10 groups. For the features in each group, we first draw a real-value from a standard normal distribution and then  X  X poil X  the feature with a random Gaussian noise to get 3 correlated features. The noise Gaussian has a zero mean and standard variance 0.05. Here and in all the remaining experiments, we use the sub-gradient method (Ratliff et al., 2007) to solve the QP problem in both M 3 N and LapM 3 N. We use the learn-ing rate and complexity constant that are suggested by the authors, that is,  X  t = 1  X  is a parameter we introduced to adjust  X  t and C . We do K-fold CV on each data set and take the av-erage over the 10 data sets as the final results. Like (Taskar et al., 2003), in each run we choose one part to do training and test on the rest K-1 parts. We vary K from 20, 10, 7, 5, to 4. In other words, we use 50, 100, about 150, 200, and 250 samples during the train-ing. We use the same grid search to choose  X  and  X  respectively. Results are shown in Figure 3. We can get the same conclusions as in the previous results. 6.2. Real-World OCR Data Set The OCR data set is partitioned into 10 subsets for 10-fold CV (Taskar et al., 2003; Ratliff et al., 2007). We randomly select N samples from each fold for our ex-periments. We vary N from 100, 150, 200, to 250, and denote the selected data sets by OCR100, OCR150, OCR200, and OCR250 respectively. When  X  = 4 on OCR100 and OCR150,  X  = 2 on OCR200 and OCR250, and  X  = 36, results are shown in Figure 4. Overall, as the number of training data increases, all algorithms achieve lower error rates and smaller vari-ances. Generally, LapM 3 N consistently outperforms all the other models. M 3 N outperforms the standard, non-regularized, CRFs and the L 1 -CRFs. Again, L 2 -CRFs perform comparably to M 3 N. This is a bit sur-prising but still reasonable due to the understanding of their only difference on loss functions (Globerson et al., 2007). By examining the prediction accuracy, we can see an obvious over-fitting in CRFs and L 1 CRFs. In contrast, L 2 -CRFs are very robust. This is because unlike the synthetic data sets, features in real-world data are usually not completely irrelevant. In this case, putting small weights to zero as in L CRFs will hurt generalization ability and also lead to instability to regularization constants as shown later. Instead, L 2 -CRFs do not put small weights to zero but shrink them towards zero as in LapM 3 N. The non-regularized MLE can also easily lead to over-fitting. 6.3. Sensitivity to Regularization Constants Figure 5 shows the error rates of different models on OCR100. From the results, we can see that the L 1 CRFs are much sensitive to the regularization con-stants. However, L 2 -CRFs, M 3 N, and LapM 3 N are much less sensitive. Among all the models, LapM 3 N is the most stable one. The stability of LapM 3 N is due to the posterior weighting instead of hard-thresholding to set small weights to zero as in L 1 -CRFs. We proposed a Structured Maximum Entropy Discrim-ination formalism for Bayesian max-margin learning in structured prediction. This formalism gives rise to a general class of Bayesian M 3 Ns and subsumes the standard M 3 N as a spacial case where the predictive model is assumed to be linear and the parameter prior is a standard normal. We show that the adoption of a Laplace prior of the parameter leads to a Laplace M 3 N that enjoys properties expected from a sparsi-fied Bayesian M 3 N. Unlike the L 1 -regularized MLE which sets small weights to zeros to achieve sparsity, LapM 3 N weights the parameters a posteriori . Features with smaller weights are shrunk more. This posterior weighting effect makes LapM 3 N more stable with re-spect to the magnitudes of the regularization coeffi-cients and more generalizable.

