 Continuous state-space Markov Decision Processes (MDPs) a re notoriously difficult to solve. Ex-cept for a few rare cases, including linear Gaussian models w ith quadratic cost, there is no closed-form solution and approximations are required [4]. A large number of methods have been proposed in the literature relying on value function approx imation and policy search; including [3, 10, 14, 16, 18]. In this paper, we follow the policy learni ng approach because of its promise and remarkable success in complex domains; see for example [13, 15]. Our work is strongly motivated by a recent formulation of stochastic planning and control p roblems as inference problems. This line of work appears to have been initiated in [5], where the autho rs used EM as an alternative to standard stochastic gradient algorithms to maximize an expected cos t. In [2], a planning problem under un-certainty was solved using a Viterbi algorithm. This was lat er extended in [21]. In these works, the number of time steps to reach the goal was fixed and the plans we re not optimal in expected reward. An important step toward surmounting these limitations was taken in [20, 19]. In these works, the standard discounted reward control problem was expressed i n terms of an infinite mixture of MDPs. To make the problem tractable, the authors proposed to trunc ate the infinite horizon time. Here, we make the observation that, in this probabilistic in terpretation of stochastic control, the objective function can be written as the expectation of a pos itive function with respect to a trans-dimensional probability distribution, i.e. a probability distribution defined on a union of subspaces of different dimensions. By reinterpreting this function a s a (artificial) marginal likelihood, it is easy to see that it can also be maximized using an EM-type algo rithm in the spirit of [5]. However, the observation that we are dealing with a trans-dimensiona l distribution enables us to go beyond EM. We believe it creates many opportunities for exploiting a large body of sophisticated inference algorithms in the decision-making context.
 In this paper, we propose a full Bayesian policy search alter native to the EM algorithm. In this distribution which is proportional to the prior times the ex pected reward. In the simpler context of myopic Bayesian experimental design, a similar method wa s developed in [11] and applied suc-cessfully to high-dimensional problems [12]. Our method ca n be interpreted as a trans-dimensional dimensional MCMC algorithm, which only involves a simple mo dification of the MCMC algorithm developed to implement the EM.
 Although the Bayesian policy search approach can benefit fro m gradient information, it does not require gradients. Moreover, since the target is proportio nal to the expected reward, the simulation is guided to areas of high reward automatically. In the fixed p olicy case, the value function is often computed using importance sampling. In this context, our al gorithm could be reinterpreted as an MCMC algorithm sampling from the optimal importance distri bution. We consider the following class of discrete-time Markov dec ision processes (MDPs): X X  valued state process, { A n } is the A X  valued action process, { R n } is a positive real-valued re-If we have a deterministic policy then  X   X  ( a | x ) =  X   X  f (  X | x ) assumes the parametrization f  X  (  X | x ) . The reward model could also be parameterized as g (  X | x ) . It should be noted that for this work we will be working withi n a model-based framework and as a result will require knowledge of the transition mode l (although it could be learned). We are here interested in maximizing with respect to the para meters of the policy  X  the expected future reward defined in (1). As shown in [20], it is possible to re-write thi s objective of optimizing an infinite horizon discounted reward MDP (where the reward happens at e ach step) as one of optimizing an infinite mixture of finite horizon MDPs (where the reward only happens at the last time step). In particular, we note that by introducing the trans-dimens ional probability distribution on X k  X A k  X  R + given by we can easily rewrite V  X   X  (  X  ) as an infinite mixture model of finite horizon MDPs, with the re ward happening at the last horizon step; namely at k . Specifically we have: for a randomized policy. Similarly, for a deterministic pol icy, the representation (3) also holds for the trans-dimensional probability distribution defined on The representation (3) was also used in [6] to compute the val ue function through MCMC for a applied to this problem, proceeds as follows at iteration i where Unlike [20], we are interested in problems with potentially nonlinear and non-Gaussian properties. In these situations, the Q function cannot be calculated exactly. The standard Monte C arlo EM sequently be drawn in regions of high reward. This is a particularly interesting feature in situations which is often the case in high-dimensional control setting s. Note that if we wanted to estimate timal zero-variance importance distribution.
 in [20] to derive forward-backward algorithms to implement the E-step which can be implemented here using Sequential Monte Carlo (SMC) techniques. We have in fact done this using the smoothing algorithms proposed in [9]. However, we will focus the discu ssion on a different MCMC approach based on trans-dimensional simulation. As shown in the expe riments, the latter does considerably better.
 Finally, we remark that for a deterministic policy, we can in troduce the trans-dimensional distribu-tion: In addition, and for ease of presentation only, we focus the d iscussion on deterministic policies and reward functions g  X  ( r n | x n ) =  X  r ( x is straightforward. The EM algorithm is particularly sensitive to initializati on and might get trapped in a severe lo-cal maximum of V  X   X  (  X  ) . Moreover, in the general state-space setting that we are co nsidering, the particle smoothers in the E-step can be very expensive compu tationally.
 To address these concerns, we propose an alternative full Ba yesian approach. In the simpler context of experimental design, this approach was successfully dev eloped in [11], [12]. The idea consists new artificial probability distribution defined on  X   X  By construction, this target distribution admits the follo wing marginal in  X  and we can select an improper prior distribution p (  X  )  X  1 if If we could sample from p (  X  ) , then the generated samples  X  ( i ) would concentrate themselves in regions where V  X   X  (  X  ) is large. We cannot sample from p (  X  ) directly but we can developed a trans-dimensional MCMC algorithm which will generate asym ptotically samples from p (  X , k, x 1: k ) , hence samples from p (  X  ) .
 Our algorithm proceeds as follows. Assume the current state of the Markov chain targeting using a combination of birth, death and update moves using th e reversible jump MCMC algorithm [7, 8, 17]. Then we propose to update  X  conditional upon the current value of ( k, x 1: k ) . This can be achieved using a simple Metropolis-Hastings algorithm o r a more sophisticated dynamic Monte Carlo schemes. For example, if gradient information is avai lable, one could adopt Langevin diffu-sions and the hybrid Monte Carlo algorithm [1]. The overall a lgorithm is depicted in Figure 1. The details of the reversible jump algorithm are presented in th e following section. We note that for a given  X  the samples of the states and horizon generated by this Marko v chain us to side-step the need for expensive smoothing algorithms in the E-step. The trans-dimensional simulation approach has the advantage that the samples will concentrate themselves automatically in regions where e p  X  ( k ) has high probability masses. Moreover, unlike in the EM fram ework, it is no longer necessary to truncate the time domain. We present a simple reversible jump method composed of two re versible moves (birth and death) and several update moves. Assume the current state of the Mar kov chain targeting e p  X  ( k, x 1: k ) is ( k, x 1: k ) . With probability 1 b k , we propose a birth move; that is we sample a location uniformly in the interval { 1 , ..., k + 1 } , i.e. J  X  U{ 1 , ..., k + 1 } , and propose the candidate A birth = min { 1 ,  X  birth } where we have for j  X  X  2 , ..., k  X  1 } for j = 1 and j = k + 1 With probability d k , we propose a death move; that is J  X  X { 1 , ..., k } and we propose the candidate j  X  X  2 , ..., k  X  1 } for j = 1 and for j = k The  X  birth and  X  death terms derived above can be thought of as ratios between the di stribution over the newly proposed state of the chain ( i.e. after the birth/death) and the current state. These terms must also ensure reversibility and the dimension-matching requirement for reversible jump MCMC. For more information see [7, 8].
 Finally with probability u k = 1  X  b k  X  d k , we propose a standard (fixed dimensional) move where we update all or a subset of the components x 1: k using say Metropolis-Hastings or Gibbs moves. There are many design possibilities for these moves. In gene ral, one should block some of the variables so as to improve the mixing time of the Markov chain . If one adopts a simple one-at-a candidate is accepted with probability A update = min { 1 ,  X  update } where for j  X  X  2 , ..., k  X  1 } for j = 1 and for j = k kernel will be irreducible and aperiodic and hence will gene rate asymptotically samples from the target distribution e p  X  ( k, x 1: k ) .
 ing the reversible moves accepted will be reasonable. Stand ard Bayesian applications of reversible jump MCMC usually do not enjoy this property and it makes it mo re difficult to design fast mixing algorithms. In this respect, our problem is easier. It should be noted from the outset that the results presented in this paper are preliminary, and serve mainly as an illustration of the Monte Carlo algorithms pres ented earlier. With that note aside, even these simple examples will give us some intuition about the a lgorithms X  performance and behavior. models, but space has not allowed us to present simulations f or this class of models here. We will consider state-and action-spaces X = A = R 2 such that each state x  X  X  is a 2d position and each action a  X  A is a vector corresponding to a change in position. A new state at time n is given by X n = X n  X  1 + A n  X  1 +  X  n  X  1 where  X  n  X  1 denotes zero-mean Gaussian noise. Finally we will let  X  be a normal distribution about the origin, and consider a rew ard (as in [20]) given by illustration of this space can be seen in Figure 2 where m = (1 , 1) .
 For these experiments we chose a simple, stochastic policy p arameterized by  X   X  [0 , 2  X  ] . Under distributed random variables and w is some (small) constant step-length. Intuitively, this po licy cor-responds to choosing a direction  X  in which the agent will walk. While unrealistic from a real-w orld perspective, this allows us a method to easily evaluate and p lot the convergence of our algorithm. For a state-space with initial distribution and reward func tion defined as in Figure 2 the optimal policy corresponds to  X  =  X / 4 .
 We first implemented a simple SMC-based extension of the EM al gorithm described in [20], wherein a particle filter was used for the forwards/backwards filters . The plots in Figure 3 compare the SMC-based and trans-dimensional approaches performing on this synthetic example. Here the in-ferred value of  X  is shown against CPU time, averaged over 5 runs. The first thin g of note is the terrible performance of the SMC-based algorithm X  X n fact we had to make the reward broader and closer to the initial position in order to ensure that the alg orithm converges in a reasonable amount of time. This comes as no surprise considering the O( N 2 k 2 max ) time complexity necessary for com-puting the importance weights. While there do exist methods [9] for reducing this complexity to O( N log N k 2 max ), the discrepancy between this and the reversible jump MCMC method suggests that the MCMC approach may be more adapted to this class of pro blems. In the finite/discrete case updates only using messages from the backwards recursion. T he SMC method might further be im-we used a vague Gaussian centered on the relevant state-spac e. It is however possible that any added benefit from a more informative  X  distribution is counterbalanced by the time required to cal culate this  X  , for example by simulating particles forward in order to find the invariant distribution, etc. Also shown in figure 3 is the performance of a Monte Carlo EM alg orithm using reversible jump MCMC in the E-step. Both this and the fully Bayesian approach perform comparably, although the fully Bayesian approach shows less in-run variance, as well as less variance between runs. The EM algorithm was also more sensitive, and we were forced to incr ease the number of samples N used by the E-step as the algorithm progressed, as well as control ling the learning rate with a smoothing parameter. For higher dimensional and/or larger models it i s not inconceivable that this could have an adverse affect on the algorithms performance.
 Finally, we also compared the proposed Bayesian policy expl oration method to the PEGASUS [14] approach using a local search method. We initially tried usi ng a policy-gradient approach, but because of the very highly-peaked rewards the gradients bec ome very poorly scaled and would have required more tuning. As shown in Figure 4, the Bayesian stra tegy is more efficient in this rare event setting. As the dimension of the state-space increase s, we expect this difference to become even more pronounced. We believe that formulating stochastic control as a trans-d imensional inference problem is fruitful. This formulation relies on minimal assumptions and allows u s to apply modern inference algorithms to solve control problems. We have focused here on Monte Carl o methods and have presented X  to the best of our knowledge X  X he first application of reversi ble jump MCMC to policy search. Our results, on an illustrative example, showed that this tr ans-dimensional MCMC algorithm is more effective that standard policy search methods and alte rnative Monte Carlo methods relying on particle filters. However, this methodology remains to be te sted on high-dimensional problems. For such scenarios, we expect that it will be necessary to develo p more efficient MCMC strategies to explore the policy space efficiently.
 Figure 4: Convergence of PEGASUS and our Bayesian policy search algor ithm when started from  X  = 0
