 Over the past few years, research on neural net-works (NN) architectures for Natural Language Processing has been rejuvenated. Boosted by early successes in language modelling for speech recog-nition (Schwenk, 2007; Le et al., 2011), NNs have since been successufully applied to many other tasks (Socher et al., 2013; Huang et al., 2012; Yang et al., 2013). In particular, these techniques have been applied to Statistical Machine Trans-lation (SMT), first to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014), and more recently to implement end-to-end translation sys-tems (Cho et al., 2014; Sutskever et al., 2014).
In most SMT settings, CTMs are used as an ad-ditional feature function in the log-linear model, and are conventionally trained by maximizing the regularized log-likelihood on some parallel train-ing corpora. Since this objective function requires to normalize scores, several alternative training objectives have recently been proposed to speed up training and inference, a popular and effec-tive choice being the Noise Contrastive Estimation (NCE) introduced in (Gutmann and Hyv  X  arinen, 2010). In any case, NN training is typically per-formed (a) in isolation from the other components of the SMT system and (b) using a criterion that is unrelated to the actual performance of the SMT system (as measured for instance by BLEU). It is therefore likely that the resulting NN parameters are sub-optimal with respect to their intended use.
In this paper, we study an alternative training regime aimed at addressing problems (a) and (b). To this end, we propose a new objective func-tion used to discriminatively train or adapt CTMs, along with a training procedure that enables to take the other components of the system into account. Our starting point is a non-normalized extension of the n -gram CTM of (Le et al., 2012) that we briefly restate in section 2. We then introduce our objective function and the associated optimization procedure in section 3. As will be discussed, our new training criterion is inspired both from max-margin methods (Watanabe et al., 2007) and from pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). This proposal is evaluated in an N -best rescoring step, using the framework of n -gram-based systems, within which they in-tegrate seamlessly. Note, however that it could be used with any phrase-based system. Experi-mental results for two translation tasks (section 4) clearly demonstrate the benefits of using discrimi-native training on top of an NCE-trained model, as it almost doubles the performance improvements of the rescoring step in all settings. The n -gram-based approach in Machine Trans-lation is a variant of the phrase-based ap-proach (Zens et al., 2002). Introduced in (Casacu-berta and Vidal, 2004), and extended in (Mari  X  no et al., 2006; Crego and Mari  X  no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand. 2.1 n -gram-based Machine Translation Let ( s , t ) denote a sentence pair made of a source s and target t sides. This sentence pair is decom-posed into a sequence of L bilingual units called tuples defining a joint segmentation. In this frame-work, tuples constitute the basic translation units: like phrase pairs, they represent a matching be-tween a source and a target chunk . The joint prob-ability of a synchronized and segmented sentence pair can be estimated using the n -gram assump-tion. During training, the segmentation is obtained as a by-product of source reordering, (see (Crego and Mari  X  no, 2006) for details). During the infer-ence step, the SMT decoder will compute and out-put the best derivation in a small set of pre-defined reorderings.

Note that the n -gram translation model manipu-lates bilingual tuples. The underlying set of events is thus much larger than for word-based models, while the training data (parallel corpora) are typ-ically order of magnitude smaller than monolin-gual resources. As a consequence, data sparsity issues for such models are particularly severe. Ef-fective workarounds consist in factorizing the con-ditional probabitily of tuples into terms involv-ing smaller units: the resulting model thus splits bilingual phrases in two sequences of respectively source and target words, synchronised by the tuple segmentation. Such bilingual word-based n -gram models were initially described in (Le et al., 2012). We assume here a similar decomposition. 2.2 Neural Architectures The estimation of n -gram probabilities can be per-formed via multi-layer NN structures, as described in (Bengio et al., 2003; Schwenk, 2007) for a monolingual language model. The standard feed-forward structure is used to estimate the trans-lation models sketched in the previous section. We give here a brief description, more details are in (Le et al., 2012): first, each context word is pro-jected into language dependent continuous spaces, using two projection matrices for the source and target languages. The continuous representations are then concatenated to form the representation of the context, which is used as input for a feed-forward NN predicting a target word.

In such architecture, the size of output vocab-ulary is a bottleneck when normalized distribu-tions are expected. Various workarounds have been proposed, relying for instance on a struc-tured output layer using word-classes (Mnih and Hinton, 2008; Le et al., 2011). A more effective alternative, which however only delivers quasi-normalized scores, is to train the network using the Noise Contrastive Estimation or NCE (Gut-mann and Hyv  X  arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs and has been adopted here. We therefore assume that the NN outputs a positive score b word w given its context c ; this score is simply computed as b a ( w, c ) is the activation at the output layer;  X  de-notes all the network free parameters. In SMT, the primary role of CTMs is to help the system in ranking a set of hypotheses so that the top scoring hypotheses correspond to the best translations, where quality is measured using au-tomatic metrics such as BLEU (Papineni et al., 2002). Given the computational burden of con-tinuous models, the prefered use of CTMs is to rescore a list of N-best hypotheses, a scenario we favor here; note that their integration in a first pass search is also possible (Niehues and Waibel, 2012; Vaswani et al., 2013; Devlin et al., 2014). The im-portant point is to realize that the CTM score will in any case be composed with several scores com-puted by other components: reordering model(s), monolingual language model(s), etc. In this sec-tion, we propose a discriminative training frame-work which implements a tight integration of the CTM with the rest of the system. 3.1 A Discriminative Training Framework The decoder generates a list of N hypotheses for each source sentence s . Each hypothesis h is com-posed of a target sentence t along with its associ-ated derivation and is evaluated as follows:
G where M conventional feature functions 1 f 1 ...f M , estimated during the training phase, are scaled by coefficients  X  1 ... X  M . The introduction of a con-tinuous model during the rescoring step is imple-mented by adding the feature f Algorithm 1 Joint optimization of  X  and  X  1: Init. of  X  and  X  2: for each iteration do 3: for P mini-batch do .  X  is fixed 4: Compute the sub-gradient of L (  X  ) for 5: Update  X  6: end for 7: Update  X  on development set .  X  is fixed 8: end for cumulates, over all contexts c and word w , the CTM log-score log b
G and on the log-linear coefficients  X  . We pro-pose to train these two sets of parameters, by al-ternatively updating  X  through SGD on the train-ing corpus, and updating  X  using conventional al-gorithms on the development data. This proce-dure, which has also been adopted in recent stud-ies (e.g. (He and Deng, 2012; Gao and He, 2013)) is sketched in algorithm 1. In practice, the train-ing data is successively divided into mini-batches of 128 sentences. Each mini-batch is used to com-pute the sub-gradient of the training criterion (see section 3.2) and to update  X  . After each training iteration of the CTM,  X  s are retuned on the de-velopment set; we use here the K-Best Mira algo-rithm of Cherry and Foster (2012) as implemented 3.2 Loss function The training criterion considered here draws in-spiration both from max-margin methods (Watan-abe et al., 2007) and from the pair-wise ranking (PRO) (Hopkins and May, 2011; Simianer et al., 2012). The choice of a ranking loss seems to be the most appropriate in our setting; as in many recent studies on discriminative training for MT (e.g. (Chiang, 2012; Flanigan et al., 2013)), the integration of the translation metric into the loss function is critical to obtain parameters that will yield good translation performance.

Translation hypotheses h i are scored using a sentence-level approximation of BLEU denoted SBLEU ( h i ) . Let r i be the rank of hypothesis h i when hypotheses are sorted according to their sentence-level BLEU. Critical hypotheses are de-A pair of hypotheses is thus deemed critical when a large difference in SBLEU is not reflected by the difference of scores, which falls below a threshold. This threshold is defined by the differ-ence between their sentence-level BLEU, multi-plied by  X  . Our loss function L (  X  ) is defined with
Initialization is an important issue when opti-mizing NN. Moreover, our training procedure de-pends heavily on the log-linear coefficients  X  . To initialize  X  , preliminary experiments (Do et al., 2014; Do et al., 2015) show that it is more effi-cient to start from a NN pre-trained using NCE, while the discriminative loss is used only in a fine-tuning phase. Given the pre-trained CTM X  X  scores, we initialize  X  by optimizing it on the develop-ment set. This strategy forces the training of  X  to focus on errors made by the system as a whole. 4.1 Tasks and Corpora The discriminative optimization framework is evaluated both in a training and in an adaptation scenario. In the training scenario, the CTM is trained on the same parallel data as the one used for the baseline system. In the adaptation sce-nario, large out-of-domain corpora are used to train the baseline SMT system, while the CTM is trained on a much smaller, in-domain corpus and only serves for rescoring. An intermediate situa-tion ( partial training ) is when only a fraction of the training data is re-used to estimate the CTM: this situation is interesting because it allows us to train the CTM much faster than in the training sce-Two domains are investigated. For the contains 180 K sentence pairs; the out-of-domain
Table 1: BLEU scores for the TED Talkstasks. data is much larger and contains all corpora al-lowed in the translation shared task of WMT X 14 (English-French), amounting to 12 M parallel sen-tences. The second task is the medical transla-which we use all authorized corpora. The Patent-Abstract corpus, made of 200 K parallel sentence pairs, is used either for adaptation or partial train-ing for the CTM. Experimental results are re-ported on official evaluation sets, as well as on the CTM training set.

All translation systems are based on the open source implementation 8 of the bilingual n -gram approach to MT. For the NN structure, each vo-cabulary X  X  word is projected into a 500 -dimension space followed by two hidden layers of 1000 and 500 units. For the discriminative training and adaptation tasks, baseline SMT systems are used to generate respectively 600 and 300 best hypothe-4.2 Experimental results Results in Table 1 measure the impact of discrim-inative training on top of an NCE-trained model for the two TED Talks conditions. In the adapta-tion task, the discriminative training of the CTM gives a large improvement of 0 . 9 BLEU score over the CTM only trained with NCE and 1 . 9 over the baseline system. However, for the training sce-nario, these gains are reduced respectively to 0 . 4 and 1 . 2 BLEU points. The BLEU scores (in the train column) measured on the N -best lists used to train the CTM provide an explanation for this difference: in training, the N -best lists contain hy-potheses with an overoptimistic BLEU score, to be compared with the ones observed on unseen data. As a result, adding the CTM significantly
Table 2: BLEU scores for the medical tasks. worsens the performance on the discriminative training data, contrarily to what is observed on the development and test sets. Even if the results of these two conditions cannot be directly compared (the baselines are different), it seems that the pro-posed discriminative training has a greater impact on performance in the adaptation scenario, even though the out-of-domain system initially yields lower BLEU scores.

The medical translation task represents a dif-ferent situation, in which a large-scale system is built from multiples but domain-related corpora, among which, one is used to train the CTM. Nev-ertheless, results reported in Table 2 exhibit a sim-ilar trend. For both conditions, the discrimina-tive training gives a significant improvement, up to 0 . 7 BLEU score over the one only trained with NCE and up to 1 . 7 over the baseline system. Ar-guably, the difference between the two conditions is much smaller than what was observed with the TED Talks task, due to the fact that the Patent-Abstract corpus used to discriminatively train the CTM only corresponds to a small subset of the parallel data. However, the best strategy seems, here again, to exclude the data used for the CTM from the data used to train the baseline system. It is important to notice that similar discrimina-tive methods have been used to train phrase table X  X  scores (He and Deng, 2012; Gao and He, 2013; Gao et al., 2014), or a recurrent NNLM (Auli and Gao, 2014). In recent studies, the authors tend to limit the number of iterations to 1 (Gao et al., 2014; Auli and Gao, 2014), while we still advocate the general iterative procedure sketched in Algo. 1. Initialization is also an important is-sue when optimizing NN. In this work, we ini-tialize CTM X  X  parameters by using a pre-training procedure based on the model X  X  probabilistic in-terpretation and NCE algorithm to produce quasi-normalized scores , while similar work in (Auli and Gao, 2014) only uses un-normalized scores . The initial values of  X  also needs some investigation. Gao et al. (2014) and Auli and Gao (2014) ini-tialize  X  M +1 to 1 , and normalize all other coef-ficients; here we initialize  X  by optimizing it on the development set using the pre-trained CTM X  X  scores. This strategy forces the training of  X  to focus on errors made by the system as a whole. The fundamental difference of this work hence lays in the use of the ranking loss described in Section 3.2, whereas previous works use expected BLEU loss. We plan a systematic comparison be-tween these two criteria, along with some other discriminative losses in a future work.

About the CTM X  X  structure, our used model is based on the feed-forward CTM described in (Le et al., 2012) and extended in (Devlin et al., 2014). This structure, though simple, have been shown to achieve impressive results, and with which effi-cient tricks are available to speed up both train-ing and inference. While models in (Le et al., 2012) employ a structured output layer to reduce softmax operation X  X  cost, we prefer the NCE self-normalized output which is very efficient both in training and inference. Another form of self-normalization is presented in (Devlin et al., 2014) but does not seem to have fast training. Finally, although N -best rescoring is used in this work to facilitate the discriminative training, other CTM X  X  integration into SMT systems exist, such as lat-tice reranking (Auli et al., 2013) or direct decod-ing with CTM (Niehues and Waibel, 2012; Devlin et al., 2014; Auli and Gao, 2014). In this paper, we have proposed a new discrimina-tive training procedure for continuous-space trans-lation models, which correlates better with trans-lation quality than conventional training meth-ods. This procedure has been validated using an n -gram-based CTM, but the general idea could be applied to other continuous models which com-pute a score for each translation hypothesis. The core of the method lays in the definition of a new objective function inspired both from max-margin and Pairwise Ranking approach in MT, which en-ables us to effectively integrate the CTM into the SMT system through N -best rescoring. A major difference with most past efforts along these lines is the joint training of the CTM and the log-linear parameters. In all our experiments, discriminative training, when applied on a CTM initially trained with NCE, yields substantial performance gains. This work has been partly funded by the European Union X  X  Horizon 2020 research and innovation programme under grant agreement No. 645452 (QT21).

