 Personalization is a long-standing problem in data mining and ma-chine learning. Companies make personalized product recommen-dations to millions of users every second. In addition to the recom-mendation problem, with the emerging of personal devices, many conventional problems, e.g., recognition, need to be personalized as well. Moreover, as the number of users grows huge, solving per-sonalization becomes quite challenging. In this paper, we formalize the generic personalization problem as an optimization problem. We propose several ADMM algorithms to solve this problem in a distributed way including a new Asynchronous ADMM that re-moves all synchronous barriers to maximize the training through-put. We provide a mathematical analysis to show that the proposed Asynchronous ADMM algorithm holds a linear convergence rate which is the best to our knowledge. The distributed personaliza-tion allows training to be performed in either a cluster or even on a user X  X  device. This can improve the privacy protection as no per-sonal data is uploaded, while personal models can still be shared with each other. We apply this approach to two industry problems, Facial Expression Recognition and Job Recommendation . Experi-ments demonstrate more than 30% relative error reduction on both problems. Asynchronous ADMM allows faster training for prob-lems with millions of users since it eliminates all network I/O wait-ing time to maximize the cluster CPU throughput. Experiments demonstrate 4 times faster than original synchronous ADMM al-gorithm.
 H.2.8 [ Database Application ]: Data mining; H.3.4 [ Systems and Software ]: Distributed systems Big Data, Distributed Optimization, Personalization Distributed computing, User modeling, ADMMs
Personalization is a key component in many applications such as advertising [9, 5, 21], recommendation [17, 18], and person-alized medicine [23], where an impression, a product and a type of drug is predicted according to the user X  X  profile. Sparse Ma-trix Factorization approaches [18, 7, 19] that seek a shared low-dimensional representation for both users and items have inspired significant amount of research. Both theoretical work and empiri-cal work demonstrated that with the low rank assumption, i.e., very few dominating factors for individual users, these approaches have achieved state-of-the-arts accuracies on many datasets [7]. How-ever, they are restricted by the matrix formulation, and less flex-ible to incorporate contextual information, therefore, not general enough to address the personalization aspect in broader machine learning problems. For example, in movie recommendation, ex-tra information about movies like the directors and the budgets are not encoded in the rating matrix. In this paper, we address the personalization differently. Each individual is assigned one model with a regularization term to penalize the difference between indi-vidual models and a global consensus model. The regularization term allows individual experiences to be shared among the popula-tion, while allowing the models to be different from each other. A parameter in the regularization term controls how much of a differ-ence these models are. This formulation has been proposed under Multi-task Learning problem [15, 3] where each individual model is a specific task. These tasks are related to each other, so solving one task provides information to solve others. Our proposed algo-rithm can be applied to these problem naturally, especially for tasks distributed among a network, e.g., remote sensing and multi-agent planning.

Solving this personalization problem in a distributed environ-ment is quite challenging, especially for data from large companies like LinkedIn and Microsoft. In this paper, we study algorithms based on alternating direction method of multipliers (ADMM) [6]. We propose several variants of the original ADMM algorithm in-cluding a novel Asynchronous ADMM algorithm that frees up the waiting barriers to speed optimization. As the number of worker nodes grows, the processing speed, computation power, network delay usually vary among different worker nodes. Iterating until all nodes finish is expensive as nodes have to wait for the slowest worker node before the next iteration starts. Asynchronous training addresses the problem and achieves better optimization throughput. We provide a mathematical analysis to prove the proposed Asyn-chronous ADMM converges at a comparable rate as the original Synchronus ADMM .

We apply our personalization model to solve two industry prob-lems, facial expression recognition and job recommendation . The experimental results demonstrate a great gain in accuracy, and the proposed Asynchronous ADMM speeds up the training process dramatically.

In summary, our proposed approach enables systematical train-ing of personalized models jointly in a distributed manner. To our knowledge, this is the first solution of large scale personalization problems via asynchronous distributed machine learning. In addi-tion, the proposed asynchronous ADMM is the first algorithm that demonstrates linear convergence both in theory and empirically.
Before detailing the distributed algorithm, we first formulate our personalization framework as an optimization problem. Equation 1 provides a general formalization where f i ( w i ) is a arbitrary ma-chine learning task for the i -th user, w i is the personal model for that user, and z is the consensus model. Equation 2 shows an ex-ample f i ( w i ) where l is a convex loss function, x ij feature vector, and y ij is the target to predict.  X  is a regularizatino term inversely coupling the individual and consensus models. With only a handful of samples per model, we need a relatively large  X  to prevent overfitting.
We focus on distributed personalization of classification tasks and as such assume that the number of users is proportional to the total number of samples, and finally, that each user X  X  data is drawn i.i.d. from the distribution D i . These are captured in Assumption 1 where N i is the number of training samples for user i , N is the total number of training samples, and M is the total number of users.
A SSUMPTION 1. We make the following assumptions: 1. Let N = P M i N i where M = O ( N ) . 2. For each user i  X  [1 ...M ] , { x ij ,y ij } X  D i . 3. Network latency is i.i.d. with mean  X  .

Applying distributed stochastic optimization approaches [1, 10, 2, 12] to this distributed personalization problem is non-trivial. Al-though the consensus model can be obtained, there aren X  X  enough samples to reliably estimate individual user models. In addition, tracking the learning rate for each user is difficult.

We experiment with a synchronous ADMM algorithm [6, 25] from the literature, a novel versioned adaptation, and propose a faster asynchronous algorithm. A key observation is that alternately optimizing w i and z converges slowly  X  a low sample count per user drives a large  X  large coupling the personal and consensus models which, in turn, leads to histeresis.
We first derive a synchronous ADMM to solve Equation 1. By introducing local consensus variables q i , we rewrite the problem as in Equation 3. Turning it into the augmented Lagrangian, we have Equation 4. L ( z , { q i , w i ,  X  i } ) = An synchronous ADMM solution can be obtained by Equations [5,6,7]. In Equation 5, q i has analytical relation to w i , we obtain the up-dating rules in Equation [9,10,11,8], where  X  =  X  / X  .  X  i, w t +1 i = arg min From Equation 9, even when  X  is huge, the regularization is up-per bounded by  X  2 that avoids obtaining a w over-influenced by z , and leads to faster convergence. Note that we update  X  i update z at the end of the iteration for a simpler derivation.
In synchronous ADMM, Equations [8,9,10] are performed in each worker node, while Equation 11 is performed in the master node. The master node waits for all sub-problems to be solved and their local models submitted, before merging and broadcast-ing a new consensus model, see Figure 1. Waiting for all nodes to finish slows down the distributed computation process, and each user benefits from the latest consensus less frequently. The asyn-chronous ADMM essentially unblocks the master and allows the consensus model to be updated freely. Therefore, the aggregated knowledge can be shared between individuals more frequently.
Different users have different numbers of samples to optimize, and different workers have different hardware latency. The submit-ted local models q i , and dual variables  X  i come to the master out of order. It is easy to see that z can be updated whenever the master is ready as long as it is broadcast at the end of the iteration. On the other hand, we can broadcast z to the workers at any time, and force the remaining workers to restart the sub-problem based on the new consensus z , i.e., at any moment, there is only one version of consensus. Theorem 2 proves that this Single-Versioned ADMM converges at the same rate as the original Synchronous ADMM. The convergence analysis for Synchronous ADMM can be found in literature [[16],[22]]. Figure 1: Synchronous ADMM updating schedule. Although all personal models proceed in parallel, the master has to wait for all updates come to merge. This induces a great overhead. Figure 2: A Single-Versioned ADMM updating schedule. The master updates the consensus periodically to allow faster shar-ing, but it adds additional barriers that potentially stalls the process more.

T HEOREM 2. Let  X  i = ( w i , q i ,  X  i ) , Lagrangian in Equa-tion 4 can be rewritten as L ( {  X  i } , z ) . Assuming  X  i , f vex function, at any point, update z t +1 k = arg min z L ( {  X  {  X  i } , z ) , then any optimization sequence, i.e.,  X  tion.

P ROOF . For any iteration t , Lagrangian L t ( {  X  t i intermediate consensus updates, we can verify that On the other hand, on expectation, an optimization step w is equivalent to  X  t i , w t i , q t i , i.e., Combining these two, the optimization sequence for the Single-Versioned ADMM is dominated by the sequence of the original Synchronous ADMM . If f i is convex, both sequences arrive at a saddle point.

If the master and individual workers optimize independently, it is possibly that users X  personal models branch out on different ver-sions of the consensus, and request to merge at different pace. These delayed updates do not necessarily dominate the synchronous ADMM optimization. However, we can show that they dominate an optimization sequence for a multiple block ADMM formula-tion [13]. The technique is to create the auxiliary variables for these different versions of consensus models. Assuming for the k -th merge, we have a set of users B k  X  [1 ...M ] submitted local Figure 3: A Multi-Versioned ADMM updating schedule. Simi-lar to Single-Versioned ADMM, this allows the consensus peri-odically updated, but no additional barriers are added. updates. For each one of them j  X  B k , the sub-problem is solved by using the consensus model  X  z h ( j ) , where h ( j )  X  [1 ...k  X  1] is the mapping from the user to the consensus model version. We first update  X  z h ( j ) for all j  X  B k , then update z The consensus updating rules for this multi-versioned ADMM are defined in Equations 12 and 13.
R EMARK 3. When  X  j,k,h ( j ) = k  X  1 , multi-versioned ADMM reduces to single-verioned ADMM .

To prove that multi-versioned ADMM converges, we first show that a Multi-Block ADMM problem updates using rules in Equa-tions 12 and 13. Consider the optimization problem in Equation 14, where K is the total number of consensus updates. In this problem, we have multiple blocks, { q i , w i } ,  X  z k and z . According to [8], ex-tending the ADMM algorithm directly by alternating optimizations over these blocks achieves the same linear convergence when the equality constraints satisfy the condition in Theorem 4.
T HEOREM 4. [8] Assuming a constraint optimization has con-straint A 1 x 1 + A 2 x 2 + A 3 x 3 = 0 , if A T 1 A 2 = 0 or A direct extension of ADMM converges in O ( 1 T ) .

In our case, it is easy to see that A k for  X  z k is canceling each other, and therefore A k = 0 . The dual variables for constraints  X  z cancel out, so Equation [12,13] are equivalent to the ADMM up-dating rules for  X  z k and z . Therefore, for any such consensus up-dating schedules, we can construct a Multi-Block ADMM program that updates correspondingly. The only difference is that Multi-Versioned ADMM also updates z immediately after  X  z is updated. Similar to Theorem 2, optimizing z multiple times within the same iteration dominates the original optimization sequence, so we con-clude convergence as stated in Theorem 5. Figure 4: Asynchronous ADMM updating schedule. This algo-rithm removes the iteration barriers and free up the consensus updates completely.

T HEOREM 5. For any optimization sequence that follows Equa-verges at rate O ( 1 T ) .
For the algorithms described in previous section, the last syn-chronous property holds that no one proceeds until the end of the current iteration. The master keeps track of the versions of the con-sensus models, which are the only dependencies to update the new version of the consensus. We can simply remove this iteration lock, and let the whole process runs. This results in the proposed Asyn-chronous ADMM algorithm as shown in Algorithm 1.

K is the latest version number ; input : {  X  j ,h ( j ) | j  X  B } for j  X  B do end
K = K + 1 ; Broadcast ( z K , K ); z k is the current consensus ; input : x , y , i Add x , y to samples for user i ; Update  X  i , q i , w i according to Equations [8,10,9];
Let  X  o i and q o i be the original values; Reduce (  X  i , k )
By assuming that networking latency has an identical distribu-tion with mean  X  . We can show that this AsynchADMM converges at a similar rate as synchronous ADMM. Note that the concept of iterations does not exist any more. Here, T refers to the number of operations from the master, and is about M times the number of iterations in Synchronous setting. Therefore, the convergence rate is comparable for both of them.

T HEOREM 6. With assumptions 1, Asynchronous ADMM Algo-rithms [1,2] converge at O ( M X  X  T )
One interesting fact about the Theorem 6 is that the personaliza-tion strength  X  influences the convergence too. Intuitively, if the personalized models are independent from the consensus model, Figure 6: Facial expression of happy emotion on different peo-ple X  X  faces. i.e.,  X  = 0 , the local model q i can be simply assigned to the con-sensus z , and the iteration stops. On the other hand, if different persons have very different personal preference, enforcing them to reach the consensus is very difficult. Allowing the models to be diverged effectively improves convergence speed.
Human emotion study is a critical topic in several fields, includ-ing human-computer interaction, psychology research, medical di-agnosis and treatment, entertainment, education, etc. Facial expres-sion is one of the most useful non-verbal information for detecting human emotion. For decades, people have already investigated in automatic facial expression analysis through image, video, or au-dio [20]. As the camera becomes prevalent in modern devices, like mobile phone, laptop, desktop, and even wearable device, automat-ically recognizing the facial expression from image or video has gained much attention. Great amount of research [4] and indus-try products 1 2 are proposed to solve this problem. The state of the art techniques [4] includes three major steps as shown in figure 5. First of all, human face is detected from the video frame, and the landmark points are located, such as eyes, nose, and mouth. After that, features are extracted from the face image. Popular fea-tures are optical flow, Gabor filter, gradient feature, local binary patterns, motion signature, geometry displacement, etc. These fea-tures can effectively represent the characteristics of different ex-pressions. Given the feature set extracted from the collected data, classifiers have been trained with various machine learning models such as linear classifier , neural networks , hidden Markov model , random forests . However, due to different personalities, cultures, ages, and appearances, the facial expressions of the same emotion vary greatly. For instance, in figure 6, three people performs happy face but with different visual appearance. Hence, using a single model is not sufficient enough to represent all the users. There-fore, personalization will be the key to enhance the accuracy and improve the user experience.

To support this, we consider the data set collected for the facial expression recognition app developed by Microsoft in Xbox One Kinect 3 . The following experiments are conducted based on a sub-set of this data set. The subset includes 151,413 samples collected from 250 users. Conventionally, one model is trained for a classifier of a facial expression. In this way, the individual accuracy is sacri-ficed for obtaining fair average accuracy. With our Asynchronous ADMM framework, we can train a personalized model for each user which can better represent his/her feature distribution. We fol-low the state-of-the-art techniques in face and landmarks detection
FaceReader:http://www.noldus.com/human-behavior-research/products/facereader
Kaggle Challenge:https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge http://www.shacknews.com/article/79301/xbox-one-kinect-reading-emotions-and-heart-rate Figure 7: ROC curves of personalized model and baseline (con-sensus) model in Facial expression data set.  X  id set as 50 in this case. and feature extraction. The features we extract are popular features used in computer vision domain, including, spatial filter responses, temporal filter responses, statistical values, and geometry values.
In addition to the accuracy improvement, our framework reduces the privacy concern of the users. People are usually unwilling to share their privacy data to others, especially this kind of visual data from which the subject X  X  identity can be easily revealed. In the existing framework, in order to train a facial expression classifier, the personal data must be uploaded to somewhere hence loses the privacy. In our distributed framework, training can be performed locally, such as in a smart device or private cluster. The personal data does not need to be uploaded to other places. Although the per-sonal model is still exposed to others, the privacy of the data itself is preserved which largely mitigates the concerns of the privacy.
First, we compare the difference between the results with person-alization and without personalization in accuracy. For each user, we use 5 positive samples and 5 negative samples for training. Fig-ure 7 shows the ROC curve of the overall accuracy among all the users. When the personalization is considered, the accuracy is bet-ter than the one without personalization(baseline model). The base-line model is obtained based on conventional ADMM where all the users train a single consensus model. This result is not surprising since one model does not fit all the users. This result also tells during the real world application, the personalized models would provide better satisfaction to the users than the consensus model.
As we discussed in the previous section, the strength of personal-ization plays the key role for the generalization ability. To demon-strate such a behavior, we study the accuracies in different  X  set-tings. The larger the  X  is, the more the personal model is towards the consensus model. We show the distribution of the ROC curves across different users. We compute the ROC curves for each indi-vidual user, and sample the data points along the curve to compute the mean and the variance. Figure 8 represents a mean ROC curve and the standard deviation interval along the curve to show how consistent the model performs across the population. From the fig-ures, we demonstrate that  X  = 50 (chosen by a validation set) pro-duces the best prediction at the testing set (with better mean ROC curve and narrower variance range), while  X  = 10  X  6 and  X  = 10 produce inferior results. At one extreme side,  X  = 10  X  6 sponds to a model trained completely by using personal data. The overfitted model introduces high variance. On the other extreme side,  X   X   X  , everyone is using nearly the same model which is too generalized to represent different users X  distribution. By tuning the  X  , we find the balance between generalization and personaliza-tion. In the future, our vision is  X  value can be an user-controlled setting. Users can freely select how personalized their own models should be. Our general recommendation is: when the user has only few data, it is always intuitive to have the consensus model to be the cold start model. After the user accumulates the personal data, the model can be relaxed a little bit to support divergence. When the user has enough personal data, the model can be personalized completely, and the consensus model has less and less influence.
One of the benefits of the distributed personalization is to lever-age community knowledge to improve individual user X  X  learning curve. In this experiment, we vary the number of training samples to show the progress of the accuracy improvement. Figure 9 and 10 show the testing ROC curves using personalized models trained on different amounts of training samples. The training samples are set as 1 sample per user, 3 samples per user, 5 samples per user, and 10% of an user X  X  data for each curve, respectively. The cor-responding area under curve (AUC) is shown in Table 1. From 1 sample to 3 samples, the learning speed is really fast that increases the accuracy a lot which demonstrates that our framework enables the users to share the knowledge with each other efficiently. On the contrary, if we use consensus model, the improvement of the accuracy is slow compare to the increase of training samples. It is true that the consensus model performs better than the personal-ized model when using only 1 training sample per user. It means 1 sample of the personal data is not sufficient enough to represent the intra-user variance for each user. With more training data, it covers the variation within each user, i.e., it adapts individual user well, the accuracy of personalized models starts to outperform the consensus model.

This experiment demonstrates that the learning speed of per-sonalized model is fast as the size of training set increases, i.e., users only need few samples to improve their personal models. This is relatively important when we consider the applications in 10 commercial products. In most of the existing personalization prod-ucts/services, e.g., ads, recommendation system, users are usually asked to provide some personal information, answer questions, or do data collection and tagging. However, users sometimes do not have patience to provide so much information. With our frame-work, even though users only provide little amount of data, the system can make the models adapt quickly. It encourages the users to provide more data since they can feel the improvement of the accuracy. With more data, the models fit individuals better, and the whole community is also benefited from the ability of knowledge sharing in our framework.
 Figure 9: ROC curves of the personalized models trained on different number of samples.  X  = 1 in this experiment.
 Figure 10: ROC curves of the baseline (Consensus) models trained on different number of samples.  X  = 1 in this experi-ment.
One of the premium member value propositions of LinkedIn is the jobs recommendation product, Jobs You May Be Interested In (JYMBII), where LinkedIn members can impress active job post-ings relevant to their professional profiles. It is one of most valuable Personalized 0.7771 0.8812 0.9084 0.936 product contributing to member engagement on LinkedIn. Mem-bers can consume JYMBII recommendations in different channels, such as LinkedIn homepage feeds, Email, and LinkedIn Jobs Home (i.e. the  X  X obs X  tab on LinkedIn home page).
 As many members maintain an up-to-date professional profile on LinkedIn, it is very important that JYMBII can recommend highly relevant and personalized jobs for members, whether they are ac-tive job seekers or passive ones who just want gain some awareness of the job market. Without personalization, there are circumstances that a bad job recommendation can significantly impair user X  X  sat-isfaction, for example, an executive in a large-size company may feel offended by seeing entry-level job recommendation, an expe-rienced professional may feel uncomfortable receiving intern-level job recommendations. Even though current recommendation en-gine has already taken into consideration of members X  personal ca-reer characteristics, such as their professional skills, current job po-sition, and their next job preference (either explicit or implicit), the model coefficients for these features are global as we train a single model for all of our members, which inevitably return less satis-factory recommendations for certain members. The Asynchronous ADMM model we propose, on the other hand, will train a personal-ized model for each member with sufficient job seeking history, and still maintain a global consensus model for those without enough historical record. In doing so, it can provide additional personal-ization in a way such that it greatly improve user satisfaction for member X  X  personal job seeking experience.

In order to compare personalization ADMM model against cur-rent baseline consensus model trained with synchronized ADMM setting, we have trained our personalization ADMM model with different gammas, which controls the strength of personalization, with larger gamma indicating more personalization. Each example is a (member, job, label) tuple, with label being either 1 or 0 indi-cating whether or not the member has applied the job. The feature set is the same for baseline and treatment, it mainly includes field based content matching features and member preference features. Each content matching feature is a cosine similarity score between the two fields X  tf.idf vectors, with one field from member profile and the other field from job posting, for example, member X  X  expe-rience field matching against job X  X  description field, member X  X  pro-file summary field matching against job X  X  desired skills field, etc. 11 illustrates the field layout of a (member,job) pair on LinkedIn. The member preference features include job location preference, industry preference, and company size preference. They can be members X  explicit preference they set on the web UI, or their latent preference mined by a model [24]. The final model includes one hundred of such features for each example, and the dataset includes about a million samples. Figure 12 illustrates the ROC curves and their AUCs for model comparison.

The personalized model has the best AUC of 0.91 with gamma = 100, comparing to AUC of 0.79 for the most personalization model (with gamma = 1), and AUC of 0.86 for the least personalization model (with gamma = 10000). It suggests that neither personaliza-tion nor extreme personalization performs the best, but the optimal strength for personalization is somewhere falls in between. Figur e 11: A Member Profile (upper) and A Job Posting (lower) Figure 12: ROC of JYMBII baseline model and personalization models
Comparing to the baseline without personalization (AUC=0.86), it is interesting to see all of the personalization models performs much better in the operation area with low false positive rates (e.g. better with the online system which observes much more negative examples than positive examples, thus it is a necessity to operate with a low false positive rate all the time.
Due to the asynchronous nature of Algorithm 1&amp;2, it is diffi-cult to implement such algorithms in a typical Map/Reduce archi-tecture [11], which requires a synchronization step between each map/reduce stage. We develop an asynchronous computation frame-work based on the Publish/Subscribe message paradigm [14] to fa-cilitate the parallel computation interfaces. The framework consists of a master node and multiple worker nodes. Each worker holds a subset of users and their data, initializes a personal model and subscribes a message channel for each user. The training events are triggered through user interactions. The corresponding worker fetches the current best-known consensus model after receiving a training event, performs a local optimization and sends the up-dated personal model back to the master. The consensus model then gets updated at the reception of each updating event. In syn-chronous ADMM (SYNC-ADMM), all personal models start train-ing at the same time using the same consensus model, the next round is triggered after all personal models are merged into the consensus. For multi-versioned ADMM (MV-ADMM), all per-sonal models start training at approximately the same time using their current known consensus model. The consensus model does not block any training event in any single round and merges con-tinuously, but it waits for the completion of all user models be-fore triggering another round. In asynchronous ADMM (ASYNC-ADMM), each user training event only depends on the completion of the user X  X  previous round. The consensus model merges any re-ported personal model immediately. During the entire experiment process, the total number of training events triggered is more than 10 million. In Figure 13, for each ADMM model, we divide the Figure 13: Convergence rate of different ADMM models. Note that the objective function is in log scale. time window into 300 buckets and only report the average objec-tive function value observed within each time bucket. We choose the starting time when all user models are trained at least once. The flat blank periods within SYNC-ADMM and MV-ADMM models are the blocking periods when they wait for all personal models being merged into the consensus. As demonstrated in Figure 13, ASYNC-ADMM converges at a close-to-identical rate compared to other models, but performs 2  X  4 times faster by eliminating all blocking period.
In this paper, we propose a distributed personalization frame-work and present its successful application to two industry prob-lems. Our approach enables distributed personalization not only for traditional recommendation problems, but also for recognition. Provided each individual task can be formulated as a convex op-timization, our framework can scale, for both training and per-sonalization, to millions of simultaneous users. One of the major contribution that enables this distributed learning system is a novel Asynchronous ADMM algorithm that removes all network I/O wait barriers to maximize the CPU throughput. One of the interesting features of our publish/subscribe based distributed computation ap-proach allows worker nodes to execute on personal devices. Unlike other popular distributed learning algorithms, we do not rely on a decaying step size to control convergence. Instead, Asynchronous ADMM keeps track of versions of the consensus model, thereby avoiding complicated learning rate adjustments. In the future, we plan to extend our approach to the real online setting. Problems like warm-start, online model adaptation, and in-session personal-ization will be studied in-depth. Furthermore, we currently employ only one master to update the consensus. This wastes some net-work bandwidth. Much research has suggested that a Minimum Spanning Tree topology can provide optimal network communi-cation to efficiently leverage computational resources  X  a natural extension to the proposed algorithm.
 [1] A. Agarwal, O. Chapelle, M. Dud X k, and J. Langford. A [2] A. Agarwal and J. C. Duchi. Distributed delayed stochastic [3] S. Ben-David, J. Gehrke, and R. Schuller. A theoretical [4] V. Bettadapura. Face expression recognition and analysis: the [5] M. Bilenko and M. Richardson. Predictive client-side profiles [6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis-[7] E. Cand X   X  ls and B. Recht. Exact matrix completion via con-[8] C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of [9] Y. Chen, D. Pavlov, and J. F. Canny. Large-scale behavioral [10] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Bet-[11] J. Dean and S. Ghemawat. Mapreduce: Simplified data pro-[12] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Opti-[13] W. Deng, M.-J. Lai, Z. Peng, and W. Yin. Parallel Multi-[14] G. Eisenhauer, K. Schwan, and F. E. Bustamante. Publish-[15] T. Evgeniou and M. Pontil. Regularized multi X  X ask learn-[16] M. Hong and Z.-Q. Luo. On the Linear Convergence of the [17] Y. Koren. Factorization meets the neighborhood: A multi-[18] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization tech-[19] D. D. Lee and H. S. Seung. Algorithms for non-negative ma-[20] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of [21] C. Perlich, B. Dalessandro, T. Raeder, O. Stitelman, and [22] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin. On the lin-[23] L. Veer and R. Bernards. Enabling personalized cancer [24] J. Wang and D. Hardtke. User latent preference model for bet-[25] C. Zhang, H. Lee, and K. G. Shin. Efficient distributed linear
