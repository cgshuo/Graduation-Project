 Uncertain data emerging in real-world applications has led to the renewed interests in probabilistic database. Most work of the area is concerned about uncertain data model models have been proposed for expressing data uncertainty and correlations, and sys-tems have been developed based on them for efficient query evaluation [1] [3] [4] [7] [11] [12] [16]. As an example, in a tuple-independent probabilistic database as Mys-tool for uncertain data management. evaluated in PTIME on a tuple-independent probabilistic database, while [10] and [6] tion is equivalent to computing marginal probabilities of boolean formulas associated formula can be factorized into a read-once form where every variable appears at most proximately by formula factorization [8] [9] and Monte-Carlo simulation [5] [7]. 
In this paper, we study the problem of given a set of boolean formulas {  X  1 ...  X  n } once formulas and approximate probabilities for NP-hard ones. A naive approach is to evaluate each formula first with an algorithm which identifies read-once formulas and approximate algorithm will be invoked. Obviously, the naive approach may duplicate formula factorization and double the cost for NP-hard formulas. read-once formulas can be decomposed to smaller read-once partitions, NP-hard for-mulas can also be factorized into smaller read-once partitions and NP-hard partitions. For Example 1, while boolean formula  X  is NP-hard, it contains read-once partitions x and approximate confidence computation for probabilistic query processing, and par-allelizes the evaluation on a much smaller scale of the original problem. 
Our main contributions are concluded as follows:  X  sults of a conjunctive query without self-joins on a tuple-independent probabilistic database. It outputs accurate probabilities for read-once formulas and approximate probabilities with a fixed error for NP-hard formulas.  X  Section 3).  X  (approximate) evaluation on formulas.  X 
Experimental evaluation demonstrates that, for complex boolean formulas, our read-once or NP-hard formulas. 
In the next section, we provide background knowledge for the concepts used in this evaluation based on H-tree . Experimental results demonstrate the performance of our approach in Section 5. We conclude the paper in Section 6. In this section, we present background knowledge of conjunctive query without self-joins on a tuple-independent probabilistic database and read-once formula. 2.1 Conjunctive Query without Self-joins on Tuple-Independent Probabilistic from the context, we refer to the tuple by its random variable as in Example 1. A tu-choosing a (sub)set of tuples in R i to be present. and via boolean formulas. During the evaluation, a boolean formula is built for each algebra operators  X  ,  X , and  X  for building formulas are: Approximate confidence computation with a fixed error is defined as follows, and we focus on the absolute  X  -approximation in this paper. 2.2 Read-Once Formula form where every variable appears at most once, the accurate probability can be com-can appear in DNFs equivalent to  X  [14].  X  determined by  X  . Fig. 2 (a) shows an example of the co-occurrence graph. The special some clause of its DNF. Theorem 2. Let  X  be a boolean formula associated with a result tuple produced by a conjunctive query without self-joins on a tuple-independent probabilistic database.  X  is a unate, normal, and k-monotone DNF (k is the number of relations involved). [13] not. We also discuss the relationship between our algorithm and the state of the art. 3.1 Definition of H-Tree two decomposition methods in this paper:  X 
Independent-or: Factorize  X  into independent  X   X  , X   X   X  X  s.t.  X = X   X   X  X   X  ;  X  Independent-and: Factorize  X  into independent  X   X  , X   X   X  X  s.t.  X = X   X   X  X   X  . is called  X  -complement or  X  -complement of  X  1 on  X  , respectively. Obviously, for any formula  X  , it is a sub-formula for itself. than 3 edges. Furthermore, if  X  is read-once, it is called naive read-once . A naive formula is P 4 -free. However, it may be not read-once in some cases. For ex-ample  X  = zx X yz X xy ,  X  is not read-once for it is not normal. Definition 4 (Minimal Read-once/NP-hard Sub-formula). Given a boolean formula  X  hard sub-formula of  X  and has no sub-formula except  X  2 itself. leaf by an equivalent (partial) H-tree is called a refinement . Fig. 3 shows the complete H-tree for formula  X  of Example 1. Theorem 3. Given a boolean formula  X  , there must exist a complete H-tree for  X  . Proof. We present the following steps for building a complete H-tree M t for  X  :  X  such as [14] [17].  X  hard sub-formula for itself, M t is a complete H-tree for  X  .  X  replacing  X  with  X  1 ,  X  2 and the corresponding operator.  X  Restart to handle  X  1 and  X  2 as  X  from the first step, respectively. 3.2 H-Tree Generation compiling formula  X  of Example 1, and the H-tree is shown in Fig. 3. Theorem 4. For a boolean formula  X  produced by a conjunctive query without self-joins on a tuple-independent probabilistic database, Algorithm 1 builds a complete H-tree M t for  X  . Definition 3 and Theorem 2, it is obviously naive read-once by Theorem 1. that could happen. H-tree for  X  .  X  Algorithm 1. Compile(  X  )
Output: M t , a complete H-tree for  X  1. Make an H-tree M t with  X  as its only node; 2. If  X  is naive then 3. Return M t ; 4. End if 5. If  X  is minimal NP-hard then 6. Return M t ; 7. End if 8. Compute sub-formulas  X  1 ,  X  2 of  X  s.t.  X  =  X  1  X   X  2 or  X  =  X  1  X   X  2 ; 9. Refine M t by replacing  X  with  X  1 ,  X  2 and the corresponding operator; 10. Compile(  X  1 ); 11. Compile(  X  2 ); 12. Return M t ; just satisfy the needs of our algorithm. The relationship between Algorithm 1 and the state of the art is as follows:  X 
The purpose for Algorithm 1 is to factorize a boolean formula, regardless of read-once or NP-hard, into small partitions. While Algorithm 1 utilizes read-once algo-rithms to compute sub-formulas, it handles NP-hard formulas seamlessly.  X  makes the compilation stop at a proper level for parallelizing the confidence com-putation in the next section. Let n X  be the number of variables involved in the input formula at the current recur-sive call (with different occurrences of the same variable counted multiple times), we analysis the time complexity of Algorithm 1 as follows:  X 
The process checking a naive formula takes O( n X  ) at the current recursive call.  X  same time. Specifically, when the read-o nce algorithm triggers its NP-hard condi-tions, the input formula at the current recursive call is actually minimal NP-hard .  X  has less recursive calls and a lower tree structure. rithm in this section. 
As an example, we use the algorithm [14] for accurate confidence computation of read-once leaves in an H-tree, and the algorithm [8] for approximate confidence com-monotonically, the probability bounds can be propagated from leaves to the root of an H-tree. With the complete H-tree M t built by Algorithm 1, we present a parallel algo-is equal to [8] when the original formula is minimal NP-hard itself. Algorithm 2. Para_Appro(M t ,  X  )
Input: M t , a complete H-tree with m NP-hard leaves 
Output: [L, U], the lower and uppe r probability bounds of M t 1. Foreach leaf  X  in M t 2. If  X  is naive read-once then 3. Create a thread to compute p as the algorithm [14]; 4. Else 6. End if 7. End foreach 8. Compute [L, U] for M t from bottom to up; 9. Return [L, U]; the probability bounds [L, U] that satisfy  X  for  X  .  X  as follows (let a X  = p X - X   X  , b X  = p X +  X   X  ). Case 1: If r is  X  , we have: with l -1 and consider l ( l &gt; 2), we have: Case 2: If r is  X  , we have: As Case 1, we prove  X  X  X  X  X  X   X  by induction on l similarly. 
For read-once leaves have  X  = 0, the probability bounds [L, U] generated by Algo-rithm 2 satisfy  X  for  X  .  X  Obviously, the algorithms [14] and [8] used in Algorithm 2 can be replaced by other state-of-the-art algorithms that evaluate read-once and NP-hard formulas respectively. mula into minimal read-once / NP-hard sub-formulas as a complete H-tree, and Algo-sub-formulas. This section presents performance experiments for our approach (called H-tree). The are either read-once or NP-hard, we specify five kinds of formulas shown in Table 1 for the experiments. Experimental results demonstrate that, for complex boolean for-tree) which are designed for evaluating read-once and NP-hard formulas respectively. formulas with a small  X  . 5.1 Read-Once Evaluation formula can be factorized into n minimal read-once sub-formulas. Specifically, Com-compiles the formula to H-tree first and computes the probability in parallel based on almost twice that of CompRO. However, when the input formula is composed of 3 or formula is larger. 5.2 NP-Hard Evaluation In this section, we compute approximate probabilities for NP-hard formulas and com-posed to several sub-formulas, the execution time of H-tree is determined by the larg-est minimal NP-hard sub-formula. As shown in Fig. 5 (b), when the input formula is composed of one minimal NP-hard sub-formula and several read-once sub-formulas, H-tree just needs to approximate the NP-hard sub-formula and is more efficient than D-tree. In Fig. 5 (c), when the input formula can be decomposed to two minimal NP-help users to choose a proper  X  when evaluating NP-hard formulas. boolean formulas, especially for NP-hard ones. Acknowledgments. This work is partly supported by the Important National Science No.61170012). 
