 The task of assessing text readability aims to clas-sify text into different levels of difficulty, e.g., text comprehensible by a particular age group or second language learners (Petersen and Ostendorf, 2009; Feng, 2010; Vajjala and Meurers, 2014). There have been efforts to automatically simplify Wikipedia to cater its content for children and English language learners (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Siddharthan and Mandya, 2014). A related attempt of Vajjala and Meurers (2016) studied the usage of linguistic features for automatic classifi-cation of a pair of sentences  X  one from Standard Wikipedia and the other its corresponding simplifi-cation from Simple Wikipedia  X  into COMPLEX and SIMPLE . As syntactic features, they use informa-tion from phrase structure trees produced by a non-incremental parser, and found them useful.

However, psycholinguistic theories suggest that humans process text incrementally, i.e., humans build syntactic analysis interactively by enhancing current analysis or choosing an alternative analy-sis on the basis of the plausibility with respect to context (Marslen-Wilson, 1973; Altmann and Steed-man, 1988; Tanenhaus et al., 1995). Besides being cognitively possible, incremental parsing has shown to be useful for many real-time applications such as language modeling for speech recognition (Chelba and Jelinek, 2000; Roark, 2001), modeling text read-ing time (Demberg and Keller, 2008), dialogue sys-tems (Stoness et al., 2004) and machine translation (Schwartz et al., 2011). Furthermore, incremental parsers offer linear time speed. Here we explore the usefulness of incremental parsing for predicting rel-ative sentence readability.

Given a pair of sentences  X  one sentence a sim-plified version of the other  X  we aim to classify the sentences into SIMPLE or COMPLEX . We use the sentences from Standard Wikipedia (W IKI ) paired with their corresponding simplifications in Simple Wikipedia (S IMPLE W IKI ) as training and evaluation data. We pose this problem as a pairwise classifi-cation problem (Section 2). For feature extraction, we use an incremental CCG parser which provides a trace of each step of the parse derivation (Section 3). Our evaluation results show that incremental parse features are more useful than non-incremental parse features (Section 5). With the addition of psycholin-guistic features, we attain the best reported results on this task. We make our system available for pub-lic usage. Initially Vajjala and Meurers (2014) trained a bi-nary classifier to classify sentences in S IMPLE W IKI to the class SIMPLE , and sentences in W IKI to the class COMPLEX . This model performed poorly on relative readability assessment. Noting that not all S
IMPLE W IKI sentences are simpler than every other sentence in W IKI , Vajjala and Meurers (2016) re-framed the problem as a ranking problem according to which given a pair of parallel S IMPLE W IKI and W
IKI sentences, the former must be ranked better than the latter in terms of readability. Inspired by Vajjala and Meurers (2016), we also treat each pair together, and model relative readability assessment as a pairwise classification problem. Let a , b be a pair of parallel sentences. Let a , b represent their corresponding feature vectors. We define our classi-fier  X  as  X ( a  X  b ) = 1 if a  X  SIMPLE &amp; b  X  COMPLEX
The motivation for our modelling is that relative features (difference) are more useful than absolute features, e.g., intuitively shorter sentences are sim-ple to read, but length can only be defined in com-parison with another sentence. Below we provide necessary background, and then present the features. 3.1 Combinatory Categorial Grammar (CCG) CCG (Steedman, 2000) is a lexicalized formalism in which words are assigned syntactic types encod-ing subcategorization information. Figure 1 dis-plays an incremental CCG derivation. Here, the syntactic type (category) (S \ NP) / NP on ate indi-cates that it is a transitive verb looking for a NP (object) on the righthand side and a NP (subject) on the lefthand side. Due to its lexicalized and strongly typed nature, the formalism offers attrac-tive properties like elegant composition mechanisms which impose context-sensitive constraints, effi-cient parsing algorithms, and a synchronous syntax-semantics interface. In Figure 1, the category of with (NP \ NP) / NP combines with the category of mush-rooms NP on its righthand side using the combina-tory rule of forward application (indicated by &gt; ), to form the category NP \ NP representing the phrase with mushrooms . This phrase in turn combines with other contextual categories using CCG combinators to form new categories representing larger phrases.
In contrast to phrase structure trees, CCG deriva-tion trees encode a richer notion of syntactic type and constituency. For example, in a phrase struc-ture tree, the category (constituency tag) of ate would be VBD irrespective of whether it is transi-tive or intransitive, whereas the CCG category dis-tinguishes these types. As the linguistic complexity increases, the complexity of the CCG category may increase, e.g., the relative pronoun has the category (NP \ NP) / (S \ NP) in relative clause constructions. In addition, CCG derivation trees have combinators annotated at each level which indicate the way in which the category is derived, e.g., in Figure 1 the category S / NP of John ate is formed by first type-raising (indicated by &gt; T) John and then applying forward composition (indicated by &gt; B) with ate . CCG combinators can throw light into the linguistic complexity of the construction, e.g., crossed com-position is an indicator of long-range dependency. Phrase structure trees do not have this additional in-formation encoded on their nodes. 3.2 Incremental CCG Ambati et al. (2015) introduced a shift-reduce in-ference between this incremental version and stan-dard non-incremental CCG parsers such as Zhang and Clark (2011) is that as soon as the grammar al-lows two types to combine, they are greedily com-bined. For example, in Figure 1, first John is pushed on the stack but is immediately reduced when its head ate appears on the stack (i.e., John  X  X  category combines with ate  X  X  category to form a new cate-gory), and similarly when salad is seen, it is reduced with ate . When with appears it waits to be reduced until its head mushrooms appears on the stack, and later mushrooms is reduced with salad via ate us-ing a special revealing operation (indicated by R &gt; ) followed by a sequence of operations. The revealing operation is performed when a category has greedily consumed a head in advance of a subsequently en-countered post-modifier to regenerate the head. In the non-incremental version, salad is not reduced with ate until with mushrooms is reduced with it.
Consider the following sentences (A) and (B) where (B) is a simpler version of (A).
Figures 2 and 3 present the incremental deriva-tions for both these sentences. Consider the CCG category for to in both the sentences. In (A), the category of to is (S[dcl] \ NP)/(S[to] \ NP) which is more complex compared to the category of to in (B) which is PP/NP . Both the derivations have one right reveal action (indicated by R &gt; ). In (A), the depth of this action is two since it is a VP coordi-information can be useful in predicting the complex-ity of a sentence. 3.3 Features As discussed above, as the complexity of a sentence increases, the complexity of CCG categories, com-binators and the number of revealing operations in-crease in the incremental analysis. We exploit this information to assess the readability of a sentence. For each sentence, we build a feature vector using the features defined below extracted from its incre-mental CCG derivation.
 Sentence Level Features. These features include sentence length, height of the CCG derivation, and the final number of constituents. A CCG derivation may have multiple constituents if none of the combi-nators allow the constituents to combine. This hap-pens mainly in ungrammatical sentences.
 CCG Rule Counts. These features include the number of applications, forward applications, back-ward applications, compositions, forward compo-sitions, backward compositions, left punctuations, right punctuations, coordinations, type-raisings, type-changing, left revealing, right revealing oper-ations used in the CCG derivation. Each combinator is treated as a different feature dimension with its count as the feature value. For the revealing opera-tions, we also add additional features which indicate the depth of the revealing which is analogous to sur-prisal (Hale, 2001).
 CCG Categories. We define the complexity of a CCG category as the number of basic syntac-tic types used in the category, e.g., the complexity of (S[pss] \ NP) / (S[to] \ NP) is 4 since it has one S[pss] , one S [ to ] , and two NP s. Note that CCG type S[pss] indicates a sentence but of the subtype passive . We use average complexity of all the CCG categories used in the derivation as a real valued fea-ture. In addition, we define integer-valued features representing the frequency of specific subtypes (we have 21 subtypes each defined as a different dimen-sion) and the frequency of the top 8 syntactic types (each as a different dimension). 4.1 Evaluation Data As evaluation data, we use W IKI and S IMPLE W IKI parallel sentence pairs collected by Hwang et al. (2015), a newer and larger version compared to Zhu et al. (2010) X  X  collection. We only use the pairs from the section GOOD consisting of 150K pairs. We further removed pairs containing identical sentences which resulted in 117K clean pairs. We randomly divided the data into training (60%), development (20%) and test (20%) splits. 4.2 Implementation details As our classifier (see Section 2) we use SVM with Sequential Minimal Optimization in Weka toolkit (Hall et al., 2009) following its popularity in read-ability literature (Feng, 2010; Hancke et al., 2012; al. (2015) X  X  CCG parser for extracting CCG deriva-tions. This parser requires a CCG supertagger to limit its search space for which we use EasyCCG tagger (Lewis and Steedman, 2014). 4.3 Baseline N
ON -I NCREMENTAL PST. Following Vajjala and Meurers (2016), we use features extracted from P hrase S tructure T rees (PST) produced by the Stan-ford parser (Klein and Manning, 2003), a non-incremental parser. We use the exact code used by Vajjala and Meurers (2016) to extract these features which include part-of-speech tags, constituency fea-tures like the number of noun phrases, verb phrases and preposition phrases, and the average size of the constituent trees. Vajjala and Meurers (2016) used a First we analyze the impact of incremental CCG features (and so the name I NCREMENTAL CCG ). Table 1: Impact of different syntactic features. Table 1 presents the results of predicting rela-TAL CCG achieves 72.12% accuracy, a signif-I
NCREMENTAL PST (71.68%) indicating that in-cremental CCG features are empirically more use-ful than non-incremental phrase structure features. We also evaluate if this result holds for incremen-tal vs. non-incremental CCG parse features. Am-bati et al. (2015) can also produce non-incremental CCG parses by turning off a flag. Note that in the non-incremental version, revealing features are ab-sent. This version achieves an accuracy of 72.02%, around 0.1% lower than the winner I NCREMENTAL CCG, yet higher than N ON -I NCREMENTAL PST showing that CCG derivation trees offer richer syn-tactic information than phrase structure trees. POS taggers used for Stanford and CCG parsers gave similar accuracy. This shows that the improvements are indeed due to the incremental CCG parse fea-tures rather than the POS features.
 Apart from the syntactic features, Vajjala and Meurers (2016) have also used psycholinguistic fea-tures such as age of acquisition of words, word im-agery ratings, word familiarity ratings, and ambigu-ity of a word, collected from the psycholinguistic repositories Celex (Baayen et al., 1995), MRC (Wil-son, 1988), AoA (Kuperman et al., 2012) and Word-Net (Fellbaum, 1998). These features are found to be highly predictive for assessing readability. We enhance our syntactic models N ON -I NCREMENTAL PST and I NCREMENTAL CCG by adding these psy-cholinguistic features to build N ON -I NCREMENTAL PST++ and I NCREMENTAL CCG++ respectively. Table 2 presents the final results along with the pre-vious state-of-the-art results of Vajjala and Meurers Table 2: Performance of models with both syntactic and psycholinguistic features. ditionally the performance gap between our mod-els decrease (from 0.44 to 0.19) showing some of the psycholinguistic features also model a subset of the syntactic features. I NCREMENTAL CCG++ achieves an accuracy of 78.77% outperforming the previous best system of Vajjala and Meurers (2016) by a wide margin.
 Speed. In addition to accuracy, parsing speed is important in real-time applications. The Stanford parser took 204 minutes to parse the test data with a speed of 3.8 sentences per second. The incremental CCG parser took 16 minutes with an average speed of 47.5 sentences per second, a 12X improvement over the Stanford parser. These numbers include POS tagging time for the Stanford parser, and POS tagging and supertagging time for the incremental CCG parser. All the systems are run on the same hardware (Intel i5-2400 CPU @ 3.10GHz). Our empirical evaluation on assessing relative sen-tence complexity suggests that syntactic features ex-tracted from an incremental CCG parser are more useful than from a non-incremental phrase struc-ture parser. This result aligns with psycholinguis-tic findings that human sentence processor is incre-mental. Our incremental model enhanced with psy-cholinguistic features achieves the best reported re-sults on predicting relative sentence readability. We experimented with Simple Wikipedia and Wikipedia data from Hwang et al. (2015). We can explore the usefulness of our system on other datasets like On-eStopEnglish (OSE) corpus (Vajjala and Meurers, 2016) or the dataset from Xu et al. (2015). We are also currently exploring the usefulness of incremen-tal analysis for psycholinguistic data by switching off the lookahead feature. We thank Sowmya Vajjala and Dave Howcroft for providing data and settings for the baseline. We also thank the three anonymous reviewers for their useful suggestions. This work was supported by ERC Ad-vanced Fellowship 249520 GRAMPLUS, EU IST Cognitive Systems IP Xperience and a Google PhD Fellowship for the second author.

