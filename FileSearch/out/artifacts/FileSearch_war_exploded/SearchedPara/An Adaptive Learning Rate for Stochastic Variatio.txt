 Rajesh Ranganath rajeshr@cs.princeton.edu Princeton University, 35 Olden St., Princeton, NJ 08540 Chong Wang chongw@cs.cmu.edu David M. Blei blei@cs.princeton.edu Princeton University, 35 Olden St., Princeton, NJ 08540 Eric P. Xing epxing@cs.cmu.edu Natural gradient of the -ELBO. We can com-pute the natural gradient in Eq. 7 at by first finding the corresponding optimal local parameters ent of L .; / , i.e., the ELBO where we fix D . These are equivalent because The notation r is the Jacobian of as a function of , and we use that r L .;/ is zero at D .
 Derivation of the adaptive learning rate. To compute the adaptive learning rate we minimize
 X J. t / j t  X  at each time t. Expanding E n  X J. t / j t  X  , we get We can compute this expectation in terms of the mo-ments of the sample optimum in Eq. 15 Setting the derivative of E n  X J. t / j t  X  with respect to t equal to 0 yields the optimal learning in Eq. 16. Convergence of the idealized learning rate. We show convergence of t to a local optima with our idealized learning rate through martingale convergence. Let M t C 1 D Q.a t / , then M t is a super-martingale with respect to the natural filtration of the sequence t , Since M t is a non-negative supermartingale by the martingale convergence theorem, we know that a finite M 1 exists and M t ! M 1 almost surely. Since the M t converge, the sequence of expected values E  X M t  X  converge to E  X M 1  X  . This means that the sequence of expected values form a Cauchy sequence, so the difference between elements of the sequence goes to zero, Substituting the idealized optimal learning rate into this expression gives D Since the D t  X  X  are a sequence of nonpositive random variables whose expectation goes to zero and that the variances are bounded (by assumption), the square portion of Eq. 1 must go to zero almost surely. This quantity going to zero implies that either t ! or t ! t . If t D t , then t is a local optima under the assumption that the two parameter ( and for the ELBO) function we are optimizing can be optimized via coordinate ascent. Putting everything together gives us that t goes to a local optima almost
 Rajesh Ranganath rajeshr@cs.princeton.edu Princeton University, 35 Olden St., Princeton, NJ 08540 Chong Wang chongw@cs.cmu.edu Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA, 15213 David M. Blei blei@cs.princeton.edu Princeton University, 35 Olden St., Princeton, NJ 08540 Eric P. Xing epxing@cs.cmu.edu Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA, 15213 Stochastic variational inference lets us use complex probabilistic models to analyze massive data (Hoff-man et al., to appear). It has been applied to topic models (Hoffman et al., 2010), Bayesian nonparamet-ric models (Wang et al., 2011; Paisley et al., 2012), and models of large social networks (Gopalan et al., 2012). In each of these settings, stochastic variational inference finds as good posterior approximations as traditional variational inference, but scales to much larger data sets.
 Variational inference tries to find the member of a sim-ple class of distributions that is close to the true poste-rior (Bishop, 2006). The main idea behind stochastic variational inference is to maximize the variational objective function with stochastic optimization (Rob-bins &amp; Monro, 1951). At each iteration, we follow a noisy estimate of the natural gradient (Amari, 1998) obtained by subsampling from the data. Following these stochastic gradients with a decreasing learning rate guarantees convergence to a local optimum of the variational objective.
 Traditional variational inference scales poorly because it has to analyze the entire data set at each iteration. Stochastic variational inference scales well because at each iteration it needs only to analyze a subset. Fur-ther, computing the stochastic gradient on the subset is just as easy as running an iteration of traditional inference on data of the subset X  X  size. Thus any imple-mentation of traditional inference is simple to adapt to the stochastic setting.
 However, stochastic inference is sensitive to the learning rate, a nuisance that must be set in advance. With a quickly decreasing learning rate it moves too cautiously; with a slowly decreasing learning rate it makes erratic and unreliable progress. In either case, convergence is slow and performance suffers.
 In this paper, we develop an adaptive learning rate for stochastic variational inference. The step size de-creases when the variance of the noisy gradient is large, mitigating the risk of taking a large step in the wrong direction. The step size increases when the norm of the expected noisy gradient is large, indicating that the algorithm is far away from the optimal point. With this approach, the user need not set any learning-rate pa-rameters to find a good variational distribution, and it is implemented with computations already made within stochastic inference. Further, we found it consistently led to improved convergence and estimation over the best decreasing and constant rates.
 Figure 1 displays three learning rates: a constant rate, a rate that satisfies the conditions of Robbins &amp; Monro (1951), and our adaptive rate. These come from three fits of latent Dirichlet allocation (LDA) (Blei et al., 2003) to a corpus of 1.8M New York Times articles. At each iteration, the algorithm subsamples a small set of documents and updates its estimate of the posterior. We can see that the adaptive learning rate exhibits a special pattern. The reason is that in this example we subsampled the documents in two-year increments. This engineers the data stream to change at each epoch, and the adaptive learning rate adjusts itself to those changes. The held-out likelihood scores (in the top right) indicate that the resulting variational distribu-tion gave better predictions than the two competitors. (We note that the adaptive learning rate also gives bet-ter performance when the data are sampled uniformly. See Figure 3.) Stochastic variational inference assumes that data are subsampled uniformly at each iteration and is sensitive to the chosen learning rate (Hoffman et al., to appear). The adaptive learning rate developed here solves these problems. It accommodates practical data streams, like the chronological example of Figure 1, where it is difficult to uniformly subsample the data; and it gives a robust way to use stochastic inference without hand-tuning.
 In the main paper, we review stochastic variational in-ference, derive our algorithm for adaptively setting the step sizes, and present an empirical study using LDA on three large corpora. In the appendices, we present proofs and a discussion of convergence. Our adaptive algorithm requires no hand-tuning and performs better than the best hand-tuned alternatives. Variational inference performs approximate posterior inference by solving an optimization problem. The idea is to posit a family of distributions with free vari-ational parameters, and then fit those parameters to find the member of the family close (in KL divergence) to the posterior. In this section, we review mean-field variational inference for a large class of models. We first define the model class, describe the variational ob-jective function, and define the mean-field variational parameters. We then derive both the  X  X lassical X  coordi-nate ascent inference method and stochastic variational inference, a scalable alternative to coordinate ascent inference. In the next section, we derive our method for adaptively setting the learning rate in stochastic variational inference.
 Model family. We consider the family of models in Figure 2 (Hoffman et al., to appear). There are three types of random variables. The observations are x 1: n , the local hidden variables are z 1: n , and the global hidden variables are  X  . The model assumes that the observations and their corresponding local hidden variables are conditionally independent given the global hidden variables, Further, the distributions are assumed to be in the exponential family, p ( z i ,x i |  X  ) = h ( z i ,x i ) exp  X  &gt; t ( z i ,x where we overload notation for the base measures h (  X  ), sufficient statistics t (  X  ), and log normalizers a (  X  ). The term t (  X  ) has the form t (  X  ) = [  X  ;  X  a (  X  )]. Finally, the model satisfies conditional conjugacy . The prior p (  X  |  X  ) is conjugate to p ( z i ,x i |  X  ). This means that the distribution of the global variables given the observations and local variables p (  X  | z 1: n ,x 1: n ) is in the same family as the prior p (  X  |  X  ). This differs from classical Bayesian conjugacy because of the local vari-ables. In this model class, the conditional distribution given only the observations p (  X  | x 1: n ) is not generally in the same family as the prior.
 This is a large class of models. It includes Bayesian Gaussian mixtures, latent Dirichlet alloca-tion (Blei et al., 2003), probabilistic matrix factoriza-tion (Salakhutdinov &amp; Mnih, 2008), hierarchical linear regression (Gelman &amp; Hill, 2007), and many Bayesian nonparametric models (Hjort et al., 2010).
 Note that in Eq. 1 and 2 we used the joint conditional p ( z i ,x i |  X  ). This simplifies the set-up in Hoffman et al. (to appear), who assume the local prior p ( z i |  X  ) is conjugate to p ( x i | z i , X  ). We also make this assumption, but writing the joint conditional eases our derivation of the adaptive learning-rate algorithm.
 Variational objective and mean-field family. Our goal is to approximate the posterior distribution p (  X ,z 1: n | x 1: n ) using variational inference. We approx-imate it by positing a variational family q (  X ,z 1: n ) over the hidden variables  X  and z 1: n and then finding the member of that family close in KL divergence to the posterior. This optimization problem is equivalent to maximizing the evidence lower bound (ELBO), a bound on the marginal probability p ( x 1: n ),
L ( q ) = E q [log p ( x 1: n ,z 1: n , X  )]  X  E q [log q ( z To solve this problem, we must specify the form of the variational distribution q (  X ,z 1: n ). The simplest family is the mean-field family , where each hidden variable is independent and governed by its own variational parameter, We assume that each variational distribution comes from the same family as the conditional.
 With the family defined, we now write the ELBO in terms of the variational parameters
L (  X , X  1: n ) = E q [log p (  X  |  X  )]  X  E q [log q (  X  |  X  )] Mean-field variational inference optimizes this function. Coordinate ascent variational inference. We fo-cus on optimizing the global variational parameter  X  . We write the ELBO as a function of this parameter, implicitly optimizing the local parameters  X  1: n for each value of  X  , We call this the  X  -ELBO. It has the same optimum as the full ELBO L (  X , X  1: n ).
 Define the  X  -optimal local variational parameters  X   X  = arg max  X  L (  X , X  ). 1 Using the distributions in Eq. 2 and Eq. 3, the  X  -ELBO is L (  X  ) = a (  X  ) +  X   X  a (  X  ) &gt;  X   X  +  X  + P n i =1  X  that does not depend on  X  .
 The natural gradient (Amari, 1998) of the  X  -ELBO is We derive this in the supplement.
 With this perspective, coordinate ascent variational inference can be interpreted as a fixed-point iteration. Define This is the optimal value of  X  when  X  1: n is fixed at the optimal  X   X  1: n for  X  =  X  t . Given the current estimate of the global parameters  X  t , coordinate ascent iterates between (a) computing the optimal local parameters for the current setting of the global parameters  X  t and (b) using Eq. 8 to update  X  t +1 =  X   X  t . Each setting of  X  t in this sequence increases the ELBO.
 Hoffman et al. (to appear) point out that this is ineffi-cient. At each iteration Eq. 8 requires analyzing all the data to compute  X   X  t 1: n , which is infeasible for large data sets. The solution is to use stochastic optimization. Stochastic variational inference. Stochastic infer-ence optimizes the ELBO by following noisy estimates of the natural gradient, where the noise is induced by repeatedly subsampling from the data.
 Let i be a random data index, i  X  Unif (1 ,...,n ). The ELBO with respect to this index is The expectation of L i (  X  ), with respect to the random data index, is equal to the  X  -ELBO in Eq. 6. Thus we can obtain noisy estimates of the gradient of the ELBO by sampling a data point index and taking the gradient of L i (  X  ). We follow such estimates with a decreasing step-size  X  t . This is a stochastic optimization (Robbins &amp; Monro, 1951) of the variational objective. We now compute the natural gradient of Eq. 9. Notice that L i (  X  ) is equal to the  X  -ELBO, but where x i is repeatedly seen n times. Therefore we can use the nat-ural gradient in Eq. 7 to compute the natural gradient of L i (  X  ), Following noisy gradients with a decreasing step-size  X  t gives us stochastic variational inference. At iteration t : 1. Sample a data point i  X  Unif(1 ,...,n ). 2. Compute the optimal local parameter  X   X  t i for the 3. Compute intermediate global parameters to be the 4. Set the global parameters to be a weighted average This is much more efficient than coordinate ascent inference. Rather than analyzing the whole data set before updating the global parameters, we need only analyze a single sampled data point.
 As an example, and the focus of our empirical study, consider probabilistic topic modeling (Blei et al., 2003). A topic model is a probabilistic model of a text cor-pus. A topic is a distribution over a vocabulary, and each document exhibits the topics to different degree. Topic modeling analysis is a posterior inference prob-lem: Given the corpus, we compute the conditional distribution of the topics and how each document ex-hibits them. This posterior is frequently approximated with variational inference.
 The global variables in a topic model are the topics, a set of distributions over the vocabulary that is shared across the entire collection. The local variables are the topic proportions, hidden variables that encode how each document exhibits the topics. Given the topics, these local variables are conditionally independent. At any iteration of coordinate ascent variational infer-ence, we have a current estimate of the topics and we use them to examine each document. With stochas-tic inference, we need only to analyze a subsample of documents at each iteration. Stochastic inference lets topic modelers analyze massive collections of docu-ments (Hoffman et al., 2010).
 To assure convergence of the algorithm the step sizes  X  t (used in Eq. 12) need to satisfy the conditions of Rob-bins &amp; Monro (1951), Choosing this sequence can be difficult and time-consuming. A sequence that decays too quickly may take a long time to converge; a sequence that decays too slowly may cause the parameters to oscillate too much. To address this, we propose a new method to adapt the learning rate in stochastic variational inference. Our method for setting the learning rate adapts to the sampled data. It is designed to minimize the expected distance between the stochastic update in Eq. 12 to the optimal global variational parameter  X   X  t in Eq. 8, i.e., the setting that guarantees the ELBO increases. The expectation of the distance is taken with respect to the randomly sampled data index. The adaptive learning rate is pulled by two signals. It grows when the current setting is expected to be far away from the coordinate optimal setting, but it shrinks with our uncertainty about this distance.
 In this section, we describe the objective function and compute the adaptive learning rate. We then describe how to estimate this rate, which depends on several unknown quantities, by computing moving averages within stochastic variational inference.
 The adaptive learning rate. Our goal is to set the learning rate  X  t in Eq. 12. The expensive batch update  X  t in Eq. 8 is the update we would make if we processed the entire data set; the cheaper stochastic update  X  t +1 in Eq. 12 only requires processing one data point. We estimate the learning rate that minimizes the expected error between the stochastic update and batch update. The squared norm of the error is Note this is a random variable because  X  t +1 depends on a randomly sampled data point. We obtain the adaptive to a stochastic update that is close in expectation to the batch update. 2 The randomness in J (  X  t ) from Eq. 14 comes from the intermediate global parameter  X   X  t in Eq. 11. Its mean and covariance are Minimizing E n [ J (  X  t ) |  X  t ] with respect to  X  t gives The derivation can be found in the supplement. The learning rate grows when the batch update  X   X  t is far from the current parameter  X  t . The learning rate shrinks, through the trace term, when the intermediate parameter has high variance (i.e., uncertainty) around the batch update.
 However, this learning rate depends on unknown quantities X  X he batch update  X   X  t and the variance  X  of the intermediate parameters around it. We now describe our algorithm for estimating the adaptive learning rate within the stochastic variational infer-ence algorithm. Estimating the adaptive learning rate. In this section we estimate the adaptive learning rate in Eq. 16 within the stochastic variational inference algorithm. We do this by expressing it in terms of expectations of the noisy natural gradient of the ELBO X  X  quantity that is easily computed within stochastic inference X  and then approximating those expectations with expo-nential moving averages.
 Let g t be the sampled natural gradient defined in Eq. 10 at iteration t . Given the current estimate of global variational parameters  X  t , the expected value of g t is the difference between the current parameter and noisy batch update, Its covariance is equal to the covariance of the interme-diate parameters  X   X  t We can now rewrite the denominator of the adaptive learning rate in Eq. 16, Using this expression and the expectation value of the noisy natural gradient in Eq. 17, we rewrite the adaptive learning rate as Note that this transformation collapses estimating the required matrix  X  into the estimation of a scalar. We could form a Monte Carlo estimate of these expec-tations by drawing multiple samples from the data at the current time step t and repeatedly forming noisy natural gradients. Unfortunately, this reduces the com-putational benefits of using stochastic optimization. Instead we adapt the method of Schaul et al. (2012), approximating the expectations within the stochastic algorithm with moving averages across iterations. Let the moving averages for E [ g t ] and E [ g &gt; t g t noted by g t and h t respectively. Let  X  t be the window size of the exponential moving average at time t . The updates are Plugging these estimates into Eq. 18, we can approxi-mate the adaptive learning rate with Algorithm 1 Learning Rate Free Inference. The moving averages are less reliable after large steps, so we update our memory size using the following, The full algorithm is in Algorithm 1.
 We initialize the moving averages through Monte Carlo estimates of the expectations at the initialization of the global parameters  X  1 and initialize  X  1 to be the number of samples used in to construct the Monte Carlo estimate. Finally, while we have described the algorithm with a single sampled data point at each iteration, it generalizes easily to mini-batches, i.e., when we sample a small subset of data at each iteration. Convergence. In Section 4 we found that our algo-rithm converges in practice. However, proving conver-gence is an open problem. As a step towards solving this problem, we prove convergence of  X  t under a learn-ing rate that minimizes the error to an unspecified local optimum  X   X  rather than to the optimal batch update. (The complexity with the optimal batch up-date is that it changes at each iteration.) We call the resulting learning rate a t the idealized learning rate . The objective we minimize is the expected value of The idealized optimal learning rate that minimizes the objective Q is The supplement gives a proof of convergence to a local optimum with learning rate a  X  t . We cannot compute this learning rate because it depends on  X   X  and  X   X  Further,  X   X  is hard to estimate.
 Compared to the adaptive learning rate in Eq. 16, the idealized rate contains an additional term in the numerator. If the batch update  X   X  t is close to the local optimum  X   X  then the learning rates are equivalent. Related work. Schaul et al. (2012) describe the op-timal rate for a noisy quadratic objective by minimizing the expected objective at each time step. They used their rate to fit neural networks. A possible general-ization of their work to stochastic inference would be to take the Taylor expansion of the ELBO around the current parameter and maximize it with respect to the learning rate. However, we found (empirically) that this approach is inadequate. The Taylor approximation is poor when the step size is large, and the Hessian of the ELBO is not always negative definite. This led to unpredictable behaviors in this algorithm.
 Our algorithm is in the same spirit of the approach pro-posed in Chien &amp; Fu (1967). They studied the problem of stochastically estimating the mean of a normal distri-bution in a Bayesian setting. They derived an adaptive learning rate by minimizing the squared error to the unknown posterior mean. This has also been pursued for tracking problems by George &amp; Powell (2006). Our approach differs from theirs in that our objective is tailored to variational inference and is defined in terms of the per-iteration coordinate optima (rather than a global optimum). Further, our estimators are based on the sampled natural gradient. We evaluate our adaptive learning rate with latent Dirichlet allocation (LDA). We consider stochastic in-ference in two settings: where the data are subsampled uniformly (i.e., where the theory holds) and where they are subsampled in a non-stationary way. In both settings, adaptive learning rates outperform the best hand-tuned alternatives.
 Data sets. We analyzed three large corpora: Nature , New York Times , and Wikipedia . The Nature corpus contains 340K documents and a vocabulary of 4,500 terms; the New York Times corpus contains 1.8M doc-uments and a vocabulary vocabulary of 8,000 terms; the Wikipedia corpus contains 3.6M documents and a vocabulary of 7,700 terms.
 Evaluation metric. To evaluate our models, we held out 10,000 documents from each corpus and cal-culated its predictive likelihood. We follow the metric used in recent topic modeling literature (Asuncion et al., 2009; Wang et al., 2011; Hoffman et al., to appear). For a document w d in D test , we split it in into halves, w d = ( w d 1 ,w d 2 ), and computed the predictive log like-lihood of the words in w d 2 conditioned on w d 1 and D train . A better predictive distribution given the first half should give higher likelihood to the second half. The per-word predictive log likelihood is defined as Here | w d 2 | is the number of tokens in w d 2 . This evalu-ation measures the quality of the estimated predictive distribution. It lets us compare methods regardless of whether they provide a bound. However, this quantity is intractable in general and so we used the same strat-egy as in Hoffman et al. (to appear) to approximate the predictive log likelihood.
 Hyperparameters and alternative learning rates. We set the mini-batch size to 100 documents. We used 100 topics and set the Dirichlet hyperparam-eter on the topics to be 0.01. LDA also contains a Dirichlet prior over the topic proportions, i.e., how much each document exhibits each topic. We set this to the uniform prior. These are the same values used in Hoffman et al. (2010).
 We compared the adaptive learning rate to two others. Hoffman et al. (2010) use a learning rate of the form where  X   X  (0 . 5 , 1]. This choice satisfies the Robbins-Monro conditions Eq. 13. They used a grid search to find the best parameters for this rate. Note that Snoek et al. (2012) showed that Bayesian optimization can speed up this search.
 We also compared to a small constant learning rate (Col-lobert et al., 2011; Nemirovski et al., 2009). We found We report our results against the best Robbins-Monro learning rate and the best small constant learning rate. Results. We compared the algorithms in two sce-narios. First, we ran stochastic inference as described above. At each iteration, we subsample uniformly from the data, analyze the subsample, and use the learning rate to update the global variational parameters. We ran the algorithms for 20 hours.
 Figure 3 shows the results. On all three corpora, the adaptive learning rate outperformed the best Robbins-Monro rate and constant rate. 3 It converged faster and formed better predictive distributions. Figure 4 shows how the learning rate behaves. Without tuning any parameters, it gave a similar shape to the Robbins-Monro rate.
 Second, we ran stochastic inference where the data were subsampled non-uniformly. The theory breaks down in such a scenario, but this matches many applied situations and illustrates some of the advantages of an adaptive learning rate. We split the New York Times data in into 10 segments based their publication dates. We ran the algorithm sequentially, training from each segment and testing on the next. Each pair of training and testing segments form an epoch. We trained on each of these 5 epochs for three hours. Throughout the epochs we maintained one approximate posterior. We compared the adaptive learning rate to the best Robbins-Monro and constant learning rates from the previous experiment.
 Figure 1 shows the results. The adaptive learning rate spikes when the underlying sampling distribution changes. (Note that the algorithm does not know when the epoch is changing.) Further, it led to better predictive distributions. We developed and studied an algorithm to adapt the learning rate in stochastic variational inference. Our approach is based on the minimization of the expected squared norm of the error between the global variational parameter and the coordinate optimum. It requires no hand-tuning and uses computations already found inside stochastic inference. It works well for both sta-tionary and non-stationary subsampled data. In our study of latent Dirichlet allocation, it led to faster con-vergence and a better optimum when compared to the best hand-tuned rates.
 We thank Tom Schaul and the reviewers for their help-ful comments. Rajesh Ranganath is supported by an NDSEG fellowship. David M. Blei is supported by NSF IIS-0745520, NSF, IIS-1247664, NSF IIS-1009542, ONR N00014-11-1-0651, and the Alfred P. Sloan foundation. Chong Wang and Eric P. Xing are supported by AFOSR FA9550010247, NSF IIS1111142, NIH 1R01GM093156 and NSF DBI-0546594.
 Amari, S. Natural gradient works efficiently in learning. Neural computation , 10(2):251 X 276, 1998.
 Asuncion, A., Welling, M., Smyth, P., and Teh, Y. On smoothing and inference for topic models. In Uncertainty in Artificial Intelligence , 2009. Bishop, C. Pattern Recognition and Machine Learning . Springer New York., 2006.
 Blei, D., Ng, A., and Jordan, M. Latent Dirichlet allocation. Journal of Machine Learning Research , 3:993 X 1022, January 2003.
 Chien, Y. and Fu, K. On Bayesian learning and stochas-tic approximation. Systems Science and Cybernetics, IEEE Transactions on , 3(1):28  X 38, jun. 1967. Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research , 12:2493 X 2537, 2011.
 Gelman, A. and Hill, J. Data Analysis Using Regression and Multilevel/Hierarchical Models . Cambridge Univ. Press, 2007.
 George, A. and Powell, W. Adaptive stepsizes for re-cursive estimation with applications in approximate dynamic programming. Machine learning , 65(1): 167 X 198, 2006.
 Gopalan, P., Mimno, D., Gerrish, S., Freedman, M., and Blei, D. Scalable inference of overlapping com-munities. In Neural Information Processing Systems , 2012.
 Hjort, N., Holmes, C., Mueller, P., and Walker, S. Bayesian Nonparametrics: Principles and Practice . Cambridge University Press, Cambridge, UK, 2010. Hoffman, M., Blei, D., and Bach, F. Online inference for latent Drichlet allocation. In Neural Information Processing Systems , 2010.
 Hoffman, M., Blei, D., Wang, C., and Paisley, J. Stochastic variational inference. Journal of Machine Learning Research , to appear.
 Nemirovski, A., Juditsky, A., Lan, G., and Shapiro,
A. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Opti-mization , 19(4):1574 X 1609, 2009.
 Paisley, J., Wang, C., and Blei, D. The discrete infinite logistic normal distribution. Bayesian Analysis , 7(2): 235 X 272, 2012.
 Robbins, H. and Monro, S. A stochastic approximation method. The Annals of Mathematical Statistics , 22 (3):pp. 400 X 407, 1951.
 Salakhutdinov, R. and Mnih, A. Probabilistic matrix factorization. In Neural Information Processing Sys-tems , 2008.
 Schaul, T., Zhang, S., and LeCun, Y. No more pesky learning rates. ArXiv e-prints , June 2012.
 Snoek, J., Larochelle, H., and Adams, R. Practi-cal Bayesian optimization of machine learning al-gorithms. In Neural Information Processing Systems , 2012.
 Wang, C., Paisley, J., and Blei, D. Online variational inference for the hierarchical Dirichlet process. In
International Conference on Artificial Intelligence
