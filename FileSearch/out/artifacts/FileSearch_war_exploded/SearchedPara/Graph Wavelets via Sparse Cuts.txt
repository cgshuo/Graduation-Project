  X  Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Pro-cessing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner. How-ever, how to discover wavelet bases that capture the geom-etry of the data with respect to the signal as well as the graph structure remains an open problem. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an efficient heuristic. We formulate the ba-sis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regular-ized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal, pro-ducing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene net-works) and significantly outperforming the best baseline. Categories and Subject Descriptors: H.2.8 [ Database Management ]: Database applications  X  data mining General Terms: Algorithms, Experimentation Keywords: Graph mining, Spectral Theory, Wavelets
Graphs are the model of choice in several applications, ranging from social networks to the Internet-of-things (IoT) . In many of these scenarios, the graph represents an underly-ing space in which information is generated, processed and transferred. For instance, in social networks, opinions prop-agate via social interactions and might produce large cas-cades across several communities. In IoT, different objects (e.g. cars) collect data from diverse sources and communi-cate with each other via the network infrastructure. As a consequence, exploiting the underlying graph structure in order to manage and process data arising from these appli-cations has become a key challenge.

Signal processing on graphs (SPG) is a framework for the analysis of data residing on vertices of a graph [28, 26]. The idea generalizes traditional signal processing (e.g. compres-sion, sampling) as means to support the analysis of high-dimensional datasets. In particular, SPG has been applied in the discovery of traffic events using speed data collected by a sensor network [22]. Moreover, graph signals are a pow-erful representation for data in machine learning [13, 11]. As in traditional signal processing, the fundamental operation in SPG is the transform , which projects the graph signal in the frequency (or other convenient) domain. Real sig-nals are expected to be smooth with respect to the graph structure X  X alues at nearby vertices are similar X  X nd an ef-fective transform should lead to rapidly decaying coefficients for smooth signals. The most popular transform in SPG, known as Graph Fourier Transform [25, 28], represents a sig-nal as a linear combination of the eigenvectors of the graph Laplacian. However, as its counterpart in traditional signal processing, Graph Fourier fails to localize signals in space (i.e. within graph regions). This limitation has motivated recent studies on graph wavelets [6, 16, 5].

An open issue in SPG is how to link properties of the sig-nal and underlying graph to properties of the transform [28]. Gavish et al. [13] makes one of the first efforts in this direc-tion, by relating the smoothness of the signal with respect to a tree structure of increasingly refined graph partitions and the fast decay of the wavelet coefficients in a Haar-like ex-pansion. However, as explicitly stated in their paper, their approach  X  X aises many theoretical questions for further re-search, in particular regarding construction of trees that best capture the geometry of these challenging datasets X .
In this paper, we study the problem of computing wavelet trees that encode both the graph structure and the signal in-formation. A wavelet tree defines a hierarchical partitioning used as basis for a graph wavelet transform. Good wavelet trees should produce fast decaying coefficients, which sup-port a low-dimensional representation of the graph signal. The particular application scenario we consider is the lossy graph signal compression. This task arises in many relevant data management and analytics applications, including IoT and social networks, where values associated to intercon-nected entities have to be represented in a compact form. Figure 1: Graph wavelet transforms for two different wavelet trees and the same piecewise smooth graph signal (values set to vertices). A wavelet tree contains one average coefficient and several weighted difference coefficients associated with vertex partitions. Basis A is better than B because it pro-duces fast decaying difference coefficients. Moreover, basis A can be approximately encoded as a sequence of sparse graph a compact and accurate representation of the graph signal.
Figure 1 shows two wavelet trees, A and B, and their respective transforms for a piecewise smooth graph signal defined over seven vertices. The wavelet transform contains a single average coefficient and a set of weighted difference coefficients associated to each node of the tree. Weighted difference coefficients are computed as a function of the val-ues in each partition and the partition sizes (see Equation 2 for a formal definition). Notice that these two bases produce very different wavelet transforms for the same signal. While tree A is characterized by fast decaying difference coeffi-cients, tree B has relatively large coefficients at every level. This indicates that tree A supports a better representation for the signal than does tree B. However, good wavelet trees must also capture properties of the graph structure.
We measure the relationship between a wavelet tree and the graph structure using the notion of sparse cuts. A graph cut is a set of edges that connect two disjoint sets of vertices and sparse cuts (i.e. those with a small number of edges) are a natural way to model graph partitions [9]. As each node of the wavelet tree separates a set of vertices into two subsets, a sparse wavelet tree can be approximately encoded by a sequence of sparse cuts. This work is the first effort to connect graph cuts and graph signal processing. In particu-lar, we show how problems that arise in the construction of optimal wavelet trees are related to hard cut problems, such as graph bisection [2] and multiway-cuts [7].

In Figure 1, we also show the cuts associated to each level of the wavelet tree together with the signal approximation for the respective level. Basis A can be approximately en-coded by the cutting four edges: { ( b,d ) , ( c,d ) } (level 1) and { ( e,f ) , ( e,g ) } (level 2). The resulting compact wavelet tree can effectively represent the graph signal using only the two top wavelet coefficients, leading to a relative L 2 error of 1%. On the other hand, basis B does not have such a compact approximation with small error via sparse cuts.

In this paper, we formalize the problem of computing sparse wavelet bases (or trees) for graph wavelet transforms. This problem, which we call sparse graph wavelet transform (SWT) consists of identifying a sequence of sparse graph cuts that leads to the minimum error in the reconstruction of a given graph signal. We show that this problem is NP-hard, even to approximate by a constant. In fact, we are able to show that computing each individual cut in the tree construction is an NP-hard problem.

As the main contribution of this paper, we propose a novel algorithm for computing an SWT via spectral graph theory. The algorithm design starts by formulating a relaxation of our problem as an eigenvector problem, which follows the lines of existing approaches for ratio-cuts [15], normalized-cuts [27] and max-cuts [33]. We further show how the pro-posed relaxation leads to a regularization of pairwise values by the graph Laplacian, which relates to existing work on graph kernels [31, 19]. In order to improve the computa-tional efficiency of our algorithm, we design a fast graph wavelet transform (FSWT) using several techniques includ-ing Chebyshev Polynomials and the Power method .
Generalizing the existing signal processing framework to signals that reside on graphs is the main focus of Signal Pro-cessing on Graphs (SPG) [28, 26]. Operations such as fil-tering, denoising, and downsampling, which are well-defined for signals in regular Euclidean spaces, have several appli-cations also when signals are embedded in sparse irregular spaces that can be naturally modeled as graphs. For in-stance, sensor networks [22], brain imaging [20], computer network traffic [6], and statistical learning [31, 19, 11], are examples of scenarios where graph signals have been studied. The main idea in SPG is the so called Graph Fourier Trans-form (GFT) [25], which consists of applying eigenvectors of the Laplacian matrix of a graph as a basis for graph signals. Laplacian eigenvectors oscillate at different frequencies over the graph structure, capturing a notion of frequency similar to complex exponentials in the standard Fourier Transform. As is the case for its counterpart for Euclidean spaces, GFT fails to localize graph signals, i.e. capture differences within graph regions. This aspect has motivated the study of graph wavelets [6, 13, 16, 5]. Crovella and Kolaczyk [6] introduced wavelets on graphs for the analysis of network traffic. Their design extracts differences in values within a disc (i.e. a center node and a fixed radius) and a surrounding ring as means to identify traffic anomalies. Coiffman and Maggioni [5] proposed a more sophisticated design, known as diffusion wavelets , based on compressed representations of dyadic powers of a diffusion operator. In [16], Hammond et al. present a wavelet design using kernel functions that modulate eigenvectors around vertices at multiple scales.
An assumption shared by existing work on graph wavelets is that good bases can be computed based solely on the graph structure. However, as shown in Figure 1, a proper choice of graph wavelet bases can lead to significantly more effective transforms. In this paper, we study the problem of comput-ing optimal graph wavelet bases for a given signal via sparse graph cuts. A graph cut partitions the vertices of a graph into two disjoint subsets and optimization problems associ-ated with graph cuts are some of the most traditional prob-lems in graph theory [8, 12]. In particular, graph cuts (e.g. min-cut, max-cut) are a natural way to formulate graph par-titioning problems [9]. Here, we constraint the size of the cut, in number of edges, associated to a graph wavelet ba-sis in order to discover bases that are well-embedded in the graph. A similar constraint also appears in the min-cut [8], graph bisection [2], and multiway-cut [7] problems.
Learning bases tailored for classes of signals is an impor-tant problem in signal processing, known as dictionary learn-ing [32]. This problem differs from ours since our wavelet bases are adapted to each signal, which leads to more com-pact representations. In [29], the authors show how impor-tance sampling can support the discovery of center-radius partitions for attribute compression. However, their ap-proach does not generalize to arbitrarily shaped partitions.
Many relevant problems on graphs have been solved using the framework of Spectral Graph Theory (SPG) [4], which studies combinatoric graph properties via the spectrum of matrices associated with them. For instance, the relation-ship between eigenvectors of the Laplacian and graph par-titions can be traced back to Cheeger X  X  inequality [3]. More recently, SPG has led to efficient graph partitioning algo-rithms (e.g. ratio-cuts [15], normalized-cuts [27]). In this paper, we propose a spectral algorithm for computing sparse graph wavelet bases. Interestingly, our analysis show that these bases are related to existing work on graph kernels [31, 19], including the wavelet design by Hammond et al. [16].
A graph is a tuple G ( V,E ), where V is a set of n vertices and E is a set of m (unweighted) edges, respectively. A signal W : V  X  R is a real-valued function defined on the set of vertices V . In other words, W ( v ) is the value of the signal for a vertex v  X  V . In Figure 1 we show an example of a graph G for which we define a signal W .

A graph wavelet tree is a binary tree structure X ( G ) that partitions the graph recursively as follows. A root node X contains all the vertices in the graph (i.e. X 1 1 general, X ` k  X  V is the k -th node at level ` with children X i and X and X ` +1 i  X  X ` +1 j = X ` k . We focus on binary trees since they have the same encoding power as n -ary trees in this model.
The tree X ( G ) defines spaces of functions, V ` and W analogous to Haar wavelet spaces in harmonic analysis [21]. The space V 1 contains functions that are constant on V . And, in general, V ` contains functions that are piecewise constant on nodes in X ` k at the ` -level of X ( G ). Let V be the space of functions that are constant on individual nodes in V . Bases to span such spaces can be constructed us-ing functions 1 X ` (box functions). This formulation leads to a multiresolution V 1  X  X  2  X  ... V for function spaces. Another set of function spaces in the form W ` contains wavelet functions  X  k,` with the following properties: (1) are piecewise constant on X and X ` +1 j , (2) are orthogonal to 1 X ` are 0 everywhere else. It follows that any function in W be represented using V ` +1 . Also, for any level ` , V ` and V `  X  X  ` = V ` +1 , where  X  is the orthogonal sum.
We combine wavelet functions with 1 V to produce an or-thonormal basis for G . Intuitively, this basis supports the representation of any graph signal W as a linear combina-tion of the average  X  ( W ) plus piecewise functions defined on recursive partitions of the vertices V (see Figure 1). A graph wavelet transform  X W is a set of difference coefficients a
In particular, except for a 0 , 0 , we can write a k,` as:
The sizes | X ` k | , | X ` +1 i | and | X ` +1 j | are taken into account because partitions might be unbalanced. Analogously, the wavelet inverse  X   X  1 W is defined as: where:
Figure 1a shows the graph wavelet transform for a toy ex-ample. For instance, the value of a 2 , 2 = (2 . (  X  4 + (  X  6))  X  2 . (  X  9 + (  X  9))) / 4 = 4 and the inverse  X   X  1 W ( e ) = 0 + (  X  28) / 4 + 4 / 2 + (  X  1) / 1 =  X  6 = W ( e ). An important prop-erty of the graph wavelet transform, known as Parseval X  X  relation, is that the signal and its transform are equivalent representations (i.e.  X   X  1  X W = W ) for any signal W and wavelet tree X ( G ). More formally, we can define the L energy of a graph wavelet coefficient as: Using Equation 2, we can show the Parseval X  X  relation:
In particular, a lossy compressed representation of W can be constructed by the following procedure: (1) Compute transform  X W , (2) set the lowest energy coefficients a to 0, (3) return the non-zero wavelet coefficients  X  0 W from  X W . In this setting, the error of the compression is the sum of the energies of the dropped coefficients. If W has a sparse representation in the transform (frequency domain), where most of the energy is concentrated in a few high-level coefficients, it can be compressed with small error.
Figure 1a illustrates a sparse representation of a graph sig-nal W (basis A). The fast decay of the difference coefficients a k,` in the wavelet transform as the level ` increases leads to a high compression using the aforementioned algorithm. The signal can be approximated within L 2 error of 1% using the top coefficients a 1 , 1 and a 2 , 2 . However, by keeping the top coefficients for basis B (Figure 1b), the error is 22%.
In [13] (see theorems 1-3), the authors show that, if the energy of a wavelet coefficient a k,` is bounded as a function of the size of its corresponding vertex set X ` k and the tree X ( G ) is almost balanced, then there is a sparse represen-tation of W as a wavelet transform. Here, we tackle the problem from a more practical and data-driven perspective, where a tree X ( G ) that leads to a sparse representation of W is unknown. Moreover, we add sparsity constraints to the description size of X ( G ) in order to enforce wavelet bases that are embedded in the graph structure. In the next sec-tion, we formalize the problem of computing wavelet basis using sparse cuts and characterize its hardness.
The existence of a good basis (or tree) for a signal W in a graph G provides relevant information about both W and G . We measure the description length of a wavelet tree X ( G ) as the size |X ( G ) | E of its edge cut. The edge cut of a wavelet tree is the number of edges in the set E 0  X  E that, if removed, separates the leaf nodes of X ( G ). In other words, there is no path between any pair of vertices u  X  X a i , v  X  X in G ( V,E  X  E 0 ) whenever X a i and X b j are leaves of X ( G ). A tree X ( G ) associated with a sparse cut requires a few edges to be removed in order to disconnect its leaf nodes.
If |X ( G ) | E &lt; | E | , the energy of at least one coefficient a k of any transform  X W will be always set to 0 and, as a consequence, the inverse  X   X  1  X W ( v ) will be the same for any vertex v  X  X ` k . As graphs have a combinatorial number of possible cuts, we formalize the problem of finding an optimal sparse wavelet basis in terms of ( L 2 ) error minimization.
Definition 1. Optimal graph wavelet basis via sparse cuts . Given a graph G ( V,E ) , a signal W , and a cut size q compute a wavelet tree X ( G ) with a cut |X ( G ) | that minimizes || W  X   X   X  1  X W || 2 .

Figure 2 shows two candidate wavelet trees with cut size q = 4 for the same graph signal shown previously in Figure 1a. While the tree from Figure 2b achieves an error of 22%, the one from Figure 2a is the optimal basis of cut size 4 for our example, with an error of 1%. As discussed in Section 3, a good basis generates sparse transforms, which maxi-mize the amount of energy from the signal that is conserved in a few coefficients. In the remainder of this section, we analyze the hardness of computing sparse wavelet bases by connecting it to well-known problems in graph theory.
Theorem 1. Computing an optimal graph wavelet basis is NP-hard.

Please refer to the extended version of this paper [30] for proofs of Theorems 1, 2 and 5. Theorem 1 shows that find-ing an optimal basis is NP-hard using a reduction from the 3-multiway cut problem [7], which leads to the question of whether such problem can be approximated within a con-stant factor in polynomial time. Theorem 2 shows that our problem is also NP-hard to approximate by any constant.
Theorem 2. Computing an optimal graph wavelet basis is NP-hard to approximate by a constant. Figure 2: Two graph wavelet bases with cut of size 4 for the same signal. Reconstructed values are set to leaf nodes. The basis from Figure 2a achieves 1% error and is optimal. An alternative basis with 22% error is shown in Figure 2b.
Connecting the construction of sparse wavelet basis to a hard problem such as the 3-multiway cut is a key step for proving Theorems 1 and 2. However, these constructions as-sume wavelet trees X ( G ) with a number of levels ` strictly larger than 2 (i.e. more than two partitions are generated). A final question we ask regarding the hardness of our prob-lem is whether there is an efficient algorithm for partitioning a set of nodes X ` k into children X ` +1 i and X ` +1 j . If so, one could apply such an algorithm recursively in a top-down manner in order to construct a reasonably good wavelet ba-sis. We can pose such a problem using the notion of L energy of graph wavelet coefficients from Equation 5.
Definition 2. Optimal graph wavelet cut . Given a graph G ( V,E ) , a signal W , a constant k , and a set of nodes X k  X  V , compute a partition of X ` k into X ` +1 i and X that maximizes || a k,` || 2 .

Theorem 3 rules out the existence of an efficient algorithm that solves the aforementioned problem optimally.

Theorem 3. Computing an optimal sparse graph wavelet cut is NP-hard.

Our proof (in the appendix) is based on a reduction from the graph bisection [12] and raises an interesting aspect of good graph wavelet bases, which is balancing . The problem of finding balanced partitions in graphs has been extensively studied in the literature, specially in the context of VLSI design [15], image segmentation [27] and other applications of spectral graph theory [4]. In the next section, we propose a spectral algorithm for computing graph wavelet bases.
Our approach combines structural and signal information as a vector optimization problem. By leveraging the power of spectral graph theory, we show how a relaxed version of this formulation is a regularized eigenvalue problem, which can be solved using 1-D search and existing eigenvalue com-putation procedures. Our discussion focuses on computing a single cut (Definition 2) and extends to the computation of a complete basis. Section 5.3 is focused on performance.
First, we introduce some notation. The degree d v of a vertex v is the number of vertices u  X  V such that ( u,v )  X  E . The degree matrix D of G is an n  X  n diagonal matrix with D v,v = d v for every v  X  V and D u,v = 0, for u 6 = v . The adjacency matrix A of G is an n  X  n matrix such that A u,v = 1 if ( u,v )  X  E and A u,v = 0, otherwise 1 . The Laplacian of G is defined as L = D  X  A . We also define a second matrix C = n I  X  1 n  X  n , where I is the identity matrix and 1 n  X  n is an n  X  n matrix of 1 X  X . The matrix C can be interpreted as the Laplacian of a complete graph with n vertices. The third matrix, which we call S , is a matrix of pairwise squared differences with S u,v = ( W ( u )  X  W ( v )) for any pair of nodes u,v  X  V . Notice that these matrices can also be computed for an induced subgraph G 0 ( X ` k ,E where E 0 = { ( u,v ) | u  X  X ` k  X  v  X  X ` k } .

In order to formulate the problem of finding an optimal sparse wavelet cut in vectorial form, we define a | X ` k sional indicator vector x for the partition of X ` k into X and X ` +1 j . For any v  X  X ` k , x v =  X  1 if v  X  X ` +1 if v  X  X ` +1 j . By combining the matrices ( C,S,L ) and the indicator vector x , the following Theorem shows how the problem from Definition 2 can be rewritten as an optimiza-tion problem over vectors (see appendix for the proof).
Theorem 4. The problem of finding an optimal sparse graph wavelet partition (Definition 2) can be written as: where a ( x ) = x | CSCx x | Cx and q is the maximum cut size.
Theorem 4 does not make the problem of computing an optimal wavelet basis any easier. However, we can now de-fine a relaxed version of our problem by removing the con-straint that x i  X  { X  1 , 1 } . Once real solutions ( x i allowed, we can compute an approximate basis using eigen-vectors of a well-designed matrix. The next corollary follows directly from a variable substitution and properties of La-grange multipliers in eigenvalue problems [10, chapter-12].
Corollary 1. A relaxed version of the problem from Def-inition 2 can be solved as a regularized eigenvalue problem:
Our method can be generalized to weighted graphs. where y  X  = min y y | My y | y , M = (( C +  X L ) + ) 1 2  X L ) + ) 1 2 ,y = ( C +  X L ) 1 2 x , ( C +  X L ) + is the pseudoinverse of ( C +  X L ) and  X  is a regularization factor.

This eigenvalue problem is well-defined due to properties of the matrix M , which is real and symmetric. In fact, M is negative semidefinite, since the energy || a k,` || 2 of a wavelet coefficient is non-negative. We apply the pseudoinverse ( C +  X L ) + because C and L are positive semidefinite and thus their standard inverses are not well-defined X  X hey both have at least one zero eigenvalue.

At this point, it is not clear how the matrix M captures both signal and structural information as means to produce high-energy sparse wavelet cuts. In particular, we want to provide a deeper insight into the role played by the regular-ization factor  X  in preventing partitions that are connected by many edges in G . To simplify the notation and without loss of generality, let us assume that X ` k = V and that V has 0-mean. The next theorem gives an explicit form for the entries of M based on the node values and graph structure: Theorem 5. The matrix M is in the form: where (  X  r ,e r ) is an eigenvalue-eigenvector pair of the matrix ( C +  X L ) such that  X  r &gt; 0 .

Based on Theorem 5, we can interpret M as a Lapla-cian regularized matrix and Expression 8 as a relaxation of a maximum-cut problem in a graph with Laplacian matrix  X  M . In this setting, the largest eigenvalue of  X  M is known to be a relaxation of the maximum cut in the corresponding graph. The matrix ( C +  X L ) is the Laplacian of a graph G 00 associated to G with the same set of vertices but edge weights w u,v = 1 +  X  if ( u,v )  X  G , and w u,v = 1, otherwise. Intuitively, as  X  increases, G 00 becomes a better representa-tion of a weighted version of G with Laplacian matrix  X L . For instance, if  X  = 0, G 00 is a complete graph with all non-zero eigenvalues equal to n and G has no effect over the weights of the cuts in M . In other words, the wavelet cut selected will simply maximize the sum of (negative) prod-ucts  X  W ( u ) .W ( v ) and separate nodes with different values. On the other hand, for large  X  , the eigenvalues  X  r will cap-ture the structure of G and have a large magnitude. The relative importance of a product  X  W ( u ) .W ( v ) will be re-duced whenever u and v are well-connected to nodes i and j , respectively, in G . As a consequence, the cuts selected will rather cover edge pairs ( i,j ) for which far away nodes u and v in G have different values for the signal W .
Expressions in the form P r g (  X  r ) e i e | i define regulariza-tions via the Laplacian, which have been studied in the con-text of kernels on graphs [31, 19] and also wavelets [16, 20].
Notice that the regularization factor  X  is not known a priori, which prevents the direct solution of the relaxation given by Expression 8. However, we can apply a simple 1-D search algorithm (e.g. golden section search [18]) in order to compute an approximate optimal  X  within a range [0 , X  max Algorithm 1 Spectral Algorithm Figure 3: Example of a cut of size q = 2 found by the spectral algorithm. The eigenvector x is rounded using a sweep procedure and the best wavelet cut is selected.
Algorithm 1 describes our spectral algorithm for comput-ing sparse graph wavelet cuts. Its inputs are the graph G , the signal W , a set of nodes X ` k from G , the regularization constant  X  , and the cut size q . As a result, it returns a cut ( X || a k,` || 2 and has at most q edges. The algorithm starts by constructing matrices C , L and S based on G and W (lines 1-3). The best relaxed cut x  X  is computed using Equation 8 (line 4) and a wavelet cut is obtained using a standard sweeping approach [27] (lines 5-6). Vertices in X ` k are sorted in non-decreasing order of their value in x  X  . For each value x , the algorithm generates a candidate cut ( X 1 ,X setting x v =  X  1 if v &lt; u , and x v = 1, otherwise (line 5). The cut with size | ( X ` +1 i ,X ` +1 j ) | at most q that maximizes the energy || a k,` || is selected among the candidate ones (line 6) and is returned by the algorithm.

Figure 3 illustrates a wavelet cut of size q = 2 discovered by our spectral algorithm. The input graph and its signal are given in Figure 3a. Moreover, we show the value of the eigenvector x that maximizes Expression 8 for each vertex and the resulting cut after rounding in Figure 3b. Notice that x captures both signal and structural information, as-signing similar values to vertices that have small difference regarding the signal and are near in the graph. The energy || a k,` || 2 associated with the cut is 457 (96% of the energy of the signal), which is optimal in this particular setting.
We evaluate Algorithm 1 using several datasets in our ex-periments. However, an open question is whether such an algorithm provides any quality guarantee regarding its solu-tion (for a single cut). One approach would be computing a lower bound on the L 2 energy of the wavelet cuts generated by the rounding algorithm, similar to the Cheeger X  X  inequal-ity for the sparsest cut [4]. Unfortunately, proving such a bound has shown to be quite challenging and will be left as future work. For a similar proof regarding an approximation for the max-cut problem, please refer to [33].

We apply Algorithm 1 recursively in order to construct a complete graph wavelet basis. Starting with the set of nodes V , we repeatedly compute new candidate wavelet cuts and select the one with maximum L 2 energy (i.e. it is a greedy algorithm). Once there is no feasible cut given the remaining budget of edges, we compute the remaining of the basis using ratio-cuts, which do not depend on the signal.
Here, we study the performance of the algorithm described in the previous section and describe how it can be approx-imated efficiently. Although performance is not the main focus of this paper, we still need to be able to compute wavelets on large graphs. The most complex step of Algo-rithm 1 is computing the matrix M (see Corollary 1), which involves (pseudo-)inverting and multiplying dense matrices. Moreover, the algorithm also requires the computation of the smallest eigenvalue/eigenvector of M .

A naive implementation of our spectral algorithm would take O ( n 3 ) time to compute the pseudo-inverse ( C +  X L ) O ( n 3 ) time for computing matrix products, and other O ( n time for the eigen-decomposition of M . Assuming that the the optimal value of  X  (Equation 10) is found in s itera-tions, the total complexity of this algorithm is O ( sn 3 would hardly enable the processing of graphs with more than a few thousand vertices. Therefore, we propose a fast ap-proximation of our algorithm by removing its dependence of  X  and using Chebyshev polynomials and the Power Method.
Our original algorithm searches for the optimal value of the regularization constant  X  using golden-search, which re-quires several iterations of Algorithm 1. However, our ob-servations have shown that typical values of  X  found by the search procedure are large, even for reasonable values of q , compared to the number of edges in G . Thus, we propose simplifying Equation 8 to the following:
As a consequence, we can compute a wavelet cut with a single execution of our spectral algorithm. Using Theorem 5, we can show that dropping the matrix C from the de-nominator has only a small effect over the resulting matrix M . First, consider the eigenvalue-eigenvector pairs (  X  of ( C +  X L ) and let (  X  l ,e l ) and (  X  c ,e c ) be the eigenvalue-eigenvector pairs for non-zero eigenvalues of L and C , re-spectively. Given that C is the Laplacian of a complete graph, we know that  X  c = n , for any c , and every vector orthogonal to the constant vector 1 n is an eigenvector of C . In particular, any eigenvector e l of L is an eigenvector of C . From the definition of eigenvalues/eigenvectors, we get that ( C +  X L ) e l = ( n +  X  X  l ) e l and thus ( n +  X  X  l eigenvalue-eigenvector pair of ( C +  X L ).
 Nevertheless, computing all the eigenvalues of the graph Laplacian L might still be prohibitive in practice. Thus, we avoid the eigen-decomposition by computing an approxi-mated version of M using Chebyshev polynomials [16]. These polynomials can efficiently approximate an expression in the form  X   X ,f  X  , where  X  i = P r g (  X  r ) e r,i e r,j and f is a real vec-tor. We can apply the same approach to approximate the product (( L + ) 1 2  X  CSC ) i,j by setting g and f as: where  X  r  X  [1 ,n ] and : ,j is an index for a matrix column.
Chebyshev polynomials can be computed iteratively with cost dominated by a matrix-vector multiplication by L . By truncating these polynomials to p terms (i.e. iterations), each one with cost O ( mn ), where m is the number of edges, and n is the number of nodes, we can approximate this ma-trix product in O ( pmn ) time. For sparse matrices ( m = O ( n )) and small p , pmn n 3 , which leads to significant performance gains over the naive approach. In order to compute M , we can repeat the same process with f = (( L + ) 1 2  X  CSC ) j, : , where j, : is an index for a matrix row.
Once the matrix M is constructed, it remains to compute its eigenvector associated to the smallest eigenvalue. A triv-ial solution would be computing all the eigenvectors of M , which can be performed in time O ( n 3 ). However, due to the fact that our matrix is negative semidefinite, its small-est eigenvector can be approximated more efficiently using the Power Method [14], which requires a few products of a vector and M . Assuming that such method converges to a good solution in t iterations, we can approximate the small-est eigenvalue of M in time O ( tn 2 ). Moreover, the compu-tation of x from y using ( L + ) 1 2 can also be performed via Chebyshev polynomials in time O ( pm ).

The time taken by our improved algorithm to compute a single cut is O ( pmn + tn 2 ), where p is the number of terms in the Chebyshev polynomial, m = | E | , n = | V | , and t is the number of iterations of the Power method. This complexity is a significant improvement over the O ( sn 3 ) time taken by its naive version whenever p , m , and t are small compared to n . For computing all the cuts, the total worst-case time complexity of the algorithm is O ( qpmn + qtn 2 ), where q is the size of the cut of the wavelet tree X ( G ). However, notice that good bases tend to be balanced (see Theorem 3) and in such case our complexity decreases to O ( pmn + tn 2 ).
We evaluate our algorithms for computing sparse wavelet bases using synthetic and real datasets. We start by ana-lyzing the scalability and quality of our efficient approxima-tion compared to the original algorithm. Next, we compare our approach against different baselines and using four real datasets in the signal compression task. This section ends with some visualizations of the sparse wavelet formulation, which provides further insights into our algorithm. All the implementations are available as open-source and we also provide the datasets applied in this evaluation 2 .
The results discussed in this section are based on a syn-thetic data generator for both the graph and an associated signal. Our goal is to produce inputs for which the best wavelet cut is known. The data generator can be summa-rized in the following steps: (1) Generate sets of nodes V and V 2 such that | V 1 | = | V 2 | ; (2) Generate m edges such that the probability of an edge connecting vertices in V 1 and V is given by a sparsity parameter h ; (3) Assign average values  X  1 and  X  2 to V 1 and V 2 , respectively, so that the energy of the cut ( V 1 ,V 2 ) is equal to an energy parameter  X  ; (4) Draw values from a Gaussian distribution N (  X  i , X  ) for each vertex set V i , where  X  is a noise parameter.

Proper values for the averages are computed using Equa-tion 13. We set default values for each parameter as follows: number of vertices n = 500 and edges m = 3 n , sparsity h = . 5, and noise  X  = |  X  i | . These parameters are varied in each experiment presented in Figure 4. For SWT, we fix the value of  X  max in the golden search to 1000 and, for the fast approximation (FSWT), we vary the number of Cheby-shev polynomials applied (5, 20, and 50). The number of iterations of the Power method to approximate the eigen-vectors of M is fixed at 10, which achieved good results in our experiments. Figure 4a compares FSWT and the orig-inal algorithm (SWT) varying the graph size ( n ), showing that FSWT is up to 100 times faster than SWT. In Fig-ures 4b-4d, we compare the approaches in terms of the en-ergy || a 1 , 1 || 2 of the first wavelet cut discovered varying the synthetic signal parameters. The results show that FSWT achieves similar or better results than SWT for relatively few coefficients ( p = 20) in all the settings.
We evaluate our spectral algorithm for sparse wavelet bases in the signal compression task. Given a graph G and a signal W , the goal is to compute a compact representation W 0 that minimizes the L 2 error ( || W  X  W 0 || 2 ). For the baselines, the size of the representation is the number of coefficients of the transform kept in the compression, relative to the size of the https://github.com/arleilps/sparse-wavelets dataset. We also take into the account the representation cost of the cuts (log( m ) bits/edge) for our approach. Datasets: Four datasets are applied in our evaluation. Small Traffic and Traffic are road networks from California for which vehicle speeds  X  X easured by sensors X  are modeled as a signal, with n = 100 and m = 200, and n = 2 K and m = 6 K , respectively [23]. Human is a gene network for Homo Sapiens with expression values as a signal where n = 1 K and m = 1 K [24]. Wiki is a sample of Wikipedia pages where the (undirected) link structure defines the graph and the signal is the number of page views for each page with n = 5 K and m = 25 K . Blogs is a network of blogs with political leaning (-1 for left and 1 for right) as vertex attributes [1] ( n = 1 K and m = 17 K ). Notice that these graphs have sizes in the same scale as the ones applied by existing work on signal processing on graphs [28, 13, 11]. We normalize the values to the interval [0 , 1] to make the comparisons easier.
Baselines: We consider the Graph Fourier Transform (FT) [25, 28] and the wavelet designs by Hammond et al. (HWT) [16] and Gavish et al. (GWT) [13] as baselines. In-stead of the original bottom-up partitioning algorithm pro-posed for GWT, we apply ratio-cuts [15], which is more scal-able and achieves comparable results in practice.
 Figure 6a shows compression results for Small Traffic . The best baselines (GWT and FT) incur up to 5 times larger error than our approaches (SWT and FSWT). Figures 5a-5d show the results for FSWT, GWT, and FT using the remaining datasets. Experiments for HWT and SWT took too long to finish and were terminated. FSWT outperforms the baselines in most of the settings, achieving up to 5, 6, 2, and 80 times lower error than the best baseline (GWT) for Traffic , Human , Wikipedia , and Blogs , respectively. FT per-forms surprisingly well for Blogs because vertex values are almost perfectly separated into two communities, and thus some low frequency eigenvectors are expected to approxi-mately match the separation (see [1, Fig. 3]). As the size of the representation increases, FSWT is the only method able to separate values at the border of the communities.
These results offer strong evidence that our sparse wavelet bases can effectively encode both the graph structure and the signal. The main advantage of our approach is building bases that are adapted to the signal by cutting few edges in the graph. The compression times of our algorithm are comparable with the baselines, as shown in Table 6b.
Finally, we illustrate some interesting features of our sparse wavelet formulation using graph drawing. Eigenvectors of Figure 6: Compression results for Small Traffic and com-pression times for all methods and the datasets. Our ap-proaches (SWT and FSWT) outperform the baselines while taking comparable compression time. the Laplacian matrix are known to capture the community structure of graphs, and thus can be used to project vertices in space. In particular, if e 2 and e 3 are the second ( Fiedler ) and the third eigenvectors of the Laplacian matrix, we can draw a graph in 2-D by setting each vertex v i  X  V to the position ( e 2 ( i ) ,e 3 ( i )). Following the same approach, we ap-ply the smallest eigenvectors of the matrix M (see Corollary 1) to draw graphs based on both the structure and a signal.
Figure 7 presents drawings for two graphs, one is the tra-ditional Zachary X  X  Karate club network with a synthetic heat signal starting inside one community and the other is Small Traffic . Three different drawing approaches are applied: (1) The Scalable Force Directed Placement (SFDP) [17] 3 Laplacian eigenvectors, and the wavelet eigenvectors. Both SFDP and the Laplacian are based on the graph structure only. The drawings demonstrate how our wavelet formula-tion separates vertices based on both values and structure.
Signal Processing in Graphs (SPG) is a powerful frame-work for modeling complex data arising from several applica-
Implemented by GraphViz : http://www.graphviz.org/ Figure 7: Drawing graphs using SFDP (a,d) and Laplacian (b,e) and wavelet eigenvectors (c,f). Vertices are colored based on values (red=high, green=average and blue=low). Different from the other schemes, wavelet eigenvectors are based on both signal and structure (better seen in color). tions. A major challenge in SPG is relating properties of the graph signal, the graph structure and the transform. Graph wavelets are able to effectively model a smooth graph signal conditioned to the existence of a hierarchical partitioning of the graph that captures the geometry of the graph structure as well as the signal. Our work is the first effort to build such hierarchies in a compact fashion. We first introduced the problem of computing graph wavelet bases via sparse cuts and show that it is NP-hard X  X ven to approximate by a constant X  X y connecting it to existing problems in graph theory. Then, we proposed a novel algorithm for comput-ing sparse wavelet bases by solving regularized eigenvalue problems using spectral graph theory. While naively con-sidering both structure and values can lead to computation-ally intensive operations, we introduced an efficient solution using several techniques. These approaches are extensively evaluated using real and synthetic datasets and the results provide strong evidence that our solution produces compact and accurate representations for graph signals in practice. This work opens several lines for future investigation: (i) It remains an open question whether approximating a single optimal wavelet cut is NP-hard. (ii) The wavelet design ap-plied in this work maps only to a particular type of wavelets ( Haar ); extending our approach to other wavelet functions (e.g. Mexican hat, Meyer [21]) might lead to better repre-sentations for particular classes of signals. (iii) Generalizing the ideas presented here to time-varying graph signals might lead to novel algorithms for anomaly detection, event discov-ery, and data compression.
 Acknowledgment . Research was sponsored by the Army Research Laboratory and was accomplished under Coopera-tive Agreement Number W911NF-09-2-0053 (the ARL Net-work Science CTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either ex-pressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] L. A. Adamic and N. Glance. The political [2] T. Bui, S. Chaudhuri, F. Leighton, and M. Sipser. [3] J. Cheeger. A lower bound for the smallest eigenvalue [4] F. R. Chung. Spectral graph theory . American [5] R. Coifman and M. Maggioni. Diffusion wavelets. [6] M. Crovella and E. Kolaczyk. Graph wavelets for [7] E. Dahlhaus, D. Johnson, C. Papadimitriou, [8] J. Edmonds and R. M. Karp. Theoretical [9] S. Fortunato. Community detection in graphs. Physics [10] J. Friedman, T. Hastie, and R. Tibshirani. The [11] A. Gadde, A. Anis, and A. Ortega. Active [12] M. R. Garey and D. S. Johnson. Computers and [13] M. Gavish, B. Nadler, and R. Coifman. Multiscale [14] G. H. Golub and C. F. Van Loan. Matrix [15] L. Hagen and A. B. Kahng. New spectral methods for [16] D. Hammond, P. Vandergheynst, and R. Gribonval. [17] Y. Hu. Efficient, high-quality force-directed graph [18] J. Kiefer. Sequential minimax search for a maximum. [19] J. Lafferty and G. Lebanon. Diffusion kernels on [20] N. Leonardi and D. Van De Ville. Tight wavelet [21] S. Mallat. A wavelet tour of signal processing . [22] D. Mohan, M. T. Asif, N. Mitrovic, J. Dauwels, and [23] M. Mongiovi, P. Bogdanov, and A. Singh. Mining [24] F. Moser, R. Colak, A. Rafiey, and M. Ester. Mining [25] A. Sandryhaila and J. Moura. Discrete signal [26] A. Sandryhaila and J. Moura. Big data analysis with [27] J. Shi and J. Malik. Normalized cuts and image [28] D. Shuman, S. Narang, P. Frossard, A. Ortega, and [29] A. Silva, P. Bogdanov, and A. K. Singh. Hierarchical [30] A. Silva, X.-H. Dang, P. Basu, A. Singh, and [31] A. Smola and R. Kondor. Kernels and regularization [32] I. To X si  X c and P. Frossard. Dictionary learning. IEEE [33] L. Trevisan. Max cut and the smallest eigenvalue. Proofs for Theorems 1, 2 and 5 are provided in [30]. Proof of Theorem 3
Proof. We use a reduction from graph bisection , which given a graph G 0 ( V 0 ,E 0 ) and a constant q , asks whether there is a set of q edges in E 0 that, if removed, would break G into two equal parts (assume | V 0 | is even). Graph bisec-tion is NP-complete [12]. By substituting Expression 2 in Expression 5, we obtain the following expression:
For a given instance of the graph bisection problem, we generate | V 0 | ( | V 0 | X  1) / 2 instances of the sparse wavelet basis problem, one for each pair of vertices ( u,v ) in V . Set V = V 0  X  X  s,t } , E = E 0  X  X  ( s,u ) , ( v,t ) } , W ( s ) = 1, W ( t ) =  X  1, and W ( u ) = 0 for u  X  V 0 . From Expression 13, we get that s and t have to be separate from each other in an optimal partitioning. Moreover, since | X ` +1 i | + | X ` +1 j | is fixed, the energy is maximized when the partitions have equal size, with value 4( | V 0 | + 1) 2 / ( | V 0 | + 2) 2 .
 Proof of Theorem 4
Proof. We start by rewriting Expression 5 in terms of pairwise differences (we drop the index ` ): | X k | is a constant and can be dropped from the denomi-nator. x | Cx is the quadratic form of the Laplacian C , thus:
Similarly, x | Lx is the standard quadratic form for the size of the cut between two partitions in G : x
Lx = X
Regarding x | CSCx : x | C = [ x 1 ...x n ]  X  ( x | C ) b can take two possible values, depending on x b
Also Cx = ( x | C | ) | = ( x | C ) | . Therefore, x | CSCx is also a quadratic form for the matrix S : where z = Cx . This ends the proof.
