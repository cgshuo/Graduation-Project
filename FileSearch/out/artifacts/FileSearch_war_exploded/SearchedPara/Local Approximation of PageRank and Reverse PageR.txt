 We consider the problem of approximating the PageRank of a tar-get node using only local information provided by a link server. We prove that local approximation of PageRank is feasible if and only if the graph has low in-degree and admits fast PageRank conver-gence. While natural graphs, such as the web graph, are abundant with high in-degree nodes, making local PageRank approximation too costly, we show that reverse natural graphs tend to have low in-degree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasi-ble than computing PageRank locally. Finally, we demonstrate the usefulness of Reverse PageRank in five different applications. Categories and Subject Descriptors: H.3.3: Information Search and Retrieval.
 General Terms: Algorithms.
 Keywords: PageRank, reverse PageRank, lower bounds, local ap-proximation.
Over the past decade PageRank [6] has become one of the most popular methods for ranking nodes by their  X  X rominence X  in a net-work. 1 PageRank X  X  underlying idea is simple but powerful: a promi-nent node is one that is  X  X upported X  (linked to) by other prominent nodes. PageRank was originally introduced as means for ranking web pages in search results. Since then it has found uses in many other domains, such measuring centrality in social networks, evalu-ating the importance of scientific publications, prioritizing pages in a crawler X  X  frontier, personalizing search results, combating spam, measuring trust, selecting pages for indexing, and more. While the significance of PageRank in ranking search results seems to have diminished, due to the emergence of other effective alternatives (e.g., clickthrough-based measures), it is still an important tool in search infrastructure, social networks, and analysis of large graphs. Local PageRank approximation. The vast majority of algorithms for computing PageRank, whether they are centralized, parallel, or decentralized, have focused on global computation of the PageR- X  ank vector. That is, PageRank scores for all the graph X  X  nodes are computed. While in many applications of PageRank a global com-putation is needed, there are situations in which one is interested in computing PageRank scores for just a small subset of the nodes.
Consider, for instance, a web site owner (e.g, a small or a large business), who would like to promote the web site in search engine rankings in order to attract traffic of potential clients. As PageR-ank is used by search engines to determine whether to crawl/index pages and to calculate their relevance scores, tracking the PageR-ank of the web site would enable the web site owner to better un-derstand its position in search engine rankings and potentially take actions to improve the web site PageRank. In this case, the web site owner is interested only in the PageRank score of his own web site (and maybe also in the scores of his competitors X  web sites), but not in the PageRank scores of all other web pages.

Major search engines choose to keep the PageRank scores of web pages confidential, since there are many variations of the PageRank formula, and making the exact PageRank values public may enable spammers to promote illegitimate web sites. Some search engines publish crude PageRank values (e.g., through the Google Toolbar), but these are usually given in a 1 to 10 logarithmic scale. Users who wish to obtain more accurate PageRank scores for pages of their choice are left to compute them on their own. Global PageRank computation for the entire web graph is out of the question for most users, as it requires significant resources and knowhow. This brings up the following natural question: can one compute the PageRank score of a single web page using reasonable resources?
The same question arises in other natural contexts, where PageR-ank is used. For example, a Facebook 2 user may be interested in measuring her PageRank popularity by probing the friendship graph. Can this be done efficiently without traversing the whole network?
Chen, Gan, and Suel [1] were the first to introduce the problem of local PageRank approximation . Suppose we are given access to a large graph G through a link server , which for every given query node x , returns the edges incident to x (both incoming and outgoing). 3 Can we then use a small number of queries to the link server to approximate the PageRank score of a target node x to within high precision?
Chen et al. proposed an algorithm for solving this problem. Their algorithm crawls backwards a small subgraph around the target node, applies various heuristics to guess the PageRank scores of the nodes at the boundary of this subgraph, and then computes the PageRank of the target node within this subgraph. Chen et al. em-pirically showed this algorithm to provide good approximations on average. However, they noted that high in-degree nodes sometimes make the algorithm either very expensive or inaccurate.
 Lower bounds. In this work we study the limits of local PageRank approximation. We identify two factors that make local PageRank approximation hard on certain graphs: (1) the existence of high in-degree nodes; (2) slow convergence of the PageRank random walk. 4
In order to demonstrate the effect of high in-degree nodes, we exhibit for every n a family of graphs of size n whose maximum in-degree is high (  X ( to send  X ( PageRank approximations. For very large n , fetching from the network or sending costly (for example, for the web graph n  X  10 B , and thus 128 K ). The lower bound we prove applies to both randomized and deterministic algorithms. For deterministic algorithms, we are able to prove an even stronger (and optimal)  X ( n ) lower bound.
Similarly, to demonstrate the effect of slow PageRank conver-gence, we present a family of graphs on which the PageRank ran-dom walk converges rather slowly (in  X (log n ) steps) and on which every algorithm needs to submit  X ( n 1 2  X   X  ) queries in order to ob-tain good PageRank approximations (  X  &gt; 0 is a small constant that depends on the PageRank damping factor). Again, this lower bound holds for both randomized and deterministic algorithms. For deterministic algorithms, we show an optimal  X ( n ) lower bound.
We note that the two lower bounds do not subsume each other, as the family of hard graphs constructed in the first lower bound has very fast PageRank convergence (2 iterations), while the family of hard graphs constructed in the second lower bound has bounded in-degree (2).
 Sufficiency. Having proved that local PageRank approximation is hard for graphs that either have high in-degree or do not ad-mit quick PageRank convergence, it is natural to ask whether lo-cal PageRank approximation is feasible for graphs of bounded in-degree and on which PageRank converges quickly. We present a variation of the algorithm by Chen et al. and prove that it works well for such graphs: if the PageRank random walk converges on the graph in r steps, then the algorithm needs to crawl backwards a subgraph of radius r around the target node. In particular, if the maximum in-degree in this subgraph is d , then the algorithm re-quires at most d r queries to the link server. This demonstrates that the two conditions we showed to be necessary for fast local PageR-ank approximation are also sufficient.
 PageRank vs. Reverse PageRank. As natural graphs, like the web graph and social networks, are abundant with high in-degree nodes, our first lower bound suggests that local PageRank approx-imation is frequently infeasible to do on such graphs. We substan-tiate this observation with an empirical analysis of a 280,000 crawl of the www.stanford.edu site. 5 We show that this graph has a rela-tively high in-degree (38,606) and as a result the sizes of subgraphs of even small radii around target nodes tend to be high. These findings provide analytical and empirical explanations for the diffi-culties encountered by Chen et al .

We then demonstrate that reverse natural graphs (the graphs ob-tained by reversing the directions of all links) are more suitable for local PageRank approximation. By analyzing the stanford.edu crawl, we show that the reverse web graph, like the web graph, ad-mits quick PageRank convergence (on 80% nodes of the reverse graph, PageRank converged within 20 iterations). We also show that the reverse graph has a much lower in-degree (only 255 in the stanford.edu crawl), and thus crawl growth rates around tar-get nodes are moderate. These findings hint that local PageRank approximation should be feasible on the reverse graph.

To test this hypothesis, we measured the performance of our al-gorithm on a sample of nodes from the stanford.edu graph. To do the comparison, we selected random nodes from the graph as fol-lows. We ordered all the nodes in the graph by their PageRank, from highest to lowest. We divided the nodes into buckets of expo-nentially increasing sizes (the first bucket had the top 12 nodes, the second one had the next 24 nodes, and so on). We picked from each bucket 100 random nodes (if the bucket was smaller we took all its nodes). We then calculated, for each bucket, the average cost (num-ber of queries to the link server) of the runs on samples from that bucket. The plot for the reverse graph was constructed analogously.
The results of this experiment show that the cost of the algorithm on the reverse graph is significantly lower than on the regular graph, especially for highly ranked nodes. For example, the average cost of the algorithm on the first bucket of PageRank was three times higher than the cost of the algorithm on the first bucket of Reverse PageRank (PageRank of the reverse graph;  X  X PR X  in short). On the other hand, for the low ranked nodes, the performance of the algorithm on the reverse graph and on the regular graph are almost the same. For example, the average cost of the algorithm on the last bucket of PageRank was 13 and for Reverse PageRank it was 14.
We conclude from the above that computing Reverse PageRank is more feasible to do locally than computing regular PageRank. Social networks and other natural graphs possess similar properties to the web graph (power law degrees, high in-degree vs. low out-degree) and are thus expected to exhibit similar behavior. Applications of Reverse PageRank. While locally approximat-ing RPR is easier than locally approximating PageRank, why would one want to compute RPR in the first place? We observe that RPR has a multitude of applications. It has been used before to select good seeds for the TrustRank measure [3], to detect highly influ-ential nodes in social networks [5], and to find hubs in the web graph [2]. We present two additional novel applications of RPR: (1) finding good seeds for crawling; and (2) measuring the  X  X eman-tic relatedness X  of concepts in a taxonomy.
 We consider the problem of approximating the PageRank of a target node using only local information provided by a link server. This problem was originally studied by Chen, Gan, and Suel (CIKM 2004), who presented an algorithm for tackling it. We prove that local approximation of PageRank, even to within modest approxi-mation factors, is infeasible in the worst-case, as it requires probing the link server for  X ( n ) nodes, where n is the size of the graph. The difficulty emanates from nodes of high in-degree and/or from slow convergence of the PageRank random walk.

We show that when the graph has bounded in-degree and admits fast PageRank convergence, then local PageRank approximation can be done using a small number of queries. Unfortunately, natu-ral graphs, such as the web graph, are abundant with high in-degree nodes, making this algorithm (or any other local approximation algorithm) too costly. On the other hand, reverse natural graphs tend to have low in-degree while maintaining fast PageRank con-vergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally.
We demonstrate that Reverse PageRank is useful for several ap-plications, including computation of hub scores for web pages, finding influencers in social networks, obtaining good seeds for crawling, and measurement of semantic relatedness between con-cepts in a taxonomy.
 Categories and Subject Descriptors: H.3.3: Information Search and Retrieval.
 General Terms: Algorithms.
 Keywords: PageRank, reverse PageRank, lower bounds, local ap-proximation.  X 
Over the past decade PageRank [27] has become one of the most popular methods for ranking nodes by their  X  X rominence X  in a net-work. 1 PageRank X  X  underlying idea is simple but powerful: a promi-nent node is one that is  X  X upported X  (linked to) by other prominent nodes. PageRank was originally introduced as means for rank-ing web pages in search results. Since then it has found uses in many other domains, such as measuring centrality in social net-works [20], evaluating the importance of scientific publications, prioritizing pages in a crawler X  X  frontier [10], personalizing search results [8], combating spam [17], measuring trust, selecting pages for indexing, and more. While the significance of PageRank in ranking search results seems to have diminished, due to the emer-gence of other effective alternatives (e.g., clickthrough-based mea-sures), it is still an important tool in search infrastructure, social networks, and analysis of large graphs.
 Local PageRank approximation. The vast majority of algorithms for computing PageRank, whether they are centralized, parallel, or decentralized, have focused on global computation of the PageR-ank vector. That is, PageRank scores for all the graph X  X  nodes are computed. While in many applications of PageRank a global com-putation is needed, there are situations in which one is interested in computing PageRank scores for just a small subset of the nodes.
Consider, for instance, a web site owner (e.g, a small or a large business), who would like to promote the web site in search engine rankings in order to attract traffic of potential clients. As PageR-ank is used by search engines to determine whether to crawl/index pages and to calculate their relevance scores, tracking the PageR-ank of the web site would enable the web site owner to better un-derstand its position in search engine rankings and potentially take actions to improve the web site X  X  PageRank. In this case, the web site owner is interested only in the PageRank score of his own web site (and maybe also in the scores of his competitors X  web sites), but not in the PageRank scores of all other web pages.

Major search engines choose to keep the PageRank scores of web pages confidential, since there are many variations of the PageRank formula, and making the exact PageRank values public may enable spammers to promote illegitimate web sites. Some search engines publish crude PageRank values (e.g., through the Google Toolbar ), but these are usually given in a 1 to 10 logarithmic scale. Users who wish to obtain more accurate PageRank scores for pages of their choice are left to compute them on their own. Global PageRank computation for the entire web graph is out of the question for most users, as it requires significant resources and knowhow. This brings up the following natural question: can one compute the PageRank score of a single web page using reasonable resources?
The same question arises in other contexts, where PageRank is used. For example, a Facebook 2 user may be interested in measur-ing her PageRank popularity by probing the friendship graph. Can this be done efficiently without traversing the whole network? Chen et al. [9] were the first to introduce the problem of local PageRank approximation . Suppose we have an access to a large graph G through a link server 3 , which for every given query node x , returns incoming and outgoing edges incident to x . 4 use a small number of queries to the link server to approximate the PageRank score of a target node x with high precision?
Chen et al. proposed an algorithm for solving this problem. Their algorithm crawls backwards a small subgraph around the target node, applies various heuristics to guess the PageRank scores of the nodes at the boundary of this subgraph, and then computes the PageRank of the target node within this subgraph. Chen et al. em-pirically showed this algorithm to provide good approximations on average. However, they noted that high in-degree nodes sometimes make the algorithm either very expensive or inaccurate. Lower bounds. We study the limits of local PageRank approxi-mation. We identify two factors that make local PageRank approx-imation hard on certain graphs: (1) the existence of high in-degree nodes; (2) slow convergence of the PageRank random walk. 5
In order to demonstrate the effect of high in-degree nodes, we exhibit for every n a family of graphs of size n whose maximum in-degree is high (  X ( n ) ) and on which any algorithm would need to send  X ( PageRank approximations. For very large n , fetching from the network or sending costly (for example, for the web graph n  X  10 B , and thus 128 K ). The lower bound we prove applies to both randomized and deterministic algorithms. For deterministic algorithms, we are able to prove an even stronger (and optimal)  X ( n ) lower bound.
Similarly, to demonstrate the effect of slow PageRank conver-gence, we present a family of graphs on which the PageRank ran-dom walk converges rather slowly (in  X (log n ) steps) and on which every algorithm needs to submit  X ( n 1 2  X  ) queries in order to ob-tain good PageRank approximations ( &gt; 0 is a small constant that depends on the PageRank damping factor). Again, this lower bound holds for both randomized and deterministic algorithms. For deterministic algorithms, we show an optimal  X ( n ) lower bound.
We note that the two lower bounds do not subsume each other, as the family of hard graphs constructed in the first bound has very fast PageRank convergence (2 iterations), while the family of hard graphs constructed in the second bound has bounded in-degree (2). Sufficiency. Having proved that local PageRank approximation is hard for graphs that have high in-degree or do not admit quick PageRank convergence, it is natural to ask whether local PageRank approximation is feasible for graphs of bounded in-degree and on which PageRank converges quickly. We observe that a variation of the algorithm of Chen et al. works well for such graphs: if the PageRank random walk converges on the graph in r steps and if the maximum in-degree of the graph is d , then the algorithm crawls a subgraph of size at most d r and thus requires at most this number of queries to the link server. When d and r are small, the algorithm is efficient. This demonstrates that the conditions we showed to be necessary for fast local PageRank approximation are also sufficient. PageRank vs. Reverse PageRank. As natural graphs, like the web graph and social networks, are abundant with high in-degree nodes, our first lower bound suggests that local PageRank approx-imation is frequently infeasible on such graphs. We substantiate this observation with an empirical analysis of a 280,000 crawl of the www.stanford.edu site. We show that locally approximat-ing PageRank is especially difficult for the high PageRank nodes. These findings provide analytical and empirical explanations for the difficulties encountered by Chen et al .

We then demonstrate that reverse natural graphs (the graphs ob-tained by reversing the directions of all links) are more suitable for local PageRank approximation. By analyzing the stanford. edu crawl, we show that the reverse web graph, like the web graph, admits quick PageRank convergence (on 80% nodes of the reverse graph, PageRank converged within 20 iterations). We also show that the reverse graph has low in-degree (only 255 as opposed to 38,606 in the regular graph). These findings hint that local PageR-ank approximation should be feasible on the reverse graph.
To put this hypothesis to test, we measured the performance of our variation of the Chen et al. algorithm on a sample of nodes from the stanford.edu graph. We show that for highly ranked nodes the performance of the algorithm on the reverse graph is up to three times better than on the regular graph.

We conclude from the above that the reverse web graph is much more amenable to efficient local PageRank approximation than the regular web graph. Thus, computing Reverse PageRank (PageRank of the reverse graph;  X  X PR X  in short) is more feasible to do locally than computing regular PageRank. Social networks and other nat-ural graphs possess similar properties to the web graph (power law degrees, high in-degree vs. low out-degree) and are thus expected to exhibit similar behavior.
 Applications of Reverse PageRank. While locally approximating RPR is easier than locally approximating PageRank, why would one want to compute RPR in the first place? We observe that RPR has a multitude of applications: it has been used to select good seeds for the TrustRank measure [17], to detect highly influ-ential nodes in social networks [20], and to find hubs in the web graph [15]. We present two additional novel applications of RPR: (1) finding good seeds for crawling; and (2) measuring the  X  X eman-tic relatedness X  of concepts in a taxonomy. In three of the above applications of RPR, local computation is useful: in estimating the influence score of a given node in a social network, in computing the hub score of a given page on the web, and in measuring the semantic relatedness of two given given concepts.
There is a large body of work on PageRank computation, varying from centralized algorithms (e.g., [22, 5]), to parallel algorithms (e.g., [24, 23]), to decentralized P2P algorithms (e.g., [34, 28]). All of these are designed to compute the whole PageRank vector and are thus not directly applicable to our problem. See a survey by Berkhin [3] for an extensive review of PageRank computation techniques.

Apart from Chen et al. , also Davis and Dhillon [12] and Wu [35] consider computations of global PageRank values over subgraphs. These two works, however, do not rely on a link server and thus work in a different model than what we consider in this paper. PageRank overview. Let G =( V, E ) be a directed graph on n nodes. Let M be the n  X  n probability transition matrix of the simple random walk on G : Let U be the probability transitio n matrix of the uniform random walk, in which at each step a node is chosen uniformly at random independently of the history: U ( u, v )= 1 n .
 Given a damping factor 0  X   X   X  1 , PageRank [27] (denoted PR
G (  X  ) ) is defined as the limit distribution of the random walk in-duced by the following convex combination of M and U :  X  =0 . 85 is a typical choice, and we use it in our experiments. Personalized PageRank [18, 21] is a popular generalization of PageR-ank, in which the uniform distribution in U is replaced by a differ-ent,  X  X ersonalized X , distribution. Everything we do in this paper can be rather easily generalized to the personalized case. For sim-plicity of exposition we choose to stick to the uniform case. Local PageRank approximation. A local algorithm working on an input graph G is given access to G only through a  X  X ink server X . Given an id of a node u  X  V , the server returns the IDs of u  X  X  neighbors in G (both in-neighbors and out-neighbors).

D EFINITION 1. An algorithm is said to locally approximate PageR-ank , if for any graph G =( V, E ) , for which it has local access, any target node u  X  V , and any error parameter &gt; 0 , the algorithm outputs a value PR ( u ) satisfying: If the algorithm is randomized, it is required to output, for any inputs G, u, ,a 1  X  approximation of PR G ( u ) with probability at least 1  X   X  ,where 0 &lt; X &lt; 1 is the algorithm X  X  confidence parameter . The probab ility is over the algorithm X  X  internal coin tosses.

We measure the performance of such algorithms in terms of their query cost , which is the number of queries they send to the link server for the worst-case choice of graph G and target node u . Typ-ically, the actual resources used by these algorithms (time, space, bandwidth) are proportional to their query cost. We will view poly-logarithmic cost ( O (log O (1) ( n )) as feasible and polynomial cost (  X ( n 1  X  ) for some &gt; 0 ) as infeasible.
 PageRank and influence. Jeh and Widom [21] provide a useful characterization of PageRank in term of the notion of  X  X nfluence X . We present a different variation of influence, which divides the in-fluence of a node into layers. This will make the analysis easier.
The influence [9] of a node v  X  G on the PageRank of u  X  G is the fraction of the PageRank score of v that flows into u , excluding the effect of decay due to the damping factor  X  :
D EFINITION 2. For a path p =( u 0 ,u 1 ,...,u t ) , define Let paths t ( v, u ) be the set of all paths of length t from v to u .The influence of v on u at radius t is: (For t =0 , we define inf 0 ( u, u )=1 and inf 0 ( v, u )=0 , for all v = u .) The total influence of v on u is: Note that the same node v may have influence on u at several differ-ent radii. Using influence, we define the PageRank value of u at ra-represents the cumulative PageRank score that flows into u from nodes at distance at most r from u . We show below a characteriza-tion of PageRank in terms of influence, which is similar to the one appearing in the work of Jeh and Widom [21].

T HEOREM 3. For every node u  X  G , PR G ( u ) = lim r  X  X  X  PR G P ROOF . First, let us express PR as a power series in terms of and M : L EMMA 4. For every r  X  1 , The proof of the lemma can be found in the full version of the paper 6 . Assuming the correctness of the lemma: As M r +1 is a probability transition matrix, M r +1 (1 ,u ) thus  X  r +1 M r +1 &lt; 1 . Therefore, as r  X  X  X  ,  X  r +1 M 0 .Thismeansthat: lim r  X  X  X  PR G r ( u ) = lim r  X  X  X  P r +1
Now Consider the initial distribution p 0 =(1 , 0 ,..., 0) ,and let p r = p 0 P r . Recall that lim r  X  X  X  ( p r )= PR G . In particular, lim r  X  X  X  ( p r ( u )) = PR G ( u ) .As p r ( u )= P r (1 ,u ) , we conclude that: lim r  X  X  X  PR G r ( u )= PR G ( u ) .

Note that PR G r ( u ) approaches PR G ( u ) from below. Throughout this paper, we will use the following notion of influence conver-gence rate, which is reminiscent of the standard mixing time [32] of Markov Chains:
D EFINITION 5. For a graph G , a target node u , and an approx-imation parameter &gt; 0 , define the pointwise influence mixing time as: The standard convergence rate of PageRank is defined as the rate at which the rows of the matrix P r approach the PageRank vector as r  X  X  X  . Lemma 4 implies that the difference from the above notion of mixing time is at most O (log(1 / )) and thus the two notions are essentially equivalent. Therefore, for the rest of the paper when we say that  X  X he PageRank random walk converges in r iterations on a node u  X , we will actually mean that T ( G, u )
In this section we present four lower bounds on the query com-plexity of local PageRank approximation, which demonstrate the two major sources of hardness for this problem: high in-degrees and slow PageRank convergence. The first two lower bounds (one for randomized and another for deterministic algorithms) address high in-degrees, while the other two address slow PageRank con-vergence. For lack of space, we provide a proof of only the first lower bound. The other proofs appear in the full draft of this paper. High in degree. The first two lower bounds demonstrate that graphs with high in-degree can be hard for local PageRank approx-imation. The hard instances constructed in the proofs are 3-level  X  X ree-like X  graphs with ve ry high branching factors. 7 Thus, PageR-ank converges very quickly on these graphs (in merely 2 iterations), yet local PageRank approximation requires lots of queries, due to the high degrees. The first lower bound (  X ( gorithm, even randomized ones, and the second lower bound (an optimal  X ( n ) ) holds for deterministic algorithms only.
T HEOREM 6. Fix any  X   X  (0 , 1) ,  X   X  (0 , 1) , and  X  (0 , Let A be an algorithm that locally approximates PageRank to within relative error and confidence 1  X   X  . Then, for every sufficiently large n , there exists a graph G on at most n nodes and a node u G on which A uses  X ( in-degree of G is  X ( iterations on G .

P ROOF . We prove the lower bound by a reduction from the OR problem. In the OR problem, an algorithm is given a vector bits ( x 1 ,...,x m ) , and is required to output the OR x x order to recover any bit x i , the algorithm must send a query to an external server. The goal is to compute the OR with as few queries to the server as possible. A simple sensitivity argument (cf. [6, 1]) shows that m (1  X  2  X  ) queries are needed for computing OR to within confidence 1  X   X  .

We reduce the OR problem to local PageRank approximation as follows. We assume n  X  max { ( 1  X  +1) 2  X  ( 4  X  +1)+1 , 36 choice of n , m  X  1 , k  X  1 .Furthermore, m  X   X ( be the maximum number of queries A uses on graphs of size We will use A to construct an algorithm B that computes the OR function on input vectors of length m using at most S queries. That would imply S  X  m (1  X  2  X  )= X (
We map each input vector x =( x 1 ,...,x m ) into a graph G on n = m ( k +1)+1 nodes (see Figure 1). Note that n  X  n and therefore A uses at most S queries on G x . G x contains a tree of depth 2, whose root is u . The tree has one node at level 0 (namely, u ), m nodes at level 1 ( v 1 ,...,v m ), and mk nodes at level 2( w 11 ,...,w 1 k ,...,w m 1 ,...,w mk ). All the nodes at level 1 link to u . For each i =1 ,...,m ,the k nodes w i 1 ,...,w ik either all link to v i (if x i =1 ) or all link to themselves (if x i =0 ). Finally, u links to itself. Note that G x is sink-free and has a maximum in-degree  X  m  X   X ( is of length 2 (excluding self loops), PageRank converges in merely 2 steps on any node in G .

For each node y , we denote by PR G x ( y ) the PageRank of y in the graph G x . The following claim shows that PR G x ( u ) is determined by the number of 1 X  X  in x : C LAIM 7. Let | x | be the number of 1 X  X  in x . Then,
P ROOF . Using the influence characterization of PageRank (The-orem 3), In G x every node v  X  G x has at most one path to u .Further-more, all the nodes along this path are of out-degree 1. There-fore, inf t ( v, u )=1 , if the path from v to u is of length t ,and inf t ( v, u )=0 , otherwise. There is one node ( u ) whose path to u is of length 0 , m nodes ( v 1 ,...,v m ) whose path to u is of length 1, and k | x | nodes (nodes w ij for i  X  X  s.t. x i =1 and j =1 ,...,k ) whose path to u is of length 2. We can now rewrite Equation 1 as follows: PR G x ( u )= 1  X   X  n (1 +  X m +  X  2 k | x | ) .
Note that PR G x ( u ) is the same for all x that have the same num-ber of 1 X  X . Furthermore, it is monotonically increasing with Let p 0 = 1  X   X  n (1 +  X m ) and p 1 = 1  X   X  n (1 +  X m +  X 
The algorithm B now works as follows. Given an input x , B simulates A on G x and on the target node u . In order to simulate the link server for G x , B may resort to queries to its own external server (which returns bits of x ): (a) If A probes the link server for u , B returns u , v 1 ,...,v m as the in-neighbors and u as the single out-neighbor. In this case, B  X  X  simulation of the link server is inde-pendent of x , so there is no need to probe the external server. (b) If A probes a node v i ,for i =1 ,...,m , B sends i to its own server; if the answer is x i =1 , B returns w i 1 ,...,w ik as the in-neighbors and u as the out-neighbor; if the answer is x i =0 , B returns only u as the out-neighbor. (c) If A probes a node w ij , B sends i to the external server; if x i =1 , B returns v i as the out-neighbor; if x ter the simulation of A ends, B declares the OR to be 1, if A  X  X  estimation of PR G x ( u ) is at least p 1 (1  X  ) ,and0otherwise.
Note that each query A sends to the link server incurs at most one query to B  X  X  server. So B uses a total of at most S queries. To prove that B is always correct, we analyze two cases.
 Case 1: m i =1 x i =1 . In this case | x | X  1 . Therefore, by Claim 7, PR p (1  X  ) with probability  X  1  X   X  . In this case B outputs 1, as needed.
 Case 2: m i =1 x i =0 . In this case | x | =0 . Therefore, by Claim 7, PR with probability  X  1  X   X  . The following claim shows that this value is less than p 1 (1  X  ) , and thus B outputs 0, as needed. C LAIM 8. p 0 (1 + ) &lt;p 1 (1  X  ) .

P ROOF . To prove the claim, it suffices to show that p 1 . Expanding the LHS, we have: p 1  X  p 0 p choice of n , m = n Therefore, 2+2  X m  X  4  X m , and thus From k  X  X  definition, k m  X  n  X  1 n  X  Since &lt; 1 2 , 1  X  &gt; 1 . Therefore, k m  X  4  X  &gt; 4
For deterministic algorithms, we are able to strengthen the lower bound to the optimum  X ( n ) :
T HEOREM 9. Fix any  X   X  ( 1 2 , 1) and  X  (0 , 2 4+  X  ) .Let A be a deterministic algorithm that locally approximates PageRank to within a factor of 1  X  . Then, for every n&gt; 4 , there exists a graph G on at most n nodes and a node u  X  G on which A uses  X ( n ) queries. Furthermore, the maximum in-degree of G is  X ( n ) and PageRank converges in merely 2 iterations on G .

The proof uses a reduction from the  X  X ajority-by-a-margin X  prob-lem (determine whether a sequence of m bits has at least ( 1 X  X  or ( 1 2 + ) m 0 X  X ). As majority-by-a-margin has a  X ( bound for randomized algorithms [7, 1], when the approximation factor is small (  X  1  X  n ), we obtain an  X ( n ) lower bound also for randomized algorithms. It remains open to determine whether an  X ( n ) lower bound holds for randomized algorithms when the approximation factor is large.
 Slow PageRank convergence. The next two lower bounds demon-strate that slow PageRank convergence is another reason for the in-tractability of local PageRank approximation. We show an  X ( n lower bound for randomized algorithms (where  X &lt; 1 2 depends on  X  )andan  X ( n ) lower bound for deterministic algorithms. The hard instances constructed in the proofs are deep binary trees. So, the maximum in-degree in these graphs is 2, and the high query costs are incurred by the slow convergence ( O (log n ) iterations). The proofs of these two lower bounds are similar to the proofs of Theo-rems 6 and 9. They essentially trade fast convergence for bounded in-degree, by transforming the hard input graphs from shallow trees of large in-degree into deep trees of bounded in-degree.
T HEOREM 10. Fix any  X   X  ( 1 2 , 1) ,  X  (0 , 1) , and  X   X  Let A be an algorithm that locally approximates PageRank to within relative error and confidence 1  X   X  . Then, for every sufficiently large n , there exists a graph G on at most n nodes and a node u maximum in-degree of G is 2 and PageRank converges in  X (log n ) iterations on u .
 Note that the lower bound depends on  X  . The closer  X  is to 1, the closer is the lower bound to  X (
T HEOREM 11. Fix any  X   X  ( 1 2 , 1) and  X  (0 , 2  X   X  1 2  X  be a deterministic algorithm that locally approximates PageRank to within relative error . Then, for every n&gt; 4 , there exists a graph G on at most n nodes and a node u  X  G on which A uses  X ( n ) queries. Furthermore, the maximum in-degree of G is 2 and PageRank converges in  X (log n ) iterations on u .

As before, when the approximation factor is small (  X  1  X  proof of this theorem gives also an  X ( n ) lower bound for random-ized algorithms. It remains open to determine whether an  X ( n ) lower bound holds for randomized algorithms when the approxi-mation factor is large. The above lower bounds imply that high in-degrees and slow PageRank convergence make local PageRank approximation dif-ficult. We next show that local PageRank can be approximated efficiently on graphs that have low in-degrees and that admit fast PageRank convergence. In fact, a variant of the algorithm proposed by Chen et al. [9] is already sufficient for this purpose. In the fol-lowing we present this novel variant. We also explain the difference between the variant and the original algorithm below.
 The algorithm. The algorithm performs a brute force computation of PR G r ( u ) (see Figure 2). Recall that The algorithm crawls the subgraph of radius r around u  X  X ack-wards X  (i.e., it fetches all nodes that have a path of length u ). The crawling is done in BFS order. For each node v at layer t , the algorithm calculates the influence of v on u at radius t .Itsums up the influence values, weighted by the factor 1  X   X  n  X  to compute the influence values, the algorithm uses the following recursive property of influence: That is, the influence of v on u at radius t equals the average in-fluence of the out-neighbors of v on u at radius t  X  1 . Thus, the influence values at layer t can be computed from the influence val-ues at layer t  X  1 . Note that for nodes w that do not have a path of length t  X  1 to u , inf t  X  1 ( w, u )=0 . Therefore, in the expression 2, we can sum only over out-neighbors w of v that have a path of length t  X  1 to u . In the pseudo-code below, layer t consists of all nodes that have a path of length t to u . procedure LocalPRAlpgorithm( u ) 2: layer 0 := { u } 3: inf 0 ( u, u ) := 1 4: for t =1 ,...,r do 5: layer t := Get all in-neighbors of nodes in layer t  X  1 6: for each v  X  layer t do 8: end for 10: end for 11: return PR G r ( u )
Recall that PR G r ( u ) converges to PR G ( u ) as r  X  X  X  (Theorem 3). So, ideally, we would like to choose r = T ( G, u ) .Since the algorithm computes PR G r ( u ) , it is immediate from the defini-tion of T ( G, u ) that if the algorithm runs with r = T ( G, u ) , it is guaranteed to output a value which is in the interval [(1 So, we can do one of two things: (1) run the algorithm with r , which is guaranteed to be an upper bound on T ( G, u ) (see below for details); or (2) run the algorithm without knowing r apriori,and stop the algorithm whenever we notice that the value of PR does not change by much. This latter approach is not guaranteed to provide a good approximation but it works well in practice. Difference from the algorithm of Chen etal. . Also the algorithm of Chen et al. constructs a subgraph by crawling the graph back-wards from the target node. There are two major differences be-tween our variant and their algorithm, though. First, the algorithm of Chen et al. attempts to estimate the influence of the "boundary" of the graph that was not crawled, while our algorithm refers only to the impact of the crawled subgraph. Thus, while our algorithm al-ways provides an under-estimate of the true PageRank value, their algorithm can also over-estimate it. Second, our algorithm itera-tively computes the "influence at radius r" on the target node, while their algorithm applies the standard iterative PageRank computa-tion. The advantage in our approach is that one can bound the gap between the produced approximation and the real PageRank value in terms of the PageRank convergence rate. On the other hand, the heuristic estimation Chen et al. provide for the boundary influence may sometimes lead to large approximation errors.
 Complexity analysis. The following notion will be used to quan-tify the number of nodes the local PR algorithm needs to crawl:
D EFINITION 12. For a graph G , a target node u , and r  X  the neighborhood of u at radius r is:
N G r ( u )= { v  X  G | X  a path from v to u whose length  X  N r ( u ) consists of all the nodes in the graph whose distance to u is at most r . The following is immediate from the above definition:
P ROPOSITION 13. If the local PR algorithm runs for r itera-tions, then its cost is | N G r ( u ) | .
 Thus, the performance of the local PR algorithm depends on two factors: (1) how large r needs to be in order to guarantee a good ap-proximation of PR G ( u ) ; and (2) how quickly the neighborhood of u grows with r . The following is a trivial worst-case upper bound on the latter:
P ROPOSITION 14. Let d be the maximum in-degree of G . Then, Thus, the size of the neighborhood grows at most exponentially fast with the number of iterations r , where the base of the ex-ponent is the maximum in-degree d (in practice, the growth rate could be much lower than exponential). If d is constant, then a sub-logarithmic r (e.g., a constant r ) would guarantee that the al-gorithm X  X  cost is sub-linear (i.e., | G | ).

Next, we provide a bound on the number of iterations that the lo-cal PR algorithm needs to run. We show that r = O (log(1 / PR is always sufficient (in practice much lower r may be enough). Hence, if the PR of u is large, few iterations will be needed. The minimum PR value of any node is at least 1  X   X  | G | , and thus in the worst-case O (log(1 / PR G ( u ))) = O (log( | G | ) iterations are needed. T HEOREM 15. Let G be any directed graph and let u  X  G . Then, for any &gt; 0 , It would follow from Definition 5 that r  X  T ( G, u ) . Let us denote by P the PageRank transition matrix.
 PR G ( u )  X  PR G r ( u ) We will show that each of the above two terms is at most / 2 .We start with the first term. According to Sinclair X  X  bound on the point-wise mixing time [31] (see Proposition 2.1, pages 47 X 48), where |  X  1 | X |  X  2 | X  X  X  X  X  X |  X  | G | | are the eigenvalues of by absolute values and  X  max =max {|  X  i | :2  X  i  X | G |} second largest eigenvalue. Haveliwala and Kamvar showed in [19] that for the PageRank matrix, |  X  max | X   X  , and therefore, To bound the latter, we use the following calculation: The proof of claim 16 can be found in the full version of this paper.
This shows that the first term in Equation 3 is at most / 2 .We now bound the second term. By Lemma 4, Since M r +1 (1 ,u )  X  1 , |  X  r +1 M r +1 (1 ,u ) | X   X  r Claim 16 shows that the latter is at most / 2 . The theorem fol-lows.
 Optimizing by pruning. To lower the cost of the local PR al-gorithm in practice, we follow Chen et al. and apply a pruning heuristic. The idea is simple: if the influence of a node v on u at radius r is small, then only a small fraction of its score eventually propagates to PR G r ( u ) and thus omitting v from the computation of PR G r ( u ) should not do much harm. Furthermore, nodes whose paths to u pass only through low influence nodes are likely to have low influence on u as well, and thus pruning the crawl at low influ-ence nodes is unlikely to neglect high influence nodes.
The pruning heuristic is implemented by calling the procedure depicted in Figure 3. The procedure removes all nodes whose in-fluence is below some threshold value T from layer r . Thus, these nodes will not be expanded in the next iteration.

The problem of the pruning heuristic is that stopping the crawl whenever the PageRank value does not change much does not guar-antee an approximation. In the full version of the paper we give an example for that. procedure Prune( r ) 1: for each v  X  layer r do 2: if  X  r inf r ( v ) &lt;T then 3: remove v from layer r 4: end if 5: end for
In the previous sections we established that there are two nec-essary and sufficient conditions for a graph to admit efficient local PageRank approximation: (1) quick PageRank convergence; and (2) bounded in-degree. In this section we compare two graphs in light of these criteria: the web graph and the reverse web graph. We demonstrate that while both graphs admit fast PageRank conver-gence, the reverse web graph has bounded in-degree and is there-fore more suitable for local PageRank approximation. We also show empirically that the local approximation algorithm performs better on the reverse web graph rather than on the web graph. Experimental setup. We base our empirical analysis on a 280,000 page crawl of the www.stanford.edu domain performed in September 2002 by the WebBase project 8 . We built the adjacency matrices of these graphs, which enabled us to calculate their true PR and RPR as well as to simulate link servers for the local ap-proximation algorithm. In the PR and RPR iterative computations we used the uniform distribution as the initial distribution. The same stanford.edu crawl has been previously used by Kamvar et al. [22] to analyze the convergence rate of PageRank on the web graph. Kamvar et al. also showed that the convergence rate of PageRank on a much larger crawl of about 80M pages is almost the same as the one on the stanford.edu crawl. In addition, Dill et al. [14] showed that the structure of the web is  X  X ractal-like X , i.e., cohesive sub-regions exhibit the same characteristics as the web at large. These observations hint that the results of our ex-periments on the relatively small 280,000 page crawl are applicable also to larger connected components of the web graph.
 Convergence rate. We start by analyzing the PageRank conver-gence rate. Kamvar et al. [22] already observe d that PageRank con-verges very quickly on most nodes of the web graph (in less than 15 iterations on most nodes, while requiring about 50 iterations to con-verge globally). In Figure 4, we show that a similar phenomenon holds also for the reverse web graph. The two histograms specify for each integer t , the number of pages in the stanford.edu graph on which PageRank and Reverse PageRank converge in t it-erations. We determine that PageRank converges on a page u in t steps, if | PR sults, RPR converges only slightly slower than PR: on about 80% of the nodes it converges in less than 20 iterations.
 Crawl growth rate. Previous studies [30] have already shown that the maximum out-degree of the web graph is much lower than its maximum in-degree. The same holds in the stanford.edu graph, whose maximum in-degree is 38,606, while its maximum out-degree is only 255. We show a more refined analysis, which demonstrates that the average growth rate of backward BFS crawls around target nodes with high PageRank is much slower in the re-verse web graph than in the web graph.

In Figure 5, we plot the average size of a backward BFS crawl as a function of the crawl depth for the stanford.edu graph and Figure 4: Convergence times for PageRank and Reverse PageRank on the stanford.edu graph. for its reverse. To create the plot for the regular graph, we selected random nodes from the graph as follows. We ordered all the nodes in the graph by their PageRank, from highest to lowest. We divided the nodes into buckets of exponentially increasing sizes (the first bucket had the top 12 nodes, the second one had the next 24 nodes, and so on). We picked from each bucket 100 random nodes (if the bucket was smaller we took all its nodes), and performed a back-ward BFS crawl from each sample node up to depth 9. For each bucket and for each t =1 ,..., 9 , we calculated the average num-ber of nodes crawled up to depth t when starting the crawl from a node in the bucket. The plot for the reverse graph was constructed analogously. We present in Figure 5 the results for the top bucket (12 pages with highest PageRank/Reverse PageRank), the middle bucket (768 pages with intermediate PR/RPR) and the last bucket (85,000 pages with lowest PR/RPR). Figure 5: Average growth rates of backward BFS crawls at the stanford.edu graph and its reverse.
 The graph clearly indicates that the growth rate of the backward BFS crawl in the reverse web graph is slower than in the regular graph for pages with high PR/RPR. For example, the average crawl size at depth 6 in the top bucket on the regular graph was 77,480, while the average crawl size at depth 6 in the top bucket on the re-verse graph was 15,980 (a gap of 80%). The situation was opposite for the low ranked nodes. For example, the average crawl size at depth 6 in the last bucket on the reverse graph was 10,835, while the average crawl size at depth 6 in the last bucket on the regular graph was 4,701 (a gap of 57%). As we show below, the decreased crawl growth rate for the highly ranked nodes well pays off the increase in crawl growth rate for the low ranked nodes.
 Algorithm X  X  performance. We made a direct empirical compar-ison of the performance of the algorithm on the web graph vs. the reverse web graph. To do the comparison, we used the same buck-ets and samples as the ones used for evaluating the crawl growth rate. We then calculated, for each bucket, the average cost (num-ber of queries to the link server) of the runs on samples from that bucket. The results are plotted in Figure 6. Figure 6: Average cost of the local approximation algorithm by PageRank/Reverse PageRank values. Results of runs on the stanford.edu graph and its reverse.

The graph shows that the cost of the algorithm on the reverse graph is significantly lower than on the regular graph, especially for highly ranked nodes. For example, the average cost of the al-gorithm on the first bucket of PR was three times higher than the cost of the algorithm on the first bucket of RPR. On the other hand, for the low ranked nodes, the increased crawl growth rate on the reverse graph and the regular graph are almost the same. For exam-ple, the average cost of the algorithm on the last bucket of PR was 13 and for RPR it was 14. RPR has already been used in the past to select good seeds for the TrustRank measure [17], to detect highly influential nodes in social networks [20], and to find hubs in the web graph [15]. In this sec-tion we present two additional novel applications: (1) finding good seeds for crawling; and (2) measuring the  X  X emantic relatedness X  of concepts in a taxonomy.

We note that local RPR approximation is potentially useful in several of these applications. For example, to estimate the influence score of a given node in a social network, the hub score of a given page on the web, or the semantic relatedness of two given concepts in a taxonomy. Social networks exhibit similar properties to the web graph [26], such as the power law degree distribution and the gap between in-and out-degrees. As shown below, the same holds for the taxonomy graph extracted from the Open Directory Project. Discoverability of the web. Motivated by the fast pace of web growth and the need of crawlers to discover new content quickly, Dasgupta et al. [11] have recently posed the following question: how can a crawler discover as much new content as possible, while incurring as little  X  overhead X  as possible? Dasgupta et al. define the overhead of a crawler to be the average number of old pages it needs to refetch per new discovered pages. Formally, if the crawler refreshes a set S of  X  X eed X  pages previously crawled, resulting in aset N ( S ) of new pages being discovered, then the overhead is |
S | / | N ( S ) | . Dasgupta et al. present crawling algorithms that en-sure low overhead and analyze them theoretically and empirically. Selecting seeds using Reverse PageRank. We show that RPR is an effective strategy for finding good seeds. Our algorithm simply chooses the nodes with highest Reverse PageRank values to be the seed set. The intuition behind this is the following. A page p has high RPR if many pages are reachable from p by short paths, and moreover these pages are not reachable from many other pages. Thus, by selecting p as a seed, we benefit from discovering many new pages without doing too many fetches (because the paths lead-ing to them are short) and furthermore these new pages are not  X  X overed X  by other potential seeds. Assuming the web graph does not change drastically between two crawls, we can predict the Re-verse PageRank of the nodes in the new graph by calculating RPR on the already known sub-graph. Experimental results. To evaluate this seed selection strategy, we used two 1 million page Stanford WebBase crawls 9 .Thetwo crawls are of the same sites and were conducted one week apart in May 2006. The later crawl consists of 132,000 new pages. We compared four seed selection strategies: the k pages with highest RPR scores, the k pages with highest PR scores, the k pages with largest out-degree, and k random pages. We chose the seeds from the nodes of the first crawl and performed a BFS crawl for t levels starting from these seeds on the second crawl. Figure 7 shows the results for t =4 . We can see that RPR performs significantly better than the rest of the strategies, discovering more than twice new content with less overhead compared to any other strategy.
Semantic relatedness indicates how much two concepts are re-lated to each other. Semantic relatedness is used in many appli-cations in natural language processing, such as word sense disam-biguation, information retrieval, interpretation of noun compounds, and spelling correction (cf. [33]).

In the experiments below, we focus on measuring semantic re-latedness between concepts represented as nodes in the Open Di-rectory Project 11 (ODP) taxonomy. Given two nodes in ODP, we wish to find the relatedness between the concepts corresponding to these nodes. Note that the ODP is a directed graph, whose links represent an is-a relation between concepts. Thus two concepts should be related if the sets of nodes that are reachable from them are  X  X imilar X .

Previously, Strube and Ponzetto [33] used Wikipedia for com-puting semantic relatedness. Given a pair of words w 1 and w their method, called WikiRelate!, searches for Wikipedia articles, p and p 2 that respectively contain w 1 and w 2 in their titles. Se-mantic relatedness is then computed using various distance mea-sures between p 1 and p 2 . Also the ODP was previously used to measure semantic relatedness by Gabrilovich and Markovitch in [16]. The authors used machine learning techniques to explicitly represent the meaning of any text as a weighted vector of concepts. We show that (personalized) Reverse PageRank can be also used to measure semantic relatedness. Note that to this end we do not use any textual analysis of the taxonomy, only its graph structure.
Given two nodes x, y in the ODP graph, we compute two per-sonalized Reverse PageRank vectors RPR x and RPR y . RPR x is the personalized Reverse PageRank vector of the ODP graph cor-responding to a personalization vector that has 1 in the position corresponding to x and 0 everywhere else. Note that for a node a , a high value of RPR x ( a ) implies there are many short paths from x to a . This implies a is a prominent sub-concept of x and it is not a prominent sub-concept for (many) other nodes. Thus, the vector RPR x represents x by the weighted union of its sub-concepts.
We evaluate two alternative techniques for using these vectors in measuring semantic relatedness: (1) Reverse PageRank: the mea-sure of y as a sub-concept of x is the score RPR x ( y ) and the mea-sure of x as a sub-concept of y is the score RPR y ( x ) ;(2)Reverse PageRank similarity: two concepts will be similar in case they have significant overlap between their Reverse PageRank vectors. Therefore, the similarity between x and y is the cosine similarity between the vectors RPR x and RPR y . At first glance, RPR similar-ity seems more accurate than RPR, but RPR has a computational advantage since we can calculate RPR x ( y ) by using the local ap-proximation algorithm. In our experiments we compare the quality of these two measures.

An alternative graph-based approach for finding related nodes in a graph is the cocitation algorithm [13]. Two nodes are cocited if they share a common parent. The number of common parents of two nodes is their degree of cocitation . This measure is not suitable for us, since parent sharing is quite rare in the ODP. Another mea-sure of semantic relatedness is the path-based measure [29], which defines the semantic distance (inverse of relatedness) between two nodes as the length of the shortest path between them in the graph. Experimental results. We base our experiment on a 110 , 000 page crawl of the ODP. First, we verified that the reverse ODP graph admits the two conditions of efficient local PageRank ap-proximation. We analyzed the Reverse PageRank convergence rate and saw that more than 90% of the nodes converged in less than 20 iterations. The maximum out-degree of the graph was 2745 .
To evaluate the semantic relatedness measures, we chose a col-lection of concepts ( X  X ain concepts X ) from the ODP and ranked another collection of concepts ( X  X est concepts X ) according to their relatedness to the main concepts. We used three methods for mea-suring relatedness: Reverse PageRank, Reverse PageRank similar-ity, and inverse path-based. Table 8(a) shows the ordering of the test concepts by relatedness to the main concept  X  X instein X  using each one of the techniques. Table 8(b) shows a similar comparison for the main concept  X  X ce climbing X .
 Figure 8: Test concepts ordered by their relatedness to a main concept.

As can be seen from the results, the Reverse PageRank-based rankings were much better than the path-based ranking: while the path-based measure ranked  X  X griculture X  and  X  X nternet X  as very re-lated concepts to  X  X instein X , both our measures ranked  X  X hysics prize X  and  X  X ewton, Issac X  on the top of the list. For the  X  X ce climbing X  concept, the path-based measure ranked  X  X asketball X  and  X  X ard game X  before  X  X ountaineering X  and  X  X iking X , while both of them were ranked high by the RPR measures. We can also see from the experiment that the quality of RPR measure is almost the same as RPR similarity measure, which means we can use the local approximation algorithm to find semantic relatedness.
In this paper we have studied the limitations of local PageR-ank approximation. We have shown that in the worst-case  X ( queries to the link server are needed in order to obtain a good PageRank approximation. For deterministic algorithms, a stronger (and optimal)  X ( n ) lower bound was shown. For future work, it will be interesting to determine whether an  X ( n ) lower bound holds for randomized algorithms as well.

We have identified two graph properties that make local PageR-ank approximation hard: abundance of high in-degree nodes and slow convergence of the PageRank random walk. We have shown that graphs that do not have these properties do admit efficient local PageRank approximation. We note that our lower bounds are based on worse-case examples of graphs. It would be interesting to an-alyze more  X  X ealistic X  graph models, such as scale-free networks, and check whether local approximation is hard for them as well (we suspect they are, due to high in-degree nodes). Another future direction could be to explore whether it is easier to estimate the rel-ative order of PageRank values locally, rather than approximating the actual PageRank values.

As the web graph has many high in-degree nodes, we suspect that it is not suitable for local PageRank approximation. We have validated this conclusion by empirical analysis over a large crawl. We then have shown that the reverse web graph is amenable to ef-ficient local PageRank approximation, as it has bounded in-degree and it admits quick PageRank convergence. We have demonstrated empirically that the local approximation algorithm indeed performs much better on the reverse web graph than on the web graph. We leave for a future work to evaluate the property of the crawl growth rate for certain models of the Web graphs, such as preferential at-tachment [2], the copying model [25], etc.
 Finally, we have presented two novel applications of Reverse PageRank. The first application is detecting good seeds for crawl-ing. In our experiments we have compared the Reverse PageRank to three other methods for seeds choice. As part of future work it would be interesting to compare our method to additional known methods in different crawler models, for example, the ones that were presented by Cho et al. in [10]. The second novel applica-tion is measuring semantic relatedness between concepts in a tax-onomy. The experimental study we have conducted on the ODP taxonomy shows promising directions. In the future it will be in-teresting to evaluate the Reverse PageRank measures on the more complex wikipedia 12 graph.
