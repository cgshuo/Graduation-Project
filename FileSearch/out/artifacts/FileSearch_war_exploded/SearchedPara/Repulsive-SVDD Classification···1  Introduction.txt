 Support vector data description (SVDD) [ 1 ] was proposed by Tax and Duin to train a hyperspherically shaped boundary around a normal dataset while keeping all abnormal data samples outside the hypersphere. This SVDD has been a successful approach to solving one-class problems such as outlier detection since the volume of this data description is kept minimal. One-class support vector machine (OC-SVM) [ 2 ] is a similar approach proposed earlier to estimate the support of a high-dimensional distribution. Although this method uses a maximal-margin hyperplane instead of a hypersphere to separate the normal data from the abnormal data, it has the same optimisation problem as SVDD. In both OC-SVM and SVDD, the boundary in the feature space when mapped back to the input space can produce a complex and tight description of the data distribution.
 approach was proposed in [ 3 ] for novelty detection problems where a minimal hypersphere was trained to include most of normal examples while the margin between the hypersphere and outliers is as large as possible. A further extension using two large margins instead of one was proposed in [ 4 ], where an interior margin between the hypersphere and the normal data and an exterior margin between the hypersphere and the abnormal data both are maximised. In [ 5 ], the authors define an optimisation problem as maximising the separation ratio margin. It is shown to be equivalent to minimising ( R 2  X  parameter to adjust between minimising R and maximising d . Hao et al. [ 6 ] also used a similar formulation in which several similarity functions were used to compute the distance to centres. Another extension of SVDD is [ 7 ] in which the use of two SVDDs for the description of data with two classes was proposed. However all of those models are for one-class problems in which the task is to provide a tight data description or to detect outliers. When applying to a two-class problem where the numbers of data samples of two classes are not much different, the boundary of one-class methods is inappropriate. To overcome this problem, the first straight forward approach is to train two SVDDs, one for each class and define the decision boundary as the bisector between two surfaces of the hyperspheres. Although this approach improves the performance of one-class methods for two-class problems, they are limited by the small-sphere constraint of the data description.
 In this paper, we propose a method using two SVDDs, one enclosing posi-tive samples and the other enclosing negative samples, for binary classification tasks. The minimum bounding hypersphere constraint is relaxed to allow the hyperspheres to acquire larger regions. This is achieved by imposing a criterion that maximises the distance between two hyperspheres while still keeping the data inside the spheres. A margin variable is added to the optimisation to fur-ther improve the classification boundary. Since the proposed method trains two SVDDs that repel each other, we call it repulsive-SVDD classification (RSVC). RSVC decision boundary can be considered as a compromise between the bound-ary of a SVM boundary and a bisector boundary of two SVDDs X  surfaces, this is controlled by a trade off parameter to adjust the balance between describing the data and maximising the distance between the two sphere centres. The rest of the paper is organized as follows. The theory of the proposed RSVC will be presented in Section 2. Comparison of RSVC with Two SVDDs will be discussed in Section 3. Experimental results are presented to show the performance of the proposed method in Section 4. Finally, Section 5 presents our conclusions. To apply SVDD for binary classification problems, we construct a hypersphere criminate the two classes. First, the hypersphere constraint in SVDD is relaxed This is achieved by imposing a criterion that maximises the distance between two Second, the margin (i.e., the distance between surfaces of the two hypersphere) is maximised, similar to the maximal margin philosophy of a support vector machine.
 determines a maximum margin hyperplane without considering data distribu-tions of positive and negative classes. Whereas in the middle figure, SVDDs determine two minimal hyperspheres without considering the margin between the two classes, and the decision boundary is the perpendicular hyperplane of the line segment connecting the two hypersphere centres.
 and SVDDs. Given the problem in Fig. 1 , the RSVC optimisation problem attempts to keep the radii minimum while maximising the distance between the two hyperspheres. As a result, the hyperspheres will expand in the direction that increases the distance between the two hyperspheres. Moreover, the weights of these two directions can be controlled by a parameter. 2.1 Problem Formulation Consider a dataset { x i } ,i =1 ,...,n with two classes, positive class with n samples and negative class with n 2 data samples, n 1 + n of RSVC is to determine two optimal hyperspheres ( a 1 ,R encloses data samples of the positive class and the other encloses data samples of the negative class, and at the same time maximise the distance between the two centres. In addition, all positive and negative data samples are forced to stay outside the margin  X  1 and  X  2 of the positive hypersphere and the margin of the negative hypersphere respectively. The optimisation problem is formulated as follows: where k is a parameter which represents the repulsive degree between two cen-tres,  X  1 and  X  2 are two parameters controlling the support vectors, and  X  is the mapping to transform the vector x i to a feature space.
 The above problem is for separable datasets. In practice, to allow errors, the constraints are relaxed by introducing slack variables  X  1 ized terms are added to its objective function. In addition, if we combine the constraints in this problem to have a simpler form, the optimisation problem becomes: where  X  1 and  X  2 are parameters controlling the number of support vectors, together with  X  1 and  X  2 . They will be explained in Proposition 1 below. 2.2 Convex Formulation of RSVC Although the optimisation in ( 7 ) has a non-convex objective function, it can be reformulated to have a convex form as follows: multipliers  X  1 i , X  2 i , X  1 i , X  2 i , X  1 , X  2 , X , X  : Equations ( 28 ) and ( 29 ) leads to By substituting the KKT conditions into the Lagrangian function we obtain the dual form of the optimisation: min +2 k where the inner product between vectors has been replaced by the kernel K , and the Lagrange multipliers  X  1 i  X  0 , X  2 i  X  0 , X  1  X  0 , X  parameter chosen in the range k  X  [0 , 1 2 ).
 It can be seen that if k is set to 0 in the above optimisation problem then the RSVC optimisation problem ( 35 ) can be broken into two independent optimi-sation problems similar to SVDDs except for the extra constraints and i  X  2 i =  X  2 resulting from the margin requirements in the original RSVC problem ( 1 ).
 Solving the problem ( 35 ) gives a set of  X  1 i , X  2 i . Then the centres a be determined from Equations ( 34 ).
 To determine the radius R 1 , the support vector x t that lies on the surface of the hypersphere ( a 1 ,R 1 ) and corresponds to the smallest  X  selected. Then the radius R 1 is calculated as R 1 = d 1 ( x distance from x t to the centre a 1 and is determined as follows: d ( x t )=  X  ( x t )  X  a 1 2 = K ( x t ,x t )  X  function: 2.3  X  -Property Following [ 8 ], a data sample x i is called a support vector if it has Lagrange variable  X  i &gt; 0.
 to train the RSVC.
 Proposition 1. Let m 1 and m 2 denote the number of margin errors of the positive sphere and negative sphere respectively, and let s numbers of support vectors. Then for parameters  X  1 , X  2 , X  2. The feasible ranges of  X  1 , X  2 , X  1 and  X  2 are: Proof. We first prove for the positive hypersphere. 1. By the KKT conditions, all data points with  X  1 i &gt; 0 imply  X  vectors of positive hypersphere, plus  X  1 i = 0 for non-support vectors, and from ( 37 )wehave: sphere. 2. From ( 42 )wehave0 &lt; X  1  X  1  X  1. In addition, from ( 36 )wehave or
Since  X  1 i  X  0  X  i , this leads to  X  1 = Combining these results we have the proof of ( 43 ).
 The proposed RSVC is for binary classification problems. It can be extended for multi-class classification problems by using  X  X ne-against-the rest X  approach or  X  X ne-against-one X  approach. Following [ 9 ], we use the one-against-one app-roach in this paper where data of every pair of classes are used to train a binary classifier that separates the two classes, resulting in M ( M a M -class classification problem. In the test phase, a voting strategy is used: each binary classification of a test sample generates a vote, and the class with the maximum number of votes for this test data sample is output as the overall choose the class appearing first in the array of storing class names as in [ 9 ]. SVDD can be extended to two SVDDs to describe a data set of two classes. Consider a data set { x i } ,i =1 ,...,n of two classes, positive class with n samples and negative class with n 2 data samples, n 1 + n problem is formulated as follows [ 7 ]: where ( a 1 ,R 1 ) and ( a 2 ,R 2 ) are two hyperspheres,  X  This optimisation can produce a description of two minimal hyperspheres enclosing two classes. The decision boundary can be defined as the bisector between their surfaces. However this model is for one-class problems in which to a two-class problem where the data samples of two classes are balance the boundary of one-class methods is inappropriate. The RSVC can overcome this problem by allowing hyperspheres to acquire a larger area by minimising a || 2 and creating a larger margin by minimising  X   X  to provide data description for two classes. 4.1 2-D Demonstration of RSVC Figure 2 shows visual results for experiments performed on a simple 2-D datasets using RSVC. When parameter k = 0, the RSVC optimisation function becomes the optimisation function for two SVDDs, hence two SVDDs is a special case of RSVC. It can be seen that when k increases, two hyperspheres repulsed each the hyperspheres but inside this margin are penalised by a cost proportional hyperspheres X  surfaces. The first row in Figure 2 shows that when parameter k increases, the hypersphere enclosing positive samples is moving away from neg-ative samples while keeping all the positive samples inside it. The second row outside the hyperspheres.
 subset contained 50% of the data is for training and the other 50% for testing. The training process was done using 5-fold cross validation. The parameters for the methods are as follows. Gaussian mixture models (GMM) [ 10 ] use 64 mix-ture components. OC-SVM parameters are searched in  X   X  X  2 tive examples (Two SVDDs) are searched in  X   X  X  2  X  13  X   X  X  2  X  5 , 2  X  4 ,..., 2  X  2 } . SVM parameters are search in  X  and C  X  X  2  X  1 , 2 3 ,..., 2 15 } ; and RSVC parameters are searched in  X  { 2 k  X  X  0 . 5 , 0 . 7 , 0 . 9 } .
 that in SVM, while  X  1 n 1 and  X  2 n 2 are searched in a roughly similar number of options as of parameter C . This is to produce a sparse number of support vectors and avoid over fitting of the two SVDDs. Parameter k is to favour classification more than tight description. After the best parameters are selected in the cross validation step, the models are trained again with them on the whole training set and are tested on the 50% unseen test set. Experiments were repeated 10 times and the results were averaged with standard deviations given.
 Table 2 shows the prediction rates in cross validation training. Table 3 shows the prediction rates on unseen test sets with best parameters selected. It can be seen that the GMM, OCSVM and SVDD have undesirable perfor-mance in the classification task.
 The two SVDDs have much higher performance than these one-class meth-ods since they describe two minimal hyperspheres enclosing two classes and the decision boundary is the bisector between their surfaces. It can be seen that SVM has higher performance than two SVDDs, it trains a maximal-margin sep-arating hyperplane rather than two minimal hyperspheres. RSVC show highest performance in most datasets. RSVC can overcome the limitation of two SVDDs for the classification task by training two SVDDs that repel each other, allowing spheres to acquire a larger area and creating a larger margin while still trying to provide data description for two classes.
 We have proposed the repulsive-SVDD classification to extend SVDD for binary classification problems. Two hyperspheres are trained in an optimisation problem to describe the distribution of two classes. Additional requirements are added to the optimisation problem to help with the discrimination task. First, the distance between two hypersphere centres is maximised to allow hyperspheres to expand. Second, margins between the hypersphere surfaces and data are maximised. The resulting method can create a decision boundary that takes information not only from distributions of the classes but also the boundary X  X  margins. Experimental results on 9 datasets validate the good performance of the proposed method.
