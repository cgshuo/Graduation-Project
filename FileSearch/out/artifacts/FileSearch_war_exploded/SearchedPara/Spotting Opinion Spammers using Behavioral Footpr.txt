 Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities . However, due to the difficulty of human labeling needed for supervised learning and eval uation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling spamicity as latent. An unsupervised model , called Author Spamicity Model (ASM ), is proposed. It works in the Bayesian setting , which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers . The intuition is that opinion spammers have different behavioral distributions than non-s pammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learn ing the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real -life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform the state -of-the-art competitors. H.1.2 [ Information Systems ]: Human Factors ; J.4 [ Computer Applications ]: Social and Behavioral S ciences Algorithms , Experimentation, Theory Abuse, Opinion S pam, Deceptive and Fake Reviewer Detection
Online reviews of products and services are used extensively by consumers and businesses to make cri tical purchase, product design, and customer service decisions. However, due to the financial incentives associated with positive reviews, i mposters try to game the system by posting fake reviews and giving unfair ratings to promote or demote target products and services. Such individuals are called opinion spammers and their activities are called opinion spamming [14]. T he problem of opinion spam is widespread, and many high-profile cases have been reported in the news [40] . In fact the menace has become so serious that Yelp.com has launched a  X  X ting X  operation to publicly shame busi nesses who buy fake reviews [39]. 
In recent years, r esearchers have also studied the problem and proposed several techniques. However, the problem is still wide open. Unlike many other forms of spam ming , the key difficulty for solving the opinion spam problem is that it is hard to find gold-standard data of fake and non-fake review s for model building because it is very difficult, if not impossible , to manually recognize/ label fake/non-fake reviews by mere reading [14, 34] . 
Since it was first studied in [14] , various methods have been proposed to detect opinion spam. One of the main methods is supervised learning [10, 14, 34] . However, due to the lack of reliable ground truth label of fake /non-fake review data, existing works have relied mostly on ad-hoc or pseudo fake/non-fake labels for model building. In [14], duplicate and near duplicate reviews we re assumed to be fake reviews , which is restrictive and can be unreliable. In [25] , a manually labeled dataset was used , which also has reliability issues because it has been shown that the accuracy of human labeling of fake reviews is very poor [34] . In [34] , Amazon Mechanical Turk (AMT) was employed to crowdsource fake hotel reviews by paying (US$1 per review) anonymous online workers (called Turkers ) to write fake reviews for some hotels. Although the se reviews are fake, they do not reflect the dynamics of fake reviews i n a commercial website [31] as t he Turkers do not have the same psychological state of mind when they write fake reviews as that of fake review ers in a commercial website who have real business interests to promote or to demote. Also, Turkers may not have sufficient domain knowledge or experience to write convincing fake reviews . Due to the lack of labeled data, unsupervised methods have also been proposed for detecting individual [26, 42] and group [29, 30] spammers , time -series [46] and distributional [9] analysis, and mining reviewing patterns as unexpected association rules [15] and reviewing burs tiness [8]. 
In a wide field, a study of bias a nd controversy of research paper reviews was reported in [24] . However, research paper reviews do not usually involve faking. Web spam [3, 38, 45], email spam [5], blog spam [22] , clickbots [19], auction fraud [35], soci al networks [13] , etc. have also been widely investigated . However, the dynamics of these forms of spamming are quite different from those of opinion spamming in reviews . 
The above existing works i n opinion spam have made good progress es. However, they are largely based on heuristics and/ or hinge on ad-hoc fake/non-fake labels for model building. No principled or theoretical models have been proposed so far.

This paper proposes a novel and principled technique to model and to detect opinion spamming in a Bayesian framework. It transcends the existing limitations discussed above and presents an unsupervised method for detecting opinion spam. We take a fully Bayesian approach and formulate opinion spam detection as a clustering problem. The Bayesian setting allows us to model spamicity of reviewer s as latent with other observed behavioral features in our Author Spamicity Model (ASM ). Spamicity here means the degree of being spam ming . The key motivation hinges on the hypothesis that opinion spammers differ from others on behavioral dimensions [29] . This creates a separation margin between population distributions of two naturally occurring clusters: spammers and non-spammers . Inference in ASM results in learn ing the distributions of two clusters (or classes) based on a set of behavioral features. Various extensions of ASM are also proposed exploiting different priors.

In summary, this paper m akes the following contributions: 1. It proposes a novel and principled method to exploit observed behavior al footprints to detect spammers (fake reviewers) in a n unsupervised Bayesian framework precluding the need of any manual labels for learning which is both hard [14] and noisy [34]. A key advantage of employing Bayesian inference is that the model facilities characterization of various spamming activities using estimated latent variables and the posterior. It facilitates both detection and analysis in a single framework rendering a deep insight into the opinion spam problem . This cannot be done using existing methods. To our knowledge, t his is the first principled model for solving this problem. 2. It proposes a novel technique to evaluate the results without using any labeled data. This method use s reviews of the top ranked and bottom ranked authors produced by the model as two classes of data to build a supervised classifier . The key idea is that the classification uses a complete different se t of features than those used in modeling. Thus, i f this classifier can classify accurately, it gives a good confidence that the unsupervised spamicity model is effective (details in  X 3.3). 3. It conduct s a comprehensive set of experiments to evaluate the proposed model based on the classification evaluation method above and also human expert judgment . It also compare s with a set of strong baseline techniques . The results show that the proposed models outperform the baselines significantly. 
This section details the proposed unsupervised model. We first discuss the basic intuition ( X 2.1 ) and the observed features ( X 2.2). In  X 2.3, we explain the generative process of our model and detail inference methods in  X 2.4 and  X 2.5. 
As discussed above , the proposed model formulates spam detection as an unsupervised clustering problem in the Bayesian setting . It belongs to the class of generative models for clustering [6] based on a set of observed features. It models spamicity,  X  (degree/tendency of spamming in the range [0, 1]) of an author ,  X  ; and spam/non-spam label,  X   X  of a review,  X  as latent variables.  X  is essentially the class variable reflect ing the cluster memberships (we have two clusters,  X  = 2, spam and non-spam) for every review instance. Each author /reviewer (and respectively each review) has a set of observed features (behavioral c lues) emitted according to the corresponding latent prior class distributions. Model inference learns the latent populat ion distributions of the two clusters across various behavioral dimensions, and also the cluster assignments of reviews in the unsupervised setting based on the principle of probabilistic model -based clustering [37]. 
As t he generative process of ASM conditions review spam labels on author spamicites , inference also results in author spamicity estimates ( probability of spamming) facilitating ranking of authors based on spamicit y which is our main focus . 
Here we propose some characteristics of abnormal behaviors which are likely to be linked with spamming and thus can be exploited as observed features in our model for learning the spam and non-spam clusters. We first list the author features and then the review features. The notations are listed in Table 1.
 Author Features: A uthor features have values in [0, 1]. A value close to 1 (respectively 0) indicate s spamming (non-spamming). 1. Content Similarity (  X  X  ): As crafting a new review every time is time consuming, spammers are likely to copy reviews across similar products. It is thus useful to capture the content similarity of reviews (using cosine similarity) of the same author. We chose the maximum similarity to capture the worst spamming behavior. 2. Maximum Number of Reviews (  X  X  X  X  ): Posting many reviews in a single day also indicates an abnormal behavior. This feature computes the maximum number of reviews in a day for an author and normalizes it by the maximum value for our data. 3. Reviewing Bu rstiness (  X  X  X  X  ): The study in [29] reports that opinion spammers are usually not longtime members of a site. Genuine reviewers, however, use their accounts from time to time to post reviews. It is thus useful to exploit the activity freshness of an account in detect ing spamming. We define reviewing burstiness using the activity window (difference of first and last review posting dates). If reviews are posted over a reasonably long timeframe, it probably indicates normal activity. However, when all reviews are posted within a very short burs t (  X  = 28 days, estimated in  X 3.1), it is likely to be a spam infliction. 4. Ratio of First Reviews (  X  X  X  X  ): Spamming early can impact the initial sales as people rely on the early reviews. Hence, spammers would try to be among the first reviewers for product s as this enables them to control the sentiment [26] . We compute the ratio of first reviews to total reviews for each author. F irst reviews refer to reviews where the author is the first reviewer for the products. Review Features: We have 5 binary review features. Values of 1 indicate spamming while 0 non-spamming. 5. Duplicate/Near Duplicate Reviews (  X  X  X  X  ): Spammers ofte n post multiple reviews which are duplicate/near-duplicate versions of previous reviews on the same product to boost ratings [26] . To capture this phenomenon, we compute the review feature (  X  duplicate reviews on the same product as follows:  X  X  X  X  attains a value of 1 for a review  X   X  by an author  X  on product  X  , if it is similar (using cosine similarity based on some threshold,  X  = 0.7 (say)) to any other review on  X  .  X  1 is estimated in  X 3.1 . 6. Extreme Rating (  X  X  X  X  ): On a 5 -star (  X  ) rating scale, it reflects the intuition that to inflict spam, spammers are likely to give extreme ratings (1  X  or 5  X  ) in order to demote/ promote products. 7. Rating Deviation (  X  X  X  ): Review spamming usually involves wrong projection either in the positive or negative light so as to alter the true sentiment on products. T his hints that ratings of spammers often deviate from the average ratings given by other reviewers. Th is feature attains the value of 1 if the rating deviation of a review exceeds some threshold  X  2 .  X  2 is estimated in  X  3.1 . We normalize by the maximum deviation, 4 on a 5 -star scale. The e xpectation is taken over all reviews on product  X  =  X  (  X  other authors ,  X   X   X  X  X  , to get the average rating on  X  . 8. Early Time Frame (  X  X  X  X  ): [26] argues that spammers often review early to inflict spam as the early reviews can greatly impact people X  X  sentiment on a product. To capture this spamming characteristic, we propose the following feature:  X  X  X  X  (  X   X  ,  X  ) captures how early an author  X  reviewed the product  X  .  X  = 7 months is a threshold for denoting earliness (estimated in  X 3.1). The definition says that if the latest review is beyond 7 months of product launch, it is no longer considered to be early. At the other extreme, if reviews are posted just after launch this feature attains a value of 1.  X  3 is the corresponding threshold indicating spamming and is estimated in  X 3.1. 9. Rating Abuse (  X  X  X  ): This feature captures the abuse caused by multiple ratings on the same product. M ultiple ratings/reviews on the same product are unusual. Although this feature is similar to  X  X  X  X  , it focuses on the rating dimension rather than content . Rating abuse,  X  X  X  (  X  ,  X  ) is defined by the similarity of ratings of an author,  X  towards a product,  X  across multiple reviews by the author weighted by the total number of reviews on the product. The similarity of multiple star rating is computed using the difference between maximum and minimum star rating on a 5-star scale to capture consistent high/low ratings. The normalization constant is 4 as it is the maximum possible rating difference. For multiple ratings in genuine cases where ratings change (e.g., after correct use), the feature attains lower values.  X  4 is the rating abuse threshold indicating spamming and is estimated in  X 3.1. 
In ASM , spam detection is influenced by review and author features. Normalized continuous author features in [0, 1] are modeled as following a Beta distribution (  X   X  ,  X   X  1). This enables ASM to capture more fine grained dependencies of author X  X  behaviors with spamming. However, review features being more objective, we found that they are better captured when modeled as binary variables being emitted from a Bernoulli feature  X   X  {  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  } denote the per class/cluster (spam vs. non-spam) probability of emitting feature  X  . 
Latent variables  X   X  and  X   X  denote the spamicity of an author,  X  and the (spam/non-spam) class of each review ,  X  . The objective of ASM is to learn the latent behavior distributions for spam and non-spam clusters (  X  = 2) along with spamicites of authors from the observed features. We now detail its generative process . 1. For each class/cluster,  X   X  {  X  X  ,  X  X  X  } : 2. For each (author),  X   X  {1 ...  X  } : i. Draw spamicity,  X   X  ~  X  X  X  X  X  X  (  X   X  ) ; ii. For each review,  X   X   X  {1 ...  X   X  } : 
We note that the observed author features are placed in the review plate (Figure 1). This is because each author behavior can be thought of as percolating through reviews of that author and emitted across each review to some extent. Doing this renders two key advantages: i) It permits us to exploit a larger co-occurrence domain. ii) It paves the way for a simpler sampling di stribution providing for faster inference. We employ approximate posterior inference with Monte Carlo Gibbs sampling, and use Rao-Blackwellization [2] to reduce sampling variance by collapsing on the latent variables  X  and  X  As observed author features obtaining values in [0, 1] are modeled as continuous Beta distributions, sparsity is considerably reduced to ensure speed, we estimate  X   X   X  using the method of moments, once per sweep of Gibbs sampling. The Gibbs sampler is given by: The subscript  X   X  denotes counts excluding review  X  =  X  The Beta shape parameter updates for  X   X   X  using method of moments are given as follows: where  X   X   X  and  X   X   X  denote the mean and biased sample variance for feature  X  corresponding to class  X  . Algorithm 1 details the full inference procedure for learning ASM using the above Gibbs conditional. Omission of a latter index denoted by [ ] corresponds to the row vector spanning over the latter index. Algorithm 1 performs inference using uninformed priors (i.e., hyperparameters  X  and  X  are set to (1, 1) ). Posterior estimates of spamicity can be improved if hyperparameters  X  and  X  are estimated from the data. This is because the priors for author spamicity and latent review behaviors (  X  and  X  ) directly affect spam/non-spam cluster assignment to reviews. Algorithm 2 details hyperparameter estimation using the single sample Monte Carlo EM , which is recommended by [4] as it is both computationally efficient and often outperforms multiple -sample Monte Carlo EM . 
Algorithm 2 learns hyperparameters  X  and  X  which maximize the model X  X  complete log -likelihood, L . We employ an L -BFGS optimizer [48] for maximization. L -BFGS is a quasi -Newton method which does not require the Hessian matrix of second order derivatives. It approximates the Hessian using rank-one updates of first order gradient. A careful observation of the model X  X  complete log-likeliho od reveals that it is a separable function in  X  and  X  allowing the hyperparameters to be maximized independently. Due to space limits , we only provide the final update equations: where,  X  (  X  ) denotes the digamma function.
We now evaluate the proposed ASM model. W e use the reviews of manufactured products from Amazon.com . We consider only authors/reviewers with at least 3 reviews as authors with fewer reviews have few behavior characteristics. For reviewers with fewer review s, the method in [46] can be applied . Our final data comprises of 50,704 reviewers, 985,765 reviews , and 112,055 products. Below we describe parameter estimation, baseline systems, and evaluation results.
As noted in  X 2.2, the proposed feature constructions contain thresholds. T he thresholds can either be set heuristically or learned using some weak supervision. In this work, we use weak supervision to learn the thresholds from the Amazon group spam dataset in [29] , which provides a small set of labeled spammer groups and their reviews. Using this small set of available data is not a limitation of our method because the actual spamicty modeling of ASM still remains unsupervised as it does not use any spamicty labels for authors/reviews in model building. In fact, ASM does not have any response variable (e.g., [28, 36]) where supervision can be fed using labels . To keep evaluation fair, the labeled data in [29] is not used for our evaluation in  X 3.3.
Thresholds of burstiness ,  X  = 28 days and earliness ,  X  = 7 months (in  X 2.2) were estimated using greedy hill -climbing search maximizing the likelihood of the data in [29]. It is also important to note that above thresholds apply to feature constructions in  X 2.2 and not specific to ASM . As we will see in  X  3.2, the same features are used by other baseline methods , so improvements of ASM are attributed to its process and not the choice of feature threshol ds.
Thresholds for binary discretiz ation of continuous review features  X  1 ... 4 (in  X 2.2) are learned using Recursive Minimal Entropy Partitioning (RMEP) [7]. The estimated values are as follows:  X  1 = 0.72,  X  2 = 0.63 ,  X  3 = 0.69,  X  4 = 2.01.
Although ASM clusters reviews and estimates author spamicities , in this work, our focus is to evaluate the ranking of authors based on estimated author spamicities . ASM with Uninformed Priors (ASM -UP) : This is the fully unsupervised version of ASM . Uninformed priors are used for Beta distributed variables,  X   X  ,  X   X  ,  X   X  {  X  X  X  X  , . . ,  X  X  X  } , i.e.,  X  X   X   X  ,  X   X   X  (1, 1) ;  X   X   X  (1, 1) . This setting is called uninformed because any value in [0, 1] is equally likely to be assigned to the Beta distributed variables and the model doesn X  X  benefit from any domain knowledge based priors . Posterior estimates are drawn after 3000 iterations with an initial burn-in of 250 iterations. ASM with Informed Priors (ASM -IP): Here we employ guidance using some domain knowledge heuristics. Amazon tags each review with AVP (Amazon Verified Purchase) if the reviewer actually bought the product. Keeping other settings the same as ASM-UP, f or each author,  X  , if none of his reviews have AVP tag s, then we set  X   X  ~  X  X  X  X  X  X  (5, 1) , else we set  X  ~  X  X  X  X  X  X  (1, 5) 1 . The rationale is that authors who have not bought a single product on Amazon are likely to be less reliable (hence probably spamming) than those who also review products that they have purchased on Amazon (i.e., receiving AVP tags). ASM with Hyperparameter Estimation (ASM -HE): This setting estimates the hyperparameters  X   X  ,  X   X  X  X  and  X  {  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  } using Algorithm 2 keeping all other settings fixed as ASM -UP.
ASM estimates reviewer spamicities as scores in [0, 1] (the posterior on  X  ~  X  X  X  X  X  X  ). We can also regard the observed behaviors as ranking functions in [0, 1] with extreme values 0 (respectively 1) indicating non-spamming (spamming) on various behavior dimensions. Th en, estimating the final spamicites of authors becomes unsupervised rank learning using aggregation [21] . The problem setting is described as follows.

Let  X   X  X  X  denote an item (e.g., author) in the instance space  X  (e.g., set of authors/revi ewers) that need to be ranked relative to each other according to some criterion (e.g., spamicity). L et  X   X  X  X  denote a query and  X  X  X  X   X   X   X  X  X  denote a ranking function whose output governs the rank position of an item, i.e.,  X  (  X  ,  X  ) &gt;  X  (  X  ,  X   X  ) specifies t hat  X  is ranked higher than  X   X  on query  X  using ranking function  X  . The notation  X   X   X   X   X   X  ranking function  X   X  is better than  X   X  with respect to a certain evaluation criterion (e.g., NDCG, MAP, L 2 loss, etc. ). Given a set of ranking functions, {  X   X  }  X =1  X  , rank aggregation learns the optimal ranking function,  X   X  X  X  X  (using a weighted combination of {  X  such that  X  X  ,  X   X  X  X  X   X   X   X   X  . This problem setting is also called unsupervised rank fusion or ensemble ranking [44]. In the supervised setting, one usually employs regression to learn the weights [41] . In the unsupervised setting, the approach is two-fold: i) D erive an incidental / surrogate supervision signal. ii) Employ a learning algorithm to learn the parameters of  X  [21] , the surrogate supervision signal was computed using some ranker agreement heuristics and iterative gradient descent was used as the learning algorithm.

In our case of ranking reviewers according to spamicites, we have 9 ranking functions {  X   X  }  X =1  X =9 corresponding to the 9 behavior feature dimensions (in  X 2.2) . For each author feature,  X  {  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  } , we directly use the value of the feature as the ranking function while for each review feature,  X  {  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  X  ,  X  X  X  } , we take the expected value of the feature across all reviews of an author to compute the corresponding author feature. We then directly use the value of each author feature as a ranking function to produce 9 trai ning ranks. Given no other knowledge, t his is a reasonable approach since  X  1 ... 9 being anomalous reviewing behaviors are likely to be correlated with spam mers (the nature of correlation will be detailed using posterior density analysis i n  X 4). T hus, the training ranking produced by each feature function is based on a certain spamicity dimension. However, none of the training rankings may be optimal. We employ l earning to rank [27] to learn an optimal ranking function by aggregating 9 ranking functions {  X 
Learni ng to rank [27] i s a supervised technique that takes a set of rankings for training and produces a single optimal aggregated ranking function. Each of our training rank ings is produced by sorting instances (authors) based on values of a feature in {  X  Each instance /author in a training ranking is represented as a vector of  X  1 ... 9 spam features. We experiment ed with two popular learning to rank algorithms: SVMRank [17] and RankBoost [47] . For SVMRank, we used the system in [17] . RankBoost was from RankLib 2 . We use the pair -wise L 2 loss metric as the optimization criterion for RankBoost and SVMRank. We al so experimented with RankNet, AdaRank, Coordinate Ascent in RankLib , but their results were poorer and hence not included. 
For a more comprehensive comparison, we also experiment with the following baseline approaches: Feature Sum (FSum): As each feature  X  1 ... 9 measures spam behavior on a specific dimension, an obvious approach is to rank the authors in descending order of the sum of all feature values. Helpfulness Score (HS): In many review sites (e.g., Amazon) , readers can provide helpfulness feedback to each review. It is reasonable to assume that spam reviews should get less helpfulness feedback. HS uses the mea n helpfulness score (percentage of people who found a review helpful) of reviews of each reviewer to rank reviewers in ascen ding order of the scores.
Finally, we n ote that although in ASM spam detection is modeled as clustering ( X 2.3), our key task is to estimate author spamcities (the posterior on the latent variable,  X  for ranking authors according to their spamicities. . Standard clustering algorithms (e.g., k -means) are not suitable baselines because they only produce clusters, but do not generate a ranking of authors. As noted in  X 1, we are not aware of any gold-standard ground truth labeled data for opinion spammers. Hence, this work focuses on Bayesian inference in the unsupervised settin g. T o evaluate the author spamicities computed by different systems, we use two methods: review classification and human evaluation . 
Running the systems in  X 3.2 on our data generates a ranking of 50,704 reviewers. However, human evaluation on all authors is clearly impossible. Even for review classification, the number of reviews is huge for such a large number of reviewers. We thus sample the rank positions 1, 10, 20,..., 50000 (with a sampling lag/interval of 10 ranks) to construct the evaluation set,  X  of 5000 rank positions. Using a fixed sampling lag ensures that performance on  X  is a good approximation over the entire range. 5000 is reasonable for review classification, but for human evaluation we need to use a subset (see below). This is a new evaluation method in the unsupervised s etting. The idea is that i f ASM is effective, it must rank the highly likely spammers at the top and highly likely non-spammers at the bottom. We use a supervised classification of likely spammers and likely non-spammers to evaluate this ranking. Instead of classifying review ers, we classify their reviews.

Prior works in psycholinguistic research (e.g. [32] ) ha ve shown that faking or lying usually involves more use of personal pronouns and associated verb actions to justify fake statements eventually resulting in more use of positive sentiments and emotion words . The hypothesis has been attested in [34] where text classification using n -gram features have been shown quite effective in detecting spam and non-spam reviews. Thus, if our classification of reviews based on text features is good, it implies that the ASM ranking of reviewers according to spamicites is effective because text classification concurs with the abnormal behavior spam detection of ASM . T he key characteristic of this classification is that the text features have not been used in ASM . Clearly, using the same set of features will not be meaningful . 
For this set of experiments, we consider the reviews from the top k % ranked authors to be in the spam (+) class while reviews from the bottom k % of the ranked authors to be in the non-spam ( X ) class. Although t he actual p ercentage of spammers is unknown, deception prevalence studies [33, 43] have reported 8-15% spam rate in online review sites . W e thus report results for k = 5%, 10%, and 15%. We induce a linear kernel SVM SVM Light [16] and report 5-fold cross validation results in Table 2 . The features are 1-3-grams. We note the following observations: 1. As k increase s, we find a monotonic degradation in review classification performance (except HS) which is expected as the spamicities of top and bottom authors get closer which makes the corresponding review classification harder. 2. For k = 5% , ASM models performs best on F1 and Accuracy metri cs. Next in order are FSum, RankBoost, and SVMRank. It is interesting that the simple baseline FSum performs quite well for k = 5%. The reason is attributed to the fact that the top positions are mostly populated by heavy spammers while the bottom positions are populated by genuine reviewers and hence a na X ve un-weighted FSum could capture this phenomenon. 3. For k = 10, 15%, FSum does not perform so well (SVMRank and RankBoost outperform Fsum ). This is because for k = 10, 15%, the ranked positions involve more difficult cases of authors/reviewers and a mere sum is not able to balance the feature weights as not all features are equally discriminating. 4. For k = 10%, SVMRank and RankBoost outperform ASM -UP and perform cl ose to ASM-IP. ASM -HE still outperforms authors. Classification on this set of reviews (coming from two sets of authors belonging to spammers ) yielded 57% accuracy. Similarly, classification of reviews by two sets of authors belonging to non -spammers (bottom 15% ranked authors of ASM-HE) yielded 5 8% accuracy. We tried different random selection of authors which also yielded similar accuracies. We note that the accuracy is greater than 50% but less th an the classification accuracy using ASM -HE (Table 2). This shows that the above experiment is likely to separate authors and not spam vs. non-spam reviews. As different authors have differences in writing, the accuracy is greater than 50% (random guessing). But it is lower than spam and non-spam review classification because reviews of both classes come from spammers (or non-spammers) and hence noisy. Our result of author classification is supported by prior studies in [20] which showed that authorship attribution using n-grams is not very effective. It also indirectly shows that a lower accuracy obtained by various competitors in Table 2 have more noise in ranking authors based on spamicity.

Th ese results render confidence in the proposed models. It also indirectly shows that there are some linguistic differences between spam and non-spam reviews.
Our second evaluation is based on human expert judgment, which is commonly used in research on spam , e.g., Web [3, 38] , email [5] , and even blogs and social spam [22] . Human evaluation has also been used for opinion spam in prior works [42, 46] . It is however important to note that just by reading a single review without any context, it is very hard to determine whether a review is fake (spam ) or not [34] . However, it has been shown in [29] that when sufficient context is provided e.g., reviewing patterns, ratings, type/brand of products reviewed, posting activity trails , etc., human expert evaluation becomes easier.
 For this work, we used 3 domain expert judges: employees at Rediff Shopping; eBay.in for evaluating our ranked reviewers based on spamicites. The judges had sufficient background in reviews of products and sellers due to their nature of their work in online shopping. The judges were briefed with many opinion spam signals: i) Having zero caveats, and full of empty adjectives. ii) Purely glowing praises with no downsides. iii) Suspicious brand affinity/aversion, posting trails , etc. , from prior findings and consumer sites [1, 12] . These signals are sensible as they have been compiled by consumer domain experts with extensive know -how on fake reviews. Our judges were also familiar with Amazon reviews and given access to additional metadata, e.g., reviewing profile, demographic information, etc. Although the judges were not provided the proposed features, they were encouraged to use their own signals along with th e above existing signals and reviewer metadata . It is important here to note that providing various signals compiled from prior works and domain experts in consumer sites [1, 12] to the judges does not introduce a bias but enhances judgment. Without any signals, as mentioned above, it is very difficult to judge by merely reading reviews. It is also hard for anyone to know a large number of signals without extensive experience in opinion spam detection. Given a reviewer and his reviews, the judges were asked to independently examine his entire profile and to provide a label as spammer or non-spammer.
Due t o the large number (5000) of reviewers in the e valuation set,  X  , it would have taken too much time for human judges to assess all the reviewers in  X  . We selected the following three ranked buckets ( B 1 , B 2 , B 3 ) for evaluation by our judges: B (Top 50) : Reviewers ranked from 1 to 50 by each system in  X  . B (Middle 50): Reviewers ranked from 2501 to 2550. B (Bottom 50): Reviewers ranked at bottom 50 rank s in  X  . 
This is reasonable because these rank-positions in  X  reflect the performance trend on the entire range. Table 3 reports the results of each judge (as the count of reviewers labeled as spammers) for each bucket across each method. Additionally, we report the agreement of judges using Fleiss multi -rater kappa [11] for each method in the last row of Table 3. We note the following observations: 1. All 3 judges perform similarly with slight variation (e.g., J seems to be stricter and identifies more spammers in each bucket than J 3 ). This shows that the judges have consensus in spam judgments. Further, the kappa values being in the range of substantial agreement according to scale 4 in [23] bolsters confidence in the judgments. 2. For all methods except HS, we find the average number of spammers detected by judges decreasing monotonically for buckets B 1 , B 2 , and B 3 which is expected from the ranking produced by the algorithm s. HS performs poorly (placing more spammers in B2, B3 which is undesireable) as also observed in  X 3.3.1. Thus, helpfulness votes are n X  X  useful for spam detection. 3. For B 1 , ASM -HE performs best. On average, it is able to place 39 spammers in the top 50 rank positions. ASM -IP, R ankBoost, and SVMRank perform poorer than ASM -HE. ASM -UP does not perform well as uninformed priors are weaker. 4. For B 2 , the performance order is ASM -HE  X  ASM-IP  X  
RankBoost  X  FSum  X  ASM-UP  X  SVMRank. At this bucket, a good method should place fewer spammer s as B 2 the middle. This is indeed the case in Table 3 . Note that an ideal ranking should not place any spammers in the middle range as it is very unlikely that 50% of the reviewers are spammers. 5. For the last bucket, B 3 , we find ASM -HE and FSum performing best by not placing any spammers in the bottom ranks. ASM -IP does quite well too. Next in performance order are RankBoost  X  SVMRank  X  ASM -UP. FSum performed very well in B because its ranking (based on descending order of feature sum value) placed authors who ranked very low in all features. As the features are all abnormal and indicate suspicious behaviors, reviewers attain ing very low FSum values are likely to be genuine reviewers which explain the good results of FSum.

In summary, we can conclude that proposed ASM -HE is effective and outperforms other methods and baselines. ASM -IP and ASM -UP are slightly inferior which is reasonable as they use weaker priors. Learning to rank methods SVMRank and RankBoost using proposed behaviors are strong competitors showing that the proposed behavior features are effective . Apart from generating a ranking of authors based on spamicity (  X  ), ASM also estimates the latent distributions of spam and non-spam reviews and authors corresponding to each observed author and review feature dimension  X  as reflected in the spam vs. non-dimension  X  . Although these are model X  X  estimate s of spam and non-spam distributions for each anomalous/spamming dimension or feature ( X 2.2 ), it is still worthwhile to study the es timate s to see whether they reflect our understanding of spamming behaviors. 
We report the posterior on the latent spam and non-spam distributions under each feature  X  (  X   X  X  X  {  X   X  ,  X   X  } ASM -HE using MC-EM (Algorithm 2). Due to space constraints, we only focus on ASM -HE as it performed best. We plot the spam clusters in Figure 2 . For each estimated latent distribution (  X   X  ,  X  and non-spam in blue (dotted) lines . Furthermore , the spam and non-spam densities being different are shown in their respective scales (secondary y -axes, left: blue/dotted for non-spam and right: solid/red for spam). The x -axis is the domain of the Beta distribution, [0, 1] which is also the range of the corresponding observed (author and review) feature where values close to 0 and 1 show extremes of non-spamming and spamming (author and review). Also shown are the expected values for each latent behavior for spam (red/dashed lines ) and non-spam (blue/dash-dot ted lines ) in respective scales and figure captions . We compare and explain the estimated trends of each behavior feature (see  X 2.2) across spam and non-spam classes below.
 Content Similarity (  X   X  X  ) : From the densities in Figure 2 (a), we see that most spammers have a lot of content similarity in t heir reviews while non-spammers exhibit much less review similarity. The expected value of content similarity (using cosine similarity) for non-spammers is 0.09 , much lower than 0.7 for spammers.
 Maximum Number of Reviews (  X   X  X  X  X  ): Figure 2 (b) shows the difference of densities for this behavior. The density curve for non-spammers attaining th e peak towards the left of the plot has majority of the probability mass concentrated towards the left. This shows that non-spammers attain much lower values for  X  X  X  X  . Spammers on the other hand have their masses concentrated at the middle with the density curve attaining a peak at a feature value of 0.34. We recall that this feature,  X  X  X  X  is normalized by the largest value for this feature (maximum number of reviews in a day) in our data which happens to be 21. This imples that a significant perce ntage of spammers have written close to 0.34  X  21  X  7 reviews in a single day. In expectation, we see that the maximum number of reviews in a day for non-spammers is 0. 11, i.e., 0.11  X  21  X  2 reviews while 0.28  X  21  X  5 reviews for spammers .
 Reviewing Burst iness (  X   X  X  X  X  ): R eviewing busrtiness measures the account activity (according to difference between last and first post date) within the estimated threshold  X  = 28 days . At one extreme, reviewers with overall posting activity more than 28 days obtain a value of 0 while at other extreme reviewers whose last and first post are very close attain value close to 1. From Figure 2 (c ), we find that density of non-spam class attains a peak at the feature value 0, showing a large percentage of non-spammers p ost reviews spanned over more than 28 days. This is intuitive as genuine reviewers use their account s from time to time to post reviews. For spammers, the density attains a peak at feature value 0.89 (i.e., 28  X (1-0. 8 9)  X  3 days showing that a considerable percentage of spmamers never use their accounts after 3 days from the first post date. Further the expected account acitvity period for spam mers is 0.75, i.e., 28  X  (1-0.75 )  X  7 days. The expected value for non-spammers is 0.01 which is negligible showing most non-spammers have account activities (by posting reviews) for more than 28 days.
 Ratio of First Reviews (  X   X  X  X  X  ): As defined earlier, t his behavior computes the ratio of first reviews to total reviews per account (reviewer). First review means that a review was posted first for a given product. From Figure 2 (d), we see that unlike previous behaviors, the separation between spammers and non-spammers is not so disticnt. Although in expectation, the estimated behavior attains a value of 0.36 for spammers which is higher than 0.29 for non-spammers, the density profile for non-spammers remains somewhat high. A plausible reason for this phenomenon is that there are many enthusiastic genuine reviewers who review newly launched products. S pammers obtain higher values in expectation as reviewing first /early can give them an edge as early reviews have more impact [26].
 Duplicate/Near Dulpicate Reviews on Products (  X   X  X  X  X  ): This review feature measures whether a review posted on a product is similar to other reviews on that product. This behavior can detect some sockpuppet s where a person posts similar reviews on a product with multiple ids . There can also be multiple simi lar review posts by a single id. In either cases, this behvarior would return high values for a review when it is a duplicate/near dulpicate to other reviews on that product. From Figure 2 (e), we see stark difference of densities of spam and non-spam reviews based on this behavior. Spam reviews attain higher values (with dens ity peak at extreme right) while non-spam reviews attain very low values (with peak density at extreme left).
 Extreme Rating (  X   X  X  X  X  ): Extreme review rating is a boolean review feature. Its corresponding latent behavior,  X  the extent to which reviews are rated with extreme rating (1 or 5 star rating) on the scale [0, 1]. Form Figure 2 (f), we see that the density of spam reviews is concentrated towards the extreme right. Its expected value is 0.86 which implies tha t about 86% percent of spam reviews are rated with either 1 or 5 stars. F or non -spam reviews, we find a somewhat evenly distributed density profile because genuine reviewers usually have different rating levels. The expected value for non-spam reviews is 0 .36, i.e., about 36% of non-spam reviews are extreme while the rest 64% is distributed ac ross 2, 3, and 4 star ratings according to the density. Rating Deviation (  X   X  X  X  ): R ecall that r ating deviation for a review is the extent to which its rating deviates from the average rating (general rating consensus) on the product. Its corresponding normalized latent behavior is  X   X  X  X  X  . From the densit ies in Figure 2 (g), we find that spam reviews deviate a great deal in rating from the general rating consensus. For non-spam reviews we find the density peak is attained at the feature value of 0.12 showing that a considerable percentage of non-spam reviews don X  X  deviate very much in ratings from the average rating . The expected rating deviation is 0.68 for spam reviews which is almost double of the expected deviation of 0.34 for non-spam reviews. Early Time Frame (  X   X  X  X  X  ): ETF behavior measures how early was a review posted in reference to the product X  X  launch date. We find something very interesting in its corresponding latent bevarior,  X   X  X  X  X  . We r ecall from  X 3.1 that the estimated threshold for this feature is  X  = 7 months. So, 0 values indicate that reviews were posted after 7 months of launch date and values close to 1 indicate review posts close to the lauch date. From the density plot in Figure 2 (h), we find that the mass for non-spam cluster is concentrated close to 0 with the expected value of 0.28 showing that relativel y fewer (28%) of non-sp am reviews for a product are posted early within 7 months of launch. This is possible as not all genuine review s for a product are posted early within 7 months of launch . In fact , the bulk of the genuine reviews get accumulated overtime. We note that these statistics are in expectation and not for any single product. The 28% of genuine early reviews can be attributed to reviewing enthusiasts with keen interests in the products. For spam reviews , we find that the density tends to lean towa rds the right, but not extremely concentrated towards the right. It attains an expected value of 0.61 showing that about 61% spam reviews are posted within 7 months of product lauch date while the rest spam reviews accumulate later in the timeline. This dovetails with the findings in [26] that spammers usually post reviews early enough to cast a bigger impact.
 Rating Abuse (  X   X  X  X  ): Rating abuse measures the extent products are spammed with multiple ratings . The densities for its corresponding latent beharior,  X   X  X  X  X  are shown in Figure 2 (i). We find that a reasonably large percentage of spam reviews (70% in expectation) have been instances of imparting rating abuse (i.e., among the multiple reviews/ratings for the same product by the same reviewer -id). However for non-spam reviews, the expected value for this behavior is 0.13 . Thus, most non-spam reviews ( X  100-13 = 87%) are not instances of rating abuse (i.e., are single reviews by a reviewer on a product). Again, we note that our model estimates 13% of non-spam reviews being instances of rating abuse (i.e., multiple ratings on a product by a reviewer) which is a sma ll percentage. However, from the non -spam density we can say that the feature values for those reviews must be very low as the density drops to almost 0 beyond the feature value of 0.5. Again, this is understandable as sometimes a reviewer might review a prodcut more than once when the product experience changes from the first review (e.g., a installation fault, not knowing the correct me thod of using, e tc.).

Las tly, we note that although it is interesting to analyze the relative discriminative strength of each feature, it is not directly possible as we do not have ground-truth spamicity labels for reviewers in our data. Instead, the posterior denisity es timates of latent bahviors give us some clues to relative feature strengths.
This paper proposed a novel and principled method to exploit observed reviewing behaviors to detect opinion spammers (fake reviewers) in an unsupervised Bayesian infer ence framework. To our knowledge, this is the first such attempt. Existing methods are mostly based on heuristics and/or ad-hoc labels for opinion spam detection. The proposed model has its basis in the theoretic foundation of probabilistic model based clustering. The Bayesian framework facilitates characterization of many behavioral phenomena of opinion spammers using the estimated latent population distributions. It also enables detection and posterior density analysis in a single framework. This cannot be done by any of the existing methods. The paper also proposed a novel way to evaluate the results of unsupervised opinion spam models using supervised classification without the need of any manually labeled data. Finally, a comprehensive set of experiments based on the proposed automated classification evaluation and human expert evaluation have been conducted to evaluate the proposed model. The results across both evaluation metrics show that the proposed model is effective and outperforms strong competitors . This project is supported in part by a grant from HP L abs Innovation Research Program and a grant from National Science Foundation (NSF) under grant no. IIS -1111092.
