 Proteins are essential polymers involved in almost all biological functions. Di-rectly finding out the function of a protein is not easy, but knowing the localiza-tion of a protein in a cell can give us hints to understand the protein functions. Experimental determination of subcellular location is time-consuming and costly. With the number of protein sequences entering databanks rapidly increasing, the importance of developing a powerful tool to automatically identify protein sub-cellular location from protein sequences has become self-evident.
 dict protein subcellular locations. Most of them are grounded on global sequence properties, in particular, the compositions of amino acids under investigation. The amino acid consists of 20 components, each representing the occurrence fre-quency of one of the 20 native amino acids in a given protein. This approach was first suggested by Nakashima and Nishikwa [8]. They found that the intracel-lular and extracellular proteins could be accurately discriminated with only by amino acid composition. Several machine learning methods were then employed to improve the prediction accuracy. Cedano [2] adopted a statistical method with Mahalanobis distance for prediction; Reinhardt [9] used neural networks while Hua [5] constructed a prediction system using SVMs.
 pected that a higher accuracy should be gained with new sequence encoding schemes that can capture the sequence order features. Park [6] introduced the concept of amino acid pair compositions and applied SVMs classifier for pre-diction. Recently, Ying [10] employed a fuzzy k-NN algorithm based on amino acid dipeptide composition and his result was superior compared to the other methods.
 of dipeptide occurrences. In fact, the length of protein sequences is so varied that we need to consider it when estimating the dipeptide composition. Hereby, we propose a new representation of amino acid pair (including dipeptide) compo-sitions using normalization by sequence length. Furthermore, in order to detect more sequence-order features, we use different types of compositions to construct multiple classifiers, and then a combination method is employed expecting that prediction accuracy can be improved. Proteins are sequences that consist of a chain of units called amino acids. An amino acid is labeled as a letter in { A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y } thus a protein is presented as a string of letters. The length of proteins is varied from 50 to thousands so that it is not efficient to pass the whole sequences to the prediction system. Thus, to encode protein sequences, the compositions of a single amino acid and amino acid pair were proposed in previous works (see Introduction).
 amino acid pair) as number of dipeptide occurrences. If two proteins, one is long and the other is short, are in the same class, the dipeptide occurrences are so different that distance-based classifiers, such as fuzzy k-NN, could misclassify them. Therefore, we should consider the sequence length when defining the com-position. Statistically, there must be a relation between dipeptide occurrences and the sequence length. To simulate this relation we devise a positive strength parameter  X  which determine how heavily the sequence length is weighted when calculating the composition. The composition of n-gapped amino acid pair is then defined as follow: acid pair compositions) are proposed in order to extract more sequence-order features. In a sequence, an n -gapped amino acid pair is a couple of amino acids between which there are n other amino acids located. There are 20 natural amino acids, so a sequence is represented by a 400-dimensonal vector of amino acid pair compositions. 3.1 Fuzzy k-Nearest Neighbor Algorithm The k-nearest neighbor (k-NN) rule [4] is one of the oldest and simplest methods for performing nonparametric classification. The main idea of k-NN can be stated as following: given a test pattern x with unknown label, its label is assigned according to the labels of its k nearest neighbors in the training set. The k-NN is widely used in machine learning and has many variations. Among them, fuzzy k-NN [7] usually gives better classification performance, especially in biological and medical data classification problems [10].
 { c 1 ,c 2 , .., c c sified. At the beginning, the fuzzy k-NN assigns membership values for each pattern to different categories rather than a particular class as in k-NN rule. mated in several ways. In crisp initialization, v i ( x j )isassignedto1if x j belongs to class c i , otherwise it is assign to 0. After initializing membership values for all patterns in the training data, membership value of x to class c i is calculated as following equation: where k is the number of nearest neighbors; m is a fuzzy-strength parameter which determines how heavily the distance is weighted when calculating each nearest neighbor X  X  contribution to the membership value; d ( x, x ( j ) ) is the dis-tance between x and its j th nearest neighbor x ( j ) . Finally, the pattern x is classified to class to which the membership value of x is maximum. 3.2 Voting Fuzzy kNN Classifiers As stated above, we use amino acid composition and 0-3 gapped amino acid pair compositions as features to build prediction system. For each type of compo-sitions we construct a fuzzy k-NN classifier then a voting scheme is applied to combine the classifiers. By incorporating many classifiers, the model is expected to capture more sequence-order features and be more stable.
 we assign membership values for input pattern to every classes. Given unlabeled pattern x ,let  X  ( j ) i ( x ) be the membership value that jth classifier assign for x to class i , the membership value assigned by combined classifier is estimated as Eq. 2, and x is predicted as the class to which the total membership value is maximum. Three datasets that were used in the previous works are investigated, including FuzzyLoc dataset [10], PLoc dataset [6] and Reinhardt X  X  dataset [9]. In FuzzyLoc dataset, there are 7203 eukaryotic proteins classified in 11 subcellular locations. The PLoc dataset contains 7579 eukaryotic proteins in 12 locations and Rein-hardt X  X  dataset has 2427 protein located in 4 locations. Jackknife test (leave-one-out) is employed to evaluate the algorithm performance. 4.1 Searching for Optimal  X  Value To search for the value of  X  that can best adapt to fuzzy k-NN classifier we applied that classification algorithm to the three datasets with various  X  values. dipeptide compositions as features for classifying. While calculating dipeptide compositions,  X  value is varied from 0 to 1 with step 0.1. Classification algorithm performance is showed in Fig. 1. Ignoring sequence length factor (  X  =0), the accuracies are 85.2 for Reinhardt X  X  dataset, 80.1% for PLoc dataset and 80.1% for FuzzyLoc dataset, almost the same as Ying X  X  result. On the contrary, when the composition is estimated as fraction of dipeptide (  X  =1), the accuracies are dramatically decreased. Maximum accuracies are reached when  X  is equal to 0.5. We have investigated this test with various values of fuzzy strength parameter m and number of nearest neighbor parameter k . In all the cases 0.5 is still the best value. Therefore, when estimating the composition of dipeptides, we select  X  as 0.5. 4.2 Prediction Accuracy Our voting procedure is devised in order to best utilize the potentials of 5 differ-ent representations of sequences. To show the effectiveness of our voting scheme, here, we apply it to FuzzyLoc dataset then analyze the result. The accuracies of individual classifiers and the voted one are shown in Table 1 (column 2 to 7). As shown in the table, there is a definite trend of improved accuracies in every protein classes. Apparently, the incorporation of different types of compositions can better capture sequence-order effects in protein sequences. The last column in Table 1 shows Ying X  X  results. He used fuzzy k-NN based on amino acid dipep-tide composition, but the equation of the composition is different from ours. By proposing a new formula and adding more sequence-order features, we can make 5.7% improvement compared to Ying X  X  result.
 and Reinhardt X  X  dataset then compare the result with other previous works. Park and his colleague used SVMs with the same features as ours, but they did not explain the formula for calculating the amino acid pair compositions. The overall accuracy they got was 78.2% on PLoc, their own dataset. Our new formula and voting fuzzy k-NN method can achieve 8.8% improvement compared to Park X  X  result. On Reinhardt X  X  dataset, there were many methods being applied including neural networks, SVMs, Markov chain, fuzzy k-NN. Among them, our method can give the best performance (result is not shown here). Although this dataset may be old and cover only 4 protein classes, the result can demonstrate applicability of our relative simple method. In this paper, for classifying protein subcellular locations we first proposed an equation for estimating the composition of amino acid pairs from protein se-quences. Then, we employed a voting procedure for combining fuzzy k-NN classi-fiers which were built from multiple amino acid pair compositions. This method could take the advantage of sequence-order and sequence-length effects in se-quences. We have applied our method to several datasets and high predictive accuracies were achieved using a jackknife test. Our method is simple and prac-tical because it just needs raw sequence data. In the future we will use this method to annotate protein database.

