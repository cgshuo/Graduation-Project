 Non-negative matrix factorization (NMF) has been success-fully applied in document clustering. However, experiments on short texts, such as microblogs, Q&amp;A documents and news titles, suggest unsatisfactory performance of NMF. An major reason is that the traditional term weighting schemes, like binary weight and t df , cannot well capture the terms' discriminative power and importance in short texts, due to the sparsity of data. To tackle this problem, we pro-posed a novel term weighting scheme for NMF, derived from the Normalized Cut (Ncut) problem on the term affinity graph. Different from idf , which emphasizes discriminabil-ity on document level, the Ncut weighting measures terms' discriminability on term level. Experiments on two data sets show our weighting scheme signi cantly boosts NMF's performance on short text clustering.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.3 [ Pattern Recognition ]: Clus-tering Short Text, Clustering, NMF, Normalized Cut
Short texts are prevalent on the web nowadays, such as mi-croblogs, SNS statuses, and instant messages, etc. They are with limited document length, typically only tens of words or even less on average. Successfully clustering these data is very important in many web applications, e.g. emerging top-ics discovery, efficient index and retrieval, and personalized recommendation. However, traditional document clustering c orresponding author Fi gure 1: frequency of (a) tf values, (b) idf values of terms in the Questions data. methods cannot accomplish the task effectively due to the insufficient representation of short texts.

One crucial question, for conventional clustering methods like Kmeans and NMF[5], is how to weight terms in such short documents. For normal texts, the widely used weight-ing method is t df , de ned as follows: where tf t;d is the term frequency of term t in document d , measuring the importance of t in d ; df t is the document frequency of term t in corpus, measuring the discriminative power of t over the entire corpus; and N is the total number of documents in corpus. However, in short text both tf and idf are not very differentiable. Figure 1(a) shows the distribution of tf values in the Tweets data, suggesting that about 96% of them with equal to 1. In other words, most of terms usually occur only once in a short document. Figure 1(b) shows the distribution of idf values, which is dominated by values larger than 8. What is even worse is that about 65% of terms have the same idf value 11.51|the highest one of all.

Both of the two problems of t df come from the insuffi-cient representation of documents, indicating it is not a good idea to weight terms with only document level information. Instead, we propose a novel term weighting scheme for NMF on short text clustering based on words co-occurrence infor-mation. This weight is derived from the Normalized Cut (Ncut) problem[3] on term affinity graph, referred as Ncut-weight of terms (Section 2). We show the detail of the Ncut-weighted NMF solution with the Alternating Non-negative Least Squares (ALNS) algorithm (section 3). Both quali-ta tive and quantitative evaluations were conducted on two short text data sets: tweets, and web page titles (Section 4). The results demonstrate the effectiveness of the Ncut-weighted NMF.
NMF clusters documents and words simultaneously. How-ever, we rstly consider the sub-problem of words clsutering alone, which can be formulated as a graph cut problem. Then, we derive the new term weighting scheme by bridging Ncut on predi ned term affinity graph and the traditional NMF method on term-document matrix.

Notations : Let M be the number of distinct terms, N be the number of documents, K be the number of clusters. X = f x ij g2 R M N denotes the term-document matrix 1 . Moreover, let x i denote the i th column vector of X , x ( j ) denote the j th row vector of X .
It is known that words semantic relations can be induced from their co-occurrence frequency. The basic assumption is that if words co-occur frequently, they are likely to seman-tically related. Based on this assumption, we constructed a term affinity graph G = f V; E g to model term similarity according to their co-occurrences. In which, V is given by the term set in corpus, while edge set E determined by pre-de ned adjacent matrix S = f s ij g2 R M M . For example, while inner product is used to measure the similarity, occurrences of term i and term j . If each x ( i ) is normalized with unit length, s ij is the cosine similarity.
Clustering terms is equivalents to cut graph G into K sub-graphs. A typical criterion to do that is called the normalized cut criterion[3] that minimizes the normalized weight summation of edges between these sub-graphs. Let f
G k g k =1 ;::;K be a partition, sub-graph G k b e the comple-ment of sub-graph G k , and S ( G k ; G k  X  ) = i.e. the weight summation of edges between sub-graph G k mizing the following discrete objective function: where S ( G k ; G ) = S ( G k ; G k ) + S ( G k ; G k ).
Let D 2 R M M be the diagonal degree matrix of S , with non-zero entries d ii = matrix U 2 R M K , with each entry u ik indicates whether term i belongs to sub-graph G k :
Th ere are various various schemes to determine x ij , such as tf or t df . In this paper, we takes x ij as binary weight, i.e. if term i occurs in document j , x ij = 1, otherwise 0. It has been proven that the normalized cut criterion can be represented by the following trace maximization problem[6]: where U subjects to constraint (3), which has U T U = I .
Directly tackling the problem of (4) is NP-hard. However, we will show that the approximation solution to this problem can be obtained by NMF with the appropriate weighting scheme.
 Theorem 1. Non-negative factorization on matrix Y = D 1 = 2 X equals to solving (4) with the discrete constraint Eq. (3) relaxed.

Proof. Let  X  X  X  F denote the Frobenius norm. The ob-jective function of NMF on Y can be written as By set the gradient of J over V to zero:
If U T U is non-singular 2 , we get V = ( U T U ) 1 U T Y . Sub-stitute it to J and discard constants, minimization of J is equivalent to If with the constraint U T U = I , we have max When U 0 and U T U = I , V = U T Y 0. Therefore, NMF on Y solves problem (4) with the discrete constraint Eq. (3) relaxed. In previous section, we derive a term weighting matrix D 1 = 2 , i.e. the weight of term i is In practice, it is better to scale all weights into [0 ; 1], by simply dividing each w i by max ( f w i g i =1 ;:::;M ). We have to stress that our Ncut-weight is a term weight scheme. [5] uses a similar weight, but actually it is document weight.
From Eq. (5), we can see that if a term co-occurs more frequently with more others, its Ncut-weight will be lower. Comparing with idf , the Ncut-weight favors terms with low co-occurrence frequency, rather than document frequency. That is because the words co-occuring frequently tend to to be meaningless or polysemous words, which is not dis-criminative for clustering. Besides, the term co-occurrence frequency is not highly depend on the document length as document frequency does.
I t is always true, since K  X  M .
F or Ncut-weighted NMF, we also add  X  2 norm regularizer to avoid over tting for such sparse data. The overall object function of Ncut-weighted NMF is We employ the alternating non-negative least squares(ANLS) algorithm [1] to solve it, as shown in Algorithm 1. A lgorithm 1: The ANLS algorithm for Ncut-weighted NMF
We carried out experiments on two data sets. 1) Tweets data, collected from twitter.com. 2) Titles data, news titles with assigned class labels from some news websites, which is published by Sogou Lab 3 .

The raw data is preprocessed via the following steps: 1) stemming and removing stop words; 2) removing words with document frequency less than 6; 3) removing documents con-taining less than 4 words. The data characteristics after preprocessing are summarized in Table 1.
 Da ta sets #d oc #w ord a vg words y #c lass Tw eets 4 520 25 02 8. 5958 un available
Ti tles 2 630 14 03 5. 2684 9 Our baseline methods include: h ttp://www.sogou.com/labs/dl/tce.html Fi gure 2: Distribution of Ncut-weights on Tweets data.
 Table 2: idf and Ncut-weight behave different as in this example from the Twitter Tweets data term id f (rank) Nc ut-weight(rank)  X  X a nk h umidity 5. 238(2054) 0. 147(640) +1 414 pittsburgh 5. 931(1454) 0. 130(988) +4 66 video 6. 625(659) 0. 200(161) +4 98 ca p 6. 626(524) 0. 141(764) -2 40 org 6. 114(1217) 0. 108(1477) -2 60 refuse 6. 018(1380) 0. 103(1578) -1 98
As we saw in section 1, the idf weighting scheme has the problem of skew to high values in short texts. However, the Ncut-weight refrains from such problem by counting the term co-occurrence frequency instead of the document fre-quency. Figure 2 shows the distribution of Ncut-weights in Tweets data after preprocessed, which resembles a Gaussian distribution, and is much atter than idf.

We further extracted some terms from the Tweets Data to explain that the two weighting schemes behave differently. In Table 2, the second and third columns show idf and the Ncut-weights of the example terms, respectively. Numbers in the parentheses denote the rank of terms while ordering by the weights in descending order. We can see that the rst three words are very discriminative, but the weight is underestimated by idf . While the last three words with weak discriminability are overestimated by idf . Yet Ncut-weight scheme produces more reasonable weights of them.
We extracted four clusters from results on Tweets data as shown in Table 3. Top 5 weighted terms are presented in each cluster. It shows that: 1) some clusters found by Kmeans are not meaningful, like cluster 2 and cluster 3; 2) RLSI with idf is better than Kmeans, but still with some noise words in top weighted terms. For example, \febru-ary" ranks in the rst place in cluster 3, but it is not di-rectly related to weather; 3) \RLSI+nc" works better than \RLSI+idf"; 4) NMF with binary weights fails in cluster 4; 5) NMF with idf nds clusters more readable than\NMF+01". But some common words still rank higher than some others with more discriminability, like \social" and \medium" rank higher than \company"; 6) \NMF+nc" produces the most readable result. Besides, the results are very similar with \RLSI+nc" in these clusters.
Since each document in the Titles data has a unique class label, we can evaluate the clustering results automatically. Three popular measures in clustering are used: purity, ad-justed random index (ARI), and normalized mutual infor-mation (NMI)[2].

Figure 3 shows the clustering results of all the methods with respect to different K on Titles data. It is clear that \NMF+nc" outperforms all the baselines signi cantly in all evaluation metrics, especially when K is larger than 3. Be-sides, it is not surprise that \RLSI+nc" also shows great improvement than \RLSI+idf". Additionally, \NMF+idf" achieves slightly better result than \NMF+01", since the bi-nary weights are least discriminative, while Kmeans works worst on these short texts.
Term weighting is important for NMF in document clus-tering. Conventional weighting schemes, like binary weights and t df , are not effective for short text clustering. We have proposed a novel term weight called Ncut-weight, which measures term's discriminability according to the words co-occurrences. The experiments show that the clustering per-formance of NMF is greatly improved with terms weighted by the Ncut-weight. This research work was funded by the National Natural Science Foundation of China under Grant No. 60933005, No. 61173008, No. 61003166 and 973 Program of China under Grants No. 2012CB316303.
