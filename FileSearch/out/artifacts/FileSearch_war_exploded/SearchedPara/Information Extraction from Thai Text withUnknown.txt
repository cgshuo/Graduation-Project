 By incorporation of ontology-based semantic annotations and appropriate ex-traction filtering techniques, it is expect ed that information extraction (IE) based on patterns of triggering class tags and triggering plain words can be realized for Thai text without deep sentence analysis . Initially inspired by the need to gen-erate multi-slot semantic frames representing symptom descriptions from Thai text in a large-scale medical-related knowledge base construction project and subsequently by similar needs in other application domains, we aim to develop a framework for IE from Thai textual documents. A well-known supervised rule learning algorithm, called WHISK [2], is used as the core algorithm for con-structing pattern-based IE rules automatically from a set of hand-tagged training phrases. Information sources of our target IE tasks are, however, collections of text paragraphs, called information entries , rather than collections of text por-tions identified beforehand as potential target phrases. Each information entry typically contains several target phrases along with other text portions. Locat-ing potential target phrases in an information entry requires at least a shallow parser (chunk parser), which is not currently available for Thai text. A method, called rule application using sliding windows (RAW) , is introduced for applying IE rules to information entries without predetermining target-phrase boundaries.
Using sliding windows, IE rules are applied to text portions regardless of phrase boundaries and, therefore, tend to make many false-positive extractions. An extraction filtering module, called wildcard-instantiation filtering (WIF) ,is employed for removal of incorrect extrac tions by employment of a binary classi-fier to predict rule application across a target-phrase boundary. Application of standard classification models, such as Na  X   X ve Bayes, k -Nearest Neighbor, Sup-port Vector Machine, and Decision Tree, to such prediction is evaluated. The framework is applied to textual information entries in three different application domains with different target-phrase density and average lengths, i.e., medical-symptom descriptions, soccer match repor ts, and housing advertisements. Infor-mation entries in the first two domains are free text, i.e., grammatical plain text, while those in the third domain are semi-structured, i.e., a class of ungrammat-ical text that is often telegraphic in style but does not follow any rigid format. To begin with, Section 2 gives an overview of our framework. After recalling the basic idea of WHISK, Section 3 explains the RAW method. Section 4 describes WIF and Section 5 presents experimental results. Text paragraphs are taken as input information entries for our IE tasks. Word segmentation is applied to all information entries as part of a preprocessing step. A domain-specific ontology, along with a lexicon for it, is then employed to partially annotate word-segmented phrases with tags denoting the semantic classes of occurring words with respect to the lexicon. As outlined in Fig. 1, from manually identified target phrases in a training corpus, the WHISK algorithm is used for automatically constructing a set of IE rules. During rule learning, target-phrase lengths are observed whe n a rule makes correct extractions on the training instances and they are used for determining an appropriate window size for RAW. In order to prepare training data for construction of binary classifiers for predicting rule instantiation across target-phrase boundaries in the WIF module, the IE rules obtained from WHISK are applied using RAW to the partially annotated information entries in the training corpus. RAW and WIF are then used together to extract semantic frames from text in a test corpus.
Fig. 2 illustrates a portion of word-segmented and partially annotated infor-mation entry describing acute bronchitis, obtained from the text-preprocessing phase, where  X  |  X  indicates a word boundary,  X   X   X  signifies a space, and the tags  X  X ec, X   X  X ol, X   X  X ym, X   X  X rg, X  and  X  X time X  denote the semantic classes  X  X ecre-tion, X   X  X olor, X   X  X ymptom, X   X  X rgan, X  and  X  X ime period, X  respectively, in our medical-symptom domain ontology. The portion contains three target symptom phrases, which are underlined in the figure. Fig. 3 shows the frame required to be extracted from the second underlined symptom phrase in Fig. 2. It contains three slots, i.e., Sym , Loc ,and Per , which stand for  X  X ymptom, X   X  X ocation, X  and  X  X eriod, X  respectively. WHISK [2] induces rules top-down, starting from the most general rule that covers all training instances, and then specializing the initial rule by adding triggering terms one at a time in order to prevent rule application with incorrect extractions. Fig. 4 gives a typical example of a WHISK IE rule. Its pattern part contains three triggering class tags, one triggering plain word, and four instanti-ation wildcards. The three triggering class tags also serve as slot markers  X  X he terms into which they are instantiated are taken as fillers of their respective slots in the resulting extracted frame. When instantiated into the target phrase in Fig. 3, this rule yields the extracted frame shown in the same figure.
A WHISK rule does not have ability to automatically segment a free-text information entry so that the rule can be applied to the relevant portion of text [1]. A sliding window technique is introduced to locate text portions for rule application X  X  method called rule application using sliding windows (RAW) . Using a k -word sliding window, a rule r is applied to each k -word portion of an information entry one-by-one sequentially. More precisely, assume that an information entry E consisting of n words is given and for any l , m such that 1  X  l  X  m  X  n ,the[ l, m ] -portion of E is the portion beginning at the l th word position and ending at the m th word position of E .Then r is applied to the [ i, i + k  X  1]-portion of E for each i such that 1  X  i  X  n  X  k + 1. An application that results in a duplicated frame is discarded.

As illustrated in Fig. 5, when the rule in Fig. 4 is applied to the information entry in Fig. 2 using a 10-word sliding window, it makes extractions from the [21 , 30]-portion, the [33 , 42]-portion, and the [34 , 43]-portion of the entry. Table 1 shows the resulting extracted frames. Only the extractions made from the first and third portions are correct. When the rule is applied to the second portion, the slot filler taken through the first slot marker of the rule, i.e.,  X  X ym, X  does not belong to the symptom phrase containing the filler taken through the second slot marker of it, i.e.,  X  X rg, X  whence an incorrect extraction occurs. WHISK learns an extraction pattern in terms of triggering terms for making an extraction from a  X  X ingle X  target phrase. Using RAW, however, an obtained IE rule may be instantiated across a target-phrase boundary (cf. Fig. 5 and Table 1), yielding an extracted frame containing unrelated slot fillers, which is definitely an incorrect extraction, i.e., a false positive. Instantiations of wildcards occurring between the first and the last slot markers of a rule, called internal wildcards , provide a clue to detect such an undesirable extraction X  X f an internal wildcard of a rule is instantiated across a target-phrase boundary, then unrelated slot fillers are extracted by the resulting rule instantiation. Based on this idea, a filtering module, called wildcard-instantiation filtering (WIF) , is developed.
A binary classifier is constructed for each rule from observations obtained from applying the rule to textual information entries in the training corpus by assuming that rules obtained from WHISK are error-free with respect to their training target phrases. Under this assumption, an incorrect extraction in the training corpus implies the existence of an internal-wildcard instantiation across a target-phrase boundary. Given a text portion T and a rule r containing m internal wildcards w 1 ,...,w m ,the instantiation feature vector for w i (1  X  i  X  m ) observed at T is a vector f =[ f 1 ,...,f n ] consisting of n features selected for representing the instantiation of w i when r is applied to T ,andthe wildcard-instantiation feature vector for r observed at T is then defined as a vector x = f w j observed at T and  X   X  denotes vector concatenation.

The rule in Fig. 4, for example, has three internal wildcards, i.e., those occur-ring between  X  X ym X  and  X  X time X  in its pattern. Table 2 illustrates the instanti-ation feature vectors for the first internal wildcard of this rule observed at the three text portions in Fig. 5 when the number of spaces, the number of plain words, and the presence of the specific plain word indicated in the title of the fifth column are selected as features. At the [21 , 30]-portion, for example, the first internal wildcard is instantiated into a string consisting of no occurrence of space and two plain words, one of which is the indicated specific word, yielding [0 , 2 , 1] as the resulting instantiation feature vector. Likewise, the instantiation feature vectors for this wildcard observed at the [33 , 42]-portion and the [34 , 43]-portion are [0 , 2 , 0] and [0 , 1 , 1], respectively. Data Sets, Output Templates, and Training Process The proposed frame-work is evaluated in three different domains of Thai text: medical-symptom de-scriptions (MD) , soccer match reports (SR) ,and housing advertisements (HA) . An information entry in the MD domain contains free text describing various symptoms of a disease and that in the SR domain contains news-story-style free text reporting a soccer match in details . Text in the HA domain, by contrast, is semi-structured by nature. 115, 86, and 189 information entries with the average length of 45.0, 68.6, and 64.3 words per entry in the MD, SR, and HA domains, respectively, are used in our exploratory evaluation. The collected information entries are preprocessed using a word segmentation program and are then par-tially annotated with semantic class tags using predefined ontology lexicons.
Four types of target phrases are considered in our experiments: two of them are from the MD domain, referred to as Type-MD1 and Type-MD2 ;onefromtheSR domain, referred to as Type-SR ; and the other one from the HA domain, referred to as Type-HA . Table 3 gives the output-template forms for the four types along with their intended meanings. The slot Per in the Type-MD1 template as well as the slot Time in the Type-SR template is optional. One of the slots Loc and Per , but not both, may be omitted in the Type-MD2 template. One arbitrary slot in the Type-HA template may be omitted.

For each template type, rules are crea ted using WHISK by randomly selecting and adding hand-tagged target phrases as new training instances until the 10 most recently selected phras es cause no creation of any new rule. All remaining information entries are then used as test data. Table 4 shows the number of all distinct target phrases in the training data set and the test data set obtained accordingly for each template type, and characterizes the data sets in terms of target-phrase length and target-phrase density.

Using our implementation of WHISK, 15, 11, 8, and 9 rules, respectively, are au-tomatically generated for Type-MD1, Type-MD2, Type-SR, and Type-HA. The length of the longest target phrase obser ved when a rule yields correct extrac-tions on its training set is taken as the base window size for the rule. By apply-ing the obtained rules to the information entries in their respective training sets using RAW, wildcard instantiations are observed and used as training data for constructing classifiers for the WIF module. Two kinds of features are used for representing wildcard instantiations: first, the number of spaces, the number of plain words, and the number of annotated words occurring in a text portion into which a wildcard is instantiated; and s econdly, the presence or absence of certain specific terms in such a text portion. For feature selection, the information gain calculated from the training data is used as a measure of the relevance of each fea-ture. Top 10% of features ranked according to their information gain values are selected. The number of selected features accordingly obtained for different rules varies from 2 to 8, with an average of 4 sel ected features per rule. The number of space occurrences is the most commonly s elected feature. The average numbers of specific words whose presence is used as a selected feature are 1.5, 3.8, and 2.6, respectively, in the MD, SR, and HA domains.
 Experimental Results. The Weka machine learning suite is employed for classifier learning and evaluation, using its default parameters. Four standard models are used, i.e., Na  X   X ve Bayes (NB), k -Nearest Neighbor ( k NN), Support Vector Machine (SVM) based on the RBF k ernel, and Decision Tree (DT) using C4.5. 3NN performs slightly better than 1NN, 5NN, and 7NN in our training data sets and is chosen as a representative of k NN. Recall and precision are used as performance measures, where the former is the proportion of correct extractions to relevant target phrases and the latter is that of correct extractions to all obtained extractions. Table 5 shows the evaluation results obtained from RAW without any extraction filtering and RAW with WIF using NB, k NN, SVM, and DT classifiers, where  X  X  X  and  X  X  X  stand for recall and precision, which are given in percentage. Compared to the results obtained using RAW alone, each of the four classification models impro ves precision while satisfac torily preser ving the recall value of RAW in every row of this table. None of the four classifiers performs obviously better than another one.

Table 6 presents the overall results obtained by doubling the window size previously used. Compared to Table 5, recall for Type-MD1, most phrase in-stances of which are short (cf. Table 4), increases only slightly; by contrast, for Type-SR and Type-HA, phrase instances of which are relatively long, obvious improvements of recall are obtained. While k NN, SVM, and DT yield slightly higher recall than NB, all the four classifiers give similar precision.
To compare our framework to rule application with known target-phrase boundaries, we manually locate all target phrases in the test sets and apply the generated rules directly to these manua lly identified text portions. The recall values obtained from such known-boundary extraction for Type-MD1, Type-MD2, Type-SR, and Type-HA are 88.8, 100.0, 96.2, and 94.9, respectively, with the precision of 97.9, 100.0, 96.2, and 84.0, respectively. Compared to Tables 5 and 6, in the MD domain, where the average target-phrase length is small, the performance obtained when the base window size is used is already close to that of known-boundary extraction, and an extension of the window size does not ob-viously affect the extract ion performance. However, in the SR and HA domains, where target phrases are longer, increasing the window size makes the extraction performance significantly closer to that of the known-boundary case. To apply WHISK IE rules to textual paragraph-like information entries without predetermining target-phrase boundaries, rule application using sliding windows (RAW) is introduced. A filtering component is proposed for removal of false positives resulting from rule application across target-phrase boundaries (WIF). The experimental results show that this filtering method improves extraction precision while satisfactorily preserving the recall performance of sliding-window rule application. When the window size is sufficiently large, the extraction per-formance of the proposed framework appears to be fairly close to that of rule application with manually located target phrases.
 Acknowledgement. This work was supported by the Thailand Research Fund (TRF), under Grant No. PHD/0056/2550 (TRF-RGJ) and No. BRG50800013.

