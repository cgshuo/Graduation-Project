 Producing large-scale training, validation and test sets is vital for many applications. Most often this job has to be carried out  X  X y hand X  and thus it is delicate, expensive, and tedious. Services such as Amazon Mechanical Turk (MTurk) have made it easy to distribute simple labeling tasks to hundreds of workers. Such  X  X rowdsourcing X  is increasingly popular and has been used to annotate large datasets in, for example, Computer Vision [8] and Natural Language Processing [7]. As some annotators are unreliable, the common wisdom is to collect multiple labels per exemplar and rely on  X  X ajority voting X  to determine the correct label. We propose a model for the annotation process with the goal of obtaining more reliable labels with as few annotators as possible.
 It has been observed that some annotators are more skilled and consistent in their labels than others. We postulate that the ability of annotators is multidimensional; that is, an annotator may be good at some aspects of a task but worse at others. Annotators may also attach different costs to different kinds of errors, resulting in different biases for the annotations. Furthermore, different pieces of data may be easier or more difficult to label. All of these factors contribute to a  X  X oisy X  annotation process resulting in inconsistent labels. Although approaches for modeling certain aspects of the annotation process have been proposed in the past [1, 5, 6, 9, 13, 4, 12], no attempt has been made to blend all characteristics of the process into a single unified model.
 This paper has two main contributions: (1) we improve on current state-of-the-art methods for crowdsourcing by introducing a more comprehensive and accurate model of the human annota-tion process, and (2) we provide insight into the human annotation process by learning a richer representation that distinguishes amongst the different sources of annotator error. Understanding the annotation process can be important toward quantifying the extent to which datasets constructed from human data are  X  X round truth X .
 We propose a generative Bayesian model for the annotation process. We describe an inference algorithm to estimate the properties of the data being labeled and the annotators labeling them. We show on synthetic and real data that the model can be used to estimate data difficulty and annotator Figure 1: (a) Sample MTurk task where annotators were asked to click on images of Indigo Bunting (described biases, while identifying annotators X  different  X  X reas of strength X . While many of our results are valid for general labels and tasks, we focus on the binary labeling of images. The advantages and drawbacks of using crowdsourcing services for labeling large datasets have been explored by various authors [2, 7, 8]. In general, it has been found that many labels are of high quality [8], but a few sloppy annotators do low quality work [7, 12]; thus the need for efficient algorithms for integrating the labels from many annotators [5, 12]. A related topic is that of using paired games for obtaining annotations, which can be seen as a form of crowdsourcing [10, 11]. Methods for combining the labels from many different annotators have been studied before. Dawid and Skene [1] presented a model for multi-valued annotations where the biases and skills of the annotators were modeled by a confusion matrix. This model was generalized and extended to other annotation types by Welinder and Perona [12]. Similarly, the model presented by Raykar et al. [4] considered annotator bias in the context of training binary classifiers with noisy labels. Building on these works, our model goes a step further in modeling each annotator as a multidimensional classifier in an abstract feature space. We also draw inspiration from Whitehill et al. [13], who modeled both annotator competence and image difficulty, but did not consider annotator bias. Our model generalizes [13] by introducing a high-dimensional concept of image difficulty and combin-ing it with a broader definition of annotator competence. Other approaches have been proposed for non-binary annotations [9, 6, 12]. By modeling annotator competence and image difficulty as multidimensional quantities, our approach achieves better performance on real data than previous methods and provides a richer output space for separating groups of annotators and images. An annotator, indexed by j , looks at image I i and assigns it a label l ij . Competent annotators provide accurate and precise labels, while unskilled annotators provide inconsistent labels. There is also the possibility of adversarial annotators assigning labels that are opposite to those assigned by competent annotators. Annotators may have different areas of strength, or expertise , and thus provide more reliable labels on different subsets of images. For example, when asked to label images containing ducks some annotators may be more aware of the distinction between ducks and geese while others may be more aware of the distinction between ducks, grebes, and cormorants (visually similar bird species). Furthermore, different annotators may weigh errors differently; one annotator may be intolerant of false positives, while another is more optimistic and accepts the cost of a few false positives in order to get a higher detection rate. Lastly, the difficulty of the image may also matter. A difficult or ambiguous image may be labeled inconsistently even by competent annotators, while an easy image is labeled consistently even by sloppy annotators. In modeling the annotation process, all of these factors should be considered. We model the annotation process in a sequence of steps. N images are produced by some image capture/collection process. First, a variable z i decides which set of  X  X bjects X  contribute to producing an image I i . For example, z i  X  X  0 , 1 } may denote the presence/absence of a particular bird species. A number of  X  X uisance factors, X  such as viewpoint and pose, determine the image (see Figure 1). Each image is transformed by a deterministic  X  X isual transformation X  converting pixels into a vector of task-specific measurements x i , representing measurements that are available to the visual system of an ideal annotator. For example, the x i could be the firing rates of task-relevant neurons in the brain of the best human annotator. Another way to think about x i is that it is a vector of visual attributes (beak shape, plumage color, tail length etc) that the annotator will consider when deciding on a label. The process of transforming z i to the  X  X ignal X  x i is stochastic and it is parameterized by  X  , which accounts for the variability in image formation due to the nuisance factors.
 An annotator j  X  J i , selected to label image I i , does not have direct access to x i , but rather to y ij = x i + n ij , a version of the signal corrupted by annotator-specific and image-specific  X  X oise X  n ij . The noise process models differences between the measurements that are ultimately available to individual annotators. These differences may be due to visual acuity, attention, direction of gaze, etc. The statistics of this noise are different from annotator to annotator and are parametrized by  X  j . Most significantly, the variance of the noise will be lower for competent annotators, as they are more likely to have access to a clearer and more consistent representation of the image than confused or unskilled annotators.
 The vector y ij can be understood as a perceptual encoding that encompasses all major components that affect an annotator X  X  judgment on an annotation task. Each annotator is parameterized by a unit vector  X  w j , which models the annotator X  X  individual weighting on each of these components. In this way,  X  w j encodes the training or expertise of the annotator in a multidimensional space. The scalar assigns a label l ij = 1 , and l ij = 0 otherwise. Putting together the assumptions of the previous section, we obtain the graphical model shown in Figure 1. We will assume a Bayesian treatment, with priors on all parameters. The joint probability distribution, excluding hyper-parameters for brevity, can be written as variables. This section describes further assumptions on the probability distributions. These as-sumptions are not necessary; however, in practice they simplify inference without compromising the quality of the parameter estimates.
 Although both z i and l ij may be continuous or multivalued discrete in a more general treatment of the model [12], we henceforth assume that they are binary, i.e. z i ,l ij  X  { 0 , 1 } . We assume a Bernoulli prior on z i with p ( z i = 1) =  X  , and that x i is normally distributed 1 with variance  X  2 z , where  X  z =  X  1 if z i = 0 and  X  z = 1 if z i = 1 (see Figure 2a). If x i and y ij are multi-dimensional, then  X  j is a covariance matrix. These assumptions are equivalent to using a mixture of Gaussians prior on x i .
 The noisy version of the signal x i that annotator j sees, denoted by y ij , is assumed to be generated by We assume that each annotator assigns the label l ij according to a linear classifier. The classifier is Figure 2: Assumptions of the model. (a) Labeling is modeled in a signal detection theory framework, where out y ij and put l ij in direct dependence on x i , where  X (  X  ) is the cumulative standardized normal distribution, a sigmoidal-shaped function. In order to remove the constraint on  X  w j being a direction, i.e. k  X  w j k 2 = 1 , we reparameterize the we give them Gaussian priors parameterized by  X  and  X  respectively. The prior on  X  j is centered at the origin and is very broad (  X  = 3 ). For the prior on w j , we kept the center close to the origin to be initially pessimistic of the annotator competence, and to allow for adversarial annotators (mean 1, std 3). All of the hyperparameters were chosen somewhat arbitrarily to define a scale for the parameter space, and in our experiments we found that results (such as error rates in Figure 3) were quite insensitive to variations in the hyperparameters. The modified Equation 1 becomes, The only observed variables in the model are the labels L = { l ij } , from which the other parameters have to be inferred. Since we have priors on the parameters, we proceed by MAP estimation, where we find the optimal parameters ( x ? ,w ? , X  ? ) by maximizing the posterior on the parameters, where we have defined m ( x,w, X  ) = log p ( L ,x,w, X  ) from Equation 4. Thus, to do inference, we need to optimize To maximize (6) we carry out alternating optimization using gradient ascent. We begin by fixing the x parameters and optimizing Equation 6 for ( w, X  ) using gradient ascent. Then we fix ( w, X  ) and optimize for x using gradient ascent, iterating between fixing the image parameters and annotator parameters back and forth. Empirically, we have observed that this optimization scheme usually converges within 20 iterations. Figure 3: (a) and (b) show the correlation between the ground truth and estimated parameters as the number In the derivation of the model above, there is no restriction on the dimensionality of x i and w j ; they may be one-dimensional scalars or higher-dimensional vectors. In the former case, assuming  X  w j = 1 , the model is equivalent to a standard signal detection theoretic model [3] where a signal y ij is generated by one of two Normal distributions p ( y ij | z i ) = N ( y ij |  X  z ,s 2 ) with variance s =  X  2 z +  X  2 j , centered on  X  0 =  X  1 and  X  1 = 1 for z i = 0 and z i = 1 respectively (see Figure 2a). In signal detection theory, the sensitivity index, conventionally denoted d 0 , is a measure of how well the annotator can discriminate the two values of z i [14]. It is defined as the Mahalanobis distance between  X  0 and  X  1 normalized by s , Thus, the lower  X  j , the better the annotator can distinguish between classes of z i , and the more  X  X ompetent X  he is. The sensitivity index can also be computed directly from the false alarm rate f distribution [14]. Similarly, the  X  X hreshold X , which is a measure of annotator bias, can be computed by  X  =  X  1 2  X   X  1 ( h ) +  X   X  1 ( f ) . A large positive  X  means that the annotator attributes a high cost to false positives, while a large negative  X  means the annotator avoids false negative mistakes. Under the assumptions of our model,  X  is related to  X  j in our model by the relation  X  =  X   X  j /s . In the case of higher dimensional x i and w j , each component of the x i vector can be thought of as an attribute or a high level feature. For example, the task may be to label only images with a particular bird species, say  X  X uck X , with label 1 , and all other images with 0 . Some images contain no birds at all, while other images contain birds similar to ducks, such as geese or grebes. Some annotators may be more aware of the distinction between ducks and geese and others may be more aware of the distinction between ducks, grebes and cormorants. In this case, x i can be considered to be 2-dimensional. One dimension represents image attributes that are useful in the distinction between ducks and geese, and the other dimension models parameters that are useful in distinction between ducks and grebes (see Figure 2c). Presumably all annotators see the same attributes, signified by x i , but they use them differently. The model can distinguish between annotators with preferences for different attributes, as shown in Section 5.2.
 Image difficulty is represented in the model by the value of x i (see Figure 2b). If there is a particular annotators to label. This is because the annotators see a noise corrupted version, y ij , of x i . How well the annotators can label a particular image depends on both the closeness of x i to the ground truth decision plane and the annotator X  X   X  X oise X  level,  X  j . Of course, if the annotator bias  X  j is far from the ground truth decision plane, the labels for images near the ground truth decision plane will be consistent for that annotator, but not necessarily correct. 5.1 Synthetic Data To explore whether the inference procedure estimates image and annotator parameters accurately, we tested our model on synthetic data generated according to the model X  X  assumptions. Similar to the experimental setup in [13], we generated 500 synthetic image parameters and simulated between 4 and 20 annotators labeling each image. The procedure was repeated 40 times to reduce the noise in the results.
 We generated the annotator parameters by randomly sampling  X  j from a Gamma distribution (shape 1.5 and scale 0.3) and biases  X  j from a Normal distribution centered at 0 with standard deviation (a) (b) (c) (d) 0.5. The direction of the decision plane w j was +1 with probability 0 . 99 and  X  1 with probability 0 . 01 . The image parameters x i were generated by a two-dimensional Gaussian mixture model with two components of standard deviation 0.8 centered at -1 and +1. The image ground truth label z , and thus the mixture component from which x i was generated, was sampled from a Bernoulli distribution with p ( z i = 1) = 0 . 5 .
 For each trial, we measured the correlation between the ground truth values of each parameter and the values estimated by the model. We averaged Spearman X  X  rank correlation coefficient for each parameter over all trials. The result of the simulated labeling process is shown Figure 3a. As can be seen from the figure, the model estimates the parameters accurately, with the accuracy increasing as the number of annotators labeling each image increases. We repeated a similar experiment with 2-dimensional x i and w j (see Figure 3b). As one would expect, estimating higher dimensional x i and w j requires more data.
 We also examined how well our model estimated the binary class values, z i . For comparison, we also tried three other methods on the same data: a simple majority voting rule for each image, the bias-competence model of [1], and the GLAD algorithm from [13] 2 , which models 1-d image difficulty and annotator competence, but not bias. As can be seen from Figure 3c, our method presents a small but consistent improvement. In a separate experiment (not shown) we generated synthetic annotators with increasing bias parameters  X  j . We found that GLAD performs worse than majority voting when the variance in the bias between different annotators is high (  X  &amp; 0 . 8 ); this was expected as GLAD does not model annotator bias. Similarly, increasing the proportion of difficult images degrades the performance of the model from [1]. The performance of our model points to the benefits of modeling all aspects of the annotation process. 5.2 Human Data We next conducted experiments on annotation results from real MTurk annotators. To compare the performance of the different models on a real discrimination task, we prepared dataset of 200 images of birds (100 with Indigo Bunting, and 100 with Blue Grosbeak), and asked 40 annotators per image if it contained at least one Indigo Bunting; this is a challenging task (see Figure 1). The annotators were given a description and example photos of the two bird species. Figure 3d shows how the performance varies as the number of annotators per image is increased. We sampled a subset of the annotators for each image. Our model did better than the other approaches also on this dataset. To demonstrate that annotator competence, annotator bias, image difficulty, and multi-dimensional decision surfaces are important real life phenomena affecting the annotation process, and to quantify our model X  X  ability to adapt to each of them, we tested our model on three different image datasets: one based on pictures of rotated ellipses, another based on synthetically generated  X  X reebles X , and a third dataset with images of waterbirds.
 Ellipse Dataset: Annotators were given the simple task of selecting ellipses which they believed to be more vertical than horizontal. This dataset was chosen to make the model X  X  predictions quan-Figure 5: Estimated image parameters (symbols) and annotator decision planes (lines) for the greeble ex-tifiable, because ground truth class labels and ellipse angle parameters are known to us for each test image (but hidden from the inference algorithm).
 gradually become easier to classify as the angle moves away from 45  X  . We used a total of 180 ellipse images, with rotation angle varying from 1 -180  X  , and collected labels from 20 MTurk annotators for each image. In this dataset, the estimated image parameters x i and annotator parameters w j are 1-dimensional, where the magnitudes encode image difficulty and annotator competence respectively. Since we had ground truth labels, we could compute the false alarm and hit rates for each annotator, and thus compute  X  and d 0 for comparison with  X   X  j /s and 2 /s (see Equation 7 and following text). The results in Figure 4b-d show that annotator competence and bias vary among annotators. More-over, the figure shows that our model accurately estimates image difficulty, annotator competence, and annotator bias on data from real MTurk annotators.
 Greeble Dataset: In the second experiment, annotators were shown pictures of  X  X reebles X  (see Figure 5) and were told that the greebles belonged to one of two classes. Some annotators were told that the two greeble classes could be discriminated by height, while others were told they could be discriminated by color (yellowish vs. green). This was done to explore the scenario in which annotators have different types of prior knowledge or abilities. We used a total of 200 images with 20 annotators labeling each image. The height and color parameters for the two types of greebles were randomly generated according to Gaussian distributions with centers (1 , 1) and (  X  1 ,  X  1) , and standard deviations of 0 . 8 .
 The results in Figure 5 show that the model successfully learned two clusters of annotator decision surfaces, one (green) of which responds mostly to the first dimension of x i (color) and another (red) responding mostly to the second dimension of x i (height). These two clusters coincide with the sets of annotators primed with the two different attributes. Additionally, for the second attribute, we observed a few  X  X dversarial X  annotators whose labels tended to be inverted from their true values. This was because the instructions to our color annotation task were ambiguously worded, so that some annotators had become confused and had inverted their labels. Our model robustly handles these adversarial labels by inverting the sign of the  X  w vector.
 Waterbird Dataset : The greeble experiment shows that our model is able to segregate annotators looking for different attributes in images. To see whether the same phenomenon could be observed in a task involving images of real objects, we constructed an image dataset of waterbirds. We collected 50 photographs each of the bird species Mallard, American Black Duck, Canada Goose and Red-necked Grebe. In addition to the 200 images of waterbirds, we also selected 40 images without any birds at all (such as photos of various nature scenes and objects) or where birds were too small be seen clearly, making 240 images in total. For each image, we asked 40 annotators on MTurk if they could see a duck in the image (only Mallards and American Black Ducks are ducks). The hypothesis Figure 6: Estimated image and annotator parameters on the Waterbirds dataset. The annotators were asked was that some annotators would be able to discriminate ducks from the two other bird species, while others would confuse ducks with geese and/or grebes.
 Results from the experiment, shown in Figure 6, suggest that there are at least three different groups of annotators, those who separate: (1) ducks from everything else, (2) ducks and grebes from every-thing else, and (3) ducks, grebes, and geese from everything else; see numbered circles in Figure 6. Interestingly, the first group of annotators was better at separating out Canada geese than Red-necked grebes. This may be because Canada geese are quite distinctive with their long, black necks, while the grebes have shorter necks and look more duck-like in most poses. There were also a few outlier annotators that did not provide answers consistent with any other annotators. This is a common phenomenon on MTurk, where a small percentage of the annotators will provide bad quality labels in the hope of still getting paid [7]. We also compared the labels predicted by the different models to the ground truth. Majority voting performed at 68 . 3% correct labels, GLAD at 60 . 4% , and our model performed at 75 . 4% . We have proposed a Bayesian generative probabilistic model for the annotation process. Given only binary labels of images from many different annotators, it is possible to infer not only the underlying class (or value) of the image, but also parameters such as image difficulty and annota-tor competence and bias. Furthermore, the model represents both the images and the annotators as multidimensional entities, with different high level attributes and strengths respectively. Experi-ments with images annotated by MTurk workers show that indeed different annotators have variable competence level and widely different biases, and that the annotators X  classification criterion is best modeled in multidimensional space. Ultimately, our model can accurately estimate the ground truth labels by integrating the labels provided by several annotators with different skills, and it does so better than the current state of the art methods.
 Besides estimating ground truth classes from binary labels, our model provides information that is valuable for defining loss functions and for training classifiers. For example, the image parame-ters estimated by our model could be taken into account for weighing different training examples, or, more generally, it could be used for a softer definition of ground truth. Furthermore, our find-ings suggest that annotators fall into different groups depending on their expertise and on how they perceive the task. This could be used to select annotators that are experts on certain tasks and to discover different schools of thought on how to carry out a given task.
 Acknowledgements [1] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using [2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale [3] D.M. Green and J.M. Swets. Signal detection theory and psychophysics . John Wiley and Sons [4] V.C. Raykar, S. Yu, L.H. Zhao, A. Jerebko, C. Florin, G.H. Valadez, L. Bogoni, and L. Moy. [5] V.S. Sheng, F. Provost, and P.G. Ipeirotis. Get another label? improving data quality and data [6] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective [7] R. Snow, B. O X  X onnor, D. Jurafsky, and A.Y. Ng. Cheap and Fast -But is it Good? Evaluating [8] A. Sorokin and D. Forsyth. Utility data annotation with amazon mechanical turk. In First [9] M. Spain and P. Perona. Some objects are more equal than others: measuring and predicting [10] L. von Ahn and L. Dabbish. Labeling images with a computer game. In SIGCHI conference [11] L. von Ahn, B. Maurer, C. McMillen, D. Abraham, and M. Blum. reCAPTCHA: Human-based [12] Peter Welinder and Pietro Perona. Online crowdsourcing: rating annotators and obtaining cost-[13] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more: [14] T. D. Wickens. Elementary signal detection theory . Oxford University Press, United States,
