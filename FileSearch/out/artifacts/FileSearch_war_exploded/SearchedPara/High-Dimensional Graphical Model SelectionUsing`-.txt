 Keywords: Graphical models; Markov random fields; structure learning ; ` 1 -regularization; model selection; convex risk minimization; high-dimensio nal asymptotics; concentration. Consider a p -dimensional discrete random variable X = ( X 1 ,X 2 ,...,X p ) where the dis-tribution of X is governed by an unknown undirected graphical model. In thi s paper, we investigate the problem of estimating the graph structure f rom an i.i.d. sample of n data in a broad range of applications where graphical models are u sed as a probabilistic repre-sentation tool, including image processing, document anal ysis and medical diagnosis. Our variables, and to use the sparsity pattern of the regression vector to infer the underlying neighborhood structure. The main contribution of the paper is a theoretical analysis show-ing that, under suitable conditions, this procedure recove rs the true graph structure with probability one, in the high-dimensional setting in which b oth the sample size n and graph size p = p ( n ) increase to infinity.
 The problem of structure learning for discrete graphical mo dels X  X ue to both its importance and difficulty X  X as attracted considerable attention. Const raint based approaches use hy-pothesis testing to estimate the set of conditional indepen dencies in the data, and then determine a graph that most closely represents those indepe ndencies [8]. An alternative ap-proach is to view the problem as estimation of a stochastic mo del, combining a scoring metric on candidate graph structures with a goodness of fit measure t o the data. The scoring met-ric approach must be used together with a search procedure th at generates candidate graph structures to be scored. The combinatorial space of graph st ructures is super-exponential, however, and Chickering [1] shows that this problem is in gen eral NP-hard. The space of candidate structures in scoring based approaches is typica lly restricted to directed models (Bayesian networks) since the computation of typical score metrics involves computing the normalization constant of the graphical model distributio n, which is intractable for general undirected models. Estimation of graph structures in undir ected models has thus largely been restricted to simple graph classes such as trees [2], po lytrees [3] and hypertrees [9]. The technique of ` 1 regularization for estimation of sparse models or signals h as a long history in many fields; we refer to Tropp [10] for a recent surv ey. A surge of recent work has regression to the problem of inferring graph structure. The technique is computationally efficient and thus well-suited to high dimensional problems, since it involves the solution only of standard convex programs. Our main result establish es conditions on the sample size n , graph size p and maximum neighborhood size d under which the true neighborhood structure can be inferred with probability one as ( n,p,d ) increase. Our analysis, though asymptotic in nature, leads to growth conditions that are su fficiently weak so as to require only that the number of observations n grow logarithmically in terms of the graph size. Consequently, our results establish that graphical struct ure can be learned from relatively sparse data. Our analysis and results are similar in spirit t o the recent work of Meinshausen and B  X uhlmann [5] on covariance selection in Gaussian graph ical models, but focusing rather on the case of discrete models.
 The remainder of this paper is organized as follows. In Secti on 2, we formulate the problem and establish notation, before moving on to a precise statem ent of our main result, and a high-level proof outline in Section 3. Sections 4 and 5 detai l the proof, with some technical details deferred to the full-length version. Finally, we pr ovide experimental results and a concluding discussion in Section 6. Let G = ( V,E ) denote a graph with vertex set V of size | V | = p and edge set E . We denote graphical model with graph G is a family of probability distributions for a random variab le X = ( X 1 ,X 2 ,...,X p ) given by p ( x )  X  our attention to the case where each x s  X  { 0 , 1 } is binary, and the family of probability distributions is given by the Ising model Given such an exponential family in a minimal representatio n, the log partition function  X (  X  ) is strictly convex, which ensures that the parameter matri x  X  is identifiable. We address the following problem of graph learning. Given n samples x ( i )  X  X  0 , 1 } p drawn Our set-up includes the important situation in which the num ber of variables p may be large vary with sample size. (For notational clarity we will somet imes omit subscripts indicating as n  X  X  X  . Equivalently, we consider the problem of estimating neigh borhoods b N n ( s )  X  V n model provides a compact representation where the size of th e neighborhoods are typically small X  X ay d s p for all s  X  V n . Our goal is to use ` 1 -regularized logistic regression to estimate these neighborhoods; for this paper, the actual va lues of the parameters  X  ij is a secondary concern.
 binary response, logistic regression involves minimizing the negative log likelihood We focus on regularized version of this regression problem, involving an ` 1 constraint on (a of  X  except the one in position s . For the graph learning task, we regress each variable X s onto the remaining variables, sharing the same data x ( i ) across problems. This leads to the following collection of optimization problems ( p in total, one for each graph node): z s = 1. The parameter  X  s acts as a bias term, and is not regularized. Thus, the quantit y b  X  t can be thought of as a penalized conditional likelihood esti mate of  X  s,t . Our estimate of the neighborhood N ( s ) is then given by Our goal is to provide conditions on the graphical model X  X n p articular, relations among the number of nodes p , number of observations n and maximum node degree d  X  X hat ensure that the collection of neighborhood estimates (2), one for each n ode s of the graph, is consistent with high probability.
 We conclude this section with some additional notation that is used throughout the sequel. yield the gradient and Hessian, respectively, of the negati ve log likelihood (2): In this section, we begin with a precise statement of our main result, and then provide a high-level overview of the key steps involved in its proof. 3.1 Statement of main result We begin by stating the assumptions that underlie our main re sult. A subset of the assump-tions involve the Fisher information matrix associated wit h the logistic regression model, defined for each node s  X  V as S to denote the neighborhood N ( s ), and S c to denote the complement V  X  X  ( s ). Our first two assumptions (A1 and A2) place restrictions on the de pendency and coherence structure of this Fisher information matrix. We note that th ese first two assumptions are analogous to conditions imposed in previous work [5, 10, 11, 12] on linear regression. Our third assumption is a growth rate condition on the triple ( n,p,d ). [A1] Dependency condition: We require that the subset of the Fisher information matrix corresponding to the relevant covariates has bounde d eigenvalues: namely, there exist constants C min &gt; 0 and C max &lt; +  X  such that These conditions ensure that the relevant covariates do not become overly dependent, and can be guaranteed (for instance) by assuming that b  X  s, X  lies within a compact set. [A2] Incoherence condition: Our next assumption captures the intuition that the large number of irrelevant covariates (i.e., non-neighbors of no de s ) cannot exert an overly strong effect on the subset of relevant covariates (i.e., neighbors of node s ). To formalize this intuition, we require the existence of an  X  (0 , 1] such that Analogous conditions are required for the success of the Las so in the case of linear regres-sion [5, 10, 11, 12]. [A3] Growth rates: Our second set of assumptions involve the growth rates of the number of observations n , the graph size p , and the maximum node degree d . In particular, we require that: Note that this condition allows the graph size p to grow exponentially with the number of for model selection in graphical models, one is typically in terested in node degrees d that remain bounded (e.g., d = O (1)), or grow only weakly with graph size (say d = o (log p )). With these assumptions, we now state our main result: Theorem 1. Given a graphical model and triple ( n,p,d ) such that conditions A1 through A3 are satisfied, suppose that the regularization parameter  X  n is chosen such that (a) n X  n  X  2 log( p )  X  +  X  , and (b) d X  n  X  0 . Then P [ n  X  +  X  . 3.2 Outline of analysis We now provide a high-level roadmap of the main steps involve d in our proof of Theo-rem 1. Our approach is based on the notion of a primal witness : in particular, focusing our attention on a fixed node s  X  V , we define a constructive procedure for generating a zero-subgradient optimality conditions associated with t he convex program (3). We then show that this construction succeeds with probability conv erging to one under the stated conditions. A key fact is that the convergence rate is sufficie ntly fast that a simple union bound over all graph nodes shows that we achieve consistent n eighborhood estimation for all nodes simultaneously.
 To provide some insight into the nature of our construction, the analysis in Section 4 shows k b z conditions (a) and (c) hold. The remainder of the analysis is then devoted to establishing that properties (b) and (d) hold with high probability.
 In the first part of our analysis, we assume that the dependenc e (A1) mutual incoherence (A2) conditions hold for the sample Fisher information matrices Q s (  X   X  ) defined below equa-tion (4b). Under this assumption, we then show that the condi tions on  X  n in the theorem statement suffice to guarantee that properties (b) and (d) hol d for the constructed pair ( b  X , b z ). The remainder of the analysis, provided in the full-lengt h version of this paper, is devoted to showing that under the specified growth condition s (A3), imposing incoherence and dependence assumptions on the population version of the Fisher information Q  X  (  X   X  ) guarantees (with high probability) that analogous conditi ons hold for the sample quantities Q s (  X   X  ). While it follows immediately from the law of large numbers that the empirical the delicacy is that we require controlling this convergenc e over subsets of increasing size. Our analysis therefore requires the use of uniform laws of la rge numbers [7]. Basic convexity theory can be used to characterize the solut ions of ` 1 -regularized logistic regression. We assume in this section that  X  1 corresponds to the unregularized bias term, and omit the dependence on sample size n in the notation. The objective is to compute to k  X  \ 1 k 1  X  b for some b . The dual function is h (  X  ) = inf  X  L (  X , X  ). solution b  X  . These facts are summarized below.
 Lemma 1. If p  X  n then a unique solution to (9) exists. If p  X  n then the set of solutions and | X   X  Any optimum of (9) must satisfy  X  t = 0 in the true model  X  We now fix a variable X s for the logistic regression, denoting the set of variables i n its neighborhood by S . From the results of the previous section we observe that the ` 1 -regularized regression recovers the sparsity pattern if an d only if there exists a primal-dual Our proof proceeds by showing the existence (with high proba bility) of a primal-dual pair ( b  X , b z ) that satisfy these conditions. We begin by setting b  X  S c = 0, so that (a) holds, and incoherence conditions are imposed on the sample Fisher inf ormation Q n . The remaining analysis, deferred to the full-length version, establishe s that the incoherence assumption (A2) on the population version ensures that the sample versi on also obeys the property with probability converging to one exponentially fast.
 Theorem 2. Suppose that Then P b N ( s ) = N ( s ) = 1  X  O (exp(  X  cn  X  )) for some  X  &gt; 0 .
 Proof . Let us introduce the notation Substituting into the subgradient optimality condition (1 0) yields the equivalent condition By a Taylor series expansion, this condition can be re-writt en as where the remainder R n is a term of order k R n k 2 = O ( k b  X   X   X   X  k 2 ).
 form as: It can be shown that the matrix Q n SS is invertible w.p. one, so that these conditions can be rewritten as Re-arranging yields the condition Analysis of condition (d): We now demonstrate that k b z S c k  X  &lt; 1. Using triangle inequality and the sample incoherence bound (11) we have tha t We complete the proof that k b z S c k  X  &lt; 1 with the following two lemmas, proved in the full-length version.
 Lemma 2. If n X  2 n  X  log( p )  X  +  X  , then at rate O (exp  X  n X  2 n + log( p ) ) .
 Lemma 3. If n X  2 n  X  log( p )  X  +  X  and d X  n  X  0 , then we have at rate O (exp  X  n X  2 n + log( p ) ) .
 We apply these two lemmas to the bound (17) to obtain that with probability converging to one at rate O (exp exp n X  2 n  X  log( p ) Analysis of condition (b): We next show that condition (b) can be satisfied, so that sgn( b  X  S ) = sgn(  X   X  S ). Define  X  n := min i  X  S |  X   X  S | . From equation (14b), we have sign(  X   X  S ), it suffices to show that Using our eigenvalue bounds, we have In fact, the righthand side tends to zero from our earlier res ults on W and R , and the assumption that  X  n d  X  0. Together with the exponential rates of convergence estab lished by the stated lemmas, this completes the proof of the result. We briefly describe some experimental results that demonstr ate the practical viability and performance of our proposed method. We generated random Isi ng models (1) using the following procedure: for a given graph size p and maximum degree d , we started with a graph with disconnected cliques of size less than or equal to ten, a nd for each node, removed edges present in the resulting random graph, we chose the edge weig ht  X  st  X  X  [  X  3 , 3]. We drew n i.i.d. samples from the resulting random Ising model by exac t methods. We implemented the ` -regularized logistic regression by setting the ` 1 penalty as  X  n = O ((log p ) 3 the convex program using a customized primal-dual algorith m (described in more detail in the full-length version of this paper). We considered vario us sparsity regimes, including constant ( d =  X (1)), logarithmic ( d =  X  log( p )), or linear ( d =  X p ). In each case, we evaluate a given method in terms of its average precision (one minus the fraction of falsely included edges), and its recall (one minus the fraction of falsely excluded edges). Figure 1 for the AND method (respectively the OR) method, in which an e dge ( s,t ) is included if Note that both the precision and recall tend to one as the numb er of samples n is increased. We have shown that a technique based on ` 1 -regularization, in which the neighborhood of any used for consistent model selection in discrete graphical m odels. Our analysis applies to the high-dimensional setting, in which both the number of nodes p and maximum neighborhood sizes d are allowed to grow as a function of the number of observation s n . Whereas the current analysis provides sufficient conditions on the tripl e ( n,p,d ) that ensure consistent neighborhood selection, it remains to establish necessary conditions as well [11]. Finally, the ideas described here, while specialized in this paper to the binary case, should be more broadly applicable to discrete graphical models.
 Research supported in part by NSF grants IIS-0427206, CCF-0 625879 and DMS-0605165. [1] D. Chickering. Learning Bayesian networks is NP-comple te. Proceedings of AI and [2] C. Chow and C. Liu. Approximating discrete probability d istributions with dependence [3] S. Dasgupta. Learning polytrees. In Uncertainty on Artificial Intelligence , pages 134 X  [4] D. Donoho and M. Elad. Maximal sparsity representation v ia ` 1 minimization. Proc. [5] N. Meinshausen and P. B  X uhlmann. High dimensional graph s and variable selection with [6] A. Y. Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In [7] D. Pollard. Convergence of stochastic processes . Springer-Verlag, New York, 1984. [8] P. Spirtes, C. Glymour, and R. Scheines. Causation, pred iction and search. MIT Press , [9] N. Srebro. Maximum likelihood bounded tree-width Marko v networks. Artificial Intel-[10] J. A. Tropp. Just relax: Convex programming methods for identifying sparse signals. [11] M. J. Wainwright. Sharp thresholds for high-dimension al and noisy sparsity recovery [12] P. Zhao and B. Yu. Model selection with the lasso. Techni cal report, UC Berkeley,
