 Dan Zhang 1 danzhang2008@gmail.com Jingrui He 2 jingrui.he@gmail.com Luo Si 3 lsi@cs.purdue.edu Richard D. Lawrence 4 ricklawr@us.ibm.com
Facebook Incorporation, Menlo Park, CA 94025  X  solution for problem (10). So, w  X   X  (5), R = C max { max i,j k B ij k , max i k B i k} . Proof of Theorem 2: The algorithm terminates un-der two conditions. We show under either of the two nates. It is clear that the method cannot execute the step (7) to step (9) more than log  X  1 2) Condition 2: Suppose the conditions stated in step decrease F ( w ) by at least m 2  X  bounded by E and lower bounded by 0. Step (4) to step (6) cannot be executed for at most 2 E X  0 By summarizing these two conditions, we can conclude that the total number iterations should not exceed
If C 1 &lt; C 2 , max { C 2 C (1  X  a ) C 2 = C 2 .
 If C 2  X  C 1 , max { C 2 So, max { C 2 By summarizing these two conditions, we can get the
 Dan Zhang 1 danzhang2008@gmail.com Jingrui He 2 jingrui.he@gmail.com Luo Si 3 lsi@cs.purdue.edu Richard D. Lawrence 4 ricklawr@us.ibm.com
Facebook Incorporation, Menlo Park, CA 94025 IBM T.J. Watson Research Center, Yorktown Heights, NY 10562 Traditional learning methods usually consider each example as one non-separable entity, and represent the whole content of the example by one feature vector. However, the semantic meanings of an ex-ample sometimes vary among its constituent parts. Multiple Instance Learning (MIL) has been proposed to deal with problems whose output information is only known for bags of items/instances, as opposed to for each example. More precisely, in a MIL set-ting, each example/bag is divided into several differ-ent parts/instances. The labels are assigned to bags, rather than individual instances. A bag is labeled as positive if it contains more than one positive in-stance; otherwise it is labeled as negative. In this paper, for each example, the feature vector extracted by using the same way as we do for traditional non-MIL methods (i.e., treating each example as an in-tegral entity) is referred to as the global represen-tation of this example, while its local representa-tion is a set of instances extracted for each part of this example, as in MIL. To some extent, the global representation for each example can also be consid-ered as its bag level features. Numerous methods have been developed for MIL classification ( Andrews et al. , 2003 ; Dietterich et al. , 1998 ; Kim &amp; la Torre , 2010 ) and its variants, such as outlier detection ( Wu et al. , 2010 ), online learning ( Babenko et al. , 2011 ), rank-ing ( Hu et al. , 2008 ), etc. These methods have been widely employed in areas such as text mining ( Andrews et al. , 2003 ) and localized content based im-age retrieval (LCBIR) ( Rahmani &amp; Goldman , 2006 ). Most previous MIL methods focused on improving classification performance under local representation. However, few of them investigated whether the local representation is always better than the global one. This problem has posed a big challenge for researchers to decide what kind of algorithms should be used when facing real world applications. In ( Ray &amp; Craven , 2005 ), the authors compared the performances of tra-ditional and MIL methods. However, their work is still based on the local representation, and adapts the tra-ditional learning methods to the local representation. Although rarely studied, it is intuitive that the true positive rates in positive bags could affect the per-formances of local and global representations signif-icantly. This is because if the true positive rate in a positive bag is low, then its global representation will be dominated by the irrelevant parts of this ex-ample, while methods based on local representation could pick the true positive instances for training. On the contrary, if an example has few irrelevant parts, then the global representation tends to be more infor-mative than the local one, since methods based on lo-cal representations normally focus on some local parts of each example. This intuition can also be verified empirically by the experiments conducted in Section 4.1 . When incorporating this intuition into real appli-cations, the major challenge is how to learn for each training example, whether local representation is bet-ter or global one tends to prevail.
 To solve this challenge, a novel research framework  X  Multiple Instance LEArning with Global Embed-ding (MILEAGE) is proposed. MILEAGE leverages the benefits from both local and global representations such that in general it can achieve a better perfor-mance than both MIL and traditional learning meth-ods. From another perspective, local and global fea-ture representations can be treated as two information sources, and each of them carries some auxiliary infor-mation to improve classification performance, which is similar to the basic motivation of multi-view learn-ing methods ( Joachims et al. , 2001 ). To solve the pro-posed framework, a novel method is designed by adap-tively tuning the importance of the two different rep-resentations. It is based on the intuition that local representation tends to perform better when the pos-itive ratio is small. An iterative method is employed to solve the derived optimization problem. To acceler-ate the optimization speed, inspired by ( Fuduli et al. , 2004 ), we adapt the bundle method to solve the result-ing non-convex non-smooth problem by explicitly con-sidering the convex regularization and the non-convex loss terms. Some discussions and theoretical analysis have been provided on important properties such as convergence rate and generalized error rate of the pro-posed method. Experiments on image, text datasets and a novel application  X  Insider Threat Detection, demonstrate the advantages of the proposed method. 2.1. Problem Statement and Notation Suppose a set of examples: D = { ( B i , B i  X  , Y i ) , i = 1 , . . . , n } are given, where B i  X  X  d  X  1 denotes the global representation for the i -th example and Y i  X  { 1 ,  X  1 } is its binary label. Along with the global fea-ture representation, for each example, its local fea-ture representations, i.e., instances for different parts of this example, are also available (The notions of global and local representations are defined in Sec-tion 1). The instances in the i -th bag are denoted the number of instances in the i -th bag. Throughout the paper, subscript  X  means j = 1 , . . . , n i . Given an unlabeled example B u and its associated local repre-sentations, i.e., B u  X  , the objective of Multiple Instance LEArning with Global Embedding (MILEAGE) is to design a function f : ( B u , B u  X  )  X  X  , such that the classification on this unlabeled example is accurate. If f ( B u , B u  X  ) &gt; 0, this example is classified as positive and otherwise negative. 2.2. Method For each bag, a weight variable is introduced to bal-ance the importance of the two representations. The weight is decided by both the prior knowledge from the positive ratio for each bag and the fitness of the data. Without loss of generality, given a specific example B i and its associated instances B i  X  , the classifier takes the following form: where 1  X   X  i  X  0 is the convex combination coef-ficient for the i -th example, w  X  X  d  X  1 is the linear classifier and we assume that the bias has already been absorbed into feature vectors. max j w T B ij is the out-put from the local representation of the i -th example 2 , whereas w T B i is the output from its global represen-tation. f ( B i , B i  X  ) balances these two outputs through the weight  X  i . From a Bayesian perspective, given a dataset, the logarithm of the posterior distribution for w and  X  can be written as follows: where  X  = [  X  1 , . . . ,  X  n ]. Here, we assume that the examples are i.i.d. generated. P ( w ) follows the Gaus-sian distribution N (0 , I ). P (  X  i ) follows the Beta dis-tribution with beta(  X e  X  r i ,  X e  X  (1  X  r i ) ), where and  X  are the hyper-parameters and partially control the mean and skewness of the distribution. r i  X  [0 , 1] is the prior knowledge on the positive ratio for the i -th bag, and can be obtained through various ways. For example, r i can be simply set to 0 . 5 if no prior knowledge is available. In practice, a preliminary clas-sifier can be trained beforehand by using SVM on applying this classifier on the instances in each bag. It w and  X  , the probability of generating a dataset D can be described by the hinge loss as: P ( D| w ,  X  )  X  is a parameter. Then, maximizing Eq.( 2 ) is equivalent to solving the following problem:
This formulation is non-convex and cannot be solved directly. An iterative method is employed to solve this problem. In particular, for the k -th iteration, given w ( k  X  1) ,  X  1 , . . . ,  X  n can be updated by: of this objective function cannot be determined, since clear. But some methods, such as the adapted sub-gradient method, can still be used to find its optimal or local optimal solution efficiently. Given  X  from the previous step, w ( k ) can be optimized by:
It is still a non-convex non-smooth optimization prob-lem. But the form is much less complicated than that of problem ( 3 ). It can be solved through var-ious ways, such as constrained concave-convex pro-cedure (CCCP) ( Yuille &amp; Rangarajan , 2003 ). How-ever, the computational cost for solving this problem is non-trivial. In several recent works, the bundle method has shown its superior performance in both ef-ficiency and effectiveness over state-of-the-art methods ( Joachims , 2006 ; Joachims et al. , 2009 ; Smola et al. , 2007 ; Teo et al. , 2010 ). However, one major drawback for this method is that it can only be employed to solve convex optimization problems. In ( Fuduli et al. , 2004 ; Hare &amp; Sagastiz  X abal , 2010 ; Noll , 2012 ), several heuris-tics are employed to handle this issue for the bun-dle method. In this paper, inspired by ( Fuduli et al. , 2004 ), we adapt the bundle method to solve this pro-posed optimization problem in the next section. Based on these updating schemes, problem ( 4 ) and problem ( 5 ) will be conducted iteratively until convergence. It is clear that the proposed formulation is induc-tive on the classifier but transductive on  X  i . So, if we only need to predict the unlabeled instances in the unlabeled set, then we can directly apply the learned classifier. If the prediction is made on the bag level, on an unlabeled example ( B u , B u  X  ) , j = 1 , . . . , n u . Its hidden variable  X  u can be estimated as:  X  u = E (  X  u | B u , B u  X  ) = e where r u is the positive instance ratio within this bag estimated from the learned classifier w . Then, f ( B u , B u  X  ) =  X   X  u max j w T B uj + (1  X   X   X  u ) w f ( B u , B u  X  ) &gt; 0, the example is labeled as positive and otherwise it is labeled as negative. 2.3. Bundle Method for Non-Convex The traditional bundle method looks for a set of cut-ting planes that could serve as lower bounds of the original convex objective function. For non-convex optimization problems, however, these cutting planes could no longer serve as lower bounds of the objective functions, as shown in Fig. 1 . Some research works con-sider shifting of affine pieces downwards ( Noll , 2012 ; Schramm &amp; Zowe , 1992 ). However, the amount of the shifting appears arbitrary ( Fuduli et al. , 2004 ). In this section, the bundle method, which is based on first order approximation, is adapted to solve problem ( 5 ). In particular, the intended objective function can be casted as the following framework: where  X ( w ) is a non-negative convex differentiable reg-ularizer, and R emp ( w ) is a non-convex non-smooth loss function. In problem ( 5 ),  X ( w ) = 1 2 k w k 2 This method handles this non-convex non-smooth problem in an iterative way and exhibits a kind of both convex and nonconvex behavior relative to the current point in the iterative procedure. More precisely, for the t -th iteration of bundle method, it maintains two sets of cutting planes, i.e., I + , { j |  X  ( t ) j  X  0 } , I { j |  X  ( t ) j &lt; 0 } , where j = 1 , . . . , t  X  1 and
Here, g j  X   X  w R emp ( w ( j ) ) 3 . Then, the following two sets of affine functions are defined as: It is clear that  X  + ( w ) is an approximation of R emp ( w )  X  R emp ( w ( t  X  1) ), while  X   X  ( w ) is its locally pessimistic estimation. These approximations are only locally valid around the local minimal point. Here, the meanings of  X  ( t ) j and the locality property can be shown in Fig. 1 . Therefore, during each itera-tion, the new optimal point should tradeoff minimizing  X  + ( w ) and proximity k w  X  w ( t  X  1) k with the constraint  X  + ( w )  X   X   X  ( w ) as follows: where  X  ( t ) is the non-negative proximity control pa-rameter for the t -th iteration that balances the objec-tive function value and the proximity of the updated point. This problem can be solved efficiently through its dual form, since both of the sets I + and I  X  are small. Suppose w ( t ) = arg min w P ( w ,  X  ( t ) ). If not computationally expensive, a line search can be per-formed between w ( t ) and w ( t  X  1) on F ( w ) such that a better solution can be found.
 If the optimal solution can result in a drastic decrease in the objective function F ( w ), it is called a serious step and the optimal solution for w will be updated. Otherwise, it is considered as a null step, the optimal solution for the previous step is kept, and the prox-imity parameter will shrink for a better solution. If the proximity parameter will also shrink to do a more thorough search within that region.
 The classic bundle method usually checks whether the difference between the objective function value and the cutting plane function value is less than a thresh-old. If so, the iteration terminates. Here, this strat-egy cannot be used because the cutting planes of the non-convex function cannot be considered as the lower bounds for the original objective function any more. In the proposed method, during each iteration, two stopping criteria will be checked. The first stopping criteria is to check whether  X  ( t ) is smaller than a spec-ified threshold  X  1 . This is because although we hope that the new updated point should fall within a small likely to deviate too much from w ( t  X  1) , and the re-sults will not be meaningful. An extreme example is criteria is to check whether 0  X   X  X  ( w ( t ) ), i.e., whether w ( t ) can be considered as a stationary point for F ( w ). In practice, we check whether k o  X  k /F ( w ( t ) )  X   X  , J + = { i  X  I + |  X  where G is a matrix with its columns be-ing the subgradients g j from J + and  X   X  can be optimized by solving  X   X  = arg min  X  T G T G  X  + 2(  X   X ( w ( t ) )) T G  X  s.t.  X  T 1 = 1 ,  X   X  0. The proposed bundle method is summarized in Table 1 . It is clear that the major advantage of the proposed method over ( Fuduli et al. , 2004 ) is that the proposed method better exploits the structure of the objective function by treating the convex and non-convex parts separately. It therefore eliminates the unnecessary first order approximation for the convex part. In this way, theoretically the cutting plane approximation for the whole objective function is more accurate than the one used in ( Fuduli et al. , 2004 ).
 In ( Bergeron et al. , 2012 ), the authors directly ap-plied ( Fuduli et al. , 2004 ) to MIL. However, there are several major differences between these two pa-pers. 1. ( Bergeron et al. , 2012 ) only focuses on the traditional MIL, and can not be used to solve MILEAGE. 2. By directly employing ( Fuduli et al. , 2004 ), ( Bergeron et al. , 2012 ) does not treat the con-vex and non-convex parts separately either and there-fore its first order approximation is less accurate than the one used in this paper.
 In ( Do &amp; Arti`eres , 2009 ), the non-convex formulation for hidden markov models is also solved by adapting the bundle method to the non-convex case, and treat-ing the convex and non-convex parts separately. The adapted method is reasonable by tuning the cutting plane at each iteration according to the comparison with the previous  X  X ptimal X  cutting plane. However, even with this tuning, the obtained cutting plane is still not able to serve as the lower bound of the objec-tive function. On the contrary, the proposed method does not focus on looking for the lower bound, but some important local properties around each point. Furthermore, based on the proposed bundle method, some important properties are analyzed in Theorem 1 and Theorem 2.
 Theorem 1: Suppose D = max t  X ( w ( t ) ) and R = max j k g j k , then  X   X  2 0 2 R 2  X  P ( w ( t ) ,  X  ( t ) )  X   X  0 D . In solving problem Proof: Please refer to Supplemental Materials. Theorem 2: The bundle method terminates after at  X ( w ) is upper bounded by E . In solving problem ( 5 ), the algorithm terminates after at most log  X  1  X  Proof: Please refer to Supplemental Materials. Suppose the class of classifier satisfies k w k X  B and  X  are obtained from iterative updates. Since the proposed method can be easily extended to the kernel case, F B is defined as: { f | f : ( B i , B i  X  )  X   X  max j w T  X  ( B ij ) + (1  X   X  i ) w T  X  ( B i ) , k w k X  B } , where  X  is a nonlinear map with kernel function K ( , ). The generalized error bound can be derived by the fol-lowing theorems: Theorem 3: The empirical Rademacher com-plexity of the functional space F B on D = Proof: Please refer to Supplemental Materials. Theorem 4: Fix  X   X  (0 , 1). Then, with prob-ability at least 1  X   X  , every f  X  X  B satisfies: P ( y 6 = sign ( f ( B i , B i  X  )))  X  1 n P n i =1 max { 0 , 1  X  Y (  X  i max j w T B ij + (1  X   X  i ) w T B i ) } + Proof: It can be proved by applying Theorem 3 to Theorem 4.9 in ( Shawe-Taylor &amp; Cristianini , 2004 ). From Theorem 3 and Theorem 4, it can be seen that the derived Rademacher complexity and generalized error bound are related to both the local and global feature representations. Theorem 5 states the case when the Rademacher Complexity can be improved, compared with both local and global feature represen-tations.
 Theorem 5: Suppose a  X   X  i  X  max { C 2 C Proof: Please refer to Supplemental Materials. In Theorem 5, C 1 indicates the Rademacher Com-plexity derived from the local representation, while C 2 represents the Rademacher Complexity for the global representation. It can be concluded that, under some restrictions, the Rademacher Complexity of the pro-posed method is guaranteed to be less than the max-imum one of the Rademacher Complexities for local and global representations. 4.1. Synthetic Experiments The synthetic dataset is designed to verify the intu-itions conveyed in this paper, i.e., local representation works better when the true positive ratios in positive bags are lower, while global representation works bet-ter when the ratios are higher. In particular, we design two sets of experiments as shown in Fig. 2 . For each set of experiments, positive instances are generated from a Gaussian distribution, and negative ones are generated from another three Gaussian distributions, with different amounts of overlap, as shown in Fig. 2(a) and Fig. 2(c) . Based on these two data distributions, for each set of experiments, 6 toy datasets are created with each positive bag containing a certain ratio of positive and negative instances and each negative bag containing all negative instances. Each bag contains 10 instances. For each dataset 1000 positive bags and 1000 negative ones are i.i.d. generated. SVM and its MIL variant  X  MISVM ( Andrews et al. , 2003 ) (We re-port the comparison results of these two methods, be-cause their objective functions are the same except for the local and global representation part) are used for comparison, where the average feature representation of each bag is used as its global feature representation and used by SVM. For each experiment, 50% exam-ples are randomly picked for training, and the rest for testing. The averaged results of 20 independent runs under different ratios of positive instances in positive bags are reported in Fig. 2(b) and Fig. 2(d) for datasets generated from Fig. 2(a) and Fig. 2(c) respectively, with the parameters tuned by 5-fold cross validation. It can be partially concluded from the experiments that: (1) The local representation is not always better than the global one; (2) The local representation tends to perform better than the global one when the pos-itive ratio is low; (3) There is no universally  X  X ood X  positive ratio below which the local representation is definitely better than the global one. (4) It seems that if the amount of overlap between positive and negative distributions is high, the local representation is likely to be worse than that of the global representation 4 . 4.2. Real Applications These experiments are conducted on three datasets, i.e., an image dataset from Corel ( Andrews et al. , 2003 ), a text dataset from Reuters21578 as well as newly proposed application  X  Insider Threat Detec-tion. In MIL, MUSK ( Dietterich et al. , 1998 ) is also a commonly used benchmark dataset. But its perfor-mance is not reported here, because the meaning of the global representation for MUSK is not clear. In MUSK, instances represent different conformations of molecule. For each molecule, a set of conformations do not convey physical meanings in global representa-tion. But for images and documents, each image or document itself can be considered a concrete object. The Corel dataset is divided into three sub-datasets, i.e., Fox, Elephant and Tiger. For a detailed de-scription of these three datasets, please refer to ( Andrews et al. , 2003 ). For each picture/example in Corel, the global feature vector is the average of the instances on all dimensions.
 For Reuters21578, documents from 4 sub-categories, as well as some negative documents, are randomly picked. For each of the sub-dataset, after removing the stop words and stemming, tf-idf ( Manning et al. , 2008 ) features are extracted and processed by PCA ( Berry &amp; Castellanos , 2007 ). The resulting dimen-sionality is 249. For each document/bag, the global feature vector is extracted from the whole content; while the instance features are derived through a slid-ing window with fixed length ( Andrews et al. , 2003 ). For Reuters1, Reuters2, Reuters3, Reuters4, they con-tain 1602, 1256, 1100, 502 bags, and 3006, 2181, 2249, 920 instances, respectively.
 For Insider Threat Detection (ITD), We obtained this real dataset from a big IT company. ITD is a project which is devoted to find the potential harmful insid-ers through analyzing their online behaviors, such as sending emails, login, logout, downloaded files. In this dataset, some experts are hired to decide whether dur-ing each period (around 30 days), each person in the database did malicious things or not. Each online be-havior is quantified as a feature value. However, it is highly possible that if a person did malicious things during a period, it does not mean that he did malicious things every day. Out of this motivation, the features for the online behaviors within one day is considered as an instance and the instances during each period is treated as a bag. If a person is known to do some malicious things in a specific period, then the corre-sponding collection of instances (days) is considered as a positive bag. Otherwise, this collection of instances will be considered as negative. The global feature rep-resentation for each bag is extracted from the corre-sponding period as a whole. The whole dataset con-tains 1000 negative bags and 166 positive bags, where each bag contains around 30 instances and each in-stance is represented by 32 features. On this dataset, due to the imbalance of the dataset, F1 score for the top 20 returned results is used here for measurement. 4.3. Comparison Results In the proposed method, parameters C ,  X  and are set through 5-fold cross validation on the training set respectively. To show the advantages of the pro-posed large margin method, we compare it with sev-eral baseline methods, including traditional large mar-gin methods, SVM-B, SVM-I, and multiple instance learning methods: Citation KNN ( Wang &amp; Zucker , 2000 ), MISVM ( Andrews et al. , 2003 ), miSVM ( Andrews et al. , 2003 ), MILES ( Chen et al. , 2006 ), and ISMIL ( Fu &amp; Robles-Kelly , 2009 ).
 For SVM-B, SVM is used on the bag/global features for training and prediction. For SVM-I, the bag la-bels are assigned to their corresponding instances, and SVM is used on these labeled instances. For each un-labeled bag, if at least one of its instances is labeled as positive by SVM-I, then its bag label is positive. Otherwise, it is negative. As for MIL methods, Ci-tation KNN is an adaptation of traditional K nearest neighbor to MIL. MISVM and miSVM are two large margin multiple instance classification methods, de-rived from SVM. MILES tries to represent each bag by using one feature vector, and then design a clas-sifier based on that. For ISMIL, the algorithm maps bags into a space spanned by some selected instances, and designs a classifier based on that. The parameters of the baseline methods are also tuned by 5-fold cross validation. For the large margin methods, for the fair of comparison, only linear classifiers are used. The average accuracy of 20 independent runs are re-ported in Table 2 and 3 . For each experiment, 90% examples are randomly sampled as training examples, while the remaining ones are used for testing. It is clear that MIL methods are not better than the tradi-tional learning methods on all of these datasets, which further verifies that the local representation for MIL may not be always better than the global representa-tion. From these experimental results, in most cases, MILEAGE shows the best performance. This is be-cause MILEAGE takes advantage of both local and global representations adaptively. These two differ-ent representations can be considered as two different information sources, and both of them convey some useful information in improving the performance. For time comparisons, the proposed method is com-parable with most of the other MIL methods. The efficiency of the proposed bundle method plays an im-portant role. For example, MISVM needs to solve a non-convex problem similar to problem ( 5 ) only once for each experiment, but the proposed method needs to solve problems ( 4 ) and ( 5 ) for around 15 times before convergence. So, the average amount of time needed for each independent execution of the bundle method is small, compared with that of MISVM. On the other side, traditional learning methods such as SVM-B and SVM-I tend to be more efficient because they can eas-ily apply convex optimization methods such as Se-quential Maximization Optimization to their convex objective functions once with only one kind of repre-sentations. But the proposed MILEAGE framework generate more accurate results in most cases due to the more realistic non-convex setting of both global representations and local representations.
 To show the robustness of the proposed method, some parameter sensitivity experiments are conducted on C , ,  X  , and shown in Fig. 3 . The averaged experiments of 20 independent runs on Reuters4 are reported. From these experiments, it can be seen that the proposed method is relatively robust with respect to these pa-rameters. We also observed similar patterns from ex-periments on the other datasets. This paper presents a novel machine learning problem  X  Multiple Instance LEArning with Global Embedding (MILEAGE) for integrating the global feature repre-sentations into multiple instance learning. To solve the proposed problem, a large margin method is proposed, which adaptively tunes the weights for the two differ-ent feature representations imposed on each bag and trains the classifier. To solve the resulted non-convex non-smooth problem efficiently, an alternative method is employed and the bundle method that explicitly treats the convex and non-convex parts is suggested. Some theoretical analysis, such as the time complex-ity and generalized error rate, are provided thereafter. The experimental results on both the text and image datasets, as well as the newly proposed application  X  Insider Threat Detection, clearly demonstrate the ad-vantages of the proposed method.
 This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work is also partially supported by the Center for Science of Information (CSoI) under grant agree-ment CCF-0939370.

