 Modern Standard Arabic (MSA) is a morpho-syntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine transla-tion (SMT). While comprehensive Arabic prepro-cessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sa-dat and Habash (2006), Zollmann et al. (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. (2009), Crego and Habash (2008), Habash (2007)). Arabic verbal constructions are par-ticularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped ( X  X ull subject X ) constructions. As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily pre-verbal subject languages (SVO) such as En-glish.

These issues are particularly problematic in phrase-based SMT (Koehn et al., 2003). Standard phrase-based SMT systems memorize phrasal translation of verb and subject constructions as ob-served in the training bitext. They do not cap-ture any generalizations between occurrences in VS and SV orders, even for the same verbs. In addition, their distance-based reordering models are not well suited to handling complex reorder-ing operations which can include long distance dependencies, and may vary by context. Despite these limitations, phrase-based SMT systems have achieved competitive results in Arabic-to-English benchmark evaluations. 1 However, error analysis shows that verbs are still often dropped or incor-rectly translated, and subjects are split or garbled in translation. This suggests that better syntactic modeling should further improve SMT.

We attempt to get a better understanding of translation patterns for Arabic verb constructions, particularly VS constructions, by studying their occurrence and reordering patterns in a hand-aligned Arabic-English parallel treebank. Our analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb sub-ject translation. In order to detect VS construc-tions, we use our state-of-the-art Arabic depen-dency parser, which is essentially the CATIB E X baseline in our subsequent parsing work in Mar-ton et al. (2010), and is further described there. We show that VS subjects and their exact boundaries are hard to identify accurately. Given the noise in VS detection, existing strategies for source-side reordering (e.g., Xia and McCord (2004), Collins et al. (2005), Wang et al. (2007)) or using de-Table 1: How are Arabic SV and VS translated in the manually word-aligned Arabic-English paral-lel treebank? We check whether V and S are trans-lated in a  X  X onotone X  or  X  X nverted X  order for all VS and SV constructions.  X  X verlap X  represents instances where translations of the Arabic verb and subject have some English words in common, and are not monotone nor inverted. pendency parses as cohesion constraints in decod-ing (e.g., Cherry (2008); Bach et al. (2009)) are not effective at this stage. While these approaches have been successful for language pairs such as German-English for which syntactic parsers are more developed and relevant reordering patterns might be less ambiguous, their impact potential on Arabic-English translation is still unclear.
In this work, we focus on VS constructions only, and propose a new strategy in order to bene-fit from their noisy detection: for the word align-ment stage only, we reorder phrases detected as VS constructions into an SV order. Then, for phrase extraction, weight optimization and decod-ing, we use the original (non-reordered) text. This approach significantly improves both BLEU and TER on top of strong medium and large-scale phrase-based SMT baselines. We use the manually word-aligned parallel Arabic-English Treebank (LDC2009E82) to study how Arabic VS constructions are translated into English by humans. Given the gold Arabic syn-tactic parses and the manual Arabic-English word alignments, we can determine the gold reorder-ings for SV and VS constructions. We extract VS representations from the gold constituent parses by deterministic conversion to a simplified depen-dency structure, CATiB (Habash and Roth, 2009) (see Section 3). We then check whether the En-glish translations of the Arabic verb and the Ara-bic subject occur in the same order as in Arabic (monotone) or not (inverted). Table 1 summa-rizes the reordering patterns for each category. As expected, 98% of Arabic SV are translated in a monotone order in English. For VS constructions, the picture is surprisingly more complex. The monotone VS translations are mostly explained by changes to passive voice or to non-verbal con-structions (such as nominalization) in the English translation.

In addition, Table 1 shows that verb subjects oc-cur more frequently in VS order (70%) than in SV order (30%). These numbers do not include pro-dropped ( X  X ull subject X ) constructions. Even if the SMT system had perfect knowledge of VS reordering, it has to accurately detect VS constructions and their spans in order to apply the reordering correctly. For that purpose, we use our state-of-ther-art parsing model, which is essentially the CATIB E X baseline model in Mar-ton et al. (2010), and whose details we summa-rize next. We train a syntactic dependency parser, MaltParser v1.3 with the Nivre  X  X ager X  algorithm (Nivre, 2003; Nivre et al., 2006; Nivre, 2008) on the training portion of the Penn Arabic Treebank part 3 v3.1, hereafter PATB3 (Maamouri et al., 2008; Maamouri et al., 2009). The training / de-velopment split is the same as in Zitouni et al. (2006). We convert the PATB3 representation into the succinct CATiB format, with 8 dependency relations and 6 POS tags, which we then extend to a set of 44 tags using regular expressions of the basic POS and the normalized surface word form, similarly to Marton et al. (2010), following Habash and Roth (2009). We normalize Alif Maq-sura to Ya, and Hamzated Alifs to bare Alif, as is commonly done in Arabic SMT.

For analysis purposes, we evaluate our subject and verb detection on the development part of PATB3 using gold POS tags. There are various ways to go about it. We argue that combined de-tection statistics of constructions of verbs and their subjects (VATS), for which we achieve an F-score of 74%, are more telling for the task at hand. 2 These scores take into account the spans of both the subject and the specific verb it belongs to, and potentially reorder with. We also provide statistics of VS detection separately (F-score 63%), since we only handle VS here. This low score can be explained by the difficulty in detecting the post-verbal subject X  X  end boundary, and the correct verb the subject belongs to. The SV construction scores are higher, presumably since the pre-verbal sub-ject X  X  end is bounded by the verb it belongs to. See Table 2.

Although not directly comparable, our VS scores are similar to those of Green et al. (2009). Their VS detection technique with conditional random fields (CRF) is different from ours in by-passing full syntactic parsing, and in only detect-ing maximal (non-nested) subjects of verb-initial clauses. Additionally, they use a different train-ing / test split of the PATB data (parts 1, 2 and 3). They report 65.9% precision and 61.3% F-score. Note that a closer score comparison should take into account their reported verb detection accuracy of 98.1%.
 Table 2: Precision, Recall and F-scores for con-structions of Arabic verbs and their subjects, eval-uated on our development part of PATB3. construction P R F VS 66.62 59.41 62.81 SV 86.75 61.07 71.68
SV or VS 72.19 59.95 65.50 Based on these analyses, we propose a new method to help phrase-based SMT systems deal with Arabic-English word order differences due to VS constructions. As in related work on syntactic reordering by preprocessing, our method attempts to make Arabic and English word order closer to each other by reordering Arabic VS constructions into SV. However, unlike in previous work, the re-ordered Arabic sentences are used only for word alignment. Phrase translation extraction and de-coding are performed on the original Arabic word order. Preliminary experiments on an earlier ver-sion of the large-scale SMT system described in Section 6 showed that forcing reordering of all VS constructions at training and test time does not have a consistent impact on translation qual-ity: for instance, on the NIST MT08-NW test set, TER slightly improved from 44.34 to 44.03, while BLEU score decreased from 49.21 to 49.09.

Limiting reordering to alignment allows the sys-tem to be more robust and recover from incorrect changes introduced either by incorrect VS detec-tion, or by incorrect reordering of a correctly de-tected VS. Given a parallel sentence ( a, e ) , we proceed as follows: 1. automatically tag VS constructions in a 2. generate new sentence a 0 = reorder ( a ) by 3. get word alignment wa 0 on new sentence pair 4. using mapping from a to a 0 , get correspond-We use the open-source Moses toolkit (Koehn et al., 2007) to build two phrase-based SMT systems trained on two different data conditions:  X  medium-scale the bitext consists of 12M  X  large-scale the bitext consists of several
Except from this difference in training data, the two systems are identical. They use a standard phrase-based architecture. The parallel corpus is word-aligned using the GIZA++ (Och and Ney, 2003), which sequentially learns word alignments for the IBM1, HMM, IBM3 and IBM4 models. The resulting alignments in both translation di-rections are intersected and augmented using the grow-diag-final-and heuristic (Koehn et al., 2007). Phrase translations of up to 10 words are extracted in the Moses phrase-table. We apply statistical significance tests to prune unreliable phrase-pairs and score remaining phrase-table entries (Chen et al., 2009). We use a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on the NIST MT06 test set.

For all systems, the English data is tokenized using simple punctuation-based rules. The Arabic side is segmented according to the Arabic Tree-bank (PATB3) tokenization scheme (Maamouri et al., 2009) using the MADA+TOKAN morpholog-ical analyzer and tokenizer (Habash and Rambow, 2005). MADA-produced Arabic lemmas are used for word alignment. We evaluate translation quality using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores on three standard evaluation test sets from the NIST evaluations, which yield more than 4400 test sentences with 4 reference transla-tions. On this large data set, our VS reordering method remarkably yields statistically significant improvements in BLEU and TER on the medium and large SMT systems at the 99% confidence level (Table 3).

Results per test set are reported in Table 4. TER scores are improved in all 10 test configurations, and BLEU scores are improved in 8 out of the 10 configurations. Results on the MT08 test set show that improvements are obtained both on newswire and on web text as measured by TER (but not BLEU score on the web section.) It is worth noting that consistent improvements are obtained even on the large-scale system, and that both baselines are full-fledged systems, which include lexicalized re-ordering and large 5-gram language models.

Analysis shows that our VS reordering tech-nique improves word alignment coverage (yield-ing 48k and 330k additional links on the medium and large scale systems respectively). This results in larger phrase-tables which improve translation quality. To the best of our knowledge, the only other ap-proach to detecting and using Arabic verb-subject constructions for SMT is that of Green et al. (2009) (see Section 3), which failed to improve Arabic-English SMT. In contrast with our reorder-ing approach, they integrate subject span informa-tion as a log-linear model feature which encour-Table 3: Evaluation on all test sets: on the total of 4432 test sentences, improvements are statisti-cally significant at the 99% level using bootstrap resampling (Koehn, 2004) medium baseline 44.35 48.34 + VS reordering 44.65 (+0.30) 47.78 (-0.56) large baseline 51.45 42.45 + VS reordering 51.70 (+0.25) 42.21 (-0.24) ages a phrase-based SMT decoder to use phrasal translations that do not break subject boundaries.
Syntactically motivated reordering for phrase-based SMT has been more successful on language pairs other than Arabic-English, perhaps due to more accurate parsers and less ambiguous reorder-ing patterns than for Arabic VS. For instance, Collins et al. (2005) apply six manually defined transformations to German parse trees which im-prove German-English translation by 0.4 BLEU on the Europarl task. Xia and McCord (2004) learn reordering rules for French to English trans-lations, which arguably presents less syntactic dis-tortion than Arabic-English. Zhang et al. (2007) limit reordering to decoding for Chinese-English SMT using a lattice representation. Cherry (2008) uses dependency parses as cohesion constraints in decoding for French-English SMT.

For Arabic-English phrase-based SMT, the im-pact of syntactic reordering as preprocessing is less clear. Habash (2007) proposes to learn syntac-tic reordering rules targeting Arabic-English word order differences and integrates them as deter-ministic preprocessing. He reports improvements in BLEU compared to phrase-based SMT limited to monotonic decoding, but these improvements do not hold with distortion. Instead of apply-ing reordering rules deterministically, Crego and Habash (2008) use a lattice input to represent alter-nate word orders which improves a ngram-based SMT system. But they do not model VS construc-tions explicitly.

Most previous syntax-aware word alignment models were specifically designed for syntax-based SMT systems. These models are often bootstrapped from existing word alignments, and could therefore benefit from our VS reordering ap-proach. For instance, Fossum et al. (2008) report improvements ranging from 0.1 to 0.5 BLEU on Arabic translation by learning to delete alignment metrics, and 2 MT systems medium baseline 45.95 44.94 48.05 44.86 32.05 medium baseline 48.77 46.45 45.00 47.74 58.02 links if they degrade their syntax-based translation system. Departing from commonly-used align-ment models, Hermjakob (2009) aligns Arabic and English content words using pointwise mutual in-formation, and in this process indirectly uses En-glish sentences reordered into VS order to collect cooccurrence counts. The approach outperforms GIZA++ on a small-scale translation task, but the impact of reordering alone is not evaluated. We presented a novel method for improving over-all SMT quality using a noisy syntactic parser: we use these parses to reorder VS constructions into SV for word alignment only. This approach in-creases word alignment coverage and significantly improves BLEU and TER scores on two strong SMT baselines.

In subsequent work, we show that matrix (main-clause) VS constructions are reordered much more frequently than non-matrix VS, and that limit-ing reordering to matrix VS constructions for word alignment further improves translation qual-ity (Carpuat et al., 2010). In the future, we plan to improve robustness to parsing errors by using not just one, but multiple subject boundary hypothe-ses. We will also investigate the integration of VS reordering in SMT decoding.

