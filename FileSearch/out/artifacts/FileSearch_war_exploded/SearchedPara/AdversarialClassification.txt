 Essen tially all data mining algorithms assume that the data-generating pro cess is indep enden t of the data miner's activ-ities. However, in man y domains, including spam detec-tion, intrusion detection, fraud detection, surv eillance and coun ter-terrorism, this is far from the case: the data is ac-tively manipulated by an adv ersary seeking to mak e the clas-si er pro duce false negativ es. In these domains, the perfor-mance of a classi er can degrade rapidly after it is deplo yed, as the adv ersary learns to defeat it. Curren tly the only so-lution to this is rep eated, man ual, ad hoc reconstruction of the classi er. In this pap er we dev elop a formal framew ork and algorithms for this problem. We view classi cation as a game between the classi er and the adv ersary , and pro duce a classi er that is optimal given the adv ersary's optimal strat-egy . Exp erimen ts in a spam detection domain sho w that this approac h can greatly outp erform a classi er learned in the standard way, and (within the parameters of the problem) automatically adapt the classi er to the adv ersary's evolving manipulations.
 H.2.8 [ Database Managemen t ]: Database Applications| data mining ; I.2.6 [ Arti cial Intelligence ]: Learning| conc ept learning, induction, parameter learning ; I.5.1 [ Patt-ern Recognition ]: Mo dels| statistic al ; I.5.2 [ Pattern Re-cognition ]: Design Metho dology| classi er design and eval-uation, featur e evaluation and sele ction ; G.3 [ Mathematics of Computing ]: Probabilit y and Statistics| multivariate statistics Algorithms Cost-sensitiv e learning, game theory , naiv e Bayes, spam de-tection, integer linear programming
Man y ma jor applications of KDD share a characteristic that has so far receiv ed little atten tion from the researc h comm unit y: the presence of an adv ersary activ ely manip-ulating the data to defeat the data miner. In these do-mains, deplo ymen t of a KDD system causes the data to change so as to mak e the system ine ectiv e. For example, in the domain of email spam detection, standard classi ers like naiv e Bayes were initially quite successful (e.g., [23]). Unfortunately , spammers soon learned to fool them by in-serting \non-spam" words into emails, breaking up \spam" ones with spurious punctuation, etc. Once spam lters were mo di ed to detect these tric ks, spammers started using new ones [4]. E ectiv ely, spammers and data miners are engaged in a nev er-ending game where data miners con tinually come up with new ways to detect spam, and spammers con tinually come up with new ways to avoid detection.

Similar arms races are found in man y other domains: com-puter intrusion detection, where new attac ks circum vent the defenses put in place against old ones [17]; fraud detection, where perp etrators learn to avoid the actions that previously gave them away [5, 25]; coun ter-terrorism, where terrorists disguise their iden tity and activities in ever-shifting ways [11]; aerial surv eillance, where targets are camou aged with increasing sophistication [22]; comparison shopping, where merc han ts con tinually change their Web sites to avoid wrap-ping by shopb ots [3]; le sharing, where media companies try to detect and frustrate illegal cop ying, and users nd ways to circum vent the obstacles [14]; Web searc h, where webmasters manipulate pages and links to in ate their rank-ings, and searc h engines reengineer their ranking functions to de ate them bac k again [9, 16]; etc.

In man y of these domains, researc hers have noted the pres-ence of adaptiv e adv ersaries and the need to tak e them into accoun t (e.g., [4, 5, 11]), but to our kno wledge no system-atic approac h for this has so far been dev elop ed. The result is that the performance of deplo yed KDD systems in ad-versarial domains can degrade rapidly over time, and much human e ort and cost is incurred in rep eatedly bringing the systems bac k up to the desired performance level. This pa-per prop oses a rst step towards automating this pro cess. While complete automation will nev er be possible, we be-liev e our approac h and its future extensions have the poten-tial to signi can tly impro ve the speed and cost-e ectiv eness of keeping KDD systems up to date with their adv ersaries.
Notice that adv ersarial problems cannot simply be solv ed by learners that accoun t for concept drift (e.g., [10]): while these learners allo w the data-generating pro cess to change over time, they do not allo w this change to be a function of the classi er itself.

We rst formalize the problem as a game between a cost-sensitiv e classi er and a cost-sensitiv e adv ersary (Section 2). Focusing on the naiv e Bayes classi er (Section 3), we de-scrib e the optimal strategy for the adv ersary against a stan-dard (adv ersary-una ware) classi er (Section 4), and the op-timal strategy for a classi er playing against this strategy (Section 5). We pro vide ecien t algorithms for computing or appro ximating these strategies. Exp erimen ts in a spam detection domain illustrate the sometimes very large util-ity gains that an adv ersary-a ware classi er can yield, and its abilit y to co-ev olve with the adv ersary (Section 6). We conclude with a discussion of future researc h directions (Sec-tion 7).
Consider a vector variable X = ( X 1 ; : : : ; X i ; : : : ; X ere X i is the i th feature or attribute, and let the instance space X be the set of possible values of X . An instance x is a vector where feature X i has the value x i . Instances can belong to one of two classes: positiv e (malicious) or negativ e (inno cen t). Inno cen t instances are generated i.i.d. (indep en-den t and iden tically distributed) from a distribution P ( X j and malicious ones likewise from P ( X j + ). The global dis-tribution is thus P ( X ) = P ( -) P ( X j -) + P ( + ) P ( X j the training set S and test set T be two sets of ( x; y ) pairs, where x is generated according to P ( X ) and y is the \true" class of x . We de ne adv ersarial classi cation as a game between two players: Classifier , whic h attempts to learn from S a function y C = C ( x ) that will correctly predict the classes of instances in T , and Adversar y , whic h attempts to mak e Classifier classify positiv e instances in T as neg-ativ e by mo difying those instances from x to x 0 = A ( x ). ( Adversar y cannot mo dify negativ e instances, and thus A ( x ) = x for all x 2 -.) Classifier is characterized by a set of cost/utilit y parameters (see Table 1 for a summary of the notation used in this pap er): 1. V i : Cost of measuring X i . Dep ending on their costs, 2. U C ( y C ; y ): Utilit y of classifying as y C an instance with Adversar y has a corresp onding set of parameters: 1. W i ( x i ; x 0 i ) : Cost of changing the i th feature from x 2. U A ( y C ; y ): Utilit y accrued by Adversar y when Clas-
The goal of Classifier is to build a classi er C that will maximize its exp ected utilit y, taking into accoun t that in-stances may have been mo di ed by Adversar y : U where Y = f + ; -g and X C ( x ) f X 1 ; : : : ; X n g is the set of features measured by C , possibly dep enden t on x . We call C the optimal strategy of Classifier .

The goal of Adversar y is to nd a feature change strat-egy A that will maximize its own exp ected utilit y: We call A the optimal strategy of Adversar y . Notice that Adversar y will not change instances if the cost of doing so exceeds the utilit y of fooling Classifier . For example, a spammer will not mo dify his emails to the point where they no longer help sell his pro duct. In practice, U C and U are estimated by averages over T : U C = (1 = jTj ) P ( x;y ) 2T [ U C ( C ( A ( x )) ; y ) P X i 2X C ( x ) V i ], etc.
Giv en two players, the actions available to eac h, and the payo s from eac h com bination of actions, classical game the-ory is concerned with nding a com bination of strategies suc h that neither player can gain by unilaterally changing its strategy . This com bination is kno wn as a Nash equilib-rium [7]. In our case, the actions are classi ers C and feature change strategies A , and the payo s are U C and U A . As the follo wing theorem sho ws, some realizations of the adv ersar-ial classi cation game alw ays have a Nash equilibrium.
Theorem 2.1. Consider a classi c ation game with a bi-nary cost model for Adversar y , i.e., given a pair of in-stanc es x and x 0 , Adversar y can either change x to x 0 curring a unit cost) or it cannot (the cost is in nite). This game always has a Nash equilibrium, which can be found in time polynomial in the numb er of instanc es.

We omit the pro of due to lack of space. Unfortunately , the calculation of the Nash equilibrium requires complete and perfect kno wledge of the probabilities of all the in-stances, whic h in practice Adversar y and Classifier will not have. Computing Nash equilibria will generally be in-tractable. The chief dicult y is that even in nite domains the num ber of available actions is doubly exp onen tial in the num ber of features n . The best kno wn algorithms for nd-ing Nash equilibria in general (nonzero) sum games have worst-case exp onen tial time in the num ber of actions, mak-ing them triply exp onen tial in our case. Even using the more general notion of correlated equilibria, for whic h poly-nomial algorithms exist, the computational cost is still dou-bly exp onen tial. Recen t years have seen substan tial work on computationally tractable approac hes to game theory , but they focus mainly on scaling up with the num ber of play-ers, not the num ber of actions [12]. Further, equilibrium strategies, either mixed or pure, assume optimal play on the part of the opp onen t, whic h is highly unrealistic in our case. When this assumption is not met, standard game theory gives no guidance on how to play. (This, and computa-tional intractabilit y, have signi can tly limited its practical use.) We thus leave the general existence and form of Nash or other equilibria in adv ersarial classi cation as an open X = ( X 1 ; X 2 ; : : : ; X n ) Instance.
 C an instance of class y .

W i ( x i ; x 0 i ) ; W ( x; x 0 ) Cost of changing the i to x 0 i and instance x to x 0 , resp ectiv ely.
 th attribute is changed to x 0 i 2X i .
 question, and prop ose instead to start from a set of assump-tions that more closely resem bles the way adv ersarial clas-si cation tak es place in practice: Classifier initially oper-ates assuming the data is untain ted (i.e., A ( x ) = x for all x ); Adversar y then deplo ys an optimal plan A ( x ) against this classi er; Classifier in turn deplo ys an optimal classi-er C ( A ( x )) against this adv ersary , etc. This approac h has some commonalit y with evolutionary game theory [26], but the latter mak es a num ber of assumptions that are inappro-priate in our case (in nite population of players rep eatedly matc hed at random, symmetric payo matrices, players hav-ing o spring prop ortional to average payo , etc.).
In this pap er, we focus mainly on the single-shot version of the adv ersarial classi cation game: one move by eac h of the players. We touc h only brie y on the repeated version of the game, where players con tinue to mak e moves inde nitely . A num ber of learning approac hes to rep eated games have been prop osed [6], but these are also intractable in large ac-tion spaces. Other learning approac hes focus on games with sequen tial states (e.g., [15]), while classi cation is stateless.
We mak e the assumption, standard in game theory , that all parameters of both players are kno wn to eac h other. Al-though this is unlik ely to be the case in practice, it is gener-ally plausible that eac h player will be able to mak e a rough guess of the other's (and, indeed, its own) parameters. Clas-si cation with imprecisely kno wn costs and other parame-ters has been well studied in KDD (e.g., [20]), and extending this to the adv ersarial case is an imp ortan t item for future work.
In this pap er, we will focus on naiv e Bayes as the classi-er to be made adv ersary-a ware [2]. Naiv e Bayes is attrac-tive because of its simplicit y, eciency , and excellen t perfor-mance in a wide range of applications, including adv ersarial ones like spam detection [23]. Naiv e Bayes estimates the probabilit y that an instance x belongs to class y as and predicts the class with highest P ( y j x ). The denomina-tor P ( x ) is indep enden t of the class, and can be ignored. P ( x j y ) = Q n i =1 P ( x i j y ) is the naiv e Bayes assumption. The relev ant probabilities are learned simply by coun ting the cor-resp onding occurrences in the training set S .

We begin by extending naiv e Bayes to incorp orate the measuremen t costs V i and classi cation utilities U C ( y de ned in the previous section, and to maximize the ex-pected utilit y U C (Equation 1). For now, we assume that no adv ersary is presen t (i.e., A ( x ) = x for all x ). We remo ve this restriction in the next sections.

Cost-sensitiv e learning has been the object of substan tial study in the KDD literature [1, 27]. Giv en a classi cation utilit y matrix U C ( y C ; y ), the Bayes optimal prediction for an instance x is the class y C that maximizes the conditional utilit y U ( y C j x ): This is simply Equation 1 conditioned on a particular x , and ignoring the adv ersary and measuremen t costs V i . In naiv e Bayes, P ( y j x ) is computed using Equation 3.

Measuremen t costs are incorp orated into the choice of whic h subset of features to measure, X C f X 1 ; : : : ; X tuitiv ely, we want to measure feature X i only if this impro ves the exp ected utilit y U C by more than V i . Since a feature's e ect on U C will in general dep end on what other features are being measured, nding the optimal X C requires a po-ten tially exp onen tial searc h. In practice, X C can be found using standard feature selection algorithms with U C as the evaluation function. We use greedy forw ard selection ([13]) in our exp erimen ts. (Feature selection can also be carried out online, but we do not pursue that approac h here.)
In this section, we formalize the notion of an optimal strat-egy for Adversar y . We mo del it as a constrained optimiza-tion problem, whic h can be form ulated as an integer linear program. We then prop ose a pseudo-linear time solution to the integer LP, based on dynamic programming. We mak e the follo wing assumptions.

Assumption 1. Complete Information: Both Clas-sifier and Adversar y know all the relevant parameters: V ; U C ; W i ; U A and the naive Bayes model learne d by Clas-sifier on S (including X C , P ( y ) , and P ( x i j y ) for each fea-ture and class).

Assumption 2. Adversar y assumes that Classifier is unawar e of its presenc e (i.e., Adversar y assumes that C ( x ) is the naive Bayes model describ ed in the previous section).
To defeat Classifier , Adversar y needs only to mo dify features in X C , since the others are not measured. From Equation 3: For brevit y, we will use the notation LO C ( x ) = log P ( + j x ) Naiv e Bayes classi es an instance x as positiv e if the ex-pected utilit y of doing so exceeds that of classifying it as negativ e, i.e., if U C ( + ; + ) P ( + j x ) + U C ( + ; P ( + j x ) + U C ( -; -) P ( -j x ), or
Let the log of the righ t hand side be LT ( U C ) ( log thresh-old ). Then naiv e Bayes classi es instance x as positiv e if gap ( x ) = LO C ( x ) LT ( U C ). If the instance is classi ed as negativ e, Adversar y does not need to do anything. Let us assume, then, that x is classi ed as positiv e, i.e., gap ( x ) &gt; 0. The objectiv e of Adversar y is to mak e some set of feature changes to x that will cause it to be classi ed as negativ e, while incurring the minim um possible cost. This causes Ad-versar y to gain a utilit y of U A = U A ( -; + ) U A ( Thus Adversar y will transform x as long as the total cost incurred is less than U A and not otherwise.

We form ulate the problem of nding an optimal strategy for Adversar y as an integer linear program. Recall that X i is the domain of X i . For x 0 i 2 X i , let i;x 0 i be an integer (binary) variable whic h tak es the value one if the feature X is changed from x i to x 0 i , and zero otherwise. Let the new data item thus obtained be x 0 . The cost of transforming x log odds is LO C ( x 0 ) LO C ( x ) = LO C ( x 0 i ) LO C sar y 's objectiv e of making the instance negativ e. Note that
LO i;x i = 0; this represen ts the case where X i has not been changed. To transform x so that the new instance is classi-ed as negativ e, Adversar y needs to change the values of some features suc h that the sum of their gains (decrease in log odds) is more that gap ( x ).

Thus, to nd the minim um cost changes required to trans-form this instance into a negativ e instance, we need to solv e the follo wing integer (binary) linear program: whic h values. The optimizing equation minimizes the cost incurred in this transformation. The rst constrain t mak es sure that the new instance will be classi ed as negativ e. The second constrain t enco des the requiremen t that a feature can only have a single value in an instance. We will call the transformed instance obtained by solving this integer linear program the minimum cost camou age ( M CC ) of x . In other words, M CC ( x ) is the nearest instance (cost wise) to x whic h naiv e Bayes classi es as negativ e.

After solving this integer LP, Adversar y transforms the instance only if the minim um cost obtained is less than U Therefore, letting N B ( x ) be the naiv e Bayes class prediction for x , A ( x ) =
The above integer (binary) LP problem is NP-hard, as the 0-1 knapsac k problem can be reduced to it [8]. However, a pseudo-linear time algorithm can be obtained by discretiz-ing LO C , whic h allo ws dynamic programming to be used. Although the algorithm is appro ximate, it can compute the solution to arbitrary precision.
 The pro cedure is sho wn in Algorithm 1. Function Find-MCC ( i; w ) computes the minim um cost needed to change the log odds of x by w using only the rst i features. It re-turns the pair ( M inC ost; M inList ) where M inC ost is the minim um cost and M inList is a list of feature-v alue pairs denoting the changes to be made to x . (In eac h pair, i is the feature index and x 0 i is the value it should be changed to.) To obtain the optimal adv ersary strategy , we need to compute FindMCC ( n; W ), where the integer W is gap ( x ) after discretization and n is the num ber of features in X Note that LO i;x 0 discretized log odds space.

The algorithm can be ecien tly implemen ted using top-down recursion with memoization (so that no recursiv e call is computed more than once). Note that although the fea-tures can be considered in any order, some orderings may nd solutions faster than the others. If, in the discretized space, instance x requires a gap of W to be lled by the transformation, then the algorithm runs in time at most O ( W P i jX i j ) (since the for loop in FindMCC is called at most W times and eac h time it tak es O ( P i jX i j ) time). Hence it is pseudo-linear in the num ber of features. Pseudo-linearit y may be exp ensiv e for large values of W or for cases where features have large domains. We now presen t two pruning rules, one for use in the rst situation, and one for the second.
 Algorithm 1 FindMCC ( i , w ) if w 0 then end if if i = 0 then end if M inC ost 1
M inList Unde ne d for x 0 i 2X i do end for return ( M inC ost; M inList ) Algorithm 2 A ( x )
W gap ( x ) (discretized). ( M inC ost; M inList ) FindMCC ( n; W ) if N B ( x ) = + and M inC ost &lt; U A then else end if
Lemma 4.1. If then A ( x ) = x .

This lemma is easy to pro ve and can be used to detect the instances for whic h M inC ost &gt; U A . Instances whic h are positiv e by very large gap ( x ) values can thus be pruned early on, and we need to run the algorithm only for more reasonable values of gap ( x ).

Our second pruning strategy can be emplo yed in situa-tions where the cost metric is sucien tly coarsely discretized. We globally sort all the ( i; x 0 i ) tuples in increasing order of W ( x i ; x 0 i ) 0. For iden tical values of W i ( x i ; x decreasing order of LO i;x 0 particular i , W i ( x i ; x 0 i ) com bination, over all i , we can re-move all but the rst entry in the list. This is valid because, if the X i is changed in the optimal solution, then taking the value x 0 i with the highest LO i;x 0 solution. We can prune even further by only considering the rst k tuples in eac h W suc h that P i 1 j =1 LO j;x 0 pruning does not a ect the optimal solution.

Thus, if the feature-c hanging costs W i are sucien tly co-arsely discretized, we will nev er need to consider more than a few tuples for eac h integer value of W i . Our algorithm will thus run ecien tly even when the domains of features are large.
We now describ e how Classifier can adapt to the adv er-sary strategy describ ed in the previous section. We deriv e the optimal C ( x ) taking into accoun t A ( x ), and give an ef-cien t algorithm for computing it. We mak e the follo wing additional assumptions.

Assumption 3. Classifier assumes that Adversar y uses its optimal strategy to modify test instanc es (Algorithm 2).
Assumption 4. The training set S used for learning the initial naive Bayes classi er is not tamp ered with by Ad-versar y (i.e., S is drawn from the real distribution of ad-versarial and non-adversarial data).

Assumption 5. 8 X i 2 X , W i ( x i ; x 0 i ) is a semi-metric, i.e., it has the following properties: 1. W i ( x i ; x 0 i ) 0 and the equality holds i x i = x 2. W i ( x i ; x 00 i ) W i ( x i ; x 0 i ) + W i ( x 0 i The above also implies that W ( x; x 00 ) W ( x; x 0 )+ W ( x The triangular inequalit y for cost holds in most real do-mains. This is because to change a feature from x i to x 00 the adv ersary alw ays has the option of changing it via x i.e., with x 0 i as an intermediate value.

The goal of Classifier , as in Section 3, is to predict for eac h instance x 0 the class that maximizes its conditional utilit y (Equation 4). The di erence is that now we want to tak e into accoun t the fact that Adversar y has tamp ered with the data. Of all the probabilities used by Classifier (Equation 3), the only one that is changed by Adversar y P ( x 0 j + ) be the post-adv ersary version of P ( x 0 j + In other words, the probabilit y of observing an instance x the probabilit y that the adv ersary generates some instance x and then mo di es it into x 0 , summed over all x . Since P where X A ( x 0 ) = f x : x 0 = A ( x ) g . There are two cases where Adversar y will leave an instance x untamp ered (i.e., A ( x ) = x ): when naiv e Bayes predicts it is negativ e, since then no action is necessary , and when there is no transfor-mation of x whose cost is lower than the utilit y gained by making it app ear negativ e. Thus where X 0 A ( x 0 ) = X A ( x 0 ) n f x 0 g , I ( x 0 ) = 1 if N B ( x or W ( x 0 ; M CC ( x 0 )) U A , and I ( x 0 ) = 0 otherwise (see Equation 7 and Algorithm 2). The untamp ered probabilities P ( x 0 j + ) are estimated using the naiv e Bayes mo del (Equa-tion 3): P ( x 0 j + ) = Q X adv ersary-a ware classi cation algorithm C ( x 0 ) is sho wn be-low, with ^ P () used to denote the probabilit y P () estimated from the training data S . ^ P A ( x 0 j + ) is given by Equation 10 using the empirical estimates of P ( x 0 j + ). The second term in Equation 10, I ( x 0 ) P ( x 0 j + ), is easy to compute given calls to N B ( x 0 ) and Algorithm 1 to determine if x has a feasi-ble camou age. The remainder of this section is dev oted to ecien tly computing the rst term, P x 2X 0 Algorithm 3 C ( x 0 ) if U ( + j x 0 ) &gt; U ( -j x 0 ) then else end if
One solution is to iterate through all possible positiv e ex-amples and chec k if x 0 is their minim um cost camou age. This is, of course, not feasible. We now study some theo-retical prop erties of the M CC function whic h will later be used to prune this searc h. Recall that if N B ( x ) = -then gap ( x ) &lt; 0 and vice versa. We de ne x [ i = x 0 stance whic h is iden tical to x except that its i th feature is changed to x 0 i .
 Lemma 5.1. Let x A be any positive instanc e and let x 0 = M CC ( x A ) . Then, 8 i , &lt; W ( x A ; x 0 ), since x 00 di ers from x A on one less feature than x 0 . Also gap ( x 00 ) = gap ( x 0 ) + LO C (( x A Since x 0 is M CC ( x A ) and W ( x A ; x 00 ) &lt; W ( x must be + , and therefore gap ( x 00 ) &gt; 0, pro ving the result.
Giv en a negativ e instance x 0 , for eac h feature we compute all values v that satisfy Lemma 5.1. To compute X 0 A ( x we only need to tak e com binations of these feature-v alue pairs and chec k if x 0 is their MCC. This can substan tially reduce the num ber of positiv e instances in our searc h space. The searc h space can still poten tially con tain an exp onen tial num ber of instances. However, after we emplo y the next theorem, we obtain a fast algorithm for estimating the set X
A ( x 0 ).
Notice that the optimal feature subset X C for the adv ersary-a ware classi er may be di eren t from the adv ersary-una ware one, but can be found using the same metho ds (see Section 3).

Theorem 5.2. Let x A be a positive instanc e such that x 0 = M CC ( x A ) . Let D be the set of featur es that are change d in x A to obtain x 0 . Let E be a non-trivial subset of D , and let x 0 A be an instanc e that matches x 0 for all fea-tures in E and x A for all others, i.e., ( x 0 A ) i = x 0 ( x A ) i = ( x A ) i otherwise. Then x 0 = M CC ( x 0 A ) .
Proof. By con tradiction. Supp ose x 00 = M CC ( x 0 A ) and x 00 6 = x 0 . Then W ( x 0 A ; x 00 ) &lt; W ( x 0 A ; x 0 by de nition of W ( x; y ) we have W ( x A ; x 0 ) = W ( x W ( x 0 A ; x 0 ). So by the triangle inequalit y since then x 0 6 = M CC ( x A ). This completes the pro of.
The above theorem implies that if x A is a positiv e instance suc h that x 0 6 = M CC ( x A ) then x 0 cannot be the MCC of any other instance x 0 A suc h that the changed features from x to x 0 form a sup erset of the changed features from x A to x . We now use the follo wing result to obtain bounds on X A ( x 0 ).

Cor ollar y 5.3. Let F V be the set of featur e-value pairs that satisfy Lemma 5.1. Let GV F V be such that ( i; x i ) 2 of featur e-value pairs wher e x A and x 0 di er form a subset of GV .

From the above corollary , after we compute GV , we only need to consider the com binations of feature-v alue pairs that are in GV and change those in the observ ed instance x 0 . Theorem 5.2 also implies that performing single changes from GV returns instances in X 0 A ( x 0 ). This gives us the follo wing bounds on P x 2X 0
Theorem 5.4. Let x 0 be any instanc e and let GV be the set de ne d in Cor ollary 5.3. Let G = f i j9 x i ( i; x i and let X G i = f x i 2X i j ( i; x i ) 2 GV g . Then
X
Proof. The pro of of the rst inequalit y follo ws directly from Theorem 5.2, whic h states that changing any single feature of x 0 to any value in GV returns an instance from X
A ( x 0 ). To pro ve the second inequalit y, we observ e that the expression on the righ t side, when expanded, gives the sum of probabilities of all possible changes in x 0 due to the set GV , and X 0 A ( x 0 ) is a subset of those instances.
Giv en these bounds, we can classify a test instance as fol-lows. If plugging the lower bound into Algorithm 3 gives U ( + j x 0 ) &gt; U ( -j x 0 ), then the instance can be safely classi-ed as positiv e. Similarly , if using the upp er bound gives U ( + j x 0 ) &lt; U ( -j x 0 ), then the instance is negativ e. If GV is large, so is the lower bound on P A ( X 0 A ( x 0 ) j + small, we can do an exhaustiv e searc h over the subsets of GV and chec k if eac h of the items considered belongs to X 0 In our exp erimen ts we nd that using the lower bound for making predictions works well in practice.
We implemen ted an adv ersarial classi er system for the spam ltering domain. Spam is an attractiv e testb ed for our metho ds because of its practical imp ortance, its rapidly evolving adv ersarial nature, the wide availabilit y of data (in con trast to man y other adv ersarial domains), the fact that naiv e Bayes is the de facto standard classi er in this area, and its richness as a challenge problem for KDD [4]. One disadv antage of spam as a testb ed is that feature measure-men t costs are generally negligible, leaving this part of our framew ork untested. (In con trast, in a domain like coun ter-terrorism feature measuremen ts are a ma jor issue, often requiring large num bers of personnel and exp ensiv e equip-men t, raising priv acy issues, and imp osing costs on millions of individuals and transactions.) We used the follo wing two datasets in our exp erimen ts: Ling-Spam [24]: This corpus con tains the legitimate dis-Email-Data [19]: This corpus consists of texts from 1431
Eac h of these datasets was divided into ten parts for ten-fold cross-v alidation. We de ned three scenarios, as de-scrib ed below, and applied our implemen tation of naiv e Bayes (NB) and the adv ersary-a ware classi er (AC) to eac h. We used ifile [21] for prepro cessing emails.
The three spam ltering scenarios that we implemen ted di er in how the email is represen ted for the classi er, how the adv ersary can mo dify the features, and at what cost. Add Words (AW): This the simplest scenario. The bino-Add Length (AL): This mo del is very similar to AW, ex-Synon ym (SYN): Generally , spammers want to avoid de-Table 2: Utilit y matrices for Adv ersary and Classi-er used in the exp erimen ts.
It is easy to see that the costs used in all scenarios are metrics, so we can apply Lemma 5.1 and Theorem 5.2.
The U A classi cation utilit y matrix for Adversar y we used is suc h that whenev er a spam email is classi ed as non-spam the adv ersary receiv es a utilit y of 20, and all other entries are 0. Thus, in the SYN and AW scenarios 20 word replacemen ts/additions are allo wed. In the AL scenario, the cost of adding a character is set to 0.1, and as a result 200 character additions are allo wed.

For Classifier , we ran the exp erimen ts with three dif-feren t utilit y matrices ( U C ). All matrices had a utilit y of 1 for correctly classifying an email and a penalt y (negativ e utilit y) of 1 for incorrectly classifying a spam email as non-spam. The penalt y for incorrectly classifying a non-spam email as spam was set to 10 in one matrix, 100 in another, and 1000 in the third. This re ects the fact that, in spam ltering, the critical and dominan t cost is that of false posi-tives: letting a single spam email get through to the user is a relativ ely insigni can t event, but ltering out a non-spam email is highly undesirable (and poten tially disastrous). The di eren t U C ( + ; -) values corresp ond to the di eren t values of the parameter in Sakkis et al [24]. Table 2 summarizes the utilit y parameters used in the exp erimen ts. The results of running the various algorithms on the Ling-Spam and Email-data datasets are sho wn in Figures 1 and 2 resp ectiv ely. The gures sho w the average utilities ob-tained (with a maxim um value of 1.0) by naiv e Bayes and the adv ersary-a ware classi er under the di eren t scenarios and di eren t U C matrices. The utilit y of naiv e Bayes on the original, untamp ered data (\PLAIN") is represen ted by the blac k bar on the left. The remaining blac k bars rep-resen t the performance of naiv e Bayes on tain ted data in the three scenarios, and the white bars the performance of the corresp onding adv ersary-a ware classi er. We observ e that Adversar y signi can tly degrades the performance of naiv e Bayes in all three scenarios and with all three Classi-fier utilit y matrices. This e ect is more pronounced in the Email-Data set because it has a higher percen tage of spam Figure 1: Utilit y results on the Ling-Spam dataset for di eren t values of U C ( + ; -) .
 Figure 2: Utilit y results on the Email-Data set for di eren t values of U C ( + ; -) . emails than Ling-Spam. For naiv e Bayes on Email-Data, the cost of misclassifying spam emails exceeds the utilit y of the correct predictions, causing the overall utilit y to be neg-ativ e. In con trast, Classifier was able to correctly iden tify a large percen tage of the spam emails in all cases, and its accuracy on non-spam emails was also quite high.

To help in interpreting these results, we rep ort the num-bers of false negativ es and false positiv es for the Ling-Spam dataset in Table 3. We observ e that as the misclassi ca-tion penalt y for non-spam increases, few er non-spam emails are classi ed incorrectly , but naturally more spam emails are misclassi ed as non-spam. Notice that the adv ersarial classi er nev er pro duces false positiv es (except for the SYN scenario with U C ( + ; -) = 10). As a result, its average utilit y stays appro ximately constan t even when U C ( + ; -) changes by two orders of magnitude. An interesting observ ation is that Adversar y 's manipulations can actually help Classi-fier to reduce the num ber of false positiv es. This is because Adversar y is unlik ely to send a spam mail unaltered, and as a result man y non-spam emails whic h were previously Table 3: False positiv es and false negativ es for naiv e Bayes and the adv ersary-a ware classi er on the Ling-Spam dataset. The total num ber of posi-tives in this dataset is 481, and the total num ber of negativ es is 2412. classi ed as positiv e are now classi ed as negativ e.
We also compared the running times of our algorithms for the three scenarios, for both the adv ersary and classi-er strategies. For both AW and SYN mo dels, the aver-age running times were less than 5 ms per email. For AL, the average running time of the classi er strategy was less than 5 ms per mail while the running time of the adv ersary strategy was around 500 ms per email. The adv ersary run-ning time for AW was small because one can use a simple greedy algorithm to implemen t the adv ersary strategy . In the SYN mo del, the searc h space is small because there are few synon yms per word. Hence the time tak en by both algo-rithms is small. However, in the AL mo del, when the LO C emails was high ( &gt; 50), the adv ersary took longer. On the other hand, the adv ersarial classi er, after using the pruning strategies, had to consider very few instances and these had small LO C . Hence its running time was quite small. From the exp erimen ts we can conclude that in practice we can use the pruning strategies for Adversar y and Classifier to reduce the searc h space and time without compromising accuracy .

To sim ulate the e ects of non-adv ersarial concept drift (a realit y in spam and man y other adv ersarial domains), we also tried classifying the mails in the Email-data set using NB and AC trained on the Ling-Spam data. As the fre-quencies of spam and non-spam mails are di eren t in the two datasets, we ran the classi ers without considering the class priors. For both algorithms, the results obtained were only marginally worse than the results obtained by training on the Email-data set itself, demonstrating the robustness of the algorithms.
In Sections 4 and 5 we discussed one round of the game that goes on between Adversar y and Classifier . It con-sists of one ply of Adversar y in whic h it nds the best strategy to fool Classifier and then one ply of Classifier to adapt to it. Both parties can con tinue playing this game. However, Classifier is no longer using a simple naiv e Bayes algorithm. In these exp erimen ts, we mak e the simplifying assumption that Adversar y con tinues to mo del the classi-er as Naiv e Bayes, and uses the techniques that we have dev elop ed for Naiv e Bayes.
 At the end of eac h round, Adversar y learns a Naiv e Bayes classi er based on the outputs of the actual classi-er that Classifier is using in that round. We denote the classi er used by Classifier in round i 1 by C i 1 . Let N B i 1 be the classi er that Adversar y learns from it. Then A i ( x ) is de ned as the optimal adv ersary strategy (as in Algorithm 2) to fool N B i 1 instead of the original NB learned on S . The data coming from Adversar y in round i is A i applied to the original test data to pro duce T i T = A i ( T ). Classifier uses Algorithm 3 based on N B i 1 to classify T i , i.e., Y i = C i ( T i ). The key insigh t is that a new naiv e Bayes mo del N B i can be trained on ( T i ; Y that can serv e as the \ Classifier " for the next round. We compared the performance of N B i with that of C i and found them to be very similar, justifying our assumption, as Ad-versar y is not reacting to a \crippled" Classifier but to one whic h performs almost as well as the \optimal" Clas-sifier . This pro cedure can then rep eated for an arbitrary num ber of rounds.
 Figure 3 sho ws the results of this exp erimen t on the Ling-Spam dataset for the AW scenario. The X-axis is round i of the game, and the Y-axis is the average utilit y obtained by Y i (the i th adv ersary-a ware classi er). The graphs also sho w the average utilit y obtained by N B i 1 ( T i ), to demon-strate the e ect of using an adv ersary-a ware classi er at eac h round.

In all rounds of the game, Classifier using the adv ersary-aware strategy performs signi can tly better than the plain naiv e Bayes. As exp ected, the di erence is highest when the penalt y for misclassifying non-spam is 1000. Furthermore, in this scenario Classifier and Adversar y nev er reac h an equilibrium, and utilit y alternates between two values. This is surprising at rst glance, but a closer examination eluci-dates the reason. In the AW scenario, Adversar y can only add words. So the only way of tamp ering with an instance is to add \go od" words with very low (negativ e) LO C (based on N B i 1 in the i th round). Let the top few \go od" words be GW i 1 . These would have a high frequency of occur-rence in spam mails of T i . When N B i is learned on ( T these words no longer have a low LO C and hence are not in GW i . Thus, A i +1 ignores these words, making them have a high LO C in N B i +1 and be in GW i +1 ! This phenomenon causes LO C values for a word to oscillate, giving rise to the perio dic average utilit y in Fig. 3. Figure 3: Utilit y of naiv e Bayes and the adv ersarial classi er for a rep eated game in the AW scenario and Ling-Spam dataset. The num ber in paren theses is U C ( + , -).
This pap er is only the rst step in a poten tially very rich researc h direction. The next steps include: Rep eated games. In realit y, Adversar y and Classifier Theory . We would like to answ er questions like: What are Incomplete information. When Classifier and Adver-Appro ximately optimal strategies. When nding the Generalization to other classi ers. We would like to ex-Interaction with humans. Because adv ersaries are resour-Multiple adv ersaries. Classi cation games are often played Varian ts of the problem. Our problem de nition does not Other domains and tasks. We would like to apply ad-
In domains ranging from spam detection to coun ter-terror-ism, classi ers have to con tend with adv ersaries manipulat-ing the data to pro duce false negativ es. This pap er formal-izes the problem and extends the naiv e Bayes classi er to optimally detect and reclassify tain ted instances, by taking into accoun t the adv ersary's optimal feature-c hanging strat-egy . When applied to spam detection in a variet y of sce-narios, this approac h consisten tly outp erforms the standard naiv e Bayes, sometimes by a large margin. Researc h in this direction has the poten tial to pro duce KDD systems that are more robust to adv ersary manipulations and require less human interv ention to keep up with them.
 We are grateful to Daniel Lowd, Foster Pro vost and Ted Sen-ator for their insigh tful commen ts on a draft of this pap er. This researc h was partly supp orted by a Sloan Fello wship awarded to the second author. [1] P. Domingos. MetaCost: A general metho d for making [2] P. Domingos and M. Pazzani. On the optimalit y of the [3] R. B. Dooren bos, O. Etzioni, and D. S. Weld. A [4] T. Fawcett. \In vivo" spam ltering: A challenge [5] T. Fawcett and F. Pro vost. Adaptiv e fraud detection. [6] D. Fuden berg and D. Levine. The The ory of Learning [7] D. Fuden berg and J. Tirole. Game The ory . MIT [8] M. R. Garey and D. S. Johnson. Computers and [9] L. Guernsey . Retailers rise in Google rankings as rivals [10] G. Hulten, L. Spencer, and P. Domingos. Mining [11] D. Jensen, M. Rattigan, and H. Blau. Information [12] M. Kearns. Computational game theory . Tutorial, [13] R. Koha vi and G. John. Wrapp ers for feature subset [14] B. Krebs. Online piracy spurs high-tec h arms race. [15] M. L. Littman. Mark ov games as a framew ork for [16] B. Llo yd. Been gazump ed by Google? Trying to mak e [17] M. V. Mahoney and P. K. Chan. Learning [18] A. McCallum and K. Nigam. A comparison of event [19] F. Nielsen. Email data, 2003. [20] F. Pro vost and T. Fawcett. Robust classi cation for [21] J. Rennie. I le spam classi er, 2003. [22] P. Rob ertson and J. M. Brady . Adaptiv e image [23] M. Sahami, S. Dumais, D. Hec kerman, and E. Horvitz. [24] G. Sakkis, I. Androutsop oulos, G. Paliouras, [25] T. Senator. Ongoing managemen t and application of [26] J. M. Smith. Evolution and the The ory of Games . [27] P. Turney . Cost-sensitiv e learning bibliograph y. Online [28] WordNet 2.0: A lexical database for the English
