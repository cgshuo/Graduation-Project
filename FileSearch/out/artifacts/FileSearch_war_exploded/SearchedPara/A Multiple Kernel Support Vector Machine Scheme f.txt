 chine learning method, support vector machine (SVM) has strong theoretical founda-explanation capacity hinders SVM from going fu rther in the applications especially in the medical field. according to some defined measure. It is their drawback that they are usually depend-needs to implement the optimization of SVM repeatedly and results in the expensive computational cost. 
There are few papers published in the cases of rule extraction from SVM. A typi-cal rule extraction approach treats SVM as a black-box and the output of SVM are used to train a machine learning method with explanation capacity such as decision tree to generate rules [4]. But there is lack of theoretical explanations to guarantee based on the linear SVM [5]. This method is not suitable for the nonlinear situation while the nonlinear mapping and the kernel trick is one of the important characteris-tics of SVM. 
In this paper, the feature selection and rule extraction are united in a scheme. This idea comes from the study of multiple kernel learning [6]. The single feature kernel transformed into an ordinary multiple parameter learning problem [7]. The optimiza-datasets. Given a set of data points mal separating hyperplane is found by solving the following regularized optimization problem which is identical with SVM: where c is a regularization parameter and ) ( x  X  is the nonlinear mapping. By introducing the Lagrange function and differentiating with respect to w the following dual programming is gotten: 
If each basic kernel uses a single feature, the kernel function can be described as: where d i x , denotes the th d component of the input vector i x feature coefficients m R  X   X  changes into: 
A two stage iterative procedure is used in this paper. The feature coefficient d  X  is be seen as a multiple parameter learning problem. They can be obtained by minimiz-error function is minimized to obtain the sparse solution: coefficients. 
The dual of this linear programming is: Algorithm 1 lowing steps: 5. Output: the sparse feature coefficients  X  axis-parallel faces and has on e vertex on the hyperplane. Then a rule defined as above has the following formulation: In equation (12), one of the lower bound ( l lying on the hyperplane. region. SVM with linear kernel can be shown as: vertex of this rule is one of the corners of the whole region. gion or another vector lying on the hyperplane. Proof: According to the Kuhn-Tucker condition, the separating hyperplane of MK-SVM with nonlinear kernel can be viewed as the weighted sum of the nonlinear functions: where According to the proposition 1, i z rule are the solutions of the following nonlinear equations: of above univariate nonlinear equations, the other vertex of the hypercube may be the corner of the region or another vector lying on the hyperplane. classification accuracy (hit rate) and point coverage rate (number of samples covered Algorithm 2 1. Implement the algorithm 1 to get the support vectors with the selected features. 2. Derive the rules from support vectors according to proposition 1 and 2. 4. Discard the region the extracted rules covered and get a new given region. 5. Go to step 3 to extract a new rule in the new region. 6. Stop. The performance of this method is measured on three widely used datasets: The breast cancer dataset, the heart disease dataset and the PIMA dataset. fied divided by the total number of negative samples) and specificity (number of posi-evaluate the classification accuracy. In this experiment, the Gaussian kernel is used. SVM outperforms SVM in most of those measures. And the selected features are used to extract rules in follows. The extracted rules for MK-SVM on the breast cancer dataset are shown in table 3. Table 3 also shows the hit rate and point coverage rate for each rule and the average ones for each class. For MK-SVM, only two rules for the negative class and one rule point coverage rate are all superior to 90%. They are promising results. 1 Pos. 3 2 Pos. 5 3 Neg. 4 4 Neg. 4
Table 4 shows the number of extracted rules and coverage rate on the breast cancer dataset using MK-SVM, in compared with three neural network based rule extraction methods (see details in [8]). It is seen that MK-SVM extracts fewer rules and achieves higher coverage rate at most of cases. shown in table 5. The extracted rules are omitted here. It is seen that a small number of rules are extracted on each dataset. The coverage rate of the negative class can be improved by adding new rules, but it will result in a lower hit rate. selected features achieve good performance. 
