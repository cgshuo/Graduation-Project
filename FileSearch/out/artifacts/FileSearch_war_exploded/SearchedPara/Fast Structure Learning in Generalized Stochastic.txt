 Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multi-variate time series data. In this paper, we analyze a flex-ible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions un-der which the impact of hidden variables appears as an ad-ditive term to the evolution matrix estimated with the max-imum likelihood. In particular, we examine three exam-ples, including two popular models for count data, i.e, Pois-son and Conwey-Maxwell Poisson vector auto-regressive pro-cesses, and one powerful model for extreme value data, i.e., Gumbel vector auto-regressive processes. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guaran-tee for it. Experiments on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models. H.3.3 [ Information Storage and Retrieval ]: Time Series Analysis Time Series Analysis, Latent Factors, Generalized Linear Models
In many applications, an enormous amount of time series data is collected, which requires us to develop faster and more efficient algorithms for analysis and forecasting pur-poses. A major challenge with which we are confronted in practical applications is the incompleteness of the data, i.e., certain influential time series are missing in the real-world datasets. For example, in social media analysis, the external events influence large clusters of users, while the news propa-gates through the local connections in the network. In order to identify the true influence patterns among the users, we need to take into consideration the impact of external un-observed events. In climate data analysis, the local terrain characteristics play an important role in the air mass prop-agation while large weather systems, which are usually not observed in the dataset collected by local weather stations, influence wide areas on the ground.

The traditional approach to capture the impact of unob-served variables is to include them in the graphical models and infer their impact on the model via the EM algorithm [9]. However, this approach has two main weaknesses: (1) often times, the EM algorithm only identifies a local opti-mum. (2) While several techniques have been developed to speed up the EM algorithm, usually the inference cannot scale to large datasets. Recent progress shows that in the Gaussian linear undirected graphical [ 4] and vector auto-regressive [ 13] models, the impact of hidden variables ap-pears as an additive low rank matrix in the precision and evolution matrices, respectively. Thus, one can use scalable convex optimization algorithms to decompose the parameter matrix into a sparse local dependency and another low-rank global impact matrix which models the impact of hidden variables.

While the convex sparse plus low-rank decomposition in the linear vector auto-regressive models is promising, the model applies to a very limited class of time series data. For example, in social media applications, in which the number of mentions of key words by users is a counting process, the Gaussian linear vector auto-regressive model obviously is not applicable. In many climatology applications the distribu-tion of the data exhibits heavy tails [ 6, 2]. For e.g. climate change is mostly characterized by increasing probabilities of extreme weather patterns such as temperature or precipita-tion reaching extremely high values [26]. In search of more general and flexible time series models, we construct sev-eral auto-regressive processes and show that the maximum likelihood estimate of their evolution matrices can be decom-posed into a sparse and a low-rank matrix with the latter capturing the impact of unobserved processes. For counting processes, we analyze the Poisson [30 ]andConway-Maxwell Poisson [32 ] auto-regressive processes. The latter distribu-tion has recently attracted researchers X  attention because of its flexibility in modeling t he under-dispersion and over-dispersion of discrete data [ 25, 23]. For extreme value time series, we propose a novel heavy-tailed auto-regressive time series model, by choosing the distribution of the data to be the Gumbel distribution.

For fast solutions, we develop a scalable greedy sparse plus low-rank decomposition algorithm for maximizing the likelihood functions of GLARP models based on the work in [27 , 24]. Providing an upper bound on the convergence rate, we show that the greedy algorithms can be used for composite atoms , i.e., vectors that are obtained by concate-nating sparse plus low-rank atoms. We also show why the single atom selection per iteration yields slower rate of con-vergence. To our best knowledge, the composite atoms have not been studied prior to this work. Extensive experiments on two synthetic datasets, one climatology dataset and one social network dataset are shown to demonstrate the supe-rior performance of the proposed algorithms.
In this paper, we denote a single random variable with lower-case letters (for e.g. x ) and a vector of random vari-ables by bold letters (for e.g. x ). We can represent a set of N time series of length T by its elements x i ( t ) which rep-resents the value of the i th time series at time t .Usingthe notation, x ( t ) denotes the value of all time series at time t .
The Generalized Linear Model [ 18] describes the connec-tion between the response variables y and the predictor vari-ables x via the following linear dependence model: where the strictly monotone function g ( . ) is called the link function and A and b are linear regression coefficients. Based on generalized linear models, we can define the stochastic process model for time series x ( t )for t =1 ,...,T accord-ing to the following Generalized Linear Auto-regressive Pro-cesses (GLARP) model: where the matrices A ( ) for =1 ,...,K , K denoting the maximum lag in time, are called the Evolution Matrices and E H ( t ) emphasizes the point that the expectation is per-formed given the history before time t . The generative pro-cess corresponding to the model above can be described as follows: at time t , compute the conditional mean of x ( t ) using the outcomes at time t  X  K,...,t  X  1, i.e. x ( t  X  K ) ,..., x ( t  X  1); then generate x ( t ) according to the com-puted mean. Examples of the generalized linear auto-regressive models are vector auto-regressive models that are widely used for jointly modeling multiple continuous time series and Poisson auto-regressive processes for modeling multiple time series of count data.

We build the temporal dependency graph G ( V,E ) corre-sponding to the evolution matrices A ( ) for =1 ,...,K by representing every time series x i by a node v i  X  V .Weadd a directed edge e i  X  j to the set E if at least one of the entries A j,i for =1 ,...,K is non-zero. Figure 1: Decomposition of the evolution matrix in Eq. ( 5) into low-rank and sparse matrices.

In order to achieve a consistent estimate of a high dimen-sional matrix from a limited number of observations, we are required to impose a low-dimensional structure on the es-timated matrix. One of the most popular structures is the sparse plus low-rank structure which assumes that the true value of the matrix is approximately equal to a low-rank part plus a sparse part (Fig. 1). Examples of the applications that exhibit this low-dimensional structure are Robust PCA [5, 3, 20], Robust covariance estimation [ 1] and Multi-task regression [17 , 21].

In many real world applications, observing all influential quantities can be expensive or even not possible. The hidden time series can be the quantities that are hard to measure or have corrupted measurements; they can also represent im-measurable events such as disease outbreak news and its im-pact on social networks. Thus, taking into consideration the possible existence of a few hidden variables in the analysis makes the analysis significantly more accurate and realistic. The most common approach to capture the effect of hidden variables is based on the EM algorithm [9 ]. While the EM framework is quite general, it suffers from getting trapped into the local optima. In this work we are interested in find-ing a convex programming solution which does not depend on the initialization point.

In many real world datasets there are unobserved vari-ables that impact large groups of observed variables; this phenomenon is called the global influence .Examplesofthis phenomenon include the global impact of airwaves in clima-tology and the network-wide impact of external news on so-cial networks. At the same time, the observed variables have local sparse connectivity with each other. Examples of the local dependency are the users in social networks who share their friends X  posts or influence of a region on to another one due to their spatial proximity. [ 4] shows that in undirected graphs with unobserved variables with global impact, the precision matrix of the joint distribution of observed vari-ables have the sparse plus low-rank structure. [ 13]shows that in the vector auto-regressive model with unobserved variables with global impact, the evolution matrix estimated via maximizing the likelihood of the observed data only, will result in the sparse plus low-rank structure as well.
In this section, after describing the generalized linear auto-regressive processes with latent factors, we introduce and analyze two GLARP models for modeling count data and another one for modeling extreme value time series. Theo-rem 3.1 shows that in these models, the maximum likelihood estimate of the evolution matrix can be decomposed into a sparse and a low-rank matrix with the latter capturing the impact of unobserved processes. Then, in Section 3.2 we propose an algorithm to uncover the true evolution matrix and guarantee its convergence to the global optimum of the objective function (Theorem 3.2).

Consider the following model for generalized linear autore-gressive processes with hidden factors: for t = K +1 ,...,T ,where x ( t ), a p  X  1 vector, represents the observed variables, z ( t ), a r  X  1 vector, denotes the un-observed variables and the function g is the link function. The density function of the observations at time t is denoted the distribution that are functions of the evolution matrices A ( ) ,B ( ) ,C ( ) and D ( ) and the past values of time series x ( t )and z ( t ).

The maximum likelihood estimation of the model parame-ters in absence of the time series z ( t ) is performed as follows: where { A ( ) } represents the set of evolution matrices A for =1 ,...,K .
In this section, we define three time series models for two applications: (i) count data obtained from binning of point processes in social networks and (ii) heavy-tailed continuous data in the climate applications. In all of these models the hidden variables create the sparse plus low-rank structure in the evolution matrix.
Recently, point processes have been successfully applied to social networks analysis [ 31, 16, 15]. A popular approach in analysis of temporal dependency among multiple point pro-cesses is to count the number of events in regularly spaced intervals and analyze the resulting count time series [ 30, 15].
The Poisson distribution is one of the most commonly used distributions for modeling count data. According to the Poisson autoregressive point process model [ 30], the dis-tribution of variables at time t is a Poisson distribution with arate conditioned on the history modeled as follows: where  X  ( t ) represents the rate parameter for the Poisson distribution. The negative log-likelihood function for this model is convex and can be efficiently minimized.
An important limitation of the Poisson regression is that the variance of a Poisson distributed variable is equal to its mean, i.e., the Poisson model does not allow over-dispersion and under-dispersion which describe variances above and below the mean, respectively. The Conwey-Maxwell Pois-son distribution (in short COM-Poisson) is a two-parameter extension of the Poisson distribution with a parameter for modeling the dispersion. Historically, it was introduced in [8] and recently studied comprehensively in [ 25]. The COM-Poisson distribution is defined based on the following prop-erty: where  X  is called the dispersion parameter, and  X &lt; 1mod-eling over dispersion and  X &gt; 1 modeling underdispersion. The main advantage of the COM-Poisson distribution over other generalizations of the Poisson distribution, such as Double Poisson [ 10] and Generalized Poisson [ 7] distribu-tions, is its flexibility in modeling a greater range of disper-sion [32 ]. The COM-Poisson distribution is equivalent to the Poisson distribution when  X  = 1, the Geometric distribution when  X  = 0 and the Bernoulli distribution as  X   X  X  X  .The COM-Poisson GLARP is defined as follows [ 32]: log  X  ( t )+ 1 where  X  ( t ) is the rate parameter and S (  X  i ( t ) , X  ) is the nor-malization term. Given a constant (invariant with time) value for the dispersion parameter  X  , the negative log-likelihood function is convex and can be minimized efficiently.
In many applications, such as climate analysis, time se-ries data usually exhibit a heavy-tailed distribution which is significantly different from the commonly assumed Gaus-sian distribution. The generalized extreme value theorem states that the maximum of a set of independently and identically distributed random variables asymptotically con-verges to the Extreme Value Distribution, [ 6, 2]. Hence, the Generalized Extreme Value distribution and its special case, the Gumbel distribution, are the distributions of choice for modeling the extreme value data. In this paper, we define a Gumbel GLARP model as follows:  X  ( t )+  X  X  E = E H ( t ) [ x ( t )] = where  X  ( t )and  X  denote the location and scale parameters of the Gumbel distribution and  X  E  X  0 . 5771 is the Euler constant. Given a constant scale parameter  X  , the negative log-likelihood function is convex and can be minimized effi-ciently. Note that there are other ways to define a Gumbel autoregressive process, [ 29], however the above novel model is defined to have the sparse and low rank decomposition property for hidden variables.

For all of the GLARP models described above, we have the following theorem:
Theorem 3.1. Suppose a generalized linear auto-regressive and Eq. ( 7). Suppose the number of unobserved processes r and number of lags K are much smaller than the number of observed ones, i.e. r, K p . Then, asymptotically as T  X  X  X  , the maximum likelihood estimate of { A ( ) } is sum of two matrices: where L ( ) a low-rank matrix with rank ( L ( ) )  X  r.K .
The solution relies on two main ideas: 1) Asymptotically, the maximum likelihood estimation pro-cedure is equivalent to minimization of the KL-distance be-tween the true model and the observed model. We can write: where L True and L Obs denote the log-likelihood of the true andobservedmodels,respectively. 2) For point processes, suppose we divide the time into small intervals such that the probability of observing more than one event in each interval is small. We can approxi-mate the likelihood of the observed time series for any point process in a unified form given its rate function, as shown in [30 ]. This allows the computation of A MLE for all point processes in a unified way.

The details of the proof is provided in the Appendix.
Using the result of Theorem 3.1 we need to solve the fol-lowing optimization algorithm to capture the effect of unob-served variables: Subject to: where the L 0 norm of the matrices is equal to the number of non-zeros elements of the matrices and L denotes the likelihood of the stochastic process defined in Eq. ( 3). There are two main approaches to solve the problem in Eq. ( 10). The first approach uses a convex relaxation of the L 0 norm with the L 1 norm and the rank constraint with the nuclear norm L  X  :
The optimization problem in Eq. ( 11)isconvexandcan be solved via Singular Value Thresholding (SVT) in each it-eration of the Accelerated Proximal Gradient algorithm [ 19] as described in [ 28]. The second approach is to combine the greedy sparse and greedy low rank [ 12, 24] matrix learning algorithms in the unified framework provided by [ 27]. The greedy approach does not rely on the L  X  and L 1 heuristics and directly solves Eq. ( 10); i.e. it iteratively constructs the optimal sparse and low rank matrices along the sparse and low-rank directions. The greedy low-rank learning has been shown to be faster and more scalable than the SVT ap-proach [12 , 24]; hence, we develop Algorithm 1 in the greedy framework.

In Algorithm 1, for notation simplicity, we show the pa-rameters in the sparse and low rank matrices by w  X  R (2 K +1) p
Algorithm 1: Greedy Sparse plus Low-Rank Decompo-sition 1 Let w denote concatenation of L ( ) , A ( ) and b .
Initialize w 1  X  0 . 2for  X   X  1 , 2 , 3 ,... do 7end 8return L ( ) ,A ( ) ,for =1 ,...,K . where the its first Kp 2 elements w ( S ) contain the elements of A ( ) , the second Kp 2 elements w ( L ) contain the elements of L ( ) for =1 ,...,K and the last p elements contain b . The algorithm iteratively selects the atoms from two sets of atoms: (1) 2 Kp 2 sparse atoms which are created by placing  X  1inplaceoffirst Kp 2 elements of a .Thistakes O ( p 2 ) operations. (2) The low-rank atom in the atom identifi-cation step can be found via singular value decomposition, as described in [24 , 27]. In fact, we only need to find an approximate leading singular vector which can be done in O ( N s log( p )) where N s is the number of non-zero elements of the gradient matrix [24 ]. We update b after addition of each composite atom.

Following the framework in [ 27], we can derive the follow-ing convergence guarantee for Algorithm 1:
Theorem 3.2. The solution of Algorithm 1 at n th iter-ation is bounded towards the optimal solution w according to the following equation: where the bound constant for the sparsity atom is defined as B ness constant of the likelihood function as defined in [ 27] and ||A S || 2 =sup a  X  X  S a ( S ) where A S denotes the set of sparse atoms. The bound term for the low-rank atoms B L and B b are defined similarly.

A formal proof is given in the Section 5. Note that the solution always stays inside the constraints, thus the op-timization algorithm does not have to deal with the non-differentiability of the Lagran gian in the constraint bound-aries. Further analysis in the Appendix shows that similar performance bound for the algorithm that selects only one atom per iteration is larger than the bound in Eq. ( 12)at least by the ratio of the Lipschitz constant and the restricted smoothness constant of the likelihood function. As discussed in [27 ], the difference can be very large; hence, the speed up due to composite atom selection can be large, as well.
In this section, we study two types of data (1) point pro-cess, including a synthetic dataset and a social networks dataset and (2) heavy-tailed data including a climate sci-ence dataset.
We created a synthetic dataset according to the Poisson autoregressive point process model in Eq. ( 3) to study the accuracy of the algorithms in recovering the true underlying temporal dependency graph in the presence of hidden vari-ables. We fix the number of observed variables at 60 and vary the number of hidden variables from r =1to5. We also varied the length of observed time series to study the asymptotic behavior of the algorithms. For generation of time series, only one unit of time lag, K =1,isused. The elements of the A matrix in Eq. ( 5) for the point processes are generated at random, and we choose a sufficiently large negative value for b to stabilize the time series. The global impact of the hidden variables is modeled in the datasets by setting an edge from the hidden variables to all other ob-served variables. We generate 10 random datasets of each type and report the average performance on them. Due to space limit, we only report the results on the Poisson point process synthetic datasets.

We used a complete Twitter dataset to analyze the tweets about  X  X aiti earthquake X  by applying different temporal de-pendency analysis methods to identify the potential top in-fluencer on this topic (i.e. those Twitter accounts with the highest number of effect to the others). We divided the 17 days after the Haiti Earthquake on Jan. 12, 2010 into 1000 intervals and generated a multivariate time series dataset by counting the number of tweets on this topic for the top 1000 users who tweeted most about it. The resulting time series have on average 0 . 0225 tweets per user per bin which shows how infrequent the events in the dataset are. For accurate modeling, we removed the users that were highly correlated with each other, most of which were operated by the same users and tweeted exactly the same contents. We also re-moved robot-like user-accounts who tweeted on very regular intervals, which led to a subset of 100 users.

The study of extreme value of wind speed and gust speed is of great interest to the climate scientists and wind power engineers. A collection of wind observations is provided by AWS Convergence Technologies, Inc. of Germantown, MD. It consists of the observations of surface wind speed (mph) and gust speed (mph) every five minutes. We choose 153 weather stations located on a grid laying in the 35 N  X  50 N and 70 W  X  90 W block. Following the standard practice in this domain, we generated extreme value time series ob-servations, i.e, daily maximum values, at different weather stations. The objective is to examine how the global weather systems impact the local influence patterns at different lo-cations and how well we can make predictions on future precipitation.
For the synthetic datasets, since we have access to the un-derlying graph structure we can report the graph learning accuracy. We choose the Area Under the Curve (AUC) ac-curacy measure as it is a good performance measure for the datasets with unbalanced ratio of positive and negative la-bels. The value of AUC is the probability that the algorithm assigns a higher value to a randomly chosen positive (exist-ing) edge than a randomly chosen negative (non-existing) Algorithm Description
GLARP-PoG GLARP with Poisson distribution
Poisson-EM GLARP with Poisson distribution
Poisson GLARP with Poisson distribution
GLARP-COMG GLARP with COM-Poisson distri-
COM-P EM GLARP with COM-Poisson distri-
COM-P GLARP with COM-Poisson distri-
Transfer Entropy Transfer Entropy, a non-parametric Algorithm Description
GLARP-GumG GLARP with Gumbel distribution
Gumbel-EM GLARP with Gumbel distribution
Gumbel GLARP with Gumbel distribution
Gaussian VAR Gaussian VAR with hidden vari-
Transfer Entropy Continuous Transfer Entropy [ 14] edge in the graph. Since we don X  X  have the true underlying influence graph in the wind speed dataset, we only report the prediction accuracy and the visualization of the results. In all the experiments, we tune the penalization parameters via 5 fold cross-validation.

Since we do not have access to the true underlying in-fluence graph in the social networking, we use the retweet network as the ground truth. The retweet network G RT ( n ) is constructed by adding an edge from user i to user j if user j has retweeted at least n of the tweets of user i ,where n is varied from 1 to 5. Clearly, the retweet network is not the actual underlying temporal dependency graph, mainly be-cause there are possible implicit influence patterns as well. However, it is the best possible metric that we could obtain for graph learning accuracy evaluation in our dataset. The retweet network for the 100 selected users is sparse. For e.g., G RT (1) has only 279 out of 10,000 possible edges.
For predictive analysis, in all the datasets, we split them into the training/testing parts with ratio 9/1 based on time and report the root mean square error (RMSE) and nor-malized RMSE on the test set. In particular, we trained the models with the observations between t =1 ,..., 9 10 T and predicted the observations at t = 9 10 T,...,T using K past observations at t  X  K, t  X  K +1 ,...,t  X  1. In other words, we evaluate the 1-step prediction performance of the algorithms. We reported the average RMS error on the test samples. The predictive analysis is plausible in our Twitter dataset because it is the full dump of the twitter messages, not a sub-sampled version of it.

To compare the performance of sparse plus low-rank de-composition, we use several state-of-art baselines (see Table 1 for details). Specifically, the EM algorithm solutions use Figure 2: Synthetic dataset results on the point process dataset (a) Graph learning accuracy as the length of the time series increases. (b) Graph learn-ing accuracy as the number of hidden variables in-creases. the EM algorithm to learn the parameters of the GLARP model in Eq. ( 3). The parameters in the EM algorithm are initialized to zeros. Transfer Entropy [ 22, 14] algorithms perform pairwise temporal dependency analysis among time series by measuring the amount of uncertainty resolved in the future of a time series by knowing the past values an-other time series, given its own past values.
The results on the synthetic datasets are shown in Fig. 2. In first set of experiments, we have only one hidden variable and vary the length of time series to measure the graph learning accuracy of the algorithms. As we expect, the performance of the algorithms uniformly increases with the length of the time series. The algorithms which cap-ture the impact of hidden variables outperform the other algorithms by a large margin. Among the hidden variable detection algorithms, the superior performance of our pro-posed algorithms is because they are convex programming; while the EM-based algorithms can be stuck in some subop-timal local optima. The performance of Transfer Entropy is only comparable to the Poisson process, in Fig. 2,andwith large number of samples its performance approaches to the point process.

In the second set of experiments, we fix the time series length at 500 and vary the number of hidden variables. The performance of our algorithms slightly drop, mainly because as we increase the number of hidden variables, the rank of the low-rank matrix L increases and it becomes harder to estimate [ 24]. With five hidden variables in Fig. 2(b) , it reaches to the performance of the EM algorithm which does not rely on the r p assumption.The performance of Transfer Entropy and Poisson degrade too, since the true un-derlying model deviates more from their assumption about existence of no hidden variables.

As shown in Fig. 3, the performance of all the algorithms increase as we increase the number of retweets requirement n for the ground truth influence graph G RT ( n ) (defined in Sec-tion 4.2). This means all the algorithms detect the strong influence edges with higher accuracy. In all of the COM-Poisson auto-regressive models, we have set the dispersion parameter  X  to a fixed large number to model the large un-derdispersion in the twitter time series. Capturing under-dispersion in the data, all the COM-Poisson based models outperform their Poisson counterparts. As we expected the GLARP-COMG algorithm outperforms the EM counterpart Figure 3: The graph learning accuracy when the number of retweets requirement n for the ground truth influence graph G RT ( n ) is varied. The perfor-mance of (a) Poisson and (b) COM-Poisson autore-gressive processes confirms that they make better predictions for the stronger influence edges.
 Table 2: The RMS prediction error of the algorithms in the Twitter dataset. Results have been normal-ized by the the mean.
 by avoiding the local minima. The prediction performance in Table 2 confirms this trend as well. The inferior perfor-mance of the EM algorithm is due to propagation of error; in other words, EM first infers the values of past hidden vari-ables (accruing some error) and then uses them to predict observed time series. The lower prediction performance of COM-Poisson based algorithms is due to the approximation error in estimation of the mean E H ( t ) [ x ( t )] in Eq. (6 ).
The transfer entropy results are (0 . 5427, 0 . 5915, 0 . 5924, 0 . 5785, 0 . 5442) for n =1 ,..., 5. In order to keep the res-olution of the graph high, they are not shown in the graph because they were far below the rest of the algorithms. The poor performance of Transfer Entropy can be attributed to the extreme sparsity of the Twitter time series and the fact that, unlike the rest of the parametric algorithms, it does not have any procedure to benefit from sparsity of the un-derlying data generation model. To evaluate the prediction performance of Transfer Entropy, we used the graph esti-mated by Transfer Entropy in the Poisson auto-regressive process and measured its prediction performance.

In order to evaluate the speedup of using sparse plus low rank decomposition over the EM solution, we recorded the run time on the Twitter dataset on an i7 2.67 GHz laptop running Windows. The Poisson and GLARP-PoG spent 48 and 98 seconds while each iteration of the EM algorithm took 928 seconds. Given 5 iterations of the EM algorithm, the speedup by sparse plus low-rank decomposition is near 47 fold.

We next examine whether any meaningful hidden homophily can be detected by GLARP-COMG. Using the result in Eq. (28) and because we have identified only one hidden pro-cess, we can see that the summation of the B ( ) matrices for =1 ,...,K should be proportional to the value in the left hand side. In other words, we can find the aver-age impact of the hidden processes on the observed ones by the K =1 L ( )  X  x .Figure 4 shows the results of the Figure 4: The hidden structure identified by GLARP-COMG approach from the Haiti Dataset.
 In the Haiti dataset, a single hidden variable is iden-tified by our method. The matrix represents B in Eq. ( 3) which corresponds to the effects of the hid-den variable on the input users; the darker the color, the larger the influence of the hidden variable on the user. Figure 5: (a) The spatial-temporal dependency graph obtained via the Gum bel auto-regressive pro-cess. Note the denseness of the graph. (b) The sparse part of the spatial-temporal dependency graph obtained via GLARP-GumG. Removing the low rank global effect leaves only two main local ter-rain impacts: one is the local impact of the Ap-palachian mountains along the east coast and the other one is the local impact of the Great Lakes on the weather pattern of their surrounding lands.
 GLARP-COMG method on the Haiti dataset. An immedi-ate observation is that the hidden variables mostly impact the users on the left side of the matrix, which corresponds to those Twitter accounts with more tweets. This is reason-able since the users who are more concerned about the topic will get key information more from external news sources, such as TV, radio or personal communications, which act as hidden external variables in the model. When we zoom into the group of users affected by the hidden variable, we can see many of them are organizations or persons with possible close connections to the authority of Haiti, such as missionmanna (Mission Manna provides medical care for malnourished children and continuing health care education for adults in and around Montrouis, Haiti), haitiinfocus (HCN provides a safe facility in Thomassin Haiti where Haitian students can go to school online) and pierrecote (Realtime transmedia strategist, producer, director, writer and advisor to the Prime Minister of Haiti).
 The prediction performance of the algorithms is listed in Table 3. The results show that the GLARP-GumG out per-forms the rest of the algorithms. Two patterns are different Table 3: The RMS prediction error of the algorithms in the wind speed dataset.
 in this dataset: first the EM algorithm has lower perfor-mance than the simple Gumbel VAR algorithm. The sec-ond observation is that due to short length of time series, the Transfer Entropy faces the high dimensionality problem and cannot perform better than the Gaussian model. To evalu-ate the prediction performance of Transfer Entropy, we used the graph estimated by Transfer Entropy in the Gaussian auto-regressive process and measured its prediction perfor-mance.

The GLARP-GumG algorithm detects only one hidden variable in the wind speed dataset. The impact of the de-tected hidden variable can be seen in Fig. 5(a) and 5(b) which show the spatial-temporal dependency graph obtained via the Gumbel auto-regressive process and the sparse part of the spatial-temporal dependency graph obtained via GLARP GumP, respectively. Comparing the two graphs, we observe that GLARP-GumG removes the main global weather im-pact in this season which can be attributed to the summer weather system in the region. Two main local influence pat-terns are detected by our algorithm: (i) the impact of the Appalachian mountains in the stretch of east coast and (ii) the local impact of the Great Lakes on the weather pattern of their surrounding lands.
In this paper, we studied three instances of the general-ized linear autoregressive processes (GLARPs), in which the impact of hidden variables in time series data appears as an additive low-rank matrix in the maximum likelihood estima-tion of the evolution matrices. We demonstrated that the convex programming solution solution indeed yields better prediction and graph learning accuracy than the alterna-tive EM-based algorithms, and our model is fast enough for large-scale applications. For future work, we are interested in generalization of the framework and establishing the sta-tistical guarantees.
 [1] A. Agarwal, S. Negahban, and M. J. Wainwright. [2] J. Beirlant, Y. Goegebeur, J. Segers, and J. Teugels. [3] E.J.Cand` es, X. Li, Y. Ma, and J. Wright. Robust [4] V. Chandrasekaran, P A. Parrilo, and A S. Willsky. [5] V. Chandrasekaran, S. Sanghavi, P A. Parrilo, and [6] S. Coles. An introduction to statistical modeling of [7] P. C. Consul and G. C. Jain. A generalization of the [8] R.W. Conway and W.L. Maxwell. A queuing model [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. [10] B. Efron. Double exponential families and their use in [11] G R. Grimmett and D R. Stirzaker. Probability and [12] M. Jaggi and M. Sulovsky. A simple algorithm for [13] A. Jalali and S. Sanghavi. Learning the Dependence [14] A. Kaiser. Information transfer in continuous [15] G. Kim, F-F. Li, and E. P. Xing. Web Image [16] A. Myers, S, C. Zhu, and J. Leskovec. Information [17] S. Negahban and M. J. Wainwright. Estimation of [18] J. A. Nelder and R. W. M. Wedderburn. Generalized [19] Y. Nesterov. Gradient methods for minimizing [20] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed [21] A. Rohde and A. B. Tsybakov. Estimation of [22] T. Schreiber. Measuring Information Transfer. [23] K F. Sellers and G. Shmueli. A flexible regression [24] S. Shalev-Shwartz, A. Gonen, and O. Shamir.
 [25] G. Shmueli, T P. Minka, J B. Kadane, S. Borle, and [26] S. Solomon, D. Qin, M. Manning, Z. Chen, [27] A. Tewari, P D. Ravikumar, and I S. Dhillon. Greedy [28] KC. Toh and S. Yun. An accelerated proximal [29] G. Toulemonde, A. Guillou, P. Naveau, M. Vrac, and [30] W. Truccolo, U. T. Eden, M. R. Fellows, J. P. [31] D. Q. Vu, A. U. Asuncion, D. R. Hunter, and [32] F. Zhu. Modeling time series of counts with Without loss of generality, we prove the case where K =1 and b = 0 . The proof for the general case is straightforward, but algebraically more involved, extension of the simpler case.
 Consider the following true model for the time series: denote the aggregation of the both observed and unobserved variables. In the maximum likelihood solution with unob-served time series z ( t ), we fit the data to the following ob-served model:
First we show how we can derive the likelihood for any point process given its rate function, [ 30]. Suppose we have divided the time into small enough so that the probability of x i ( t )=1for i =1 ,...,p becomes small in each interval [11] and we have:
The probability of observing x ( t )inthe t th interval can be written as following:
Now we can approximate the negative log-likelihood func-tion as follows using the fact that when  X  i ( t ) is small, we can write log(1  X   X  i ( t ))  X  X  X   X  i ( t )andlog(  X  i ( t )[1  X  log(  X  i ( t )) [ 30].

Substituting the value of  X  i ( t ) from the observed model in Eq. ( 14)intoEq. ( 19), we can see that A MLE is the solution of the following problem: A MLE =argmax where we have written the equations in the compact vector format. Differentiation of L with respect to A and setting it to zero yields the following results: where A and B are the true values in Eq. ( 13). Since u i { 0 , 1 } with high probability, by taking the expectation with respect to each individual u i we can see that Eq. ( 24)is satisfied if and only if the following equality holds: E u ( t  X  1) exp([ AB ] u ( t  X  1))  X  exp( A x ( t  X  1)) = 0 . (25)
Suppose A, B ,and A are bounded. Since u i  X  X  0 , 1 } ,the values inside the exponential functions are bounded, and the exponential function is one to one. Thus, Eq. ( 25) is equivalent to the following equation: (which can also be obtained by Taylor expansion.) where Eq. ( 28) is the result of linearity of expectation op-erator. Since Eq. ( 28) holds for all values of  X  and  X  ,the column space of A  X  A is equal to the column space of B . Thus, rank of L = A  X  A canbeatmosttherankofcolumn space of B ; i.e. rank ( L )  X  r . This concludes the proof. The proof also holds for Bernoulli and COM-Poisson processes, due to the fact that Eq. ( 19) holds for them too [ 30]. Consider the following true model for the Gumbel time se-ries:
In the maximum likelihood solution with unobserved time series z ( t ), we fit the data to the following observed model:
Similar to the previous theorem, our goal is to find the expression for A MLE as in Eq. ( 20). The key to approxi-mation of A MLE is to assume that E [ x ( t )] = 0 and A x ( t )is small; both of these assumptions can be satisfied in the data by pre-processing. Proceeding with the proof, we have:
Using the fact that E [ x ( t )] = 0 and differentiation with respect to A yields: E True x ( t  X  1) x ( t )  X  A x ( t  X  1)  X  0 , (34) E u ( t  X  1) x ( t  X  1) A x ( t  X  1) + B z ( t  X  1)  X  A x ( t A
The step from ( 32)to( 33) is due to the Taylor expansion of the exponential function around zero; the step from ( 33) to (34)isdoneusingthefactthat E [ x ( t )] = 0 ;thestep from ( 34)to( 35) is done by expectation with respect to conditional distribution of x ( t )given u ( t ) under the true model; and the final step is done via the definition of the co-variance matrices.
 Due to space limit, we provide our proof as a continuation of the proof in [ 27]. Given a set S and a norm , we define the Restricted Smoothness Property constant of the likelihood function L as defined in Eq. (3) in [ 27]. Following the same steps, we have:
Similarly, we can define and show that:
Plugging Eq. ( 39)intoEq.( 37) and following the reason-ing in [27 ], we can show that:  X  For t =0,choose  X ,  X  = 1 on the right hand side to get  X   X  2( L S  X  S R 2 S + L L  X  L R 2 L ). Since  X  t is decreasing, we can ing  X  =4( L S  X  S R 2 S + L L  X  L R 2 L ) yields for all t&gt; 1:  X   X  Solving this yields the desired result. The impact of opti-mization of b can be captured similarly.

In the performance analysis of the greedy algorithm that selects only one sparse or low-rank atom per iteration we should observe that in Eq. ( 38)either  X  X  ( w t ) , w , ( L )  X  X  ( w t ) , w , ( S ) remains unbounded. Bounding this term introduces the Lipschitz constant of the likelihood function. Plugging the additional term into Eq. ( 37) yields  X  L S  X  or  X  L L  X  L R 2 L instead of  X  2 L S  X  S R 2 S or  X  2 L S L
S denotes the maximum Lipschitz constant of the likeli-hood function inside the convex hull of the sparsity norm. Since  X &lt; 1andalways L S &lt;L S and L L &lt;L L ,weobserve that the bound for the single atom selection should be at least greater by the differences of the Lipschitz and the re-stricted smoothness constant of the likelihood function for one of the atoms.
