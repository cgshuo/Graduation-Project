 Yu-Xiang Wang yuxiangwang@nus.edu.sg Huan Xu mpexuh@nus.edu.sg Collaborative prediction of user preferences has at-tracted fast growing attention in the machine learning community, best demonstrated by the million-dollar Netflix Challenge. Among various models proposed, matrix factorization is arguably the most widely ap-plied method, due to its high accuracy, scalability (Su &amp; Khoshgoftaar, 2009) and flexibility to incorporat-ing domain knowledge (Koren et al., 2009). Hence, not surprisingly, matrix factorization is the centerpiece of most state-of-the-art collaborative filtering systems, including the winner of Netflix Prize (Bell &amp; Koren, 2007). Indeed, matrix factorization has been widely applied to tasks other than collaborative filtering, in-cluding structure from motion, localization in wireless sensor network, DNA microarray estimation and be-yond. Matrix factorization is also considered as a fun-damental building block of many popular algorithms in regression, factor analysis, dimension reduction, and clustering (Singh &amp; Gordon, 2008).
 Despite the popularity of factorization methods, not much has been done on the theoretical front. In this paper, we fill the blank by analyzing the stability vis a vis adversarial noise of the matrix factorization meth-ods, in hope of providing useful insights and guide-lines for practitioners to design and diagnose their al-gorithm efficiently.
 Our main contributions are three-fold: In Section 3 we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error. In Section 4, we treat the ma-trix factorization as a subspace fitting problem and an-alyze the difference between the solution subspace and the ground truth. This facilitates an analysis of the prediction error of individual users, which we present in Section 5. To validate these results, we apply them to the problem of collaborative filtering under manip-ulator attack in Section 6. Interestingly, we find that matrix factorization are robust to the so-called  X  X ar-geted attack X , but not so to the so-called  X  X ass at-tack X  unless the number of manipulators are small. These results agree with the simulation observations. We briefly discuss relevant literatures. Azar et al. (2001) analyzed asymptotic performance of matrix fac-torization methods, yet under stringent assumptions on the fraction of observation and on the singular val-ues. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users  X  a situation that rarely happens in practice. Srebro (2004) consid-ered the problem of the generalization error of learning a low-rank matrix. Their technique is similar to the proof of our first result, yet applied to a different con-text. Specifically, they are mainly interested in binary prediction (i.e.,  X  X ike/dislike X ) rather than recovering the real-valued ground-truth matrix (and its column subspace). In addition, they did not investigate the stability of the algorithm under noise and manipula-tors.
 Recently, some alternative algorithms, notably Sta-bleMC (Candes &amp; Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Can-des &amp; Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods. Indeed, our theoret-ical results reassure these empirical observations, see Section 3 for a detailed comparison of the stability re-sults of different algorithms. 2.1. Matrix Factorization with Missing Data Let the user ratings of items (such as movies) form a matrix Y , where each column corresponds to a user and each row corresponds to an item. Thus, the ij th entry is the rating of item-i from user-j . The valid range of the rating is [  X  k, + k ]. Y is assumed to be a rank-r matrix 1 , so there exists a factorization of this rating matrix Y = UV T , where Y  X  R m  X  n , U  X  R m  X  r , V  X  R n  X  r . Without loss of generality, we assume m  X  n throughout the paper.
 Collaborative filtering is about to recover the rating matrix from a fraction of entries possibly corrupted by noise or error. That is, we observe b Y ij for ( ij )  X   X  the sampling set (assumed to be uniformly random), and b Y = Y + E being a corrupted copy of Y , and we want to recover Y . This naturally leads to the optimization program below: where P  X  is the sampling operator defined to be: We denote the optimal solution Y  X  = U  X  V  X  T and the error  X  = Y  X   X  Y. 2.2. Matrix Factorization as Subspace Fitting As pointed out in Chen (2008), an alternative interpre-tation of collaborative filtering is fitting the optimal r -dimensional subspace N to the sampled data. That is, one can reformulate (1) into an equivalent form 2 : min where y i is the observed entries in the i th column of Y , N is an m  X  r matrix representing an orthonormal basis 3 of N , N i is the restriction of N to the observed entries in column i , and P i = N i ( N T i N i )  X  1 N T projection onto span( N i ).
 After solving (3), we can estimate the full matrix in a column by column manner via (4). Here y  X  i denotes the full i th column of recovered rank-r matrix Y  X  . Due to error term E , the ground truth subspace N gnd can not be obtained. Instead, denote the optimal subspace of (1) (equivalently (3)) by N  X  , and we bound the gap between these two subspaces using Canonical angle. The canonical angle matrix  X  is an r  X  r diagonal matrix, with the i th diagonal entry  X  = arccos  X  i (( N gnd ) T N  X  ).
 The error of subspace recovery is measured by  X  = k sin  X  k 2 , justified by the following properties adapted from Chapter 2 of Stewart &amp; Sun (1990): 2.3. Algorithms We focus on the stability of the global optimal solution of Problem (1). As Problem (1) is not convex, finding the global optimum is non-trivial in general. While this is certainly an important question, it is beyond the scope of this paper. Instead, we briefly review some results on this aspect.
 The simplest algorithm for (1) is perhaps the alter-nating least square method (ALS) which alternatingly minimizes the objective function over U and V until convergence. More sophisticatedly, second-order algo-rithms such as Wiberg, Damped Newton and Leven-berg Marquadt are proposed with better convergence rate, as surveyed in Okatani &amp; Deguchi (2007). Spe-cific variations for CF are investigated in Tak  X acs et al. (2008) and Koren et al. (2009).
 From an empirical perspective, Mitra et al. (2010) re-ported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set. We show in this section that when sufficiently many entries are sampled, the global optimal solution of fac-torization methods is stable vis a vis noise  X  i.e., it recovers a matrix  X  X lose to X  the ground-truth. This is measured by the root mean square error (RMSE): Theorem 1. There exists an absolute constant C , such that with probability at least 1  X  2 exp(  X  n ) , RMSE  X  Notice that when |  X  | nr log( n ) the last term di-minishes, and the RMSE is essentially bounded by the  X  X verage X  magnitude of entries of E , i.e., the factor-ization methods are stable.
 Comparison with related work We recall similar RMSE bounds for StableMC of Can-des &amp; Plan (2010) and OptSpace of Keshavan et al. (2010a):  X  Albeit the fact that these bounds are for different al-gorithms and under different assumptions (see Table 1 sults with Theorem 1. We observe that Theorem 1 is tighter than (7) by a scale of p min ( m,n ), and tighter than (8) by a scale of p n/ log( n ) in case of adversarial noise. However, the latter result is stronger when the noise is stochastic, due to the spectral norm used. Compare with an Oracle We next compare the bound with an oracle, introduced in Candes &amp; Plan (2010), that is assumed to know the ground-truth column space N a priori and recover the matrix by projecting the observation to N in the least square sense column by column via (4). It is shown that RMSE of this oracle satsifies, Notice that Theorem 1 matches this oracle bound, and hence it is tight up to a constant factor. 3.1. Proof of Stability Theorem We briefly explain the proof idea first. By definition, the algorithm finds the optimal rank-r matrices, mea-sured in terms of the root mean square (RMS) on the sampled entries. To show this implies a small RMS on the entire matrix , we need to bound their gap  X  ( X ) , To bound  X  ( X ), we require the following theorem. Theorem 2. Let  X  L ( X ) = 1  X  L ( X ) = 1  X  function respectively. Furthermore, assume entry-wise constraint max i,j | X i,j |  X  k . Then for all rank-r ma-trices X , with probability greater than 1  X  2 exp(  X  n ) , there exists a fixed constant C such that Indeed, Theorem 2 easily implies Theorem 1.

Proof of Theorem 1. The proof makes use of the fact that Y  X  is the global optimal of (1).

RMSE = Here, (a) holds from definition of  X  ( X ), and (b) holds because Y  X  is optimal solution of (1). Since Y  X   X  S r applying Theorem 2 completes the proof.
 The proof of Theorem 2 is deferred to Appendix A due to space constraints. The main idea, briefly speaking, is to bound, for a fixed X  X  S r , using Hoeffding X  X  inequality for sampling without re-placement; then bound  X  L ( X )  X  X  ( X ) using and finally, bound sup X  X  S  X  net argument.
 In this section we investigate the stability of recovered subspace using matrix factorization methods. Recall that matrix factorization methods assume that, in the idealized noiseless case, the preference of each user be-longs to a low-rank subspace. Therefore, if this sub-space can be readily recovered, then we can predict preferences of a new user without re-run the matrix factorization algorithms. We analyze the latter, pre-diction error on individual users, in Section 5. To illustrate the difference between the stability of the recovered matrix and that of the recovered subspace, consider a concrete example in movie recommendation, where there are both honest users and malicious ma-nipulators in the system. Suppose we obtain an output subspace N  X  by (3) and the missing ratings are filled in by (4). If N  X  is very  X  X lose X  to ground truth subspace N , then all the predicted ratings for honest users will be good. On the other hand, the prediction error of the preference of the manipulators  X  who do not follow the low-rank assumption  X  can be large, which leads to a large error of the recovered matrix. Notice that we are only interested in predicting the preference of the honest users. Hence the subspace stability provides a more meaningful metric here. 4.1. Subspace Stability Theorem Let N , M and N  X  , M  X  be the r -dimensional column space-row space pair of matrix Y and Y  X  respec-tively. We X  X l denote the corresponding m  X  r and n  X  r orthonormal basis matrix of the vector spaces using N , M , N  X  , M  X  . Furthermore, Let  X  and  X  de-note the canonical angles  X  ( N  X  , N ) and  X  ( M  X  , M ) respectively.
 Theorem 3. When Y is perturbed by additive error E and observed only on  X  , then there exists a  X  sat-isfying k  X  k X  q mn |  X  | k P  X  ( E ) k F + k E k F + such that: k sin  X  k X  where k X k is either the Frobenious norm or the spectral norm, and  X  =  X   X  r , i.e., the r th largest singular value of the recovered matrix Y  X  .
 Furthermore, we can bound  X  by:  X   X   X  where  X  Y N = Y + P N  X  and  X  Y M = Y + ( P M  X  T ) T . Notice that in practice, as Y  X  is the output of the al-gorithm, its r th singular value  X  is readily obtainable. Intuitively, Theorem 3 shows that the subspace sen-sitivity vis a vis noise depends on the singular value distribution of original matrix Y . A well-conditioned rank-r matrix Y can tolerate larger noise, as its r th sin-gular value is of the similar scale to k Y k 2 , its largest singular value. 4.2. Proof of Subspace Stability Proof of Theorem 3. In the proof, we use k X k when a result holds for both Frobenious norm and for spectral norm. We prove the two parts separately.
 Part 1: Canonical Angles.
 Let  X  = Y  X   X  Y . By Theorem 1, we have k  X  k  X  q proof relates  X  with the deviation of spaces spanned by the top r singular vectors of Y and Y  X  respectively. Our main tools are Weyl X  X  Theorem and Wedin X  X  The-orem (Lemma F.1 and F.2 in Appendix F).
 We express singular value decomposition of Y and Y  X  in block matrix form as in (F.1) and (F.2) of Ap-pendix F, and set the dimension of  X  1 and  X   X  1 to be r  X  r . Recall, rank( Y ) = r , so  X  1 = diag(  X  1 ,..., X   X  2 = 0, b  X  1 = diag(  X  obtained Y 0 , the nearest rank-r matrix to Y  X  . Observe that N  X  =  X  L 1 , M  X  = (  X  R 1 ) T .
 To apply Wedin X  X  Theorem (Lemma F.2), we have the residual Z and S as follows: which leads to Substitute this into the Wedin X  X  inequality, we have p k sin  X  k 2 + k sin  X  k 2  X  where  X  satisfies (F.3) and (F.4). Specifically,  X  =  X   X  Observe that Equation (10) implies To reach the equations presented in the theorem, we can tighten the above bound by decomposing  X  into two orthogonal components.

Y  X  = Y +  X  = Y + P N  X  + P N  X   X  :=  X  Y N + P N  X   X  . It is easy to see that column space of Y and  X  Y N are identical. So the canonical angle  X  between Y  X  and Y are the same as that between Y  X  and  X  Y N . Therefore, we can replace  X  by P N  X   X  to obtain the equation presented in the theorem. The corresponding result for row subspace follows similarly, by decomposing  X  T to its projection on M and M  X  .
 Part 2: Bounding  X  .
 We now bound  X  , or equivalently  X   X  r . By Weyl X  X  theo-rem (Lemma F.1), we have Moreover, Applying Weyl X  X  theorem on Equation (11), we have Similarly, we have This establishes the theorem. In this section, we analyze how confident we can pre-dict the ratings of a new user y  X  X  gnd , based on the subspace recovered via matrix factorization methods. In particular, we bound the prediction k  X  y  X   X  y k , where  X  y  X  is the estimation from partial rating using (4), and y is the ground truth.
 Without loss of generality, if the sampling rate is p , we assume observations occur in first pm entries, such that y = 5.1. Prediction of y With Missing data Theorem 4. With all the notations and definitions above, and let N 1 denote the restriction of N on the observed entries of y . Then the prediction for y  X  where  X  = k sin  X  k (see Theorem 3),  X  min is the small-est non-zero singular value of N 1 ( r th when N 1 is non-degenerate).
 Proof. By (4), and recall that only the first pm entries are observed, we have Let y  X  be the vector obtained by projecting y onto sub-space N , and denote y  X  = y  X  e , we have: Then Finally, we bound e 1 as follows k e 1 k X k e k = k y  X  y  X  k X k ( P gnd  X  P N ) y k X   X  k y k , which completes the proof.
 Suppose y 6 X  N gnd and y = P gnd y + ( I  X  P gnd ) y := y k e 1 k X k ( P gnd  X  P N ) y k + k y gnd which leads to 5.2. Bound on  X  min To complete the above analysis, we now bound  X  min . Notice that in general  X  min can be arbitrarily close to zero, if N is  X  X piky X . Hence we impose the strong inco-herence property introduced in Candes &amp; Tao (2010) (see Appendix C for the definition) to avoid such sit-uation. Due to space constraint, we defer the proof of the following to the Appendix C.
 Proposition 1. If matrix Y satisfies strong incoher-ence property with parameter  X  , then: For Gaussian Random Matrix Stronger results on  X  min is possible for randomly gen-erated matrices. As an example, we consider the case that Y = UV where U , V are two Gaussian ran-dom matrices of size m  X  r and r  X  n , and show that  X  Proposition 2. Let G  X  R m  X  r have i.i.d. zero-mean Guassian random entries. Let N be its orthonormal basis 4 . Then there exists an absolute constant C such that with probability of at least 1  X  Cn  X  10 , Due to space limit, the proof of Proposition 2 is de-ferred to the Supplementary material. The main idea is to apply established results about the singular val-ues of Gaussian random matrix G (e.g., Rudelson &amp; Vershynin, 2009; Silverstein, 1985; Davidson &amp; Szarek, 2001), then show that the orthogonal basis N of G is very close to G itself.
 We remark that the bound on singular values we used has been generalized to random matrices following subgaussian (Rudelson &amp; Vershynin, 2009) and log-concave distributions (Litvak et al., 2005). As such, the the above result can be easily generalized to a much larger class of random matrices. In this section, we apply our results to study the  X  X rofile injection X  attacks on collaborative filtering. According to the empirical study of Mobasher et al. (2006), matrix factorization, as a model-based CF al-gorithm, is more robust to such attacks compared to similarity-based CF algorithms such as kNN. However, as Cheng &amp; Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommenda-tion system is robust. Rather, it may due to the fact that that common attack schemes, effective to similar-ity based-approach, do not exploit the vulnerability of the model-based approach.
 Our discovery is in tune with both Mobasher et al. (2006) and Cheng &amp; Hurley (2010). Specifically, we show that factorization methods are resilient to a class of common attack models, but are not so in general. 6.1. Attack models Depending on purpose, attackers may choose to in-ject  X  X ummy profiles X  in many ways. Models of differ-ent attack strategies are surveyed in Mobasher et al. (2007). For convenience, we propose to classify the models of attack into two distinctive categories: Tar-geted Attack and Mass Attack .
 Targeted Attacks include average attack (Lam &amp; Riedl, 2004), segment attack and bandwagon attack (Mobasher et al., 2007). The common characteristic of targeted attacks is that they pretend to be the honest users in all ratings except on a few targets of interest. Thus, each dummy user can be decomposed into: where e gnd  X  X  and s is sparse.
 Mass Attacks include random attack, love-hate at-tack (Mobasher et al., 2007) and others. The com-mon characteristic of mass attacks is that they insert dummy users such that many entries are manipulated. Hence, if we decompose a dummy user, where e gnd = P N e and e gnd  X  = ( I  X  P N ) e  X  X   X  , then both components can have large magnitude. This is a more general model of attack. 6.2. Robustness analysis By definition, injected user profiles are column-wise: each dummy user corresponds to a corrupted column in the data matrix. For notational convenience, we re-arrange the order of columns into [ Y | E ], where Y  X  R m  X  n is of all honest users, and E  X  R m  X  n e contains all dummy users. As we only care about the prediction of honest users X  ratings, we can, without loss of generality, set ground truth to be [ Y | E and the additive error to be [ 0 | E gnd  X  ]. Thus, the recovery error Z = [ Y  X   X  Y | E  X   X  E gnd ].
 Proposition 3. Assume all conditions of Theorem 1 hold. Under  X  X argeted Attacks X , there exists an abso-lute constant C , such that RMSE  X  4 k Here, s max is maximal number of targeted items of each dummy user.
 Proof. In the case of  X  X argeted Attacks X , we have (re-Substituting this into Theorem 1 establishes the proposition.
 Remark 1. Proposition 3 essentially shows that ma-trix factorization approach is robust to the targeted attack model due to the fact that s max is small. In-then RMSE converges to zero as m increases. This co-incides with empirical results on Netflix data (Bell &amp; Koren, 2007). In contrast, similarity-based algorithms (kNN) are extremely vulnerable to such attacks, due to the high similarity between dummy users and (some) honest users.
 It is easy to see that the factorization method is less ro-bust to mass attacks, simply because k E gnd  X  k F is not sparse, and hence s max can be as large as m . Thus, the right hand side of (12) may not diminish. Nev-ertheless, as we show below, if the number of  X  X ass Attackers X  does not exceed certain threshold, then the error will mainly concentrates on the E block. Hence, the prediction of the honest users is still acceptable. Proposition 4. Assume sufficiently random sub-space N (i.e., Propostion 2 holds), above definition of  X  X ass Attacks X , and condition number  X  . If n e &lt; 1 /m 1 / 4 , furthermore individual sample rate of each ability of at least 1  X  cm  X  10 , the RMSE for honest users and for manipulators satisfies:
RMSE Y  X  C 1  X k for some universal constant c , C 1 and C 2 .
 The proof of Proposition 4, deferred in the supplemen-tary material, involves bounding the prediction error of each individual users with Theorem 4 and sum over Y block and E block separately. Subspace difference  X  is bounded with Theorem 1 and Theorem 3 together. Finally,  X  min is bounded via Proposition 2. 6.3. Simulation To verify our robustness paradigm, we conducted sim-ulation for both models of attacks. Y is generated by multiplying two 1000  X  10 gaussian random matrix and n e attackers are appended to the back of Y . Targeted Attacks are produced by randomly choosing from a column of Y and assign 2  X  X ush X  and 2  X  X uke X  tar-gets to 1 and -1 respectively. Mass Attacks are gener-ated using uniform distribution. Factorization is per-formed using ALS. The results of the simulation are summarized in Figure 1 and 2. Figure 1 compares the RMSE under two attack models. It shows that when the number of attackers increases, RMSE under targeted attack remains small, while RMSE under ran-dom attack significantly increases. Figure 2 compares RMSE E and RMSE Y under random attack. It shows that when n e is small, RMSE Y RMSE E . How-ever, as n e increases, RMSE Y grows and eventually is comparable to RMSE E . Both figures agree with our theoretic prediction.
 This paper presented a comprehensive study of the stability of matrix factorization methods. The key re-sults include a near-optimal stability bound, a sub-space stability bound and a worst-case bound for indi-vidual columns. Then the theory is applied to the no-torious manipulator problem in collaborative filtering, which leads to an interesting insight of MF X  X  inherent robustness.
 Matrix factorization is an important tool both for ma-trix completion task and for PCA with missing data. Yet, its practical success hinges on its stability  X  the ability to tolerate noise and corruption. This paper is a first attempt to understand the stability of matrix factorization, which we hope will help to guide the ap-plication of matrix factorization methods.
 We list some possible directions to extend this research in future. In the theoretical front, the arguably most important open question is that under what conditions matrix factorization can reach a solution near global optimal. In the algorithmic front, we showed here that matrix factorization methods can be vulnerable to gen-eral manipulators. Therefore, it is interesting to de-velop a robust variation of MF that provably handles arbitrary manipulators.
 This research is partially supported by the National University of Singapore under startup grant R-265-000-384-133.

