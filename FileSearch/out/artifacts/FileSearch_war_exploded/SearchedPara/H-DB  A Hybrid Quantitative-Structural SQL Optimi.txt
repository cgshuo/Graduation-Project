 Structural decomposition methods are query optimization methods specifically conceived in the database theory com-munity to efficiently answer (near-)acyclic queries. We pro-pose to demonstrate H-DB, an SQL query optimizer that combines classical quantitative optimization techniques with such structural decomposition methods, which so far have been just analyzed from the theoretical viewpoint. The sys-tem provides support to optimizing SQL queries with ar-bitrary output variables, aggregate operators, ORDER BY statements, and nested queries. H-DB can be put on top of any existing database management system supporting JDBC technology, by transparently interacting/replacing its standard query optimization module. However, to push at maximum its optimization capabilities, H-DB should be cou-pled with an ad-hoc physical semi-join operator, which (as a relevant example) we implemented and integrated within the PostgreSQL database management system.
 H.2.4 [ Database Management ]: Systems X  Query process-ing; Relational databases ; F.2.2 [ Analysis of Algorithms and Problem Complexity ]: Nonnumerical Algorithms and Problems X  Computations on discrete structures Algorithms, Design, Performance Query Optimization, Structural Decomposition Methods
Optimization strategies for query evaluation are usually based on restricting the search space of query plans to very simple structures (e.g., left-deep trees) and on choosing the most promising one by exploiting quantitative information been subsequently spent to investigate invariants that are best suited to identify nearly-acyclic hypergraphs, leading to the definition of a number of so-called structural decom-position methods (such as the (generalized) hypertree [3], the fractional hypertree [5], and the component hypertree [4] de-compositions). These methods aim at transforming a given cyclic hypergraph into an acyclic one, by organizing its edges (or its nodes) into a polynomial number of clusters, and by suitably arranging these clusters as a tree, called decom-position tree. The original problem instance can then be evaluated over the decomposition tree, with a cost that is ex-ponential in the cardinality of the largest cluster, also called width of the decomposition, and polynomial if this width is bounded by some constant.

As an example, consider the (2-width) hypertree decom-position of H ( Q 5 ) depicted in Figure 2, and assume that the goal is just to check whether Q 5 admits a solution (i.e., we get rid of output variables and aggregate operators). Then, this Boolean query can be answered over the decomposition tree using a two-step plan. First, for each cluster (vertex of the decomposition tree), one computes the join of the rela-tions involved in it. Second, starting from the leaves, one moves bottom-up towards the root, by performing upward semi-joins of intermediate results.

Despite their very nice computational properties, struc-tural decomposition methods have not had any serious im-pact on the design of commercial database optimizers, and the interest for these techniques mainly remained at a theo-retical level. This is mainly because decomposition methods flatten in the query hypergraph all the quantitative aspects of data, do not take care of the output of the variables, and do not support aggregate operators and nested queries.
In this paper, we propose to demonstrate H-DB, an SQL query optimizer that combines classical quantitative opti-mization techniques with structural decomposition methods. The system implements the techniques discussed in [1, 7], and provides support to optimizing SQL queries with ar-bitrary output variables, aggregate operators, ORDER BY statements, and nested queries. H-DB can be put on top of any existing database management system supporting JDBC technology, by transparently interacting/replacing its standard query optimization module. However, to push at maximum its optimization capabilities, H-DB should be cou-pled with an ad-hoc physical semi-join operator, which (as a relevant example) we implemented and integrated within the PostgreSQL database management system.

In addition to its optimization capabilities, H-DB comes equipped with useful auxiliary modules, in particular, with graphical environments to compose and analyze user queries, and to compare internal query plans with the structure-aware query plans.
 tributes appearing in the node, by invoking the underlying system catalog. The computed hypergraph and its associ-ated statistics are then passed to the cost-k -decomp mod-ule 3 that computes a hypertree decomposition ,whosemax-imum width is fixed by the user. Note that, according to the q-decomposition strategy, we force all output variables (i.e., selected items, attributes appearing in group by or in order by clauses) to appear in the root of the decomposition. Furthermore, we also request that the attributes appearing in some non-join condition are covered together by at least a cluster of the computed decomposition.

Finally, the decomposition is translated into an actual query plan by the View Builder , which rephrases it in terms of a tree of SQL views, possibly materialized. Dur-ing this phase, all non-join constraints are assigned to the node of the decomposition, for being later evaluated. The computed optimized plan is then returned to the SubPlan Hadler and finally to the HDOpitmizer module, which arranges the computed plan node as an element of the cur-rent query plan tree. Once all nodes have been process, the resulting plan tree is  X  X inalized X .

H-DB supports two kinds of finalization modes. In the query execution mode ,the Executor immediately runs the structure-optimized query on the target database system and reports simple performance statistics at the end of exe-cution. In the explainer mode , a more comprehensive picture of the overall optimization process is given by the Explainer subsystem. In fact, the Explainer provides facilities for visualization and navigation of a plan, for comparing dif-ferent plans (e.g., comparison between structure-optimized plans and plans produced by the native optimizer, and eval-uation of optimized decompositions w.r.t. non-optimized ones), and for reporting cost estimations. Additionally, it supports user interaction and enables on-the-fly tuning of the HDOptimizer module to refine available plans.
The concrete implementation of H-DB poses various in-teresting technical challenges.

First, one needs to force the target DBMS to execute the various physical operators according to the rewritten plan, i.e., the internal underlying optimizer has to be inhibited. However, to take advantage of specific DBMS-dependent optimization strategies, such optimizer can be allowed to optimize joins within each cluster of the decomposition.
Second, physical semi-join operators are needed to push structural optimization benefits to a maximum. In fact, while semi-join operators are usually supported by DBMSs and can be, in principle, used to execute H-DB plans in the case of simple queries, they cannot be coupled with structure-aware optimization techniques in the case of queries involving aggregate operators (see, e.g., [8]). Indeed, in tra-ditional settings, an outer (left-side) relation and an inner (right-side) relation are involved in a semi-join, and the re-sult is given by the set of outer tuples matching some inner tuple w.r.t. the given join conditions. When semi-joins are combined with (e.g.) counting, one would like operators that directly compute the (cardinality of) support for each outer tuple. These operators however are missing in most of the available systems, and our experimentation evidenced that their simulation via pure JDBC statements is rather See http://www.deis.unical.it/scarcello/Hypertrees/. we included SEMI JOIN (resp., SEMI HASH JOIN )askeywords in the parsing rules system to specify semi-joins. The syn-tax we use follows the one of standard join in which one can also optionally specify counter attributes. As a second step, we provided the specification of the semi-join nodes within the planner and the executor of the PostgreSQL backend. When building a plan tree for a semi-join node, we force the PostgreSQL optimizer to compute a path where the join-ing order of the outer relation and the inner one is the one specified in the query. To preserve such an order, H-DB puts in the plan special setting configurations retrieved by an SQLQuery Transducer module. The actual cost is evaluated by estimating the access cost of a semi-join path, depending on which semi-join operator has been used X  X e enhanced the existing cost model to take this into account. Note that, when a standard join is specified or no assump-tions are made on the order of the joins in a query, the PostgreSQL optimizer uses its original built-in strategy to determine the best way to execute joins, choosing between its native alternatives (i.e., hash join path, merge join path, nested loop path). Once an execution plan has been selected, the PostgreSQL executor processes it, by visiting nodes and calling appropriate physical operators.

In order to achieve the correct execution of our custom semi-join paths, we defined a Plan Tree Dispatcher ,which is in charge of supporting the existing executor module. When a semi-join path (resp., a semi-hash-join path) is en-countered, the Plan Tree Dispatcher automatically for-wards it to the SEMI JOIN (resp., SEMI HASH JOIN ) operator, which actually performs the semi-join and retrieves result-ing tuples for the next plan state execution.
Attendees will be guided through the execution of syn-thetic queries, taken from both our own generated set and from the whole TPC-H benchmark set. By using the plan-explaining facilities of the system, attendees will graphically compare standard PostgreSQL plans with the plans pro-duced by H-DB, in terms of costs and performance gains.
As a concrete example, we report in Figure 5 some high-lights on a demo showcase for query Q 5 . First, attendees will evaluate the impact of database quantitative aspects (e.g., cardinality of relations, selectivity of attributes) on plans quality. In this sample experimentation, databases of size 2 gb, 5 gb and 10 gb are considered. As shown on top of Figure 5, the PostgreSQL optimizer is faster than H-DB only for the smallest database, while H-DB realizes an increasing gain w.r.t. PostgreSQL performances at the growing of the database size (up to 20% for 10 gb). Second, attendees will consider manipulation on query specification, by highlighting the role of some parameters (e.g., number of outputs attributes, number of aggregates, etc.), and of query qualifications on overall performances. The bottom of Figure 5 depicts the execution times for different sizes of the requested order date interval (a database of 2 gb is used): At the growing of the size of the interval (i.e., for an higher number of processed tuples) the execution time of the standard optimizer increases faster than the one of H-DB. Indeed, the increasing number of tuples passing the qualification leads to a growing size of intermediate joins computed by PostgreSQL. On the other hand, this blow-up does not occur by performing semi-joins, thereby evidencing very small increments of the execution time for H-DB.
