 statistics: squashing generalizes the sufficiency principle database research: a squashed data set is a lossy materi-algorithms: the steps in the GMG pipeline can individu-machine learning: extending existing ML methods for One of the chief obstacles to effective data mining is the clumsiness of managing and analyzing a very large data set. The process of model search and model fitting often requires many passes over the data, yet it is infeasible to squeeze large data sets into physical memory. 
There are two basic approaches to this problem: construct summaries of the large data set on which to base the desired analysis, or to analyze a random sample from the large data set. Each approach has disadvantages. It is hard to devise general-purpose a set of multiple regression analyses, then statistical theory often suggests sufficient statistics that can be computed in a single pass over the large data file without holding the file in memory. But the appropriate summaries will depend on having the desired model fixed in advance. This is a classical chicken and egg problem: to specify the summary, you need a model; to iterate to a well fitting model, you need to mine the data. 
The other strategy, drawing a random sample of the rows of the large data set, is easy to achieve. The sample can be analyzed with complete flexibility in choice of statistical method, unrestricted by a possibly unfortunate choice of summary statistics. The biggest disadvantage of this strategy is the inaccuracy introduced by sampling variance. 
In this paper we introduce a technique we call squashing that attempts to combine the best features of a pre-processed data summary and random sampling. Data squashing effectively summarizes a large data set with a smaller version (often by several orders of magnitude) having the same variables as the original data set. The squashed data set is constructed to emulate the multivariate distribution of the larger data set more accurately than random sampling. Each element of the squashed data set has a weight, where the sum of the weights equals the number of elements in the original data set. A theory based on a Taylor series approximation to the likelihood function suggests that a weighted analysis of the squashed data can provide accurate approximations to the results that would be found from fitting almost any smooth model to the larger data set. 
Data squashing can be seen as a form of lossy database compression. A significant body of recent work in the database literature has examined methods for the lossy compression of databases, and especially data cubes (Barbara 1997). However, data squashing has a somewhat different goal. Lossy data cube compression can be judged acceptable if aggregate queries over ranges of the data cube have a small error. Data squashing is acceptable if a different type of query has a small error, e.g., the fitting of statistical models. Thus, the techniques proposed in this paper are valuable for statistical analysis data warehouses. 
In what follows, we assume that the large data set has a simple Ylat X  structure, consisting of a very large number of rows or elements, each of a fixed number of attributes or variables. The variables are either categorical (nominal scale) or quantitative (measurement scale). The technique of data squashing consists of the following steps (applied sequentially): Group the data in regions: Categorical variables in-Calculate moments within regions: The number of Generate squashed data elements and weights: The squashed data set is smaller (by orders of magni-tude) than the original data set. It preserves analy-sis flexibility without sacrificing accuracy due to sam-pling variability. The only restriction on analyzing the squashed data, compared to the methods available to analyze the original file, is that the analysis method or software must be able to make use of the induced weights. 
The requirement that the squashed data summarize the large data set as well as possible is taken to mean that for all or almost all statistical models that might be fitted to the data, the result from a weighted model fit of the squashed data is nearly the same as if the same model had been fit to the original data. A naive attempt at creating a squashed data set is to take a 1% random sample of the data set and then add a weight variable equal to 100 for every row in the sample. 
However, parameter estimates created from this sample will typically differ from the corresponding full data estimates by about 10 standard errors (since standard errors of efficient estimates are inversely proportional to the square root of the sample size). The goal is to create squashed data sets that yield parameter estimates that are within one standard error of the full data estimates. 
Another possible strategy for creating a squashed data set is to perform a cluster analysis on the original data set, where each element of the large data set is in exactly one cluster. Then the squashed file could consist of the centers of each cluster, and the weight variable would be the size of the corresponding cluster. There are two impediments to this approach. First, clustering algorithms are typically O(n X ) or may require many passes over the data so as to render clustering infeasible for very large data sets (see however Bradley, Fayyad, and Reina 1998). Second, replacing each cluster by a single weighted point at the cluster mean implicitly reduces the overall variance of every quantitative variable in the squashed data set and would thus tend to distort the results of most statistical models. 
In the following sections we first discuss the theoreti-cal framework for data squashing. We then discuss the computational aspects of the GMG pipeline. Each step can be viewed as an individual module with its own choice of methodology and with its own trade-offs in speed, space, and accuracy. In particular for the group-ing step we discuss how to avoid the curse of dimen-sionality by utilizing data spheres (Johnson and Dasu 1998). We apply data squashing to a real data set and demonstrate the flexibility of the method through the choice of plug-in modules. Finally we discuss related work and future research directions. In what follows, we assume that the large data set has a simple  X  X lat X  structure, consisting of a very large number, N, of rows or elements, each consisting of a fixed number of attributes or variables. The variables are either categorical (nominal scale) or quantitative (measurement scale). Suppose the large data set has columns Al, . . , AC, Xl,...,XQ. The As are categorical variables, while the Xs are quantitative variables. Let A = Ai, and X = Xij; i = l,..., N; c = l,..., C; j = l,..., Q, denote the original N x C and N x Q data matrices of categorical and quantitative variables, respectively. Let the squashed data set, having M &lt;&lt; N rows, be represented by corresponding matrices B = B;, and Y=k;:j; i=l,..., M; c=l,..., C;j=l,..., Q, andlet wi be the weight assigned to row i, where CE, wi = N. Suppose further that a statistical model is being fit to the data. That is, a modeling assumption is being made that the large data set is the result of N independent draws from the probability model where 0 is a vector of parameters that are estimated from the data. The function f defines the particular model being fit, and we want to have a squashed version of the data that provides nearly identical analyses for a wide choice of f. By  X  X dentical analysis X  we mean that we want the information about 0 to be the same for the full data set (A, X) as for the squashed data set (B, Y, w). This means that we want the product of probabilities in Eq( 1) to represent the same function of 6 X  for the two possible data sets, or, in statistical terminology, we want the two samples to have the same likelihood function. Equating the two log-likelihoods, we have the following requirement: 
We now make the assumption that for every set of fixed values of A and 8, f(A, X, 8) is a relatively smooth function of (Xi,. . , XQ), so that log(f(Al, . . . , AC, XI,. . , XQ; 0)) can be well repre-sented by a Taylor series in the neighborhood of any point 2 = (21, . . . , xQ). That is, we have the approxi-mate equality 
In Eq(3) there are I( terms in the Taylor series, the coefficients gk depend on (Al,. . . , AC), 0, and 2, but not on (Xi,. . . , XQ), and the power vectors (pkl, . . . ,pkQ) are all vectors of Q nonnegative integers satisfying cj pkj 5 d, where d is the degree of the approximation. In order to use Eq(3) to solve Eq(2), we divide the space of X into a finite set of neighborhoods and partition the space (A,X) into regions where A is constant and X is confined to a single neighborhood. Suppose there are R such regions and that the large data set has N, points in region r, where C, X =, N,. = N. Likewise, we assume that the squashed data set will have M,. points in region r, and that the summation Czi w; = N,.. Within the rth region, the likelihood equivalence Eq(2) can be separately enforced, which amounts to setting every Bi, equal to the corresponding constant value of A,, c= 1, . , C, and replacing M and N by M, and N,., respectively. Combining Eq(2) and Eq(3), we have the approximate constraints 
In order for Eq(4) to hold for arbitrary functions f, and thus for arbitrary coefficients gk, the factors multiplying each gk must be equated separately. This leads to the set of equations 
M, Q 
This result suggests that within each region (where the categorical variables are constant and the quanti-tative variables are confined to a compact subregion) and for each power vector (pki, . . , pkQ), the weighted mixed moment of the squashed data set should approx-imately equal the corresponding mixed moment of the original data set. If d = 2, only means, variances and covariances are preserved within each region. Although this would be sufficient to preserve the results of many linear statistical models such as multiple regression, the squashed data set might not be a good enough approx-imation for the nonlinear models used often in data mining applications. In later sections we discuss the issues involved in relating the degree of approximation in Eq(5) and the size of the squashed data set. The goal of a binning strategy is to partition the data into compact subregions. Within each of these subregions we then calculate moments, and create pseudo points via the squashing algorithm. An obvious choice for creating bins is to use all qualitative variables Hyper-rectangles: We can create bins by categoriz-Data Spheres: Data Spheres are a way of inducing HypRect: The bins are all combinations of categori-DSphere: The bins are all combinations of categorical complexity if necessary. In order to match the number of moments to the number of pseudo points, we define degrees of freedom as df = m(Q + l), where m is the number of pseudo points in a bin. Note that df is the number of free parameters (values of  X  X , and Y) that the pseudo data contain. We choose the li M df lowest order moments from among those listed above. Choice of pseudo sample size. Within the rth bin, the number of points, M,., in the pseudo sample depends on the corresponding number of points, N,., in the original data. The choice of M, depends on how much data reduction is desired, and M,. should increase slowly with N,. We use the somewhat arbitrary formula where (Y is a number that determines how much data reduction is achieved. In our example, we compare (Y = 0, a ) 1, and 2. When Q = 0, only one point is used to approximate each bin, positioned at the bin mean for each variable. When (Y &gt; 0, the method uses more points and moments as (Y increases for a better-fitting pseudo sample. The overall data reduction factor is thus approximately C,. N,./ C, M,. x N/a C, log, N,. . 
The moment calculations have a strong advantage in data mining applications. They can be calculated sequentially, so that when a new set of data arrives, they can be used to update the moments without recalculating the moments for the whole set. This is important when we are dealing with millions of records which are streaming in daily; we do not want to have to re-compute moments each time we receive fresh data. We search for an approximate solution to Eq(5) sepa-rately for each region indexed by r. Within each re-gion, we can assume that the xj in Eq(5) are equal to the mean of the Xij contained in the region, and, by re-centering the Xs within each region to have mean 0, assume that every x~j =O. Therefore, for each T (in what follows, we do not bother to add an index T to many variables that are defined separately for each region), we search for an M,. x Q matrix Y and an M,.-length vector w to approximately satisfy the equations This is a system of Ii equations in M,.(Q+l) unknowns. As mentioned above, we choose Ii for each r by selecting enough low-order moments so that IC &gt; M,(Q + 1) up to a maximum K as discussed in Section 8. Even if K &lt; M,(Q + 1) there may often be no exact solutions to Eq(7), because we enforce the constraints 
We rule out negative weights and variable values out-side the ranges of variables found in the correspond-ing region of the original data set because we want the squashed data set to be similar to the original data set. However, we do not insist that the values of Y be re-stricted to values that occur in the corresponding X. For example, if a given Xj is  X  X umber of children in the family X , and varies from 0 to 10 in the original file, the value 8.5 would be an acceptable value for Yj, but not 10.5 or -1. If it is desired to maintain the set of actually occurring values, then we make the variable categorical (a column of A). The search for (Y, w) is treated as the search for a least squares estimate to minimize 
Finding a least squares solution. In Eq(8), the positive multipliers uk are used to ensure that the lower-order moments are approximated more accurately. Since it is trivial to scale w and each column of Y so that the moments of order 0 and 1, and the pure squares of order 2, are fit exactly, the corresponding values of k have  X  X lk = 1000. All the other uk sum to 1, with moments of order 2 having larger uk than those of order 3, which are in turn larger than the uk for moments of order higher than 3. (In addition to being centered at 0, the Xs are originally scaled to have variance 1, so that moments of different orders are comparable.) Computation of the moments zk is done using an updating algorithm, so that the centered and scaled moments within each region can be computed in a single pass over the large data set with a minimum of round-off error. Since S(Y, w) in Eq(8) is a simple polynomial function of the unknowns, it is easy to compute the required derivatives for a second-order Newton-Raphson iterative scheme. In order to maintain the constraints of Eq(7) at each update step, we transform the unknowns using a logistic transform, so that the finite range is converted to an unbounded range and out-of-bound updates are prevented. In addition, at each update the step size is reduced if necessary to ensure that Eq(8) gets smaller at each update. There will rarely be a unique local minimum of Eq(8). However, in our experience the multiple local minima often have very similar values of S(Y, w), and will serve equally well for the purpose of creating a squashed data file. The Newton-Raphson search can be repeated with different random starting points to guard against the occasional solution that is much poorer than average. At AT&amp;T we routinely see data sets with millions, hundreds of millions or even billions of records. Many of these data sets have a fixed number of variables, which are somewhat restricted by the space we have to keep them or the processing time it takes to update them. These types of data sets are the ones we feel will get the most benefit from squashing, since we can keep a smaller version of the data set handy for quick analyses, and we can update the data set as new data come in through sequential updating of the moments. Here we describe a relatively small data set and use it to illustrate a variety of squashing strategies and their associated performance. 
The goal of our analysis is to see if we are able to detect customers who have switched to another long distance carrier. Because almost all information about our customers goes through the local phone companies, sometimes we do not find out about the departing customers for weeks. It is to our benefit to be able to detect these customers as soon as possible. 
We collected a set of variables describing customer behavior which we thought might provide information about those who have left, whom we X  X l call Defectors. Two of these variables were 3 level categorical variables (Al, AZ), and five others are continuous (Xl,. . .,X5). 
We gathered a training set of 744,963 records. A data set this size is large enough to show the benefits of squashing, but also small enough so that we can do a non-linear analysis and compare the results of the full data to the results from a variety of squashed versions. Our goal is classification and we will use logistic regression to predict Defectors using the 7 other independent variables. 
We stress that our purpose in presenting this example is not to show how to solve a particular classification problem. Rather, we assume that we are required to solve a nonlinear problem that has no acceptable traditional solution computable via one or two passes over the data set. Estimation of logistic regression coefficients is intended to serve as a prototype of such a problem. We assume that examination of coefficients from a complicated model will answer an important business problem. 
As discussed earlier, we have many different squash-ing options with respect to number and type of bins and the number of pseudo points fit in each bin. These two options will determine the number of moments used and the computational complexity involved. In general, using a method that creates many bins decreases the number of pseudo points per bin in order to keep the same level of data reduction. We investigated the per-formance of 6 different squashing techniques defined by the binning strategies and data reduction parameter Q. Table 1 summarizes the binning and moment strategies for these methods. As mentioned earlier, we use fewer moments per bin when a bin size (based on Q log, N,.) is small. 
As the table shows, we combine the values cr = 0, 1,2 with DSphere grouping (R = 394 regions), and the values Q = 0, .5,1 with HypRect grouping (R = 3710 regions), to provide pseudo data set sizes of M = 394,2183,4475,3710,8373,17386, respectively. These are compared to a random 1 % sample of size 7576. The sample reduction factor is N/M, which is 98 for the random sample, and ranges from 43 to 1891 for the squashed samples. The table also shows that the number of pseudo points per bin and the number of moments used per bin are larger for DSphere than for HypRect, and also that they increase with CY. 
We fit logistic regressions predicting the Defectors from the other 7 variables to the full data set, the sampled data and the 6 squashed data sets. One regression consists of only main effects while the other adds all second order terms, including interactions. To assess the performance of the methods, we assume that the coefficients from the full data set are the truth, and look at how close the other methods approximate this truth. 
To quantify how close the reduced data sets approx-imate the full data, we look at standardized residu-als from the true coefficients (Figures 1 and 2). The last two columns of Table 1 summarize the plotted val-ues in the Figures. Each MSE in the table is an av-erage squared value of standardized coefficient errors (Coef -True)/StE rr f or a particular sample. Note that, theoretically, we expect MSE = Reduction Factor for a random sample, as in row 2 of the Table. A squashing method works well if MSE &lt;&lt; Reduction Factor. Table 1 shows that this is true whenever (Y &gt; 0, but this is not true for cr = 0. In other words, basing the logistic regression solely on bin means is worse than taking a random sample of the same size. This is probably re-lated to the fact that the bin means have much reduced variances compared to the original data. (Note that the figures do not show the results for (Y = 0, since to include them would force the vertical range of the fig-ures to increase so much that comparative detail for the other methods would be lost.) To summarize, for the first order model, all of the squashing techniques except for Means (i.e. CY = 0) do significantly better than 1% sampling. Most of the estimated coefficients from the squashed data sets are within one standard error of the true value, whereas the majority of the sampled-data coefficients are greater than 5 standard errors away. For the second order model, which has 48 coefficients to estimate excluding the intercept, the improvement of squashing over sampling is less pronounced, but still ap-parent, especially for the HypRect method with CY = 1. Figure 1: Standardized errors for logistic regression coefficients from reduced samples, main-effects model. F,lot symbol denotes method. [ +: DSphere(1) Random Sample ] 
Another way of checking the efficacy of squashing is to see how well it emulates the full data set in the output of the model. In a logistic regression model, each element of the data set is assigned a probability of being a Defector. We used the coefficients from 
Figure 2: Standardized errors for logistic regression coefficients from reduced samples, second order model. 
Plot symbol denotes method. [ +: DSphere(1) *: DSphere(2), 0: HypRect(i), X: HypRect(l), S: 
Random Sample ] the 6 squashed data sets and the 1% sample , as fit to the first order logistic regression, to assign this probability. This probability was then compared with the  X  X rue X  probability of being a Defector from the full model. For each row of the full data set, a residual is defined as (Probability based on reduced data set) -(Probability based on full data set), multiplied by 10000 for descriptive purposes. Table 2 describes the there are B bins due to categorical variables, and 
Q quantitative variables, with N points in the large data set. We always assume that the data set has been sorted according to the quantitative variables, and the overhead for this sorting step, roughly QNlog(N) operations, is part of every version of squashing in this paper. The squashing is done independently within each of the B bins, and can be parallelized. Suppose the total number of populated regions (combinations of quantitative variable bins within categorical variable bins) is R. Cost of Grouping For either of the two binning strategies, it takes one pass over the data to assign each point to one of the bins (the Data Sphere method requires an extra pass over the data to determine bin boundaries). Memory requirements are proportional to NQ and CPU is proportional to NQ. Cost of Computing Moments The total number of moments considered is li X  = 5Q + 3Q(Q -1) + 2Q(Q -l)(Q -2)/3 + Q(Q -l)(Q -2)(Q -3)/W which is asymptotically equivalent to Q4/24, but for moderate Q the lower-order terms in the expression for K predominate. Using a one-pass algorithm, the memory requirements are proportional to RK and CPU is proportional to NK. However, if the data are first sorted according to the R regions, then only O(K) units of memory are required. Cost of Computing the pseudo points If there are M,. pseudo points to estimate in region T, the task is to estimate M,.(Q + 1) unknowns as the least squares so-lution to K, equations, where K, is the number of mo-ments used in region r. The task must be repeated 
R times. The dependence on N, the total number of points in the large data set, comes from the relations M,. = [a log,( Nr)] and A X , = M,.(Q+l). The total CPU is proportional to the sum over T of CPU(rr, N,, Q), where CPU(o, N,., Q) is the cost of iterating to a so-lution when MT = [o logz(N,.)] and I&lt;,. = Mp(Q + 1). 
The resulting total CPU depends on the distribution of N,. across the R regions. When R is large, the vast majority of the MT are quite small and the to-tal CPU is virtually proportional to R, the number of regions, with relatively little dependence on N. Ex-amining our estimation algorithm in detail, each itera-tion is dominated by function evaluations that involve 
O(M,.K,) p t X  o era ions. Thus the total CPU is approx-imately O(CrZ1 M,.K,.) = O(a X  X  ~~=,(log(N,.))2) = 
O(a X  X Q(log(N/R)) X ). Note that whenever M, = 1 there is a trivial solution to the equations found by equating the pseudo points to the means of the orig-inal points. 
The CPU cost of converging to an iterative solution is impossible to specify in advance and depends on the particular estimation algorithm and the data. We typically perform fewer than 100 function evaluations for each solution. Note that for the timings given in 
Table 1, the total CPU went up somewhat slower with cr and faster with R than the above formula suggests. 
The four ratios 1000 CPU/(02RQ(log(N/R))2) are 0.4, 0.3, 2.9, and 1.1, respectively. 
The storage required for our Newton-Raphson algo-rithm is proportional to 02Q2 max,.(log(N,))2. 
Cost Summary With respect to memory, all the methods and steps of the squashing technology are extremely economical and scale very well in both N and 
Q. With respect to arithmetic operations, the grouping and momentizing steps involve an effort no more onerous than sorting and a very few passes over the data set of similar effort. The pseudo point generation step is much more computationally intensive, but has the tremendous advantages that it increases only with log(N) and can make efficient use of parallel processing. 
In our example, the grouping and momentizing steps took a matter of minutes, while the pseudo point generation took hours on workstations with no parallel processing. Had there been one hundred times as many rows, but the same number of columns, in the original data, the entire squashing procedure would still have taken hours, rather than days. 
We have demonstrated that it is possible to scale down large data sets while preserving micro-structure that might be important for subsequent analysis. We view squashing as a breakthrough since it immediately opens up the possibility of applying all current machine learning and statistical techniques to large data sets -all that is required is that these techniques accept element weights. Our method is novel in several respects: l it constructs a replacement for the original data set l it consists of modular pieces that can be individually l it has a theory to guide further refinements and to 
Squashing is demonstrably more powerful than tak-ing a simple random sample from the large data set. It is possible that special-purpose samples might do better believe that the limiting factor is not the number of 
