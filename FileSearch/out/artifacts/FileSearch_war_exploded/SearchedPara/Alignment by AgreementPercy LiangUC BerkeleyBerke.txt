 Word alignment is an important component of a complete statistical machine translation pipeline (Koehn et al., 2003). The classic approaches to un-supervised word alignment are based on IBM mod-els 1 X 5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison). One can classify these six models into two groups: sequence-based models (models 1, 2, and HMM) and fertility-based models models are tractable and easily implemented, the more accurate fertility-based models are intractable and thus require approximation methods which are difficult to implement. As a result, many practition-ers use the complex GIZA++ software package (Och and Ney, 2003) as a black box, selecting model 4 as a good compromise between alignment quality and efficiency.

Even though the fertility-based models are more accurate, there are several reasons to consider av-enues for improvement based on the simpler and faster sequence-based models. First, even with the highly optimized implementations in GIZA++, models 3 and above are still very slow to train. Sec-ond, we seem to have hit a point of diminishing re-turns with extensions to the fertility-based models. For example, gains from the new model 6 of Och and Ney (2003) are modest. When models are too complex to reimplement, the barrier to improvement is raised even higher. Finally, the fertility-based models are asymmetric, and symmetrization is com-monly employed to improve alignment quality by intersecting alignments induced in each translation direction. It is therefore natural to explore models which are designed from the start with symmetry in mind.

In this paper, we introduce a new method for word alignment that addresses the three issues above. Our development is motivated by the observation that in-tersecting the predictions of two directional models outperforms each model alone. Viewing intersec-tion as a way of finding predictions that both models agree on, we take the agreement idea one step fur-ther. The central idea of our approach is to not only make the predictions of the models agree at test time, but also encourage agreement during training. We define an intuitive objective function which incor-porates both data likelihood and a measure of agree-ment between models. Then we derive an EM-like algorithm to maximize this objective function. Be-cause the E-step is intractable in our case, we use a heuristic approximation which nonetheless works well in practice.

By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task. To our knowledge, this is the lowest published unsupervised AER result, and it is com-petitive with supervised approaches. Furthermore, our approach is very practical: it is no harder to implement than a standard HMM model, and joint training is no slower than the standard training of two HMM models. Finally, we show that word alignments from our system can be used in a phrase-based translation system to modestly improve BLEU score. We briefly review the sequence-based word align-ment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation. All three models are generative models of the form p ( f | e ) = P where e = ( e f = ( f 1 , . . . , f J ) is the French sentence, and a = ( a 1 , . . . , a J ) specifies the position of an English word aligned to each French word. All three models factor in the following way: p ( a , f | e ) = where j French word before position j . 2
The translation parameters p t ( f rameterized by an (unsmoothed) lookup table that stores the appropriate local conditional probability distributions. The distortion parameters p d ( a a a j = 0 is null-aligned): p d ( a j = i 0 6 = 0 | a j where p tains the distortion parameters for each offset argu-ment. We set the null-word probability p depending on the length of the English sentence, which we found to be more effective than using a constant p
In model 1, the distortion p d (  X | X  ) specifies a uni-form distribution over English positions. In model 2, p d (  X | X  ) is still independent of a j depend on j and i 0 through c (  X  ) . In the HMM model, there is a dependence on a c ( i  X  i 0 ) .

We parameterize the distortion c (  X  ) using a multi-nomial distribution over 11 offset buckets c (  X  distortion parameters, one for transitioning into the first state, one for transitioning out of the last state, and one for all other transitions. This works better than using a single set of parameters or ignoring the transitions at the two ends. To motivate our joint training approach, we first consider the standard practice of intersecting align-ments. While the English and French sentences play a symmetric role in the word alignment task, sequence-based models are asymmetric: they are generative models of the form p ( f | e ) (E  X  F), or p ( e | f ) (F  X  E) by reversing the roles of source and target. In general, intersecting the alignment predic-tions of two independently-trained directional mod-els reduces AER, e.g., from 11% to 7% for HMM models (Table 2). This suggests that two models make different types of errors that can be eliminated upon intersection. Figure 1 (top) shows a common type of error that intersection can partly remedy. In this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. Intersection eliminates the spurious alignments, but at the expense of recall.

Intersection after training produces alignments that both models agree on. The joint training pro-cedure we describe below builds on this idea by en-couraging the models to agree during training. Con-sider the output of the jointly trained HMMs in Fig-ure 1 (bottom). The garbage-collecting rare word is no longer a problem. Not only are the individual E  X  F and F  X  E jointly-trained models better than their independently-trained counterparts, the jointly-trained intersected model also provides a signifi-cant overall gain over the independently-trained in-tersected model. We maintain both high precision and recall.

Before we introduce the objective function for joint training, we will write the two directional mod-els in a symmetric way so that they share the same alignment spaces. We first replace the asymmetric alignments a with a set of indicator variables for each potential alignment edge ( i, j ) : z = { z { 0 , 1 } : 1  X  i  X  I, 1  X  j  X  J } . Each z can be thought of as an element in the set of generalized alignments , where any subset of word pairs may be aligned (Och and Ney, 2003). Sequence-based mod-els p ( a | e , f ) induce a distribution over p ( z | e , f ) by letting p ( z | e , f ) = 0 for any z that does not correspond to any a (i.e., if z contains many-to-one alignments).

We also introduce the more compact notation x = ( e , f ) to denote an input sentence pair. We put arbitrary distributions p ( e ) and p ( f ) to remove the conditioning, noting that this has no effect on the optimization problem in the next section. We can now think of the two directional sequence-based models as each inducing a distribution over the same space of sentence pairs and alignments ( x , z ) : 3.1 A joint objective In the next two sections, we describe how to jointly train the two models using an EM-like algorithm. We emphasize that this technique is quite general and can be applied in many different situations where we want to couple two tractable models over input x and output z .

To train two models p independently, we maximize the data likelihood Q separately, k  X  X  1 , 2 } : Above, the summation over x enumerates the sen-tence pairs in the training data.

There are many possible ways to quantify agree-ment between two models. We chose a particularly simple and mathematically convenient measure  X  the probability that the alignments produced by the two models agree on an example x : We add the (log) probability of agreement to the standard log-likelihood objective to couple the two models: 3.2 Optimization via EM We first review the EM algorithm for optimizing a single model, which consists of iterating the follow-ing two steps: In the E-step, we compute the posterior distribution of the alignments q ( z ; x ) given the sentence pair x and current parameters  X  . In the M-step, we use ex-pected counts with respect to q ( z ; x ) in the maxi-mum likelihood update  X  :=  X  0 .

To optimize the objective in Equation 3, we can derive a similar and simple procedure. See the ap-pendix for the derivation.
 E: q ( z ; x ) := 1
M:  X  0 = argmax where Z decouples neatly into two independent optimization problems, which lead to single model updates using the expected counts from q ( z ; x ) . To compute Z the E-step, we must sum the product of two model posteriors over the set of possible z s with nonzero probability under both models. In general, if both posterior distributions over the latent variables z decompose in the same tractable manner, as in the context-free grammar induction work of Klein and Manning (2004), the summation could be carried out efficiently, for example using dynamic programming. In our case, we would have to sum over the set of alignments where each word in English is aligned to at most one word in French and each word in French is aligned to at most one word in English. Unfortunately, for even very simple models such as IBM 1 or 2, computing the normalization constant over this set of alignments is a # P -complete problem, by a reduction from counting matchings in a bipartite graph (Valiant, 1979). We could perhaps attempt to compute q us-ing a variety of approximate probabilistic inference techniques, for example, sampling or variational methods. With efficiency as our main concern, we opted instead for a simple heuristic procedure by letting q be a product of marginals: where each p probability of the ( i, j ) edge being present (or ab-sent) in the alignment according to each model, which can be computed separately and efficiently.
Now the new E-step only requires simple marginal computations under each of the mod-els. This procedure is very intuitive: edges on which the models disagree are discounted in the E-step because the product of the marginals p this new procedure is not guaranteed to increase our joint objective. Nonetheless, our experimental re-sults show that it provides an effective method of achieving model agreement and leads to significant accuracy gains over independent training. 3.3 Prediction Once we have trained two models, either jointly or independently, we must decide how to combine those two models to predict alignments for new sen-tences.
 First, let us step back to the case of one model. Typically, the Viterbi alignment argmax is used. An alternative is to use posterior decoding, where we keep an edge ( i, j ) if the marginal edge posterior p ( z 1 . In symbols, z = { z
Posterior decoding has several attractive advan-tages over Viterbi decoding. Varying the threshold  X  gives a natural way to tradeoff precision and re-call. In fact, these posteriors could be used more di-rectly in extracting phrases for phrase-based trans-lation. Also, when we want to combine two mod-els for prediction, finding the Viterbi alignment argmax HMM models (by a reduction from quadratic as-signment), and a hard intersection argmax x )  X  argmax On the other hand, we can threshold the product of two edge posteriors quite easily: z = { z p ( z ij = 1 | x ) p 2 ( z ij = 1 | x )  X   X  } .
We noticed a 5.8% relative reduction in AER (for our best model) by using posterior decoding with a validation-set optimized threshold  X  instead of using hard intersection of Viterbi alignments. We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sen-tences, a validation set of 37 sentences, and a test set of 447 sentences. The validation and test sentences have been hand-aligned (see Och and Ney (2003)) and are marked with both sure and possible align-ments. Using these alignments, alignment error rate (AER) is calculated as: where A is a set of proposed edges, S is the sure gold edges, and P is the possible gold edges. As a preprocessing step, we lowercased all words. Then we used the validation set and the first 100 sen-tences of the test set as our development set to tune our models. Lastly, we ran our models on the last 347 sentences of the test set to get final AER results. 4.1 Basic results We trained models 1, 2, and HMM on the Hansards data. Following past work, we initialized the trans-lation probabilities of model 1 uniformly over word pairs that occur together in some sentence pair. Models 2 and HMM were initialized with uni-form distortion probabilities and model 1 translation probabilities. Each model was trained for 5 itera-tions, using the same training regimen as in Och and Ney (2003). Table 1: Comparison of AER between independent and joint training across different size training sets and different models, evaluated on the development set. The last column shows the relative reduction in AER.

Table 1 shows a summary of the performance of independently and jointly trained models under var-ious training conditions. Quite remarkably, for all training data sizes and all of the models, we see an appreciable reduction in AER, especially on the HMM models. We speculate that since the HMM model provides a richer family of distributions over alignments than either models 1 or 2, we can learn to synchronize the predictions of the two models, whereas models 1 and 2 have a much more limited capacity to synchronize.

Table 2 shows the HMM models compared to model 4 alignments produced by GIZA++ on the test set. Our jointly trained model clearly outperforms not only the standard HMM but also the more com-plex IBM 4 model. For these results, the threshold used for posterior decoding was tuned on the devel-opment set.  X  X IZA HMM X  and  X  X MM, indep X  are the same algorithm but differ in implementation de-tails. The E  X  F and F  X  E models benefit a great deal by moving from independent to joint training, and the combined models show a smaller improve-ment.
 Our best performing model differs from standard IBM word alignment models in two ways. First and most importantly, we use joint training instead of Table 2: Comparison of test set AER between vari-ous models trained on the full 1.1 million sentences. Table 3: Contributions of using joint training versus independent training and posterior decoding (with the optimal threshold) instead of Viterbi decoding, evaluated on the development set. independent training, which gives us a huge boost. The second change, which is more minor and or-thogonal, is using posterior decoding instead of Viterbi decoding, which also helps performance for model 2 and HMM, but not model 1. Table 3 quan-tifies the contribution of each of these two dimen-sions.
 Posterior decoding In our results, we have tuned our threshold to minimize AER. It turns out that AER is relatively insensitive to the threshold as Fig-ure 2 shows. There is a large range from 0.2 to 0.5 where posterior decoding outperforms Viterbi de-coding.
 Initialization and convergence In addition to im-proving performance, joint training also enjoys cer-tain robustness properties. Specialized initialization is absolutely crucial for an independently-trained Figure 2: The precision, recall, and AER as the threshold is varied for posterior decoding in a jointly trained pair of HMMs.
 HMM model. If we initialize the HMM model with uniform translation parameters, the HMM converges to a completely senseless local optimum with AER above 50%. Initializing the HMM with model 1 pa-rameters alleviates this problem.

On the other hand, if we jointly train two HMMs starting from a uniform initialization, the HMMs converge to a surprisingly good solution. On the full training set, training two HMMs jointly from uni-form initialization yields 5.7% AER, only slightly higher than 5.2% AER using model 1 initialization. We suspect that the agreement term of the objective forces the two HMMs to avoid many local optima that each one would have on its own, since these lo-cal optima correspond to posteriors over alignments that would be very unlikely to agree. We also ob-served that jointly trained HMMs converged very quickly X  X n 5 iterations X  X nd did not exhibit over-fitting with increased iterations.
 Common errors The major source of remaining errors are recall errors that come from the shortcom-ings of the HMM model. The E  X  F model gives 0 probability to any many-to-one alignments and the F  X  E model gives 0 probability to any one-to-many alignments. By enforcing agreement, the two mod-els are effectively restricted to one-to-one (or zero) alignments. Posterior decoding is in principle ca-pable of proposing many-to-many alignments, but these alignments occur infrequently since the poste-riors are generally sharply peaked around the Viterbi alignment. In some cases, however, we do get one-to-many alignments in both directions.

Another common type of errors are precision er-rors due to the models overly-aggressively prefer-ring alignments that preserve monotonicity. Our HMM model only uses 11 distortion parameters, which means distortions are not sensitive to the lex-ical context of the sentences. For example, in one sentence, le is incorrectly aligned to the as a mono-tonic alignment following another pair of correctly aligned words, and then the monotonicity is broken immediately following le  X  the . Here, the model is insensitive to the fact that alignments following arti-cles tend to be monotonic, but alignments preceding articles are less so.

Another phenomenon is the insertion of  X  X tepping stone X  alignments. Suppose two edges ( i, j ) and ( i +4 , j +4) have a very high probability of being in-cluded in an alignment, but the words between them are not good translations of each other. If the inter-vening English words were null-aligned, we would have to pay a big distortion penalty for jumping 4 positions. On the other hand, if the edge ( i +2 , j +2) were included, that penalty would be mitigated. The translation cost for forcing that edge is smaller than the distortion cost. 4.2 BLEU evaluation To see whether our improvement in AER also im-proves BLEU score, we aligned 100K English-French sentences from the Europarl corpus and tested on 3000 sentences of length 5 X 15. Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035. By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051. While this improvement is very modest, we are currently inves-tigating alternative ways of interfacing with phrase table construction to make a larger impact on trans-lation quality. Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999). One key difference in our work is that we rely exclusively on data like-lihood to guide the two models in an unsupervised manner, rather than relying on an initial handful of labeled examples.

The idea of exploiting agreement between two la-tent variable models is not new; there has been sub-stantial previous work on leveraging the strengths of two complementary models. Klein and Man-ning (2004) combine two complementary mod-els for grammar induction, one that models con-stituency and one that models dependency, in a man-ner broadly similar to the current work. Aside from investigating a different domain, one novel aspect of this paper is that we present a formal objective and a training algorithm for combining two generic mod-els. We have described an efficient and fully unsuper-vised method of producing state-of-the-art word alignments. By training two simple sequence-based models to agree, we achieve substantial error re-ductions over standard models. Our jointly trained HMM models reduce AER by 29% over test-time intersected GIZA++ model 4 alignments and also increase our robustness to varying initialization reg-imens. While AER is only a weak indicator of final translation quality in many current translation sys-tems, we hope that more accurate alignments can eventually lead to improvements in the end-to-end translation process.
 Acknowledgments We thank the anonymous re-viewers for their comments.
 Appendix: Derivation of agreement EM To simplify notation, we drop the explicit reference to the parameters  X  . Lower bound the objective in Equation 3 by introducing a distribution q ( z ; x ) and using the concavity of log : where C depends only on  X  but not q and D de-pends only q but not  X  . The E-step chooses q given a fixed  X  to maximize the lower bound. Equation 6 is exactly P mized by setting q proportional to p chooses  X  given a fixed q . Equation 7 decomposes into two separate optimization problems.
