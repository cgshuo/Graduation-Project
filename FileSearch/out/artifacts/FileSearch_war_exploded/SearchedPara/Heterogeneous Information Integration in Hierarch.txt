 Text Classification (TC) is a process of assigning text documents into one or more topical categories. It is an important research problem in information retrieval and from different research communities. As a result, many TC algorithms have been proposed, such as Na X ve Bayes, Support Vector Machines (SVM) and their variations [6][11]. 
More recently, with the explosive growth of the World Wide Web, hierarchical maintaining of large-scale Web page corpora such as the Yahoo! Directory and the Open Directory Project (ODP). Other than simply using the hierarchical taxonomy to organize classifiers, empirical studies also showed that by exploiting the distance between categories (i.e. the path length between categories) in the taxonomy tree, the classification performance can be improved [2][4][5]. For example, [2] showed that bounding the margin between two classifiers as a function of the corresponding category distance can achieve obvious performance increase. Inspired by this result, we propose to use more categorical information in the data corpus to further improve the classification performance. 
Actually, as we know, the path length in the taxonomy tree is totally based on the prior knowledge of the human editors. Theref ore it is not necessarily consistent with the real data distribution. This phenomenon is especially serious for those multi-label datasets. So, to further improve the performance, we should also leverage the document distribution of a category in the feature space. For this purpose, we propose an algorithm to integrate these two types of category relations, by using the principle of multi-objective programming (MOP). In particular, we embed the categories into a similarities defined both in the taxonomy tree and by the document/term distributions as much as possible. To get this embedding, we construct a two-objective optimization problem: one objective is to minimize the difference between the category distance in the new space and the corresponding path length in the taxonomy tree, while the other is to minimize the difference between the category similarity in the new space and in the original feature space. By solving this MOP problem, we eventually get a refined category distance ( RCD ) to improve existing hierarchical classifiers such as Hieron [2]. In addition, if the dimension of this embedded Euclidean space is equal to the dimension of term space, we can regard it as a translation of the categories in the original term space. Thus we actually derive a new vector space model (called refined text vectors ( RTV )), which can also help improve the hierarchical classifiers. Experiments on both synthetic and real-world datasets showed the effectiveness of the refined category distance and refined text vectors . algorithm is proposed. Then a trick for complexity reduction is discussed in Section 3. In Section 4, experimental results are presented to evaluate our algorithm. Concluding remarks and future work are discussed in the last section. 2.1 General Approach As mentioned in the introduction, there are two types of relations between categories relation is the hierarchy of categories, while the second is category-document and document-term relations. Our basic idea is to integrate these two types of category relations by using multi-objective optimization. In particular, we propose to embed the categories into a new space in which they preserve the similarities defined both by the path length in the taxonomy tree and by the document/term distributions in the feature space as much as possible. 
Mathematically, for any pair of nodes (representing categories) i and j in the taxonomy tree, let d ( i, j ) denote the path length (the number of edges in the path) from ( n*m ) matrix between categories and terms and let b ( i ) be the i -th row of B 1 . Suppose there is a ( k -dimension) Euclidean space, in which the representations of the k -dimension vector. Then our proposed algorithm can be written as in (1), where the first objective is to minimize the difference between the distance of categories calculated in the new Euclidean space and the path length in the taxonomy tree, while the second objective is to minimize the difference between the distance of categories calculated in the original feature space and the new Euclidean space. It is clear that this is a multi-objective programming (MOP) problem. Without loss of generality and for simplicity, we convert this MOP problem to a single-objective one by means of linear combination as follows. optimization algorithms [1] can hardly ha ndle such kind of large-scale problems because they need second-order informa tion in the optimization process, which corresponds to space complexity of O( n 2 k 2 ). To tackle this problem, we use a recently-proposed method, named Global Barzilai and Borwein (GBB) algorithm [9] refined category distance ( RCD ) matrix. This matrix can be used directly as the category distances in hierarchical classifier such as Hieron [2]. 2.2 Further Discussion As discussed in the above subsection, by solving (2), we embed the categories into a new ( k -dimension) Euclidean space and k is usually smaller than the dimension of the original term space. However, it may be interesting to discuss what will happen if k is equal to the dimension of the original term space ( m ). Actually, in such a special case, we can regard the new embedding space as just a translation of the categories in the original term space. And accordingly, we can come out another approach to improve hierarchical classifiers as follows. document belonging to category i in the original term space, and denote b ( i ) the row vector in B corresponding to category i . Then, we can refine the document vectors as follows, category by considering the information cont ained in the hierarchical taxonomy. After 
To sum up, we take RCD and RTV as two manners of information integration in hierarchical text classification. Both their effectiveness was tested in our experiments. Considering that many real hierarchical text corpora have tens of thousands of categories, the complexity is still very high even if we use the GBB algorithm. To matrix decomposition. Note that the following discussions are meaningful only if k  X  n . Otherwise, we assume that the complexity has not been high enough and the corresponding optimization problem can be solved efficiently already. 
First of all, we will conduct eigenvalue decomposition (EVD) for BB T . Actually if have the following approximation of BB T which has the same dimension with X. 
Then if we can make 2 || || kk F UX  X  X  sufficiently small, 3 we are able to guarantee that 2 || || TT F BB XX  X  is also very small due to the characteristics of eigenvalue decomposition. With this fact, we can simplify our second objective function from to be (SVD) of B as well, which can be much more efficiently computed than the EVD of BB T . One may find that the above singular value decomposition can actually be regarded as the spectral embedding of the category-term bipartite graph (See Fig.2). This implies some problem of our aforementioned method for computation reduction because the graph shown in Fig.2 sometimes is too sparse and even unconnected. In such a case, the SVD will not be as robust as we expect. To tackle this problem, we add a smoothing item to matrix B before conducting SVD, so as to improve the connectivity of its corresponding bipartite graph: where e = [1,1,..,1] T . Actually the same trick as above has been widely used in many other works such as PageRank [8] and so on. 4.1 Experiment Setting In this section, we present our experimental evaluation of the proposed algorithms. First of all, we will introduce the experimental settings. 
In our experiments, Hieron was used as the baseline for testing the effectiveness of margin hierarchical classifier, which enforces a margin among multiple categories. The basic optimization formulation of Hieron is as given in (8). label, dist ( y i ,  X  versions of Hieron were proposed. The first one simplified the distance between any two categories to 1 (denoted by Flat Hieron) and the second used the path length between two categories in the taxonomy tree (or the tree distance) as the category distance (denoted by Tree-Hieron). 
To evaluate the performance of our first method, we replaced the tree distance in the Tree-Hieron by the refined category distance and other elements remained the same as the standard Tree-Hieron classifier. Note that in this evaluation, we set k =1000 for the RCD method. And to evaluave our second method, we used the refined text vectors as the training input, and other elements remained the same as the dimension of the original term space. For the evaluation, we used both Micro-averaged F1 and Macro-averaged F1 4 (denoted by MicroF1 and MacroF1 in brief) as the metrics. 
In our experiments, both synthetic and real-world data sets were used. The synthetic datasets are very similar to that used in [2], which were generated as follows. First, a symmetric ternary tree of depth 4 was constructed as the taxonomy hierarchy. This hierarchy contains 121 vertices, each of which was assigned a base vector w u (where u represents a vertex). Then each example was generated by setting to a leaf node y , and  X  is a random vector sampled from the distribution N (0, 0.16). Furthermore, we  X  X isturbed X  the above synthetic dataset by randomly selecting 20 this strategy, we generated two synthetic datasets of different sizes. Each category in the first dataset (denoted by DS1) contains 10 training documents and 5 test documents, while that in the second datase t (denoted by DS2) contains 20 training documents and 10 test documents. For the real-world dataset, the 20NG [13] dataset was used. We randomly divided the documents in each category of the 20NG dataset average performance accordingly. As can be seen, for either the synthetic or the real-world data set, the number of categories is only tens or hundreds. Since this number is smaller than k , we actually did not apply the tricks described in Section 3. However, those deductions are surely meaningful for those who want to conduct experiments with much larger scales. 4.2 Experimental Results on the Synthetic Datasets In this subsection, we report the performance of our methods on the synthetic datasets. As can be seen in Fig.3, the curve for the RCD method is very smooth, parameter  X  . Without loss of generality, we set  X  = 0.5 in our further experiments. And comparatively speaking, the curve for RTV drops significantly when  X  is very close to 1. This is because it is not reason able to modify the original document vectors too much with the human-defined taxonomy tree which is very subjective and not data dependent. 
Further comparisons with the Hieron baselines are shown in Table 1. From this Hieron. And both RCD and RTV led to much higher classification performances. This improvement is consistent regardless of the size of the data set. 4.3 Experimental Results on the 20NG Dataset In this subsection, we report the experimental results on the 20NG dataset. 
From Fig.4 we can draw very similar conclusion to what we have got in Section 4.2. That is, the performance of the RCD method does not depend heavily on the accuracy. Furthermore, from the comparison listed in Table 2 we can see that the improvement of classification accuracy is even more significant as compared to that on the synthetic dataset. For example, the MicroF1 of flat Hieron and Tree-Hieron are only 0.78 and 0.83 respectively, while the MicroF1 of RTV is about 0.89 and the MicroF1 of RCD is even more than 0.91. 
Method Mean Flat Hieron 0.78130 0.00288 0.76593 0.00277 Tree-Hieron 0.83402 0.00192 0.81796 0.00248 RCD 0.91091 0.00038 0.90793 0.00053 
Besides, we have another interesting observation from Table 2: when we conducted our experiments for 10 times, the variances of the classification performance for different classifiers are quite different. As can be seen, our RCD and RTV methods performed stable with very small variances, while the variances of Flat Hieron and Tree-Hieron are much larger. Our explanation to this is as follows. Since we randomly sampled the training and test set, in some cases the tree distance used in Tree-Hieron (or the identical distance in Flat Hieron) may be consistent with the training data while in other cases it may be rather inconsistent. Comparatively speaking, by introducing our MOP formulation, we can better adapt to the real data distribution thus the corresponding classification becomes much more robust. 
To sum up, our experiments show that it is very benefitcial to leverage the infotmation contained in both the taxonomy tree and the data distriution, either in terms of classification performance, or in terms of the robustness of the classifiers. In this paper, we proposed an algorithm for the integration of heterogeneous information in the application of hierarchical text classification, which is based on multi-objective optimization. Experiments on both synthetic and real-world datasets showed that the proposed approach can improve both the classification performance and the robustness of the classifiers. For the future work, we plan to investigate whether the same idea can be used in other applications, such as the mining of click-through data, and the analysis of scientific citation graph. 
