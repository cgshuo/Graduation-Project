 Understanding temporal intents behind users X  queries is es-sential to meet users X  time-related information needs. In order to classify queries according to their temporal intent (e.g. Past or Future ), we explore the usage of time-series data derived from Wikipedia page views as a feature source. While existing works leverage either proprietary search en-gine query logs or highly processed and aggregated data (such as Google Trends) for this purpose, we investigate the utility of a freely available data source for this purpose. Our experiments on the NTCIR-12 Temporalia-2 dataset show, that Wikipedia pageview-based time-series data can signif-icantly improve the disambiguation of temporal intents for specific types of queries, in particular those without tempo-ral expressions present in the query string.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval Keywords: temporal intents; disambiguation
Understanding users X  temporal query intents is an impor-tant step to meet their time-related information needs [8]. A query X  X  temporal intent may be ambiguous though, as (in particular in Web search) queries often consist of 2-3 keywords only and users X  information needs may be multi-faceted.

This ambiguity can be analyzed on at least two levels, the semantic level and the temporal level. On the semantic level, the same query string may refer to di erent concepts with very di erent temporal intents. For example, a query  X  X ttack the movie X  issued by a user in May 2013 may ei-ther refer to (i) a 1956 film named Attack (in this case the temporal intent would be Past ), (ii) an  X  with respect to 2013  X  upcoming and already announced 2015 film named Attack ( Future intent) or, (iii) war movies in general ( Atem-poral intent). On the temporal level, ambiguity is often the result of queries referring to periodically occurring events. For example, the query  X  X BA playo  X  issued on May 1, 2013 c intuitively has two strong intents: Recency (the 2013 play-o s were running at the time and a user may be interested in this specific playo instance) and Atemporal (a user may be interested in the concept of NBA playo s in general).
Large-scale query logs o er a rich source of temporal sig-nals that may be useful to determine temporal intent [12, 8, 11, 7]. They are however proprietary. Although search engines release highly-processed and aggregated temporal often employed in research [1], we believe that a detailed investigation into the impact of time-series features on tem-poral query intent disambiguation is only possible with an openly accessible and large-scale data log. To this end, in this paper we employ the Wikipedia page view logs (contain-ing information on how often an article has been viewed dur-ing a time period) as a source of time-series based features. We propose a two-step temporal disambiguation approach which (1) extracts a set of concepts from a query string and expands this set with related concepts, and (2) derives time-series features of all concepts found in the previous step (based on the page views of that concept on Wikipedia) which are then fed into a machine learning framework. We explore the following research question:
RQ: Do features derived from an accessible time-series data source improve the disambiguation of users X  temporal intents?
Jones and Diaz [6] introduced the use of time-series data for the analysis of query ambiguity, exploring three types of temporal ambiguity: atemporal, unambiguous and am-biguous. Their experiments centered around a news corpus (with all documents being timestamped according to their creation date) and they exploited the change in term fre-quencies over time to generate time-series data for queries. Similarly, Radinsky et al. [10] leveraged time-series data gen-erated from the New York Times collection to measure the relatedness of text. While useful in some contexts, time-series data generated from document collections may not be suitable to disambiguate the temporal intents of the general Web search user.
 Query logs are a more suitable resource to disambiguate Web users X  queries and research in the temporality of query logs is ongoing: Kulkarni et al. [8] monitored one hundred queries over ten weeks to learn more about the dynamics of temporal queries, while Shokouhi [12] analyzed the monthly frequency of 259 queries (issued to the Bing search engine) https://www.google.com/trends/ between 2006 to 2010. Although query logs would o er the most direct evidence of users X  temporal intents, they are proprietary.

As a world-wide knowledge base which contains millions meaningful resource for understanding users X  queries. Whit-ing et al. [13] use topics in Wikipedia disambiguation pages to represent ambiguous queries &amp; various query intents and employed the time-series data from Wikipedia X  X  page view from these works and employ this data source for the specific task of temporal intent disambiguation.

Lastly, it should be pointed out that most prior works that make use of time-series data to understand query intents [6, 8, 12, 13, 7] focus on the overview dynamics, while our work attempts to disambiguate the temporal intents of each query at their particular issue time.
The approach we propose for temporal query intent dis-ambiguation is depicted in Figure 1. We consider two levels: the semantic and the temporal level. On the semantic level, employing Wikipedia concepts (a concept is operationalized as a Wikipedia page) to represent the variate intents of queries is a common choice [10, 13]. On the temporal level, each Wikipedia concept is linked to the page view statistics of its corresponding Wikipedia page. Based on the time-series data of Wikipedia concepts, several temporal features can be extracted for the estimation of query temporal in-tents. Our model is similar to and inspired by the model proposed in [10], the main di erence being that instead of relying on time-series data directly to compare entities, we extract features from the time-series data and incorporate the features into our machine learning framework. Based on features extracted from semantic level and temporal level of queries, machine learning models are trained and leveraged to predict the probabilities of temporal queries intents where queries may belong to.

Figure 1: Conceptual overview of our approach.
We extract two types of features from the queries: (i) content features, and, (ii) time-series based features. We discuss them in turn. https://www.wikipedia.org/ http://dumps.wikimedia.org/other/pagecounts-raw/
Our 209 query-content features (i.e. features that are not relying on any external data sources) are based on those proposed in [14]. We extract lemmas and named entities via the Stanford CoreNLP toolkit 4 . Since verb tenses can be good indicators of temporality, as [14] we represent the detected verbs in queries by their uppermost verb tense ( UVB_tense ) and verb tense with lemma ( tense_lemma ). For example, the query  X  X hen was television invented X  has three verb features UVB_VBD , VBD_be and VBD_invent . Temporal expressions (TEs) are extracted with the SUTime module of Stanford CoreNLP and the relation of the de-tected TEs and the query issue time (assumed to be known) are encoded in five features: SUTime produces high quality temporal annotations, but is not able to detect all TEs, especially if the surrounding textual evidence is weak or misleading (e.g. in the query  X  X hen to File 2014 Taxes X  5 SUTime does not tag  X 2014 X  as TE;  X 2014 X  is though detected as numerical lemma by Stanford CoreNLP ). The final three query-content features query issue time: As a concrete example, the query "NBA playo s 2012 2013" issued May 1, 2012 will result in the following non-zero fea-
The time-series based features and the evaluation of their impact on temporal query intent disambiguation are our main contribution in this work. We employ the TAGME toolkit [2] to detect Wikipedia concepts in our queries, as it has been shown to perform well for short texts. We de-rive 13 features per query, based on the Wikipedia concept relatedness score as computed by TAGME ):
Overall, for each query we generate 222 features based on query content and time-series data. http://stanfordnlp.github.io/CoreNLP/  X  20 years of query issue time. tion of a variable number of time-series features is a non-trivial task considered in future work.
We utilize the benchmark dataset published at the NTCIR-12 Temporalia-2 task [5] for the temporal intent disambigua-tion ( TID ) subtask. It consists of dry-run (93 queries in to-tal, 73 of those had their ground truth released for training purposes) and formal-run data (300 queries as test data). Each query has an assigned issue time. The ground truth for each query are the probabilities of the query falling into the four temporal intent classes: ( Past , Recency , Future , Atemporal ) . For example, the query  X  X  X emorial day X  (issue time May 1, 2013) has the following ground truth assign-is assigned to the Future category.

In addition, for the exploratory analysis of temporal query intents, we also rely on the formal-run data (300 queries) released in the previous X  year X  X  (NTCIR-11) edition of the benchmark [4]. The task that year was slightly easier: queries had to be classified into a single temporal category, instead of deriving a probability distribution. The task was called temporal query intent classification ( TQIC ), a label we em-ploy here as well to distinguish it from the TID task/data. Figure 2: Overview of highly related Wikipedia con-cepts in the TID query set.
 Concept detection : We first explored to what extent TAGME was able to detect concepts in our set of queries. All detected Wikipedia concepts are considered related concepts ( rW iKi ) while the concepts with relatedness scores  X  0 . (suggested in [2]) are considered as highly related concepts ( hrW iki ). The number of rW iKi and hrW iki of queries in the dry-run dataset and the formal-run dataset of TID are shown in Figure 2. Almost every query contains at least rW iki concept and more than 60% of the queries contain at least two such concepts. Highly-related concepts appear considerably fewer, about 16% of queries do not contain any hrW iki .

Prevalence of Temporal Expressions (TEs) : To ver-ify the intuition that queries with higher temporal ambigu-ity have fewer TEs, we explore the prevalence of TEs in our query sets. The results are shown in Table 1. In the older TQIC query set, more than 40% of queries contained at least organizers through crowdsourcing one TE (which in many cases makes it easier to classify tem-poral intent). In the newly released TID dataset however, less than 20% of the queries contain TEs, indicating the in-creased need for temporal features from other sources. It is also reported that the queries with explicit temporal expres-sions only represent about 1.5% of all queries [9]. Therefore, it is worth to generate temporal features from other sources.
Page view sparsity : We explore Wikipedia page view data as one such source. If, however, the page views of our detected Wikipedia concepts were too low, no sensible time-series data features could be generated. We define those concepts whose average (maximum) daily page views are fewer than 22 (186) as queries without su cient page views. The cuto values were derived from the 5 th percentiles of the average (maximum) daily page views across all detected concepts in our TID and TQIC datasets. In total, we find 7 queries in our datasets whose most related Wikipedia con-cepts (i.e. the concept to compute the time-series features from) has page views below these thresholds.
The TID taks is evaluated through: (i) the average cosine similarity between the ground truth temporal intent distri-bution and the predicted distribution, (ii) and the mean ab-solute error (MAE) which can be computed not only across all categories, but also separately for each temporal class.
Baseline: Our baseline [14] is the best performing ap-proach for the TQIC task 9 , relying on query content fea-tures alone (no time-series data). As the TQIC task di ers from TID (instead of predicting class labels, we now pre-dict probabilities), we employed Ridge regression instead of Logistic regression. The parameter settings are selected by 10-fold cross-validation on the TID training data.
In order to test the e ectiveness of time-series features (our main research question in this work), we extract them as described in Section 3.2. In contrast to the baseline, our model ( BrTS ) contains the additional time-series features we hypothesize will improve the temporal intent disambigua-tion.

We incorporate the time-series data in two ways: (i) we train a single model across all TID training queries, and (ii) we train two separate models, by splitting the TID training queries into two sets, according to whether or not they con-tain TEs. The results of the TID formal-run data are shown in Tables 2 and 3. The results indicate that time-series fea-tures improve the temporal disambiguation results of those queries that do not contain TEs, in particular for the cat-egories Future and Atemporal . In contrast, the time-series features hurt the predictions for queries that do already con-tain TEs, especially for the Recency category. Comparing the single vs. two-model setup indicates that the training of two separate models aids the accuracy of the prediction. Fi-nally, we also computed an aggregate run ( Aggr in Table 3), which employs the baseline approach for queries with TEs and the BrTS approach for those without (as this combina-tion performs best on the training data). This combination yields a significant improvement over the baseline across all queries, not just those missing temporal expressions.
To open avenues for future work, we conducted a qualita-tive failure analysis on the queries where the baseline per-formed much worse than our BrTS approach. We found that the BrTS performs worse when (i) the detected Wikipedia concepts cannot capture the whole meaning of the query well, or (ii) the Future category has the highest intent prob-ability and the page view log of the respective Wikipedia concepts contains multiple large irregular spikes (which lead to a larger probability of the Past category).

To summarize, these results suggest that 1) Time-series data can improve the disambiguation of queries with no TEs significantly, and 2) the disambiguation of users X  temporal intent behind queries should be processed separately based on whether they have TEs, which is consistent with the clas-sification of temporal queries in [7].
 Table 2: Overview of TID results (formal-run data). A single model was trained.  X  indicates a significant improvement ( p&lt; 0 . 05 ).
 Table 3: Overview of TID results (formal-run data). Two models were trained: one based on the train-ing queries containing TEs, and one based on the training queries not containing TEs.  X  indicates a significant improvement ( p&lt; 0 . 05 ).
In this work, we have presented our investigation into the use of time-series data, extracted from an openly accessi-ble data source as an approximation and a proxy for large-scale query log data to predict the temporal intents of search queries.

We have provided an analysis of the NTCIR TID dataset within the context of our goal (deriving time-series data from Wikipedia page views) and found the data sparsity not to be a significant issue. Our experiments show that time-series features derived from Wikipedia page views can aid the temporal intent prediction, if su cient care is taken to separate the easy queries (containing TAs) from the di cult ones during training.

Future work will explore additional mechanisms to incor-porate the time-series features of a wider range of Wikipedia concepts (instead of the query X  X  dominating one) and the eventual application of temporal intent prediction to the di-versification and clustering of search results.
