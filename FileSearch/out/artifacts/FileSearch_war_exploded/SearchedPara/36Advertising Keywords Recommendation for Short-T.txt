 GUI-RONG XUE, Aliyun.com In the last decade, online advertising has become a prominent economic force and the main income source for a variety of Web sites and services. According to Interactive Advertising Bureau X  X  (IAB 1 ) annual Internet advertising report, yearly advertising revenues have grown from US$6.01 billion in 2002 to US$26.04 billion in 2010 [IAB and PricewaterhouseCoopers 2011]. Some reports have already shown that online ad-vertising has overtaken TV to become the largest advertising medium in countries such as the UK [Sweney 2009]. Among the several existing online advertising chan-nels, search advertising, a search engine-based method of placing ads on Web pages, has become the driving force behind the large-scale monetization process of Web ser-vices through online marketing [Cristo et al. 2006].

Advertising keywords recommendation is an indispensable process of sponsored search and contextual advertising , which are the two main approaches of search ad-vertising. In sponsored search, the ads are displayed at the top or the right of the result pages of search engines, largely based on the degree of matching between the keywords of user queries and an advertiser X  X  bid keywords. The clicks on these ad links will take users to the Web pages of advertisers, generally called landing pages. A natural question for an advertiser is which keywords should be bid on for her land-ing pages? As users can express their search intent in a variety of different queries, it is almost impossible to conceive all the relevant keywords for the landing pages [Cristo et al. 2006]. Thus a sponsored search system, as an important service for the advertisers, provides recommendations of advertising keywords for the landing pages. In contextual advertising, on the other hand, it is also desirable to display relevant ads on the target Web pages [Anagnostopoulos et al. 2007]. This is mostly done by first extracting advertising keywords from the target Web pages and then retrieving the relevant ads using these advertising keywords. Figure 1 illustrates the keyword-based contextual advertising process. In summary, it is essential to accurately extract advertising keywords in order to display highly relevant ads, both in sponsored search and in contextual advertising.

Because of its importance, it is not surprising that a variety of approaches for ad-vertising keywords recommendation for Web pages have been proposed including the supervised learning-based algorithm proposed by Yih et al. [2006], the KEA system [Fang et al. 2005; Jones and Paynter 2001; Witten et al. 1999], and the unsupervised learning algorithm proposed by Matsuo [2003]. However, these existing advertising keywords recommendation algorithms largely rely on the textual content of a Web page itself despite of the fact that a substantial number of Web pages mainly contain multimedia contents such as images or videos. As an illustration, we collected statis-tics about the distribution of word count of the Web pages from the Open Directory Project 2 in Figure 2. From Figure 2, we find that 27.24% pages in ODP have less than 100 words, which are generally called short-text Web pages in this article. If we con-sider ODP as a reasonable representation of an essential part of the whole Web, we can conclude that a substantial proportion of pages on the Web contain very little textual contents.

It is not surprising that traditional advertising keywords recommendation algo-rithms do not work well on these short-text Web pages. We can single out two main reasons: Firstly, the short-text Web pages offer less textual information. They prob-ably contain very simple content structure and the content is poor, which makes it difficult for a recommendation system to rank the keywords well and thus leads to low accuracy. Secondly, in some cases, the situation is even worse; the short-text Web pages do not contain enough candidate terms or phrases.

A natural idea to overcome the preceding two issues is to enrich the set of advertis-ing keywords by introducing new advertising keywords which do not occur in, but are still relevant to, the target Web pages. There are two possibilities. The first is to sim-ply enrich the content of target Web pages and then use the enriched content to do the keywords recommendation work [Ribeiro-Neto et al. 2005]. The second one, which we develop in this article, requires to analyze the relationship between advertising key-words and then obtain new advertising keywords that are semantically relevant to the existing ones. So how can we identify the relationship between keywords? It turns out that Wikipedia 3 is an ideal resource to do that: It is a Web-based collaborative encyclo-pedia and contains, for example, more than 3 million entities in the English language. The entity articles cover a diverse set of topics in a large number of areas. And the number of its entities is still growing rapidly. Moreover, each entity is described by a relatively complete and concise article with hyperlinks linking to other Wikipedia entities indicating the semantic relationship between them. Therefore, they can be ex-ploited to obtain high-quality advertising keywords relationships for recommendation.
In this article, we propose a novel approach of advertising keywords recommenda-tion that makes use of entities and links from Wikipedia. As mentioned before, our approach can recommend advertising keywords that are highly relevant to the target Web page even if they do not occur in it. These keywords are called leveraged keywords in this article. Our approach makes use of Wikipedia entites as the dictionary to rec-ommend keywords. The usefulness of Wikipedia entities is evidenced by the fact that more than 99.8% of ODP Web pages contain one or more Wikipedia entities 4 . This high proportion makes Wikipedia a valuable thesaurus for Web pages.

Structurally, Wikipedia can also be viewed as a directed graph with vertices and edges corresponding to its entities and links among the entities, respectively. This allows us to generate the related advertising keywords by propagating the keywords on the graph using a Markov Random Walk. Specifically, we will use the algorithm of PageRank to implement the propagation process. Furthermore, inspired by topic-sensitive PageRank proposed in Haveliwala [2002], we introduce two kinds of bias, namely content bias and advertisement bias , into the propagation process, making it possible to recommend advertising keywords that are both relevant to a target Web page and valuable for advertising. In our experiments, we compare our algorithm to several baseline and state-of-the-art algorithms for advertising keywords recommen-dation. In particular, we focus on evaluating our algorithm on short-text Web pages. The result shows that our approach achieves substantial improvement over the su-pervised learning approach, measured by the precision of the top 20 recommended keywords, demonstrating the effectiveness of our algorithm.

The main contributions of our work are summarized as follows.  X  The problem of the keywords recommendation for short-text Web pages is emphasized.  X  A two-stage approach is proposed to solve the problem. In the first stage, candi-date keywords are extracted from the target Web page while in the second stage, related keywords to the target Web page are recommended, using a random walk-based algorithm applied to the Wikipedia graph. The experimental result shows a significant improvement on the recommendation performance for short-text Web pages.  X  To the best of our knowledge, our proposed content-and advertisement-sensitive PageRank is the first of its kind for multitopic-sensitive PageRank algorithms. The rest of this article is organized as follows. In Section 2, we discuss several re-lated works about search advertising, keywords recommendation, and application of Wikipedia. Some preliminary works are presented in Section 3. In Section 4, we present our approach using the content-and advertisement-sensitive PageRank on the Wikipedia graph. In Section 5, we describe the experiments and analyze the results. Finally, we present our conclusion and future work in Section 6. According to the Internet Advertising Revenue Report for the year 2010 [IAB and PricewaterhouseCoopers 2011], 45% of the total revenue from online advertising in the United States is contributed by search advertising, which continues to lead in the market and is followed by Display Banners (26%) and Classifieds (9%). Searching ad-vertising is a search engine-based approach of placing online ads on Web pages. It is an interesting subfield of Information Retrieval that involves large-scale search, content analysis, information extraction, statistical models, machine learning, and microeco-nomics. Described by IAB, the two main forms of search advertising are sponsored search and contextual advertising . 2.1.1. Sponsored Search. When a query is submitted to the search engine, two searches are performed. The first one is organic search which returns the Web pages with relevant content. The second one is sponsored search which returns the paid ads [Becker et al. 2009]. Sponsored search was introduced by Overture 5 in 1998 and now Google 6 offers the largest service. The search engine retrieves the ads of sponsors mainly by the keywords of the user query and displays them on the top or right of the search result pages. Generally, there are three forms of cost: Cost-Per-Click (CPC), Cost-Per-Mille (CPM), and Cost-Per-Action (CPA). The sponsored search system makes auctions on every keyword and the advertisers bid on some keywords for their ads. It is more likely for their ads to be ranked higher if the advertisers pay more for the impressions and clicks of their ads.

The ranking mechanism for sponsored search decides which ads retrieved should be ranked higher. In the work of Feng et al. [2003], two mainstream ranking mechanisms are compared: ranking by Willingness To Pay (WTP) and ranking by Willingness To Pay  X  Relevance (WTP  X  Rel). Through computational simulations, they found WTP  X 
Rel performs better in almost all cases, while WTP is better when the correlation between the relevance and WTP is large.

Besides the works on the ranking mechanism, more academic research focuses on the matching strategy for the improvement of the relevance between ads and user queries [Hillard et al. 2010; Broder et al. 2008; Raghavan and Hillard 2009]. Since the content of ads and user queries are both short, short content matching algorithms are used. Some query expansion-based work is presented in Section 2.2.2. Other work makes use of some external information such as the content and types of landing pages [Becker et al. 2009; Choi et al. 2010]. 2.1.2. Contextual Advertising. Contextual advertising, introduced by Google 7 in 2002, refers to the placement of ads on third-party Web pages based on the content of the target Web pages and the ads. The publishers and search engine will share some revenue once any ad on their Web pages is clicked. Some studies [Wang et al. 2002] have already shown that the relevance between the content of target Web pages and the ads makes a large difference in the click-through rate. Intuitively, the content of target Web pages suggests the users X  interest and if the ads are relevant to the Web page content, they are more likely to attract users. Therefore, the matching work of the target Web pages and the ads is the key point of contextual advertising.
Keyword-based approaches are widely used in contextual advertising. This kind of approach first extracts keywords from the target Web pages and then uses these key-words to retrieve the ads just like sponsored search. However, due to the vagary of key-words extraction and the lack of Web page content, keyword-based approaches always lead to irrelevant ads. The work of Ribeiro-Neto et al. [2005] is a typical keyword-based approach, which matches the ads with the target Web pages X  content and extracted keywords to get the winning strategy. Besides keyword-based approaches, the authors in Broder et al. [2007] make use of semantic information to enhance the matching work. They classify both pages and ads into a common taxonomy and merge the key-words matching work with the taxonomy matching work together to rank the ads. As analyzing the entire page content is costly and thus new or dynamically created Web pages could not be processed to match the ads ahead of time, the authors in Anagnostopoulos et al. [2007] proposed a summarization-based approach to enhance the efficiency of contextual advertising with an ignorable decrease in effectiveness. Generally speaking, keywords recommendation refers to finding the relevant keywords to a given target for some application. According to the type of the target, two major keywords recommendation problems are Web page keywords recommendation (also called keywords extraction) and related keywords recommendation. 2.2.1. Web Page Keywords Recommendation. As an important part of contextual adver-tising, Web page keywords recommendation is indispensable. In this process, valuable advertising keywords are automatically found to match the ads. It is obvious that the more accurate and valuable these found keywords are, the more relevant the ads will be delivered on the Web pages. Two kinds of approaches for Web page keywords rec-ommendation are widely used currently. One is supervised learning and the other is unsupervised learning.

In supervised learning, a set of example pages that have been labeled with keywords by human editors are given as the training data. Features of each word should be care-fully selected. There are several approaches, such as the traditional TF  X  IDF model, GenEx system [Turney 2000], the KEA system [Fang et al. 2005], and Yih X  X  et al. X  X  approach [2006]. GenEx system is one of the best known programs for keywords rec-ommendation. It is a rule-based approach with 12 tuned parameters and is well used for pure textual content such as journal articles and email messages. However, key-words recommendation for Web pages should be considered more. Web pages contain various content structure with all kinds of multimedia information. Features of the Web page should be taken into consideration to train the supervised learning model. The improved KEA [Turney 2003] and Yih et al. X  X  [2006] approach bring various Web-related features such as the metadata, URL, anchor, and so on. In the experiment, we select Yih et al. X  X  [2006] approach as the baseline keywords recommendation approach and more details will be presented in Section 4.3.1. In the work of Ravi et al. [2010], candidate bid phrases are generated through a translation model and then well-formed ones get ranked up by a language model.

The general idea of unsupervised learning is to create some appropriate formulas based on the features similar to supervised learning to score these candidate keywords. Those who have scores in top k are recommended as keywords. In the work of Litvak and Last [2008], the authors proposed to use the HITS algorithm [Kleinberg 1999] to get the importance of the blocks (words, phrases, sentences, etc.) in lexical or semantic graphs extracted from text documents. On the other hand, Matsuo proposes a new approach of unsupervised learning based on the co-occurrence information to optimize the recommendation result [Matsuo 2003]. Moreover, some approaches trying to enrich the target Web page content to improve the performance are also proposed, such as the work of Ribeiro-Neto et al. [2005].

Although these approaches are effective in the experiments and have been widely used, a problem remains that these approaches highly depend on rich structure or content of the target Web pages. Thus for the short-text Web pages, these approaches can hardly provide high performance. 2.2.2. Related Keywords Recommendation. Given a target keyword, related key-words recommendation refers to finding the relevant keywords, such as synonyms, semantically relevant phrases, and some rewrite from the target keyword. It has been widely used in the field of information retrieval and search advertising.
In the information retrieval field, query expansion or query substitution is an im-portant topic. The raw user queries will be processed to be substituted by one or a list of keywords to obtain better search results [Jones et al. 2006; Mitra et al. 1998]. Much has been done about query substitution. Boldi et al. [2008, 2009] make use of the user search session data to build a query flow graph and then use the random walk on the graph to get related queries. Besides the session data, the search engine click-through data is used for mining the similarity between queries in the work of Cao et al. [2008].
There are also many works about related keywords recommendation for a given key-word in search advertising. In search advertising, broad match helps indirectly match the user queries with the advertising bid keywords. Most of state-of-the-art matching algorithms expand the user query using a variety of external resources, such as Web search results [Abhishek and Hosanagar 2007; Joshi and Motwani 2006; Radlinski et al. 2008], page and ad click-through data [Antonellis et al. 2008], search sessions, taxonomy [Broder et al. 2009] or concept hierarchy [Chen et al. 2008]. In addition, Radlinski et al. [2008] consider more about the feasibility of matching ads and the search engine revenue. Wang et al. [2009a] improve the efficiency of query expansion for sponsored search by proposing a novel index structure and adapting a spreading activation mechanism. Wikipedia is a free online encyclopedia in which large set of concepts are well ex-pressed by experts and volunteers. It provides a considerable knowledge base, cov-ering areas such as art, history, society, and science. Wikipedia is considered as an ideal knowledge base for not only readers and researchers to look up knowledge but also for modern data mining systems to find auxiliary data to improve performance. Specifically, the articles of each Wikipedia entity contain a detailed explanation from various aspects. Moreover, the content of these articles is organized in well-structured format. This advantage can help automatic learning systems easier fetch information of entities. Furthermore, lots of links in the corpus of entities can imply a semantic re-lationship between linked entities, which can help automatic concept recognizers find related information.

Because of the diversity of content and the structured information [Medelyan et al. 2009], Wikipedia has attracted more and more researchers taking these advantages on the typical topics. Besides the application on keywords recommendation as introduced in this article, many improvements have been achieved in other areas. Sch  X  onhofen [2006] exploits the titles and categories of Wikipedia articles to identify the topics of documents. In the work of Carmel et al. [2009], Wikipedia is applied to enhance clus-ter labeling and the authors claim that using Wikipedia entities to label each cluster outperforms using keywords directly in the text. Wang et al. improve text classifica-tion by enriching the document with the entities of Wikipedia [Wang and Domeniconi 2008; Wang et al. 2009b]. Hu et al. map the target to a Wikipedia thesaurus and use the entity content and links to enhance the query intent identification [Hu et al. 2009] and text clustering [Hu et al. 2008]. Yu et al. evaluate ontology based on categories in Wikipedia [Yu et al. 2007]. Before we discuss our algorithm, we first introduce some basic materials to set the stage for further discussion.
 The PageRank algorithm [Brin and Page 1997; Brin et al. 1998; Page 1997] is a widely used algorithm to obtain the static quality of Web pages based on the link graph. The basic idea is to perform a random walk on the link graph and propagate the quality score of a Web page to the ones it is linked to. Formally, the PageRank can be charac-terized by the following iteration the m th iteration and r ( m ) i stands for the PageRank score of the Web page i . Moreover, B =[ 1 n ] n  X  1 is the damping vector and the decay factor  X  limits the effect of the propa-gation of PageRank. The matrix G represents the row-normalized adjacency matrix of the link graph, that is, if there is an edge from vertex i to vertex j ,then G j , i = 1 o o represents the out-degree of vertex i .

The topic-sensitive PageRank proposed in Haveliwala [2002] extends PageRank by allowing the iteration process to be biased to a specific topic. Specifically, the damping vector B is biased to the specific topic so that the Web pages of this particular topic are more likely to have high scores. The iterative scheme is the same as Eq. (1) except for the damping vector B =[ b 1 ,  X  X  X  , b n ] T with the damping value b i indicating the relevance of page i to the topic in question. It is easy to see that the propagation process is biased by the damping vector B at each iteration. Consequently, pages with higher damping values can propagate higher scores to their neighbors in the link graph. In this section, we consider the construction of the Wikipedia graph. Firstly, we take each entity as a vertex in the graph. Then we aim at connecting two entities with an edge of the graph if they are semantically related. To this end, we notice that there are many hyperlinks in each Wikipedia article linking to the pages of other articles of Wikipedia entities. These hyperlinks are just potential edges for the graph we want to construct, because many Wikipedia articles link to (other articles about) dates and regions or other entities that are general but otherwise semantically unrelated. To address this issue, we make use of the entity category information. Specifically, we re-move the edges between the entities of different first-level categories in the Wikipedia category tree. This way, there are only edges linking the entities in the same topic. Therefore, the Wikipedia graph reduces to  X  subgraphs, where  X  stands for the num-ber of first-level categories, specifically  X  = 10 for a Wikipedia version in January 2010. Additionally, we also consider weighting the edges by the number of links between two entities. This is reasonable because more edges from entity i to entity j means en-tity j is more related than other neighbors of entity i . Therefore, given the whole set of Wikipedia articles, we can construct a directed graph, which is called Wikipedia Graph . In the rest of the article, we will denote the row-normalized Wikipedia graph link matrix as G ,whichisan n  X  n matrix where n stands for the size of the entity set. Element G i , j of the matrix is given by where  X  ( j , i ) denotes the edge number from entity j to entity i . Figure 3 gives an illustration of a Wikipedia subgraph.
 The problem we need to address is: Given a target Web page p as the input, partic-ularly, one with short-text content. The output should be k ranked keywords for p , required to be both relevant to the content of p and valuable for advertising. Here we describe the details of our algorithm of content-and advertisement-sensitive PageRank on the Wikipedia graph.

As our problem focuses on short-text Web page, the textual information of these pages themselves tends to be very limited. This information, however, is still an indis-pensable part for the proposed algorithm. Specifically, the textual content of p is first parsed and the Wikipedia entities occurring in p are identified with the same approach in Wang and Domeniconi [2008]. Then those entities are scored using traditional ap-proaches such as, in our experiment, the supervised learning approach similar to Yih X  X  work [Yih et al. 2006]. The score, defined as content relevant score , indicates the rele-vance of the entity to the content of p and measures the possibility of the entity to be a keyword of p .

Secondly, the output keywords of our algorithm should be valuable for advertis-ing. Intuitively, the frequency of an entity name in some advertisement content can be a criterion to measure its advertising value. Therefore, another score on each entity is given, which we term as advertisement relevant score , defined as the percentage of oc-currences of each entity name in a given set of ads. This score measures how likely the entity will occur in advertisement texts and can be used to retrieve the corresponding advertisement. In addition, this score is not based on the input target Web page, and thus can be calculated offline. As a consequence, every entity has two topic scores: the content relevant score and the advertisement relevant score.
 Thirdly, we use the two scores in the content-and advertisement-sensitive PageRank iterations. In this process, the entities that are related to the content of target Web pages and valuable for advertising will get higher score because of the two scores on content and advertisement bias. The neighbors of these entities will also get a higher score because of the propagation process of the PageRank score.

Finally, we rank and choose k entities with the highest content-and advertisement-sensitive PageRank score as the output of our algorithm. The overall steps of the algorithm are depicted in Figure 4.

We next describe the details of the content relevant score and the advertisement relevant score in Section 4.3. Utilizing the two scores as the damping vectors, the work of content and advertisement PageRank is described in Section 4.5. Considering that the recommended keywords should be both relevant to the tar-get Web pages and valuable for advertising, we define two factors for each entity, corresponding to two types of damping vectors in the expression of topic-sensitive PageRank. 4.3.1. Content Damping Vector Setup. Given the target Web page p , for each entity i , its content damping value c i stands for the relevance between entity i and the content of p .

We used a supervised learning approach to score each entity and determine whether an entity has a score high enough to be treated as a keyword. In our approach, we im-plement the approach of Yih et al. [2006] to generate the feature vector for each entity in p . Firstly, p is parsed and Wikipedia entities in p are extracted. Then these entities in p are scored by a regression approach. The regression approach is implemented by a Support Vector Machine , which has been trained with a large set of Web pages with keywords extracted by human editors. The entity with a higher score implies that it is more important to p . The features we selected for regression are listed in Table I.
The last feature in the list, QueryLog, which is proposed in Yih et al. X  X  work [2006], appears to be a novel feature to help improve the performance of supervised learning. It is claimed that the entities that occur in the user query are more likely to be key-words since the users use them in the search engine to retrieve some pages they want. In our experiment, we use the query log from AOL 8 .

These features are of different importance, for example, an entity appearing in the title is more likely to be a keyword than an entity just appearing somewhere in the content body. Therefore, we weight those features and tune the weight to obtain the best performance. 4.3.2. Advertising Damping Vector Setup. Using advertisement-sensitive PageRank, we can obtain more commercial entities which are suitable for advertising keywords. The entities which are more relevant to the advertisement topic should have higher adver-tisement damping value. In our approach, we record the frequency of each Wikipedia entity i in the text of an advertisement set, defined as a i . For the calculation of the PageRank iteration, we let a i be the damping value biased to the advertisement for each entity i . Based on the topic-sensitive PageRank and the two topic relevant scores, we can con-struct two topic-sensitive PageRanks: content-sensitive PageRank and advertisement-sensitive PageRank . Given the content relevant score c i for each entity i to the target Web page p , we can obtain the content damping vector C . The iteration formula of content-sensitive PageRank is m th iteration and the parameter  X  controls the impact of the content relevant score to the content-sensitive PageRank value 9 . Similar to topic-sensitive PageRank on Web pages, this process can propagate the relevance scores from the seed entities with high content relevant scores to other relevant entities even if they do not occur in the target Web page. As a result, we can enrich the keywords set of the target Web page with the help of content-sensitive PageRank.

Similarly, given the advertisement relevant score a i for each entity i , we can obtain the advertisement damping vector A . The iteration formula of advertisement-sensitive PageRank is where  X  is the parameter that controls the impact of the advertisement relevant score to the advertisement-sensitive PageRank value. Likewise, the entities that are related to the advertisement topic are more likely to get higher scores after every iteration by using the impact of advertisement damping vector A . In addition, neighboring entities can share those scores which may also be relevant to the advertisement. Thus, the advertisement bias is incorporated into the random walk process. As we emphasized before, the recommended keywords should be both relevant to the target Web page and valuable for advertising. These two requirements can be satisfied by the two topic-sensitive PageRanks introduced earlier. We propose an approach that combines the two different topic-sensitive PageRanks to simultaneously address the preceding two requirements. The iteration formula combining the two topic-sensitive PageRanks is In the previous equation, there are two damping vectors: C , biased to the target Web page content, and A , biased to the advertisement topic. Intuitively, in each iteration, the PageRank of an entity that is both relevant to the target Web page content and the advertisement topic will get a relatively high score boost by the factor  X  c i +  X  a i ,and each entity will also distribute its score to its neighbors. Therefore, entities that are relevant to both the content of the target Web page and the advertisement topic can obtain higher scores after the convergence of the iteration process. 4.6.1. Initial PageRank Vector Setup. As is well-known the convergence of PageRank will not depend on the initial value for each entity (the element of the start vector R 0 ). The number of iterations and thus execution time, however, are greatly affected by the initial values. Thus an appropriately chosen initial value for each entity will improve the efficiency of the PageRank iteration process.
 We propose to set the initial vector as where  X  and  X  are the same parameters in Eq. (5) while C and A have been determined in Section 4.3.1 and Section 4.3.2. In our experiment, it takes about 25% less time to get convergence, compared with setting the initial vector with the uniform value. 4.6.2. Computational Complexity. In this section, we discuss the computational com-plexity of content-and advertisement-sensitive PageRank. The computation process (1  X   X   X   X  ) G  X  R category edges when constructing the Wikipedia graph, we need not traverse all the entities in each iteration process; instead we just traverse the subgraphs in the cat-egories of the seed entities. Therefore, in each iteration, O ( n ) vertices are involved, where n = n / X  stands for the average number of entities in each subgraph, and  X  has been mentioned in Section 3.2. Furthermore, the Wikipedia graph is a sparse graph with each entity having on average 18.3 in-edges, thus G is a sparse matrix where each row has on average no more than 18.3 nonzero numbers. Taking advantage of this, we only record the in-edge neighbors for each entity and thus the calculation process re-duces to an O ( n + n +18 . 3 n )time.

Given the initial PageRank vector, the number of iterations is still dependent on the two parameters  X  and  X  . Generally, the larger (1  X   X   X   X  ) is, the more slowly the convergence is. In our experiments, six iterations to reduce the fluctuation of 10  X  5 . In our experiment, the average real runtime for each test case is 8.26 seconds (Java Platform, Windows, 2GB memory, 2.6 GHz). Furthermore, the efficiency can be improved with the optimization work discussed in the next subsection. 4.6.3. Optimization for Efficiency Improvement. Several optimizations can be incorporated for accelerating the computation. In this subsection, we discuss the optimization work which has been implemented in our experiment.

In PageRank calculation, it is time consuming for updating the PageRank value of the whole entity set. For our model, the number of entities with nonzero initial value is extremely low (20, in our experiment). Thus in each calculation, those entities with zero value and without nonzero neighbors can be ignored. We just consider the involved entity set, in which the entity PageRank value will be changed. Before the i th iteration of the computation, define the set of entities with nonzero PageRank score as S  X  1 . It is clear that in this step of computation, only the entities in S i  X  1 or the ones with an in-edge from S i  X  1 will have a nonzero entry. Therefore, in the i th iteration of the computation, we only consider S i in the computation instead of all the entities in the WikiGraph. In our experiment, the average real runtime for each test case is reduced to 2.95 seconds with this optimization. This section mainly discusses the experimental results for our proposed algorithm, including the data preparation, comparisons with existing algorithms, evaluation metrics, performance results, and further discussions. 5.1.1. Wikipedia Graph. DBpedia 10 , according to its description, is a community effort to extract structured information from Wikipedia and to make this information avail-able to the general public. The structured information can be downloaded from its Web site.

In our experiment, we take the Pagelinks dataset with the date of September 2009. It has 81.83 million triples and each triple represents a relation where the first entity has a page link to the second entity, resulting an initial graph with 81.83 million directed edges and 9.54 million entities. We refine the graph by removing the entities without out-edges and the ones which have some characters with ASCII &gt; 127. As mentioned in Section 3.2, we also removed the cross-category edges with the help of the Article Categories and Categories(Skos) dataset on DBpedia. With this preprocess-ing, the entity number reduces to 3.12 million and the edge number 57.29 million, as is shown in Table II. 5.1.2. Advertisement Set. We use a set of 9 million textual ads, which are crawled by a commercial search engine using the AOL query log data. Each advertisement consists of the title and the description of the ads, which contain up to 10 and 30 words, respectively. Both of the two sections of the ads are composed by advertisers. 5.1.3. Training Data. We use 6422 human-labeled Web pages as our training pages. We extract the Wikipedia entities from the training pages and construct the vector for each entity with the feature described in Table I. Since those entities have been labeled, we can use these vectors with labels to train a classifier. 5.1.4. Test Data. We use the corpus of ODP Web pages as test pages in our ex-periments. The number of sampled test pages should be determined so that the ex-periments can convincingly demonstrate the difference of performance among the algorithms compared. In addition, the test pages should cover more topics so as to be representative. In our experiments, we generated two test datasets. The first dataset consists of 100 randomly selected short-text 11 ODP Web pages, called the short-text Web page set . The second one consists 103 Web pages, with 50 in short text and the other 53 in moderate or long text. We call the second test dataset the overall Web page set . These Web pages cover the topics of business, agriculture, art, computers, entertainment, automobiles, sports, Internet, life products, medicine, music stars, and so on. To demonstrate the effectiveness of our approach, we use some other approaches as the baseline or control in our experiments. All the approaches are listed next, including our proposed approaches. (1) TF Counting (TF). We simply take out the k entities with the highest frequency in (2) Supervised Learning (SL). By training an SVM with the training pages labeled by (3) Co-occurrence in Ads (CA). We use a process to mine the co-occurrence of two enti-(4) Query Click-through Bipartite Gragh (QCBG). Here we implement one traditional (5) Content-sensitive PageRank (CPR). We have proposed this approach in Section 4.4. (6) Content-and Advertisement-sensitive PageRank (CAPR). We have proposed this The input of the experiment is a target Web page p and the output is k keywords to p . For the gold-standard of the evaluation work, we invited five colleagues to judge the relevance of each page-keyword pair as follows.  X  Relevant and advertisable. The keyword is relevant to the content of the target page and also it has a possibility to be valuable for advertising, scored as 1.  X  Otherwise. The keyword is not considered as relevant to the content of the target page or it is impossible to be used as an advertising keyword, scored as 0. Each page-keyword pair has at least two human judges. After the judgment work, we average the scores for each page-keyword pair. The scores can be interpreted as the possibility of relevance and advertisability 12 . Then we evaluate the performance of the algorithms using the evaluation measure described next.

In our experimental studies, we use P@n as the evaluation measure. Precision at position n (P@n) is defined to be the fraction of the top-n retrieved keywords that are relevant [Baeza-Yated and Ribeiro-Neto 2008].
 In Eq. (7),  X  i denotes the average rate score for the pair of the target Web page and the i th recommended keyword. Since we not only accept the good keywords occurring in p , but also the related ones, there is no good measure to evaluate the recall of each approach. Our experiments are divided into five parts. In the first part, we normally do the keywords recommendation with four approaches TF, SL, CPR, and CAPR. The output keywords can be both in-page keywords and leveraged keywords, called universal key-words . In the second part, we focus on the leveraged keywords recommendation (with-out in-page keywords in the result), using approaches implemented from algorithms CA, QCBG, CPR, and CAPR. The third part reports the impact of the parameters of CAPR. In the fourth part, we present a case study to analyze the ability of CAPR for dealing with the ambiguity of Wikipedia entities. In the last part, we demonstrate the recommendation performance against the word number of the target Web pages. 5.4.1. Universal Keywords Recommendation. Universal keywords recommendation judges the practical effectiveness of each algorithm, where both in-page and leveraged keywords are recommended. For every target Web page p , each approach provides a re-sult of 20 keywords for p . We demonstrate the overall precision of the four approaches in top 5, top 10, top 15, and top 20 keywords recommended. Here we compare the results of the four approaches: TF, SL, CPR, and CAPR.

First we use the short-text Web page set for testing. The performance of the four approaches is shown in Table III and Figure 5.

Several observations are interesting to note: (i) CAPR shows significant improve-ment over other approaches on the short-text Web page set. Compared with SL, CAPR has an improvement of 37.09%, 71.82%, 79.15%, and 104.94% on top 5, 10, 15, and 20 respectively. It verifies that combining content-and advertisement-sensitive PageR-ank helps to find leveraged keywords which are more relevant to the target Web page than in-page keywords. (ii) Somewhat unexpectedly, the traditional supervised learn-ing approach SL is no more effective than the TF counting approach, which is very simple and considered to be the baseline in the experiments. The possible reason is that SL over-emphasizes the page content and structure, which is sparse and very diverse in short-text Web pages. (iii) For most short-text target Web pages, the tra-ditional approaches (TF, SL) could not even provide more than 15 keywords and the precision of the keywords is not so good, less than 45%. This reveals the problem of traditional keywords recommendation on short-text Web pages. In Section 5.4.5, we will give a panoramic view of the performance of these approaches against the size of target Web page. (iv) Compared with CPR, CAPR has an improvement of 0.76%, 2.81%, 5.90%, and 12.32% on top 5, 10, 15, and 20 respectively, which verifies the effectiveness of incorporating advertisement bias in CAPR.

Besides the short Web pages, we also demonstrate the performance of these ap-proaches on overall Web pages set, not just short-text Web pages. The performance is shown in Table IV and Figure 6.

As is shown in Figure 6, CAPR performs the best in all the comparisons and has an improvement of 5.30%, 14.29%, 20.18%, and 29.92% on top 5, 10, 15, and 20 over SL respectively. It proves that CAPR also works well on the long-text Web pages even though the improvement is not so significant as on short-text Web pages and adver-tisement bias also helps improve the performance. Moreover, SL performs better than the baseline TF, which indicates that SL works well mainly on long-text Web pages. Compared with those on short-text Web pages, the performance of each algorithm has lower standard error, shown as error bars.

Since CAPR recommends both in-page keywords and leveraged keywords, we also did an analysis of the in-page keywords proportion in the recommendation of CAPR, as is shown in Table V and Figure 7. From the result we have following observations: (i) As the number of recommended keyword increases, the in-page keyword propor-tion decreases. For each target Web page, the in-page keyword number has an upper limit. Thus after the most suitable in-page keywords are recommended, more and more leveraged keywords are more likely to be recommended than the remaining in-page keywords. (ii) On overall Web pages set, the in-page proportion is smaller than that in the overall Web pages set. This is also reasonable because there are fewer in-page keywords in the short-text Web pages and leveraged keywords are more likely to occur in the result.

As a case study of the comparison for the approaches of TF, SL, and CAPR, we here provide their performance on a specific short-text test Web page case of ION Media Networks 13 . The performance is shown in Table VI. From the performance result, we can know that TF hits 6 keywords, SL hits 6 keywords, and CAPR hits up to 11 keywords in the top 20, which dominates in the case study. 5.4.2. Leveraged Keywords Recommendation. Here we investigate the performance only on the recommended leveraged keywords. Among the compared algorithms, CA, QCBG, CPR, and CAPR have the ability to recommend leveraged keywords of the tar-get Web pages, simply by filtering out the in-page keywords in the results. Similar to Section 5.4.1, we first demonstrate the precision of the four approaches on top 5, 10, 15, and 20 keywords recommendation on the short-text Web page set. The performance of those approaches is shown in Table VII and Figure 8.

From the results, we can know that: (i) CAPR gives a high performance in the work of leveraged keywords recommendation. It has an improvement of 16.52%, 32.18%, 50.46%, and 53.69% on top 5, 10, 15, and 20 against QCBG. Its performance of lever-aged keywords precision could even compare to the universal keywords recommenda-tion performance. (ii) On the other hand, CA is not as suitable for related advertising keywords recommendation. For one reason, the short-text Web page makes it much more difficult to determine the topic or the keywords from which to get the leveraged keywords; for another reason, there is so much noise in the advertisement text, such as misleading and ambiguous expressions in the advertisement text and the unbalance of the entities X  frequency in the advertisement set, which misguides the relation of two entities in the mining process of co-occurrence in ads. (iii) QCBG performs a little worse than CAPR on the top 5 results. However, as the recommended keyword number increases, the performance gap between these two algorithms increases significantly. This is because the user queries are highly diverse and thus query expansion will possibly import some irrelevant keywords. (iv) Furthermore, the result also demon-strates that advertisement bias helps improve the performance of leveraged keywords by 0.53%, 3.82%, 5.59%, and 11.43% on top 5, 10, 15, and 20. From the performance figures, we can conclude that the ability to find leveraged keywords is very important in the field of advertising keywords recommendation, especially on the short-text Web page set.
 We also make a universal keywords recommendation for comparison on an overall Web page set, not just short-text Web pages. The performance is shown in Table VIII and Figure 9.

As is shown in Table VIII, CAPR dominates in each comparison and has an im-provement of 36.67%, 71.10%, 77.69%, and 71.34% on top 5, 10, 15, and 20 over QCBG respectively. From the result we can know CAPR still works well on the long Web pages and advertisement bias also helps improve the performance by 1.21%, 10.14%, 16.96%, and 13.99% on top 5, 10, 15, and 20 respectively. Thus it verifies that CAPR is the most competent to recommend leveraged keywords to Web pages. 5.4.3. Parameter Tuning. As is shown in Eq. (5), there are two parameters,  X  and  X  ,in CAPR that need to be tuned. In this section, we focus on the impact of the parameters on the performance of the algorithm.

In our experiment, the two parameters are tuned separately. First, we fix beta to 5 . 0  X  10  X  5 and tune  X  from 0.30 to 0.98 (some preliminary experiments have shown that this area produces the best result). Secondly, we fix  X  to 0.9, which is sensitive to our Wikipedia graph, and tune  X  from 1 . 0  X  10  X  5 to 1 . 0  X  10  X  3 . The precision on the top 20 recommended keywords can be depicted against  X  and  X  ,whichisshownin Figure 10.

From Figure 10, it can be noted that there is a trade-off between the weight of content bias and the propagation of PageRank by tuning  X  and when  X  is larger than 2 . 0  X  10  X  5 , the performance reduces as  X  increases. Finally, we set  X  =0 . 85 and  X  =1 . 5  X  10  X  5 to run CAPR. It is necessary to point out that  X  and  X  here have already been combined with the normalization factor of the respective damping vectors, which explains the gap between the orders of magnitude  X  and  X  .
 5.4.4. Noise Impact and Ambiguity Analysis. As has been discussed in Section 4.2, the seed entities of the propagation step are given by traditional recommendation approaches. Will the propagation step of CAPR amplify the errors made by these approaches if there are some irrelevant keywords, called noise , in the seed entity list? Moreover, in the application of Wikipedia, one common problem is the ambigu-ity of entity names. In the area of keywords recommendation, this problem still ex-ists. For example, if a term Apple occurs in the target Web page, there is a question as to whether it means the fruit or the Apple brand 14 . In Wikipedia, the term Apple will directly be matched to the one under the fruit category. So if the term Apple in the page actually means the brand of Apple, the ambiguity problem exists.

However, from the preceding experiments, it is found that the noise and ambiguity problems do not seem to sinificantly reduce the performance of CAPR. We claim that topic-sensitive PageRank can reduce the impact of noise and ambiguity of Wikipedia entities. Here we provide a preliminary analysis. Given some seed entities, several of which may be noisey or ambiguous and can match different Wikipeida categories. Since we have removed the cross-category edges in the graph, the ambiguous enti-ties cannot distribute much PageRank value to their neighbors unless the ambiguous entities outnumber the entities under the correct first-level category. For a concise example, for a target Web page about the electronic product of Apple, we are given three seed entities: Apple, IPod, and IPhone , where Apple , as is mentioned before, is a noise in the seed entities. We present the top 20 recommended keywords from CAPR in Table IX.
From Table IX we can see that 17 keywords are in the topic of Electronic Products, 2 keywords are Wikipedia noise entities, and only Apple belongs to the fruit topic. This example verifies the disambiguating ability of content-and advertisement-sensitive PageRank. 5.4.5. Performance against Target Web Page Content Size. In the last part of our experi-ment, we analyze the performance against the content size of target Web pages. We make a comparison on TF, SL, and CAPR on the page size domain of 1 to 15104. Specif-ically, we divide the pages into 4 groups by their word numbers, the intervals of which are 1  X  99, 100  X  399, 400  X  799, and 800+. The performance on the top 10 keywords recommended of three algorithms on each group of pages is in Table X and Figure 11.
From the result we observe the following. (i) On average, the performance of key-words recommendation improves as the content size of the target Web page increases while when the page content size is especially small, for example, less than 100, the traditional approaches (TF and SL) for keywords recommendation do not work well (less than 40% in our experiment). (ii) Our approach CAPR still works well on the short-text pages (55.01% in our experiment), making an improvement of 91.30% on SL. (iii) As the content size increases, traditional approaches show large variation (SL varies from 37.50% to 67.86%, increasing by 80.96%) but our algorithm indicates more robustness and less sensitivity to the content size of the target Web pages (from 55.01% to 73.57%, increasing by 33.74%). Based on the five parts of the experiment, we claim that content-and advertisement-sensitive PageRank works well for advertising keywords recommendation. It provides a significant improvement over several state-of-the-art approaches on short-text Web pages. For short-text target Web pages, the keywords recommended by traditional ap-proaches are always with noise. However, in the propagation step of our approach, the noise is reduced and the keywords in the main topics get higher ranks due to the propagation of the content-and advertisement-sensitive PageRank score. In addition, leveraged keywords which do not occur in but are still relevant to the target Web page are also recommended. As a result, the problems of poor content, simple structure, and lack of candidate keywords for short-text Web pages are solved. Traditional approaches depending on the abundance of textual information in the tar-get Web pages do not work well in the context of short-text Web pages. To address this important problem, we propose a novel approach using content-and advertisement-sensitive PageRank on the Wikipedia graph. In the experiment, our approach yields a high improvement over traditional approaches in the precision of top 20 keywords on short-text target Web pages. It verifies that content-and advertisement-sensitive PageRank is an effective approach to advertising keywords recommendation on short-text Web pages.

In the future work, we plan to refine the Wikipedia graph, such as refining the edges to be more precise to identify the semantic similarity between two entities. For the efficiency improvement of our system, we will implement parallelization. As the WikiGraph has been divided into  X  subgraphs based on the category, the computa-tion on these subgraphs can be done in parallel. Furthermore, more parallelism can be achieved using more sophisticated technology of distributed computation for each subgraph. For other applications, we can change the PageRank bias into user profile information and implement our algorithm in personalized search. In addition, our algorithm can be adapted to product recommendation on a Web store with the inter-linked product pages.

