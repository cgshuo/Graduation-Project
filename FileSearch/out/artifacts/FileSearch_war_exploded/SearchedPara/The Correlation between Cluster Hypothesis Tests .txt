 We present a study of the correlation between the extent to which the cluster hypothesis holds, as measured by various tests, and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval. We show that the correlation can be affected by several factors, such as the size of the result list of the most highly ranked docu-ments that is analyzed. We further show that some cluster hypothesis tests are often negatively correlated with one an-other. Moreover, in several settings, some of the tests are also negatively correlated with the relative effectiveness of cluster-based retrieval.

The cluster hypothesis states that X  X losely associated doc-uments tend to be relevant to the same requests X  [19]. The hypothesis plays a central role in information retrieval. Var-ious tests were devised for estimating the extent to which the hypothesis holds [5, 20, 3, 17]. Furthermore, inspired by the hypothesis, document retrieval methods that utilize document clusters were proposed (e.g., [10, 11, 6, 7, 15]).
There are, however, only a few reports regarding the cor-relation between the cluster hypothesis tests and the rela-tive effectiveness of cluster-based retrieval with respect to document-based retrieval [20, 3, 13]. Some of these are con-tradictory: while it was initially argued that Voorhees X  near-est neighbor cluster hypothesis test is not correlated with retrieval effectiveness [20], it was later shown that this test is actually a good indicator for the effectiveness of a specific cluster-based retrieval method [13].

The aforementioned reports focused on a single cluster hypothesis test (the nearest neighbor test), used a specific retrieval method which is not state-of-the-art and were eval-uated using small documents collections which were mostly composed of news articles. Here, we analyze the correla-tion between cluster hypothesis tests and the relative effec-tiveness of cluster-based retrieval with respect to document-based retrieval using a variety of tests, state-of-the-art re-trieval methods and collections.

We found that (i) in contrast to some previously reported results [3], cluster hypothesis tests are in many cases either negatively correlated with one another or not correlated at all; (ii) cluster hypothesis tests are often negatively corre-lated or not correlated at all with the relative effectiveness of cluster-based retrieval methods; (iii) the correlation between the tests and the relative effectiveness of the retrieval meth-ods is affected by the number of documents in the result list of top-retrieved documents that is analyzed; and, (iv) the type of the collection (i.e., Web vs. newswire) is a strong in-dicator for the effectiveness of cluster-based retrieval when applied over short retrieved document lists.
The correlation between cluster hypothesis tests was stud-ied using small document collections, most of which were composed of news articles [3]. We, on the other hand, use a variety of both (small scale) newswire and (large scale) Web collections. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval methods was studied using only a single test  X  Voorhees X  nearest neigh-bor test [20, 13]. Each study also focused on a different cluster-based retrieval method. This resulted in contradic-tory findings. In contrast, we use several cluster hypothesis tests and retrieval methods.

Document clusters can be created either in a query depen-dent manner, i.e., from the list of documents most highly ranked in response to a query [21] or in a query independent fashion from all the documents in a collection [5, 10]. In this paper we study the correlation between cluster hypothesis tests and the effectiveness of retrieval methods that utilize query dependent clusters [6, 7, 15]. The reason is threefold. First, these retrieval methods were shown to be highly effec-tive. Second, we use for experiments large-scale document collections; clustering all the documents in these collections is computationally difficult. Third, the cluster hypothesis was shown to hold to a (much) larger extent when applied to relatively short retrieved lists than to longer ones or even to the entire corpus [18].
To study the correlation between tests measuring the ex-tent to which the cluster hypothesis holds and the effective-ness of cluster-based retrieval methods, we use several tests an d (state-of-the-art) retrieval methods.

Let D init be an initial list of n documents retrieved in response to query q using some retrieval method. The re-trieval method scores document d by score ( q, d ). (Details of the scoring function used in our experiments are provided in Section 4.) All the cluster hypothesis tests and the re-trieval methods that we consider operate on the documents in D init . In what follows we provide a short description of these tests and methods.
 ceptually represents the Overlap test [5]. The test is based on the premise that, on average, the similarity between two relevant documents should be higher than the similarity be-tween a relevant and a non-relevant document. Formally, let R ( D init ) be the set of relevant documents in D init N ( D init ) the set of non-relevant documents; n R and n note the number of documents in R ( D init ) and N ( D init spectively. The score assigned by the Overlap test to D init an inter-text similarity measure described in Section 4. 1 score is averaged over all the tested queries for which n n N are greater than 1 to produce the final test score.
We next consider Voorhees X  Nearest Neighbor test ( NN ) [20]. For each relevant document d i (  X  R ( D init )) we count the number of relevant documents among d i  X  X  k  X  1 nearest neighbors in D init ; k is a free parameter. These counts are averaged over all the relevant documents retrieved for all the tested queries. The nearest neighbors of d i are determined based on sim ( d i , d j ).

The Density test [3] is defined here as the ratio between the average number of unique terms in the documents in D init and the number of terms in the vocabulary. The un-derlying assumption is, as for the tests from above, that rel-evant documents are more similar to each other than they are to non-relevant documents. Now, if the number of terms that are shared by documents in the initial list is high, then presumably relevant documents could be more easily distin-guished from non-relevant ones.

We also explore the Normalized Mean Reciprocal Distance test ( nMRD ) [17]. The test is based on using a complete relevant documents graph. Each vertex in the graph rep-resents a different document in R ( D init ); each pair of ver-tices is connected with an edge. The edge weight repre-sents the distance between the documents. The distance between documents d i and d j is defined as the rank of d in a ranking of all the documents d  X   X  X  init ( d  X  6 = d that is created using sim ( d i , d  X  ); the rank of the highest ranked document is 1. The score assigned by the nMRD test spd ( d i , d j ) is the shortest path distance between d in the graph. This score is averaged over all tested queries for which n R &gt; 1 to produce the final nMRD score. Cluster-based document retrieval methods. Let C l ( D init be the set of clusters created from the documents in D init using some clustering algorithm. All the cluster-based re-
W e use both sim ( d i , d j ) and sim ( d j , d i ) as the similarity measure that was used for experiments is asymmetric. Fur-ther details are provided in Section 4. trieval methods that we consider re-rank the documents in D init using information induced from clusters in C l ( D init
The interpolation-f method ( Interpf in short) [6] directly ranks the documents in D init . The score assigned to docu-
The cluster-based retrieval methods that we consider next are based on a two steps procedure. First, the clusters in C l ( D init ) are ranked based on their presumed relevance to the query. Then, the ranking of clusters is transformed to a ranking over the documents in D init by replacing each cluster with its constituent documents (and omitting repeats).
The AMean and GMean methods [12, 16] rank the clus-ters based on the arithmetic and geometric mean of the orig-inal retrieval scores of the documents in a cluster, respec-tively. Specifically, AMean assigns cluster c with the score
P d  X  c sc ore ( q, d ) where | c | is the number of documents in c . The score assigned to c by GMean is Q d  X  c score ( q, d )
A nother cluster ranking method that we use is Clus-tRanker [7]. ClustRanker assigns cluster c with the score cent ( c ) are estimates of the centrality of a document d in D init and that of a cluster c in C l ( D init ), respectively. These estimates are computed using a PageRank algorithm that utilizes inter-document and inter-cluster similarities [9, 7]. We also use the recently proposed state-of-the-art ClustMRF cluster ranking method [15]. ClustMRF uses Markov Random Fields which enable to integrate various types of cluster-relevance evidence.
Experiments were conducted using the datasets specified in Table 1. WSJ, AP and ROBUST are small (mainly) newswire collections. WT10G is a small Web collection and GOV2 is a crawl of the .gov domain. CW09B is the Category B of the ClueWeb09 collection and CW09A is its Category A English part. We use two additional settings, CW09BF and CW09AF, for categories B and A [2], respectively. These settings are created by filtering out from the initial ranking documents that were assigned with a score below 50 and 70 by Waterloo X  X  spam classifier for CW09B and CW09A, respectively. Thus, the initial lists, D init , used for these two settings presumably contain fewer spam documents.
The Indri toolkit was used for experiments 2 . Titles of topics served for queries. We applied Krovetz stemming to documents and queries. Stopwords were removed only from queries using INQUERY X  X  list [1].

We use the nearest neighbor clustering algorithm to cre-ate the set of clusters C l ( D init ) [4]. A cluster is created from each document d i  X  X  init . The cluster contains d i the k  X  1 documents d j  X  X  init ( d j 6 = d i ) with the high-est sim ( d i , d j ). We set k = 5. Recall that k is also the number of nearest neighbors in the NN cluster hypothesis test. Using such small overlapping clusters was shown to be highly effective, with respect to other clustering schemes, for cluster-based retrieval [4, 11, 7, 14, 15].

The similarity between texts x and y , sim ( x, y ), is defined as exp  X   X  CE  X  p Dir [0] x ( )  X   X  tropy measure and p Dir [  X  ] z ( ) is the Dirichlet-smoothed (with the smoothing parameter  X  ) unigram language model in-duced from text z [8]. We set  X  = 1000 in our experi-ments [22]. This similarity measure was found to be highly effective, specifically for measuring inter-document similar-ities, with respect to other measures [9]. The measure is used to create D init  X  i.e., score ( q, d ) def = sim ( q, d )  X  to compute similarities between the query, documents and clusters. We represent a cluster by the concatenation of its constituent documents [10, 6, 8, 7]. Since we use unigram language models the similarity measure is not affected by the concatenation order.

To study the correlation between two cluster hypothesis tests, we rank the nine experimental settings (WSJ, AP, RO-BUST, WT10G, GOV2 and the ClueWeb09 settings) based on the score assigned to them by each of the tests. Kendall X  X - X  correlation between the rankings of experimental settings is the estimate for the correlation between the tests. We note that Kendall X  X - X  is a rank correlation measure that does not depend on the actual scores assigned to the settings by the tests. Kendall X  X - X  ranges from  X  1 to +1 where  X  1 represents perfect negative correlation, +1 represents perfect positive correlation, and 0 means no correlation.

The correlation between a cluster hypothesis test and the relative effectiveness of a cluster-based retrieval method is also measured using Kendall X  X - X  . The experimental settings are ranked with respect to a cluster-based retrieval method by the performance improvement it posts over the original document-based ranking. Specifically, the ratio between the Mean Average Precision at cutoff n (MAP@ n ) of the rank-ing induced by the method and the MAP@ n of the initial ranking is used; n is the number of documents in D init . The free-parameter values of Interpf, ClustRanker and ClustMRF were set using 10-fold cross validation. Query IDs were used to create the folds; MAP@ n served for the optimization criterion in the learning phase. The value of  X  which is used in Interpf and ClustRanker is selected from { 0 , 0 . 1 , . . . , 1 } . To compute the document and cluster cen-trality estimates in ClustRanker, the dumping factor and the number of nearest neighbors that are used in the PageRank 30 , 40 , 50 } , respectively. The implementation of ClustMRF follows that in [15]. w ww.lemurproject.org/indri
Thus, the initial ranking is induced by a standard language-model-based approach.
 Table 2: The correlation between cluster hypothesis t ests (measured in terms of Kendall X  X - X  ). n is the number of documents in D init .
The correlations between the cluster hypothesis tests are presented in Table 2 for different values of n . With the ex-ception of the Overlap test, we can see that the correlation between all other pairs of tests increases with increasing val-ues of n , but can be negative or zero for low values of n . The Overlap test is negatively correlated with all the other tests across almost all values of n .

A decent positive correlation is attained between Density and NN for n  X  100. For n  X  250 a decent positive correla-tion is also attained between nMRD and NN. While nMRD is a global test that considers the relations between all the documents in D init , NN is a more local test that only consid-ers the relations between a document and its nearest neigh-bors.

For n  X  X  250 , 500 } , nMRD and Density are the most cor-related tests. This finding is surprising since these tests are based on completely different properties of the initial list. While nMRD is based on directly measuring inter-document similarities, the Density test is based on the num-ber of unique terms in the documents which presumably attests to the ability to differentiate between relevant and non-relevant documents.
 study the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval meth-ods. For reference, we report the correlation numbers with respect to a ranking of the experimental settings induced by the size of the corresponding collections ( Size ). The results are presented in Table 3.

We observe a negative correlation between the Density test and all five cluster-based retrieval methods for n = 50. This finding can be explained as follows. First, the size of the collection is positively correlated with the number of terms in the vocabulary. (Refer back to Table 1.) Now, by defini-tion, this number is negatively correlated with Density. Sec-ond, the size of the collection is positively correlated with the effectiveness of cluster-based retrieval methods as observed in Table 3 for the Size correlations for n = 50. We note that here, the Web collections are larger than the newswire col-lections and are in general noisier. Thus, we conclude that the type of the collection, i.e., Web vs. newswire, can have Table 3: The correlation between cluster hypothesis t ests and the relative effectiveness of cluster-based retrieval methods. The Size  X  X est X  ranks experimen-tal settings by the number of documents in the col-lections. n is the number of documents in D init . an influence on the effectiveness of cluster-based retrieval methods for short retrieved lists.

Another observation that we make based on Table 3 is that for n = 50 the correlation attained for the nMRD and NN tests is often lower than that attained for Overlap and Size. The relatively high positive correlation attained for the Overlap and the Size tests for n = 50 suggests that these tests are very strong indicators for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval when applied to short retrieved lists. For larger values of n , NN and nMRD, as well as Density, start to post more positive correlations while the reverse holds for Overlap and Size.

We can also see that none of the retrieval methods is cor-related only positively or only negatively with all the tests for any fixed value of n . In addition, only in a few cases a test is either only positively or only negatively correlated with all the retrieval methods for a fixed value of n . Thus, we conclude that the correlation between the effectiveness of a retrieval method and a cluster hypothesis test can substan-tially vary (both positively and negatively) across retrieval methods and tests.
We studied the correlation between cluster hypothesis tests and cluster-based retrieval effectiveness. We showed that the correlation between the two depends on the specific tests and methods that are used, and on the number of documents in the result list that is analyzed. We also showed that the type of the collection, i.e., Web or newswire, can be a stronger indicator for the relative effectiveness of cluster-based re-trieval with respect to document-based retrieval, for short retrieved lists, than tests designed for estimating the extent to which the cluster hypothesis holds.
 Acknowledgments We thank the reviewers for their com-ments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Cen-ter. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.
