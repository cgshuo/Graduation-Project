 Text classification (TC) is the task of classifying documents into a set of pre-defined categories. It has long been an important research topic in information retrieval (IR). Many statistical classification methods and machine learning (ML) techniques have been developed to TC, such as the naive Bayes classifier [12], the support vector machines [10], the k -nearest neighbor method [20], and the boosting method [16]. In addition, text classification based on term associations [1] is also a promising approach. The performance of text classification highly depends on the document representation. Most of the existing methods repre-sent a document using a vector space model (VSM) or a language model (LM). Generally, the bag-of-words (BoW) method is a widely used data representation in IR and TC. Under this scheme, each document is modeled as a vector with a dimension equal to the size of the dictionary, and each element of the vector denotes the frequency that a word appears in the document. Basically, all the words are treated independently.

One of the important restrictions in most of the existing TC methods may lie in that the individual terms are usually too general and that these methods do not consider the associations between word s in the documents. In some cases of TC, ument. For example, a document with  X  X huttle launch X  may be assumed to belong to the  X  X all game X  class. However, if the word  X  X ASA X  is an association term, it is very likely that the document should be assigned to the  X  X eronautics X  class.
It is well-known that the relationships between words are very important for statistical language mode ling. Using LM for TC has been studied recently [2,14]. Although N -gram LM can exploit the relation ships between words, they only consider the dependencies of neighbori ng words [5]. For example, the trigram LM is unable to characteriz e word dependence beyond the span of three succes-sive words. In [22], the trigram LM was improved by integrating with the trigger pairs, which extract the word relationships from the sequence of historical words. Nevertheless, a trigger pair is word order dependent. In other words, a word can only be triggered by the previous cont ext. Recent studies have revealed that modeling term associations could provide richer semantics of documents for LM and IR [4,18,19]. Cao et al. [4] integrated the word co-occurrence information and the WordNet information into language models. Wei and Croft [18] investigated the use of term associations to improve the performance of LM-based IR. In [19], the word associations were integrated into the topic modeling paradigm. Adding word associations to represent a docume nt inevitably increases the model X  X  com-plexity, but the new information reduces the ambiguity mentioned above. Gen-erally, any set of words co-occur in the contexts can be considered having a strong association and collected as the associative words, e.g.,  X  X neven bars X  and  X  X alance X  in the class of gymnastics and  X  X erofoil X  and  X  X et engine X  in the class of airplane transportation. However, the associative words are not necessary to co-occur in a document. We believe that a language model considering term associations would be definitely more useful in TC.

In this paper, we propose a novel model for text classification by incorporate the strengths of term associations into the translation LM framework. Different from the traditional TC techniques and algorithms in the literature, we model the associations between words existing i n the documents of a class. To discover the associative terms in the documents, we learn the translation language model based on the joint probability (JP) of the associative terms through the Bayes rule and based on the mutual information (MI) of the associative terms.
The remainder of this paper is organi zed as follows. In Section 2, we briefly review the framework of the naive Bayes classifier and language models. The proposed models for text classification a re presented in Section 3. Experimental setup and results are discussed in Section 4. Finally, we give the conclusions in Section 5.
 2.1 Terminology We begin by defining the notation and terminology in this paper. A word or term is a linguistic building block for text. A word is denoted by w  X  V = { 1 , 2 ,..., | V |} ,where | V | is number of distinct words/terms. A document ,rep-by q = { q 1 ,  X  X  X  ,q T } ,isastringof T words. A collection ofdocumentsisde-D .A background model , denoted by M B , is the language model estimated in is the number of distinct classes. A LM M is a probability function defined on a set of word strings. This includes the important special case of the probability P ( w |M ) of a word w .A class LM , denoted by M estimated based on class c . 2.2 Naive Bayes Classifier The naive Bayes classifier (NBC) is a popular machine learning technique for text classification. The method assumes a probabilistic generative model for text. A common and simple representation of a document in TC is the bag of words (BoW) model. The model ignores the word order and just captures the number of occurrences of each word in the doc ument. The NBC classifies a document through two stages: the learning stage and the classifying stage. It is assumed that the probability of each word in a document is independent to that of other words, and each document is drawn from a multinomial distribution of words. In P ( c | d ) , which represents the probability that a document c belongs to a class d . Using the Bayes rule, we have where P ( d | c ) is the likelihood of document d under class c . By assuming that all words in d are independent of each other, P ( d | c ) can be further decomposed into the product of individual feature (word) probabilities as follows from the training documents with Laplace smoothing as follows where n ( w, c ) is the number of times word w occurs in the training documents that belong to class c ; N ( c ) is the total number of words in the training doc-uments that belong to class c ; n ( d, c ) denotes the number of documents that belong to class c ;and | D | is total number of training documents.
Several extensions of the naive Bayes cl assifier have been proposed. For exam-ple, Nigam et al. [13] combined the Expectation-Maximization (EM) algorithm and the naive Bayes classifier to learn from both labeled and unlabeled doc-uments in a semi-supervised manner. Mo re recently, Dai et al. [7] proposed a However, these methods all assume that the words in a document are indepen-dent of each other; hence, they cannot cope well with the term dependence and association. 2.3 Language Models for Information Retrieval Statistical language modeling plays an important role in automatic speech recog-nition (ASR) and IR. Most ASR systems are built by combining the N -gram language model and the acoustic hidden Markov model (HMM) to predict the best word sequence corresponding to an input speech utterance. In an IR sys-tem, the word sequence of an input query is adopted to retrieve the relevant text documents. In Ponte and Croft X  X  work that applied LM in IR [15], the re-trieval performance was improved by statistical modeling of natural language. According to the maximum a posteriori decision rule, the ranking function f (  X  ) is established as a posterior probability, relevance, the ranking can be done according to the likelihood of the N -gram language model { q mum likelihood (ML) criterion as follows, document model is generally adopted i n the IR community [15]. However, the document terms are often too few to tra in a reliable ML-based model because the unseen words lead to zero unigram probabilities. Zhai and Lafferty [21] have used several smoothing methods to deal with the data sparseness problem in LM-based IR.

Since previous research [4,18,19] have shown that some relationships exist between words, we utilize them in the document model rather than using the traditional unigram document model for text classification. 3.1 Language Models for Text Classification LM was first introduced to TC by Peng and Schuurmans [14]. The score of a class c for a given document d can be estimated by (1). Then, the class of the document can be decided as follows Assuming that P ( c ) is uniformly distributed and applying the unigram class LM in the task, the decision can be rewritten as The traditional naive Bayes classifier usually uses Laplace smoothing to deal with the zero probability problem. However, some previous research has shown that it is not as effective as the smoothing methods for language modeling [2,14]. Therefore, we can interpolate a unigr am class LM with the unigram collection background model by using the Jelinek-Mercer smoothing method as follows, where  X  can be tuned empirically. In this paper, the method based on (10) is denoted as NBC-UN, and  X  is set to 0.5.

In order to discover the association between two terms w i and w ,weare probability P ( w i | w ) in that the words w i and w are not limited to occur in order and adjacently in the former. Then, the term association information can be integrated into the unigram class model as follows, c , which can be computed via the maximu m likelihood estimate. By replacing P ( w The model in (12) is obviously more computationally intensive than the model and the word probability distribution for each class beforehand. To discover the associative terms in the training documents, we learn the translation LM based on the joint probability of the associative terms through the Bayes rule and based on the mutual information (MI) of the associative terms. 3.2 Translation Model Estimation Using Joint Probability Model This section describes our first way of co nstructing the term translation proba-joint probability of words w i and w over the probability of word w where the join probability of w i and w can be expressed as if w class model c , and the probability of w can be expressed as form prior P ( c ) ,weobtain The method based on (12) with P t ( w i | w ) computed by (16) is denoted as TATM-JP (the term association translation model estimated by the joint probability of terms). 3.3 Translation Model Estimation Based on Mutual Information Our second way of constructing the term translation probability P t ( w i | w ) is based on the mutual information (MI). In information theory, the MI of two random variables is a quantity that measures their mutual dependence. MI is a good measure to assess how two words are related to each other [6,22]. We use the average mutual information (AMI) [22] to measure the strength of the association between words w i and w . The AMI between w i and w is defined as follows P ( w the ratio of the number of documents that contain w and the total number of documents; P (  X  w ) is estimated as the ratio of the number of documents that do not contain w and the total number of documents; and the other probabilities are estimated in a similar way. According to [11], the term translation probabil-follows If the two words w i and w tend to associate with each other, the probability would be higher. The method based on (12) with P t ( w i | w ) computed by (20) is denoted as TATM-MI (the term association translation model estimated based on the mutual information of terms). 4.1 Corpora We evaluate the proposed TC methods on two standard document collec-tions: Reuters-21578 (Reuters) 1 and 20 Newsgroups (20NG) 2 .Accordingtothe ModApte split, the Reuters corpus is separated into 7,194 documents for training and 2,788 documents for testing. 135 categories have been defined, but only 118 categories have documents assigned to them. Following Debole and Sebastiani X  X  work in [8], we consider the most frequent ten categories in the experiments. The 10 categories and the numbers of documents used for training and test-ing in each category are listed in Table 1. The 20NG dataset is a collection of 19,974 documents collected from 20 different newsgroups. We consider the 20 newsgroups as the 20 categories. For each category, we randomly select 60% of the documents for training and the remaining 40% for testing. Since the 20NG collection distributes roughly evenly across 20 newsgroups, each category has almost the same number of training (or testing) documents. 4.2 Performance Measure In the following experiments, the performance of text classification is evaluated To evaluate the average performance across classes, we use the micro-averaged score and macro-averaged score [20]. The micro-averaged score is calculated by mixing together the documents across all the classes. The macro-averaged score is obtained by taking the average of the recall, precision, and F -measure values for each category 4.3 Experimental Results We compare our term association translation models (TATM-JP and TATM-MI) with the naive Bayes classifier with Laplace smoothing (NBC) and the naive Bayes classifier with the unigram language model (NBC-UN).
 Table 2 shows the results of text classification experiments evaluated on the Reuters collection. Th e measure used is the F -measure on the ten most populated Reuters-21578 categories and the micro-averaged F -measure (micro-F )overall categories. Comparing the results of NBC and NBC-UN, it is obvious that us-ing language models improves the classification effectiveness of the naive Bayes classifier. Both proposed methods consist ently outperform NBC and respectively perform better than NBC-UN in four out of ten categories. The micro-average F -measure of TATM-MI is 0.731, which is better than that of TATM-JP (0.727), NBC-UN (0.720) and NBC (0.709). The relative improvement in the micro-F by TATM-MI is 3.1% over NBC and 1.5% over NBC-UN.

Table 3 shows the experimental results for the 20NG dataset in terms of the micro/macro-averaged precision, recall, and F -measure. The micro-F of TATM-JP is 0.817 and TATM-MI is 0.82, which is better than that obtained by NBC (0.801) and NBC-UN (0.808). The relative improvement by TATM-JP over NBC and NBC-UN is 2%, and 1.11%, respectivel y. Similarly, the relative improvement in micro-F by TATM-MI over NBC and NBC-UN is 2.37%, and 1.49%, respec-tively. The improvements of TATM-JP and TATM-MI over NBC and NBC-UN ation translation model estimating based on the mutual information for all data sets is more efficient than learning the te rm association translation model by the joint probability . As expected, the performance in micro-F on the 20NG dataset is very similar to that in macro-F because each class has a similar number of training and testing documents. Again, we can see that TATM-MI performs the best.

Several observations can be drawn from the results. First, the performance of text classification can be improved by incorporating language models into the naive Bayes classifier. Second, the propo sed document model with term associ-ation modeling leads to improvemen ts over NBC and NBC-UN. The new model could be applied to other topic document models. The use of term associations for TC has attracted great interest. This paper has presented a new term association t ranslation model, which models term associations, for TC. The proposed mod el can be learned based on the joint probability of the associative terms through the Bayes rule or based on the mutual information of the associative terms. The experimental results show that the new model learned in either way out performs the traditional TC methods. For future work, we plan to investigate t he effect of the feature selection method into the topic models such as probability latent semantic analysis (PLSA) [9] or latent Dirichlet allocation (LDA) [3] for text classification. Another interesting direction is to combine the term associat ion document model with the relevance-based document model, and apply the combined model in TC.

