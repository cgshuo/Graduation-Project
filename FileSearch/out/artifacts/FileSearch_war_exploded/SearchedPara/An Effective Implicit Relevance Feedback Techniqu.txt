 The effectiveness of various behavioural signals for implicit relevance feedback models has been exhaustively studied. Despite the advantages of such techniques for a real time in-formation retrieval system, most of the behavioural signals are noisy and therefore not reliable enough to be employed. Among many, a combination of dwell time and task infor-mation has been shown to be effective for relevance judge-ment prediction. However, the task information might not be available to the system at all times. Thus, there is a need for other sources of information which can be used as a sub-stitute for task information. Recently, affective and phys-iological signals have shown promise as a potential source of information for relevance judgement prediction. How-ever, their accuracy is not high enough to be applicable on their own. This paper investigates whether affective and physiological signals can be used as a complementary source of information for behavioural signals (i.e. dwell time) to create a reliable signal for relevance judgement prediction. Using a video retrieval system as a use case, we study and compare the effectiveness of the affective and physiological signals on their own, as well as in combination with be-havioural signals for the relevance judgment prediction task across four different search intentions: seeking information, re-finding a particular information object, and two different entertainment intentions (i.e. entertainment by adjusting arousal level, and entertainment by adjusting mood). Our experimental results show that the effectiveness of studied signals varies across different search intentions, and when affective and physiological signals are combined with dwell time, a significant improvement can be achieved. Overall, these findings will help to implement better search engines in the future.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval -Information Search and Retrieval -Search Process General Terms: Performance, Experimentation Keywords: Implicit Relevance Feedback, Affective, Physi-ological, Behavioural, Dwell Time, Search Intentions
It has been understood for some time that the formulated queries in an information seeking process do not always pro-vide an adequate description necessary to retrieve relevant documents [39], since it is only an approximation of the ac-tual information need [33]. Likewise, queries formulated may not adequately define the characteristics of relevant doc-uments, or indeed any relevant information because of an ill-defined information need situation. Since more complete representations generally lead to more precise search results, supplementing textual queries with additional sources of in-formation, such as documents users have found relevant [26], can be advantageous [37]. Research has shown that searchers can specify the relevance of a document to their needs more easily than explicitly specifying their information needs [31]. Therefore, in order to help the searcher, IR systems have pre-viously employed a range of relevance feedback techniques, where feedback is gathered through explicit [20], implicit [37], and/or affective feedback [1].

Despite the robustness of explicit feedback in improving retrieval effectiveness [20], it is not always applicable or re-liable due to the cognitive burden that it places on users [38]. To overcome this cognitive burden, implicit relevance feedback (IRF) is proposed; with relevance inferred instead from behaviour signals (i.e. interactional data) in an indi-rect and unobtrusive manner [18]. For example, researchers try to understand how dwell time (the time spent by a user on a retrieved document) [19] and task [37] relate to rele-vance. Dwell time on a document has got a particular inter-est since it can be detected in real-time [22]. Recent studies have shown that dwell time combined with task information can improve IRF algorithm performance [37]. However, in a real-time situation, task information might not be apparent or easy to obtain in many cases. Hence, there is a need for other sources of information which can be used as a substi-tute for the task information.

Due to recent advances in technology, new sensory devices have been developed. It is now possible to leverage affective and physiological signals from user while they are interacting with a system. For instance, facial expression recognition, galvanic skin response, heart rate monitoring and electroen-cephalography provide more information about interaction than ever before. Currently, there are numerous efforts in disciplines that extend beyond IR to understand how such information can be used to improve human-machine inter-actions, e.g., [29]. The output of these sensory devices has gained notable attention from the IR community, in par-ticular for their effectiveness in predicting relevance. As a result of such studies, affective feedback has been proposed [1] where the intention is to capture facial expression [4], and physiological signals [3] (such as skin temperature) and use them as implicit relevance judgement. Despite the great potential of this information for IRF techniques, the accu-racy of such information in the past was not high enough to be used on its own in a real time system.

Our hypothesis is that signals obtained from sensory chan-nels can be considered as a suitable replacement of task in-formation for IRF techniques based on dwell time informa-tion. This hypothesis is motivated by the findings of recent studies from both Poddar et al. [30] where they show that tasks with different characteristics stimuli different emotions in the searcher, and Arapakis et al. [1] where they show that relevance information that derives from the selected sen-sory channels is correlated to user affective behaviour. They investigated whether they can deduce topical relevance by measuring physiological signals taken from the participants, in the context of online information seeking. However, the effect of combining signals obtained from sensory devices with behavioural data has yet to be studied. Successful re-sults steer the situation for using IRF in a real-time system. This paper is an attempt in this vein of research.

To further investigate our hypothesis, we study the effec-tiveness of such IRF techniques for search tasks with dif-ferent intentions. Recent advances in query log analysis for search engines such as Google, Yahoo! and Bing has shown that Web searchers X  intentions do not always fit into the typi-cal taxonomy of informational, navigational or transactional intentions [34]. Although informational (i.e. to acquire in-formation present on one or more sites), navigational (i.e. to reach a particular site) and transactional (i.e. to perform some Web-mediated activity) intentions are all commonly found in query logs [7], there is an increasing body of evi-dence showing that re-finding and re-retrieving the visited information is a regular activity by searchers [34, 12]. In addition, there is also recent interest in the notion of en-tertainment, where the searchers do not have any particular information need [13] and the prime motivation behind their search is to satisfy an emotion need [27]. Thus the effective-ness of these sources of information on their own and in combination with behavioural data (i.e. dwell time) has yet to be studied for search processes with different intentions, i.e. information seeking, re-finding information and/or en-tertainment.

Therefore, two important questions emerge. First, whether combining sensory channels and behavioural signals provide a reliable set of features for IRF techniques. Second, what would be the effect of different search intentions (e.g. seeking information, re-finding or entertainment) on the relevance judgement predictability of this set of features. In order to investigate our research questions, we use an interactive information retrieval evaluation framework. In particular, we devised four search tasks each of which cover a search intention. We then capture affective and physiological sig-nals of the participants while they were performing the tasks along with their relevance assessment for the visited videos. Finally, we investigate the discriminative power of the cap-tured signals with and without dwell time for predicting the relevance of the assessed videos across different search in-tentions.

This paper has three novel contributions. First, we stud-ied and compared the discriminative power of multitude sen-sory channels (i.e. EEG, heart rate, facial expression, skin temperature, etc.) for relevance judgement prediction task. Second, we investigated the possibility of substituting task information with these sensory signals for dwell time based IRF techniques. Finally, we investigated the prediction ac-curacy of these signals for search processes with different intentions. The rest of the paper is organised as follows: re-lated work is presented in Section 2, the experiment method-ology is described in Section 3. Results and discussion are presented in Section 4, and finally, the paper is concluded in Section 5.
Dwell Time and IRF: Much past research has at-tempted finding a reliable set of features for IRF techniques [18]. One of the main features investigated researchers is dwell time, due to its applicability for real time systems [22]. Despite early studies by Morita et al. [25] and Clay-pool et al. [8] where they proposed dwell time as a reliable behavioural signal for IRF, later studies conducted by Kelly and Belkin [19] show that dwell time by its own is not reli-able and that it can differ significantly according to specific task, and according to specific user. More recent studies in this path by White and Kelly [37] show that a combination of dwell time and task information is a reliable source of in-formation for IRF techniques. However, information about the search task is not available all the time. Thus many approaches try to predict this information. One of the com-mon way to do so is to use the behavioural signals such as dwell time [21]. However, this approach cannot be useful if the predicted task information is required to be combined with the dwell time itself for IRF. Therefore, clearly there is a need for other sources of information that can be sub-stituted with the task information for dwell time based IRF techniques. This paper is an attempt to investigate this re-search question.

Task and Emotion: In recent years the relation be-tween the search tasks within an information seeking process (ISP) and emotion has gained much attention. For exam-ple, several works studied the emotional impact of search tasks within the ISP [30, 2, 23]. Poddar and Ruthven [30] examined how participants emotion responses are influenced by tasks of different natures. In particular, their results in-dicate that (i) artificial tasks have higher uncertainty and less sense of ownership than genuine search tasks, and (ii) more complex search tasks have lower positive emotions and more uncertainty before and after searching. Similar find-ings were earlier reported also by [2] in an information seek-ing activity. They concluded that users X  emotions progres-sively transit from positive to negative valence as the de-gree of task difficulty increases. They also found that emo-tions both interweave with different physiological, psycho-logical and cognitive processes during an information seek-ing process, and form distinctive patterns according to spe-cific tasks. However, [23] reported no significant relationship between searchers X  mood and search tasks due to the com-plexity involved in such studies. In another study, [17] in-vestigated the relationship between the subjective (e.g. hap-piness levels, feeling lost during search, etc.) and objective (e.g. search outcomes and search task characteristics, etc.) factors in the ISP. Their results show that  X  X igher happiness levels before the search and during the search correlate with better feelings after the search, but also correlate with worse search outcomes and lower satisfaction, suggesting that, per-haps, it pays off to feel some  X  X ain X  during the search in or-der to  X  X ain X  quality outcomes X  [17]. These valuable insights into the relation between emotion and search task motivated our research. This paper investigates the possibility of sub-stituting task information with experienced emotion to be combined with dwell time in order to have a reliable real time features for IRF techniques. The question that might be raised is how one can capture human emotion.

Affective Relevance Feedback: Recent advances in information technology have facilitated the development of sensory channels to capture human emotion [28, 29, 11]. The argument is that such changes are often expressed through a psycho-physiological mobilisation that is reflected by a se-ries of more or less observable cues, such as facial expres-sions, body movements, localised changes in the electroder-mal activity, variations in the skin temperature, and many more [3]. The affective and physiological (A&amp;P) signals obtained from such devices have provided researchers with additional sources of information not previously available. Thus these A&amp;P signals have gained significant attention in the IR community, and their effectiveness has been studied for numerous tasks and purposes, e.g [1, 3, 4].

The most related work to this paper are the studies con-ducted by Arapakis and his colleagues on the effectiveness of A&amp;P information for relevance judgment prediction (known as affective feedback) [2]. For example, Arapakis et al. [4] show that affective features captured from facial expression during an information seeking process can be used to develop a multimodal recommendation system. In another study, Arapakis et al. [3] explored the role of affective (captured via facial expression) and physiological signals (captured via skin temperature and body movement) in designing multi-media search systems. They investigated whether topical relevance could be deduced by measuring key physiological signals taken from the user. Finally, In a more recent study, the role of affective signals was studied in a personalised system [1] for topical relevance.

However, two main limitations can be associated with these studies. First, the accuracy of A&amp;P information in the past was not high enough to be used by its own in a real time system. For example, Feild [16] found little benefit from the physiological signals when they tried to used them for searcher frustration prediction where three physical sen-sors were used: a mental state camera, a pressure sensitive mouse, and a pressure sensitive chair. Given the relation between task characteristics and stimulated emotion in the searcher (explained above) and the possibility of extracting emotion using A&amp;P sensory channels, we hypothesis that a combination of A&amp;P signals with behavioural signals (in particular dwell time) can lead to a reliable set of features for IRF. Second, the effectiveness of A&amp;P information was only investigated for information seeking scenarios where a clear information need is defined (i.e. topical relevance). Thus the effectiveness of such sensory channels for a more realistic representation of real life search process has yet to be studied, where variety of tasks with different intentions are proposed. This paper is an attempt to investigate this research problem.

Seeking Information on the Web: Traditionally, the general belief of the Information Retrieval (IR) community was that the dominant intention of people engaging in search processes was informational [7]. However, Broder [7] shows the existence of search intents other than the assumed infor-mational intent, i.e. navigational and transactional. In more recent studies, researchers have identified a wider range of search intents that go beyond Broader X  X  taxonomy of inten-tions, i.e. re-finding [34] and entertainment [13]. Re-finding the visited information has been shown to be a regular ac-tivity by searchers [34, 12]. Teevan et al. [34], by analysing Yahoo! Web query logs showed that up to 40% of the sub-mitted queries were re-finding queries. Elsweiler et al. [12], in an empirical study, showed that it is possible to isolate re-finding behaviour in search logs through various qualita-tive and quantitative analyses. Given this evidence, it is important to study the effect of affective, physiological and behavioural signals in the re-finding intention.

In addition, recently there is an increased effort in the research community to explore search processes with enter-tainment intents, e.g.  X  X ntertain Me X  [5] and  X  X earch4FUN X  [14] workshops. Besides the works published in these work-shops, [13] attempted to understand the needs and moti-vation underlying leisure-based activities in the context of television viewing. They reported that the nature of the need and motivation are different between leisure-based and work-based situations where most of the needs reported for leisure-based situation are motivated by a desire to change mood, emotion, or arousal level. Further research related to leisure-based activity is the work by [32] which studies plea-sure reading behaviour. Their results show that pleasure readers find information without having any prior informa-tion need. In addition, [40] and in a more in-depth study, [15], reported that in casual search, the motivation is not to resolve an information need, but hedonistic, e.g. entertain-ment driven.

Given the emergence of entertainment seeking intentions on the Web and their unexplored characteristics, it is also important to study the effect of affective, physiological and behavioural signals in entertainment seeking intentions. The defined entertainment seeking intentions in this paper are based on research done by Zillmann [36], who was instru-mental in the establishing a range of theories in this do-main. Zillmann and his colleagues posit that entertainment choices are a reflection of a basic human need to enhance or retain positive states, and to lessen or steer clear of negative ones [27]. They have suggested two possible states in which there may be a need for regulation: physiological arousal and affect. In the case of physiological arousal, their study sug-gests that users might be over-stimulated (i.e., stressed) or under-stimulated (i.e., boredom). In the case of affect states, it suggests that users might be in negative (i.e. dysphoric) or positive (i.e. upbeat) moods. An individual experiencing such states will choose their entertainment content accord-ing to their expectations of what would lead them back to an optimal state [27]. Thus, we investigate two entertainment seeking intentions: entertainment by adjusting arousal level and entertainment by adjusting mood.
This study used a within subject design. The independent variable was the search intents (with four levels:  X  X nforma-tion Seeking X  (INS),  X  X nformation Re-finding X  (INF),  X  X n-tertainment by adjusting Arousal X  (ENA),  X  X ntertainment by adjusting Mood X  (ENM)), which was controlled by the simulated search task given to the participants (see Section 3.2). We did not perform any control on the number of rele-vant and irrelevant results, in order to simulate a real search scenario situation as much as possible. The dependent vari-ables were: (i) task perception and (ii) relevance judgment prediction accuracy.
Four search tasks were devised, each simulating a search intent. The structural framework of simulated need situa-tions [6] were used to present search tasks. By doing so, short cover stories were introduced that helped us describe to our participants the source of their information need, the problem to be solved and the environment of the situation. This facilitated a better understanding of the search objec-tive and introduced a layer of realism, while preserving well-defined relevance criteria. In the following, each of the search tasks is explained in detail.

INS Task: This search task simulates the information seeking search intent. Information seeking is the most stud-ied search intent in the information seeking and retrieval domain. This task is designed as a control group since the majority of studies on relevance judgement prediction in the past were based on a similar search task intent. For this search task, we prepared a number of search topics that cov-ered a variety of contexts in order to capture participants X  interests as best as possible. The topics, presented in Ta-ble 1, were all checked manually prior to the experiment, to ensure the availability of relevant documents. The simu-lated search scenario for INS task was as follows:  X  X magine you have graduated recently and are going to interview for a job in a local company. As part of the interview process, you are asked to explain and expand on the area you will be working on. You feel very enthusiastic about the interview; however, due to your lack of knowledge you would like to find out more about this particular topic before taking part in the interview. X  Each participant was then asked to choose one of the topics that they were unfamiliar with but consider interesting. Using the video retrieval system, they had to find as many relevant videos as possible so that they could acquire a good knowledge about their selected topic.
INF Task: This search task simulates the information re-finding search intent. There are two differences between this task and the previous one: (i) there is only one docu-ment that can satisfy the information need of the searcher, and (ii) the searcher has seen the relevant documents at some point before initiating the search process and is now at-tempting to re-find it. The similarity between this task and the previous one is that in both cases searchers have an in-formation need. For this search task, we prepared a number of videos that covered a variety of contexts in order to cap-ture participants X  interests as best as possible. The videos were intentionally selected as they would likely be very hard to find because they lacked textual description. The moti-vation was to simulate a challenging information re-finding process where the recalled terms are very ambiguous and do not lead directly to the relevant item. This would better represent many realistic re-finding tasks, when the user can-not recall the exact description of the item they are looking for. The simulated search scenario for INF task was as fol-lows:  X  X magine you are discussing a video which you have seen few days ago with your friends. They are interested in seeing the video and have asked you to send them a link to it. You can remember the content of the video but you cannot remember its title or any textual information which can help you in retrieving it. X  Each participant was then asked to select and watch one of the three videos presented to them (an animal 1 , martial arts 2 , or a science video are unfamiliar with and consider interesting. Once partici-pants watched the video, they were asked to find it as fast as possible.

ENA Task: This search task simulates the entertainment-based search intent where searchers adjust their arousal level. The main difference between this task and the two tasks be-fore is that the primary need is hedonistic rather than infor-mational. Therefore, to accurately simulate such search pro-cesses, we avoid introducing any explicit information need. Thus, for this search task, we did not prepare any specific search topic. This decision is motivated by the literature in sociology [27] and the information seeking and retrieval do-main on entertainment [13]. The simulated search scenario for the ENA task was as follows:  X  X magine you are working in a factory as a night-guard. You have just finished your routine checks and will be taking out more checks shortly. You are tired so you decide to watch some videos to wake yourself up and make yourself ready for your next round of checks. X  Each participant was asked to find as many relevant videos as possible that make them feel excited.

ENM Task: This search task simulates the entertainment-based search intent where searchers adjust their mood. Sim-ilar to ENA task, searchers engage in such search processes with hedonistic rather than informational needs. Therefore, to accurately simulate this search process, we avoid intro-ducing any explicit information need. Thus, similar to the ENA task, we did not provide any pre-prepared search topic. The simulated search scenario for the ENM task was as fol-lows:  X  X magine your boyfriend/girlfriend is travelling and communication access is very limited. It is now a few days since he/she has gone and you are missing him/her very much. You are feeling very sad and in order to change your mood, you have decided to watch some videos. X  Each partic-ipant was asked to find as many relevant videos as possible that make them feel happy.

Questionnaires: At the beginning of the experiment, the participants were given an entry questionnaire . This gathered background and demographic information, and in-quired about previous experience with online videos, in par-ticular, browsing and searching habits including their inten-tions. At the end of each task, the participants completed a post-task questionnaire to elicit the subject X  X  viewpoint on certain aspects of the search process, in particular their perception of the encountered task. All of the questions included in these questionnaires were a forced-choice type. Finally, an exit questionnaire was introduced at the end of the study. In this questionnaire we gathered information about the encountered system as well as the user study in general: which task they preferred and why, and their gen-eral comments about the user study.

Procedure: The user study was carried out in the fol-lowing manner. The formal meeting with the participants took place in a laboratory setting. At the beginning of the session the participants were given an information sheet which explained the conditions of the experiment. They were then asked to sign a Consent Form and were notified about their right to withdraw at any point during the study, without affecting their legal rights or benefits. Then, they were given an Entry Questionnaire to fill in.

The session proceeded with a brief tutorial on the use of the search interface. Then participants were helped to wear the sensory devices, followed by a calibration of the web-camera. After completion of this step, each participant had to complete four search tasks (explained in Section 3.2), one for each level of search process intentions (see Section 3.1). To negate the order and fatigue effects we counter-balanced the task distribution using a Latin Square design. The sub-jects were asked every time to provide judgment for any video that they watch, and were given 10 minutes to com-plete their task, during which they were left unattended to work. At the end of each task, the subjects were asked to complete a post-task questionnaire. Questions in the post-task questionnaire were randomised to avoid the effect of fatigue. Between each task, a cooling-off period was applied to minimise the carry-over effect. An exit questionnaire was administered at the end of the session. Finally, the partici-pants were asked to sign a payment form, prior to receiving the payment of  X  12.

Each study took approximately 120 minutes to complete; this is from the time they accepted the conditions until they signed the payment receipt. Users could only participate once in the study. The total cost of the evaluation was  X  348, including the cost of the pilot studies. A user study with the procedure explained above was conduced over a period of 10 days from 16th to 26th of July 2012. The results of these studies are presented in Section 4.

Apparatus: For our experiment we used one desktop computer, equipped with a monitor, keyboard, and mouse. The computer provided access to a custom-made video re-trieval system explained in Section 3.4. The web-camera (Creative Live! Cam Optia AF with a 2.0 megapixels sen-sor) was used in combination with eMotion [35], for real-time facial expression analysis. In addition, we used three unobtrusive wearable devices to capture participants X  phys-iological signals: (i) Polar RS800 Heart Rate Monitor, and (ii) BodyMedia SenseWear R  X  Pro3 Armband (iii) NeuroSky MindKit-EM TM headset. All devices and systems were log-ging using a common system time. Finally, we used entry, post and exit questionnaires in each session.
Affective Signals: In this study we considered facial expression as our affective signal. This decision was moti-vated by the findings of previous studies in IR related to this paper [1] as well as findings from other domains show-ing that emotions are primarily communicated through fa-cial expressions [28] and facial expressions can be associated with universally distinguished emotions, e.g. happiness, sad-ness, anger, fear, disgust, and surprise [11]. We applied a real-time facial expression analysis using an automatic fa-cial expression recognition system with emotion-detection capabilities (i.e. eMotion) [35]. The process of recognition occurred as follows: initially, eMotion would locate certain facial landmark features (eyebrows, corners of the mouth, etc.) and construct a 3-dimensional wireframe model of the face, consisting of surface patches wrapped around it. Af-ter construction of the model, head motion or any other facial deformation would be tracked and measured in terms of motion-units (MU X  X ), and, finally, classified into one of the six detectable emotion categories plus neutral. eMo-tion applies a generic classifier that has been trained on a diverse data set, combining data from the Cohn-Kanade database. The main advantage of this system is its rea-sonable performance across all individuals, irrespective of the variation introduced from mixed-ethnicity groups. Re-sults of the person-dependent and person-independent tests presented in [35] support our performance-related assump-tions. For additional information regarding the advantages and limitations of eMotion the reader is referred to [35].
Physiological Signals: In order to capture physiolog-ical signals we consider multiple sensory channels including heart rate monitoring, skin temperature, and neural activ-ity. This decision was also motivated by the findings of pre-vious studies in IR related to this paper [3]. Emotions can be expressed through several sensory channels and are re-flected by a series of more or less observable cues, such as localised changes in the electrodermal activity, variations in skin temperature, neural activity and many more. It has been shown that transitions between emotional states are correlated with temporal changes in physiological states [9], which cannot be easily faked. In this work we monitor participants X  physiological responses by using three sensory devices: Polar RS800, BodyMedia SenseWear R  X  Pro3 Arm-band and NeuroSky X  X  MindKit-EM TM .
 Polar RS800 Heart Rate Monitor consists of the Polar RS800 Running Computer, a wrist-watch that displays and records the heart rate data, an elastic strap with two elec-trodes and Polar WearLink R  X  31, a wireless transmitter. The elastic strap is worn around the chest (below the chest mus-cles), allowing the built-in soft textile electrodes to detect the heartbeat and then transmit the heart rate signal to the running computer, via the Polar WearLink R 31, which is attached to the strap.

BodyMedia SenseWear R  X  Pro3 Armband is an unobtru-sive, lightweight, multi-sensor hub, which is worn above the triceps area of the right arm. It can simultaneously mea-sure five low-level physiological metrics: (i) galvanic skin re-sponse, (ii) skin temperature, (iii) near-body ambient tem-perature, (iv) heat flux, and (v) motion, via a 3-axis ac-celerometer. From those vital sign streams it can produce accurate statements about the human body states and be-haviours. Moreover, the existence of multiple sensors allows for the disambiguation of contexts, which a single sensor would have not interpreted accurately.

NeuroSky MindKit-EM TM consists of hardware and soft-ware components for simple integration of bio-sensor tech-nology into consumer and industrial end-applications. Neu-roSky MindKit-EM TM features two key technologies: (i) ThinkGear-EM TM headset and (ii) eSense-EM TM software (i.e. brainwave interpretation software). The headset is used to extract, filter, and amplify brainwave (EEG) signals and convert that information into digital mental state out-puts for eSense-EM TM software. The EEG signals read by the MindKit-EM TM are detected on the forehead via points Fp1 (electrode placement system by the International Fed-eration in Encephalography and Clinical Neurophysiology). The headset has three dry active sensors: one sensor located on the forehead and two sensors are located behind the ears as ground/reference sensors. It also has electronic circuitry that filters and amplifies the brainwaves. The eSense-EM software further processes and analyses the obtained brain-wave signals into two useful neuro-sensory values: the user X  X  Attention 4 and Meditation 5 levels at any given moment. The output of eSense-EM TM software has been tested over a wide population and under different environmental condi-tions, to work across a wide spectrum of individuals.
For our affective features, we considered 19 features (re-ferred to as  X  X X X ) extracted from the output of eMotion: 7 features related to the classified emotions (i.e. happiness, sadness, anger, fear, disgust, and surprise) plus neutral and 12 features related to the motion units (MU X  X ) data, which is a low-level category of features very similar to Ekman X  X  action-units (AU X  X ) [10]. For our physiological features, we considered the heart rate data (referred to as  X  X R X ) from the output of the Polar RS800 Heart Rate Monitor; the galvanic skin response, skin temperature, near-body ambient temper-ature, heat flux, and motion data (referred to as  X  X B X ) from the output of the BodyMedia SenseWear R  X  Pro3 Armband; and the Attention or Meditation data (referred to as  X  X V X ) from the output of the eSense-EM TM software. For our be-havioural signal, we considered the dwell time (referred to as  X  X T X ) logged by the system as our dwell time feature. Finally the task intention was considered as task feature (referred to as  X  X ask X ).

Preprocessing: For each visited video, the value of each sensory feature (for both affective and physiological features) was calculated by averaging the data logged by its sensory device during the dwell time period. Since none of the instruments we used normalised the data, we scaled signal values before applying any classification method, to avoid having attributes in greater numeric ranges dominat-ing those in smaller numeric ranges.
For the completion of the search tasks we used a custom-made search environment (named VideoHunt ) that was de-signed to resemble the basic layout of existing video retrieval services, while retaining a minimum of graphical elements and distractions. VideoHunt works on top of the Bing Video Search API. For every query submitted, it returned a list of 100 results (36 results on each page), stripped of their title, snippet and any other metadata. This layout was inten-tional to ensure that the participants would judge the rele-vance of videos only after they have examined them. Even though this approach introduced our participants into artifi-cial search situations which differ from real-life experiences, it was a necessary trade-off for capturing affective responses exhibited towards the viewed content.

Search Interface: The layered architecture approach of VideoHunt interface is shown in Figure 1. The first layer of the interface is dedicated to supporting any interaction that occurs during the early stages of the search process (such as query formulation and search execution). Any out-put generated during this phase is presented in the second layer (shown in Figure 1 -A). From there, participants could easily select and preview any of the retrieved clips. The con-tent of a clip is shown on a separate panel, in the foreground (shown in Figure 1 -B), which corresponds to the third layer of our system. The main reason behind this layered archi-tecture was to isolate the viewed content from all possible distractions that reside on the desktop screen; therefore, es-tablishing additional ground truth that allowed us to relate participants X  affective responses to the source of stimuli (in our case, the perused videos). This was an important aspect of our experimental methodology, since we were interested in isolating content-specific emotions. In addition, in the context of video retrieval, we would expect less interaction with the system during the dwell time making the captured physiological signals particularly valuable in this type of en-vironment. Upon viewing the clip, the participants had to explicitly indicate the relevance of the video. Finally, the length of time a user spent watching a clip (dwell time) was monitored and logged by the search interface.

Pilot Studies: Prior to running the actual user study, a pilot study was performed using 5 participants to confirm that the process worked correctly and smoothly. A number of changes were made to the system based on feedback from the pilot study. The changes consisted of modifications to the system to improve logging capabilities and improvements to the tasks. After the final pilot, it was determined that the participants were able to complete the user study without problems and that the system was capturing all necessary data.

Participants: Participants consisted of 24 healthy peo-ple with equal gender distribution (12 female and 12 male) all under the age of 41, with the largest group between the ages of 18-23 (45.8%) followed by the group between ages of 24-29 (36%). Participants tended to have a high school diploma or equivalent (4.16%), college degree (4.16%), bach-elors (41.66%) or graduate degree (50%). They were pri-marily students (62.5%), though there were a number of self-employed (16.6%), not employed (4.16%) and employed by a company or organisation (16.6%).
In this section we present the experimental findings of our study, based on the 96 search sessions that were carried out by 24 subjects. We first discuss the task perception ex-pressed in the questionnaire. Following this, we discuss the predictability of search intentions based on features derived from the user interaction with the system in Section 4.1.
Task Perception: Figures 2 shows the box plots for the qualitative analysis of users X  perception of the four tasks (i.e. INS, INF, ENA, and ENM). Each box plot reports data ag-gregated from 24 participants, along with five key statistics: the minimum, first, second (median), third, and maximum quartiles. 6 We performed an ANOVA test between measures obtained at each phase, across four search tasks for each user to check the significance of the difference among them. The test is suitable for this data as we have four groups of data, therefore we need to compare four means and variances. We use (*) and (**) to denote the fact that a measure had re-sults different across four search tasks with the confidence levels ( p &lt; 0 . 05) and ( p &lt; 0 . 01), respectively.
In the post-task questionnaire we measured participants X  perception of their performed task in terms of the diffi-culty of the task, the familiarity of the participant with the task, the extent to which they found the task stressful, in-teresting and clear by asking the following question  X  X he task we asked you to perform was [easy/stressful/interesting/ clear/familiar] (answer: 1:  X  X trongly Disagree X , 2:  X  X is-agree X , 3:  X  X eutral X , 4:  X  X gree X , 5:  X  X trongly Agree X ) X  . The results shown in Figure 2 indicate that participants found the INF task difficult and stressful, followed by the INS task, whereas they found other two tasks easy and not-stressful (the differences were statistically significant). The differ-ence in the answer provided by the participants for interest-ing, clear and familiar measures is not statistically signifi-cant. In the post-task questionnaire we also asked the opin-ion of the participants with respect to the following state-ment  X  X  had enough time to do an effective search.] (an-swer: 1:  X  X trongly Disagree X , 2:  X  X isagree X , 3:  X  X eutral X , 4:  X  X gree X , 5:  X  X trongly Agree X ) X  . The results show that they found the time given enough to do an effective search task, (INS: M=4.0 SD=0.88, INF: M=3.8 SD=0.96, ENA: M=4.0 SD=0.97, ENM: M=4.3 SD=0.56, the differences are however not statistically significant across the tasks).
In this section, we investigate our research questions in-troduced in Section 1. We first study the main effect of using affective, physiological and behavioural signals for the relevance judgement prediction task, by combining the rele-vance judgement obtained from our four search intents. We then study the interaction effect of these signals with search intents for the relevant judgment prediction task. For this purpose, the features explained in Section 3.3.1 were used. In total, participants judged 488 videos (with 247 relevant, 50.60%), of which 141 (with 82 relevant, 64.53%), 128 (with 91 relevant, 64.06%), 133 (with 73 relevant, 54.88%), and 86 (with 1 relevant, 1.17%) videos belong to  X  X NM X ,  X  X NA X ,  X  X NS X , and  X  X NF X  respectively. Due to the ratio of relevant to non-relevant videos for INF task, we consider two combi-nation scenarios: one where the relevance judgments of all tasks are combined (referred to as  X  X LL X ) and another one where the relevance judgments of all tasks are combined, except those for the INF task (referred to as  X  X LL -INF X ).
For relevance prediction, we have a binomial classifica-tion problem where the classes are  X +1 X  (relevant) and  X -1 X  (non-relevant). We used SMO, an implementation of SVM in Weka, 7 to discriminate between the two classes explained above. We trained our models using a polynomial kernel which in the majority of cases outperformed other SVM ker-nels (e.g. radial-basis) based on our analysis, not presented due to the space limits. For each classification method we present only the model which achieved the best performance among the rest in its category.

Table 2 shows the classification performance averaged over the 24 participants of the study across four different search tasks. It reports the accuracy of the model (i.e. fraction of items in the test set for which the models X  predictions were correct) using 10-fold cross-validation.

For the scenarios where only sensory signals are used to train a model, the baseline represents an untrained model where all its predictions are biased to a dominant class (re-ferred to as  X  X L1 X ). This baseline is more realistic than a baseline which is based on a random choice (i.e. where its accuracy is set to 50%). Any feature set that results in higher accuracy than what is reported for BL1 is as a re-sult of the discriminative power of the set itself. For the scenarios where dwell time and sensory signals are used to train a model, the baseline represents a model trained on the dwell time feature only (referred to as  X  X L2 X ). Finally, for the scenarios where dwell time, task and sensory signals are used to train a model, the baseline represents a model trained based on dwell time and task features (referred to as  X  X L3 X ). We also performed a paired Wilcoxon test be-tween the predictions obtained for each model to check the significance of the difference with its baseline. We use [(*) and (**)], [(  X  ) and (  X  X  )], and [(  X  ) and (  X  X  )] to denote the fact that a model trained on a set of features had results different from that of  X  X L1 X ,  X  X L2 X , and  X  X L3 X  with the confidence levels [( p &lt; 0 . 05) and ( p &lt; 0 . 01)] respectively.
Main Results: The key findings which emerged from the results are that combining dwell time with sensory sig-nals provides a reliable set of features for IRF techniques. This means that sensory signals can be used as a good sub-stitution for task information for dwell time based IRF tech-niques. We also observed that the effectiveness of dwell time as well as sensory channel signals vary across the search in-tentions. In the remainder of this section we discuss each of our research questions in more detail.

RQ1: which combination of affective, physiologi-cal and behavioural signals provide a reliable set of features for IRF techniques? In order to answer this research question we study the main effect of these signals for the relevance prediction task. This is an attempt to sim-ulate a real life scenario where the system has to predict the relevance judgement for search processes with different (and in the majority of the cases, unidentified) intentions. Therefore, we only consider the results obtained for the sce-narios where the model has to predict the relevance judge-ment gathered from multiple search intentions (i.e.  X  X ll -INF X  and  X  X LL X  columns).

Behavioural and Sensory Signals: Considering dwell time with only one sensory signal at a time as a feature set, the findings show that the combination of facial expression fea-tures and dwell time results in the highest accuracy (i.e.  X  X T+FX X  row). The findings show that a higher accuracy can be achieved when the model has been trained on a fea-ture set that has dwell time and all the sensory signals (i.e.  X  X T+FX+AB+HR+NV X  row). This finding suggests the complementary nature of the information obtained through various behavioural and sensory signals. Furthermore, they also outperform the scenario where dwell time and task fea-tures are used to train a model, showing that this set of features is a reliable substitute for IRF techniques
Sensory Signals: Considering each sensory signal indi-vidually as a feature set, the findings show that for the sit-uation where the model has to predict the relevance judge-ment gathered for all studied search intentions (i.e.  X  X LL X  column), facial expression features results in the highest ac-curacy (i.e.  X  X X X  row). However such an observation cannot be made for the situation where the model has to predict the relevance judgement gathered for multiple search inten-tions except those of the re-finding intention (i.e.  X  X ll -INF X  column). The difference in the accuracy of facial ex-pression features between  X  X LL X  and  X  X ll -INF X  situations suggest that facial expression features can discriminate rele-vance judgments of the tasks which have a significant differ-ence in their difficulty. The results indicate that if there is no significant difference in the task difficulty, the heart rate feature provides the highest accuracy in relevance judgement prediction (i.e.  X  X R X  row). The findings show that a higher accuracy again can be achieved when the model has been trained on a feature set that is based on all sensory signals (i.e.  X  X X+AB+HR+NV X  row).

Behavioural Signals: Considering only dwell time, the findings show that for the situation where the model has to predict the relevance judgement gathered for all studied search intentions (i.e.  X  X LL X  column), the dwell time feature results in a high accuracy level. However, once the model has to predict the relevance judgements gathered for multi-ple search intentions except those of re-finding intention (i.e.  X  X ll -INF X  column), its effectiveness decreases. Our findings also show that the combination of dwell time and task in-formation improves the accuracy of the relevance judgement prediction significantly, only if the performed tasks varies in some aspect, such as difficulty (as it is in our case). This finding supports the results obtained by Kelly and Belkin [19]. We observe that if the performed tasks does not vary significantly in their characteristics, task information do not provide any additional value and it can also significantly harm the accuracy.

In the Presence of Task Information: Finally, we investi-gated the effect of having task information along with dwell time and all the sensory signals on the accuracy of relevance judgment prediction (i.e.  X  X T+Task+FX+AB+HR+NV X  row). The findings show that the prediction accuracy of a model trained on these features is significantly higher than the prediction accuracy of a model trained on dwell time and task features significantly (i.e.  X  X T+Task X  row). The results also show that the prediction accuracy of such a model is even higher than a model trained on all features except task one (i.e.  X  X T+FX+AB+HR+NV X  row). This show that re-searches on task prediction are complementary to this study rather than contradictory.

RQ2: What would be the accuracy of these fea-ture sets with respect to different search intentions? In order to answer this research question we study the in-teraction effect of these signals with search intentions for relevance prediction task. Therefore, we consider the results obtained for the scenarios where the model has to predict the relevance judgement gathered from one search intention at a time (i.e.  X  X NS X ,  X  X NM X ,  X  X NA X  and  X  X NF X  columns). Overall, the results do not show any general pattern, mean-ing that the effectiveness of different signals are task de-pendent. We exclude the  X  X NF X  intention from our further analysis, since for  X  X NF X  intention, almost all of the judged videos were non-relevant, additionally the accuracy of  X  X L1 X  model is too high to be outperformed by any of our models.
The findings suggests that sensory channels combined with dwell time performs better for the search intentions which do not have a clear information need (i.e.  X  X NM X  and  X  X NA X ) than the ones that do (i.e  X  X NS X ). For search processes with entertainment-based intentions, a combination of heart rate and dwell time features results in the highest accuracy. For search processes with information seeking intention, com-bination of facial expression and dwell time feature is the highest accuracy. In particular, the results suggest that for the ENM task, heart rate and skin temperature; for ENA task, heart rate and EEG; for INS task, facial expression and skin temperature are the best two sensory signals, once they are combined with dwell time. Search processes for which the purpose is to satisfy an information need, facial expres-sion and skin temperature are the best two sensory channels once they are combined with dwell time information.
An interesting finding is that the discriminative power of sensory signals changes once they are combined with dwell time, even though they show no such power individually. For example, a sensory feature that was not discriminative on its own for a task (e.g.  X  X R X  feature for  X  X NM X  task), when combined with dwell time, can result in the highest predic-tion accuracy (i.e.  X  X T+HR X  features for  X  X NM X  task). In addition, the findings suggest that across all search tasks, combining each sensory signal with dwell time results in a higher accuracy compared to the situation where only either of them is used. The combination of all the sensory channels does not result in the highest accuracy, except for the  X  X NA X  task. Similar behaviour is observed when these features are combined with dwell time one.

Finally, comparison of the accuracy obtained from BL2 (dwell time) and BL1 show that dwell time performs bet-ter for tasks based on information needs (i.e.  X  X NS X ) than tasks based on hedonistic needs (i.e.  X  X NA X  and  X  X NM X ). However, the findings show that the accuracy of the dwell time feature and sensory channels by themselves is not high and is task dependent. This finding supports the results ob-tained by previous studies, i.e. [19] for dwell time feature and [3] for sensory features.
In this paper we investigated the discriminative power of different sensory channel signals as a reliable substitute for task information for IRF techniques based on dwell time, given different search intentions. In order to do so, we de-vised four search tasks, each simulating one of the following search intentions: information seeking, re-finding, entertain-ment adjusting arousal level and entertainment adjusting mood. Using a video retrieval system as a use case, we con-ducted a user study with 24 participants. We then studied two research questions: (1) which combination of affective, physiological and behavioural signals provide a reliable set of features for IRF techniques? and (2) what would be the ac-curacy of these feature sets with respect to different search intentions? The findings show that combining dwell time with sensory signals provides a reliable set of features for IRF techniques, and the sensory signals can be used as a good substitution of task information for dwell time based IRF techniques. We also observed that the effectiveness of both the dwell time feature as well as sensory channel sig-nals vary across different search intentions. The major im-plication of our work is in developing real time interactive systems.

Finally, although the findings of this paper are limited to the video retrieval domain and not generalizable, they moti-vate the exploration of similar hypotheses in other domains. Even though significant improvement in relevance prediction were achieved by adding affective and physiological measures across different search task types, we acknowledge the fol-lowing limitations: first, the experiment was conduced in an artificial situation (i.e. a laboratory setting) which lacks the ecological validity of a naturalistic study. Second, only one behavioural signal (i.e. dwell time) was considered in this experiment. Our positive findings encourage us to explore more exhaustively behavioural signals such as click-through and mouse movement as well as eye movement in combi-nation with affective, physiological signals for future work. Finally, it will be some time before physiological signals can be used by practical applications. Therefore, the study in this area is in its infancy. There are still many research ques-tions left to be answered. The findings of this paper lay the foundation for further studies to address these areas.
This work was supported by the EU FP7 LiMoSINe project (288024).
