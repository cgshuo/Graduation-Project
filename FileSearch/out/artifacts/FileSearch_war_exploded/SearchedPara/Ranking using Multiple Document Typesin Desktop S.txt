 A typical desktop environment contains many document types (email, presentations, web pages, pdfs, etc.) each with different metadata. Predicting which types of documents a user is looking for in the context of a given query is a crucial part of providing effective desktop search. The problem is similar to selecting resources in distributed IR, but there are some important differences.

In this paper, we quantify the impact of type prediction in producing a merged ranking for desktop search and in-troduce a new prediction method that exploits type-specific metadata. In addition, we show that type prediction per-formance and search effectiveness can be further enhanced by combining existing methods of type prediction using dis-criminative learning models. Our experiments employ pseudo-desktop collections and a human computation game for ac-quiring realistic and reusable queries.
 H.3.3 [ Information Storage and Retrieval ]: [Informa-tion Search and Retrieval] Algorithms Information Retrieval, Desktop Search, Semi-structured Doc-ument Retrieval, Type Prediction, Human Computation Game
People have many types of documents on their desktop with different sets of metadata for each type. For instance, emails have sender and receiver fields, whereas office doc-uments have filename and author fields. Considering that personal information is now increasingly spread across var-ious places on the web, this diversity of document types is continuing to increase.

Desktop search systems, which is now a standard feature of most platforms, have tried to exploit this type information by presenting search results for each document type sepa-rately (e.g., Apple Spotlight) or showing type information distinctively in a single ranked list (e.g., Google Desktop).
In both scenarios, a critical aspect of the system is be-ing able to predict which type(s) of document(s) a user is looking for given a query. If the system displays separate type-specific results, it can rank them by their type scores. Alternatively, the system can incorporate type scores into document ranking as a feature.

The type prediction problem bears some similarity to the vertical or resource selection problem in aggregated or feder-ated search in that the system tries to score the results from each vertical, resource, or collection based on predicted rel-evance for a given query. In this sense, all these problems can be put in a broad category of collection scoring. There are, however, several notable differences.

First, type-specific sub-collections in the desktop are co-operative in that all the documents are available to a sin-gle system. This means that sampling techniques used for federated search may not be necessary for desktop search; second, unlike typical collections used for aggregated search, the sub-collections in the desktop environment are small and have considerable topical overlap. This makes it challenging to apply content-based collection scoring techniques (e.g., CORI [4]) directly; third, each sub-collection in the desktop has unique metadata that has not been exploited in existing collection scoring methods.

The main goal of this paper is to show how the retrieval effectiveness of a desktop search system can be enhanced by improving type prediction performance. We focus on known-item queries, which are the most frequent type of request in the desktop environment [8], and assume that the system displays a final rank list by merging type-specific results.
Our work makes several contributions: first, we demon-strate the impact of sub-collection retrieval and type pre-diction performance on the quality of the final rank list; second, we suggest a type prediction method that exploits type-specific metadata and show that the new method has better performance than a state-of-the-art collection scoring method; third, we find that a combination of collection scor-ing methods can improve the performance further; fourth, we employ a game interface to collect a large quantity of known-item queries in a reasonably realistic setting.
The rest of this paper is organized as follows. We pro-vide an overview of related work in the next section. Then we introduce the retrieval model, type prediction methods, Figure 1: Suggested retrieval model for desktop search. and test collection generation methods we used. In the ex-periments, we report retrieval and type prediction perfor-mance using pseudo-desktop collections and a computer sci-ence (CS) collection where queries are collected using a game interface.
Related work can be found in several different areas: desk-top search, vertical search, and distributed IR.

For desktop search, people have studied user interface issues [7], retrieval models, and evaluation methods [10]. Among these, Thomas et al. [19] views desktop search as a meta-search problem where the results from many servers are merged. They compared several server selection methods using documents collected from various sources, concluding that a selection method based on Kullback-Leibler diver-gence [18] performed the best. Our work extends this work by suggesting a prediction method that exploits the field structure and a combination method whose performance can be improved by interaction with the user.

To evaluate desktop search, methods for building test col-lections [5] [10] have been proposed. Among these, the pseudo-desktop method by Kim et al. [10] generated test collections automatically by simulation, based on a tech-nique suggested by Azzopardi et al.[3]. Our work employs these pseudo-desktop collections for the retrieval experiments and improves the procedure of gathering human-generated queries by employing a game interface.

In the context of aggregated search and distributed IR, re-searchers have proposed many methods of scoring collections against a given query. Approaches such as CORI [4] and KL-Divergence [18] treat collections as large documents and apply document scoring techniques for scoring collections. Other methods, such as ReDDE [17] model the distribution of relevant documents for each collection. Recently, Arguello et al. proposed a classification approach [1] [2] where many sources of evidences can be combined for mapping a user X  X  query into one or more collections. Our combination ap-proach is similar to this work but we use features and eval-uation methods more suitable for our problem domain.
In this section we introduce a retrieval model for desktop search. In our retrieval model, as depicted in Figure 1, type-specific results (rank list and type score) are merged into a final rank list. We first explain methods used for the retrieval of sub-collections corresponding to each file type. Then we introduce the type prediction and result merging methods we used.
 The following notation will be used throughout this paper. We assume that a query Q = ( q 1 ,...,q m ) is composed of m words and each collection C contains documents with n field types ( F 1 ,...,F n ) where n can be different for each collection. Each document d in the collection may include fields ( f 1 ,...,f n ), where each field is marked using lowercase letters to distinguish it from the corresponding field type in the schema. Model-specific parameters will be explained as they appear.
The first step in our retrieval model is ranking documents from each sub-collection. Since our focus is on type predic-tion, we employ retrieval models used in the recent work by Kim et al. [10] on desktop search, which includes document query-likelihood (DLM), the probabilistic retrieval model for semistructured data (PRM-S) and the interpolation of DLM and PRM-S (PRM-D). We explain the PRM-S model in the following section.
The probabilistic retrieval model for semistructured data (PRM-S) [11] scores documents by combining field-level query-likelihood scores similarly to other field-based retrieval mod-els [13]. The main feature of the PRM-S model is that weights for combining field-level scores are estimated based on the predicted mapping between query terms and doc-ument fields, which can be efficiently computed based on collection term statistics.

More formally, using Bayes X  theorem, we can estimate the posterior probability P M ( F j | w ) that a given query term w is mapped into document field F j by combining the prior probability P M ( F j ) and the probability of a term occurring in a given field type P M ( w | F j ).

Here, P M ( w | F j ) is calculated by dividing the number of occurrences for term w by total term counts in the field F across the whole collection. Also, P M ( F j ) denotes the prior probability of field F j mapped into any query term before observing collection statistics.

With the mapping probabilities estimated as described above, the probabilistic retrieval model for semistructured data (PRM-S) can use these as weights for combining the scores from each field P QL ( w | f j ) into a document score, as follows:
This model was shown to have better performance than other field-based retrieval methods, such as the mixture of field language models [13] and BM25F [15], for a semi-structured document retrieval task using the IMDB [11] and TREC email [10] collections.
As briefly mentioned in the introduction, an integral part of our retrieval model is the type prediction component, which scores each collection given a user query. The type prediction method will produce scores for each sub-collection and these scores can be used for merging results into the final rank list or ranking sub-collection results. Section 4 deals with type prediction methods in detail.
With type-specific rank lists from sub-collection retrieval and collection scores from the type prediction component, we can produce the final rank list by rank-list merging algo-rithms. In this work, we use the well-known CORI algorithm for merging [4].
 Here, C 0 i and D 0 are normalized collection and document score, computed using the maximum and minimum of col-lection scores ( C max / C min ) and document scores ( D max D min ), respectively. Given C 0 i and D 0 , the final document score D 00 can be computed by combining these two scores.
In this section, we introduce our type prediction methods in detail. We first describe existing methods for type predic-tion which are adopted from recent works on aggregated and federated search [1] [2]. Then we introduce a new type pre-diction method that exploits document metadata. Lastly, we explain how type prediction methods can be combined using several learning methods.
Many traditional resource selection methods (e.g. CORI) are computed from collection term statistics. Among these, we use collection query-likelihood (CQL)[18], which is a re-source selection method based on the language modeling ap-proach. The approach here is to collapse all documents in each collection into one giant  X  X ocument X  and use the query-likelihood score for the document as the collection score:
Here, C is the language model of each sub-collection and G is the language model of the whole collection. The smooth-ing parameter  X  adjusts the interpolation ratio of P ( q | C ) and P ( q | G ). CQL was shown to be the most effective among resource selection methods in a recent evaluation [19].
Another source of evidence for the type prediction is the aggregated query terms used for finding documents that belong to each sub-collection. As done in previous work [1] [2], we use the query-likelihood score of the language model (QQL) built by queries targeted for sub-collection C as shown below:
Here, L C is the language model of the query log corre-sponding to collection C . L G is similarly defined using the query log across all collections.
Another class of resource selection methods combine the score of top documents to evaluate each collection given the user query. Seo et al. [16] proposed using the geometric mean of the top m documents as the combination method, where D top is the set of top m documents from the collec-tion and the score P ( Q | d ) is padded with P min ( Q | d ) if fewer than m documents are retrieved.
ReDDE [17] [2] scores a target collection based on the expected number of documents relevant to the query. Al-though previous work used a centralized sample index to derive this expectation, we can estimate this directly from the target collection, which is equivalent to using the sum of the top document scores belonging to each collection. Intuitively, this results in a higher score for the collection with more documents in higher positions.
So far, most of our methods have been derived from re-source selection techniques developed in the context of dis-tributed IR. Query performance prediction methods can also be used for type prediction by assigning a higher score for the collection with higher predicted performance. Among such methods, we employ Query Clarity [6], which predicts performance using the KL divergence between a query lan-guage model and a collection language model.

Here, query language model L Q is estimated from the top m documents from the collection.
In some cases, users provide direct clues about which file type they intended to search, by including terms such as  X  X ender X (for email),  X  X df X (for office document) or  X  X ww X (for webpage). Although these terms may not occur in a ma-jority of queries, they can be a strong indication of type membership for a given query. We built the dictionary for each sub-collection by using the names of the collection and metadata fields.
Although some of methods introduced above use the col-lection term statistics, none use the field structure of doc-uments available for desktop collection. Considering that the retrieval effectiveness of semi-structured document col-lections have been improved by exploiting this structure [11], we can expect similar benefits for the type prediction prob-lem.

Field-based collection query likelihood (FQL)  X  our new method for type prediction  X  extends the collection query likelihood model for collection scoring by combining the query-likelihood score for each field of the collection instead of us-ing the score for the whole collection. In other words, if we borrow the view of query-term and field mapping described in Section 3.1.1, we try to infer the mapping between a user query and each collection by combining mapping probabili-ties for the fields of each collection.

More formally, for a collection C that contains documents of n field types ( F 1 ,...,F n ), we can combine the language model score of each field as follows:
Here, F i is a smoothed language model of the i th field of the collection and comb can be any function that can combine n numbers into one. We experimented with many variations of comb function and found that arithmetic mean gives the best performance.
Considering that the type prediction methods introduced so far are derived from different sources, it is plausible that we can get further performance benefits by combining indi-vidual methods in a linear model where weights are found using learning methods. In this section, we describe three learning methods with different objective functions: grid search of parameter values, a multi-class classifier and a rank-learning method.
Since we have only seven features to be combined, It is fea-sible to perform a grid search of parameter values that max-imize the performance of a training set of queries. Specif-ically, we can find the optimal value for each parameter in turns while fixing the values of the parameters previously found, and repeating the whole procedure until we reach convergence. In searching for the optimum value of each parameter, we employed Golden Section Search[14].
Given that we want to predict one of k document types for a given query, this is typical multi-class classification scenario where each type corresponds to a class. Among many choices of such methods, we used a one-vs-rest (OVR) support vector machine classifier (MultiSVM) available in Liblinear Toolkit 1 . Since the output of this classifier is not suitable to be used directly as type scores, we used a simple linear transform to convert the scores into probabilities.
Alternatively, one can cast the type prediction task as a ranking problem where we try to rank the relevant collec-tions higher than non-relevant collections. This approach can be especially beneficial for the case where the user is finding multiple documents with different types, since such http://www.csie.ntu.edu.tw/ cjlin/liblinear/ situation is hard to model with typical multi-class classifica-tion methods. RankSVM [9] was used as the rank-learning method.
Evaluation of desktop search has been considered a chal-lenging issue due to the private nature of collections. As a solution, methods for building reusable test collections have emerged recently [5] [10]. Here we review the test collec-tion generation methods and introduce our game interface for gathering known-item queries.
Kim et al. [10] introduced the pseudo-desktop method of automatically building a reusable test collection for desktop search. They collected documents with similar characteris-tics to a typical desktop environment and generated queries by statistically taking terms from each of the target doc-uments. The generated queries are validated against a set of manually written queries, which were gathered by show-ing people documents and asking them to write queries for finding those documents.

Although this pseudo-desktop method provides a cost-effective way to generate a large quantity of test data under perfect experimental control, there are several limitations: first, the query generation procedure is too simple in that it independently chooses words from the target document, while real queries contain phrases; second, only terms in tar-get documents are used as queries; third, the procedure of getting manual queries is not realistic in that people were asked to formulate queries for documents they were not fa-miliar with.

It should be possible to mitigate the problems above by refining the query generation method, but we decided in-stead to improve the procedure of gathering manual queries by developing a human computation game, as explained in the next section.
Human computation games [20] have recently been sug-gested as method for getting a large amount of human anno-tations in a way that motivates participants using a game-like setting. In the context of IR research, Ma et al. [12] introduced Page Hunt, which is a game designed to collect web search log data by showing each participant webpages and asking her to find them with the search interface pro-vided.

By adapting Page Hunt to our problem domain, we devel-oped the DocTrack game for gathering known-item queries in the desktop environment, as shown in Figure 2. In addi-tion to using documents of many types that might be found in a desktop instead of random webpages, we made several modifications to the original Page Hunt game: first, since people generally have good knowledge of their own desk-tops, we collected documents that participants are familiar with and let each of them browse the collection for some time before starting the game; second, to simulate a typical known-item search scenario, we showed participants multi-ple documents and asked them to find one of them without specifying which one is the target document; third, we used a document viewer that can show documents of any types Figure 2: The screenshot of the DocTrack game. The user is being shown a document. (e.g. pdf, doc and ppt) in the same way they are seen on the desktop.
 Compared to the method of collecting manual queries in Kim et al.[10], using the DocTrack game, we could gather many more realistic queries together with the whole session log data. This in turn allowed us to perform the training of discriminative learning models which typically requires large amounts of training data.
In this section we describe the experiments for verifying the type prediction and retrieval performance. We used three pseudo-desktop collections with generated queries for the first experiment, where we compared several type pre-diction methods and showed the impact of type prediction on the final ranking. We then report on experiments us-ing a computer science (CS) collection where queries were collected by the DocTrack game.

In the indexing of both collections, each word was stemmed using the Krovetz stemmer and standard stopwords were eliminated. Indri 2 was used as a retrieval engine for all the retrieval experiments. We used prediction accuracy to eval-uate type prediction performance, since we have only one correct collection for each query. Mean Reciprocal Rank was used as the measure of retrieval performance for all ex-periments, since this is a known-item task where each query has only one relevant document.

Four retrieval methods were used for each sub-collection (DLM / PRM-S / PRM-D / Best) and four methods (Uni-form / CQL / FQL / Oracle) were used for type prediction. We compared only CQL and FQL for the pseudo-desktop experiment since CQL was shown to be the most effective among collection scoring method [19] and FQL is the ex-tension of CQL for semi-structured document collections. Section 6.2 provides the comparison with other type predic-tion methods using the CS collection and queries from the DocTrack game.

For the Best retrieval method, we used the retrieval method with the best aggregate performance for each sub-collection, http://www.lemurproject.org Table 1: Number and average length of documents for pseudo-desktop collections.

Type Jack Tom Kate email 6067 555 6930 558 1669 935 html 953 3554 950 3098 957 3995 pdf 1025 8024 1008 8699 1004 10278 doc 938 6394 984 7374 940 7828 ppt 905 1808 911 1801 729 1859 Table 2: Query examples with corresponding target collections for pseudo-desktop collections.
 making the assumption that the best-performing retrieval method is known in advance. For Uniform and Oracle col-lection scoring, we considered that each collection has the same chance of containing the relevant document (Uniform) or that we have the perfect knowledge of the collection that contains the relevant document (Oracle).
Three pseudo-desktop collections described in Kim et al. [10] were used for these experiments. Each collection con-tained typical file types such as email, webpage and office documents related to three individuals, as shown in Table 1. For email, the indexed fields were title , content , date , sender and receiver . For the other document types, title , content , date , URL , summary were indexed.

To generate queries, we first chose the target document and took each query word based on the term frequency in a randomly chosen field of the document. For instance, the query  X  X rg address X  shown in Table 2 was generated by taking the term  X  X rg X  from the URL field and  X  X ddress X  from the title field, respectively. Kim et al. reported that queries generated using this method have the highest similarity to a set of manual queries.

For each experiment, we generated 50 queries of average length 2 where target documents were taken from each sub-collection in proportion to the number of documents it con-tains. Table 2 shows several queries we used. Lastly, all the experiments were repeated three times since the query generation procedure involves some randomness.
In Table 3, we compare the accuracy of type prediction in pseudo-desktop collections for the CQL and FQL methods, where FQL shows a clear improvement over CQL method. Although this result should be interpreted with some reser-vations because we are using simulated queries, the same trend was found in the experiment using manual queries. We also observe that both methods show reasonable perfor-mance in the Jack and Tom collections, which contain far more email documents than other types. From this, we can conclude that both methods are relatively robust against an imbalance of sub-collection sizes.
We now report the retrieval performance for the same Table 3: Accuracy of type prediction in pseudo-desktop collections.
 Table 5: Number and average length of documents in a computer science (CS) collection.
 queries in Table 4. The first noticeable trend is that both the choice of type-specific retrieval model and type prediction method has a big impact on the final result. Especially, Or-acle type prediction was much better than the FQL method, which in turns outperformed CQL across all collections. On the other hand, the Best retrieval method was not much better than the PRM-D and PRM-S methods.
Next we report on experiments using a computer science (CS) collection, where the documents of various types are collected from many public sources in the Computer Science department the authors belong to. As shown in Table 5, the CS collection contained emails from the department mailing list, news articles and blog postings on technology, calendar items of department announcements, webpages and office documents crawled from the department and lab websites. The documents in the CS collection are much shorter than in the other pseudo-desktop collections.

For all document types, title and content fields were in-dexed. Also, there were type-specific fields such as date , sender and receiver for email, tag and author for news ar-ticles, starttime and location for calendar items, URL for webpages, filename for office documents.

We used the DocTrack game for collecting queries. We had 20 participants who were students, faculty members and staff in the department, all familiar with the documents in the collection. In total, 66 DocTrack games were played and 984 queries were collected using 882 target documents, some of which are shown in Table 6.

The average length of queries was 3 . 97, which is longer than the reported length (2) in the other desktop search studies [7]. This may be due to people paying more attention to the task in the competitive game setting compared to typical desktop search. The standard deviation (1 . 85) of the query length was also quite high, implying that there is a considerable variation among the querying behavior of individuals.

The participants were generally in favor of the game, say-ing that playing the game was fun and felt reasonably similar to their search experience in the desktop.

Since some of features required data for estimation, we used 528 queries to obtain query-log feature (QQL) values and training parameters for other features. The rest (456) of the queries were used to evaluate the type prediction per-formance of features and combination methods by 10-fold Table 6: Query examples with corresponding target collections for a CS collection.
 Table 8: Significance test result for type prediction accuracy in a CS collection. Each cell shows the p-value of paired t-test between the accuracy of two methods.
 Method CQL FQL Grid RankSVM MultiSVM CQL 0.03 0.00 0.00 0.00 FQL 0.69 0.27 0.02 Grid 0.41 0.02
RankSVM 0.07 cross-validation. For the retrieval experiments, since many queries did not return any documents, we used only queries where the relevant document was ranked in the Top 50 result set during the game.
Table 7 summarizes the prediction accuracy result, com-paring two of the best-performing single feature runs (CQL / FQL) and combination methods (Grid / RankSVM / Mul-tiSVM). The result shows that all the combination runs im-proved performance over the best single feature runs given by FQL, which outperformed CQL in this collection as well. MultiSVM was shown to be the most effective among combi-nation methods. This is understandable considering that we had one target collection for each query, which is a natural setting for multi-class classification. RankSVM was slightly better than Grid but the difference was not significant.
The result of a significance test is reported in Table 8, which shows that the performance differences between CQL and all the other methods are significant with a p-value of 0 . 05 and that MultiSVM outperforms all the other meth-ods significantly with a p-value of 0 . 1 (using paired t-test). Overall, this means that suggested type prediction method (FQL) improves the performance of the CQL method and that the combination of features improves the performance further.
Table 9 shows the retrieval performance, comparing four retrieval methods (DLM / PRM-S / PRM-D / Best) and the same set of type prediction methods as above in addition to Oracle and Uniform methods.

The result mostly shows the same trends as the pseudo-desktop collections despite the big difference in experimental conditions (remember that queries were algorithmically gen-erated for the pseudo-desktop collections). FQL was better than CQL and all the combination methods outperformed CQL and FQL significantly (with a p-value of 0 . 05 using paired t-test).

The only exception is that the performance of MultiSVM was slightly worse than RankSVM. Given the superior pre-diction accuracy of MultiSVM, it seems that the procedure of converting the SVM output into the type prediction score caused some problems. We can also see that Oracle type pre-diction method and the Best retrieval method outperform methods and type prediction methods.
 CS collection.
 Table 11: Correlation among the prediction per-formance of features. Each cell shows the pearson correlation coefficient between the accuracy of two methods.
 Feature FQL Dict QQL Clarity GAVG ReDDE CQL 0.68 0.10 0.29 -0.03 -0.01 0.04 FQL 0.05 0.32 0.01 -0.01 0.01 Dict 0.28 0.02 -0.00 0.04 QQL -0.02 0.04 0.05 Clarity 0.17 0.05
GAVG 0.62 other methods, which leaves room for further improvement in both aspects.
For further analysis on the effectiveness of individual fea-tures, we next report the result of single-feature and leave-one-out experiments in Table 10, where we used only one feature (single-feature) or all but one feature (leave-one-out) to measure the performance.

All in all, features with higher single-feature performance had bigger impacts when they were omitted, as shown in the case of content-based features (CQL / FQL) and query log feature (QQL). On the other hand, features based on the initial retrieval result (GAVG / ReDDE) was shown to be ineffective. Presumably, this is because the documents of different types had different characteristics in this collection, which make the scores almost incomparable.

On the other hand, in some cases, low single-feature pre-diction accuracy did not necessarily translate into small dif-ference in the leave-one-out experiment. The dictionary-based feature (Dict) and the performance prediction feature (Clarity) were shown to have a high impact on combination performance despite having the lowest single-feature results.
The result above makes more sense when considered to-gether with the correlation among features in Table 11. Meth-ods based on collection term statistics (CQL / FQL) and top result set (GAVG / ReDDE) have high correlations within the group, which explains why performance does not suffer much when one of high-performing features (e.g. CQL and FQL) are omitted. On the other hand, that QQL does not correlate highly with any other features explains its high impact on the leave-one-out experiment.
In the experiments so far we have used the queries from all participants together. Another benefit of the feature com-bination approach is that it can adjust the result based on the querying behavior of each individual. For instance, if Table 12: Feature weights trained using queries written by each user. The last row shows the weights trained using all queries. All the names are anonymized.
 Jane 1 1 0.15 1 0.09 Yao 0.38 0.56 0.15 1 0 Rajiv 0.58 1 0 0 0 Tim 0.62 1 0 1 0.18 David 1 1 0 0 0
All Users 1 1 0 1 0.09 a user frequently includes terms that indicate a particular collection, the learning method can improve type prediction performance by assigning higher weight to the Dict or QQL features.

To explore this potential value of this personalized type prediction, we tried training feature combination methods only with queries written by each user and looked at how much variation it causes for resulting feature weights.
The results in Table 12 compare the weights trained for each user. It shows that content-based features (CQL / FQL) are not very helpful for some users (Yao), whereas the query log does not make a difference for others (Ra-jiv / David). Although this is a preliminary result, the weights show considerable variation, implying that person-alized training data produced different results for each user. We leave further analysis of this benefit of personalization for future work.
In this paper, we suggest a retrieval model for desktop search where type-specific retrieval results are merged into a final rank list based on type prediction scores. Using the pseudo-desktop and CS collections, we demonstrated that improving the type prediction method can produce signifi-cantly better final retrieval performance.

We also introduced field-based collection query likelihood (FQL)  X  a new type prediction method that exploits type-specific metadata  X  which shows superior performance to competitive baseline methods in both collections we tested. Although FQL is used in the context of desktop search, it can be used as a resource selection method for aggregated or federated search, as long as each document has field struc-ture.

Moreover, we developed a human computation game for collecting queries in a more realistic setting and used this data to train discriminative learning models that combines type prediction methods as features. Our results show that prediction methods.
 the combination method can improve type prediction per-formance compared to any of existing methods. We also explored the benefit of personalizing type prediction results, which may potentially improve the performance further.
Our work leaves many interesting challenges. Although this work showed the value of improving type prediction performance, better type-specific retrieval and result merg-ing algorithms should bring further performance gains. For example, we used only textual features for sub-collection re-trieval, and it should be possible to incorporate type-specific features (e.g. PageRank score for webpage).

Also, although we focused on the known-item search task in this paper, the retrieval model suggested here will be equally applicable for the ad-hoc search scenario. To gather training data for this scenario, the DocTrack game can be modified to gather ad-hoc queries using topic descriptions and corresponding sets of relevant documents.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by NSF grant #IIS-0707801. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] J. Arguello, J. Callan, and F. Diaz.
 [2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. [3] L. Azzopardi, M. de Rijke, and K. Balog. Building [4] J. P. Callan, Z. Lu, and W. B. Croft. Searching [5] S. Chernov, P. Serdyukov, P.-A. Chirita, [6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [7] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, [8] D. Elsweiler and I. Ruthven. Towards task-based [9] T. Joachims. Optimizing search engines using [10] J. Kim and W. B. Croft. Retrieval experiments using [11] J. Kim, X. Xue, and W. B. Croft. A Probabilistic [12] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta. [13] P. Ogilvie and J. Callan. Combining document [14] W. Press, S. Teukolsky, W. Vetterling, and [15] S. Robertson, H. Zaragoza, and M. Taylor. Simple [16] J. Seo and W. B. Croft. Blog site search using [17] L. Si and J. Callan. Relevant document distribution [18] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language [19] P. Thomas and D. Hawking. Server selection methods [20] L. von Ahn and L. Dabbish. Designing games with a
