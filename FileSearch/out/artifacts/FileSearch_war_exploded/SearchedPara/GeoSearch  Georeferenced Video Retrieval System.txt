 Conventional video search systems, to find relevant videos, rely on textual data such as video title s, annotations, and text around the video. Nowadays, video recording devices such as cameras, smartphones and car blackboxes are equipped with GPS sensors and able to capture videos with spatiotemporal information such as time, location and camera direction. We call such videos georefer-enced videos . This paper presents a georeferenced video retrieval system, GeoSearch , which efficiently retrieves videos containing a certain point or range in the map. To enable a fast search of georeferenced videos, GeoSearch adopts a novel data structure MB T R (Minimum Bounding Tilted Rectangle) in the leaf nodes of R-Tree. New algorithms are developed to build MB T Rs from geo-referenced videos and to efficiently process point and range queries on MB T Rs. We demonstrate our system on real georeferenced videos, and show that, compared to previous methods, GeoSearch substantially reduces the index size and also improves the search speed for georeferenced video data. Our online demo is available at  X  X ttp://dm.hwanjoyu.org/geosearch X .
 H.2.4 [ Database Management ]: Systems X  Query processing ; H.2.4 [ Database Management ]: Systems X  Multimedia databases ; H.2.8 [ Database Management ]: Database Applications X  Spatial databases and GIS Algorithms, Performance Georeferencing, Video search, Spatial Indexing  X 
This work was partially supported by the Brain Korea 21 Project in 2012 and Mid-career Researcher Program through NRF grant funded by the MEST (No. KRF-2011-0016029). This work was also supported by IT Consilience C reative Program of MKE and NIPA (C1515-1121-0003).  X  corresponding author
With the rapid popularization of video recording devices, not only the media made by professionals but also made by ordinary users called UCC (User-Created Contents) are taking a large por-tion in the web. Since identifying related video requires under-standing of video contents, which is a very heavy and complex process, current video search systems mostly rely on textual data such as video titles, annotations, and text around the video.
Nowadays, video capturing devices such as cameras, smartphones and car blackboxes are equipped with GPS sensors and able to cap-ture videos with spatiotemporal information such as time, location and camera direction. We call such videos georeferenced videos . For some applications, such spatiotemporal information plays key roles in querying georeferenced videos. For example, some people may want to find videos of specific event that occurred at specific time and location, e.g., videos of a traffic accident captured by car blackboxes, videos of a goal scene in a soccer play captured by users, or videos of a concert with celebrities captured by users.
This paper presents GeoSearch , a georeferenced video retrieval system, which efficiently retrieves videos containing a certain point or range in the map. GeoSearch adopts a novel data structure MB T R (Minimum Bounding Tilted Rectangle) in the leaf nodes of R-Tree, in order to enable a fast search of georeferenced videos, While traditional MBR (Minimum Bounding Rectangle) is popu-larlyusedtodescribeanareainspatialindexessuchasR-Tree, MBR is not suitable for describing the areas of moving scenes where its location and direction are continously changing. MB T R is designed to describe an area (or scenes) of moving location and direction. New algorithms are developed to build MB T Rs from georeferenced videos and to effi ciently process point and range queries on MB T Rs. We demonstrate our system on real georef-erenced videos, and show that, compared to previous methods, by adopting MB T R in the R-Tree, GeoSearch substantially reduces the index size and also improves the search speed for georeferenced video data.
 Our demo is available at  X  X ttp://dm.hwanjoyu.org/geosearch X . Due to space limitation, this paper focuses on a high-level overview of techniques and demonstration. Technical details are described in our technical report [10]. Georeferenced Multimedia Search: There are several approaches to the use of location information in image search. Toyama et al [21] used metadata in image search and developed a database for it. They enabled spatial image search by recording images with longitudinal and latitudinal coordinates and timestamps. Several commercial websites, such as Flickr and Woophy [1], also provide georeferenced image search. They focus on showing static images of the locations in a map and support simple query such as nearest neighbor query. Google Street View [2] provides streams of street view images of their own. We can reference these images by cam-era X  X  location and viewing direction.

Trajectory indexing methods were developed in order to effi-ciently store and search moving objects [16, 8]. Our indexing method for supporting moving scenes can be considered an exten-sion of trajectory indexing which takes into consideration camera directions as well as the location of the cameras.

Research associated with  X  X rajectory-based video indexing X  [7, 17] and  X  X patial indexing for video X  [20] have been published in the computer vision community. However, the problem they address is different from ours; their goal is to index the relative location of objects in the video, not the viewable scenes. They also use video content retrieval methods such as object segmentation and motion tracking to detect objects, while we use metadata to search georeferenced videos.

A viewable scene model for enabling georeferenced video search has been recently proposed by Ay et al. [3, 5]. Based on the view-able scene model, they developed methods for supporting point and range queries using MBR-based filtering. (We compare this method as a baseline method in our experiments in Section 3.) Our method builds an index GeoTree based on their viewable scene model.

They also developed a relevance video search method based on the viewable scene model, and proposed metrics to measure the relevance of video to the given range query [3]. The metrics com-pute the relevance scores based on the size of the overlapping area. Since computing the exact size of the overlapping area is computa-tionally expensive, they approximate it using grids and histograms. However, their approach requires vast storage to keep all the videos stored in grids, and also the accuracy is compromised due to the running of the query on the gridded data instead of the original data. Moreover, the overlapping area-based relevance may not re-flect the true relevance implied in the user X  X  query; user X  X  relevance may be more related to parts that are overlapped and the distance to the overlapping area. Inducing a good relevance function is itself a non-trivial research problem.

Ay et al. also proposed a method for generating synthetic meta-data for georeferenced video search [4]. Since it is often difficult to obtain a large set of georeferenced video data, this method can be used for evaluating new methods for georeferenced video search. Spatial Indexing: There are numerous works on spatial indexing and query processing in the database community. Many of the in-dexing structures are based on R-Tree [9, 13, 19]. Several works focus on querying location trajectories of moving objects [14, 18]. Also, several effective indexing structures such as STR-Tree and TB-tree were proposed [15]. These works mostly focus on dealing with temporal dimension and complex queries, rather than dealing with complex spatial data such as viewable scenes.

Spatiotemporal trajectory compressions for saving data storage and enabling fast query processing are also being researched [12, 8]. Their methods focus on the data of points or location trajectories that are not directly applicable to viewable scenes. There are also works that focus on indexing range data or region data that consider static rectangular objects [6, 11].

No effective indexing structure has yet been proposed that sup-ports exact queries on camera viewable scenes of moving locations and directions.
 Preliminary: Viewable Scene Model: The viewable scene changes, as the camera moves or rotates. The area captured by the camera is referred to as the Field-Of-View scene ( FOVscene ) [5]. As illus-trated in Figure 1(a), in a 2D space, four parameters are required to express a single FOVscene : (1) location of the camera P , (2) di-rection of the camera d , (3) viewable angle of the camera  X  ,and(4) visible distance R . Camera location P is represented as a (latitude, longitude) pair that can be obtained using a GPS sensor. Camera direction d can be obtained using a compass sensor. Camera view-able angle  X  and visible distance R can be obtained from the cam-era lens X  property and zoom level. As assumed in [5, 3], we also fixed  X  and R by assuming the use of static camera lens. (Some video recording devices such as smartphones and car blackboxes use static camera lenses.)
The camera view or the viewable scene information at a particu-lar moment is entirely expressed by an FOVscene .An FOVscene corresponds to a frame in the video. A georeferenced video can be represented by a sequence of FOVscene s. We use the term FOVstream to denote a sequence of FOVscene s. As done in [5, 3], we restrict FOVscene to a 2D space, however it can be ex-tended to a 3D space in a straightforward manner. Altitude and pitch should be considered in the 3D model (Figure 1(b)). but they do not alter the nature of the FOVscene model.
 Index Construction and Query Processing: To enable an efficient indexing of georeferenced vide o search, videos must be recorded with the metadata of ( P, d,  X , R ) for each FOVscene . Video record-ing devices such as car blackboxes and smartphones are nowadays equipped with sensors that have the ability to record metadata with videos. We construct a spatial index called GeoTree using the metadata. When a point or range query is submitted by users, it is processed on the GeoTree by finding the FOVstream s that over-lap the query area.
 GeoTree adopts a novel structure called Minimum Bounding Tilted Rectangle (MB T R). GeoTree uses MB T R instead of MBR (Minimum Bounding Rectangle) to represent an area of moving scenes in the index. GeoTree is a kind of R-tree using MBR in the nonleaf nodes and MB T R in the leaf nodes. Figure 2 depicts examples of MBR and MB T R representing areas of moving scenes. MB T R is more suitable for representing an area of moving scenes; as video recording devices such as car blackboxes are often moving piece-wise linearly, MB T R produces less false positive areas. For example, when a query, i.e., the red circle in the figure, is submitted, the MBR in Figure 2(a) includes the query area, thereby incurring a large area of false scanning of videos while the MB T R in Figure 2(b) efficiently prunes the videos. Figure 2: Examples of MBR (Minimum Bounding Rectangle) and MB T R (Minimum Bounding Tilted Rectangle). Red circle is the querying area. Blue circular sector is a viewable scene
Building MB T Rs requires modeling moving scenes piece-wise linearly such that each MB T R covers as large an area as possible while producing as small a number of false positive areas as pos-sible. Therefore, we first identify markup FOVscene s from mov-ing scenes, while considering the changing patterns of P and d . Markup FOVscene s are those at the edge of an MB T R, such that the FOVscene s between markup FOVscene s are moving linearly in terms of P and constant in terms of d , and thus any FOVscene s can be estimated from the markup FOVscene s. Onceasetof markup FOVscene s are found, MB T Rs from pairs of adjacent markup FOVscene s are constructed.
 A point or range query is processed in a similar fashion to R-Tree in the nonleaf nodes, as the nonleaf nodes of GeoTree are also described by MBRs. Once a query reaches a leaf node in the GeoTree , it is processed in two steps  X  (1) MB T R filtering and (2) MB T R lookup. MB T R filtering ascertains if the query can overlap the MB T R. If MB T R filtering returns false , this means that there is no FOVscene in the MB T R that overlaps the query. However, if MB T R filtering returns true ,MB T R lookup is called to compute and return an expected subsequence of FOVscene s that overlap the query. Since an MB T R typically contains numerous FOVscene s, it would be inefficient to scan all the FOVscene sintheMB T R to find the overlapping FOVscene s. We directly compute, using the MB T R structure, a sequence of FOVscene s that is expected to contain the query point. We compare the performance of GeoTree to R-Tree and MBR-Filter, i.e., the MBR-based filtering proposed in [3]. We evaluate the performance of processing point queries and range queries in terms of query processing time and also memory usage. Our ex-periments were done using a machine with the following specifica-tions: dualcore CPU (2.4GHz) and 4GB of Memory.
 DataSet: Experiments were performed on real FOVstream data sets. We captured real scenes using an Android mobile phone equipped with a GPS receiver and a 3D compass. We ignored al-titude and pitch angles from the 3D compass, as we are only inter-ested in 2D. Thirty FOVscene s were captured every second. Lat-itudinal and longitudinal data were converted into meter metrics in the 2D plane. No zoom level change was simulated, thus viewable angle  X  and visible distance R were kept constant and thus fixed to  X  =55  X  and R =50 m .

FOVscene s ranged in the area of 3.3km long in the north-south and 3.8km long in east-west. Test data corresponded to 11 streams with a total duration of 180 minutes. The number of FOVscene s was 325,313 in total. This number of streams corresponded to approximately 4.4GB of typical video data. The sum of all tra-jectories X  length was approximately 68km. For the leaf nodes of GeoTree with less than 5 FOVscene s, we simply used simple one-by-one search and did not perform MB T R filtering and lookup, because, when an MB T R contains just a few FOVscene s, scan-ning FOVscene s one-by-one runs faster due to some overhead of MB T R filtering and lookup.
 Result Summary: Table 1 summarizes the results of the query processing time for each method. For the range query test, we randomly generated 10,000 queries and measured the accumulated processing time for the queries. For the point query test, we ran-domly generated 100,000 queries and also measured the accumu-lated processing time for the queries. The processing times are averaged by 30 runs. For GeoTree , we first tuned the error thresh-old parameters on the dataset, and fixed the parameter values for all runs, these were. P =1 m and  X  =10  X 
MBR-Filter was one hundred times slower than the other tree-based searches. We omitted the naive sequential search results be-cause it was even much slower than MBR-Filter. GeoTree gave a consistently faster query processing time than R-Tree. Although the query processing times of R-Tree and GeoTree were not sub-stantially different, their memory usages were an order of mag-nitude different. As Figure 3 shows, the performance difference became more significant, as data size increased.
We plan to demonstrate our system on a real map of the cam-pus of POSTECH 1 . We captured various places of the campus and uploaded the videos to YouTube. We then built an online web-site,  X  X ttp://dm.hwanjoyu.org/geosearch/ X , which can search corre-sponding video frames using point or range queries. In our online search website, a user submits a point or range query by mouse clicks on the map, and our system returns corresponding video frames using Tubechop 2 . Figure 4(a) illustrates an example of a range query in our website. Figure 4(b) shows the result in text and links to video views. Once the user clicks a link, our system plays corresponding video frames as shown in Figure 4(c). http://www.postech.ac.kr www.tubechop.com: a site that shows a segment of YouTube videos
