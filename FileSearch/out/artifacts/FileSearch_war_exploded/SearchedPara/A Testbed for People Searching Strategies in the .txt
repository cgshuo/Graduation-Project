 This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names X  ambiguity and locat-ing relevant information characterising every individual un-der the same name.
 H.3.3 [ Information Storage and Retrieval ]: Clustering; H.3.4 [ Systems and Software ]: Performance evaluation Information Retrieval, Information Extraction, Web search Finding people -information about people-in the World-Wide-Web is one of the most common activities of Internet users: around 30% of search engine queries include person names [2]. Person names, however, are highly ambiguous: for instance, only 90,000 different names are shared by 100 million people according to the U.S. Census Bureau [2]. In are a mixture of pages about different people that share the same name. Instead of a ranked list of results, an ideal search engine would return a list of people descriptions, from and directly access all relevant information for this person. Figure 1 illustrates this idea.

In this paper, we describe the creation of a testbed to evaluate strategies addressing this people searching task on web documents. We provide: (i) a corpus of web pages retrieved using person names as queries to web search en-gines; (ii) a classification of pages according to the different people (with the same name) they refer to; (iii) manual an-notations of relevant information -found in the web pages-describing them (e-mail, image, profession, phone number, Figure 1: Mock-up interface of a search engine able to resolve person names ambiguity etc.); (iv) the results of applying a general purpose clus-tering algorithm to that annotated data, which serve as a baseline for the ambiguity resolution problem.
The creation of the WePS ( Web People Search ) corpus consisted of the following steps: 1. Generating ten English person names, using random combinations of the most frequent first and last names in the U.S. Census 1990 1 . 2. Collecting the first 100 web pages retrieved by the Google search engine for every (quoted) person name. 3. Grouping documents according to the person they refer to, for every person name. 4. Classifying every web document in the collection as a (i) homepage entry (ii) part of a homepage (iii) reference page (exclusively containing information about the person) and (iv) other . 5. Annotating all the occurrences of certain types of http://www.census.gov/genealogy/www/freqnames.html category instances per name per person home page 28 2.8 0.06 part of h.p. 15 1.5 0.03 reference p. 412 41.2 1 other 532 53.2 1.2 tags instances per name per person name 5,374 537.4 11.60 job 2,105 210.5 4.55 author of 1,823 182.3 3.94 definition 438 43.8 0.95 date birth 387 38.7 0.84 date death 256 25.6 0.55 image 232 23.2 0.50 place birth 386 38.6 0.83 email 282 28.2 0.61 location 185 18.5 0.40 phone num 136 13.6 0.29 address 86 8.6 0.19 place death 85 8.5 0.18 fax num 37 3.7 0.08
Total 11,812 1,181.2 25.51 descriptive information: name, job, person image, date of birth/death, place of birth/death, email address, postal ad-dress, fax/phone number, location (where the person lives scription (a brief definition of the person).

Table 1 summarises the results of this exhaustive anno-tation process. A total of 11,812 text fragments have been semantically annotated, with an average of 25.51 annota-tions per person. The ambiguity of our set of person names is very high, with an average of 41 different people sharing each person name (see Table 2). This indicates that they are very common names, and also that, in general, none of them corresponds to any web celebrity (i.e. a person dominating top-ranked web hits). The most common tags are name (which includes name variants), job and author of (mostly titles of books and other written materials). Note that there are few pages classified as home page , and even each identified person has, in average, one explicit descrip-tion ( reference page ).
How difficult is the ambiguity resolution task? Does it demand strategies specifically designed for it, or will generic clustering techniques suffice? Is it necessary to consider the full content of web pages, or the snippets provided by search engines provide enough information for an accurate grouping of results?
To provide initial answers to these questions, we have im-plemented and tested the Agglomerative Vector Space clus-tering algorithm, which has been previously used to evaluate similar tasks [1], and does not require fixing the number of clusters ( K ) a priori. In our experiments, terms have been weighted with a logarithmic tf-idf criteria.

The clustering method has been tested using two approaches to build the vector representation of the docu-ments: the first one ( full text ) uses all the textual contents of the web page as input for the algorithm, and the sec-ond one ( snippets ) only considers the snippets in the ranked Table 2: Clustering using full text/snippets as a bag of terms Ann Hill 55 51/38 .88/.81 .88/.88 Angela Thomas 36 34/37 .81/.88 .82/.88 Brenda Clark 23 30/27 .88/.87 .85/.84 Christine King 29 33/44 .67/.74 .70/.70 Helen Miller 38 46/64 .62/.65 .60/.57 Lisa Harris 30 33/36 .83/.79 .83/.76 Mary Johnson 54 40/41 .75/.77 .83/.83 Nancy Thompson 47 33/42 .81/.78 .81/.77 Samuel Baker 38 26/31 .79/.84 .87/.87 Sarah Wilson 62 35/47 .70/.86 .81/.86
Mean 41 36/40 .77/.79 .80/.79 lists provided by Google. Roughly speaking, they consist of a window of approximately 18 terms around one or more oc-currences of the person name in the web page. In both cases, we only take into account text inside the html &lt;body&gt; tag, removing stopwords and html tags. Words were stemmed using Porter X  X  algorithm. The similarity threshold for the clustering algorithm has been empirically adjusted to 0 . 1.
Table 2 shows the results of the experiment. Two differ-ent evaluation measures are reported: F  X  =0 . 5 is a harmonic mean of purity and inverse purity, and F  X  =0 . 2 is a version of F that gives more importance to inverse purity. Ratio-nale for using F  X  =0 . 2 is that, from a user X  X  point of view, it which has all the information needed, than having to col-lect the relevant information across many different clusters. In average, clustering with full text obtains F  X  =0 . 2 which can be seen as a reasonably strong baseline for the task. In addition, using only snippets (which can be much more efficient when searching online) gives F  X  =0 . 2 = . 79 (-1.3%). This small difference suggests that snippets can be useful for clustering. But, of course, the next step would be extracting all descriptive features of each person, a task for which the full web page content is necessary.
The WePS corpus provides an initial testbed to test peo-ple search strategies over the web. We are currently work-ing to expand and balance the corpus, including two addi-tional types of person names: less frequent names, on one hand (for which ambiguity should be lower), and  X  X elebrity X  names, where one person dominates the top-ranked results of search engine results. The expanded corpus will be avail-able at http://nlp.uned.es. This work has been partially supported by the Spanish government, project R2D2 (TIC-2003-7180). [1] C. H. Gooi and J. Allan. Cross-Document Coreference [2] R. V. Guha and A. Garg. Disambiguating People in
