 In this paper, we present the vision of the usage of an object-based video data storage format for similarity search. The efficient (fast) and effective (accurate) search in video streams is an ongoing and still unsolved problem. Using an object-based format of multimedia data, all the information that is needed to answer queries is already available in a machine-accessible format. This way, the process of creating (video) descriptors as well as the similarity search becomes easier, because the data is already organized in a manner that al-lows fast access to specific information. To demonstrate the concept of similarity search process using the object-based 3D video format, we present experiments conducted on gen-erated clouds of points (an abstraction of 3D video data). H.3.1 [ Information storage and retrieval ]: Content Anal-ysis and Indexing X  Indexing methods ; H.2.4 [ Database man-agement ]: Systems X  Multimedia databases Theory, Experimentation, Design similarity search, video retrieval, object-based video coding
The application of the new trends of information record-ing and retrieval to the traditional industry manufacture is an eternal challenge for modern science. Furthermore, new information technologies are often introduced only for  X  X oy problems X , while the industry often does not recognize (or accept) new information technologies. One of the reasons of this problem is lack of the broadly acceptable data inter-change format that can be simply captured and efficiently analyzed. Nevertheless, the situation is progressively chang-ing as more precise devices and sensor networks are capable
The recently introduced new form of movie representation by 3D objects and set of actions removes the identification obstacle, so that movie scenes recorded into these video for-mats can be more thoroughly investigated. Furthermore, such kind of data representation reduces also necessary stor-age capacity and so more data can be captured. The ques-tion is  X  how can we effectively and efficiently search huge 3D video data? Trivially, we could consecutively generate scenes from the 3D representation and use similarity searching to compare them with the query sketch. However, this method can be time consuming due to sequential processing and an expensive similarity function used. Nevertheless, similar-ity search indexes can help to solve this task, thus there is no more obstacles for switching to more sophisticated 3D scene representations that can be utilized by engineers from all areas of the industry. In this paper, we point on the 3D object-based video data format and connect it with the content-based similarity search approach. This model can be utilized for efficient and effective 3D video retrieval from huge data collections produced by various sensors and de-vices (the schema for the data interchange is depicted in Figure 1). In a simple experiment, we demonstrate that 3D data can be efficiently indexed, which can lead to two orders of magnitude faster retrieval.
To store data used to perform a similarity search, it is desired to use formats that provide fast and easy accessi-bility to the content. For the search in large databases of video data, this is a big and still unsolved problem, since the information contained in video is not easily extractable. Therefore, an alternative representation of video data, which potentially can save storage space and provide easier acces-sibility to the information contained in a scene, is presented here. The format is a codec for an object-based representa-tion of video data, of which a first version is standardized in MPEG 4 Part 25 [8] and a usable, full working version of the codec is presented in [9]. An in depth comparison to traditional, pixel-based video in terms of file size is given in [10]. As input the codec requires Collada, which is an XML based scene description format that contains all the infor-mation in a scene as well as it can link to textures and other Collada files [11]. A Collada file is composed of an asset, different libraries and one or more scenes. The asset con-tains general information about the file such as the author, tools used, creation/modification date, units, up-axis and so on. The libraries are the main information storage part. All the information stored in the libraries can be accessed us-ing the corresponding URLs. To create a scene, an instance of a visual scene has to be created using the appropriate URL. A visual scene is stored in the library visual scenes . Inside this library the scene content is represented in nodes which again have links to other libraries such as library ge-ometries, library animation, library cameras , and so on. If not the whole scene is of interest but specific information is needed, it can be accessed in the according libraries directly.
The encoded data streams are stored in an MPEG 4 con-tainer (file extension, e.g., *.mp4). In the streams differ-ent information contained in a scene is stored. There are the data with a useful annotation. Hence, instead of formu-lating a query using some structured language or combina-tion of keywords, a particular data object (or its sketch) is provided to the system, that tries to find the most similar objects in the database.

When the database is processed, only the content of the stored objects represented by descriptors from a descriptor space U is considered. The content of the objects is com-pared by a particular similarity measure, often modeled as a distance function  X  . Having a ranking distance function  X  and a finite database S  X  U , the two most popular similar-ity queries are the range query and the k nearest neighbors query [13]. The performance of the content-based retrieval systems strongly depends on the employed descriptors and the similarity measure. While the descriptors available in the 3D data format are discussed in the previous section, we provide a short overview of popular similarity measures in Table 1. Especially, we focus on the descriptor type ac-cessible from the Collada format and the time complexity of the measure (where n represents the size of a descriptor in terms of the number of bins, centroids, etc.), that is im-portant also for the efficiency of the retrieval. More details about the measures can be found in [13, 14].
 Table 1: Examples of popular distance measures.

When a suitable similarity model (descriptors and a dis-tance function) for a 3D video retrieval application is se-lected, the data management system should also provide tools for efficient query processing. In general, there are two options  X  either some domain-specific solution is em-ployed (applicable only on one problem) or a more general distance-based indexing method is utilized. Especially, when the distance measure is a metric distance, the metric access methods can be used to improve the efficiency of the re-trieval [15, 13]. When the efficiency of the retrieval with metric indexes is still insufficient, the model can be further fine-tuned using its inner parameters [14], or parallel [16] and approximation [13] techniques can be utilized. Although the domain-specific solutions often outperform the general metric access methods, there are several arguments for met-ric indexing. First, the principle of the metric indexing is very simple, intuitive and applicable to any metric distance measure. Second, metric indexing can be used in situations where there is no domain specific-solution available.
The basic principle of the metric indexing is the  X  X ower-bounding X  that employs precomputed distances  X  ( o i ,p j ) be-tween database objects o i and a set of k pivots p j , where pivots are objects preselected from the database S . When a range (or kNN) query q is issued, first the distances  X  ( q,p j ) to pivots are evaluated and then the lower bound LB (  X  ( o i ,q )) is greater than the actual query radius, the database object o can be filtered without costly evaluation of  X  ( o i ,q ). Let us query is received and the search is performed successfully, the results have to be retrieved from the data storage, which in this case is in Collada Format or an MPEG 4 Part 25 compressed version of it. There are three possible ways of storing clouds of points in Collada and compressing it with MPEG 4 Part 25. First, each cloud can be thought of as a set of vertices, which favors each cloud to be stored as an independent model in the library geometries. Since each geometry has an ID, every cloud found by similarity search can be accessed directly.

The second way the representation of the clouds in Collada and the compression in MPEG 4 Part 25 can be conducted works only if the form of the cloud itself is insignificant, but the position of each cloud is of interest. If this is the case, one cloud would be stored in the library geometries and virtually unlimited instances of this cloud can be created and stored in a visual scene. Every cloud is then represented by a node in the scene, where an individual position, size, and orientation can be stored.

Third, if the clouds have the same cardinality, it could be favorable to store one cloud as geometry and represent all the other clouds as an animation. In that case, the individual position, size, deformation, and orientation of each cloud is stored at a virtual time point of an animation stream in the library animations. A time point is here referred to be virtual, because it is not a real animation over time, but the possibility to store animations effectively is misused here to store the clouds without infringing the Collada standard. To compress the data effectively, the cloud properties are stored relatively to the ones of its neighbor, which is stored at a virtual time point before it. With this approach, redundancy between the clouds can be discovered during compression. A particular cloud can be retrieved by using the virtual time, at which it is stored, to access its data.
In this paper, we present the vision of using object-based video data storage format in connection with the content-based similarity search approach. We discuss the possibili-ties of creating descriptors as well as the efficient retrieval of the data, where the format allows fast access to specific in-formation. To validate the concept, an example of randomly generated clouds of points is given with which a full similar-ity search process is described using the object-based video format. In the preliminary experiments, we also demon-strate the promising filtering power of metric indexes that enable high performance of the 3D object-based video re-trieval applications.
 This research has been supported in part by Czech Science Foundation projects P202/11/0968 and P202/12/P297. [1] I.F. Akyildiz, Weilian Su, Y. Sankarasubramaniam, [2] Deepak Ganesan, Deborah Estrin, and John
