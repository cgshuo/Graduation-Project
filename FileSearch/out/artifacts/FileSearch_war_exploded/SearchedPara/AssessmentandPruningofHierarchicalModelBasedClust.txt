 The goal of clustering is to iden tify distinct groups in a dataset. The basic idea of mo del-based clustering is to ap-pro ximate the data densit y by a mixture mo del, typically a mixture of Gaussians, and to estimate the parameters of the comp onen t densities, the mixing fractions, and the num ber of comp onen ts from the data. The num ber of distinct groups in the data is then tak en to be the num ber of mixture com-ponen ts, and the observ ations are partitioned into clusters (estimates of the groups) using Bayes' rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be \distinct" in the most common sense of the word -con tiguous, densely populated areas of feature space, separated by con tiguous, relativ ely empt y regions. If the groups are not Gaussian, however, this corresp ondence may break down; an isolated group with a non-elliptical dis-tribution, for example, may be mo deled by not one, but sev eral mixture comp onen ts, and the corresp onding clusters will no longer be well separated. We presen t metho ds for as-sessing the degree of separation between the comp onen ts of a mixture mo del and between the corresp onding clusters. We also prop ose a new clustering metho d that can be regarded as a hybrid between mo del-based and nonparametric cluster-ing. The hybrid clustering algorithm prunes the cluster tree generated by hierarc hical mo del-based clustering. Starting with the tree corresp onding to the mixture mo del chosen by the Bayesian Information Criterion, it progressiv ely merges clusters that do not app ear to corresp ond to di eren t mo des of the data densit y.
 I.5.3 [ Pattern Recognition ]: Clustering; I.5.1 [ Pattern
Supp orted by NSA gran t 62-2948. y
Supp orted by NSF gran t DMS-9803226 and NSA gran t 62-2948.
 Recognition ]: Mo dels| Statistic al ; G.3 [ Probabilit y and Statistics ]: Multiv ariate Statistics Mo del-based Clustering Mo del-based Clustering, Nonparametric Clustering, Densit y Estimation, Unimo dalit y
The goal of clustering is to iden tify distinct groups in a dataset X = f x 1 ; : : : ; x n g R m . For example, when pre-sen ted with (a typically higher dimensional version of) a dataset like the one in Figure 1a we would like to detect that there app ear to be two groups, and assign a group lab el to eac h observ ation. (Throughout this pap er we distinguish between \groups" and \clusters", whic h are estimates for the groups.) as a statistical problem we regard the data x 1 ; : : : ; sample from some unkno wn probabilit y densit y p ( x ). Model-based clustering (see [9] and references therein) relies on the premise that eac h group g is represen ted by a densit y p that is a mem ber of some parametric family , typically the multiv ariate Gaussian distributions. In this case p ( x ) is a Gaussian mixture: where G is the num ber of groups, g is the prior probabilit y of group g , and p ( x ; ; ) denotes the Gaussian densit y with mean and covariance matrix . For xed G we can esti-mate the parameters g ; g , and g by maxim um likeliho od, using the EM-algorithm [9, Chapter 2.8]. There are man y ways of estimating G [9, Chapter 6], e.g. by maximizing the Bayesian Information Criterion (BIC) [11, 5]: Here, L ( G ) is the log-lik eliho od of the best G comp onen t mo del, r is the num ber of parameters of the mo del and n is the num ber of observ ations.
While attractiv e conceptually , the straigh t-forw ard ap-proac h to mixture mo deling | t mo dels for man y di eren t values of G using the EM algorithm, and then choose the mo del that maximizes the BIC | is slow. Follo wing a sug-gestion by Fraley and Raftery [5], we address this problem by using a hierarc hical approac h: Find a mo del with G 1 comp onen ts by merging the two groups of the G comp onen t mo del for whic h the merge leads to the smallest decrease in log-lik eliho od. Among the sequence of mo dels thus gener-ated choose the one maximizing the BIC.

There have been sev eral recen t adv ances in extending the normal mixture mo del to large datasets [2, 13].
 Mo del-based clustering relies on the premise that mixture comp onen ts in the mo del corresp ond to distinct groups in the data. If the groups are Gaussian, then the resulting clusters will indeed tend to be \distinct" in the most com-mon sense of the word -con tiguous, densely populated areas of feature space, separated by con tiguous, relativ ely empt y regions [3]. If the groups are not Gaussian, however, the cor-resp ondence between groups and mixture comp onen ts may break down. An isolated group with a non-elliptical distri-bution, for example, may be mo deled by not one, but sev eral mixture comp onen ts, and the corresp onding clusters will no longer be distinct. This problem is illustrated in Figure 1a. Most observ ers would probably agree that the data in this gure fall into two separate groups. The BIC criterion, how-ever, chooses a mixture mo del with four comp onen ts; Fig-ure 1b sho ws regions con taining 60% of the mass of eac h comp onen t.
 for assessing the degree of separation between the comp o-nen ts of a mixture mo del and between the corresp onding clusters. We also prop ose an algorithm for pruning the clus-ter tree generated by hierarc hical mo del-based clustering. The algorithm starts with the tree corresp onding to the mix-ture mo del chosen by the Bayesian Information Criterion. It then progressiv ely com bines mixture comp onen ts that do not app ear to corresp ond to di eren t mo des of the data den-sity and merges the corresp onding clusters. Eac h cluster in the nal partition may therefore be mo deled by more than one mixture comp onen t. The resulting pro cedure can be re-garded as a hybrid between nonparametric and mo del-based clustering: we look for mo des in the data distribution using the mixture mo del as a densit y estimate.
Roughly speaking, we would exp ect mixture comp onen ts mo deling di eren t groups in the data to be well separated. On the other hand, mixture comp onen ts mo deling parts of the same group would be exp ected to exhibit signi can t overlap.

We now put this concept in probabilit y terms. Assume that we have mo deled the distribution of the observ ed data by a mixture densit y p ( x ) = observ ations from this densit y by rst generating a comp o-nen t lab el Y with P ( Y = g ) = g , and then generating X from p Y . According to Bayes' rule, the posterior probabilit y Figure 1: Data set with tted Gaussian mixture.
 The mo des of the mixture are indicated by the two white dots. (This example is referred to as the run-ning example in the remainder of the pap er.) P ( Y = g j X ) is Comp onen t g is well separated from all the other comp o-nen ts if P ( Y = g j X ) only tak es extreme values, either close to zero or close to one -one for observ ations actually gener-ated from comp onen t g , and zero for all others.
Exactly evaluating the distributions of P ( Y = g j X ) for the G comp onen ts is generally imp ossible when the dimen-sion m is larger than 1. To see why this is so, de ne the ran-dom variable h ( X ) = P ( Y = g j X ). Its distribution function F ( u ) is given by where I ( ) denotes the indicator function. Except in triv-ial cases, (suc h as G = 2, 1 = 2 ) the region of feature space de ned by the indicator function has a complex shap e describ ed in terms of conic sections. Hence in general this integral can not be evaluated analytically and we resort to Mon te Carlo sim ulation.
In the follo wing we presen t three metho ds for assessing the separation between mixture comp onen ts, based on the posterior probabilities, the margins, and the misclassi ca-tion probabilities. All these were estimated by sim ulating from the mo del. Figure 2 sho ws rootograms of the posterior probabilities P ( Y = g j X ) for the four comp onen ts of the mixture mo del in our running example. (A rootogram is a varian t of a histogram where the heigh ts of the bars enco de the square roots of the bin coun ts, instead of the bin coun ts themselv es. This mak es low coun ts more visible.) The rootograms are based on 20,000 data points generated from the estimated mixture mo del. We have omitted the bin con taining P ( Y = g j X ) = 0, because it would have by far the largest bin coun t and would obscure the information in the remaining bins.
The rootogram for comp onen t one (top panel) has a large peak at P ( Y = 1 j X ) = 1 and is essen tially zero elsewhere, indicating clear separation of comp onen t one from all the other comp onen ts. On the other extreme, the rootogram for comp onen t four has no peak at P ( Y = 4 j X ) = 1. This is due to the fact that comp onen t four is completely over-lapp ed by comp onen ts two and three, and hence there is al-ways a substan tial posterior probabilit y that an observ ation generated from p 4 migh t have come from p 2 or p 3 . Further-more, the signi can t mass away from P ( Y = g j X ) = 1 in the rootograms for comp onen ts two, three, and four sho ws that these comp onen ts are not well separated. Figure 2: Running example: Rootograms of the pos-terior probabilities P ( Y = g j X ) for X distributed ac-cording to the mixture mo del. The bin con taining zero is not sho wn in order not to obscure the pattern in the other bins.
An alternativ e to looking at the posterior probabilities is to consider the margins. Let ^ Y ( X ) be the estimated com-ponen t lab el assigned to X by Bayes' rule: The margin of X dra wn from comp onen t Y of the mo del is given by margin ( X; Y ) = P ( ^ Y ( X ) = Y j Y ) max Table 1: Misclassi cation matrix for the running ex-ample.
 Note that a negativ e margin means that X is assigned to the wrong comp onen t, and that a small margin means that X lies in a region where comp onen ts overlap signi can tly.
Figure 3 sho ws the cum ulativ e distribution function (cdf ) of the margin for observ ations dra wn from the four com-ponen t mixture mo del of our running example. There is a large prop ortion of small margins indicating substan tial overlap between the comp onen ts. Figure 3: Running example: Cum ulativ e distribu-tion function of the margin.
When the num ber of clusters is mo derate, we can look at the misclassi cation matrix to detect well separated as well as overlapping comp onen ts of a mixture mo del. Table 1 sho ws the misclassi cation matrix for the mixture mo del in our running example. Let m gg 0 be the probabilit y that the Bayes' rule assigns an observ ation from comp onen t g to comp onen t g 0 .

From the misclassi cation matrix we can extract informa-tion at three di eren t levels of detail. At the coarsest level we can look at the overall misclassi cation probabilit y given by the separation. At the next higher level of detail, we can look at the comp onen t-wise misclassi cation probabilities M C In our example (Table 1) the misclassi cation probabilit y for comp onen t one is very small ( M C 1 = 0 : 002), indicating that comp onen t one is well separated. The misclassi ca-tion probabilities for the other comp onen ts are substan tially larger. On the most detailed level, the values of m gg 0 and m g 0 g indicate whic h other comp onen ts overlap comp onen t g . The pattern of entries in Table 1 sho ws that comp onen ts two, three and four are mutually overlapping. We could not see this from the less detailed views.
A mixture mo del is only an estimate for the true underly-ing densit y of the data. Therefore the degree of separation between mixture comp onen ts (or lack thereof ) does not al-ways accurately re ect the actual separation between the clusters.

We cannot compute the matrix of misclassi cation prob-abilities for the observ ed data x 1 ; : : : ; x n , nor the margins, (as we did in the previous section for observ ations sim ulated from the estimated mo del) because those require kno wing the true lab els. However we can compute the posterior probabilities P ( Y = g j x i ), and therefore generate a plot analogous to Figure 2, sho wn in Figure 4. The rootogram for P ( Y = 4 j x i ) (bottom panel) looks basically at, from whic h we can conclude that cluster 4 almost certainly does not corresp ond to a distinct group in the data. Figure 4: Running example: Rootograms of the pos-terior probabilities P ( Y = g j x i ) for the data.
Hierarc hical mo del-based clustering generates a hierarc hy of mixture mo dels: The mo del with m 1 mixture com-ponen ts is obtained by merging the two clusters of the m comp onen t mo del for whic h the change leads to the small-est decrease in log-lik eliho od. The result of this merging pro cess can be represen ted by a binary tree T . The leaves of the tree are the observ ations. Eac h interior node N of the tree is assigned a gener ation between 1 and n 1, in-dicating where in the sequence of merges it was generated. The interior node corresp onding to the i -th merge in the se-quence is assigned generation n i ; the root node therefore has generation 1. Eac h node N is also asso ciated with the cluster formed by its descenden t leaves.

The merge sequence de nes a sequence of trees: T m is ob-tained from T by remo ving the o spring of all nodes with generation greater than or equal to m . By construction, T has m leaves and corresp onds to a mixture mo del with m mixture comp onen ts. Let G be the num ber of mixture com-ponen ts chosen by the BIC, and let T G be the corresp onding tree.

If the distinct groups in the data all have Gaussian dis-tributions, then we exp ect roughly a one-to-one corresp on-dence between groups and mixture comp onen ts asso ciated with the leaves of T G . Also, the clusters asso ciated with the leaves of T G will be similar to the groups. (\Roughly" because G , after all, is only an estimate.) If the groups are not Gaussian, however, eac h group may be mo deled by more than one mixture comp onen t, and consequen tly will be the union of sev eral clusters.
 The idea of hybrid clustering is to test, for eac h node of T
G whose daugh ters are leaves, whether the corresp onding clusters are well separated. If they are not, then the clusters probably corresp ond to the same group, and we merge them. The new cluster is then mo deled by the sum of the mixtures mo deling the daugh ters that were merged. This pruning pro cess is rep eated until no further clusters can be merged.
Before describing its ingredien ts in more detail, let us see the pruning pro cess in action. The upp er panel of Figure 5 sho ws the tree T 4 whose leaves corresp ond to the mixture mo del t to the data in our running example. The circled node is the one being tested. The lower panel of Figure 5 sho ws the pro jection of its asso ciated cluster onto the Fisher discriminant direction , whic h is the direction that best sep-arates the pro jections of the two daugh ter clusters [6][8, Chapter 11.5]. The grey curv e is the kernel densit y estimate for the pro jected data with the smallest bandwidth that yields a unimo dal densit y [12, Chapter 6.3 and 6.4]. The blac k curv e is the kernel densit y estimate with the smallest bandwidth that yields a bimo dal densit y. The dot plot of the pro jected data looks unimo dal, and the unimo dal and bimo dal distributions are almost iden tical, whic h indicates that the daugh ter clusters are not well separated in feature space. A formal test for unimo dalit y of the pro jected data (Section 4.2) would reject the null hypotheses of unimo dal-ity at level = 0 : 49, meaning that the evidence against uni-mo dalit y is weak. We therefore prune the daugh ters. The new tree is the one sho wn in blac k in Figure 6. The diag-nostic plot is qualitativ ely similar to the one in Figure 5; the daugh ter clusters of the node being tested do not seem to be well separated, with unimo dalit y being rejected at level = 0 : 12. We therefore prune again and are left with the tree sho wn in Figure 7. Now the picture is di eren t: The diagnostic plot rev eals a clear separation between the clus-ters, and a formal test rejects the hypothesis of unimo dalit y at level = 0 : 002. We conclude that there app ear to be two distinct groups in the data, one mo deled by three mix-ture comp onen ts, and the other one mo deled by one mixture comp onen t.
In order to automate the pruning pro cess describ ed in Sec-tion 4.1 we need a way of measuring the amoun t of evidence against unimo dalit y for a univ ariate data set (the pro jec-tion of a cluster onto the Fisher discriminan t direction best separating its daugh ters). Even if we carry out the pruning pro cess interactiv ely, by looking at diagnostic plots like the ones in Figures 5-7, suc h a measure of evidence still pro vides a useful guideline.

Let x 1 ; : : : ; x n be a set of (univ ariate) data sampled from some densit y f ( x ), and let F n ( x ) be the empirical cdf of the sample. To test the null hypotheses that f ( x ) is unimo dal we use J.A. Hartigan and P.M. Hartigan's DIP test describ ed in [7]. The test statistic is the DIP where H is the unimo dal cdf closest to F n . Bic kel and Fan [1] Figure 5: Running example: Tree generated by hier-archical mo del-based clustering and diagnostic plot for the circled node. Figure 6: Running example: Tree generated by hi-erarc hical mo del-based clustering after rst step of pruning, and diagnostic plot for the circled node. sho w that the nonparametric maxim um likeliho od estimate of the closest unimo dal cdf, given the mo de location m 0 the greatest con vex minoran t of F n on ( 1 ; m 0 ] and the least conca ve ma joran t on [ m 0 ; 1 ). (The greatest con vex minoran t of F n on ( 1 ; m 0 ] is the con vex function G not exceeding F n on ( 1 ; m 0 ] that minimizes sup x m G ( x ) j . The least conca ve ma joran t is de ned analogously .)
Bic kel and Fan [1] also sho w that this estimate is robust against inaccuracy in the estimate of the mo de. We could estimate the mo de location by minimizing the DIP . How-ever, this would be computationally exp ensiv e. Instead we estimate the mo de using a kernel smo other, as suggested by Silv erman [12, Chapter 6.3 and 6.4]. Figure 8 sho ws the empirical cdf of a sample (blac k curv e), and the closest uni-mo dal cdf (grey curv e). The DIP is the maxim um absolute di erence between the two curv es, indicated by the hea vy vertical line. The estimated mo de location is sho wn by the grey vertical line.

The distribution of the DIP under the null hypotheses of unimo dalit y is not available in closed form but can be esti-mated by Mon te Carlo. As before, let H ( x ) be the uni-Figure 7: Running example: Tree generated by hi-erarc hical mo del-based clustering after second step of pruning, and diagnostic plot for the circled node. mo dal cdf closest to F n ( x ). We generate M samples of size n from H ( x ) ( M = 100, say) and compute the DIPs D ; : : : ; D M . If the DIP D orig for the original sample is the k -th largest among f D orig ; D 1 ; : : : ; D M g then we reject the null hypotheses of unimo dalit y at level k= ( M + 1).
Hybrid clustering is based on the premise that groups can corresp ond to collections of mixture comp onen ts, not just individual comp onen ts. The purp ose of our metho d is to iden tify those collections, not to nd a better tting mixture mo del. This is in con trast to the work by Sand and Mo ore [10] on repairing fault y mixture mo dels.

Automatic pruning requires speci cation of a signi cance level for the DIP tests; the larger the level, the larger the pruned tree. The signi cance level should not be tak en too literally: the total pruning pro cedure does not constitute a level test for unimo dalit y of the multiv ariate feature distribution.

First, there is the problem of multiplicit y: If we are car-rying out man y tests at a given level , then the probabilit y of erroneously rejecting one or more of the null hypotheses is greater than .

Second, we are choosing the pro jection directions to max-imize the separation between the clusters. This becomes an issue if the dimensionalit y of the feature space is large rela-tive to the total num ber of observ ations in the two clusters whic h are under consideration. For example, if we have a total of p + 1 observ ations in a p dimensional feature space then there will alw ays be a direction for whic h the obser-vations in the two clusters pro ject onto exactly two points, one for eac h cluster. We deal with this problem by rst pro jecting the com bined observ ations from the two clusters onto their k largest principal comp onen ts and then nding the Fisher discriminan t direction in this lower dimensional subspace. We chose k to be one third of the total num ber of observ ations in the two clusters.
We sho w two examples of Hybrid clustering, one with real data and one with syn thetic data sim ulated from a kno wn Gaussian mixture. Figure 9: Oliv e oil data: Original tree (all nodes) and pruned tree (blac k nodes). Figure 10: Oliv e oil data: Histograms of posterior probabilities P ( Y = g j x i ) for the data, before pruning
The data for our rst example consist of measuremen ts of eigh t chemical concen trations on 572 samples of oliv e Figure 11: Oliv e oil data: Misclassi cation proba-bilities M C g for the 20 comp onen ts of the mixture mo del. oils from nine di eren t areas of Italy . Applying hierarc hi-cal mo del-based clustering with diagonal covariance matri-ces and using the BIC to estimate the num ber of mixture comp onen ts results in a mixture mo del with 20 comp onen ts, corresp onding to the 20 leaves of the tree sho wn in Figure 9. The 20 columns of Figure 10 are histograms of P ( Y = g j for g = 1 ; : : : ; 20, with the coun ts enco ded as grey levels; the columns thus are a di eren t graphical represen tation of the rootograms making up the rows of Figure 4. The bars in the upp er panel of Figure 10 enco de the observ ation coun ts in the clusters. If the clusters were all well separated, then eac h observ ation would have posterior probabilit y one for one of the mixture comp onen ts and zero for all the others, and the plot would have a solid blac k strip e at the top and be white elsewhere. We are obviously quite far remo ved from this ideal situation. This impression is con rmed by Figure 11. Some of the mixture comp onen ts are not very isolated; observ ations generated from mixture comp onen t 1, for example, have roughly an 9% probabilit y of being as-signed to some other comp onen t.

Applying our pruning algorithm with signi cance level = 0 : 01 prunes the nodes sho wn in grey in Figure 9 and results in 7 clusters, four of whic h are mo deled by more than one mixture comp onen t. Figure 12 sho ws a typical diagnos-tic plot for a node whose daugh ters are pruned ( = 0 : 88), and Figure 13 sho ws a typical plot for a node whose daugh-ters are retained ( = 0 : 01). These two nodes are circled in Figure 9.

Figure 14 is the post-pruning analog to Figure 10. It is much closer to the ideal of \blac k strip e, white elsewhere". The misclassi cation probabilities sho wn in Figure 15 also have decreased signi can tly; the largest one is now 1.5% instead of 9%. Figure 14: Oliv e oil data: Histograms of posterior probabilities P ( Y = g j x i ) for the data, after pruning.
Figure 16 sho ws the cdf 's of the margins for the two clus-terings, pre-pruning in blac k, post-pruning in grey . If the mixture comp onen ts were perfectly separated then the cdf of the margin would be a step function with a single step at margin = 1. Pruning brings us closer to this ideal.
In our example we kno w the group lab els of the observ a-tions -we kno w the area of origin for eac h oliv e oil and it seems reasonable to assume that any groups in the data re-ect the areas of origin. We therefore assess how closely the clusters matc h the areas. Figure 17 sho ws a two way con-tingency table of areas on the vertical axis versus clusters on the horizon tal axis, before pruning. Notice that areas 3, 8 and 9 are eac h brok en up into sev eral clusters and the clustering pro cedure has not been able to separate out areas 1, 2 and 4. Figure 18 sho ws the corresp onding con tingency table after pruning. Note that areas 3, 8, and 9 now corre-spond to single clusters and 1, 2, and 4 have been com bined into one cluster. This raises the question how well areas 1, 2, and 4 are in fact separated in the 8 dimensional feature Figure 15: Oliv e oil data:Misclassi cation probabil-ities for the mo del, after pruning.
 Figure 16: Oliv e oil data: Cum ulativ e distribution function of the margins before pruning (blac k line) and after pruning (grey line). space. Figure 19 sho ws a pro jection of the observ ations from those areas onto the two linear discriminan t coordinates [8, Chapter 11.5]. There is no obvious separation of the points into groups.

It is con venien t to have a numerical measure summarizing the degree of agreemen t between groups (areas) and clus-ters. We use the Fowlk es-Mallo ws index [4] for this purp ose. The index is the geometric mean of two probabilities: the probabilit y that two randomly chosen observ ations are in the same cluster given that they are in the same group, and the probabilit y that two randomly chosen observ ations are in the same group given that they are in the same cluster. Hence a Fowlk es-Mallo ws index near 1 means that the clus-ters are a good estimate of the groups. For our example, the Fowlk es-Mallo ws index before pruning is 0.52, compared to an index of 0.81 after pruning. This sho ws that pruning substan tially impro ved the agreemen t between groups and clusters.
In Example 1, pruning was successful in that it signi -can tly impro ved the agreemen t between clusters and areas. The purp ose of the second example is to illustrate how hy-brid clustering performs on data whic h were in fact gener-ated from a Gaussian mixture mo del. We choose a mixture mo del that mimics the oliv e oil data: we estimate mean Figure 17: Oliv e oil data: Tw o way con tingency ta-ble of areas on the vertical axis versus clusters on the horizon tal axis Figure 18: Oliv e oil data: Tw o way con tingency ta-ble of areas on the vertical axis versus clusters on the horizon tal axis and covariance for eac h area and then generate a sample of the same size as the oliv e oil data from the corresp onding mixture mo del.

Applying hierarc hical mo del-based clustering with diago-nal covariance matrices and using the BIC to estimate the num ber of mixture comp onen ts results in a mixture mo del with 9 comp onen ts, corresp onding to the 9 leaves of the tree sho wn in Figure 20. Pruning leads to a partition into 7 clusters corresp onding to the leaves of the subtree dra wn in blac k, and increases the Fowlk es-Mallo ws index from 0.71 to 0.86. Figures 21 and 22 sho w the con tingency tables of areas versus clusters before and after pruning, resp ectiv ely. Prun-ing remo ves the split of area 3 and merges the two impure clusters 1 and 2.
The basic premise of mo del-based clustering is that eac h group in the data corresp onds to a single comp onen t of the estimated mixture densit y. If this premise holds, then the abilit y to estimate the num ber of mixture comp onen ts (equal to the num ber of groups) is a ma jor strength of mo del-based Figure 19: Oliv e oil data: Pro jection of areas 1, 2, and 4 on linear discriminan t directions.
 Figure 20: Sim ulated oliv e oil data: Original tree (all nodes) and pruned tree (blac k nodes). clustering compared to nonparametric clustering metho ds.
On the other hand, if the premise does not hold, the re-sult of mo del-based clustering can be misleading, because sev eral mixture comp onen ts may mo del the same group. Consequen tly the num ber of mixture comp onen ts will over-estimate the num ber of groups, and the clusters corresp ond-ing to individual mixture comp onen ts will no longer be well separated. It is therefore imp ortan t to be able to decide whether or not the premise holds and, in case the premise does not hold, to determine whic h mixture comp onen ts cor-resp ond to the same group.

We have introduced metho ds for assessing the degree of separation between the comp onen ts of a mixture mo del, and between the corresp onding clusters. We have also presen ted an algorithm for pruning the cluster tree generated by hier-archical mo del-based clustering. The algorithm starts with the tree corresp onding to the mixture mo del chosen by the BIC. It then progressiv ely merges clusters that do not ap-pear to corresp ond to di eren t mo des of the data densit y.
We have applied mo del-based clustering to a simple syn-thetic example in whic h the premise was violated. In this case the metho d indeed exhibited the de ciencies that we had anticipated. We have also sho wn that our prop osed Figure 21: Sim ulated oliv e oil data: Tw o way con tin-gency table of areas versus clusters before pruning. Figure 22: Sim ulated oliv e oil data: Tw o way con-tingency table of areas versus clusters after pruning. diagnostic tools rev eal the true structure of the data and lead to more accurate clustering. Application of our new techniques to a real-w orld example has also been encourag-ing. Our diagnostics have sho wn that most probably the premise of mo del-based clustering was violated in this case as well, and our Hybrid clustering metho d has signi can tly impro ved the qualit y of the clustering. [1] P. Bic kel and J. Fan. Some problems of the estimation [2] P. Bradley , U. Fayyad, and C. Reina. Scaling EM [3] J.W Carmic hael, G.A. George, and R.S. Julius. [4] E. B. Fowlk es and C. L. Mallo ws. A metho d for [5] C. Fraley and A. Raftery . How man y clusters? whic h [6] R. Gnanadesik an, J.R. Kettenring, and J.M.
 [7] J.A. Hartigan and P.M. Hartigan. The dip test of [8] K. V. Mardia, J. T. Ken t, and J. M. Bibb y.
 [9] G.J. McLac hlan and D. Peel. Finite Mixtur e Models . [10] P. Sand and A. Mo ore. Repairing fault y mixture [11] G. Schwarz. Estimating the dimension of a mo del. [12] B W Silv erman. Density Estimation for Statistics and [13] Jerem y Tantrum, Alejandro Murua, and Werner
