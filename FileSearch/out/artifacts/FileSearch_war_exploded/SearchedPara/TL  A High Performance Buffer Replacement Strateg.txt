 With the booming of e-Commerce, Web applications are facing with the challenge of higher pressure than ever. Read-write splitting is an important strategy being adopted systems to improve the performance by minimizing the access gap between disk and main memory by maintaining some hot data in the main memory. We use flash-based Solid State Drives (SSD) replacing hard disk as to improve the performance. 
Traditional buffer management policies take advantage of the temporal locality of page request to reduce the number of disk access [4]. The primary criterion of traditional buffer memory. So we should use the total I/O cost rather than hit ratio as the primary criterion to evaluate the performance of buffer replacement strategies on flash memory. 
However, existing flash-aware buffer replacement strategies have several limita-tions. To begin with, all of them adopt the LRU (Least Recently Used) method, which evicts the least recently used page. But these strategies fail to take frequency of page request into account. So they cannot make fu ll use of the history information of page transaction severs usually have access pattern with poor locality. In this case, the per-formance of LRU method will be suboptimal. 
In this paper, we propose a novel buffer replacement strategy Tri-List (TL for short) to address the limitations of previous stud ies. We partition the buffer pool into three parts according to page state (clean/dirty) as well as  X  X otness X  of page request. TL is implemented with low computational overhead. Thus the overall performance of TL can be guaranteed. We perform trace-driven simulation experiments using ben-chmarking workload. Experimental results show that TL achieves up to 30.3% im-provements than state-of-art flash-aware buffer replacement strategies. design of TL in detail. In Section 5 we present the results of performance study. Finally we conclude in Section 6. Buffer management has attracted significant attention for several decades. LRU and LFU are two important buffer replacement strategies considering recency and fre-quency, respectively. FBR [14] is a frequency-based algorithm. LRU-K [11] is a va-riant of LRU that keeps track of last K references of each page to make decisions about page eviction. It achieves higher hit ratio by considering both frequency and recency. But LRU-K incurs logarithm time complexity. 2Q [6] is a clock-based algorithm that solve this problem. It reaches the same goal of LRU-K with only O (1) time complex-ity. LIRS [5] is another variant of LRU; it uses a new criterion inter-reference recency to combine recency and frequency of page request. 
There are many buffering strategies on flash memory. REF [14] and BPLRU [8] are block-level buffer management policies for embedded system. Flash memory is also used as the extension of buffer pool of database systems [3]. Some flash-based buffer replacement strategies have recently been proposed to improve the I/O performance of database system on flash memory. Such as CFLRU [18] LRUWSR [7], CCFLRU [9], CASA [12] divides the buffer pool into two parts to distinguish clean and dirty pages. According to previous studies [1], the acce ss pattern of operation significantly influ-ences the overall I/O performance. CFDC [13] proposes the technique named write clustering to improve the locality of write operation. [2, 15] have explored par titioning the buffer pool into several regions. DBMIN [2] similar idea to partition the buffer pool into different regions to distinguish the infre-quent read/write pages from other pages. Due to the read-write asymmetry of flash memory, hit ratio is not consistent with overall I/O performance. Thus we choose I/O cost as primary criterion in this paper. Specifically, we consider average I/O time per page request. In this way, the I/O cost includes two parts: the cost of fetching pages into the buffer and the cost of writing dirty pages back to disk. portion of dirty pages in all evicted pages. The total I/O cost can be represented as: 
To represent the read-write asymmetry and then quantify the total I/O cost, we de-fine the asymmetry factor  X  X  X   X  /C  X  . Its value can be estimated according to history information as previous work did [12]. In most cases it is difficult for us to decide the value of  X   X  in (1). So we estimate the I/O cost using the total times of read and write operations as follows: to make a proper tradeoff between read and write operations instead of giving priority to evict clean pages. Besides, since the access gap between flash disk and memory is smaller than that on hard disk, we also need to avoid complicated data structures with high computational overhead. In this section, we will introduce the implementation of TL in detail. 4.1 Overview In order to solve the problem of LRU based methods, we need to take frequency of page request into consideration. All the frequency-based methods incur logarithm time complexity since they need to compare the value of request time. 
The main idea of previous flash-aware buffer replacement strategies is to keep dirty written again in the near future, we call it infrequent write pages. Such a page should not be kept in the buffer for a long time although it is dirty. We should evict it in order to avoid unnecessary extra read operations. 4.2 Data Structure and Algorithms We implement Tri-List (TL) by dividing the buffer pool into three parts based on the requested once. As such pages are just fetched into the buffer, they are clean pages. List  X  contains two kinds of pages: clean pages that are requested at least two times and dirty pages that are written only once. List  X   X  contains dirty pages that are written at least two times. When a dirty page in the second list is written again, it will be moved to the third list. The data structure of TL is shown in Figure 1. 
The main procedure of reading a page is shown in Algorithm 1. TL uses a hash table to judge whether a page is in the buffer in O (1) time. If a page is hit, TL will adjust the recently used page in  X   X  or  X   X  as victim according to the number of pages in the two lists, as is shown in Algorithm 3. Then the requested page is fetched into the buffer and the hash table is updated. The procedure of writing a page is similar to that of reading a page. The only difference is that the page needs to be marked as dirty after it is written. The time complexity of Algorithm 1 is O (1).  X  In other cases, the page being hit is just moved to the front of the corresponding list. Then we discuss about how TL chooses the victim page when buffer miss occurs. threshold, the page at the end of list  X   X  will be selected as victim. Otherwise, the page at the end of list  X   X  will be selected as victim. This bound is necessary because if the  X  cannot act well to filer infrequent read pages. Details of discussion about the largest size of  X   X  will be shown in Section 5.1. In this section, we use trace-driven simulation to evaluate the performance of TL. All the experiments were run on a PC with Win 7 operating system. The storage device was a 128GB Samsung 840 Series SSD. We used both synthetic and benchmarking work-loads. The two realistic benchmarking workloads were TPC-C [17] and TATP [16]. To get the page request trace for simulation, we ran each benchmark on PostgreSQL 9.3.1 with default setting (the page size is 8KB). We ran the test for around 3 hours for each benchmark as previous study did [10] and then used the collected traces as the input for simulation test. Details of traces are shown in Table 2. 5.1 Discussion An important issue to ensure the performance is the value of the max size of the list  X   X  larger than 0.05, the overall performance of TL will deteriorate drastically. So we set  X   X  to 0.05. Figure 2 shows the result of changing value of parameter  X   X  , which denotes the buffer sizes. Therefore, TL doesn X  X  need parameter tuning and we can set the parame-ters  X   X  and  X   X  empirically. (a) Under TATP workload (b) Under TPC-C workload 5.2 Performance Study We compare the performance of TL with state-of-art buffer replacement strategies CFLRU [18], CFDC [13], CASA [12] and LIRS [5]. We don X  X  use LRU as baseline because all above algorithms outperform LRU as is shown in previous studies. All the baselines are well tuned according to the references. The window size of CFLRU is set to 10%; the size of working region of CFDC is set to 40%. 
Figure 4 shows the results under TATP and TPC-C benchmarking workload. As is shown in Figure 4 (a), the performance of each algorithm under TATP workload is similar to each other. The reason is that the portion of write operation in TATP work-load is rather small (only 4.8%). Since a ll the flash-aware algorithms trade read oper-ations for write operations, the optimization will be limited. In this case, the disk-oriented algorithm LIRS even outpe rforms most flash-aware algorithms. 
Figure 3 shows the results under TPC-C workload. As we can see from Figure 5, TL has the best performance. Since TL can evict infrequent write pages by only protecting the pages that are written more than once in list  X   X  , compared with TL, LIRS fails to LIRS. In this paper, we propose a novel buffer replacement strategy TL on flash memory for read-write splitting Web applications. By dividing the buffer pool into three parts, TL can take the frequency of page request into account. Besides, compared to other flash-aware algorithms, TL also avoids extra read operations to guarantee the hit ratio by distinguishing frequently written pages. Results of experiments under both synthetic and benchmarking show that TL achieves up to 30.3% improvement than other state-of-art flash-aware buffer replacement strategies. Acknowledgement. Our work is supported by National Basic Research Program of China (973 Program) No.2011CB302302, the Support Program of the National '12th Five-Year-Plan' of China under Grant No. 2012AA09A408, Key S&amp;T Projects of Press and Publication under Grant No GXTC-CZ-1015004/02, Tsinghua University Initiative Scientific Research. 
