 1. Background and overview
As electronic texts become more available to researchers (and humans in general), an interesting dichotomy has emerged. On one hand, Web texts cater to users X  abilities to read and analyze that information; Web pub-lishers design the data X  X  structure to be easy for humans to digest. Hence it must adhere to conventional syn-tactic and semantic constraints of the users X  natural language. On the other hand, humans have very limited computational capacity for analyzing the vast amounts of electronic information now available. Information extraction research focuses on helping humans access and process large quantities of Web data. Often this work involves devising new strategies and algorithms to convert electronic natural language text into various formats that feed subsequent automatic processing.

The task is complicated by several types of textual layout formats. Text is often classified into one of three categories: unstructured (or free), structured, and semistructured [1] . Unstructured text is the most natural for humans to process, but treating the information automatically is nontrivial. Structured text is stored in a very rigid format (e.g. a database or a table) and hence more readily processed automatically, but is often less natural for humans to work with. Semistructured text falls somewhere in between; some structure is imposed X  X ften just enough to render it not quite grammatical X  X hough not enough to help in automatically processing the contents. With the advent of various markup languages and other annotation conventions, Web text often includes other extratextual information that may or may not aid in extracting information. In this paper we discuss processing a repository of semistructured medical text.
 ous levels of linguistic processing. Some systems are only concerned with parsing out the extracted informa-tion and therefore only require the use of a syntactic parser. Others need more in-depth processing and include a semantic component that can give some meaning to the extracted information. Yet other systems are depen-dent on real-world knowledge and require a pragmatic component to relate the data gathered from the system to outside information.
 research done with medical literature has involved developing systems that extract different types of relation-ships from text. For example, NLP techniques have been used to extract interesting and novel relationships from Medline 1 abstracts. The Medline repository contains vast amounts of useful information about various disease-and health-related issues. Many researchers have succeeded in extracting various types of relation-ships found in this repository, including gene relations [2] , protein relationships [3,4] , acronym X  X eaning pairs [5] , abbreviation definitions [6] , and molecular binding relationships [7] .
 mat, that specify the vast knowledge required for medical research and patient services. Highly specialized tools for representing clinical information and patient data have also been developed. Unfortunately, there has been only a modest amount of crossover between the NLP and medical informatics fields. The topic of information extraction is a salient one for demonstrating how applications can leverage the developments from both fields.
 regarding medical clinical trials. Fig. 1 shows an overview of the system. In Step 1, extraction and formula generation, we extract patient criteria from a web-based natural language description of qualifications for clinical trial participants, and create predicate logic expressions (PLE X  X ) that reflect the semantic content of the text. In Step 2, code generation, the system processes parsed criteria and their PLE X  X . The system then attempts to map the criteria to concepts in an electronic medical record. For the criteria that map success-fully, the system outputs appropriate logic for computing patient eligibility. In Step 3, eligibility assess-ment, the system evaluates the eligibility of a potential participant by executing the logic generated in Step 2 against that patient X  X  electronic medical record. The system generates a report that can help a cli-nician make an informed decision about whether to further evaluate the patient for enrollment in the clin-ical trial.

In Section 2 , we describe Step 1 of the system, which involves the NLP component. Section 3 describes the subsequent medical records database query component. We then discuss the system evaluation in Section 4 . Finally, we sketch ways the system could be enhanced in the future to provide better results. 2. Extraction and formula generation
The domain that our system addresses is clinical trials, which medical professionals use as a tool to assess diagnostic and therapeutic agents and procedures. Such trials require voluntary human subjects to undergo the new treatments or receive experimental medications. With the increasing cost of bringing experimental new drugs to the public, there is a crucial need for improving and automating access to the information in clinical trials including the directed recruitment of experimental participants, which is otherwise costly and labor-intensive.

Greater enrollment of subjects leads to greater confidence in the experimental results, but identifying ideal participants is costly and time-consuming. Trials often have very specific criteria for age, gender, state of a given disease, number and types of co-existing diseases, and trial timeframe/location.

Eligible potential participants are identified in various ways. One way is for clinicians to recommend their own patients. This results in few recommendations, though, since the clinician is limited to current patients and must be aware of which trials are soliciting patients.

Another common method for identifying candidates is through advertisements distributed via television, radio, the internet, newspapers or magazines. This method reaches a large number of people, including those who are not seeing a clinician. Disadvantages include the high cost of advertising, the inability of the general public to understand and self-diagnose complex technical criteria, and the cost and time involved in having a clinician screen applicants as potential participants.

A third method for identifying candidates is a systematic review of medical records. Screeners with some clinical training can perform this work, though it is laborious and costly. Furthermore, the patient informa-tion may be out-of-date or incomplete, so in-person evaluations are usually necessary. Since patient medical data is increasingly available in electronic form, a variation on this third approach is becoming increasingly feasible. Automated processes can sift through the available data, identifying possible trial participants.
In this section, we first discuss the web corpus we have targeted. Then we sketch the first stage of the sys-tem X  X ow the pertinent text is processed by the NLP components of the system. 2.1. The corpus: clinical trials
From 1997 to 1999 the US National Library of Medicine (NLM) and the National Institutes of Health (NIH) developed an online repository of clinical trials [8] . This repository currently contains about 41,000 tri-als which are sponsored by various governmental and private organizations 2 ; the repository receives about 20,000,000 page views per month. 3
Providers develop web pages for the clinical trials website using a simple user interface 4 including a text box for the eligibility criteria. No format restrictions are currently enforced on the text, though some boilerplate material can be entered (e.g. patient ages and gender) via dropdown boxes.
Each trial in the online repository comprises a series of sections that contain specific information regarding the trial that is useful to providers and patients. Fig. 2 shows a sample web page for an individual clinical trial and the hierarchy of different components it contains.

For this paper we extract information from one section of the web page: the Eligibility section. This section contains a listing of the requirements that a person must satisfy in order to participate in the trial. For exam-ple, nearly every Eligibility section specifies the patient age and also the gender.

Each web page undergoes two levels of preprocessing: (i) locating, retrieving, and converting the Eli-gibility section to an XML format with each item embedded in h criterion i tags; and (ii) manipulating the natural language text of some criteria to enable further processing. Often eligibility criteria are expressed telegraphically, for example with elided subjects or as standalone noun phrases. Parsing works best on full sentences, but only a small percentage have eligibility criteria structured as complete sen-tences. For elided subjects, a dummy subject and verb (i.e. A criterion equals ... ) are prepended to the criterion.

In other instances the first word in the criterion needs to be nominalized in order to produce a grammatical sentence. For example, the criterion able to swallow capsules is reformulated as an ability to swallow capsules , and then the dummy subject and verb are prepended.

Fig. 2 shows an example clinical trials web page, its corresponding XML version, and the linguistically-annotated rendition of its eligibility criteria. 2.2. Deriving syntactic and semantic information
The next step in the process involves using a syntactic parser to process the natural language criteria and produce a corresponding syntactic representation. We use the link grammar (LG) parser [9] . We chose this tool because of its open-source availability, efficiency, robustness in the face of ungrammaticality and out-of-vocabulary words, and flexibility. 5
Most traditional parsers are based on theoretical approaches to syntactic constituency and consequently, produce phrase-structure trees that encapsulate these assumptions. For information extraction purposes, such parsers are often too time-consuming to execute, are too complex to manage, and produce output that is too detailed for efficient downstream use. Recently dependency parsers and statistical-based approaches have become more widely used to parse text for run-time applications. The LG parser is similar to a depen-dency-based parser, though subtle differences exist. The differences between dependency and link grammar parses are summarized in Table 1 ; a more complete discussion is found in [10] .

The system reads in a.txt file containing each criterion (as extracted from the XML file described above) on a separate line in the file and parses each sentence individually. Because of structural ambiguities in English, a single input sentence might produce multiple parses; in this project, we only consider the highest scored parse for subsequent processing. Fig. 3 shows how a parse of A criterion equals serious heart problems is represented syntactically by the LG parser. Different labeled links connect the words in the sentence in a way that expresses their dependencies. These links are the key to the next step, extracting the semantic meaning from the syntactic output. Three properties characterize a successful parse: planarity (i.e. links cannot cross), con-nectivity (i.e. links must indirectly connect all words together), and satisfaction (i.e. the link-word correspon-dences must follow the grammar X  X  specifications). However, the LG parser will often output a partial parse even when a complete parse is impossible; this property is leveraged whenever necessary in subsequent processing.
 tics conversion engine. This is a component (that was previously developed for other applications) specifically designed to take the output from the LG parser and convert its content to PLE X  X  (though other semantic for-mats are also supported by the system).
 operator-based approach to problem solving [11,12] . The system involves hierarchical on-line machine learn-ing to  X  X  X hunk up X  X  prior actions, creating new operators which can be invoked recognitionally when situations arise that are similar to those encountered in prior experience [13] .
For the task at hand, sentences are fed one-by-one from the LG parser output to the Soar engine. For the highest scoring parse, all words and links with their associated labels are created on the agent X  X  input buffer. The system creates a discourse model and populates it with concepts representing each entity, property, and action (or state) derivable from the lexical and link content of the input. The relationships between these con-cepts are annotated, and the result is a data structure encoding the salient semantic features of the input sentence.

The engine then executes operators (specified via several dozen pertinent hand-crafted rules) to map com-ponents of the discourse semantic model to equivalent logical predicates and their associated arguments. Vari-ables are generated for predicates to specify with appropriate arity which referents the predicates refer to. The system attempts to build nominal compounds, which are frequent in the domain, based on the link specifica-tions. Table 2 shows some sample input fragments with corresponding PLE X  X .

Further pursuing our criteria from Fig. 2 , the parsed sentence A criterion equals serious heart problems. would yield the PLE:  X  X  X riterion(N2) &amp; serious(N6) &amp; heart_problems(N6) &amp; equals(N2,N6) X  X . Note that the dummy subject and verb, which were added for parsing purposes, are present in the PLE. For this reason, a postprocessing stage removes this extraneous information. Then the resulting PLE is placed in the above-mentioned XML file.

Fig. 3 illustrates the parse, its PLE, and the XML file after the NL processing stages have finished. 3. Query generation
Once the source web page has undergone the NL processing techniques described above, the resulting extracted information feeds a database query stage to match them with patient medical records. In this section we can only briefly mention the technologies germane to the task at hand; more details are available elsewhere [14] . 3.1. The target
Medical information systems manage patient information for a wide variety of tasks including patient care, administration (e.g. billing), research, and regulatory reporting. Coded medical vocabularies have been devel-oped in order to ensure consistency, computability, and sharability. Often they are conceptually based and have associated lexicons or vocabularies which are sometimes hierarchical in nature. For example, the SNOMED-CT [15] coded vocabulary has a code  X  X 254837009 X  X  that represents the concept  X  X  X reast cancer X  X .
Representing patient data usually requires more information than simple concepts. A data model called a detailed clinical model defines relationships between coded concepts or (other data values) and information of clinical interest. For example, a detailed clinical model might define a diagnosis in terms of a type and a sub-ject/person, so that a statement  X  X  X he patient has breast cancer. X  X  could be encoded with the diagnosis type from SNOMED-CT as described above, and the subject/person with the relevant patient ID number. Detailed clinical models thus combine coded concepts into meaningful expressions of a higher-order nature. We make extensive use of both coded concepts and detailed clinical models in the concept mapping process shown in Step 2 in Fig. 1 .
 itory (CDR). 7 The CDR makes extensive use of coded vocabularies; it also defines detailed clinical models using Abstract Syntax Notation One (ASN.1) [16] , an ISO standard for describing electronic messages [17] , including binary and XML encodings for many different application areas ranging from telecommunications to genome databases.
 coded vocabulary (over 800,000 concepts with over 4 million synonyms). The names of all the detailed clinical models used in the CDR and the fields they contain are defined as concepts in the HDD.
 (for security, auditing, and error handling), the services crucially provide for handling of detailed clinical mod-els as the basis for information access and retrieval. For example, an application can pass an instance of a detailed clinical model to the services, which will then return relevant instances of other detailed clinical models.
 handling medical data. Arden Syntax is written in units called medical logic modules (MLMs). Each MLM contains the logic necessary for making one medical decision. One category of information in an MLM defines knowledge required for making clinical decisions; this category is what we use in this project. The most sig-nificant slots in this category are the data slot and the logic slot. The data slot contains mappings of symbols used in an MLM to data in the target electronic medical record. The logic slot, as its name implies, contains the logic that operates on the data.
 been useful to use an abstraction called the virtual medical record (VMR) [20] . This assures that any number of healthcare organizations can write, maintain, and share clinical decision logic no matter what the structure of their own repositories. For eligibility criteria we use a small subset of VMR attributes called observations. 3.2. Concept mapping each criterion to concepts and data structures in the target electronic medical record. For each successful mapped criterion we generate executable code for determining if any patients meet the criterion. coded concepts are in the HDD, the mapping task involves matching words and phrases from the eligibility criteria to concepts in the HDD that represent either names or values in detailed clinical models. coded concepts from the HDD used in the CDR X  X  detailed clinical models. The system uses multiple matching strategies executed sequentially, and once a match is found, subsequent matches are not sought. Seven deci-sion points formulate the matching strategy; we sketch each below: (4) Match the predicate with a measurement. Measurements are extracted as predicates; they include mag-(5) Match name X  X alue pairs. The predicate names are processed to find possible name X  X alue pair relation-(6) Match a conjunction/disjunction. Often criteria are conjoined, and in such cases we process all elements. (7) Partial match. The best possible match with all available predicate names is attempted, preferring nouns 3.3. Code generation
The second stage of Step 2 is code generation, where we generate executable code from the output of the concept mapping process. The code that we generate for this project is an Arden Syntax MLM (Medical Logic Module) that specifies VMR queries for data access. 8 The process has two steps.

The first step takes place in tandem with the mapping process described above. Each database mapping for a criterion spawns a related VMR query. Abstracting away from the details, this process can be summarized as a rather straightforward conversion from and to nested attribute/value structures.

The second and subsequent step in generating code involves creating the Arden Syntax MLM. For each criterion that does not have a mapping to the target electronic medical record, we generate a comment stating that this criterion could not be mapped, but we do not generate any executable code. For the criteria that do have mappings, we generate an Arden Syntax  X  X  X ead X  X  statement. The VMR queries generate a non-empty return value when the criterion is satisfied. After iterating through the criteria, we generate code that writes out the results.

Even though the vast majority of slots in an MLM are required by the specification, only a handful are useful for machine execution; most of the remaining slots are intended for human perusal. Therefore, for this project we populate only the small number of slots that are useful for automated processing, primarily in the knowledge category slots for type, data, and logic. The only valid value for the type slot is  X  X  X ata-driven X  X , so we populate it appropriately.

To generate the data slot, we iterate through the eligibility criteria. For each criterion that does not have a mapping to the target electronic medical record, we generate a comment stating that this criterion could not be mapped, but we do not generate any executable code. For the criteria that do have mappings, we generate an Arden Syntax  X  X  X ead X  X  statement. The VMR queries generate a non-empty return value when the criterion is satisfied. After iterating through the criteria, we generate code that writes out the results.

The third code generation step, assessing the applicability of an encoded criterion, involves the straightfor-ward querying of electronic patient records. A report summarizes for the clinician which criteria parsed and matched the stated values. Fig. 4 shows an Arden Syntax VMR query and a sample eligibility report.
Note that the system as currently configured runs in batch mode, so except for the report just mentioned, the end-user context remains speculative at this point. Any eventual user protocols and interfaces would depend on the scenario implemented. We envision that the system could be used in a number of different ways.
It could drive a process that searches through a large collection of patient records, looking for candidates for a given trial. A threshold could be set for the percentage of criteria necessary for suggesting a given patient for further consideration. Alternatively, a threshold could specify the number of patients to suggest for the trial in question. Another use of the MLM would be to incorporate it in a process that evaluates patient records after a patient schedules an appointment and before the visit, so that the clinician can suggest possible trial partici-pation during the visit. 4. Evaluation results standalone evaluation of that stage of processing [21] . We followed the logical form identification evaluation method developed by organizers of the Senseval-3 competition. 9 This was the first standardized logical form identification evaluation task, and the organizers developed a gold standard and an open-source evaluation tool for assessing PLE identification for a selection of texts. 10 The results are returned in terms of precision and recall for both predicates and for their arguments.

In order to test this part of the system, we randomly selected 12 different trials from the clinical trials web-site. From these trials, 102 criteria were extracted and, after exact duplicates were removed, a total of 77 cri-teria remained. These 77 criteria were run through the system, and 34 were successfully parsed. For these criteria, we thus achieved the results shown in Table 3 . Though the results fall short of predicate extraction from newspaper texts (around 90%), it compares favorably with other kinds of information extracted in the medical domain (see [3] ).

Encouraged by these results, we recently carried out an end-to-end system performance evaluation involv-ing both the natural language and code generation components. From http://www.clinicaltrials.gov we ran-domly chose one hundred unseen clinical trials and ran them through Steps 1 and 2 in Fig. 1 . Afterwards we manually inspected each report, comparing them to the generated queries, and characterizing their success or failure. We tallied these results numerically, and a summary appears in Fig. 5 .

The 85 parsable trials varied in size and complexity, having from 3 to 71 criteria per trial. They also varied widely in subject matter, covering conditions from cancer to infertility to gambling. Two main factors induced failure in 15 trials: some had unexpected special characters (e.g. the HTML character  X  X &amp;#252 X  X ; representing the umlat u character), and others had sentences so complex that the parser failed.

These 85 trials yielded 1545 eligibility criteria; logical forms were successfully created for 473 of these cri-teria. All but 49 of these yielded queries, and another 96 queries could be generated without logical forms, so a total of 520 queries were formulated. Of these, 140 completely and exactly represented their original eligibility criteria. Another 113 of the queries were not entirely correct or complete but still yielded useful information for clinician decision-making. Four queries were technically well-formed based on the logical form though did not reflect the intent of the original criteria. In total, 257 queries were either completely correct, usefully cor-rect, or technically correct. The remaining 263 queries were neither correct nor useful in determining eligibility. lenges, and issues. 5. Discussion and future work sible. The system currently generates useful queries for about half of the number of criteria that produce for-mulas. We are encouraged by these preliminary results, and anticipate that planned improvements like those discussed below will substantially increase system accuracy and performance.
 member qualified enough in all three areas of medicine, medical informatics, and computer science to be able to assess the correctness of the queries formulated by the system. These findings could be strengthened by involving other independent testers and calculating the inter-rater reliability.

Tighter editorial controls could help solve this problem. A solution less intrusive to the users would be to develop collection of medical knowledge in the form of potentially reusable ontologies and axioms that could be used to assist in bridging the gap.
 sometimes, though, there are already complete sentences that this preprocessing strategy renders ungrammat-ical. More intelligent preprocessing could help in determining when dummy subjects are in fact required. least three ways: (i) extending the range of acceptable grammatical structures; (ii) refining the parse scoring algorithm to return the most plausible parse; and (iii) integrating it with a large-scale medical lexicon as others have done [22] . Currently the semantics engine only handles certain syntactic structures X  X ar less than those provided by the LG parser. We have also not yet experimented with the semantic engine X  X  inherent machine learning capabilities.
 portion, rendering the query incorrect. For example, consider the criterion blood products or immunoglobulins within 6 months prior to entering the study . The system found a mapping to an appropriate concept,  X  X  X lood products used X  X ; it also found a mapping to the valid concept  X  X  X onths X  X . However, the latter is not a permis-sible value for the former, so processing failed. If appropriate constraint checking could mediate name X  X alue pairings, the system would be able to more gracefully reformulate such instances.

The synonyms supplied by the HDD produced frequent successes, but occasional ambiguity proved prob-lematic. The system mapped the abbreviation  X  X  X CP X  X  to the drug  X  X  X hencyclidine X  X , whereas the trial intended  X  X  X neumocystic carinii pneumonia X  X . It also mapped  X  X  X G X  X  to  X  X  X hosphatidyl glycerol X  X  whereas the trial used it in an ad hoc fashion for  X  X  X athological gambling X  X .

Often unsuccessful queries reflected an absence of relevant concepts from the HDD. This is not unexpected, given the domain X  X  focus on experimental medications. We could use additional sources of clinical concepts such as the National Library of Medicine X  X  Unified Medical Language System [23] or a database of experimen-tal drugs. New concepts, though, would not be helpful unless patient records contain such concepts, which is unlikely.

Several queries provided partial information that was useful, but could not fully assess eligibility. For example, the system mapped the criterion  X  X  X terine papillary serous carcinoma X  X , to the concept  X  X  X apillary car-cinoma X  X . Matching  X  X  X apillary carcinoma X  X  in a patient X  X  record does not necessarily satisfy the criterion, but it could suggest further action by a clinician.

With some criteria a match will never be possible. EMR X  X  typically do not store patient information that would reflect such criteria as  X  X  X lans to become pregnant during the study X  X  or  X  X  X ale partners of women who are pregnant X  X .

Criteria we missed could be evaluated based on data in the EMR, by adding further inferencing with exter-nal knowledge. For example,  X  X  X eets psychiatric diagnostic criteria for depression X  X  requires the system to know what these diagnostic criteria are before this criterion can be evaluated.

Another possibility for improving the system includes mapping criteria to more VMR classes than just the observation class. This would facilitate more accurate queries against information such as procedures, demo-graphics, and medications.

Finally, the scalability and portability of the system and the approach should be further investigated. We purposely chose a relatively technical, narrow, and data-rich domain to address in this prototype work. One might naturally wonder how successful the system would be when (i) processing other data repositories (vs. the clinical trials website), (ii) focusing on other topics (e.g. intelligence gathering), (iii) processing other natural languages than English, (iv) utilizing other knowledge sources (e.g. different terminological databases), and (v) leveraging other components (e.g. another NL parser or predicate extraction engine).

The scalability question is the easiest to answer. Given the large-scale knowledge sources already in use (i.e. the HDD and a substantial lexicon for the LG parser), the efficiency of the NL components, and the fact that the system runs in batch mode performing an offline process, the system is especially scalable in terms of processing resources required. Addressing the many areas for further development listed earlier in this section should help improve the scalability, as well as the accuracy, of linguistic performance and query generation.

We are also cautiously optimistic about the system X  X  portability. Having already incorporated extensive medical knowledge into the system, we are confident that the system would perform well on related problems. For example, another separate X  X ut extensive X  X linical trials website exists solely for documenting over 5000 cancer trials 11 ; the system would perform well on information stored there, with minimal infrastructure adap-tation. Since the latter half of the system is built on medical informatics technologies, its usefulness would be limited for non-medical applications. However, the first half of the system (the LG parser/Soar nexus for per-forming syntactic/semantic analysis) effectively extracts PLE X  X  from newspaper headlines [24] and biographi-cal/genealogical data from text [25] . We have also integrated a Link Grammar parser for the Persian language with the Soar engine to extract predicates of interest from Persian newswire text [26] . Our system is modular enough that other specialized parsers and/or predicate extraction engines could replace ours if this were deemed necessary for treating other domains.
 References
