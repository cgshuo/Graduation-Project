 In various applications, such as gene expression, image retrieval, etc., one is often con-fronted with high dimensional data [1]. Dimension reduction, which maps data points in a high-dimensional space into those in a low-dimensional space, is thus viewed as one of the most crucial preprocessing steps of data analysis. Dimension reduction meth-ods can be divided into three categories, which are supervised ones [2], unsupervised ones[3], and semi-supervised ones[4]. The input data in these three categories are la-beled data, unlabeled data, and both of them, respectively. In a typical real-world ap-plication, only a small number of labeled data points are available due to the high cost to obtain them [4]. Hence the semi-supervised dimension reduction may be considered to fit into the practical setting. Instead of labeled data points, some semi-supervised methods assume pairwise constraints, for it is easier for experts to specify them than to assign the class labels of data points. More specifically speaking, pairwise constraints consist of must-link constraints and cannot-link constraints. The pair of data points in a must-link constraint shares the same class label, while the pair of data points in a cannot-link constraint is given different class labels.

From another viewpoint, dimension reduction methods can be divided into nonlinear and linear ones. The former allows a nonlinear transformation in the mapping while the latter restricts itself to linear transformation. We consider a complex distribution of points that are distributed in multiple subclasses. In other words, the data points of one class form several separated clusters. A nonlinear method has a higher degree of freedom and hence can handle data with comple x distribution effectively while a linear method tends to be incompetent in such a case.

In this paper, we restrict our attention to the linear semi-supervised dimension re-duction for the data of multiple subclasses with pairwise constraints. Previously rele-vant methods [5] [6] [7] [8] implicitly assume that a class consists of a single cluster. If the points are of multiple subclasses, handling the pairwise constraints to project the points into multiple subclasses in the transformed space is challenging for linear di-mension reduction. For a deep analysis, we particularly classify the must-link constraint into two categories. If two points in a must-link constraint reside in a single subclass, we define such a must-link constraint as an intra-subclass must-link constraint .Onthe contrary, if two points in a must-link constraint come from different subclasses, we define such kind of must-link constraint as an inter-subclass must-link constraint .We attribute the reason of the improper behavior of current linear methods to the fact that the inter-subclass must-link constraint most probably confuses the discriminant criteria of existing methods. The problem resulted from the inter-subclass must-link constraint is also encountered by the constraint transformation. For instance, a method in [9] trans-forms multiple must-link cons traints, which are connected via points in two different classes, into a cannot-link constraint between the centroids of the points of two classes. This method fails to give a comprehensible meaning if the points belong to different subclasses because the centroids may f all into the region of another class.
To overcome above problems, we propose SODRPaC, which consists of two steps. In the first step, must-link constraints which satisfy several conditions are transformed into cannot-link constraints and the remaining must-link constraints are deleted. The idea behind this step is to reduce the harmfulness of the inter-subclass must-link constraints while exploiting the must-link constraint information as much as possible by respecting the cluster assumption [10]: nearby points on the same manifold structure in the original space are likely to belong to the same class. In the second step, we obtain a projection mapping by inventing a new discriminant criterion for dimension reduction, which is suitable for the data of multiple subclasses, and employing the manifold regularization [11], which is helpful for discovering the local structure of data. The problem setting is defined as follows. We are given a set of N points X = { x 1 , x 2 , ..., x N } ,where x i represents a point in a d -dimensional space, a set of must-link constraints M = { m 1 ,m 2 ,...,m N ML } , and a set of cannot-link constraints C = { c while c i consists of a pair of points belonging to different classes. The output is a d  X  l transformation matrix W which consists of l projective vectors { w Hence y i = W T x i where y i represents a point in a l -dimensional space. After mak-ing data projection, we only consider the cla ssification task in the transformed space. For avoiding the bias caused by the choice of the classification method, the accuracy of nearest neighborhood (1-NN) classifier is considered as a measurement for the good-ness of dimension reduction with the 20  X  5-fold cross validation.
Fig. 1 presents the motivating examples, where d =2 and l =1 . The task for di-mension reduction here is thus to project the two dimensional data onto a line, where the points from different classes can be differentiated. A horizontal line is supposed to be the best projection while a vertical one is the worst projection. To better illustrate the motivation of our method, previously relevant methods are firstly retrospected. In the aspect of pairwise constraints, SSDR [5] and CMM [6] are to maximize the aver-age distance between the points in cannot-link constraints, and to minimize the average distance between the points in must-link constraints simultaneously. We can see that minimizing the average distance between the points in must-link constraints is reason-able in the case shown in Fig. 1a, where all the must-link constraints are intra-subclass must-link constraints . However, it disturbs to maximiz e the average distance between the points in cannot-link constraints in the case shown in Fig. 1b, where all the must-link constraints are inter-subclass must-link constraints . CLPP [7] builds an affinity matrix, each entry of which indicates the similarity between two points. To utilize the constraint information, the affinity matrix is revised by setting the similarity degree be-tween non-neighboring points involved in pairwise constraints. For example, given a must-link constraint, the similarity degree between two points is revised to be 1 , indi-cating two points are close (similar) to each ot her, no matter the two points are distant (dissimilar) or not. Suppose that the must-link constraint is inter-subclass must-link constraint , it implies that the two points are not geometrically nearby each other. This arbitrary updating may damage the geometrical structure of data points. This problem is also confronted by NPSSDR [8]. The above analysis explains the reason why CMM, SSDR, CLPP and NPSSDR are capable of obtaining excellent performance as shown in Fig. 1a, while they fail to reach the same fine performance in the multiple subclass case shown in Fig. 1b.

In the light of observations, we argue that the inter-subclass must-link constraint is probably harmful for the discriminant criteria of existing methods. For this reason, we attempt to design a new discriminant criterion that is able to behave appropriately in the case of multiple subclasses. The new discrimination criterion marked as  X  X iscriminant Criterion X  is able to obtain almost the same performance as others, as shown in Fig. 1a, and can even outperform previous methods, as shown in Fig. 1b. Moreover, the manifold regularization is helpful for discovering the local structure of data which is considered as one of the most principle characteristics of the data of multiple subclasses [12]. We therefore consider to make the new discriminant criterion and the manifold regularization work together in a collaborative way. Fig. 1b also demonstrates that our method SODRPaC, which is the combination of the new discrimination criterion and the manifold regularization, can obtain the best performance. The overview of our SODRPaC involves two steps described as follows: (1) Transformation . This step transforms must-link constraints into cannot-link con-(2) Dimension reduction . This step includes two components. The first component 3.1 Transformation from Must-Link Constraints Although a method that transforms must-link constraints into cannot-link constraints is provided in [9], we would point out that its purpose that the plentiful amount of constraints are reduced is substantially different from ours. Moreover, it becomes in-effective due to the inter-subclass must-link constraint . In a high dimensional space, the boundaries of subclasses and the number of subclasses within one class can not be explicitly detected by using the unlabeled data and link constraints. Thus, it is difficult to identify whether a must-link constraint is of inter-subclass must-link constraint or not. To reduce the harmfulness of inter-subclass must-link constraints , removing all the must-link constraints is, therefore, the m ost straightforward way. However, it can be regarded as a waste of must-link constraint information. Preserving the useful must-link constraints as much as possible in the form of cannot-link constraints is then the fundamental idea behind the transformation.

In our method, the transformation from must-link constraints into cannot-link con-straints basically occurs when a must-link constraint and a cannot-link constraint are connected. Under the cluster assumption, it is natural to consider two nearby points as another form of must-link constraint, so t hat we have more opportunities to transform must-link constraints into cannot-link constraints. In this paper, we employ shared near-est neighbor (SNN) [13] to formulate the sense of  X  X earby X  points. A set of shared near-number of the pairs of shared nearest neighbors, where | X | denotes the cardinality of a set. The value of SNN between x i and x j is defined as the number of points shared by their neighbors SNN ( i, j )= | N ( x i )  X  N ( x j ) | . The larger the value of SNN between two points is, the closer the two points are. It should be noted that we design a N  X  N matrix L to specify a kind of reliability for cannot -link constraints, which could be also deemed as the trustiness to them. Suppose that all the previously specified constraints are correct, for the previously given cannot-link constraints and the generated cannot-link constraints by using must-link constraints, their reliabilities are set to be 1. For the generated cannot-link constraints by using shared nearest neighbors, their reliabilities are equal to the similarities between the s hared nearest neighbors. It is because that transformation by employing shared nearest neighbors are considered to be less trustful than that by using must-link constraints. We believe it is natural to take the similarity between the shared nearest neighbors as a measurement for the trustiness. For exam-ple, given a pair of shared nearest neighbors { x i , x j } , we represent the reliability of a generated cannot-link constraint by using it as a Gaussian kernel, which is a simple kernel and has been widely applied in research fields. The reliability is formulated as  X  ( x i , x j )=exp(  X  x i  X  x j 2 / X  ) ,where  X  denotes the Euclidian norm and  X  is the kernel parameter. Note that, for the convenient access to the matrix L , given a cannot-link constraint c = { x i , x j } ,weuse L ( c ) to denote the entries of L ij and L ji , thus L is a symmetric matrix.
Fig. 2 shows four fundamental scenarios of the transformation. The set { a , b , e , f } , and { c , d } represent different classes of data points. We explain these four scenarios in a metaphorical way where the must-link constraint is taken as a friend relationship while the cannot-link constraint is considered as an enemy one. Standing from the viewpoint of point  X  X  X , it is given a friend relationship, say { a , e } , as shown in Fig. 2a, which is called as a basic form. If  X  X  X  is my enemy, instead of keeping my friend  X  X  X , consider that  X  X  X  is the enemy of my enemy  X  X  X . Fig. 2b shows an extension of the basic form with an enemy X  X  friend rule. If my enemy  X  X  X  has a friend  X  X  X ,  X  X  X  is the enemy of my friend  X  X  X  and me. In these two cases, the reliabilities for the new enemy relationships are set to be 1. Fig. 2c presents an extension of the basic form, which is called as a proximity form. If I have no enemy but my neighbor  X  X  X  has an enemy  X  X  X ,  X  X  X  is the enemy of my friend  X  X  X  and me. Fig. 2d show s an extension of the proximity form with the enemy X  X  friend rule. Note that, in the latter two cases, the reliabilities for the new enemy relationships are set to be the similarity between my neighbor  X  X  X  and me. The pseudo code for the summary of these four cases is illustrated in Algorithm 1. Algorithm 1. Transformation from Must-link Constraints into Cannot-link Constraints 3.2 Dimension Reduction In this section, we explain the dimension reduction which is based on a novel discrim-inant criterion and the manifold regularization. As mentioned in section 2, minimizing the average distance between the points in must-link constraints is inappropriate when the must-link constraints are inter-subclass must-link constraints . Under the cluster as-sumption, the shared nearest neighbors could be naturally deemed as another kind of intra-subclass must-link constraints . Thus, minimizing the average distance between the points in intra-subclass must-link constraints could be relaxed as making the shared nearest neighbors closer in the transformed s pace. Furthermore, the pair of points in the shared nearest neighbors probably resides in the same subclass, such that this re-laxation would not suffer from the harmfulness of inter-subclass must-link constraints . Therefore, the discriminant criterion, wh ich maximizes the average distance between the points in cannot-link constraints and minimizes the average distance between the shared nearest neighbors, is expected to be su itable for the data of multiple subclasses.
Suppose that x i and x j are projected to the image y k i and y k j along the direction w k , the new discriminant criterion is defined as follows: where the elements of H are given below: Inspired by the local scatter [14], the intuition behind the latter part of the right side of Eq. 1 could be regarded as the compactness of shared nearest neighbors, since two points are more likely to be close if the value of SNN between them is large. The differ-ence from the local scatter lies in the fact that a weighted matrix H which handles the similarity degree between shared nearest neighbors is employed. Since SNN provides a robust property that the side effect caused by the noisy points could be reduced to some degree, the compactness of shared nearest neighbors is more reliable than that of local scatter. The compactness of shared nearest neighbors could be also re-written as follows: follows: where D is a diagonal matrix whose entries are column sums of H , D ii = Similarly, the first part of right hand of Eq. 1 could be reformulated as: where S 2 = 1 | C | X G X T  X  X L X T where G is a diagonal matrix whose entries are column sums of L , G ii = where P = D  X  H ,and Q = G  X  L . For all the w k , k =1 , ..., l , we can arrive at where tr denotes the trace operator. As illustrated in Fig. 1b, the manifold regularization [11] is helpful for enhancing the performance obtained by the new discriminant crite-rion. We therefore incorporate it into our dimension reduction framework. The manifold regularization is defined as: where M = I  X  K  X  1 / 2 UK  X  1 / 2 is defined as a normalized graph Laplacian. I is a unit matrix, and K is a diagonal matrix whose entries are column sums of U , K ii = where U is defined as follows:  X  is expected to be minimized in order to preserve the sub-manifold of data. At last, the final objective function that combines Eq. 7 and Eq. 8 together is expected to be maximized, and is derived as where  X  is a parameter to control the impact of manifold regularization. By introducing the Lagrangian, the objective function is given by the maximum eigenvalue solution to the following generalized eigenvector problem: where  X  is the eigenvalue of P  X  Q  X   X  M ,and w is the corresponding eigenvector. One may argue that, when the graph of SNN is equal to the k -NN graph of the manifold regularization, Q is almost equivalent to M on preserving the local structure. As shown in [13], this situation would rarely happen since the two types of graph are dramatically different from each other in the general cas e. Moreover, to minimize the average dis-tance between the shared nearest neighbors, which are considered as another form of must-link constraints, is conceptually distinct from preserving the local structure. 4.1 Experiments Setup We use public data sets to evaluate the performance of SODRPaC. Table 1 summarizes the characteristics of the data sets. All the data come from the UCI repository [15] except for GCM [16] that is of very high dimensionality. For the  X  X onks-1 X ,  X  X onks-2 X , and  X  X onks-3 X  data, we combined the train and test sets into a whole one. For the  X  X etter X  data, we chose  X  X  X ,  X  X  X ,  X  X  X , and  X  X  X  letters from the train and test sets respectively by randomly picking up 100 samples for each lette r, and then assembled them into a whole set.

AsshowninEq.10,  X  is the parameter that cont rols the balance between P  X  Q and M . In this experiments setting, the parameter  X  is searched from 2  X  ,where  X   X  {  X  | X  5  X   X   X  10 , X   X  Z } . A weighted 5-nearest-neighbor graph is employed to construct the manifold regularizer. In addition, the kernel parameter  X  follows the sug-where  X  is the mean norm of data. The parameter  X  and the manifold regularizer are then optimized by means of the 5-fold cross-validation. As to the parameter settings of other competitive methods, we follow the parameters recommended by them, which are considered to be optimal. Without specific explanation, the number of must-link equilibrium between must-link constraints a nd cannot-link constraints is favorable for the existing methods. In addition, the value of k for searching shared nearest neigh-bors is set to be 3 . The reason of this setting is to guarantee that the pairs of points in shared nearest neighbors reside in the same subclass, and to make the constraint transformation have more opportunities to be p erformed. In our experiments, must-link constraints and cannot-link c onstraints are selected acco rding to the ground-truth of data labels. 4.2 Analysis of Experiments First, the effectiveness of SODRPaC is exhibited by changing the number of constraints. Apart from the semi-supervised dimension reduction methods, we also take PCA as the baseline. As illustrated in Fig. 3, SODRPaC always keeps the best performance when the number of available constraints increases from 10 to 150 . As seen in Fig. 3a, Fig. 3b, Fig. 3d, and Fig.3f, CLPP is inferior to PCA even if the number of constraints is small. Thesideeffectof inter-subclass must-link constraints , in this case, can be neglected. The reason is probably that the feature of discovering the local structure of data points could not help CLPP to outperform PCA. However, our SODRPaC, which also utilizes the manifold regularization due to its property of discovering the local structure, ob-tains the best performance. We can judge that the new discriminant criterion boosts the performance. It is also presented in Fig . 3d that the performance of SSDR decreases to some extent with the increase of the number of constraints. The possible reason is that increasing the number of available constraints makes the opportunity higher that inter-subclass must-link constraints exist, which deteriorates the optimization on the fine dimension reduction. It should be also pointed out that SODRPaC does not sig-nificantly outperform other methods. A possible reason is that the Euclidean distance, which is employed to formula te the similarity between points in the original space, is likely to be meaningless in t he high dimensional space.

We then examine the relative impact between must-link constraints and cannot-link constraints on the performance of SODRPaC. In this experiment, given 150 available constraints, the ratio of must-link constraints to cannot-link constraints is varied from 0 to 1 . Fig. 4 presents that SODRPaC has a much smoother behavior than others with the change of ratio. It indicates that SODRPaC is more robust than other semi-supervised methods in terms of the imbalance between must-link constraints and cannot-link con-straints. As shown in Fig. 4b and Fig. 4f, SODRPaC presents an obvious degradation of performance when all constraints are must-link ones. The most probable reason would be that the transformation from must-link constraints into cannot-link constraints can not be performed when the necessary cannot-link constraints lack. This behavior is consistent with the conclusion demonstrated in [9] that cannot-link constraints are more important than must-link constraints in guiding the dimension reduction.

As implicated in the previous sections, the parameter  X  that controls the balance between P  X  Q and M , and the factor  X  that is related to computing the similarity between two points would influence the performance of SODRPaC. An analysis on the two parameters is necessary to provide the guideline about how to choose their values. PCA is employed as the baseline because existing methods can not hold such two parameters simultaneously. Because of the different scale between  X  and  X  ,  X  -axis and  X  -axis are thus plotted as  X / (1 +  X  ) and  X / (1 +  X  ) , respectively. The axis values are then in the interval (0 , 1) . We empirically uncover two interesting patterns for most of data sets and reduced dimensions as well. There are two regions where SODRPaC are more likely to obtain its best performance. The first region is where  X / (1 +  X  ) is small, as shown Fig. 5a, Fig. 5b, Fig. 5c, Fig. 5d, Fig. 5e and Fig. 5g. In this situation, the variation of  X / (1 +  X  ) would not cause the dramatic change for the performance of SODRPaC. The second region is where both  X / (1 +  X  ) and  X / (1 +  X  ) are large, as shown in Fig. 5b, Fig. 5e, Fig. 5f, and Fig. 5h. In this paper, we have proposed a new linear dimension reduction method with must-link constraints and cannot-link constraints, called SODRPaC, that can deal with the multiple subclasses data. Inspired by the observation that handling the inter-subclass must-link constraint is challenging for the existing methods, a new discriminant crite-rion is invented by primarily transforming must-link constraints into cannot-link con-straints. We also combine the manifold regularization into our dimension reduction framework. The results of extensive experiments show the effectiveness of our method.
There are some other aspects of this work that merit further research. Although the empirical choice of  X  and  X  is suggested, we do not as yet have a good understanding of how to choose these two parameters which are also correlated with choice of the number of the reduced dimensionality. Therefore, we are interested in automatically identifying these three parameters and uncovering relationships among them. Another possible would be to integrate the semi-supervised dimension reduction and clustering in a joint framework with automatic subspace selection.
 Acknowledgments. A part of this research is supported by the Strategic International Cooperative Program funded by Japan Science and Technology Agency (JST) and by the grant-in-aid for scientific research on fundamental research (B) 21300053 from the Japanese Ministry of Education, Culture, Sports, Science and Technology. Bin Tong is sponsored by the China Scholarship Council (CSC).

