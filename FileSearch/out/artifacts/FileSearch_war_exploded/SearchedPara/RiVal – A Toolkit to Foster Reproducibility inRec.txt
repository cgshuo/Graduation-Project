 Currently, it is difficult to put in context and compare the results from a given evaluation of a recommender system, mainly because too many alternatives exist when design-ing and implementing an evaluation strategy. Furthermore, the actual implementation of a recommendation algorithm sometimes diverges considerably from the well-known ideal formulation due to manual tuning and modifications observed to work better in some situations. RiVal  X  a recommender system evaluation toolkit  X  allows for complete control of the different evaluation dimensions that take place in any experimental evaluation of a recommender system: data split-ting, definition of evaluation strategies, and computation of evaluation metrics. In this demo we present some of the functionality of RiVal and show step-by-step how RiVal can be used to evaluate the results from any recommendation framework and make sure that the results are comparable and reproducible.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information filtering; H.5.1 [ Multimedia Information Systems ]: Evaluation/methodology Experimentation; Documentation; Measurement; Performance Recommender Systems; Evaluation; Benchmarking; Repro-ducibility; Recommendation Frameworks; Experiments
The recommender system research community has ac-cess to multiple open source recommendation frameworks, e.g. Apache Mahout 1 , LensKit 2 , MyMediaLite 3 . One of the emerging problems with having many recommendation frame-works is the difficulty when comparing results across frame-works, i.e. the reported accuracy of an algorithm in one framework will often differ from the same algorithm in a different framework. There are multiple causes for this, some of these are related to minor differences in algorithmic imple-mentation, data management, and evaluation [1]. An example of this is shown in Table 1 which illustrates the different RMSE and nDCG values obtained through each framework X  X  internal evaluation mechanisms (under the same evaluation conditions) using the same algorithms in Apache Mahout (AM), LensKit (LK) and MyMediaLite (MML) on the same dataset  X  Movielens100k. The table highlights the vast dif-ferences between the different frameworks showing that the same algorithm, dataset, and metric can differ several orders of magnitude across frameworks.

This demonstration shows a cross-framework recommender system evaluation toolkit  X  RiVal 4 . RiVal provides a trans-parent evaluation setting, allowing the practitioner complete control of the various evaluation settings. Table 1: Root-mean-square error (RMSE) and normalized Discounted Cumulative Gain (nDCG) for item-based and user-based (kNN, k=50) collaborative filtering algorithms using Pearson correlation, and matrix factorization (50 di-mensions, FunkSVD in Mahout and Lenskit; SVD++ in MyMediaLite).
RiVal is an open source Java toolkit which allows for fine-grained control of the complete evaluation methodology. We have defined the following four stages in the recommendation-evaluation process i ) data splitting ; ii ) item recommendation ; iii ) candidate item generation ; iv ) performance measurement . http://mahout.apache.org http://lenskit.grouplens.org http://mymedialite.net http://rival.recommenders.net Since RiVal is not a recommendation framework, step ( iii ) is not performed by RiVal, but can be performed by any of the three integrated frameworks (Mahout, LensKit and MyMediaLite), or outside of the RiVal pipeline. In this case, in step ( ii ), the preferred recommendation framework is given the data splits generated in the previous step and the rec-ommendations produced by the framework are then given as input to step ( iii ) of RiVal.

The toolkit can either be used as Maven dependencies, or ran as a standalone program for each of the steps. When running the toolkit in standalone mode, the type of evalua-tion to perform, recommendation algorithms, and framework to use are specified in property files which instantiate the necessary setup and execute each step. Listing 1 shows an example of a configuration file which sets up RiVal to pre-pare a set of datasets to perform cross validation on. The configuration instantiates the MovielensParser, which as-sumes the input data has a structure similar to the Movielens datasets (tab-or colon-separated columns). The resulting data splits will be written in the ml100kcv folder, separating training and test files through prefixes and/or suffixes, e.g. mov100k_fold_1_global.train would be the training file for the first cross validation fold.
 dataset.file=dataset.csv dataset.parser=net.recommenders.rival.split.\ dataset.splitter=net.recommenders.rival.split.\ split.peruser=false split.seed=2014 split.cv.nfolds=5 split.output.folder=./ml100kcv/ split.training.prefix=mov100k_fold split.test.prefix=mov100k_fold split.training.suffix=_global.train split.test.suffix=_global.test
Analogously, Listing 2 shows an example configuration of a recommendation step performing user-based recommendation using cosine similarity with a neighborhood size set to 50 with the LensKit recommendation framework. The evaluation step, which encompasses both candidate item generation and performance measurement is configured in a similar fashion (not included here due to space constraints). Examples of all configurations are found in the source code of RiVal 5 . recommender=org.grouplens.lenskit.knn.user.\ similarity=org.grouplens.lenskit.vectors.\ neighborhood=50 training=./trainset.scv test=./testset.csv output=./results.csv framework=lenskit
An example of an evaluation (RMSE) performed on a set of different algorithms (user/item-based CF, SVD), a http://github.com/recommenders/rival Figure 1: RMSE for a controlled evaluation. IB and UB refer to set of different data splits (cross-validation, per-user, global random) and frameworks (Mahout, LensKit, MyMediaLite) is shown in Fig. 1. The figure highlights that not only does the internal evaluation of each framework differ, even when the evaluation is fully controlled, the results should not be directly compared across frameworks.
In the demonstration, we will be showing how to quickly set up RiVal and run cross-framework comparison (benchmark-ing) using LensKit, MyMediaLite, and Mahout as recommen-dation frameworks. Each step in the evaluation protocol (data splitting, recommendation, candidate item generation, perfor-mance measurement) will be shown in detail and comparisons across frameworks and different evaluation strategies will be shown in order to highlight the importance of a transparent evaluation setup.

We will also be showing the evaluation setup used by all participants in the 2014 ACM RecSys Challenge 6 as RiVal is the tool used by both participants and organizers in order to measure the performance of the algorithms developed in the scope of the challenge.
This work was partly carried out during the tenure of an ERCIM  X  X lain Bensoussan X  Fellowship Programme. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreements n  X  246016 and n  X  610594, and the Spanish Ministry of Science and Innovation (TIN2013-47090-C3-2). [1] A. Said and A. Bellog  X  X n. Comparative recommender http://2014.recsyschallenge.com
