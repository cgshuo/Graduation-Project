 Abstract The evaluation of diversified web search results is a relatively new research topic and is not as well-understood as the time-honoured evaluation methodology of tra-ditional IR based on precision and recall. In diversity evaluation, one topic may have more than one intent , and systems are expected to balance relevance and diversity. The recent NTCIR-9 evaluation workshop launched a new task called INTENT which included a diversified web search subtask that differs from the TREC web diversity task in several aspects: the choice of evaluation metrics, the use of intent popularity and per-intent graded relevance, and the use of topic sets that are twice as large as those of TREC. The objective of this study is to examine whether these differences are useful, using the actual data recently obtained from the NTCIR-9 INTENT task. Our main experimental findings are: (1) The D ] evaluation framework used at NTCIR provides more  X  X  X ntuitive X  X  and statisti-cally reliable results than Intent-Aware Expected Reciprocal Rank; (2) Utilising both intent popularity and per-intent graded relevance as is done at NTCIR tends to improve dis-criminative power, particularly for D ] -nDCG; and (3) Reducing the topic set size, even by just 10 topics, can affect not only significance testing but also the entire system ranking; when 50 topics are used (as in TREC) instead of 100 (as in NTCIR), the system ranking can be substantially different from the original ranking and the discriminative power can be halved. These results suggest that the directions being explored at NTCIR are valuable. Keywords Diversity Evaluation Intents NTCIR Search result diversification Test collections TREC Web search 1 Introduction The evaluation of diversified web search results is a relatively new research topic and is not as well-understood as the time-honoured evaluation methodology of traditional IR based on precision and recall. In diversity evaluation, one topic may have more than one intent, and systems are expected to balance relevance and diversity. This corresponds to a real web search situation where the query input by the user is either ambiguous (e.g.  X  X  X ffice X  X  may be a workplace or a software) or underspecified (e.g.  X  X  X arry potter X  X  may be a book, a film or the main character; Clarke et al. 2009 ), for which the web search engine attempts to produce the first search engine result page (SERP) that satisfies as many users or user intents as possible. In fact, one could argue that every query is underspecified to some degree, unless it is a perfect representation of the underlying information need or intent.
The TREC 2009 web diversity task (Clarke et al. 2010 ) was the first to evaluate web search result diversification and to construct a web diversity test collection within the context of an evaluation forum. The third round of this task was concluded in November 2011. Whereas, the recent NTCIR-9 launched a new task called INTENT which included a diversified web search subtask that differs from the TREC web diversity task in several aspects (Song et al. 2011 ). The concluding workshop of NTCIR-9 was held in December 2011. The main differences between these two web diversity tasks are:  X  While TREC currently uses a version of Intent-Aware Expected Reciprocal Rank  X  While TREC uses binary relevance assessments for each intent and assumes that each  X  While TREC uses 50 topics each year for evaluation, NTCIR used 100 topics for the
The objective of this study is to examine whether these new features of NTCIR are useful for diversity evaluation, using the actual data recently obtained from the NTCIR-9 INTENT task. 2
Our main experimental findings are: 1. The D ] evaluation framework used at NTCIR provides more  X  X  X ntuitive X  X  and 2. Utilising both intent popularity and per-intent graded relevance as is done at NTCIR 3. Reducing the topic set size, even by just 10 topics, can affect not only significance These results suggest that the directions being explored at NTCIR are valuable. While Finding 3 is not surprising in the context of traditional IR evaluation, to the best of our knowledge, we are the first to have studied the effect of topic set size for diversity evaluation.

The remainder of this paper is organised as follows. Section 2 discusses prior art related to the present study. Section 3 describes the NTCIR-9 INTENT data, as well as a graded-relevance version of the TREC 2009 web diversity data (Sakai and Song 2011 ) which we use in our additional experiments. The section also provides formal definitions of the evaluation metrics we discuss. Section 4 discusses the choice of diversity metrics by means of concordance tests. Section 5 discusses the effect of utilising intent popularity and per-intent graded relevance on discriminative power and on the entire system ranking, and Sect. 6 discusses the effect of reducing the topic set size. Finally, Sect. 7 provides our conclusions and discusses future work. 2 Related work 2.1 Comparing diversity evaluation metrics The evaluation of diversified search results is a fairly new topic, although related IR tasks such as subtopic retrieval (Zhai et al. 2003 ) and facted topic retrieval (Carterette and Chandar 2009 ) have been discussed earlier. Hence, there are only a small number of existing studies that  X  X  X valuates the evaluation methodology X  X  for search result diversifi-cation. Exceptions include recent studies by Chapelle et al. ( 2011 ), Chandar and Carterette ( 2011 ), Clarke et al. ( 2011a ), Sakai and Song ( 2011 ), Sakai ( 2012 ) and Sanderson et al. ( 2010 ), as discussed below.

Chapelle et al. ( 2011 ) showed that ERR-IA and a -nDCG are members of a family of metrics called Intent-Aware Cascade-Based Metrics (CBM-IAs). We shall briefly discuss their work in Sect. 3 .

Chandar and Carterette ( 2011 ) compared a -nDCG, ERR-IA and an Intent-Aware ver-sion of average precision using the TREC 2009 web diversity track data, and isolated different effects such as diversity, relevance and ranking by means of Analysis of Variance. Their basic idea of separating out these different effects is similar in spirit to Sakai X  X  concordance test (Sakai 2012 ) which we shall discuss and utilise later in this paper.
Clarke et al. ( 2011a ) compared different evaluation metrics using the TREC 2009 web diversity track data in terms of discriminative power and rank correlation (i.e. how a pair of metrics resemble each other in ranking systems). The metrics examined include ERR-IA, a -nDCG and subtopic recall (which we call intent recall or I-rec ) amongst others. In their experiments, intent recall was the most discriminative among all the metrics that were examined. Some limitations of this study from the viewpoint of our objective are: (1) They used only one test collection, 4 which lacked both the intent popularity and the per-intent graded relevance information, even though some of the metrics examined were capable of utilising the information; (2) They did not discuss which metrics are actually  X  X  X ood X  X  (i.e. measuring what we want to measure) for diversity evaluation.
 Sakai and Song ( 2011 ) added graded relevance assessments to the aforementioned TREC 2009 web diversity data, and compared different metrics in terms of discriminative power and rank correlation. They also examined the ranked lists manually to discuss the  X  X  X ntuitiveness X  X  of diversity metrics. They proposed the D ] evaluation framework which plots an overall-relevance-oriented metric (e.g. D-nDCG ) against the diversity-oriented intent recall, and showed that it can fully utilise intent probabilities and per-intent graded relevance. In particular, their results suggested that (a) D ] measures are superior to Intent-Aware (IA) metrics in terms of discriminative power; and (b) D ] -nDCG rewards high-diversity systems more than a -nDCG and nDCG-IA do. Some limitations of this study from the viewpoint of our objective are: (1) Again, they used only one test collection, although their version of the TREC 2009 data contained graded relevance assessments as well as artificial intent probabilities; (2) Their discussion of  X  X  X ntuitiveness X  X  was only anecdotal and qualitative.

Sakai ( 2012 ) compared diversity metrics for an evaluation environment where each intent of a given topic is tagged with either informational or navigational . This applies to the TREC web diversity test collections but not to the NTCIR INTENT test collections. Using the aforementioned graded-relevance version of the TREC 2009 data, Sakai showed that intent recall, D ] -nDCG and its variant may be more discriminative than a -nDCG. More importantly, he showed that D ] -nDCG and its variant are more  X  X  X ntuitive X  X  than a -nDCG according to concordance tests with intent recall and effective precision . 5 How-ever, Sakai X  X  experiments were also limited to the case of the TREC 2009 web diversity data.

Sanderson et al. ( 2010 ) used a part of the TREC 2009 web diversity data (with the original binary relevance assessments only) to study the predictive power of traditional and diversity metrics: if a metric prefers one ranked list over another, does the user also prefer the same list? Using Amazon Mechanical Turkers as surrogate users, Sanderson et al. complex diversity metrics such as a -nDCG in predicting user preferences. While their study complements the aforementioned  X  X  X ser-free X  X  studies of Clarke et al. ( 2011a ) and Sakai and Song ( 2011 ), it should be noted that the Mechanical Turkers were given a subtopic rather than the entire topic when asked to judge which of a given pair of ranked lists is better: in fact, it is not straightforward to conduct a user study for diversity metrics, as they have been designed specifically to satisfy a population of users and their intents rather than a single user.

In the present study, we use the concordance test of Sakai ( 2012 ) to discuss the  X  X  X ntuitiveness X  X  of metrics, using intent recall, precision and Precision for the Most Pop-ular intent (PMP) as the gold-standard metrics. Intent recall (as a gold-standard metric) represents the ability of a metric to diversify; precision represents the ability to retrieve documents relevant to at least one intent of a topic; and PMP represents the ability to emphasise a popular intent in the SERP. The concordance test is similar to the predictive power of Sanderson et al. in that it involves comparisons of ranked list pairs, except that we replace the Turkers with the gold standard metrics, and that we can avoid treating each subtopic as an independent topic.

While all of the aforementioned studies concern the evaluation of a diversified ranked list of URLs, we note that this is not the only possible solution to presenting diversified search results to the user. For example, Brandt et al. ( 2011 ) propose a tree-like, dynamic presentation of diversified URLs, together with an evaluation method for that particular presentation method. However, even though search engine interfaces are becoming increasingly rich (e.g. in that they can use interactive presentation, multiple blocks and media etc.), a ranked list of items remains a simple and primary result presentation method today. 2.2 Diversity evaluation forums The diversity task was introduced at the TREC 2009 web track (Clarke et al. 2010 ). Fifty topics were developed by sampling  X  X  X orso X  X  (i.e. medium popularity) queries from web search logs, and their subtopics were developed using a clustering algorithm so that they cover different intents that are  X  X  X earby in the space of user clicks and reformulations. X  X  The diversity task had a total of 20 runs from 18 different groups. a -nDCG and Intent-Aware Precision were used for evaluating the runs. Also, the TREC blog track used the same diversity metrics for evaluating diversified blog post rankings (Macdonald et al. 2010 ).
At the TREC 2010 web track, a version of ERR-IA was chosen as the primary eval-uation metric for the diversity task (Clarke et al. 2011b ). Again, 50 topics with subtopics were developed and the diversity task received 32 runs from 12 groups. The most recent TREC 2011 web diversity task was also very similar (Clarke et al. 2012 ): 50 topics were created (this time focussing on more  X  X  X bscure X  X  queries) and ERR-IA was continued to be used as the primary evaluation metric. The task received 25 runs from nine groups.
The above three rounds of the TREC diversity task used the ClueWeb09 corpus. The task assumes that all intents are equally likely, and that per-intent relevance is binary. 6 The INTENT task, launched at NTCIR-9 held in 2011, consisted of two subtasks: Subtopic Mining and Document Ranking (Song et al. 2011 ). By mining torso queries from query logs, one hundred Chinese topics (for searching a Chinese web corpus called SogouT ) and another one hundred Japanese topics (for searching the Japanese portion of the ClueWeb09) were created. In Subtopic Mining, participating systems were required to submit a ranked list of subtopic strings in response to an input query. The submitted subtopic strings were pooled and manually clustered to form a set of intents for that query. Furthermore, ten assessors were hired to vote on the importance of each intent, and the votes were used to estimate the intent probabilities. The same topics, intents and the intent probabilities were used to evaluate the Document Ranking runs (i.e. diversified search results) using intent recall, D-nDCG and a linear combination of the two metrics, namely, D ] -nDCG. 24 runs from seven groups were submitted to Chinese Document Ranking; 15 runs from three groups were submitted to Japanese Document Ranking. For document relevance assessments, two assessors were hired for each topic, who provided per-intent graded relevance assessments on a tertiary scale: nonrelevant, relevant and highly relevant. Then these labels were consolidated across the two assessors to form a five-point relevance scale: from L 0 (judged nonrelevant) to L 4 (highest relevance).

Although not evaluation forums per se, diversity evaluation is also intensively discussed at the Diversity in Document Retrieval workshops. 7 3 Data and metrics In the present study, we mainly use the NTCIR-9 INTENT Chinese and Japanese docu-ment ranking test collections and their runs (Song et al. 2011 ). Some statistics of the data are shown in Table 1 a, b. As was mentioned in Sect. 1 , these NTCIR test collections differ from the TREC diversity test collections in that they contain the intent popularity infor-mation, per-intent graded relevance assessments and topic sets that are twice as large. To examine the effects of these features, we shall create simplified versions of the NTCIR collections by removing the intent popularity information and graded relevance assess-ments, and also create reduced topic sets . Where applicable, we shall conduct some additional experiments using the graded-relevance version of the TREC 2009 web diversity task data which we obtained from Sakai and Song ( 2011 ): statistics are given in Table 1 c. Note that we refer to the TREC subtopics as  X  X  X ntents X  X  in this paper, to be consistent with the NTCIR terminology.

Another important difference between the current practices at the TREC web diversity task and those at NTCIR INTENT is that while the former uses a version of ERR-IA as the primary evaluation metric, the latter uses the D ] framework. Hence, the present study shall metrics in question.

Intent recall (I-rec), also known as subtopic recall (Zhai et al. 2003 )or cluster recall (Sanderson et al. 2010 ), is the number of different intents covered by a search output divided by the total number of possible intents. In this paper, as we are interested in diversifying the first SERP, we use the document cutoff of 10 for I-rec and for all other evaluation metrics 8 .
 Let Pr ( i | q ) denote the probability of intent i given query q , where let g i ( r ) denote the  X  X  X ocal X  X  gain value for the document at rank r with respect to intent i : throughout this study, we let the local gain value be 1, 2, 3, and 4 for per-intent relevance levels L 1, L 2, L 3 and L 4, respectively. In the D ] framework, the global gain for the document at rank r is defined as GG ( r ) = define D-measures , such as D-nDCG : sorting all documents in descending order of the global gain, and l is the document cutoff, which in our case is 10. Note that exactly one (globally) ideal list is defined for a given topic.

In the D ] framework, D-measure values (representing the overall relevance of a SERP) are plotted against I-rec (representing the diversity of a SERP). In addition, a simple single-value metric that combines the two axes can be computed, e.g.: where c is a parameter, set to 0.5 throughout this study. D ] -nDCG is generally not so sensitive to the choice of c as D-nDCG and I-rec are already highly correlated (Sakai and Song 2011 ).

Next, we define a version of ERR-IA as implemented in the NTCIREVAL toolkit. 9 (All other metrics are also computed using NTCIREVAL in this study.) Let Pr i ( r ) denote the relevance probability of a document at rank r with respect to intent i : in this study, we let following the aforementioned gain value setting for D-nDCG. 10 The normalised version of  X  X  X ocal X  X  ERR can be expressed as: where Pr i  X  r  X  is the relevance probability of a document at rank r in an ideal ranked list for intent i . Note that this  X  X  X ocally ideal X  X  list needs to be defined for each intent .
Finally, our Intent-Aware nERR is defined as: Note that while nERR is a properly normalised metric, nERR-IA is an undernormalised metric: the maximum value reachable is usually less than one as a single ranked list is highly unlikely to be locally ideal for every intent (Sakai and Song 2011 ). This formulation of nERR-IA is slightly different from that described by Clarke et al. ( 2011a ) but serves our purpose: normalisation is expected to slightly improve the discriminative power of the raw ERR-IA (Sakai and Song 2011 ) and it does not affect the concordance test as it compares two ranked lists per topic .

As we have mentioned in Sect. 2.1 , Chapelle et al. ( 2011 ) showed that ERR-IA and a -nDCG are members of a family of metrics called Intent-Aware Cascade-Based Metrics (CBM-IAs). A CBM-IA discounts returned relevant documents based on relevant docu-ments already seen for each intent: by encouraging  X  X  X ovelty X  X  for each intent, it encourages diversity across the intents. On the other hand, D-nDCG is an overall relevance metric that aggregates intent probabilities and per-intent graded relevance, and does not explicitly encourage diversity. This is why I-rec, a pure diversity metric, is used to compute the summary metric D ] -nDCG. As was mentioned earlier, note that D-nDCG is plotted against I-rec at NTCIR to see whether participating systems are relevance-oriented or diversity-oriented. [This is similar to the practice at TREC where precision is plotted against I-rec (Clarke et al. 2010 ).] Put another way, while the relevance and diversity features are elegantly embedded in a CBM-IA, D ] -nDCG enables isolation of these properties when evaluating the participating runs.

Table 2 a, b shows the Kendall X  X  s and symmetric s ap values (Yilmaz et al. 2008 ) for all pairs of metrics when the NTCIR-9 INTENT runs are ranked using the official data, with intent probabilities and per-intent graded relevance. s ap is similar to s but is by design more sensitive to the changes near the top ranks. Table 2 c shows similar results for TREC 2009 with per-intent graded relevance data (but with uniform intent probabilities). It can be observed, for example, that D-nDCG and nERR-IA produce similar run rankings even though they are based on quite different principles. For example, the s between the two metrics with the INTENT Chinese runs is .942. What we are interested in, however, is when these metrics disagree with each other, as we shall discuss below. 4  X  X  X ntuitiveness X  X  of metrics 4.1 Concordance tests To discuss which diversified runs are better than others, TREC primarily uses a version of ERR-IA, while NTCIR uses the D ] framework. Diversity metrics try to balance relevance and diversity for ranked retrieval and inevitably tend to be complex, which makes it particularly hard for researchers to discuss which metrics are  X  X  X easuring what we want to measure. X  X  To address this problem, Sakai ( 2012 ) proposed a simple method for quanti-fying  X  X  X hich metric is more intuitive than the other. X  X 
The concordance test algorithm (Sakai 2012 ) is shown in Fig. 1 . The algorithm com-putes relative concordance scores for a given pair of metrics M 1 and M 2 (e.g. nERR-IA and D ] -nDCG) and a gold-standard metric M GS which should represent a basic property that we want the candidate metrics to satisfy. For the purpose of our study, we consider the following three simple set retrieval metrics as the gold standards: I-rec Intent recall, which represents the ability of a metric to reward diversity; Prec Precision, which represents the ability of a metric to reward relevance (where a PMP Precision for the Most Popular intent, which represents the ability of a metric to
Note that none of these gold standards is sufficient as a stand-alone diversity metric: for example, none of them takes document ranks and graded relevance into account. The more complex diversity metrics.

The algorithm shown in Fig. 1 obtains all pairs of ranked lists for which M 1 and M 2 counts how often each metric agrees with the gold standard metric. In this way, we can discuss which of the two metrics is the more  X  X  X ntuitive. X  X  Moreover, as we can argue that an ideal diversity metric should be consistent with all of the above three gold standards, we will also extend the algorithm in Fig. 1 and count how often a candidate metric agrees with all three gold standards at the same time .

Note that we are treating I-rec as one of the gold-standard metrics in the concordance tests, as no other metric better represents diversity. Because D ] -nDCG directly depends on I-rec ( 2 ), it is no surprise that it agrees very well with I-rec. However, we have much more informative findings, as we shall discuss below.

Table 3 shows the results of our concordance tests for the two NTCIR data sets. For example, Table 3 a(I) shows that, of all the ranked list pairs from the INTENT Chinese runs (there are 100 topics times 24*23/2 run pairs = 27,600 ranked list pairs in total), D-nDCG and nERR-IA disagree with each other for 4,320 pairs; D-nDCG agrees with I-rec for 54 % of these pairs while nERR-IA agrees with I-rec for 74 %; and this difference is statistically significant at a = 0.01 according to a two-sided sign test. These results suggest that nERR-IA maybe a more diversity-oriented metric than D-nDCG is (although it turns out that this does not hold for the TREC data, as discussed in Sect. 4.2 ). However, within the same setting where I-rec is used as the gold standard, D ] -nDCG far outperforms nERR-IA as the former directly depends on I-rec. From from the table, we can observe that: (I) In terms of the ability to reward diversity (as measured by agreement with I-rec), D ] -(II) In terms of the ability to reward relevance (as measured by agreement with Prec), (III) In terms of the ability to emphasise a popular intent (as measured by agreement with (IV) In terms of the agreement with all three gold standards, D ] -nDCG significantly
Note, in particular, the results in Table 3 (IV): not only D ] -nDCG, but also D-nDCG (which does not directly depend on I-rec) significantly outperforms nERR-IA as a metric that emphasises diversity, overall relevance and the relevance to the most popular intent at the same time. These results are consistent across the two data sets, which strongly suggests that the D ] framework offers more  X  X  X ntuitive X  X  evaluation than nERR-IA, provided that we accept the three gold standards as representative of the desirable properties in diversity metrics. 4.2 Concordance tests: additional TREC results Is the above claim about the  X  X  X ntuitiveness X  X  of metrics too strong? Do the results gener-alise to non-NTCIR data? To address these concerns, we conducted similar concordance tests with the graded-relevance version of the TREC 2009 web diversity data. Because this data set lacks the intent popularity information, we cannot use PMP as a gold standard metric. Hence we rely only on I-rec and Prec (i.e. diversity and relevance).

Table 4 shows the concordance results for TREC. It can be observed that the results are generally consistent with the NTCIR ones: D ] -nDCG is the most diversity-oriented metric (Table 4 (I)); D-nDCG is the most relevance-oriented metric (Table 4 (II)); and as a metric that rewards both diversity and relevance, D ] -nDCG is the clear winner (Table 4 (III)). Note, in particular, that not only D ] -nDCG but also D-nDCG far outperforms nERR-IA in Table 4 (III), by agreeing with both I-rec and Precision far more often than nERR-IA does. The only pairwise comparison that lacks statistical significance is that for D-nDCG and nERR-IA when I-rec is used as the gold standard: they perform comparably here, in contrast to our NTCIR results where nERR-IA significantly outperformed D-nDCG with I-rec as the gold standard.

It is generally recommended to evaluate systems with multiple evaluation metrics and thereby examine them from different perspectives. Thus we are not necessarily looking for a  X  X  X ne and only X  X  evaluation metric. However, it would be fair to say that, based on our results, the introduction of the D ] framework at NTCIR was certainly worthwhile. 5 Graded relevance and intent popularity 5.1 Simplified test collections We now examine the effects of incorporating per-intent graded relevance and intent popularity in diversity evaluation. In order to do this, we created four simplified versions of each NTCIR-9 INTENT test collection: popularity 1 binary The per-intent graded relevance assessments are collapsed into uniform 1 graded The intent popularity information is dropped and it is assumed uniform 1 binary Combination of the above two, which mimics the TREC diversity linear 1 graded Instead of using the absolute intent probabilities estimated We compare these four variants with each of the original (or popularity 1 graded ) NTCIR-9 INTENT test collection in terms of discriminative power as well as changes in the system ranking.

Given a test collection with a set of runs, discriminative power is measured by con-significant differences for a fixed confidence level. While the original discriminative power method relied on the pairwise bootstrap test (Sakai 2006 ), any pairwise test inevitably results in a family-wise error rate of 1 -(1 -a ) k where a is the probability of Type I Error for each pairwise test and k is the total number of run pairs. We therefore use a randomised version of the two-sided Tukey X  X  Honestly Significant Differences test which takes the entire set of runs into account (Carterette 2012 ). This test is naturally more conservative than pairwise significance tests that disregard all other available runs, but the relative discriminative powers of different metrics remain similar (Sakai 2012 ). We create 1,000 randomly permutated topic-by-run matrices (Carterette 2012 ; Sakai 2012 ) for esti-mating Achieved Significance Levels (ASLs, a.k.a. p values).

Discriminative power is a measure of reliability, or how a metric can consistently provide the same conclusion regardless of the topic set used in the experiment. Note that we are interested in metrics that are strictly functions of a ranked list of items (i.e. system output) and a set of judged items (i.e. right answers); we are not interested in a  X  X  X etric X  X  that knows one ranked list is from (say) Bing and the other is from Google and uses this knowledge to prefer one list over the other. Also, it should be stressed that discriminative power does not tell us whether a metric is right or wrong: that is why we have also discussed the concordance test earlier.
 Figure 2 shows the ASL curves (Sakai 2006 ) with the original NTCIR-9 INTENT Chinese and Japanese data for D ] -nDCG, D-nDCG, nERR-IA and I-rec. The axes rep-resent the ASLs and the run pairs sorted by ASLs, respectively, and curves that are closer to the origin represent more reliable metrics. Figure 3 shows similar graphs with the uniform 1 binary (i.e. TREC-like) setting. (I-rec is omitted here as it is not affected by test collection simplification.) It can be observed that nERR-IA underperforms D ] -nDCG and D-nDCG in all cases.

Table 5 summarises the results of our discriminative power experiments with the official and simplified data sets, for a = 0.05. Parts (I) and (IV) correspond to Figs. 2 and 3 , respectively. For example, Table 5 a(I) shows that D ] -nDCG manages to detect 140 significant differences out of 276 comparisons (50.7 %), and that, given 100 topics, an absolute difference of around 0.07 is usually statistically significant (a conservative esti-mate). The results can be summarised as follows: 1. In all settings with the two data sets, nERR-IA consistently underperforms D ] -nDCG 2. By comparing the results in (I) with those in (IV), it can be observed that introducing 3. At least for D ] -nDCG, both intent popularity and graded relevance appear to 4. By comparing (I) with (V), we can observe that the effect of transforming the original Based on Finding (1), it is probably fair to conclude that D ] -nDCG is superior to nERR-IA not only in terms of the concordance test but also in terms of discriminative power. Furthermore, Findings (2) and (3) suggest that D ] -nDCG fully utilises the intent popularity and graded relevance information.

Table 6 shows the s and symmetric s ap values when the system rankings using the simplified test collections are compared with the original ranking (i.e. popularity 1 graded ). While the general picture is that all of the reduced data provide system rankings that are very similar to the original ranking, there are a few more interesting observations. First, while dropping the graded relevance information seems to affect D  X  ]  X  -nDCG more than it affects nERR-IA (see the popularity 1 binary column), dropping the intent popularity information seems to affect nERR-IA more than it affects D  X  ]  X  -nDCG (see the uniform 1 graded column). Second, and perhaps more importantly, the linear 1 graded column shows that transforming the absolute intent probabilities into relative values have very little impact on system ranking. Along with Finding (4) mentioned above, this result seems to support our hypothesis that relative intent popularity may do just as well as absolute intent popularity in diversity evaluation, and therefore that accurate estimation of intent probabilities may not be necessary.
 5.2 Simplified test collections: additional TREC results For completeness, this section uses the TREC 2009 diversity data to back up the results with the simplified NTCIR data, which we discussed in Sect. 5.1 Recall that the original TREC 2009 diversity test collection lacks the intent probabilities and per-intent graded relevance assessments, and therefore that it resembles the uniform 1 binary data we constructed in the last section. Moreover, the graded relevance version of the same test collection (Sakai and Song 2011 ) corresponds to the uniform 1 graded data.

Table 7 shows the discriminative power results for the above two sets of TREC data: note that it corresponds to Parts (III) and (IV) of Table 5 13 . The TREC results are in line with Finding (1) in Sect. 5.1 : nERR-IA is the least discriminative metric, although in this case the difference between nERR-IA and D-nDCG is negligible. As for the differences between uniform 1 graded and uniform 1 binary , the results are inconclusive. On the other hand, note that, while the discriminative power values in Table 7 are similar to the NTCIR results shown in Table 5 , the performance differences  X  D  X  required to achieve a significant difference at a are much higher for the TREC case. This is mainly due to the difference in topic set size, which is the subject of the next section.

Table 8 shows the s and s ap rank correlations between the uniform 1 binary ranking and the uniform 1 graded ranking for each metric. Similar to the popularity 1 binary column of Table 6 (where the simplified setting is compared with the official popularity 1 graded ), dropping the graded relevance information affects D-nDCG most ( s = .933), and nERR-IA least ( s = .987).
 6 Topic set size 6.1 Reduced topic sets In this section, we discuss the gap in terms of the topic set size between the TREC web diversity task and the NTCIR-9 INTENT task: the former uses 50 topics every year, while the latter used 100 topics for Chinese and another 100 for Japanese. As we have seen in Table 1 , the INTENT Chinese topic set contains 860 intents, while the Japanese set contains 1,016 intents. Assessing per-intent relevance for such data is no easy task. Could the NTCIR organisers have used 50 topics just like TREC and yet obtained similar evaluation results? To address the above research question, we created reduced topic sets from the INTENT Chinese and Japanese topic sets, so that each set contains 90, 70, 50, 30 and 10 topics, respectively. To consider the worst case where the topics included in the set happen variance in D ] -nDCG across all runs, and gradually removed topics with the highest variances (i.e. the most informative topics). A similar topic set reduction method was used by Sakai and Mitamura ( 2010 ).

As we have already shown that D ] -nDCG and D-nDCG are superior to nERR-IA in terms of the concordance test and discriminative power, we will focus henceforth on the D ] evaluation framework.

Figures 4 , 5 , 6 show the effect of topic set reduction on the ASL curves for D ] -nDCG and its components, i.e. D-nDCG and I-recall. It can be observed that the discriminative power of each metric steadily declines as the topic set is reduced. (There is an anomaly in the graph for D-nDCG Chinese, where the  X  X 90 topics X  X  curve actually slightly outperforms the original  X  X 100 topics X  X  curve. Similarly, in the graph for D-nDCG Japanese, the  X  X 30 topics X  X  curve slightly outperforms the  X  X 50 topics X  X  curve.) Based on these graphs, Table 9 shows the discriminative power results for a = 0.05. It can be observed that removing just ten topics can result in the loss of some significant differences. (While removing 10 topics that are the least informative may not affect the outcome of significance testing this much, note that we have no established way of knowing in advance which topics are useful.) Moreover, if we compare the  X  X 100 topics X  X  column (i.e. NTCIR setting) with the  X  X 50 topics X  X  column (i.e. TREC setting), it can be observed that the discriminative power is roughly halved if we remove 50 topics: for Chinese, the discriminative power of D ] -nDCG goes down from 50.7 to 27.2 %; for Japanese, it goes down from 39.0 to 16.2 %. These results show that using 100 topics for evaluating the INTENT runs was certainly worth-while in terms of statistical significance testing.
 Statistical significance is always associated with some probabilities of errors, however. Let us therefore disregard statistical significance for a while and focus on the entire system ranking. Can we preserve the official INTENT run rankings using fewer topics?
According our experimental results, the answer to the above question is no . Table 10 shows the s and symmetric s ap values when the system rankings based on the reduced topic sets are compared with the original ranking with the full topic set. It can be observed that the system rankings cannot be preserved even with 90 topics: the results are better for the Japanese case, but recall that the Chinese results should be regarded as more representative as they involved more teams and runs (Table 1 ). It can be observed that the Chinese run ranking in terms of I-rec collapses completely when only 10 topics are available ( s = .058).

Figures 7 and 8 provide more information on the effect of topic set reduction on the system ranking for 90 topics and for 50 topics. For example, Fig. 7 (left) shows that removing 10 topics would swap the top two runs, and that the run officially ranked number 5 will go down to rank 11. This also suggests that the construction of 100 topics for each language at NTCIR was well worth the effort. Moreover, note that with 50 topics, the system rankings are very different. For example, the s for I-rec with Chinese rankings (100 topics vs. 50) is below 0.6; see also the wild disagreements in Figs. 7 and 8 (right). This suggests that, if the TREC diversity task also used 100 topics in one round, the run rankings might have been substantially different from those that have been officially announced. 14
In short, using more topics pays, both from the viewpoint of significance testing and that of obtaining a reliable system ranking. This is of course in line with literature in traditional IR, e.g. Carterette and Smucker ( 2007 ), Sanderson and Zobel ( 2005 ), Webber et al. ( 2008 ), but to our knowledge, no other studies have looked at this issue for diversity evaluation. 6.2 Reduced topic sets: additional TREC results For completeness, we finally report on a topic set reduction experiment with the TREC 2009 diversity data with graded relevance. As the test collection has only 50 topics, we constructed reduced topics sets of 30 and 10 topics using the variance-based method we described in Sect. 6.1
Figures 9 , 10 , 11 show the effect of topic set reduction on the ASL curves for D ] -nDCG and its components, which should be compared with Figs. 4 , 5 , 6 . Because D-nDCG loses its discriminative power very rapidly with this data set, its ASL curve for 10 topics is not visible in Fig. 10 . Based on the ASL curves, Table 11 shows the discriminative power results for a = 0.05, in a way similar to Table 9 . Again, it can be observed that D-nDCG loses its discriminative power very rapidly here: while its discriminative power for the NTCIR data with 30 topics is 22.5 % for Chinese and 11.4 % for Japanese (Table 9 ), that for the TREC data with 30 topics is only 4.3 %. This suggests that the TREC runs are relatively similar to each other in terms of the ability to return relevant documents at least for that particular reduced topic set. However, the overall results are consistent with the NTCIR ones: losing 20 topics means losing a large number of significant differences.
Finally, Table 12 a shows how topic set reduction affects the TREC run ranking in terms of s and s ap : note that, unlike Table 10 , the baseline ranking is based on 50 topics rather than 100. Thus, for reference, Table 12 b,c shows similar results with the NTCIR data, where the rankings based on 50 topics are treated as baselines. It can be observed that, as before, topic set reduction gradually and surely destroys the  X  X  X riginal X  X  ranking. The I-rec ranking for the Chinese INTENT runs is completely different even from the ranking based on 50 topics ( s = .051). 7 Conclusions and future work In contrast to traditional IR evaluation, diversity evaluation has only a few years of history. In this study, we examined some features of the NTCIR-9 INTENT Document Ranking task that differ from those of the TREC web track diversity task, namely, the use of the D ] evaluation framework, intent popularity, per-intent graded relevance, and 100 topics per language.

Our main experimental findings are: 1. The D ] evaluation framework used at NTCIR provides more  X  X  X ntuitive X  X  and 2. Utilising both intent popularity and per-intent graded relevance as is done at NTCIR 3. Reducing the topic set size, even by just 10 topics, can affect not only significance These results suggest that the directions being explored at NTCIR are valuable.

As diversity evaluation is still in its infancy, it would probably be prudent at this stage evaluation forums. 15 While the present study highlighted the benefits of the new features of the NTCIR INTENT task as compared to the TREC web diversity task, TREC also has some features that NTCIR currently lacks. Perhaps the most important are the ambiguous and faceted tags for the topics and the navigational and informational tags for the intents (subtopics). Clarke et al. ( 2009 ) have discussed the explicit use of the ambiguous and faceted tags for diversity evaluation, while Sakai ( 2012 ) has proposed to utilise the nav-igational and informational tags. Whether these new approaches will bring benefit to evaluation forums like TREC and NTCIR is yet to be verified.

Another important topic that we did not cover in this study is the effect of defining the intent sets for each topic on the evaluation outcome. For example, while the INTENT Chinese and Japanese topic sets had 20 equivalent topics, the intents identified (through the Subtopic Mining subtask) are quite different across the two languages (Song et al. 2011 ). Moreover, it is always difficult to define what the  X  X  X ight X  X  granularity of an intent/subtopic should be. This research question could be addressed to some extent by devising multiple intent sets for the same topic set and then comparing the evaluation results. A more challenging question would be: can we evaluate diversified IR without explicitly defining intents? For example, can we design intent-free, nugget-based evaluation approaches (Pavlu et al. 2012 ; Sakai et al. 2011 ) for diversity evaluation?
Finally, although we did discuss the  X  X  X ntuitiveness X  X  of evaluation metrics by means of the concordance test, we do not deny that real users are missing in this study (although the topics were mined from real user queries and at least some of the intents arise from user session and clickthrough data). Clickthrough-based and user-based verification of diversity evaluation metrics would be useful complements to our work.
 References
